{"id": "40572657", "url": "https://en.wikipedia.org/wiki?curid=40572657", "title": "2MASS 1503+2525", "text": "2MASS 1503+2525\n\n2MASS J15031961+2525196 (2MASS 1503+2525) is a nearby brown dwarf of spectral type T5.5, located in constellation Boötes at approximately 20.7 light-years from Earth.\n\n2MASS 1503+2525 was discovered in 2003 by Adam J. Burgasser \"et al.\" in wide-field search for T dwarfs using the Two Micron All Sky Survey (2MASS).\n\nCurrently the most precise distance estimate of 2MASS 1503+2525 is a trigonometric parallax, published by Dupuy and Liu in 2012: 157.2 ± 2.2 mas, corresponding to a distance 6.36 ± 0.09 pc, or 20.7 ± 0.3 ly.\n\n2MASS 1503+2525 distance estimates\n"}
{"id": "39723835", "url": "https://en.wikipedia.org/wiki?curid=39723835", "title": "Atmospheric optics ray-tracing codes", "text": "Atmospheric optics ray-tracing codes\n\nAtmospheric optics ray tracing codes - this article list codes for light scattering using ray-tracing technique to study atmospheric optics phenomena such as rainbows and halos. Such particles can be large raindrops or hexagonal ice crystals. Such codes are one of many approaches to calculations of light scattering by particles.\n\nRay tracing techniques can be applied to study light scattering by spherical and non-spherical particles under the condition that the size of a particle is much larger than the wavelength of light. The light can be considered as collection of separate rays with width of rays much larger than the wavelength but smaller than a particle. Rays hitting the particle undergoes reflection, refraction and diffraction. These rays exit in various directions with different amplitudes and phases. Such ray tracing techniques are used to describe optical phenomena such as rainbow of halo on hexagonal ice crystals for large particles. \nReview of several mathematical techniques is provided in series of publications.\n\nThe 46° halo was first explained as being caused by refractions through ice crystals in 1679 by the French physicist Edmé Mariotte (1620–1684) in terms of light refraction \nJacobowitz in 1971 was the first to apply the ray-tracing technique to hexagonal ice crystal. Wendling et al. (1979) extended Jacobowitz's work from hexagonal ice particle with infinite length to finite length and combined Monte Carlo technique to the ray-tracing simulations. \n\nThe compilation contains information about the electromagnetic scattering by hexagonal ice crystals, large raindrops, and relevant links and applications.\n\n\n\n"}
{"id": "972894", "url": "https://en.wikipedia.org/wiki?curid=972894", "title": "Beakman's World", "text": "Beakman's World\n\nBeakman's World was an American educational children's television show. The program is based on the Universal Press Syndicate syndicated comic strip \"You Can with Beakman and Jax\" created by Jok Church. The series premiered on Wednesday, September 16, 1992, on TLC, and on various other channels a few days later through syndication on 220 other channels.\n\nOn September 18, 1993, it moved from national syndication to CBS's Saturday morning children’s lineup. At the peak of its popularity, it was seen in nearly 90 countries around the world. The series was canceled in 1998. Reruns returned to national syndication in September 2006, after which it was transferred to local stations such as KICU. The show debuted a year prior to \"Bill Nye the Science Guy\", which covered similar topics. The show's host, Paul Zaloom, still performs as Beakman in live appearances around the globe.\n\nThe program starred Paul Zaloom as Beakman, an eccentric scientist who performed comical experiments and demonstrations in response to viewer mail to illustrate various scientific concepts from density to electricity to flatulence. When his experiments were successful, he would often exclaim \"Zaloom!\", referring to his last name.\n\nOver the years, Beakman was aided in his experiments by a female assistant just as in the comic strip on which it was based. The assistant's name changed throughout the show's run; for the episodes of season 1, it was Josie (played by Alanna Ubach); for the episodes of seasons 2 and 3, it was Liza (played by Eliza Schneider); and for the episodes of season 4, it was Phoebe (played by Senta Moses). Beakman was also assisted by his \"lab rat\" Lester. In the pilot episode, Lester was a puppet, but in every subsequent episode he was simply a clueless, crude man (Mark Ritts) in a tattered rat suit. In a running joke, it was sometimes implied that his character was actually supposed to be a rat, particularly in moments where he would appear to be in pain because someone was standing on his tail, because he was being tickled, something was on his prosthetic nose, etc. Just as frequently, however, he was specifically identified by himself and others as a guy in a rat suit, or as a serious actor with a bad agent. Frequently unwilling to help with challenges or other segments, Lester was often persuaded by Beakman with the promise of food. \n\nAnother occasional cast member is the unseen cameraman \"Ray\", who is played by prop-master Ron Jancula's hands. Ray assists Beakman by handing him various items, such as the \"boguscope\". It is suggested throughout the program that Ray has a romantic crush on the show's unnamed make-up lady. Actress Jean Stapleton also appeared on the show as Beakman's mother, \"Beakmom\". In some of the skits during the show the character Professor I. M. Boring (also played by Paul Zaloom, in a dual role) makes appearances and talks about various science topics. \n\nZaloom also appeared as various \"guest scientists\" and historic figures, such as Thomas A. Edison, Robert H. Goddard and Philo T. Farnsworth. When Senta Moses was added to the show's cast, the producers began to use a majority of the sound effects from the NBC game show \"Scrabble\".\n\nOne segment of the show was the famed \"Beakman Challenge\". During this segment, Beakman would challenge Lester to do a stunt that illustrated a basic scientific feat. During the first season, virtually every challenge related to either air pressure or Bernoulli's principle. The show addressed this during the second season, by having Lester exclaim to Beakman (as he was explaining the science behind a trick) \"AIR PRESSURE! IT'S ALWAYS AIR PRESSURE!\" In later episodes, the rest of the cast would sometimes have their turn to perform a \"Beakman Challenge\" under their own name (e.g. \"The Lester Challenge\" or \"The Liza Challenge\", etc.) and challenge Beakman to accomplish the feat.\n\nDuring the show, the following verbal warning was given: \"Any experiment performed at home should be done with adult supervision and all appropriate safety precautions should be taken. All directions should be followed exactly and no substitutions should be used.\"\n\nAt the beginning and end of the show, as well as before or after commercial breaks, the show featured short scenes portraying puppet penguins, Don (voiced by Bert Berdis) and Herb (voiced by Alan Barzman), at the South Pole watching \"Beakman's World\" on television. The penguins were named after Don Herbert, who starred as Mr. Wizard in \"Mr. Wizard's World\". Mark Ritts (Lester) was also one of the puppeteers operating the penguins.\n\n\"Beakman's World\" plays in weekend syndication in the United States and in several other countries. It is distributed by Sony Pictures Television in the U.S. and in other countries.\n\nThe \"Beakman's World\" theme song was composed by Mark Mothersbaugh of Devo fame. The \"Beakman's World\" theme is an amalgam of Zydeco and Synthpop. An accordion is used for its main riff. The song also prominently features a wide array of wacky sound effects.\n\nOn September 7, 2004, a DVD entitled \"The Best of Beakman's World\" was released. This DVD is a direct transfer of the VHS tape of the same name, and features only experiments and segments taken from \"The Beakman Challenge\". There have yet to be any full-episode releases on VHS or DVD.\n\nAll 4 seasons were available on Netflix with the exception of the following five episodes: 9 (1-9), 24 (1-24), 31 (2-5), 51 (2-25) and 66 (4-1), as noted in the chart above. Their streaming license ended on September 30, 2014, and the content was removed from their site. Beakman's World returned to television on MeTV beginning on October 2, 2016, showing two episodes every Sunday, followed by an hour of Bill Nye the Science Guy.\n\nIn 1998, the Cincinnati Museum Center at Union Terminal opened an interactive exhibit called \"Beakman's World On Tour\", based on the television show. The exhibit toured dozens of cities in the United States.\n\n\"Beakman’s World\" was nominated for and won numerous awards:\n\n"}
{"id": "30818338", "url": "https://en.wikipedia.org/wiki?curid=30818338", "title": "Children and Libraries", "text": "Children and Libraries\n\nChildren and Libraries is a triannual peer-reviewed academic journal covering library services to children. It is the official journal of the Association of Library Service to Children. It was established in 2003 and succeeds the \"Journal of Youth Services\" (formerly \"Top of the News\"), which was published until 2002 in collaboration with the Young Adult Library Services Association.\n\n"}
{"id": "29900123", "url": "https://en.wikipedia.org/wiki?curid=29900123", "title": "Chronosequence", "text": "Chronosequence\n\nA chronosequence (in forest sciences) is a set of forested sites that share similar attributes but are of different ages. Since many processes in forest ecology take a long time (decades or centuries) to develop, chronosequence methods are used to represent and study the time-dependent development of a forest. Field data from a chronosequence can be collected in a short period of several months. For example, chronosequences are often used to study the changes in plant communities during succession. A classic example of using chronosequences to study ecological succession is in the study of plant and microbial succession in recently deglactiated zones. For example, a study from 2005 used the distance from the nose of a glacier as a proxy for site age.\n\nA common assumption in establishing chronosequences is that no other variable besides age (such as various abiotic components and biotic components) has changed between sites of interest. Because this assumption cannot always be tested for environmental study sites, the use of chronosequences in field successional studies has recently been debated.\n"}
{"id": "47058285", "url": "https://en.wikipedia.org/wiki?curid=47058285", "title": "DENIS@Home", "text": "DENIS@Home\n\nDENIS@Home is a distributed computing project hosted by San Jorge University and running on the Berkeley Open Infrastructure for Network Computing (BOINC) software platform. The primary goal of DENIS@Home is to compute large amounts of cardiac electrophysiological simulations.\n\nDENIS@Home was initially released on March 20, 2015. Since then, it has been developed by a team of three people aided by four undergraduate students. All members of the development team are a part of the Biomedical Signal Interpretation and Computational Simulation research group.\n\n\n"}
{"id": "43241407", "url": "https://en.wikipedia.org/wiki?curid=43241407", "title": "Deinococcus indicus", "text": "Deinococcus indicus\n\nDeinococcus indicus is a species of arsenic-resistant bacterium. It is Gram-negative, rod-shaped, non-motile, non-sporulating and red-pigmented, with type strain Wt/1a (=MTCC 4913 =DSM 15307).\n\n\n"}
{"id": "51449624", "url": "https://en.wikipedia.org/wiki?curid=51449624", "title": "E7ham Arivu", "text": "E7ham Arivu\n\nEzham Arivu: Search for Tamil Nadu’s Young Scientist () is a 2016-2017 Tamil science reality show which aired on DD Podhigai from 18 August 2016 to 11 August 2017 on Monday to Friday at 7:30PM (IST) for 172 Episodes. The show is hosted by Antony Raj. The motto of the show is a search for Tamilagathin Sirantha Illam Vigyanikana Thedal (Tamil Nadu’s Young Scientist).\n\nThe show concept was designed and created by Agnishwar Jayaprakash to pay tribute to the late Dr. APJ Abdul Kalam, Hon. Former President of India whose dream was to enlighten students’ mind through innovation.\n\nThe show is a tailor made to showcase the different and diverse scientific innovations of the students at the same time the program is value added with inspiring background stories and preparatory clips of students creating the inventions. \n\nThis show is aimed at bringing out the young scientist who thinks out of the box, by serving as an ideal platform to exhibit their talent. The show is a tribute to the late Dr. APJ Abdul Kalam, Hon. Former President of India whose dream was to enlighten students’ mind through innovation. \n\n\n\n\n\n"}
{"id": "28066721", "url": "https://en.wikipedia.org/wiki?curid=28066721", "title": "Edmond Modeste Lescarbault", "text": "Edmond Modeste Lescarbault\n\nEdmond Modeste Lescarbault (1814, Châteaudun - 1894), was a French doctor and an amateur astronomer, best remembered for his 1859 supposed observation of the non-existent planet Vulcan.\n\nHe graduated and obtained his diploma in 1848. He then started to work as a doctor in Orgères-en-Beauce and worked there until 1872 (the street where he worked is now named after him). A keen astronomer, he built an observatory with a 3.75 inches (95 mm) refractor by his house and began correspondence with various scientific societies. On 26 March 1859 he saw a small object transiting the Sun and having heard of Le Verrier's theory of an intramercurial planet named Vulcan, he wrote a letter to the astronomer and was consequently visited by him in December 1859. Le Verrier announced the discovery on 2 January 1860. Lescarbault became Chevalier of the Légion d'honneur and was invited to appear before numerous learned societies.\n\nHis manuscripts, including correspondence with Camille Flammarion, are kept in Bibliothèque Municipale in Châteaudun. He died in 1894.\n\n\n"}
{"id": "522062", "url": "https://en.wikipedia.org/wiki?curid=522062", "title": "Engineering tolerance", "text": "Engineering tolerance\n\nEngineering tolerance is the permissible limit or limits of variation in:\nDimensions, properties, or conditions may have some variation without significantly affecting functioning of systems, machines, structures, etc.\nA variation beyond the tolerance (for example, a temperature that is too hot or too cold) is said to be noncompliant, rejected, or exceeding the tolerance.\n\nA primary concern is to determine how wide the tolerances may be without affecting other factors or the outcome of a process. This can be by the use of scientific principles, engineering knowledge, and professional experience. Experimental investigation is very useful to investigate the effects of tolerances: Design of experiments, formal engineering evaluations, etc.\n\nA good set of engineering tolerances in a specification, by itself, does not imply that compliance with those tolerances will be achieved. Actual production of any product (or operation of any system) involves some inherent variation of input and output. Measurement error and statistical uncertainty are also present in all measurements. With a normal distribution, the tails of measured values may extend well beyond plus and minus three standard deviations from the process average. Appreciable portions of one (or both) tails might extend beyond the specified tolerance.\n\nThe process capability of systems, materials, and products needs to be compatible with the specified engineering tolerances. Process controls must be in place and an effective Quality management system, such as Total Quality Management, needs to keep actual production within the desired tolerances. A process capability index is used to indicate the relationship between tolerances and actual measured production.\n\nThe choice of tolerances is also affected by the intended statistical sampling plan and its characteristics such as the Acceptable Quality Level. This relates to the question of whether tolerances must be extremely rigid (high confidence in 100% conformance) or whether some small percentage of being out-of-tolerance may sometimes be acceptable.\n\nGenichi Taguchi and others have suggested that traditional two-sided tolerancing is analogous to \"goal posts\" in a football game: It implies that all data within those tolerances are equally acceptable. The alternative is that the best product has a measurement which is precisely on target. There is an increasing loss which is a function of the deviation or variability from the target value of any design parameter. The greater the deviation from target, the greater is the loss. This is described as the Taguchi loss function or \"quality loss function\", and it is the key principle of an alternative system called \"inertial tolerancing\".\n\nResearch and development work conducted by M. Pillet and colleagues at the Savoy University has resulted in industry-specific adoption. Recently the publishing of the French standard NFX 04-008 has allowed further consideration by the manufacturing community.\n\nDimensional tolerance is related to, but different from fit in mechanical engineering, which is a \"designed-in\" clearance or interference between two parts. Tolerances are assigned to parts for manufacturing purposes, as boundaries for acceptable build. No machine can hold dimensions precisely to the nominal value, so there must be acceptable degrees of variation. If a part is manufactured, but has dimensions that are out of tolerance, it is not a usable part according to the design intent. Tolerances can be applied to any dimension. The commonly used terms are:\n\n\nFor example, if a shaft with a nominal diameter of 10 mm is to have a sliding fit within a hole, the shaft might be specified with a tolerance range from 9.964 to 10 mm (i.e. a zero fundamental deviation, but a lower deviation of 0.036 mm) and the hole might be specified with a tolerance range from 10.04 mm to 10.076 mm (0.04 mm fundamental deviation and 0.076 mm upper deviation). This would provide a clearance fit of somewhere between 0.04 mm (largest shaft paired with the smallest hole, called the \"maximum material condition\") and 0.112 mm (smallest shaft paired with the largest hole). In this case the size of the tolerance range for both the shaft and hole is chosen to be the same (0.036 mm), meaning that both components have the same International Tolerance grade but this need not be the case in general.\n\nWhen no other tolerances are provided, the machining industry uses the following standard tolerances:\n\nWhen designing mechanical components, a system of standardized tolerances called International Tolerance grades are often used. The standard (size) tolerances are divided into two categories: hole and shaft. They are labelled with a letter (capitals for holes and lowercase for shafts) and a number. For example: H7 (hole, tapped hole, or nut) and h7 (shaft or bolt). H7/h6 is a very common standard tolerance which gives a tight fit. The tolerances work in such a way that for a hole H7 means that the hole should be made slightly larger than the base dimension (in this case for an ISO fit 10+0.015−0, meaning that it may be up to 0.015 mm larger than the base dimension, and 0 mm smaller). The actual amount bigger/smaller depends on the base dimension. For a shaft of the same size h6 would mean 10+0-0.009, which means the shaft may be as small as 0.009 mm smaller than the base dimension and 0 mm larger. This method of standard tolerances is also known as Limits and Fits and can be found in ISO 286-1:2010 (Link to ISO catalog).\n\nThe table below summarises the International Tolerance (IT) grades and the general applications of these grades:\nAn analysis of fit by Statistical interference is also extremely useful: It indicates the frequency (or probability) of parts properly fitting together.\n\nAn electrical specification might call for a resistor with a nominal value of 100 Ω (ohms), but will also state a tolerance such as \"±1%\". This means that any resistor with a value in the range 99 Ω to 101 Ω is acceptable. For critical components, one might specify that the actual resistance must remain within tolerance within a specified temperature range, over a specified lifetime, and so on.\n\nMany commercially available resistors and capacitors of standard types, and some small inductors, are often marked with coloured bands to indicate their value and the tolerance. High-precision components of non-standard values may have numerical information printed on them.\n\nThe terms are often confused but sometimes a difference is maintained. See Allowance (engineering)#Confounding of the engineering concepts of allowance and tolerance.\n\nIn civil engineering, clearance refers to the difference between the loading gauge and the structure gauge in the case of railroad cars or trams, or the difference between the size of any vehicle and the width/height of doors or the height of an overpass as well as the air draft under a bridge.\n\n\n"}
{"id": "28778853", "url": "https://en.wikipedia.org/wiki?curid=28778853", "title": "Falanja", "text": "Falanja\n\nFalanja (فلنجه) is a red seed (perhaps the seed of cubeb) used in the making of perfumes. It was used to make perfumes by women in the court of Jahangir.\n"}
{"id": "46599616", "url": "https://en.wikipedia.org/wiki?curid=46599616", "title": "Field strength (theoretical physics)", "text": "Field strength (theoretical physics)\n\nIn theoretical physics, field strength is another name for the curvature form. For the electromagnetic field, the curvature form is an antisymmetric matrix whose elements are the electric field and magnetic field: the electromagnetic tensor.\n\n"}
{"id": "7877098", "url": "https://en.wikipedia.org/wiki?curid=7877098", "title": "Fitnah (crater)", "text": "Fitnah (crater)\n\nFitnah is an impact crater in the northern hemisphere of Saturn's moon Enceladus. Fitnah was first observed in \"Cassini–Huygens\" images during that mission's February 2005 flyby of Enceladus. It is located at 45.1° North Latitude, 290.6° West Longitude and is 16.5 kilometers across. The topography of the impact crater appears very subdued, suggesting that the crater has undergone significant viscous relaxation since its formation, leaving behind only the raised rim behind. \n\nFitnah is named after the daughter of Ayyub and brother of Ghanim, in the \"Tale of Ghanim Bin Ayyub, the Distraught, the Thrall O’ Love\" in \"The Book of One Thousand and One Nights\". Incidentally, craters named Ghanim and Ayyub are found near Fitnah.\n"}
{"id": "2771364", "url": "https://en.wikipedia.org/wiki?curid=2771364", "title": "Giuga number", "text": "Giuga number\n\nA Giuga number is a composite number \"n\" such that for each of its distinct prime factors \"p\" we have formula_1, or equivalently such that for each of its distinct prime factors \"p\" we have formula_2. \n\nThe Giuga numbers are named after the mathematician Giuseppe Giuga, and relate to his conjecture on primality.\n\nAlternative definition for a Giuga number due to Takashi Agoh is: a composite number \"n\" is a Giuga number if and only if the congruence\n\nholds true, where \"B\" is a Bernoulli number and formula_4 is Euler's totient function.\n\nAn equivalent formulation due to Giuseppe Giuga is: a composite number \"n\" is a Giuga number if and only if the congruence\n\nand if and only if\n\nAll known Giuga numbers \"n\" in fact satisfy the stronger condition\n\nThe sequence of Giuga numbers begins\n\nFor example, 30 is a Giuga number since its prime factors are 2, 3 and 5, and we can verify that\n\n\nThe prime factors of a Giuga number must be distinct. If formula_8 divides formula_9, then it follows that formula_10, where formula_11 is divisible by formula_12. Hence, formula_13 would not be divisible by formula_12, and thus formula_9 would not be a Giuga number. \n\nThus, only square-free integers can be Giuga numbers. For example, the factors of 60 are 2, 2, 3 and 5, and 60/2 - 1 = 29, which is not divisible by 2. Thus, 60 is not a Giuga number.\n\nThis rules out squares of primes, but semiprimes cannot be Giuga numbers either. For if formula_16, with formula_17 primes, then\nformula_18, so formula_19 will not divide formula_20, and thus formula_9 is not a Giuga number.\nAll known Giuga numbers are even. If an odd Giuga number exists, it must be the product of at least 14 primes. It is not known if there are infinitely many Giuga numbers.\n\nIt has been conjectured by Paolo P. Lava (2009) that Giuga numbers are the solutions of the differential equation \"n' = n+1\", where \"n' \" is the arithmetic derivative of \"n\". (For square-free numbers formula_22, formula_23, so \"n' = n+1\" is just the last equation in the above section \"Definitions\", multiplied by \"n\".)\n\nJosé Mª Grau and Antonio Oller-Marcén have shown that an integer \"n\" is a Giuga number if and only if it satisfies \"n' = a n + 1\" for some integer \"a\" > 0, where \"n' \" is the arithmetic derivative of \"n\". (Again, \"n' = a n + 1\" is identical to the third equation in \"Definitions\", multiplied by \"n\".)\n\n\n"}
{"id": "39637532", "url": "https://en.wikipedia.org/wiki?curid=39637532", "title": "Hermann Friedrich Emmrich", "text": "Hermann Friedrich Emmrich\n\nHermann Friedrich Emmrich (Meiningen, February 7, 1815 – Meiningen, 24 January 1879) was a German geologist.\n\nHe received his Ph.D. in philosophy and taught at the Institute of Meiningen (Henfling-Gymnasium Meiningen).\n\nHe described the trilobite genera \"Phacops\", \"Odontopleura\" and \"Trinucleus\".\n\nHe published \"Zur Naturgeschichte der Trilobiten\" (On the natural history of trilobites) in 1839 and \"Geologischem Geschichte des Alpes\" (Geological history of the Alps) in 1874.\n\nThe trilobite genus \"Emmrichops\" was named in his honor.\n"}
{"id": "18877437", "url": "https://en.wikipedia.org/wiki?curid=18877437", "title": "History and naming of human leukocyte antigens", "text": "History and naming of human leukocyte antigens\n\nHuman leukocyte antigens (HLA) began as a list of antigens identified as a result of transplant rejection. The antigens were initially identified by categorizing and performing massive statistical analyses on interactions between blood types. This process is based upon the principle of serotypes. HLA are not typical antigens, like those found on surface of infectious agents. HLAs are \"allo\"antigens, they vary from individual to individual as a result of genetic differences.\nAn organ called the thymus is responsible for ensuring that any T-cells that attack self proteins are not allowed to live. In essence, every individual's immune system is tuned to the specific set of HLA and self proteins produced by that individual; where this goes awry is when tissues are transferred to another person. Since individuals almost always have different \"banks\" of HLAs, the immune system of the recipient recognizes the transplanted tissue as non-self and destroys the foreign tissue, leading to transplant rejection. It was through the realization of this that HLAs were discovered.\n\nThe thought that the mammalian body must have some way of identifying introduced foreign tissues first arose during World War II. It started with a plane crash in the height of the London Blitz. The pilot suffered severe burns requiring skin grafts; however, skin grafts were a risky business at the time, often being rejected for unknown reasons. Numerous theories were proposed and it wasn't until 1958 that the first of these \"identifying\" proteins was found. The first standardized naming system was established in 1968 by the WHO Nomenclature Committee for Factors of the HLA System. HLA research didn't heat up until the 1980s when a group of researchers finally elucidated the shape of the HLA-A*02 protein (just one of many specific HLA proteins). Even more recently, in 2010, the WHO committee responsible for naming all HLA proteins revised their standards for naming to introduce more clarity and specificity in the naming system.\n\nPeter Medawar was a zoologist turned clinician, who specialized in burn trauma. A plane crash near his home changed the path of his career, turning his work with burns from mere academia to a full on quest to save lives. Medawar and a Scottish surgeon, Tom Gibson, were tasked with working the Burns Unit of the Glasgow Royal Infirmary. The first insight came when the pair decided to experiment, and grafted part of a wound with the patient's skin, and another part with skin from the patient's brother. Within days the skin grafts from the brother were completely destroyed. Successive skin grafts from the brother were destroyed even faster, a fact that gave them the evidence they needed to implicate the immune system. Medawar later repeated this experiment on rabbits and 625 surgeries later validated their initial conclusions. Medawar then set out in search of the reason why rabbits rejected non-self grafts.\n\nMedawar continued his work, this time with a team of three at the University College London during the 1950s. Medawar's coworkers were Leslie Brent, a PhD student, and Rupert Billingham, Medawar's first graduate student at Oxford several years prior. Through carefully planned experimentation, the trio showed that mice exposed to cells of unrelated mice as fetuses did \"not\" reject skin grafts from those same mice. For this discovery, Medawar and Australian scientist Macfarlane Burnet earned the 1960 Nobel Prize.\n\nBurnet, independently of Medawar, came to the conclusion that the immune system must learn to tolerate any self cells, and hypothesized that this must occur during fetal development. For this, he jointly was awarded the Nobel Prize in 1960. Burnet's work continued and in 1957 along with Niels Jerne published a paper that modified and revolutionized antibody theory. \"Burnet speculated that one cell makes one particular shape of antibody and that all our antibody-making immune cells together make an unimaginably vast repertoire of 10 billion antibodies, each having a slightly different shape\". Thus, whenever a non-self molecule appears in the human body, one of these antibodies will have an accurate enough shape to bind to that molecule. This idea is known as Clonal Selection Theory. At the time, many leading scientists including Linus Pauling and James Watson completely rejected the idea, but repeated experimentation intended to disprove the theory actually served to build up a large body of evidence supporting Burnet and Jerne's theory.\n\nThe biggest weakness in Burnet's theory was that he had no explanation for how the body selected for immune cells that only identified non-self. In 1961, Jacques Miller published a paper offering an explanation. Miller was a PhD student at the Chester Beatty Research Institute in London. His discovery centered on the thymus. The thymus had long been regarded as nothing more than a repository for dead cells. Miller didn't buy this hypothesis. By removing the thymus of leukemic mice early in life, he found that the mice had a drastically weakened immune system. Taking inspiration from Medawar's skin transplant work, he performed a series of skin-graft experiments that showed that these immunocompromised mice didn't reject skin grafts from non-genetically identical mice. Miller then hypothesized that the thymus was essential in the construction and maintenance of the immune system. At this point Burnet came back into the picture, extending the hypothesis to specify that the dead cells found in the thymus are not any old immune cells, but instead the cells that are activated by self molecules. In other words, any molecule that binds to and hence \"recognizes\" a self molecule is killed before exiting the thymus. These cells were later found to be one of two types of immune cells, the T-cells (named for their origin, the thymus).\n\nThe discovery of the first HLA was very much a mystery. In 1958 Jean Dausset noticed that blood serum from one person could react with the white blood cells of another. He had no idea why, but he named the causative agent MAC. Around the same time other researchers were making similar discoveries. Rose Payne and Jon van Rood made an identical conclusion from observations of interactions between the blood of women who had been pregnant multiple times and the white blood cells of others. They hypothesized that this was because they had been \"sensitized\" (an immunological term meaning previously exposed to and thus more reactive towards) to the non-self proteins of the father through tissue damage during birth. At this point the researchers all realized that the sheer quantity of data they were capable of obtaining was vastly greater than that of any previous study and so collaboration would be essential. The first international meeting, in 1964, highlighted the difficulties of such massive collaborative work. Different experimental methods and inconsistency in the execution of the same tests and a non-homogeneity of naming systems added together to make collaboration incredibly difficult.\n\nIn 1967 the World Health Organization (WHO) decided that the HLA research needed an official naming system. This in turn would aid in organization and would more easily facilitate the unification of data being collected at numerous laboratories across the world. This committee is still in existence today and vastly accelerated the rate of HLA research. The first meeting of this committee in 1968 set forth guidelines and rules that govern HLAs. First, compatibility genes were divided into two types, class I and class II. Class I molecules were identified via reactions between blood serum and cells. Class II molecules were identified by mixtures of white blood cells. Second, the compatibility genes were renamed Human Leukocyte Antigens (HLA). Despite this clarification and the ever-increasing number of identified HLAs, nobody knew how they worked.\n\nLate in 1973 a pair of researchers in Australia, Rolf Zinkernagel and Peter Doherty made a revelatory discovery that altered the thinking of immunologists forever. The pair was doing research on viral infections in mice and noticed that T-cells that prevented viral infections in some mice wouldn't always prevent the same infection in other mice. After looking at the MHCs present in the mice, they realized that cytotoxic T-cells could only identify virus infections in cells with the right Class I compatibility gene. Traditional thinking was that the immune system identified infections directly but this discovery turned that theory on its head. Compatibility genes were essential in immune system mediated viral clearing. The pair coined the term \"MHC Restriction\" to describe this relationship between T-cells, specific MHC proteins, and viral detection. In 1975, in an article in the journal \"Lancet\", they introduced the idea of \"altered self\", meaning that viruses alter the MHC proteins and this alteration is detected by T-cells. For their work they won the 1996 Nobel Prize. It took the work of many others to determine how T-cells made this identification.\n\nNearly all important molecules in the body are proteins. Proteins work by each having a specific sequence of amino acids and a specific shape. Determining the order of amino acids is relatively simple. Finding the shape requires the use of x-ray crystallography and is anything but easy. It took a team of three researchers at Harvard, Don Wiley, Jack Strominger, and Pamela Bjorkman, eight years to ferret out the structure of the HLA protein. They worked specifically with HLA-A*02. Bjorkman did the majority of the leg work and in the seven years managed to piece together the structure of 90% of the protein. That last 10% was elusive though. It took another year of work to finally unveil the complete structure of HLA-A*02. They completed their work in the spring of 1987, discovering that the final 10% made a \"cup\" (of sorts) located on top of the molecule. It was the perfect size to hold peptides. Other researchers had previously determined that T-Cells can recognize cells infected with a virus, cells injected with a single protein from a virus, and even cells injected with pieces of protein from a virus. The discovery of the HLA protein structure made it starkly clear that the HLA proteins hold viral peptides in their binding groove. But the research team from Harvard wasn't done. They also observed that there was clearly a peptide in the binding groove of the HLA molecules they used to determine the shape. However, the cells they had extracted the protein from were definitely not infected by any disease causing viruses. The conclusion they made and the conclusion that has stuck to this day, is that HLA molecules can bind both self, and non-self peptides.\n\nThe most recent HLA naming system was developed in 2010 by the WHO Committee for Factors of the HLA System. There are two types of MHCs, Class I and Class II. Both are named using the same system. Currently there are 7,678 Class I alleles and 2,268 Class II alleles.\n\nHLA Naming can be quite confusing at first. All alleles start with \"HLA\", signifying they are part of the human MHC genes. The next portion (HLA-A or HLA-B) identifies which gene the allele is a modification of. The first two numbers (HLA-A*02) signifies what antigen type that particular allele is, which typically signifies the serological antigen present. In other words, HLAs with the same antigen type (HLA-A*02:101 and HLA-A*02:102) will not react with each other in serological tests. The next set of digits (HLA-A*02:101) indicates what protein the allele codes for, and these are numbered sequentially in the order they are discovered. Any HLA that has a different number here produces a different protein (AKA has a nucleotide change that replaces an amino acid with another). The third set of numbers (HLA-A*02:101:01) indicates an allele variant that has a different DNA sequence but produces the same protein as the normal gene. The final set of numbers (HLA-A*02:101:01:01) is used to designate a single or multiple nucleotide polymorphism in a non-coding region of the gene. The final aspect of HLA naming is a letter (HLA-A*02:101:01:01L). There are six letters, each with a different meaning.\n\nA person can have 2 antigen proteins per genetic-locus (one gene from each parent). When first discovered, identified antigens were clustered, creating groups in which no more than two antigens per cluster were found in a given person. Serotype group \"A\" consisted HL-A1, A2, A3, A9, A10, A11. Another cluster, \"B\", contained A7, A8, A12, A13, A14, A15. HL-A4 antigen was found to occur on lymphoid cells. Since the \"HL-Antigens\" no longer belonged to a single group, a new naming system was needed.\n\nIn 1968 the WHO Nomenclature Committee for Factors of the HLA System first met. They established a system that divided the HLAs into HLA-A and HLA-B, A and B corresponding to a group of reactive serotypes. For example, \"HL-A2\" became HLA-A2, \"HL-A7\" became HLA-B7 and \"HL-A8\" became HLA-B8.\n\nIn this arrangement there were cells that were 'blank' or had new specificities, these new antigens were called \"W\" antigens, and as they were reassigned to new groups, for example \"A\" serotypes, they became Aw or Bw antigens. It was found that some antigens that behaved like A and B antigens but could be excluded based on '2-type max' exclusion. Thus a new group, \"C\" was created. Classification of C antigens is still ongoing, and they have retained the name Cw as many serotypes have not been developed.\n\nThe classification of the \"A4\" antigens was complicated. The \"A4\" subset evolved to become D-region antigens, which was a large cluster of genes that encoded MHC class II. Several renamings occurred. The D-region has 8 major coding loci that combine to form 3 different protein groups; DP, DQ, and DR. DRw antigens were the first to be split, a process made easy by the virtue of having an invariant alpha chain, but complicated by 4 beta chain loci (DRB1, DRB3, DRB4, and DRB5). Serotypes to DQ reacted with alpha and beta chains, or both of certain isoforms. The proper classification was greatly aided by gene sequencing and PCR. Classification and description of DP antigens is ongoing.\n\nThe naming of human leukocyte antigens HLA \"antigens\" is deeply rooted in the discovery history of their serotypes and alleles. There is no doubt that HLA terminology can be bewildering, this terminology is a consequence of the complex genetics as well as the way these antigens were characterized.\n\nHistorical perspective is important to an understanding of how the HLA were systematized. In organ transplant the goal was to explain graft rejection for recipients, and of course, to prevent future rejection. From this perspective, the cause of rejections were found to be \"antigens\". In the same way bacterial antigens can cause inflammatory response, HLA antigens from the donor of the organ caused an inflammatory response when placed in a recipient. This is called allograft [allo = different, graft(medical) = transplant] rejection.\n\nTo explain rejection in a nutshell, certain immune system components are highly variable, the agents are called the Major histocompatibility (MHC) antigens. MHC antigens cause rejection of improperly matched organ transplants. The variability stems from genetics. From the perspective of human evolution, why are antigens of the MHC so variable when many other human proteins lack variability? The cause of host-versus-graft-disease may actually stem from the functions of the system.\n\nThe use of the word alloantigen actually masks the fact that HLA are infrequently autoantigens in the donor, and therefore their function is not as antigens, but something else. But the naming of these antigens is not borne out of function but the need to match organ donors with recipients.\n\nIn the early 1960s, some physicians began more aggressive attempts at organ transplantation. Knowing little about \"compatibility factors\", they attempted transplantation between humans and between non-humans and humans. Immunosuppressive drugs worked for a time, but transplanted organs would either always fail or the patients would die from infections. Patients received skin, white blood cell or kidney donations from other donors (called allografts, meaning 'of different genetics' grafts). If these allografts were rejected, it was found that the 'rejection' response was accompanied by an antibody mediated agglutination (biology) of red blood cells (See figure). The search for these cell surface antigens began. There are several processes by which antibodies can reduce function: \n\nIn the accompanying figure, two similar haplotypes (unknown to early clinicians) are identical, except for the one \"antigen\" in the top haplotype. The transplant may not be rejected, but if rejection does occur that allotypic protein, the allo\"antigen\", in the donor tissue may have induced the dominant allo-reactive antibody in the recipient.\n\nHemagglutination assay. In generating an immune response to an antigen, the B-cells go through a process of maturation, from surface IgM production, to serum IgM production, to maturation into a plasma cell producing IgG. Graft recipients who generate an immune response have both IgM and IgG. The IgM can be used directly in hemagglutination assays, depicted on the right. IgM has 10 antigen binding regions per molecule, allowing cross-linking of cells. An antiserum specific for HLA-A3 will then agglutinate HLA-A3 bearing red blood cells if the concentration of IgM in the antiserum is sufficiently high. Alternatively, a second antibody to the invariable (F) region of the IgG can be used to cross-link antibodies on different cells, causing agglutination.\n\nComplement fixation assay. The complement fixation test was modified to assay Antiserum mediated RBC lysis.\n\nChromium release assay. This assay measures the release of (biological) radioactive chromium from cells as a result of killer cell activity. These cells are attracted to class I antigens that either carry foreign antigens, or are foreign to the immune system.\n\nEach person has two HLA haplotypes, a cassette of genes passed on from each parent. The haplotype frequencies in Europeans are in strong linkage disequilibrium. This means there are much higher frequencies of certain haplotypes relative to the expectation based on random sorting of gene-alleles. This aided the discovery of HLA antigens, but was unknown to the pioneering researchers.\n\nIn the tables a fortuitous transplant between two unrelated individual has resulted in an antiserum to single alloantigen. By discovering these close-but-non-identical matches, the process with somewhat related haplotypes surface antigens were identified for HLA A, and in the table below, HLA B at the time however these were all grouped together as HL-Antigens. On the left the \"B\" and \"cw\" antigens are matched (B and C are close together so if B matches then C likely also matches), but A antigens are not matched. The antisera that is produced by the recipient is most likely to be A3, but if the direction of transplant is reversed A2 is the likely alloantigen. Two of the first three alloantigens are thus readily easy to detect because of the similarity and frequency of the A2-B7 and A3-B7 haplotypes (see example 1). \n\nIn these instances, the A1/A2, A2/A3, A1/A3 are matched, decreasing the probability of a rejection because many are linked to a given haplotype. Occasionally the 'recombinant' A1-Cw7-B7(rare), B7 becomes the alloantigen in a recipient with A1-Cw7-B8(common).\n\nThis linkage disequilibrium in Europeans explains why A1, A2, A3, \"A7\"[B7], and \"A8\"[B8] were identified, first. It would have taken substantially longer to identify other alleles because frequencies were lower, and haplotypes that migrated into the European population had undergone equilibration or were from multiple sources.\n\nThis is the genetic background against which scientists tried to uncover and understand the histocompatibility antigens.\n\nIn the late 1960s, scientist began reacting sera from patients with rejecting transplants to donor or 'third party' tissues. Their sera (the liquid part of the blood when blood clots) was sensitized to the cells from donors - it was \"alloreactive\". By testing different anti-sera from recipients they were able to uncover some with unique reactivities. As a result, scientists were able to identify a few antigens. At first the first antigens were called the Hu-1 antigens and tentatively tagged as gene products of the Human equivalent of the mouse histocompatibility locus (H2). In 1968, it was discovered that matching these antigens between kidney donor and recipient improved the likelihood of kidney survival in the recipient. The antigen list still exists, although it has been reorganized to fit what we have since learned about genetics, refined, and greatly expanded.\n\nAs the study of these 'rejection' sera and \"allo\"-antigens progressed, certain patterns in the antibody recognition were recognized. The first major observation, in 1969, was that an allotypic antibodies to \"4\" (\"Four\") was only found on lymphocytes, while most of the antigens, termed \"LA\", recognized most cells in the body.\n\nThis group \"4\" antigen on lymphocytes would expand into \"4a\", \"4b\" and so on, becoming the \"D\" series (HLA-D (Class II) antigens) DP, DQ, and DR. This is an interesting history in itself.\n\nThe Hu-1 antigens were renamed the Human-lymphoid (HL) allo-antigens (HL-As). Allo-antigen comes from the observation that a tolerated protein in the donor becomes antigenic in the recipient. This can be compared with an autoantigen, in which a person develops antibodies to one or more of their own proteins. This also suggested the donor and recipient have a different genetic makeup for these antigens. The \"LA\" group thereafter was composed of HL-A1, A2, A3, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14 and A15 until further divisions and renaming were necessary. Some of the antigens above, for example HL-A1, are similar to HLA-A1, as they are the same serotype. Some of the above, like A5, are not mentioned within the last few years, as they have been renamed.\n\nDuring these early studies it became known that there were associations with many autoimmune diseases. And the HLA A1-B8 haplotype is linked to a very long piece of conserved chromosome 6 variant called AH8.1 haplotype. In these studies \"HL-A1,8\" were frequently found co-linked to disease. This linkage is not necessarily a function of either gene, but a consequence of the way AH8.1 evolved.\n\nA series of tests on cultured cells revealed that, within the \"LA\" group, a donor tissue might have some antigens but not others. For example, an antiserum may react with patterns (on a given tissue):\n\n\nBut fail to react in the following patterns:\n\n\nIf 2 members of the series (A1, 2, 3, 9, 10, 11) were typed, a reaction with a third member of the series to the donor was not observed. This 'exclusivity' identified series \"A\". One might notice the similarities of this numeric series with the , as series \"A\" antigens are the first six members of HLA-A. Inadvertently, the scientist had discovered an antibody set that recognized only gene products from one locus, HLA-A gene the \"antigens\" being the gene products. The implication is that an alloreactive anti-sera can be a tool for genetic identification.\n\nNot long after the series A antigens were separated from the (rapidly expanding) list of antigens, it was determined another group also could be separated along the same \"logical\" lines. This group included HL-A5, A7, A8, A12. This became the series \"B\".\nNote the similarity of Series \"B\" to the first few members . The names of these antigens were necessarily changed to fit the new putative series they were assigned to. From HL-A# to HLA-B#. The problem was that the literature was using \"A7\" and would soon be using \"B7\" as shorthand for HLA-B7.\n\nSince it was now certain, by the early 1970s, that the \"antigens\" were encoded by different series, implicit loci, numeric lists became somewhat cumbersome. Many groups were discovering antigens. In these instances an antigen was assigned a temporary name, like \"RoMa2\" and after discussion, the next open numeric slot could be assigned, but not to an \"A\" or \"B\" series until proper testing had been done. To work around this problem a 'workshop' number \"w#\" was often assigned while testing continued to determine which series the antigen belonged to.\n\nBefore too long, a series \"C\" was uncovered. Series C has proved difficult to serotype, and the alleles in the series still carry the \"w\" tag signifying that status; in addition, it reminds us that Series C were not assigned names the same way as Series A and B, it has its own numeric list Cw1, Cw2, Cw3.\n\nBy the mid 1970s, genetic research was finally beginning to make sense of the simple list of antigens, a new series \"C\" had been discovered and, in turn genetic research had determined the order of HLA-A, C, B and D encoding loci on the human 6p. With new series came new antigens; Cw1 and 2 were quickly populated, although Cw typing lagged. Almost half of the antigens could not be resolved by serotyping in the early 90s. Currently genetics defines 18 groups.\n\nAt this point, Dw was still being used to identify DR, DQ, and DP antigens. The ability to identify new antigens far exceeded the ability to characterize those new antigens.\n\nAs technology for transplantation was deployed around the world, it became clear that these antigens were far from a complete set, and in fact hardly useful in some areas of the world (e.g., Africa, or those descended from Africans). Some serotyping antibodies proved to be poor, with broad specificities, and new serotypes were found that identified a smaller set of antigens more precisely. These broad antigen groups, like A9 and B5, were subdivided into \"split\" antigen groups, A23 & A24 and B51 & B52, respectively. As the HL-A serotyping developed, so did identification of new antigens.\n\nIn the early 1980s, it was discovered that a restriction fragment segregates with individuals who bear the HLA-B8 serotype. By 1990, it was discovered that a single amino acid sequence difference between HLA-B44 (B*4401 versus B*4402) could result in allograft rejection. This revelation appeared to make serotyping based matching strategies problematic if many such differences existed. In the case of B44, the antigen had already been split from the B12 broad antigen group. In 1983, the cDNA sequences of HLA-A3 and Cw3 All three sequences compared well with mouse MHC class I antigens. The Western European HLA-B7 antigen had been sequenced (although the first sequence had errors and was replaced). In short order, many HLA class I alleles were sequenced\nincluding 2 Cw1 alleles.\n\nBy 1990, the full complexity of the HLA class I antigens was beginning to be understood. At the time new serotypes were being determined, the problem with multiple alleles for each serotype was becoming apparent by nucleotide sequencing. RFLP analysis helped determine new alleles, but sequencing was more thorough. Throughout the 1990s, PCR kits, called SSP-PCR kits were developed that allowed, at least under optimal conditions, the purification of DNA, PCR and Agarose Gel identification of alleles within an 8-hour day. Alleles that could not be clearly identified by serotype and PCR could be sequenced, allowing for the refinement of new PCR kits.\n\nSerotypes like B*4401, B*4402, B*4403, each abundant within those with B44 serotypes could be determined with unambiguous accuracy. The molecular genetics has advanced HLA technology markedly over serotyping technology, but serotyping still survives. Serotyping had identified the most similar antigens that now form the HLA subgroups. Serotyping can reveal whether an antigen coded by the relevant HLA gene is expressed. An HLA allele coding non-expressed gene is termed \"Null Allele\", for example: HLA-B*15:01:01:02N. The expression level can also detected by serotyping, an HLA gene coding for antigens which has low protein expression on the cell surface is termed \"Low Expresser\", for example: HLA-A*02:01:01:02L.\n\n\nThe scientific problem has been to explain the natural function of a molecule, such as a self cell-surface receptor involved in immunity. It also seeks to explain how variation developed (perhaps by evolutionary pressure), and how the genetic mechanisms works (dominant, codominant, semidominant, or recessive; purifying selection or balancing selection).\n"}
{"id": "29127124", "url": "https://en.wikipedia.org/wiki?curid=29127124", "title": "Immunosurgery", "text": "Immunosurgery\n\nImmunosurgery is a method of selectively removing the external cell layer (trophoblast) of a blastocyst through a cytotoxicity procedure. The protocol for immunosurgery includes preincubation with an antiserum, rinsing it with hES derivation media to remove the antibodies, exposing it to complement, and then removing the lysed trophoectoderm through a pipette. This technique is used to isolate the inner cell mass of the blastocyst. The trophoectoderm's cell junctions and tight epithelium \"shield\" the ICM from antibody binding by effectively making the cell impermeable to macromolecules.\n\nImmunosurgery can be used to obtain large quantities of pure inner cell masses in a relatively short period of time. The ICM obtained can then be used for stem cell research and is better to use than adult or fetal stem cells because the ICM has not been affected by external factors, such as manually bisecting the cell. However, if the structural integrity of the blastocyst is compromised prior to the experiment, the ICM is susceptible to the immunological reaction. Thus, the quality of the embryo used is imperative to the experiment's success. In addition, when using complement derived from animals, the source of the animals matters. They should be kept in a specific-pathogen-free environment to increase the likelihood that the animal has not developed natural antibodies against the bacterial carbohydrates present in the serum (which can be obtained from a different animal).\n\nSolter and Knowles developed the first method of immunosurgery with their 1975 paper \"Immunosurgery of Mouse Blastocyst\". They primarily used it for studying early embryonic development. Though immunosurgery is the most prevalent method of ICM isolation, various experiments have improved the process, such as through the use of lasers (performed by Tanaka, et al.) and micromanipulators (performed by Ding, et al.). These new methods reduce the risk of contamination with animal materials within the embryonic stem cells derived from the ICM, which can cause complications later on if the embryonic stem cells are transplanted into a human for cell therapy.\n"}
{"id": "649418", "url": "https://en.wikipedia.org/wiki?curid=649418", "title": "Interactive computation", "text": "Interactive computation\n\nIn computer science, interactive computation is a mathematical model for computation that involves input/output communication with the external world \"during\" computation. This is in contrast to the traditional understanding of computation which assumes reading input only before computation and writing output only after computation, thus defining a kind of \"closed\" computation.\n\nThe Church-Turing thesis attempts to define computation and computability in terms of Turing machines. Because the Turing machine model only provides an answer to the question of what computability of \"functions\" means, but interactive tasks are not always reducible to functions, it fails to capture a broader intuition of computation and computability. It was not until recently that the theoretical computer science community realized the necessity to define adequate mathematical models of interactive computation.\n\nAmong the currently studied mathematical models of computation that attempt to capture interaction are Giorgi Japaridze's hard- and easy-play machines elaborated within the framework of computability logic, Dina Q. Goldin's Persistent Turing Machines (PTMs), and Yuri Gurevich's abstract state machines. Peter Wegner has additionally done a great deal of work on this area of computer science .\n\n\n\n"}
{"id": "56784783", "url": "https://en.wikipedia.org/wiki?curid=56784783", "title": "Jacquelyn S. Fetrow", "text": "Jacquelyn S. Fetrow\n\nJacquelyn S. (Jacque) Fetrow (born 1960) is a computational biologist, college administrator, and the 15th president of Albright College. Previously, she served as Provost, Vice President of Academic Affairs, and Professor of Chemistry at the University of Richmond, in Richmond, Virginia. Prior to that appointment, she served as Dean of the College at Wake Forest University in Winston-Salem, North Carolina. She also co-founded a company, GeneFormatics, for which she served as Director and Chief Scientific Officer for four years.\n\nFetrow is a native of Camp Hill, Pennsylvania. Her mother, Mildred F. Fetrow, was a public school teacher in the West Shore School District, teaching kindergarten, first grade and second grade for many years. Her father, David E. Fetrow, was a carpenter. He also worked as a truck salesman, real estate salesman, and office manager.\n\nFetrow attended Camp Hill public schools through twelfth grade, Albright College for her bachelor’s degree (Biochemistry), and Penn State College of Medicine for a Ph.D. (Biological Chemistry), which she earned in 1986 working with George D. Rose. She did post-doctoral stints at the University of Rochester School of Medicine under the mentorship of Fred Sherman, and at the Whitehead Institute at Massachusetts Institute of Technology under the tutelage of Peter S. Kim.\n\nFetrow worked at the University at Albany, SUNY, from 1990 to 1998, serving as assistant and then associate professor of biological sciences. She then accepted a position at The Scripps Research Institute. Technology that she helped develop at Scripps served as the foundation for GeneFormatics, Inc., the company that Fetrow co-founded and at which she served as Chief Scientific Officer and Director. In August 2003 she was appointed Reynolds Professor of Computational Biology at Wake Forest University, and in January 2009 she was appointed as Dean of Wake Forest College. She moved to the University of Richmond to serve as Provost and Vice President of Academic Affairs at the University of Richmond in 2014. In 2017 she was appointed president of her alma mater, Albright College.\n\nFetrow was the first to describe the non-regular protein structure, omega loop, a structure she identified and studied for her doctoral dissertation (work published under the name Jacquelyn F. Leszczynski). Her early research work involved the experimental analysis of these structures in the protein cytochrome \"c\".\n\nLater, her worked turned to the classification of functional sites in protein structures, resulting in Fuzzy Functional Forms and active site profiling. This work formed the foundation for the company she co-founded, GeneFormatics. Subsequent development of the active site profiling technology was used to create the Peroxiredoxin Classification Index. This technology has been used to cluster other superfamilies, including the enolases, into functionally relevant clusters.\n\n\n\n"}
{"id": "844746", "url": "https://en.wikipedia.org/wiki?curid=844746", "title": "Journal of Mundane Behavior", "text": "Journal of Mundane Behavior\n\nThe Journal of Mundane Behavior was a triannual peer-reviewed academic journal of sociology covering everyday behavior and experiences. It was published online with three issues a year. The journal's first issue came out in February 2000 and the last issue appeared in 2004. \n\nThe journal was dedicated to exploring \"the minor, redundant and commonplace scenes of life\" and celebrating \"the majesty of the obvious\". The first issue included articles about the behavior of Japanese people on elevators, the arrangement of books on library shelves, and the social implications of facial hair. The journal reflected a recent trend among sociologists to \"investigate the largely unconscious verbal and nonverbal conventions of everyday social interactions,\" in contrast to the field's historical focus on deviant behavior.\n\nThe editor-in-chief was Scott Schaffer (Millersville University of Pennsylvania). The original concept for the journal came from Schaffer and founding co-editor Myron Orleans (California State University, Fullerton). They were \"inspired in part by an article in the scholarly journal \"Sociological Theory\" that called for closer inspection of those parts of life that we routinely ignore.\" The two universities co-hosted the site.\n"}
{"id": "1065280", "url": "https://en.wikipedia.org/wiki?curid=1065280", "title": "Kosmos 140", "text": "Kosmos 140\n\nKosmos 140 ( meaning \"Cosmos 140\") was an unmanned flight of the Soyuz spacecraft. It was the third attempted test flight of the Soyuz 7K-OK model, after orbital (Kosmos 133) and launch (Soyuz 11A511) failures of the first two Soyuz spacecraft.\n\nThe followup to Cosmos 133 was planned for 14 December, but ended disastrously. At liftoff, the Blok A core stage of the 11A57 booster ignited, but not the strap-ons. A shutdown command was immediately sent and pad crews began to move the service towers back in place and drain the propellants. This task was completed for the core stage and strap-ons, and then about 30 minutes after the attempted launch, the escape tower suddenly fired. Its exhaust caused the Blok I third stage propellant tanks to overheat and explode, killing one person on the ground and damaging the Soyuz and core stage/strap-ons beyond repair. LC-1 was also badly damaged and took a month of repair work in the frigid Kazakhstan winter to be restored to use. The reason for the LES firing was thought to be either a timer being activated due to the Earth's rotation affecting the gyroscope package in the launch vehicle or perhaps one of the service towers bumping it.\n\nIn February 1967, the backup booster and spacecraft were set up at LC-1 and the planned mission could be carried out.\n\nThe spacecraft suffered attitude control problems and excessive fuel consumption in orbit, but remained controllable. An attempted maneuver on the 22nd orbit still showed problems with the control system. It malfunctioned yet again during retrofire, leading to a steeper than planned ballistic reentry and a hole being burned in the heat shield.\n\nAlthough the event would have been lethal to any human occupants, the capsule's recovery systems operated and the capsule crashed through the ice of the frozen Aral Sea, hundreds of kilometers short of its landing zone. The spacecraft finally sank in 10 meters of water and had to be retrieved by divers. The test performance was nonetheless deemed \"good enough\"; the manned docking missions of Soyuz 1 and Soyuz 2 were approved for the next flight.\n\n"}
{"id": "16913870", "url": "https://en.wikipedia.org/wiki?curid=16913870", "title": "Kurt H. Debus", "text": "Kurt H. Debus\n\nKurt Heinrich Debus (November 29, 1908 – October 10, 1983) was a German V-2 rocket scientist during World War II who, after being brought to the United States under Operation Paperclip, in 1962 became the first director of NASA's Launch Operations Center (later the Kennedy Space Center).\n\nDebus directed the design, development, construction and operation of NASA's Saturn launch facilities at the north end of Cape Canaveral and adjacent Merritt Island in Florida. Under him, NASA conducted 150 launches of military missiles and space vehicles, including 13 Saturn V rockets, the booster for the Apollo manned moon landings.\n\nBorn to Heinrich and Melly Debus in Frankfurt, Germany in 1908, Debus received all his schooling in that country. He attended Technische Hochschule Darmstadt where he earned his initial and advanced degrees in mechanical and electrical engineering. He served as a graduate assistant on the faculty for electrical engineering and high-voltage engineering while studying for his master's degree.\n\nIn 1939, he obtained his engineering doctorate with a thesis on surge voltages, and was appointed assistant professor at the university. During World War II, Debus was a member of the Nazi Party, a member of the Sturmabteilung (SA) and Himmler's SS (since 1940). Debus was appointed by Hitler as the V-weapons flight test director and was actively engaged in the rocket research program at Peenemünde and the development of the V-2 rocket, Debus led the Test Stand Group personnel at Peenemünde and was the engineer in charge at Test Stand VII.\n\nAt the end of the war, Debus and a small group of the V-2 engineers led by Wernher von Braun's brother sought out the advancing American 44th Infantry Division near Schattwald on May 2, 1945. Debus was detained by the U.S. Army with the rest of the Peenemünde scientists at Garmisch–Partenkirchen.\n\nDebus served as both a technical and diplomatic liaison between German rocket engineers and the British during Operation Backfire, a series of V-2 test launches from an abandoned German naval gun range near Cuxhaven, Germany, in October 1945.\n\nIn late 1945, Debus was transferred to Fort Bliss, Texas, under contract as a \"special employee\" of the U.S. Army, as were the other German rocket specialists. He was deputy director at the Guidance and Control Branch through December 1948, when he was promoted to assistant technical director to von Braun at the Redstone Arsenal in Huntsville, Alabama.\n\nThe arsenal became the focal point of the Army's rocket and space projects; larger rockets were launched first from White Sands Missile Range in New Mexico, and later from Cape Canaveral. The Army assigned von Braun as chairman of a Development Board, and Debus supervised the development program of the Guided Missile Branch until November 1951.\n\nThe Army Ordnance Department reorganized the team and called it the Ordnance Guided Missile Center. By November 1951, the pace had picked up and a new missile program, the Redstone, was taking shape. Von Braun named Debus to lead a new Experimental Missiles Firing Branch. Debus' organization also launched the first U.S. missiles carrying atomic warheads in the Pacific Ocean area during a series of tests.\n\nStarting in 1952, Debus supervised the development and construction of rocket launch facilities at Cape Canaveral for the Redstone, Jupiter, Jupiter-C, Juno and Pershing military configurations continuing through 1960. The organization he directed was transferred from the Army to NASA.\n\nBeginning in 1961, Debus directed the design, development and construction of NASA's Saturn launch facilities at the north end of Cape Canaveral and adjacent Merritt Island.\n\nOn July 1, 1962, the Florida launch facility at Cape Canaveral was officially designated as NASA's Launch Operations Center (renamed to honor President John Kennedy after his assassination in 1963) and Debus was officially named its first director. In October 1965, he became responsible for NASA unmanned launch operations at the Eastern and Western Ranges, assuming the additional title of Kennedy Space Center (KSC) director of launch operations until Rocco Petrone took the post in 1966.\n\nUnder Debus' leadership, NASA and its team of contractors built what was hailed as the Free World's Moonport — KSC's Launch Complex 39 — as well as tested and launched the Saturn family of rockets for the Apollo and Skylab programs. Debus retired as KSC director in November 1974.\n\nDebus married Irmgard \"Gay\" (née Brueckmann) June 30, 1937; they had two daughters while still in Germany: Siegrid and Ute.\n\nA small lunar crater on the far side of the Moon to the east-southeast of the crater Ganskiy, past the eastern limb, is named for Debus; as is The Kurt Debus Conference Center at the Kennedy Space Center Visitor Complex. Debus was inducted into the National Space Hall of Fame in 1969.\n\nSince 1990, the National Space Club of Florida has presented its annual Debus Award to recognize significant aerospace achievements in Florida, including individuals associated with launch vehicles, spacecraft operations, ground support services, range activities, space education and spaceport research and development. The award was conceived as an adjunct to the Goddard Award given each year by the National Space Club in Washington, D.C. to an individual in the aerospace field on a national level.\n\n"}
{"id": "18421218", "url": "https://en.wikipedia.org/wiki?curid=18421218", "title": "Last Child in the Woods", "text": "Last Child in the Woods\n\nLast Child in the Woods: Saving Our Children From Nature-Deficit Disorder is a 2005 book by author Richard Louv that documents decreased exposure of children to nature in American society and how this \"nature-deficit disorder\" harms children and society. The book examines research and concludes that direct exposure to nature is essential for healthy childhood development and for the physical and emotional health of children and adults. The author also suggests solutions to the problems he describes. A revised and expanded edition was published in 2008.\n\nThe book was on the New York Times best seller list for best paper nonfiction. The author received the Audubon Medal \"for sounding the alarm about the health and societal costs of children's isolation from the natural world—and for sparking a growing movement to remedy the problem.\"\n\n\nThe success of Last Child in the Woods inspired the creation of Children & Nature Network co-founded and chaired by the book’s author, Richard Louv, to encourage and support the people and organizations working to reconnect children with nature.\n\nGreen Hour is an organization that provides information on how to reverse Nature-Deficit Disorder, and encourages parents to let their children explore and reconnect with the outdoors.\n\n\n"}
{"id": "14485594", "url": "https://en.wikipedia.org/wiki?curid=14485594", "title": "List of members of the National Academy of Sciences (Biochemistry)", "text": "List of members of the National Academy of Sciences (Biochemistry)\n\nThe designation (d) after the name means the member is deceased. \n"}
{"id": "58609951", "url": "https://en.wikipedia.org/wiki?curid=58609951", "title": "List of pathology mnemonics", "text": "List of pathology mnemonics\n\nThis is a list of pathology mnemonics, categorized and alphabetized. For mnemonics in other medical specialities, see this list of medical mnemonics.\n\n5 Ps:\n\nPain in the abdomen\n\nPolyneuropathy\n\nPsychological abnormalities\n\nPink urine\n\nPrecipitated by drugs (including barbiturates, oral contraceptives, and sulfa drugs)\n\n6 P's:\n\nPain\n\nPallor\n\nPulselessness\n\nParalysis\n\nParaesthesia\n\nPerishingly cold\n\nABCD:\n\nAcute blood loss\n\nBone marrow failure\n\nChronic disease\n\nDestruction (hemolysis)\n\nANEMIA:\n\nAnemia of chronic disease\n\nNo folate or B12\n\nEthanol\n\nMarrow failure & hemaglobinopathies\n\nIron deficient\n\nAcute & chronic blood loss\n\n\"You're a SAD BET with these risk factors\":\n\nSex: male\n\nAge: middle-aged, elderly\n\nDiabetes mellitus\n\nBP high: hypertension\n\nElevated cholesterol\n\nTobacco\n\nCARCinoid:\n\nCutaneous flushing\n\nAsthmatic wheezing\n\nRight sided valvular heart lesions\n\nCramping and diarrhea\n\nCUSHING:\n\nCentral obesity/ Cervical fat pads/ Collagen fiber weakness/ Comedones (acne)\n\nUrinary free corisol and glucose increase\n\nStriae/ Suppressed immunity\n\nHypercortisolism/ Hypertension/ Hyperglycemia/ Hirsutism\n\nIatrogenic (Increased administration of corticosteroids)\n\nNoniatrogenic (Neoplasms)\n\nGlucose intolerance/Growth retardation\n\nketONEbodies are seen in type ONEdiabetes.\n\n5 F's:\n\nFat\n\nFemale\n\nFair (gallstones more common in Caucasians)\n\nFertile (premenopausal- increased estrogen is thought to increase cholesterol levels in bile and decrease gallbladder contractions)\n\nForty or above (age)\n\nCommon are 3 C's:\n\nCirrhosis\n\nCarcinoma\n\nCardiac failure\n\nRarer are 3 C's:\n\nCholestasis\n\nCysts\n\nCellular infiltration\n\nMURDER\n\nMuscle weakness\n\nUrine: oliguria, anuria\n\nRespiratory distress\n\nDecreased cardiac contractility\n\nEKG changes (peaked T waves; QRS widening)\n\nReflexes: Hyperreflexia or areflexia (flaccid)\n\nFRIED SALT\n\nFRIED\n\nFever (low), Flushed skin\n\nRestless (irritable)\n\nIncreased fluid retention, Increased blood pressure\n\nEdema (peripheral and pitting)\n\nDecreased urinary output, Dry mouth\n\nSALT\n\nSkin flushed\n\nAgitated\n\nLow-grade fever\n\nThirst\n\nCrohn's has Cobblestones on endoscopy.\n\nMORPHINES:\n\nMiosis\n\nOrthostatic hypotension or \"Out of it\"\n\nRespiratory depression\n\nPain suppression or Pneumonia\n\nHistamine release or Hormonal alterations or Hypotension\n\nIncreased ICT or Infrequency (constipation, or urinary retention)\n\nNausea\n\nEuphoria or Emesis\n\nSedation\n\nFLAME:\n\nFatty\n\nLiver\n\nAnemia\n\nMalabsorption\n\nEdema\n\nI GET SMASHED:\n\nIdiopathic\n\nGallstones\n\nEthanol\n\nTrauma\n\nSteroids\n\nMumps\n\nAutoimmune\n\nScorpion sting\n\nHyperlipidaemia/hypercalcaemia\n\nERCP\n\nDrugs\n\nPKU:\n\nPale hair, skin\n\nKrazy (neurological abnormalities)\n\nUnpleasant smell\n\n\"MorPHINE:Fine. AmPHETamine:Fat\":\n\nMorphine overdose: pupils constricted (fine).\n\nAmphetamine overdose: pupils dilated (fat).\n\nPERICarditis:\n\nPulsus paradoxus\n\nECG changes\n\nRub\n\nIncreased JVP\n\nChest pain [worse on inspiration, better when leaning forward]\n\nP\nseduogout crystals are:\n\nP\nositive birefringent\n\nP\nolygon shaped\n\nGout therefore is the negative needle shaped crystals.\nAlso, gout classically strikes the great \"Toe\", and its hallmark is Tophi.\n\nabcdefghij\n\nAsterixis, Ascites, Ankle oedema, Atrophy of testicles\n\nBruising\n\nClubbing/ Colour change of nails (leuconychia)\n\nDupuytren’s contracture\n\nEncephalopathy / palmar Erythema\n\nFoetor hepaticus\n\nGynaecomastia\n\nHepatomegaly\n\nIncrease size of parotids\n\nJaundice\n"}
{"id": "49433646", "url": "https://en.wikipedia.org/wiki?curid=49433646", "title": "List of things named after Joseph Liouville", "text": "List of things named after Joseph Liouville\n\nSeveral concepts from mathematics and physics are named after the French mathematician Joseph Liouville.\n\n\nLiouville's theorem: several theorems - thus named from number theory, analysis, mechanics and so on. See disambiguation page for full information.\n"}
{"id": "4692179", "url": "https://en.wikipedia.org/wiki?curid=4692179", "title": "Low-energy electron diffraction", "text": "Low-energy electron diffraction\n\nLow-Energy electron diffraction (LEED) is a technique for the determination of the surface structure of single-crystalline materials by bombardment with a collimated beam of low energy electrons (20–200 eV) and observation of diffracted electrons as spots on a fluorescent screen.\n\nLEED may be used in one of two ways:\n\n\nThe theoretical possibility of the occurrence of electron diffraction first emerged in 1924 when Louis de Broglie introduced wave mechanics and proposed the wavelike nature of all particles. In his Nobel laureated work de Broglie postulated that the wavelength of a particle with linear momentum \"p\" is given by \"h/p\", where \"h\" is Planck's constant.\nThe de Broglie hypothesis was confirmed experimentally at Bell Labs in 1927 when Clinton Davisson and Lester Germer fired low-energy electrons at a crystalline nickel target and observed that the angular dependence of the intensity of backscattered electrons showed diffraction patterns. These observations were consistent with the diffraction theory for X-rays developed by Bragg and Laue earlier. Before the acceptance of the de Broglie hypothesis diffraction was believed to be an exclusive property of waves.\n\nDavisson and Germer published notes of their electron diffraction experiment result in Nature and in Physical Review in 1927. One month after Davisson and Germer's work appeared, Thompson and Reid published their electron diffraction work with higher kinetic energy (thousand times higher than the energy used by Davisson and Germer) in the same journal. Those experiments revealed the wave property of electrons and opened up an era of electron diffraction study.\n\nThough discovered in 1927, low energy electron diffraction did not become a popular tool for surface analysis until the early 1960s. The main reasons were that monitoring directions and intensities of diffracted beams was a difficult experimental process due to inadequate vacuum techniques and slow detection methods such as a Faraday cup. Also, since LEED is a surface sensitive method, it required well-ordered surface structures. Techniques for the preparation of clean metal surfaces first became available much later.\nNonetheless, H. E. Farnsworth and coworkers at Brown University pioneered the use of LEED as a method for characterizing the absorption of gases onto clean metal surfaces and the associated regular adsorption phases, starting shortly after the Davisson and Germer discovery into the 1970's.\n\nIn the early 1960s LEED experienced a renaissance, as ultra high vacuum became widely available and the post acceleration detection method was introduced by none less than Germer and his coworkers at Bell Labs using a flat phosphor screen. Using this technique diffracted electrons were accelerated to high energies to produce clear and visible diffraction patterns on a fluorescent screen. Ironically the post-acceleration method had already been proposed by Ehrenberg in 1934. In 1962 Lander and colleagues introduced the modern hemispherical screen with associated hemispherical grids. In the mid sixties, modern LEED systems became commercially available as part of the ultra-high vacuum instrumentation suite by Varian Associates and triggered an enormous boost of activities in surface science. Notably future Nobel prize winner Gerhard Ertl started his studies of surface chemistry and catalysis on such a Varian system.\n\nIt soon became clear that the kinematic (single scattering) theory, which had been successfully used to explain X-ray diffraction experiments, was inadequate for the quantitative interpretation of experimental data obtained from LEED. At this stage a detailed determination of surface structures, including adsorption sites, bond angles and bond lengths was not possible.\nA dynamical electron diffraction theory which took into account the possibility of multiple scattering was established in the late 1960s. With this theory it later became possible to reproduce experimental data with high precision.\n\nIn order to keep the studied sample clean and free from unwanted adsorbates, LEED experiments are performed in an ultra-high-vacuum environment (10 mbar). \nThe most important elements in a LEED experiment are:\n\n\nA simplified sketch of an LEED setup is shown in figure 2.\n\nThe sample is usually prepared outside the vacuum chamber by cutting a slice of around 1 mm in thickness and 1 cm in diameter along the desired crystallographic axis.\nThe correct alignment of the crystal can be achieved with the help of x-ray methods and should be within 1° of the desired angle.\nAfter being mounted in the UHV chamber the sample is chemically cleaned and flattened. Unwanted surface contaminants are removed by ion sputtering or by chemical processes such as oxidation and reduction cycles. The surface is flattened by annealing at high temperatures.\nOnce a clean and well-defined surface is prepared, monolayers can be adsorbed on the surface by exposing it to a gas consisting of the desired adsorbate atoms or molecules.\n\nOften the annealing process will let bulk impurities diffuse to the surface and therefore give rise to a re-contamination after each cleaning cycle. The problem is that impurities which adsorb without changing the basic symmetry of the surface, cannot easily be identified in the diffraction pattern. Therefore, in many LEED experiments Auger electron spectroscopy is used to accurately determine the purity of the sample.\n\nIn the electron gun, monochromatic electrons are emitted by a cathode filament which is at a negative potential, typically 10-600 V, with respect to the sample. The electrons are accelerated and focused into a beam, typically about 0.1 to 0.5 mm wide, by a series of electrodes serving as electron lenses. Some of the electrons incident on the sample surface are backscattered elastically, and diffraction can be detected if sufficient order exists on the surface. This typically requires a region of single crystal surface as wide as the electron beam, although sometimes polycrystalline surfaces such as highly oriented pyrolytic graphite (HOPG) are sufficient.\n\nA LEED detector usually contains three or four hemispherical concentric grids and a phosphor screen or other position-sensitive detector. The grids are used for screening out the inelastically scattered electrons. Most new LEED systems use a reverse view scheme, which has a minimized electron gun, and the pattern is viewed from behind through a transmission screen and a viewport. Recently, a new digitized position sensitive detector called a delay-line detector with better dynamic range and resolution has been developed.\n\nThe LEED contains a retarding field analyzer to block inelastically scattered electrons.\nBecause only spherical fields around the sampled point are allowed and the geometry of the sample and the surrounding area is not spherical, no field is allowed. Therefore, the first grid screens separates the space above the sample from the retarding field. The next grid is at a potential to block low energy electrons, it is called the suppressor or the gate. To make the retarding field homogeneous and mechanically more stable this grid often consists of two grids. The fourth grid is only necessary when the LEED is used like a tetrode and the current at the screen is measured, when it serves as screen between the gate and the anode.\n\nTo improve the measured signal in Auger electron spectroscopy, the gate voltage is scanned in a linear ramp. An RC circuit serves to derive the second derivative, which is then amplified and digitized. To reduce the noise, multiple passes are summed up. The first derivative is very large due to the residual capacitive coupling between gate and the anode and may degrade the performance of the circuit. By applying a negative ramp to the screen this can be compensated. It is also possible to add a small sine to the gate. A high Q RLC circuit is tuned to the second harmonic to detect the second derivative.\n\nA modern data acquisition system usually contains a CCD/CMOS camera pointed to the screen for diffraction pattern visualization and a computer for data recording and further analysis.\n\nThe shown images are examples of LEED diffraction patterns. The difference between image 1 and 2 is remarkable; where image 1 is of a clean (100) platinum-rhodium single crystal, and image 2 of the same crystal with CO adsorbed on the surface. The original surface order of the clean crystal is clearly visible in image 1, it shows a C(1x1) structure; the extra spots in image 2 are caused by the CO on the surface and are an example of a C(2x2) structure. The diffraction spots are generated by acceleration of elastically scattered electrons onto a hemispherical fluorescent screen, a retarding field analyzer. In the middle one can see the bright spot of the electron gun which generates the primary electron beam.\n\nThe basic reason for the high surface sensitivity of LEED is that for low-energy electrons the interaction between the solid and electrons is especially strong. Upon penetrating the crystal, primary electrons will lose kinetic energy due to inelastic scattering processes such as plasmon- and phonon excitations as well as electron-electron interactions.\nIn cases where the detailed nature of the inelastic processes is unimportant they are commonly treated by assuming an exponential decay of the primary electron beam intensity, I, in the direction of propagation:\n\nformula_1\n\nHere \"d\" is the penetration depth and formula_2 denotes the inelastic mean free path, defined as the distance an electron can travel before its intensity has decreased by the factor \"1/e\". While the inelastic scattering processes and consequently the electronic mean free path depend on the energy, it is relatively independent of the material. The mean free path turns out to be minimal (5–10 Å) in the energy range of low-energy electrons (20–200 eV). This effective attenuation means that only a few atomic layers are sampled by the electron beam and as a consequence the contribution of deeper atoms to the diffraction progressively decreases.\n\nKinematic diffraction is defined as the situation where electrons impinging on a well-ordered crystal surface are elastically scattered only once by that surface. In the theory the electron beam is represented by a plane wave with a wavelength in accordance to the de Broglie hypothesis:\n\nThe interaction between the scatterers present in the surface and the incident electrons is most conveniently described in reciprocal space. In three dimensions the primitive reciprocal lattice vectors are related to the real space lattice {a, b, c} in the following way:\n\nFor an incident electron with wave vector formula_5 and scattered wave vector formula_6, the condition for constructive interference and hence diffraction of scattered electron waves is given by the Laue condition\n\nwhere (\"h\",\"k\",\"l\") is a set of integers and\n\nis a vector of the reciprocal lattice. The magnitudes of the wave vectors are unchanged, i.e. formula_9, since only elastic scattering is considered.\nSince the mean free path of low energy electrons in a crystal is only a few angstroms, only the first few atomic layers contribute to the diffraction. This means that there are no diffraction conditions in the direction perpendicular to the sample surface. As a consequence the reciprocal lattice of a surface is a 2D lattice with rods extending perpendicular from each lattice point. The rods can be pictured as regions where the reciprocal lattice points are infinitely dense.\nTherefore, in the case of diffraction from a surface equation (1) reduces to the 2D form:\n\nwhere formula_11 and formula_12 are the primitive translation vectors of the 2D reciprocal lattice of the surface and formula_13 denote the component of respectively the reflected and incident wave vector parallel to the sample surface. formula_11 and formula_12 are related to the real space surface lattice in the following way:\n\nThe Laue condition equation (2) can readily be visualized using the Ewald's sphere construction.\nFigure 4 shows a simple illustration of this principle: The wave vector formula_17 of the incident electron beam is drawn such that it terminates at a reciprocal lattice point. The Ewald's sphere is then the sphere with radius formula_18 and origin at the center of the incident wave vector.\n\nBy construction, every wave vector centered at the origin and terminating at an intersection between a rod and the sphere will then satisfy the Laue condition and thus represent an allowed diffracted beam.\n\nFigure 4 shows the Ewald's sphere for the case of normal incidence of the primary electron beam, as would be the case in an actual LEED setup. It is apparent that the pattern observed on the fluorescent screen is a direct picture of the reciprocal lattice of the surface. The size of the Ewald's sphere and hence the number of diffraction spots on the screen is controlled by the incident electron energy. From the knowledge of the reciprocal lattice models for the real space lattice can be constructed and the surface can be characterized at least qualitatively in terms of the surface periodicity and the point group. Figure 5.a shows a model of an unreconstructed (100) face of a simple cubic crystal and the expected LEED pattern. The spots are indexed according to the values of \"h\" and \"k\".\n\nWe now consider the case of an overlaying superstructure on a substrate surface. If the LEED pattern of the underlying (1×1) surface is known, spots due to the superstructure can be identified as \"extra spots\" or \"super spots\". Figure 5.b shows the simple example of a (2×1) superstructure on a square lattice.\n\nFor a commensurate superstructure the symmetry and the rotational alignment with respect to adsorbent surface can be determined from the LEED pattern. This is easiest shown by using a matrix notation, where the primitive translation vectors of the superlattice {a,b} are linked to the primitive translation vectors of the underlying (1x1) lattice {a,b} in the following way\n\nThe matrix for the superstructure then is\n\nG is related to in the following way\n\nAn essential problem when considering LEED patterns is the existence of symmetrically equivalent domains. Domains may lead to diffraction patterns which have higher symmetry than the actual surface at hand. The reason is that usually the cross sectional area of the primary electron beam (~1 mm²) is large compared to the average domain size on the surface and hence the LEED pattern might be a superposition of diffraction beams from domains oriented along different axes of the substrate lattice.\n\nHowever, since the average domain size generally is larger than the coherence length of the probing electrons, interference between electrons scattered from different domains can be neglected. Therefore, the total LEED pattern emerges as the incoherent sum of the diffraction patterns associated with the individual domains.\n\nFigure 6 shows the superposition of the diffraction patterns for the two orthogonal domains (2x1) and (1x2) on a square lattice, i.e. for the case where one structure is just rotated by 90° with respect to the other. The (2x1) structure and the respective LEED pattern are shown in figure 5.b. It is apparent that the local symmetry of the surface structure is twofold while the LEED pattern exhibits a fourfold symmetry.\n\nFigure 1 shows a real diffraction pattern of the same situation for the case of a Si(100) surface. However, here the (2x1) structure is formed due to surface reconstruction.\n\nThe inspection of the LEED pattern gives a qualitative picture of the surface periodicity i.e. the size of the surface unit cell and to a certain degree of surface symmetries. However it will give no information about the atomic arrangement within a surface unit cell or the sites of adsorbed atoms. For instance if the whole superstructure in figure 5.b is shifted such that the atoms adsorb in bridge sites instead of on-top sites the LEED pattern will be the same.\n\nA more quantitative analysis of LEED experimental data can be achieved by analysis of so-called I-V curves, which are measurements of the intensity versus incident electron energy. The I-V curves can be recorded by using a camera connected to computer controlled data handling or by direct measurement with a movable Faraday cup. The experimental curves are then compared to computer calculations based on the assumption of a particular model system. The model is changed in an iterative process until a satisfactory agreement between experimental and theoretical curves is achieved. A quantitative measure for this agreement is the so-called \"reliability\"- or R-factor. A commonly used reliability factor is the one proposed by Pendry. It is expressed in terms of the logarithmic derivative of the intensity:\n\nThe R-factor is then given by:\n\nwhere formula_25 and formula_26 is the imaginary part of the electron self-energy. In general, formula_27 is considered as a good agreement, formula_28 is considered mediocre and formula_29 is considered a bad agreement. Figure 7 shows examples of the comparison between experimental I-V spectra and theoretical calculations.\n\nThe term \"dynamical\" stems from the studies of X-ray diffraction and describes the situation where the response of the crystal to an incident wave is included self-consistently and multiple scattering can occur. The aim of any dynamical LEED theory is to calculate the intensities of diffraction of an electron beam impinging on a surface as accurately as possible.\n\nA common method to achieve this is the self-consistent multiple scattering approach. One essential point in this approach is the assumption that the scattering properties of the surface, i.e. of the individual atoms, are known in detail. The main task then reduces to the determination of the effective wave field incident on the individual scatters present in the surface, where the effective field is the sum of the primary field and the field emitted from all the other atoms. This must be done in a self-consistent way, since the emitted field of an atom depends on the incident effective field upon it. Once the effective field incident on each atom is determined, the total field emitted from all atoms can be found and its asymptotic value far from the crystal then gives the desired intensities.\n\nA common approach in LEED calculations is to describe the scattering potential of the crystal by a \"muffin tin\" model, where the crystal potential can be imagined being divided up by non-overlapping spheres centered at each atom such that the potential has a spherically symmetric form inside the spheres and is constant everywhere else. The choice of this potential reduces the problem to scattering from spherical potentials, which can be dealt with effectively. The task is then to solve the Schrödinger equation for an incident electron wave in that \"muffin tin\" potential.\n\nIn LEED the exact atomic configuration of a surface is determined by a trial and error process where measured I-V curves are compared to computer-calculated spectra under the assumption of a model structure. From an initial reference structure a set of trial structures is created by varying the model parameters. The parameters are changed until an optimal agreement between theory and experiment is achieved. However, for each trial structure a full LEED calculation with multiple scattering corrections must be conducted. For systems with a large parameter space the need for computational time might become significant. This is the case for complex surfaces structures or when considering large molecules as adsorbates.\n\nTensor LEED is an attempt to reduce the computational effort needed by avoiding full LEED calculations for each trial structure. The scheme is as follows: One first defines a reference surface structure for which the I-V spectrum is calculated. Next a trial structure is created by displacing some of the atoms. If the displacements are small the trial structure can be considered as a small perturbation of the reference structure and first-order perturbation theory can be used to determine the I-V curves of a large set of trial structures.\n\nA real surface is not perfectly periodic but has many imperfections in the form of dislocations, atomic steps, terraces and the presence of unwanted adsorbed atoms. This departure from a perfect surface leads to a broadening of the diffraction spots and adds to the background intensity in the LEED pattern.\n\nSPA-LEED is a technique where the intensity of diffraction beams is measured in order to determine the diffraction spot profiles. The spots are sensitive to the irregularities in the surface structure and their examination therefore permits more-detailed conclusions about some surface characteristics. Using SPA-LEED may for instance permit a quantitative determination of the surface roughness, terrace sizes or surface steps.\n\n\n\n\n"}
{"id": "9843891", "url": "https://en.wikipedia.org/wiki?curid=9843891", "title": "Lyman–Werner photons", "text": "Lyman–Werner photons\n\nA Lyman-Werner photon is an ultraviolet photon with a photon energy in the range of 11.2 to 13.6 eV, corresponding to the energy range in which the Lyman and Werner absorption bands of molecular hydrogen (H) are found. A photon in this energy range, with a frequency that coincides with that of one of the lines in the Lyman or Werner bands, can be absorbed by H, placing the molecule in an excited electronic state. Radiative decay (that is, decay into photons) from this excited state occurs rapidly, with roughly 15% of these decays occurring into the vibrational continuum of the molecule, resulting in its dissociation. This two-step photodissociation process, known as the Solomon process, is one of the main mechanisms by which molecular hydrogen is destroyed in the interstellar medium.\nIn reference to the figure shown, Lyman-Werner photons are emitted as described below:\n"}
{"id": "2483815", "url": "https://en.wikipedia.org/wiki?curid=2483815", "title": "MV-algebra", "text": "MV-algebra\n\nIn abstract algebra, a branch of pure mathematics, an MV-algebra is an algebraic structure with a binary operation formula_1, a unary operation formula_2, and the constant formula_3, satisfying certain axioms. MV-algebras are the algebraic semantics of Łukasiewicz logic; the letters MV refer to the \"many-valued\" logic of Łukasiewicz. MV-algebras coincide with the class of bounded commutative BCK algebras.\n\nAn MV-algebra is an algebraic structure formula_4 consisting of\nwhich satisfies the following identities:\n\nBy virtue of the first three axioms, formula_18 is a commutative monoid. Being defined by identities, MV-algebras form a variety of algebras. The variety of MV-algebras is a subvariety of the variety of BL-algebras and contains all Boolean algebras.\n\nAn MV-algebra can equivalently be defined (Hájek 1998) as a prelinear commutative bounded integral residuated lattice formula_19 satisfying the additional identity formula_20\n\nA simple numerical example is formula_21 with operations formula_22 and formula_23 In mathematical fuzzy logic, this MV-algebra is called the \"standard MV-algebra\", as it forms the standard real-valued semantics of Łukasiewicz logic.\n\nThe \"trivial\" MV-algebra has the only element 0 and the operations defined in the only possible way, formula_24 and formula_25\n\nThe \"two-element\" MV-algebra is actually the two-element Boolean algebra formula_26 with formula_1 coinciding with Boolean disjunction and formula_8 with Boolean negation. In fact adding the axiom formula_29 to the axioms defining an MV-algebra results in an axiomatization of Boolean algebras.\n\nIf instead the axiom added is formula_30, then the axioms define the MV algebra corresponding to the three-valued Łukasiewicz logic Ł. Other finite linearly ordered MV-algebras are obtained by restricting the universe and operations of the standard MV-algebra to the set of formula_31 equidistant real numbers between 0 and 1 (both included), that is, the set formula_32 which is closed under the operations formula_1 and formula_8 of the standard MV-algebra; these algebras are usually denoted MV.\n\nAnother important example is \"Chang's MV-algebra\", consisting just of infinitesimals (with the order type ω) and their co-infinitesimals.\n\nChang also constructed an MV-algebra from an arbitrary totally ordered abelian group \"G\" by fixing a positive element \"u\" and defining the segment [0, \"u\"] as { \"x\" ∈ \"G\" | 0 ≤ \"x\" ≤ \"u\" }, which becomes an MV-algebra with \"x\" ⊕ \"y\" = min(\"u\", \"x\"+\"y\") and ¬\"x\" = \"u\"−\"x\". Furthermore, Chang showed that every linearly ordered MV-algebra is isomorphic to an MV-algebra constructed from a group in this way.\n\nD. Mundici extended the above construction to abelian lattice-ordered groups. If \"G\" is such a group with strong (order) unit \"u\", then the \"unit interval\" { \"x\" ∈ \"G\" | 0 ≤ \"x\" ≤ \"u\" } can be equipped with ¬\"x\" = \"u\"−\"x\", \"x\" ⊕ \"y\" = \"u\"∧ (x+y), \"x\" ⊗ \"y\" = 0∨(\"x\"+\"y\"−\"u\"). This construction establishes a categorical equivalence between lattice-ordered abelian groups with strong unit and MV-algebras.\n\nC. C. Chang devised MV-algebras to study many-valued logics, introduced by Jan Łukasiewicz in 1920. In particular, MV-algebras form the algebraic semantics of Łukasiewicz logic, as described below.\n\nGiven an MV-algebra \"A\", an \"A\"-valuation is a homomorphism from the algebra of propositional formulas (in the language consisting of formula_35 and 0) into \"A\". Formulas mapped to 1 (or formula_80) for all \"A\"-valuations are called \"A\"-tautologies. If the standard MV-algebra over [0,1] is employed, the set of all [0,1]-tautologies determines so-called infinite-valued Łukasiewicz logic.\n\nChang's (1958, 1959) completeness theorem states that any MV-algebra equation holding in the standard MV-algebra over the interval [0,1] will hold in every MV-algebra. Algebraically, this means that the standard MV-algebra generates the variety of all MV-algebras. Equivalently, Chang's completeness theorem says that MV-algebras characterize infinite-valued Łukasiewicz logic, defined as the set of [0,1]-tautologies.\n\nThe way the [0,1] MV-algebra characterizes all possible MV-algebras parallels the well-known fact that identities holding in the two-element Boolean algebra hold in all possible Boolean algebras. Moreover, MV-algebras characterize infinite-valued Łukasiewicz logic in a manner analogous to the way that Boolean algebras characterize classical bivalent logic (see Lindenbaum–Tarski algebra).\n\nIn 1984, Font, Rodriguez and Torrens introduced the Wajsberg algebra as an alternative model for the infinite-valued Łukasiewicz logic. Wajsberg algebras and MV-algebras are isomorphic.\n\nIn the 1940s Grigore Moisil introduced his Łukasiewicz–Moisil algebras (LM-algebras) in the hope of giving algebraic semantics for the (finitely) \"n\"-valued Łukasiewicz logic. However, in 1956 Alan Rose discovered that for \"n\" ≥ 5, the Łukasiewicz–Moisil algebra does not model the Łukasiewicz \"n\"-valued logic. Although C. C. Chang published his MV-algebra in 1958, it is faithful model only for the ℵ-valued (infinitely-many-valued) Łukasiewicz–Tarski logic. For the axiomatically more complicated (finitely) \"n\"-valued Łukasiewicz logics, suitable algebras were published in 1977 by Revaz Grigolia and called MV-algebras. MV-algebras are a subclass of LM-algebras; the inclusion is strict for \"n\" ≥ 5.\n\nThe MV-algebras are MV-algebras which satisfy some additional axioms, just like the \"n\"-valued Łukasiewicz logics have additional axioms added to the ℵ-valued logic.\n\nIn 1982 Roberto Cignoli published some additional constraints that added to LM-algebras are proper models for \"n\"-valued Łukasiewicz logic; Cignoli called his discovery \"proper \"n\"-valued Łukasiewicz algebras\". The LM-algebras that are also MV-algebras are precisely Cignoli’s proper n-valued Łukasiewicz algebras.\n\nMV-algebras were related by Daniele Mundici to approximately finite-dimensional C*-algebras by establishing a bijective correspondence between all isomorphism classes of AF C*-algebras with lattice-ordered dimension group and all isomorphism classes of countable MV algebras. Some instances of this correspondence include:\n\nThere are multiple frameworks implementing fuzzy logic (type II), and most of them implement what has been called a multi-adjoint logic. This is no more than the implementation of a MV-algebra.\n\n\n\n"}
{"id": "7399614", "url": "https://en.wikipedia.org/wiki?curid=7399614", "title": "Maksim Ponomaryov", "text": "Maksim Ponomaryov\n\nMaksim Vladimirovich Ponomaryov (), born on February 20, 1980, captain of Russian Air Force, and was a Russian cosmonaut, selected in 2006, who retired in 2012 after no missions to space.\n\n"}
{"id": "3571585", "url": "https://en.wikipedia.org/wiki?curid=3571585", "title": "Mangrove swamp", "text": "Mangrove swamp\n\nA mangrove swamp was a distinct saline woodland or shrubland habitat formed by mangrove trees. They are characterized by depositional coastal environments, where fine sediments (often with high organic content) collect in areas protected from high-energy wave action. The saline conditions tolerated by various mangrove species range from brackish water, through pure seawater (3 to 4%), to water concentrated by evaporation to over twice the salinity of ocean seawater (up to 9%).\n\nMangrove swamps are home and sanctuaries for thousands of aquatic bird species, including:\n\n\nMangrove swamps protect coastal areas from erosion, storm surge (especially during hurricanes), and tsunamis. The mangroves' massive root systems are efficient at dissipating wave energy. Likewise, they slow down tidal water enough so its sediment is deposited as the tide comes in, leaving all except fine particles when the tide ebbs. In this way, mangroves build their own environments. Because of the uniqueness of mangrove ecosystems and the protection against erosion they provide, they are often the object of conservation programs, including national biodiversity action plans.\n\nThe protective value of mangrove swamps is sometimes overstated. Wave energy is typically low in areas where mangroves grow, so their effect on erosion can only be measured over long periods. Their capacity to limit high-energy wave erosion is limited to events such as storm surges and tsunamis. Erosion often occurs on the outer sides of bends in river channels that wind through mangroves, while new stands of mangroves are appearing on the inner sides where sediment is accruing.\n\n"}
{"id": "51492765", "url": "https://en.wikipedia.org/wiki?curid=51492765", "title": "Marianne Horak", "text": "Marianne Horak\n\nMarianne Horak (born 1944) is a Swiss-Australian entomologist who specialises in Australian Lepidoptera, particularly the phycitine and tortricid moths. She also did important research on the scribbly gum moths, during which eleven new species of \"Ogmograptis\" were discovered.\n\nHorak was born in Glarus, Switzerland, where she was inspired to study entomology from her childhood growing up in a valley in the Alps. She studied at the Federal Institute of Technology (ETH) in Zürich, earning her M.Sc. in 1970 and Ph.D. in 1983. She did extensive field work in New Zealand (1967–69), New Guinea (1971–73), and Indonesia (1985) before settling permanently in Australia.\n\nShe is the current editor-in-chief of \"Monographs of Australian Lepidoptera\", chairperson of the Australian Lepidoptera Research Endowment, and honorary research fellow in Lepidoptera systematics at the Australian National Insect Collection at CSIRO, where she works as Lepidoptera curator and was head of Lepidoptera research until her retirement in 2010.\n\nHorak has discovered several new species of Lepidoptera, including multiple species of \"Cadra\", \"Heterochorista\", and \"Ogmograptis\". She also is the taxon authority for several genera, including \"Aglaogonia\", \"Atriscripta\" and \"Cnecidophora\".\n\nHorak was the first recipient of the J.O. Westwood Medal for excellence in insect taxonomy for \"her outstanding monograph entitled \"The Olethreutine Moths of Australia (Lepidoptera: Tortricidae)\"\". The moth species \"Coleophora horakae\", \"Hilarographa mariannae\", and \"Myrtartona mariannae\" are dedicated to her.\n\nHorak is considered one of the worldwide leading experts on the systematics of Tortricidae.\n\nShe was previously married to the Austrian mycologist Egon Horak.\n"}
{"id": "35400879", "url": "https://en.wikipedia.org/wiki?curid=35400879", "title": "Martin Albrow", "text": "Martin Albrow\n\nMartin Albrow (born 1937) is a British sociologist, noted for his works on globalization, the theory of the global age and global civil society. He was appointed in 1963 as the first full-time sociologist at Reading University, and subsequently worked at University College Cardiff, where he was Head of Department, and at Roehampton University. He has also held visiting or guest positions at Ludwig Maximilian University of Munich, the London School of Economics, the State University of New York at Stony Brook, the Beijing Foreign Studies University, and the University of Bonn.\n\nAlbrow was President of the British Sociological Association from 1985 to 1987, and the editor-in-chief of its journal \"Sociology\" from 1981 to 1984. Additionally, he was the founding editor of the International Sociological Association's journal, \"International Sociology\". He wrote \"The Global Age: State and Society beyond Modernity\", awarded the 1997 European Amalfi Prize, which argued against the view that globalization was an inevitable one-way process, and that a new age had supplanted both the modern and postmodern ages. He is a Fellow of the Academy of Social Sciences.\n\nAlbrow lives in London with his wife, Sue Owen.\n\nHis books include: \"Bureaucracy\", London, Pall Mall, 1970; \"Max Weber's Construction of Social Theory\", London, Macmillan, 1990; \"The Global Age: State and Society beyond Modernity\", Cambridge, Polity 1996; \"Do Organizations Have Feelings?\", London and New York, Routledge, 1997; \"Sociology: The Basics\", London and New York, Routledge, 1999; and \"Global Age Essays on Social and Cultural Change\", Frankfurt am Main, Klosterman, 2014. The English-language edition of his book \"China's Role in a Shared Human Future\" was published in 2018.The book was launched at the London Book Fair.\n"}
{"id": "326471", "url": "https://en.wikipedia.org/wiki?curid=326471", "title": "Mathematics education", "text": "Mathematics education\n\nIn contemporary education, mathematics education is the practice of teaching and learning mathematics, along with the associated scholarly research.\n\nResearchers in mathematics education are primarily concerned with the tools, methods and approaches that facilitate practice or the study of practice; however, mathematics education research, known on the continent of Europe as the didactics or pedagogy of mathematics, has developed into an extensive field of study, with its own concepts, theories, methods, national and international organisations, conferences and literature. This article describes some of the history, influences and recent controversies.\n\nElementary mathematics was part of the education system in most ancient civilisations, including Ancient Greece, the Roman Empire, Vedic society and ancient Egypt. In most cases, a formal education was only available to male children with a sufficiently high status, wealth or caste.\nIn Plato's division of the liberal arts into the trivium and the quadrivium, the quadrivium included the mathematical fields of arithmetic and geometry. This structure was continued in the structure of classical education that was developed in medieval Europe. Teaching of geometry was almost universally based on Euclid's \"Elements\". Apprentices to trades such as masons, merchants and money-lenders could expect to learn such practical mathematics as was relevant to their profession.\n\nIn the Renaissance, the academic status of mathematics declined, because it was strongly associated with trade and commerce, and considered somewhat un-Christian. Although it continued to be taught in European universities, it was seen as subservient to the study of Natural, Metaphysical and Moral Philosophy. The first modern arithmetic curriculum (starting with addition, then subtraction, multiplication, and division) arose at reckoning schools in Italy in the 1300s. Spreading along trade routes, these methods were designed to be used in commerce. They contrasted with Platonic math taught at universities, which was more philosophical and concerned numbers as concepts rather than calculating methods. They also contrasted with mathematical methods learned by artisan apprentices, which were specific to the tasks and tools at hand. For example, the division of a board into thirds can be accomplished with a piece of string, instead of measuring the length and using the arithmetic operation of division.\n\nThe first mathematics textbooks to be written in English and French were published by Robert Recorde, beginning with \"The Grounde of Artes\" in 1540. However, there are many different writings on mathematics and mathematics methodology that date back to 1800 BCE. These were mostly located in Mesopotamia where the Sumerians were practicing multiplication and division. There are also artifacts demonstrating their own methodology for solving equations like the quadratic equation. After the Sumerians some of the most famous ancient works on mathematics come from Egypt in the form of the Rhind Mathematical Papyrus and the Moscow Mathematical Papyrus. The more famous Rhind Papyrus has been dated to approximately 1650 BCE but it is thought to be a copy of an even older scroll. This papyrus was essentially an early textbook for Egyptian students.\n\nThe social status of mathematical study was improving by the seventeenth century, with the University of Aberdeen creating a Mathematics Chair in 1613, followed by the Chair in Geometry being set up in University of Oxford in 1619 and the Lucasian Chair of Mathematics being established by the University of Cambridge in 1662. However, it was uncommon for mathematics to be taught outside of the universities. Isaac Newton, for example, received no formal mathematics teaching until he joined Trinity College, Cambridge in 1661.\n\nIn the 18th and 19th centuries, the Industrial Revolution led to an enormous increase in urban populations. Basic numeracy skills, such as the ability to tell the time, count money and carry out simple arithmetic, became essential in this new urban lifestyle. Within the new public education systems, mathematics became a central part of the curriculum from an early age.\n\nBy the twentieth century, mathematics was part of the core curriculum in all developed countries.\n\nDuring the twentieth century, mathematics education was established as an independent field of research. Here are some of the main events in this development:\n\n\nIn the 20th century, the cultural impact of the \"electronic age\" (McLuhan) was also taken up by educational theory and the teaching of mathematics. While previous approach focused on \"working with specialized 'problems' in arithmetic\", the emerging structural approach to knowledge had \"small children meditating about number theory and 'sets'.\"\n\nAt different times and in different cultures and countries, mathematics education has attempted to achieve a variety of different objectives. These objectives have included:\n\n\nThe method or methods used in any particular context are largely determined by the objectives that the relevant educational system is trying to achieve. Methods of teaching mathematics include the following:\n\n\n\nDifferent levels of mathematics are taught at different ages and in somewhat different sequences in different countries. Sometimes a class may be taught at an earlier age than typical as a special or honors class.\n\nElementary mathematics in most countries is taught in a similar fashion, though there are differences. In the United States fractions are typically taught starting from 1st grade, whereas in other countries they are usually taught later, since the metric system does not require young children to be familiar with them. Most countries tend to cover fewer topics in greater depth than in the United States. K-12 topics include elementary arithmetic (addition, subtraction, multiplication, and division), and pre-algebra.\n\nIn most of the U.S., algebra, geometry and analysis (pre-calculus and calculus) are taught as separate courses in different years of high school. Mathematics in most other countries (and in a few U.S. states) is integrated, with topics from all branches of mathematics studied every year. Students in many countries choose an option or pre-defined course of study rather than choosing courses \"à la carte\" as in the United States. Students in science-oriented curricula typically study differential calculus and trigonometry at age 16–17 and integral calculus, complex numbers, analytic geometry, exponential and logarithmic functions, and infinite series in their final year of secondary school. Probability and statistics may be taught in secondary education classes.\n\nScience and engineering students in colleges and universities may be required to take multivariable calculus, differential equations, linear algebra. Applied mathematics is also used in specific majors; for example, civil engineers may be required to study fluid mechanics, while \"math for computer science\" might include graph theory, permutation, probability, and proofs. Mathematics students would continue to study potentially any area.\n\nThroughout most of history, standards for mathematics education were set locally, by individual schools or teachers, depending on the levels of achievement that were relevant to, realistic for, and considered socially appropriate for their pupils.\n\nIn modern times, there has been a move towards regional or national standards, usually under the umbrella of a wider standard school curriculum. In England, for example, standards for mathematics education are set as part of the National Curriculum for England, while Scotland maintains its own educational system. In the USA, the National Governors Association Center for Best Practices and the Council of Chief State School Officers have published the national mathematics Common Core State Standards Initiative.\n\nMa (2000) summarised the research of others who found, based on nationwide data, that students with higher scores on standardised mathematics tests had taken more mathematics courses in high school. This led some states to require three years of mathematics instead of two. But because this requirement was often met by taking another lower level mathematics course, the additional courses had a “diluted” effect in raising achievement levels.\n\nIn North America, the National Council of Teachers of Mathematics has published the Principles and Standards for School Mathematics, which boosted the trend towards reform mathematics. In 2006, they released \"Curriculum Focal Points\", which recommend the most important mathematical topics for each grade level through grade 8. However, these standards are enforced as American states and Canadian provinces choose. A US state's adoption of the Common Core State Standards in mathematics is at the discretion of the state, and is not mandated by the Federal Government. \"States routinely review their academic standards and may choose to change or add onto the standards to best meet the needs of their students.\" The National Council of Teachers of Mathematics has state affiliates that have different education standards at the state level. For example, Missouri has the Missouri Council of Teachers of Mathematics (MCTM) which has its own pillars and standards of education listed on its website. The MCTM also offers membership opportunities to teachers and future teachers so they can stay up to date on the changes in math educational standards. \n\n\"Robust, useful theories of classroom teaching do not yet exist\". However, there are useful theories on how children learn mathematics and much research has been conducted in recent decades to explore how these theories can be applied to teaching. The following results are examples of some of the current findings in the field of mathematics education:\n\n\n\n\n\n\n\nAs with other educational research (and the social sciences in general), mathematics education research depends on both quantitative and qualitative studies. Quantitative research includes studies that use inferential statistics to answer specific questions, such as whether a certain teaching method gives significantly better results than the status quo. The best quantitative studies involve randomized trials where students or classes are randomly assigned different methods in order to test their effects. They depend on large samples to obtain statistically significant results.\n\nQualitative research, such as case studies, action research, discourse analysis, and clinical interviews, depend on small but focused samples in an attempt to understand student learning and to look at how and why a given method gives the results it does. Such studies cannot conclusively establish that one method is better than another, as randomized trials can, but unless it is understood \"why\" treatment X is better than treatment Y, application of results of quantitative studies will often lead to \"lethal mutations\" of the finding in actual classrooms. Exploratory qualitative research is also useful for suggesting new hypotheses, which can eventually be tested by randomized experiments. Both qualitative and quantitative studies therefore are considered essential in education—just as in the other social sciences. Many studies are “mixed”, simultaneously combining aspects of both quantitative and qualitative research, as appropriate.\n\nThere has been some controversy over the relative strengths of different types of research. Because randomized trials provide clear, objective evidence on “what works”, policy makers often take only those studies into consideration. Some scholars have pushed for more random experiments in which teaching methods are randomly assigned to classes. In other disciplines concerned with human subjects, like biomedicine, psychology, and policy evaluation, controlled, randomized experiments remain the preferred method of evaluating treatments. Educational statisticians and some mathematics educators have been working to increase the use of randomized experiments to evaluate teaching methods. On the other hand, many scholars in educational schools have argued against increasing the number of randomized experiments, often because of philosophical objections, such as the ethical difficulty of randomly assigning students to various treatments when the effects of such treatments are not yet known to be effective, or the difficulty of assuring rigid control of the independent variable in fluid, real school settings.\n\nIn the United States, the National Mathematics Advisory Panel (NMAP) published a report in 2008 based on studies, some of which used randomized assignment of treatments to experimental units, such as classrooms or students. The NMAP report's preference for randomized experiments received criticism from some scholars. In 2010, the What Works Clearinghouse (essentially the research arm for the Department of Education) responded to ongoing controversy by extending its research base to include non-experimental studies, including regression discontinuity designs and single-case studies.\n\nThe following are some of the people who have had a significant influence on the teaching of mathematics at various periods in history:\n\n\nThe following people all taught mathematics at some stage in their lives, although they are better known for other things:\n\n\n\n\n\n\n\n"}
{"id": "50504678", "url": "https://en.wikipedia.org/wiki?curid=50504678", "title": "NGC 137", "text": "NGC 137\n\nNGC 137 is a lenticular galaxy in the constellation of Pisces. It was discovered by William Herschel on November 23, 1785.\n"}
{"id": "51494017", "url": "https://en.wikipedia.org/wiki?curid=51494017", "title": "NGC 165", "text": "NGC 165\n\nNGC 165 is a barred spiral galaxy located in the constellation Cetus. It was discovered in 1882 by Wilhelm Tempel and was described by as \"faint, large, star in centre, eastern of 2\" by John Louis Emil Dreyer.\n"}
{"id": "19494116", "url": "https://en.wikipedia.org/wiki?curid=19494116", "title": "Neuralgia-inducing cavitational osteonecrosis", "text": "Neuralgia-inducing cavitational osteonecrosis\n\nNeuralgia-inducing cavitational osteonecrosis (NICO) is a controversial diagnosis whereby a putative jawbone cavitation causes chronic facial neuralgia; this is different from osteonecrosis of the jaw.. In NICO the pain is said to result from the degenerating nerve (\"neuralagia\"). The condition is probably rare, if it does exist.\n\nAlso called Ratner's bone cavity, a neuralgia-inducing cavitational osteonecrosis was first described in dental literature by G V Black in 1920. Several decades later, oral pathologist Jerry E Bouquot took especial interest in NICO. \n\nThe diagnostic criteria for NICO are imprecise, and the research offered to support it is flawed. The diagnosis is popular among holistic dentists who attempt to treat NICO by surgically removing the dead bone they say is causing the pain.\n\nIt has been rejected as quackery by some dentists and maxillofacial surgeons. In its position statement, dated 1996, the American Association of Endodontists asserted that although NICO occur and are treatable in toothless areas, NICO occurrence and treatment at endodontically treated teeth is generally implausible, that the diagnosis ought to be a last resort, and that routine extraction of endodontically treated teeth is misguided.\n\n\n"}
{"id": "4786254", "url": "https://en.wikipedia.org/wiki?curid=4786254", "title": "Ogden tables", "text": "Ogden tables\n\nOgden tables are a set of statistical tables and other information for use in court cases in the UK.\n\nTheir purpose is to make it easier to calculate future losses in personal injury and fatal accident cases. The tables take into account life expectancy and provide a range of discount rates from -2.0% to 3.0% in steps of 0.5%. The discount rate is fixed by the Lord Chancellor under section 1 of the Damages Act 1996; as of 27 February 2017, this rate is -0.75%.\n\nThe most recent edition of the tables (7th Edition) makes changes to the discount rate range (previously 0.0% to 5.0% revised to -2.0% to 3.0%) to allow for a revision of the discount rate by the Lord Chancellor (currently under consideration as at 24 October 2011) and to provide for the implications of the case of \"Helmot v. Simon\".\n\nThe Civil Evidence Act 1995 permitted their use in the UK and they were first used by the House of Lords in \"Wells v. Wells\" in July 1999.\n\nThe full, and official, name of the tables is \"Actuarial Tables with explanatory notes for use in Personal Injury and Fatal Accident Cases\" but the unofficial name became common parlance following the Civil Evidence Act 1995, where this shorthand name was used as a subheading – Sir Michael Ogden QC having been the chairman of the Working Party for the first four editions\n\nThere are 28 tables of data in the Ogden Tables. Table 1 (Males) and Table 2 (Females) are for life expectancy and loss for life. Tables 3 to 14 are for loss of earnings up to various retirement ages. Tables 15 to 26 are for loss of pension from various retirement ages. Table 27 is for discounting to a period in the future and Table 28 is for a recurring loss over a period of time.\n\nTo calculate life expectancy, you need to use Table 1 (for males) or Table 2 (for females) and use the data in the 0% column. So for a 45 year old female, using Table 2 you would look down the first column to find 45 and then across to the 0% column which gives a figure of 43.93. In cases where the age is not a whole number, i.e. female who is 45.75 years, then you use the figure for 45 years (43.93) and the figure for 46 years (42.87) and interpolate between the two (46-45.75) x 43.93 + (45.75-45) x 42.87 to give 43.14 years.\n\nIf the claimant is to suffer a loss that will last their entire life, you need to use Table 1 (for males) or Table 2 (for females) and use the data in the 2.5% column. So for a 50-year-old male, using Table 1 you would look down to first column to find 50 and then across to the 2.5% column which gives a figure of 22.69.\n\nIf the claimant needs to pay for something in the future, then the present value can be worked out using Table 27. Look up the period in the future in the first column and then across to the 2.5% column for the multiplier. For example, a purchase required in 10 years time would need to multiplier by 0.7812.\n\nIf the claimant has a recurring loss over a period of say 15 years, then use Table 28 looking up 15 in the first column and then across to the 2.5% column which gives a multiplier of 12.54. If the loss does not start until some time in the future, then you can combine Table 27 and Table 28 to give an overall multiplier. For example a loss over a period of 15 years that starts in 10 years time would have a Table 27 multiplier of 0.7812 and a Table 28 multiplier of 12.54 giving an overall multiplier of 9.80.\n\n"}
{"id": "3018033", "url": "https://en.wikipedia.org/wiki?curid=3018033", "title": "Oldřich Pelčák", "text": "Oldřich Pelčák\n\nOldřich Pelčák (born November 2, 1943 in Zlín, Czechoslovakia) was a Czech cosmonaut and engineer. He graduated from Gagarin Air Force Military Academy. In 1976, Pelčák was selected as backup of Vladimír Remek for the Soyuz 28 mission. They were the first cosmonauts who were neither Americans nor Soviets.\n\n"}
{"id": "6501751", "url": "https://en.wikipedia.org/wiki?curid=6501751", "title": "Przybylski's Star", "text": "Przybylski's Star\n\nPrzybylski's Star , or HD 101065, is a rapidly oscillating Ap star at roughly from the Sun in the southern constellation of Centaurus.\n\nIn 1961, the Polish-Australian astronomer Antoni Przybylski discovered that this star had a peculiar spectrum that would not fit into the standard framework for stellar classification. Przybylski's observations indicated unusually low amounts of iron and nickel in the star's spectrum, but higher amounts of unusual elements like strontium, holmium, niobium, scandium, yttrium, caesium, neodymium, praseodymium, thorium, ytterbium, and uranium. In fact, at first Przybylski doubted that iron was present in the spectrum at all. Modern work shows that the iron-group elements are somewhat below normal in abundance, but it is clear that the lanthanides and other exotic elements are highly overabundant. Lanthanide elements are from 1000 to 10,000 times more abundant than in the Sun. As a result of these peculiar abundances this star belongs firmly in the Ap star class.\n\nPrzybylski's Star also contains many different short-lived actinide elements with actinium, protactinium, neptunium, plutonium, americium, curium, berkelium, californium, and einsteinium being detected. Other radioactive elements discovered in this star include technetium and promethium.\n\nCompared to neighboring stars, HD 101065 has a high peculiar velocity of .\n\nBecause of the odd properties of this star, there are numerous theories about why the oddities occur. The most interesting of them is that the star contains some long-lived nuclides from the island of stability (e.g. Fl, 120, or 126) and that the observed short-lived actinides are the daughters of these progenitors, occurring in secular equilibrium with their parents.\n\nHD 101065 is the prototype star of the rapidly oscillating Ap star (roAP) variable star class. In 1978, it was discovered to pulsate photometrically with a period of 12.15 min.\n\nA potential companion had also been detected, a 14th magnitude star (in infrared) 8 arc seconds away. This would be a separation of nearly 1,000 AU. However, Gaia Data Release 2 suggests that this star lies at a distance of light-years, more than twice the distance of Przybylski's Star.\n\n\n"}
{"id": "47986604", "url": "https://en.wikipedia.org/wiki?curid=47986604", "title": "Simon Ratcliffe (astronomer)", "text": "Simon Ratcliffe (astronomer)\n\nSimon Ratcliffe is a South African astronomer known for his promotion of the Square Kilometre Array project. The media have dubbed him the \"barefoot astronomer\" for his habit of working without shoes.\n\n"}
{"id": "8745118", "url": "https://en.wikipedia.org/wiki?curid=8745118", "title": "Spacesuits in fiction", "text": "Spacesuits in fiction\n\nScience fiction authors have designed imaginary spacesuits for their characters almost since the beginning of fiction set in space.\n\nOften, comic book creators seem unaware of the effects of internal pressure which tends to inflate a spacesuit in vacuum, and draw their imaginary spacesuits as hanging in folds like a boilersuit; this can often be seen in the Dan Dare stories, where the artist often drew from actual or photographed posed actors. Many space story writers merely mention a \"spacesuit\" without considering or describing design details, in the same way as they mention a raygun or a spaceship without considering how its mechanism would work.\n\nThe breathing apparatus which is part of the Primary Life Support System of real space suits is always a rebreather type system. However, in illustrations in fiction such as comics, a spacesuit's life support system is often largely composed of two big backpack cylinders, as if it was open circuit; at least one fictional scenario has liquid breathing spacesuits.\n\nFrom \"Edison's Conquest of Mars\" (1898):\n\nThis illustration of the suit appears to be skintight (note the wrinkles), and to have a soft hood with a built-in fullface mask, rather than a hard helmet, although according to the story the suits had helmets.\n\nThis common early idea for a spacesuit would have not worked in reality for several reasons:\n\nSkintight spacesuits (skinsuits) appear in the original Buck Rogers comics published from 1928 on. This comic was so popular that expressions such as \"Buck Rogers outfit\" for real protective suits that look somewhat like spacesuits entered common usage.\n\nWith the rise of the Science fiction pulp magazines in the 1920s many depictions of imaginary spacesuits were created from scratch by artists such as Frank R. Paul, often appearing on the covers of the magazines. Very often these artists' creations were absurd, with such errors as a helmet whose neck hole is too narrow for the head to get through.\n\nOften fictional spacesuits are drawn with two large backpack cylinders as their only life-support gear, as if the exhaled gas is vented to space as in an ordinary open-circuit scuba set.\n\nThe Lensman series by E.E. \"Doc\" Smith features armored spacesuits used in hand-to-hand combat. Some especially heavily armored spacesuits in the series use motors to help the wearer move about.\n\nFollowing World War II, fictional spacesuits were influenced both by the real life pressure suits and G-suits which had seen use during the war for high-altitude aviation and also by the speculative articles on space travel which were published in magazines like the Saturday Evening Post and Collier's Weekly by such space pioneers as Wernher von Braun and Willy Ley and which featured carefully considered spacesuit designs.\n\nSome early space travel fiction films showed characters in spacesuits much more often than Star Trek and afterwards.\n\nIn H.G. Wells's original novel, \"The First Men in the Moon\", published in 1901, the Moon has a breathable atmosphere during its two-week-long day and spacesuits are not needed; the spacecraft has an airtight hatch, but no airlock.\nIn the film version, made in 1964, the Moon has no atmosphere and no surface vegetation. Two types of spacesuits are featured.\n\nIn the Dan Dare comic series, which started in April 1950 in the \"Eagle\" comic, the standard Spacefleet spacesuit had no backpack, had a corselet as per Standard Diving Dress, and its life-support system was stated to be between the layers of a double-walled helmet. The spacesuits used in the Dan Dare scenario \"Operation Saturn\" by the villain Blasco are a different design and have small life-support backpacks. The Dan Dare stories also show various alien spacesuits.\n\nAuthor Robert A. Heinlein's novel \"Have Space Suit—Will Travel\" (1958) drew both on these contemporary articles and on his experience designing pressure suits during World War II and featured a detailed description of a very realistic space suit with constant volume joints and fixed helmet and shoulder yoke, which was entered through a frontal gasketed zipper (similar to that in a drysuit).\n\nFront cover illustrations (one shown here, one linked to in its caption) for the novel obviously inspired by contemporary diving apparatus show its life-support backpack as a correctly drawn old-type open-circuit two-cylinder aqualung as used for scuba diving with manifold and large round regulator and A-clamp. The artist avoided the error found in most comic-strip drawings of old-type aqualungs, of drawing each breathing tube coming directly from a cylinder top and no regulator. But to make this type of aqualung (as shown here) work in space, its regulator's existing perforated \"wet-side\" cover would have to be replaced by a sealed cover with a spring-loaded exit valve to keep a breathable pressure on the \"wet\" side of the regulator diaphragm. And the whole breathing system would have to be checked for leaks which would be harmless in scuba diving but would blow in space vacuum.\n\nThe spacesuits in these drawings differ much, but all depict the helmet base as being wide enough for the wearer to get it on over his head, showing that their artists had paid little attention to the writer's detailed descriptions.\nIn a description of the spacesuit Heinlein appears to be confused about the various effects of oxygen toxicity and bends and nitrogen narcosis.\n\nHeinlein's description of pressure regulation came very close to the experience of astronauts in the Apollo program. His characters preferred to keep the pressure of their suits just high enough for survival, but not high enough to make it difficult to move around, much like the selected design pressure range of the real Apollo A7L suits.\n\nThe life support system of the suits in \"Have Space Suit—Will Travel\" was very similar to the backup Oxygen Purge System on the real Apollo Primary Life Support System, the only major difference being that the Apollo suits had a largely automatic pressure regulator, and Heinlein's suit had manual pressure regulation.\n\nOne major component of modern pressure suit Primary Life Support System backpacks which he missed was the lithium hydroxide canister which absorbs carbon dioxide from the air in the suit: see rebreather. Without this, his suit's breathing apparatus would have to be open circuit and limited to approximately two hours on a filling of oxygen or air, with the time varying according to exertion and cylinder size and his body size.\n\nAnother was the cooling system. He correctly recognised that overheating would be a major problem for the wearer of the suit. His cooling system was the same as the Apollo oxygen purge system: waste oxygen by letting it flow at a high rate and use it to dump heat. In practice, real suits used a water supply feeding a sublimator to provide cooling.\n\nIn \"The Cat Who Walks Through Walls\" the suits are strongly influenced by experience in the space program. He correctly describes a technique for helping an injured man in a pressure suit by decompressing the suit for less than a minute. Earlier books such as \"Rocket Ship Galileo\" described horrible injuries for people decompressed for short times.\n\nIn \"The Moon Is a Harsh Mistress\" Heinlein has people going onto the moon surface for about half a day in daylight and suffering from radiation exposure. In practice, overheating was the biggest risk of lunar surface operations, and cooling systems were easy to build.\n\nThe front cover of the first issue of the German pulp science fiction series Perry Rhodan, published in 1961, design with two large backpack cylinders instead of a modern life-support pack.\n\n\n\nAfter the establishment of NASA, and the first space missions, fictional spacesuits tended to follow real spacesuit design, including such features as a large rectangular backpack to hold life support components, except in low-budget science fiction movies and comics which were still inspired more by imagination than by reality.\n\nThis film by Stanley Kubrick was groundbreaking for its time and for decades later. At the debut of , (1968) cosmonaut Alexey Leonov and astronaut Ed White had made space walks starting in 1965, followed by a handful of other astronauts, while the first lunar landing was still over a year in the future. Thus space suits had been tested but not on the lunar surface. In the film, space suits play notable roles several times, including in the lunar EVA when the monolith is inspected, and during different events in the journey aboard the Discovery One. The design of the Space Odyssey helmets with a down-facing face plate and jutting top plate was (is) most strikingly different from both the actual 1960's designs including the Apollo lunar suit, and from the advancements in design seen in the decades leading to the real 2001. Kubrick and co-author Arthur C. Clark forecast rather optimistically that by 2001 there would be ongoing exploration and settlement of the moon and an advanced space construction program, and designs based on their speculations for 30 years later were bold but are still scientifically plausible.\n\nDuring the production of the spacesuits and stillsuits for the film \"Dune\", the prop and costume designers stated a need to avoid \"the standard outer-space stuff ... that sort of NASA look\".\n\nThe Gerry Anderson's \"UFO\" series of the late 1960s/early 1970s features two types of spacesuit:\n\nThe design of the alien spacesuits was revised during filming; in some episodes they are partly covered with bright metallic chainmail, and in some they are as per the image shown. The studio which made the series seems to have had only two alien spacesuit costumes. In the episode \"Ordeal\" where two aliens carry a human (Foster) who is in an alien spacesuit, one of the aliens has to be out of shot, or else 3 alien spacesuits would have been needed. The helmet splits into front and back halves to get it on over his head.\nIn this other Anderson's series (a spin-off of \"UFO\"), spacesuits are orange and yellow with a white mechanical object on the torso of any astronaut and with an openable personalised helmet; often the episodes feature a recurring blooper: the astronaut's helmet is accidentally open.\n\nSpacesuits are commonly used in the Gundam anime metaseries, but are often renamed to avoid confusion with space-use mobile suits. In the Universal Century timeline, spacesuits are called \"normal suits\"; the After Colony timeline calls them \"astrosuit\". Two types of spacesuit are frequently seen - as well as the more traditional bulky style, mobile suit pilots wear a thinner lighter suit, better designed for operating a mobile suit's controls. Gundam spacesuits often have a pouch full of adhesive strips, used to temporarily seal tears in the suit (as demonstrated in Mobile Suit Gundam) or cracks in the helmet (as demonstrated in Char's Counterattack).\n\nThe German pulp science fiction series \"Perry Rhodan\" features a type of spacesuit known as a SERUN, for Semi-Reconstituent Recycling Unit. This suit contains advanced recycling systems that can provide the necessary oxygen, water, and food to keep the inhabitant alive for weeks. It also contains a sophisticated computerized medical treatment system, antigrav units for propulsion, and a generator for a defense screen.\n\nThe 2013 film \"Gravity\", by Alfonso Cuarón with Sandra Bullock and George Clooney, was both appreciated and criticized for its use of space suits. Besides objections to the unrestricted look of extravehicular activities was the lack of protective undergarments, displayed (not) by Bullock when she removed her suit—lacking critical items such as a liquid cooling and ventilation garment or socks.\n\nIn the 2014 film \"Interstellar\", NASA uses futuristic spacesuits during the \"Lazarus\" missions and during Cooper and Brand's missions. Cooper also uses a black spacesuit at the film's end.\n\nIn the episode \"The Old College Try\", to survive in subzero temperatures near a quantum computer, Walter wears an A7L spacesuit from the Apollo program.\n\nSylvester and Happy wears vintage spacesuits, too, but they are probably fictional (like the two characters themselves).\n\nIn Andy Weir's 2011 novel \"The Martian\", astronauts from the Ares missions use white spacesuits to walk on Mars' surface. The suits in the book are described as similar to the EMU suit.\n\nThe 2015 adaptation of the novel, directed by Ridley Scott, features these suits:\nUsed by Watney during his period on Mars, this is a small orange and grey spacesuit, equipped with a GoPro-like \"suitcam\", a vocal device which signals about malfunctions and a backpack.\nWatney crashes his helmet during the explosion of the Hab, but it is replaced by another Ares III helmet (during the rest of the period of Mars) and an Ares IV helmet (during the spaceflight).\nUsed by the \"Hermes\" crew and projected also for the Ares IV one, is a specialized spacesuit used during the Mars-Earth and Earth-Mars spaceflights on NASA's \"Hermes\" spacecraft.\nCommander Melissa Lewis also uses a Manned Maneuvering Unit (MMU) during the film's end.\n\nUsed by Rick Martinez and the other Ares V astronauts during the end credits of the film, is another suit, seen only during the launch of the Ares V mission.\n\nThe potential for greater mobility and simpler operation with a skintight spacesuit, generally referred to as a space activity suit or mechanical counterpressure suit, make this type of space suit an attractive choice for fiction, where flexibility of use can be a boon to plot development.\nSome space story writers whose work mentions flexible skin-tight spacesuits include:\n\nSpider and Jeanne Robinson's novel \"Starseed\" (the second volume of their Stardance trilogy) and John Varley's Eight Worlds universe both feature alien symbionts which act as living space suits, supplying their wearer with oxygen and recycling waste gases and deriving their energy from solar power. James Blish's novella \"How Beautiful With Banners\" features a spacesuit formed from the protein coat of a genetically modified virus. The suit is able to be controlled by small electrical impulses, supplied by a control box mounted on the wearer's belt.\n\nIn some space fiction, space suits are largely absent. Spacesuits were seen only once in the original \"\" TV series (1966–1969), in the episode \"The Tholian Web\", mostly due to television budget constraints. They play a more significant part in several of the movies: \"\" (1979), \"\" (1982), \"\" (1996), and several episodes of the TV series \"\" (1995–2001). Space suits are far more frequently used in the prequel series \"\" (2001–2005), though they also doubled as environmental-hazard suits.\nSpacesuits appear in all the original Star Wars movies, but only used by pilots of fighter-type spacecraft. Spacesuits used outside spacecraft occur in some Star Wars novels and comics.\n\nSome fiction scenarios, instead of spacesuits, have a personal force field which keeps a bubble of breathable atmosphere around the user. Examples are:\n\n"}
{"id": "38728116", "url": "https://en.wikipedia.org/wiki?curid=38728116", "title": "Stauroscope", "text": "Stauroscope\n\nA stauroscope is an optical instrument used in determining the position of the planes of light-vibration in sections of crystals. The word comes from Greek for cross + scope. It was invented by Wolfgang Franz von Kobell in 1855.\n"}
{"id": "11024084", "url": "https://en.wikipedia.org/wiki?curid=11024084", "title": "Stephen Blundell", "text": "Stephen Blundell\n\nStephen John Blundell (born 1967) is a professor of physics at the University of Oxford. He was previously head of Condensed Matter Physics at Oxford, and is also a professorial fellow of Mansfield College, Oxford. His research is concerned with using muon-spin rotation and magnetoresistance techniques to study a range of organic and inorganic materials, particularly those showing interesting magnetic, superconducting, or dynamical properties.\n\nBlundell completed both his undergraduate and graduate studies at the University of Cambridge, attending Peterhouse, Cambridge for his undergraduate degree in physics and theoretical physics and doing his PhD at the Cavendish Laboratory at Cambridge.\n\nHe was subsequently offered a Science and Engineering Research Council (SERC) research fellowship which involved a move to the Clarendon Laboratory at Oxford; he was later awarded a junior research fellowship at Merton College, Oxford, where he began research in organic magnets and superconductors using muon-spin rotation. In 1997 he was appointed to a university lectureship in the Oxford Physics Department and a tutorial fellowship at Mansfield College, Oxford, and was subsequently promoted to Reader. On 28 July 2004, at the age of 35, he was appointed professor of physics.\n\nBlundell is also involved in teaching for the Honour School of Physics – including running two undergraduate lecture courses on thermal and statistical physics, and tutoring second-, third- and fourth-year undergraduates. He has also authored two textbooks, the first being \"Magnetism in Condensed Matter\" , which covers the quantum mechanical nature of magnetism. Most recently he has co-authored, with his wife and colleague, astrophycist Katherine Blundell of St John's College, Oxford, a textbook entitled \"Concepts in Thermal Physics\". It provides an introduction to the topics of thermal physics and statistical mechanics covered in a typical undergraduate course in physics. Additionally, he has authored the \"Very Short Introduction to Superconductivity\", part of the Very Short Introductions series published by Oxford University Press.\n\nHe has authored or co-authored over 300 articles ranging right across the world of solid-state physics.\n\nHe was a joint winner of the Daiwa Adrian Prize in 1999 for his work on organic magnets.\n\nBlundell lives in Oxford with his wife, Professor Katherine Blundell. In 2001, he was quoted in \"Science\" as saying, \"Ultimately your marriage is more important than your career.\"\n"}
{"id": "3868748", "url": "https://en.wikipedia.org/wiki?curid=3868748", "title": "Table of Newtonian series", "text": "Table of Newtonian series\n\nIn mathematics, a Newtonian series, named after Isaac Newton, is a sum over a sequence formula_1 written in the form\n\nwhere \n\nis the binomial coefficient and formula_4 is the rising factorial. Newtonian series often appear in relations of the form seen in umbral calculus.\n\nThe generalized binomial theorem gives\n\nA proof for this identity can be obtained by showing that it satisfies the differential equation\n\nThe digamma function: \n\nThe Stirling numbers of the second kind are given by the finite sum\n\nThis formula is a special case of the \"k\"th forward difference of the monomial \"x\" evaluated at \"x\" = 0:\n\nA related identity forms the basis of the Nörlund–Rice integral:\n\nwhere formula_11 is the Gamma function and formula_12 is the Beta function.\n\nThe trigonometric functions have umbral identities:\n\nand \n\nThe umbral nature of these identities is a bit more clear by writing them in terms of the falling factorial formula_4. The first few terms of the sin series are\n\nwhich can be recognized as resembling the Taylor series for sin \"x\", with (\"s\") standing in the place of \"x\".\n\nIn analytic number theory it is of interest to sum\nwhere \"B\" are the Bernoulli numbers. Employing the generating function its Borel sum can be evaluated as \nThe general relation gives the Newton series\nwhere formula_20 is the Hurwitz zeta function and formula_21 the Bernoulli polynomial. The series does not converge, the identity holds formally.\n\nAnother identity is \nformula_22\nwhich converges for formula_23. This follows from the general form of a Newton series for equidistant nodes (when it exists, i.e. is convergent)\n\n\n"}
{"id": "44148255", "url": "https://en.wikipedia.org/wiki?curid=44148255", "title": "Thomas H. Fraser", "text": "Thomas H. Fraser\n\nThomas Henry Fraser is a ichthyologist and expert in cardinalfishes. According to the Australian Museum website, \"He is a world expert on the taxonomy of Cardinalfishes.\"\n"}
{"id": "182679", "url": "https://en.wikipedia.org/wiki?curid=182679", "title": "Time signal", "text": "Time signal\n\nA time signal is a visible, audible, mechanical, or electronic signal used as a reference to determine the time of day. \n\nChurch bells or voices announcing hours of prayer gave way to automatically operated chimes on public clocks; however, audible signals (even signal guns) have limited range. Busy seaports used a visual signal, the dropping of a ball, to allow mariners to check the chronometers used for navigation. The advent of electrical telegraphs allowed widespread and precise distribution of time signals from central observatories. Railways were among the first customers for time signals, which allowed synchronization of their operations over wide geographic areas. Dedicated radio time signal stations transmit a signal that allows automatic synchronization of clocks, and commercial broadcasters still include time signals in their programming. \n\nToday, GPS navigation radio signals are used to precisely distribute time signals over much of the world. There are many commercially available radio controlled clocks available to accurately indicate the local time, both for business and residential use. Computers often set their time from an Internet atomic clock source. Where this is not available, a locally connected GPS receiver can precisely set the time using one of several software applications.\n\nOne sort of public time signal is a striking clock. These clocks are only as good as the clockwork that activates them, but they have improved substantially since the first clocks from the 14th century. Until modern times, a public clock such as Big Ben was the only time standard the general public needed.\n\nAccurate knowledge of time of day is essential for navigation, and ships carried the most accurate marine chronometers available, although they did not keep perfect time. A number of accurate audible or visible time signals were established in many seaport cities to enable navigators to set their chronometers.\n\nIn Vancouver, British Columbia, a \"9 O'Clock Gun\" is still shot every night at 9 pm. (This gun was brought to Stanley Park in 1894 by the Department of Fisheries originally to warn fishermen of the 6:00 pm Sunday closing of fishing.) The 9:00 pm firing was later established as a time signal for the general population. Until a time gun was installed, the nearby Brockton Point lighthouse keeper detonated a stick of dynamite. Elsewhere in Canada, a \"Noon Gun\" is fired daily from the citadels in Halifax and Quebec City.\n\nIn the same manner, a noon gun has been fired in Cape Town, South Africa, since 1806. The gun is fired daily from the Lion Battery at Signal Hill.\n\nA cannon was fired at one o'clock every weekday at Liverpool, England, at the Castle in Edinburgh, Scotland, and also at Perth in Australia to establish the time. The Edinburgh \"One O'Clock Gun\" is still in operation. A cannon located at the top of Santa Lucia Hill, in Santiago, Chile, is shot every noon.\n\nIn Rome, on the Janiculum, a hill west of the Tiber since 1904 a cannon is fired daily at noon towards the river as a time signal. This was introduced in 1847 by Pope Pius IX to synchronise all the church bells of Rome. It was situated in Castel Sant'Angelo until 1903 when it was moved to Monte Mario for a few months until it was placed in its current position. The cannon was silenced from the start of WWII for about twenty years until 21 April 1959, the 2712th anniversary of Rome's founding, and has been in use since then.\n\nFor many years an old cannon was fired \"about noon\" from a mountain near Kabul, Afghanistan. \n\nIn many Midwestern US cities where tornadoes are a common hazard, the emergency sirens are tested regularly at a specified time (say, noon each Saturday); while not primarily intended to mark the time, local people often check their watches when they hear this signal. In many non-seafaring communities, loud factory whistles served as public time signals before radio made them obsolete. Sometimes, the tradition of a factory whistle becomes so deeply entrenched in a community that the whistle is maintained long after its original function as a time keeper became obsolete. For example the University of Iowa's power plant whistle has been reinstated several times by popular demand after numerous attempts to silence it.\n\nIn 1861 and 1862, the Edinburgh Post Office Directory published time gun maps relating the number of seconds required for the report of the time gun to reach various locations in the city. Because light travels much faster than sound, visible signals enabled greater precision than audible ones, although audible signals could operate better under conditions of reduced visibility. The first time ball was erected at Portsmouth, England in 1829 by its inventor Robert Wauchope. One was installed in 1833 on the roof of the Royal Observatory in Greenwich, London, and the time ball has dropped at 1:00 pm every day since then. The first American time ball went into service in 1845. In New York City, the ceremonial Times Square Ball drop on New Year's Eve in Times Square is a vestige of a visual time signal.\n\nTelegraph signals were used regularly for time coordination by the United States Naval Observatory starting in 1865.\n\nSandford Fleming proposed a single 24-hour clock for the entire world. At a meeting of the Royal Canadian Institute on 8 February 1879 he linked it to the anti-meridian of Greenwich (now 180°). He suggested that standard time zones could be used locally, but they were subordinate to his single world time.\n\nStandard time came into existence in the United States on 18 November 1883. Earlier, on 11 October 1883, the General Time Convention, forerunner to the American Railway Association, approved a plan that divided the United States into several time zones. On that November day, the US Naval Observatory telegraphed a signal that coordinated noon at Eastern standard time with 11 am Central, 10 am Mountain, and 9 am Pacific standard time.\n\nA March 1905 issue of \"The Technical World\" describes the role of the United States Naval Observatory as a source of time signals:\n\nThe telegraphic distribution of time signals was made obsolete by the use of AM, FM, shortwave radio, Internet Network Time Protocol servers as well as atomic clocks in satellite navigation systems. Since 1905 time signals have been transmitted by radio. There are dedicated radio time signal stations around the world.\n\nTime stations operating in the longwave radio band have highly predictable radio propagation characteristics, which gives low uncertainty in the received time signals. Stations operating in the shortwave band can cover wider areas with relatively low-power transmitters, but the varying distance that the signal travels increases the uncertainty of the time signal on a scale of milliseconds.\n\nRadio time signal stations broadcast the time in both audible and machine-readable time code form that can be used as references for radio clocks and radio-controlled watches.\nThe audio portions of the shortwave WWV and WWVH broadcasts can also be heard by telephone. The time announcements are normally delayed by less than 30 ms when using land lines from within the continental United States, and the stability (delay variation) is generally < 1 ms. When mobile phones are used, the delays are often more than 100 ms due to the multiple access methods used to share cell channels. In rare instances when the telephone connection is made by satellite, the time is delayed by 250 to 500 ms.\nThese broadcasts are available by telephone by dialling US numbers (303) 499-7111 for WWV (Colorado), and (808) 335-4363 for WWVH (Hawaii). Calls, which are not toll-free, are disconnected after 2 minutes.\nLoran-C time signals may also be used for radio clock synchronization, by augmenting their highly accurate frequency transmissions with external measurements of the offsets of LORAN navigation signals against time standards.\n\nAs radio receivers became more widely available, broadcasters included time information in the form of voice announcements or automated tones to accurately indicate the hour. The BBC has included time \"pips\" in its broadcasts from 1922.\n\nIn the United States many information-based radio stations (full-service, all-news and news/talk) also broadcast time signals at the beginning of the hour. In New York, WCBS and WINS have distinctive beginning-of-the-hour tones, though the WINS signal is only approximate (several seconds error). WINS also has a tone at 30 minutes past the hour for those setting their clocks. WTIC uses the Morse code V for victory to the tune of Beethoven's 5th Symphony at the beginning of the hour continuously since 1943.\n\nStations using iBiquity Digital Corporation's \"HD Radio\" system are contractually required to delay their analog broadcast by about eight seconds so it remains in sync with the digital stream. Thus, network-generated time signals and service cues will also be delayed by about eight seconds (for this reason, when WBEN-AM in Buffalo, New York was broadcasting time markers and was simulcast on an FM station that broadcast in HD, the FM signal did not carry the time signal; WBEN does not broadcast in HD). Local signals may also be delayed.\n\nThe all-news radio stations of the CBS Radio Network, of which WCBS is the flagship, air a \"bong\" (at a frequency of 440 Hz, the standard musical note \"A\") that immediately precedes each top-of-the-hour network newscast. (The same bong could be heard on the CBS Television Network, at the top of the hour immediately before the beginning of any televised program, in the 1960s and 1970s.) An automated \"chirp\" at one second before the hour signals a switch to the radio network broadcast. As an example, KNX, the CBS Radio Network all-news station in Los Angeles, broadcasts this \"bong\" sound on the hour. However, due to buffering of the digital broadcast on some computers, this signal may be delayed as much as 20 seconds from the actual start of the hour (this is presumably the same situation for all CBS Radio stations, as each station's digital stream is produced and distributed in a similar manner), though unlike program content which is on a broadcast delay for content concerns, the time signal airs as-is over-the-air, meaning it can sometimes be talked over during a live news event or sports play-by-play. KYW-AM in Philadelphia broadcasts a time signal at the top of the hour along with its jingle.\n\nBonneville International-owned news/talk station KSL (AM-FM) in Salt Lake City uses a \"clang\" that originates from the Nauvoo Bell on Temple Square in Salt Lake City which has been a staple on the station since the early 1960s.\n\nIn Canada, the national English-language non-commercial CBC Radio One network has broadcast the daily National Research Council Time Signal since November 5, 1939; the simulcast occurs daily at 1pm Eastern Time. Its French-language counterpart, Radio-Canada, broadcasts a similar signal at noon. Vancouver radio station CKNW also broadcasts time signals, using a chime every half-hour. The CBC's predecessor, the Canadian National Railways Radio network, broadcast the time signal over its Ottawa station, CNRO (originally CKCH), at 9 pm daily and also on its Moncton station, CNRA, beginning in 1923. CNRA closed in 1931 but the broadcasts continued on CNRO when the station was acquired by the Canadian Radio Broadcasting Commission in 1933 and by the CBC in 1936 before going national in 1939.\n\nIn Australia, many information-based radio stations broadcast time signals at the beginning of the hour, and a speaking clock service is also available. However, the VNG dedicated time signal service has been discontinued.\n\nProgram material, including time signals, that is transmitted digitally (e.g. DAB, Internet radio) can be delayed by tens of seconds due to buffering and error correction, making time signals received on a digital radio unreliable when accuracy is needed.\n\n\n\n"}
{"id": "25991150", "url": "https://en.wikipedia.org/wiki?curid=25991150", "title": "Woodrow Wilson Skirvin", "text": "Woodrow Wilson Skirvin\n\nWoodrow Wilson Skirvin (September 19, 1916 – February 7, 1961 ) was an American inventor, industrialist, aviator and a prominent Indiana businessmen in the mid-20th century. His contribution to US aircraft engineering lead to the United States' setting of multiple world air speed records.\n\nW.W. Skirvin was the youngest of ten children (six boys, four girls). They all lived together on tobacco farms in Kentucky and Indiana. At a young age Skirvin learned to work the fields.\n\nAs a child in the 1920s Skirvin curiosity got the best of him including a dynamite explosion and a near deafening joy-ride. He once stripped a buggy to the wheels and raced it down a steep incline. It picked up so much speed that he lost control, and as he rolled down the hill a stick, poking up from the ground struck him in his ear.\n\nAt age 13, Skirvin and his brother hopped a freight train on a \"hobo jaunt\" heading down south. They slept in hobo camps and picked up food where they could find it.\n\nSkirvin cleaned windows, drove cabs, and even pushed a vegetable cart door to door before making his first real estate purchase in the late 1930s. It was Woody's Billiard Parlor on E. Michigan Street in Indianapolis, Indiana. The building had a pool room which he operated, and apartments upstairs.\n\nIn World War II there were many jobs with good wages in the war plants. Skirvin and wife Leona moved to Detroit, Michigan where he worked at a tool and die shop.\n\nTwo engine-lathes and two bench drill-presses in a two-car garage: such was the start of the Skirvin Tool & Engineering Company in 1944.\nThe first project was an aircraft job. Due to its success, more orders came in and the little garage was not large enough. So a series of \"forced\" moves led to the building of a completely new 25,000 sq. ft. plant in Fountain Square area Indianapolis. In 1947, aircraft parts engineered by Skirvin & Co. were used on the United States Air Force Lockheed P-80 Shooting Star & the Navy D-558 Douglas Skystreak. These airplanes set new world air speed records.\nAmong other inventions, In 1953, Skirvin produced the Cycle-Scoot, designed and manufactured in Indianapolis, IN USA. Skirvin's throttle and brake design was unique in the scooter industry. The rider pushed down with the toe to go, pushed down with the heel to stop. Indy 500, three-time winner, Wilbur Shaw officially tested and proved the scooter on the 500 track in 1954 before the race. The Indianapolis Cycle-Scoot was widely distributed in the US.\n\nSkirvin & Sons bought and operated other real estate including the Indiana Parking Garages, Indianapolis downtown parking lots & housing.\n"}
{"id": "1027992", "url": "https://en.wikipedia.org/wiki?curid=1027992", "title": "Yuri Shargin", "text": "Yuri Shargin\n\nYuri Georgiyevich Shargin () is a retired cosmonaut of the Russian Space Forces.\n\nHe was born March 20, 1960, in Engels, Saratov Oblast, Russian SFSR. He is divorced and has two children.\n\nShargin graduated from the Military Engineering Academy for Aeronautics and Astronautics located in Leningrad. He is a lieutenant colonel in the Russian Space Forces.\n\nHe was selected as a cosmonaut on February 9, 1996.\n\nHe was selected in 2004, to be the flight engineer on the Soyuz TMA-5 mission to the International Space Station. Shargin was the first Russian military cosmonaut on board and had a secret mission.\n\n"}
