{"id": "29430342", "url": "https://en.wikipedia.org/wiki?curid=29430342", "title": "100 Year Starship", "text": "100 Year Starship\n\nThe 100 Year Starship (100YSS) is a joint U.S. Defense Advanced Research Projects Agency (DARPA) and National Aeronautics and Space Administration (NASA) grant project to a private entity. The goal of the study is not to have the government fund the actual building of spacecraft, but rather to create a business plan that can last 100 years in order to help foster the research needed for interstellar travel.\n\nThe 100 Year Starship effort was announced by NASA Ames Research Center director Pete Worden in a talk at San Francisco's Long Conversation conference in October 2010. In a DARPA press release officially announcing the effort, program manager Paul Eremenko, who served as the study coordinator, explained that the endeavor was meant to excite several generations to commit to the research and development of breakthrough technologies to advance the eventual goal of interstellar space travel.\n\nThe 100 Year Starship study was the name of a one-year project to assess the attributes of and lay the groundwork for an organization that can carry forward the 100 Year Starship vision.\n\nThe winning bid to spearhead the 100 Year Starship effort was the Dorothy Jemison Foundation for Excellence, partnering with Icarus Interstellar and the Foundation for Enterprise Development, led by the American physician and former NASA astronaut Mae Jemison. In 2013, the consortium was awarded a $500,000 grant for further work. The new organization maintains the organizational name 100 Year Starship.\n\nNeither Icarus Interstellar nor the Foundation for Enterprise Development are any longer involved in 100 Year Starship although initial grant negotiations took place.\n\nBefore the solicitation for the foundation, the 100 Year Starship project was preceded by a conference held in Orlando, Florida, from September 30 to October 2, 2011, co-sponsored by DARPA and NASA, organized by DARPA's Tactical Technology Office director, David Neyland. The conference included presentations on the technology, biology, physics, philosophy, sociology, and economics of interstellar flight. Selected papers from the conference were published in the Journal of the British Interplanetary Society.\n\nAfter the Jemison Foundation was named as winner of the grant, a second symposium was held in 2012 in Houston. Papers on a number of subjects related to interstellar flight and organization of the foundation were presented. 2013 and 2014 Symposia were held in Houston, and a fifth on September 2015.\n\nIn 2015, the 100 Year Starship project hosted its first annual Canopus Awards for excellence in interstellar writing. The winners were announced October 30, 2015 at the symposium:\n\nThe 100 Year Starship was named in 2012 by U.S. Senator Tom Coburn as one of the 100 most wasteful government spending projects. Coburn specifically cited a 100 Year Starship workshop that included one session, entitled “Did Jesus die for Klingons too?” that debated the implications for Christian philosophy should life be found on other planets. The original meetings were hard science; not so for the ones that followed under Jemison's leadership.\n\n"}
{"id": "3474975", "url": "https://en.wikipedia.org/wiki?curid=3474975", "title": "AMPAC", "text": "AMPAC\n\nAMPAC is a general-purpose semiempirical quantum chemistry program. It is marketed by Semichem, Inc. and was developed originally by Michael Dewar and his group.\n\nThe first version of AMPAC (2.1) was made available in 1985 through the Quantum Chemistry Program Exchange (QCPE). Subsequent versions were released through the same source, representing minor updates and optimized versions for other platforms.\n\nIn 1992, Semichem, Inc. was formed at Professor Dewar's urging to maintain and market the program. AMPAC 4.0 with Graphical User Interface was released in August of that year. Semichem's current version of AMPAC is 9.2.\n\nAMPAC current implements the SAM1, AM1, MNDO, MNDO/d, PM3, MNDOC MINDO/3, RM1 and PM6 semi-empirical methods. See this page for a detailed description of AMPAC's current capabilities.\n\n"}
{"id": "13676683", "url": "https://en.wikipedia.org/wiki?curid=13676683", "title": "Alfred Kröner", "text": "Alfred Kröner\n\nProfessor Alfred Kröner (8 September 1939 in Kassel, Germany ) is a retired Professor of Geology at Johannes Gutenberg University of Mainz in Mainz, Germany. He specializes in the pre-Cambrian geology of Africa. \n\nThe American Journal of Science, published by Yale University, in dedicating a special issue to him on his 60th birthday, said that it was celebrated with a symposium in Mainz. It was here at the Johannes Gutenberg-University that Alfred was appointed full Professor of Geology in 1977. Since then ... he continues to conduct his activities over several continents as a truly cosmopolitan geoscientist of overwhelming efficiency, recognized and appreciated worldwide in the Geoscience community. \n\n\n"}
{"id": "39348319", "url": "https://en.wikipedia.org/wiki?curid=39348319", "title": "App Academy", "text": "App Academy\n\nApp Academy is a twelve-week intensive computer programming school founded by Ned Ruggeri and Kush Patel.\n\nIntroduced in 2012, App Academy’s \"job-guaranteed\" financing model demonstrates a tuition-free structure that promises individuals a career at the conclusion of the program. App Academy’s model \"reverses the traditional incentive structure for higher education\"; this has received media attention and was described by Wired as \"flipping the script on student loans\". 93% of App Academy’s first cohort found jobs, paying an average salary of $83,000. If a student does not find a career within the first year of graduation, the individual is not charged for tuition. Currently, 98% of App Academy graduates find jobs, with an average salary of $105,000 in San Francisco and $89,000 in New York City. Since the establishment of App Academy, over 700 graduates have been placed at tech companies such as Google, Facebook, Pinterest, Cisco, and more.\n\nApp Academy, as of 2014, reported an acceptance rate of about 5%. Individuals who applied for the program ranged from \"top Ivy League graduates to someone who used to be a night janitor.\" During the admissions process, students are required to complete introductory level coding work to show programmer potential, however, applicants do not need to have prior coding experience to apply. Once accepted, students can expect to allocate 90–100 hours per week for coding. The curriculum covers the full stack of web development, but primarily focuses on Ruby on Rails and Javascript while spending substantial time on React, Flux, JQuery and SQL. App Academy also provides job assistance for life, allowing students to learn new languages although they have graduated.\n\nApp Academy’s 12-week program is broken into 3 sections with over 500 hours of instructional time for every student who attends the course;\n\nWeeks 1-5: Begins with introductory level programming concepts, and results in a deeper understanding of Ruby and SQL.\n\nWeeks 6-9: Introduces Javascript, React, and Flux, amongst others.\n\nWeeks 10-12: Focuses on advanced algorithms and the job search. The job search curriculum focuses on resume help, whiteboarding, technical interview training, salary negotiation and culminates in a hiring day for graduates.\n\nThroughout a day at App Academy, students are given a lecture at the beginning of every session, with the majority of the time being allocated to projects and pair programming. From 9:00am–12:00pm, students start off the day with introductory lectures for the concepts to be learned that day. From 12:00pm–1:00pm, students are given a break to relax and connect with their colleagues. Afterwards, students begin pair-programming and work on collaborative projects together until the end of the working day, allowing individuals to talk through problems together to find collective solutions.\n\nAdditionally, App Academy offers tutoring for potential applicants who seek assistance during the admissions process. Tutors are instructors from the App Academy Staff and tutoring can range from teaching coding fundamentals to advanced concepts within the full-time program.\n\nSome of App Academy’s graduates have reportedly found a job within 3 weeks of interviewing. After finding a job as a software engineer, App Academy collects a fee of 18% of the graduate’s first year’s salary.\n\nIn 2015, App Academy was located in the South of Market district of San Francisco and near Chelsea in Manhattan, New York.\n\nAs of January 2016, App Academy's San Francisco Location has moved to 160 Spear Street inside the Financial District.\n\nNed Ruggeri and Kush Patel met at the University Of Chicago, where Ruggeri studied Mathematics and Patel majored in Economics. Prior to App Academy, Ruggeri worked for Google on the search engine indexing team and Patel worked at a hedge fund in Mumbai. Together, they started App Academy in an effort to allow programming education \"while making it affordable and accessible to everyone.\" In an interview with Patel, he explains the initial structure of App Academy, stating the need to teach \"as much language and framework agnostic software development as we can.\"\n\nFounded in 2012, App Academy has been featured in sources such as Slate magazine and VentureBeat, reflecting Ruggeri and Patel’s mission of \"finding the most cost-effective way possible of providing credible training.\" Kush reported to Yahoo News stating the need to \"give [students] real-world skills they can use and actually get them a job.\" \"If they can’t find a job, we’ve screwed up somehow.\" says Patel.\n\nThrough an interview with \"Financial Times\", Joshua Penman, a graduate of App Academy, explained that the boot camp model \"is attractive because it is so short and provides people with real skills.\" Initially, many individuals who enrolled in the first class at App Academy did not have enough capital to \"invest in themselves.\" However, since then, applicants are seen to come from diverse backgrounds, whether it be individuals who have followed traditional college endeavors or individuals wanting a mid-career change.\n\n"}
{"id": "419386", "url": "https://en.wikipedia.org/wiki?curid=419386", "title": "Applied science", "text": "Applied science\n\nApplied science is the application of existing scientific knowledge to practical applications, like technology or inventions.\n\nWithin natural science, disciplines that are basic science, also called pure science, develop basic \"information\" to \npredict and perhaps explain and understand phenomena in the natural world. Applied science is the use of scientific processes and knowledge as the means to achieve a particular practical or useful result. This includes a broad range of applied science related fields from engineering, business, medicine to early childhood education.\n\nApplied science can also apply formal science, such as statistics and probability theory, as in epidemiology. Genetic epidemiology is an applied science applying both biological and statistical methods.\n\nApplied research is the practical application of science. It accesses and uses accumulated theories, knowledge, methods, and techniques, for a specific, state-, business-, or client-driven purpose. Applied research is contrasted with pure research (basic research) in discussion about research ideals, methodologies, programs, and projects.\nApplied research deals with solving practical problems and generally employs empirical methodologies. Because applied research resides in the messy real world, strict research protocols may need to be relaxed. For example, it may be impossible to use a random sample. Thus, transparency in the methodology is crucial. Implications for interpretation of results brought about by relaxing an otherwise strict canon of methodology should also be considered. Since applied research has a provisional close-to-the-problem and close-to-the-data orientation, it may also use a more provisional conceptual framework such as working hypotheses or pillar questions. \nThe OECD's Frascati Manual describes applied research as one of the three forms of research, along with basic research & experimental development.\n\nDue to its practical focus, applied research information will be found in the literature associated with individual disciplines.\n\nEngineering fields include thermodynamics, heat transfer, fluid mechanics, statics, dynamics, mechanics of materials, kinematics, electromagnetism, materials science, earth sciences, engineering physics.\n\nMedical sciences, for instance medical microbiology and clinical virology, are applied sciences that apply biology toward medical knowledge and inventions, but not necessarily medical technology, whose development is more specifically biomedicine or biomedical engineering.\n\nIn Canada, the Netherlands and other places the Bachelor of Applied Science (BASc) is equivalent to the Bachelor of Engineering, and is classified as a professional degree. The BASc tends to focus more on the application of the engineering sciences. In Australia and New Zealand this degree is awarded in various fields of study and is considered a highly specialized professional degree.\n\nIn the United Kingdom's educational system, Applied Science refers to a suite of \"vocational\" science qualifications that run alongside \"traditional\" General Certificate of Secondary Education or A-Level Sciences. Applied Science courses generally contain more coursework (also known as portfolio or internally assessed work) compared to their traditional counterparts. These are an evolution of the GNVQ qualifications that were offered up to 2005.\nThese courses regularly come under scrutiny and are due for review following the Wolf Report 2011;\nhowever, their merits are argued elsewhere.\n\nIn the United States, The College of William & Mary offers an undergraduate minor as well as Master of Science and Doctor of Philosophy degrees in \"applied science.\" Courses and research cover varied fields including neuroscience, optics, materials science and engineering, nondestructive testing, and nuclear magnetic resonance. In New York City, the Bloomberg administration awarded the consortium of Cornell-Technion $100 million in City capital to construct the universities' proposed Applied Sciences campus on Roosevelt Island.\n"}
{"id": "41667186", "url": "https://en.wikipedia.org/wiki?curid=41667186", "title": "Arindam Ghosh (professor)", "text": "Arindam Ghosh (professor)\n\nArindam Ghosh is an Associate Professor in the Department of Physics, Indian Institute of Science, Bangalore, India. He was awarded the Shanti Swarup Bhatnagar Prize for science and technology, the highest science award in India, for the year 2012 in physical science category. He did his PhD at the Indian Institute of Science. His current research interests include the transport properties of two-dimensional electronic systems in semiconductors, carbon-based low-dimensional systems, optoelectronic properties of atomically-thin semiconductor membranes, magnetic nanostructures, and structural stability of nanoscale systems such as metallic nanowires and nanoparticles. \n"}
{"id": "11106681", "url": "https://en.wikipedia.org/wiki?curid=11106681", "title": "Arsenical", "text": "Arsenical\n\nArsenicals are chemical compounds that contain arsenic. In a military context, the term arsenical refer to toxic arsenic compounds that are used as chemical warfare agents. This include blister agents, blood agents and vomiting agents.\n\n\n\n"}
{"id": "3469926", "url": "https://en.wikipedia.org/wiki?curid=3469926", "title": "Association for the Scientific Study of Consciousness", "text": "Association for the Scientific Study of Consciousness\n\nThe Association for the Scientific Study of Consciousness (ASSC) is a professional membership organization that aims to encourage research on consciousness in cognitive science, neuroscience, philosophy, and other relevant disciplines in the sciences and humanities, directed toward understanding the nature, function, and underlying mechanisms of consciousness.\n\nThe organization was created in 1994 in Berkeley immediately after the first Tucson meeting by Patrick Wilken. The original aim of the organization was to act as a framework by which the international academic community could generate meetings devoted to the academic study of consciousness. The original founding members included Bernard Baars, William Banks, George Buckner, David Chalmers, Stanley Klein, Bruce Mangan, Thomas Metzinger, David Rosenthal, and Patrick Wilken. Since 1994 the organization has put on eleven meetings and assumed many other activities, including an e-print archive and the online journal \"Psyche\".\n\nIn 2008 the executive committee of the association was composed as follows: Michael Gazzaniga (Past-President), David Rosenthal (President), Giulio Tononi (President-Elect); and six Members-at-Large, Christof Koch, Paula Droege, John-Dylan Haynes, Susana Martinez-Conde, and Alva Noë. In 2007 Christof Koch took over as Director and Chair of the Board from Patrick Wilken.\n\nSince 1997, the ASSC has organised annual conferences to promote interaction and spread knowledge of scientific and philosophical advances in the field of consciousness research. The 2008 meeting was organized by Allen Houng and Ralph Adolphs, and held between the 19th and 22 June at the Gis Convention Center, National Taiwan University in Taipei. The June 2009 meeting was held in Berlin, and organized by Patrick Wilken and Michael Pauen.\n\nIn addition to organizing annual meetings, the association promotes the academic study of consciousness in a number of other ways:\n\n\n\n"}
{"id": "28913986", "url": "https://en.wikipedia.org/wiki?curid=28913986", "title": "Balchen Glacier", "text": "Balchen Glacier\n\nBalchen Glacier () is a crevassed glacier in Antarctica, flowing west to Block Bay between the Phillips Mountains and the Fosdick Mountains in Marie Byrd Land. It was discovered on December 5, 1929, by the Byrd Antarctic Expedition and named by Richard E. Byrd for Bernt Balchen, chief pilot of the expedition.\n"}
{"id": "58759966", "url": "https://en.wikipedia.org/wiki?curid=58759966", "title": "Brief Answers to the Big Questions", "text": "Brief Answers to the Big Questions\n\nBrief Answers to the Big Questions is a popular-science book written by physicist Stephen Hawking, and published by Hodder & Stoughton (Hardcover) and Bantam Books (Paperback) on 16 October 2018. The book examines some of the universe greatest mysteries, and promotes the view that science is very important in helping to solve problems on planet Earth. The publisher describes the book as \"a selection of [Hawking's] most profound, accessible, and timely reflections from his personal archive\", and included, according to a book reviewer, drawing upon \"half a million or so words\" from his essays, lectures and keynote speeches. The book was incomplete at the time of the author's passing in March 2018, but was completed with “his academic colleagues, his family and the Stephen Hawking Estate”. A foreword to the book was written by Eddie Redmayne, Oscar-winning actor who portrayed Stephen Hawking in the 2014 film, \"The Theory of Everything\"; an introduction by Kip Thorne, Nobel-prize winning physicist; and an afterword by Lucy Hawking, the author's daughter. A portion of the royalties from the book are to go to the Motor Neurone Disease Association and the Stephen Hawking Foundation.\n\nThe book is divided into four sections: \"Why Are We Here? Will We Survive? Will Technology Save Us or Destroy Us? How Can We Thrive?\". \n\nThe ten big questions that are considered include: Is there a God? How did it all begin? What is inside a black hole? Can we predict the future? Is time travel possible? Will we survive on Earth? Is there other intelligent life in the universe? Should we colonise space? Will artificial intelligence outsmart us? How do we shape the future? \n\nThe book discusses many of today's challenges, including the biggest threat to the planet (an \"asteroid collision\", like the one that wiped out the dinosaurs 66 million years ago ... \"we have no defense” against that), climate change (“a rise in ocean temperature would melt the ice caps and cause the release of large amounts of carbon dioxide ... [making] our climate like that of Venus with a temperature of \"), the threat of nuclear war (\"at some point in the next 1,000 years, nuclear war or environmental calamity will 'cripple Earth'\"), nuclear power (\"that would give us clean energy with no pollution or global warming\"), the development of artificial intelligence (AI) (“in the future AI could develop a will of its own, a will that is in conflict with ours”) and humans (\"a genetically-modified race of superhumans, say with greater memory and disease resistance, would imperil the others\"). \n\nThe book also discusses the \"big questions\", including life (\"in the next 50 years, we will come to understand how life began and possibly discover whether life exists elsewhere in the universe\"), time (“You can’t get to a time before the Big Bang [because] there was no time before the Big Bang ... If the concept of time only exists within our universe and the universe came to be spontaneously ... and with it, brought time into existence, there’s simply no 'before' to consider. Further, Hawking believed the universe could reach an end point, either through an eventual cosmic \"crunch or an expansion\" ... \"In the interim ... We are all time travelers, journeying together into the future. But let us make that future a place we want to visit”), the possibility of time travel (“asking if time travel is possible is a 'very serious question' that our current understanding cannot rule out\"), and God (“knowing the mind of God is knowing the laws of nature ... My prediction is that we will know the mind of God by the end of this century” further, \"if you like, you can call the laws of science ‘God,’ but it wouldn’t be a personal God that you would meet and put questions to ... [nevertheless] the simplest explanation is that God does not exist and there is no reliable evidence for an afterlife, though people could live on through their influence and genes\"). \n\nAccording to Hawking in the book, education and science are \"in danger now more than ever before\", and urged young people \"to look up at the stars and not down at your feet ... Try to make sense of what you see, and wonder about what makes the universe exist ... It matters that you don't give up. Unleash your imagination. Shape the future.\"\n\nPhysicist Marcelo Gleiser, reviewing the book for NPR, writes: \"Stephen Hawking is one of those rare luminaries whose life symbolizes the best humanity has to offer ... [his book is one] every thinking person worried about humanity's future should read ... If there is a unifying theme across the book, it is Hawking's deep faith in science's ability to solve humanity's biggest problems ... His answers to the big questions illustrate his belief in the rationality of nature and on our ability to uncover all its secrets. His optimism permeates every page ... Although Hawking touches on the origin of the universe, the physics of black holes and some of his other favorite topics, his main concern in this book is not physics. It's humanity and its collective future ... Focusing his attention in the book on three related questions — the future of our planet, colonization of other planets, and the rise of artificial intelligence — he charts his strategy to save us from ourselves ... Only science, Hawking argues, can save us from our mistakes ... Hawking believes that humanity's evolutionary mission is to spread through the galaxy as a sort of cosmic gardener, sowing life along the way. He believes ... that we will develop a positive relation with intelligent machines and that, together, we will redesign the current fate of the world and of our species.\"\n\nAccording to award-winning science editor Tim Radford, writing for \"The Guardian\", Hawking's book is \"effortlessly instructive, absorbing, up to the minute and – where it matters – witty.\" Radford quotes Hawking, “If the universe adds up to nothing, then you don’t need a God to create it. The universe is the ultimate free lunch\"; \"our worst mistake ever\" [if we are dismissive about artificial intelligence]; “Our future is a race between the growing power of our technology and the wisdom with which we use it. Let’s make sure that wisdom wins.\" and \"If humanity is to continue for another million years, our future lies in going boldly [with more manned space exploration] where no one else has gone before.” Radford writes, \"People who argue for good education for all, a decently funded NHS (National Health Service) and serious investment in research will rediscover him as a friend.\"\n\nReviewer Abigail Higgins, writing for \"Vox\", notes that author Hawking, in the book, is \"funny and optimistic, even as he warns us that artificial intelligence is likely to outsmart us, that the wealthy are bound to develop into a superhuman species, and that the planet is hurtling toward total inhabitability ... [the] book is ultimately a verdict on humanity’s future. At first blush, the verdict is that we’re doomed. But dig deeper and there’s something else here too, a faith that human wisdom and innovation will thwart our own destruction, even when we seem hellbent on bringing it about.\" According to a book review by science journalist Matin Durraniin, current editor of \"Physics World\": \"Hawking ticks off all the big ideas you’d expect from one of his books. General relativity. The Big Bang. Inflation. Galaxy formation. Gravitational waves ... This book will stand as Hawking’s manifesto. Optimistic, upbeat and visionary, it sees science – and scientific understanding – as vital for the future of humanity.\"\n\nAccording to John Horgan, science journalist writing for \"The Wall Street Journal\", Hawking, in his book, prefers string theory as a way of explaining the \"theory of everything\" (which Hawking predicts to be solved by \"the end of this century\") and, based on quantum mechanics, considers empty space as filled with virtual particles, \"popping into and out of existence\", suggesting our entire universe began as one of those particles, and additionally, that our cosmos is \"just a miniscule bubble in an infinite ocean, or 'multiverse'\". In this regard, Horgan refers to the concern of German physicist Sabine Hossenfelder, in her 2018 book \"Lost in Math: How Beauty Leads Physics Astray\", that \"physicists working on strings and multiverses are not really practicing physics,\" and quotes Hossenfelder, “I’m not sure anymore that what we do here, in the foundations of physics, is science”. Nevertheless, according to Ephrat Livni writing for Quartz, Hawking believed \"The law of nature itself tell us that not only could the universe have popped into existence without any assistance, like a proton, and have required nothing in terms of energy, but also it is possible that nothing caused the Big Bang. Nothing.\"\n\nJon Christian, writing for \"Futurism\" and published in \"Science Alert\", notes that Hawking, in his book, makes several predictions, including predictions about gene editing, artificial intelligence and religion, with which some experts may not fully agree. Reviewer Zayan Guedim, commenting on \"EdgyLabs\", writes, \"The book is not a culmination of all of the great scientist’s works, and it doesn’t provide any particularly new discoveries. However, it does show us the importance of our future, the 'Big Questions' and the growing necessity of looking after our planet.\"\n\n"}
{"id": "10780895", "url": "https://en.wikipedia.org/wiki?curid=10780895", "title": "Celestial cartography", "text": "Celestial cartography\n\nCelestial cartography, uranography, astrography or star cartography is the fringe of astronomy and branch of cartography concerned with mapping stars, galaxies, and other astronomical objects on the celestial sphere. Measuring the position and light of charted objects requires a variety of instruments and techniques. These techniques have developed from angle measurements with quadrants and the unaided eye, through sextants combined with lenses for light magnification, up to current methods which include computer-automated space telescopes. Uranographers have historically produced planetary position tables, star tables, and star maps for use by both amateur and professional astronomers. More recently computerized star maps have been compiled, and automated positioning of telescopes is accomplished using databases of stars and other astronomical objects.\n\nThe word \"uranography\" derived from the Greek \"ουρανογραφια\" (Koine Greek \"ουρανος\" \"sky, heaven\" + \"γραφειν\" \"to write\") through the Latin \"uranographia\". In Renaissance times, \"Uranographia\" was used as the book title of various celestial atlases. During the 19th century, \"uranography\" was defined as the \"description of the heavens\". Elijah H. Burritt re-defined it as the \"geography of the heavens\". The German word for uranography is \"\"Uranographie\", the French is \"uranographie\" and the Italian is \"uranografia\"\".\n\nA determining fact source for drawing star charts is naturally a star table. This is apparent when comparing the imaginative \"star maps\" of \"Poeticon Astronomicon\" – illustrations beside a narrative text from the antiquity – to the star maps of Johann Bayer, based on precise star-position measurements from the \"Rudolphine Tables\" by Tycho Brahe.\n\n\n\n\n\n\n\n\n\n"}
{"id": "48883228", "url": "https://en.wikipedia.org/wiki?curid=48883228", "title": "Clannism", "text": "Clannism\n\nClannism (also qabiilism) is a prejudice based on clan affiliation. The most noted discourse around these occurrences and phenomena centers around Somalia and Somalis in general. Although Somalia is by and large a racially homogeneous society, with a common language, appearance, religion, an overlapping culture and a shared religious denomination affiliation, it is nonetheless a patriarchal society. This patriarchy has resulted in a culture wherein the paternal lineage of the average person has become among the foremost anthropological feature of day-to-day life.\n"}
{"id": "6315586", "url": "https://en.wikipedia.org/wiki?curid=6315586", "title": "Communal garden", "text": "Communal garden\n\nA communal garden (often used in the plural as communal gardens) is a (normally formal) garden for shared use by a number of local residents, typically in an urban setting. The term is especially used in the United Kingdom. The centre of many city squares and crescents (especially in London, for example) are maintained as communal gardens.\n\nDespite the name, and the fact that they typically look like small public parks, such gardens are normally privately or jointly owned, with sharing of maintenance costs. Access may be restricted by locked gates, with keys available for residents, or only unlocked during daytime. They are often surrounded by tall railings designed to keep people out.\n\nOne of the scenes in the 1999 film \"Notting Hill\" involves the two main characters, Anna (Julia Roberts) and William (Hugh Grant), breaking into private and locked communal gardens by climbing over the wall at night after a dinner party. The communal gardens used were Rosmead Gardens in Rosmead Road, Notting Hill, London.\n\n"}
{"id": "4579350", "url": "https://en.wikipedia.org/wiki?curid=4579350", "title": "Continental margin", "text": "Continental margin\n\nThe continental margin is of the three major zones of the ocean floor, the other two being deep-ocean basins and mid-ocean ridges. The continental margin is the shallow water area found in proximity to continent. The continental margin consists of three different features: the continental rise, the continental slope, and the continental shelf. Continental margins constitute about 28% of the oceanic area.\n\nThe continental shelf is the portion of the continental margin that transitions from the shore out towards to ocean. They are believed to make up 7 percent of the sea floor. The width of continental shelves worldwide varies from a 30 meters to 1500 kilometers. It is generally flat, and ends at the shelf break, where there is a drastic increase in slope angle. The mean slope of continental shelves worldwide is 0° 07’ degrees, and typically steeper closer to the coastline than it is near the shelf break. At the shelf break begins the continental slope, which can be one to five kilometers above the deep-ocean floor. The continental slope often exhibits features called submarine canyons. Submarine canyons often cut into the continental shelves deeply, with near vertical slopes, and continue to cut the morphology to the abyssal plain. The valleys are often V-shaped, and can sometime enlarge onto the continental shelf. At the base of the continental slope, there is a sudden decrease in slope, and the sea floor begins to level out towards the abyssal plain. This portion of the seafloor is called the continental rise, and marks the end of the continental margin.\n\nThere are two types of continental margins: \"active\" and \"passive\" margins.\n\nActive margins are typically associated with lithospheric plate boundaries. These active margins can be convergent or transform margins, and are also places of high tectonic activity, including volcanoes and earthquakes. The West Coast of North America and South America is considered an active margin. Active continental margins are typically narrow from coast to shelf break, with steep descents into trenches. Convergent active margins occur where oceanic plates meet continental plates. The denser oceanic plate subducts below the less dense continental plate. Convergent active margins are the most common type of active margin. Transform active margins are more rare, and occur when an oceanic plate and a continental plate are moving parallel to each other in opposite directions. These transform margins are often characterized by many offshore faults, which causes high degree of relief offshore, marked by islands, shallow banks, and deep basins. This is known as the continental borderland.\n\nPassive margins are often located in the interior of lithospheric plates, away from the plate boundaries, and lack major tectonic activity. They often face mid-ocean ridges. The East Coast of the United States is an example of a passive margin. These margins are much wider and less sloped than active margins.\n\nAs continental crust weathers and erodes, it degrades into mainly sands and clays. Many of these particles end up in streams and rivers that then dump into the ocean. Of all the sediment in the stream load, 80% is then trapped and dispersed on continental margins. While modern river sediment is often still preserved closer to shore, continental shelves show high levels of glacial and relict sediments, deposited when sea level was lower. Often found on passive margins are several kilometers of sediment, consisting of terrigenous and carbonate (biogenous) deposits. These sediment reservoirs are often useful in the study of paleoceanography and the original formation of ocean basins. These deposits are often not well preserved on active margin shelves due to tectonic activity.\n\nEconomically, the continental shelf is the most economically valuable part of the ocean. It often is the most productive portion of the continental margin, as well as the most studied portion, due to its relatively shallow, accessible depths.\n\nDue to the rise of offshore drilling, mining and the limitations of fisheries off the continental shelf, the United Nations Convention on “Law of the Sea” was established. The edge of the continental margin is one criterion for the boundary of the internationally recognized claims to underwater resources by countries in the definition of the \"continental shelf\" by the United Nations Convention on the Law of the Sea (although in the UN definition the \"legal continental shelf\" may extend beyond the geomorphological continental shelf and vice versa). Such resources include fishing grounds, oil and gas accumulations, sand, gravel, and some heavy minerals in the shallower areas of the margin. Metallic minerals resources are thought to also be associated with certain active margins, and of great value.\n\n"}
{"id": "47194113", "url": "https://en.wikipedia.org/wiki?curid=47194113", "title": "Corbett's electrostatic machine", "text": "Corbett's electrostatic machine\n\nCorbett's electrostatic machine is a high voltage static electricity generating device that was used by Shaker doctors for medical treatment in the early nineteenth century. Corbett's machine is in the collection of the Mount Lebanon Shaker Village in the state of New York USA.\n\nThis electrostatic machine was made by a Shaker pharmacist named Thomas Corbett in 1810 for medical treatment. Corbett was the first physician botanist for the Shakers and was known for his herbal medicines and unorthodox medical \"cures\".\n\nCorbett's electrostatic machine consists of a wooden base platform sitting on a frame, forming a box. The wooden platform is about wide and about deep. To one side of the wooden platform is mounted a small axle on pivots, which holds a rotating glass jar cylinder about the size of a Mason jar. This glass jar is attached to a crank wheel of about with a leather belt. The crank wheel can be turned by hand. The wooden platform also contains a Leyden jar-style battery in one corner, standing some high. The Leyden jar is a glass receptacle with a metal rod in the center, covered by a metal ball on top which collects and releases a high voltage electrical charge.\n\nThe crank wheel was turned by an operator using the knob handle and then the glass cylinder would rotate by an attached belt. The glass jar rubbed against a layered silk cloth pad or a textile cloth pad, developing a high-voltage positive electrical charge. This static electricity charge was taken off the glass cylinder through a small metal rake and transferred by wire to the Leyden jar storage battery for later use (see close-up illustration). The high voltage stored electrostatic charge kept in the Leyden jar was on the metal ball on top of the high voltage storage battery. It would produce a spark visible to the eye when the set of small metal globes attached to the battery were brought close enough to each other as a test experiment in a discharge.\n\nThe high voltage static charge on top of the Leyden jar storage battery was applied to a patient as he or she sat on a chair or stool atop a special platform. The four glass legs of the chair insulated the platform from the ground. The operator channeled the stored charge from the Leyden jar to the patient using metal attachments that were connected to the patient. The electrical charge produced a shock that was similar to \"touching a doorknob after walking across carpet in dry weather\".\n\nThe electrical treatment from Corbett's electrostatic machine supposedly \"cured\" the sufferer of a variety of illnesses, or at least had some electrotherapeutic value. It was especially designed to treat rheumatism. More likely, however, the electrical shock temporarily diverted the sufferer's mind from his or her aches and pains. Shaker Elizabeth Lovegrove recorded in a journal of 1837 that an Elder Sister was being treated by the Corbett machine. She reported that she felt better after each treatment, at least temporarily.\n\nCorbett's experiments with electricity and its principles shows his interest in science and medicine. The electrostatic machine and his electrical experiments in the early 1800s took place between Benjamin Franklin's electrical experiments of the mid-1700s and that of Thomas Edison's electrical inventions. The electrical principles of Corbett's electrostatic machine were later used by Edison.\n\n\nCitations\nSources\n"}
{"id": "331454", "url": "https://en.wikipedia.org/wiki?curid=331454", "title": "Deke Slayton", "text": "Deke Slayton\n\nDonald Kent \"Deke\" Slayton (March 1, 1924 – June 13, 1993), (Major, USAF) was an American World War II pilot, aeronautical engineer, and test pilot who was selected as one of the original NASA Mercury Seven astronauts, and became NASA's first Chief of the Astronaut Office.\n\nAfter joining NASA, Slayton was selected to pilot the second U.S. manned orbital spaceflight, but was grounded in 1962 by atrial fibrillation, an irregular heart rhythm. He then served as NASA's Director of Flight Crew Operations, making him responsible for crew assignments at NASA from November 1963 until March 1972. At that time he was granted medical clearance to fly, and was assigned as the docking module pilot of the 1975 Apollo–Soyuz Test Project, at age 51 becoming the oldest person to fly in space at the time. This record was surpassed in 1983 by 54-year-old William E. Thornton, 59-year-old (and fellow ASTP crew member) Vance Brand in 1990, 61-year-old Story Musgrave in 1996, and in 1998 by Slayton's fellow Project Mercury astronaut John Glenn, who at the age of 77 flew on Space Shuttle mission STS-95.\n\nSlayton died at the age of 69 on June 13, 1993, from a malignant brain tumor.\n\nSlayton was born on March 1, 1924, on a farm near Sparta, Wisconsin, to parents Charles Sherman Slayton (1887–1972) and Victoria Adelia Slayton (; 1895–1970). He was of English and Norwegian descent. In 1929, a childhood farm equipment accident left him with a severed left ring finger. He attended elementary school in Leon, Wisconsin, and graduated from Sparta High School in 1942.\n\nHe entered the United States Army Air Forces as a cadet in 1942, training as a B-25 bomber pilot and received his wings in April 1943 (Class 43E) after completing flight training at Vernon and Waco, Texas. He flew 56 combat missions with the 340th Bombardment Group over Europe during World War II and later flew seven combat missions over Japan in a Douglas A-26 Invader as part of the 319th Bombardment Group. In the meantime, he returned to the United States in mid-1944 as a B-25 instructor pilot at Columbia, South Carolina, and later served with a unit responsible for checking pilot proficiency on the A-26 light bomber. Slayton served again as a B-25 instructor for one year following the end of the war.\n\nAfter the war, Slayton graduated with a Bachelor of Science degree in Aeronautical Engineering from the University of Minnesota, in 1949. Following graduation, he worked for two years as an engineer with the Boeing Aircraft Corporation at Seattle, Washington before being recalled to active duty in 1951 with the Minnesota Air National Guard.\n\nUpon reporting for duty, Slayton was a maintenance flight test officer of an F-51 squadron located in Minneapolis, then served eighteen months as a technical inspector at Headquarters Twelfth Air Force in Wiesbaden Army Airfield, West Germany, and a tour as fighter pilot and maintenance officer with the 36th Fighter Day Wing at Bitburg Air Base, West Germany. He graduated from the Squadron Officer School at Maxwell Air Force Base, Alabama in 1952, at that time an arm of Air Command and Staff College.\n\nReturning to the United States in June 1955, Slayton attended and graduated from U.S. Air Force Test Pilot School (Class 55C) to become a test pilot at Edwards Air Force Base in California. He tested supersonic Air Force fighters, including the F-101, F-102, F-105, and F-106, and was responsible for determining stall-spin characteristics for the large F-105, which became the principal fighter bomber used by the Air Force over North Vietnam. In 1958 he was one of two USAF test pilots given the opportunity to familiarize themselves with the English Electric P1B Lightning in England. Slayton subsequently commented:\n\n44\n\nIn his Air Force career, he logged 6,600 hours flying time including more than 5,100 hours in jet aircraft.\n\nIn 1959, Slayton was one of 110 military test pilots selected by their commanding officers as candidates for the newly formed National Aeronautics and Space Administration's Project Mercury, the first U.S. manned space flight program. Following a series of physical and psychological tests, NASA selected Slayton to be one of the original group of seven Mercury astronauts.\n\nSlayton was scheduled to fly in May 1962 on the program's second orbital flight (Mercury-Atlas 7), which he would have named \"Delta 7\". However, during the summer of 1961, Slayton was diagnosed with an erratic heart rate (idiopathic atrial fibrillation). NASA doctors did not see the condition as disqualifying, but he was grounded by other officials because they were unsure how his heart would perform under weightlessness.\n\nThe flight was given to Scott Carpenter instead and Slayton was grounded from flights completely the following September. He ended up as the only one of the Mercury Seven to not fly a Mercury mission.\n\nAfter his diagnosis, he started exercising and making lifestyle changes, including quitting smoking, coffee and cocktails in an effort discover the cause. Flight doctors recommended a cardiac catheterization to rule out a congenital condition, but NASA considered it too risky, given that he was already grounded. In a 1972 interview, Slayton said; \"It was a political, not a medical decision\".\n\nWhen NASA grounded Slayton, the Air Force followed suit. From September 1, 1962 until November 1963, he obtained the unofficial title of \"chief astronaut\" when he was assigned as Coordinator of Astronaut Activities, a position later re-designated as Chief of the Astronaut Office. Despite forfeiting a pension that he would have earned following twenty combined years of active duty, Air Guard and Reserve service in 1964, Slayton resigned his Air Force commission in 1963 and continued to work for NASA as a civilian executive, first as Assistant Director of Flight Crew Operations until 1966, and then as Director of Flight Crew Operations. As Director, he oversaw the activities of the astronaut office (managed by Chief of the Astronaut Office Alan Shepard, also grounded due to Ménière's disease), the aircraft operations office, the flight crew integration division, the crew training and simulation division, and the crew procedures division. He had the decisive role in choosing the crews for the Gemini and Apollo programs, including the decision of who would be the first person on the Moon.\n\nIn 1972, Slayton was awarded the Society of Experimental Test Pilots James H. Doolittle Award.\n\nWhile grounded, Slayton took several measures to attempt to be restored to flight status, including a daily exercise program, quitting cigarette smoking and coffee, and drastically reducing consumption of alcoholic beverages. He also took massive doses of vitamins.\n\nIn 1970 his palpitations became more frequent and he started taking experimental daily doses of quinidine, a crystalline alkaloid. This treatment was successful, but concerned that taking medication would still disqualify him from solo flying, Slayton stopped taking it against doctors orders. The fibrillation did not return.\n\nA comprehensive review of his medical status by NASA's director of life sciences and the Federal Aviation Agency, including a heart catheterization, resulted in the full restoration of his flight status in March 1972. Slayton celebrated with an hour of aerobatic maneuvers in a NASA T-38 jet trainer.\n\nAfter he was restored to flight status, Slayton was selected in February 1973 as docking module pilot for the Apollo–Soyuz Test Project (ASTP), a docking between an American Apollo spacecraft and a Soviet Soyuz spacecraft. The American crew immediately began an intensive two-year training program, which included learning the Russian language and making frequent trips to the USSR, where astronauts trained for weeks at Star City, the cosmonaut training center near Moscow. Slayton resigned as Director of Flight Crew Operations in February 1974.\n\nOn July 17, 1975, the two craft joined up in orbit, and astronauts Slayton, Thomas P. Stafford and Vance D. Brand conducted crew transfers with cosmonauts Alexey Leonov and Valeri Kubasov. At the end of the flight, an erroneous switch setting led to noxious nitrogen tetroxide fumes from the command module's RCS thrusters being sucked into the cabin during landing, and the crew was hospitalized as a precaution in Honolulu, Hawaii, for two weeks. During hospitalization, a lesion was discovered on Slayton's lung and removed. It was determined to be benign, but he would have almost certainly been grounded from ASTP if this had been discovered before the flight.\n\nDuring his first and only spaceflight, he spent 217 hours in space.\n\nAfter the Apollo–Soyuz flight, he became head of the Approach and Landing Tests (ALT) of NASA's Space Shuttle.\n\nFollowing the ALT program, Slayton served as Manager for Orbital Flight Test, directing orbital flight mission preparations and conducting mission operations. He was responsible for OFT operations scheduling, mission configuration control, preflight stack configuration control, as well as conducting planning reviews, mission readiness reviews, and postflight mission evaluations. He was also responsible for the Shuttle Carrier Aircraft program.\n\nAlthough Slayton expressed his hope of flying on a Shuttle mission, new NASA management did not favor him, making it clear that they considered him part of the past and that they planned to recruit a new young group of astronauts for the Shuttle era.\n\nSlayton retired from NASA in 1980, but retained unofficial ties as a consultant until 1982. After retirement, he served as president of Space Services Inc., a Houston-based company earlier founded to develop rockets for small commercial payloads. He served as mission director for a rocket called the Conestoga, which was successfully launched on September 9, 1982, and was the world's first privately funded rocket to reach space. Slayton also became interested in aviation racing. In addition to serving as a consultant to some aerospace corporations, he was President of International Formula One Pylon Air Racing and Director of Columbia Astronautics. He also served on the Department of Transportation's Commercial Space Advisory Committee.\n\nSlayton penned an autobiography with space historian Michael Cassutt entitled \"Deke!: U.S. Manned Space from Mercury to the Shuttle\". As well as Slayton's own astronaut experiences, the book describes how he made crew choice selections, including choosing the first person to walk on the Moon. Numerous astronauts have noted that only when reading this book did they learn why they had been selected for certain flights decades earlier.\n\nSlayton's name also appears with three other co-authors, including fellow astronaut Alan Shepard, on the book \"Moon Shot: The Inside Story of America's Race to the Moon\", published in 1994. The book was also made into a documentary film of the same name. Slayton died before either \"Moon Shot\" project was finished or released, and the book did not receive any input from him. However, the film was narrated from Slayton's point of view (voiced by Barry Corbin) and includes a brief tribute to him at the end.\n\nSlayton married Marjorie \"Marge\" Lunney (1921–1989) in May 1955, and they had one son, Kent Sherman, born April 8, 1957. They eventually divorced, and Slayton later married Bobbie Belle Jones (1945–2010) in 1983. They remained married until his death. His hobbies were hunting, fishing, shooting, and airplane racing.\n\nSlayton was a close friend of fellow astronaut Gus Grissom.\n\nShortly after he moved to League City, Texas, in 1992, Slayton was diagnosed with a malignant brain tumor. He died from the illness, at the age of 69, on June 13, 1993. He was cremated and his ashes scattered over his family farm in Wisconsin.\n\nSlayton was a member of numerous organizations. He was a fellow of the Society of Experimental Test Pilots and the American Astronautical Society; associate fellow of the American Institute of Aeronautics and Astronautics; member of the Experimental Aircraft Association, the Space Pioneers, and the Confederate Air Force; life member of the Order of Daedalians, the National Rifle Association of America, the Veterans of Foreign Wars, and the Fraternal Order of Eagles; honorary member of the American Fighter Aces Association, the National WWII Glider Pilots Association and the Association of Space Explorers.\n\nMilitary and NASA decorations and medals:\n\nSlayton's other awards include:\n\nThe Collier Trophy; the SETP Iven C. Kincheloe Award; the Gen. Billy Mitchell Award; the John J. Montgomery Award (1963); the SETP James H. Doolittle Award (1972); the National Institute of Social Sciences Gold Medal (1975); the Zeta Beta Tau’s Richard Gottheil Medal (1975); the Wright Brothers International Manned Space Flight Award (1975); the Veterans of Foreign Wars National Space Award (1976); the American Heart Association’s Heart of the Year Award (1976); the FAI Yuri Gagarin Gold Medal (1976); 3the District 35-R Lions International American of the Year Award (1976); the AIAA Special Presidential Citation (1977); the University of Minnesota’s Outstanding Achievement Award (1977); the Houston Area Federal Business Association’s Civil Servant of the Year Award (1977); the AAS Flight Achievement Award for 1976 (1977); the AIAA Haley Astronautics Award for 1978; Honorary D.Sc. from Carthage College, Carthage, Illinois, in 1961; Honorary Doctorate in Engineering from Michigan Technological University in Houghton, Michigan, in 1965.\n\nWith the other Mercury astronauts, Slayton was awarded the Collier Trophy in 1962 \"for pioneering manned space flight in the United States\".\n\nDeke Slayton was inducted into the U.S. Astronaut Hall of Fame on May 11, 1990.\n\nSlayton was inducted into the International Space Hall of Fame in 1990.\n\nSlayton was enshrined in the National Aviation Hall of Fame in 1996.\n\nIn 2001, Slayton was inducted into the International Air & Space Hall of Fame at the San Diego Air & Space Museum.\n\nThe Texas Oncology-Deke Slayton Cancer Center (located on Medical Center Blvd. in Webster, Texas) was named in his honor in 2000.\n\nThe main stretch of road in League City, Texas, FM 518, was renamed Deke Slayton Highway.\n\nThe Deke Slayton Memorial Space & Bicycle Museum in Sparta, Wisconsin, was named in his honor. The Slayton biographical exhibit includes his Mercury space suit, his Ambassador of Exploration Award, which showcases a lunar sample, and more. In nearby La Crosse, Wisconsin, an annual summer aircraft air show, the Deke Slayton Airfest, has been held in his honor, featuring modern and vintage military and civilian aircraft, along with NASA speakers.\n\nThe Cygnus CRS Orb-4 Orbital ATK space vehicle, launched to the International Space Station on December 6, 2015, was named \"S.S. Deke Slayton II\" in his honor.\n\n\n\n"}
{"id": "46852724", "url": "https://en.wikipedia.org/wiki?curid=46852724", "title": "Depuration (seafood)", "text": "Depuration (seafood)\n\nDepuration of seafood is the process by which marine or freshwater animals are placed into a clean water environment for a period of time to allow purging of biological contaminants (such as \"Escherichia coli\") and physical impurities (such as sand and silt). The most common subjects of depuration are such bivalves as oysters, clams, and mussels.\n\nMost research and publications focus primarily on the depuration of seafood rather than freshwater animals. The commonly defined depuration process has been practiced since the 1800s originating as a method to prevent typhoid fever (via \"Salmonella enterica\" subsp. \"enterica\", serovar Typhi\")\" and other illnesses attributed to polluted shellfish being consumed. As coastal seawater became increasingly contaminated with sewage-borne bacteria, early research investigated the use of disgorging tanks. Today, modern seafood depuration is performed in segregated physical tanks using treated seawater that is sterilised either by way of chlorine, ultraviolet, or ozone.\n\nSeafood depuration is legislated or regulated in many countries, including the United States, such EEU members as France, Ireland, and Italy, and Japan. Oversight is managed by internationally recognised agencies, for example, in the United States, the National Shellfish Sanitation Program (NSSP) administered by the Interstate Shellfish Sanitation Conference sets guidelines on depuration and is recognised by the U.S. Food and Drug Administration. In Canada, the Canadian Food Inspection Agency (CFIA), Fisheries and Oceans Canada (DFO) and Environment and Climate Change Canada (ECCC) actively compile the manual for the Canadian Shellfish Sanitation Program (CSSP). The Codex Alimentarius, which is overseen by the World Health Organization (WHO) and Food and Agriculture Organisation of the United Nations (FAO), both recognises and encourages the application of seafood depuration.\n\nAccording to the FAO in December 2006, France had 1422 depuration facilities, Italy had 144, Japan had over 1,000. The long history of the depuration process which expands across many countries and is recognised by international agencies is contrasted by the scarce or nonexistent data in the commercial space of the seafood industry. In a majority formal research, publications and governmental regulations, depuration is approached in the form of \"public protection\" and not in the form of \"public awareness\". One research study attempts to link the benefits of consumer awareness of shellfish depuration and found that surveyed restaurants were reluctant to sell depurated seafood. Whereas in the same study, consumers surveyed indicated they were prepared to pay a premium for depurated oysters. However, the willingness to pay a premium was expressed after the consumer was informed about depuration and depurated seafood indicating the average consumer was unaware about the depuration process.\n"}
{"id": "2482438", "url": "https://en.wikipedia.org/wiki?curid=2482438", "title": "Dollar roll", "text": "Dollar roll\n\nA dollar roll is similar to a reverse repurchase agreement and provides a form of collateralized short-term financing with mortgage-backed securities comprising the collateral. The investor sells a mortgage-backed security for settlement on one date and buys it back for settlement at a later date. The investor gives up the principal and interest payments during the roll period, but can invest the proceeds and usually is able to buy back the mortgage for a lower price than the sale price. The difference in the prices is called the drop. The value of the drop plus interest earned on the proceeds of the sale less the forgone interest and principal payments on the mortgage is considered the roll specialness or financing advantage. With repurchase agreements exactly the same security is returned to the investor, while with dollar rolls the investor buys a substantially similar—but not necessarily identical—security. This difference produces complex results under certain areas of the United States Internal Revenue Code.\n\nDollar rolls help investors achieve various objectives, such as staying invested in mortgages while earning a trading spread. Likewise, if an investor faces operational or delivery obstacles with respect to a certain mortgage-backed security, a dollar roll may help the investor retain the economic exposure while avoiding the operational difficulties.\n"}
{"id": "4577289", "url": "https://en.wikipedia.org/wiki?curid=4577289", "title": "Fabrication and testing of optical components", "text": "Fabrication and testing of optical components\n\nOptical fabrication and testing spans an enormous range of manufacturing procedures and optical test configurations. \n\nThe manufacture of a conventional spherical lens typically begins with the generation of the optic's rough shape by grinding a glass blank. This can be done, for example, with ring tools. Next, the lens surface is polished to its final form. Typically this is done by lapping—rotating and rubbing the rough lens surface against a tool with the desired surface shape, with a mixture of abrasives and fluid in between. \nTypically a carved pitch tool is used to polish the surface of a lens. The mixture of abrasive is called slurry and it is typically made from cerium or zirconium oxide in water with lubricants added to facilitate pitch tool movement without sticking to the lens. The particle size in the slurry is adjusted to get the desired shape and finish. \n\nDuring polishing, the lens may be tested to confirm that the desired shape is being produced, and to ensure that the final shape has the correct form to within the allowed precision. The deviation of an optical surface from the correct shape is typically expressed in fractions of a wavelength, for some convenient wavelength of light (perhaps the wavelength at which the lens is to be used, or a visible wavelength for which a source is available). Inexpensive lenses may have deviations of form as large as several wavelengths (λ, 2λ, etc.). More typical industrial lenses would have deviations no larger than a quarter wavelength (λ/4). Precision lenses for use in applications such as lasers, interferometers, and holography have surfaces with a tenth of a wavelength (λ/10) tolerance or better. In addition to surface profile, a lens must meet requirements for surface quality (scratches, pits, specks, etc.) and accuracy of dimensions.\n\nI. Glass blank manufacturing\n\nII. Diamond shaping techniques\n\nIII. Loose grit fabrication techniques:\n\nIV. Single-point diamond turning processes and equipment\n\nV. Glass moulding techniques\n\n\n\n\n"}
{"id": "33338392", "url": "https://en.wikipedia.org/wiki?curid=33338392", "title": "Faster-than-light neutrino anomaly", "text": "Faster-than-light neutrino anomaly\n\nIn 2011, the OPERA experiment mistakenly observed neutrinos appearing to travel faster than light. Even before the mistake was discovered, the result was considered anomalous because speeds higher than that of light in a vacuum are generally thought to violate special relativity, a cornerstone of the modern understanding of physics for over a century.\n\nOPERA scientists announced the results of the experiment in with the stated intent of promoting further inquiry and debate. Later the team reported two flaws in their equipment set-up that had caused errors far outside their original confidence interval: a fiber optic cable attached improperly, which caused the apparently faster-than-light measurements, and a clock oscillator ticking too fast. The errors were first confirmed by OPERA after a ScienceInsider report; accounting for these two sources of error eliminated the faster-than-light results.\n\nIn March 2012, the collocated ICARUS experiment reported neutrino velocities consistent with the speed of light in the same short-pulse beam OPERA had measured in November 2011. ICARUS used a partly different timing system from OPERA and measured seven different neutrinos. In addition, the Gran Sasso experiments BOREXINO, ICARUS, LVD and OPERA all measured neutrino velocity with a short-pulsed beam in May, and obtained agreement with the speed of light.\n\nOn June 8, 2012 CERN research director Sergio Bertolucci declared on behalf of the four Gran Sasso teams, including OPERA, that the speed of neutrinos is consistent with that of light. The press release, made from the 25th International Conference on Neutrino Physics and Astrophysics in Kyoto, states that the original OPERA results were wrong, due to equipment failures.\n\nOn July 12, 2012 OPERA updated their paper by including the new sources of errors in their calculations. They found agreement of neutrino speed with the speed of light.\n\nNeutrino speeds \"consistent\" with the speed of light are expected given the limited accuracy of experiments to date. Neutrinos have small but nonzero mass, and so special relativity predicts that they must propagate at speeds slower than light. Nonetheless, known neutrino production processes impart energies far higher than the neutrino mass scale, and so almost all neutrinos are ultrarelativistic, propagating at speeds very close to that of light.\n\nThe experiment created a form of neutrinos, muon neutrinos, at CERN's older SPS accelerator, on the Franco–Swiss border, and detected them at the LNGS lab in Gran Sasso, Italy. OPERA researchers used common-view GPS, derived from standard GPS, to measure the times and place coordinates at which the neutrinos were created and detected. As computed, the neutrinos' average time of flight turned out to be less than what light would need to travel the same distance in a vacuum. In a two-week span up to , the OPERA team repeated the measurement with a different way of generating neutrinos, which helped measure travel time of each detected neutrino separately. This eliminated some possible errors related to matching detected neutrinos to their creation time.\nThe OPERA collaboration stated in their initial press release that further scrutiny and independent tests were necessary to definitely confirm or refute the results.\n\nIn a analysis of their data, scientists of the OPERA collaboration reported evidence that neutrinos they produced at CERN in Geneva and recorded at the OPERA detector at Gran Sasso, Italy, had traveled faster than light. The neutrinos were calculated to have arrived approximately 60.7 nanoseconds (60.7 billionths of a second) sooner than light would have if traversing the same distance in a vacuum. After six months of cross checking, on , the researchers announced that neutrinos had been observed traveling at faster-than-light speed. Similar results were obtained using higher-energy (28 GeV) neutrinos, which were observed to check if neutrinos' velocity depended on their energy. The particles were measured arriving at the detector faster than light by approximately one part per 40,000, with a 0.2-in-a-million chance of the result being a false positive, \"assuming\" the error were entirely due to random effects (significance of six sigma). This measure included estimates for both errors in measuring and errors from the statistical procedure used. It was, however, a measure of precision, not accuracy, which could be influenced by elements such as incorrect computations or wrong readouts of instruments. For particle physics experiments involving collision data, the standard for a discovery announcement is a five-sigma error limit, looser than the observed six-sigma limit.\n\nThe preprint of the research stated \"[the observed] deviation of the neutrino velocity from \"c\" [speed of light in vacuum] would be a striking result pointing to new physics in the neutrino sector\" and referred to the \"early arrival time of CNGS muon neutrinos\" as an \"anomaly\". OPERA spokesperson Antonio Ereditato explained that the OPERA team had \"not found any instrumental effect that could explain the result of the measurement\". James Gillies, a spokesperson for CERN, said on September 22 that the scientists were \"inviting the broader physics community to look at what they [had] done and really scrutinize it in great detail, and ideally for someone elsewhere in the world to repeat the measurements\".\n\nIn November, OPERA published refined results where they noted their chances of being wrong as even less, thus tightening their error bounds. Neutrinos arrived approximately 57.8 ns earlier than if they had traveled at light-speed, giving a relative speed difference of approximately one part per 42,000 against that of light. The new significance level became 6.2 sigma. The collaboration submitted its results for peer-reviewed publication to the Journal of High Energy Physics.\n\nIn the same paper, the OPERA collaboration also published the results of a repeat experiment running from to . They detected twenty neutrinos consistently indicating an early neutrino arrival of approximately 62.1 ns, in agreement with the result of the main analysis.\n\nIn February 2012, the OPERA collaboration announced two possible sources of error that could have significantly influenced the results.\n\n\nIn March 2012 an LNGS seminar was held, confirming the fiber cable was not fully screwed in during data gathering. LVD researchers compared the timing data for cosmic high-energy muons hitting both the OPERA and the nearby LVD detector between 2007–2008, 2008–2011, and 2011–2012. The shift obtained for the 2008–2011 period agreed with the OPERA anomaly. The researchers also found photographs showing the cable had been loose by October 13, 2011.\n\nCorrecting for the two newly found sources of error, results for neutrino speed appear to be consistent with the speed of light.\n\nOn July 12, 2012 the OPERA collaboration published the end results of their measurements between 2009–2011. The difference between the measured and expected arrival time of neutrinos (compared to the speed of light) was approximately . This is consistent with no difference at all, thus the speed of neutrinos is consistent with the speed of light within the margin of error. Also the re-analysis of the 2011 bunched beam rerun gave a similar result.\n\nIn March 2012, the co-located ICARUS experiment refuted the OPERA results by measuring neutrino velocity to be that of light. ICARUS measured speed for seven neutrinos in the same short-pulse beam OPERA had checked in November 2011, and found them, on average, traveling at the speed of light. The results were from a trial run of neutrino-velocity measurements slated for May.\n\nIn May 2012, a new bunched beam rerun was initiated by CERN. Then in June 2012, it was announced by CERN that the four Gran Sasso experiments OPERA, ICARUS, LVD, and BOREXINO measured neutrino speeds consistent with the speed of light, indicating that the initial OPERA result was due to equipment errors.\n\nIn addition, Fermilab stated that the detectors for the MINOS project were being upgraded. Fermilab scientists closely analyzed and placed bounds on the errors in their timing system. In June 8, 2012 MINOS announced that according to preliminary results, the neutrino speed is consistent with the speed of light.\n\nThe OPERA experiment was designed to capture how neutrinos switch between different identities, but Autiero realized the equipment could be used to precisely measure neutrino speed too. An earlier result from the MINOS experiment at Fermilab demonstrated that the measurement was technically feasible. The principle of the OPERA neutrino velocity experiment was to compare travel time of neutrinos against travel time of light. The neutrinos in the experiment emerged at CERN and flew to the OPERA detector. The researchers divided this distance by the speed of light in vacuum to predict what the neutrino travel time should be. They compared this expected value to the measured travel time.\n\nThe OPERA team used an already existing beam of neutrinos traveling continuously from CERN to LNGS, the CERN Neutrinos to Gran Sasso beam, for the measurement. Measuring speed meant measuring the distance traveled by the neutrinos from their source to where they were detected, and the time taken by them to travel this length. The source at CERN was more than away from the detector at LNGS (Gran Sasso). The experiment was tricky because there was no way to time an individual neutrino, necessitating more complex steps. As shown in Fig. 1, CERN generates neutrinos by slamming protons, in pulses of length 10.5 microseconds (10.5 millionths of a second), into a graphite target to produce intermediate particles, which decay into neutrinos. OPERA researchers measured the protons as they passed a section called the beam current transducer (BCT) and took the transducer's position as the neutrinos' starting point. The protons did not actually create neutrinos for another kilometer, but because both protons and the intermediate particles moved almost at light speed, the error from the assumption was acceptably low.\n\nThe clocks at CERN and LNGS had to be in sync, and for this the researchers used high-quality GPS receivers, backed up with atomic clocks, at both places. This system timestamped both the proton pulse and the detected neutrinos to a claimed accuracy of 2.3 nanoseconds. But the timestamp could not be read like a clock. At CERN, the GPS signal came only to a receiver at a central control room, and had to be routed with cables and electronics to the computer in the neutrino-beam control room which recorded the proton pulse measurement (Fig. 3). The delay of this equipment was 10,085 nanoseconds and this value had to be added to the time stamp. The data from the transducer arrived at the computer with a 580 nanoseconds delay, and this value had to be subtracted from the time stamp. To get all the corrections right, physicists had to measure exact lengths of the cables and the latencies of the electronic devices. On the detector side, neutrinos were detected by the charge they induced, not by the light they generated, and this involved cables and electronics as part of the timing chain. Fig. 4 shows the corrections applied on the OPERA detector side.\n\nSince neutrinos could not be accurately tracked to the specific protons producing them, an averaging method had to be used. The researchers added up the measured proton pulses to get an average distribution in time of the individual protons in a pulse. The time at which neutrinos were detected at Gran Sasso was plotted to produce another distribution. The two distributions were expected to have similar shapes, but be separated by 2.4 milliseconds, the time it takes to travel the distance at light speed. The experimenters used an algorithm, maximum likelihood, to search for the time shift that best made the two distributions to coincide. The shift so calculated, the statistically measured neutrino arrival time, was approximately 60 nanoseconds shorter than the 2.4 milliseconds neutrinos would have taken if they traveled just at light speed. In a later experiment, the proton pulse width was shortened to 3 nanoseconds, and this helped the scientists to narrow the generation time of each detected neutrino to that range.\n\nDistance was measured by accurately fixing the source and detector points on a global coordinate system (ETRF2000). CERN surveyors used GPS to measure the source location. On the detector side, the OPERA team worked with a geodesy group from the Sapienza University of Rome to locate the detector's center with GPS and standard map-making techniques. To link the surface GPS location to the coordinates of the underground detector, traffic had to be partially stopped on the access road to the lab. Combining the two location measurements, the researchers calculated the distance, to an accuracy of 20 cm within the 730 km path.\n\nThe travel time of the neutrinos had to be measured by tracking the time they were created, and the time they were detected, and using a common clock to ensure the times were in sync. As Fig. 1 shows, the time measuring system included the neutrino source at CERN, the detector at LNGS (Gran Sasso), and a satellite element common to both. The common clock was the time signal from multiple GPS satellites visible from both CERN and LNGS. CERN's beams-department engineers worked with the OPERA team to provide a travel time measurement between the source at CERN and a point just before the OPERA detector's electronics, using accurate GPS receivers. This included timing the proton beams' interactions at CERN, and timing the creation of intermediate particles eventually decaying into neutrinos (see Fig. 3).\n\nResearchers from OPERA measured the remaining delays and calibrations not included in the CERN calculation: those shown in Fig. 4. The neutrinos were detected in an underground lab, but the common clock from the GPS satellites was visible only above ground level. The clock value noted above-ground had to be transmitted to the underground detector with an 8 km fiber cable. The delays associated with this transfer of time had to be accounted for in the calculation. How much the error could vary (the standard deviation of the errors) mattered to the analysis, and had to be calculated for each part of the timing chain separately. Special techniques were used to measure the length of the fiber and its consequent delay, required as part of the overall calculation.\n\nIn addition, to sharpen resolution from the standard GPS 100 nanoseconds to the 1 nanosecond range metrology labs achieve, OPERA researchers used Septentrio’s precise PolaRx2eTR GPS timing receiver, along with consistency checks across clocks (time calibration procedures) which allowed for common-view time transfer. The PolaRx2eTR allowed measurement of the time offset between an atomic clock and each of the Global Navigation Satellite System satellite clocks. For calibration, the equipment was taken to the Swiss Metrology Institute (METAS). In addition, highly stable cesium clocks were installed both at LNGS and CERN to cross-check GPS timing and to increase its precision. After OPERA found the superluminal result, the time calibration was rechecked both by a CERN engineer and the German Institute of Metrology (PTB). Time-of-flight was eventually measured to an accuracy of 10 nanoseconds. The final error bound was derived by combining the variance of the error for the individual parts.\n\nThe OPERA team analyzed the results in different ways and using different experimental methods. Following the initial main analysis released in September, three further analyses were made public in November. In the main November analysis, all the existing data were reanalyzed to allow adjustments for other factors, such as the Sagnac effect in which the Earth's rotation affects the distance traveled by the neutrinos. Then an alternative analysis adopted a different model for the matching of the neutrinos to their creation time. The third analysis of November focused on a different experimental setup ('the rerun') which changed the way the neutrinos were created.\n\nIn the initial setup, every detected neutrino would have been produced sometime in a 10,500 nanoseconds (10.5 microseconds) range, since this was the duration of the proton beam spill generating the neutrinos. It was not possible to isolate neutrino production time further within the spill. Therefore, in their main statistical analyses, the OPERA group generated a model of the proton waveforms at CERN, took the various waveforms together, and plotted the chance of neutrinos being emitted at various times (the global probability density function of the neutrino emission times). They then compared this plot against a plot of the arrival times of the 15,223 detected neutrinos. This comparison indicated neutrinos had arrived at the detector 57.8 nanoseconds faster than if they had been traveling at the speed of light in vacuum. An alternative analysis in which each detected neutrino was checked against the waveform of its associated proton spill (instead of against the global probability density function) led to a compatible result of approximately 54.5 nanoseconds.\n\nThe November main analysis, which showed an early arrival time of 57.8 nanoseconds, was conducted blind to avoid observer bias, whereby those running the analysis might inadvertently fine-tune the result toward expected values. To this end, old and incomplete values for distances and delays from the year 2006 were initially adopted. With the final correction needed not yet known, the intermediate expected result was also an unknown. Analysis of the measurement data under those 'blind' conditions gave an early neutrino arrival of 1043.4 nanoseconds. Afterward, the data were analyzed again taking into consideration the complete and actual sources of errors. If neutrino and light speed were the same, a subtraction value of 1043.4 nanoseconds should have been obtained for the correction. However, the actual subtraction value amounted to only 985.6 nanoseconds, corresponding to an arrival time 57.8 nanoseconds earlier than expected.\n\nTwo facets of the result came under particular scrutiny within the neutrino community: the GPS synchronization system, and the profile of the proton beam spill that generated neutrinos. The second concern was addressed in the November rerun: for this analysis, OPERA scientists repeated the measurement over the same baseline using a new CERN proton beam which circumvented the need to make any assumptions about the details of neutrino production during the beam activation, such as energy distribution or production rate. This beam provided proton pulses of 3 nanoseconds each with up to 524 nanosecond gaps. This meant a detected neutrino could be tracked uniquely to its generating 3 nanoseconds pulse, and hence its start and end travel times could be directly noted. Thus, the neutrino's speed could now be calculated without having to resort to statistical inference.\n\nIn addition to the four analyses mentioned earlier—September main analysis, November main analysis, alternative analysis, and the rerun analysis—the OPERA team also split the data by neutrino energy and reported the results for each set of the September and November main analyses. The rerun analysis had too few neutrinos to consider splitting the set further.\n\nAfter the initial report of apparent superluminal velocities of neutrinos, most physicists in the field were quietly skeptical of the results, but prepared to adopt a wait-and-see approach. Experimental experts were aware of the complexity and difficulty of the measurement, so an extra unrecognized measurement error was still a real possibility, despite the care taken by the OPERA team. However, because of the widespread interest, several well-known experts did make public comments. Nobel laureates Steven Weinberg, George Smoot III, and Carlo Rubbia, and other physicists not affiliated with the experiment, including Michio Kaku, expressed skepticism about the accuracy of the experiment on the basis that the results challenged a long-held theory consistent with the results of many other tests of special relativity. Nevertheless, Ereditato, the OPERA spokesperson, stated that no one had an explanation that invalidated the experiment's results.\n\nPrevious experiments of neutrino speed played a role in the reception of the OPERA result by the physics community. Those experiments did not detect statistically significant deviations of neutrino speeds from the speed of light. For instance, Astronomer Royal Martin Rees and theoretical physicists Lawrence Krauss and Stephen Hawking stated neutrinos from the SN 1987A supernova explosion arrived almost at the same time as light, indicating no faster-than-light neutrino speed. John Ellis, theoretical physicist at CERN, believed it difficult to reconcile the OPERA results with the SN 1987A observations. Observations of this supernova restricted 10 MeV anti-neutrino speed to less than 20 parts per billion (ppb) over lightspeed. This was one of the reasons most physicists suspected the OPERA team had made an error.\n\nPhysicists affiliated with the experiment had refrained from interpreting the result, stating in their paper: \n\nTheoretical physicists Gian Giudice, Sergey Sibiryakov, and Alessandro Strumia showed that superluminal neutrinos would imply some anomalies in the velocities of electrons and muons, as a result of quantum-mechanical effects. Such anomalies could be already ruled out from existing data on cosmic rays, thus contradicting the OPERA results.\nAndrew Cohen and Sheldon Glashow predicted that superluminal neutrinos would radiate electrons and positrons and lose energy through vacuum Cherenkov effects, where a particle traveling faster than light decays continuously into other slower particles. However, this energy attrition was absent both in the OPERA experiment and in the colocated ICARUS experiment, which uses the same CNGS beam as OPERA. This discrepancy was seen by Cohen and Glashow to present \"a significant challenge to the superluminal interpretation of the OPERA data\".\n\nMany other scientific papers on the anomaly were published as arXiv preprints or in peer reviewed journals. Some of them criticized the result, while others tried to find theoretical explanations, replacing or extending special relativity and the standard model.\n\nIn the months after the initial announcement, tensions emerged in the OPERA collaboration. A vote of no confidence among the more than thirty group team leaders failed, but spokesperson Ereditato and physics coordinator Autiero resigned their leadership positions anyway on March 30, 2012. In a resignation letter, Ereditato claimed that their results were \"excessively sensationalized and portrayed with not always justified simplification\" and defended the collaboration, stating, \"The OPERA Collaboration has always acted in full compliance with scientific rigor: both when it announced the results and when it provided an explanation for them.\"\n\n\n\n"}
{"id": "1890042", "url": "https://en.wikipedia.org/wiki?curid=1890042", "title": "Five Ws", "text": "Five Ws\n\nThe Five Ws (sometimes referred to as Five Ws and How, 5W1H, or Six Ws) are questions whose answers are considered basic in information gathering or problem solving. They are often mentioned in journalism (\"cf.\" news style), research and police investigations. They constitute a formula for getting the complete story on a subject. According to the principle of the Five Ws, a report can only be considered complete if it answers these questions starting with an interrogative word:\n\nSome authors add a sixth question, \"how\", to the list: \n\nEach question should have a factual answer — facts necessary to include for a report to be considered complete. Importantly, none of these questions can be answered with a simple \"yes\" or \"no\".\n\nIn the United Kingdom (excluding Scotland), the Five Ws are used in Key Stage 2 and Key Stage 3 lessons.\n\nThe Five Ws and How were long attributed to Hermagoras of Temnos. But in 2010, Sloan established Aristotle's Nicomachean Ethics as the source of the elements of circumstance or \"Septem Circumstantiae\". Thomas Aquinas had much earlier acknowledged Aristotle as the originator of the elements of circumstances, providing a detailed commentary on Aristotle 's system in his “Treatise on human acts” and specifically in part one of two Q7 “Of the Circumstances of Human Acts”. Thomas Aquinas examines the concept of Aristotle's voluntary and involuntary action in his \"Summa Theologia\" as well as a further set of questions about the elements of circumstance. Primarily he asks \"Whether a circumstance is an accident of a human act\" (Article 1), \"Whether Theologians should take note of the circumstances of human acts?\" (Article 2), \"Whether the circumstances are properly set forth (in Aristotle's) third book of Ethics\" (Article 3) and \"Whether the most important circumstances are 'Why' and 'In What the act consists'? (Article 4). For in acts we must take note of \"who\" did it, by what aids or instruments he did it (\"with\"), \"what\" he did, \"where\" he did it, \"why\" he did it, \"how\" and \"when\" he did it.For Aristotle, the elements are used in order to distinguish voluntary or involuntary action. Because Aristotle employs this schema as a primordial crucible for defining the difference between voluntary and involuntary agents (a topic of incalculable importance in the works of Aristotle), the benefits of locating this schema within Aristotle, and ultimately providing clarification of the passage, \"may prove helpful to a number of disciplines\"(Sloan 2010, 236).These elements of circumstances are used by Aristotle as a framework to describe and evaluate moral action in terms of What was/should be done, Who did it, How it was done, Where it happened, and most importantly for what reason (Why), and so on for all the other elements. He outlines them as follows in the Ethics as translated by Sloan.Therefore it is not a pointless endeavor to divide these circumstances by kind and number; (1) the \"Who\", (2) the \"What\", (3) around what place (\"Where\") or (4) in which time something happens (\"When\"), and sometimes (5) with what, such as an instrument (\"With\"), (6) for the sake of what (\"Why\"), such as saving a life, and (7) the (\"How\"), such as gently or violently…And it seems that the most important circumstances are those just listed, including the \"Why\".For Aristotle (in Sloan), ignorance of any of these elements can imply involuntary action.Thus, with ignorance as a possibility concerning all these things, that is, \"the circumstances of the act\", the one who acts in ignorance of any of them seems to act involuntarily, and especially regarding the most important ones. And it seems that the most important circumstances are those just listed, including the \"Why\"In the Politics, Aristotle illustrates why the elements are important in terms of human (moral) action.I mean, for instance (a particular circumstance or movement or action), How could we advise the Athenians whether they should go to war or not, if we did not know their strength (\"How much\"), whether it was naval or military or both (\"What kind\"), and how great it is (\"How many\"), what their revenues amount to (\"With\"), Who their friends and enemies are (\"Who\"), what wars, too they have waged (\"What\"), and with what success; and so on.Essentially, these elements of circumstances provide a theoretical framework that can be used to particularize, explain or predict \"any\" given set of circumstances of action. Hermagoras went so far as to claim that \"all\" hypotheses are derived from these seven circumstances.In other words, no hypothetical question, or question involving particular persons and actions, can arise without reference to these circumstances, and no demonstration of such a question can be made without using them.In any particular act or situation, one needs to interrogate these questions in order to determine the actual circumstances of the action.It is necessary for students of virtue to differentiate between the Voluntary and Involuntary; such a distinction should even prove useful to the lawmaker for assigning honors and punishments. This aspect is encapsulated by Aristotle in Rhetoric as \"forensic speech\" and is used to determine \"\"The characters and circumstances which lead men to commit wrong, or make them the victims of wrong”\" in order to accuse or defend. It is this application of the elements of circumstances that was emphasised by latter rhetoricians.\n\nEven though the classical origin of these questions as situated in ethics had long been lost, they have been a standard way of formulating or analyzing rhetorical questions since antiquity. The rhetor Hermagoras of Temnos, as quoted in pseudo-Augustine's \"De Rhetorica\", applied Aristotle's \"elements of circumstances\" (μόρια περιστάσεως) as the loci of an issue:\n\nSt. Thomas Aquinas also refers to the elements as used by Cicero in De Inventione (Chap. 24 DD1, 104) as:\"Quis, quid, ubi, quibus auxiliis, cur, quomodo, quando.\"Similarly, Quintilian discussed \"loci argumentorum\", but did not put them in the form of questions.\n\nVictorinus explained Cicero's application of the elements of circumstances by putting them into correspondence with Hermagoras's questions:\nJulius Victor also lists circumstances as questions.\n\nBoethius \"made the seven circumstances fundamental to the arts of prosecution and defense\":\n\nThe question form was taken up again in the 12th century by Thierry de Chartres and John of Salisbury.\n\nTo administer suitable penance to sinners, the 21st canon of the Fourth Lateran Council (1215) enjoined confessors to investigate both sins and the circumstances of the sins. The question form was popular for guiding confessors, and it appeared in several different forms:\n\nThe method of questions was also used for the systematic exegesis of a text.\n\nIn the 16th century, Thomas Wilson wrote in English verse:\nWho, what, and where, by what helpe, and by whose:<br>\nWhy, how, and when, doe many things disclose.\n\nIn the United States in the 19th century, Prof. William Cleaver Wilkinson popularized the \"Three Ws\" – What? Why? What of it? – as a method of Bible study in the 1880s, although he did not claim originality. This would also became the \"Five Ws\", but the application was rather different from that in journalism:\n\n\"What? Why? What of it?\" is a plan of study of alliterative methods\nfor the teacher emphasized by Professor W.C. Wilkinson not as original\nwith himself but as of venerable authority. \"It is, in fact,\" he says,\n\"an almost immemorial orator's analysis. First the facts, next the\nproof of the facts, then the consequences of the facts. This analysis\nhas often been expanded into one known as \"The Five Ws:\" \"When? Where? Who?\nWhat? Why?\" Hereby attention is called, in the study of any lesson: to the\ndate of its incidents; to their place or locality; to the person\nspeaking or spoken to, or to the persons introduced, in the narrative; to\nthe incidents or statements of the text; and, finally, to the\napplications and uses of the lesson teachings.\n\nThe \"Five Ws\" (and one H) were memorialized by Rudyard Kipling in his \"Just So Stories\" (1902), in which a poem, accompanying the tale of \"The Elephant's Child\", opens with:\n\nBy 1917, the \"Five Ws\" were being taught in high-school journalism classes, and by 1940, the tendency of journalists to address all of the \"Five Ws\" within the lead paragraph of an article was being characterized as old-fashioned and fallacious:\n\nStarting in the 2000s, the Five W's were sometimes misattributed to Kipling, especially in the management and quality literature, and contrasted with the 5 Whys.\n\nIn each of English and Latin, most of the interrogative words begin with the same sound, \"wh\" in English, \"qu\" in Latin. This is not a coincidence, as they both come from the Proto-Indo-European root \"ko-\", reflected in Proto-Germanic as \"χa-\" or \"kha-\" and in Latin as \"qu-\".\n\n"}
{"id": "2376649", "url": "https://en.wikipedia.org/wiki?curid=2376649", "title": "Flags of Estonian counties", "text": "Flags of Estonian counties\n\nThe flags of the 15 counties of Estonia are all white and green, with the coat of arms of the respective county on the white part. This design was first established in 1938. The list also includes the historical flag of Petseri County, which in 1944 was occupied by Soviet forces and became Pechorsky District in Pskov Oblast, present-day Russia. The district was claimed by Estonia after the re-establishing of independence in 1991, but the claim was dropped in 1995.\n\n"}
{"id": "18475290", "url": "https://en.wikipedia.org/wiki?curid=18475290", "title": "GRB 070125", "text": "GRB 070125\n\nGRB 070125 is a gamma-ray burst that occurred on 2007 January 25. It is unique in that it did not occur in a galaxy, but in intergalactic space. This is unusual, since GRBs are caused by hypernovae of young massive stars, which usually means having to reside in a galaxy, as almost all stars are formed in galaxies, particularly high mass ones. It has a redshift of 1.55, which equals to a light travel distance of 9.5 billion years.\n\nIt is theorized that the star formed in the tidal tail resulting from the interaction of two nearby galaxies, deep in intergalactic space. \n\nA month after it was detected, the Large Binocular Telescope observed a 26th magnitude optical afterglow from the gamma ray burst\n\n"}
{"id": "8358101", "url": "https://en.wikipedia.org/wiki?curid=8358101", "title": "George William Card", "text": "George William Card\n\nGeorge William Card, F.G.S., A.R.S.M. (? – 2 June 1943) was an Australian petrographer.\n\nCard was responsible for the collections of the Mining Museum in Sydney and was a friend of William Rowan Browne; he was awarded the Clarke Medal by the Royal Society of New South Wales in 1935.\n\nCard published \"Handbook to the Mining and Geological Museum, Sydney\" in 1902.\n\n\n"}
{"id": "22946604", "url": "https://en.wikipedia.org/wiki?curid=22946604", "title": "Grindavik (crater)", "text": "Grindavik (crater)\n\nGrindavik is an impact crater in the Oxia Palus quadrangle of Mars, located at 25.39° North and 39.07° West. It is 12 km in diameter and was named after a town in Iceland.\nImpact craters generally have a rim with ejecta around them, in contrast volcanic craters usually do not have a rim or ejecta deposits. As craters get larger (greater than 10 km in diameter) they usually have a central peak. The peak is caused by a rebound of the crater floor following the impact.\n\nPunsk Crater belongs to the class of craters called \"Rampart craters\" of the single-ejecta variety. Single-layered ejecta craters are one type of rampart carter. They have one ejecta lobe that extends 1 to 1.5 crater radii from the rim of the crater. They have an average diameter of 10 km. Although present at all latitudes, they are most common near the equator. There average size increases the more distant from the equator. It has been suggested that these types of craters are produced by impact into icy ground. Specifically, it is an impact that does not go entirely through the icy layer. The increase in size away from the equator is explained by a possible greater thickness in the icy layer away from the equator.\n\n"}
{"id": "9721414", "url": "https://en.wikipedia.org/wiki?curid=9721414", "title": "Hans K. Ziegler", "text": "Hans K. Ziegler\n\nHans K. Ziegler (March 1, 1911, Munich, Germany – December 11, 1999 Colts Neck Township, New Jersey, United States) was a pioneer in the field of communication satellites and the use of photovoltaic solar cells as a power source for satellites.\n\nHans Ziegler was born in Munich, Germany. There, he studied at the \"Technische Hochschule\", which is today the \"Technische Universität München\" (TUM), and began his career as \"Wissenschaftlicher Assistant\" (Scientific Assistant), which corresponds to a \"professor\" at US universities. Following that, he was a researcher in German industry for ten years. During the Second World War, he worked for the company Rosental Selb in Bavaria on high tension porcelain.\n\nIn 1947, he came to the USA with Wernher von Braun under Operation Paperclip, by means of which the USA gained German engineers and scientists. He went to the U. S. Army Signal Corps' Laboratories in Fort Monmouth, New Jersey, and became a U.S. citizen in 1954.\n\nDr. Hans K. Ziegler's work in the USA was very influential in the development of military electronics, especially in the electronics for the early phases of the U.S. space program. During the thirty years he worked as an engineer in the field of electronics and electrical engineering in the research and development department of the U.S. Army in Fort Monmouth, N.J. (from 1947 to 1976), he held the top position of Chief Scientist for 12 years. In Fort Monmouth, he worked as a Scientific Consultant, Assistant Director of Research, Director of the Astro-Electronics Division and Chief Scientist (1959). After the Army was restructured, he became Deputy for Science and Chief Scientist of the US Army Electronics Command in 1963 and Director of the US Army Electronics Technology & Devices Laboratory from 1971 until his retirement.\n\nIn May 1954, after examining the solar cells of Daryl Chapin, Calvin Fuller and Gerald Pearson at Bell Laboratories, Ziegler wrote, \"Future development [of the silicon solar cell] may well render it into an important source of electrical power [as] the roofs of all our buildings in cities and towns equipped with solar [cells] would be sufficient to produce this country's entire demand for electrical power.\"\n\nReferring to silicon solar cells, he said to the head of the U. S. Signal Corps, General James O'Connell, at a meeting in September 1955, \"In fact, in the long run, mankind has no choice but to turn to the sun if it wants to survive.\"\n\nHe and his team produced a report on the prospects for application of this technique in the field of communication and they named the supply of energy for artificial satellites as the most important application. He knew that he was not the first to suggest this application. For example, the science fiction author, Arthur C. Clarke had already made this suggestion in 1945, but without having a concrete technology for it at that time.\n\nZiegler participated in the development of the first planned satellites. The first satellite,\nExplorer 1, still went into space without solar cells, since it was a quick, less-than-ideal solution after the start of the Sputnik to show the American public that America's scientists could also start a satellite. The actual scheduled satellite project, Project Vanguard, successfully put a satellite named Vanguard I in orbit around Earth on March 17, 1958.\n\nOver the objections of the Navy, which still thought that solar cells were not a mature technology, this satellite had four solar cells on its outer hull, due to the persistent work of Hans Ziegler, which powered the instruments and performed their duties reliably for more than seven years. After this success, solar cells were established as the energy supply for satellites. He was also involved with the development of the first communication satellite in the world, SCORE, which was started in 1958.\n\nHe was awarded the Meritorious Civilian Service Award by the U.S. Department of Defense in 1963 as a \"\" world pioneer in communications satellites and solar energy systems to power satellites\". When he retired in 1977, he was decorated with the highest award of the Army for \"exceptional civilian service\"\".\n\nDr. Hans K. Ziegler was the author of many technical papers, a member of the IEEE, and represented the USA, in military and civilian matters, in many national and international committees. In 1958, he was a member of the US delegation to the International Geophysical Year in Moscow, USSR, and in 1964, he gave advice on the scientific activities in Antarctica and at the South Pole, under the direction of the U.S. National Science Foundation.\n\nHans Ziegler's wife Friederike died in 1996. He last lived in Colts Neck Township, New Jersey and died at the age of 88 on December 11, 1999. He was survived by his daughters, Christine Griffith and Friederike Meindl, and his son, Hans.\n\n\"Relevant links on Fort Monmouth history site:\"\n\n\"Other sites:\"\n\n"}
{"id": "38216204", "url": "https://en.wikipedia.org/wiki?curid=38216204", "title": "Hermanus Haga", "text": "Hermanus Haga\n\nHerman Haga (Oldeboorn, 24 January 1852 – Zeist, 11 September 1936) was a Dutch physicist.\n\nHaga studied physics from 1871 to 1876 at the University of Leiden. He received his PhD with thesis \"Over de absorptie van stralende warmte door waterdamp\" (On the absorption of radiant heat by water vapor) under the direction of Pieter L. Rijke. From 1886 to 1921 Haga was professor of physics at the University of Groningen, where he designed and oversaw the building of a new physics laboratory, opened in 1892. From 1900 Haga was also the rector of the University of Gronigen. He performed experiments on the voltage of the Weston cell; these experiments lead to the modern definition of the volt. With Cornelis Wind, Haga passed X-rays through a 15-micrometer slit as a source with a V-shaped narrowing slit as a target. The target slit was 27 micrometers at the opening and nearly 0 micrometers at the output. Haga and Wind interpreted a diffuse broadening of the X-rays emitted at the narrower end of the slit as a diffraction pattern.\n\nHaga was one of the founders of the \"Nederlandse Natuurkundige Vereniging\" (Dutch Physics Association). In addition to Cornelis Harm Wind (1867–1911), Haga's doctoral students include the crystallographer Pieter Terpstra (1886–1973) and Ekko Oosterhuis (1886–1966), who was the second scientist to join the \"Philips Natuurkundig Laboratorium\".\n\nIn 1896 Haga became a member of the Royal Netherlands Academy of Arts and Sciences.\n\n"}
{"id": "17423952", "url": "https://en.wikipedia.org/wiki?curid=17423952", "title": "History of the steam engine", "text": "History of the steam engine\n\nThe first recorded rudimentary steam engine was the aeolipile described by Heron of Alexandria in 1st-century Roman Egypt. Several steam-powered devices were later experimented with or proposed, such as Taqi al-Din's steam jack, a steam turbine in 16th-century Ottoman Egypt, and Thomas Savery's steam pump in 17th-century England. In 1712, Thomas Newcomen's atmospheric engine became the first commercially successful engine using the principle of the piston and cylinder, which was the fundamental type steam engine used until the early 20th century. The steam engine was used to pump water out of coal mines\n\nDuring the Industrial Revolution, steam engines started to replace water and wind power, and eventually became the dominant source of power in the late 19th century and remaining so into the early decades of the 20th century, when the more efficient steam turbine and the internal combustion engine resulted in the rapid replacement of the steam engines. The steam turbine has become the most common method by which electrical power generators are driven. Investigations are being made into the practicalities of reviving the reciprocating steam engine as the basis for the new wave of advanced steam technology.\n\nThe earliest known rudimentary steam engine and reaction steam turbine, the aeolipile, is described by a mathematician and engineer named Heron of Alexandria (Heron) in 1st century Roman Egypt, as recorded in his manuscript \"Spiritalia seu Pneumatica\". Steam ejected tangentially from nozzles caused a pivoted ball to rotate. Its thermal efficiency was low. This suggests that the conversion of steam pressure into mechanical movement was known in Roman Egypt in the 1st century. Heron also devised a machine that used air heated in an altar fire to displace a quantity of water from a closed vessel. The weight of the water was made to pull a hidden rope to operate temple doors. Some historians have conflated the two inventions to assert, incorrectly, that the aeolipile was capable of useful work.\n\nAccording to William of Malmesbury, in 1125, Reims was home to a church that had an organ powered by air escaping from compression \"by heated water\", apparently designed and constructed by professor Gerbertus.\n\nAmong the papers of Leonardo da Vinci dating to the late 15th century is the design for a steam-powered cannon called the Architonnerre which works by the sudden influx of hot water into a sealed red hot cannon.\n\nA rudimentary impact steam turbine was described in 1551 by Taqi al-Din, a philosopher, astronomer and engineer in 16th century Ottoman Egypt, who described a method for rotating a spit by means of a jet of steam playing on rotary vanes around the periphery of a wheel. A similar device for rotating a spit was also later described by John Wilkins in 1648. These devices were then called \"mills\" but are now known as steam jacks. Another similar rudimentary steam turbine is shown by Giovanni Branca, an Italian engineer, in 1629 for turning a cylindrical escapement device that alternately lifted and let fall a pair of pestles working in mortars. The steam flow of these early steam turbines, however, was not concentrated and most of its energy was dissipated in all directions. This would have led to a great waste of energy and so they were never seriously considered for industrial use.\n\nIn 1605 French mathematician Florence Rivault in his treatise on artillery wrote on his discovery that water, if confined in a bombshell and heated, would explode the shells.\n\nIn 1606, the Spaniard, Jerónimo de Ayanz y Beaumont demonstrated and was granted a patent for a steam powered water pump. The pump was successfully used to drain the inundated mines of Guadalcanal, Spain.\n\n“The discoveries that, when brought together by Thomas Newcomen in 1712, resulted in the steam engine were:\"\n\nIn 1643 Evangelista Torricelli conducted experiments on suction lift water pumps to test their limits, which was about 32 feet (Atmospheric pressure is 32.9 feet or 10.03 meters. Vapor pressure of water lowers theoretical lift height.). He devised an experiment using a tube filled with mercury and inverted in a bowl of mercury (a barometer) and observed an empty space above the column of mercury, which he theorized contained nothing, that is, a vacuum.\n\nInfluenced by Torricelli, Otto von Guericke invented a vacuum pump by modifying an air pump used for pressurizing an air gun. Guericke put on a demonstration in 1654 in Magdeburg, Germany, where he was mayor. Two copper hemispheres were fitted together and air was pumped out. Weights strapped to the hemispheres could not pull them apart until the air valve was opened. The experiment was repeated in 1656 using two teams of 8 horses each, which could not separate the Magdeburg hemispheres.\n\nGaspar Schott was the first to describe the hemisphere experiment in his \"Mechanica Hydraulico-Pneumatica\" (1657).\n\nAfter reading Schott’s book, Robert Boyle built an improved vacuum pump and conducted related experiments.\n\nDenis Papin became interested in using a vacuum to generate motive power while working with Christiaan Huygens and Gottfried Leibniz in Paris in 1663. Papin worked for Robert Boyle from 1676 to 1679, publishing an account of his work in \"Continuation of New Experiments\" (1680) and gave a presentation to Royal Society in 1689. From 1690 on Papin began experimenting with a piston to produce power with steam, building model steam engines. He experimented with atmospheric and pressure steam engines, publishing his results in 1707.\n\nIn 1663 Edward Somerset, 2nd Marquess of Worcester published a book of 100 inventions which described a method for raising water between floors employing a similar principle to that of a coffee percolator. His system was the first to separate the boiler (a heated cannon barrel) from the pumping action. Water was admitted into a reinforced barrel from a cistern, and then a valve was opened to admit steam from a separate boiler. The pressure built over the top of the water, driving it up a pipe. He installed his steam-powered device on the wall of the Great Tower at Raglan Castle to supply water through the tower. The grooves in the wall where the engine was installed were still to be seen in the 19th century. However, no one was prepared to risk money for such a revolutionary concept, and without backers the machine remained undeveloped.\n\nSamuel Morland, a mathematician and inventor who worked on pumps, left notes at the Vauxhall Ordinance Office on a steam pump design that Thomas Savery read. In 1698 Savery built a steam pump called “The Miner’s Friend.” It employed both vacuum and pressure. These were used for low horsepower service for a number of years.\n\nThomas Newcomen was a merchant who dealt in cast iron goods. Newcomen’s engine was based on the piston and cylinder design proposed by Papin. In Newcomen's engine steam was condensed by water sprayed inside the cylinder, causing atmospheric pressure to move the piston. Newcomen’s first engine installed for pumping in a mine in 1712 at Dudley Castle in Staffordshire.\n\nDenis Papin (22 August 1647 – c. 1712) was a French physicist, mathematician and inventor, best known for his pioneering invention of the steam digester, the forerunner of the pressure cooker. In the mid-1670s Papin collaborated with the Dutch physicist Christiaan Huygens on an engine which drove out the air from a cylinder by exploding gunpowder inside it. Realising the incompleteness of the vacuum produced by this means and on moving to England in 1680, Papin devised a version of the same cylinder that obtained a more complete vacuum from boiling water and then allowing the steam to condense; in this way he was able to raise weights by attaching the end of the piston to a rope passing over a pulley. As a demonstration model the system worked, but in order to repeat the process the whole apparatus had to be dismantled and reassembled. Papin quickly saw that to make an automatic cycle the steam would have to be generated separately in a boiler; however, he did not take the project further. Papin also designed a paddle boat driven by a jet playing on a mill-wheel in a combination of Taqi al Din and Savery's conceptions and he is also credited with a number of significant devices such as the safety valve. Papin's years of research into the problems of harnessing steam was to play a key part in the development of the first successful industrial engines that soon followed his death.\n\nThe first steam engine to be applied industrially was the \"fire-engine\" or \"Miner's Friend\", designed by Thomas Savery in 1698. This was a pistonless steam pump, similar to the one developed by Worcester. Savery made two key contributions that greatly improved the practicality of the design. First, in order to allow the water supply to be placed below the engine, he used condensed steam to produce a partial vacuum in the pumping reservoir (the barrel in Worcester's example), and using that to pull the water upward. Secondly, in order to rapidly cool the steam to produce the vacuum, he ran cold water over the reservoir.\n\nOperation required several valves; when the reservoir was empty at the start of a cycle a valve was opened to admit steam. The valve was closed to seal the reservoir and the cooling water valve turned on to condense the steam and create a partial vacuum. A supply valve was opened, pulling water upward into the reservoir, and the typical engine could pull water up to 20 feet. This was closed and the steam valve reopened, building pressure over the water and pumping it upward, as in the Worcester design. The cycle essentially doubled the distance that water could be pumped for any given pressure of steam, and production examples raised water about 40 feet.\n\nSavery's engine solved a problem that had only recently become a serious one; raising water out of the mines in southern England as they reached greater depths. Savery's engine was somewhat less efficient than Newcomen's, but this was compensated for by the fact that the separate pump used by the Newcomen engine was inefficient, giving the two engines roughly the same efficiency of 6 million foot pounds per bushel of coal (less than 1%). Nor was the Savery engine very safe because part of its cycle required steam under pressure supplied by a boiler, and given the technology of the period the pressure vessel could not be made strong enough and so was prone to explosion. The explosion of one of his pumps at Broad Waters (near Wednesbury), about 1705, probably marks the end of attempts to exploit his invention.\n\nThe Savery engine was less expensive than Newcomen's and was produced in smaller sizes. Some builders were manufacturing improved versions of the Savery engine until late in the 18th century. Bento de Moura Portugal, , introduced an ingenious improvement of Savery's construction \"to render it capable of working itself\", as described by John Smeaton in the Philosophical Transactions published in 1751.\n\nIt was Thomas Newcomen with his \"atmospheric-engine\" of 1712 who can be said to have brought together most of the essential elements established by Papin in order to develop the first practical steam engine for which there could be a commercial demand. This took the shape of a reciprocating beam engine installed at surface level driving a succession of pumps at one end of the beam. The engine, attached by chains from other end of the beam, worked on the atmospheric, or vacuum principle.\n\nNewcomen's design used some elements of earlier concepts. Like the Savery design, Newcomen's engine used steam, cooled with water, to create a vacuum. Unlike Savery's pump, however, Newcomen used the vacuum to pull on a piston instead of pulling on water directly. The upper end of the cylinder was open to the atmospheric pressure, and when the vacuum formed, the atmospheric pressure above the piston pushed it down into the cylinder. The piston was lubricated and sealed by a trickle of water from the same cistern that supplied the cooling water. Further, to improve the cooling effect, he sprayed water directly into the cylinder. \nThe piston was attached by a chain to a large pivoted beam. When the piston pulled the beam, the other side of the beam was pulled upward. This end was attached to a rod that pulled on a series of conventional pump handles in the mine. At the end of this power stroke, the steam valve was reopened, and the weight of the pump rods pulled the beam down, lifting the piston and drawing steam into the cylinder again.\n\nUsing the piston and beam allowed the Newcomen engine to power pumps at different levels throughout the mine, as well as eliminating the need for any high-pressure steam. The entire system was isolated to a single building on the surface. Although inefficient and extremely heavy on coal (compared to later engines), these engines raised far greater volumes of water and from greater depths than had previously been possible. Over 100 Newcomen engines were installed around England by 1735, and it is estimated that as many as 2,000 were in operation by 1800 (including Watt versions).\n\nJohn Smeaton made numerous improvements to the Newcomen engine, notably the seals, and by improving these was able to almost triple their efficiency. He also preferred to use wheels instead of beams for transferring power from the cylinder, which made his engines more compact. Smeaton was the first to develop a rigorous theory of steam engine design of operation. He worked backward from the intended role to calculate the amount of power that would be needed for the task, the size and speed of a cylinder that would provide it, the size of boiler needed to feed it, and the amount of fuel it would consume. These were developed empirically after studying dozens of Newcomen engines in Cornwall and Newcastle, and building an experimental engine of his own at his home in Austhorpe in 1770. By the time the Watt engine was introduced only a few years later, Smeaton had built dozens of ever-larger engines into the 100 hp range.\n\nWhile working at the University of Glasgow as an instrument maker and repairman in 1759, James Watt was introduced to the power of steam by Professor John Robison. Fascinated, Watt took to reading everything he could on the subject, and independently developed the concept of latent heat, only recently published by Joseph Black at the same university. When Watt learned that the University owned a small working model of a Newcomen engine, he pressed to have it returned from London where it was being unsuccessfully repaired. Watt repaired the machine, but found it was barely functional even when fully repaired.\n\nAfter working with the design, Watt concluded that 80% of the steam used by the engine was wasted. Instead of providing motive force, it was instead being used to heat the cylinder. In the Newcomen design, every power stroke was started with a spray of cold water, which not only condensed the steam, but also cooled the walls of the cylinder. This heat had to be replaced before the cylinder would accept steam again. In the Newcomen engine the heat was supplied only by the steam, so when the steam valve was opened again the vast majority condensed on the cold walls as soon as it was admitted to the cylinder. It took a considerable amount of time and steam before the cylinder warmed back up and the steam started to fill it up.\n\nWatt solved the problem of the water spray by removing the cold water to a different cylinder, placed beside the power cylinder. Once the induction stroke was complete a valve was opened between the two, and any steam that entered the cylinder would condense inside this cold cylinder. This would create a vacuum that would pull more of the steam into the cylinder, and so on until the steam was mostly condensed. The valve was then closed, and operation of the main cylinder continued as it would on a conventional Newcomen engine. As the power cylinder remained at operational temperature throughout, the system was ready for another stroke as soon as the piston was pulled back to the top. Maintaining the temperature was a jacket around the cylinder where steam was admitted. Watt produced a working model in 1765.\n\nConvinced that this was a great advance, Watt entered into partnerships to provide venture capital while he worked on the design. Not content with this single improvement, Watt worked tirelessly on a series of other improvements to practically every part of the engine. Watt further improved the system by adding a small vacuum pump to pull the steam out of the cylinder into the condenser, further improving cycle times. A more radical change from the Newcomen design was closing off the top of the cylinder and introducing low-pressure steam above the piston. Now the power was not due to the difference of atmospheric pressure and the vacuum, but the pressure of the steam and the vacuum, a somewhat higher value. On the upward return stroke, the steam on top was transferred through a pipe to the underside of the piston ready to be condensed for the downward stroke. Sealing of the piston on a Newcomen engine had been achieved by maintaining a small quantity of water on its upper side. This was no longer possible in Watt's engine due to the presence of the steam. Watt spent considerable effort to find a seal that worked, eventually obtained by using a mixture of tallow and oil. The piston rod also passed through a gland on the top cylinder cover sealed in a similar way.\n\nThe piston sealing problem was due to having no way to produce a sufficiently round cylinder. Watt tried having cylinders bored from cast iron, but they were too out of round. Watt was forced to use a hammered iron cylinder. The following quotation is from Roe (1916):\n\n\"When [John] Smeaton first saw the engine he reported to the Society of Engineers that 'neither the tools nor the workmen existed who could manufacture such a complex machine with sufficient precision' \"\n\nWatt finally considered the design good enough to release in 1774, and the Watt engine was released to the market. As portions of the design could be easily fitted to existing Newcomen engines, there was no need to build an entirely new engine at the mines. Instead, Watt and his business partner Matthew Boulton licensed the improvements to engine operators, charging them a portion of the money they would save in reduced fuel costs. The design was wildly successful, and the Boulton and Watt company was formed to license the design and help new manufacturers build the engines. The two would later open the Soho Foundry to produce engines of their own.\n\nIn 1774 John Wilkinson invented a boring machine with the shaft holding the boring tool supported on both ends, extending through the cylinder, unlike the then used cantilevered borers. With this machine he was able to successfully bore the cylinder for Boulton and Watt's first commercial engine in 1776.\n\nWatt never ceased improving his designs. This further improved the operating cycle speed, introduced governors, automatic valves, double-acting pistons, a variety of rotary power takeoffs and many other improvements. Watt's technology enabled the widespread commercial use of stationary steam engines.\n\nHumphrey Gainsborough produced a model condensing steam engine in the 1760s, which he showed to Richard Lovell Edgeworth, a member of the Lunar Society. Gainsborough believed that Watt had used his ideas for the invention; however James Watt was not a member of the Lunar Society at this period and his many accounts explaining the succession of thought processes leading to the final design would tend to belie this story.\n\nPower was still limited by the low pressure, the displacement of the cylinder, combustion and evaporation rates and condenser capacity. Maximum theoretical efficiency was limited by the relatively low temperature differential on either side of the piston; this meant that for a Watt engine to provide a usable amount of power, the first production engines had to be very large, and were thus expensive to build and install.\n\nWatt developed a double-acting engine in which steam drove the piston in both directions, thereby increasing the engine speed and efficiency. The double-acting principle also significantly increased the output of a given physical sized engine.\n\nBoulton & Watt developed the reciprocating engine into the rotative type. Unlike the Newcomen engine, the Watt engine could operate smoothly enough to be connected to a drive shaft – via sun and planet gears – to provide rotary power along with double-acting condensing cylinders. The earliest example was built as a demonstrator and was installed in Boulton's factory to work machines for lapping (polishing) buttons or similar. For this reason it was always known as the \"Lap Engine\". In early steam engines the piston is usually connected by a rod to a balanced beam, rather than directly to a flywheel, and these engines are therefore known as beam engines.\n\nEarly steam engines did not provide constant enough speed for critical operations such as cotton spinning. To control speed the engine was used to pump water for a water wheel, which powered the machinery.\n\nAs the 18th century advanced, the call was for higher pressures; this was strongly resisted by Watt who used the monopoly his patent gave him to prevent others from building high-pressure engines and using them in vehicles. He mistrusted the boiler technology of the day, the way they were constructed and the strength of the materials used.\n\nThe important advantages of high-pressure engines were: \n\nThe disadvantages were: \n\nThe main difference between how high-pressure and low-pressure steam engines work is the source of the force that moves the piston. In the engines of Newcomen and Watt, it is the condensation of the steam that creates most of the pressure difference, causing atmospheric pressure (Newcomen) and low-pressure steam, seldom more than 7 psi boiler pressure, plus condenser vacuum (Watt), to move the piston. In a high-pressure engine, most of the pressure difference is provided by the high-pressure steam from the boiler; the low-pressure side of the piston may be at atmospheric pressure or connected to the condenser pressure. Newcomen's indicator diagram, almost all below the atmospheric line, would see a revival nearly 200 years later with the low pressure cylinder of triple expansion engines contributing about 20% of the engine power, again almost completely below the atmospheric line.\n\nThe first known advocate of \"strong steam\" was Jacob Leupold in his scheme for an engine that appeared in encyclopaedic works from around 1725. Various projects for steam propelled boats and vehicles also appeared throughout the century one of the most promising being Nicolas-Joseph Cugnot's who demonstrated his \"fardier\" (steam wagon), in 1769. Whilst the working pressure used for this vehicle is unknown, the small size of the boiler gave insufficient steam production rate to allow the fardier to advance more than a few hundred metres at a time before having to stop to raise steam. Other projects and models were proposed, but as with William Murdoch's model of 1784, many were blocked by Boulton and Watt.\n\nThis did not apply in the US, and in 1788 a steamboat built by John Fitch operated in regular commercial service along the Delaware River between Philadelphia, Pennsylvania, and Burlington, New Jersey, carrying as many as 30 passengers. This boat could typically make 7 to 8 miles per hour, and traveled more than during its short length of service. The Fitch steamboat was not a commercial success, as this route was adequately covered by relatively good wagon roads. In 1802 William Symington built a practical steamboat, and in 1807 Robert Fulton used a Watt steam engine to power the first commercially successful steamboat.\n\nOliver Evans in his turn was in favour of \"strong steam\" which he applied to boat engines and to stationary uses. He was a pioneer of cylindrical boilers; however Evans' boilers did suffer several serious boiler explosions, which tended to lend weight to Watt's qualms. He founded the Pittsburgh Steam Engine Company in 1811 in Pittsburgh, Pennsylvania.\nThe company introduced high-pressure steam engines to the riverboat trade in the Mississippi watershed.\n\nThe first high-pressure steam engine was invented in 1800 by Richard Trevithick.\n\nThe importance of raising steam under pressure (from a thermodynamic standpoint) is that it attains a higher temperature. Thus, any engine using high-pressure steam operates at a higher temperature and pressure differential than is possible with a low-pressure vacuum engine. The high-pressure engine thus became the basis for most further development of reciprocating steam technology. Even so, around the year 1800, \"high pressure\" amounted to what today would be considered very low pressure, i.e. 40-50 psi (276-345 kPa), the point being that the high-pressure engine in question was non-condensing, driven solely by the expansive power of the steam, and once that steam had performed work it was usually exhausted at higher-than-atmospheric pressure. The blast of the exhausting steam into the chimney could be exploited to create induced draught through the fire grate and thus increase the rate of burning, hence creating more heat in a smaller furnace, at the expense of creating back pressure on the exhaust side of the piston.\n\nOn 21 February 1804, at the Penydarren ironworks at Merthyr Tydfil in South Wales, the first self-propelled railway steam engine or steam locomotive, built by Richard Trevithick, was demonstrated.\n\nAround 1811 Richard Trevithick was required to update a Watt pumping engine in order to adapt it to one of his new large cylindrical Cornish boilers. When Trevithick left for South America in 1816, his improvements were continued by William Sims. In a parallel, Arthur Woolf developed a compound engine with two cylinders, so that steam expanded in a high-pressure cylinder before being released into a low-pressure one. Efficiency was further improved by Samuel Groase, who insulated the boiler, engine, and pipes.\n\nSteam pressure above the piston was increased eventually reaching or even and now provided much of the power for the downward stroke; at the same time condensing was improved. This considerably raised efficiency and further pumping engines on the Cornish system (often known as Cornish engines) continued to be built new throughout the 19th century. Older Watt engines were updated to conform.\n\nThe take-up of these Cornish improvements was slow in textile manufacturing areas where coal was cheap, due to the higher capital cost of the engines and the greater wear that they suffered. The change only began in the 1830s, usually by compounding through adding another (high-pressure) cylinder.\n\nAnother limitation of early steam engines was speed variability, which made them unsuitable for many textile applications, especially spinning. In order to obtain steady speeds, early steam powered textile mills used the steam engine to pump water to a water wheel, which drove the machinery.\n\nMany of these engines were supplied worldwide and gave reliable and efficient service over a great many years with greatly reduced coal consumption. Some of them were very large and the type continued to be built right down to the 1890s.\n\nThe Corliss steam engine (patented 1849) was called the greatest improvement since James Watt. The Corliss engine had greatly improved speed control and better efficiency, making it suitable to all sorts of industrial applications, including spinning.\n\nCorliss used separate ports for steam supply and exhaust, which prevented the exhaust from cooling the passage used by the hot steam. Corliss also used partially rotating valves that provided quick action, helping to reduce pressure losses. The valves themselves were also a source of reduced friction, especially compared to the slide valve, which typically used 10% of an engine's power.\n\nCorliss used automatic variable cut off. The valve gear controlled engine speed by using the governor to vary the timing of the cut off. This was partly responsible for the efficiency improvement in addition to the better speed control.\n\nThe Porter-Allen engine, introduced in 1862, used an advanced valve gear mechanism developed for Porter by Allen, a mechanic of exceptional ability, and was at first generally known as the Allen engine. The high speed engine was a precision machine that was well balanced, achievements made possible by advancements in machine tools and manufacturing technology.\n\nThe high speed engine ran at piston speeds from three to five times the speed of ordinary engines. It also had low speed variability. The high speed engine was widely used in sawmills to power circular saws. Later it was used for electrical generation.\n\nThe engine had several advantages. It could, in some cases, be directly coupled. If gears or belts and drums were used, they could be much smaller sizes. The engine itself was also small for the amount of power it developed.\n\nPorter greatly improved the fly-ball governor by reducing the rotating weight and adding a weight around the shaft. This significantly improved speed control. Porter's governor became the leading type by 1880.\n\nThe efficiency of the Porter-Allen engine was good, but not equal to the Corliss engine.\n\nThe uniflow engine was the most efficient type of high-pressure engine. It was invented in 1911 and was used in ships, but was displaced by steam turbines and later marine diesel engines.\n\n\n"}
{"id": "8431748", "url": "https://en.wikipedia.org/wiki?curid=8431748", "title": "Information and computer science", "text": "Information and computer science\n\nInformation and computer science (ICS) or computer and information science (CIS) (plural forms, i.e., \"sciences\", may also be used) is a field that emphasizes \"both\" computing and informatics, upholding the strong association between the fields of information sciences and computer sciences and treating computers as a tool rather than a field.\n\n\"Information science\" is one with a long history, unlike the relatively very young field of computer science, and is primarily concerned with gathering, storing, disseminating, sharing and protecting any and all forms of information. It is a broad field, covering a myriad of different areas but is often referenced alongside computer science because of the incredibly useful nature of computers and computer programs in helping those studying and doing research in the field – particularly in helping to analyse data and in spotting patterns too broad for a human to intuitively perceive. While information science is sometimes confused with information theory, the two have vastly different subject matter. Information theory focuses on one particular mathematical concept of information while information science is focused on all aspects of the processes and techniques of information.\n\n\"Computer science\", in contrast, is less focused on information and its different states, but more, in a very broad sense, on the use of computers – both in theory and practice – to design and implement algorithms in order to aid the processing of information during the different states described above. It has strong foundations in the field of mathematics, as the very first recognised practitioners of the field were renowned mathematicians such as Alan Turing.\n\nInformation science and computing began to converge in the 1950s and 1960s, as information scientists started to realize the many ways computers would improve information storage and retrieval.\n\nDue to the distinction between computers and computing, some research groups refer to \"computing\" or \"datalogy\". The French refer to computer science as \"\". The term \"information and communications technology\" (ICT), refers to how humans communicate with using machines and computers, making a distinction from \"information and computer science\", which is how computers use and gain information.\n\nInformatics is also distinct from \"computer science\", which encompasses the study of logic and low-level computing issues.\n\nUniversities may confer degrees of ICS and CIS, not to be confused with a more specific Bachelor of Computer Science or respective graduate computer science degrees.\n\nThe QS World University Rankings is one of the most widely recognised and distinguished university comparisons. They ranked the top 10 universities for \"computer science\" and \"information systems\" in 2015. \n\nThey are:\n\nA Computer Information Science degree gives students both network and computing knowledge which is needed to design, develop, and assist information systems which helps to solve business problems and to support business problems and to support business operations and decision making at a managerial level also.\n\nDue the nature of this field, many topics are also shared with computer science and information systems.\n\nThe discipline of \"Information and Computer Science\" spans a vast range of areas from basic computer science theory (algorithms and computational logic) to in depth analysis of data manipulation and use within technology.\n\nThe process of taking a given algorithm and encoding it into a language that can be understood and executed by a computer. There are many different types of programming languages and various different types of computers, however, they all have the same goal: to turn algorithms into machine code.\n\nPopular programming languages used within the academic study of CIS include, but are not limited to;\n\nThe academic study of software and hardware systems that process large quantities and data, support large scale data management and how data can be used. This is where the field is unique from the standard study of computer science. The area of information systems focuses on the networks of hardware and software that are required to process, manipulate and distribute such data.\n\nThe process of analysing computer architecture and various logic circuits. This involves looking at low level computer processes at bit level computation. This is an in-depth look into the hardware processing of a computational system, involving looking at the basic structure of a computer and designing such systems. This can also involve evaluating complex circuit diagrams, and being able to construct these to solve a main problem.\n\nThe main purpose behind this area of study is to achieve an understanding of how computers function on a basic level, often through tracing machine operations.\n\nThis is the study into fundamental computer algorithms, which are the basis to computer programs. Without algorithms, no computer programs would exist. This also involves the process of looking into various mathematical functions behind computational algorithms, basic theory and functional (low level) programming.\n\nIn an academic setting, this area would introduce the fundamental mathematical theorems and functions behind theoretical computer science which are the building blocks for other areas in the field. Complex topics such as; proofs, algebraic functions and sets will be introduced during studies of CIS.\n\nInformation and computer science is a field that is rapidly developing with job prospects for students being extremely promising with 75.7% of graduates gaining employment. Also the IT industry employs one in twenty of the workforce with it predicted to increase nearly five times faster than the average of the UK and between 2012 and 2017 more than half a million people will be needed within the industry and the fact that nine out of ten tech firms are suffering from candidate shortages which is having a negative impact on their business as it delays the creation and development of new products, and it’s predicted in the US that in the next decade there will be more than one million jobs in the technology sector than computer science graduates to fill them. Because of this programming is now being taught at an earlier age with an aim to interest students from a young age into computer and information science hopefully leading more children to study this at a higher level. For example, children in England will now be exposed to computer programming at the age of 5 due to an updated national curriculum.\n\nDue to the wide variety of jobs that now involve computer and information science related tasks, it is difficult to provide a comprehensive list of possible jobs in this area, but some of the key areas are artificial intelligence, software engineering and computer networking and communication. Work in this area also tends to require sufficient understanding of mathematics and science. Moreover, jobs that having a CIS degree can lead to, include: systems analyst, network administrator, system architect, information systems developer, web programmer, or software developer.\n\nThe earning potential for CIS graduates is quite promising. A 2013 survey from the National Association of Colleges and Employers (NACE) found that the average starting salary for graduates who earned a degree in a computer related field was $59,977, up 4.3% from the prior year. This is higher than other popular degrees such as business ($54,234), education ($40,480) and math and sciences ($42,724). Furthermore, Payscale ranked 129 college degrees based on their graduates earning potential with engineering, math, science, and technology fields dominating the ranking. With eight computer related degrees appearing among the top 30. With the lowest starting salary for these jobs being $49,900. A Rasmussen College article describes various jobs CIS graduates may obtain with software applications developers at the top making a median income of $98,260.\n\nAccording to the National Careers Service an Information Scientist can expect to earn £24,000+ per year as a starting salary.\n"}
{"id": "2290843", "url": "https://en.wikipedia.org/wiki?curid=2290843", "title": "Interface Message Processor", "text": "Interface Message Processor\n\nThe Interface Message Processor (IMP) was the packet switching node used to interconnect participant networks to the ARPANET from the late 1960s to 1989. It was the first generation of gateways, which are known today as routers. An IMP was a ruggedized Honeywell DDP-516 minicomputer with special-purpose interfaces and software. In later years the IMPs were made from the non-ruggedized Honeywell 316 which could handle two-thirds of the communication traffic at approximately one-half the cost. An IMP requires the connection to a host computer via a special bit-serial interface, defined in BBN Report 1822. The IMP software and the ARPA network communications protocol running on the IMPs was discussed in RFC 1, the first of a series of standardization documents published by the Internet Engineering Task Force (IETF).\n\nThe idea of an \"Interface computer\" was first developed in 1966 by Donald Davies for the NPL network in England. The same idea was independently developed in early 1967 at a meeting of principal investigators for the Department of Defense's Advanced Research Projects Agency (ARPA) to discuss interconnecting machines across the country. Larry Roberts, who led the ARPANET implementation, initially proposed a network of host computers. Wes Clark suggested inserting \"a small computer between each host computer and the network of transmission lines\", i.e. making the IMP a separate computer.\n\nThe IMPs were built by the Massachusetts-based company Bolt Beranek and Newman (BBN) in 1969. BBN was contracted to build four IMPs, the first being due at UCLA by Labor Day; the remaining three were to be delivered in one-month intervals thereafter, completing the entire network in a total of twelve months. When Massachusetts Senator Edward Kennedy learned of BBN's accomplishment in signing this million-dollar agreement, he sent a telegram congratulating the company for being contracted to build the \"Interfaith Message Processor\".\n\nThe team working on the IMP called themselves the \"IMP Guys\":\n\nBBN began programming work in February 1969 on modified Honeywell DDP-516s. The completed code was six thousand words long, and was written in the Honeywell 516 assembly language. The IMP software was produced primarily on a PDP-1, where the IMP code was written and edited, then run on the Honeywell.\n\nBBN designed the IMP simply as \"a messenger\" that would only \"store-and-forward\". BBN designed only the host-to-IMP specification, leaving host sites to build individual host-to-host interfaces. The IMP had an error-control mechanism that discarded packets with errors without acknowledging receipt; the source IMP, upon not receiving an acknowledging receipt, would subsequently re-send a duplicate packet. Based on the requirements of ARPA's request for proposal, the IMP used a 24-bit checksum for error correction. BBN chose to make the IMP hardware calculate the checksum, because it was a faster option than using a software calculation. The IMP was initially conceived as being connected to one host computer per site, but at the insistence of researchers and students from the host sites, each IMP was ultimately designed to connect to multiple host computers.\n\nThe first IMP was delivered to Leonard Kleinrock's group at UCLA on August 30, 1969. It used an SDS Sigma-7 host computer. Douglas Engelbart's group at the Stanford Research Institute (SRI) received the second IMP on October 1, 1969. It was attached to an SDS-940 host. The third IMP was installed in University of California, Santa Barbara on November 1, 1969. The fourth and final IMP was installed in the University of Utah in December 1969. The first communication test between two systems (UCLA and SRI) took place on October 29, 1969, when a \"login\" to the SRI machine was attempted, but only the first two letters could be transmitted. The SRI machine crashed upon reception of the 'g' character. A few minutes later, the bug was fixed and the login attempt was successfully completed.\n\nBBN developed a program to test the performance of the communication circuits. According to a report filed by Heart, a preliminary test in late 1969 based on a 27-hour period of activity on the UCSB-SRI line found \"approximately one packet per 20,000 in error;\" subsequent tests \"uncovered a 100% variation in this number - apparently due to many unusually long periods of time (on the order of hours) with no detected errors.\"\n\nA variant of the IMP existed, called the TIP, which connected terminals as well as computers to the network; it was based on the Honeywell 316, a later version of the 516. Later, some Honeywell-based IMPs were replaced with multiprocessing BBN Pluribus IMPs, but ultimately BBN developed a microprogrammed clone of the Honeywell machine.\n\nIMPs were at the heart of the ARPANET until DARPA decommissioned the ARPANET in 1989. Most IMPs were either taken apart, junked or transferred to MILNET. Some became artifacts in museums; Kleinrock placed IMP Number One on public view at UCLA. The last IMP on the ARPANET was the one at the University of Maryland.\n\n\"BBN Report 1822\" specifies the method for connecting a host computer to an IMP. This connection and protocol is generally referred to as \"1822\", the report number.\n\nThe initial version of the 1822 protocol was developed in 1969: since it predates the OSI model by a decade, 1822 does not map cleanly into the OSI layers. However, it is accurate to say that the 1822 protocol incorporates the physical layer, the data link layer, and the network layer. The interface visible to the host system passes network layer addresses directly to a physical layer device.\n\nTo transmit data, the host constructs a message containing the numeric address of another host on the network (similar to an IP address on the Internet) and a data field, and transmits the message across the 1822 interface to the IMP. The IMP routes the message to the destination host using protocols that were eventually adopted by Internet routers. Messages could store a total length of 8159 Bits, of which the first 96 were reserved for the header (“leader”).\n\nWhile packets transmitted across the Internet are assumed to be unreliable, 1822 messages were guaranteed to be transmitted reliably to the addressed destination. If the message could not be delivered, the IMP sent to the originating host a message indicating that the delivery failed. In practice, however, there were (rare) conditions under which the host could miss a report of a message being lost, or under which the IMP could report a message as lost when it had in fact been received.\n\n\n\n"}
{"id": "431877", "url": "https://en.wikipedia.org/wiki?curid=431877", "title": "Internet research", "text": "Internet research\n\nInternet research is the practice of using Internet information, especially free information on the World Wide Web, or Internet-based resources (like Internet discussion forum) in research.\n\nInternet research has had a profound impact on the way ideas are formed and knowledge is created. Common applications of Internet research include personal research on a particular subject (something mentioned on the news, a health problem, etc.), students doing research for academic projects and papers, and journalists and other writers researching stories.\n\n\"Research\" is a broad term. Here, it is used to mean \"looking something up (on the Web)\". It includes any activity where a topic is identified, and an effort is made to actively gather information for the purpose of furthering understanding. It may include some post-collection analysis like a concern for quality or synthesis.\n\nThrough searches on the Internet hundreds or thousands of pages can often be quickly found with some relation to a given topic. In addition, email (including mailing lists), online discussion forums (aka message boards, BBS's), and other personal communication facilities (instant messaging, IRC, newsgroups, etc.) can provide direct access to experts and other individuals with relevant interests and knowledge.\n\nInternet research is distinct from library research (focusing on library-bound resources) and commercial database research (focusing on commercial databases). While many commercial databases are delivered through the Internet, and some libraries purchase access to library databases on behalf of their patrons, searching such databases is generally not considered part of “Internet research”. It should also be distinguished from scientific research (research following a defined and rigorous process) carried out on the Internet, from straightforward retrieving of details like a name or phone number, and from research \"about\" the Internet.\n\nInternet research can provide quick, immediate, and worldwide access to information, although results may be affected by unrecognized bias, difficulties in verifying a writer's credentials (and therefore the accuracy or pertinence of the information obtained) and whether the searcher has sufficient skill to draw meaningful results from the abundance of material typically available. The first resources retrieved may not be the most suitable resources to answer a particular question. Popularity is often a factor used in structuring Internet search results but popular information is not always most correct or representative of the breadth of knowledge and opinion on a topic.\n\nWhile conducting commercial research fosters a deep concern with costs, and library research fosters a concern with access, Internet research fosters a deep concern for quality, managing the abundance of information and with avoiding unintended bias. This is partly because Internet research occurs in a less mature information environment: an environment with less sophisticated / poorly communicated search skills and much less effort in organizing information. Library and commercial research has many search tactics and strategies unavailable on the Internet and the library and commercial environments invest more deeply in organizing and vetting their information.\n\nThe most popular search tools for finding information on the Internet include Web search engines, meta search engines, Web directories, and specialty search services. A Web search engine uses software known as a Web crawler to follow the hyperlinks connecting the pages on the World Wide Web. The information on these Web pages is indexed and stored by the search engine. To access this information, a user enters keywords in a search form and the search engine queries its algorithms, which take into consideration the location and frequency of keywords on a Web page, along with the quality and number of external hyperlinks pointing at the Web page.\n\nA Meta search engine enables users to enter a search query once and it runs against multiple search engines simultaneously, creating a list of aggregated search results. Since no single search engine covers the entire web, a meta search engine can produce a more comprehensive search of the web. Most meta search engines automatically eliminate duplicate search results. However, meta search engines have a significant limitation because the most popular search engines, such as Google, are not included because of legal restrictions.\n\nA Web directory organizes subjects in a hierarchical fashion that lets users investigate the breadth of a specific topic and drill down to find relevant links and content. Web directories can be assembled automatically by algorithms or handcrafted. Human-edited Web directories have the distinct advantage of higher quality and reliability, while those produced by algorithms can offer more comprehensive coverage. The scope of Web directories are generally broad, such as DOZ, Yahoo! and The WWW Virtual Library, covering a wide range of subjects, while others focus on specific topics.\n\nSpecialty search tools enable users to find information that conventional search engines and meta search engines cannot access because the content is stored in databases. In fact, the vast majority of information on the web is stored in databases that require users to go to a specific site and access it through a search form. Often, the content is generated dynamically. As a consequence, Web crawlers are unable to index this information. In a sense, this content is \"hidden\" from search engines, leading to the term invisible or deep Web. Specialty search tools have evolved to provide users with the means to quickly and easily find deep Web content. These specialty tools rely on advanced bot and intelligent agent technologies to search the deep Web and automatically generate specialty Web directories, such as the Virtual Private Library.\n\nWhen using the Internet for research, countless websites appear for whatever search query is entered. Each of these sites has one or more authors or associated organizations. Who authored or sponsored a website is very important to the accuracy and reliability of the information presented on the website.\n\nWhile it is very imperative that the authorship be determined for every website during Internet research, who authored or sponsored a website is essential culture when one cares about the accuracy and reliability of the information, bias, and/or web safety. For example, a website about civil rights that is authored by a member of an extremist group most likely will not contain accurate or unbiased information.\n\nThe author or sponsoring organization of a website may be found in several ways. Sometimes the author or organization can be found at the bottom of the website home page. Another way is by looking in the ‘Contact Us’ section of the website. It may be directly listed, determined from the email address, or by emailing and asking. If the author’s name or sponsoring organization cannot be determined, one should question the trustworthiness of the website. If the author’s name or sponsoring organization is found, a simple Internet search can provide information that can be used to determine if the website is reliable and unbiased.\n\nInternet research software captures information while performing Internet research. This information can then be organized in various ways included tagging and hierarchical trees. The goal is to collect information relevant to a specific research project in one place, so that it can be found and accessed again quickly.\n\nThese tools also allow captured content to be edited and annotated and some allow the ability to export to other formats. Other features common to outliners include the ability to use full text search which aids in quickly locating information and filters enable you to drill down to see only information relevant to a specific query. Captured and kept information also provides an additional backup, in case web pages and sites disappear or are inaccessible later.\n\n\n"}
{"id": "8407262", "url": "https://en.wikipedia.org/wiki?curid=8407262", "title": "Jean-Luc Brédas", "text": "Jean-Luc Brédas\n\nJean-Luc Brédas is an American chemist, working at the Georgia Institute of Technology. He was born in Fraire, Belgium, on May 23, 1954.\n\nHe received his Ph.D. from the University of Namur, Belgium, in 1979. In 1988, he was appointed Professor at the University of Mons, Belgium, where he established the Laboratory for Chemistry of Novel Materials. While keeping an \"Extraordinary Professorship\" appointment in Mons, he moved to the US in 1999 and became Full Professor of Chemistry at the University of Arizona. In 2003, he moved to the Georgia Institute of Technology as Full Professor of Chemistry and Biochemistry. In July 2014, he took a 2-½-year leave of absence to King Abdullah University of Science and Technology (KAUST) where he served as Distinguished Professor of Materials Science and Engineering and Director of the KAUST Solar and Photovoltaics Research and Engineering Center. At Georgia Tech, where he resumed his activities in January 2017, he is Regents’ Professor of Chemistry and Biochemistry and holds the Vasser-Woolley and Georgia Research Alliance Chair in Molecular Design. He is a Georgia Research Alliance Eminent Scholar since 2005.\n\nJean-Luc Bredas is a Member of the International Academy of Quantum Molecular Science, the Royal Academy of Belgium, and the European Academy of Sciences. He is the recipient of multiple awards, including the 1997 Francqui Prize, the 2000 Quinquennial Prize of the Belgian National Science Foundation, the 2001 Italgas Prize, the 2003 Descartes Prize of the European Union, the 2010 Charles H. Stone Award of the American Chemical Society, the 2013 David Adler Award in Materials Physics of the American Physical Society, and the 2016 Award in the Chemistry of Materials of the American Chemical Society. He is a Fellow of the American Chemical Society (Inaugural Class of 2009), American Physical Society, Optical Society of America, Royal Society of Chemistry, and Materials Research Society (Inaugural Class of 2008), and an Honorary Professor of the Institute of Chemistry of the Chinese Academy of Sciences. He holds Honorary Degrees from the University of Linköping, Sweden, and the Free University of Brussels. He has published over 1,000 refereed articles (that –as of May 2018– have garnered over 70,000 citations, leading to a Web of Science h-index of 121) and given over 500 invited presentations. Since 2008, he has served as an Editor for \"Chemistry of Materials\", published by the American Chemical Society.\n\n"}
{"id": "6389068", "url": "https://en.wikipedia.org/wiki?curid=6389068", "title": "Johannes Theodor Reinhardt", "text": "Johannes Theodor Reinhardt\n\nJohannes Theodor Reinhardt (3 December 1816, in Copenhagen – 23 October 1882, in Frederiksberg) was a Danish zoologist and herpetologist. He was the son of Johannes Christopher Hagemann Reinhardt.\n\nHe participated as botanist in the first Galathea Expedition (1845—1847). In 1848 he became a curator at the \"Kongelige Naturhistoriske Museum\" in Copenhagen (now University of Copenhagen Zoological Museum). He taught classes in zoology at the \"Danmarks Tekniske Universitet\" (1856–1878) and at the University of Copenhagen (1861–1878). In 1854 he received the title of professor.\n\nDuring the 1840s and 1850s he periodically worked in Brazil as an assistant to palaeontologist Peter Wilhelm Lund (1801–1880). He was an early supporter of Charles Darwin’s theory of evolution, and from his research of extinct species, was critical of George Cuvier's concept of \"anti-evolutionary catastrophism\".\n\nWith Christian Frederik Lütken (1827–1901), he was co-author of \"Bidrag til Kundskab om Brasiliens Padder og Krybdyr\" (Contributions to the knowledge of Brazilian amphibians and reptiles).\n\nHe described 25 new species of reptiles, some with Lütken.\n\nIn 1848, Hermann Schlegel named the Calabar \"python\", \"Calabaria reinhardtii\", in his honor. \n\n"}
{"id": "5275725", "url": "https://en.wikipedia.org/wiki?curid=5275725", "title": "King City weather radar station", "text": "King City weather radar station\n\nThe King City weather radar station (ICAO site identifier CWKR) is a weather radar located in King City, Ontario, Canada. It is operated by Environment Canada and is part of the Canadian weather radar network, for which it is the primary research station.\n\nThe 16.45 hectare site is listed at an elevation of 360 m, and the tower is 27 m tall.\n\nMounted on the tower is a 5 cm weather radar, and a C-band dual-polarization radar system was installed at the site in 2004.\n\nThe station serves a number of research roles, and collects data to fulfill those observational needs. It is \"responsible for providing national leadership on radar meteorology research applications\".\n\nIn 1984, the Research Directorate of the Atmospheric Environment Service established the first Canadian weather radar with Doppler capability in King City. In 2004, a dual-polarization radar was installed for further research. These systems are used for predictive purposes, and the data collected is used for weather forecasts for the Greater Toronto Area and the Golden Horseshoe.\n\nFurther, under the auspices of the Cloud Physics and Severe Weather Research Section of Environment Canada, the King Doppler Weather Radar Research Facility collects data for research.\n\nThe C-band radar can be useful for observing bird migration patterns, especially when data is taken in aggregate with that of other radar stations. Current active research in dual-polarization radar includes winter precipitation, detection and short-term forecasting of high-impact weather events, quantitative precipitation estimation, satellite validation, and particle type identification.\n\nThe WSR82D radar installed in 1982 had a fiberglass laminate radome, and a parabolic reflector with a diameter of 6.1m and linear horizontal polarisation. Its gain was 48 dB. The radar emitted a 260kW beam with a frequency of 5625 MHz and wavelength of 5.33cm having a beam width of 0.65 degrees. In conventional operation, it had a pulse duration of 2µs, pulse repetition frequency of 250 pulses per second, and performed 6.0 scanning rotations per minute. In Doppler operation, it had a pulse duration of 0.5µs, pulse repetition frequency of 892 or 1190 pulses per second, and a scan rate of 0.75 rotations per minute. In long-range Doppler operation, a pulse repetition frequency of 650 pulses per second and a scan rate of 2 rotations per minute were used.\n\nIt was replaced in 2004 by a similar radar but with dual polarization capabilities.\n\n\n"}
{"id": "5220510", "url": "https://en.wikipedia.org/wiki?curid=5220510", "title": "List of centroids", "text": "List of centroids\n\nThe following is a list of centroids of various two-dimensional objects. The centroid of an object formula_1 in formula_2-dimensional space is the intersection of all hyperplanes that divide formula_1 into two parts of equal moment about the hyperplane. Informally, it is the \"average\" of all points of formula_1. For an object of uniform composition (mass, density, etc.) the centroid of a body is also its center of mass. In the case of two-dimensional objects shown below, the hyperplanes are simply lines.\n\nFor each two-dimensional shape below, the area and the centroid coordinates formula_5 are given:\n\n\n"}
{"id": "7631543", "url": "https://en.wikipedia.org/wiki?curid=7631543", "title": "List of things named after Albert Einstein", "text": "List of things named after Albert Einstein\n\nThis is a list of things named after Albert Einstein.\n\n\n\n\n\n\n\n\n\n"}
{"id": "14345397", "url": "https://en.wikipedia.org/wiki?curid=14345397", "title": "Lucas Friedrich Julius Dominikus von Heyden", "text": "Lucas Friedrich Julius Dominikus von Heyden\n\nLucas Friedrich Julius Dominikus von Heyden (22 May 1838, Frankfurt – 13 September 1915, Frankfurt) was a German entomologist specialising in Coleoptera beetles. He wrote with Edmund Reitter and Ján Weiss \"Catalogus coleopterorum Europae, Caucasi et Armeniae rossicae\". Edn 2. Berlin, Paskau, Caen(1902).\n\n"}
{"id": "45472925", "url": "https://en.wikipedia.org/wiki?curid=45472925", "title": "Nanostrain", "text": "Nanostrain\n\nNanostrain is an EU-funded project to characterise piezoelectric materials for future fast digital switch designs.\n\nThe switching may only need a much lower voltage and be faster with lower power consumption than CMOS.\n\n"}
{"id": "18748415", "url": "https://en.wikipedia.org/wiki?curid=18748415", "title": "Pamela Dalton", "text": "Pamela Dalton\n\nPamela Dalton is a cognitive psychologist. She has a Ph.D. in experimental psychology and a Masters in Public Health. Dalton is frequently quoted by the popular press as an authority on environmental odors.\n\nShe is most notable for her contributions to the research toward the fields of sick building syndrome and multiple chemical sensitivity.\nIn the past she has worked with the United States Department of Defense on \"nonlethal weapons development\", or the enhancement of bad odors as weapons. \nShe currently works at the Monell Chemical Senses Center.\n\nShe also was a contributor to the NIH Toolbox for the Assessment of Neurological and Behavioral Function (www.nihtoolbox.org), as a member of the NIH Toolbox steering committee and the Olfaction team, developing the NIH Toolbox Odor Identification Test. The contract for the NIH Toolbox for the Assessment of Neurological and Behavioral Function (www.nihtoolbox.org) was initiated by the NIH Blueprint for Neuroscience Research (www.neuroscienceblueprint.nih.gov) in 2006 to develop a set of state-of-the-art measurement tools to enhance collection of data in large cohort studies in biomedical research.\n\n"}
{"id": "53844107", "url": "https://en.wikipedia.org/wiki?curid=53844107", "title": "Peter H. Fisher", "text": "Peter H. Fisher\n\nPeter H. Fisher from the Massachusetts Institute of Technology, was awarded the status of Fellow in the American Physical Society, after they were nominated by their Division of Particles and Fields in 2006, for \"initiating Tau Polarization Asymmetry Measurements and W-Boson self couplings, leading to a top mass prediction (found later at FNAL). First proof of single W-production in e+ e- annihilation. Determination of sin20w with proposal to expand to highest accuracy of 10-5 at LHC.\"\n"}
{"id": "59100265", "url": "https://en.wikipedia.org/wiki?curid=59100265", "title": "Planctomicrobium", "text": "Planctomicrobium\n\nPlanctomicrobium is an aerobic genus of bacteria from the family of Planctomycetaceae with one known species (\"Planctomicrobium piriforme\"). \"Planctomicrobium piriforme\" has been isolated from littoral wetland from the Valaam Island in Russia.\n"}
{"id": "916394", "url": "https://en.wikipedia.org/wiki?curid=916394", "title": "Primary constraint", "text": "Primary constraint\n\nIn Hamiltonian mechanics, a primary constraint is a relation between the coordinates and momenta that holds without using the equations of motion . A secondary constraint is one that is not primary, in other words it holds when the equations of motion are satisfied, but need not hold if they are not satisfied . The secondary constraints arise from the condition that the primary constraints should be preserved in time. (A few authors use more refined terminology, where the non-primary constraints are divided into secondary, tertiary, quaternary... constraints: the secondary constraints arise directly from the condition that the primary constraints are preserved by time, the tertiary constraints arise from the condition that the secondary ones are also preserved by time, and so on.) Primary and secondary constraints were introduced by and developed by .\n\nThe terminology of primary and secondary constraints is confusingly similar to that of first and second class constraints. These divisions are independent: both first and second class constraints can be either primary or secondary, so this gives altogether four different classes of constraints.\n\n"}
{"id": "882289", "url": "https://en.wikipedia.org/wiki?curid=882289", "title": "Project Vela", "text": "Project Vela\n\nProject Vela was a project undertaken by the United States Department of Defense to develop and implement methods to monitor compliance with the 1963 Partial Test Ban Treaty, which banned the testing of nuclear weapons in the Atmosphere, in Outer Space, and Underwater, effectively meaning nuclear tests were only to be permitted underground. The Vela Project started off as a small budget research program within the DARPA Projects Agencies until around 1961, when it was granted greater funding and resources through the authority of the Deputy Assistant Secretary of the Army, likely prompted by the same increased caution over Domestic Nuclear Affairs that was the effect of the 1961 Goldsboro B-52 crash, as well as in anticipation of the 1963 PTB Treaty.\n\nThe initial project was limited to the development of the Vela Satellites, but was then expanded and reorganized into three distinct levels of monitoring: Vela Uniform, which was the portion of the project dedicated towards monitoring seismic signals, in order to detect underground and underwater nuclear testing, Vela Hotel, which was a continuation of the satellite program to detect nuclear testing solely beyond the atmosphere at an extended range, and Vela Sierra, which was a set of advanced satellites that were designed to detect testing both in the close reaches of outer space and within the atmosphere of the planet itself. The project’s space and atmospheric divisions ran until around the emplacement of satellites in the Defense Support Program (DSP), and were eventually integrated into the Global Positioning System set of satellites. Currently, all satellites under Project Vela are under the management of IONDS. The seismic division of Project Vela remains in activity today.\n\nThough there is little to no actual documentation that directly concerns the issue of the Initial Vela Project, due to its low funding, staffing, resources and status of priority. However, it is known that the majority of the work done on the initial Vela Project was implemented into Vela Hotel, which monitored outer space for nuclear tests. The Vela Hotel satellite program was mostly developed at Los Alamos Scientific Laboratory, under the supervision of the United States Air Force. The method through which they ended up monitoring for nuclear testing beyond the atmosphere utilized a system of X-ray detection, as well as Neutron and Gamma-Ray detection.\n\nThe Satellites’ actual instrumentation count is 12 X-ray detectors, 18 internal Neutron and Gamma-ray detectors, transmission gear to send the information back down to Earth, and a set of Solar Panels which generated a net power of 90 watts for the use of all instrumentation on each Satellite. Of the Vela Hotel series, which was launched a week after the PTB Treaty on October 17, 1963, 6 total satellites were manufactured and launched, each with a design life of six months. After deployment, they remained operational and online for 5 years before being shut down.\n\nThe Vela Hotel series of satellites never actually did detect any weapons being tested in outer space, but it did bring the scientific community valuable knowledge regarding the mechanics of the solar system, namely, this series of satellites was the set responsible for discovering Gamma-Ray Bursts, which are now recognized as the most violent events in the universe, marking the collapsing of stars and black holes. The discovery of Gamma-Ray Bursts has allowed scientists to be able to map the universe to a heretofore unparalleled degree, as sensors no longer have to rely on light alone to determine things far off in deep space.\n\nAfter the completion of development on the first set of Vela Satellites in the Hotel series, DARPA began work on the next phase of the Vela satellite program, namely, on Vela Sierra, which was intended to monitor the atmosphere for covert nuclear tests. Following the deployment of the Hotel series, Los Alamos resumed development on the satellites, this time working towards the Advanced Vela series. These new satellites were created with the intention that they would replace the Hotel Series, as well as function as the satellites that would complete the objectives of the Vela Sierra subset.\n\nThe new set of Advanced Vela satellites were equipped with the same detection equipment as the Hotel series, those being the 12 X-ray detectors and 18 internal Neutron and Gamma-Ray detectors, but were also equipped with a new set of instruments. The Advanced Vela series came equipped with two non-imaging photodiode sensors called bhangmeters. These measured light levels over sub-millisecond intervals, and could detect a nuclear explosion to within 3,000 miles. This addition was necessary due to the singular effects that atmospheric nuclear explosions produce, called either the “Double Flash” or the “Double-Humped Curve,” which is a pair of flashes of light caused by an explosion, one lasting a millisecond which is quite intense and bright, the other far more prolonged and less intense. This “Double Flash” is what distinguishes atmospheric nuclear explosions from other kinds of explosions in the atmosphere, along with radiation, since the only feasible event that could theoretically produce a double event like an atmospheric nuclear explosion would be something incredibly rare like a meteoroid generated lightning superbolt. However, radiation in the atmosphere is far less easy to calculate, as the Van Allen radiation belts can mask radioactive wavelengths and signatures. The Advanced Vela satellites were additionally fitted with larger solar panels, since the new design required 120 watts as opposed to the Hotel series’ 90 watts, as well as electromagnetic sensors for detecting an electromagnetic pulse from an in-atmosphere detonation. The new set of satellites had a design life of 18 months, but instead lasted on average 7 years, with the longest lasting of the set (Vehicle 9) being shut down in 1984, lasting around 15 years before being shut down.\n\nVela Sierra’s most famous actual event was the “Double Flash” detected on September 22, 1979 by Vela satellite 6911 near the Prince Edward Islands. However, due to the lack of any corroborating evidence that a bomb ever went off in the area other than the readings the satellite provided, it is widely regarded as either a malfunction or a magnetospheric event affecting the instruments aboard the satellite. It should be noted that at the time of the event, President Jimmy Carter deemed the event evidence of a joint Israeli-South African nuclear test, though since then no evidence has been uncovered to corroborate whether that is true or untrue.\n\nVela Uniform was the second half of Project Vela that began its development soon after the PTB Treaty was put into effect. Vela Uniform was created with the intention of monitoring seismic activity in order to determine the magnitude and location of any covert nuclear weapons tests beneath the surface of the earth, or underwater at any point on the Earth’s surface. The methods DARPA used to create a system with which they could determine the difference between a regular seismic event and an underground covert nuclear test was through the rigorous measurement of seven nuclear tests in the Continental United States and Alaska, as well as numerous smaller experiments using conventional high explosives rather than nukes.\n\nOver time, Vela Uniform’s methods and techniques for measuring seismic activity to determine the existence and location of covert underground nuclear tests (which have been illegal under international law since the widespread ratification of the Comprehensive Nuclear-Test-Ban Treaty or CTBT) and stations of seismographical monitoring exist all around the world. This has had the effect of giving more nations the ability to better understand their own country’s seismic activities, and has given more and more countries fundamentally more access to aid in situations of Earthquakes, Tsunamis, and other such events, as well as ensuring the detection of any future covert underground nuclear tests.\n\nThe name \"Vela\" is shared with a part of a constellation known as \"Argo Navis,\" which is supposed to represent a boat. The part named \"Vela\" refers to the sails of the ship. \n"}
{"id": "8978774", "url": "https://en.wikipedia.org/wiki?curid=8978774", "title": "Quantum nonlocality", "text": "Quantum nonlocality\n\nIn theoretical physics, quantum nonlocality most commonly refers to the phenomenon by which measurements made at a microscopic level contradict a collection of notions known as \"local realism\" that are regarded as intuitively true in classical mechanics. However, some quantum mechanical predictions of multi-system measurement statistics on entangled quantum states cannot be simulated by any local hidden variable theory. An explicit example is demonstrated by Bell's theorem, which has been verified by experiment.\n\nExperiments have generally favoured quantum mechanics as a description of nature, over local hidden variable theories. Any physical theory that supersedes or replaces quantum theory must make similar experimental predictions and must therefore also be nonlocal in this sense; quantum nonlocality is a property of the universe that is independent of our description of nature.\n\nQuantum nonlocality does not allow for faster-than-light communication, and hence is compatible with special relativity. However, it prompts many of the foundational discussions concerning quantum theory.\n\nIn October 2018, physicists reported that quantum behavior can be explained with classical physics for a single particle, but not for multiple particles as in quantum entanglement and related nonlocality phenomena.\n\nIn 1935, Einstein, Podolsky and Rosen published a thought experiment with which they hoped to expose the incompleteness of the Copenhagen interpretation of quantum mechanics in relation to the violation of local causality at the microscopic scale that it described. Afterwards, Einstein presented a variant of these ideas in a letter to Erwin Schrödinger, which is the version that is presented here. The state and notation used here are more modern, and akin to Bohm's take on EPR. The quantum state of the two particles prior to measurement can be written as\n\nWhere formula_2.\n\nHere, subscripts \"A\" and \"B\" distinguish the two particles, though it is more convenient and usual to refer to these particles as being in the possession of two experimentalists called Alice and Bob. The rules of quantum theory give predictions for the outcomes of measurements performed by the experimentalists. Alice, for example, will measure her particle to be spin-up in an average of fifty percent of measurements. However, according to the Copenhagen interpretation, Alice's measurement causes the state of the two particles to collapse, so that if Alice performs a measurement of spin in the z-direction, that is with respect to the basis formula_3, then Bob's system will be left in one of the states formula_4. Likewise, if Alice performs a measurement of spin in the x-direction, that is with respect to the basis formula_5, then Bob's system will be left in one of the states formula_6. Schrödinger referred to this phenomenon as \"steering\". This steering occurs in such a way that no signal can be sent by performing such a state update; quantum nonlocality cannot be used to send messages instantaneously and is therefore not in direct conflict with causality concerns in Special Relativity.\n\nIn the Copenhagen view of this experiment, Alice's measurement—and particularly her measurement choice—have a direct effect on Bob's state. However, under the assumption of locality, actions on Alice's system do not affect the \"true\", or \"ontic\" state of Bob's system. We see that the ontic state of Bob's system must be compatible with one of the quantum states formula_7 or formula_8, since Alice can make a measurement that concludes with one of those states being the quantum description of his system. At the same time, it must also be compatible with one of the quantum states formula_9 or formula_10 for the same reason. Therefore, the ontic state of Bob's system must be compatible with at least two quantum states; the quantum state is therefore not a complete descriptor of his system. Einstein, Podolsky and Rosen saw this as evidence of the incompleteness of the Copenhagen interpretation of quantum theory, since the wavefunction is explicitly not a complete description of a quantum system under this assumption of locality. Their paper concludes:\n\nAlthough various authors (most notably Niels Bohr) criticised the ambiguous terminology of the EPR paper, the thought experiment nevertheless generated a great deal of interest. Their notion of a \"complete description\" was later formalised by the suggestion of hidden variables that determine the statistics of measurement results, but to which an observer does not have access. Bohmian mechanics provides such a completion of quantum mechanics, with introduction of hidden variables; however the theory is explicitly nonlocal. The interpretation therefore does not give an answer to Einstein's question, which was whether or not a complete description of quantum mechanics could be given in terms of \"local\" hidden variables in keeping with the \"Principle of Local Action\".\n\nIn 1964 John Bell answered Einstein's question by showing that such local hidden variables can never reproduce the full range of statistical outcomes predicted by quantum theory. Bell showed that a local hidden variable hypothesis leads to restrictions on the strength of correlations of measurement results. If the Bell inequalities are violated experimentally as predicted by quantum mechanics, then reality cannot be described by local hidden variables and the mystery of quantum nonlocal causation remains. According to Bell:\n\nClauser, Horne, Shimony and Holt (CHSH) reformulated these inequalities in a manner that was more conducive to experimental testing (see CHSH inequality). They proposed a scheme whereby two experimentalists, Alice and Bob, make separate measurements of photon polarization in two carefully chosen directions, and derived a simple inequality that is obeyed by all local hidden variable theories, but violated by certain measurements on quantum states.\n\nBell formalized the idea of a hidden variable by introducing the parameter \"λ\" to locally characterize measurement results on each system: \"It is a matter of indifference ... whether \"λ\" denotes a single variable or a set ... and whether the variables are discrete or continuous\". However, it is equivalent (and more intuitive) to think of \"λ\" as a local \"strategy\" or \"message\" that occurs with some probability \"ρ(formula_11)\" when an entangled pair of states is created. EPR's criteria of local separability then stipulates that each local strategy defines the distributions of \"independent\" outcomes if Alice measures in direction \"A\" and Bob measures in direction \"B\":\n\nwhere formula_13 denotes addition modulo two.\n\nVarious attempts have been made to argue why Nature does not (or should not) allow for stronger nonlocality than quantum theory is already known to permit. For example, in recent publications it was found that quantum mechanics cannot be more nonlocal without violating the Heisenberg uncertainty principle. Strikingly, it has been discovered that if PR boxes did exist, any distributed computation could be performed with only one bit of communication. An even stronger result is that for any nonlocal box theory which violates Tsirelson's bound, there cannot be a sensible measure of mutual information between pairs of systems. This suggests a deep link between nonlocality and the information-theoretic properties of quantum mechanics. Nevertheless, the PR-box is ruled out by a plausible postulate of information theory.\n\nNon-signaling adversaries have recently been considered in quantum cryptography.\nSuch an adversary is constrained only by the non-signaling principle, and so may potentially be more powerful than a quantum adversary.\n\nSimilarly to de Broglie–Bohm pilot wave theory, Blasiak's model stipulates in the ontology a particle and a pilot wave, and the pilot wave ensures predictions indistinguishable from the quantum case. The main distinction is wave function collapse. De Broglie–Bohm theory avoids collapse by including the whole universe (with all particle detectors, other devices, observers etc.) into the system. Blasiak's model describes a single particle in a quantum circuit (called \"interferometric circuit\") that contains devices such as phase shifters, beam splitters and particle detectors. Usually, a detector causes collapse (non-local, discontinuous change) of a wave function. But Blasiak's model treats detection without violating the locality principle, via the following ideas.\n\nThe wave function consists of fragments. Each fragment emerges from a detection, and propagates from the point of detection, gradually crowding out older fragments. The particle moves inside the expanding region occupied by the most recent fragment. Thus, obsolete fragments are harmless, they never pilot the particle. Gradual elimination of older fragments is implemented via the third ontological component, a field that carries information on the time of the most recent detection in the causal past of a given space-time point.\n\nNonlocality does not mean that information can travel faster-than-light. Indeed, quantum field theory preserves causality, meaning that no influence can be projected between two points faster than the speed-of-light.\n\nThis can be shown by noting that the commutator of two spacelike local quantum operators is always zero (or anti-commutator for fermionic operators).\n\n\n"}
{"id": "178643", "url": "https://en.wikipedia.org/wiki?curid=178643", "title": "R-7 Semyorka", "text": "R-7 Semyorka\n\nThe R-7 () was a Soviet missile developed during the Cold War, and the world's first intercontinental ballistic missile. The R-7 made 28 launches between 1957 and 1961, but was never deployed operationally. A derivative, the R-7A, was deployed from 1959 to 1968. To the West it was known by the NATO reporting name SS-6 Sapwood and within the Soviet Union by the GRAU index 8K71. In modified form, it launched Sputnik 1, the first artificial satellite, into orbit, and became the basis for the R-7 family which includes Sputnik, Luna, Molniya, Vostok, and Voskhod space launchers, as well as later Soyuz variants.\n\nThe widely used nickname for the R-7 launcher, \"Semyorka\", means \"the 7\" in Russian.\n\nThe R-7 was long, in diameter and weighed ; it had two stages, powered by rocket engines using liquid oxygen (LOX) and kerosene and capable of delivering its payload up to , with an accuracy (CEP) of around . A single thermonuclear warhead was carried with a nominal yield of 3 megatons of TNT. The initial launch was boosted by four strap-on liquid rocket boosters making up the first stage, with a central 'sustainer' motor powering through both the first and the second stage. Each strap-on booster included two vernier thrusters and the core stage included four. The guidance system was inertial with radio control of the vernier thrusters.\n\nDesign work began in 1953 at OKB-1 in Kaliningrad in Moscow Oblast (presently Korolev, Moscow Oblast) and other divisions with the requirement for a two-stage missile of with a range of and the maximum speed of 20 mach carrying a warhead. Following first ground tests in late 1953 the initial design was heavily reworked and the final design was not approved until May 1954 and Korolev reportedly reviewed more than 100 design proposals. In 1954, the draft project was completed. \n\nContrary to claims that the R-7 was based largely on experience and assistance of German scientists, the missile is noteworthy for looking beyond past achievements that had used German ideas. For example, instead of using jet vanes for control, which increased resistance generated at the engine nozzle exhaust outlet, the R-7 used special control engines. These same engines served as the last stage’s vernier thrusters.\n\nBecause of clustered design, each booster had its own propellant tanks. The design team had to develop a system to regulate the propellant component consumption ratio and to synchronize the consumption between the boosters.\n\nStarting from the R-1, which was a copy of the German V-2, a free-standing missile was launched from a horizontal pad. It turned out that assembling a cluster of a central core and four boosters on the pad is almost impossible without it falling apart. Also, a wind gust could knock the missile off of the pad. The solution was to eliminate the pad and to suspend the entire rocket in the trusses that bear both vertical weight load as well as horizontal wind forces. The launch system simulated flight conditions with strap-on boosters pushing the central core forward.\n\nThe new missile's GRAU index was 8K71. The first flight-ready vehicle was delivered to the Baikonur Cosmodrome on 1 May 1957, and flown on 15 May. A fire broke out in the Blok D strap-on almost immediately at liftoff. It broke away from the booster at T+88 seconds, which crashed 400 km (248 miles) downrange. The next attempt on 11 June (notable the same day the United States conducted its first test launch of an ICBM), an electrical short caused the missile to start rolling uncontrollably and disintegrate 33 seconds after liftoff. The first successful long flight, of , was made on 21 August 1957. The dummy warhead impacted in the Pacific Ocean and five days later, TASS announced that the Soviet Union had \"successfully tested a multi-stage intercontinental ballistic missile\". A modified version of the missile (8K71PS) placed Sputnik 1 in orbit from Baikonur on 4 October 1957 and Sputnik 2 on 3 November 1957.\n\nThe next ICBM test took place on 30 January 1958, but the strap-ons failed to separate cleanly and damaged plumbing in the core stage, which lost thrust and impacted far off target. These early flights revealed assorted design flaws in the R-7 which necessitated multiple modifications to the missile. Testing continued through December 1959, and the last original 8K71 flew on 27 February 1961. The additional development resulted in the 8K74 (also known as R-7A), which was lighter, had better navigation systems, more powerful engines, extended its range to 12,000 km by carrying more fuel, and increased payload to . In addition, the missile was designed to be easier to take apart and service. The warhead was tested on Novaya Zemlya in October 1957 and again in 1958, yielding an estimated 2.9 Mt. Aside from the initial Sputnik launches, the 8K71 formed the basis of the 8K72 booster used for the first generation Luna probes. However, six out of nine Luna probes launched on the 8K72 failed. Combined with the failed Sputnik launch on 27 April 1958, this brought the booster's total space launch record to 6 successes in 13 attempts. The improved 8K74 would then form the basis for the later Vostok and Molniya boosters, greatly increasing reliability.\n\nThe first strategic-missile unit became operational on 9 February 1959 at Plesetsk in north-west Russia. On 15 December 1959 the R-7 missile was tested at Plesetsk for the first time. The missiles were fully deployed by 1962.\n\nTotal service was limited to no more than ten nuclear-armed missiles active at any time. A single launch pad was operational at Baikonur and from six to eight were in operation at Plesetsk.\n\nThe costs of the system were high, mostly due to the difficulty of constructing in remote areas the large launch sites required. At one point, each launch site was projected to cost 5% of the total Soviet defense budget.\n\nBesides the cost, the missile system faced other operational challenges. With the U-2 overflights, the huge R-7 launch complexes could not be hidden and therefore could be expected to be destroyed quickly in any nuclear war. Also, the R-7 took almost twenty hours to prepare for launching, and it could not be left on alert for more than a day due to its cryogenic fuel system. Therefore, the Soviet force could not be kept on permanent alert and could have been subject to an air strike before launching. Additionally, the huge payload for which it was designed, adapted to early heavy H-bombs, became irrelevant with the coming of lighter bomb technology.\n\nThe limitations of the R-7 pushed the Soviet Union into rapidly developing second-generation missiles which would be more viable weapons systems. The R-7 was phased out of military service by 1968.\n\nWhile the R-7 turned out to be impractical as a weapon, it became the basis for a series of Soviet expendable space launch vehicles, the Soyuz family of launchers. As of 2018, in modified versions (Soyuz-U, Soyuz-FG, and the Soyuz-2 (including the boosterless 2.1v variant), the vehicle is still in service, having launched over 1840 times.\n\nNote: Much developed variants of the R-7 are still active:\n\n\n\n"}
{"id": "22319076", "url": "https://en.wikipedia.org/wiki?curid=22319076", "title": "Reginald W. James", "text": "Reginald W. James\n\nReginald William James, FRS (9 January 1891 – 7 July 1964) was a student, researcher, and teacher of physics in England and South Africa. He is best known for his service in the Imperial Trans-Antarctic Expedition of 1914–1916, for which he was awarded the Silver Polar Medal.\n\nJames was born on 9 January 1891 in London. After displaying adolescent skills as a maths prodigy, he was awarded a stipend to pursue studies in St. John's College, Cambridge.\n\nJames signed on as an expedition naturalist in the Imperial Trans-Antarctic Expedition led by Sir Ernest Shackleton, which departed England on the \"Endurance\" in August 1914; James had expected to winter over at the expedition's projected base on the Weddell Sea but the ice-beset expedition vessel never made Antarctic landfall and, with the rest of the ship's company, James found himself a castaway. His journal of life on a Weddell Sea ice floe and on Elephant Island survives.\n\nUpon the rescue of the men from Elephant Island in 1916, James found his country fighting World War I. He joined the Royal Engineers, rising to the rank of captain and performing tasks relating to artillery spotting on the Western Front. With the coming of peace, James turned to academia at the University of Manchester. He was a lecturer in 1919, a senior lecturer in 1921, and a Reader in 1934. He specialised in problems of X-ray crystallography.\n\n1936–1937 saw a change in James' personal and professional life. In the first year he married Annie Watson, and in the second year he changed institutions to the University of Cape Town, which offered him the rank of professor. One of his MSc students there was Aaron Klug. His professional career reached culmination in 1953–1957 when he served as Vice-Chancellor of the university. He was elected as a Fellow of the Royal Society in 1955.\n\nJames began the process of his retirement in 1958 and, beset by progressive cardiovascular disease, wound down his teaching duties over the following five years. He died in Cape Town at age 73 on 7 July 1964, and was survived by three children.\n"}
{"id": "43447946", "url": "https://en.wikipedia.org/wiki?curid=43447946", "title": "Scanning Habitable Environments with Raman and Luminescence for Organics and Chemicals", "text": "Scanning Habitable Environments with Raman and Luminescence for Organics and Chemicals\n\nScanning Habitable Environments with Raman and Luminescence for Organics and Chemicals (SHERLOC) is an ultraviolet Raman spectrometer that uses fine-scale imaging and an ultraviolet (UV) laser to determine fine-scale mineralogy and detect organic compounds designed for the Mars 2020 rover mission. It is constructed at the Jet Propulsion Laboratory with major subsystems being delivered from Malin Space Science Systems and Los Alamos National Laboratory. The Principal Investigator is Luther Beegle, and the Deputy Principal Investigator is Rohit Bhartia.\n\nSHERLOC will have a calibration target with possible Mars suit materials, and it will measure how they change over time in the Martian surface environment.\n\nAccording to a 2017 Universities Space Research Association (USRA) report: \n\nSHERLOC will be mounted on the robotic arm of the Mars 2020 rover. It consists of both imaging and spectroscopic elements. It has two imaging components consisting of heritage hardware from the MSL MAHLI instrument. One is a built to print re-flight that can generate color images over multiple scales. The other acts as the mechanism that allows the instrument to get a contextual image of a sample and to autofocus the laser spot for the spectroscopic part of the SHERLOC investigation.\n\nFor Spectroscopy, it utilizes a NeCu laser to generate UV photons (248.6 nm) which can generate characteristic Raman and fluorescence photons from a scientifically interesting sample. The DUV laser is co-boresighted to a context imager and integrated into an autofocusing/scanning optical system that allows us to correlate spectral signatures to surface textures, morphology and visible features. The context imager has a spatial resolution of 30 µm and currently is designed to operate in the 400-500 nm wavelength range.\n\n"}
{"id": "58553622", "url": "https://en.wikipedia.org/wiki?curid=58553622", "title": "The Tale of Shim Chong", "text": "The Tale of Shim Chong\n\nThe Tale of Shim Chong () is a Korean folk story. \"Simcheongga\" is the \"pansori\" (traditional musical storytelling) of the tale. It was adapted on screen by Shin Sang-ok twice, once in South Korea in 1972, and as \"The Tale of Shim Chong\" in 1985 when he and his wife Choi Eun-hee were abducted to North Korea.\n\n"}
{"id": "36004587", "url": "https://en.wikipedia.org/wiki?curid=36004587", "title": "Thorngate's postulate of commensurate complexity", "text": "Thorngate's postulate of commensurate complexity\n\nThorngate's postulate of commensurate complexity, also referred to as Thorngate's impostulate of theoretical simplicity is the description of a phenomenon in social science theorizing. Karl E. Weick maintains that research in the field of social psychology can – at any one time – achieve only two of the three meta-theoretical virtues of \"Generality\", \"Accuracy\" and \"Simplicity.\" One of these aspects therefore must always be subordinated to the others. The postulate is named for the Canadian social psychologist Warren Thorngate of the University of Alberta, whose work is quoted by Weick.\n\nThorngate described the problem this way: \nThe postulate was a response to the debate among sociologists – mainly between Kenneth J. Gergen and Barry R. Schlenker – revolving around the meaning of sociological research. Whilst Schlenker appeared to maintain the position, that context only superficially influenced social behavior, Gergen appeared to maintain that context penetrated everything in social behavior, rendering observations as specific to the very situation observed. Thus, simplifying the discussion, the observation of social behavior would be no more than collecting historical data, since context would never be the same and the results would remain unique. In fact, sociology would be some specialized kind of historical research. Considering this, Thorngate writes\n\nThe statement was confirmed by Gergen:\n\nWeick represents this model “as a clockface with general at 12:00, accurate at 4:00, and simple at 8:00 to drive home the point that an explanation that satisfies any two characteristics is least able to satisfy the third characteristic.”\n\nAccording to Weick, research operates in this continuum: \n\nBasically, Weick maintains, that there is a \"trade-off\" between these three virtues in such a way that only two can be achieved at any given time. Research therefore must operate in different modes to capture reality in sufficient precision and granularity. The postulate therefore becomes descriptive of research and prescriptive of research methodology.\n\nThough confirming the postulate in general, Fred Dickinson, Carol Blair and Brian L. Ott criticized Weicks use of the word \"accurate\". Accuracy is hard to achieve, especially if the topic is difficult to qualify, e. g. in researching memory. They suggest replacing the term \"accurate\" with \"interpretive utility\".\n"}
{"id": "47923142", "url": "https://en.wikipedia.org/wiki?curid=47923142", "title": "Topgrading", "text": "Topgrading\n\nTopgrading is a corporate hiring and interviewing methodology that is intended to identify preferred candidates for a particular position. In the methodology, prospective employees undergo a 12-step process that includes extensive interviews, the creation of detailed job scorecards, research into job history, coaching, and more. After being interviewed and reference-checked, job candidates are grouped into one of three categories: A Players, B Players, or C Players. A Players have the most potential for high performance in their role while B and C Players may require more work to be successful. The methodology has been used by major corporations and organizations like General Electric, Lincoln Financial, Honeywell, Barclays, the American Heart Association, and others.\n\nThe term was coined by Bradford D. Smart and his son, Geoffrey, in a 1997 article in \"Directors and Boards\" magazine. The elder Smart had used practices similar to topgrading when he helped set up General Electric's hiring practices in the 1980s and 1990s. His consulting firm practiced and taught topgrading methods. In 1999, Smart also released a book called \"Topgrading: How Leading Companies Win by Hiring, Coaching and Keeping the Best People\" that detailed the topgrading process. The most recent edition of the book was released in 2012 and became a \"New York Times\" and \"Wall Street Journal\" bestseller.\n\nTopgrading is an evaluative method for identifying the most highly qualified candidate for a particular job position. It can be used in both new hires and in the promotion of current employees. The idea behind the method is to identify high-performing \"A Players\" even if the hiring manager has not seen these individuals in action.\n\nTopgrading methodology assumes that the standard interview process can be and is often plagued by dishonesty from job candidates. Resumes are often faked and interviews can be manipulated to make unqualified candidates seem appropriate for the position. To mitigate the potential for dishonesty, one of the first steps in topgrading is to tell candidates that, at the end of the interview process, they must organize their own reference checks from former managers, colleagues, and others. This is known as a Threat of Reference Check (TORC) and is implemented to weed out candidates who may be dishonest or who may lack motivation. The hiring manager can use these reference checks to verify the veracity of any claims made by the candidates. If a candidate fails to arrange these references, the assumption is that they are a \"low-performing\" B or C Player.\n\nAs outlined in Bradford Smart's \"Topgrading\", there are generally 12 steps that companies must take in order to effectively implement topgrading. Some of the most important steps include measuring hiring success before topgrading, using the Threat of Reference Check (TORC) to encourage honesty, creating a detailed job scorecard in lieu of a vague job description, recruiting from established networks, screening with topgrading software, scrutinizing career history via the Topgrading Snapshot (a visual portrayal of the candidate career history including full salary history and estimated performance ratings by all managers), conducting multiple interviews including the tandem chronological Topgrading Interview, asking candidates to organize reference meetings with prior managers and others, coaching new hires, and evaluating the effectiveness of topgrading after making new hires.\n\nThe interview process is an important aspect of the topgrading methodology. The first interview is called \"The Telephone Screening Interview\" and it simply requires an exchange of basic information and ideas to ensure that the candidate is at least fit for and legitimately interested in the position. The next interview step is a series of \"Competency Interviews.\" These interviews ask candidates to provide examples of certain competencies they possess. The hiring manager typically chooses between 6 and 8 competency categories (including Leadership, Organization, “Company Fit,” Peer Relations, and Resourcefulness) that are essential to the company. Different interviewers then ask questions that revolve around two of the categories (often \"Company Fit\" and one other).\n\nThe next interview is referred to as the Topgrading Interview. The hiring manager and another manager act as \"tandem interviewers\" and conduct an up to 4-hour interview for managers (interviews for subordinate positions are much shorter). The interview thoroughly covers every aspect of the candidate's career history. Interviewers ask up to 10 standard questions (plus follow-up questions) about each job during the process. The main questions ask candidates about successes, mistakes, key decisions, key relationship, their manager’s strengths and weaker points, how their manager would rate them, and why they left the job. The theory behind this interview method is that the TORC technique will cause many low-performers with exaggerated resumes to drop out, leaving a pool of theoretically honest and successful candidates. Candidates who are more willing to discuss the entirety of their career provide broader insights to interviewers. Patterns of encountering and solving problems can also reveal a wider variety of competencies. After the interview process, candidates are generally asked to arrange reference calls with their previous bosses.\n\nReference call interviews are the final aspect of the topgrading interview process. Candidates are asked to arrange reference calls with their previous bosses, peers, subordinates, customers, and others to verify the veracity of the candidate's statements during the interview process.\n\nAccording to the methodology, those who are \"chronic\" B or C Players should not be hired. Individuals who effectively pass through the 12-step topgrading process are often considered A Players and should be given consideration. Theoretically, companies that employ the topgrading process could have an employee base composed of 90% A Players, although this is often dependent on hiring managers, individual grading systems, and other factors. According to research by Smart, \"mis-hires\" can ultimately be costly for companies.\n\nA doctoral thesis at Georgia State University examined six different companies who employed topgrading. Between the six companies, a total of around 1,000 new hires were made during the study. Results of the study showed that the average pre-topgrading \"mis-hire\" rate was 69.3%. After the implementation of topgrading, however, the average mis-hire rate dropped to 10.5%.\n"}
