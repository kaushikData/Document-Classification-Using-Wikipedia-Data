{"id": "1419", "url": "https://en.wikipedia.org/wiki?curid=1419", "title": "Adiabatic process", "text": "Adiabatic process\n\nIn thermodynamics, an adiabatic process is one that occurs without transfer of heat or mass of substances between a thermodynamic system and its surroundings. In an adiabatic process, energy is transferred to the surroundings only as work. The adiabatic process provides a rigorous conceptual basis for the theory used to expound the first law of thermodynamics, and as such it is a key concept in thermodynamics.\n\nSome chemical and physical processes occur so rapidly that they may be conveniently described by the term \"adiabatic approximation\", meaning that there is not enough time for the transfer of energy as heat to take place to or from the system.\n\nBy way of example, the adiabatic flame temperature is an idealization that uses the \"adiabatic approximation\" so as to provide an upper limit calculation of temperatures produced by combustion of a fuel. The adiabatic flame temperature is the temperature that would be achieved by a flame if the process of combustion took place in the absence of heat loss to the surroundings.\n\nIn meteorology and oceanography, the adiabatic cooling process produces condensation of moisture or salinity and the parcel becomes oversaturated. Therefore, it is necessary to take away the excess. There the process becomes a \"pseudo-adiabatic process\" in which the liquid water/salt that condenses is assumed to be removed as soon as it is formed, by idealized instantaneous precipitation. The pseudoadiabatic process is only defined for expansion, since a parcel that is compressed become warmer and remains undersaturated.\n\nA process that does not involve the transfer of heat or matter into or out of a system, so that , is called an adiabatic process, and such a system is said to be adiabatically isolated. The assumption that a process is adiabatic is a frequently made simplifying assumption. For example, the compression of a gas within a cylinder of an engine is assumed to occur so rapidly that on the time scale of the compression process, little of the system's energy can be transferred out as heat to the surroundings. Even though the cylinders are not insulated and are quite conductive, that process is idealized to be adiabatic. The same can be said to be true for the expansion process of such a system.\n\nThe assumption of adiabatic isolation of a system is a useful one, and is often combined with others so as to make the calculation of the system's behaviour possible. Such assumptions are idealizations. The behaviour of actual machines deviates from these idealizations, but the assumption of such \"perfect\" behaviour provide a useful first approximation of how the real world works. According to Laplace, when sound travels in a gas, there is no time for heat conduction in the medium and so the propagation of sound is adiabatic. For such an adiabatic process, the modulus of elasticity (Young's modulus) can be expressed as , where is the ratio of specific heats at constant pressure and at constant volume ( ) and is the pressure of the gas .\n\nFor a closed system, one may write the first law of thermodynamics as : , where denotes the change of the system's internal energy, the quantity of energy added to it as heat, and the work done on it by its surroundings.\n\n\nNaturally occurring adiabatic processes are irreversible (entropy is produced).\n\nThe transfer of energy as work into an adiabatically isolated system can be imagined as being of two idealized extreme kinds. In one such kind, there is no entropy produced within the system (no friction, viscous dissipation, etc.), and the work is only pressure-volume work (denoted by ). In nature, this ideal kind occurs only approximately, because it demands an infinitely slow process and no sources of dissipation.\n\nThe other extreme kind of work is isochoric work (), for which energy is added as work solely through friction or viscous dissipation within the system. A stirrer that transfers energy to a viscous fluid of an adiabatically isolated system with rigid walls, without phase change, will cause a rise in temperature of the fluid, but that work is not recoverable. Isochoric work is irreversible. The second law of thermodynamics observes that a natural process, of transfer of energy as work, always consists at least of isochoric work and often both of these extreme kinds of work. Every natural process, adiabatic or not, is irreversible, with , as friction or viscosity are always present to some extent.\n\nThe adiabatic compression of a gas causes a rise in temperature of the gas. Adiabatic expansion against pressure, or a spring, causes a drop in temperature. In contrast, free expansion is an isothermal process for an ideal gas.\n\nAdiabatic heating occurs when the pressure of a gas is increased from work done on it by its surroundings, e.g., a piston compressing a gas contained within a cylinder and raising the temperature where in many practical situations heat conduction through walls can be slow compared with the compression time. This finds practical application in diesel engines which rely on the lack of heat dissipation during the compression stroke to elevate the fuel vapor temperature sufficiently to ignite it.\n\nAdiabatic heating occurs in the Earth's atmosphere when an air mass descends, for example, in a katabatic wind, Foehn wind, or chinook wind flowing downhill over a mountain range. When a parcel of air descends, the pressure on the parcel increases. Due to this increase in pressure, the parcel's volume decreases and its temperature increases as work is done on the parcel of air, thus increasing its internal energy, which manifests itself by a rise in the temperature of that mass of air. The parcel of air can only slowly dissipate the energy by conduction or radiation (heat), and to a first approximation it can be considered adiabatically isolated and the process an adiabatic process.\n\nAdiabatic cooling occurs when the pressure on an adiabatically isolated system is decreased, allowing it to expand, thus causing it to do work on its surroundings. When the pressure applied on a parcel of air is reduced, the air in the parcel is allowed to expand; as the volume increases, the temperature falls as its internal energy decreases. Adiabatic cooling occurs in the Earth's atmosphere with orographic lifting and lee waves, and this can form pileus or lenticular clouds.\n\nAdiabatic cooling does not have to involve a fluid. One technique used to reach very low temperatures (thousandths and even millionths of a degree above absolute zero) is via adiabatic demagnetisation, where the change in magnetic field on a magnetic material is used to provide adiabatic cooling. Also, the contents of an expanding universe can be described (to first order) as an adiabatically cooling fluid. (See heat death of the universe.)\n\nRising magma also undergoes adiabatic cooling before eruption, particularly significant in the case of magmas that rise quickly from great depths such as kimberlites.\n\nSuch temperature changes can be quantified using the ideal gas law, or the hydrostatic equation for atmospheric processes.\n\nIn practice, no process is truly adiabatic. Many processes rely on a large difference in time scales of the process of interest and the rate of heat dissipation across a system boundary, and thus are approximated by using an adiabatic assumption. There is always some heat loss, as no perfect insulators exist.\n\nThe mathematical equation for an ideal gas undergoing a reversible (i.e., no entropy generation) adiabatic process can be represented by the polytropic process equation \nwhere is pressure, is volume, and for this case , where \n\nFor a monatomic ideal gas, , and for a diatomic gas (such as nitrogen and oxygen, the main components of air) . Note that the above formula is only applicable to classical ideal gases and not Bose–Einstein or Fermi gases.\n\nFor reversible adiabatic processes, it is also true that\n\nwhere \"T\" is an absolute temperature. This can also be written as\n\nThe compression stroke in a gasoline engine can be used as an example of adiabatic compression. The model assumptions are: the uncompressed volume of the cylinder is one litre (1 L = 1000 cm = 0.001 m); the gas within is the air consisting of molecular nitrogen and oxygen only (thus a diatomic gas with 5 degrees of freedom, and so ); the compression ratio of the engine is 10:1 (that is, the 1 L volume of uncompressed gas is reduced to 0.1 L by the piston); and the uncompressed gas is at approximately room temperature and pressure (a warm room temperature of ~27 °C, or 300 K, and a pressure of 1 bar = 100 kPa, i.e. typical sea-level atmospheric pressure).\n\nso our adiabatic constant for this example is about 6.31 Pa m.\n\nThe gas is now compressed to a 0.1 L (0.0001 m) volume (we will assume this happens quickly enough that no heat can enter or leave the gas through the walls). The adiabatic constant remains the same, but with the resulting pressure unknown\n\nso solving for \"P\":\n\nor 25.1 bar. Note that this pressure increase is more than a simple 10:1 compression ratio would indicate; this is because the gas is not only compressed, but the work done to compress the gas also increases its internal energy, which manifests itself by a rise in the gas temperature and an additional rise in pressure above what would result from a simplistic calculation of 10 times the original pressure.\n\nWe can solve for the temperature of the compressed gas in the engine cylinder as well, using the ideal gas law, \"PV\" = \"nRT\" (\"n\" is amount of gas in moles and \"R\" the gas constant for that gas). Our initial conditions being 100 kPa of pressure, 1 L volume, and 300 K of temperature, our experimental constant (\"nR\") is:\n\nWe know the compressed gas has  = 0.1 L and  = , so we can solve for temperature:\n\nThat is a final temperature of 753 K, or 479 °C, or 896 °F, well above the ignition point of many fuels. This is why a high-compression engine requires fuels specially formulated to not self-ignite (which would cause engine knocking when operated under these conditions of temperature and pressure), or that a supercharger with an intercooler to provide a pressure boost but with a lower temperature rise would be advantageous. A diesel engine operates under even more extreme conditions, with compression ratios of 20:1 or more being typical, in order to provide a very high gas temperature, which ensures immediate ignition of the injected fuel.\n\nFor an adiabatic free expansion of an ideal gas, the gas is contained in an insulated container and then allowed to expand in a vacuum. Because there is no external pressure for the gas to expand against, the work done by or on the system is zero. Since this process does not involve any heat transfer or work, the first law of thermodynamics then implies that the net internal energy change of the system is zero. For an ideal gas, the temperature remains constant because the internal energy only depends on temperature in that case. Since at constant temperature, the entropy is proportional to the volume, the entropy increases in this case, therefore this process is irreversible.\n\nThe definition of an adiabatic process is that heat transfer to the system is zero, . Then, according to the first law of thermodynamics,\n\nwhere is the change in the internal energy of the system and is work done \"by\" the system. Any work () done must be done at the expense of internal energy , since no heat is being supplied from the surroundings. Pressure–volume work done \"by\" the system is defined as\n\nHowever, does not remain constant during an adiabatic process but instead changes along with .\n\nIt is desired to know how the values of and relate to each other as the adiabatic process proceeds. For an ideal gas the internal energy is given by\n\nwhere is the number of degrees of freedom divided by two, is the universal gas constant and is the number of moles in the system (a constant).\n\nDifferentiating equation (3) and use of the ideal gas law, , yields\n\nEquation (4) is often expressed as because .\n\nNow substitute equations (2) and (4) into equation (1) to obtain\n\nfactorize :\n\nand divide both sides by :\n\nAfter integrating the left and right sides from to and from to and changing the sides respectively,\n\nExponentiate both sides, substitute with , the heat capacity ratio\n\nand eliminate the negative sign to obtain\n\nTherefore,\n\nand\n\nSubstituting the ideal gas law into the above, we obtain\n\nwhich simplifies to\n\nThe change in internal energy of a system, measured from state 1 to state 2, is equal to\n\nAt the same time, the work done by the pressure–volume changes as a result from this process, is equal to\n\nSince we require the process to be adiabatic, the following equation needs to be true\n\nBy the previous derivation,\n\nRearranging (4) gives\n\nSubstituting this into (2) gives\n\nIntegrating,\n\nSubstituting ,\n\nRearranging,\n\nUsing the ideal gas law and assuming a constant molar quantity (as often happens in practical cases),\n\nBy the continuous formula,\n\nor\n\nSubstituting into the previous expression for ,\n\nSubstituting this expression and (1) in (3) gives\n\nSimplifying,\n\nAn adiabat is a curve of constant entropy in a diagram. Some properties of adiabats on a \"P\"–\"V\" diagram are indicated. These properties may be read from the classical behaviour of ideal gases, except in the region where \"PV\" becomes small (low temperature), where quantum effects become important.\n\n\nThe following diagram is a \"P\"–\"V\" diagram with a superposition of adiabats and isotherms:\n\nThe isotherms are the red curves and the adiabats are the black curves.\n\nThe adiabats are isentropic.\n\nVolume is the horizontal axis and pressure is the vertical axis.\n\nThe term \"adiabatic\" () is an anglicization of the Greek term ἀδιάβατος \"impassable\" (used by Xenophon of rivers).\nIt is used in the thermodynamic sense by Rankine (1866), and adopted by Maxwell in 1871 (explicitly attributing the term to Rankine). \nThe etymological origin corresponds here to an impossibility of transfer of energy as heat and of transfer of matter across the wall. \n\nThe Greek word ἀδιάβατος is formed from privative ἀ- (\"not\") and διαβατός, \"passable\", in turn deriving from διά (\"through\"), and βαῖνειν (\"to walk, go, come\").\n\nThe adiabatic process has been important for thermodynamics since its early days. It was important in the work of Joule, because it provided a way of nearly directly relating quantities of heat and work.\n\nFor a thermodynamic system that is enclosed by walls that do not allow mass transfer, energy can pass in and out only as heat or work. Thus a quantity of work can be related almost directly to an equivalent quantity of heat in a cycle of two limbs. The first is an isochoric adiabatic work process that adds to the system's internal energy. Then an isochoric and workless heat transfer returns the system to its original state. The first limb adds a definite amount of energy and the second removes it. Accordingly, Rankine measured quantity of heat in units of work, rather than as a calorimetric quantity . In 1854, Rankine used a quantity that he called \"the thermodynamic function\" that later was called entropy, and at that time he wrote also of the \"curve of no transmission of heat\", which he later called an adiabatic curve. Besides it two isothermal limbs, Carnot's cycle has two adiabatic limbs.\n\nFor the foundations of thermodynamics, the conceptual importance of this was emphasized by Bryan, by Carathéodory, and by Born. The reason is that calorimetry presupposes a type of temperature as already defined before the statement of the first law of thermodynamics, such as one based on empirical scales. Such a presupposition involves making the distinction between empirical temperature and absolute temperature. Rather, the definition of absolute thermodynamic temperature is best left till the second law is available as a conceptual basis.\n\nIn the eighteenth century, the law of conservation of energy was yet to be fully formulated or established, and the nature of heat was debated. One approach to these problems was to regard heat, measured by calorimetry, as a primary substance that is conserved in quantity. By the middle of the nineteenth century, it was recognized as a form of energy, and the law of conservation of energy was thereby also recognized. The view that eventually established itself, and is currently regarded as right, is that the law of conservation of energy is a primary axiom, and that heat is to be analyzed as consequential. In this light, heat cannot be a component of the total energy of a single body because it is not a state variable, but, rather, is a variable that describes a process of transfer between two bodies. The adiabatic process is important because it is a logical ingredient of this current view.\n\nThis present article is written from the viewpoint of macroscopic thermodynamics, and the word \"adiabatic\" is used in this article in the traditional way of thermodynamics, introduced by Rankine. It is pointed out in the present article that, for example, if a compression of a gas is rapid, then there is little time for heat transfer to occur, even when the gas is not adiabatically isolated by a definite wall. In this sense, a rapid compression of a gas is sometimes approximately or loosely said to be \"adiabatic\", though often far from isentropic, even when the gas is not adiabatically isolated by a definite wall.\n\nQuantum mechanics and quantum statistical mechanics, however, use the word \"adiabatic\" in a very different sense, one that can at times seem almost opposite to the classical thermodynamic sense. In quantum theory, the word \"adiabatic\" can mean something perhaps near isentropic, or perhaps near quasi-static, but the usage of the word is very different between the two disciplines.\n\nOn the one hand, in quantum theory, if a perturbative element of compressive work is done almost infinitely slowly (that is to say quasi-statically), it is said to have been done \"adiabatically\". The idea is that the shapes of the eigenfunctions change slowly and continuously, so that no quantum jump is triggered, and the change is virtually reversible. While the occupation numbers are unchanged, nevertheless there is change in the energy levels of one-to-one corresponding, pre- and post-compression, eigenstates. Thus a perturbative element of work has been done without heat transfer and without introduction of random change within the system. For example, Max Born writes \"Actually, it is usually the 'adiabatic' case with which we have to do: i.e. the limiting case where the external force (or the reaction of the parts of the system on each other) acts very slowly. In this case, to a very high approximation\n\nthat is, there is no probability for a transition, and the system is in the initial state after cessation of the perturbation. Such a slow perturbation is therefore reversible, as it is classically.\"\n\nOn the other hand, in quantum theory, if a perturbative element of compressive work is done rapidly, it randomly changes the occupation numbers of the eigenstates, as well as changing their shapes. In that theory, such a rapid change is said not to be \"adiabatic\", and the contrary word \"diabatic\" is applied to it. One might guess that perhaps Clausius, if he were confronted with this, in the now-obsolete language he used in his day, would have said that \"internal work\" was done and that 'heat was generated though not transferred'.\n\nIn classical thermodynamics, such a rapid change would still be called adiabatic because the system is adiabatically isolated, and there is no transfer of energy as heat. The strong irreversibility of the change, due to viscosity or other entropy production, does not impinge on this classical usage.\n\nThus for a mass of gas, in macroscopic thermodynamics, words are so used that a compression is sometimes loosely or approximately said to be adiabatic if it is rapid enough to avoid heat transfer, even if the system is not adiabatically isolated. But in quantum statistical theory, a compression is not called adiabatic if it is rapid, even if the system is adiabatically isolated in the classical thermodynamic sense of the term. The words are used differently in the two disciplines, as stated just above.\n\n\n\n"}
{"id": "8391923", "url": "https://en.wikipedia.org/wiki?curid=8391923", "title": "Andamooka Opal", "text": "Andamooka Opal\n\nThe Andamooka Opal is a famous opal which was presented to Queen Elizabeth II in 1954 on the occasion of her first visit to South Australia. It was discovered in Andamooka, South Australia, an historic opal mining town.\n\nThe opal was cut and polished by John Altmann to a weight of . It displays a magnificent array of reds, blues, and greens and was set with diamonds into an 18 karat (75%) palladium necklet. \n"}
{"id": "9580173", "url": "https://en.wikipedia.org/wiki?curid=9580173", "title": "Antoni Jakubski", "text": "Antoni Jakubski\n\nAntoni Władysław Jakubski (; 1885–1962) was a Polish zoologist and explorer.\n\nJakubski was born in Lemberg (Lwów), Galicia, Austria-Hungary (now Lviv, Ukraine) on 28 March 1885. He studied zoology from Prof. Józef Nusbaum-Hilarowicz at the Lwów University where he received a habilitation in 1917. In 1909-1910, he traveled to East Africa, becoming, on 13 March 1910, the first Pole to climb Mount Kilimanjaro. He crossed Tanganyika on foot, traveling from the Indian Ocean to the lakes Nyasa and Rukwa in order to study their fauna.\n\nDuring the First World War, Jakubski fought in the Polish Legions. For his military service, he was awarded with a fifth class Virtuti Militari order and a Cross of the Valiant. From 1919 to 1939, he worked at the Poznań University. In 1923, he set up the Maritime Fishing Laboratory at Hel on the Baltic Sea. After the Second World War, during which he was an inmate of Nazi concentration camps, Jakubski settled in the United Kingdom where he was employed in the British Museum. He died in London on 20 May 1962.\n\nJakubski's area of research comprised faunistics, zoogeography, comparative anatomy and history of zoology. His works include: \n"}
{"id": "238680", "url": "https://en.wikipedia.org/wiki?curid=238680", "title": "Avogadro's law", "text": "Avogadro's law\n\nAvogadro's law (sometimes referred to as Avogadro's hypothesis or Avogadro's principle) is an experimental gas law relating the volume of a gas to the amount of substance of gas present. The law is a specific case of the ideal gas law. A modern statement is:\nAvogadro's law states that, \"equal volumes of all gases, at the same temperature and pressure, have the same number of molecules.\"\n\nFor a given mass of an ideal gas, the volume and amount (moles) of the gas are directly proportional if the temperature and pressure are constant.\nThe law is named after Amedeo Avogadro who, in 1811, hypothesized that two given samples of an ideal gas, of the same volume and at the same temperature and pressure, contain the same number of molecules. As an example, equal volumes of molecular hydrogen and nitrogen contain the same number of molecules when they are at the same temperature and pressure, and observe ideal gas behavior. In practice, real gases show small deviations from the ideal behavior and the law holds only approximately, but is still a useful approximation for scientists.\n\nThe law can be written as:\n\nor\n\nwhere\n\nThis law describes how, under the same condition of temperature and pressure, equal volumes of all gases contain the same number of molecules. For comparing the same substance under two different sets of conditions, the law can be usefully expressed as follows:\n\nThe equation shows that, as the number of moles of gas increases, the volume of the gas also increases in proportion. Similarly, if the number of moles of gas is decreased, then the volume also decreases. Thus, the number of molecules or atoms in a specific volume of ideal gas is independent of their size or the molar mass of the gas.\n\nThe derivation of Avogadro's law follows directly from the ideal gas law, i.e.\nwhere \"R\" is the gas constant, \"T\" is the Kelvin temperature, and \"P\" is the pressure (in pascals). \n\nSolving for \"V/n\", we thus obtain\nComparing we have\nwhich is a constant for a fixed pressure and a fixed temperature.\n\nAn equivalent formulation of the ideal gas law can be written using Boltzmann constant \"k\", as\nwhere \"N\" is the number of particles in the gas, and the ratio of \"R\" over \"k\" is equal to Avogadro constant.\n\nIn this form, for \"V/N\" is a constant, we have\nIf \"T\" and \"P\" are taken at standard conditions for temperature and pressure (STP), then \"k\"'=1/\"n\", where \"n\" is Loschmidt constant.\n\nAvogadro's hypothesis (as it was known originally) was formulated in the same spirit of earlier empirical gas laws like Boyle's law (1662), Charles's law (1787) and Gay-Lussac's law (1808). The hypothesis was first published by Amadeo Avogadro in 1811, and reconciled Dalton atomic theory with the \"incompatible\" idea of Joseph Louis Gay-Lussac that some gases were composite of different fundamental substances (molecules) in integer proportions. In 1814, independently from Avogadro, André-Marie Ampère published the same law with similar conclusions. As Ampère was more well-known in France, the hypothesis was usually referred there as Ampère's hypothesis, and later also as Avogadro-Ampère hypothesis or even Ampère-Avogadro hypothesis.\n\nExperimental studies carried out by Charles Frédéric Gerhardt and Auguste Laurent on organic chemistry demonstrated that Avogadro's law explained why the same quantities of molecules in a gas have the same volume. Nevertheless, related experiments with some inorganic substances showed seeming exceptions to the law. This apparent contradiction was finally resolved by Stanislao Cannizzaro, as announced at Karlsruhe Congress in 1860, four years after Avogadro's death. He explained that these exceptions were due to molecular dissociations at certain temperatures, and that Avogadro's law determined not only molecular masses, but atomic masses as well. \n\nThe combined gas law (the combination of Boyle, Charles and Gay-Lussac laws), together with Avogadro's law, were combined by Émile Clapeyron in 1834, giving rise to the ideal gas law. At the end of the 19th century, later developments from scientists like August Krönig, Rudolf Clausius, James Clerk Maxwell and Ludwig Boltzmann, gave rise to the kinetic theory of gases, a microscopic theory from which the ideal gas law can be derived as an statistical result from the movement of atoms/molecules in a gas.\n\nAvogadro's law provides a way to calculate the quantity of gas in a receptacle. Thanks to this discovery, Johann Josef Loschmidt, in 1865, was able for the first time to estimate the size of a molecule. His calculation gave rise to the concept of the Loschmidt constant, a ratio between macroscopic and atomic quantities. In 1910, Millikan's oil drop experiment, determines the charge of the electron, together with Faraday constant (derived by Michael Faraday in 1834), one is able to determine the number of particles in a mole of substance. At the same time, precision experiments by Jean Baptiste Perrin, lead to the definition of Avogadro number, to refer to the number of molecules in one gram-molecule of oxygen. Perrin named the number to honor Avogadro, for his discovery of the namesake law. Later standardization of the International System of Units, lead to the modern definition of the Avogadro constant.\n\nTaking STP to be 101.325 kPa and 273.15 K, we can find the volume of one mole of gas:\n\nFor 100.00 kPa and 273.15 K, the molar volume of an ideal gas is 22.712 dmmol.\n\n"}
{"id": "28928589", "url": "https://en.wikipedia.org/wiki?curid=28928589", "title": "Beakley Glacier", "text": "Beakley Glacier\n\nBeakley Glacier () is a glacier on the west side of the Duncan Peninsula on Carney Island, flowing north into the Amundsen Sea. It was delineated by the United States Geological Survey from aerial photos taken by U.S. Navy Operation Highjump in January 1947, and named by the Advisory Committee on Antarctic Names for Vice Admiral W.M. Beakley, U.S. Navy, Deputy Chief of Naval Operations for Ship Operations and Readiness during the IGY period, 1957–58.\n\n"}
{"id": "175638", "url": "https://en.wikipedia.org/wiki?curid=175638", "title": "Binary phase", "text": "Binary phase\n\nIn materials chemistry, a binary phase is chemical compound containing two different elements. Some binary phases compounds are molecular, e.g. carbon tetrachloride (CCl). More typically binary phase refers to extended solids. Famous examples are the two polymorphs of zinc sulfide.\n\nPhases with higher degrees of complexity feature more elements, e.g. three elements in ternary phases, four elements in quaternary phases, \n"}
{"id": "174396", "url": "https://en.wikipedia.org/wiki?curid=174396", "title": "Bohr radius", "text": "Bohr radius\n\nThe Bohr radius (\"a\" or \"r\") is a physical constant, approximately equal to the most probable distance between the nucleus and the electron in a hydrogen atom in its ground state. It is named after Niels Bohr, due to its role in the Bohr model of an atom. Its value is .\n\nIn SI units the Bohr radius is:\n\nwhere:\n\nIn Gaussian units the Bohr radius is simply\n\nAccording to 2014 CODATA the Bohr radius has a value of (considering mass of electron as the rest mass of an electron) (i.e., approximately 53 pm or 0.53 Å).\n\nIn the Bohr model of the structure of an atom, put forward by Niels Bohr in 1913, electrons orbit a central nucleus. The model says that the electrons orbit only at certain distances from the nucleus, depending on their energy. In the simplest atom, hydrogen, a single electron orbits the nucleus and its smallest possible orbit, with lowest energy, has an orbital radius almost equal to the Bohr radius. (It is not \"exactly\" the Bohr radius due to the reduced mass effect. They differ by about 0.1%.)\n\nAlthough the Bohr model is no longer in use, the Bohr radius remains very useful in atomic physics calculations, due in part to its simple relationship with other fundamental constants. (This is why it is defined using the true electron mass rather than the reduced mass, as mentioned above.) For example, it is the unit of length in atomic units.\n\nAn important distinction is that the Bohr radius gives the position of maximum probability density, not its expected radial distance. The expected radial distance is actually 1.5 times the Bohr radius, as a result of the long tail of the radial wave function.\n\nThe Bohr radius of the electron is one of a trio of related units of length, the other two being the Compton wavelength of the electron formula_10 and the classical electron radius formula_11. The Bohr radius is built from the electron mass formula_12, Planck's constant formula_13 and the electron charge formula_14. The Compton wavelength is built from formula_15, formula_13 and the speed of light formula_17. The classical electron radius is built from formula_15, formula_17 and formula_14. Any one of these three lengths can be written in terms of any other using the fine structure constant formula_21:\n\nThe Compton wavelength is about 20 times smaller than the Bohr radius, and the classical electron radius is about 1000 times smaller than the Compton wavelength.\n\nThe Bohr radius including the effect of reduced mass in the hydrogen atom can be given by the following equation:\n\nwhere:\n\nIn the above equation, the effect of the reduced mass is achieved by using the increased Compton wavelength, which is just the Compton wavelengths of the electron and the proton added together.\n\n\n"}
{"id": "29332856", "url": "https://en.wikipedia.org/wiki?curid=29332856", "title": "Bombardier Glacier", "text": "Bombardier Glacier\n\nBombardier Glacier () is a glacier draining southeast from the edge of Detroit Plateau on Nordenskjöld Coast in Graham Land, Antarctica and through a deep trough between Darzalas Peak and Trave Peak to join Edgeworth Glacier and flow into Mundraga Bay. It was mapped from surveys by the Falkland Islands Dependencies Survey (1960–1961), and named by the UK Antarctic Place-Names Committee for Joseph-Armand Bombardier, the Canadian engineer who developed the snowmobile from 1926 to 1937, one of the earliest successful self-propelled over-snow vehicles.\n"}
{"id": "47289377", "url": "https://en.wikipedia.org/wiki?curid=47289377", "title": "Breakthrough Initiatives", "text": "Breakthrough Initiatives\n\nBreakthrough Initiatives is a science-based program founded in 2015 and funded by Yuri Milner to search for extraterrestrial intelligence over a span of at least 10 years. The program is divided into multiple projects. Breakthrough Listen will comprise an effort to search over 1,000,000 stars for artificial radio or laser signals. A parallel project called Breakthrough Message is an effort to create a message \"representative of humanity and planet Earth\". The project Breakthrough Starshot aims to send a swarm of probes to the nearest star at about 20% the speed of light. The project Breakthrough Watch aims to identify and characterize Earth-sized, rocky planets around Alpha Centauri and other stars within 20 light years of Earth.\n\nThe Breakthrough Initiatives were announced to the public on July 20, 2015, at London's Royal Society.\nPhysicist Stephen Hawking, Russian tycoon Yuri Milner, and others created the \"Initiatives\" to search for intelligent extraterrestrial life in the Universe and consider a plan for possibly transmitting messages out into space.\nThe announcement included an open letter co-signed by multiple scientists, including Hawking, expressing support for an intensified search for alien radio communications. During the public launch, Hawking said: \"In an infinite Universe, there must be other life. There is no bigger question. It is time to commit to finding the answer.\"\n\nThe cash infusion is projected to mark up the pace of SETI research over the early 2000s rate, and will nearly double the rate NASA was spending on SETI research annually in approximately 1973–1993.\n\nBreakthrough Listen is a program to search for intelligent extraterrestrial communications in the Universe. With $100 million in funding and thousands of hours of dedicated telescope time on state-of-the-art facilities, it is the most comprehensive search for alien communications to date. The project began in January 2016, and is expected to continue for 10 years.\n\nThe project uses radio wave observations from the Green Bank Observatory and the Parkes Observatory, and visible light observations from the Automated Planet Finder. Targets for the project include one million nearby stars and the centers of 100 galaxies. All data generated from the project are available to the public, and SETI@Home is used for some of the data analysis. The first results were published in April 2017, with further updates expected every 6 months.\n\nThe \"Breakthrough Message\" program is to study the ethics of sending messages into deep space. It also launched an open competition with a US$1 million prize pool, to design a digital message that could be transmitted from Earth to an extraterrestrial civilization. The message should be \"representative of humanity and planet Earth\". The program pledges \"not to transmit any message until there has been a global debate at high levels of science and politics on the risks and rewards of contacting advanced civilizations\".\n\nBreakthrough Starshot, announced April 12, 2016, is a US $100 million program to develop a proof-of-concept light sail spacecraft fleet capable of making the journey to Alpha Centauri at 20% the speed of light (60,000 km/s or 215 million km/h) taking about 20 years to get there, and about 4 years to notify Earth of a successful arrival.\n\nThe interstellar journey may include a flyby of \"Proxima Centauri b\", an Earth-sized exoplanet that is in the habitable zone of its host star in the Alpha Centauri system. From a distance of 1 Astronomical Unit (150 million kilometers or 93 million miles), the four cameras on each of the spacecraft could potentially capture an image of high enough quality to resolve surface features. The spacecraft fleet would have 1000 craft, and each craft, named \"StarChip\", would be a very small centimeter-sized craft weighing several grams. They would be propelled by several ground-based lasers of up to 100 gigawatts. Each tiny spacecraft would transmit data back to Earth using a compact on-board laser communications system. Pete Worden is the head of this project. The conceptual principles to enable this interstellar travel project were described in \"A Roadmap to Interstellar Flight\", by Philip Lubin of UC Santa Barbara. METI president Douglas Vakoch summarized the significance of the project, saying that \"by sending hundreds or thousands of space probes the size of postage stamps, Breakthrough Starshot gets around the hazards of spaceflight that could easily end a mission relying on a single spacecraft. Only one nanocraft needs to make its way to Alpha Centauri and send back a signal for the mission to be successful. When that happens, Starshot will make history.\"\n\nIn July 2017, scientists announced that precursors to \"StarChip\", named \"Sprites\", were successfully launched and flown.\n\nBreakthrough Watch is a multimillion-dollar astronomical program to develop Earth- and space-based technologies that can find Earth-like planets in our cosmic neighborhood – and try to establish whether they host life. The project aims to identify and characterize Earth-sized, rocky planets around Alpha Centauri and other stars within 20 light years of Earth, in search of oxygen and other \"biosignatures.\" \n\n"}
{"id": "40858", "url": "https://en.wikipedia.org/wiki?curid=40858", "title": "Caesium standard", "text": "Caesium standard\n\nThe caesium standard is a primary frequency standard in which electronic transitions between the two hyperfine ground states of caesium-133 atoms are used to control the output frequency. The first caesium clock was built by Louis Essen in 1955 at the National Physical Laboratory in the UK. and promoted worldwide by Gernot M. R. Winkler of the USNO.\n\nCaesium atomic clocks are the most accurate time and frequency standards, and serve as the primary standard for the definition of the second in the International System of Units (SI) (the metric system). By definition, radiation produced by the transition between the two hyperfine ground states of caesium (in the absence of external influences such as the Earth's magnetic field) has a frequency, , of exactly 9,192,631,770 Hz. That value was chosen so that the caesium second equalled, to the limit of human measuring ability in 1960 when it was adopted, the existing standard ephemeris second based on the Earth's orbit around the Sun. Because no other measurement involving time had been as precise, the effect of the change was less than the experimental uncertainty of all existing measurements.\n\nThe official definition of the second given by the BIPM at the 13th General Conference on Weights and Measures in 1967 is: ``\"The second is the duration of 9 192 631 770 periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium 133 atom.\"<nowiki>\"</nowiki> At its 1997 meeting the BIPM added to the previous definition the following specification: ``\"This definition refers to a caesium atom at rest at a temperature of 0 K.\"<nowiki>\"</nowiki>\n\nThe meaning of the preceding definition is as follows. The caesium atom has a ground state electron state with configuration [Xe] 6s and, consequently, atomic term symbol S. This means that there is one unpaired electron and the total electron spin of the atom is 1/2. Moreover, the nucleus of caesium-133 has a nuclear spin equal to 7/2. The simultaneous presence of electron spin and nuclear spin leads, by a mechanism called hyperfine interaction, to a (small) splitting of all energy levels into two sub-levels. One of the sub-levels corresponds to the electron and nuclear spin being parallel (i.e., pointing in the same direction), leading to a total spin \"F\" equal to \"F\"=7/2+1/2 =4; the other sub-level corresponds to anti-parallel electron and nuclear spin (i.e., pointing in opposite directions), leading to a total spin \"F\"=7/2-1/2=3. In the caesium atom it so happens that the sub-level lowest in energy is the one with F=3, while the F=4 sub-level lies energetically slightly above. When the atom is irradiated with electromagnetic radiation having an energy corresponding to the energetic difference between the two sub-levels the radiation is absorbed and the atom is excited, going from the \"F\"=3 sub-level to the \"F\"=4 one. After a small fraction of a second the atom will re-emit the radiation and return to its \"F\"=3 ground state. From the definition of the second it follows that the radiation in question has a frequency of exactly 9.19263177 GHz, corresponding to a wavelength of about 3.26 cm and therefore belonging to the microwave range.\n\n"}
{"id": "18665993", "url": "https://en.wikipedia.org/wiki?curid=18665993", "title": "Corrosion engineering", "text": "Corrosion engineering\n\nCorrosion Engineering is the specialist discipline of applying scientific knowledge, natural laws and physical resources in order to design and implement materials, structures, devices, systems and procedures to manage the natural phenomenon known as corrosion. Generally related to Metallurgy or Materials Science, Corrosion Engineering also relates to non-metallics including ceramics, cement, and conductive materials such as carbon / graphite. Corrosion Engineers often manage other not-strictly-corrosion processes including (but not restricted to) cracking, brittle fracture, crazing, fretting, erosion, and more typically categorized as asset management. In the 1990s, Imperial College London even offered a Master of Science degree entitled \"The Corrosion of Engineering Materials\" . \n\nIn the year 1995, it was reported that the costs nationwide in the U.S of corrosion were nearly $300 billion per year. [4]\n\nCorrosion engineering groups have formed around the world in order to prevent, slow and manage the effects of corrosion. Examples of such groups are the National Association of Corrosion Engineers (NACE) and the European Federation of Corrosion (EFC), see Corrosion societies. The corrosion engineers main task is to economically and safely manage the effects of corrosion on materials. Corrosion Engineering master's degree courses are available worldwide and are concerned with the control and understanding of corrosion.\n\nZaki Ahmad in his book \"Principles of corrosion engineering and corrosion control\"(10)states that \"Corrosion engineering is the application of the principles evolved from corrosion science to minimize or prevent corrosion. Corrosion engineering involves designing of corrosion prevention schemes and implementation of specific codes and practices. Corrosion prevention measures, like cathodic protection, designing to prevent corrosion and coating of structures fall within the regime of corrosion engineering. However, corrosion science and engineering go hand-in-hand and they cannot be separated: it is a permanent marriage to produce new and better methods of protection from time to time\". In the \"Handbook of corrosion engineering\" (4) the author Pierre R. Roberge states \"Corrosion is the destructive attack of a material by reaction with its environment. The serious consequences of the corrosion process have become a problem of worldwide significance\".\n\nMost notable contributors to Corrosion Engineering education have been:\n\n•Michael Faraday (1791–1867)\n\n•Marcel Pourbaix (1904–1998)\n\n•Melvin Romanoff\n\n•Pierre R. Roberge\n\n•Mars G. Fontana (1910–1988)\n\n•Dr. Herbert H. Uhlig (1907–1993)\n\n•Ulick Richardson Evans (1889–1980)\n\nCorrosion engineers and consultants tend to specialize in Internal or External corrosion scenarios. In both, they may provide corrosion control recommendations, failure analysis investigations, sell corrosion control products, or provide installation or design of corrosion control and monitoring systems. Every material has its weakness. Aluminum, galvanized/zinc coatings, brass, and copper do not survive well in very alkaline or very acidic pH environments. Copper and brasses do not survive well in high nitrate or ammonia environments. Carbon steels and iron do not survive well in low soil resistivity and high chloride environments. High chloride environments can even overcome and attack steel encased in normally protective concrete. Concrete does not survive well in high sulfate environments. And nothing survives well in high sulfide and low redox potential environments with corrosive bacteria.\n\nUnderground corrosion control engineers will collect soil samples to test soil chemistry for corrosive factors such as pH, minimum soil resistivity, chlorides, sulfates, ammonia, nitrates, sulfide, and redox potential. The soil samples are collected from the depth from which the infrastructure will be installed because soil properties can change from strata to strata. The minimum test of in-situ soil resistivity is measured using the Wenner 4 pin method if often performed to judge a site's corrosivity, but if the test is performed during a dry period, the soil's actual corrosivity may not be properly reported since underground condensation can occur on buried metals leaving the soil touching the metal surfaces in a more moist status. This is why measuring a soil's minimum or saturated resistivity is so important. Soil resistivity testing alone will also not identify corrosive elements. Corrosion engineers can investigate locations experiencing active corrosion using above ground survey methods and design corrosion control systems such as cathodic protection to stop or reduce the rate of corrosion.\n\nGeotechnical engineers typically do not practice corrosion engineering and will refer their clients to a corrosion engineer if the soil resistivity is measured to be below 3,000 ohm-cm or less depending which soil corrosivity categorization table they are reading. Unfortunately, an old dairy farm can have soil resistivities above 3,000 ohm-cm and still contain corrosive ammonia and nitrate levels which will lead to corrosion of copper piping or grounding rods. A general saying about corrosion is, \"If the soil is great for farming, it is great for corrosion!\"\n\nUnderwater corrosion engineers apply the same principals used in underground corrosion control but will use specially trained and certified scuba divers for condition assessment, and corrosion control system installation and commissioning. The main difference being in the type of reference cells used to collect voltage readings.\n\nAtmospheric corrosion is typically handled by use of materials selection and coatings specifications. The use of zinc coatings also known as galvanizing on steel structures is a form of cathodic protection in which small scratches are expected to occur in the coating over time. As long as the scratches are fine, condensation moisture should not corrode the underlying steel as long as both the zinc and steel are in contact with the moisture, but if the scratch or uncoated area is larger than the droplets, then corrosion can occur. As long as there is moisture, the zinc will corrode and eventually disappear.\n\nA significant amount of corrosion of fences is due to landscaper tools scratching fence coatings and irrigation sprinklers spraying these damaged fences. Recycled water typically has a higher salt content than potable drinking water, meaning that it is more corrosive than regular tap water. The same risk from damage and water spray exists for above ground piping and backflow preventers. Fiberglass covers, cages, and concrete footings have worked well to keep tools at an arm’s length. Even the location where your roof drain splashes down can matter. Drainage from a home’s roof valley can fall directly down onto a gas meter causing its piping to corrode at an accelerated rate reaching 50% wall thickness within 4 years. It is the same effect as a splash zone in the ocean or in a pool which has a lot of oxygen and agitation that can remove material as it corrodes.\n\nTanks or structural tubing such as bench seat supports or amusement park rides can accumulate water and moisture if the structure does not allow for drainage. This humid environment can then lead to internal corrosion of the structure affecting the structural integrity. The same can happen in tropical environments leading to external corrosion.\n\nThe same principals of external corrosion control can be applied to internal corrosion but due to accessibility, the approaches can be different. Thus special instruments for internal corrosion control and inspection are used that are not used in external corrosion control. Video scoping of pipes and high tech smart pigs are used for internal inspections. The smart pigs can be inserted into a pipe system at one point and \"caught\" far down the line. The use of corrosion inhibitors, material selection, and internal coatings are mainly used to control corrosion in piping while anodes along with coatings are used to control corrosion in tanks.\n\nInternal corrosion challenges apply to the following:\n\n- Water pipe corrosion\n\n- Gas pipe corrosion\n\n- Oil pipe corrosion\n\n- Water tank reservoir corrosion\n\n(1) Corrosion for students of science and engineering Tretheway K R & Chamberlain J\n\n(2) Corrosion Vols 1 and 2 Metal Environment Reactions, Edited by L. L. Shrier Pub Butterworth-Heinemann Ltd\n\n(3) Corrosion engineering Mars Guy Fontana McGraw-Hill, 1986\n\n(4) Handbook of corrosion engineering By Pierre R. Roberge\n\n(5) Corrosion engineering: principles and practice Pierre R. Roberge McGraw-Hill Prof Med/Tech, 2008\n\n(11) Corrosion Engineering by Volkan Cicek (Author) Publisher: Wiley-Scrivener; 1 edition (2 April 2014) Inc.ASIN: B00JJUL8LC\n\n(12) Corrosion Engineering: Principles and Practice [Kindle Edition] Pierre Roberge (Author) Publisher: McGraw-Hill Professional; 1 edition (25 March 2008) ASIN: B0018G4HEK\n\n"}
{"id": "8557676", "url": "https://en.wikipedia.org/wiki?curid=8557676", "title": "Demersal zone", "text": "Demersal zone\n\nThe demersal zone is the part of the sea or ocean (or deep lake) consisting of the part of the water column near to (and significantly affected by) the seabed and the benthos. The demersal zone is just above the benthic zone and forms a layer of the larger profundal zone.\n\nBeing just above the ocean floor, the demersal zone is variable in depth and can be part of the photic zone where light can penetrate and photosynthetic organisms grow, or the aphotic zone, which begins between depths of roughly and extends to the ocean depths, where no light penetrates.\n\nThe distinction between demersal species of fish and pelagic species is not always clear cut. The Atlantic cod (\"Gadus morhua\") is a typical demersal fish, but can also be found in the open water column, and the Atlantic herring (\"Clupea harengus\") is predominantly a pelagic species but forms large aggregations near the seabed when it spawns on banks of gravel.\n\nTwo types of fish inhabit the demersal zone, those that are heavier than water and rest on the seabed, and those that have neutral buoyancy and remain just above the substrate. In many species of fish, neutral buoyancy is maintained by a gas-filled swim bladder which can be expanded or contracted as the circumstances require. A disadvantage of this method is that adjustments need to be made constantly as the water pressure varies when the fish swims higher and lower in the water column. An alternative buoyancy aid is the use of lipids, which are less dense than water—squalene, commonly found in shark livers, has a specific gravity of just 0.86. In the velvet belly lanternshark (\"Etmopterus spinax\"), a benthopelagic species, 17 % of the bodyweight is liver of which 70 % are lipids. Benthic rays and skates have smaller livers with lower concentrations of lipids; they are therefore denser than water and they do not swim continuously, intermittently resting on the seabed. Some fish have no buoyancy aids but use their pectoral fins which are so angled as to give lift as they swim. The disadvantage of this is that, if they stop swimming, the fish sink, and they cannot hover, or swim backwards.\n\nDemersal fish have various feeding strategies; many feed on zooplankton or organisms or algae on the seabed; some of these feed on epifauna (invertebrates on top of the seafloor), while others specialise on infauna (invertebrates that burrow beneath the seafloor). Others are scavengers, eating the dead remains of plants or animals while still others are predators.\n\nZooplankton are animals that drift with the current, but many have some limited means of locomotion and have some control over the depths at which they drift. They use gas-filled sacs or accumulations of substances with low densities to provide buoyancy, or they may have structures that slow down any passive descent. Where the adult, benthic organism is limited to life in a certain range of depths, their larvae need to optimise their chances of settling on a suitable substrate. \n\nCuttlefish are able to adjust their buoyancy using their cuttlebones, lightweight rigid structures with cavities filled with gas, which have a specific gravity of about 0.6. This enables them to swim at varying depths. Another invertebrate that feeds on the seabed and has swimming abilities is the nautilus, which stores gas in its chambers and adjusts its buoyancy by use of osmosis, pumping water in and out.\n"}
{"id": "22380479", "url": "https://en.wikipedia.org/wiki?curid=22380479", "title": "Eduction (geology)", "text": "Eduction (geology)\n\nIn geology, eduction is a process in which the Earth's crust spreads sideways, exposing deep-seated rocks.\n\nIt is prominent in the middle layers of the Himalayas, where gravity pushes the mountains down. Together with a high grade of erosion, this activity brings deep rocks to the surface, many from more than a depth of 100 km. The unusually fast elevation preserves rare metastable minerals, e.g. diamonds and coesite.\n\n"}
{"id": "9531", "url": "https://en.wikipedia.org/wiki?curid=9531", "title": "Electrical engineering", "text": "Electrical engineering\n\nElectrical engineering is a professional engineering discipline that generally deals with the study and application of electricity, electronics, and electromagnetism. This field first became an identifiable occupation in the later half of the 19th century after commercialization of the electric telegraph, the telephone, and electric power distribution and use. Subsequently, broadcasting and recording media made electronics part of daily life. The invention of the transistor, and later the integrated circuit, brought down the cost of electronics to the point they can be used in almost any household object.\n\nElectrical engineering has now subdivided into a wide range of subfields including electronics, digital computers, computer engineering, power engineering, telecommunications, control systems, radio-frequency engineering, signal processing, instrumentation, and microelectronics. Many of these subdisciplines overlap with other engineering branches, spanning a huge number of specializations such as hardware engineering, power electronics, electromagnetics & waves, microwave engineering, nanotechnology, electrochemistry, renewable energies, mechatronics, electrical materials science, and much more. See glossary of electrical and electronics engineering.\n\nElectrical engineers typically hold a degree in electrical engineering or electronic engineering. Practicing engineers may have professional certification and be members of a professional body. Such bodies include the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (IET) \"(formerly the IEE)\".\n\nElectrical engineers work in a very wide range of industries and the skills required are likewise variable. These range from basic circuit theory to the management skills required of a project manager. The tools and equipment that an individual engineer may need are similarly variable, ranging from a simple voltmeter to a top end analyzer to sophisticated design and manufacturing software.\n\nElectricity has been a subject of scientific interest since at least the early 17th century. William Gilbert was a prominent early electrical scientist, and was the first to draw a clear distinction between magnetism and static electricity. He is credited with establishing the term \"electricity\". He also designed the versorium: a device that detects the presence of statically charged objects. In 1762 Swedish professor Johan Carl Wilcke invented a device later named electrophorus that produced a static electric charge. By 1800 Alessandro Volta had developed the voltaic pile, a forerunner of the electric battery\n\nIn the 19th century, research into the subject started to intensify. Notable developments in this century include the work of Georg Ohm, who in 1827 quantified the relationship between the electric current and potential difference in a conductor, of Michael Faraday (the discoverer of electromagnetic induction in 1831), and of James Clerk Maxwell, who in 1873 published a unified theory of electricity and magnetism in his treatise \"Electricity and Magnetism\".\n\nElectrical engineering became a profession in the later 19th century. Practitioners had created a global electric telegraph network and the first professional electrical engineering institutions were founded in the UK and USA to support the new discipline. Although it is impossible to precisely pinpoint a first electrical engineer, Francis Ronalds stands ahead of the field, who created the first working electric telegraph system in 1816 and documented his vision of how the world could be transformed by electricity. Over 50 years later, he joined the new Society of Telegraph Engineers (soon to be renamed the Institution of Electrical Engineers) where he was regarded by other members as the first of their cohort. By the end of the 19th century, the world had been forever changed by the rapid communication made possible by the engineering development of land-lines, submarine cables, and, from about 1890, wireless telegraphy.\n\nPractical applications and advances in such fields created an increasing need for standardised units of measure. They led to the international standardization of the units volt, ampere, coulomb, ohm, farad, and henry. This was achieved at an international conference in Chicago in 1893. The publication of these standards formed the basis of future advances in standardisation in various industries, and in many countries, the definitions were immediately recognized in relevant legislation.\n\nDuring these years, the study of electricity was largely considered to be a subfield of physics since the early electrical technology was considered electromechanical in nature. The Technische Universität Darmstadt founded the world's first department of electrical engineering in 1882. The first electrical engineering degree program was started at Massachusetts Institute of Technology (MIT) in the physics department under Professor Charles Cross, though it was Cornell University to produce the world's first electrical engineering graduates in 1885. The first course in electrical engineering was taught in 1883 in Cornell’s Sibley College of Mechanical Engineering and Mechanic Arts. It was not until about 1885 that Cornell President Andrew Dickson White established the first Department of Electrical Engineering in the United States. In the same year, University College London founded the first chair of electrical engineering in Great Britain. Professor Mendell P. Weinbach at University of Missouri soon followed suit by establishing the electrical engineering department in 1886. Afterwards, universities and institutes of technology gradually started to offer electrical engineering programs to their students all over the world.\nDuring these decades use of electrical engineering increased dramatically. In 1882, Thomas Edison switched on the world's first large-scale electric power network that provided 110 volts — direct current (DC) — to 59 customers on Manhattan Island in New York City. In 1884, Sir Charles Parsons invented the steam turbine allowing for more efficient electric power generation. Alternating current, with its ability to transmit power more efficiently over long distances via the use of transformers, developed rapidly in the 1880s and 1890s with transformer designs by Károly Zipernowsky, Ottó Bláthy and Miksa Déri (later called ZBD transformers), Lucien Gaulard, John Dixon Gibbs and William Stanley, Jr.. Practical AC motor designs including induction motors were independently invented by Galileo Ferraris and Nikola Tesla and further developed into a practical three-phase form by Mikhail Dolivo-Dobrovolsky and Charles Eugene Lancelot Brown. Charles Steinmetz and Oliver Heaviside contributed to the theoretical basis of alternating current engineering. The spread in the use of AC set off in the United States what has been called the \"War of Currents\" between a George Westinghouse backed AC system and a Thomas Edison backed DC power system, with AC being adopted as the overall standard.\n\nDuring the development of radio, many scientists and inventors contributed to radio technology and electronics. The mathematical work of James Clerk Maxwell during the 1850s had shown the relationship of different forms of electromagnetic radiation including possibility of invisible airborne waves (later called \"radio waves\"). In his classic physics experiments of 1888, Heinrich Hertz proved Maxwell's theory by transmitting radio waves with a spark-gap transmitter, and detected them by using simple electrical devices. Other physicists experimented with these new waves and in the process developed devices for transmitting and detecting them. In 1895, Guglielmo Marconi began work on a way to adapt the known methods of transmitting and detecting these \"Hertzian waves\" into a purpose built commercial wireless telegraphic system. Early on, he sent wireless signals over a distance of one and a half miles. In December 1901, he sent wireless waves that were not affected by the curvature of the Earth. Marconi later transmitted the wireless signals across the Atlantic between Poldhu, Cornwall, and St. John's, Newfoundland, a distance of .\n\nIn 1897, Karl Ferdinand Braun introduced the cathode ray tube as part of an oscilloscope, a crucial enabling technology for electronic television. John Fleming invented the first radio tube, the diode, in 1904. Two years later, Robert von Lieben and Lee De Forest independently developed the amplifier tube, called the triode.\n\nIn 1920, Albert Hull developed the magnetron which would eventually lead to the development of the microwave oven in 1946 by Percy Spencer. In 1934, the British military began to make strides toward radar (which also uses the magnetron) under the direction of Dr Wimperis, culminating in the operation of the first radar station at Bawdsey in August 1936.\n\nIn 1941, Konrad Zuse presented the Z3, the world's first fully functional and programmable computer using electromechanical parts. In 1943, Tommy Flowers designed and built the Colossus, the world's first fully functional, electronic, digital and programmable computer. In 1946, the ENIAC (Electronic Numerical Integrator and Computer) of John Presper Eckert and John Mauchly followed, beginning the computing era. The arithmetic performance of these machines allowed engineers to develop completely new technologies and achieve new objectives, including the Apollo program which culminated in landing astronauts on the Moon.\n\nThe invention of the transistor in late 1947 by William Shockley, John Bardeen, and Walter Brattain of the Bell Telephone Laboratories opened the door for more compact devices and led to the development of the integrated circuit in 1958 by Jack Kilby and independently in 1959 by Robert Noyce.\n\nThe microprocessor was introduced with the Intel 4004. It began with the \"Busicom Project\" as Masatoshi Shima's three-chip CPU design in 1968, before Sharp's Tadashi Sasaki conceived of a single-chip CPU design, which he discussed with Busicom and Intel in 1968. The Intel 4004 was then developed as a single-chip microprocessor from 1969 to 1970, led by Intel's Marcian Hoff and Federico Faggin and Busicom's Masatoshi Shima. The microprocessor led to the development of microcomputers and personal computers, and the microcomputer revolution.\n\nElectrical engineering has many subdisciplines, the most common of which are listed below. Although there are electrical engineers who focus exclusively on one of these subdisciplines, many deal with a combination of them. Sometimes certain fields, such as electronic engineering and computer engineering, are considered separate disciplines in their own right.\n\nPower engineering deals with the generation, transmission, and distribution of electricity as well as the design of a range of related devices. These include transformers, electric generators, electric motors, high voltage engineering, and power electronics. In many regions of the world, governments maintain an electrical network called a power grid that connects a variety of generators together with users of their energy. Users purchase electrical energy from the grid, avoiding the costly exercise of having to generate their own. Power engineers may work on the design and maintenance of the power grid as well as the power systems that connect to it. Such systems are called \"on-grid\" power systems and may supply the grid with additional power, draw power from the grid, or do both. Power engineers may also work on systems that do not connect to the grid, called \"off-grid\" power systems, which in some cases are preferable to on-grid systems. The future includes Satellite controlled power systems, with feedback in real time to prevent power surges and prevent blackouts.\n\nControl engineering focuses on the modeling of a diverse range of dynamic systems and the design of controllers that will cause these systems to behave in the desired manner. To implement such controllers, electrical engineers may use electronic circuits, digital signal processors, microcontrollers, and programmable logic controllers (PLCs). Control engineering has a wide range of applications from the flight and propulsion systems of commercial airliners to the cruise control present in many modern automobiles. It also plays an important role in industrial automation.\n\nControl engineers often utilize feedback when designing control systems. For example, in an automobile with cruise control the vehicle's speed is continuously monitored and fed back to the system which adjusts the motor's power output accordingly. Where there is regular feedback, control theory can be used to determine how the system responds to such feedback.\n\nElectronic engineering involves the design and testing of electronic circuits that use the properties of components such as resistors, capacitors, inductors, diodes, and transistors to achieve a particular functionality. The tuned circuit, which allows the user of a radio to filter out all but a single station, is just one example of such a circuit. Another example to research is a pneumatic signal conditioner.\n\nPrior to the Second World War, the subject was commonly known as \"radio engineering\" and basically was restricted to aspects of communications and radar, commercial radio, and early television. Later, in post war years, as consumer devices began to be developed, the field grew to include modern television, audio systems, computers, and microprocessors. In the mid-to-late 1950s, the term \"radio engineering\" gradually gave way to the name \"electronic engineering\".\n\nBefore the invention of the integrated circuit in 1959, electronic circuits were constructed from discrete components that could be manipulated by humans. These discrete circuits consumed much space and power and were limited in speed, although they are still common in some applications. By contrast, integrated circuits packed a large number—often millions—of tiny electrical components, mainly transistors, into a small chip around the size of a coin. This allowed for the powerful computers and other electronic devices we see today.\n\nMicroelectronics engineering deals with the design and microfabrication of very small electronic circuit components for use in an integrated circuit or sometimes for use on their own as a general electronic component. The most common microelectronic components are semiconductor transistors, although all main electronic components (resistors, capacitors etc.) can be created at a microscopic level. Nanoelectronics is the further scaling of devices down to nanometer levels. Modern devices are already in the nanometer regime, with below 100 nm processing having been standard since around 2002.\n\nMicroelectronic components are created by chemically fabricating wafers of semiconductors such as silicon (at higher frequencies, compound semiconductors like gallium arsenide and indium phosphide) to obtain the desired transport of electronic charge and control of current. The field of microelectronics involves a significant amount of chemistry and material science and requires the electronic engineer working in the field to have a very good working knowledge of the effects of quantum mechanics.\n\nSignal processing deals with the analysis and manipulation of signals. Signals can be either analog, in which case the signal varies continuously according to the information, or digital, in which case the signal varies according to a series of discrete values representing the information. For analog signals, signal processing may involve the amplification and filtering of audio signals for audio equipment or the modulation and demodulation of signals for telecommunications. For digital signals, signal processing may involve the compression, error detection and error correction of digitally sampled signals.\n\nSignal Processing is a very mathematically oriented and intensive area forming the core of digital signal processing and it is rapidly expanding with new applications in every field of electrical engineering such as communications, control, radar, audio engineering, broadcast engineering, power electronics, and biomedical engineering as many already existing analog systems are replaced with their digital counterparts. Analog signal processing is still important in the design of many control systems.\n\nDSP processor ICs are found in many types of modern electronic devices, such as digital television sets, radios, Hi-Fi audio equipment, mobile phones, multimedia players, camcorders and digital cameras, automobile control systems, noise cancelling headphones, digital spectrum analyzers, missile guidance systems, radar systems, and telematics systems. In such products, DSP may be responsible for noise reduction, speech recognition or synthesis, encoding or decoding digital media, wirelessly transmitting or receiving data, triangulating position using GPS, and other kinds of image processing, video processing, audio processing, and speech processing.\n\nTelecommunications engineering focuses on the transmission of information across a communication channel such as a coax cable, optical fiber or free space. Transmissions across free space require information to be encoded in a carrier signal to shift the information to a carrier frequency suitable for transmission; this is known as modulation. Popular analog modulation techniques include amplitude modulation and frequency modulation. The choice of modulation affects the cost and performance of a system and these two factors must be balanced carefully by the engineer.\n\nOnce the transmission characteristics of a system are determined, telecommunication engineers design the transmitters and receivers needed for such systems. These two are sometimes combined to form a two-way communication device known as a transceiver. A key consideration in the design of transmitters is their power consumption as this is closely related to their signal strength. If the signal strength of a transmitter is insufficient the signal's information will be corrupted by noise.\n\nInstrumentation engineering deals with the design of devices to measure physical quantities such as pressure, flow, and temperature. The design of such instruments requires a good understanding of physics that often extends beyond electromagnetic theory. For example, flight instruments measure variables such as wind speed and altitude to enable pilots the control of aircraft analytically. Similarly, thermocouples use the Peltier-Seebeck effect to measure the temperature difference between two points.\n\nOften instrumentation is not used by itself, but instead as the sensors of larger electrical systems. For example, a thermocouple might be used to help ensure a furnace's temperature remains constant. For this reason, instrumentation engineering is often viewed as the counterpart of control.\n\nComputer engineering deals with the design of computers and computer systems. This may involve the design of new hardware, the design of PDAs, tablets, and supercomputers, or the use of computers to control an industrial plant. Computer engineers may also work on a system's software. However, the design of complex software systems is often the domain of software engineering, which is usually considered a separate discipline. Desktop computers represent a tiny fraction of the devices a computer engineer might work on, as computer-like architectures are now found in a range of devices including video game consoles and DVD players.\n\nMechatronics is an engineering discipline which deals with the convergence of electrical and mechanical systems. Such combined systems are known as electromechanical systems and have widespread adoption. Examples include automated manufacturing systems, heating, ventilation and air-conditioning systems, and various subsystems of aircraft and automobiles.\n\n\"Electronic systems design\" is the subject within electrical engineering that deals with the multi-disciplinary design issues of complex electrical and mechanical systems.\n\nThe term \"mechatronics\" is typically used to refer to macroscopic systems but futurists have predicted the emergence of very small electromechanical devices. Already, such small devices, known as Microelectromechanical systems (MEMS), are used in automobiles to tell airbags when to deploy, in digital projectors to create sharper images, and in inkjet printers to create nozzles for high definition printing. In the future it is hoped the devices will help build tiny implantable medical devices and improve optical communication.\n\nBiomedical engineering is another related discipline, concerned with the design of medical equipment. This includes fixed equipment such as ventilators, MRI scanners, and electrocardiograph monitors as well as mobile equipment such as cochlear implants, artificial pacemakers, and artificial hearts.\n\nAerospace engineering and robotics an example is the most recent electric propulsion and ion propulsion.\n\nElectrical engineers typically possess an academic degree with a major in electrical engineering, electronics engineering, electrical engineering technology, or electrical and electronic engineering. The same fundamental principles are taught in all programs, though emphasis may vary according to title. The length of study for such a degree is usually four or five years and the completed degree may be designated as a Bachelor of Science in Electrical/Electronics Engineering Technology, Bachelor of Engineering, Bachelor of Science, Bachelor of Technology, or Bachelor of Applied Science depending on the university. The bachelor's degree generally includes units covering physics, mathematics, computer science, project management, and a variety of topics in electrical engineering. Initially such topics cover most, if not all, of the subdisciplines of electrical engineering. At some schools, the students can then choose to emphasize one or more subdisciplines towards the end of their courses of study.\n\nAt many schools, electronic engineering is included as part of an electrical award, sometimes explicitly, such as a Bachelor of Engineering (Electrical and Electronic), but in others electrical and electronic engineering are both considered to be sufficiently broad and complex that separate degrees are offered.\n\nSome electrical engineers choose to study for a postgraduate degree such as a Master of Engineering/Master of Science (M.Eng./M.Sc.), a Master of Engineering Management, a Doctor of Philosophy (Ph.D.) in Engineering, an Engineering Doctorate (Eng.D.), or an Engineer's degree. The master's and engineer's degrees may consist of either research, coursework or a mixture of the two. The Doctor of Philosophy and Engineering Doctorate degrees consist of a significant research component and are often viewed as the entry point to academia. In the United Kingdom and some other European countries, Master of Engineering is often considered to be an undergraduate degree of slightly longer duration than the Bachelor of Engineering rather than postgraduate.\n\nIn most countries, a bachelor's degree in engineering represents the first step towards professional certification and the degree program itself is certified by a professional body. After completing a certified degree program the engineer must satisfy a range of requirements (including work experience requirements) before being certified. Once certified the engineer is designated the title of Professional Engineer (in the United States, Canada and South Africa), Chartered Engineer or Incorporated Engineer (in India, Pakistan, the United Kingdom, Ireland and Zimbabwe), Chartered Professional Engineer (in Australia and New Zealand) or European Engineer (in much of the European Union).\nThe advantages of licensure vary depending upon location. For example, in the United States and Canada \"only a licensed engineer may seal engineering work for public and private clients\". This requirement is enforced by state and provincial legislation such as Quebec's Engineers Act. In other countries, no such legislation exists. Practically all certifying bodies maintain a code of ethics that they expect all members to abide by or risk expulsion. In this way these organizations play an important role in maintaining ethical standards for the profession. Even in jurisdictions where certification has little or no legal bearing on work, engineers are subject to contract law. In cases where an engineer's work fails he or she may be subject to the tort of negligence and, in extreme cases, the charge of criminal negligence. An engineer's work must also comply with numerous other rules and regulations, such as building codes and legislation pertaining to environmental law.\n\nProfessional bodies of note for electrical engineers include the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (IET). The IEEE claims to produce 30% of the world's literature in electrical engineering, has over 360,000 members worldwide and holds over 3,000 conferences annually. The IET publishes 21 journals, has a worldwide membership of over 150,000, and claims to be the largest professional engineering society in Europe. Obsolescence of technical skills is a serious concern for electrical engineers. Membership and participation in technical societies, regular reviews of periodicals in the field and a habit of continued learning are therefore essential to maintaining proficiency. An MIET(Member of the Institution of Engineering and Technology) is recognised in Europe as an Electrical and computer (technology) engineer.\n\nIn Australia, Canada, and the United States electrical engineers make up around 0.25% of the labor force (see note).\n\nFrom the Global Positioning System to electric power generation, electrical engineers have contributed to the development of a wide range of technologies. They design, develop, test, and supervise the deployment of electrical systems and electronic devices. For example, they may work on the design of telecommunication systems, the operation of electric power stations, the lighting and wiring of buildings, the design of household appliances, or the electrical control of industrial machinery.\nFundamental to the discipline are the sciences of physics and mathematics as these help to obtain both a qualitative and quantitative description of how such systems will work. Today most engineering work involves the use of computers and it is commonplace to use computer-aided design programs when designing electrical systems. Nevertheless, the ability to sketch ideas is still invaluable for quickly communicating with others.\nAlthough most electrical engineers will understand basic circuit theory (that is the interactions of elements such as resistors, capacitors, diodes, transistors, and inductors in a circuit), the theories employed by engineers generally depend upon the work they do. For example, quantum mechanics and solid state physics might be relevant to an engineer working on VLSI (the design of integrated circuits), but are largely irrelevant to engineers working with macroscopic electrical systems. Even circuit theory may not be relevant to a person designing telecommunication systems that use off-the-shelf components. Perhaps the most important technical skills for electrical engineers are reflected in university programs, which emphasize strong numerical skills, computer literacy, and the ability to understand the technical language and concepts that relate to electrical engineering.\n\nA wide range of instrumentation is used by electrical engineers. For simple control circuits and alarms, a basic multimeter measuring voltage, current, and resistance may suffice. Where time-varying signals need to be studied, the oscilloscope is also an ubiquitous instrument. In RF engineering and high frequency telecommunications, spectrum analyzers and network analyzers are used. In some disciplines, safety can be a particular concern with instrumentation. For instance, medical electronics designers must take into account that much lower voltages than normal can be dangerous when electrodes are directly in contact with internal body fluids. Power transmission engineering also has great safety concerns due to the high voltages used; although voltmeters may in principle be similar to their low voltage equivalents, safety and calibration issues make them very different. Many disciplines of electrical engineering use tests specific to their discipline. Audio electronics engineers use audio test sets consisting of a signal generator and a meter, principally to measure level but also other parameters such as harmonic distortion and noise. Likewise, information technology have their own test sets, often specific to a particular data format, and the same is true of television broadcasting.\nFor many engineers, technical work accounts for only a fraction of the work they do. A lot of time may also be spent on tasks such as discussing proposals with clients, preparing budgets and determining project schedules. Many senior engineers manage a team of technicians or other engineers and for this reason project management skills are important. Most engineering projects involve some form of documentation and strong written communication skills are therefore very important.\n\nThe workplaces of engineers are just as varied as the types of work they do. Electrical engineers may be found in the pristine lab environment of a fabrication plant, onboard a Naval ship, the offices of a consulting firm or on site at a mine. During their working life, electrical engineers may find themselves supervising a wide range of individuals including scientists, electricians, computer programmers, and other engineers.\n\nElectrical engineering has an intimate relationship with the physical sciences. For instance, the physicist Lord Kelvin played a major role in the engineering of the first transatlantic telegraph cable. Conversely, the engineer Oliver Heaviside produced major work on the mathematics of transmission on telegraph cables. Electrical engineers are often required on major science projects. For instance, large particle accelerators such as CERN need electrical engineers to deal with many aspects of the project: from the power distribution, to the instrumentation, to the manufacture and installation of the superconducting electromagnets.\n\nNote I - In May 2014 there were around 175,000 people working as electrical engineers in the US. In 2012, Australia had around 19,000 while in Canada, there were around 37,000 (), constituting about 0.2% of the labour force in each of the three countries. Australia and Canada reported that 96% and 88% of their electrical engineers respectively are male.\n\n\n\n"}
{"id": "32851664", "url": "https://en.wikipedia.org/wiki?curid=32851664", "title": "Ethnographic Museum of Tripoli", "text": "Ethnographic Museum of Tripoli\n\nThe Ethnographic Museum of Tripoli is a museum located in Tripoli, Libya.\n\n"}
{"id": "5081444", "url": "https://en.wikipedia.org/wiki?curid=5081444", "title": "Extraordinary optical transmission", "text": "Extraordinary optical transmission\n\nExtraordinary optical transmission (EOT) is the phenomenon of greatly enhanced transmission of light through a subwavelength aperture in an otherwise opaque metallic film which has been patterned with a regularly repeating periodic structure. Generally when light of a certain wavelength falls on a subwavelength aperture, it is diffracted isotropically in all directions evenly, with minimal far-field transmission. This is the understanding from classical aperture theory as described by Bethe. In EOT however, the regularly repeating structure enables much higher transmission efficiency to occur, up to several orders of magnitude greater than that predicted by classical aperture theory. It was first described in 1998.\n\nThis phenomenon that was fully analyzed with a microscopic scattering model is \"partly\" attributed to the presence of surface plasmon resonances and constructive interference. A surface plasmon (SP) is a collective excitation of the electrons at the junction between a conductor and an insulator and is one of a series of interactions between light and a metal surface called Plasmonics.\n\nCurrently, there is experimental evidence of EOT out of the optical range. Analytical approaches also predict EOT on perforated plates with a perfect conductor model. Holes can somewhat emulate plasmons at other regions of the electromagnetic spectrum where they do not exist. Then, the plasmonic contribution is a very particular peculiarity of the EOT resonance and should not be taken as the main contribution to the phenomenon. More recent work has shown a strong contribution from overlapping evanescent wave coupling, which explains why surface plasmon resonance enhances the EOT effect on both sides of a metallic film at optical frequencies, but accounts for the terahertz-range transmission.\n\nSimple analytical explanations of this phenomenon have been elaborated, emphasizing the similarity between arrays of particles and arrays of holes, and establishing that the phenomenon is dominated by diffraction.\n\nEOT is expected to play an important role in the creation of components of 'photonic' circuits. (Photonic circuits are analogous to electronic circuits.)\n\nOne of the most ground-breaking results linked to EOT is the possibility to implement a Left-Handed Metamaterial (LHM) by simply stacking hole arrays.\n\nEOT-based chemical sensing is another major area of research. Much like in a traditional surface plasmon resonance sensor, the EOT efficiency varies with the wavelength of the incident light, and the value of the in-plane wavevector component. This can be exploited as a means of transducing chemical binding events by measuring a change in the local dielectric constant (due to binding of the target species) as a shift in the spectral location of the EOT peak. EOT offers one key advantage over a Kretschmann-style SPR chemical sensor, that of being an inherently nanometer-micrometer scale device; it is therefore particularly amenable to miniaturization.\n"}
{"id": "230361", "url": "https://en.wikipedia.org/wiki?curid=230361", "title": "Fin whale", "text": "Fin whale\n\nThe fin whale (\"Balaenoptera physalus\"), also known as finback whale or common rorqual and formerly known as herring whale or razorback whale, is a marine mammal belonging to the parvorder of baleen whales. It is the second-largest species on Earth after the blue whale. The largest reportedly grow to long with a maximum confirmed length of , a maximum recorded weight of nearly , and a maximum estimated weight of around . American naturalist Roy Chapman Andrews called the fin whale \"the greyhound of the sea ... for its beautiful, slender body is built like a racing yacht and the animal can surpass the speed of the fastest ocean steamship.\"\n\nThe fin whale's body is long and slender, coloured brownish-grey with a paler underside. The fin whale is a large baleen whale that belongs to the Cetacean order, which includes all species of whale, dolphin, and porpoise. At least two recognized subspecies exist, in the North Atlantic and the Southern Hemisphere. It is found in all the major oceans, from polar to tropical waters. It is absent only from waters close to the ice pack at the poles and relatively small areas of water away from the open ocean. The highest population density occurs in temperate and cool waters. Its food consists of small schooling fish, squid, and crustaceans including copepods and krill.\n\nLike all other large whales, the fin whale was heavily hunted during the 20th century. As a result, it is an endangered species. Over 725,000 fin whales were reportedly taken from the Southern Hemisphere between 1905 and 1976; as of 1997 only 38,000 survived. Recovery of the overall population size of southern species is predicted to be at less than 50% of its pre-whaling state by 2100 due to heavier impacts of whaling and slower recovery rates.\n\nThe International Whaling Commission (IWC) issued a moratorium on commercial hunting of this whale, although Iceland and Japan have resumed hunting. The species is also hunted by Greenlanders under the IWC's Aboriginal Subsistence Whaling provisions. Global population estimates range from less than 100,000 to roughly 119,000.\n\nThe fin whale was first described by Friderich Martens in 1675 and then again by Paul Dudley in 1725. The former description was used as the primary basis of the species \"Balaena physalus\" by Carl Linnaeus in 1758. In 1804, Bernard Germain de Lacépède reclassified the species as \"Balaenoptera rorqual\", based on a specimen that had stranded on Île Sainte-Marguerite (Cannes, France) in 1798. In 1830, Louis Companyo described a specimen that had stranded near Saint-Cyprien, southern France, in 1828 as \"Balaena musculus\". Most later authors followed him in using the specific name \"musculus\", until Frederick W. True (1898) showed that it referred to the blue whale. In 1846, British taxonomist John Edward Gray described a specimen from the Falkland Islands as \"Balaenoptera australis\". In 1865, German naturalist Hermann Burmeister described a roughly specimen found near Buenos Aires about 30 years earlier as \"Balaenoptera patachonicus\". In 1903, Romanian scientist Emil Racoviță placed all these designations into \"Balaenoptera physalus\". The word \"physalus\" comes from the Greek word \"physa\", meaning \"blows\", referring to the prominent blow of the species (as described by Martens [1675, p. 132]: \"They know the \"finn-fish\" by the ... vehement blowing and spouting up of the water...\").\n\nFin whales are rorquals, members of the family Balaenopteridae, which also includes the humpback whale, the blue whale, Bryde's whale, the sei whale, and the minke whales. The family diverged from the other baleen whales in the suborder Mysticeti as long ago as the middle Miocene, although it is not known when the members of these families further evolved into their own species.\n\nRecent DNA evidence indicates the fin whale may be more closely related to the humpback whale (\"Megaptera novaeangliae\") and in at least one study the gray whale (\"Eschrichtius robustus\"), two whales in different genera, than it is to members of its own genus, such as the minke whales. \nAs of 2006, two subspecies are named, each with distinct physical features and vocalizations. The northern fin whale, \"B. p. physalus\" (Linnaeus 1758) inhabits the North Atlantic and the southern fin whale, \"B. p. quoyi\" (Fischer 1829) occupies the Southern Ocean. Most experts consider the fin whales of the North Pacific to be a third, as yet unnamed subspecies—this was supported by a 2013 study, which found that the Northern Hemisphere \"B. p. physalus\" was not composed of a single subspecies. The three groups mix at most rarely.\n\nClarke (2004) proposed a \"pygmy\" subspecies (\"B. p. patachonica\", Burmeister, 1865) that is purportedly darker in colour and has black baleen. He based this on a single physically mature female caught in the Antarctic in 1947–48, the smaller average size (a few feet) of sexually and physically mature fin whales caught by the Japanese around 50°S, and smaller, darker sexually immature fin whales caught in the Antarctic which he believed were a \"migratory phase\" of his proposed subspecies. His proposal is not widely accepted and no genetic evidence for their existence is available.\n\nThe genetic distance between blue and fin whales has been compared to that between a gorilla and human (3.5 million years on the evolutionary tree.) Nevertheless, hybrid individuals between blue and fin whales with characteristics of both are known to occur with relative frequency in both the North Atlantic and North Pacific.\n\nThe DNA profile of a sampling of whale meat in the Japanese market found evidence of blue/fin hybrids.\n\nThe fin whale is usually distinguished by its tall spout, long back, prominent dorsal fin, and asymmetrical colouration.\n\nThe animal's large size aids in identification, and it is usually only confused with the blue whale, the sei whale, or, in warmer waters, Bryde's whale.\n\nIn the Northern Hemisphere, the average size of adult males and females is about , respectively, averaging 38.5 and 50.5 tonnes (42.5 and 55.5 tons), while in the Southern Hemisphere, it is , weighing 52.5 and 63 tonnes (58 and 69.5 tons).\n\nIn the North Atlantic, the longest reported were a 24.4 m (80 ft) male caught off Shetland in 1905 and a female caught off Scotland sometime between 1908 and 1914, while the longest reliably measured were three 20.7 m (68 ft) males caught off Iceland in 1973–74 and a 22.5 m (74 ft) female also caught off Iceland in 1975. Mediterranean population are generally smaller, reaching just above 20 m (65.5 ft) at maximum, or possibly up to .\n\nIn the North Pacific, the longest reported were three 22.9 m (75 ft) males, two caught off California between 1919 and 1926 and the other caught off Alaska in 1925, and a 24.7 m (81 ft) female also caught off California, while the longest reliably measured were a 21 m (69 ft) male caught off British Columbia in 1959 and a 22.9 m (75 ft) female caught off central California between 1959 and 1970.\n\nIn the Southern Hemisphere, the longest reported for each sex were , while the longest measured by Mackintosh and Wheeler (1929) were . Major F. A. Spencer, while whaling inspector of the factory ship \"Southern Princess\" (1936–38), confirmed the length of a female caught in the Antarctic, south of the Indian Ocean; scientist David Edward Gaskin also measured a 25.9 m female as whaling inspector of the British factory ship \"Southern Venturer\" in the Southern Ocean in the 1961–62 season. Terence Wise, who worked as a winch operator aboard the British factory ship \"Balaena\", claimed that \"the biggest fin [he] ever saw\" was a specimen caught near Bouvet Island in January 1958. The largest fin whale ever weighed (piecemeal) was a pregnant female caught by Japanese whalers in the Antarctic in 1948 which weighed , not including 6% for loss of fluids during the flensing process. An individual at the maximum confirmed size of 25.9 m is estimated to weigh around 95 tonnes (104.5 tons), varying from about 76 tonnes (84 tons) to 114 tonnes (125.5 tons) depending on fat condition which varies by about 50% during the year.\n\nA newborn fin whale measures about in length and weighs about .\n\nThe fin whale is brownish to dark or light gray dorsally and white ventrally. The left side of the head is dark gray, while the right side exhibits a complex pattern of contrasting light and dark markings. On the right lower jaw is a white or light gray \"right mandible patch\", which sometimes extends out as a light \"blaze\" laterally and dorsally unto the upper jaw and back to just behind the blowholes. Two narrow dark stripes originate from the eye and ear, the former widening into a large dark area on the shoulder—these are separated by a light area called the \"interstripe wash\". These markings are more prominent on individuals in the North Atlantic than in the North Pacific, where they can appear indistinct. The left side exhibits similar but much fainter markings. Dark, oval-shaped areas of pigment called \"flipper shadows\" extend below and posterior to the pectoral fins. This type of asymmetry is seen in Omura's whale and occasionally in minke whales. It was thought to have evolved because the whale swims on its right side when surface lunging and it sometimes circles to the right while at the surface above a prey patch. However, the whales just as often circle to the left. No accepted hypothesis explains the asymmetry.\nIt has paired blowholes on a prominent splashguard and a broad, flat, V-shaped rostrum. A single median ridge stops well short of the rostrum tip. A light V-shaped marking, the chevron, begins behind the blowholes and extends back and then forward again.\nThe whale has a series of 56–100 pleats or grooves along the bottom of the body that run from the tip of the chin to the navel that allow the throat area to expand greatly during feeding. It has a curved, prominent dorsal fin that ranges in height from (usually ) and averages about , lying about three quarters of the way along the back. Its flippers are small and tapered and its tail is wide, pointed at the tip, and notched in the centre.\n\nWhen the whale surfaces, the dorsal fin is visible soon after the spout. The spout is vertical and narrow and can reach heights of or more.\n\nThe oral cavity of the fin whale has a very stretchy or extensible nerve system which aids them in feeding.\n\nMating occurs in temperate, low-latitude seas during the winter, followed by an 11- to 12-month gestation period. A newborn weans from its mother at 6 or 7 months of age when it is in length, and the accompanies the mother to the summer feeding ground. Females reproduce every 2 or 3 years, with as many as six fetuses being reported, but single births are far more common. Females reach sexual maturity between 6 and 12 years of age at lengths of in the Northern Hemisphere and in the Southern Hemisphere. Calves remain with their mothers for about one year.\n\nFull physical maturity is attained between 25 and 30 years. Fin whales have a maximum life span of at least 94 years of age, although specimens have been found aged at an estimated 135–140 years.\n\nThe fin whale is one of the fastest cetaceans and can sustain speeds between and and bursts up to have been recorded, earning the fin whale the nickname \"the greyhound of the sea\".\n\nFin whales are more gregarious than other rorquals, and often live in groups of 6–10, although feeding groups may reach up to 100 animals.\n\nLike other whales, males make long, loud, low-frequency sounds. The vocalizations of blue and fin whales are the lowest-frequency sounds made by any animal. Most sounds are frequency-modulated (FM) down-swept infrasonic pulses from 16 to 40 hertz frequency (the range of sounds that most humans can hear falls between 20 hertz and 20 kilohertz). Each sound lasts one to two seconds, and various sound combinations occur in patterned sequences lasting 7 to 15 minutes each. The whale then repeats the sequences in bouts lasting up to many days. The vocal sequences have source levels of up to 184–186 decibels relative to 1 micropascal at a reference distance of one metre and can be detected hundreds of miles from their source.\n\nWhen fin whale sounds were first recorded by US biologists, they did not realize that these unusually loud, long, pure and regular sounds were being made by whales. They first investigated the possibilities that the sounds were due to equipment malfunction, geophysical phenomena, or even part of a Soviet Union scheme for detecting enemy submarines. Eventually, biologists demonstrated that the sounds were the vocalizations of fin whales.\n\nDirect association of these vocalizations with the reproductive season for the species and that only males make the sounds point to these vocalizations as possible reproductive displays. Over the past 100 years, the dramatic increase in ocean noise from shipping and naval activity may have slowed the recovery of the fin whale population, by impeding communications between males and receptive females.\n\nWhen feeding, they blow 5–7 times in quick succession, but while traveling or resting will blow once every minute or two. On their terminal (last) dive they arch their back high out of the water, but rarely raise their flukes out of the water. They then dive to depths of up to when feeding or a few hundred feet when resting or traveling. The average feeding dive off California and Baja lasts 6 minutes, with a maximum of 17 minutes; when traveling or resting they usually dive for only a few minutes at a time.\n\nLike many large rorquals, the fin whale is a cosmopolitan species. It is found in all the world's major oceans and in waters ranging from the polar to the tropical. It is absent only from waters close to the ice pack at both the north and south extremities and relatively small areas of water away from the large oceans, such as the Red Sea although they can reach into the Baltic Sea, a marginal sea of such conditions. The highest population density occurs in temperate and cool waters. It is less densely populated in the warmest, equatorial regions.\n\nThe North Atlantic fin whale has an extensive distribution, occurring from the Gulf of Mexico and Mediterranean Sea, northward to Baffin Bay and Spitsbergen. In general, fin whales are more common north of approximately 30°N latitude, but considerable confusion arises about their occurrence south of 30°N latitude because of the difficulty in distinguishing fin whales from Bryde's whales. Extensive ship surveys have led researchers to conclude that the summer feeding range of fin whales in the western North Atlantic is mainly between 41°20'N and 51°00'N, from shore seaward to the contour.\n\nSummer distribution of fin whales in the North Pacific is the immediate offshore waters from central Baja California to Japan and as far north as the Chukchi Sea bordering the Arctic Ocean. They occur in high densities in the northern Gulf of Alaska and southeastern Bering Sea between May and October, with some movement through the Aleutian passes into and out of the Bering Sea. Several whales tagged between November and January off southern California were killed in the summer off central California, Oregon, British Columbia, and in the Gulf of Alaska. Fin whales have been observed feeding 250 miles south of Hawaii in mid-May, and several winter sightings have been made there. Some researchers have suggested that the whales migrate into Hawaiian waters primarily in the autumn and winter.\n\nAlthough fin whales are certainly migratory, moving seasonally in and out of high-latitude feeding areas, the overall migration pattern is not well understood. Acoustic readings from passive-listening hydrophone arrays indicate a southward migration of the North Atlantic fin whale occurs in the autumn from the Labrador-Newfoundland region, south past Bermuda, and into the West Indies. One or more populations of fin whales are thought to remain year-round in high latitudes, moving offshore, but not southward in late autumn. A study based on resightings of identified fin whales in Massachusetts Bay indicates that calves often learn migratory routes from their mothers and return to their mother's feeding area in subsequent years.\n\nIn the Pacific, migration patterns are poorly characterized. Although some fin whales are apparently present year-round in the Gulf of California, there is a significant increase in their numbers in the winter and spring. Southern fin whales migrate seasonally from relatively high-latitude Antarctic feeding grounds in the summer to low-latitude breeding and calving areas in the winter. The location of winter breeding areas is still unknown, since these whales tend to migrate in the open ocean.\n\nNorth Atlantic fin whales are defined by the International Whaling Commission to exist in one of seven discrete population zones: Nova Scotia-New England, Newfoundland-Labrador, western Greenland, eastern Greenland-Iceland, North Norway, West Norway-Faroe Islands, and Ireland-Spain-United Kingdom-Portugal. Results of mark-and-recapture surveys have indicated that some movement occurs across the boundaries of these zones, suggesting that they are not entirely discrete and that some immigration and emigration does occur. Sigurjónsson estimated in 1995 that total pre-exploitation population size in the entire North Atlantic ranged between 50,000 and 100,000 animals, but his research is criticized for lack of supporting data and an explanation of his reasoning. In 1977, D.E. Sergeant suggested a \"primeval\" aggregate total of 30,000 to 50,000 throughout the North Atlantic. Of that number, 8,000 to 9,000 would have resided in the Newfoundland and Nova Scotia areas, with whales summering in U.S. waters south of Nova Scotia presumably omitted. J. M. Breiwick estimated that the \"exploitable\" (above the legal size limit of 50 feet) component of the Nova Scotia population was 1,500 to 1,600 animals in 1964, reduced to only about 325 in 1973. Two aerial surveys in Canadian waters since the early 1970s gave numbers of 79 to 926 whales on the eastern Newfoundland-Labrador shelf in August 1980, and a few hundred in the northern and central Gulf of Saint Lawrence in August 1995 – 1996. Summer estimates in the waters off western Greenland range between 500 and 2,000, and in 1974, Jonsgard considered the fin whales off Western Norway and the Faroe Islands to \"have been considerably depleted in postwar years, probably by overexploitation\". The population around Iceland appears to have fared much better, and in 1981, appeared to have undergone only a minor decline since the early 1960s. Surveys during the summers of 1987 and 1989 estimated of 10,000 to 11,000 between eastern Greenland and Norway. This shows a substantial recovery when compared to a survey in 1976 showing an estimate of 6,900, which was considered to be a \"slight\" decline since 1948. A Spanish NASS survey in 1989 of the France-Portugal-Spain sub-area estimated a summer population range at 17,355.\n\nA possible resident group was in waters off the Cape Verde Islands in 2000 and 2001.\n\nSatellite tracking revealed that those found in Pelagos Sanctuary migrate southward to off Tunisia, Malta, Pantelleria, and Sicily, and also possibly winter off coastal southern Italy, Sardinia, within the Strait of Messina, Aeolian Islands, and off Catalonia, Cabrera Archipelago, Libya, Kerkennah Islands, Tuscan Archipelago, Ischia and adjacent gulfs (e.g. Naples and Pozzuoli), winter feeding ground of Lampedusa, and whales may recolonize out of the Ligurian Sea to other areas such as in Ionian and in Adriatic Sea. Biology of the species along southern and southeastern parts of the basin such as off Libya, Algeria, and northern Egypt, is unclear due to lacks of scientific approaches although whales have been confirmed off the furthermost of the basin such as along in shore waters of Levantine Sea including Israel, Lebanon, and Cyprus. Documented records within Turkish waters have been in very small numbers; one sighting off Antalya in 1994 and five documented strandings as of 2016.\n\nThe total historical North Pacific population was estimated at 42,000 to 45,000 before the start of whaling. Of this, the population in the eastern portion of the North Pacific was estimated to be 25,000 to 27,000. By 1975, the estimate had declined to between 8,000 and 16,000. Surveys conducted in 1991, 1993, 1996, and 2001 produced estimates between 1,600 and 3,200 off California and 280 and 380 off Oregon and Washington. The minimum estimate for the California-Oregon-Washington population, as defined in the \"U.S. Pacific Marine Mammal Stock Assessments: 2005\", is about 2,500. Surveys in coastal waters of British Columbia in summers 2004 and 2005 produced abundance estimates of approximately 500 animals. Surveys near the Pribilof Islands in the Bering Sea indicated a substantial increase in the local abundance of fin whales between 1975–1978 and 1987–1989. In 1984, the entire population was estimated to be less than 38% of its historic carrying capacity. Fin whales might have started returning to the coastal waters off British Columbia (a sighting occurred in Johnstone Strait in 2011) and Kodiak Island. Size of the local population migrating to Hawaiian Archipelago is unknown. Historically, several other wintering grounds were scattered in the North Pacific in the past, such as off the Northern Mariana Islands, Bonin Islands, and Ryukyu Islands (for other possible habitats, see blue whale as their habitat preferences may correspond). There was a sighting of 3 animals nearby Borneo and Palawan in 1999.\n\nFor Asian stocks, resident groups may exist in the Yellow Sea and East China Sea, and the Sea of Japan (though these populations are critically endangered and the population off China, Korea, and Japan are either near extinction or in very small numbers). Very small increases in sightings have been confirmed off Shiretoko Peninsula, Abashiri, and Kushiro in Hokkaido, Tsushima, Sado Island, off Maiduru in the Sea of Japan since in late 2000s as whales in Sea of Okhotsk might have started recolonizing into former habitats (for coastal Sakhalin, as well). Whales possibly used to migrated into Seto Inland Sea.\n\nStudies of historical catches suggest several resident groups once existed in the North Pacific—the Baja California group and the Yellow Sea–East China Sea (including Ryukyu Islands and western Kyusyu) group. Additionally, respective groups in northern Sea of Japan and the group along Pacific coasts of Japan from Hokkaido to Sanriku might have been resident or less migratory, as well. The only modern record among Ryukyu Islands was of a rotten carcass beached on Ishigaki Island in 2005. Regarding Yellow Sea, a juvenile was accidentally killed along Boryeong in 2014.\n\nThere had been congregation areas among Sea of Japan to Yellow Sea such as in East Korea Bay, along eastern coasts of Korean Peninsula, and Ulleungdo.\n\nModern sightings around the Commander Islands have been annual but not in great numbers, and whales likely to migrate through the areas rather than summering, and possible mixing of western and eastern populations are expected to occur in this waters.\n\nVery little information has been revealed about the ecology of current migration from Antarctic waters are unknown, but small increases in sighting rates are confirmed off New Zealand, such as off Kaikoura, and wintering grounds may exist in further north such as in Papua New Guinea, Fiji, and off East Timor. Confirmations in Rarotonga have been increased recently where interactions with humpback whales occur on occasions. Finbacks are also relatively abundant along the coast of Peru and Chile (in Chile, most notably off Los Lagos region such as Gulf of Corcovado in Chiloé National Park, , port of Mejillones, and Caleta Zorra. Year-round confirmations indicate possible residents off pelagic north eastern to central Chile such as around coastal and Pingüino de Humboldt National Reserve, east of Juan Fernández Islands, and northeast of Easter Island and possible wintering ground exist for eastern south Pacific population. They are known to make mixed groups with other rorquals such as blue whales and sei whales. Their recovery is confirmed vicinity to various subantarctic islands such as South Georgia and Falkland, but unknown in other historical habitats including Campbell Island, Kermadec to Chatham Islands, Tristan da Cunha, and Gough Island.\n\nAmong Northern Indian Ocean and Bay of Bengal, such as along Sri Lanka, India, and Malaysia, sightings and older records of fin whales exist.\n\nRelatively little is known about the historical and current population levels of the Southern fin whale. The IWC officially estimates that the Southern Hemisphere pre-whaling population was 400,000 whales and that the population in 1979 (at the cessation of Antarctic large scale whaling) was 85,200. Both the current and historical estimates should be considered as poor estimates because the methodology and data used in the study are known to be flawed. Other estimates cite current size to be between 15,000 (1983) and 38,000 (1997). As of 2006, there is no scientifically accepted estimate of current population or trends in abundance.\n\nThe only known predator of the fin whale is the killer whale, with at least 20 eyewitness and second-hand accounts of attack or harassment. They usually flee and offer little resistance to attack. Only a few confirmed fatalities have occurred. In October 2005, 16 killer whales attacked and killed a fin whale in the Canal de Ballenas, Gulf of California, after chasing it for about an hour. They fed on its sinking carcass for about 15 minutes before leaving the area. In June 2012, a pod of killer whales was seen in La Paz Bay, in the Gulf of California, chasing a fin whale for over an hour before finally killing it and feeding on its carcass. The whale bore numerous tooth rakes over its back and dorsal fin; several killer whales flanked it on either side, with one individual visible under water biting at its right lower jaw. In July 1908, a whaler reportedly saw two killer whales attack and kill a fin whale off western Greenland. In January 1984, seven were seen from the air circling, holding the flippers, and ramming a fin whale in the Gulf of California, but the observation ended at nightfall.\n\nThe fin whale is a filter-feeder, feeding on small schooling fish, squid and crustaceans including copepods and krill.\n\nIn the North Pacific, they feed on euphausiids in the genera \"Euphausia\", \"Thysanoessa\", and \"Nyctiphanes\", large copepods in the genus \"Neocalanus\", small schooling fish (e.g. the genera \"Engraulis\", \"Mallotus\", \"Clupea\", and \"Theragra\"), and squid. Based on stomach content analysis of over 19,500 fin whales caught by the Japanese whaling fleet in the North Pacific from 1952 to 1971, 64.1% contained only krill, 25.5% copepods, 5.0% fish, 3.4% krill and copepods and 1.7% squid. Nemoto (1959) analyzed the stomach contents of about 7500 fin whales caught in the northern North Pacific and Bering Sea from 1952 to 1958, found that they mainly preyed on euphausiids around the Aleutian Islands and in the Gulf of Alaska and schooling fish in the northern Bering Sea and off Kamchatka. In the northern Bering Sea (north of 58°N), their main prey species were capelin (\"Mallotus villosus\"), Alaska pollock (\"Theragra chalcogramma\") and Pacific herring (\"Clupea pallasii\"); they also consumed saffron cod (\"Eleginus gracilis\"). Arctic krill (\"Thysanoessa raschii\") was the only species of euphausiid found in the stomachs of fin whales in the northern Bering Sea. Off Kamchatka, they appeared to primarily feed on herring. They also took large quantities of the copepod \"Neocalanus cristatus\" around the Aleutian Islands and in Olyutorsky Bay off northeast Kamchatka, areas where the species was abundant. Five species of euphausiid (\"Euphausia pacifica\", \"Thysanoessa spinifera\", \"T. inermis\", \"T. raschii\", and \"T. longipes\") were the predominant prey around the Aleutian Islands and in the Gulf of Alaska. Prey varied by region in the Kuril Islands area, with euphausiids (\"T. longipes\", \"T. inermis\", and \"T. raschii\") and copepods (\"Neocalanus plumchrus\" and \"N. cristatus\") being the main prey in the northern area and Japanese flying squid (\"Todarodes pacificus pacificus\") and small schooling fish (e.g. Pacific saury, \"Cololabis saira\"; and Japanese anchovy, \"Engraulis japonicus\") dominating the diet in the southern area.\n\nOf the fin whale stomachs sampled off British Columbia between 1963 and 1967, euphausiids dominated the diet for four of the five years (82.3 to 100% of the diet), while copepods only formed a major portion of the diet in 1965 (35.7%). Miscellaneous fish, squid, and octopus played only a very minor part of the diet in two of the five years (3.6 to 4.8%). Fin whales caught off California between 1959 and 1970 fed on the pelagic euphausiid \"Euphausia pacifica\" (86% of sampled individuals), the more neritic euphausiid \"Thysanoessa spinifera\" (9%), and the northern anchovy (\"Engraulis mordax\") (7%); only trace amounts (<0.5% each) were found of Pacific saury (\"C. saira\") and juvenile rockfish (\"Sebastes jordani\"). In the Gulf of California, they have been observed feeding on swarms of the euphausiid \"Nyctiphanes simplex\".\n\nIn the North Atlantic, they prey on euphausiids in the genera \"Meganyctiphanes\", \"Thysanoessa\" and \"Nyctiphanes\" and small schooling fish (e.g. the genera \"Clupea\", \"Mallotus\", and \"Ammodytes\"). Of the 1,609 fin whale stomachs examined at the Hvalfjörður whaling station in southwestern Iceland from 1967 to 1989 (caught between June and September), 96% contained only krill, 2.5% krill and fish, 0.8% some fish remains, 0.7% capelin (\"M. villosus\"), and 0.1% sandeel (family Ammodytidae); a small proportion of (mainly juvenile) blue whiting (\"Micromesistius poutassou\") were also found. Of the krill sampled between 1979 and 1989, the vast majority (over 99%) was northern krill (\"Meganyctiphanes norvegica\"); only one stomach contained \"Thysanoessa longicaudata\". Off West Greenland, 75% of the fin whales caught between July and October had consumed krill (family Euphausiidae), 17% capelin (\"Mallotus\") and 8% sand lance (\"Ammodytes sp.\"). Off eastern Newfoundland, they chiefly feed on capelin, but also take small quantities of euphausiids (mostly \"T. raschii\" and \"T. inermis\"). In the Ligurian-Corsican-Provençal Basin in the Mediterranean Sea they make dives as deep as to feed on the euphausiid \"Meganyctiphanes norvegica\", while off the island of Lampedusa, between Tunisia and Sicily, they have been observed in mid-winter feeding on surface swarms of the small euphausiid \"Nyctiphanes couchi\".\n\nIn the Southern Hemisphere, they feed almost exclusively on euphausiids (mainly the genera \"Euphausia\" and \"Thysanoessa\"), as well as taking small amounts of amphipods (e.g. \"Themisto gaudichaudii\") and various species of fish. Of the more than 16,000 fin whales caught by the Japanese whaling fleet in the Southern Hemisphere between 1961 and 1965 that contained food in their stomachs, 99.4% fed on euphausiids, 0.5% on fish, and 0.1% on amphipods. In the Southern Ocean they mainly consume \"E. superba\".\n\nThe animal feeds by opening its jaws while swimming at some in one study, which causes it to engulf up to of water in one gulp. It then closes its jaws and pushes the water back out of its mouth through its baleen, which allows the water to leave while trapping the prey. An adult has between 262 and 473 baleen plates on each side of the mouth. Each plate is made of keratin that frays out into fine hairs on the ends inside the mouth near the tongue. Each plate can measure up to in length and in width.\n\nThe whale routinely dives to depths of more than where it executes an average of four \"lunges\", to accumulate krill. Each gulp provides the whale with approximately of food. One whale can consume up to of food a day, leading scientists to conclude that the whale spends about three hours a day feeding to meet its energy requirements, roughly the same as humans. If prey \"patches\" are not sufficiently dense, or are located too deep in the water, the whale has to spend a larger portion of its day searching for food. One hunting technique is to circle schools of fish at high speed, frightening the fish into a tight ball, then turning on its side before engulfing the massed prey.\n\nFin whales suffer from a number of pathological conditions. The parasitic copepod \"Pennella balaenopterae\"—usually found on the flank of fin whales—burrows into their blubber to feed on their blood, while the pseudo-stalked barnacle \"Xenobalanus globicipitis\" is generally found more often on the dorsal fin, pectoral fins, and flukes.\n\nOther barnacles found on fin whales include the acorn barnacle \"Coronula reginae\" and the stalked barnacle \"Conchoderma auritum\", which attaches to \"Coronula\" or the baleen. The harpacticid copepod \"Balaenophilus unisetus\" (heavy infestations of which have been found in fin whales caught off northwestern Spain) and the ciliate \"Haematophagus\" also infest the baleen, the former feeding on the baleen itself and the latter on red blood cells.\n\nThe remora \"Remora australis\" and occasionally the amphipod \"Cyamus balaenopterae\" can also be found on fin whales, both feeding on the skin. Infestations of the giant nematode \"Crassicauda boopis\" can cause inflammation of the renal arteries and potential kidney failure, while the smaller \"C. crassicauda\" infects the lower urinary tract.\n\nAn emaciated female fin whale, which stranded along the Belgian coast in 1997, was found to be infected with lesions of \"Morbillivirus\". In January 2011, a emaciated adult male fin whale stranded dead on the Tyrrhenian coastline of Italy was found to be infected with \"Morbillivirus\" and the protozoa \"Toxoplasma gondii\", as well as carrying heavy loads of organochlorine pollutants.\n\nIn the 19th century, the fin whale was occasionally hunted by open-boat whalers, but it was relatively safe, because it could easily outrun ships of the time and often sank when killed, making the pursuit a waste of time for whalers. However, the later introduction of steam-powered boats and harpoons that exploded on impact made it possible to kill and secure them along with blue and sei whales on an industrial scale. As other whale species became overhunted, the whaling industry turned to the still-abundant fin whale as a substitute. It was primarily hunted for its blubber, oil, and baleen. Around 704,000 fin whales were caught in Antarctic whaling operations alone between 1904 and 1975.\n\nThe introduction of factory ships with stern slipways in 1925 substantially increased the number of whales taken per year. In 1937–38 alone, over 29,000 fin whales were taken. From 1953–54 to 1961–62, the catch averaged over 30,000 per year. By 1962–63, sei whale catches began to increase as fin whales became scarce. By 1975–76, fewer than 1,000 fin whales were being caught each year. In the North Pacific, over 74,000 fin whales were caught between 1910 and 1975. Between 1910 and 1989, over 55,000 were caught in the North Atlantic. Coastal groups in northeast Asian waters, along with many other baleen species, were likely driven into serious perils or functional extinctions by industrial catches by Japan covering wide ranges of China and Korean EEZ within very short period in 20th century. Migrations of the species into Japanese EEZ and in East China Sea were likely to be exterminated relatively earlier, as the last catch records on Amami Oshima was between the 1910s and 1930s. After the cease of exploiting Asian stocks, Japan kept mass commercial and illegal hunts until 1975. Several thousand individuals were hunted from various stations mainly along coasts of Hokkaido, Sanriku, and the Gotō Islands.\n\nThe IWC prohibited hunting in the Southern Hemisphere in 1976. The Soviet Union engaged in the illegal killing of protected whale species in the North Pacific and Southern Hemisphere, over-reporting fin whale catches to cover up illegal takes of other species. In the North Pacific, they reported taking over 10,000 fin whales between 1961–79, while the true catch was less than 9,000. In the Southern Hemisphere, they reported taking nearly 53,000 between 1948 and 1973, when the true total was a little over 41,000. The fin whale was given full protection from commercial whaling by the IWC in the North Pacific in 1976, and in the North Atlantic in 1987, with small exceptions for aboriginal catches and catches for research purposes. All populations worldwide remain listed as endangered species by the US National Marine Fisheries Service and the International Conservation Union Red List. The fin whale is on Appendix 1 of CITES.\n\nThe IWC has set a quota of 19 fin whales per year for Greenland. Meat and other products from whales killed in these hunts are widely marketed within Greenland, but export is illegal. Iceland and Norway are not bound by the IWC's moratorium on commercial whaling because both countries filed objections to it.\n\nIn October 2006, Iceland's fisheries ministry authorized the hunting of 9 fin whales through August 2007. In 2009 and 2010, Iceland caught 125 and 148 fin whales, respectively. An Icelandic company, Hvalur, caught over a hundred fin whales in 2014, and exported a record quantity of 2071 tonnes in a single shipment in 2014. Since 2006, Hvalur has caught more than 500 fin whales and exported more than 5000 tonnes of whale meat to Japan.\n\nIn the Southern Hemisphere, Japan permits annual takes of 10 fin whales under its Antarctic Special Permit whaling program for the 2005–2006 and 2006–2007 seasons. The proposal for 2007–2008 and the subsequent 12 seasons allows taking 50 per year. While 10 fin whales were caught in the 2005–06 season and three in the 2006–07 season, none was caught in the 2007–2008 season. A single fin whale was caught in both the 2008–09 and 2009–10 seasons, two were taken in the 2010–11 season, and one was taken in the 2011–12 season.\n\nFin whales have been targets of illegal captures using harpoons for dolphin hunts or intentionally drive whales into nets.\n\nCollisions with ships are a major cause of mortality. In some areas, they cause a substantial portion of large whale strandings. Most serious injuries are caused by large, fast-moving ships over or near continental shelves.\n\nA 60-foot-long fin whale was found stuck on the bow of a container ship in New York harbor on 12 April 2014.\n\nShip collisions frequently occur in Tsushima Strait and result in damaging all of whales, passengers, and vessels, hence the Japanese Coast Guard has started visual recordings of large cetaceans in Tsushima Strait to inform operating vessels in the areas.\n\nSeveral fin whale skeletons are exhibited in North America. The Natural History Museum of Los Angeles County in Los Angeles, California has an exhibit entitled the \"Fin Whale Passage\", which displays a fin whale skeleton collected by former museum osteologist Eugene Fischer and field collector Howard Hill in 1926 from the Trinidad whaling station (1920–1926) in Humboldt County, northern California. A steel armature supports the skeleton, which is accompanied by sculpted flukes. Science North, a science museum in Greater Sudbury, Ontario, Canada, has a fin whale skeleton collected from Anticosti Island hanging from the fourth floor of its main building. The Grand Rapids Public Museum in Grand Rapids, Michigan contains a 76-foot-long skeleton in the Galleria section hanging above from the ceiling.\n\nSeveral fin whale skeletons are also exhibited in Europe. The Natural History Museum of Slovenia in Ljubljana, Slovenia, houses a female fin whale skeleton—the specimen had been found floating in the Gulf of Piran in the spring of 2003. The Hungarian Natural History Museum in Budapest, Hungary, displays a fin whale skeleton hanging near its main entrance which had been caught in the Atlantic Ocean in 1896 and purchased from Vienna in 1900. The Cambridge University Museum of Zoology, in Cambridge, United Kingdom, exhibits a nearly male fin whale skeleton, which had stranded at Pevensey, East Sussex, in November 1865.\n\nThe Otago Museum, in Dunedin, New Zealand, displays a fin whale skeleton, which had stranded on the beach at Nelson at the entrance of the Waimea River in 1882.\n\nFin whales are regularly encountered on whale-watching excursions worldwide. In the Southern California Bight, fin whales are encountered year-round, with the best sightings between November and March. They can even be seen from land (for example, from Point Vicente, Palos Verdes, where they can be seen lunge feeding at the surface only a half mile to a few miles offshore). They are regularly sighted in the summer and fall in the Gulf of St. Lawrence, the Gulf of Maine, the Bay of Fundy, the Bay of Biscay, Strait of Gibraltar, the Mediterranean. In southern Ireland, they are seen inshore from June to February, with peak sightings in November and December. Cruise ships en route to and from the Antarctic Peninsula sometimes encounter fin whales in the Drake Passage.\n\nThe fin whale is listed on both Appendix I and Appendix II of the Convention on the Conservation of Migratory Species of Wild Animals (CMS). It is listed on Appendix I as this species has been categorized as in danger of extinction throughout all or a significant proportion of its range and CMS Parties strive towards strictly protecting these animals, conserving or restoring the places where they live, mitigating obstacles to migration and controlling other factors that might endanger them.\n\nIt is listed on Appendix II as it has an unfavourable conservation status or would benefit significantly from international co-operation organised by tailored agreements. In addition, the fin whale is covered by the Agreement on the Conservation of Cetaceans in the Black Sea, Mediterranean Sea and Contiguous Atlantic Area (ACCOBAMS) and the Memorandum of Understanding for the Conservation of Cetaceans and Their Habitats in the Pacific Islands Region (Pacific Cetaceans MOU).\n\n\n"}
{"id": "17119527", "url": "https://en.wikipedia.org/wiki?curid=17119527", "title": "Fredrik Hagemann", "text": "Fredrik Hagemann\n\nFredrik Hagemann (born 4 March 1929) is a Norwegian geologist and bureaucrat\n\nHe was born in Andenes as the son of literary historian and literary critic Sonja Hagemann. He graduated with the cand.real. degree, and worked in the Norwegian Geological Survey from 1957 to 1966. responsible for petroleum issues (\"seksjonssjef\") in the Norwegian Ministry of Industry from 1966 to 1972 and then Director of the Norwegian Petroleum Directorate from 1972 to 1997. His successor Gunnar Berge had been appointed already in 1990, but waited several years to take the post; during this period Hagemann was the acting Petroleum Director.\n"}
{"id": "4344222", "url": "https://en.wikipedia.org/wiki?curid=4344222", "title": "Geiger tube telescope", "text": "Geiger tube telescope\n\nThe Geiger Tube Telescope is a scientific instrument that measures the intensities, energy spectra, and angular distribution of energetic electrons and protons in interplanetary space and near Jupiter and Saturn.\n\nOn Pioneer 10, the instrument used an array of seven miniature Geiger-Müller tubes, collectively known as a Geiger Tube Telescope (GTT). Each tube was a small gas-filled cylinder. When a charged particle passed through the gas, an electrical pulse was generated by the applied voltage. Individual pulses from five of the tubes and coincident pulses from three combinations of the seven tubes were transmitted. Protons of energy greater than 5 MeV and electrons with energies greater 40 keV were detected. \n\nOn Pioneer 11, one Geiger-Müller tube was replaced by a thin silicon wafer to detect protons in the specific energy range 0.61 to 3.41 MeV. Other minor changes were made to improve the characteristics of the detector system. \n\nThe trains of pulses were passed through quasi-logarithmic data processors and then to the radio telemetry system of the spacecraft. Angular distributions were measured as the spacecraft rotated. This telemetry data was transmitted to the earth by an 8 watt S band transmitter within the Pioneer probe at one of eight data rates (from 16 to 2048 bits per second).\n\n"}
{"id": "19606838", "url": "https://en.wikipedia.org/wiki?curid=19606838", "title": "Grazing-incidence small-angle scattering", "text": "Grazing-incidence small-angle scattering\n\nGrazing-incidence small-angle scattering (GISAS) is a scattering technique used to study nanostructured surfaces and thin films. The scattered probe is either photons (grazing-incidence small-angle X-ray scattering, GISAXS) or neutrons (grazing-incidence small-angle neutron scattering, GISANS). GISAS combines the accessible length scales of small-angle scattering (SAS: SAXS or SANS) and the surface sensitivity of grazing incidence diffraction (GID).\n\nA typical application of GISAS is the characterisation of self-assembly and self-organization on the nanoscale in thin films. Systems studied by GISAS include quantum dot arrays,\ngrowth instabilities formed during in-situ growth,\nself-organized nanostructures in thin films of block copolymers,\nsilica mesophases,\nand nanoparticles.\n\nGISAXS was introduced by Levine and Cohen to study the dewetting of gold deposited on a glass surface. The technique was further developed by Naudon and coworkers to study metal agglomerates on surfaces and in buried interfaces. With the advent of nanoscience other applications evolved quickly, first in hard matter such as the characterization of quantum dots on semiconductor surfaces and the in-situ characterization of metal deposits on oxide surfaces. This was soon to be followed by soft matter systems such as ultrathin polymer films, polymer blends, block copolymer films and other self-organized nanostructured thin films that have become indispensable for nanoscience and technology. Future challenges of GISAS may lie in biological applications, such as proteins, peptides, or viruses attached to surfaces or in lipid layers.\n\nAs a hybrid technique, GISAS combines concepts from transmission small-angle scattering (SAS), from grazing-incidence diffraction (GID), and from diffuse reflectometry. From SAS it uses the form factors and structure factors. From GID it uses the scattering geometry close to the critical angles of substrate and film, and the two-dimensional character of the scattering, giving rise to diffuse rods of scattering intensity perpendicular to the surface. With diffuse (off-specular) reflectometry it shares phenomena like the Yoneda/Vinyard peak at the critical angle of the sample, and the scattering theory, the distorted wave Born approximation (DWBA). However, while diffuse reflectivity remains confined to the incident plane (the plane given by the incident beam and the surface normal), GISAS explores the whole scattering from the surface in all directions, typically utilizing an area detector. Thus GISAS gains access to a wider range of lateral and vertical structures and, in particular, is sensitive to the morphology and preferential alignment of nanoscale objects at the surface or inside the thin film.\n\nAs a particular consequence of the DWBA, the refraction of x-rays or neutrons has to be always taken into account in the case of thin film studies, due to the fact that scattering angles are small, often less than 1 deg. The refraction correction applies to the perpendicular component of the scattering vector with respect to the substrate while the parallel component is unaffected. Thus parallel scattering can often be interpreted within the kinematic theory of SAS, while refractive corrections apply to the scattering along perpendicular cuts of the scattering image, for instance along a scattering rod.\n\nIn the interpretation of GISAS images some complication arises in the scattering from low-Z films e.g. organic materials on silicon wafers, when the incident angle is in between the critical angles of the film and the substrate. In this case, the reflected beam from the substrate has a similar strength as the incident beam and thus the scattering from the reflected beam from the film structure can give rise to a doubling of scattering features in the perpendicular direction. This as well as interference between the scattering from the direct and the reflected beam can be fully accounted for by the DWBA scattering theory.\n\nThese complications are often more than offset by the fact that the dynamic enhancement of the scattering intensity is significant. In combination with the straightforward scattering geometry, where all relevant information is contained in a single scattering image, in-situ and real-time experiments are facilitated. Specifically self-organization during MBE growth and re-organization processes in block copolymer films under the influence of solvent vapor have been characterized on the relevant timescales ranging from seconds to minutes. Ultimately the time resolution is limited by the x-ray flux on the samples necessary to collect an image and the read-out time of the area detector.\n\nDedicated or partially dedicated GISAXS beamlines exist at many synchrotron light sources (for instance SSRL, APS, CHESS, ESRF, HASYLAB, NSLS, Pohang Light Source) and also Advanced Light Source at LBNL.\n\nAt neutron research facilities,\nGISANS is increasingly used,\ntypically on small-angle (SANS) instruments or on reflectometers.\n\nGISAS does not require any specific sample preparation other than thin film deposition techniques. Film thicknesses may range from a few nm to several 100 nm, and such thin films are still fully penetrated by the x-ray beam. The film surface, the film interior, as well as the substrate-film interface are all accessible. By varying the incidence angle the various contributions can be identified.\n\n"}
{"id": "2679981", "url": "https://en.wikipedia.org/wiki?curid=2679981", "title": "Image file formats", "text": "Image file formats\n\nImage file formats are standardized means of organizing and storing digital images. Image files are composed of digital data in one of these formats that can be rasterized for use on a computer display or printer. An image file format may store data in uncompressed, compressed, or vector formats. Once rasterized, an image becomes a grid of pixels, each of which has a number of bits to designate its color equal to the color depth of the device displaying it.\n\nThe size of raster image files is positively correlated with the number of pixels in the image and the color depth (bits per pixel). Images can be compressed in various ways, however. A compression algorithm stores either an exact representation or an approximation of the original image in a smaller number of bytes that can be expanded back to its uncompressed form with a corresponding decompression algorithm. Images with the same number of pixels and color depth can have very different compressed file size. Considering exactly the same compression, number of pixels, and color depth for two images, different graphical complexity of the original images may also result in very different file sizes after compression due to the nature of compression algorithms. With some compression formats, images that are less complex may result in smaller compressed file sizes. This characteristic sometimes results in a smaller file size for some lossless formats than lossy formats. For example, graphically simple images (i.e. images with large continuous regions like line art or animation sequences) may be losslessly compressed into a GIF or PNG format and result in a smaller file size than a lossy JPEG format.\n\nFor example, a 640 * 480 pixel image with 24-bit color would occupy almost a megabyte of space:\n\n640 * 480 * 24 = 7,372,800 bits  = 921,600 bytes = 900 kB\n\nWith vector images the file size increases only with the addition of more vectors.\n\nThere are two types of image file compression algorithms: lossless and lossy.\n\nLossless compression algorithms reduce file size while preserving a perfect copy of the original uncompressed image. Lossless compression generally, but not always, results in larger files than lossy compression. Lossless compression should be used to avoid accumulating stages of re-compression when editing images.\n\nLossy compression algorithms preserve a representation of the original uncompressed image that may appear to be a perfect copy, but it is not a perfect copy. Often lossy compression is able to achieve smaller file sizes than lossless compression. Most lossy compression algorithms allow for variable compression that trades image quality for file size.\n\nIncluding proprietary types, there are hundreds of image file types. The PNG, JPEG, and GIF formats are most often used to display images on the Internet. Some of these graphic formats are listed and briefly described below, separated into the two main families of graphics: raster and vector.\n\nIn addition to straight image formats, Metafile formats are portable formats which can include both raster and vector information. Examples are application-independent formats such as WMF and EMF. The metafile format is an intermediate format. Most applications open metafiles and then save them in their own native format. Page description language refers to formats used to describe the layout of a printed page containing text, objects and images. Examples are PostScript, PDF and PCL.\n\nJPEG (Joint Photographic Experts Group) is a lossy compression method; JPEG-compressed images are usually stored in the JFIF (JPEG File Interchange Format) file format. The JPEG/JFIF filename extension is JPG or JPEG. Nearly every digital camera can save images in the JPEG/JFIF format, which supports eight-bit grayscale images and 24-bit color images (eight bits each for red, green, and blue). JPEG applies lossy compression to images, which can result in a significant reduction of the file size. Applications can determine the degree of compression to apply, and the amount of compression affects the visual quality of the result. When not too great, the compression does not noticeably affect or detract from the image's quality, but JPEG files suffer generational degradation when repeatedly edited and saved. (JPEG also provides lossless image storage, but the lossless version is not widely supported.)\n\nJPEG 2000 is a compression standard enabling both lossless and lossy storage. The compression methods used are different from the ones in standard JFIF/JPEG; they improve quality and compression ratios, but also require more computational power to process. JPEG 2000 also adds features that are missing in JPEG. It is not nearly as common as JPEG, but it is used currently in professional movie editing and distribution (some digital cinemas, for example, use JPEG 2000 for individual movie frames).\n\nThe Exif (Exchangeable image file format) format is a file standard similar to the JFIF format with TIFF extensions; it is incorporated in the JPEG-writing software used in most cameras. Its purpose is to record and to standardize the exchange of images with image metadata between digital cameras and editing and viewing software. The metadata are recorded for individual images and include such things as camera settings, time and date, shutter speed, exposure, image size, compression, name of camera, color information. When images are viewed or edited by image editing software, all of this image information can be displayed.\n\nThe actual Exif metadata as such may be carried within different host formats, e.g. TIFF, JFIF (JPEG) or PNG. IFF-META is another example.\n\nThe TIFF (Tagged Image File Format) format is a flexible format that normally saves eight bits or sixteen bits per color (red, green, blue) for 24-bit and 48-bit totals, respectively, usually using either the TIFF or TIF filename extension. The tagged structure was designed to be easily extendible, and many vendors have introduced proprietary special-purpose tags – with the result that no one reader handles every flavor of TIFF file. TIFFs can be lossy or lossless, depending on the technique chosen for storing the pixel data. Some offer relatively good lossless compression for bi-level (black&white) images. Some digital cameras can save images in TIFF format, using the LZW compression algorithm for lossless storage. TIFF image format is not widely supported by web browsers. TIFF remains widely accepted as a photograph file standard in the printing business. TIFF can handle device-specific color spaces, such as the CMYK defined by a particular set of printing press inks. OCR (Optical Character Recognition) software packages commonly generate some form of TIFF image (often monochromatic) for scanned text pages.\n\nGIF (Graphics Interchange Format) is in normal use limited to an 8-bit palette, or 256 colors (while 24-bit color depth is technically possible). GIF is most suitable for storing graphics with few colors, such as simple diagrams, shapes, logos, and cartoon style images, as it uses LZW lossless compression, which is more effective when large areas have a single color, and less effective for photographic or dithered images. Due to GIF's simplicity and age, it achieved almost universal software support. Due to its animation capabilities, it is still widely used to provide image animation effects, despite its low compression ratio compared to modern video formats.\n\nThe BMP file format (Windows bitmap) handles graphic files within the Microsoft Windows OS. Typically, BMP files are uncompressed, and therefore large and lossless; their advantage is their simple structure and wide acceptance in Windows programs.\n\nThe PNG (Portable Network Graphics) file format was created as a free, open-source alternative to GIF. The PNG file format supports eight-bit paletted images (with optional transparency for all palette colors) and 24-bit truecolor (16 million colors) or 48-bit truecolor with and without alpha channel - while GIF supports only 256 colors and a single transparent color. \n\nCompared to JPEG, PNG excels when the image has large, uniformly colored areas. Even for photographs – where JPEG is often the choice for final distribution since its compression technique typically yields smaller file sizes – PNG is still well-suited to storing images during the editing process because of its lossless compression.\n\nPNG provides a patent-free replacement for GIF (though GIF is itself now patent-free), and can also replace many common uses of TIFF. Indexed-color, grayscale, and truecolor images are supported, plus an optional alpha channel. The Adam7 interlacing allows an early preview, even when only a small percentage of the image data has been transmitted. PNG can store gamma and chromaticity data for improved color matching on heterogeneous platforms.\n\nPNG is designed to work well in online viewing applications like web browsers and can be fully streamed with a progressive display option. PNG is robust, providing both full file integrity checking and simple detection of common transmission errors. \n\nAnimated formats derived from PNG are MNG and APNG. The latter is supported by Mozilla Firefox and Opera and is backwards compatible with PNG.\n\nNetpbm format is a family including the portable pixmap file format (PPM), the portable graymap file format (PGM) and the portable bitmap file format (PBM). These are either pure ASCII files or raw binary files with an ASCII header that provide very basic functionality and serve as a lowest common denominator for converting pixmap, graymap, or bitmap files between different platforms. Several applications refer to them collectively as PNM (Portable aNy Map).\n\nWebP is a new open image format that uses both lossless and lossy compression. It was designed by Google to reduce image file size to speed up web page loading: its principal purpose is to supersede JPEG as the primary format for photographs on the web. WebP is based on VP8's intra-frame coding and uses a container based on RIFF.\n\nMost typical raster formats cannot store HDR data (32 bit floating point values per pixel component), which is why some relatively old or complex formats are still predominant here, and worth mentioning separately. Newer alternatives are showing up, though. RGBE is the format for HDR images originating from Radiance and also supported by Adobe Photoshop. JPEG-HDR is a file format from Dolby Labs similar to RGBE encoding, standardized as JPEG XT Part 2. \n\nJPEG XT Part 7 includes support for encoding floating point HDR images in the base 8-bit JPEG file using enhancement layers encoded with four profiles (A-D); Profile A is based on the RGBE format and Profile B on the XDepth format from Trellis Management.\n\nThe High Efficiency Image File Format (HEIF) is an image container format that was standardized by MPEG on the basis of the ISO base media file format. While HEIF can be used with any image compression format, the HEIF standard specifies the storage of HEVC intra-coded images and HEVC-coded image sequences taking advantage of inter-picture prediction.\n\nBAT was released into the public domain by C-Cube Microsystems. The \"official\" file format for JPEG files is SPIFF (Still Picture Interchange File Format), but by the time it was released, BAT had already achieved wide acceptance. SPIFF, which has the ISO designation 10918-3, offers more versatile compression, color management, and metadata capacity than JPEG/BAT, but it has little support. It may be superseded by JPEG 2000/DIG 2000: ISO SC29/WG1, JPEG - Information Links. Digital Imaging Group, \"JPEG 2000 and the DIG: The Picture of Compatibility.\"\n\nBPG (Better Portable Graphics) is a new image format. Its purpose is to replace the JPEG image format when quality or file size is an issue. Its main advantages are:\n\n\nThese image formats contain various images, layers and objects, out of which the final image is to be composed\n\nAs opposed to the raster image formats above (where the data describes the characteristics of each individual pixel), vector image formats contain a geometric description which can be rendered smoothly at any desired display size.\n\nAt some point, all vector graphics must be rasterized in order to be displayed on digital monitors. Vector images may also be displayed with analog CRT technology such as that used in some electronic test equipment, medical monitors, radar displays, laser shows and early video games. Plotters are printers that use vector data rather than pixel data to draw graphics.\n\nCGM (Computer Graphics Metafile) is a file format for 2D vector graphics, raster graphics, and text, and is defined by ISO/IEC 8632. All graphical elements can be specified in a textual source file that can be compiled into a binary file or one of two text representations. CGM provides a means of graphics data interchange for computer representation of 2D graphical information independent from any particular application, system, platform, or device.\nIt has been adopted to some extent in the areas of technical illustration and professional design, but has largely been superseded by formats such as SVG and DXF.\n\nThe Gerber format (aka Extended Gerber, RS-274X) was developed by Gerber Systems Corp., now Ucamco, and is a 2D bi-level image description format. It is the de facto standard format used by printed circuit board or PCB software. It is also widely used in other industries requiring high-precision 2D bi-level images.\n\nSVG (Scalable Vector Graphics) is an open standard created and developed by the World Wide Web Consortium to address the need (and attempts of several corporations) for a versatile, scriptable and all-purpose vector format for the web and otherwise. The SVG format does not have a compression scheme of its own, but due to the textual nature of XML, an SVG graphic can be compressed using a program such as gzip. Because of its scripting potential, SVG is a key component in web applications: interactive web pages that look and act like applications.\n\n\n\nThese are formats containing both pixel and vector data, possible other data, e.g. the interactive features of PDF.\n\n"}
{"id": "6524119", "url": "https://en.wikipedia.org/wiki?curid=6524119", "title": "James William McBain", "text": "James William McBain\n\nJames William McBain FRS (March 22, 1882 – March 12, 1953) was a Canadian chemist.\n\nHe gained a Master of Arts at Toronto University and a Doctor of Science at Heidelberg University.\n\nHe carried out pioneering work in the area of micelles at the University of Bristol. As early as 1913 he postulated the existence of \"colloidal ions\", now known as micelles, to explain the good electrolytic conductivity of sodium palmitate solutions. He was elected a Fellow of the Royal Society in May 1923 He won their Davy Medal in 1939.\n"}
{"id": "1618060", "url": "https://en.wikipedia.org/wiki?curid=1618060", "title": "John Curtis (entomologist)", "text": "John Curtis (entomologist)\n\nJohn Curtis (Born: Norwich, 3 September 1791: Died: 18 Belitha-villas, Islington, London: 6 October 1862) was an English entomologist and illustrator.\n\nJohn Curtis (1791-1862)\n\nIn John Curtis we have lost one the most successful cultivators of British zoology and entomology, one of the most accomplished delineators of insects, and one of the closest observers of the phenomena of insect life.\nMr. Curtis was born at Norwich, on 3 December 1791, and died on 6 October 1862.\n\nHis father having died before the son had reached his fourth year, his training developed upon his mother, whose lover of flowers had no doubt great influence in the developing of that love of nature which he very early manifested. It is related that his notice, as a child, having been attracted by the large hairy caterpillar of Arctia Caia, which, to his great astonishment and delight, was transformed, whilst under his care, into a beautiful moth, entomology at once became his ruling passion. About this time, also, he became acquainted with an older and well-informed youth, Richard Walker, afterwards B.D, F.L.S., and Fellow of Magdalen, and the author of ‘Flora Oxoniensis’, in company with whom numerous excursions were made in the marshy districts surrounds his native place. He thus became much interested in the insects inhabiting and found upon the aquatic plants collected by his friend. These pursuits, however, were interrupted by a severe and even dangerous attack of rheumatic fever. On his recovery he was sent to school at Norwich, where he was again fortunate in making the acquaintance of a youth named Henry Browne, whose mother possessed a collection of British Lepidoptera, the inspection of which still further increased the zeal of Curtis in his old pursuits. Whilst at school, he captured the rare Stauropus Fagi on the lime-trees surrounding the Cathedral Close – an insect then so rare as to be valued by collectors at the price of £5; and to the time of his death he preserved a specimen of the almost equally rare Heliothis dipsacea, which he had captured under his hat, on Mousehold Heath, near Norwich. At an early age, also, he manifested a great love for colouring small engravings, and making drawings of flowers and landscapes. At this period his circle of acquaintance was enlarged by the addition to it of Dr. (afterwards Sir) James Edward Smith, and of the family of Mr. Hooker, the father of Sir W. J. Hooker. The latter was at this time an ardent entomologist, and he was of great assistance to young Curtis in the naming of insects, and giving him rare and local species. An excursion to the fens of Horning was rewarded by the capture of Papilio Machaon and its lava on Selinum palustre, as well of Hypogymna dispar.\n\nWhen sixteen years of age, being obliged to choose a profession, he entered the office of a lawyer, although when there, dry legal technicalities were but little to his taste, and his desk probably contained more of natural history than of law. After two years thus occupied, Mr, Curtis became acquainted with Mr. Simon Wilkin, a wealthy land proprietor in Norfolk, who like himself, was passionately devoted to entomology. This gentleman, on reaching twenty-one, came to reside on his estate at Cossey Hall, where he invited Curtis to live with him as his companion. Here, with a well-stored library, a well-named collection of insects, and congenial associates, the two friends spent their time most happily; and an entomological society was formed, in which the names of the Revs. W. Kirby and J. Burrell, Messrs. Wilkin, Brightwell, Joseph Hooker, John Lindley, Joseph Sparshall, and ten or twelve more were enrolled a members, - Mr. Wilkin acting as President and Mr. Curtis as Secretary.\n\nMr. Wilkin having successfully studied Latreille’s ‘Genera Crustaceorum’, Curtis became so charmed with that naturalist’s system that he formed a resolution to describe and delineate all the genera, and thenceforward lost no opportunity of making dissections and drawings of the types of all he could obtain, or copies of figures of exotic genera from the most esteemed Continental works. To promote his object, he acquired the art of etching and engraving on copper, his first published essay being the plates for Kirby and Spence’s ‘Introduction to Entomology’. Of these plates five only, containing figures illustrating the different orders of insects, were at first published, in the first and second volumes of that work, the third and fourth of which did not appear till 1826. The latter two contained twenty-five plates, filled with details of the external and internal anatomy of insects, twenty of which were etched by Curtis and five by Henry Denny, the dissections having been for the most part made by Mr. Kirby.\n\nMr. Curtis, at this time, frequently made sketches from nature; and about 1816 he cultivated, more especially, a taste he had long entertained for the drawing of churches, fonts, and monuments, the views being coloured on the spot. A visit to Barham, the residence of the Rev. W. Kirby, led to his making the acquaintance of Mr. Spence and of Mr. W. S. MacLeay, friends who proved of the greatest utility to him. Here he also assisted Mr. Kirby in dissecting and illustrating the forms contained in the famous ‘Century of Insects,’ and in the descriptions of Mr. Robert Brown’s ‘Australian Insects,’ published by Mr. Kirby in the 12th volume of our Transactions (1818).\n\nIn 1819 he accompanied Mr. Kirby to London, where he was introduced to Sir Joseph Banks, who gave him the entrée to his library and soirées, at which all the élite of the scientific world of London used to assemble. He also became personally acquainted with Dr. Leach, Superintendent of the Zoological Collection of the Museum, their congenial tastes soon rendering them intimate friends, and leading Curtis to study the structure of shells and their inhabitants, in which pursuit Dr. Leach was so greatly interested that an arrangement was made to examine and dredge the whole coast of Scotland in search of Mollusca. This excursion however, was never carried out, Dr. Leach’s mind having broken down under the accumulated labours which he had heaped upon himself.\n\nThe battle of life now began in earnest; and having thus lost the advice and assistance of the first zoologists whom England has ever produced, Mr. Curtis (by the evidence of Mr. MacLeay and other friends) turned his attention to botanical drawing and engraving, which led to engagements with Dr. Sims, and introduced him to the Horticultural Society (of which his friend Dr. Lindley was Secretary), the Linnaen Society, &c. In 1822 he was elected a Fellow of this Society; and on 1 January 1824 appeared the first number of ‘British Entomology,’ \"being Illustrations and Description of the genera of Insects found in Great Britain and Ireland, containing coloured figures from nature of the most rare and beautiful species, and in manty instances of the plants upon which they are found.\" This great work extended to sixteen annual volumes, containing no less than 770 plates, occupied by what the unanimous consent of entomologists has pronounced to be the most exquisite figures of the kind ever produced. The work, as originally designed, was intended to embrace only a detailed description of the genus and of the species figured, accompanied with observations on the generic peculiarities of the group. This limited scope was proposed, partly, because it was known that the late J. Francis Stephens had long been engaged in preparing for publication a work on the species of British insects, the first number of which appeared on 1 May 1827. Entomologists, however, are but human; and it unfortunately happened that jealousies and ill-feeling soon arose between these two authors, which resulted, on the part of Mr. Curtis, in his introducing into his work, wherever possible, descriptions of each species of the different genera, or of lists of the species, at any rate when too numerous for description. The author’s attention, from this circumstance, was in some measure withdrawn from the generic to the specific details, and accordingly, to the end, continued to confine his detailed generic figures to the structure of the antennae and parts of the mouth, omitting all mention, in many instances, of particulars concerning other parts, which more profound entomologists have shown to possess generic or family value. The same spirit also led to the commencement of the publication of a second edition of the ‘British Entomology,’ in which detailed descriptions of known British species were intended to be given in the text; but of this second edition two parts only appeared.\n\nIn 1825, Mr. Curtis, in company with his friend Mr. Dale of Glanville’s Wooton, a most assiduous collector of British insects, made an entomological tour in Perthshire and the western isles of Scotland, returning by way of Edinburgh. In this tour they were successful in collecting many very rare insects, together with thirty species not previously known as British, as well numerous drawings of wild flowers for the illustration of Mr. Curtis’s great work.\n\nIn 1829 he published the first edition of a ‘Guide to an Arrangement of British Insects.’ \"printed on one side, for labelling cabinets, being a Catalogue of all the names species hitherto discovered in Great Britain and Ireland,\" a second edition of which useful work appeared in 1837. In the following year, in company with Messrs. Francis and Henry Walker, he visited France, proceeding along the western edge of Bordeaux, and thence to Fréjus. In this journey about 6000 specimens of insects were collected; but the great object of the party was to visit the gypsum-quarries at Aix in Provence, where fossil insects are frequently found, of which Mr. Curtis, in 1829, had already described in the ‘Edinburgh New Philosophical Journal,’ a number of species brought by Messrs. Murchison and Lyell.\n\nIn 1831 Mr. Curtis was elected Corresponding Member of the Royal Georgofili Society of Florence, and in the same year he published a \"Description of the Insects brought home by Commander James Clark Ross in his Second Voyage,\" forming part of the Appendix of Natural History.\nIn 1833, on the occasion of his reading a paper \"On the Structure of Insects\" before the Ashmolean Society of Oxford, he was elected an Honorary Member of that body; and in 1836 he received the same title from the Academy of Natural Sciences of Philadelphia.\n\nOn the completion of his ‘British Entomology,’ on 1 December 1839, Mr. Curtis sought for relaxation from the incessant application monthly required during the long space of sixteen years; but in 1841, his friend Dr. Lindley having commenced the ‘Gardeners’ Chronicle,’ Curtis undertook the entomological editorship, engaging to write articles on the insects injurious to gardeners and farmers, in a popular style, accompanied by figures on wood; and this task he continued to perform with unabating industry till 1847, when it was taken up by Mr. Westwood.\n\nHaving accumulated a large mass of materials relative to the economy of insects, and being invited by the Council of the Royal Agricultural Society of London to furnish reports upon the insects injurious to farm-crops, he visited Suffolk to consult the best farmers on the subject. These valuable reports, amounting to sixteen in number, were commenced in 1841, and concluded in 1857. They were published in the ‘Journal’ of the Society, each being illustrated by one or more plates containing figures of insects in their different stages, and have subsequently been collected together and published in a single volume, under the title of \"Farm Insects, being the Natural History and Economy on the Insects injurious to the field-crops of Great Britain and Ireland, and also those which infest barns and granaries, with suggestions for their destruction.\"\n\nIn 1843 Mr. Curtis made a tour in Italy, visiting Rome, Naples, and other principal cities of the south to enjoy the sight of the architectural and other artistic treasures of which he had read much when studying painting in his early life. In 1844 he left London, and went to reside at Hayes, near Uxbridge, where he occupied himself for five years studying the economy of noxious insects in the field and garden.\n\nIn 1849 he was elected a Corresponding Member of the National History Society of Nuremberg; and in 1855 he was chosen an Honorary Member of the Entomological Society of Paris, of which he had, however, been an ordinary Member since 1834. In the autumn and winter of 1850 he visited Nice, Genoa, Turin, and the North of Italy, returning by the Tyrol and Switzerland; and in the latter part of 1851, he visited Pau, the various cities along the Mediterranean, Venice, Florence, Lombardy, Switzerland, and France, continuing, nevertheless, to furnish communications on entomological subjects to various publications, and, amongst these, one to the ‘Linnean Transactions,’ in 1852, \"On the Economy of a New Species of Saw-fly (Selandria Robinsoni), the lavae of which feed upon Convallaria multiflora,\" and a \"Notice regarding a Weevil of the vine and its Parasite (Rhynchites Betuleti),\" in the ‘Proceedings’ for 1853; \"On the genus Myrmica and other indigenous Ants,\" in the ‘Linnaen Transactions,’ 1854, &c.\n\nThe publication of so national a work as the ‘British Entomology,’ together with the great practical utility of his numerous memoires on economic entomology, fully justified the grant of an annuity of £100 which was conferred some years since upon Mr. Curtis, and subsequently augmented by an additional £50 on the occurrence of a sad event which took place shortly after the first grant of the pension, namely, the total loss of sight, induced, it is supposed, by the overstraining of the eyes in the execution of his numerous and laborious works.\nResigned to this great misfortune, Mr. Curtis retired from scientific life, in which for forty years he taken such an active part; and soon afterwards his friends were grieved to learn that he was suffering from the severe illness which terminated in his decease last year.\nThe collection of British insects formed by Mr. Curtis is of great value, from its extent and the number of original types in all the different orders which it contains. It is also a model of the greatest neatness and order. This, in fact, was one of the great peculiarities of Mr. Curtis, and pervaded all he did and all he possessed – his library being in the choicest condition, and his drawings most carefully finished. A little anecdote communicated to Mr. Westwood by Mr. Frederick Smith well illustrates this peculiarity. Mr. Smith was employed to engrave some of the plates of the ‘British Entomology’ and of the memoires on ‘Farm Insects;’ and on one occasion when one of his plates was taken by Mr. Smith to be approved, Mr. Curtis, having carefully examined the impression for a considerable time, at last turned to Mr. Smith and said, \"Sir, you have only put twelve hairs upon this fly’s tail instead of thirteen!\" This complaint, indicating so great a perception of precision, had such a droll effect, that the workmen could not resist a hearty laugh.\n\nPersonally, Mr. Curtis was very reserved in his communication with such of his brother entomologists as were known to be engaged in works intended for publication; but to those with whom he held unreserved intercourse his manners were engaging and kind. To use the words, in a letter to Mr. Westwood, of Mr. Halliday, who had enjoyed an uninterrupted friendship with Mr. Curtis of more than thirty years, \"he was indeed very loveable, warm-hearted (too much so perhaps for his own tranquillity), pure-minded, and honourable.\"\n\nCurtis was a lifelong friend of the Irish entomologist Alexander Henry Haliday and of the London entomologist Francis Walker. Curtis met Haliday in December 1827, (following an exchange of letters and specimens) Curtis’s second child was named Henry Alexander and Haliday was his godfather.\n\n\" \"I was delighted to possess Ceraphron Halidayii first because I had named it after you... it is very essential to possess those insects I figure: the female of Scatophaga also was a most valuable addition. Tipula dispar I only had the male of, I never could understand the female but thought it had been killed before the wings were fully expanded, never having taken it myself and I need scarcely say there was not an insect you sent me that was not fully acceptable ... I will put into the box some British Ichneumonidae hoping you will do me the favour at your leisure to append to them their Generic names and if you know them the specific also but not to take any trouble about it and whenever there are 2 alike I beg you will take one if desirable. Pray do me the favour to answer the different questions in this letter as I have no copy or memorandum.I shall hope to hear shortly from you and sincerely wishing you in a good old English Phrase a Merry Christmas and Happy New Year.Yours most faithfully, John Curtis\" Curtis to Haliday 22 December 1832.\n\n\"To Alexander Henry Haliday, Esq., M.A., &c, of Belfast, whose extensive knowledge and munificent contributions, have so greatly enriched this work and whose kindness and friendship in its progress have been an uninterrupted source of gratification, to the author, this volume\" (British Entomology VII Homoptera. Hemiptera. Aphaniptera) \"is dedicated as a token of sincere regard\". London 1 December 1837.\n\n\"It has for several years been my wish to pay you the only public testimony in my power of my regard by dedicating a volumne of my work to you. The many and essential services you have rendered that work during its progress would entitle you to such a compliment were you only a correspondent and the numerous proofs I had of your kindness and friend-ship make me only regret that it will not be better with your acceptance. I assure you one of the greatest pleasures in the progress of my great undertaking has been the associating my name with those whom I esteem and who like myself fare devote to the study of our branch of Natural History I may have only two more opportunities of thus gratifying myself and I shall be truly happy if they afford me the same unmixed pleasures as the present one does\"\" …Curtis to Haliday 2 December 1837 \n\nJohn Curtis’ insect collection is divided between the Natural History Museum,Dublin (via Trinity College,7,656 specimens purchased by Thomas Coulter) and Museums Victoria in Melbourne, Australia, which purchased the John Curtis Collection of British and Foreign Insects - comprising 38,031 specimens - for £567 in 1862. Museums Victoria also holds the Curtis Agricultural Insect Collection, which documents British agricultural pest insects.\n\n\n"}
{"id": "6098774", "url": "https://en.wikipedia.org/wiki?curid=6098774", "title": "LAMMPS", "text": "LAMMPS\n\nLarge-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS) is a molecular dynamics program from Sandia National Laboratories. LAMMPS makes use of Message Passing Interface (MPI) for parallel communication and is free and open-source software, distributed under the terms of the GNU General Public License.\n\nLAMMPS was originally developed under a Cooperative Research and Development Agreement (CRADA) between two laboratories from United States Department of Energy and three other laboratories from private sector firms. , it is maintained and distributed by researchers at the Sandia National Laboratories and Temple University.\n\nFor computing efficiency, LAMMPS uses neighbor lists (Verlet lists) to keep track of nearby particles. The lists are optimized for systems with particles that repel at short distances, so that the local density of particles never grows too large.\n\nOn parallel computers, LAMMPS uses spatial-decomposition techniques to partition the simulation domain into small 3d sub-domains, one of which is assigned to each processor. Processors communicate and store \"ghost\" atom information for atoms that border their subdomain. LAMMPS is most efficient (in a parallel computing sense) for systems whose particles fill a 3D rectangular box with approximately uniform density.\n\n"}
{"id": "15325190", "url": "https://en.wikipedia.org/wiki?curid=15325190", "title": "Life Science Library", "text": "Life Science Library\n\nThe Life Science Library is a series of hardbound books published by Time Life between 1963 and 1967. Each of the 26 volumes explores a major topic of the natural sciences. They are intended for, and written at a level appropriate to, an educated lay readership. In each volume, the text of each of eight chapters is followed by a \"Picture Essay\" lavishly illustrating the subject of the preceding chapter. They were available in a monthly subscription from \"Life\" magazine. Each volume takes complex scientific concepts and provides explanations that can be easily understood. For example, Einstein's theory of relativity is explained in a cartoon about a spy drama involving a train traveling very close to the speed of light; probability is explained with poker hands; and the periodic table of the elements is conveyed with common household items. Although progress has overtaken much of the material in the more than 50 years since their publication, the series' explanations of basic science and the history of discovery remain valid. The consulting editors of the series are microbiologist René Dubos, physicist Henry Margenau, and physicist and novelist C. P. Snow.\n\nEach volume was written by a primary author or authors, \"and the Editors of \"LIFE\"\". The volumes are:\n\n\n"}
{"id": "356904", "url": "https://en.wikipedia.org/wiki?curid=356904", "title": "List of craters on Ganymede", "text": "List of craters on Ganymede\n\nGanymede is the largest moon in the solar system, and thus has many craters covering its hard surface. Here is a list of Ganymedean craters that have been given names. Most are named after figures from Egyptian, Mesopotamian, and other ancient Middle Eastern myths.\n\n"}
{"id": "19958682", "url": "https://en.wikipedia.org/wiki?curid=19958682", "title": "List of cultural icons of Germany", "text": "List of cultural icons of Germany\n\nThis list of cultural icons of Germany is a list of people and things from any period which are independently considered to be cultural icons characteristic of Germany. .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "50551770", "url": "https://en.wikipedia.org/wiki?curid=50551770", "title": "List of films about philosophers", "text": "List of films about philosophers\n\nThis is a list of feature films that include philosophers, or in which philosophers play a significant role.\n\nBiographical films based on real-life philosophers:\n\nFilms where one or more philosopher play the main role, but that are not otherwise about philosophy:\n\nFilms where one or more of the members of the main cast are philosophers:\n\nFilms where philosophy is central to the plot:\n\n"}
{"id": "7851193", "url": "https://en.wikipedia.org/wiki?curid=7851193", "title": "List of government animal eradication programs", "text": "List of government animal eradication programs\n\nHistorically, there have been cases where the extermination of animal species has been politically endorsed because the animals have been considered harmful. In some cases the animals have been hunted because the animals present a danger to human lives, at other times they have been hunted because they are harmful to human interests such as livestock farming. More recently, eradication efforts have focused on invasive vertebrates as they are the leading cause of extinction of native species, particularly on islands. This article refers to animals in a more limited sense; it does not include humans.\n\nIn a number of cases, such government endorsed hunting has led to the endangerment or outright extinction of the species.\n\nAlberta, Canada is one of the few non-island places in the world that is free of brown rats. Since the early 1950s, the Government of Alberta has operated a rat-control program, which has been so successful that only isolated instances of wild rat sightings are reported, usually of rats arriving in the province aboard trucks or by rail.\n\nA number of islands have had programs to remove introduced rats and improve conditions for native fauna. Some of these islands include:\n\n\n\n\n\n\n"}
{"id": "33020995", "url": "https://en.wikipedia.org/wiki?curid=33020995", "title": "List of materials science journals", "text": "List of materials science journals\n\nThis is a list of scientific journals in materials science.\n\n"}
{"id": "14783718", "url": "https://en.wikipedia.org/wiki?curid=14783718", "title": "List of nature centers in the United States", "text": "List of nature centers in the United States\n\nThe following is a list of nature centers and environmental education centers in the United states.\n\n\n"}
{"id": "458558", "url": "https://en.wikipedia.org/wiki?curid=458558", "title": "List of scientific laws named after people", "text": "List of scientific laws named after people\n\nThis is a list of scientific laws named after people (eponymous laws). For other lists of eponyms, see eponym.\n\n"}
{"id": "30234767", "url": "https://en.wikipedia.org/wiki?curid=30234767", "title": "Living Earth Simulator Project", "text": "Living Earth Simulator Project\n\nThe living Earth simulator is a proposed massive computer simulation system intended to simulate the interactions of all aspects of life, human economic activity, climate, and other physical processes on the planet Earth as part of the FuturICT project, in response to the European FP7 \"Future and Emerging Technologies Flagship\" initiative.\n\nThere are over 300 international teams seeking ~€1 billion for the 10-year Future and Emerging Technologies ‘flagship’ competition. The Earth Simulator was not selected since the two winners have been announced as of March 2013. The winners were Graphene and Human Brain.\n\n\n"}
{"id": "4839372", "url": "https://en.wikipedia.org/wiki?curid=4839372", "title": "MaMF", "text": "MaMF\n\nMaMF, or Mammalian Motif Finder, is an algorithm for identifying motifs to which transcription factors bind.\n\nThe algorithm takes as input a set of promoter sequences, and a motif width(w), and as output, produces a ranked list of 30 predicted motifs(each motif is defined by a set of N sequences, where N is a parameter).\n\nThe algorithm firstly indexes each sub-sequence of length n, where n is a parameter around 4-6 base pairs, in each promoter, so they can be looked up efficiently. This index is then used to build a list of all pairs of sequences of length w, such that each sequence shares an n-mer, and each sequence forms an ungapped alignment with a substring of length w from the string of length 2w around the match, with a score exceeding a cut-off.\n\nThe pairs of sequences are then scored. The scoring function favours pairs which are very similar, but disfavours sequences which are very common in the target genome. The 1000 highest scoring pairs are kept, and the others are discarded. Each of these 1000 'seed' motifs are then used to search iteratively search for further sequences of length which maximise the score(a greedy algorithm), until N sequences for that motif are reached.\n\nVery similar motifs are discarded, and the 30 highest scoring motifs are returned as output.\n\n"}
{"id": "1010454", "url": "https://en.wikipedia.org/wiki?curid=1010454", "title": "Maldevelopment", "text": "Maldevelopment\n\nMaldevelopment is the state of an organism or an organisation that did not develop in the \"normal\" way (used in medicine, e.g. \"brain maldevelopment of a fetus\"). It was introduced as a human and social development term in France in the 1990s by Samir Amin to challenge the concept of \"underdevelopment.\" The word \"maldéveloppement\" did not exist before then (the medical terms are \"malformation\" or \"développement anormal\"), so the word is a neologism meant to be analogous to the difference between undernutrition and malnutrition.\n\nMaldevelopment is a global concept that includes human and social development. Under the philosophy of sustainable development, economic development is only a \"tool\" that allows for greater human and social development, not the final goal. \"Under\"-development is a quantitative notion, implying that a nation has a lack and must gain something to reach a particular reference state—the state of the nation that judges another nation as underdeveloped. So this notion also implies a unique development model—the one of the judging nation.\n\n\"Mal\"-development, or \"ill\"-development, is a qualitative notion that expresses a mismatch, a discrepancy between the conditions (economic, political, meteorological, cultural, etc.) and the needs and means of the people.\n\n"}
{"id": "24074617", "url": "https://en.wikipedia.org/wiki?curid=24074617", "title": "Max Poll", "text": "Max Poll\n\nMax Fernand Leon Poll (21 July 1908 – 13 March 1991) was a Belgian ichthyologist who specialised in the Cichlidae. In the years 1946 and 1947 he organised an expedition to Lake Tanganyika.\n\nHe has described several species of Pseudocrenilabrinae, such as \"Lamprologus signatus\", \"Steatocranus casuarius\", \"Neolamprologus brichardi\", and \"Neolamprologus pulcher\".\n\nNamed after him are species and taxa such as \"Etmopterus polli\" , \"Merluccius polli\" , \"Pollichthys\" , \"Polyipnus polli\" , \"Microsynodontis polli\" , and \"Synodontis polli\" .\n\nHe was a member of The Royal Academies for Science and the Arts of Belgium, professor at Université Libre de Bruxelles, and conservator at Musée Royal du Congo Belge in Tervuren.\nHe was an honorary member of the American Society of Ichthyologists and Herpetologists.\n"}
{"id": "1265734", "url": "https://en.wikipedia.org/wiki?curid=1265734", "title": "Middle age", "text": "Middle age\n\nMiddle age is the period of age beyond young adulthood but before the onset of old age.\n\nAccording to the \"Oxford English Dictionary\" middle age is between 45 and 65: \"The period between early adulthood and old age, usually considered as the years from about 45 to 65.\" The US Census lists the category middle age from 45 to 65. Merriam-Webster lists middle age from 45 to 64, while prominent psychologist Erik Erikson saw it starting a little earlier and defines middle adulthood as between 40 and 65. The \"Collins English Dictionary\" lists it between the ages of 40 and 60, and the \"Diagnostic and Statistical Manual of Mental Disorders\" – the standard diagnostic manual of the American Psychiatric Association – used to define middle age as 40 to 60, but as of \"DSM-IV (1994)\" revised the definition upwards to 45 to 65.\n\nThis time in the lifespan is considered to be the developmental stage of those who are between 18 years old and 40 years old. Recent developmental theories have recognized that development occurs across the entire life of a person as they experience changes cognitively, physically, socially, and in personality.\n\nThis time period in the life of a person can be referred to as middle age. This time span has been defined as the time between ages 45 and 65 years old. Many changes may occur between young adulthood and this stage.\nThe body may slow down and the middle aged might become more sensitive to diet, substance abuse, stress, and rest. Chronic health problems can become an issue along with disability or disease. Approximately one centimeter of height may be lost per decade. Emotional responses and retrospection vary from person to person. Experiencing a sense of mortality, sadness, or loss is common at this age.\n\nThose in middle adulthood or middle age continue to develop relationships and adapt to the changes in relationships. Changes can be the interacting with growing and grown children and aging parents. Community involvement is fairly typical of this stage of adulthood, as well as continued career development.\n\nMiddle-aged adults may begin to show visible signs of aging. This process can be more rapid in women who have osteoporosis. Changes might occur in the nervous system. The ability to perform complex tasks remains intact. Women experience menopause in the years surrounding the age of 50, which ends natural fertility. Menopause can have many side effects, some welcome and some not so welcome. Men may also experience physical changes. Changes can occur to skin and other changes may include decline in physical fitness including a reduction in aerobic performance and a decrease in maximal heart rate. These measurements are generalities and people may exhibit these changes at different rates and times.\n\nThe mortality rate can begin to increase from 45 and onwards, mainly due to health problems like heart problems, cancer, hypertension, and diabetes.\nStill, the majority of middle-aged people in industrialized nations can expect to live into old age.\n\nErik Erikson refers to this period of adulthood as the generatitivity-versus-stagnation stage. Persons in middle adulthood or middle age may have some cognitive loss. This loss usually remains unnoticeable because life experiences and strategies are developed to compensate for any decrease in mental abilities.\n\nMarital satisfaction remains but other family relationships can be more difficult. Career satisfaction focuses more on inner satisfaction and contentedness and less on ambition and the desire to \"advance\". Even so, career changes often can occur. Middle adulthood or middle age can be a time when a person re-examines their life by taking stock, and evaluating their accomplishments. Morality may change and become more conscious. The perception that those in this stage of development or life undergo a \"mid-life\" crisis is largely false. This period in life is usually satisfying, tranquil. Personality characteristics remain stable throughout this period. The relationships in middle adulthood may continue to evolve into connections that are stable.\n\n"}
{"id": "34678990", "url": "https://en.wikipedia.org/wiki?curid=34678990", "title": "Miles Aylmer Fulton Barnett", "text": "Miles Aylmer Fulton Barnett\n\nMiles Aylmer Fulton Barnett (30 April 1901 – 27 March 1979) was a New Zealand physicist and meteorologist. He was born in Dunedin, New Zealand on 30 April 1901.\n\n"}
{"id": "53889905", "url": "https://en.wikipedia.org/wiki?curid=53889905", "title": "Mycobacterium phage packman", "text": "Mycobacterium phage packman\n\nMycobacterium phage packman is a bacteriophage known to infect bacterial species of the genus \"Mycobacterium\". It is named after the famed arcade game character Pac-Man, from the game of the same name.\n"}
{"id": "31865376", "url": "https://en.wikipedia.org/wiki?curid=31865376", "title": "Physics of the Future", "text": "Physics of the Future\n\nPhysics of the Future: How Science Will Shape Human Destiny and Our Daily Lives by the Year 2100 is a 2011 book by theoretical physicist Michio Kaku, author of \"Hyperspace\" and \"Physics of the Impossible\". In it Kaku speculates about possible future technological development over the next 100 years. He interviews notable scientists about their fields of research and lays out his vision of coming developments in medicine, computing, artificial intelligence, nanotechnology, and energy production. The book was on the New York Times Bestseller List for five weeks.\n\nKaku writes how he hopes his predictions for 2100 will be as successful as science fiction writer Jules Verne's 1863 novel \"Paris in the Twentieth Century\". Kaku contrasts Verne's foresight against U.S. Postmaster General John Wanamaker, who in 1893 predicted that mail would still be delivered by stagecoach and horseback in 100 years' time, and IBM chairman Thomas J. Watson, who in 1943 is alleged to have said \"I think there is a world market for maybe five computers.\" Kaku points to this long history of failed predictions against progress to underscore his notion \"that it is very dangerous to bet against the future\".\n\nEach chapter is sorted into three sections: Near future (2000-2030), Midcentury (2030-2070), and Far future (2070-2100). Kaku notes that the time periods are only rough approximations, but show the general time frame for the various trends in the book.\n\nKaku begins with Moore's law, and compares a chip that sings \"Happy Birthday\" with the Allied forces in 1945, stating that the chip contains much more power, and that \"Hitler, Churchill, or Roosevelt might have killed to get that chip.\" He predicts that computer power will increase to the point where computers, like electricity, paper, and water, \"disappear into the fabric of our lives, and computer chips will be planted in the walls of buildings.\"\n\nHe also predicts that glasses and contact lenses will be connected to the internet, using similar technology to virtual retinal displays. Cars will become driverless due to the power of the GPS system. This prediction is supported by the results of the Urban Challenge. The Pentagon hopes to make of the United States ground forces automated by 2015. Technology similar to BrainGate will eventually allow humans to control computers with tiny brain sensors, and \"like a magician, move objects around with the power of our minds.\"\n\nKaku discusses robotic body parts, modular robots, unemployment caused by robots, surrogates and avatars (like their respective movies), and reverse engineering the brain. Kaku goes over the three laws of robotics and their contradictions. He endorses a \"chip in robot brains to automatically shut them off if they have murderous thoughts\", and believes that the most likely scenario is one in which robots are free to wreak havoc and destruction, but are designed to desire benevolence.\n\nKaku believes that in the future, reprogramming one's genes can be done by using a specially programmed virus, which can activate genes that slow the aging process. Nanotech sensors in a room will check for various diseases and cancer, nanobots will be able to inject drugs into individual cells when diseases are found, and advancements in extracting stem cells will be manifest in the art of growing new organs. The idea of resurrecting an extinct species might now be biologically possible.\n\nKaku discusses programmable matter, quantum computers, carbon nanotubes, and the possibility of replicators. He also expects a variety of nanodevices that search and destroy cancer cells cleanly, leaving normal cells intact.\n\nKaku discusses the draining of oil on the planet by pointing to the Hubbert curve, and the rising problem of immigrants who wish to live the American dream of wasteful energy consumption. He predicts that hydrogen and solar energy will be the future, noting how Henry Ford and Thomas Edison bet on whether oil or electricity would dominate, and describing fusion with lasers or magnetic fields, and dismisses cold fusion as \"a dead end\". Kaku suggests that nations are reluctant to deal with global warming because the extravagance of oil, being the cheapest source of energy, encourages economic growth. Kaku believes that in the far future, room-temperature superconductors will usher the era of magnet-powered floating cars and trains.\n\nUnlike conventional chemical rockets which use Newton's third law of motion, solar sails take advantage of radiation pressure from stars. Kaku believes that after sending a gigantic solar sail into orbit, one could install lasers on the moon, which would hit the sail and give it extra momentum.\n\nAnother alternative is to send thousands of nanoships, of which only a few would reach their destination. \"Once arriving on a nearby moon, they could create a factory to make unlimited copies of themselves,\" says Kaku. Nanoships would require very little fuel to accelerate. They could visit the stellar neighborhood by floating on the magnetic fields of other planets.\n\nKaku discusses how Moore's law robotics will affect the future of capitalism, which nations will survive and grow, how the United States is \"brain-draining\" off of immigrants to fuel their economy.\n\nKaku ranks the civilization of the future, with classifications based on energy consumption, entropy, and information processing. Kaku stated that humans with an average economic growth may attain planetary civilization status in 100 years, \"unless there is a natural catastrophe or some calamitous act of folly, it is inevitable that we will\nenter this phase of our collective history\".\n\nThe book got mixed reviews, and was generally regarded as having interesting insights that were delivered in an over-optimistic but bland style.\n\"Kirkus Reviews\" stated \"The author’s scientific expertise will engage readers too sophisticated for predictions based on psychic powers or astrology.\" Reviewers at \"Library Journal\" have stated, \"This work is highly recommended for fans of Kaku’s previous books and for readers interested in science and robotics.\"\nThe \"Wall Street Journal\" considers it a \"largely optimistic view of the future\". \"The Telegraph\" complained that \"[Physics of the Future] is partisan about technology in a way that smacks of Gerard K. O'Neill’s deliriously technocratic vision of space exploration, \"\".\" \"The Guardian\" stated \"despite the relentless technological optimism, Kaku does conjure up a genuinely exciting panorama of revolutionary science and magical technology\".\nThe \"New York Times\" complained the book's style is often \"dull and charmless\", but acknowledged it had the ability to \"enthrall and frighten\" as well. Writing in \"Physics Today\", physicist Neil Gershenfeld said that the book has “an appealing premise” but describes “a kind of future by committee” populated by “science-fiction staples”. Gershenfeld said, “Such a forecast could have been accomplished with less effort by collating covers from popular science magazines.” Gershenfeld criticizes Kaku for “some surprising physics errors”, such as ignoring air friction on maglev vehicles. Kaku is praised for raising “profound questions”, such as the effect of affluence in the future, or the decoupling of sensory experience from reality. However, Gershenfeld laments that these questions are asked in the margins and not given a deep treatment. “It would have been more relevant to learn the author’s perspective on these questions than to find out where and to whom he’s presented lectures,” Gershenfeld said. \"The Economist\" is skeptical about prediction in general pointing out that unforeseen \"unknown unknowns\" led to many disruptive technologies over the century just past.\n\n"}
{"id": "882736", "url": "https://en.wikipedia.org/wiki?curid=882736", "title": "Project Gemini", "text": "Project Gemini\n\nProject Gemini was NASA's second human spaceflight program. Conducted between projects Mercury and Apollo, Gemini started in 1961 and concluded in 1966. The Gemini spacecraft carried a two-astronaut crew. Ten Gemini crews flew low Earth orbit (LEO) missions during 1965 and 1966, putting the United States in the lead during the Cold War Space Race against the Soviet Union.\n\nGemini's objective was the development of space travel techniques to support the Apollo mission to land astronauts on the Moon. It performed missions long enough for a trip to the Moon and back, perfected working outside the spacecraft with extra-vehicular activity (EVA), and pioneered the orbital maneuvers necessary to achieve space rendezvous and docking. With these new techniques proven by Gemini, Apollo could pursue its prime mission without doing these fundamental exploratory operations.\n\nAll Gemini flights were launched from Launch Complex 19 (LC-19) at Cape Kennedy Air Force Station in Florida. Their launch vehicle was the Gemini–Titan II, a modified Intercontinental Ballistic Missile (ICBM). Gemini was the first program to use the newly built Mission Control Center at the Houston Manned Spacecraft Center for flight control.\n\nThe astronaut corps that supported Project Gemini included the \"Mercury Seven\", \"The New Nine\", and the 1963 astronaut class. During the program, three astronauts died in air crashes during training, including the prime crew for Gemini 9. This mission was flown by the backup crew, the only time that has happened in NASA's history to date.\n\nGemini was robust enough that the United States Air Force planned to use it for the Manned Orbital Laboratory (MOL) program, which was later canceled. Gemini's chief designer, Jim Chamberlin, also made detailed plans for cislunar and lunar landing missions in late 1961. He believed that Gemini spacecraft could fly in lunar operations before Project Apollo, and cost less. NASA's administration did not approve those plans. In 1969, McDonnell-Douglas proposed a \"Big Gemini\" that could have been used to shuttle up to 12 astronauts to the planned space stations in the Apollo Applications Project (AAP). The only AAP project funded was Skylab – which used existing spacecraft and hardware – thereby eliminating the need for Big Gemini.\n\nThe constellation for which the project was named is commonly pronounced , the last syllable rhyming with \"eye\". However, staff of the Manned Spacecraft Center, including the astronauts, tended to pronounce the name , rhyming with \"knee\". NASA's public affairs office issued a statement in 1965 declaring \"Jeh-mih-nee\" to be the \"official\" pronunciation. Gus Grissom, acting as Houston capsule communicator when Ed White performed his spacewalk on Gemini 4, is heard on flight recordings pronouncing the spacecraft's call sign \"Jeh-mih-nee 4\", and the NASA pronunciation is used in the movie \"First Man\".\n\nThe Apollo program was conceived in early 1960 as a three-man spacecraft to follow Project Mercury. Jim Chamberlin, the head of engineering at the Space Task Group (STG), was assigned in February 1961 to start working on a bridge program between Mercury and Apollo. He presented two initial versions of a two-man spacecraft, then designated Mercury Mark II, at a NASA retreat at Wallops Island in March 1961. Scale models were shown in July 1961 at the McDonnell Aircraft Corporation's offices in St. Louis.\n\nAfter Apollo was chartered to land men on the Moon by President John F. Kennedy on May 25, 1961, it became evident to NASA officials that a follow-on to the Mercury program was required to develop certain spaceflight capabilities in support of Apollo. NASA approved the two-man program rechristened Project Gemini (Latin for \"twins\"), in reference to the third constellation of the Zodiac with its twin stars Castor and Pollux, on December 7, 1961. McDonnell Aircraft was contracted to build it on December 22, 1961. The program was publicly announced on January 3, 1962, with these major objectives:\n\nThe Canadian Jim Chamberlin designed the Gemini capsule, which carried a crew of two. He was previously the chief aerodynamicist on Avro Canada's Avro Arrow fighter interceptor program. Chamberlin joined NASA along with 25 senior Avro engineers after cancellation of the Arrow program, and became head of the U.S. Space Task Group's engineering division in charge of Gemini. The prime contractor was McDonnell Aircraft Corporation, which was also the prime contractor for the Project Mercury capsule.\n\nAstronaut Gus Grissom was heavily involved in the development and design of the Gemini spacecraft (the other Mercury astronauts dubbed the Gemini spacecraft the \"Gusmobile\"). Grissom wrote in his posthumous 1968 book \"Gemini!\" that the realization of Project Mercury's end and the unlikelihood of his having another flight in that program prompted him to focus all of his efforts on the upcoming Gemini program.\n\nThe Gemini program was managed by the Manned Spacecraft Center, located in Houston, Texas, under direction of the Office of Manned Space Flight, NASA Headquarters, Washington, D.C. Dr. George E. Mueller, Associate Administrator of NASA for Manned Space Flight, served as acting director of the Gemini program. William C. Schneider, Deputy Director of Manned Space Flight for Mission Operations, served as mission director on all Gemini flights beginning with Gemini 6A.\n\nGuenter Wendt was a McDonnell engineer who supervised launch preparations for both the Mercury and Gemini programs and would go on to do the same when the Apollo program launched crews. His team was responsible for completion of the complex pad close-out procedures just prior to spacecraft launch, and he was the last person the astronauts would see prior to closing the hatch. The astronauts appreciated his taking absolute authority over, and responsibility for, the condition of the spacecraft and developed a good-humored rapport with him.\n\nNASA selected McDonnell Aircraft, which had been the prime contractor for the Project Mercury capsule, in 1961 to build the Gemini capsule, the first of which was delivered in 1963. The spacecraft was long and wide, with a launch weight varying from .\n\nThe Gemini crew capsule (referred to as the Reentry Module) was essentially an enlarged version of the Mercury capsule. Unlike Mercury, the retrorockets, electrical power, propulsion systems, oxygen, and water were located in a detachable Adapter Module behind the Reentry Module. A major design improvement in Gemini was to locate all internal spacecraft systems in modular components, which could be independently tested and replaced when necessary, without removing or disturbing other already tested components.\n\nMany components in the capsule itself were reachable through their own small access doors. Unlike Mercury, Gemini used completely solid-state electronics, and its modular design made it easy to repair.\n\nGemini's emergency launch escape system did not use an escape tower powered by a solid-fuel rocket, but instead used aircraft-style ejection seats. The tower was heavy and complicated, and NASA engineers reasoned that they could do away with it as the Titan II's hypergolic propellants would burn immediately on contact. A Titan II booster explosion had a smaller blast effect and flame than on the cryogenically fueled Atlas and Saturn. Ejection seats were sufficient to separate the astronauts from a malfunctioning launch vehicle. At higher altitudes, where the ejection seats could not be used, the astronauts would return inside the spacecraft, which would separate from the launch vehicle.\n\nThe main proponent of using ejection seats was James Chamberlin, head of the engineering division of NASA's Space Force Task Group. Chamberlin had never liked the Mercury escape tower and wished to use a simpler alternative that would also reduce weight. He reviewed several films of Atlas and Titan II ICBM failures, which he used to estimate the approximate size of a fireball produced by an exploding launch vehicle and from this he gauged that the Titan II would produce a much smaller explosion, thus the spacecraft could get away with ejection seats.\n\nMaxime Faget, the designer of the Mercury LES, was on the other hand less-than-enthusiastic about this setup. Aside from the possibility of the ejection seats seriously injuring the astronauts, they would also only be usable for about 40 seconds after liftoff, by which point the booster would be attaining Mach 1 speed and ejection would no longer be possible. He was also concerned about the astronauts being launched through the Titan's exhaust plume if they ejected in-flight and later added that \"The best thing about Gemini was that they never had to make an escape.\"\n\nGemini was the first astronaut-carrying spacecraft to include an onboard computer, the Gemini Guidance Computer, to facilitate management and control of mission maneuvers. This computer, sometimes called the Gemini Spacecraft On-Board Computer (OBC), was very similar to the Saturn Launch Vehicle Digital Computer. The Gemini Guidance Computer weighed . Its core memory had 4096 addresses, each containing a 39-bit word composed of three 13-bit \"syllables\". All numeric data was 26-bit two's-complement integers (sometimes used as fixed-point numbers), either stored in the first two syllables of a word or in the accumulator. Instructions (always with a 4-bit opcode and 9 bits of operand) could go in any syllable.\n\nUnlike Mercury, the Gemini used in-flight radar and an artificial horizon—devices similar to those used in the aviation industry.\nThe original intention for Gemini was to land on solid ground instead of at sea, using a Rogallo wing rather than a parachute, with the crew seated upright controlling the forward motion of the craft. To facilitate this, the airfoil did not attach just to the nose of the craft, but to an additional attachment point for balance near the heat shield. This cord was covered by a strip of metal which ran between the twin hatches. This design was ultimately dropped, and parachutes were used to make a sea landing as in Mercury. The capsule was suspended at an angle closer to horizontal, so that a side of the heat shield contacted the water first. This eliminated the need for the landing bag cushion used in the Mercury capsule.\n\nThe adapter module in turn was separated into a Retro module and an Equipment module.\n\nThe Retro module contained four solid-fuel TE-M-385 Star-13E retrorockets, each spherical in shape except for its rocket nozzle, which were structurally attached to two beams that reached across the diameter of the retro module, crossing at right angles in the center. Re-entry began with the retrorockets firing one at a time. Abort procedures at certain periods during lift-off would cause them to fire at the same time, thrusting the Descent module away from the Titan rocket.\n\nGemini was equipped with an Orbit Attitude and Maneuvering System (OAMS), containing sixteen thrusters for translation control in all three perpendicular axes (forward/backward, left/right, up/down), in addition to attitude control (pitch, yaw, and roll angle orientation) as in Mercury. Translation control allowed changing orbital inclination and altitude, necessary to perform space rendezvous with other craft, and docking with the Agena Target Vehicle (ATV), with its own rocket engine which could be used to perform greater orbit changes.\n\nEarly short-duration missions had their electrical power supplied by batteries; later endurance missions used the first fuel cells in manned spacecraft.\n\nGemini was in some regards more advanced than Apollo because the latter program began almost a year earlier. It became known as a \"pilot's spacecraft\" due to its assortment of jet fighter-like features, in no small part due to Gus Grissom's influence over the design, and it was at this point where the American manned space program clearly began showing its superiority over that of the Soviet Union with long duration flight, rendezvous, and extravehicular capability. The Soviet Union during this period was developing the Soyuz spacecraft intended to take cosmonauts to the Moon, but political and technical problems began to get in the way, leading to the ultimate end of their manned lunar program.\n\nThe Titan II had debuted in 1962 as the Air Force's second-generation ICBM to replace the Atlas. By using hypergolic fuels, it could be stored for long periods of time and be easily readied for launch in addition to being a simpler design with fewer components, the only caveat being that the propellant mix (nitrogen tetroxide and hydrazine) was extremely toxic compared to the Atlas's liquid oxygen/RP-1. However, the Titan had considerable difficulty being man-rated due to early problems with pogo oscillation. The launch vehicle used a radio guidance system that was unique to launches from Cape Kennedy.\n\nDeke Slayton, as director of flight crew operations, had primary responsibility for assigning crews for the Gemini program. Each flight had a primary crew and backup crew, and the backup crew would rotate to primary crew status three flights later. Slayton intended for first choice of mission commands to be given to the four remaining active astronauts of the Mercury Seven: Alan Shepard, Grissom, Cooper, and Schirra. (John Glenn had retired from NASA in January 1964 and Scott Carpenter, who was blamed by some in NASA management for the problematic reentry of \"Aurora 7\", was on leave to participate in the Navy's SEALAB project and was grounded from flight in July 1964 due to an arm injury sustained in a motorbike accident. Slayton himself continued to be grounded due to a heart problem.)\n\nTitles used for the left-hand (command) and right-hand seat crew positions were taken from the U.S. Air Force pilot ratings, \"Command Pilot\" and \"Pilot\". Sixteen astronauts flew on 10 manned Gemini missions:\n\nIn late 1963, Slayton selected Shepard and Stafford for Gemini 3, McDivitt and White for Gemini 4, and Schirra and Young for Gemini 5 (which was to be the first Agena rendezvous mission). The backup crew for Gemini 3 was Grissom and Borman, who were also slated for Gemini 6, to be the first long-duration mission. Finally Conrad and Lovell were assigned as the backup crew for Gemini 4.\n\nDelays in the production of the Agena Target Vehicle caused the first rearrangement of the crew rotation. The Schirra and Young mission was bumped to Gemini 6 and they became the backup crew for Shepard and Stafford. Grissom and Borman then had their long-duration mission assigned to Gemini 5.\n\nThe second rearrangement occurred when Shepard developed Ménière's disease, an inner ear problem. Grissom was then moved to command Gemini 3. Slayton felt that Young was a better personality match with Grissom and switched Stafford and Young. Finally, Slayton tapped Cooper to command the long-duration Gemini 5. Again for reasons of compatibility, he moved Conrad from backup commander of Gemini 4 to pilot of Gemini 5, and Borman to backup command of Gemini 4. Finally he assigned Armstrong and Elliot See to be the backup crew for Gemini 5.\nThe third rearrangement of crew assignment occurred when Slayton felt that See wasn't up to the physical demands of EVA on Gemini 8. He reassigned See to be the prime commander of Gemini 9 and put Scott as pilot of Gemini 8 and Charles Bassett as the pilot of Gemini 9.\n\nThe fourth and final rearrangement of the Gemini crew assignment occurred after the deaths of See and Bassett when their trainer jet crashed, coincidentally into a McDonnell building which held their Gemini 9 capsule in St. Louis. The backup crew of Stafford and Cernan was then moved up to the new prime crew of the re-designated Gemini 9A. Lovell and Aldrin were moved from being the backup crew of Gemini 10 to be the backup crew of Gemini 9. This cleared the way through the crew rotation for Lovell and Aldrin to become the prime crew of Gemini 12.\n\nAlong with the deaths of Grissom, White, and Roger Chaffee in the fire of Apollo 1, this final arrangement helped determine the makeup of the first seven Apollo crews, and who would be in position for a chance to be the first to walk on the Moon.\n\nIn 1964 and 1965 two Gemini missions were flown without crews to test out systems and the heat shield. These were followed by ten flights with crews in 1965 and 1966. All were launched by Titan II launch vehicles. Some highlights from the Gemini program:\n\nRendezvous in orbit is not a straightforward maneuver. Should a spacecraft increase its speed to catch up with another, the result is that it goes into a higher and slower orbit and the distance thereby increases. The right procedure is to slow down and go to a lower orbit first, and then later to increase speed and go to the same orbit as the other. To practice these maneuvers special rendezvous and docking simulators were built for the astronauts.\n\nThe Gemini-Titan II launch vehicle was adapted by NASA from the U.S. Air Force Titan II ICBM. (Similarly, the Mercury-Atlas launch vehicle had been adapted from the USAF Atlas missile.) The Gemini-Titan II rockets were assigned Air Force serial numbers, which were painted in four places on each Titan II (on opposite sides on each of the first and second stages). USAF crews maintained Launch Complex 19 and prepared and launched all of the Gemini-Titan II launch vehicles. Data and experience operating the Titans was of value to both the U.S. Air Force and NASA.\n\nThe USAF serial numbers assigned to the Gemini-Titan launch vehicles are given in the tables above. Fifteen Titan IIs were ordered in 1962 so the serial is \"62-12XXX\", but only \"12XXX\" is painted on the Titan II. The order for the last three of the 15 launch vehicles was canceled on July 30, 1964, and they were never built. Serial numbers were, however, assigned to them prospectively: \"12568\" - GLV-13; \"12569\" - GLV-14; and \"12570\" - GLV-15.\n\nFrom 1962 to 1967, Gemini cost $1.3 billion in 1967 dollars ($ in ). In January 1969, a NASA report to the US Congress estimating the costs for Mercury, Gemini, and Apollo (through the first manned Moon landing) included $1.2834 billion for Gemini: $797.4 million for spacecraft, $409.8 million for launch vehicles, and $76.2 million for support.\n\n\n\nMcDonnell Aircraft, the main contractor for Mercury and Gemini, was also one of the original bidders on the prime contract for Apollo, but lost out to North American Aviation. McDonnell later sought to extend the Gemini program by proposing a derivative which could be used to fly a cislunar mission and even achieve a manned lunar landing earlier and at less cost than Apollo, but these proposals were rejected by NASA.\n\nA range of applications were considered for Advanced Gemini missions, including military flights, space station crew and logistics delivery, and lunar flights. The Lunar proposals ranged from reusing the docking systems developed for the Agena Target Vehicle on more powerful upper stages such as the Centaur, which could propel the spacecraft to the Moon, to complete modifications of the Gemini to enable it to land on the lunar surface. Its applications would have ranged from manned lunar flybys before Apollo was ready, to providing emergency shelters or rescue for stranded Apollo crews, or even replacing the Apollo program.\n\nSome of the Advanced Gemini proposals used \"off-the-shelf\" Gemini spacecraft, unmodified from the original program, while others featured modifications to allow the spacecraft to carry more crew, dock with space stations, visit the Moon, and perform other mission objectives. Other modifications considered included the addition of wings or a parasail to the spacecraft, in order to enable it to make a horizontal landing.\n\nBig Gemini (or \"Big G\") was another proposal by McDonnell Douglas made in August 1969. It was intended to provide large-capacity, all-purpose access to space, including missions that ultimately used Apollo or the Space Shuttle.\n\nThe study was performed to generate a preliminary definition of a logistic spacecraft derived from Gemini that would be used to resupply an orbiting space station. Land-landing at a preselected site and refurbishment and reuse were design requirements. Two baseline spacecraft were defined: a nine-man minimum modification version of the Gemini B called Min-Mod Big G and a 12-man advanced concept, having the same exterior geometry but with new, state-of-the-art subsystems, called Advanced Big G. Three launch vehicles-Saturn IB, Titan IIIM, and Saturn INT-20 (S-IC/S-IVB) were investigated for use with the spacecraft.\n\nThe Air Force had an interest in the Gemini system, and decided to use its own modification of the spacecraft as the crew vehicle for the Manned Orbital Laboratory. To this end, the Gemini 2 spacecraft was refurbished and flown again atop a mockup of the MOL, sent into space by a Titan IIIC. This was the first time a spacecraft went into space twice.\n\nThe USAF also had the notion of adapting the Gemini spacecraft for military applications, such as crude observation of the ground (no specialized reconnaissance camera could be carried) and practicing making rendezvous with suspicious satellites. This project was called Blue Gemini. The USAF did not like the fact that Gemini would have to be recovered by the US Navy, so they intended for Blue Gemini eventually to use the airfoil and land on three skids, carried over from the original design of Gemini.\n\nAt first some within NASA welcomed sharing of the cost with the USAF, but it was later agreed that NASA was better off operating Gemini by itself. Blue Gemini was canceled in 1963 by Secretary of Defense Robert McNamara, who decided that the NASA Gemini flights could conduct necessary military experiments. MOL was canceled by Secretary of Defense Melvin Laird in 1969, when it was determined that unmanned spy satellites could perform the same functions much more cost-effectively.\n\n\n\n\n\n"}
{"id": "47777931", "url": "https://en.wikipedia.org/wiki?curid=47777931", "title": "PubRef", "text": "PubRef\n\nPubRef is a composition and project management application used by researchers and students for scholarly writing and communication. PubRef uses an extended form of Markdown as a primary authoring format and converts this to JATS, the archive format used by the US National Library of Medicine.\n\nResearchers manage scholarly writing projects within version-controlled file repositories called \"containers\" that function similarly to git repositories and docker containers to provide enhanced reproducibility, transparency and re-usability in digital science publishing.\n\nA PubRef manuscript is a Markdown file within a \"container\" that contains extra embedded information that describes the essential front matter elements of a scholarly manuscript such as the title, short title, list of authors, author affiliations, keywords. This information is captured in embedded YAML blocks within the primary manuscript called \"Meta\". Figures, tables, equations and other special content can be similarly embedded within the context of the document via \"Meta\" descriptors.\n\nThis minimal formatting example repurposes a classic paper by Stephen Hawking and Roger Penrose \"The Singularities of Gravitational Collapse and Cosmology\" \n!journal\ntitle: Proceedings of the Royal Society of London\n\n!article\ntype: Original Research\ntitle: The Singularities of Gravitational Collapse and Cosmology\n\n!author\ncorresponding: yes\nfirst: Stephen\nlast: Hawking\ndegrees: PhD\naffiliation: cambridge\n!author\nfirst: Roger\nlast: Penrose\ndegrees: PhD\naffiliation: birbeck\n\n!affiliation id: cambridge\nuniversity: Cambridge University\ninstitute: Institute of Theoretical Astronomy\n!affiliation id: birbeck\ncollege: Birbeck College\ndept: Department of Mathematics\n\n!bibliography \nsrc: references\n\nOnce the essential frontmatter meta elements have been declared, manuscripts can be automatically submitted to academic publishers. Articles, supporting data, and dependent code can be published directly on PubRef in the form of \"personal communications\" under the DOI prefix \"10.17920/P9.pubref\".\n"}
{"id": "52397627", "url": "https://en.wikipedia.org/wiki?curid=52397627", "title": "Rajesh K. Gupta", "text": "Rajesh K. Gupta\n\nRajesh K. Gupta (born 1961) is a computer scientist and engineer, currently the Qualcomm Professor in Embedded Microsystems at University of California, San Diego. His research concerns design and optimization of Cyber-physical systems (CPS). He is a Principal Investigator in the NSF MetroInsight project and serves as Associate Director of the Qualcomm Institute (also known as California Institute for Telecommunications and Information Technology). His research contributions include SystemC and SPARK Parallelizing High-level Synthesis. Earlier he led NSF Expeditions on Variability in Microelectronic circuits.\n\nHe chaired the Computer Science and Engineering department at UC San Diego until 2016 during a time of extraordinary growth in Computer Science nationwide.\n\nHe is a Fellow of the IEEE and holds INRIA International Chair at the French international research institute in Rennes, Bretagne Atlantique. In 2017 he became a Fellow of the Association for Computing Machinery.\n\n"}
{"id": "9990022", "url": "https://en.wikipedia.org/wiki?curid=9990022", "title": "Spagyric", "text": "Spagyric\n\nSpagyric is a word in English that means \"alchemy.\" Some people have coined the use of the word to mean an herbal medicine produced by alchemical procedures. These procedures involve fermentation, distillation, and extraction of mineral components from the ash of the plant. These processes were in use in medieval alchemy generally for the separation and purification of metals from ores (see Calcination), and salts from brines and other aqueous solutions.\n\nThe word comes from Ancient Greek σπάω \"spao\" \"to draw out\" and ἀγείρω \"ageiro\" \"to gather\". It is a term probably first coined by Paracelsus. In its original use, the word \"spagyric\" was commonly used synonymously with the word \"alchemy\", however, in more recent times it has often been adopted by alternative medicine theorists and various techniques of holistic medicine.\n\nSpagyric most commonly refers to a plant tincture to which has also been added the ash of the calcined plant. The original rationale behind these special herbal tinctures seems to have been that an extract using alcohol could not be expected to contain all the medicinal properties from a living plant, and so the ash or mineral component (as a result of the calcination process) of the calcined plant was prepared separately and then added back to 'augment' (increase) the alcoholic tincture. The roots of the word therefore refer first to the extraction or separation process and then to the recombining process. These herbal tinctures are alleged to have superior medicinal properties to simple alcohol tinctures, perhaps due the formation of soap-like compounds from the essential oils and the basic salts contained within the ash. In theory these spagyrics can also optionally include material from fermentation of the plant material and also any aromatic component such as might be obtained through distillation. The final spagyric should be a re-blending of all such extracts into one 'essence'.\n\nThe concept of the spagyric remedy in turn relies upon the three cardinal principles of alchemy, termed as salt, sulfur, and mercury. \"The basis of matter was the alchemical trinity of principles – salt, sulfur, and mercury. Salt was the principle of fixity (non-action) and in-combustibility; mercury was the principle of fusibility (ability to melt and flow) and volatility; and sulfur was the principle of inflammability.\"\n\nThe three primal alchemical properties and their correspondence in spagyric remedy are:\n\nParacelsus stated that the true purpose of alchemy was not for the vulgar purpose of gold making, but rather for the production of medicines.The term 'Spagyria' has been used by Paracelsus in his book \"Liber Paragranum\", deriving from the Greek words 'spao' and 'ageiro', the essential meaning of which is to 'separate and to combine'.\n\nHe formulated that nature in itself was 'raw and unfinished', and man had the God-given task to evolve things to a higher level. As an example: The 'raw' medicinal plant would be separated into the basic components he termed 'mercurius', 'sulfur', and 'sal' and thereby cleaned of non-essential components. 'Mercurius', 'sulfur', and 'sal' were then recombined forming the medicine.\n\nIn contemporary terms, this would be the extraction of the essential oils with vapour gaining the 'sulfur'. Then fermentation of the remaining plant and distilling the alcohol produced thus gaining 'mercurius'. Extraction of the mineral components from the ash of the marc which would be the 'sal'. Diluting the essential oils in the alcohol and then dissolving the mineral salts in it would produce the final potion. (This is a simplified representation of the process which varies strongly depending on the source chosen.)\n\nJoseph Needham devoted several volumes of his monumental \"Science and Civilisation in China\" to \"Spagyrical discovery and invention\". In 1965, Malaclypse the Younger and Lord Omar Khayyam Ravenhurst popularized the term as a result of their joint seminal work Principia Discordia.\n\nThe word \"spagyrici\" is inscribed on the coffin-plate of the English Paracelsian physician Sir Thomas Browne (1605-82).\n\n\n"}
{"id": "2632438", "url": "https://en.wikipedia.org/wiki?curid=2632438", "title": "Sujoy K. Guha", "text": "Sujoy K. Guha\n\nSujoy Kumar Guha is an Indian biomedical engineer. He was born in Patna, India, 20 June 1940.\n\nHe did his undergraduate degree (\"B.Tech.\") in electrical engineering from IIT Kharagpur, followed by a master's degree in electrical engineering at IIT, and another Master's degree from the University of Illinois, Urbana-Champaign. He later received his Ph.D. in medical physiology from St. Louis University. He then founded the Centre for Biomedical Engineering , IIT Delhi and AIIMS and also obtained his MBBS degree from Delhi University. One of the founders of biomedical engineering in India, Prof. Guha is internationally known in the areas of rehabilitation engineering, bioengineering in reproductive medicine and technology for rural health care. He has received several awards and has more than 100 research papers in cited journals. In 2003 he became a chair professor at IIT Kharagpur.\n\nHis major contributions have been in the invention and development of non-hormonal polymer based injectable male contraceptive (RISUG) for which the Final Phase-III Clinical trials are underway; Problem-solving at a national level regarding contraceptives in mass usage, especially Copper T; individualized spot air-conditioning system for hospital patients and rehabilitation of the blind, with emphasis on opening automobile repair as an employment avenue.\n"}
{"id": "31646252", "url": "https://en.wikipedia.org/wiki?curid=31646252", "title": "The Charleston Advisor", "text": "The Charleston Advisor\n\nThe Charleston Advisor is a peer-reviewed publication that reviews proprietary and free Internet resources that libraries license and make available to their patrons.\n\nThe journal's tag line is \"Critical reviews of web products for informational professionals.\" It is published quarterly and was established in 1999.\n\nThe journal has a \"Readers' Choice Award\". It also provides the \"ccAdvisor\" online review facility with the magazine \"\".\n"}
{"id": "26314185", "url": "https://en.wikipedia.org/wiki?curid=26314185", "title": "The Poisoner's Handbook", "text": "The Poisoner's Handbook\n\nThe Poisoner's Handbook: Murder and the Birth of Forensic Medicine in Jazz Age New York is a \"New York Times\" best-selling non-fiction book by Pulitzer Prize-winning science writer Deborah Blum that was released by Penguin Press in 2010.\n\nIn 1918, New York City appointed Charles Norris, Bellevue Hospital's chief pathologist, as its first scientifically trained medical examiner. The book, about Norris and Alexander Gettler, the city's first toxicologist, describes the Jazz Age's poisoning cases. Before the two began working in the medical examiner's office, Blum pointed out in her book, poisoners could get away with murder. The book covers the years from 1915 to 1936, which Blum described as a \"coming-of-age\" for forensic toxicology. \"Under (Norris's) direction, the New York City medical examiner's office would become a department that set forensic standards for the rest of the country,\" Blum wrote.\n\nWhile a guest on National Public Radio’s \"Talk of the Nation/Science Friday\" to discuss the book, Blum told host Ira Flatow that she wrote the book because, \"I've always been interested in poison. I wanted to write about the mystery of how (poisons) kill us.”\n\n\"Reader's Digest\" named \"The Poisoner's Handbook\" one of its Top 10 best crime books, saying, \"This is science writing at its finest that reads like a mystery novel.\"\n\n\"The New York Times\" placed the book on its Top-rated List on March 5, 2010. In its Sunday book review, the \"Times\" said \"The Poisoner's Handbook\" was \"structured like a collection of linked short stories. Each chapter centers on a mysterious death by poison that Norris and Gettler investigate.\"\n\nThe book was listed as a \"New York Times bestseller\" in paperback nonfiction in February 2011. Also, Amazon named \"The Poisoner's Handbook\" in its Top 100 Best of 2010.\n\n\"Not only is \"The Poisoner's Handbook\" as thrilling as any 'CSI' episode,\" wrote reviewer Art Taylor with \"The Washington Post\", \"but it also offers something even better: an education in how forensics really works.\"\n\n\"Kirkus Reviews\" described the book as, \"The rollicking story of the creation of modern forensic science by New York researchers during the Prohibition era.\"\n\nBarnes and Noble's editor's review said this: \"The book is an unexpected yet appropriate open-sesame into a world that was planting seeds for the world -- with lethal toxins and cutting-edge tools -- that would later, darkly bloom.\"\n\nGlen Weldon from NPR Books said: \"Rigorously researched and thoroughly engaging, \"The Poisoner's Handbook\" is a compelling, comprehensive portrait of the time and place that transformed criminal investigation, and made it much more difficult for that most insidious of murderers to escape the law.\"\n\nPBS optioned the book for TV and produced it as an episode of \"American Experience\". It premiered on January 7, 2014.\n\n\n\"Angel Killer: A True Story of Cannibalism, Crime Fighting, and Insanity in New York City\" (The Atavist, 2012)\n\n"}
{"id": "29468127", "url": "https://en.wikipedia.org/wiki?curid=29468127", "title": "UDF 423", "text": "UDF 423\n\nUDF 423 is the Hubble Ultra Deep Field (UDF) identifier for a distant spiral galaxy. With an apparent magnitude of 20, UDF 423 is one of the brightest galaxies in the HUDF and also has one of the largest apparent sizes in the HUDF.\n\nThe \"distance\" of a far away galaxy depends on how it is measured. With a redshift of 1, light from this galaxy is estimated to have taken around 7.7 billion years to reach Earth. However, since this galaxy is receding from Earth, the present comoving distance is estimated to be around 10 billion light-years away. In context, Hubble is observing this galaxy as it appeared when the Universe was around 5.9 billion years old.\n"}
{"id": "365310", "url": "https://en.wikipedia.org/wiki?curid=365310", "title": "Vostok (spacecraft)", "text": "Vostok (spacecraft)\n\nThe Vostok (, translated as \"East\") was a type of spacecraft built by the Soviet Union. The first human spaceflight was accomplished with Vostok 1 on April 12, 1961, by Soviet cosmonaut Yuri Gagarin.\n\nThe spacecraft was part of the Vostok programme, in which six manned spaceflights were made, from 1961–63. Two further manned space flights were made in 1964 and 1965 by Voskhod spacecraft, which were modified \"Vostok\" spacecraft. By the late 1960s both were superseded by the Soyuz spacecraft, which are still used .\n\nThe Vostok spacecraft was originally designed for use both as a camera platform (for the Soviet Union's first spy satellite program, Zenit) and as a manned spacecraft. This dual-use design was crucial in gaining Communist Party support for the program. The basic Vostok design has remained in use for some 40 years, gradually adapted for a range of other unmanned satellites. The descent module design was reused, in heavily modified form, by the Voskhod program.\n\nThe craft consisted of a spherical descent module (mass 2.46 tonnes, diameter 2.3 meters), which housed the cosmonaut, instruments and escape system, and a conical instrument module (mass 2.27 tonnes, 2.25 m long, 2.43 m wide), which contained propellant and the engine system. On reentry, the cosmonaut would eject from the craft at about 7,000 m (23,000 ft) and descend via parachute, while the capsule would land separately. The reason for this was that the Vostok descent module made an extremely rough landing that could have left a cosmonaut seriously injured.\n\nThe ejector seat also served as an escape mechanism in the event of a launch vehicle failure, which at this early phase of the space program was a common occurrence. If an accident occurred in the first 40 seconds after liftoff, the cosmonaut would simply eject from the spacecraft and parachute to Earth. From 40 to 150 seconds into launch, ground controllers could issue a manual shutdown command to the booster. When the launch vehicle fell to a low enough altitude, the cosmonaut would eject. Higher altitude failures after shroud jettison would involve detaching the entire spacecraft from the booster.\n\nOne problem that was never adequately resolved was the event of a launch vehicle malfunction in the first 20 seconds, when the ejector seat would not have enough time to deploy its parachute. LC-1 at the Baikonour Cosmodrome had netting placed around it to catch the descent module should the cosmonaut eject while still on the pad, but it was of doubtful value since he would likely end up landing too close to the exploding booster. An accident in the initial seconds of launch also likely would have not put the cosmonaut in a position where he could make a survivable ejection and in all probability, this situation would have resulted in his death. A 2001 recollection by V.V. Molodsov stated that Chief Designer Sergei Korolev felt \"absolutely terrible\" about the inadequate provisions for crew escape on the Vostok during the opening seconds of launch.\n\nThere were several models of the Vostok leading up to the manned version:\n\nPrototype spacecraft.\n\nPhoto-reconnaissance and signals intelligence spacecraft. Later named Zenit spy satellite.\n\nThe Vostok 3KA was the spacecraft used for the first human spaceflights. They were launched from Baikonur Cosmodrome using Vostok 8K72K launch vehicles. The first flight of a Vostok 3KA occurred on March 9, 1961. The first flight with a crew—Vostok 1 carrying Yuri Gagarin—took place on April 12, 1961. The last flight—Vostok 6 carrying the first woman in space, Valentina Tereshkova—took place on June 16, 1963.\n\nA total of 8 Vostok 3KA spacecraft were flown, 6 of them with a human crew.\n\nSpecifications for this version are:\n\nReentry Module: Vostok SA. SA stands for - descent system. It was nicknamed \"Sharik\" ().\nEquipment Module: Vostok PA. PA stands for - instrument section.\n\nThe Vostok capsule had limited thruster capability. As such, the reentry path and orientation could not be controlled after the capsule had separated from the engine system. This meant that the capsule had to be protected from reentry heat on all sides, thus explaining the spherical design (as opposed to Project Mercury's conical design, which allowed for maximum volume while minimizing the heat shield diameter). Some control of the capsule reentry orientation was possible by way of positioning of the heavy equipment to offset the vehicle center of gravity, which also maximized the chance of the cosmonaut surviving g-forces while in a horizontal position. Even then, the cosmonaut experienced 8 to 9g.\n\nIf the retrorocket failed, the spacecraft would naturally decay from orbit within ten days, and the cosmonaut was provided with enough food and oxygen to survive until that time.\n\n\n"}
{"id": "35835955", "url": "https://en.wikipedia.org/wiki?curid=35835955", "title": "WISE 0713−2917", "text": "WISE 0713−2917\n\nWISE J071322.55−291751.9 (designation abbreviated to WISE 0713−2917) is a brown dwarf of spectral class Y0, located in constellation Canis Major at approximately 23 light-years from Earth.\n\nWISE 0713−2917 was discovered in 2012 by J. Davy Kirkpatrick and colleagues from data collected by the Wide-field Infrared Survey Explorer (WISE) in the infrared at a wavelength of 40 cm (16 in), whose mission lasted from December 2009 to February 2011. In 2012 Kirkpatrick et al. published a paper in The Astrophysical Journal, where they presented discovery of seven new found by WISE brown dwarfs of spectral type Y, among which also was WISE 0713−2917.\n\nTrigonometric parallax of WISE 0713−2917 is not yet measured. Therefore, there are only distance estimates of this object, obtained by indirect—spectrophotometric—means (see table).\n\nWISE 0713−2917 distance estimates\n\nThe other six discoveries of brown dwarfs, published in Kirkpatrick et al. (2012):\n"}
