{"id": "14873893", "url": "https://en.wikipedia.org/wiki?curid=14873893", "title": "3C 433", "text": "3C 433\n\n3C 433 is a Seyfert galaxy located in the constellation Vulpecula.\n\n"}
{"id": "2041157", "url": "https://en.wikipedia.org/wiki?curid=2041157", "title": "A System of Logic", "text": "A System of Logic\n\nA System of Logic, Ratiocinative and Inductive is an 1843 book by English philosopher John Stuart Mill. In this work, he formulated the five principles of inductive reasoning that are known as Mill's Methods. This work is important in the philosophy of science, and more generally, insofar as it outlines the empirical principles Mill would use to justify his moral and political philosophies.\nAn article in \"Philosophy of Recent Times\" has described this book as an \"attempt to expound a psychological system of logic within empiricist principles.”\n\nThis work was important to the history of science, being a strong influence on scientists such as Dirac. \"A System of Logic\" also had an impression on Gottlob Frege, who rebuked many of Mill's ideas about the philosophy of mathematics in his work The Foundations of Arithmetic.\n\nMill revised the original work several times over the course of thirty years in response to critiques and commentary by Whewell, Bain, and others.\n\n\n"}
{"id": "3195316", "url": "https://en.wikipedia.org/wiki?curid=3195316", "title": "American Educational Research Association", "text": "American Educational Research Association\n\nThe American Educational Research Association, or AERA (\"A-E-R-A\"), is a professional organization representing education researchers in the United States and around the world. As a nonprofit serving the education research field, AERA strives to advance knowledge about education and promote the use of research in practice.\n\nAERA is led by Executive Director Felice J. Levine and 2018 - 2019 President Amy Stuart Wells (Teachers College, Columbia University). AERA's governance structure includes the Council, Executive Board, standing committees, and award committees. Temporary committees, task forces, and working groups are initiated for other specific needs.\n\nAERA claims 25,000 members, including scientists, teachers, students, administrators, state and local agencies, counselors, and evaluators. The broad range of disciplines represented by the membership includes education, psychology, statistics, sociology, history, economics, philosophy, anthropology, and political science . There are 12 divisions covering different research areas in education.\n\nAERA (known originally as National Association of Directors of Educational Research) was founded in 1916 as an interest group within the National Education Association Department of Superintendence. The association’s eight founders - Burdette R. Buckingham, Albert Shiels, Leonard P. Ayres, Frank W. Ballou, Stuart A. Courtis, Edwin Hebden, George Melcher, and Joseph P. O’Hern – were all directors of education research in various parts of the United States. They met at the 1915 NEA Department of Superintendence annual meeting and came to the idea of starting an organization to advance education research. Their constitution was approved the following year, committing the association to improving public education.\n\nEarly topics of interest for early AERA included research bureau operations, measurement techniques, and particular school situations. Active membership in the early association was reserved for research bureau directors and their assistants. The association’s early years revolved around the annual convention. Between meetings, the association published an internal quarterly newsletter, the Educational Research Bulletin.\n\nBy the end of the World War I in 1918, the association had 36 active members and four honorary members and was affecting public policy, visible in the school districts that started to change student coursework and education practices as a result of standardized tests. Mental testing developments, primarily psychometrics as a result of the First World War, new subfields of education, and the growth of education research at the post-secondary level challenged the association to widen its mission. The association opened its membership to include anyone who could demonstrate their competence as a researcher indicated by their published or unpublished work. In 1922, members voted to adopt a name that represented their goal of representing the interests of all American education researchers – Educational Research Association of America. Over the years that followed, membership saw a dramatic increase, particularly among university personal, which grew from 48% to 69% between 1923 and 1927.\n\nThe association’s original publication, the Journal of Educational Research, began in 1919.\n\nThe ability of education research to provide guidance for education practitioners was a struggle throughout the association’s beginnings, with only ambiguous known relationships between testing and learning outcomes. The association recognized the need to establish theoretical foundations for the field of education research. In 1928, the association changed its name to the American Educational Research Association, as it is currently known.\n\nDuring the Great Depression, the association’s public school affiliates struggled with tight finances and uncertain employment, but during the same time, university education researchers dominated the field and emerged as a unique social entity. Also during this time, AERA officials grew their relationships with like-minded associations, and a new journal, the Review of Educational Research, began as a reference work, summarizing recent studies. While early topics in Review of Educational Research focused primarily on education psychology and administration, the publication broadened its coverage in the mid-1930s in response to diversification in the field.\n\nThe role of education research in the Progressive Education Movement was a source of contention between education researchers, some of whom felt that it should play an active role in policy issues, and others who felt that it should be used primarily for professional discourse. As the field continued to advance, much of the knowledge did not translate into practice, an issue that is still widely debated today. These divisions in the field made it difficult for education researchers to speak with one voice. Just prior to World War II, Review of Educational Research made the case that because science could not speak to goals and choices, education research should contribute as one source of many to shaping American education.\n\nThe Areas publishes books, reports and journals. AERA publishes seven peer-reviewed journals:\n\nAERA’s Annual Meeting held every spring is the largest gathering of scholars in the education research field with approximately 14,000 participants. The five-day conference is a showcase of research studies across education disciplines at all levels.\n\nAERA also hosts the annual \"Brown\" Lecture in Education Research, which highlights the role of research in advancing equality in education. The first \"Brown\" Lecture was held in 2004 to commemorate the 50th anniversary of the \"Brown v. Board of Education\" decision. \n\nAERA is expanding the availability of their resources by participating in the open access movement. AERA currently offers Educational Researcher (journal) open access, as well as an Online Paper Repository containing presentations from the 2010 Annual Meeting forward. An open access, peer-reviewed journal – AERA Open – is scheduled to launch in 2014.\n\nOn the policy front, AERA is actively involved in revisions to the common rule. Executive Director Felice J. Levine served on the National Research Council committee charged with reviewing proposed regulations. The committee published its report in early 2014.\n\nAERA helps lead an ongoing initiative as part of the social science research community to increase federal funding for education research, particularly research in the social and behavioral sciences.\n\nIn addition, AERA is active in the reauthorization of the Institute of Education Sciences bill – Strengthening Education through Research Act – which was advanced by the United States House Committee on Education and the Workforce in April 2014.\n\nAERA is involved in several education research initiatives, ranging from specific advocacy topics to supporting projects that serve the larger community. AERA supports the Education Research Conferences Program, which awards grants for conferences on ground-breaking topics. AERA’s Education Research Service Projects is designed to encourage researchers to offer their expertise to organizations and groups who may have a need but not the funds to engage their assistance.\n\nOne topic area that AERA has been immensely involved is affirmative action. In 2013, AERA presented an amicus brief on the importance of science in a major affirmative action case – \"Fisher v. University of Texas at Austin\".\n\n"}
{"id": "4444657", "url": "https://en.wikipedia.org/wiki?curid=4444657", "title": "Animal repellent", "text": "Animal repellent\n\nAnimal repellents are products designed to keep certain animals away from objects, areas, people, plants, or other animals.\n\nRepellents generally work by taking advantage of an animal's natural aversion to something, and often the thing chosen is something that the animal has learned to avoid (or instinctively avoids) in its natural environment.\n\nFor example, some animals will avoid anything that has the odor of the urine of predators. Tiger urine is thus very effective at keeping away animals. Coyote urine has gained currency as a deer repellent. Fox urine is used to repel rabbits, groundhogs, woodchucks, squirrels and chipmunks. Bobcat urine repels moles, mice, voles and other rodents. Wolf urine is used to repel moose. Used cat litter is also effective, but needs repeated application.\n\nChemical repellents mimic natural substances that repel or deter animals, or they are designed to be so irritating to a specific animal or type of animal that the targeted animal will avoid the protected object or area. Some chemical repellents combine both principles. There are many homemade deer repellent recipes on the web.\n\nFor example, the lawn fertilizer Milorganite is claimed to be an effective repellent due to its smell. Repellents fall into two main categories, odor and taste. Odor repellents work better in the warm seasons and taste repellents work better in the cold months. Taste repellents only work after the deer or other animal has taken a bite out of the plant. If you have a plant you don't want nibbled on at all, use an odor repellent or a combination of both.\n\nThe scientists from University of Florida explain that “Most of these deterrents operate through one of several mechanisms: odor aversion, taste aversion, or fear inducement”. Pepper, peppermint, tarragon, garlic, various essential oils, castor oil, diatomaceous earth, and putrescent egg solids are considered to be the so-called contact plant origin repellents operate through the first and the second mechanisms. The third type of repellents contains the ingredients of animal origin, such animals' enemies urine (such as coyotes or foxes), dried blood and hair induce pests animals' panic and make them flee, as confirmed by the University of Massachusetts Amherst specialists.\n\nOther types of non-chemical repellents are sometimes used. A simple electrified or barbed wire fence can mechanically repel livestock or predator animals. Some electrical repellent systems have been tested against sharks. High-frequency whistles have been used on vehicles to drive deer away from highways, and similar devices have been used to deter and repel certain types of insects or rodents. Repellents for domestic cats and dogs can also be found; these include ultrasonic devices which emit a high frequency noise that does not affect humans. These types of non-chemical repellents are quite controversial because their effectiveness varies from person to person. Furthermore, there have been few scientific studies conducted to prove that they do work. They are, however, a safe and humane way of disposing pests.\n\nThere are also few alternative ways to repel Cats and Dogs which are effective, such as repellent chemical Spray, electronic pet barrier, motion activated sprinkler etc. In Addition, vinegar can be used as organic pet repellent.\n\nThe ideal repellent is completely specific for the target animal; that is, it drives away the animal that one wishes to repel \"without\" affecting or harming any other animals or people. One type of animal repellent may be effective for raccoons, while another animal repellent may be more effective for skunks. It can be difficult to design a repellent method that drives away only undesirable animals while having no effect on people or other creatures.\n\n\n"}
{"id": "12591076", "url": "https://en.wikipedia.org/wiki?curid=12591076", "title": "Asset price inflation", "text": "Asset price inflation\n\nAsset price inflation is a\neconomic phenomenon denoting a rise in price of assets, as opposed to ordinary goods and services.\nAs inflation is generally understood and perceived as the rise in price of 'ordinary' goods and services, and official and central bank policies in most of today’s world have been expressly directed at minimizing 'price inflation', assets inflation has not been the object of much attention or concern. An example of this is the housing market, which concerns almost every individual household, where house prices have over the past decade consistently risen by or at least near a two digit percentage, far above that of the Consumer Price Index.\n\nSome political economists believe that assets inflation has been, either by default or by design, the outcome of purposive policies pursued by central banks and political decision-makers to combat and reduce the much more visible price inflation. This could be for a variety of reasons, some overt, but others more concealed or even disreputable. Some think that it is the consequence of a natural reaction of investors to the danger of shrinking value of practically all important currencies, which, as in 2012 e.g., seems to them highly probable due to the tremendous worldwide growth of the mass of money. Their preference for real goods pushes their price up without any purposive policies from decision-makers.\n\nAsset price inflation has often been followed by an asset price crash, a sudden and usually unexpected fall in the price of a particular asset class. Examples of asset price crashes include Dutch tulips in the 17th century, Japanese metropolitan real estate and stocks in the early 1990s, and internet stocks in 2001. It was also a contributory factor in the 2007 subprime mortgage financial crisis.However, if the money supply has the potential to induce heavy general inflation (all major currencies in 2011/2012) none of these crashes may happen.\n\n\n"}
{"id": "19245793", "url": "https://en.wikipedia.org/wiki?curid=19245793", "title": "Brouwer–Hilbert controversy", "text": "Brouwer–Hilbert controversy\n\nIn a foundational controversy in twentieth-century mathematics, L. E. J. Brouwer, a supporter of intuitionism, opposed David Hilbert, the founder of formalism.\n\nThe background for the controversy was set with David Hilbert's axiomatization of geometry in the late 1890s. In his biography of Kurt Gödel, John W. Dawson, Jr summarizes the result as follows: \"At issue in the sometimes bitter disputes was the relation of mathematics to logic, as well as fundamental questions of methodology, such as how quantifiers were to be construed, to what extent, if at all, nonconstructive methods were justified, and whether there were important connections to be made between syntactic and semantic notions.\"\n\nDawson observes that \"partisans of three principal philosophical positions took part in the debate\" – the logicists (Gottlob Frege and Bertrand Russell), the formalists (David Hilbert and his \"school\" of collaborators), and the constructivists (Henri Poincaré and Hermann Weyl); within this constructivist school was the radical self-named \"intuitionist\" L.E.J. Brouwer.\n\nBrouwer in effect founded the mathematical philosophy of intuitionism as a challenge to the then-prevailing formalism of David Hilbert and his collaborators Paul Bernays, Wilhelm Ackermann, John von Neumann and others. As a variety of constructive mathematics, intuitionism is essentially a philosophy of the foundations of mathematics. It is sometimes and rather simplistically characterized by saying that its adherents refuse to use the law of excluded middle in mathematical reasoning.\n\nIn 1908: \"... Brouwer, in a paper entitled \"The untrustworthiness of the principles of logic\", challenged the belief that the rules of the classical logic, which have come down to us essentially from Aristotle (384–322 B.C.) have an absolute validity, independent of the subject matter to which they are applied\".\n\n\"After completing his dissertation (1907: see Van Dalen), Brouwer made a conscious decision temporarily to keep his contentious ideas under wraps and to concentrate on demonstrating his mathematical prowess\" (Davis (2000), p. 95); by 1910 he had published a number of important papers, in particular the Fixed Point Theorem. Hilbert – the formalist with whom the intuitionist Brouwer would ultimately spend years in conflict – admired the young man and helped him receive a regular academic appointment (1912) at the University of Amsterdam. It was then that \"Brouwer felt free to return to his revolutionary project which he was now calling \"intuitionism\"\".\n\nIn the later 1920s, Brouwer became involved in a public and demeaning controversy with Hilbert over editorial policy at \"Mathematische Annalen\", at that time a leading learned journal. He became relatively isolated; the development of intuitionism at its source was taken up by his student Arend Heyting.\n\nThe nature of Hilbert's proof of the Hilbert basis theorem (dating from 1888) turned out to be more controversial than Hilbert could have imagined at the time. Although Kronecker had conceded, Hilbert would later respond to others' similar criticisms that \"many different constructions are subsumed under one fundamental idea\" — in other words (to quote Reid): \"Through a proof of existence, Hilbert had been able to obtain a construction\"; \"the proof\" (i.e. the symbols on the page) was \"the object\".\n\nNot all were convinced. While Kronecker would die soon after, his constructivist banner would be carried forward by sharp criticism from Poincaré, and later in full cry by the young Brouwer and his developing intuitionist \"school\"—Weyl in particular, much to Hilbert's torment in his later years (Reid 1996, pp. 148–149). Indeed, Hilbert lost his \"gifted pupil\" Weyl to intuitionism: \"Hilbert was disturbed by his former student's fascination with the ideas of Brouwer, which aroused in Hilbert the memory of Kronecker.\"\n\nBrouwer the intuitionist in particular objected to the use of the Law of Excluded Middle over infinite sets (as Hilbert had indeed used it). Hilbert would respond: \"'Taking the Principle of the Excluded Middle from the mathematician ... is the same as ... prohibiting the boxer the use of his fists.' \"The possible loss did not seem to bother Weyl.\"\n\nIn the same paper – the text of an address delivered in 1927 – Hilbert clearly expresses himself. At first he attempts to defend his axiomatic system as having \"important general philosophical significance\". For him, the statement of \"definite rules\" expresses \"the technique of our thinking\". Nothing is hidden, no tacit assumptions are admitted: \"after all, it is part of the task of science to liberate us from arbitrariness, sentiment and habit, and to protect us from the subjectivism that ... finds its culmination in intuitionism\".\n\nBut then Hilbert gets to the nub of it – the proscription of the law of excluded middle (LoEM): :\"Intuitionism's sharpest and most passionate challenge is the one it flings at the validity of the principle of excluded middle ... .\"\n\nTo doubt the LoEM—when extended over the completed infinite—was to doubt Hilbert's axiomatic system, in particular his \"logical ε-axiom\". To take away the LoEM was to destroy the \"science of mathematics\". Finally, Hilbert singles out one man—by inference, not by name—for the cause of his present tribulation: \"... I am astonished that a mathematician should doubt that the principle of excluded middle is strictly valid as a mode of inference. I am even more astonished that, as it seems, a whole community of mathematicians who do the same has so constituted itself. I am most astonished by the fact that even in mathematical circles the power of suggestion of a single man, however full of temperament and inventiveness, is capable of having the most improbable and eccentric effects.\"\n\nBrouwer answers pique with pique: \"... formalism has received nothing but benefactions from intuitionism and may expect further benefactions. The formalistic school should therefore accord some recognition to intuitionism, instead of polemicizing against it in sneering tones, while not even observing proper mention of authorship.\"\n\nHowever \"truth\" is ultimately defined, for a few mathematicians Hilbert's formalism seemed to eschew the notion. And at least with respect to his choice of axioms the case can be made that indeed he \"does\" eschew the notion. The fundamental issue is just \"how\" does one choose \"the axioms\"? Until Hilbert proposed his formalism, the axioms were chosen on an \"intuitive\" (experiential) basis. Aristotelian logic is a good example – based on one's life-experiences it just seems \"logical\" that an object of discourse either has a stated property (e.g. \"This truck is yellow\") or it does not have that property (\"This truck is not yellow\") but not both simultaneously (the Aristotelian Law of Non-Contradiction). The primitive form of the induction axiom is another – if a predicate P(n) is true for n = 0 and if for all natural numbers n, if P(n) being true implies that P(n+1) is true, then P(n) is true for all natural numbers n.\n\nHilbert's axiomatic system – his formalism – is different. At the outset it declares its axioms. But he doesn't require the selection of these axioms to be based upon either \"common sense\", a priori knowledge (intuitively derived understanding or awareness, innate knowledge seen as \"truth without requiring any proof from experience\" ), or observational experience (empirical data). Rather, the mathematician in the same manner as the theoretical physicist is free to adopt any (arbitrary, abstract) collection of axioms that they so choose. Indeed, Weyl asserts that Hilbert had \"formaliz[ed] it [classical mathematics], thus transforming it in principle from a system of intuitive results into a game with formulas that proceeds according to fixed rules\". So, Weyl asks, what might guide the choice of these rules? \"What impels us to take as a basis precisely the particular axiom system developed by Hilbert?\". Weyl offers up \"consistency is indeed a necessary but not sufficient condition\" but he cannot answer more completely except to note that Hilbert's \"construction\" is \"arbitrary and bold\". Finally he notes, in italics, that the \"philosophical result\" of Hilbert's \"construction\" will be the following: \"If Hilbert's view prevails over intuitionism, as appears to be the case, \"then I see in this a decisive defeat of the philosophical attitude of pure phenomenology\", which thus proves to be insufficient for the understanding of creative science even in the area of cognition that is most primal and most readily open to evidence – mathematics.\"\n\nIn other words: the role of innate feelings and tendencies (intuition) and observational experience (empiricism) in the choice of axioms will be removed except in the global sense – the \"construction\" had better work when put to the test: \"only the theoretical system as a whole ... can be confronted with experience\".\n\nCantor (1897) extended the intuitive notion of \"the infinite\" – one foot placed after the other in a never-ending march toward the horizon – to the notion of \"a completed infinite\" – the arrival \"all the way, way out there\" in one fell swoop, and he symbolized this notion with a single sign ℵ (aleph-null). Hilbert's adoption of the notion wholesale was \"thoughtless\", Brouwer believed. Brouwer in his (1927a) \"Intuitionistic reflections on formalism\" states: \"SECOND INSIGHT The rejection of the thoughtless use of the logical principle of the excluded middle, as well as the recognition, first, of the fact that the investigation of the question why the principle mentioned is justified and to what extent it is valid constitutes an essential object of research in the foundations of mathematics, and, second, of the fact that in intuitive (contentual) mathematics this principle is valid only for finite systems. THIRD INSIGHT. The identification of the principle of excluded middle with the principle of the solvability of every mathematical problem\".\n\nThis Third Insight is referring to Hilbert's second problem and Hilbert's ongoing attempt to axiomatize all of arithmetic, and with this system, to discover a \"consistency proof\" for all of mathematics – see more below. So into this fray (started by Poincaré) Brouwer plunged head-long, with Weyl as back-up.\n\nTheir first complaint (Brouwer's Second Insight, above) arose from Hilbert's extension of Aristotle's \"Law of Excluded Middle\" (and \"double negation\") – hitherto restricted to finite domains of Aristotelian discourse – to \"infinite\" domains of discourse\". In the late 1890s Hilbert successfully axiomatized geometry. Then he went on to successfully (or so Hilbert thought) use the Cantorian-inspired notion of the completed infinity to produce elegant, radically abbreviated proofs in analysis (1896 and afterwards). In his own words of defense Hilbert believed himself quite justified in what he had done (in the following he calls this type of proof an existence proof): \"...I stated a general theorem (1896) on algebraic forms that is a pure existence statement and by its very nature cannot be transformed into a statement involving constructibility. Purely by use of this existence theorem I avoided the lengthy and unclear argumentation of Weierstrass and the highly complicated calculations of Dedekind, and in addition, I believe, only my proof uncovers the inner reason for the validity of the assertions adumbrated by Gauss and formulated by Weierstrass and Dedekind.\" \"The value of pure existence proofs consists precisely in that the individual construction is eliminated by them and that many different constructions are subsumed under one fundamental idea, so that only what is essential to the proof stands out clearly; brevity and economy of thought are the \"raison d'être\" of existence proofs.\"\n\nWhat Hilbert had to give up was \"constructibility\" – his proofs would not produce \"objects\" (except for the proofs themselves – i.e. symbol strings), but rather they would produce contradictions of the premises and have to proceed by reductio ad absurdum extended over the infinite.\n\nBrouwer viewed this loss of constructibility as bad, but worse when applied to a generalized \"proof of consistency\" for all of mathematics. In his 1900 address Hilbert had specified, as the second of his 23 problems for the twentieth century, the quest for a generalized proof of (procedure for determining) the consistency of the axioms of arithmetic. Hilbert, unlike Brouwer, believed that the formalized notion of mathematical induction could be applied in the search for the \"generalized\" consistency proof.\n\nA consequence of this marvelous proof/procedure P would be the following: Given any arbitrary mathematical theorem T (formula, procedure, proof) put to P (thus P(T)) \"including P itself\" (thus P(P)), P would determine conclusively whether or not the theorem T (and P) was \"provable\" – i.e. derivable from its premises, the axioms of arithmetic. Thus for all T, T would be \"provable\" by P or not \"provable\" by P and \"under all conditions\" (i.e. for any assignment of numerical values to T's variables). This is a perfect illustration of the use of the Law of Excluded Middle extended over the infinite, in fact extended \"twice\" – first over all theorems (formulas, procedures, proofs) and secondly for a given theorem, for all assignment of its variables. This point, missed by Hilbert, was first pointed out to him by Poincaré and later by Weyl in his 1927 comments on Hilbert's lecture: \"For after all Hilbert, too, is not merely concerned with, say 0' or 0' ', but with any 0' ', with an \"arbitrarily concretely given\" numeral. One may here stress the \"concretely given\"; on the other hand, it is just as essential that the contentual arguments in proof theory be carried out \"in hypothetical generality\", on \"any\" proof, on \"any\" numeral. ... It seems to me that Hilbert's proof theory shows Poincaré to have been completely right on this point.\"\n\nIn his discussion preceding Weyl's 1927 comments van Heijenoort explains that Hilbert insisted that he had addressed the issue of \"whether a formula, taken as an axiom, leads to a contradiction, the question is whether a proof that leads to a contradiction can be presented to me\".\n\nIf successful the quest would result in a remarkable outcome: Given such a generalized proof, all mathematics could be replaced by an automaton consisting of two parts: (i) a formula-generator to create formulas one after the other, followed by (ii) the generalized consistency proof, which would yield \"Yes – valid (i.e. provable)\" or \"No – not valid (not provable)\" for each formula submitted to it (and every possible assignment of numbers to its variables). In other words: mathematics would cease as a creative enterprise and become a machine.\n\nIn van Heijenoort's commentary preceding Weyl's (1927) \"Comments on Hilbert's second lecture on the foundations of mathematics\" Poincaré points out to Hilbert (1905) that there are two types of \"induction\" (1) the intuitive animal-logic foot-following-foot version that gives us a sense that there's always another footstep after the last footstep, and (2) the formal version – e.g. Peano's version: a string of symbols. The gang of three – Poincaré, Weyl and Brouwer – claimed that Hilbert tacitly, and unjustifiably, adopted as one of his premises formal induction (the symbol string). Poincaré (1905) asserted that, by doing this, Hilbert's reasoning became circular. Weyl's (1927) agreement and Brouwer's polemics ultimately forced Hilbert and his disciples Herbrand, Bernays and Ackermann to reexamine their notion of \"induction\" – to eschew the assumption of a \"totality of all the objects x of an infinite collection\" and (intuitionistically) assume that the general argument proceeds one x after another, ad infinitum (van Heijenoort p. 481, footnote a). This is in fact the so-called \"induction schema\" used in the notion of \"recursion\" that was still in development at this time (\"cf.\" van Heijenoort p. 493) – this schema was acceptable to the intuitionists because it had been derived from \"the intuition\".\n\nTo carry this distinction further, Kleene 1952/1977 distinguishes between three types of mathematical induction – (1) the formal Induction Rule (Peano's axiom, see the next section for an example), (2) the inductive definition (examples: counting, \"Proof by induction\"), and (3) the definition by induction (recursive definition of \"number-theoretic functions or predicates). With regards to (3), Kleene considers primitive recursive functions:\n\nBrouwer's insistence on \"constructibility\" in the search for a \"consistency proof for arithmetic\" resulted in sensitivity to the issue as reflected by the work of Finsler and Gödel. Ultimately Gödel would \"numeralize\" his formulae; Gödel then used primitive recursion (and its instantiation of the intuitive, constructive form of induction – i.e. counting and step-by-step evaluation) rather than a string of symbols that represent formal induction. Gödel was so sensitive to this issue that he took great pains in his 1931 to point out that his Theorem VI (the so-called \"First incompleteness theorem\") \"is constructive;, that is, the following has been proved in an intuitionistically unobjectionable manner ... .\" He then demonstrates what he believes to be the constructive nature of his \"generalization formula\" 17 Gen r. Footnote 45a reinforces his point.\n\nGödel's 1931 does include the formalist's symbol-version of the Peano Induction Axiom; it looks like this, where \".\" is the logical AND, f is the successor-sign, x is a function, x is a variable, xΠ designates \"for all values of variable x\":\nBut he does not appear to use this in the formalist's sense.\n\nNote that there is contention around this point. Gödel specifies this symbol string in his I.3., i.e. the formalized inductive axiom appears as shown above – yet even this string can be \"numeralized\" using Gödel's method. On the other hand, he doesn't appear to use this axiom. Rather, his recursion steps through integers assigned to variable k (cf his (2) on page 602). His skeleton-proof of Theorem V, however, \"use(s) induction on the degree of φ\", and uses \"the induction hypothesis\". Without a full proof of this, we are left to assume that his use of the \"induction hypothesis\" is the intuitive version, not the symbolic axiom. His recursion simply steps up the degree of the functions, an intuitive act, ad infinitum. But Nagel and Newman note that Gödel's proofs are infinitary in nature, not finitary as Hilbert requested (see Hilbert's second problem) while Gödel insisted that they are intuitionistically satisfactory. These are not incompatible truths, as long as the LoEM over the infinite isn't invoked anywhere in the proofs.\n\nDespite the last-half-twentieth century's continued abstraction of mathematics, the issue has not entirely gone away. Here are two examples. First, the premises of an argument – even ones considered beyond questioning – are always fair game. A hard look at the premises of Turing's 1936–7 led Robin Gandy (1980) to propose his \"principles for mechanisms\" that throw in the speed of light as a constraint. Secondly, Breger (2000) in his \"Tacit Knowledge and Mathematical Progress\" delves deeply into the matter of \"semantics versus syntax\" – in his paper Hilbert, Poincaré, Frege, and Weyl duly make their appearances. He examines a core problem: in axiomatic proofs the tacit assumption of an experienced, thinking mind: to be successful it must come to the argument equipped with prior knowledge of the symbols and their use (the semantics behind the mindless syntax): \"Mathematics as a purely formal system of symbols without a human being possessing the know-how for dealing with the symbols is impossible [according to the chemist Polanyi (1969, 195), the ideal of a form of knowledge that is strictly explicit is contradictory because without tacit knowledge all formulas, words, and illustrations would become meaningless]\" (brackets in the original, Breger 2000:229).\n\nA serious study of this foundational controversy can be found in Stephen Kleene's \"Introduction to Metamathematics\", particularly in Chapter III: A critique of mathematical reasoning. He discusses §11. \"The paradoxes\", §12. \"First inferences from the paradoxes\" [impredicative definitions, Logicism etc.], §13. \"Intuitionism\", §14. \"Formalism\", §15. \"Formalization of a theory\". Kleene takes the debate seriously, and throughout his book he actually builds the two \"formal systems\", e.g. on page 119 he shows those logical laws such as double negation that are disallowed in the intuitionist system.\n\n"}
{"id": "53792239", "url": "https://en.wikipedia.org/wiki?curid=53792239", "title": "CANpie", "text": "CANpie\n\nCANpie (CAN Programming Interface Environment) is an open source project and pursues the objective of creating and establishing an open and standardized software API for access to the CAN bus.\nThe project was established in 2001 by MicroControl and is licensed under Apache License Version 2.0. The current version of the CANpie API covers both classical CAN frames as well as ISO CAN FD frames. The API is designed for embedded control applications as well as for PC interface boards: embedded microcontrollers are programmed in C, a C++ API is provided for OS independent access to interface boards. The API provides ISO/OSI Layer-2 (Data Link Layer) functionality. It is not the intention of CANpie to incorporate higher layer functionality (e.g. CANopen, SAE J1939).\n\nThe CANpie API supports the concept of hardware message buffers (mailboxes) with a total limit of 255 buffers. A message buffer has a unique direction (receive or transmit). As an option it is possible to connect a FIFO with arbitrary size to a message buffer for both transfer directions. The total number of CAN channels is limited to 255, the API provides a method to gather information about the features of each CAN hardware channel. This is especially important for an application designer who wants to write the code only once. The CAN frame time-stamping (specified by CiA 603, CAN Frame time-stamping – Requirements for network time management) is supported with a resolution of 1 nano-second.\n\nThe following code snippet shows the initialisation of a microcontroller.\n\nvoid MyCanInit(void)\n\nFor the Linux operating system the projects can4linux and SocketCAN provide support for Classical CAN and ISO CAN FD. The commercial AUTOSAR specification supports CAN FD since version 4.3 and is available only for AUTOSAR partners. The CMSIS-Driver (Cortex Microcontroller Software Interface Standard) specification is a software API that describes peripheral driver interfaces for middleware stacks and user applications on ARM Cortex-M processors.\n\n"}
{"id": "15702854", "url": "https://en.wikipedia.org/wiki?curid=15702854", "title": "Certified Forensic Computer Examiner", "text": "Certified Forensic Computer Examiner\n\nThe Certified Forensic Computer Examiner (CFCE) credential was the first certification demonstrating competency in computer forensics in relation to Windows based computers. The CFCE training and certification is conducted by the International Association of Computer Investigative Specialists (IACIS), a non-profit, all-volunteer organization of digital forensic professionals.\n\nIACIS was formed and commenced training in 1990. The predecessor to the CFCE was the DOS Processing Certificate (DPC). The CFCE was introduced in 1998 when the training was expanded to include examination of Windows-based computers. The course materials also covers the MAC OS operating system and it's associated file systems, however, the certificate only states proficiency in Windows.\n\nIn order to become a member of IACIS and undertake the CFCE previously a person must generally be a full-time member - sworn or unsworn - of a law enforcement agency, however this is no longer a requirement. In some of those cases, a contract employee of a law enforcement agency or retired law enforcement officer may be eligible. All IACIS members must sign agreement with the IACIS Code of Ethics.\n\nThe certification process may be taken internally or externally and is conducted in two phases: Peer Review and Certification.\n\nAn internal certification candidate attends a 2-week training course given by IACIS. Two courses are conducted annually. The US based course is conducted in the first half of the calendar year whilst the European-based course is conducted in the second half of the year. Upon successful completion of the course, the member is assigned a (volunteer) coach. The coach guides the student through the Peer Review phase, often by suggesting reading materials or experiments for the student, which is intended to assist the student in fully understanding issues with which the student may be having difficulty. Upon successful completion of the Peer Review phase the candidate is eligible to enter the Certification phase which consists of a practical exam based on a hard drive examination and a final exam.\n\nAn external certification candidate does not attend the training, however they have to have the equivalent 72 hours of training that is comparable to the IACIS training.\n\nIn order for certification to remain current, a member must undertake a proficiency test once per 3-year period after certification as well as complete 40 hours of continuing training in computer forensics or a related field. Additionally, the member must conduct as a minimum an average of 1 forensic examination per year, for a minimum of 3 examinations over the 3-year period. The member must also pay dues ($75 per year) and remain a member in good standing of IACIS.\n\nCFCE is one of the most widely recognized non-tool certifications in computer forensics for current and former law enforcement personnel. Some organizations such as the Computer Forensics Laboratory at Miami-Dade Police require their members to complete and maintain this certification.\n\n"}
{"id": "2166633", "url": "https://en.wikipedia.org/wiki?curid=2166633", "title": "Core-excited shape resonance", "text": "Core-excited shape resonance\n\nA core-excited shape resonance is a shape resonance in a system with more than one degree of freedom where, after fragmentation, one of the fragments is in an excited state. It is sometimes very difficult to distinguish a core-excited shape resonance from a Feshbach resonance.\n\nSee the definition of Feshbach resonances for more details.\n\n"}
{"id": "2768426", "url": "https://en.wikipedia.org/wiki?curid=2768426", "title": "Correct name", "text": "Correct name\n\nIn botany, the correct name according to the International Code of Nomenclature for algae, fungi, and plants (ICN) is the one and only botanical name that is to be used for a particular taxon, when that taxon has a particular circumscription, position and rank. Determining whether a name is correct is a complex procedure. The name must be validly published, a process which is defined in no less than 16 Articles of the ICN. It must also be \"legitimate\", which imposes some further requirements. If there are two or more legitimate names for the same taxon (with the same circumscription, position and rank), then the correct name is the one which has priority, i.e. it was published earliest, although names may be conserved if they have been very widely used. Validly published names other than the correct name are called synonyms. Since taxonomists may disagree as to the circumscription, position or rank of a taxon, there can be more than one correct name for a particular plant. These may also be called synonyms.\n\nThe correct name has only one correct spelling, which will generally be the original spelling (although certain limited corrections are allowed). Other spellings are called orthographical variants.\n\nThe zoological equivalent of \"correct name\" is \"valid name\".\n\nDifferent taxonomic placements may well lead to different correct names. For example, the earliest name for the fastest growing tree in the world is \"Adenanthera falcataria\" L. The \"L.\" stands for \"Linnaeus\" who first validly published the name. \"Adenanthera falcataria\" is thus one of the correct names for this plant. There are other correct names, based on different taxonomic treatments.\n\nThe four names \"Adenanthera falcataria\", \"Albizia falcataria\", \"Paraserianthes falcataria\" and \"Falcataria moluccana\" are each correct, given that the plant is placed in these four genera. Which is the 'right' genus is a problem for taxonomy, not nomenclature. Thus this tree species will have a different correct botanical name for different people.\n\n\n\n"}
{"id": "2848730", "url": "https://en.wikipedia.org/wiki?curid=2848730", "title": "Cosmological horizon", "text": "Cosmological horizon\n\nA cosmological horizon is a measure of the distance from which one could possibly retrieve information. This observable constraint is due to various properties of general relativity, the expanding universe, and the physics of Big Bang cosmology. Cosmological horizons set the size and scale of the observable universe. This article explains a number of these horizons.\n\nThe particle horizon (also called the cosmological horizon, the comoving horizon, or the cosmic light horizon) is the maximum distance from which particles could have traveled to the observer in the age of the universe. It represents the boundary between the observable and the unobservable regions of the universe, so its distance at the present epoch defines the size of the observable universe. Due to the expansion of the universe it is not simply the age of the universe times the speed of light, as in the Hubble horizon, but rather the speed of light multiplied by the conformal time. The existence, properties, and significance of a cosmological horizon depend on the particular cosmological model.\n\nIn terms of comoving distance, the particle horizon is equal to the conformal time that has passed since the Big Bang, times the speed of light. In general, the conformal time at a certain time is given in terms of the scale factor formula_1 by,\n\nor\n\nThe particle horizon is the boundary between two regions at a point at a given time: one region defined by events that have already been observed by an observer, and the other by events which cannot be observed \"at that time\". It represents the furthest distance from which we can retrieve information from the past, and so defines the observable universe.\n\nHubble radius, Hubble sphere, Hubble volume, or Hubble horizon is a conceptual horizon defining the boundary between particles that are moving slower and faster than the speed of light relative to an observer at one given time. Note that this does not mean the particle is unobservable, the light from the past is reaching and will continue to reach the observer for a while. Also, more importantly, in the current expansion model e.g., light emitted from the Hubble radius will reach us in a finite amount of time. It is a common misconception that light from the Hubble radius can never reach us. It is true that particles on the Hubble radius recede from us with the speed of light, but the Hubble radius gets larger over time (because the Hubble parameter H gets smaller over time), so light emitted towards us from a particle on the Hubble radius will be inside the Hubble radius some time later. Only light emitted from the cosmic event horizon or further will never reach us in a finite amount of time.\n\nThe Hubble velocity of an object is given by Hubble's law,\n\nReplacing formula_5 with speed of light formula_6 and solving for proper distance formula_7 we obtain the radius of Hubble sphere as\n\nIn an ever-accelerating universe, if two particles are separated by a distance greater than the Hubble radius, they cannot talk to each other from now on (as they are now, not as they have been in the past), However, if they are outside of each other's particle horizon, they could have never communicated. Depending on the form of expansion of the universe, they may be able to exchange information in the future. Today,\n\nyielding a Hubble horizon of some 4.1 Gpc. This horizon is not really a physical size, but it is often used as useful length scale as most physical sizes in cosmology can be written in terms of those factors.\n\nOne can also define comoving Hubble horizon by simply dividing Hubble radius by the scale factor\n\nThe particle horizon differs from the cosmic event horizon, in that the particle horizon represents the largest comoving distance from which light could have reached the observer by a specific time, while the event horizon is the largest comoving distance from which light emitted now can \"ever\" reach the observer in the future. The current distance to our cosmic event horizon is about 5 Gpc (16 billion light years), well within our observable range given by the particle horizon.\n\nIn general, the proper distance to the event horizon at time formula_11 is given by\n\nwhere formula_13 is the time-coordinate of the end of the universe, which would be infinite in the case of a universe that expands forever.\n\nFor our case, assuming that dark energy is due to a cosmological constant, formula_14.\n\nIn an accelerating universe, there are events which will be unobservable as formula_15 as signals from future events become redshifted to arbitrarily long wavelengths in the exponentially expanding de Sitter space. This sets a limit on the farthest distance that we can possibly see as measured in units of proper distance today. Or, more precisely, there are events that are spatially separated for a certain frame of reference happening simultaneously with the event occurring right now for which no signal will ever reach us, even though we can observe events that occurred at the same location in space that happened in the distant past. While we will continue to receive signals from this location in space, even if we wait an infinite amount of time, a signal that left from that location today will never reach us. Additionally, the signals coming from that location will have less and less energy and be less and less frequent until the location, for all practical purposes, becomes unobservable. In a universe that is dominated by dark energy which is undergoing an exponential expansion of the scale factor, all objects that are gravitationally unbound with respect to the Milky Way will become unobservable, in a futuristic version of Kapteyn's universe.\n\nWhile not technically \"horizons\" in the sense of an impossibility for observations due to relativity or cosmological solutions, there are practical horizons which include the optical horizon, set at the surface of last scattering. This is the farthest distance that any photon can freely stream. Similarly, there is a \"neutrino horizon\" set for the farthest distance a neutrino can freely stream and a gravitational wave horizon at the farthest distance that gravitational waves can freely stream. The latter is predicted to be a direct probe of the end of cosmic inflation.\n\nFor a simplified summary and overview of different horizons in cosmology, see Different Horizons in Cosmology\n"}
{"id": "13885920", "url": "https://en.wikipedia.org/wiki?curid=13885920", "title": "DECHEMA model", "text": "DECHEMA model\n\nThe DECHEMA model is a more general version of Raoult's law and under ideal conditions simplifies to Raoult's law. The DECHEMA model is a model for obtaining K values important in chemical engineering. \n"}
{"id": "2824046", "url": "https://en.wikipedia.org/wiki?curid=2824046", "title": "Ecodynamics", "text": "Ecodynamics\n\nEcodynamics is a part of applied economics. It covers knowledge on monetary value, the usage of money and the money flow. It deals with labour, and capital.\n\n\n\n"}
{"id": "31800185", "url": "https://en.wikipedia.org/wiki?curid=31800185", "title": "EgoNet", "text": "EgoNet\n\nEgoNet (Egocentric Network Study Software) for the collection and analysis of egocentric social network data. It helps the user to collect and analyse all the egocentric network data (all social network data of a website on the Internet), and provide general global network measures and data matrixes that can be used for further analysis by other software. The egonet is the result of the links that it gives and receives certain address on the Internet, and EgoNet is dedicated to collecting information about them and present it in a way useful to the users.\n\nEgonet is written in Java, so that the computer where it is going to be used must have the JRE installed. EgoNet is open source software, licensed under GPL.\n\nIts creator is Professor Christopher McCarty, of the University of Florida, United States.\nThe program allows to create questionnaires, collect data and provide comprehensive measures and arrays of data that can be used for subsequent analysis by other software.\nIts main benefits are the generation of questionnaires for relational data, the calculation of relevant General measurements for the analysis of social networks and production graphs.\n\nEgonet is composed of the following modules:\n\n\n"}
{"id": "39259307", "url": "https://en.wikipedia.org/wiki?curid=39259307", "title": "Epiphreatic zone", "text": "Epiphreatic zone\n\nIn a cave system, the epiphreatic zone or the floodwater zone is the zone between vadose zone above it and phreatic zone beneath it. It is regularly flooded and has a significant porosity. It has a great potential for cave formation.\n"}
{"id": "31684588", "url": "https://en.wikipedia.org/wiki?curid=31684588", "title": "Faculty of Sport and Tourism", "text": "Faculty of Sport and Tourism\n\nThe Faculty of Sport and Tourism () is a state-accredited private institution of high education located in Novi Sad, Serbia. The Faculty was founded in 2004 and consists of three departments - Sport, Tourism and Psychology.\n\nSince its founding, the Faculty has educated 209 bachelors, 36 magisters and masters, as well as 10 doctors.\n\nTIMS started off as an independent College for Sport Trainers and Managers (Serbian: Виша школа за тренере и менаџере у спорту / Viša škola za trenere i menadžere u sportu) in 2001, to grow into the Faculty of Sport and Tourism - TIMS, as it is known today, in 2004. That year also saw the addition of another department - Tourism, rendering TIMS the only faculty dedicated to sport and tourism in Serbia and the countries of former Yugoslavia.\n\nThe Faculty consists of two departments: the Department of Sport, and the Department of Tourism. Basic studies on both departments last 3 years, master's studies taking 2 years and doctoral studies 3 years.\n\nAccording to the nomenclature of occupations, the students acquire the following professional titles upon graduation:\n\nfrom the study programme of \"Physical education and sport\":\n\n\nfrom the study programme of \"Management and business in tourism\":\n\n\nTIMS is a member of the Association for Tourism and Leisure Education (ATLAS).\n\nThe Faculty cooperates with many other institutions of higher education across Europe, but also with some of the most significant associations, clubs and organisations in the field of sport and tourism in Serbia and the region.\n\n"}
{"id": "21477359", "url": "https://en.wikipedia.org/wiki?curid=21477359", "title": "Falling cat problem", "text": "Falling cat problem\n\nThe falling cat problem is a problem that consists of explaining the underlying physics behind the observation of the cat righting reflex: that is, how a free-falling body (cat) can change its orientation such that it is able to right itself as it falls to land on its feet, irrespective of its initial orientation, and without violating the law of conservation of angular momentum.\n\nAlthough amusing and trivial to pose, the solution of the problem is not as straightforward as its statement would suggest. The apparent contradiction with the law of conservation of angular momentum is resolved because the cat is not a rigid body, but instead is permitted to change its shape during the fall owing to the cat's flexible backbone and non-functional collar-bone. The behavior of the cat is thus typical of the mechanics of deformable bodies.\n\nThe falling cat problem has elicited interest from famous scientists including George Gabriel Stokes, James Clerk Maxwell, and Étienne-Jules Marey. In a letter to his wife, Katherine Mary Clerk Maxwell, Maxwell wrote, \"There is a tradition in Trinity that when I was here I discovered a method of throwing a cat so as not to light on its feet, and that I used to throw cats out of windows. I had to explain that the proper object of research was to find how quick the cat would turn round, and that the proper method was to let the cat drop on a table or bed from about two inches, and that even then the cat lights on her feet.\" Maxwell's and other scientists interest with so-called \"cat turning\" was reputed to have caused the death of many cats after being dropped from windows of multi-storied buildings. Maxwell in particular was interested in discovering the precise height that a cat needed to be dropped from so that the cat would be incapable of using its righting reflex to land on its feet, an endeavor in which he eventually succeeded.\n\nWhereas the cat-falling problem was regarded as a mere curiosity by Maxwell, Stokes, and others, a more rigorous study of the problem was conducted by Étienne-Jules Marey who applied chronophotography to capture the cat's descent on film using a chronophotographic gun. The gun, capable of capturing 12 frames per second, produced images from which Marey deduced that, as the cat had no rotational motion at the start of its descent, the cat was not \"cheating\" by using the cat handler's hand as a fulcrum. This in itself posed a problem as it implied that it was possible for a body in free fall to acquire angular momentum. Marey also showed that air resistance played no role in facilitating the righting of the cat's body.\n\nHis investigations were subsequently published in \"Comptes Rendus\", and a summary of his findings were published in the journal \"Nature\". The article's summary in \"Nature\" appeared thus:\n\nDespite the publication of the images, many physicists at the time maintained that the cat was still \"cheating\" by using the handler's hand from its starting position to right itself, as the cat's motion would otherwise seem to imply a rigid body acquiring angular momentum.\n\nThe solution of the problem, originally due to , models the cat as a pair of cylinders (the front and back halves of the cat) capable of changing their relative orientations. later described the Kane–Scher model in terms of a connection in the configuration space that encapsulates the relative motions of the two parts of the cat permitted by the physics. Framed in this way, the dynamics of the falling cat problem is a prototypical example of a nonholonomic system , the study of which is among the central preoccupations of control theory. A solution of the falling cat problem is a curve in the configuration space that is \"horizontal\" with respect to the connection (that is, it is admissible by the physics) with prescribed initial and final configurations. Finding an optimal solution is an example of optimal motion planning (; ).\n\nIn the language of physics, Montgomery's connection is a certain Yang-Mills field on the configuration space, and is a special case of a more general approach to the dynamics of deformable bodies as represented by gauge fields (; ), following the work of .\n\n\n\n__notoc__\n"}
{"id": "8953682", "url": "https://en.wikipedia.org/wiki?curid=8953682", "title": "Flatness (systems theory)", "text": "Flatness (systems theory)\n\nFlatness in systems theory is a system property that extends the notion of controllability from linear systems to nonlinear dynamical systems. A system that has the flatness property is called a \"flat system\". Flat systems have a (fictitious) \"flat output\", which can be used to explicitly express all states and inputs in terms of the flat output and a finite number of its derivatives.\n\nA nonlinear system\n\nformula_1\n\nis flat, if there exists an output\n\nformula_2\n\nthat satisfies the following conditions:\n\n\nIf these conditions are satisfied at least locally, then the (possibly fictitious) output is called \"flat output\", and the system is \"flat\".\n\nA linear system \nformula_14\nwith the same signal dimensions for formula_15 as the nonlinear system is flat, if and only if it is controllable. For linear systems both properties are equivalent, hence exchangeable.\n\nThe flatness property is useful for both the analysis of and controller synthesis for nonlinear dynamical systems. It is particularly advantageous for solving trajectory planning problems and asymptotical setpoint following control.\n\n\n"}
{"id": "20003745", "url": "https://en.wikipedia.org/wiki?curid=20003745", "title": "Flaw lead", "text": "Flaw lead\n\nFlaw lead is an oceanographic term for a waterway opening between pack ice and fast ice. Flaw lead occurs annually at the time when central pack ice drifts from coastal ice, thereby creating the flaw. The process begins in autumn. Flaw leads can have interconnected polynyas. The Canadian government's Circumpolar Flaw Lead System Study, through the University of Manitoba examines the physical changes and their effects on biological processes with flaw leads.\n\nA similar opening (\"lead\") can exist between pack ice and the shore, referred to as a shore lead.\n\n"}
{"id": "7914038", "url": "https://en.wikipedia.org/wiki?curid=7914038", "title": "Generalizability theory", "text": "Generalizability theory\n\nGeneralizability theory, or G theory, is a statistical framework for conceptualizing, investigating, and designing reliable observations. It is used to determine the reliability (i.e., reproducibility) of measurements under specific conditions. It is particularly useful for assessing the reliability of performance assessments. It was originally introduced in Cronbach, L.J., Nageswari, R., & Gleser, G.C. (1963). \n\nIn G theory, sources of variation are referred to as \"facets\". Facets are similar to the \"factors\" used in analysis of variance, and may include persons, raters, items/forms, time, and settings among other possibilities. These facets are potential sources of error and the purpose of generalizability theory is to quantify the amount of error caused by each facet and interaction of facets. The usefulness of data gained from a G study is crucially dependent on the design of the study. Therefore, the researcher must carefully consider the ways in which he/she hopes to generalize any specific results. Is it important to generalize from one setting to a larger number of settings? From one rater to a larger number of raters? From one set of items to a larger set of items? The answers to these questions will vary from one researcher to the next, and will drive the design of a G study in different ways. \n\nIn addition to deciding which facets the researcher generally wishes to examine, it is necessary to determine which facet will serve as the object of measurement (e.g. the systematic source of variance) for the purpose of analysis. The remaining facets of interest are then considered to be sources of measurement error. In most cases, the object of measurement will be the person to whom a number/score is assigned. In other cases it may be a group or performers such as a team or classroom. Ideally, nearly all of the measured variance will be attributed to the object of measurement (e.g. individual differences), with only a negligible amount of variance attributed to the remaining facets (e.g., rater, time, setting).\n\nThe results from a G study can also be used to inform a decision, or D, study. In a D study, we can ask the hypothetical question of \"what would happen if different aspects of this study were altered?\" For example, a soft drink company might be interested in assessing the quality of a new product through use of a consumer rating scale. By employing a D study, it would be possible to estimate how the consistency of quality ratings would change if consumers were asked 10 questions instead of 2, or if 1,000 consumers rated the soft drink instead of 100. By employing simulated D studies, it is therefore possible to examine how the generalizability coefficients (similar to reliability coefficients in Classical test theory) would change under different circumstances, and consequently determine the ideal conditions under which our measurements would be the most reliable.\n\nThe focus of classical test theory (CTT) is on determining error of the measurement. Perhaps the most famous model of CTT is the equation formula_1, where X is the observed score, T is the true score, and e is the error involved in measurement. Although \"e\" could represent many different types of error, such as rater or instrument error, CTT only allows us to estimate one type of error at a time. Essentially it throws all sources of error into one error term. This may be suitable in the context of highly controlled laboratory conditions, but variance is a part of everyday life. In field research, for example, it is unrealistic to expect that the conditions of measurement will remain constant. Generalizability theory acknowledges and allows for variability in assessment conditions that may affect measurements. The advantage of G theory lies in the fact that researchers can estimate what proportion of the total variance in the results is due to the individual factors that often vary in assessment, such as setting, time, items, and raters. \n\nAnother important difference between CTT and G theory is that the latter approach takes into account how the consistency of outcomes may change if a measure is used to make absolute versus relative decisions. An example of an absolute, or criterion-referenced, decision would be when an individual's test score is compared to a cut-off score to determine eligibility or diagnosis (i.e. a child's score on an achievement test is used to determine eligibility for a gifted program). In contrast, an example of a relative, or norm-referenced, decision would be when the individual's test score is used to either (a) determine relative standing as compared to his/her peers (i.e. a child's score on a reading subtest is used to determine which reading group he/she is placed in), or (b) make intra-individual comparisons (i.e. comparing previous versus current performance within the same individual). The type of decision that the researcher is interested in will determine which formula should be used to calculate the generalizability coefficient (similar to a reliability coefficient in CTT).\n\nReaders interested in learning more about G theory are encouraged to seek out publications by Brennan (2001), Chiu (2001), and/or Shavelson and Webb (1991).\n\n\n"}
{"id": "32552745", "url": "https://en.wikipedia.org/wiki?curid=32552745", "title": "Gloria Jean Siebrecht", "text": "Gloria Jean Siebrecht\n\nGloria Jean Siebrecht (born 1940) is an American amateur paleontologist and volunteer for the Museum of the Rockies, notable as the discoverer of \"Avisaurus Gloriae\", which was named for her, and \"Piksi barbarulna\".\n\nShe is the sixth child of James Baily Schnee and Marie Van De Rite of Kalispell, Montana. She grew up in Columbia Falls, Montana; McMinnville, Oregon; and Lincoln City, Oregon. She graduated from Taft High School in Lincoln City in 1958. She married Odell Siebrecht in 1959 and raised two sons on a farm north of Cut Bank, Montana.\n\nAs a volunteer for the Museum of the Rockies, Siebrecht spent thousands of hours on digs and in preparing fossils for display.\n"}
{"id": "29770195", "url": "https://en.wikipedia.org/wiki?curid=29770195", "title": "Gulf", "text": "Gulf\n\nA gulf is a large inlet from the ocean into the landmass, typically with a narrower opening than a bay; though this is not observable in all geographic areas so named. The term gulf was traditionally used for large, highly indented, navigable bodies of salt water which are enclosed by the coastline. Many gulfs are major shipping areas, such as the Persian Gulf, Gulf of Mexico, and Gulf of Aden. \n\n"}
{"id": "519742", "url": "https://en.wikipedia.org/wiki?curid=519742", "title": "Harold Robert Steacy", "text": "Harold Robert Steacy\n\nHarold \"Hal\" Robert Steacy (June 7, 1923 – April 7, 2012) was a Canadian mineralogist who was the curator of the Canadian National Mineral Collection at the Geological Survey of Canada in Ottawa. The mineral steacyite is named for him.\n\n"}
{"id": "3948246", "url": "https://en.wikipedia.org/wiki?curid=3948246", "title": "HomoloGene", "text": "HomoloGene\n\nHomoloGene, a tool of the United States National Center for Biotechnology Information (NCBI), is a system for automated detection of homologs (similarity attributable to descent from a common ancestor) among the annotated genes of several completely sequenced eukaryotic genomes.\n\nThe HomoloGene processing consists of the protein analysis from the input organisms. Sequences are compared using blastp, then matched up and put into groups, using a taxonomic tree built from sequence similarity, where closer related organisms are matched up first, and then further organisms are added to the tree. The protein alignments are mapped back to their corresponding DNA sequences, and then distance metrics as molecular distances Jukes and Cantor (1969), Ka/Ks ratio can be calculated.\n\nThe sequences are matched up by using a heuristic algorithm for maximizing the score globally, rather than locally, in a bipartite matching (see complete bipartite graph). And then it calculates the statistical significance of each match. Cutoffs are made per position and Ks values are set to prevent false \"orthologs\" from being grouped together. “Paralogs” are identified by finding sequences that are closer within species than other species.\n\n\"Homo sapiens, Pan troglodytes, Mus musculus, Rattus norvegicus, Canis lupus familiaris, Bos taurus, Gallus gallus, Xenopus tropicalis, Danio rerio\"\n\n\"Drosophila melanogaster, Anopheles gambiae, Caenorhabditis elegans\"\n\n\"Saccharomyces cerevisiae, Schizosaccharomyces pombe, Kluyveromyces lactis, Eremothecium gossypii, Magnaporthe grisea, Neurospora crassa\"\n\n\"Arabidopsis thaliana\"\n\n\"Oryza sativa\"\n\n\"Plasmodium falciparum\".\n\nThe HomoloGene is linked to all Entrez databases and based on homology and phenotype information of these links: \n\nAs a result, HomoloGene displays information about Genes, Proteins, Phenotypes, and Conserved Domains.\n\n"}
{"id": "38986349", "url": "https://en.wikipedia.org/wiki?curid=38986349", "title": "Indium gallium arsenide phosphide", "text": "Indium gallium arsenide phosphide\n\nIndium gallium arsenide phosphide () is a quaternary compound semiconductor material, an alloy of gallium arsenide and indium phosphide. This compound has applications in photonic devices, due to the ability to tailor its band gap via changes in the alloy mole ratios, \"x\" and \"y\".\n\nIndium phosphide-based photonic integrated circuits, or PICs, commonly use alloys of to construct quantum wells, waveguides and other photonic structures, lattice matched to an InP substrate, enabling single-crystal epitaxial growth onto InP.\n\nMany devices operating in the near-infrared 1.55 μm wavelength window utilize this alloy, and are employed as optical components (such as laser transmitters, photodetectors and modulators) in C-band communications systems.\n\n\n"}
{"id": "1991624", "url": "https://en.wikipedia.org/wiki?curid=1991624", "title": "Information access", "text": "Information access\n\nInformation access is the freedom or ability to identify, obtain and make use of data or information effectively.\n\nThere are various research efforts in information access which objective is to simplify and make it more effective for human users to access and further process large and unwieldy amounts of data and information.A person who is searching for information uses a search engine to achieve their information hunt. These searches for information can be of a wide range.\n\nSeveral technologies applicable to the general area are Information Retrieval, Text Mining, Machine Translation, and Text Categorisation.\n\nDuring discussions on free access to information as well as on information policy, information access is understood as concerning the insurance of free and closed access to information. Information access covers many issues including copyright, open source, privacy, and security.\n\nGroups such as the American Library Association, the American Association of Law Libraries, Ralph Nader's Taxpayers Assets Project have advocated for free access to legal information. The vendor neutral citation movement in the legal field is working to ensure that courts will accept citations from cases on the web which do not have the traditional (copyrighted) page numbers from the West Publishing company. There is a worldwide Free Access to Law Movement which advocates free access to legal information. The Wired Magazine Article Who Owns The Law is an introduction to the access to legal information issue. Postsecondary organizations such as K-12 work to share information. They feel it is a legal and moral obligation to provide access (including to people with disabilities or impairments) to information through the services and programs they offer.\n\n"}
{"id": "26944363", "url": "https://en.wikipedia.org/wiki?curid=26944363", "title": "Institute for Economics and Peace", "text": "Institute for Economics and Peace\n\nThe Institute for Economics & Peace (IEP), is a global think tank headquartered in Sydney, Australia with branches in New York City, Mexico City and The Hague. The IEP is chaired by technology entrepreneur Steve Killelea founder of Integrated Research.\n\nIEP is dedicated to shifting the world's focus to peace as a positive, achievable, and tangible measure of human well-being and progress.\n\nIt achieves its goals by developing global and national peace indices, calculating the \"economic cost of violence\", analysing country level risk and understanding the conditions which underpin highly peaceful societies - \"Positive Peace\".\n\nIEP produce frameworks to define peacefulness, providing metrics for measurement, uncovering the relationship between peace, business, and prosperity, and by promoting a better understanding of the cultural, economic, and political factors that drive peacefulness.\n\nIEP works in partnership with a number of think tanks, NGOs and academic institutions including the Aspen Institute, Center for Strategic and International Studies, International Peace Institute, Open Society Foundations, and King's College London. It also collaborates with intergovernmental organizations such as the Organisation for Economic Co-operation and Development (OECD), the Commonwealth Secretariat, United Nations Development Programme (UNDP), UNICEF, NATO, the World Bank Group, and UN Peacebuilding Support Office.\n\nSince 2015, IEP has been an active member of the UN Global Compact Expert Group on Responsible Business and Investment in High-Risk Areas.\n\nIEP has a strategic partnership with Rotary International and One Young World, which involves training of Global Peace Index Ambassadors the program focuses on engaging young leaders in understanding IEP’s empirical peace research.\n\nIEP is also a member of the Sustainable Development Goal 16 Data Initiative, which is a consortium that compiles existing global data to assist in tracking progress towards achieving Goal 16.\n\nThe core asset of the IEP is the Global Peace Index (GPI), which is now considered the benchmark study in measuring peace.\n\nThe GPI has been recognized by leading analysts and institutions, and has been incorporated into reports such as the Stockholm International Peace Research Institute's Year Book (2009, 2010, 2011, 2012, 2014, 2015, and 2016), and was analyzed by the World Bank's World Development Report 2011 team.\n\nThe data for the GPI is collected and collated in collaboration with the Economist Intelligence Unit (EIU), the research and analysis arm of the Economist Group, and the methodology is informed and reviewed each year by an international panel of peace and statistics experts.\n\nThe GPI is released annually with presentations typically held in London, Washington, D.C., Geneva, Paris, New York City, Sydney, Brussels and The Hague.\n\nThe GPI is used by various international organizations, such as the UN and World Bank as a source of data and information as well as being used by academics and universities around the world.\n\nIn addition, the GPI was the empirical basis for the Symposium of Peaceful Nations, a 3-day conference hosted in November 2009 to honor the most peaceful countries in each of nine regions of the world at which Helen Clark, UNDP Administrator, delivered the keynote address.\n\nThe Institute for Economics & Peace, in 2016, marked 10 years of measuring and analysing global levels of peace. In celebration of this milestone, IEP along with the Diplomatic Courier Magazine hosted the Future of Peace Summit in Washington D.C.\n\nThe 11th annual GPI launched in June 2017 includes 163 countries in the world, and the interactive map on the website explains the rankings.\n\nThe 2017 GPI recorded a small improvement in average global peace, with 93 countries recording higher levels of peace and 68 recording a deterioration.\n\nIEP has also launched a series of National Peace Indices. The first one was the United States Peace Index (USPI) in April 2011. The USPI ranks each state in the US by peacefulness and, unlike the GPI, uses only 5 indicators: incarceration rate, the number of police officers, the number of homicides, the availability of small arms, and the number of violent crimes. In 2011 Maine was ranked the most peaceful state, while Louisiana was the least peaceful.\n\nThe UK Peace Index was conducted in 2013. The UK Peace Index provides a comprehensive measure of the levels of peacefulness within the United Kingdom from 2003 to 2012.\n\nThe Mexico Peace Index (MPI) is the latest in the series of National Peace Indices. There have been 4 editions of the MPI to date, the first published in 2013, and most recently 2017.\n\nThe MPI uses five indicators to gauge the level of peace in the 32 Mexican states from 2003-2016. The indicators are: \"Homicide rates, violent crime, detention without a sentence, weapons crime\" and \"organised crime\".\n\nAn expert panel is engaged to provide independent advice and technical guidance in developing the methodology, these experts are from independent, non-partisan, civil society and academic organisations.\n\nThe 2017 MPI found that peacefulness deteriorated by 4.3% in 2016, however the homicide rate rose by 18.4% in the same year. Yucatán is the most peaceful state in Mexico, while Guerrero is the least peaceful state.\n\nThe data used to calculate the MPI comes from government bodies in Mexico, and IEP uses survey data collection by the national statistical office to adjust the figures for under reporting.\n\nAnother flagship report of the IEP is the Global Terrorism Index (GTI) which provides a comprehensive summary of the key global trends and patterns in terrorism. The report analyses data spanning the past 16 years and four editions having been released to date.\n\nThe GTI is based on data from the Global Terrorism Database (GTD). It ranks 163 countries based on four indicators, those indicators are; \"number of terrorist incidents in a given year, number of fatalities from terrorist incidents in a given year, number of injuries caused by terrorist in a given year\" and \"measure of the total property damage from terrorist incidents in a given year.\"\n\nThe 2016 GTI report found a ten per cent decrease in the number of deaths from terrorism in 2015 resulted in 3,389 fewer deaths than 2014, a global total of 29,376 deaths made 2015 the second deadliest year on record.\n\nThe Economic Value of Peace report estimates of the economic impact of violence and conflict on the global economy.\n\nIt provides an empirical basis to calculate the potential additional economic benefits from improvements in peace. It estimates the economic impact of violence for 163 countries and independent territories representing 99.5 per cent of the global economy and population.\n\nThe global economic impact of violence was $14.3 trillion PPP in 2016, equivalent to 12.6% of global GDP, or $1,953 per person, this figure represented the first decline in the economic impact of violence since 2011, which is the year that corresponded with the start of the Syrian war and ISIL's territorial gains in Iraq.\n\nOn October 26, 2010, IEP and Peace and Media Tenor released “Measuring Peace in the Media”, the first study that takes a fact-based approach into understanding the accuracy of international television networks’ coverage of peace, violence and conflict.\n\nThe results show broad inconsistencies across geographies and networks, with US broadcasters much more focused on violence and conflict than their European and Middle Eastern counterparts. Al Jazeera was found to be the network providing the most balanced coverage on Afghanistan. BBC World led the way when it came to breadth of coverage. It regularly reported on 67 countries across six continents which is nearly twice as many countries as the average level of coverage.\n\nThe study analysed 37 TV news and current affairs programmes from 23 networks in 15 countries* and then cross-referenced this with the Global Peace Index which measures the levels of peace and violence in 149 countries. BBC 2 Newsnight and ZDF Heute Journal (Germany) were found to be the programmes whose editorial policies aligned their coverage most closely with the rankings of the GPI.\n\nPositive-peace stories make up just 1.6% of the total number of stories examined in the study. These are stories that report on active steps taken to rectify violent situations. Such a small percentage may be partly related to what is considered newsworthy and dramatic, such as high-impact, violent or controversial events. However, the stereotyping of nations which are low on the GPI makes it harder for audiences to gain empathy and therefore to support governments and make headway towards creating peace.\n\nPositive Peace is an innovative and empirically based framework which has been developed by IEP to identify and understand the factors which are statistically associated with peaceful and resilient societies. Positive Peace is measured by IEP on the Positive Peace Index (PPI).\n\nWhile the Global Peace Index measures 'Negative Peace' being the absence of violence or fear of violence, Positive Peace represents the capacity for a society to meet the needs of its citizens, reduce and deal with grievances without the use of violence.\n\nThe framework is based on the quantitatively identifiable common characteristics of the world's most peaceful societies.\n\nThe defining feature of IEP’s Positive Peace framework is the systems approach to peace. IEP research has demonstrated that high levels of peacefulness are associated with strength in not just one, but all eight factors of Positive Peace. All domains are highly interrelated and work together systemically in producing a peaceful society.\n\nThe Global Go To Think Tank Index listed the Institute for Economics and Peace as a \"Think Tank to Watch\", and as one of the top 20 most impactful Think Tanks worldwide with a Budget under $5 Million.\n\nIn 2013, Steve Killelea’s founding of IEP was recognized as one of the 50 most impactful philanthropic gifts in Australia’s history by a coalition including the Myer Family Company, The Myer Foundation and Sidney Myer Fund, Pro Bono Australia, Swinburne University and Philanthropy Australia.\n\n\n"}
{"id": "1137807", "url": "https://en.wikipedia.org/wiki?curid=1137807", "title": "Jean Jacques Dozy", "text": "Jean Jacques Dozy\n\nJean Jacques Dozy (18 June 1908, in Rotterdam – 1 November 2004, in The Hague) was a Dutch geologist.\n\nIn 1936, he participated in the Dutch Carstensz Expedition in Dutch New Guinea to explore and climb Mount Carstensz, the highest mountain of the island of New Guinea. Besides succeeding in climbing the highest point at the time with Anton Colijn and Frits Wissel, Dozy discovered the presence of abundant copper ore in a mountain he called Ertsberg (English, \"Ore mountain\"). Years later this gave rise to the Grasberg copper mine. In 1939, he published an article about his find, but it was neglected due to World War II. Twenty years later, the article led to rediscovery of the Ertsberg and the development of the Ertsberg-Grasberg mine complex.\n\n"}
{"id": "7276807", "url": "https://en.wikipedia.org/wiki?curid=7276807", "title": "Józef Grzybowski", "text": "Józef Grzybowski\n\nJózef Grzybowski (March 17, 1869 – February 17, 1922), was a Polish geologist, paleontologist and foraminiferologist.\n\nGrzybowski was born in Kraków. He was educated at Jagiellonian University where he became the director of the Paleontological Laboratory. Grzybowski was a Professor of Palaeontology at the Jagiellonian University in Kraków, a pioneer in the use of microfossils for stratigraphical applications.\n\n"}
{"id": "4887671", "url": "https://en.wikipedia.org/wiki?curid=4887671", "title": "Kartvelian studies", "text": "Kartvelian studies\n\nThe Kartvelian studies () also referred as Kartvelology or Georgian studies is a field of humanities covering Kartvelian (Georgian) history, languages, religion and/or culture.\n\nIn a narrower sense, the term usually refers to the research activities conducted on these problems outside Georgia.\n\n\n\n\n"}
{"id": "58451840", "url": "https://en.wikipedia.org/wiki?curid=58451840", "title": "Leucandra villosa", "text": "Leucandra villosa\n\nLeucandra villosa is a species of sponge in the taxonomic rank of the calcareous sponges (Calcarea). The sponge lives in the sea and its Sclereid consists of calcium carbonate. \n\nThe sponge belongs to the genus of \"Leucandra\" and belongs to the family Grantiidae. The scientific name of the species was first published in 1885 by Lendenfeld.\n"}
{"id": "5808493", "url": "https://en.wikipedia.org/wiki?curid=5808493", "title": "List of essential oils", "text": "List of essential oils\n\nEssential oils are volatile and liquid aroma compounds from natural sources, usually plants. They are not oils in a strict sense, but often share with oils a poor solubility in water. Essential oils often have an odor and are therefore used in food flavoring and perfumery. They are usually prepared by fragrance extraction techniques (such as distillation, cold pressing, or Solvent extraction). Essential oils are distinguished from aroma oils (essential oils and aroma compounds in an oily solvent), infusions in a vegetable oil, absolutes, and concretes. Typically, essential oils are highly complex mixtures of often hundreds of individual aroma compounds.\n\n\n\n\n"}
{"id": "41098384", "url": "https://en.wikipedia.org/wiki?curid=41098384", "title": "List of human anatomical regions", "text": "List of human anatomical regions\n\nThis illustration labeled regions of the human body show an anterior and posterior view of the body.\n\nThe trunk of the body contains, from superior to inferior,\n\nThe pelvis and legs contain, from superior to inferior,\nThe regions of the upper limbs, from superior to inferior, are\nThe posterior view contains, from superior to inferior,\nThe regions of the back of the arms, from superior to inferior, include\nThe posterior regions of the legs, from superior to inferior, include\nSome regions are combined into larger regions. These include the trunk, which is a combination of the thoracic, mammary, abdominal, naval, and coxal regions.\nThe cephalic region is a combination of all of the head regions.\nThe upper limb region is a combination of all of the arm regions. The lower limb region is a combination of all of the leg regions.\n\nMany of these terms are Latin terms that have fallen into disuse.\n\nFront:\nBack:\n\n"}
{"id": "47749212", "url": "https://en.wikipedia.org/wiki?curid=47749212", "title": "List of optofluidics researchers", "text": "List of optofluidics researchers\n\nThis is a list of researchers in optofluidics, a research and technology area that combines microfluidics and optics and has applications in displays, biosensors, lab-on-chip devices, lenses, and molecular imaging and energy.\nThere are numerous research groups worldwide working on optofluidics, including those listed below. \n"}
{"id": "2879918", "url": "https://en.wikipedia.org/wiki?curid=2879918", "title": "List of probability distributions", "text": "List of probability distributions\n\nMany probability distributions that are important in theory or applications have been given specific names.\n\n\n\n\n\n\n\n\n\nFor any set of independent random variables the probability density function of their joint distribution is the product of their individual density functions.\n\n\n\n\n\n"}
{"id": "3209299", "url": "https://en.wikipedia.org/wiki?curid=3209299", "title": "List of social software", "text": "List of social software\n\nThis is a list of notable social software: selected examples of social software products and services that facilitate a variety of forms of social human contact.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "44948441", "url": "https://en.wikipedia.org/wiki?curid=44948441", "title": "Madhu Kinnar", "text": "Madhu Kinnar\n\nMadhu Bai Kinnar is a transgender mayor in Chhattisgarh, India. Running as an independent candidate, Madhu won the mayor election of the Raigarh Municipal Corporation, securing 33,168 votes and defeating the nearest rival, the ruling party BJP's Mahaveer Guruji, by 4,537 votes.\n\nMadhu Bai was previously known as Naresh Chauhan and has an eighth-grade education. As a teenager, Madhu left her family to join the local transgender community. She belongs to the Dalit community in India.\n\nBefore assuming office, Madhu Bai earned a living by taking up odd jobs and by singing and dancing on the streets of Raigarh and performing in trains going on the Howrah-Mumbai route. She ran for mayor's office on a budget of 60,000 to 70,000 rupees, saying that it was upon the insistence of a few aggrieved citizens that she decided to contest the elections.\n\nBefore Madhu, Kamla Jan was elected the first transgender mayor of India in Katni. Her candidacy was declared \"void\" because she contested in the female category.\n\nMadhu Kinnar wishes to work as an independent leader, as opposed to joining a political party. Her electoral victory on 4 January 2015, came about nine months after the Supreme Court NALSA verdict, which gave legal recognition to transgender people in India. In the first meeting of the Raigarh Municipal Corporation, Congress and BJP members staged a walkout, resulting in the meeting being postponed. \n\nA crucial part of her agenda has been sanitation. In an interview with The Guardian, she states that, \"“There were no proper sidewalks. The alleys were dirty and piled high with garbage. Poor people, abandoned in their old age, slept in the streets with nothing to keep them warm. We decided to do something – by running for this election.” Some of her constituents have also suggested that she take up the cleaning and filling up of lakes and ponds, as well as creating small parks and gardens. She will also be working in re-routing truck traffic to the outer roads of the city.\n"}
{"id": "58645985", "url": "https://en.wikipedia.org/wiki?curid=58645985", "title": "Mark Hay", "text": "Mark Hay\n\nMark Edward Hay (born May 3, 1952) is an American marine ecologist. He is Regents Professor and Harry and Linda Teasley Chair in the School of Biological Sciences at the Georgia Institute of Technology. A fellow of the American Association for the Advancement of Science, he is known for is research on the coral reefs of Fiji. He received the Cody Award from the Scripps Institution of Oceanography in 2012, the Lowell Thomas Award from the Explorers Club in 2015, and the Gilbert Morgan Smith Medal from the National Academy of Sciences in 2018.\n\n"}
{"id": "56573146", "url": "https://en.wikipedia.org/wiki?curid=56573146", "title": "Methodology (journal)", "text": "Methodology (journal)\n\nMethodology: European Journal of Research Methods for the Behavioral and Social Sciences is a biannual peer-reviewed scientific journal covering social and behavioral science research methodology. It was established in 2005 from the merger of two other journals: Metodologia de las Ciencias del Comportamiento and Methods of Psychological Research-Online. It is published by Hogrefe Publishing on behalf of the European Association of Methodology, of which it is the official journal. The editors-in-chief are Jost Reinecke (University of Bielefeld) and José-Luis Padilla (University of Granada). According to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 1.143.\n"}
{"id": "7931875", "url": "https://en.wikipedia.org/wiki?curid=7931875", "title": "Neumann's law", "text": "Neumann's law\n\nNeumann's law states that the molecular heat in compounds of analogous constitution is always the same. It is named after German mineralogist and physicist Franz Ernst Neumann.\n\n"}
{"id": "3997813", "url": "https://en.wikipedia.org/wiki?curid=3997813", "title": "Newtonian gauge", "text": "Newtonian gauge\n\nIn general relativity, Newtonian gauge is a perturbed form of the Friedmann–Lemaître–Robertson–Walker line element. The gauge freedom of general relativity is used to eliminate two scalar degrees of freedom of the metric, so that it can be written as\nwhere the Latin indices \"a\" and \"b\" are summed over the \"spatial\" directions and formula_2 is the Kronecker delta. Conformal Newtonian gauge is the closely related gauge in which\nwhich is related by the simple transformation formula_4. These metrics are perturbed forms of the Friedmann–Lemaître–Robertson–Walker metric. They are called Newtonian gauge because formula_5 is the Newtonian gravitational potential of classical Newtonian gravity, which satisfies the Poisson equation formula_6 for non-relativistic matter and on scales where the expansion of the universe may be neglected. It includes only scalar perturbations of the metric: by the scalar-vector-tensor decomposition these evolve independently of the vector and tensor perturbations and are the predominant ones affecting the growth of structure in the universe in cosmological perturbation theory. The vector perturbations vanish in cosmic inflation and the tensor perturbations are gravitational waves, which have a negligible effect on physics except for the so-called B-modes of the cosmic microwave background polarization. The tensor perturbation is truly gauge independent, since it is the same in all gauges.\n\nIn a universe without anisotropic stress (that is, where the stress–energy tensor is invariant under spatial rotations, or the three principal pressures are identical) the Einstein equation sets formula_7.\n\n"}
{"id": "15183022", "url": "https://en.wikipedia.org/wiki?curid=15183022", "title": "Sergei Kurzanov", "text": "Sergei Kurzanov\n\nSergei Mikhailovich Kurzanov (Сергей Михайлович Курзанов, born 1947) is a Russian (formerly Soviet) paleontologist at the Paleontological Institute of the Russian Academy of Sciences. He is known mainly for his work in Mongolia and the ex-Soviet republics in Central Asia. In 1976 he announced the discovery of \"Alioramus\". In 1981 he announced the discovery of \"Avimimus\".\n\nIn 1998 a species of iguanodont dinosaur from Mongolia was named \"Altirhinus kurzanovi\" in his honor.\n"}
{"id": "16203914", "url": "https://en.wikipedia.org/wiki?curid=16203914", "title": "Silas C. Overpack", "text": "Silas C. Overpack\n\nSilas C. Overpack (1841–1927) was a blacksmith, wheelwright, and businessman. He owned a shop (around 1868) in downtown Manistee, Michigan, at 87 Pine Street, called \"S.C. Overpack Wagon, Carriage and Blacksmith Shop\" and is associated with the invention of Michigan logging wheels. These unusually large wagon wheels (10 feet in diameter) were used in the timber industry for hauling logs in difficult terrain. \n\nSilas Overpack was born in Chemung County, Pennsylvania, on March 20, 1841. He was the sixth of ten children. He spent his childhood there where he received his public schooling. His parents were George and Remercy (Chidester) Overpack. They left Pennsylvania around 1850 and ultimately settled in Springfield Township, Oakland County, Michigan, where George rests in the Andersonville Cemetery, Andersonville, Michigan. Remercy and a number of her descendants are interred in the Oak Grove Cemetery, Manistee, Michigan.\n\nShortly after arriving in Michigan, Overpack entered into apprenticeship of learning the wagon-making trade. In 1868 he moved to Manistee and set up a blacksmith and wagon making business downtown. There he made wagons and sleighs for the thriving lumber industry of northern Michigan. He also sold salt, mill carts, blankets, harnesses, whips, and ropes. He usually employed anywhere from twelve to fifteen people at any one time. \n\nIn 1875, he began making special pairs of extra-large wagon wheels that were in diameter called \"logging wheels\" or \"big wheels\". They were double the size of ordinary wooden wagon wheels and were for hauling logs from forests that had wet terrain in the summer and heavy snows in the winter. Where he lived fit those criteria as did the Upper Peninsula and other northern states. He sold and shipped his \"logging wheels\" all over the United States and Canada.\n\nOverpack used an image of a wooden wagon wheel with spokes as a log mark as required by the 1842 Michigan law for anyone in the timber business.\n\nIn 1871 Overpack married Manistee native Millie Magoon (1858–1932). They had three children: Roy M. (1879–1964), Nellie May (1885– ), and Stella Rae (1891–1962). Roy M. graduated from University of Michigan of Ann Arbor in 1903 in Liberal Arts and received a law degree in 1905. He worked at his father's business. In 1940 Roy was the Secretary for Chamber of Commerce, in Manistee. Nellie May went to Benton Harbor College – she is a music vocal teacher and a singer. Stella Rae attends Oakwood School where Nellie May teaches. They both became talented musicians.\n\nOverpack was a member of the board of supervisors of Manistee and also served on the city council. He belonged to the Masonic order and York Rite Masonry as commandery No. 32 and associated with the Knights Templar. He was also affiliated with the Knights of Pythias. His religion was associated with the Unitarian church, although he attended Congregational services. Silas was a Republican.\n"}
{"id": "6822551", "url": "https://en.wikipedia.org/wiki?curid=6822551", "title": "Surface engineering", "text": "Surface engineering\n\nSurface engineering is the sub-discipline of materials science which deals with the surface of solid matter. It has applications to chemistry, mechanical engineering, and electrical engineering (particularly in relation to semiconductor manufacturing).\n\nSolids are composed of a bulk material covered by a surface. The surface which bounds the bulk material is called the Surface phase. It acts as an interface to the surrounding environment. The bulk material in a solid is called the Bulk phase. \n\nThe surface phase of a solid interacts with the surrounding environment. This interaction can degrade the surface phase over time. Environmental degradation of the surface phase over time can be caused by wear, corrosion, fatigue and creep. \n\nSurface engineering involves altering the properties of the Surface Phase in order to reduce the degradation over time. This is accomplished by making the surface robust to the environment in which it will be used.it provides a cost effective material for robust design. A spectrum of topics that represent the diverse nature of the field of surface engineering includes Plating technologies, Nano and emerging technologies and Surface engineering, characterization and testing.\n\nSurface engineering techniques are being used in the automotive, aerospace, missile, power, electronic, biomedical, textile, petroleum, petrochemical, chemical, steel, cement, machine tools and construction industries including road surfacing. Surface engineering techniques can be used to develop a wide range of functional properties, including physical, chemical, electrical, electronic, magnetic, mechanical, wear-resistant and corrosion-resistant properties at the required substrate surfaces. Almost all types of materials, including metals, ceramics, polymers, and composites can be coated on similar or dissimilar materials. It is also possible to form coatings of newer materials (e.g., met glass. beta-C3N4), graded deposits, multi-component deposits etc. \n\nIn 1995, surface engineering was a ₤10 billion market in the United Kingdom. Coatings, to make surface life robust from wear and corrosion, was approximately half the market. \n\nIn recent years, there has been a paradigm shift in surface engineering from age-old electroplating to processes such as vapor phase deposition, diffusion, thermal spray & welding using like plasma, laser, ion, electron, microwave, solar beams, pulsed arc, pulsed combustion, spark, friction and induction.\n\nIt's estimated that loss due to wear and corrosion in the US is approximately $500 billion. In the US, there are around 9524 establishments (including automotive, aircraft, power and construction industries) who depend on engineered surfaces with support from 23,466 industries.\n\nThere are around 65 academic institutions world-wide engaged in surface engineering research and education.\n\nSurface cleaning, synonymously referred to as dry cleaning, is a mechanical\ncleaning technique used to reduce superficial soil, dust, grime, insect droppings,\naccretions, or other surface deposits. (Dry cleaning, as the term is used in paper\nconservation, does not employ the use of organic solvents.) Surface cleaning\nmay be used as an independent cleaning technique, as one step (usually the\nfirst) in a more comprehensive treatment, or as a prelude to further treatments\n(e.g., aqueous immersion) which may cause dirt to set irreversibly in paper\nfibers.\n\nThe purpose of surface cleaning is to reduce the potential for damage to paper\nartifacts by removing foreign material which can be abrasive, acidic,\nhygroscopic, or degradative. The decision to remove surface dirt is also for\naesthetic reasons when it interferes with the visibility of the imagery or\ninformation. A decision must be made balancing the probable care of each\nobject against the possible problems related to surface cleaning.\n\n\n\n"}
{"id": "22085930", "url": "https://en.wikipedia.org/wiki?curid=22085930", "title": "Szczeciński Park Naukowo-Technologiczny", "text": "Szczeciński Park Naukowo-Technologiczny\n\nSzczeciński Park Naukowo-Technologiczny - science park in the centre of Szczecin, in north-west Poland West Pomeranian Voivodeship. In the area: University of Szczecin, West Pomeranian University of Technology, Pomeranian Medical University, Maritime University of Szczecin, West Pomeranian Business School, Szczecin Shipyard, Zakłady Chemiczne Police SA (Police), ship (Szczecin-Świnoujście Harbour and Police Harbour), road and rail transport and Szczecin-Goleniów \"Solidarność\" Airport (Goleniów).\n\n"}
{"id": "2870168", "url": "https://en.wikipedia.org/wiki?curid=2870168", "title": "Tensor–vector–scalar gravity", "text": "Tensor–vector–scalar gravity\n\nTensor–vector–scalar gravity (TeVeS), developed by Jacob Bekenstein in 2004, is a relativistic generalization of Mordehai Milgrom's Modified Newtonian dynamics (MOND) paradigm.\nThe main features of TeVeS can be summarized as follows:\n\nThe theory is based on the following ingredients:\n\nThese components are combined into a relativistic Lagrangian density, which forms the basis of TeVeS theory.\n\nMOND is a phenomenological modification of the Newtonian acceleration law. In Newtonian gravity theory, the gravitational acceleration in the spherically symmetric, static field of a point mass formula_1 at distance formula_2 from the source can be written as\n\nwhere formula_4 is Newton's constant of gravitation. The corresponding force acting on a test mass formula_5 is\n\nTo account for the anomalous rotation curves of spiral galaxies, Milgrom proposed a modification of this force law in the form\n\nwhere formula_8 is an arbitrary function subject to the following conditions:\n\nIn this form, MOND is not a complete theory: for instance, it violates the law of momentum conservation.\n\nHowever, such conservation laws are automatically satisfied for physical theories that are derived using an action principle. This led Bekenstein to a first, nonrelativistic generalization of MOND. This theory, called AQUAL (for A QUAdratic Lagrangian) is based on the Lagrangian\n\nwhere formula_11 is the Newtonian gravitational potential, formula_12 is the mass density, and formula_13 is a dimensionless function.\n\nIn the case of a spherically symmetric, static gravitational field, this Lagrangian reproduces the MOND acceleration law after the substitutions formula_14 and formula_15 are made.\n\nBekenstein further found that AQUAL can be obtained as the nonrelativistic limit of a relativistic field theory. This theory is written in terms of a Lagrangian that contains, in addition to the Einstein–Hilbert action for the metric field formula_16, terms pertaining to a unit vector field formula_17 and two scalar fields formula_18 and formula_19, of which only formula_19 is dynamical. The TeVeS action, therefore, can be written as\n\nThe terms in this action include the Einstein–Hilbert Lagrangian (using a metric signature formula_22 and setting the speed of light, formula_23):\n\nwhere formula_25 is the Ricci scalar and formula_26 is the determinant of the metric tensor.\n\nThe scalar field Lagrangian is\n\nwhere formula_28 is a constant length, formula_29 is the dimensionless parameter and formula_30 an unspecified dimensionless function; while the vector field Lagrangian is\n\nwhere formula_32 while formula_33 is a dimensionless parameter. formula_34 and formula_33 are respectively called the scalar and vector coupling constants of the theory. The consistency between the Gravitoelectromagnetism of the TeVeS theory and that predicted and measured by the general relativity leads to formula_36\n\nIn particular, formula_37 incorporates a Lagrange multiplier term that guarantees that the vector field remains a unit vector field.\n\nThe function formula_30 in TeVeS is unspecified.\n\nTeVeS also introduces a \"physical metric\" in the form\n\nThe action of ordinary matter is defined using the physical metric:\n\nwhere covariant derivatives with respect to formula_41 are denoted by formula_42\n\nTeVeS solves problems associated with earlier attempts to generalize MOND, such as superluminal propagation. In his paper, Bekenstein also investigated the consequences of TeVeS in relation to gravitational lensing and cosmology.\n\nIn addition to its ability to account for the flat rotation curves of galaxies (which is what MOND was originally designed to address), TeVeS is claimed to be consistent with a range of other phenomena, such as gravitational lensing and cosmological observations. However, Seifert shows that with Bekenstein's proposed parameters, a TeVeS star is highly unstable, on the scale of approximately 10 seconds (two weeks). The ability of the theory to simultaneously account for galactic dynamics and lensing is also challenged. A possible resolution may be in the form of massive (around 2eV) neutrinos.\n\nA study in August 2006 reported an observation of a pair of colliding galaxy clusters, the Bullet Cluster, whose behavior, it was reported, was not compatible with any current modified gravity theories.\n\nA quantity formula_43 probing General Relativity (GR) on large scales (a hundred billion times the size of the solar system) for the first time has been measured with data from the Sloan Digital Sky Survey to be formula_44 (~16%) consistent with GR, GR plus Lambda CDM and the extended form of GR known as formula_45 theory, but ruling out a particular TeVeS model predicting formula_46. This estimate should improve to ~1% with the next generation of sky surveys and may put tighter constraints on the parameter space of all modified gravity theories.\n\nTeVeS appears inconsistent with recent measurements made by LIGO of gravitational waves.\n\n\n"}
{"id": "36643464", "url": "https://en.wikipedia.org/wiki?curid=36643464", "title": "The Black Hole War", "text": "The Black Hole War\n\nHawking proposed that information is lost in black holes, and not preserved in Hawking radiation. Susskind disagreed, arguing that Hawking's conclusions violated one of the most basic scientific laws of the universe, the conservation of information. As Susskind depicts in his book, \"The Black Hole War\" was a \"genuine scientific controversy\" between scientists favoring an emphasis on the principles of relativity against those in favor of quantum mechanics. The debate led to the holographic principle, proposed by Gerard 't Hooft and refined by Susskind, which suggested that the information is in fact preserved, stored on the boundary of a system.\n\nSean M. Carroll in the \"Wall Street Journal\" praised the book for successfully explaining the topic, despite the difficulty of the subject, in a way that lay readers could understand. Carroll writes that the\nbook contains a \"wealth of anecdotes\", and that Susskind's \"wit and storytelling abilities ... are pleasantly on display in\" the book. George Johnson of \"The New York Times\" was critical of the beginning of the book, writing that the introduction on the basic concepts of relativity and quantum mechanics was excessive, especially for readers who have already read other popular science books on theoretical physics. \"Time Magazine\"'s Lev Grossman gave the book a B+, saying that \"you could dismiss it all as nerd-on-nerd violence, but then you'd miss out on Susskind explaining why the universe is actually a hologram.\" Jesse Cohen of the \"Los Angeles Times\" criticized the book for its \"tendency to meander\" with personal anecdotes, although the book \"glows with the warmth of conversation.\" The \"New Scientist\" included the book on its 2008 editor's picks list and the \"Washington Post\" listed it as one of the best books of 2008 in their annual holiday shopping guide.\n\n"}
{"id": "22729087", "url": "https://en.wikipedia.org/wiki?curid=22729087", "title": "The Computer and the Brain", "text": "The Computer and the Brain\n\nThe Computer and the Brain is an unfinished book by mathematician John von Neumann, begun shortly before his death and first published in 1958. Von Neumann was an important figure in computer science, and the book discusses how the brain can be viewed as a computing machine. The book is speculative in nature, but von Neumann discusses several important differences between brains and computers of his day (such as processing speed and parallelism), as well as suggesting directions for future research.\n\nAt only 96 pages, the book was originally intended for Yale's Silliman lectures, but it was published posthumously. The first edition was published in 1958 with a preface by Klara Dan von Neumann. The second edition, published in 2000, contains a foreword by Paul Churchland and Patricia Churchland that places von Neumann's views in the context of science at that time. The third edition, published in 2012, features a foreword by Ray Kurzweil. It has the .\n\n"}
{"id": "40095057", "url": "https://en.wikipedia.org/wiki?curid=40095057", "title": "Time Reborn", "text": "Time Reborn\n\nTime Reborn: From the Crisis in Physics to the Future of the Universe is the fourth non-fiction book by the American theoretical physicist Lee Smolin.\n\nSmolin argues for what he calls a revolutionary view that time is real, in contrast to existing scientific orthodoxy which holds that time is merely a \"stubbornly persistent illusion\" (Einstein's words). Smolin hypothesizes that the very laws of physics are not fixed, but that they actually evolve over time.\n\n\"Time Reborn\" is divided into two parts: Part I describes established physics and its history from the time of Plato and the main established ideas, Newtonian physics (and Leibniz' philosophical views that countered Newton's e.g. on background dependent physics and his religious justification), Einstein's special and general relativity, and quantum mechanics. Part II describes Smolin's views (his \"future\" for physics, relying on his and ideas of others) on why these all are slightly wrong, that is, the need to reestablish time as fundamental (and probably space as non-fundamental, rather than vice versa, that was Einstein's view) through e.g. one idea, shape dynamics, a duality of Einstein's general relativity, that does that.\n\nSmolin argues for what he calls a revolutionary view that time is real, in contrast to existing scientific orthodoxy which holds that time is merely a \"stubbornly persistent illusion\" (Einstein's words). Smolin reasons that physicists have improperly rejected the reality of time because they confuse their mathematical models—which are timeless but deal in abstractions that do not exist—with reality. Smolin hypothesizes instead that the very laws of physics are not fixed, but that they actually evolve over time.\n\nSmolin asserts that overturning the existing orthodoxy is the best hope for finding solutions to contemporary physics problems, such as bringing gravity into line with the rest of the currently accepted models, the nature of the quantum world and its unification with spacetime and cosmology. Outside science, Smolin asserts his views have important implications for human agency, and on how our social, political, economic and environmental decisions affect our future, Smolin saying that contrary to deterministic philosophies derived from conventional physics, humans do have the power to exert control over climate change, our economic system and our technology.\n\nThe book's topic was the subject of the author's 2013 presentation at the Royal Society of Arts.\n\n\"The New York Times\"' James Gleick wrote that Smolin's arguments from science and history were \"as provocative, original, and unsettling as any I’ve read in years,\" contradicting the commonly accepted views of H.G. Wells, Hermann Minkowski, Albert Einstein, Isaac Newton and Plato; Gleick predicted it will ring false to many contemporaries in theoretical physics. Gleick further wrote that Smolin has a \"fairly puritanical view of what science should and should not do\"—disfavoring multiverses or other non-testable concepts or quests for timeless truths, but allowing that science creates “effective theories” even though they are incomplete, of limited domains, and approximate.\n\n\"Kirkus Reviews\" described the book, which omits mathematical explanations, as being as much philosophy as science, and as providing \"a flood of ideas from an imaginative thinker.\" For the Institute for Ethics and Emerging Technologies, author Rick Searle wrote that \"Time Reborn\" is \"just as much a diagnosis of contemporary economic and political ills\" as it is a book about physics. Possible economic and political similarities/\"implication\" (to physics and philosophy discussed in the book) are only mentioned in the preface and epilogue of the book.\n\n"}
