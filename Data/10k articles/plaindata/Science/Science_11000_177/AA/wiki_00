{"id": "37701294", "url": "https://en.wikipedia.org/wiki?curid=37701294", "title": "Abell 2261", "text": "Abell 2261\n\nAbell 2261 is one of 25 galaxy clusters being studied as part of the Cluster Lensing And Supernova survey with Hubble (CLASH) program, a major project to build a library of scientific data on lensing clusters.\n\nIt also has the galaxy A2261-BCG (short for Abell 2261 Brightest Cluster Galaxy) which has the largest galaxy core ever observed.\n"}
{"id": "35046077", "url": "https://en.wikipedia.org/wiki?curid=35046077", "title": "Abell 2390", "text": "Abell 2390\n\nAbell 2390 is a galaxy cluster in the Abell catalogue.\n\n"}
{"id": "37813237", "url": "https://en.wikipedia.org/wiki?curid=37813237", "title": "Alexandra Worden", "text": "Alexandra Worden\n\nAlexandra (Alex) Z. Worden (born 1970) is a marine microbial ecologist and genome scientist. She leads the microbial ecology research group at the Monterey Bay Aquarium Research Institute where she holds the position of Senior Scientist. Her laboratory also bridges with the Ocean Sciences Department at the University of California Santa Cruz where she is Professor Adjunct.\nWorden’s research focuses on the physiology and ecology of eukaryotic phytoplankton, unicellular organisms that are responsible for a large portion of ocean primary production (photosynthetic uptake of atmospheric carbon dioxide). Worden’s early work focused on methods development for directly investigating populations in the nature environment and their roles in the carbon cycle. This theme has persisted throughout her research career.\n\nAs a postdoctoral fellow at Scripps Institution of Oceanography Worden was recognized for establishing the importance of small eukaryotic phytoplankton known as picoeukaryotes. A second study by Worden while in the laboratory of Farooq Azam overturned the idea that \"Vibrio cholerae\" existed primarily attached to copepods in aquatic systems. This was considered important for understanding ecology of this human pathogen and vectors for transmission of infective cells. She and Azam also introduced the concept of Ecosystems Biology (also spelled Eco-systems Biology or (Eco)-systems Biology), coining the term in a 2004 perspective. The concept was embraced by the scientific community in several later perspectives, and is being pursued by human microbiome-biologist Jeroen Raes and microbial oceanographer Edward DeLong. A Jacques Monod conference on Marine Eco-Systems Biology was initiated in 2015.\n\nWorden pioneered eukaryotic \"targeted metagenomics\" wherein cells of particular interest are separated from the masses using flow cytometry (on a ship) and genomes are then sequenced from only the cells of greatest interest. Using this approach Worden and collaborators at the DOE Joint Genome Institute sequenced partial genomes from uncultured eukaryotic algae whilst showing the distribution of these photosynthetic protists in the ocean. Her laboratory also investigates ancestral components of land plants, evolutionary biology and distributions of uncultured taxa and interactions between viruses and phytoplankton host cells. In 2015, she and co-authors called for a \"rethinking of the marine carbon cycle\". Worden publishes in the fields of environmental microbiology, evolutionary biology, genome science and oceanography.\n\nShe is a proponent of STEM education and innovation and has highlighted the need for relevant \"...role models to inspire greater diversity and creativity\" in science.\n\nWorden started her laboratory in 2004 as Assistant Professor at the Rosenstiel School of Marine and Atmospheric Science in Miami, Florida USA. In 2004 she was also awarded a Young Investigator in Marine Microbiology Award by the Gordon and Betty Moore Foundation. In 2007 she was hired to build a microbial ecology research group at MBARI on the U.S. West Coast under the leadership of Marcia McNutt. In 2009, Worden was named a scholar of the Canadian Institute for Advanced Research (CIFAR), later becoming a fellow of CIFAR (2011). She became a GBMF Marine Investigator in 2013, an award given for her \"creativity, innovation, and potential to make major, new breakthroughs\" and was elected as a Fellow of the American Academy of Microbiology in 2016.\n\nWorden attended Wellesley College, where she received a B.A. in history, and performed a concentration in Earth, Atmospheric and Planetary Sciences coursework at the Massachusetts Institute of Technology. At the latter she worked in the laboratories of John M. Edmond, Reginald Newell and Sallie W. Chisholm. Worden received her Ph.D. from the Odum School of Ecology in 2000. Engineering is a component of her oceanographic research. Worden's initial engineering exposure was through computer programming and work after high school at BBN Technologies and with the MIT solar electric car project. The team included several individuals who leading innovators in the tech world, including Gill Pratt and Megan Smith, and was led by her brother James Worden.\n\nWorden has a partner and two children.\n\n\n"}
{"id": "2673451", "url": "https://en.wikipedia.org/wiki?curid=2673451", "title": "Amil Kumar Das", "text": "Amil Kumar Das\n\nAmil Kumar Das (1902 – February 18, 1961) was an Indian astronomer.\nDuring the International Geophysical Year, observatories in Madrid, India, and Manila were responsible for monitoring solar effects. The Kodaikanal Solar Observatory in South India performed this monitoring using their recently built solar tunnel telescope. Dr. Das was the director of the Kodaikanal observatory at this time. In 1960 he was responsible for installing a tower/tunnel telescope at the facility that would be used to perform some of the first ever helioseismology investigations.\nThe crater Das on the far side of the Moon is named after him.\n\n"}
{"id": "42231", "url": "https://en.wikipedia.org/wiki?curid=42231", "title": "Aristid Lindenmayer", "text": "Aristid Lindenmayer\n\nAristid Lindenmayer (17 November 1925 – 30 October 1989) was a Hungarian biologist. In 1968 he developed a type of formal languages that is today called L-systems or Lindenmayer Systems. Using those systems Lindenmayer modelled the behaviour of cells of plants. L-systems nowadays are also used to model whole plants.\nLindenmayer worked with yeast and filamentous fungi and studied the growth patterns of various types of algae, such as the blue/green bacteria \"Anabaena catenula\". Originally the L-systems were devised to provide a formal description of the development of such simple multicellular organisms, and to illustrate the neighbourhood relationships between plant cells. Later on, this system was extended to describe higher plants and complex branching structures.\n\nLindenmayer studied chemistry and biology at the University of Budapest from 1943 to 1948. He received his Ph.D. in plant physiology in\n1956 at the University of Michigan. In 1968 he became professor in Philosophy of Life Sciences and Biology at the University of Utrecht, The Netherlands. From 1972 onward he headed the Theoretical Biology Group at Utrecht University.\n\n"}
{"id": "14307661", "url": "https://en.wikipedia.org/wiki?curid=14307661", "title": "Arnold Spencer-Smith", "text": "Arnold Spencer-Smith\n\nArnold Patrick Spencer-Smith (1883–1916) was a British clergyman and amateur photographer who joined Sir Ernest Shackleton's Imperial Trans-Antarctic Expedition, 1914–17, as Chaplain and photographer on the Ross Sea party. The hardship of the expedition resulted in Spencer-Smith's death. Cape Spencer-Smith on White Island at is named in his honour.\n\nBorn in Streatham (he shared his birthday, 17 March, with Captain Lawrence Oates but was three years younger), he attended Westminster City School, King's College London and Queen's College, Cambridge. He did not attend his exams and was given a pass degree in history. After a few years teaching at Merchiston Castle School, Edinburgh, Spencer-Smith was ordained as deacon into the Scottish Episcopal Church in 1910, subsequently being appointed curate of All Saints, Edinburgh. He was ordained as priest shortly before leaving England to join the Aurora.\n\nIt is unclear how he came to join the expedition. One version is that he had wanted to enlist in the army at the outbreak of war, but as a clergyman was barred from combatant service. He therefore volunteered himself to Shackleton as a replacement for one of the original party who had left for active service. After arrival in Antarctica his unfamiliarity with polar work and limited physical stamina were in evidence during the first (January–March 1915) depot-laying journey, before he was sent back to base by expedition leader Aeneas Mackintosh. During the 1915 winter season he worked at the Cape Evans base, mainly in the darkroom where he sometimes held religious services.\n\nThe circumstances of the expedition, after the depletion of the shore party following the loss of SY Aurora in May 1915, meant that Spencer-Smith was required for the main depot journey to the Beardmore Glacier during the 1915–16 summer season, irrespective of his physical limitations. In this he showed no reluctance and worked tirelessly. However, worn down by the preliminary work of hauling stores up to the base depot at Minna Bluff during the four-month period September–December 1915, he was unable to sustain the physical effort required on the main depot-laying journey south, and collapsed before the Beardmore was reached. Thereafter he had to be carried on the sledge, unable to help himself and dependent on Ernest Wild for his most basic needs. The party nevertheless completed its depot-laying mission and struggled back northward in worsening weather conditions, each man growing weaker as scurvy took hold, and progress forward was with acute difficulty. Spencer-Smith, uncomplaining but in the latter stages occasionally delirious, died on the Barrier on 9 March 1916, aged 32, two days before the safety of Hut Point was finally reached. He was buried in the ice.\n\nArnold Spencer-Smith was unmarried. He dedicated a final diary entry, 7 March 1916, to his father, mother, brothers and sisters. He is commemorated by Cape Spencer-Smith on White Island at .\n\nIn 1999 a team of investigators entered Captain Scott's hut at Cape Evans, and found a wallet with three photographs of a camping expedition in it. After extensive investigations it was established that this wallet had belonged to Arnold Spencer-Smith. The wallet, mislaid in 1915, was thus found after 84 years.\n\n"}
{"id": "3779103", "url": "https://en.wikipedia.org/wiki?curid=3779103", "title": "Bediasite", "text": "Bediasite\n\nBediasite is a form or type of tektite. It originates in an area in the eastern part of the U.S. state of Texas centered on the small town of Bedias which is north west of Houston. They are found in about nine Texas Counties in an area of over . The largest specimen ever found is just over 200 grams.\n\nVirgil Barnes was one of the first scientists to study Bediasites in depth. The first identified Bediasite was brought to the University of Texas at Austin in 1936 by George D. Ramsey and was identified by Virgil Barnes.\n\nBediasites are part of the 34-million-year-old North American strewnfield coming from the Chesapeake Bay impact crater. Two strewnfields and tektite groups are associated with this impact: the black Bediasites in Texas and the green Georgiaites in Georgia.\n\n"}
{"id": "3720236", "url": "https://en.wikipedia.org/wiki?curid=3720236", "title": "Boundary conformal field theory", "text": "Boundary conformal field theory\n\nIn theoretical physics, boundary conformal field theory (BCFT) is a conformal field theory defined on a spacetime with a boundary (or boundaries). Different kinds of boundary conditions for the fields may be imposed on the fundamental fields; for example, Neumann boundary condition or Dirichlet boundary condition is acceptable for free bosonic fields. BCFT was developed by John Cardy.\n\nIn the context of string theory, physicists are often interested in two-dimensional BCFTs. The specific types of boundary conditions in a specific CFT describe different kinds of D-branes.\n\nBCFT is also used in condensed matter physics - it can be used to study boundary critical behavior and to solve quantum impurity models.\n\n\n 4. C. P. Herzog and K.-W. Huang, Boundary Conformal Field Theory and a Boundary Central Charge, JHEP 1710,189 (2017)\n\n"}
{"id": "2041081", "url": "https://en.wikipedia.org/wiki?curid=2041081", "title": "Carreau fluid", "text": "Carreau fluid\n\nCarreau fluid is a type of generalized Newtonian fluid where viscosity, formula_1, depends upon the shear rate, formula_2, by the following equation:\n\nWhere: formula_4, formula_5, formula_6 and formula_7 are material coefficients.\n\nformula_4 = viscosity at zero shear rate (Pa.s)\n\nformula_5 = viscosity at infinite shear rate (Pa.s)\n\nformula_6 = relaxation time (s)\n\nformula_7 = power index\nAt low shear rate (formula_12) a Carreau fluid behaves as a Newtonian fluid with viscosity formula_13. At intermediate shear rates (formula_14), a Carreau fluid behaves as a Power-law fluid. At high shear rate, which depends on the power index formula_15 and the infinite shear-rate viscosity formula_16, a Carreau fluid behaves as a Newtonian fluid again with viscosity formula_16.\n\nThe model was first proposed by Pierre Carreau.\n\n\n"}
{"id": "36250020", "url": "https://en.wikipedia.org/wiki?curid=36250020", "title": "Chou's invariance theorem", "text": "Chou's invariance theorem\n\nChou's invariance theorem, named after Kuo-Chen Chou, was developed to address a problem raised in bioinformatics and cheminformatics related to multivariate statistics. Where a distance that would, in standard statistical theory, be defined as a Mahalanobis distance cannot be defined in this way because the relevant covariance matrix is singular. One effective approach to solve this problem would be to reduce the dimension of the multivariate space until the relevant covariance matrix is invertible or well defined. This can be achievable by simply omitting one or more of the original components until the matrix concerned is no longer singular. Chou's invariance theorem says that it does not matter which of the components or coordinates are selected for removal because exactly the same final value would be obtained. \n\nWhen using Mahalanobis distance or covariant discriminant to calculate the similarity of two proteins based on their amino acid compositions, to avoid the divergence problem due to the normalization condition imposed to their 20 constituent components, a dimension-reduced operation is needed by leaving out one of the 20 components and making the remaining 19 components completely independent. However, which one of the 20 components should be removed? Will the result be different by removing a different component? The same problems also occur when the calculation is based on (20 + \"λ\")-D (dimensional) pseudo amino acid composition, where λ is an integer. \nGenerally speaking, to calculate the Mahalanobis distance or covariant discriminant between two vectors each with Ω normalized components, the dimension-reduced operation is needed and hence the aforementioned problems are always to occur. To address these problems, the Chou's Invariance Theorem was developed in 1995.\n\nAccording to the Chou’s invariance theorem, the outcome of the Mahalanobis distance or covariant discriminant will remain the same regardless of which one of the components is left out. Accordingly, any one of the constituent normalized components can be left out to overcome the divergence problem without changing the final result for Mahalanobis distance or covariant discriminant.\n\nThe rigorous mathematical proof for the theorem was given in the appendix of a paper by Chou, or appendix E of a review paper by Chou and Zhang \n\nThe theorem has been used in predicting protein subcellular localization, identifying apoptosis protein subcellular location, predicting protein structural classification, as well as identifying various other important attributes for proteins.\n"}
{"id": "2155752", "url": "https://en.wikipedia.org/wiki?curid=2155752", "title": "Citizen science", "text": "Citizen science\n\nCitizen science (CS; also known as community science, crowd science, crowd-sourced science, civic science, volunteer monitoring, or networked science) is scientific research conducted, in whole or in part, by amateur (or nonprofessional) scientists. Citizen science is sometimes described as \"public participation in scientific research,\" participatory monitoring, and participatory action research.\n\nThe term CS has multiple origins, as well as differing concepts. It was first defined independently in the mid-1990s by Rick Bonney in the United States and Alan Irwin in the United Kingdom. Alan Irwin, a British sociologist, defines CS as \"developing concepts of scientific citizenship which foregrounds the necessity of opening up science and science policy processes to the public\". Irwin sought to reclaim two dimensions of the relationship between citizens and science: 1) that science should be responsive to citizens' concerns and needs; and 2) that citizens themselves could produce reliable scientific knowledge. The American ornithologist Rick Bonney, unaware of Irwin's work, defined CS as projects in which nonscientists, such as amateur birdwatchers, voluntarily contributed scientific data. This describes a more limited role for citizens in scientific research than Irwin's conception of the term.\n\nThe terms \"citizen science\" and \"citizen scientists\" entered the Oxford English Dictionary (\"OED\") in June 2014. \"Citizen science\" is defined as \"scientific work undertaken by members of the general public, often in collaboration with or under the direction of professional scientists and scientific institutions\". \"Citizen scientist\" is defined as: (a) \"a scientist whose work is characterized by a sense of responsibility to serve the best interests of the wider community (now rare)\"; or (b) \"a member of the general public who engages in scientific work, often in collaboration with or under the direction of professional scientists and scientific institutions; an amateur scientist\". The first use of the term \"citizen scientist\" can be found in the magazine \"New Scientist\" in an article about ufology from October 1979.\n\nMuki Haklay cites, from a policy report for the Wilson Center entitled \"Citizen Science and Policy: A European Perspective\", an alternate first use of the term \"citizen science\" by R. Kerson in the magazine \"MIT Technology Review\" from January 1989. Quoting from the Wilson Center report: \"The new form of engagement in science received the name 'citizen science'. The first recorded example of the use of the term is from 1989, describing how 225 volunteers across the US collected rain samples to assist the Audubon Society in an acid-rain awareness raising campaign.\"\n\nA \"Green Paper on Citizen Science\" was published in 2013 by the European Commission's Digital Science Unit and Socientize.eu, which included a definition for CS, referring to \"the general public engagement in scientific research activities when citizens actively contribute to science either with their intellectual effort or surrounding knowledge or with their tools and resources. Participants provide experimental data and facilities for researchers, raise new questions and co-create a new scientific culture.\"\n\nCitizen science may be performed by individuals, teams, or networks of volunteers. Citizen scientists often partner with professional scientists to achieve common goals. Large volunteer networks often allow scientists to accomplish tasks that would be too expensive or time consuming to accomplish through other means.\n\nMany citizen-science projects serve education and outreach goals. These projects may be designed for a formal classroom environment or an informal education environment such as museums.\n\nCitizen science has evolved over the past four decades. Recent projects place more emphasis on scientifically sound practices and measurable goals for public education. Modern citizen science differs from its historical forms primarily in the access for, and subsequent scale of, public participation; technology is credited as one of the main drivers of the recent explosion of citizen science activity.\n\nIn March 2015, the Office of Science and Technology Policy published a factsheet entitled \"Empowering Students and Others through Citizen Science and Crowdsourcing\". Quoting: \"Citizen science and crowdsourcing projects are powerful tools for providing students with skills needed to excel in science, technology, engineering, and math (STEM). Volunteers in citizen science, for example, gain hands-on experience doing real science, and in many cases take that learning outside of the traditional classroom setting\".\nIn May 2016, a new open-access journal was started by the Citizen Science Association along with Ubiquity Press called \"Citizen Science: Theory and Practice\" (CS:T&P). Quoting from the editorial article titled \"The Theory and Practice of Citizen Science: Launching a New Journal\", \"CS:T&P provides the space to enhance the quality and impact of citizen science efforts by deeply exploring the citizen science concept in all its forms and across disciplines. By examining, critiquing, and sharing findings across a variety of citizen science endeavors, we can dig into the underpinnings and assumptions of citizen science and critically analyze its practice and outcomes.\"\n\nOther definitions for citizen science have also been proposed. For example, Bruce Lewenstein of Cornell University's Communication and S&TS departments describes 3 possible definitions:\n\nScientists and scholars who have used other definitions include Frank N. von Hippel, Stephen Schneider, Neal Lane and Jon Beckwith. Other alternative terminologies proposed are \"civic science\" and \"civic scientist\".\n\nFurther, Muki Haklay offers an overview of the typologies of the level of citizen participation in citizen science, which range from \"crowdsourcing\" (level 1), where the citizen acts as a sensor, to \"distributed intelligence\" (level 2), where the citizen acts as a basic interpreter, to \"participatory science\", where citizens contribute to problem definition and data collection (level 3), to \"extreme citizen science\", which involves collaboration between the citizen and scientists in problem definition, collection and data analysis.\n\nA 2014 Mashable article defines a citizen scientist as: \"Anybody who voluntarily contributes his or her time and resources toward scientific research in partnership with professional scientists.\"\n\nIn 2016 the Australian Citizen Science Association released their definition which states \"Citizen science involves public participation and collaboration in scientific research with the aim to increase scientific knowledge.\"\n\nIn 2016, the book \"Analyzing the Role of Citizen Science in Modern Research\" defined citizen science as \"work undertaken by civic educators together with citizen communities to advance science, foster a broad scientific mentality, and/or encourage democratic engagement, which allows society to deal rationally with complex modern problems\".\n\nIn a Smart City era, Citizen Science relays on various web-based tools (eg.WebGIS) and becomes Cyber Citizen Science. Some projects, such as SETI@home, use the Internet to take advantage of distributed computing. These projects are generally passive. Computation tasks are performed by volunteers' computers and require little involvement beyond initial setup. There is disagreement as to whether these projects should be classified as citizen science.\n\nThe astrophysicist and Galaxy Zoo co-founder Kevin Schawinski stated: \"We prefer to call this [Galaxy Zoo] citizen science because it's a better description of what you're doing; you're a regular citizen but you're doing science. Crowd sourcing sounds a bit like, well, you're just a member of the crowd and you're not; you're our collaborator. You're pro-actively involved in the process of science by participating.\"\n\nCompared to SETI@home, \"Galaxy Zoo volunteers do real work. They're not just passively running something on their computer and hoping that they'll be the first person to find aliens. They have a stake in science that comes out of it, which means that they are now interested in what we do with it, and what we find.\"\n\nCitizen policy may be another result of citizen science initiatives. Bethany Brookshire (pen name SciCurious) writes: \"If citizens are going to live with the benefits or potential consequences of science (as the vast majority of them will), it's incredibly important to make sure that they are not only well informed about changes and advances in science and technology, but that they also ... are able to ... influence the science policy decisions that could impact their lives.\"\n\nIn a research report published by the National Park Service in 2008, Brett Amy Thelen and Rachel K. Thiet mention the following concerns, previously reported in the literature, about the validity of volunteer-generated data:\n\nThe question of data accuracy, in particular, remains open. John Losey, who created the Lost Ladybug citizen science project, has argued that the cost-effectiveness of citizen science data can outweigh data quality issues, if properly managed.\n\nIn December 2016, authors M. Kosmala, A. Wiggins, A. Swanson and B. Simmons published a study in the journal Frontiers in Ecology and the Environment called \"Assessing Data Quality in Citizen Science\". The abstract describes how ecological and environmental CS projects have enormous potential to advance science. Also, CS projects can influence policy and guide resource management by producing datasets that are otherwise infeasible to generate. In the section \"In a Nutshell\" (pg3), four condensed conclusions are stated. They are:\n\nThey conclude that as CS continues to grow and mature, a key metric of project success they expect to see will be a growing awareness of data quality. They also conclude that CS will emerge as a general tool helping \"to collect otherwise unobtainable high-quality data in support of policy and resource management, conservation monitoring, and basic science.\"\n\nA study of Canadian lepidoptera datasets published in 2018 compared the use of a professionally curated dataset of butterfly specimen records with four years of data from a CS program, eButterfly. The eButterfly dataset was used as it was determined to be of high quality because of the expert vetting process used on the site, and there existed a historic dataset covering the same geographic area consisting of specimen data, much of it institutional. The authors note that, in this case, CS data provides both novel and complementary information to the specimen data. Five new species were reported from the CS data, and geographic distribution information was improved for over 80% of species in the combined dataset when CS data was included. \n\nIn March 2015, the state of Wyoming passed new laws (Senate Files 12 and 80) clarifying that trespassing laws applied even if the trespasser's intention was to gather data to further a U.S. government science program. This hampered some CS researchers who were collecting data while on other people's land.\n\nVarious studies have been published that explore the ethics of CS, including issues such as intellectual property and project design.(e.g.) The Citizen Science Association (CSA), based at the Cornell Lab of Ornithology, and the European Citizen Science Association (ECSA), based in the Museum für Naturkunde in Berlin, have working groups on ethics and principles.\n\nIn September 2015, the European Citizen Science Association (ECSA) published its \"Ten Principles of Citizen Science\", which have been developed by the \"Sharing best practice and building capacity\" working group of the ECSA, led by the Natural History Museum, London with input from many members of the association.\nThe medical ethics of internet crowdsourcing has been questioned by Graber & Graber in the Journal of Medical Ethics. In particular, they analyse the effect of games and the crowdsourcing project Foldit. They conclude: \"games can have possible adverse effects, and that they manipulate the user into participation\".\n\nIn the research paper \"Can citizen science enhance public understanding of science?\" by Bonney et al. 2016, statistics which analyse the economic worth of citizen science are used, drawn from two papers: i)Sauermann and Franzoni 2015, and\nii)Theobald et al. 2015. In \"Crowd science user contribution patterns and their implications\" by Sauermann and Franzoni (2015), seven projects from the Zooniverse web portal are used to estimate the monetary value of the CS that had taken place. The 7 projects are: Solar Stormwatch, Galaxy Zoo Supernovae, Galaxy Zoo Hubble, Moon Zoo, Old Weather, The Milky Way Project and Planet Hunters. Using data from 180 days in 2010, they find a total of 100,386 users participated, contributing 129,540 hours of unpaid work. Estimating at a rate of $12 an hour (an undergraduate research assistant's basic wage), the total contributions amount to $1,554,474, an average of $222,068 per project. It should be noted that the range over the 7 projects was from $22,717 to $654,130.\n\nIn \"Global change and local solutions: Tapping the unrealized potential of citizen science for biodiversity research\" by Theobald et al. 2015, the authors surveyed 388 unique biodiversity-based projects. Quoting: \"We estimate that between 1.36 million and 2.28 million people volunteer annually in the 388 projects we surveyed, though variation is great\" and that \"the range of in-kind contribution of the volunteerism in our 388 citizen science projects as between $667 million to $2.5 billion annually.\" \n\nWorldwide participation in citizen science continues to grow. A list of the top five citizen science communities compiled by Marc Kuchner and Kristen Erickson in July 2018 shows a total of 3.75 million participants, although there is likely substantial overlap between the communities. \n\nThere have been studies published which examine the place of CS within education.(e.g.) Teaching aids can include books and activity or lesson plans.(e.g.). Some examples of studies are:\n\nFrom the Second International Handbook of Science Education, a chapter entitled: \"Citizen Science, Ecojustice, and Science Education: Rethinking an Education from Nowhere\" by Mueller and Tippins (2011), acknowledges in the abstract that: \"There is an emerging emphasis in science education on engaging youth in citizen science.\" The authors also ask: \"whether citizen science goes further with respect to citizen development.\" The abstract ends by stating that the \"chapter takes account of the ways educators will collaborate with members of the community to effectively guide decisions, which offers promise for sharing a responsibility for democratizing science with others.\"\n\nFrom the journal Democracy and Education, an article entitled: \"Lessons Learned from Citizen Science in the Classroom\" by authors Gray, Nicosia and Jordan (GNJ) (2012) give a response to a study by Mueller, Tippins and Bryan (MTB) called \"The Future of Citizen Science\". GNJ begins by stating in the abstract that the study The Future Of Citizen Science: \"provides an important theoretical perspective about the future of \ndemocratized science and K12 education.\" But GRB state: \"However, the authors (MTB) fail to adequately address the existing \nbarriers and constraints to moving community-based science into the classroom.\" They end the abstract by arguing: \"that the resource constraints of scientists, teachers, and students likely pose problems to moving true democratized science into the classroom.\"\n\nIn 2014, a study was published called \"Citizen Science and Lifelong Learning\" by R. Edwards in the journal Studies in the Education of Adults. Edwards begins by writing in the abstract that CS projects have expanded over recent years and engaged CSs and professionals in diverse ways. He continues: \"Yet there has been little educational exploration of such projects to date.\" He describes that \"there has been limited exploration of the educational backgrounds of adult contributors to citizen science\". Edwards explains that CS contributers are referred to as volunteers, citizens or as amateurs. He ends the abstract: \"The article will explore the nature and significance of these different characterisations and also suggest possibilities for further research.\"\n\nIn the journal Microbiology and Biology Education a study was published by Shah and Martinez (2015) called \"Current Approaches in Implementing Citizen Science in the Classroom\". They begin by writing in the abstract that CS is a partnership between inexperienced amateurs and trained scientists. The authors continue: \"With recent studies showing a weakening in scientific competency of American students, incorporating citizen science initiatives in the curriculum provides a means to address deficiencies\". They argue that combining traditional and innovative methods can help provide a practical experience of science. The abstract ends: \"Citizen science can be used to emphasize the recognition and use of systematic approaches to solve problems affecting the community.\"\n\nIn November 2017, authors Mitchell, Triska and Liberatore published a study in Public Library of Science titled \"Benefits and Challenges of Incorporating Citizen Science into University Education\". The authors begin by stating in the abstract that CSs contribute data with the expectation that it will be used. It reports that CS has been used for first year university students as a means to experience research. They continue: \"Surveys of more than 1500 students showed that their environmental engagement increased significantly after participating in data collection and data analysis.\" However, only a third of students agreed that data collected by CSs was reliable. A positive outcome of this was that the students were more careful of their own research. The abstract ends: \"If true for citizen scientists in general, enabling participants as well as scientists to analyse data could enhance data quality, and so address a key constraint of broad-scale citizen science programs.\"\n\n\"Citizen science\" is a fairly new term but an old practice. Prior to the 20th century, science was often the pursuit of gentleman scientists, amateur or self-funded researchers such as Sir Isaac Newton, Benjamin Franklin, and Charles Darwin. By the mid-20th century, however, science was dominated by researchers employed by universities and government research laboratories. By the 1970s, this transformation was being called into question. Philosopher Paul Feyerabend called for a \"democratization of science\". Biochemist Erwin Chargaff advocated a return to science by nature-loving amateurs in the tradition of Descartes, Newton, Leibniz, Buffon, and Darwin—science dominated by \"amateurship instead of money-biased technical bureaucrats\".\n\nA study from 2016 indicates that the largest impact of citizen science is in research on biology, conservation and ecology, and is utilized mainly as a methodology of collecting and classifying data.\n\nAstronomy has long been a field where amateurs have contributed throughout time, all the way up to the present day.\n\nCollectively, amateur astronomers observe a variety of celestial objects and phenomena sometimes with equipment that they build themselves. Common targets of amateur astronomers include the Moon, planets, stars, comets, meteor showers, and a variety of deep-sky objects such as star clusters, galaxies, and nebulae. Observations of comets and stars are also used to measure the local level of artificial skyglow. One branch of amateur astronomy, amateur astrophotography, involves the taking of photos of the night sky. Many amateurs like to specialize in the observation of particular objects, types of objects, or types of events that interest them.\n\nThe American Association of Variable Star Observers has gathered data on variable stars for educational and professional analysis since 1911 and promotes participation beyond its membership on its Citizen Sky website.\n\nButterfly counts have a long tradition of involving individuals in the study of butterflies' range and their relative abundance. Two long-running programs are the UK Butterfly Monitoring Scheme (started in 1976) and the North American Butterfly Association's Butterfly Count Program (started in 1975). There are various protocols for monitoring butterflies and different organizations support one or more of transects, counts and/or opportunistic sightings. eButterfly is an example of a program designed to capture any of the three types of counts for observers in North America. Species-specific programs also exist, with monarchs the prominent example. Two examples of this involve the counting of monarch butterflies during the fall migration to overwintering sites in Mexico: (1) Monarch Watch is a continent-wide project, while (2) the Cape May Monarch Monitoring Project is an example of a local project. The Austrian project Viel-Falter investigated if and how trained and supervised pupils are able to systematically collect data about the occurrence of diurnal butterflies, and how this data could contribute to a permanent butterfly monitoring system. Despite substantial identification uncertainties for some species or species groups, the data collected by pupils was successfully used to predict the general habitat quality for butterflies.\n\nCitizen science projects have become increasingly focused on providing benefits to scientific research. The North American Bird Phenology Program (historically called the Bird Migration and Distribution records) may have been the earliest collective effort of citizens collecting ornithological information in the U.S. The program, dating back to 1883, was started by Wells Woodbridge Cooke. Cooke established a network of observers around North America to collect bird migration records. The Audubon Society's Christmas Bird Count, which began in 1900, is another example of a long-standing tradition of citizen science which has persisted to the present day. Citizen scientists help gather data that will be analyzed by professional researchers, and can be used to produce bird population and biodiversity indicators.\n\nRaptor migration research relies on the data collected by the hawkwatching community. This mostly volunteer group counts migrating accipiters, buteos, falcons, harriers, kites, eagles, osprey, vultures and other raptors at hawk sites throughout North America during the spring and fall seasons. The daily data is uploaded to hawkcount.org where it can be viewed by professional scientists and the public.\n\nSuch indices can be useful tools to inform management, resource allocation, policy and planning. For example, European breeding bird survey data provide input for the Farmland Bird Index, adopted by the European Union as a structural indicator of sustainable development. This provides a cost-effective alternative to government monitoring.\n\nSimilarly, data collected by citizen scientists as part of BirdLife Australia's has been analysed to produce the first-ever Australian Terrestrial Bird Indices.\n\nThe concept of citizen science has been extended to the ocean environment for characterizing ocean dynamics and tracking marine debris. For example, the mobile app Marine Debris Tracker is a joint partnership of National Oceanic and Atmospheric Administration and the University of Georgia. Long term sampling efforts such as the continuous plankton recorder has been fitted on ships of opportunity since 1931. Plankton collection by sailors and subsequent genetic analysis was pioneered in 2013 by Indigo V Expeditions as a way to better understand marine microbial structure and function.\n\nCitizen science has recently developed in Coral reef studies. \n\nUnderwater photography has become more and more popular since the early 2000s, resulting on millions of pictures posted every year on various websites and social media. This mass of documentation is endowed with an enormous scientific potential, as millions of tourists possess a much superior coverage power than professional scientists, who can not allow themselves to spend so much time in the field. \nAs a consequence, several participative sciences programs have been developped, supported by geo-localization and identification web sites (such as iNaturalist.org). Another example, the \"Monitoring through many eyes\" project collates thousands of underwater images of the Great Barrier Reef and provides an interface for elicitation of reef health indicators.\n\nAdditionally, the National Oceanic and Atmospheric Administration offers opportunities for volunteer participation. By taking measurements in The United States' National Marine Sanctuaries, citizens are able to contribute data to a variety of marine biology projects. By enabling these citizens, NOAA benefited from 137,000 hours of research during 2016.\n\nThere also exist protocols for auto-organization and self-teaching aimed at biodiversity-interested snorkelers, in order for them to turn their observations into sound scientific data, available for research. This kind of approach has been successfully used in Réunion island, allowing for tens of new records and even new species.\n\nCitizen science has a long tradition in Natural science. But nowadays, citizen science projects can also be found in various fields of science like Art history. For example, the Zooniverse project AnnoTate is a transcription tool developed to enable volunteers to read and transcribe the personal papers of British-born and émigré artists. The papers are drawn from the Tate Archive. Another example of citizen science in art history is ARTigo. ARTigo collects semantic data on artworks from the footprints left by players of games featuring artwork images. From these footprints, ARTigo automatically builds a semantic search engine for artworks.\n\nNewer technologies have increased the options for citizen science. Citizen scientists can build and operate their own instruments to gather data for their own experiments or as part of a larger project. Examples include amateur radio, amateur astronomy, Six Sigma Projects, and Maker activities. Most recently scientist Joshua Pearce has advocated for the creation of open-source hardware based scientific equipment that both citizen scientists and professional scientists, which can be replicated by digital manufacturing techniques such as 3D printing. Multiple studies have shown this approach radically reduces scientific equipment costs. Examples of this approach include water testing, nitrate and other environmental testing, basic biology and optics. Groups such as Public Labs, which is a community where citizen scientists can learn how to investigate environmental concerns using inexpensive DIY techniques, embody this approach.\n\nVideo technology has enabled expanded citizen science. The Citizen Science Center in the Nature Research Center wing of the North Carolina Museum of Natural Sciences has exhibits on how to get involved in scientific research and become a citizen scientist. For example, visitors can observe birdfeeders at the Prairie Ridge Ecostation satellite facility via live video feed and record which species they see.\n\nSince 2005, the Genographic Project has used the latest genetic technology to expand our knowledge of the human story, and its pioneering use of DNA testing to engage and involve the public in the research effort has helped to create a new breed of \"citizen scientist\". Geno 2.0 expands the scope for citizen science, harnessing the power of the crowd to discover new details of human population history. This includes supporting, organization and dissemination of personal DNA (genetic) testing. Like Amateur astronomy, citizen scientists encouraged by volunteer organizations like the International Society of Genetic Genealogy have provided valuable information and research to the professional scientific community.\n\nWith unmanned aerial vehicles, further citizen science is enabled. One example is the ESA's AstroDrone smartphone app for gathering robotic data with the Parrot AR.Drone.\n\nCitizens in Space (CIS), a project of the United States Rocket Academy, seeks to combine citizen science with citizen space exploration. CIS is training citizen astronauts to fly as payload operators on suborbital reusable spacecraft that are now in development. CIS will also be developing, and encouraging others to develop, citizen-science payloads to fly on suborbital vehicles. CIS has already acquired a contract for 10 flights on the Lynx suborbital vehicle, being developed by XCOR Aerospace, and plans to acquire additional flights on XCOR Lynx and other suborbital vehicles in the future.\n\nCIS believes that \"The development of low-cost reusable suborbital spacecraft will be the next great enabler, allowing citizens to participate in space exploration and space science.\"\n\nThe Internet has been a boon to citizen science, particularly through gamification. One of the first Internet-based citizen science experiments was NASA's Clickworkers, which enabled the general public to assist in the classification of images, greatly reducing the time to analyze large data sets. Another was the Citizen Science Toolbox, launched in 2003, of the Australian Coastal Collaborative Research Centre. Mozak is a game in which players create 3D reconstructions from images of actual human and mouse neurons, helping to advance understanding of the brain. One of the largest citizen science games is Eyewire, a brain-mapping puzzle game developed at the Massachusetts Institute of Technology that now has over 200,000 players. Another example is Quantum Moves, a game developed by the Center for Driven Community Research at Aarhus University, which uses online community efforts to solve quantum physics problems. The solutions found by players can then be used in the lab to feed computational algorithms used in building a scalable quantum computer.\n\nMore generally, Amazon's Mechanical Turk is frequently used in the creation, collection, and processing of data by paid citizens. There is controversy as to whether or not the data collected through such services is reliable, as it is subject to participants' desire for compensation. However, use of Mechanical Turk tends to quickly produce more diverse participant backgrounds, as well as comparably accurate data when compared to traditional collection methods.\n\nThe internet has also enabled citizen scientists to gather data to be analyzed by professional researchers. Citizen science networks are often involved in the observation of cyclic events of nature (phenology), such as effects of global warming on plant and animal life in different geographic areas, and in monitoring programs for natural-resource management. On BugGuide.Net, an online community of naturalists who share observations of arthropod, amateurs and professional researchers contribute to the analysis. By October 2014, BugGuide has over 808,718 images submitted by more than 27,846 contributors.\n\nThe Zooniverse is home to the internet's largest, most popular and most successful citizen science projects. The Zooniverse and the suite of projects it contains is produced, maintained and developed by the Citizen Science Alliance (CSA). The member institutions of the CSA work with many academic and other partners around the world to produce projects that use the efforts and ability of volunteers to help scientists and researchers deal with the flood of data that confronts them. On June 29, 2015, the Zooniverse released a new software version with a project-building tool allowing any registered user to create a project. Project owners may optionally complete an approval process to have their projects listed on the Zooniverse site and promoted to the Zooniverse community. A NASA/JPL picture to the right gives an example from one of Zooniverse's projects The Milky Way Project.\n\nThe website CosmoQuest has as its goal \"To create a community of people bent on together advancing our understanding of the universe; a community of people who are participating in doing science, who can explain why what they do matters, and what questions they are helping to answer.\n\nCrowdCrafting enables its participants to create and run projects where volunteers help with image classification, transcription, geocoding and more. The platform is powered by PyBossa software, a free and open-source framework for crowdsourcing.\n\nProject Soothe is a citizen science research project based at the University of Edinburgh. The aim of this research is to create a bank of soothing images, submitted by members of the public, which can be used to help others through psychotherapy and research in the future. Since 2015, Project Soothe has received over 600 soothing photographs from people in 23 countries. Anyone aged 12 years or over are eligible to participate in this research in two ways: (1) By submitting soothing photos that they have taken with a description of why the images make them feel soothed (2) By rating the photos that have been submitted by people worldwide for their soothability.\n\nThe bandwidth and ubiquity afforded by smartphone technology has vastly expanded the opportunities for citizen science. Examples include iNaturalist, the San Francisco project, the WildLab, Project Noah, and Aurorasurus. Due to their ubiquity, for example, Twitter, Facebook, and smartphones have been useful for citizen scientists, having enabled them to discover and propagate a new type of aurora dubbed \"STEVE\" in 2016.\n\nThere are also smartphone apps for monitoring birds, marine wildlife and other organisms, and the \"Loss of the Night\". \n\nAn Android app Sapelli is a mobile data-collection and -sharing platform designed with a particular focus on non-literate and illiterate users with little or no prior ICT experience. A smartphone focussed platform for Citizen Science applications is SPOTTERON, which creates synergy effects for projects by sharing a common feature set.\n\n\"The Crowd and the Cloud\" is a four-part series broadcast during April 2017, which examines citizen science. It shows how smartphones, computers and mobile technology enable regular citizens to become part of a 21st-century way of doing science. The programs also demonstrate how CSs help professional scientists to advance knowledge, which helps speed up new discoveries and innovations. The Crowd & The Cloud is based upon work supported by the National Science Foundation.\n\nSince 1975, in order to improve earthquake detection and collect useful information, the European-Mediterranean Seismological Centre monitors the visits of earthquake eyewitnesses to its website and relies on Facebook and Twitter.\n\nCitizen science has been used to provide valuable data in hydrology (catchment science), notably flood risk, water quality and water resource management. A growth in internet use and smartphone ownership has allowed users to collect and share real-time flood-risk information using, for example, social media and web-based forms. Although traditional data collection methods are well-established, citizen science is being used to fill the data gaps on a local level, and is therefore meaningful to individual communities. It has been demonstrated that citizen science is particularly advantageous during a flash flood because the public are more likely to witness these rarer hydrological events than scientists.\n\nThere are many CS projects in Africa and South America. Some examples in Africa are:\n\n\n\nCS projects in South America include:\n\n\n\nThe first Conference on Public Participation in Scientific Research was held in Portland, Oregon in August 2012. Citizen science is now often a theme at large conferences, such as the annual meeting of the American Geophysical Union.\n\nIn 2010, 2012 and 2014 there were three Citizen Cybersience summits, organised by the Citizen Cyberscience Centre in Geneva. The 2014 summit was hosted in London and attracted over 300 participants.\n\nIn January 2015, the ETH Zürich and University of Zürich hosted an international meeting on the \"Challenges and Opportunities in Citizen Science\".\n\nThe first citizen science conference hosted by the Citizen Science Association was in San Jose, California, in February 2015 in partnership with the AAAS conference. The Citizen Science Association conference, CitSci 2017, was held in Saint Paul, Minnesota, United States, between May 17 and 20, 2017. The conference had more than 600 attendees. The next CitSci is in March 2019 in Raleigh, USA.\n\nThe platform \"Österreich forscht\" hosts the annual Austrian citizen science conference since 2015.\n\n\n"}
{"id": "41033577", "url": "https://en.wikipedia.org/wiki?curid=41033577", "title": "Dioptrique", "text": "Dioptrique\n\n\"La dioptrique\" (in English \"Dioptrique\", \"Optics\", or \"Dioptrics\"), is a short treatise published in 1637 included in one of the \"Essays\" written with \"Discourse on the Method\" by Rene Descartes. In this essay Descartes uses various models to understand the properties of light. This essay is known as Descartes' greatest contribution to optics, as it is the first publication of the Law of Refraction.\n\nThe first discourse captures Descartes' theories on the nature of light. In the first model, he compares light to a stick that allows a blind person to discern his environment through touch. Descartes says:\n\nDescartes' second model on light uses his theory of the elements to demonstrate the rectilinear transmission of light as well as the movement of light through solid objects. He uses a metaphor of wine flowing through a vat of grapes, then exiting through a hole at the bottom of the vat.\n\nDescartes uses a tennis ball to create a proof for the laws of reflection and refraction in his third model. This was important because he was using real-world objects (in this case, a tennis ball) to construct mathematical theory. Descartes' third model creates a mathematical equation for the Law of Refraction, characterized by the angle of incidence equalling the angle of refraction. In today's notation, the law of refraction states,\n\nThe astronomer Jean-Baptiste Morin was noted as one of the first people to question Descartes' method in creating his theories.\n"}
{"id": "9917106", "url": "https://en.wikipedia.org/wiki?curid=9917106", "title": "Donald F. Sangster", "text": "Donald F. Sangster\n\nDonald F. Sangster is a Canadian economic geologist. He has worked for the Geological Survey of Canada.\n\nSangster was president of the Society of Economic Geologists in 1994.\n\n\n\n"}
{"id": "4900609", "url": "https://en.wikipedia.org/wiki?curid=4900609", "title": "Effective evolutionary time", "text": "Effective evolutionary time\n\nThe hypothesis of effective evolutionary time attempts to explain gradients, in particular latitudinal gradients, in species diversity. It was originally named \"time hypothesis\".\n\nLow (warm) latitudes contain significantly more species than high (cold) latitudes. This has been shown for many animal and plant groups, although exceptions exist (see latitudinal gradients in species diversity). An example of an exception is helminths of marine mammals, which have the greatest diversity in northern temperate seas, possibly because of small population densities of hosts in tropical seas that prevented the evolution of a rich helminth fauna, or because they originated in temperate seas and had more time for speciations there. It has become more and more apparent that species diversity is best correlated with environmental temperature and more generally environmental energy. These findings are the basis of the hypothesis of effective evolutionary time. Species have accumulated fastest in areas where temperatures are highest. Mutation rates and speed of selection due to faster physiological rates are highest, and generation times which also determine speed of selection, are smallest at high temperatures. This leads to a faster accumulation of species, which are absorbed into the abundantly available vacant niches, in the tropics. Vacant niches are available at all latitudes, and differences in the number of such niches can therefore not be the limiting factor for species richness. The hypothesis also incorporates a time factor: habitats with a long undisturbed evolutionary history will have greater diversity than habitats exposed to disturbances in evolutionary history.\n\nThe hypothesis of effective evolutionary time offers a causal explanation of diversity gradients, although it is recognized that many other factors can also contribute to and modulate them.\n\nSome aspects of the hypothesis are based on earlier studies. Bernhard Rensch, for example, stated that evolutionary rates also depend on temperature: numbers of generation in poikilotherms, but sometimes also in homoiotherms (homoiothermic), are greater at higher temperatures and the effectiveness of selection is therefore greater. Ricklefs refers to this hypothesis as \"hypothesis of evolutionary speed\" or \"higher speciation rates\". Genera of Foraminifera in the Cretaceous and families of Brachiopoda in the Permian have greater evolutionary rates at low than at high latitudes. That mutation rates are greater at high temperatures has been known since the classical investigations of Timofeev-Ressovsky et al. (1935), although few later studies have been conducted. Also, these findings were not applied to evolutionary problems.\n\nThe hypothesis of effective evolutionary time differs from these earlier approaches as follows. It proposes that species diversity is a direct consequence of temperature-dependent processes and the time ecosystems have existed under more or less equal conditions. Since vacant niches into which new species can be absorbed are available at all latitudes, the consequence is accumulation of more species at low latitudes. All earlier approaches remained without basis without the assumption of vacant niches, as there is no evidence that niches are generally narrower in the tropics, i.e., an accumulation of species cannot be explained by subdivision of previously utilized niches (see also Rapoport's rule). The hypothesis, in contrast to most other hypotheses attempting to explain latitudinal or other gradients in diversity, does not rely on the assumption that different latitudes or habitats generally have different \"ceilings\" for species numbers, which are higher in the tropics than in cold environments. Such different ceilings are thought to be, for example, determined by heterogeneity or area of the habitat. But such factors, although not setting ceilings, may well modulate the gradients.\n\nA considerable number of recent studies support the hypothesis. Thus, diversity of marine benthos, interrupted by some collapses and plateaus, has risen from the Cambrian to the Recent, and there is no evidence that saturation has been reached. Rates of diversification per time unit for birds and butterflies increase towards the tropics. Allen et al. found a general correlation between environmental temperature and species richness for North and Central American trees, for amphibians, fish, Prosobranchia and fish parasites. They showed that species richness can be predicted from the biochemical kinetics of metabolism, and concluded that evolutionary rates are determined by generation times and mutation rates both correlated with metabolic rates which have the same Boltzmann relation with temperature. They further concluded that these findings support the mechanisms for latitudinal gradients proposed by Rohde. Gillooly et al. (2002) described a general model also based on first principles of allometry and biochemical kinetics which makes predictions about generation times as a function of body size and temperature. Empirical findings support the predictions: in all cases that were investigated (birds, fish, amphibians, aquatic insects, zooplankton) generation times are negatively correlated with temperature. Brown et al.(2004) further developed these findings to a general metabolic theory of ecology. Indirect evidence points to increased mutation rates at higher temperatures, and the energy-speciation hypothesis is the best predictor for species richness of ants. Finally, computer simulations using the Chowdhury eosystem model have shown that results correspond most closely to empirical data when the number of vacant niches is kept large. Rohde gives detailed discussions of these and other examples. Of particular importance is the study by Wright et al. (2006) which was specifically designed to test the hypothesis. It showed that molecular substitution rates of tropical woody plants are more than twice as large as those of temperate species, and that more effective genetic drift in smaller tropical populations cannot be responsible for the differences, leaving only direct temperature effects on mutation rates as an explanation. Gillman et al. (2009) examined 260 mammal species of 10 orders and 29 families and found that substitution rates in the cytochrome B gene were substantially faster in species at warm latitudes and elevations, compared with those from cold latitudes and elevations. A critical examination of the data showed that this cannot be attributed to gene drift or body mass differentials. The only possibilities left are a Red Queen effect or direct effects of thermal gradients (including possibly an effect of torpor/hibernation differentials). Rohde (1992, 1978) had already pointed out that “it may well be that mammalian diversity is entirely determined by the diversity of plants and poikilothermic animals further down in the hierarchy”, i.e., by a Red Queen effect. He also pointed out that exposure to irradiation including light is known to cause mutations in mammals, and that some homoiothermic animals have shorter generation times in the tropics, which - either separately or jointly - may explain the effect found by Gillman et al. Gillman et al. (2010) extended their earlier study on plants by determining whether the effect is also found within highly conserved DNA. They examined the 18S ribosomal gene in the same 45 pairs of plants. And indeed, the rate of evolution was 51% faster in the tropical than their temperate sister species. Furthermore, the substitution rate in 18S correlated positively with that in the more variable ITS. These result lend further very strong support to the hypothesis. Wright et al. (2010) tested the hypothesis on 188 species of amphibians belonging to 18 families, using mitochondrial RNA genes 12S and 16S, and found substantially faster substitution rates for species living in warmer habitats at both lower latitudes and lower elevations. Thus, the hypothesis has now been confirmed for several genes and for plants and animals.\n\nVázquez, D.P. and Stevens, R.D. (2004) conducted a metaanalysis of previous studies and found no evidence that niches are generally narrower in the tropics than at high latitudes. This can be explained only by the assumption that niche space was not and is not saturated, having the capacity to absorb new species without affecting the niche width of species already present, as predicted by the hypothesis.\n\nSpecies diversity in the deepsea has been largely underestimated until recently (e.g., Briggs 1994: total marine diversity less than 200,000 species). Although our knowledge is still very fragmentary, some recent studies appear to suggest much greater species numbers (e.g., Grassle and Maciolek 1992: 10 million macroinvertebrates in soft bottom sediments of the deepsea). Further studies must show whether this can be verified. A rich diversity in the deepsea can be explained by the hypothesis of effective evolutionary time: although temperatures are low, conditions have been more or less equal over large time spans, certainly much larger than in most or all surface waters.\n"}
{"id": "52297623", "url": "https://en.wikipedia.org/wiki?curid=52297623", "title": "Emanuel Anthony Posselt", "text": "Emanuel Anthony Posselt\n\nEmanuel Anthony Posselt (1858–1921) was an authority on Jacquard looms and weaving. His book on the Jacquard machine is considered to be a classic.\n\nPosselt, the son of Emanuel Anthony and Elizabeth (Demuth) Posselt, was born in Reichenberg, Austrian Empire (current Liberec, Czech Republic) on 21 August 1858. Emanuel graduated in 1876 from the Imperial Government Weaving School, also in Reichenberg. During the next two years, he visited most of the important textile manufacturing cities in Europe, and during this time he managed his father's textile mills. In August 1878, he went to the United States, where he worked until 1884 for various textile mills in New England and Pennsylvania.\n\nIn 1884, Posselt became the inaugural director of the textile division of Pennsylvania Museum and School of Industrial Art. In 1891, he resigned that position to accept the editorship of the Textile Record of North America, at the time a leading trade journal.\n\nPosselt then established a private textile academy and a textile trade book business. He published his own work on the Jacquard machine, the technology of textile design, textile calculations, and cotton, wool and silk industries; as well as works by other authors, such as Gardner on wool dyeing. The series \"Hand Books of the Textile Industry\" and \"Posselt's Textile Library\", as well as \"Posselt's Textile Journal\" that ran between 1907 and 1923, are his own. The last was the immediate forerunner of Textile World.\n\nPosselt died in 1921.\n\nPosselt married in July 1884 at Colchester, Vermont Anna Clera Pollinger, by whom he had three children: Anna (1896), Elwood (1906) and Gertrude (1908).\n\n"}
{"id": "209561", "url": "https://en.wikipedia.org/wiki?curid=209561", "title": "Enewetak Atoll", "text": "Enewetak Atoll\n\nEnewetak Atoll (; also spelled Eniwetok Atoll or sometimes Eniewetok; , , or , ) is a large coral atoll of 40 islands in the Pacific Ocean and with its 850 people forms a legislative district of the Ralik Chain of the Marshall Islands. With a land area total less than , it is no higher than 5 meters and surrounds a deep central lagoon, in circumference. It is the second-westernmost atoll of the Ralik Chain and is west from Bikini Atoll. \n\nNuclear testing by the US totaling more than 30 megatons of TNT took place during the cold war; in 1977–1980, a concrete dome (the Runit Dome) was built on Runit Island to deposit radioactive soil and debris.\n\nThe Runit Dome is deteriorating and could be breached by a typhoon, though the sediments in the lagoon are even more radioactive than those which are contained.\n\nThe U.S. government referred to the atoll as \"Eniwetok\" until 1974, when it changed its official spelling to \"Enewetak\" (along with many other Marshall Islands place names, to more properly reflect their pronunciation by the Marshall Islanders).\n\nEnewetak Atoll formed atop a seamount. The seamount was formed in the late Cretaceous. This seamount is now about below sea level. It is made of basalt, and its depth is due to a general subsidence of the entire region and not because of erosion.\n\nEnewetak has a mean elevation above sea level of .\n\nHumans have inhabited the atoll since about 1,000 B.C.\n\nThe first European visitor to Enewetak, Spanish explorer Alvaro de Saavedra, arrived on 10 October 1529. He called the island \"Los Jardines\" (The Gardens). In 1794 sailors aboard the British merchant sloop \"Walpole\" called the islands \"Brown's Range\" (thus the Japanese name \"Brown Atoll\"). It was visited by about a dozen ships before the establishment of the German colony of the Marshall Islands in 1885. With the rest of the Marshalls, Enewetak was captured by the Imperial Japanese Navy in 1914 during World War I and mandated to the Empire of Japan by the League of Nations in 1920. The Japanese administered the island under the South Pacific Mandate, but mostly left affairs in hands of traditional local leaders until the start of \nWorld War II. The atoll, together with other part of Marshall Islands located to the west of 164°E, was placed under the governance of Pohnpei district during the Japanese administration period, and is different from rest of Marshall Islands.\n\nIn November 1942, the Japanese built an airfield on Engebi Island. As they used it only for refueling planes between Truk and islands to the east, no aviation personnel were stationed there and the island had only \ntoken defenses. When the Gilberts fell to the United States, the Imperial Japanese Army assigned defense of the atoll to the 1st Amphibious Brigade, formed from the 3rd Independent Garrison, which had previously been stationed in Manchukuo. The 1st Amphibious Brigade arrived on January 4, 1944. Some 2,586 of its 3,940 men were left to defend Eniwetok Atoll, supplemented by aviation personnel, civilian employees, and laborers. However, they were unable to finish the fortifications before the American attack came in February. During the ensuing Battle of Eniwetok, the Americans captured Enewetak in a five-day amphibious operation. Fighting mainly took place on Engebi Islet, site of the most important Japanese installation, although some combat occurred on the main islet of Enewetak itself and on Parry Island, where there was a Japanese seaplane base.\n\nFollowing its capture, the anchorage at Enewetak became a major forward base for the U.S. Navy. The daily average of ships present during the first half of July 1944 was 488; during the second half of July the daily average number of ships at Enewetak was 283. \n\nIn 1950, John C. Woods, who executed the Nazi war criminals convicted at the Nuremberg Trials, was accidentally electrocuted here.\n\nAfter the end of World War II, Enewetak came under the control of the United States as part of the Trust Territory of the Pacific Islands until the independence of the Marshall Islands in 1986. During its tenure, the United States evacuated the local residents many times, often involuntarily. The atoll was used for nuclear testing as part of the Pacific Proving Grounds. Before testing commenced, the U.S. exhumed the bodies of United States servicemen killed in the Battle of Enewetak and returned them to the United States to be re-buried by their families. Forty-three nuclear tests were fired at Enewetak from 1948 to 1958.\n\nThe first hydrogen bomb test, code-named Ivy Mike, occurred in late 1952 as part of Operation Ivy; it vaporized the islet of Elugelab. This test included B-17 Flying Fortress drones to fly through the radioactive cloud to test onboard samples. B-17 mother ships controlled the drones while flying within visual distance of them. In all 16 to 20 B-17s took part in this operation, of which half were controlling aircraft and half were drones. To examine the explosion clouds of the nuclear bombs in 1957/58 several rockets (mostly from rockoons) were launched. One USAF airman was lost at sea during the tests.\nA radiological survey of Enewetak was conducted from 1972 to 1973. In 1977, the United States military began decontamination of Enewetak and other islands. During the three-year, $100 million cleanup process, the military mixed more than of contaminated soil and debris from the islands with Portland cement and buried it in an atomic blast crater on the northern end of the atoll's Runit Island. The material was placed in the deep, wide crater created by the May 5, 1958, \"Cactus\" nuclear weapons test. A dome composed of 358 concrete panels, each thick, was constructed over the material. The final cost of the cleanup project was $239 million. The United States government declared the southern and western islands in the atoll safe for habitation in 1980, and residents of Enewetak returned that same year. The military members who participated in that cleanup mission are suffering from many health issues, but the U.S. Government is refusing to provide health coverage. \n\nSection 177 of the 1983 Compact of Free Association between the governments of the United States and the Marshall Islands establishes a process for Marshallese to make a claim against the United States government as a result of damage and injury caused by nuclear testing. That same year, an agreement was signed to implement Section 177 which established a $150 million trust fund. The fund was intended to generate $18 million a year, which would be payable to claimants on an agreed-upon schedule. If the $18 million a year generated by the fund was not enough to cover claims, the principal of the fund could be used. A Marshall Islands Nuclear Claims Tribunal was established to adjudicate claims. In 2000, the tribunal made a compensation award to the people of Enewetak consisting of $107.8 million for environmental restoration; $244 million in damages to cover economic losses caused by loss of access and use of the atoll; and $34 million for hardship and suffering. In addition, as of the end of 2008, another $96.658 million in individual damage awards were made. Only $73.526 million of the individual claims award has been paid, however, and no new awards were made between the end of 2008 and May 2010. Due to stock market losses, payments rates that have outstripped fund income, and other issues, the fund was nearly exhausted as of May 2010 and unable to make any additional awards or payments. A lawsuit by Marshallese arguing that \"changed circumstances\" made Nuclear Claims Tribunal unable to make just compensation was dismissed by the Supreme Court of the United States in April 2010. \n\nThe 2000 environmental restoration award included funds for additional cleanup of radioactivity on Enewetak. Rather than scrape the topsoil off, replace it with clean topsoil, and create another radioactive waste repository dome at some site on the atoll (a project estimated to cost $947 million), most areas still contaminated on Enewetak were treated with potassium. Soil that could not be effectively treated for human use was removed and used as fill for a causeway connecting the two main islands of the atoll (Enewetak and Parry). The cost of the potassium decontamination project was $103.3 million.\n\nIt is projected that the majority of the atoll will be fit for human habitation by the year 2026–2027 after nuclear decay, de-contamination and environmental remediation efforts create sufficient dose reductions. However, in November 2017, the Australian Broadcasting Corporation reported that rising sea levels caused by climate change are seeping inside the dome, causing radioactive material to leak out.\n\nMarshall Islands Public School System operates Enewetak Elementary School. Marshall Islands High School on Majuro serves the community.\n\nMen from the 110th Naval Construction Battalion arrived on Eniwetok between 21 and 27 February 1944 and began clearing the island for construction of a bomber airfield. A by runway with taxiways and supporting facilities was built. The first plane landed on 11 March. By 5 April the first operational bombing mission was conducted.\nThe base was later named for Lieutenant John H. Stickell.\n\nIn mid-September 1944 operations at Wrigley Airfield on Engebi Island were transferred to Eniwetok.\n\nUS Navy and Marine units based at Eniwetok included:\n\n\nThe airstrip is now abandoned and its surface partially covered by sand.\n\nThe Imperial Japanese Navy had developed a seaplane base on Parry Island. Following its capture on 22 February, Seebees from the 110th Naval Construction Battalion expanded the base, building a coral-surfaced parking area and shops for minor aircraft and engine overhaul. A marine ways was installed on a Japanese pier and boat-repair shops were also erected.\n\nUS Navy and Marine units based at Parry Island included:\n\n\non\n\n\n"}
{"id": "10575807", "url": "https://en.wikipedia.org/wiki?curid=10575807", "title": "Expectancy-value theory", "text": "Expectancy-value theory\n\nExpectancy-value theory has been developed in many different fields including education, health, communications, marketing and economics. Although the model differs in its meaning and implications for each field, the general idea is that there are expectations as well as values or beliefs that affect subsequent behavior.\n\nJohn William Atkinson developed the expectancy-value theory in the 1950s and 1960s in an effort to understand the achievement motivation of individuals. In the 1980s, Jacquelynne Eccles expanded this research into the field of education. According to expectancy-value theory, students' achievement and achievement related choices are most proximally determined by two factors, expectancies for success, and subjective task values. Expectancies refer to how confident an individual is in his or her ability to succeed in a task whereas task values refer to how important, useful, or enjoyable the individual perceives the task. Theoretical and empirical; work suggests that expectancies and values interact to predict important outcomes such as engagement, continuing interest, and academic achievement. Other factors, including demographic characteristics, stereotypes, prior experiences, and perceptions of others' beliefs and behaviors affect achievement related outcomes indirectly through these expectancies and values. This model has most widely been applied and used in research in the field of education.\n\nExpectancies are specific beliefs individuals have regarding their success on certain tasks they will carry out in the short-term future or long-term future. The expectancies an individual has shaped their behaviors as well as the choices they make. For example, a high school student might believe that they really struggle on standardized tests. This leads them to expect that they will perform poorly on the SAT. These beliefs then impact their actual performance on the SAT. These expectancies are tied to concepts such as self-concept and self-efficacy. Self-concept is a broad concept that involves one's beliefs about their own abilities to reach their goals. Self-efficacy is similar in an academic context, because it involves one's beliefs about their abilities and competence; however, it is specific to certain domains, such as math and history.\n\nAccording to Eccles and colleagues subjective task value can be thought of the motivation that allows an individual to answer the question \"Do I Want to do This Activity and Why?\" Subjective task values can be broken into four subcategories: Attainment Value (Importance for identity or self), Intrinsic Value (Enjoyment or Interest), Utility Value (Usefulness or Relevance), and Cost(loss of time, overly-high effort demands, loss of valued alternatives, or negative psychological experiences such as stress). Traditionally, attainment value and intrinsic value are more highly correlated. What's more, these two constructs tend to be related to intrinsic motivation, interest, and task persistence. Alternatively, utility value has both intrinsic and extrinsic components. and has been related to both intrinsic and extrinsic outcomes such as course performance and interest. Other research shows that utility value has time-dependent characteristics as well. Cost has been relatively neglected in the empirical research; however, the construct has received some attention more recently. Feather combined subjective task values with more universal human values and suggested that the former are just one type of general human motives that help to direct behavior.\n\nResearchers have found that expectancies and values can be distinguished as separate types of motivation as early as 6 years old. Similarly, types of value (e.g., attainment vs. utility) can be distinguished within an academic domain as early as fifth grade. Generally speaking, Eccles and colleagues implicate a wide array of different factors that determine an individual's expectancies and values, including: \nExperts agree that student motivation tends to decline throughout their time in school. Longitudinal research has confirmed this general trend of motivational decline and also demonstrated that motivation is domain specific. Researchers have also demonstrated that there are gender differences in motivation. Motivation decline is particularly steep for Math achievement, but less so for reading or sports domains among both boys and girls. Researchers offer two general explanations for these declines in motivation. The first is that students' conceptualizations of different domains become more complex and nuanced—they differentiate between subdomains, which results in an appearance of mean-level decrease. In fact, children as young as 11 years old have demonstrated that they can differentiate between academic domains. The second is that the focus of their environment changes as they age. As students reach higher grades, the focus shifts from learning to achievement. In fact, a large body of research exists showing that shifts from learning to performance as an educational focus can be detrimental to student motivation.\n\nExpectancy-value theory constructs can and have been applied to intervention programs that strive to change motivational beliefs. These interventions are able to increase expectancy and value or decrease cost. Such interventions not only target motivation, but also ultimately increase general student achievement and help to close traditionally problematic achievement gaps. For example, value- focused interventions have been developed to help teachers design their curriculum in ways that allow students to see the connections between the material they learn in the classroom and their own lives. Promoting interest and performance in high school science classes. This intervention is able to boost student's performance and interest, particularly for students who have low initial expectancy. According to the expectancy-value theory, this intervention is effective because it increases students interest in the material.\n\nExpectancy-value theory was originally created in order to explain and predict individual's attitudes toward objects and actions. Originally the work of psychologist Martin Fishbein, the theory states that attitudes are developed and modified based on assessments about beliefs and values. Primarily, the theory attempts to determine the mental calculations that take place in attitude development. Expectancy-value theory has been used to develop other theories and is still utilized today in numerous fields of study.\n\nDr. Martin Fishbein is credited with developing the expectancy-value theory (EVT) in the early to mid-1970s. It is sometimes referred to as Fishbein's expectancy-value theory or simply expectancy-value model. The primary work typically cited by scholars referring to EVT is Martin Fishbein and Icek Ajzen's 1975 book called \"Belief, Attitude, Intention, and Behavior: An Introduction to Theory and Research\". The seed work of EVT can be seen in Fishbein's doctoral dissertation, \"A Theoretical and Empirical Investigation of the Interrelation between Belief about an Object and the Attitude toward that Object\" (1961, UCLA) and two subsequent articles in 1962 and 1963 in the journal \"Human Relations\". Fishbein's work drew on the writings of researchers such as Ward Edwards, Milton J. Rosenberg, Edward Tolman, and John B. Watson.\n\nEVT has three basic components. First, individuals respond to novel information about an item or action by developing a belief about the item or action. If a belief already exists, it can and most likely will be modified by new information. Second, individuals assign a value to each attribute that a belief is based on. Third, an expectation is created or modified based on the result of a calculation based on beliefs and values. For example, a student finds out that a professor has a reputation for being humorous. The student assigns a positive value to humor in the classroom, so the student has the expectation that their experience with the professor will be positive. When the student attends class and finds the professor humorous, the student calculates that it is a good class. EVT also states that the result of the calculation, often called the \"attitude\", stems from complex equations that contain many belief/values pairs. Fishbein and Ajzen (1975) represented the theory with the following equation where attitudes (a) are a factorial function of beliefs (b) and values (v).\n\nTheory of Reasoned Action:\nFormula\nIn its simplest form, the TRA can be expressed as the following equation:\nformula_1\n\nwhere:\nformula_2 = behavioral intention\n\nformula_3 = one's attitude toward performing the behavior\n\nformula_4 = empirically derived weights\n\nformula_5 = one's subjective norm related to performing the behavior\n\nIn the late 1970s and early 1980s, Fishbein and Ajzen expanded expectancy-value theory into the theory of reasoned action (TRA). Later Ajzen posited the theory of planned behavior (TPB) in his book \"Attitudes, Personality, and Behavior\" (1988). Both TRA and TPB address predictive and explanatory weaknesses with EVT and are still prominent theories in areas such as health communication research, marketing, and economics. Although not used as much since the early 1980s, EVT is still utilized in research within fields as diverse as audience research (Palmgreen & Rayburn, 1985) advertising (Shoham, Rose, & Kahle 1998; Smith & Vogt, 1995), child development (Watkinson, Dwyer, & Nielsen, 2005), education (Eklof, 2006; Ping, McBride, & Breune, 2006), health communication (Purvis Cooper, Burgoon, & Roter, 2001; Ludman & Curry, 1999), and organization communication (Westaby, 2002).\n\n\n\n"}
{"id": "37936179", "url": "https://en.wikipedia.org/wiki?curid=37936179", "title": "FRUCT", "text": "FRUCT\n\nThe Finnish-Russian University Cooperation in Telecommunication (FRUCT) is an independent Open Innovation Association developing ICT R&D ecosystem of Russia and Finland. In the beginning association was oriented on technologies of Nokia corporation. Association is known for its educational and R&D activities focused on improvement of the innovation ecosystem of Russia and Finland, competitiveness of the graduate students and development of cooperation between universities and industrial research groups.\n\nHistory of FRUCT Association started in 2007 when Sergey Balandin (Principal Scientist of Nokia Research Center), Eugeny Krouk (Professor of Saint Petersburg State University of Aerospace Instrumentation) and Jarkko Paavola (Professor of University of Turku) agreed to create joint work group to increase relevance and quality of students' research by involving IT and ICT experts from academia and industry to supervision of the projects. From the beginning FRUCT attracted attention of the regional media. The first seminar of the work group was held in St. Petersburg in May 2007, where the group was reorganized into the independent informal open innovations community. FRUCT Association is voluntary community that combines R&D forces of its members using Open Innovations principles to create critical mass of competences and world-class competitive results.\nNowadays more than twenty teams from universities of Russia, Finland, Ukraine and Denmark, , Nokia, Nokia Siemens Networks have joined FRUCT Association. The Association is represented and managed by companies FRUCT LLC (Russia) and FRUCT Oy (Finland). FRUCT Association is an official partner of IEEE Communications Society and a member of European Connected Health Alliance.\n\nFRUCT Association is an incubator of competences and new businesses developed in research cooperation of universities and industry. The main aim of FRUCT projects is gradual establishment of cooperation and long-term partnership between the member teams. FRUCT Association actively involves in research process undergraduates and PhD students, helps in formation of research communities in IT and ICT areas and promotes prestige of research career among young people. In most of FRUCT projects undergraduates and PhD students are the main developers, who solve problems with help and supervision of academia and industrial experts.\nThe set of activities is targeted to create the complete innovation cycle for competence and business development. The cycle starts by identification of the most promising research topics based on public information about long-term research vision and priorities of industrial leaders. Depending on the problem scale and long-team importance of the topic, FRUCT creates working group or project team to address it. Each group and project team is formed as a distributed R&D consortium with clear definition of member roles. The teams solve scientific and technological tasks based on own vision and industrial requests. Administratively project team members belong to regional FRUCT laboratories. FRUCT laboratories are organized at regional FRUCT universities. Most of the laboratory members are official employees of universities, which most of time working for FRUCT projects. After the required level of competences is gained the team either organizes innovation startup or start large research project by using regional and cross-border grants.\n\nIdeas of FRUCT projects can be originated by university professors and industrial experts. In this case the project initiator takes obligation to be the project supervisor. Also undergraduate and PhD students can propose topics of new projects. Student proposals are reviewed and commented by FRUCT advisory board. The preference is given to proposals that have large R&D potential and project targeted in development of practical applications and innovative software-hardware solutions. For approved proposals FRUCT appoints supervisors and scientific advisors.\nAccepted FRUCT projects can apply for support to cover part of direct implementation expenses, e.g., traveling to conferences, books, purchases of hardware, etc. The best students are recommended to take part in various contests, grant and scholarship programs, etc.\nThe project progress should be presented twice a year at FRUCT conferences. Project progress update consists of a report in form of conference paper, presentation and training or demo when applicable. As a result of each project at least one paper should be published in the proceedings of an international conference or journal. FRUCT conference has its own proceedings and the best papers from them are recommended for IEEE and journals publication. In addition, for papers accepted to publication in the recommended conferences, the main student author can apply for a grant to cover related travel expenses. Such travel grants are provided by FRUCT industrial members, e.g., Nokia Corporation.\n\nActualization and renewal of education is the key priority of FRUCT association. Association regularly organizes open free of charge conferences, trainings and special courses. The main FRUCT conferences are organized twice a year - in the last week of April (in Russia) and second week of November (in Finland). The conference program consists of technological trainings, lectures of world-class IT and ICT experts, reports of FRUCT teams and demos of developed solutions. The conference length is 5 days, including days for trainings and developer contests. Accepted conference papers are published in proceedings (), which are also available for free download at FRUCT site.\nFRUCT organizes free-of-charge winter and summer schools (duration 1–3 weeks). Main topics of the schools are chosen accordingly with the industrial trends and regional FRUCT teams priorities. FRUCT cooperates and is a partner in many international educational programs such as PERCCOM.\nActive academic exchange between association members is used for popularizations of new disciplines and courses, plus organization of open lectures. When it is appropriate the academic course is transformed to intensive 3–7 days course. With the help of the industrial partners best students are getting material support to participate in such programs.\nFRUCT supporting development of the exchange programs between FRUCT member universities. For example, international master programs in IT field of Tampere University of Technology and Erasmus Mundus NordSecMob of .\n\nFRUCT supports development and coordinates work of 4 professional communities: Russian Mobile Linux Community, Russian Qt community, Regional mobile Healthcare (m-Health) community and regional IoT and Smart Spaces community «Are You Smart».\nThe network of professional communities is implemented by FRUCT subsidiary E-WeREST (acronym of East-West Research and Education Society on Telecommunications). Regional activities of E-WeREST communities are coordinated by FRUCT labs at , LETI, YarSU, NNGU, SUAI and other universities.\n\n\n"}
{"id": "14075975", "url": "https://en.wikipedia.org/wiki?curid=14075975", "title": "Fuzzy mathematics", "text": "Fuzzy mathematics\n\nFuzzy mathematics forms a branch of mathematics related to fuzzy set theory and fuzzy logic. It started in 1965 after the publication of Lotfi Asker Zadeh's seminal work \"Fuzzy sets\". A fuzzy subset \"A\" of a set \"X\" is a function \"A:X→L\", where \"L\" is the interval [0,1]. This function is also called a membership function. A membership function is a generalization of a characteristic function or an indicator function of a subset defined for \"L\" = {0,1}. More generally, one can use a complete lattice \"L\" in a definition of a fuzzy subset \"A\"\n\nThe evolution of the fuzzification of mathematical concepts can be broken down into three stages: \n\nUsually, a fuzzification of mathematical concepts is based on a generalization of these concepts from characteristic functions to membership functions. Let \"A\" and \"B\" be two fuzzy subsets of \"X\". \nIntersection \"A\" ∩ \"B\" and union \"A\" ∪ \"B\" are defined as follows: (\"A\" ∩ \"B\")(\"x\") = min(\"A\"(\"x\"),\"B\"(\"x\")), (\"A\" \"∪\" \"B\")(\"x\") = max(\"A\"(\"x\"),\"B\"(\"x\")) for all \"x\" ∈ \"X\". Instead of \"min\" and \"max\" one can use t-norm and t-conorm, respectively \n, for example, \"min(a,b)\" can be replaced by multiplication \"ab\". A straightforward fuzzification is usually based on \"min\" and \"max\" operations because in this case more properties of traditional mathematics can be extended to the fuzzy case. \n\nA very important generalization principle used in fuzzification of algebraic operations is a closure property. Let * be a binary operation on \"X\". The closure property for a fuzzy subset \"A\" of \"X\" is that for all \"x,y\" ∈ \"X\", \"A\"(\"x\"*\"y\") ≥ min(\"A\"(\"x\"),\"A\"(\"y\")). Let (\"G\",*) be a group and \"A\" a fuzzy subset of \"G\". Then \"A\" is a fuzzy subgroup of \"G\" if for all \"x,y\" in \"G\", \"A\"(\"x\"*\"y\") ≥ min(\"A\"(\"x\"),\"A\"(\"y\")). \n\nA similar generalization principle is used, for example, for fuzzification of the transitivity property. Let \"R\" be a fuzzy relation in \"X\", i.e. \"R\" is a fuzzy subset of \"X×X\". Then \"R\" is transitive if for all \"x,y,z\" in \"X\", \"R\"(\"x\",\"z\") ≥ min(\"R\"(\"x\",\"y\"),\"R\"(\"y\",\"z\")).\n\nFuzzy subgroupoids and fuzzy subgroups were introduced in 1971 by A. Rosenfeld\n. Hundreds of papers on related topics have been published. Recent results and references can be found in \n\nand.\n\nMain results in fuzzy fields and fuzzy Galois theory are published in a 1998 paper.\n\nFuzzy topology was introduced by C.L. Chang in 1968 and further was studied in many papers.\n\nMain concepts of fuzzy geometry were introduced by Tim Poston in 1971, A. Rosenfeld in 1974, by J.J. Buckley and E. Eslami in 1997 and by D. Ghosh and D. Chakraborty in 2012-14 \n\nBasic types of fuzzy relations were introduced by Zadeh in 1971.\n\nThe properties of fuzzy graphs have been studied by A. Kaufman, A. Rosenfel, and by R.T. Yeh and S.Y. Bang. Recent results can be found in a 2000 article.\n\nPossibility theory, nonadditive measures, fuzzy measure theory and fuzzy integrals are studied in the cited articles and treatises.\n\nMain results and references on formal fuzzy logic can be found in these citations.\n\n\n"}
{"id": "43608439", "url": "https://en.wikipedia.org/wiki?curid=43608439", "title": "Harvey Barnett", "text": "Harvey Barnett\n\nTudor Harvey Barnett (25 December 1925 – 23 June 1995) was an Australian intelligence officer who was Director-General of Security, the head of the Australian Security Intelligence Organisation (ASIO), from 1981 to 1985.\n\nBarnett was born in Albany, Western Australia to a family of shopkeepers. He attended an Anglican boarding school near Perth, and in February 1944 enslisted in the Royal Australian Navy during World War II. At the time of his discharge on 29 May 1956, he held the rank of Lieutenant in the RANVR Special Branch and was stationed at the shore training base HMAS \"Cerberus\".\n\nAfter the war, Barnett returned to university to complete a Bachelor of Arts at the University of Western Australia, where he was a contemporary of future Prime Minister Bob Hawke. He spent some time travelling in Europe where he taught in England and Germany.\n\nOn his return to Australia in the mid-1950s, Barnett was recruited into the Australian Secret Intelligence Service (ASIS), Australia's foreign intelligence agency. He was head of the Australian stations in Singapore, Cambodia and South Vietnam, and later rose to the position of Deputy Director-General of ASIS.\n\nIn September 1981, Barnett was appointed Director-General of Security. On 20 April 1983, Barnett requested and was granted an urgent meeting with the newly elected Prime Minister, Bob Hawke, whereupon he briefed him on what he called a 'matter of national security'—ASIO was aware of several meetings between David Combe, the national secretary of the Australian Labor Party, and First Secretary of the Soviet Embassy in Australia, Valery Ivanov. In what became known as the Combe–Ivanov affair, Hawke resolved to remove any possibility of Combe being recruited by Soviet intelligence. Ivanov was expelled from Australia, and although the second Hope Royal Commission in 1984 established that attempts were being made to recruit Combe, no intelligence breaches had taken place.\n\nThree years after his 1985 retirement from ASIO, Barnett published his memoirs, \"Tale of the Scorpion\" ().\n"}
{"id": "46187677", "url": "https://en.wikipedia.org/wiki?curid=46187677", "title": "History of human migration", "text": "History of human migration\n\nHuman migration is the movement by people from one place to another with the intention of settling temporarily or permanently in the new location. It typically involves movements over long distances and from one country or region to another. \n\nHistorically, early human migration includes the peopling of the world, i.e. migration to world regions where there was previously no human habitation, during the Upper Paleolithic. Since the Neolithic, most migrations (except for the peopling of remote regions such as the Arctic or the Pacific), migration was predominantly warlike, consisting of conquest or \"Landnahme\" on the part of expanding populations. Colonialism involves expansion of sedentary populations into previously only sparsely settled territories or territories with no permanent settlements. In the modern period, human migration has primarily taken the form of migration within and between existing sovereign states, either controlled (legal immigration) or uncontrolled and in violation of immigration laws (illegal immigration).\n\nMigration can be voluntary or involuntary. Involuntary migration includes forced displacement (in various forms such as deportation, slave trade, trafficking in human beings) and flight (war refugees, ethnic cleansing).\n\nThe pre-modern migration of human populations begins with the movement of \"Homo erectus\" out of Africa across Eurasia about 1.75 million years ago. \"Homo sapiens\" appears to have occupied all of Africa about 150,000 years ago; some members of this species moved out of Africa 70,000 years ago (or, according to more recent studies, as early as 125,000 years ago into Asia, and even as early as 270,000 years ago), and had spread across Australia, Asia and Europe by 40,000 BC.Migration to the Americas took place 20,000 to 15,000 years ago. By 2000 years ago humans had established settlements in most of the Pacific Islands. Major population-movements notably include those postulated as associated with the Neolithic Revolution and with Indo-European expansion. The Early Medieval Great Migrations including Turkic expansion have left significant traces. In some places, such as Turkey and Azerbaijan, there was a substantial cultural transformation after the migration of relatively small elite populations. Historians see elite-migration parallels in the Roman and Norman conquests of Britain, while \"the most hotly debated of all the British cultural transitions is the role of migration in the relatively sudden and drastic change from Romano-Britain to Anglo-Saxon Britain\", which may be explained by a possible \"substantial migration of Anglo-Saxon Y chromosomes into Central England (contributing 50%–100% to the gene pool at that time).\"\n\nEarly humans migrated due to many factors, such as changing climate and landscape and inadequate food-supply. The evidence indicates that the ancestors of the Austronesian peoples spread from the South Chinese mainland to the island of Taiwan around 8,000 years ago. Evidence from historical linguistics suggests that seafaring peoples migrated from Taiwan, perhaps in distinct waves separated by millennia, to the entire region encompassed by the Austronesian languages. Scholars believe that this migration began around 6,000 years ago. Indo-Aryan migration from the Indus Valley to the plain of the River Ganges in Northern India is presumed to have taken place in the Middle to Late Bronze Age, contemporary with the Late Harappan phase in India (around 1700 to 1300 BC). From 180 BC, a series of invasions from Central Asia followed in the northwestern Indian subcontinent, including those led by the Indo-Greeks, Indo-Scythians,Indo-Parthians and Kushans.\n\nFrom 728 BC, the Greeks began 250 years of expansion, settling colonies in several places, including Sicily and Marseille. Europe provides evidence of two major migration movements: the Celtic peoples in the first millennium BC, and the later Migration Period of the first millennium AD from the North and East. Both may be examples of general cultural change sparked by primarily elite and warrior migration. A smaller migration (or sub-migration) involved the Magyars moving into Pannonia (modern-day Hungary) in the 9th century AD. Turkic peoples spread from their homeland in modern Turkestan across most of Central Asia into Europe and the Middle East between the 6th and 11th centuries AD. Recent research suggests that Madagascar was uninhabited until Austronesian seafarers from Indonesia arrived during the 5th and 6th centuries AD. Subsequent migrations from both the Pacific and Africa further consolidated this original mixture, and Malagasy people emerged.\n\nBefore the expansion of the Bantu languages and their speakers, the southern half of Africa is believed to have been populated by Pygmies and Khoisan-speaking people, whose descendants today occupy the arid regions around the Kalahari Desert and the forests of Central Africa. By about 1000 AD, Bantu migration had reached modern-day Zimbabwe and South Africa. The Banu Hilal and Banu Ma'qil were a collection of Arab Bedouin tribes from the Arabian Peninsula who migrated westwards via Egypt between the 11th and 13th centuries. Their migration strongly contributed to the Arabisation and Islamisation of the western Maghreb, which was until then dominated by Berber tribes. Ostsiedlung was the medieval eastward migration and settlement of Germans. The 13th century was the time of the great Mongol and Turkic migrations across Eurasia.\n\nBetween the 11th and 18th centuries, numerous migrations took place in Asia. The Vatsayan Priests migrated from the eastern Himalaya hills to Kashmir during the Shan invasion in the 13th century. They settled in the lower Shivalik Hills in the 13th century to sanctify the manifest goddess. In the Ming occupation, the Vietnamese started expanded southward in the 11th century; this is known in Vietnamese as nam tiến (southward expansion). Manchuria was separated from China proper by the Inner Willow Palisade, which restricted the movement of the Han Chinese into Manchuria during the early Qing Dynasty (founded in 1636), as the area was off-limits (British English: out of bounds) to the Han until the Qing started colonizing the area with them (late 18th century) later on in the dynasty's rule.\n\nThe Age of Exploration and European colonialism has led to an accelerated pace of migration since Early Modern times. In the 16th century, perhaps 240,000 Europeans entered American ports. In the 19th century over 50 million people left Europe for the Americas alone. The local populations or tribes, such as the Aboriginal people in Canada, Brazil, Argentina, Australia, Japan\nand the United States, were often numerically overwhelmed by incoming settlers.\n\nWhen the pace of migration had accelerated since the 18th century already (including the involuntary slave trade), it would increase further in the 19th century. Manning distinguishes three major types of migration: labor migration, refugee migrations, and urbanization. Millions of agricultural workers left the countryside and moved to the cities causing unprecedented levels of urbanization. This phenomenon began in Britain in the late 18th century and spread around the world and continues to this day in many areas.\n\nIndustrialization encouraged migration wherever it appeared. The increasingly global economy globalized the labour market. The Atlantic slave trade diminished sharply after 1820, which gave rise to self-bound contract labour migration from Europe and Asia to plantations. Overpopulation, open agricultural frontiers, and rising industrial centres attracted voluntary migrants. Moreover, migration was significantly made easier by improved transportation techniques.\n\nRomantic nationalism also rose in the 19th century, and, with it, ethnocentrism. The great European industrial empires also rose. Both factors contributed to migration, as some countries favored their own ethnicity over outsiders and other countries appeared to be considerably more welcoming. For example, the Russian Empire identified with Eastern Orthodoxy, and confined Jews, who were not Eastern Orthodox, to the Pale of Settlement and imposed restrictions. Violence was also a problem. The United States was promoted as a better location, a \"golden land\" where Jews could live more openly. Another effect of imperialism, colonialism, led to the migration of some colonizing parties from \"home countries\" to \"the colonies\", and eventually the migration of people from \"colonies\" to \"home countries\".\n\nTransnational labor migration reached a peak of three million migrants per year in the early twentieth century. Italy, Norway, Ireland and the Guangdong region of China were regions with especially high emigration rates during these years. These large migration flows influenced the process of nation state formation in many ways. Immigration restrictions have been developed, as well as diaspora cultures and myths that reflect the importance of migration to the foundation of certain nations, like the American melting pot. The transnational labor migration fell to a lower level from the 1930s to the 1960s and then rebounded.\n\nThe United States experienced considerable internal migration related to industrialization, including its African American population. \nFrom 1910 to 1970, approximately 7 million African Americans migrated from the rural Southern United States, where blacks faced both poor economic opportunities and considerable political and social prejudice, to the industrial cities of the Northeast, Midwest and West, where relatively well-paid jobs were available. This phenomenon came to be known in the United States as its own Great Migration, although historians today consider the migration to have two distinct phases. The term \"Great Migration\", without a qualifier, is now most often used to refer the first phase, which ended roughly at the time of the Great Depression. \nThe second phase, lasting roughly from the start of U.S. involvement in World War II to 1970, is now called the Second Great Migration. With the demise of legalised segregation in the 1960s and greatly improved economic opportunities in the South in the subsequent decades, millions of blacks have returned to the South from other parts of the country since 1980 in what has been called the New Great Migration.\n\nThe First and Second World Wars, and wars, genocides, and crises sparked by them, had an enormous impact on migration. Muslims moved from the Balkan to Turkey, while Christians moved the other way, during the collapse of the Ottoman Empire. In April 1915 the Ottoman government embarked upon the systematic decimation of its civilian Armenian population. The persecutions continued with varying intensity until 1923 when the Ottoman Empire ceased to exist and was replaced by the Republic of Turkey. The Armenian population of the Ottoman state was reported at about two million in 1915. An estimated one million had perished by 1918, while hundreds of thousands had become homeless and stateless refugees. By 1923 virtually the entire Armenian population of Anatolian Turkey had disappeared. Four hundred thousand Jews had already moved to Palestine in the early twentieth century, and numerous Jews to America, as already mentioned. The Russian Civil War caused some three million Russians, Poles, and Germans to migrate out of the new Soviet Union. Decolonization following the Second World War also caused migrations.\n\nThe Jewish communities across Europe, the Mediterranean and the Middle East were formed from voluntary and involuntary migrants. After the Holocaust (1938 to 1945), there was increased migration to the British Mandate of Palestine, which became the modern state of Israel as a result of the United Nations Partition Plan for Palestine.\n\nProvisions of the Potsdam Agreement from 1945 signed by victorious Western Allies and the Soviet Union led to one of the largest European migrations, and the largest in the 20th century. It involved the migration and resettlement of close to or over 20 million people. The largest affected group were 16.5 million Germans expelled from Eastern Europe westwards. The second largest group were Poles, millions of whom were expelled westwards from eastern Kresy region and resettled in the so-called Recovered Territories (see Allies decide Polish border in the article on the Oder-Neisse line). Hundreds of thousands of Poles, Ukrainians (Operation Vistula), Lithuanians, Latvians, Estonians and some Belarusians were expelled eastwards from Europe to the Soviet Union. Finally, many of the several hundred thousand Jews remaining in Eastern Europe after the Holocaust migrated outside Europe to Israel and the United States.\n\nIn 1947, upon the Partition of India, large populations moved from India to Pakistan and vice versa, depending on their religious beliefs. The partition was created by the Indian Independence Act 1947 as a result of the dissolution of the British Indian Empire. The partition displaced up to 17 million people in the former British Indian Empire, with estimates of loss of life varying from several hundred thousand to a million. Muslim residents of the former British India migrated to Pakistan (including East Pakistan, now Bangladesh), whilst Hindu and Sikh residents of Pakistan and Hindu residents of East Pakistan (now Bangladesh) moved in the opposite direction.\n\nIn modern India, estimates based on industry sectors mainly employing migrants suggest that there are around 100 million circular migrants in India. Caste, social networks and historical precedents play a powerful role in shaping patterns of migration. \n\nResearch by the Overseas Development Institute identifies a rapid movement of labor from slower- to faster-growing parts of the economy. Migrants can often find themselves excluded by urban housing policies, and migrant support initiatives are needed to give workers improved access to market information, certification of identity, housing and education.\n\nIn the riots which preceded the partition in the Punjab region, between 200,000 and 500,000 people were killed in the retributive genocide. U.N.H.C.R. estimates 14 million Hindus, Sikhs and Muslims were displaced during the partition. Scholars call it the largest mass migration in human history: Nigel Smith, in his book \"Pakistan: History, Culture, and Government\", calls it \"history's greatest migration.\"\n\n\n\nBooks\n\nJournals\n\nOnline Books\n\n\n"}
{"id": "53639854", "url": "https://en.wikipedia.org/wiki?curid=53639854", "title": "ICarbonX", "text": "ICarbonX\n\niCarbonX is a company founded by Chinese genomicist Jun Wang, former CEO of Beijing Genomic Institute (BGI), in 2015. iCarbonX combines genomics with other health factors such as metabolites, bacteria and lifestyle choices to create a digitalized form of life.\n\niCarbonX has raised over $600 million in investment. Tencent Holdings Limited – owner of social-media app WeChat – and Zhongyuan Union Cell & Gene Engineering Corp. invested $200 million in iCarbonX, which made iCarbonX one of only three Chinese healthcare startups with a valuation of more than $1 billion (Unicorn). The company has about 100 employees.\niCarbonX was founded on October 27, 2015. On September 10, 2016 iCarbonX acquired Imagu Vision Technologies, an Israeli AI and image processing company, in order to establish an iCarbonX-Israel R&D center. On January 5, 2017 iCarbonX announced its Digital Life Alliance with seven other companies including SomaLogic, HealthTell, PatientsLikeMe, AOBiome, GALT, Imagu and Robustnique.\n\nOn January 5, 2017 iCarbonX released Meum, a digital health management platform. The company name, “iCarbonX,” symbolizes the use of the internet and artificial intelligence to improve life, of which a central element is carbon. The “i” and “X” indicate the company’s plans to combine the Internet and artificial intelligence to create something new.\n"}
{"id": "15251344", "url": "https://en.wikipedia.org/wiki?curid=15251344", "title": "Institute of Physics James Clerk Maxwell Medal and Prize", "text": "Institute of Physics James Clerk Maxwell Medal and Prize\n\nThe James Clerk Maxwell Medal and Prize is awarded annually by the Institute of Physics to recognize outstanding early-career contributions to theoretical physics. Named after James Clerk Maxwell, the medal is made of bronze and accompanied by a prize of £1000. \n\nSource: Institute of Physics\n"}
{"id": "25231251", "url": "https://en.wikipedia.org/wiki?curid=25231251", "title": "Iotatorquevirus", "text": "Iotatorquevirus\n\nIotatorquevirus is a recently discovered genus in the new family of Anelloviridae, in group . It encompasses the two type species of the Torque Teno Sus Virus 1 and 2.\n\nThe virons are small and non enveloped.\n\nThe viruses are usually acquired soon after birth and may invade virtually any tissue in the body.\n\nThey are widespread in the pig population\n\nThe genome is a circular single stranded DNA molecule of negative polarity.\n\nPostweaning multisystemic wasting syndrome has been causally associated with porcine circovirus type 2. The Iotatorquevirus have also been linked with this syndrome but a causative role—if one exists—has yet to be established.\n\n"}
{"id": "49492887", "url": "https://en.wikipedia.org/wiki?curid=49492887", "title": "Jones Dole equation", "text": "Jones Dole equation\n\nThe Jones-Dole equation or Jones-Dole expression is an empirical expression that describes the relationship between the viscosity of a solution and the concentration of solute within the solution (at a fixed temperature and pressure). The Jones-Dole equation is written as:formula_1where\n\nThe Jones-Dole \"B\" coefficient is often used to classify ions as either structure makers (kosmotropes) or structure breakers (chaotropes) according to their supposed strengthening or weakening of the hydrogen-bond network of water. The Jones-Dole expression works well up to about 1 M but at higher concentrations breaks down as the viscosity of all solutions increase rapidly at high concentrations.\n\nThe large increase in viscosity as a function of solute concentration seen in all solutions above about 1 M is the effect of a jamming transition at a high concentration. As a result, the viscosity increases exponentially as a function of concentration and diverges at a critical concentration. This has been referred to as the Mayonnaise Effect as the viscosity of mayonnaise (essentially a solution of oil in water) is extremely high because of the jamming of micrometer scale droplets.\n"}
{"id": "34174113", "url": "https://en.wikipedia.org/wiki?curid=34174113", "title": "Life in the Twenty-First Century", "text": "Life in the Twenty-First Century\n\nLife in the Twenty-First Century is a Penguin Special book, published in Great Britain in 1960. It features predictions by 29 Soviet scientists concerning technology and science. It was edited by M Vassilev and S Gouschev. The English translation was performed by R J Watson and H E Crowcroft.\n\nThe original British hardback was published by Souvenir Press in London in the same year as the Penguin edition.\n\n"}
{"id": "4505158", "url": "https://en.wikipedia.org/wiki?curid=4505158", "title": "List of flags of Kenya", "text": "List of flags of Kenya\n\nThis is a list of flags used in Kenya.\n"}
{"id": "54111746", "url": "https://en.wikipedia.org/wiki?curid=54111746", "title": "List of things named after Edward Teller", "text": "List of things named after Edward Teller\n\nThis article is a list of things named after Edward Teller Hungarian-American theoretical physicist, regarded by some as \"the father of the hydrogen bomb\".\n\n"}
{"id": "7120009", "url": "https://en.wikipedia.org/wiki?curid=7120009", "title": "List of volcanoes in Fiji", "text": "List of volcanoes in Fiji\n\nThis is a list of active and extinct volcanoes in Fiji.\n\n"}
{"id": "7120116", "url": "https://en.wikipedia.org/wiki?curid=7120116", "title": "List of volcanoes in Malaysia", "text": "List of volcanoes in Malaysia\n\nThere is only one volcano in the territory of Malaysia.\n\n"}
{"id": "9440754", "url": "https://en.wikipedia.org/wiki?curid=9440754", "title": "Lithoprobe", "text": "Lithoprobe\n\nLithoprobe is a Canadian national geoscience research project funded by the Natural Sciences and Engineering Research Council. Its aim is to research and map the lithosphere structure and composition. Lithoprobe derives from \"probing the lithosphere\".\n\n\n"}
{"id": "191133", "url": "https://en.wikipedia.org/wiki?curid=191133", "title": "Local community", "text": "Local community\n\nA community has been defined as a group of interacting people living in a common location. The word is often used to refer to a group that is organized around common values and is attributed with social cohesion within a shared geographical location, generally in social units larger than a household. The word can also refer to the national community or global community.\nThe word \"community\" is derived from the Old French communité which is derived from the Latin communitas (cum, \"with/together\" + munus, \"gift\"), a broad term for fellowship or organized society.\n\nA sense of community refers to people's perception of interconnection and interdependence, shared responsibility, and common goals.\n\nUnderstanding a community entails having knowledge of community needs and resources, having respect for community members, and involving key community members in programs.\n\nThe author Robert Putnam refers to the value which comes from social networks as social capital in his book \"Bowling Alone: The Collapse and Revival of American Community.\" He writes that social capital \"makes an enormous difference in our lives\", that \"a society characterized by generalized reciprocity is more efficient that a distrustful society\" and that economic sociologists have shown a minimized economic wealth if social capital is lacking.\n\nPutnam reports that the first use of the social capital theory was by L. J. Hanifan, a practical reformer during the Progressive Era in the United States of America. The following description of social capital is a quote from L.J. Hanifan in Putnam's Book:\n\nPutnam reported that many studies have shown that the highest predictor of job satisfaction is the presence of social connection in the . He writes that \"people with friends at work are happier at work.\" And that \"social networks provide people with advice, a bonus, a promotion, and other strategic information, and letters of recommendation.\"\n\nCommunity engagement has been proven to counteract the most negative attributes of poverty and a high amount of social capital has been shown to reduce crime.\n\n\"Social connectedness matters to our lives in the most profound way.\" -Robert Putnam.\n\nRobert Putnam reports, in the chapter \"Health and Happiness\" from his book \"Bowling Alone\", that recent public research shows social connection impacts all areas of human health, this includes psychological and physical aspects of human health. Putnam says \"...beyond a doubt that social connectedness is one of the most powerful determinates of our well being.\" In particular it is face to face connections which have been show to have greater impacts then non-face to face relationships.\n\nSpecific health benefits of strong social relationships are a decrease in the likelihood of: seasonal viruses, heart attacks, strokes, cancer, depression, and premature death of all sorts.\n\nThere are online initiatives to improve local community's like LOCAL (www.localchange.com).\n\nSustainability in community programs is the capacity of programs (services designed to meet the needs of community members) to continuously respond to community issues.\n\nA sustained program maintains a focus consonant with its original goals and objectives, including the individuals, families, and communities it was originally intended to serve. Programs change regarding the breadth and depth of their programming. Some become aligned with other organizations and established institutions, whereas others maintain their independence.\nUnderstanding the community context in which programs serving the community function has an important influence on program sustainability and success.\nSee table:\n\nAccording to Washington State's Sustain South Sound organization, the top ten reasons to buy locally are:\n\n\n\n"}
{"id": "36342524", "url": "https://en.wikipedia.org/wiki?curid=36342524", "title": "Max Bartel", "text": "Max Bartel\n\nMax Bartel (1879 – 2 July 1914, Nürnberg) was a German entomologist\n\nMax Bartel was an insect dealer (Insektenhändler) in Berlin. He specialised in Lepidoptera. He edited \"Die palaearktischen Grossschmetterlinge und ihre Naturgeschichte\". Band 1. Leipzig, (a monograph on butterflies) with Fritz Rühl and wrote pars Sesiidae in Adalbert Seitz Macrolepidoptera of the World - Bartel, M., 1912.– 24. Familie: Ageriidae (Sesiidae) pp. 375–416, pl. 51-52, In A. Seitz (Ed.), 1906-1913.\"Gross-Schmett.Erde\", 2: 479 pp., 56 pls.\n\n"}
{"id": "2990689", "url": "https://en.wikipedia.org/wiki?curid=2990689", "title": "Modeling perspective", "text": "Modeling perspective\n\nA modeling perspective in information systems is a particular way to represent pre-selected aspects of a system. Any perspective has a different focus, conceptualization, dedication and visualization of what the model is representing. \n\nThe traditional way to distinguish between modeling perspectives is structural, functional and behavioral/processual perspectives. This together with rule, object, communication and actor and role perspectives is one way of classifying modeling approaches.\n\nThis approach concentrates on describing the static structure. The main concept in this modeling perspective is the entity, this could be an object, phenomena, concept, thing etc. \n\nThe data modeling languages have traditionally handled this perspective, examples of such being: \n\nLooking at the ER-language we have the basic components:\n\nLooking at the generic semantic modeling language we have the basic components:\n\nThe functional modeling approach concentrates on describing the dynamic process. The main concept in this modeling perspective is the process, this could be a function, transformation, activity, action, task etc. A well-known example of a modeling language employing this perspective is data flow diagrams.\n\nThe perspective uses four symbols to describe a process, these being:\n\nNow, with these symbols, a process can be represented as a network of these symbols.\nThis decomposed process is a DFD, data flow diagram.\n\nBehavioral perspective gives a description of system dynamics. The main concepts in behavioral perspective are states and transitions between states. State transitions are triggered by events. State Transition Diagrams (STD/STM), State charts and Petri-nets are some examples of well-known behaviorally oriented modeling languages. Different types of State Transition Diagrams are used particularly within real-time systems and telecommunications systems.\n\nRule perspective gives a description of goals/means connections. The main concepts in rule perspective are rule, goal and constraint. A rule is something that influences the actions of a set of actors. The standard form of rule is “IF condition THEN action/expression”. Rule hierarchies (goal-oriented modeling), Tempora and Expert systems are some examples of rule oriented modeling.\n\nThe object-oriented perspective describes the world as autonomous, communicating objects. An object is an “entity” which has a unique and unchangeable identifier and a local state consisting of a collection of attributes with assignable values. The state can only be manipulated with a set of methods defined on the object. The value of the state can only be accessed by sending a message to the object to call on one of its methods. An event is when an operation is being triggered by receiving a message, and the trace of the events during the existence of the object is called the object’s life cycle or the process of an object. Several objects that share the same definitions of attributes and operations can be parts of an object class. The perspective is originally based on design and programming of object oriented systems. Unified Modelling Language (UML) is a well known language for modeling with an object perspective.\n\nThis perspective is based on language/action theory from philosophical linguistics. The basic assumption in this perspective is that person/objects cooperate on a process/action through communication within them.\n\nAn illocutionary act consists of five elements: Speaker, hearer, time, location and circumstances. It is a reason and goal for the communication, where the participations in a communication act is oriented towards mutual agreement. In a communication act, the speaker generally can raise three claims: truth (referring an object), justice (referring a social world of the participations) and claim to sincerity (referring the subjective world of the speaker).\n\nActor and role perspective is a description of organisational and system structure. An actor can be defined as a phenomenon that influences the history of another actor, whereas a role can be defined as the behaviour which is expected by an actor, amongst other actors, when filling the role. Modeling within these perspectives is based both on work with object-oriented programming languages and work with intelligent agents in artificial intelligence. I* is an example of an actor oriented language.\n\n\n"}
{"id": "25432303", "url": "https://en.wikipedia.org/wiki?curid=25432303", "title": "Multi-conjugate Adaptive optics Demonstrator", "text": "Multi-conjugate Adaptive optics Demonstrator\n\nMulti-conjugate Adaptive optics Demonstrator (MAD) is an instrument that allowed the European Southern Observatory's Very Large Telescope to observe celestial objects with most of the atmosphere's blurring removed. As other adaptive optics systems, it works by performing real-time corrections to the atmospheric turbulence by means of computer-controlled deformable mirrors. Its particularity is the correction over a wider field of view than previous systems, that only used a single deformable mirror.\n"}
{"id": "40485054", "url": "https://en.wikipedia.org/wiki?curid=40485054", "title": "Olecranon fracture", "text": "Olecranon fracture\n\nThere are several classifications that describe different forms of olecranon fractures, yet none of them have gained widespread acceptance:\n\nBased on the stability, the displacement and the comminution of the fracture. It is composed of three types, and each type is divided in two subtypes: subtype A (non-comminuted) and subtype B (comminuted).\n\nThis classification incorporates all fractures of the proximal ulna and radius into one group, subdivided into three patterns:\n\n\n\n\n\n\n\n\n\n\n\n\nPeople with olecranon fractures present with intense elbow pain after a direct blow or fall. Swelling over the bone site is seen and an inability to straighten the elbow is common. Due to the proximity of the olecranon to the ulnar nerve, the injury and swelling may cause numbness and tingling at the 4th and 5th fingers. Examination can bring out a palpable defect at the site of the fracture.\n\nOlecranon fractures are common. Typically they are caused by direct blows to the elbow (e.g. motor vehicle accidents), and due to falls when the triceps are contracted. \"Side-swipe\" injury when driving a motor vehicle with an elbow projecting outside the vehicle resting on an open window's edge is an example.\n\nDirect trauma: This can happen in a fall with landing on the elbow or by being hit by a solid object. Trauma to the elbow often results in comminuted fractures of the olecranon.\n\nIndirect trauma: by falling and landing with an outstretched arm.\n\nPowerful pull of the triceps muscle can also cause avulsion fractures.\n\nTo assess an olecranon fracture, a careful skin exam is performed to ensure there is no open fracture. Then a complete neurological exam of the upper limb should be documented. Frontal and lateral X-ray views of the elbow are typically done to investigate the possibility of an olecranon fracture. A true lateral x-ray is essential to determine the fracture pattern, degree of displacement, comminution, and the degree of articular involvement.\n\nIn fractures with little or no displacement, immobilization with a posterior splint may be sufficient. Elbows be immobilized at 45-90º of flexion for 3 weeks, followed by limited (90º) flexion exercises.\n\nMost olecranon fractures are displaced and are best treated surgically:\n\nTension band fixation is the most common form of internal fixation used for non-comminuted olectranon fractures. It is typically reserved for noncomminuted fractures that are proximal to the coronoid. This procedure is performed using Kirschner wire (K-wires) which converts tensile forces into compressive force.\n\nSingle intramedullary screws can be used to treat simple transverse or oblique fractures. Plates can be used for all proximal ulna fracture types including Monteggia fractures, and comminuted fractures.\n\nThis method is indicated for cases when open reduction and internal fixation is unlikely to be successful. For example: extensive comminutions, elderly patients with osteoporotic bone, and small or non-union fractures.\n\nOlecranon fractures are rare in children, constituting only 5 to 7% of all elbow fractures. This is because in early life, olecranon is thick, short and much stronger than the lower extremity of the humerus.\n\nHowever, olecranon fractures are a common injury in adults. This is partly due to its exposed position on the point of the elbow.\n"}
{"id": "349661", "url": "https://en.wikipedia.org/wiki?curid=349661", "title": "Outline of ants", "text": "Outline of ants\n\nThe following outline is provided as an overview of and topical guide to ants:\n\nAnts – social insects with elbowed antennae and a distinctive node-like structure that forms a slender waist. Ants are of the family Formicidae and evolved from wasp-like ancestors in the mid-Cretaceous period between 110 and 130 million years ago, diversifying after the rise of flowering plants. More than 12,500 out of an estimated total of 22,000 species have been classified.\n\n\n\nAnt\n\n\n\n"}
{"id": "57164504", "url": "https://en.wikipedia.org/wiki?curid=57164504", "title": "Power plant engineering", "text": "Power plant engineering\n\nPower plant engineering or power station engineering is a division of power engineering, and is defined as \"the engineering and technology required for the production of central station electric power.\" The field is focused on the generation of power for industries and communities, not for household power production. The field is an interdisciplinary field, using the theoretical base of both mechanical and electrical engineering. The engineering aspect of power plant management has evolved with technology and has become progressively more complicated. The introduction of nuclear technology and the progression of other existing technologies have allowed power to be created in more ways and on a larger scale than was previously possible. The assignment of different types of engineers to the design, construction, and operation of a new power plant is dependent on the type of system being built such as whether it is a nuclear power plant, hydroelectric plant, or solar plant.\n\nPower plant engineering got its start in the 1800s when small systems were used by individual factories to provide electrical power. Originally the only source of power came from DC, or direct current, systems. While this was suitable for business, electricity was not accessible for most of the public body. During these times, the coal powered steam engine was costly to run and there was no way for the power to be transmitted over distances. Hydroelectricity was one of the most utilized forms of power generation as water mills could be used to create power to transmit to small towns.\n\nIt wasn't until the introduction of AC, or alternating current, power systems that allowed for the creation of power plants as we know them today. AC systems allowed power to be transmitted over larger distances than DC systems allowed and thus, large power stations were able to be created. One of the progenitors of long-distance power-transmission was the Lauffen to Frankfurt power plant which spanned 109 miles. The Lauffen-Frankfurt demonstrated how three-phase power could be effectively applied to transmit power over long distances. Three-phase power had been the progeny of years of research in power distribution and the Lauffen-Frankfurt was the first exhibition to show its real potential for future use.\n\nThe engineering knowledge needed to perform these tasks enlists the help of several fields of engineering including mechanical, electrical, nuclear and civil engineers. When power plants were up and coming, engineering tasks needed to create these facilities mainly consisted of mechanical, civil, and electrical engineers. These disciplines allowed for the planning and construction of power plants. But when nuclear power plants were created it introduced nuclear engineers to perform the calculations necessary to maintain safety standards.\n\nIn simple terms, the first law of thermodynamics states that energy cannot be created nor destroyed; however, power can be converted from one form of energy to another form of energy. This is especially important in power generation because power production in nearly all types of power plants relies upon the use of a generator. Generators are used to convert mechanical energy into electrical energy; for example, wind turbines utilize a large blade connected to a shaft which turns the generator when rotated. The generator then creates electricity due to the interaction of a conductor within a magnetic field. In this case, the mechanical energy generated by the wind is converted, through the generator, into electric energy. Most power plants rely on these conversions to create usable electric power.\n\nThe second law of thermodynamics conceptualizes that the entropy of a closed system can never decrease. As the law relates to power plants, it dictates that heat is to flow from a body at high temperature to a body at low temperature (the device in which electricity is being generated). This law is particularly pertinent to thermal power plants which derive their energy from the combustion of a fuel source.\n\nAll power plants are created with the same goal: to produce electric power as efficiently as possible. However, as technology has evolved, the sources of energy used in power plants has evolved as well. The introduction of more renewable/sustainable forms of energy has caused an increase in the improvement and creation of certain power plants.\n\nHydroelectric power plants generate power using the force of water to turn generators. They can be categorized into three different types; impoundment, diversion and pumped storage. Impoundment and diversion hydroelectric power plants operate similarly in that each involves creating a barrier to keep water from flowing at an uncontrollable rate, and then controlling the flow rate of water to pass through turbines to create electricity at an ideal level. Mechanical engineers are in charge of calculating flow rates and other volumetric calculations necessary to turn the generators at the electrical engineers specifications. Pumped storage hydroelectric power plants operate in a similar manner but only function at peak hours of power demand. At calm hours the water is pumped uphill, then is released at peak hours to flow from a high to low elevation to turn turbines. The engineering knowledge required to assess the performance of pumped storage hydroelectric power plants is very similar to that of the impoundment and diversion power plants.\n\nThermal power plants are split into two different categories; those that create electricity by burning fuel and those that create electricity via prime mover. A common example of a thermal power plant that produces electricity by the consumption of fuel is the nuclear power plant. Nuclear power plants use a nuclear reactor's heat to turn water into steam. This steam is sent through a turbine which is connected to an electric generator to generate electricity. Nuclear power plants account for 20% of America's electricity generation. Another example of a fuel burning power plant is coal power plant. Coal power plants generate 50% of the United States' electricity supply. Coal power plants operate in a manner similar to nuclear power plants in that the heat from the burning coal powers a steam turbine and electric generator. There are several types of engineers that work in a Thermal Power Plant. Mechanical engineers maintain performance of the thermal power plants while keeping the plants in operation. Nuclear Engineer generally handle fuel efficiency and disposal of nuclear waste; however, in Nuclear Power Plants they work directly with nuclear equipment. Electrical Engineers deals with the power generating equipment as well as the calculations.\n\nSolar power plants derive their energy from sunlight, which is made accessible via photovoltaics (PV's). Photovoltaic panels, or solar panels, are constructed using photovoltaic cells which are made of semiconductor materials that release electrons when they are warmed by the thermal energy of the sun. The new flow of electrons generates electricity within the cell. While PV's are an efficient method of producing electricity, they do burn out after a decade and thus, must be replaced; however, their efficiency, cost of operation, and lack of noise/physical pollutants make them one of the cleanest and least expensive forms of energy. Solar power plants require the work of many facets of engineering; electrical engineers are especially crucial in constructing the solar panels and connecting them into a grid, computer engineers code the cells themselves so that electricity can be effectively and efficiently produced, and civil engineers play the very important role of identifying areas where solar plants are able to collect the most energy.\n\nWind power plants, also known as wind turbines, derive their energy from the wind by connecting a generator to the fan blades and using the rotational motion caused by wind to power the generator. Then the generated power is fed back into the power grid. Wind power plants can be implemented on large, open expanses of land or on large bodies of water such as the oceans; they simply rely on being in areas that experience significant amounts of wind. Technically, wind turbines are a form of solar power in that they rely on pressure differentials caused by uneven heating of the earth's atmosphere. Wind turbines solicit the knowledge from mechanical, electrical, and civil engineers. Knowledge of fluid dynamics from the help of mechanical engineers is crucial in determining the viability of locations for wind turbines. Electrical engineers ensure that power generation and transmission is possible. Civil engineers are important in the construction and utilization of wind turbines.\n\nPower plant engineering covers a broad spectrum of engineering disciplines. The field can solicit information from mechanical, electrical, nuclear, and civil engineers.\n\nMechanical engineers work to maintain and control machinery that used to power the plant. To work in this field, mechanical engineers require a bachelor's degree in Engineering and licenses passing both the Professional Engineering Exam (PE) and Fundamental Engineering Exam (FE).The Mechanical Engineers have additional roles that are needed to be considered based on their career. In thermal power plants work in its optimal capacity. When working in thermal power plants, mechanical engineers make sure heavy machinery like boilers and turbines, are working in optimal condition and power is continually generated. Mechanical engineers also work with the operations of the plant. In nuclear and hydraulic power plants the engineers work to make sure that heavy machinery is maintained and preventative maintenance is performed.\n\nElectrical engineers work with electrical appliances while making sure electronic instruments and appliances are working in company and state level satisfaction. They require licenses passing both the Professional Engineering Exam (PE) and Fundamental Engineering Exam (FE). It is also preferred that they have a bachelor's degree approved by the Accreditation Board of Engineering and Technology, Inc. (ABET) and field experience before getting an entry level position.\n\nNuclear engineers develops and research methods, machinery and systems concerning radiation and energy in subatomic levels. They require on-site experience and a bachelor's degree in Engineering. These engineers work in Nuclear Power plants and require licenses for practice while working in the power plant. They require work experience, passing the Professional Engineering Exam(PE), Fundamental Engineering Exam (FE), and a degree from an Accreditation Board for Engineering and Technology, Inc (ABET) approved school. Nuclear engineers work with the handling of nuclear material and operations of a nuclear power plant. These operations can range from handling of nuclear wastes, nuclear material experiments, and design of nuclear equipment.\n\nCivil engineers focuses on the construction, expenses and building of the power plant. Civil Engineers require passing the Professional Engineering Exam (PE), Fundamental Engineering Exam (FE), and a degree from an Accreditation Board of Engineering and Technology, Inc. (ABET) approved school. They work with making sure the structure of the power plant, the location and the design and safety of the power plant.\n\nWhile there are many disparities between the aforementioned engineering disciplines, they all cover material related to heat or electricity transmission. Obtaining a degree from an ABET accredited school in any one of these disciplines is essential to becoming a power plant engineer. There are also many associations which qualified engineers can join, including the American Society of Mechanical Engineers (ASME), the Institute of Electric and Electronic Engineers (IEEE), and the American Society of Power Engineers (ASOPE).\n\nPower plant operation and maintenance consists of optimizing the efficiency and power output of power plants and ensuring long term operation. These power plants are large scale, and used to supply power for communities and industry. Individual household electric power generators are not included.\n\nPower station design consists of the design of new power plant systems. There are many types of power plants, and each type requires specific expertise, as well as interdisciplinary teamwork, to build a modern system.\n\n\n"}
{"id": "34831297", "url": "https://en.wikipedia.org/wiki?curid=34831297", "title": "Random coil index", "text": "Random coil index\n\nRandom coil index (RCI) predicts protein flexibility by calculating an inverse weighted average of backbone secondary chemical shifts and predicting values of model-free order parameters as well as per-residue RMSD of NMR and molecular dynamics ensembles from this parameter.\n\nThe key advantages of this protocol over existing methods of studying protein flexibility are\n\nThe application of secondary chemical shifts to characterize protein flexibility is based on an assumption that the proximity of chemical shifts to random coil values is a manifestation of increased protein mobility, while significant differences from random coil values are an indication of a relatively rigid structure.\n\nEven though chemical shifts of rigid residues may adopt random coil values as a result of comparable contributions of shielding and deshielding effects (e.g. from torsion angles, hydrogen bonds, ring currents, etc.), combining the chemical shifts from multiple nuclei into a single parameter allows one to decrease the influence of these flexibility false positives. The improved performance originates from the different probabilities of random coil chemical shifts from different nuclei being found among amino acid residues in flexible regions versus rigid regions. Typically, residues in rigid helices or rigid beta-strands are less likely to have more than one random coil chemical shift among their backbone shifts than residues in mobile regions.\n\nThe actual calculation of the RCI involves several additional steps including the smoothing of secondary shifts over several adjacent residues, the use of neighboring residue corrections, chemical shift re-referencing, gap filling, chemical shift scaling and numeric adjustments to prevent divide-by-zero problems. 13C, 15 N and 1H secondary chemical shifts are then scaled to account for the characteristic resonance frequencies of these nuclei and to provide numeric consistency among different parts of the protocol. Once these scaling corrections have been done, the RCI is calculated. The ‘‘end-effect correction’’ can also be applied at this point. The last step of the protocol involves smoothing the initial set of RCI values by three-point averaging. \n"}
{"id": "1205299", "url": "https://en.wikipedia.org/wiki?curid=1205299", "title": "ScientificPython", "text": "ScientificPython\n\nScientificPython is an open source library of scientific tools for the Python programming language.\n\nThe library includes\nQt and Tk widget toolkits are provided for building cross-platform graphical user interfaces.\n\nScientificPython is released under the CeCILL.\n\nThe main developer and maintainer of ScientificPython is Konrad Hinsen of Orléans University who uses it as a building block for his own research code, in particular the \"molecular modeling toolkit\" MMTK and the software nMoldyn that uses molecular dynamics trajectories to predict neutron scattering spectra. Outside this particular application context, most users are likely to prefer the package SciPy, which has seen a more dynamic evolution in the decade 2000–2010, involving several active developers.\n\n\n"}
{"id": "3453400", "url": "https://en.wikipedia.org/wiki?curid=3453400", "title": "Site planning", "text": "Site planning\n\nSite planning in landscape architecture and architecture refers to the organizational stage of the landscape design process. It involves the organization of land use zoning, access, circulation, privacy, security, shelter, land drainage, and other factors. This is done by arranging the compositional elements of landform, planting, water, buildings and paving in site plans. Site planning is the design and process of planning for a new development project. Within Community Development, this stage of site planning is the organizing phase where city planners create a tactical/detailed plan of new developments. These site plans are the exact details city planners need to give their proposal to the community. This is the proposal to the community to get development plans approved of. Through site analysis and precise dimensions taken by development engineers, community members are given an exact image of what developers want to do.\n\nSite planning generally begins by assessing a potential site for development through site analysis. Information about slope, soils, hydrology, vegetation, parcel ownership, orientation, etc. are assessed and mapped. By determining areas that are poor for development (such as floodplain or steep slopes) and better for development, the planner or architect can assess optimal location and design a structure that works within this space. Within site analysis you also need to take into consideration the structure of zoning throughout a city. These are regulations that have been structured to separate the land of what can be used as residential and industrial. This allows a city to not be over powered by one type of land distinction.\n\nWhen creating a development project plan city planners take into consideration and look at other buildings site plans to see what characteristics it has that helped get their building or renovation approved. City planners must look at different aspects that may affect the citizens around them because they are the ones living next to this. City planners must take into consideration height of a building, walking space, parking for cars/bikes and does it stand out from the cities other building types, design wise. These are all things that have to be considered because citizens are not going to want buildings blocking certain views/sunlight, customers/ new residents taking up parking current residents parking and having a building style that looks out of place and like it doesn't belong because city planners wanted a new/modern style building.\n\nWithin the it plan there also is details about where everything currently is, not only buildings but water, sewer and power lines. These are necessities that need to be kept safe and taken into consideration when developing a new development project. Within creating new development plans there are regulations that all buildings have to follow that are created by the city. These are regulations to help keep new development projects going out of control and keeping our city contained and not expanding at alarming rates that we cannot control. \n\nSite Planning was created by city planners to develop a clear plan/ design of what the city planners want for our community. To start off, community members would make claims of buildings that need renovations and improvements to their community. Then the community developers want to come up with a way to satisfy their community members and this is done by creating a site plan. Planners have discovered that they have to be smart about with there designs because they have to remember they have a certain budget. All these actions of creating this design is called site planning. \n\nBy: Jefrey Caldoza"}
{"id": "16700882", "url": "https://en.wikipedia.org/wiki?curid=16700882", "title": "Systems psychology", "text": "Systems psychology\n\nSystems psychology is a branch of both theoretical psychology and applied psychology that studies human behaviour and experience in complex systems. It is inspired by systems theory and systems thinking, and based on the theoretical work of Roger Barker, Gregory Bateson, Humberto Maturana and others. Groups and individuals are considered as systems in homeostasis. Alternative terms here are \"systemic psychology\", \"systems behavior\", and \"systems-based psychology\".\n\nIn the scientific literature, different kinds of systems psychology have been mentioned:\n\n\n\n\n\n\n\nErgonomics, also called \"human factors\", is the application of scientific information concerning objects, systems and environment for human use (definition adopted by the International Ergonomics Association in 2007). Ergonomics is commonly described as the way companies design tasks and work areas to maximize the efficiency and quality of their employees’ work. However, ergonomics comes into everything which involves people. Work systems, sports and leisure, health and safety should all embody ergonomics principles if well designed.\n\nEquipment design is intended to maximize productivity by reducing operator fatigue and discomfort. The field is also called human engineering and human factors engineering. Ergonomic research is primarily performed by ergonomists who study human capabilities in relationship to their work demands. Information derived from ergonomists contributes to the design and evaluation of tasks, jobs, products, environments and systems in order to make them compatible with the needs, abilities and limitations of people.\n\nFamily system therapy, also referred to as \"family therapy\" and \"couple and family therapy\", is a branch of psychotherapy related to relationship counseling that works with families and couples in intimate relationships to nurture change and development. It tends to view the family as a system, family relationships as an important factor in psychological health. As such, family problems have been seen to arise as an emergent property of systemic interactions, rather than to be blamed on individual members. Marriage and Family Therapists (MFTs) are the most specifically trained in this type of psychotherapy.\n\nIndustrial and organizational psychology also known as \"work psychology\", \"occupational psychology\" or \"personnel psychology\" concerns the application of psychological theories, research methods, and intervention strategies to workplace issues. Industrial and organizational psychologists are interested in making organizations more productive while ensuring workers are able to lead physically and psychologically healthy lives. Relevant topics include personnel psychology, motivation and leadership, employee selection, training and development, organization development and guided change, organizational behavior, and job and family issues.\n\nPerceptual control theory (PCT) is a psychological theory of animal and human behavior originated by William T. Powers. In contrast with other theories of psychology and behavior, which assume that behavior is a function of perception — that perceptual inputs determine or cause behavior — PCT postulates that an organism's behavior is a means of controlling its perceptions. In contrast with engineering control theory, the reference variable for each negative feedback control loop in a control hierarchy is set from within the system (the organism), rather than by an external agent changing the setpoint of the controller. PCT also applies to nonliving autonomic systems.\n\n\n\n\n\n"}
{"id": "26278239", "url": "https://en.wikipedia.org/wiki?curid=26278239", "title": "The Dirty Energy Dilemma", "text": "The Dirty Energy Dilemma\n\nThe Dirty Energy Dilemma: What’s Blocking Clean Power in the United States is a 2008 book by academic Benjamin K. Sovacool, published by Praeger. In the book, Sovacool explores problems with the current U.S. electricity system and ways to overcome them.\n\nIn the first part of the book, Sovacool explores the problems with the current system of large-scale electricity generation being used in the United States, powered by fossil fuels and nuclear power reactors. He identifies \"The Big Four Energy Challenges\" as rising fossil fuel costs, increasing pollution, inefficient and brittle transmission networks, as well as widespread system vulnerability to natural disasters, sabotage, and financial manipulations. \"The Big Four Clean Solutions\" of renewable energy, efficient energy use, distributed generation, and combined heat and power will do a better job of providing needed energy while protecting consumers and the planet.\n\nSovacool suggests that the barriers to clean energy adoption are institutional, not technological, and he sees no role for nuclear power in a clean energy transition.\n\n\"The Dirty Energy Dilemma\" won a 2009 Nautilus Silver Award for Best Book in the \"Ecology/Environment/Sustainability/Green Values\" category.\n\n\n"}
{"id": "56905196", "url": "https://en.wikipedia.org/wiki?curid=56905196", "title": "Thierry Deuve", "text": "Thierry Deuve\n\nThierry Deuve (born 29 August 1956) is a French entomologist.\n\nThe moth \"Deuveia banghaasi\" is named for Deuve.\n"}
{"id": "30796421", "url": "https://en.wikipedia.org/wiki?curid=30796421", "title": "Three Principles (self-help)", "text": "Three Principles (self-help)\n\nThe \"Three Principles\" of Mind, Consciousness and Thought were first articulated by Sydney Banks, a 9th-grade educated welder, born in Scotland, living in British Columbia, Canada in the early 1970s. The Three Principles approach is also referred to as Health realization.\n\nAccording to Banks' verbal accounts, as recorded at lectures, he realised the three principles while attending a marriage seminar held on Cortes Island, in British Columbia, Canada.\n\nThe seminar encouraged couples to \"let their feelings out,\" be honest, and argue with one another. Discouraged with the process, Banks and his wife prepared to leave the seminar. As they were doing so, Banks became engaged in conversation with a therapist also attending the seminar.\n\nDescribing himself as \"an insecure mess\" at that time, Banks began elaborating on all the ways in which he felt insecure. The therapist's response, \"I've never heard such nonsense in all my life\", was a revelation to Banks:\n\"What I heard was: there’s no such thing as insecurity, it’s only Thought. All my insecurity was only my own thoughts! It was like a bomb going off in my head … It was so enlightening! It was unbelievable … [And after that,] there was such beauty coming into my life.\n\nThe three specific terms, Mind, Consciousness and Thought, were not clearly delineated during Banks' initial experience. The three words—and his definitions—would become clear later through his talks and lectures. Referring to them as \"the psychological trinity\" Banks does not take credit for finding the Principles, rather the Principles \"found him.\" \n\nRoughly 40 years later, Mr. Banks' \"insight\" has been introduced in hospitals and hospital systems, correctional institutions, social services, juvenile justice programming, community housing, drug and alcohol prevention and treatment programmes, schools, and multi-national corporations.\n\nApplication of the Three Principles of Mind, Consciousness and Thought has spread throughout the United States, and into Canada, Sweden, Norway, Denmark, Israel, England, South Africa, New Zealand, Australia and Spain.\n\nAccording to Banks, the three \"formless\" principles of Mind, Consciousness and Thought explain the entire range of human behaviour and feeling states. They are responsible for the creation of all human experience.\n\nThe three principles are defined as:\n\nThe energy and intelligence of all life, whether in the form, or formless. The \"Universal Mind,\" or the \"impersonal mind,\" is constant and unchangeable. The \"personal mind\" is in a perpetual state of change.\n\n\"Consciousness\" is the gift of awareness. Consciousness allows the recognition of form, form being an expression of Thought.\n\nThe power of \"Thought\" is not self-created. Thought is a divine gift, which serves you immediately after you are born. Thought is the creative agent we use to direct us through life.\n\nThe Three Principles have become the basis of a growing, international psycho-spiritual movement with centers in the United States and Europe. The fundamental premise of the movement is that life is spiritually generated into form from formless energy, and that our experience as human beings is created from the interaction of the Three Principles; including the experience of self-identity.\n\nPractitioners of the Three Principles believe that feeling states (and all mental states) are self-created (through mental activity ie.Thought). Scientific research by Lisa Feldman Barrett supports this notion that mental states (ie. emotions) are indeed constructed from within the human mind. Practitioners believe that beyond each person's limited, conscious, and personal thought system lies a vast reservoir of wisdom, insight and spiritual intelligence. No one person has greater access to spiritual wisdom than any other. Mental health is the resting state, or \"default\" setting of the mind, which brings with it non-contingent feelings of love, compassion, resilience, creativity and unity; both with others and with life itself. Research by George Bonnano, professor of clinical psychology at Columbia University, supports this notion that resilience, not recovery is a common response to difficult life events such as trauma and loss.\n\nIt would be difficult to provide a comprehensive list of centers worldwide that are dedicated to sharing the Three Principles. However, some prominent organizations are the Center for Sustainable Change, Three Principles Foundation, Three Principles Movies, The Cypress Initiative and One Solution.\n\nBanks, who died of metastasized cancer on Memorial Day, in May 2009, contradicted many traditional notions and practices of psychotherapy. Specifically, that for mental wellbeing, it was not important to process the past, nor that the content of peoples' personal thought systems had to be \"worked with\" and analysed.\n\nEveryone in mental institutions is sitting in the middle of mental health and they don't know it.\n\nBanks was also averse to using techniques, or creating concepts, in order to share with others. These, he felt, contradicted the essential formless and original nature of the Three Principles, which emphasises kindness, \"sharing, caring\" and the simple gift of love.\n\nA number of therapists and psychologists showed an interest in the concepts, and the teachings were spread into various private practices, social services, corporate training and consulting, psychiatry, education, community mental health and development work, and drug and alcohol treatment systems.\n\nThe Three Principles have been called by other names, including Health Realization, neo-cognitive psychology, Psychology of Mind, and Innate Health. The form of how the Three Principles has been taught has changed over the decades, with an increasing emphasis on simplicity, formlessness, and speaking from the heart. Some would say that the Three Principles is not a \"technique\" that can be taught but instead a paradigm that comes to be understood at increasingly deeper levels.\n\nWorks by Sydney Banks, currently published by Lone Pine Publishing, Edmonton, Alberta, Canada\n\n\n\n\n"}
{"id": "45276089", "url": "https://en.wikipedia.org/wiki?curid=45276089", "title": "Tracking the Wild", "text": "Tracking the Wild\n\nTracking the Wild is a social media platform built specifically for wildlife. The platform has a two-pronged approach. On the one hand, it is a social media tool to share wildlife sightings and provide a host of reserve specific information. On the other hand, the platform embraces crowdsourcing and citizen science to generate valuable wildlife sightings data for conservation research.\n\nDevelopment of Tracking the Wild started in Cape Town in early 2012 by husband and wife team John and Natalie White. The company officially launched in February 2014 with their website and Android app followed by the launch of their iPhone app in September 2014.\n\nThe Tracking the Wild platform is based on the crowdsourcing of valuable wildlife sightings by citizen science. Tracking the Wild users submit their wildlife sightings in the form of images and/or video together with a sighting’s date and time, GPS location and species name. This open data is then incorporated into an online database and shared with wildlife researchers at the University of Cape Town’s Animal Demography Unit and other accredited conservation organisations.\n\nThe platform has been built to exclude rhino sightings and restrict the location information for any species whose safety could be jeopardised by its location being made public. This can be managed on a park-by-park and individual species basis.\n\nThe Tracking the Wild platform currently covers over 40 national parks, nature reserves and game reserves across South Africa, Botswana, Namibia, Swaziland and Zimbabwe.\n\n"}
{"id": "26250710", "url": "https://en.wikipedia.org/wiki?curid=26250710", "title": "Two-dimensional correlation analysis", "text": "Two-dimensional correlation analysis\n\nTwo dimensional correlation analysis is a mathematical technique that is used to study changes in measured signals. As mostly spectroscopic signals are discussed, sometime also two dimensional correlation spectroscopy is used and refers to the same technique.\n\nIn 2D correlation analysis, a sample is subjected to an external perturbation while all other parameters of the system are kept at the same value. This perturbation can be a systematic and controlled change in temperature, pressure, pH, chemical composition of the system, or even time after a catalyst was added to a chemical mixture. As a result of the controlled change (the \"perturbation\"), the system will undergo variations which are measured by a chemical or physical detection method. The measured signals or spectra will show systematic variations that are processed with 2D correlation analysis for interpretation.\n\nWhen one considers spectra that consist of few bands, it is quite obvious to determine which bands are subject to a changing intensity. Such a changing intensity can be caused for example by chemical reactions. However, the interpretation of the measured signal becomes more tricky when spectra are complex and bands are heavily overlapping. Two dimensional correlation analysis allows one to determine at which positions in such a measured signal there is a systematic change in a peak, either continuous rising or drop in intensity. 2D correlation analysis results in two complementary signals, which referred to as the 2D synchronous and 2D asynchronous spectrum. These signals allow amongst others\n\n2D correlation analysis originated from 2D NMR spectroscopy. Isao Noda developed perturbation based 2D spectroscopy in the 1980s. This technique required sinusoidal perturbations to the chemical system under investigation. This specific type of the applied perturbation severely limited its possible applications. Following research done by several groups of scientists, perturbation based 2D spectroscopy could be developed to a more extended and generalized broader base. Since the development of generalized 2D correlation analysis in 1993 based on Fourier transformation of the data, 2D correlation analysis gained widespread use. Alternative techniques that were simpler to calculate, for example the disrelation spectrum, were also developed simultaneously. Because of its computational efficiency and simplicity, the Hilbert transform is nowadays used for the calculation of the 2D spectra. To date, 2D correlation analysis is used for the interpretation of many types of spectroscopic data (including XRF, UV/VIS spectroscopy, fluorescence, infrared, and Raman spectra), although its application is not limited to spectroscopy.\n\n 2D correlation analysis is frequently used for its main advantage: increasing the spectral resolution by spreading overlapping peaks over two dimensions and as a result simplification of the interpretation of one-dimensional spectra that are otherwise visually indistinguishable from each other. Further advantages are its ease of application and the possibility to make the distinction between band shifts and band overlap. Each type of spectral event, band shifting, overlapping bands of which the intensity changes in the opposite direction, band broadening, baseline change, etc. has a particular 2D pattern. See also the figure with the original dataset on the right and the corresponding 2D spectrum in the figure below.\n\n2D synchronous and asynchronous spectra are basically 3D-datasets and are generally represented by contour plots. X- and y-axes are identical to the x-axis of the original dataset, whereas the different contours represent the magnitude of correlation between the spectral intensities. The 2D synchronous spectrum is symmetric relative to the main diagonal. The main diagonal thus contains positive peaks. As the peaks at (\"x\",\"y\") in the 2D synchronous spectrum are a measure for the correlation between the intensity changes at \"x\" and \"y\" in the original data, these main diagonal peaks are also called \"autopeaks\" and the main diagonal signal is referred to as \"autocorrelation signal\". The off-diagonal \"cross-peaks\" can be either positive or negative. On the other hand, the asynchronous spectrum is asymmetric and never has peaks on the main diagonal.\n\nGenerally contour plots of 2D spectra are oriented with rising axes from left to right and top to down. Other orientations are possible, but interpretation has to be adapted accordingly.\n\nSuppose the original dataset D contains the \"n\" spectra in rows. The signals of the original dataset are generally preprocessed. The original spectra are compared to a reference spectrum. By subtracting a reference spectrum, often the average spectrum of the dataset, so called dynamic spectra are calculated which form the corresponding dynamic dataset \"E\". The presence and interpretation may be dependent on the choice of reference spectrum. The equations below are valid for equally spaced measurements of the perturbation.\n\nA 2D synchronous spectrum expresses the similarity between spectral of the data in the original dataset. In generalized 2D correlation spectroscopy this is mathematically expressed as covariance (or correlation).\n\nwhere:\n\nOrthogonal spectra to the dynamic dataset E are obtained with the Hilbert-transform:\n\nwhere:\n\nThe values of N, \"N\" are determined as follows:\n\nwhere:\n\nInterpretation of two-dimensional correlation spectra can be considered to consist of several stages.\n\nAs real measurement signals contain a certain level of noise, the derived 2D spectra are influenced and degraded with substantial higher amounts of noise. Hence, interpretation begins with studying the autocorrelation spectrum on the main diagonal of the 2D synchronous spectrum. In the 2D synchronous main diagonal signal on the right 4 peaks are visible at 10, 20, 30, and 40 (see also the 4 corresponding positive autopeaks in the 2D synchronous spectrum on the right). This indicates that in the original dataset 4 peaks of changing intensity are present. The intensity of peaks on the autocorrelation spectrum are directly proportional to the relative importance of the intensity change in the original spectra. Hence, if an intense band is present at position \"x\", it is very likely that a true intensity change is occurring and the peak is not due to noise.\n\nAdditional techniques help to filter the peaks that can be seen in the 2D synchronous and asynchronous spectra.\n\nIt is not always possible to unequivocally determine the direction of intensity change, such as is for example the case for highly overlapping signals next to each other and of which the intensity changes in the opposite direction. This is where the off diagonal peaks in the synchronous 2D spectrum are used for:\n\n\nAs can be seen in the 2D synchronous spectrum on the right, the intensity changes of the peaks at 10 and 30 are related and the intensity of the peak at 10 and 30 changes in the opposite direction (negative cross-peak at (10,30)). The same is true for the peaks at 20 and 40.\n\nMost importantly, with the \"sequential order rules\", also referred to as \"Noda's rules\", the sequence of the intensity changes can be determined. By carefully interpreting the signs of the 2D synchronous and asynchronous cross peaks with the following rules, the sequence of spectral events during the experiment can be determined:\n\n\nFollowing the rules above. It can be derived that the changes at 10 and 30 occur simultaneously and the changes in intensity at 20 and 40 occur simultaneously as well. Because of the positive asynchronous cross-peak at (10, 20), the changes at 10 and 30 (predominantly) occur before the intensity changes at 20 and 40.\n\nIt should be noted that in some cases the Noda rules cannot be so readily implied, predominately when spectral features are not caused by simple intensity variations. This may occur when band shifts occur, or when a very erratic intensity variation is present in a given frequency range.\n\n"}
{"id": "54753353", "url": "https://en.wikipedia.org/wiki?curid=54753353", "title": "Valdar Jaanusson", "text": "Valdar Jaanusson\n\nValdar Jaanusson (1923–1999) was an Estonian-Swedish geologist. In 1960 he introduced the concept of topostratigraphy into Swedish stratigraphy. A recognized expert on the geology of the Ordovician period, he was member of the Estonian Academy of Sciences.\n"}
{"id": "16303447", "url": "https://en.wikipedia.org/wiki?curid=16303447", "title": "Vectors in three-dimensional space", "text": "Vectors in three-dimensional space\n\nVectors in three-dimensional space (1978) is a book concerned with physical quantities defined in \"ordinary\" 3-space. It was written by J.S.R.Chisholm, an English mathematical physicist, and published by Cambridge University Press. According to the author, such physical quantities are studied in Newtonian mechanics, fluid mechanics, theories of elasticity and plasticity, non-relativistic quantum mechanics, and many parts of solid state physics. The author further states that \"the vector concept developed in two different ways: in a wide variety of physical applications, vector notation and techniques became, by the middle of this century, almost universal; on the other hand, pure mathematicians reduced vector algebra to an axiomatic system, and introduced wide generalisations of the concept of a three-dimensional 'vector space'.\" Chisholm explains that since these two developments proceeded largely independently, there is a need to show how one can be applied to the other.\n\n\"Vectors in three-dimensional space\" has six chapters, each divided into five or more subsections. The first on linear spaces and displacements including these sections: Introduction, Scalar multiplication of vectors, Addition and subtraction of vectors, Displacements in Euclidean space, Geometrical applications. The second on Scalar products and components including these sections: Scalar products, Linear dependence and dimension, Components of a vector, Geometrical applications, Coordinate systems. The third on Other products of vectors. The last three chapters round out Chisholm's integration of these two largely independent developments.\n\n"}
{"id": "29950124", "url": "https://en.wikipedia.org/wiki?curid=29950124", "title": "Vega Science Trust", "text": "Vega Science Trust\n\nThe Vega Science Trust was a not-for-profit organisation which provided a platform from which scientists can communicate directly with the public on science by using moving image, sound and other related means. The Trust closed in 2012 but the website and streaming video remains active (based at Sheffield University).\n\nFounded in 1995 by Nobel Laureate Sir Harry Kroto and BBC Education Producer Patrick Reams the Vega Science Trust was awarded a COPUS start-up grant from the Royal Society in 1995 and then went on in 1999 to be allocated core funding from the Office of Science and Technology (OST). Starting with recording science programmes for terrestrial television the Vega Science Trust produced a number of programmes such as recordings of Royal Institution Discources which were broadcast on BBC 2 and a set of Masterclasses. In 2001 Harry Kroto was awarded the Royal Society Michael Faraday Prize - the UK's premier award for science communication 'for his dedication to the notion of working scientists being communicators of their work and in particular for his establishment of the Vega Science Trust whose films and related activities reflect the excitement of scientific discovery to the public'. The Trust went on to co-produce with the BBC Open University a set of science discussion programmes covering hot topics such as Stem Cells, Energy, Mobile Phones, GM Food, Disease, Nanotechnology and Ageing. With the BBC/Open University the Trust also produced with sponsorship from HEFCE Widening Participation Team a set of award-winning career programmes featuring young scientists. Both series were broadcast on BBC2.\n\nVery early audio-visual recordings of individual scientists are relatively rare but in the recent past some recordings were carried out by such organisations as the BBC. In 1997 the Vega Science Trust embarked on a plan to record in-depth interviews with scientists such as Rotblat, Sanger, Perutz, Cornforth, Walter Kohn and Richard Ernst which could be both viewed and preserved as an historical record for the future. More recently the British Library embarked on a similar project of recording audio-visual interviews under the National Life Stories project although at present their archive consists of oral recordings of scientists. The Vega Science Trust's in-depth interviews with scientists led onto a project recording interviews with Nobel Laureates attending the annual Nobel Laureate Meetings at Lindau in 2004/5/6. In 2006 the Vega Science Trust's website received a special mention at The International Association for Media Science Awards.\n\nIn 2007 the Vega Science Trust started on-going work with Jonathan Hare BBC Rough Science on a series of short instructional films intended to show how things work. For instance a number of the films show how we can generate electricity, another shows how we can generate wind power, others the molecular structure of C60, carbon nanotubes and graphene.\n\nFrom 2007-2010 the Trust concentrated on bringing to the public's attention the process of science research. The Nano2Hybrids EU STReP project for instance was an innovative project where research scientists recorded their own progress on a research project to invent a gas sensor made using carbon nanotubes. In addition recording science in society projects such as Women in Nanotechnology and Diversity illuminate work towards promoting women scientists into decision making positions in science research environments.\n\nThe Vega Science Trust closed in March 2012 after 17 years of operation. However, the website will continue to host the existing film archive.\n\nThe Vega Science Trust was governed by a board of five Trustees who are active research scientists, media, copyright, and educational specialists. Trustees step down and/or are re-elected each year.\n\nThe Vega Science Trust employed one member of staff (and for a period, a second technical member) and operated in a mixed economy of core grant-in-aid support from Florida State University, and from research grants and sponsorship. It was an independent body with its own self-contained offices, initially in the University of Sussex chemistry department, and later at the Innovation Centre, University of Sussex, Brighton.\n\nThe Vega Science Trust aimed to see science more fully integrated into our everyday culture. Vega's vision has been to do so by providing a platform from which scientists can broadcast science programmes directly to the public.\n\nActivities\n\n\nVega Science Trust Collection\n\nThe collection of recordings also acts as an historical record and archive of world scientists and their research discoveries. Recorded to broadcast quality they provide a valuable collection, much of which is open to the public via the Vega Science Trust's website.\n\n\n"}
