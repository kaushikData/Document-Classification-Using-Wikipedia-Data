{"id": "3502563", "url": "https://en.wikipedia.org/wiki?curid=3502563", "title": "Accident of birth", "text": "Accident of birth\n\nAccident of birth is a phrase pointing out that no one has any control of, or responsibility for, the circumstances of their birth or parentage. With a modern scientific understanding of genetics, one can reasonably call any human being's entire genome an accident of birth. The place of birth of a baby has an effect in immigration law of many nations, so that an \"accidental\" birth in an airport lounge may entitle a person to a passport in later life.\n\nMore broadly, gender, family circumstances, cultural background, access to education, inheritance rights, are all examples of accidents of birth. \n"}
{"id": "42972744", "url": "https://en.wikipedia.org/wiki?curid=42972744", "title": "Anhembi virus", "text": "Anhembi virus\n\nThe Anhembi virus (AMBV) is a strain of Wyeomyia virus in the genus Bunyavirus, belonging serologically to the Bunyamwera serogroup.It is isolated from the rodent, Proechimys iheringi, and a mosquito, Phoniomyia pilicauda, in São Paulo, Brazil. Until 2001 that virus have not reported to cause disease in humans.\n"}
{"id": "4737702", "url": "https://en.wikipedia.org/wiki?curid=4737702", "title": "Asahi Prize", "text": "Asahi Prize\n\nThe , established in 1929, is an award presented by the Japanese newspaper \"Asahi Shimbun\" and Asahi Shimbun Foundation to honor individuals and groups that have made outstanding accomplishments in the fields of arts and academics and have greatly contributed to the development and progress of Japanese culture and society at large.\n\nThe Asahi Prize was created to celebrate the 50th anniversary of the foundation of \"Asahi Shimbun\". It is recognized today as one of the most authoritative private awards.\n\nPast prize winners include the following.\n\n\n"}
{"id": "9495319", "url": "https://en.wikipedia.org/wiki?curid=9495319", "title": "BOINC client–server technology", "text": "BOINC client–server technology\n\nBOINC client–server technology refers to the model under which BOINC works. The BOINC framework consists of two layers which operate under the client–server architecture. Once the BOINC software is installed in a machine, the server starts sending tasks to the client. The operations are performed client-side and the results are uploaded to the server-side.\n\n\nA major part of the BOINC system is the backend server. The server can run on one or many machines to allow BOINC to scale easily to projects of any size. BOINC servers run on Linux-based computers and use Apache, PHP, and MySQL for their web and database systems.\n\nScientific computations run on participants' computers. After uploading from the user's client to a science investigator's database, the backend server validates and analyzes the results. The validation process involves running all tasks on multiple contributor PCs and comparing the results.\n\nBOINC servers also provide these features: \n\nThe server consists of two CGI programs and (normally) five daemons, written in C++. Computations to be performed by clients are called workunits. A result describes an instance of a workunit, even if it hasn't been completed. A project does not explicitly create results; the server creates them automatically from workunits.\n\nThe scheduler CGI program handles requests from clients, receiving completed results and sending new work to compute. The scheduler doesn't get available results directly from the database. Instead, a feeder daemon loads tasks from the database and keeps them in a shared-memory block, which the scheduler reads. The feeder periodically fills empty \"slots\" in the shared-memory block after the scheduler has sent those results to a client.\n\nWhen all the results from a workunit are completed and returned, the validator checks them. One popular method would be to compare the results against each other. The validator can have custom project-code to do fuzzy comparison between results, or it can perform a bitwise comparison. If the validator determines that at least some of the results are valid, it marks the work unit and the valid results as valid, users who returned legitimate results are granted credit for it, and a \"canonical result\" is chosen. If the validator cannot determine which results are valid or declares all of the results as invalid, new results can be generated and the cycle repeated until the validator can determine which results are valid.\n\nNext, the assimilator daemon processes the canonical result using project-specific code. For example, some projects may parse the file and store information in a database, others may just copy the file somewhere else. An assimilator may also generate more workunits based on the returned data.\n\nThe file_deleter daemon deletes output files after the assimilator has processed them, and deletes input files that aren't needed anymore.\n\nThe transitioner daemon handles state transitions of workunits and results. It also generates results from workunits when they are first created, and when more are needed (for example, if a result turns out invalid).\n\n\nBOINC on the client is structured into a number of separate applications. These intercommunicate using the BOINC remote procedure call (RPC) mechanism. \n\nThese component applications are:\n\n\nSince BOINC has features that can render it invisible to the typical user, there is risk that unauthorized and difficult to detect installations may occur. This would aid the accumulation of BOINC-credit points by hobbyists who are competing with others for status within the BOINC-credit subculture.\n\n"}
{"id": "46219084", "url": "https://en.wikipedia.org/wiki?curid=46219084", "title": "Bharat Aggarwal", "text": "Bharat Aggarwal\n\nBharat B. Aggarwal is an Indian-American biochemist. His research has been in the areas of cytokines, the role of inflammation in cancer, and the anti-cancer effects of spices and herbs, particularly those of curcumin (a chemical constituent of the spice turmeric). He was a professor in the Department of Clinical Immunology, Bioimmunotherapy, and Experimental Therapeutics at University of Texas MD Anderson Cancer Center in Houston, Texas.\n\nIn 2012, MD Anderson launched a review of Aggarwal's research after the federal government notified them of allegations of fraud by academic whistleblowers in as many as 65 of Aggarwal's published papers; the concerns were about data images that had been reused and manipulated to represent different results. He retired at the end of 2015; his departure was not made public until February 2016.\n\n, 28 papers published by Aggarwal have been retracted.\n\nAggarwal holds a Bachelor of Science degree from the University of Delhi (1970), a Master of Science degree from Banares Hindu University (1972) and a Doctor of Philosophy degree from the University of California, Berkeley (1977), all in biochemistry. He completed a postdoctoral fellowship at the University of California, San Francisco and was employed as a scientist at Genentech from 1980 to 1989, where his team characterized the cytokines TNF-alpha and TNF-beta. Aggarwal then served as Chief of the Cytokine Research Section, Department of Clinical Immunology, Bioimmunotherapy, Experimental Therapeutics at MD Anderson Cancer Center of the University of Texas in Houston from 1989 to 2015. He has published over 500 articles and is an ISI Highly Cited Researcher. , he had an h-index of 160 and 94,234 citations to 433 articles.\n\nAggarwal's research has focused on potential anti-cancer properties of herbs and spices, particularly curcumin, found in the spice turmeric. Aggarwal co-founded a company in 2004 called Curry Pharmaceuticals, based in Research Triangle Park, N.C., which was seeking to develop drugs based on synthetic analogs of curcumin. SignPath Pharma, a company seeking to develop liposomal formulations of curcumin, licensed three patents invented by Aggarwal related to that approach from MD Anderson in 2013.\n\nIn 2012, MD Anderson initiated a review of Aggarwal's research after the U.S. Department of Health and Human Services' Office of Research Integrity notified the institution that academic whistleblowers had found evidence of image manipulation in 65 published papers by Aggarwal.\n\nAggarwal's lawyer proposed legal action against the blog Retraction Watch in 2013 after they wrote about several of his article corrections and retractions. In February 2016, the journal \"Biochemical Pharmacology\" retracted seven research articles with Aggarwal as a co-author, him being senior or first author on six of these. The retraction notices stated this was \"because the data integrity has become questionable\".\n\nIn January 2016, an article in \"The Times of India\" listed Aggarwal as the founder director of the Anti-inflammation Research Institute in San Diego, California. Retraction Watch noted that the institute does not have a functioning website.\n\nIn February 2016 MD Anderson Cancer Center confirmed to Retraction Watch that Aggarwal retired from the institute on December 31, 2015.\n\nIn June 2016, following an investigation by MD Anderson Cancer Center, the journal \"Molecular Pharmacology\" retracted two papers coauthored by Aggarwal, citing “inappropriate” or “unacceptable” image manipulation. By April 2018 19 of Aggarwal's articles, published in 7 research journals, were ultimately retracted by the publishers.\nIn September 2018, an additional nine articles by Aggarwal were retracted in journals published by the American Association for Cancer Research.\n\n, 19 papers had been retracted.\n\n\n"}
{"id": "38785523", "url": "https://en.wikipedia.org/wiki?curid=38785523", "title": "Bojite", "text": "Bojite\n\nBojite is a variety of gabbro characterized by the presence of plagioclase feldspar and primary hornblende and absence of clinopyroxene typically associated with gabbroic rocks. It was initially defined by geologist E. Weinschenk in 1898. The term \"bojite\" has been superseded by the usage of \"hornblende gabbro\" as defined by the 2002 IUGS Subcommission on the Systematics of Igneous Rocks for rocks composed of plagioclase + hornblende and <5% pyroxene.\n"}
{"id": "3512580", "url": "https://en.wikipedia.org/wiki?curid=3512580", "title": "Brassite", "text": "Brassite\n\nBrassite is a rare arsenate mineral with the chemical formula Mg(AsOOH)·4(HO). It was named brassite, in 1973, to honor French chemist R`ejane Brasse, who first synthesized the compound. The type locality for brassite is Jáchymov of the Czech Republic.\n\nIt occurs as an alteration of magnesium carbonate minerals by arsenic bearing solutions. It occurs associated with pharmacolite, picropharmacolite, weilite, haidingerite, rauenthalite, native arsenic, realgar and dolomite.\n\nCzech Republic:\n\nFrance:\n\nGermany:\n"}
{"id": "894139", "url": "https://en.wikipedia.org/wiki?curid=894139", "title": "Causal contact", "text": "Causal contact\n\nTwo entities are in causal contact if there may be an event that has affected both in a causal way. Every object of mass in space, for instance, exerts a field force on all other objects of mass, according to Newton's law of universal gravitation. Because this force exerted by one object affects the motion of the other, it can be said that these two objects are in causal contact.\n\nThe only objects not in causal contact are those for which there is no event in the history of the universe that could have sent a beam of light to both. For example, if the universe were not expanding and had existed for 10 billion years, anything more than 20 billion light-years away from the earth would not be in causal contact with it. Anything less than 20 billion light-years away \"would\" because an event occurring 10 billion years in the past that was 10 billion light-years away from both the earth and the object under question could have affected both. \n\nA good illustration of this principle is the Light cone:\n\nThe light cone is constructed as follows. Taking as event formula_1 a flash of light (light pulse) at time formula_2, all events that can be reached by this pulse from formula_1 form the future light cone of formula_1, whilst those events that can send a light pulse to formula_1 form the past light cone of formula_1.\n\nGiven an event formula_7, the light cone classifies all events in spacetime into 5 distinct categories:\n\n\nSee the causal structure of Minkowski spacetime for a more detailed discussion.\n"}
{"id": "2001956", "url": "https://en.wikipedia.org/wiki?curid=2001956", "title": "Computer-aided production engineering", "text": "Computer-aided production engineering\n\nComputer-aided production engineering (CAPE) is a relatively new and significant branch of engineering. Global manufacturing has changed the environment in which goods are produced. Meanwhile, the rapid development of electronics and communication technologies has required design and manufacturing to keep pace.\n\nCAPE is seen as a new type of computer-aided engineering environment which will improve the productivity of manufacturing/industrial engineers. This environment would be used by engineers to design and implement future manufacturing systems and subsystems. Work is currently underway at the United States National Institute of Standards and Technology (NIST) on CAPE systems. The NIST project is aimed at advancing the development of software environments and tools for the design and engineering of manufacturing systems.\n\nThe future of manufacturing will be determined by the efficiency with which it can incorporate new technologies. The current process in engineering manufacturing systems is often ad hoc, with computerized tools being used on a limited basis. Given the costs and resources involved in the construction and operation of manufacturing systems, the engineering process must be made more efficient. New computing environments for engineering manufacturing systems could help achieve that objective.\n\nWhy is CAPE important? In much the same way that product designers need computer-aided design systems, manufacturing and industrial engineers need sophisticated computing capabilities to solve complex problems and manage the vast data associated with the design of a manufacturing system.\n\nIn order to solve these complex problems and manage design data, computerized tools must be used in the application of scientific and engineering methods to the problem of the\ndesign and implementation of manufacturing systems. Engineers must address the entire factory as a system and the interactions of that system with its surrounding environment.\nComponents of a factory system include:\n\nCAPE must not only be concerned with the initial design and engineering of the factory, it must also address enhancements over time. CAPE should support standard engineering methods and problem-solving techniques, automate mundane tasks, and provide reference data to support the decision-making process.\n\nThe environment should be designed to help engineers become more productive and effective in their work. This would be implemented on personal computers or engineering workstations which have been configured with appropriate peripheral devices. Engineering tool developers will have to integrate the functions and data used by a number of different disciplines, for example:\n\nMany of the methods, formulas, and data associated with these technical areas currently exist only in engineering handbooks. Although some computerized tools are available, they are often very specialized, difficult to use, and do not share information or work together. Engineering tools built by different vendors must be made compatible through open systems architectures and interface standards.\n\nCAPE will be based upon computer systems providing an integrated set of design and engineering tools. These software tools will be used by a company’s manufacturing engineers to continuously improve its production systems. They will maintain information about manufacturing resources, enhance production capabilities, and develop new facilities and systems. Engineers working on different workstations will share information through a common database.\n\nUsing CAPE, an engineering team will prepare detailed plans and working models for an entire factory in a matter of days. Alternative solutions to production problems could be quickly developed and evaluated. This would be a significant improvement over current manual methods which may require weeks or months of intensive activity.\n\nTo achieve this goal, a new set of engineering tools are needed. Examples of functions which should be supported include:\n\nThe tools implementing these functions must be highly automated and integrated; and will need to provide quick access to a wide range of data. This data must be maintained in a format that is accessible and usable by the engineering tools. Some examples of the information that might be contained in these electronic libraries include:\n\nThese on-line libraries would allow engineers to quickly develop solutions based upon the work of others.\n\nAnother critical aspect of this engineering environment is affordability, which\ncan best be achieved by designing an environment that can be constructed from low cost \"off-the-shelf\" commercial products, rather than custombuilt computer hardware and software. The basic engineering environment must be affordable. For both cost and technical reasons, it must be designed to be able to support incremental upgrades. Incremental upgrades would allow companies to add capabilities as they are needed. Commercial software products must be easy to install and integrate with other software already in use. These capabilities exist to a limited extent in some general purpose commercial software today, e.g., word processors, databases, spreadsheets.\n\nMany technical issues must be considered in the design and development of new engineering tools for CAPE. These issues include:\n\nThere are three critical elements to be addressed: creating a common manufacturing systems information model; using an engineering life cycle approach; and developing a software tool integration framework.\n\nResolution of these elements will help ensure that independently developed systems will be able to work together. The common information model should identify the elements of the manufacturing system and their relationships to each other; the functions or processes performed by each element; the tools, materials, and information required to perform those functions; and measures of effectiveness for the model and its component elements.\n\nThere have been many efforts over the years to develop information models for different\naspects of manufacturing, but no known existing model fully meets the needs of a CAPE ernviroment. Therefore, a life cycle approach is needed to identify the different processes that a CAPE environment must support, and must define all phases of a manufacturing system or subsystem’s existence. Some of the major phases which may be included in a system life cycle approach are, requirements identification; system design specification; vendor selection; system development and upgrades; installation, testing, and training; and benchmarking of production operations.\n\nManagement, coordination, and administration functions need to be performed during each phase of the life cycle. Phases may be repeated over time as a system is upgraded or re-engineered to meet changing needs or incorporate new technologies.\n\nA software tool integration framework should specify how the tools could be independently designed and developed. The framework would define how CAPE tools would deal with common services, interact with each other and coordinate problem solving activities. Although some existing software products and standards currently address the common services issue, the problem of tool interaction remains largely unsolved. The problem of tool interaction is not limited to the domain of computer-aided manufacturing systems engineering—it is pervasive across the software industry.\n\nAn initial CAPE environment has been established from commercial off-the-shelf (COTS) software packages. This new environment is being used to demonstrate commercially available tools to perform CAPE functions, to develop a better understanding and define functional requirements for individual engineering tools and the overall environment, and to identify the integration issues which must be addressed to implement compatible environments in the future.\n\nSeveral engineering demonstrations using COTS tools are under development. These demonstrations are designed to illustrate the various types of functions that must be performed in engineering a manufacturing system.\n\nFunctions supported by the current COTS environment include: system specification/diagramming,\nprocess flowcharting, information modeling, computer-aided design of products, plant layout, material flow analysis, ergonomic workplace design, mathematical modeling, statistical analysis, line balancing, manufacturing simulation, investment analysis, project management, knowledge-based system development, spreadsheets, document preparation, user interface development, document illustration, forms and database management.\n"}
{"id": "9536113", "url": "https://en.wikipedia.org/wiki?curid=9536113", "title": "Computer audition", "text": "Computer audition\n\nComputer audition (CA) or machine listening is general field of study of algorithms and systems for audio understanding by machine. Since the notion of what it means for a machine to \"hear\" is very broad and somewhat vague, computer audition attempts to bring together several disciplines that originally dealt with specific problems or had a concrete application in mind. The engineer Paris Smaragdis, interviewed in \"Technology Review\", talks about these systems --\"software that uses sound to locate people moving through rooms, monitor machinery for impending breakdowns, or activate traffic cameras to record accidents.\"\n\nInspired by models of human audition, CA deals with questions of representation, transduction, grouping, use of musical knowledge and general sound semantics for the purpose of performing intelligent operations on audio and music signals by the computer. Technically this requires a combination of methods from the fields of signal processing, auditory modelling, music perception and cognition, pattern recognition, and machine learning, as well as more traditional methods of artificial intelligence for musical knowledge representation.\n\nLike computer vision versus image processing, computer audition versus audio engineering deals with understanding of audio rather than processing. It also differs from problems of speech understanding by machine since it deals with general audio signals, such as natural sounds and musical recordings.\n\nApplications of computer auditions are widely varying, and include search for sounds, genre recognition, acoustic monitoring, music transcription, score following, audio texture, music improvisation, emotion in audio and so on.\n\nComputer Audition overlaps with the following disciplines:\n\nSince audio signals are interpreted by the human ear-brain system, that complex perceptual mechanism should be simulated somehow in software for \"machine listening\". In other words, to perform on par with humans, the computer should hear and understand audio content much as humans do. Analyzing audio accurately involves several fields: electrical engineering (spectrum analysis, filtering, and audio transforms); artificial intelligence (machine learning and sound classification); psychoacoustics (sound perception); cognitive sciences (neuroscience and artificial intelligence); acoustics (physics of sound production); and music (harmony, rhythm, and timbre). Furthermore, audio transformations such as pitch shifting, time stretching, and sound object filtering, should be perceptually and musically meaningful. For best results, these transformations require perceptual understanding of spectral models, high-level feature extraction, and sound analysis/synthesis. Finally, structuring and coding the content of an audio file (sound and metadata) could benefit from efficient compression schemes, which discard inaudible information in the sound. Computational models of music and sound perception and cognition can lead to a more meaningful representation, a more intuitive digital manipulation and generation of sound and music in musical human-machine interfaces.\n\nThe study of CA could be roughly divided into the following sub-problems:\n\nComputer audition deals with audio signals that can be represented in a variety of fashions, from direct encoding of digital audio in two or more channels to symbolically represented synthesis instructions. Audio signals are usually represented in terms of analogue or digital recordings. Digital recordings are samples of acoustic waveform or parameters of audio compression algorithms. One of the unique properties of musical signals is that they often combine different types of representations, such as graphical scores and sequences of performance actions that are encoded as MIDI files.\n\nSince audio signals usually comprise multiple sound sources, then unlike speech signals that can be efficiently described in terms of specific models (such as source-filter model), it is hard to devise a parametric representation for general audio. Parametric audio representations usually use filter banks or sinusoidal models to capture multiple sound parameters, sometimes increasing the representation size in order to capture internal structure in the signal. Additional types of data that are relevant for computer audition are textual descriptions of audio contents, such as annotations, reviews, and visual information in the case of audio-visual recordings.\n\nDescription of contents of general audio signals usually requires extraction of features that capture specific aspects of the audio signal. Generally speaking, one could divide the features into signal or mathematical descriptors such as energy, description of spectral shape etc., statistical characterization such as change or novelty detection, special representations that are better adapted to the nature of musical signals or the auditory system, such as logarithmic growth of sensitivity (bandwidth) in frequency or octave invariance (chroma).\n\nSince parametric models in audio usually require very many parameters, the features are used to summarize properties of multiple parameters in a more compact or salient representation.\n\nFinding specific musical structures is possible by using musical knowledge as well as supervised and unsupervised machine learning methods. Examples of this include detection of tonality according to distribution of frequencies that correspond to patterns of occurrence of notes in musical scales, distribution of note onset times for detection of beat structure, distribution of energies in different frequencies to detect musical chords and so on.\n\nComparison of sounds can be done by comparison of features with or without reference to time. In some cases an overall similarity can be assessed by close values of features between two sounds. In other cases when temporal structure is important, methods of dynamic time warping need to be applied to \"correct\" for different temporal scales of acoustic events. Finding repetitions and similar sub-sequences of sonic events is important for tasks such as texture synthesis and machine improvisation.\n\nSince one of the basic characteristics of general audio is that it comprises multiple simultaneously sounding sources, such as multiple musical instruments, people talking, machine noises or animal vocalization, the ability to identify and separate individual sources is very desirable. Unfortunately, there are no methods that can solve this problem in a robust fashion. Existing methods of source separation rely sometimes on correlation between different audio channels in multi-channel recordings. The ability to separate sources from stereo signals requires different techniques than those usually applied in communications where multiple sensors are available. Other source separation methods rely on training or clustering of features in mono recording, such as tracking harmonically related partials for multiple pitch detection.\n\nListening to music and general audio is commonly not a task directed activity. People enjoy music for various poorly understood reasons, which are commonly referred to the emotional effect of music due to creation of expectations and their realization or violation. Animals attend to signs of danger in sounds, which could be either specific or general notions of surprising and unexpected change. Generally, this creates a situation where computer audition can not rely solely on detection of specific features or sound properties and has to come up with general methods of adapting to changing auditory environment and monitoring its structure. This consists of analysis of larger repetition and self-similarity structures in audio to detect innovation, as well as ability to predict local feature dynamics.\n\nAmong the available data for describing music, there are textual representations, such as liner notes, reviews and criticisms that describe the audio contents in words. In other cases human reactions such as emotional judgements or psycho-physiological measurements might provide an insight into the contents and structure of audio. Computer Audition tries to find relation between these different representations in order to provide this additional understanding of the audio contents.\n\n\n"}
{"id": "26107799", "url": "https://en.wikipedia.org/wiki?curid=26107799", "title": "Dextrose equivalent", "text": "Dextrose equivalent\n\nDextrose equivalent (DE) is a measure of the amount of reducing sugars present in a sugar product, expressed as a percentage on a dry basis relative to dextrose. The dextrose equivalent gives an indication of the average degree of polymerisation (DP) for starch sugars, it is another word for gelatine. As a rule of thumb, DE × DP = 120.\n\nIn all glucose polymers, from the native starch to glucose syrup, the molecular chain begins with a reducing sugar, containing a free aldehyde. As the starch is hydrolysed, the molecules become shorter and more reducing sugars are present. Therefore, the dextrose equivalent describes the degree of conversion of starch to dextrose. The standard method of determining the dextrose equivalent is the Lane-Eynon titration, based on the reduction of copper(II) sulfate in an alkaline tartrate solution, an application of Fehling's test.\n\nExamples:\nTherefore, the molecular mass of a glucose polymer can be calculated by using the formula (180*n - 18*(n-1)) with n the DP (degree of polymerisation) of the glucose polymer. The DE can be calculated as 100*(180 / Molecular mass(glucose polymer)). In this example the DE is calculated as 100*(180/(180*2-18*1)) = 52.\n\n\nBecause different reducing sugars (e.g. fructose and glucose) have different sweetness, it is incorrect to assume that there is any direct relationship between dextrose equivalent and sweetness.\n"}
{"id": "893122", "url": "https://en.wikipedia.org/wiki?curid=893122", "title": "Extreme physical information", "text": "Extreme physical information\n\nExtreme physical information (EPI) is a principle, first described and formulated in 1998 by B. Roy Frieden, Emeritus Professor of Optical Sciences at the University of Arizona, that states, the precipitation of scientific laws can be derived through Fisher information, taking the form of differential equations and probability distribution functions.\n\nPhysicist John Archibald Wheeler stated that:\n\nBy using Fisher information, in particular its loss \"I\" - \"J\" incurred during observation, the EPI principle provides a new approach for deriving laws governing many aspects of nature and human society. EPI can be seen as an extension of information theory that encompasses much theoretical physics and chemistry. Examples include the Schrödinger wave equation and the Maxwell–Boltzmann distribution law. EPI has been used to derive a number of fundamental laws of physics, biology, the biophysics of cancer growth,chemistry, and economics. EPI can also be seen as a game against nature, first proposed by Charles Sanders Peirce. The approach does require prior knowledge of an appropriate invariance principle or data.\n\nThe EPI principle builds on the well known idea that the observation of a \"source\" phenomenon is never completely accurate. That is, information present in the source is inevitably lost when observing the source. The random errors in the observations are presumed to define the probability distribution function of the source phenomenon. That is, \"the physics lies in the fluctuations.\" The information loss is postulated to be an extreme value. Denoting the Fisher information in the data as formula_1, and that in the source as formula_2, the EPI principle states that\n\nSince the data are generally imperfect versions of the source, the extremum for most situations is a minimum. Thus there is a comforting tendency for any observation to describe its source faithfully. The EPI principle may be solved for the unknown system amplitudes via the usual Euler-Lagrange equations of variational calculus.\n\n\n\n\n\n\n\n"}
{"id": "41779645", "url": "https://en.wikipedia.org/wiki?curid=41779645", "title": "Felicity Huntingford", "text": "Felicity Huntingford\n\nFelicity Anne Huntingford FRSE (born 17 June 1948) is an aquatic ecologist known for her work in fish behaviour.\n\nHuntingford's research interests include the aggression in sticklebacks and welfare of farmed fish.\n\nShe is the author and editor of several widely cited and reviewed books, including the textbook \"The Study of Animal Behaviour.\"\n\nHuntingford has served as president of the Fisheries Society of the British Isles, the Association for the Study of Animal Behaviour, and the World Council of Fisheries Societies. She is Emeritus Professor of Functional Ecology at the University of Glasgow.\n\nHuntingford was elected to the Royal Society of Edinburgh in 1996 in the discipline of organismal and environmental biology.\n\nHuntingford has presented as an invited lecturer in several named lecture series. Huntingford was awarded the 2001 Tinbergen Lecture by the Association for the Study of Animal Behaviour. She also delivered the 2012 Fisheries Society of the British Isles (FSBI) Jack Jones Lecture.\n\nHuntingford has been honoured with several major academic awards, including the 2006 ASAB Medal and the 2013 FSBI Beverton Medal.\n\nShe was awarded an Honorary Doctorate from the Swedish University of Agricultural Sciences in 2009.\n\n"}
{"id": "43810810", "url": "https://en.wikipedia.org/wiki?curid=43810810", "title": "Floral zone", "text": "Floral zone\n\nA floral zone is an area with similar distributions of plant species, usually a horizontal belt determined by elevation. In a historic report on plant distribution in the United States, \"\", naturalist Clinton Hart Merriam describes \"“Most of the desert shrubs are social plants and are distributed in well-marked belts or zones, the vertical limits of which are fixed by the temperature during the period of growth and reproduction.”\"\n"}
{"id": "585779", "url": "https://en.wikipedia.org/wiki?curid=585779", "title": "Fooled by Randomness", "text": "Fooled by Randomness\n\nFooled by Randomness: The Hidden Role of Chance in Life and in the Markets is a book by Nassim Nicholas Taleb that deals with the fallibility of human knowledge. An updated edition was released in the late 2000s with more material than the 2001 edition. The book is the first part of Taleb's multi-volume philosophical essay on uncertainty, titled the \"Incerto\", which also includes \"The Black Swan\" (2007–2010), \"The Bed of Procrustes\" (2010–2016), \"Antifragile\" (2012), and \"Skin in the Game\" (2018).\n\nTaleb sets forth the idea that modern humans are often unaware of the existence of randomness. They tend to explain random outcomes as non-random.\n\nHuman beings:\n\nOther misperceptions of randomness that are discussed include:\n\nThe book was selected by \"Fortune\" as one of the 75 \"Smartest Books of All Time.\" \"U.S.A Today\" recounted that many criticisms raised in this book of the financial industry turned out to be justified. \"Forbes\" admitted to the book being playful, self-effacing and at times insufferably arrogant, but always thought provoking. \"The Wall Street Journal\" (one of the publications that Taleb pokes fun at in his book) called Universa Investments' buys in October 2008 a \"Black Swan gain\" (alluding to the \"Black Swans\" mentioned in the book). \"The New Yorker\" (one of the publications which receives more favourable comments in this book) said that the book was to conventional Wall Street wisdom what Martin Luther’s ninety-nine \"[sic]\" theses were to the Catholic Church.\n\n\n\n"}
{"id": "5147143", "url": "https://en.wikipedia.org/wiki?curid=5147143", "title": "Forma specialis", "text": "Forma specialis\n\nForma specialis (plural: \"formae speciales\"), abbreviated f. sp. (plural ff. spp.) without italics, is an informal taxonomic grouping allowed by the International Code of Nomenclature for algae, fungi, and plants, that is applied to a parasite (most frequently a fungus) which is adapted to a specific host. This classification may be applied by authors who do not feel that a subspecies or variety name is appropriate, and it is therefore not necessary to specify morphological differences that distinguish this form. The literal meaning of the term is 'special form', but this grouping does not correspond to the more formal botanical use of the taxonomic rank of \"forma\" or form.\n\nAn example is \"Puccinia graminis\" f. sp. \"avenae\", which affects oats.\n\nAn alternative term in contexts not related to biological nomenclature is physiological race (sometimes also given as biological race, and in that context treated as synonymous with biological form), except in that the name of a race is added after the binomial scientific name (and may be arbitrary, e.g. an alphanumeric code, usually with the word \"race\"), e.g. \"\"Podosphaera xanthii\" race S\". A \"forma specialis\" is used as part of the infraspecific scientific name (and follows Latin-based scientific naming conventions), inserted after the interpolation \"f. sp.\", as in the \"\"Puccinia graminis\" f. sp. \"avenae\"\" example.\n\nThe \"forma specialis\" category was introduced and recommended in the \"International Code of Botanical Nomenclature\" of 1930, but was not widely adopted. Fungal pathogens within \"Alternaria alternata\" species have also been called pathotypes (not to be confused with pathotype as used in bacteriology) by author Syoyo Nishimura who stated:\"[E]ach pathogen should be called a distinct pathotype of \"A. alternata\"\"\n\nSome authors have subsequently used \"forma specialis\" and \"pathotype\" together for the species \"A. alternata\": \"Currently there are seven pathotypes of \"A. alternata\" described ..., but this term is not widely adopted. ... To further standardise the taxonomic terms used, the trinomial system introduced by Rotem (1994) is favoured. When differences in host affinity are observed within the isolates of one ... species, the third epithet, the \"forma specialis\", defines the affinity to this specific host in accordance with the produced toxin causing this affinity. When different toxins are produced on the same host, but these toxins affect different host species, the term pathotype should be used in addition. All isolates which are not confined to specific hosts and / or toxins should retain only the binomial name until such specificity is found.\"\n\n"}
{"id": "49278771", "url": "https://en.wikipedia.org/wiki?curid=49278771", "title": "GMO conspiracy theories", "text": "GMO conspiracy theories\n\nGMO conspiracy theories are conspiracy theories related to the production and sale of genetically modified crops and genetically modified food (also referred to as genetically modified organisms or \"GMOs\"). These conspiracy theories include claims that agribusinesses, especially Monsanto, have suppressed data showing that GMOs cause harm, deliberately cause food shortages to promote the use of GM food, or have co-opted government agencies such as the United States Food and Drug Administration or scientific societies such as the American Association for the Advancement of Science. Critics charge that GMO conspiracy theories are largely promulgated by those opposing the production and sale of GMOs, and instances of unsubstantiated conspiracy theories have lately occurred in the context of public health issues that are mostly unrelated to GMOs, including the 2015–16 Zika virus outbreak and concerns over food safety at Chipotle Mexican Grill.\n\nThe existence of conspiracy theories relating to the fear over GMOs has been attested to by scientists, journalists, and skeptics who oppose much anti-GMO activism. Such commentators include Michael Shermer (writer of a monthly \"Skeptic\" column series for \"Scientific American\"), Mark Lynas (an environmental activist and writer who opposed GMOs for years and recently switched positions), and Jon Entine (the founder and head of an advocacy organization dedicated to advancing the case in favor of genetic engineering in agriculture and biotechnology). Academics writing about bioethics and science communication have also taken note. A 2013 paper published in the journal \"PLOS ONE\" found statistical evidence that linked conspiracy theorist ideation as being a significant factor in the rejection of scientific propositions about genetically engineered food. One GMO conspiracy theory was identified by biochemist Paul Christou and horticulturalist Harry Klee as a claim that development and promotion of GMOs was done by pesticide companies to cause crops to become more vulnerable to pests and therefore require more pesticides, while philosopher Juha Räikkä identified a conspiracy theory that claims the lack of any reliable scientific evidence that show harmful effects of GMOs is due not to a lack of evidence but rather to a conspiracy to hide that evidence.\n\nConspiracy theories involving GMOs and their promoters have been invoked in a variety of contexts. For example, in commenting on the Séralini affair, an incident that involved the retraction of a much-criticized paper which claimed harmful effects of GMOs in lab rats, American biologist PZ Myers said that anti-GMO activists were claiming the retraction was a part of \"a conspiracy to Hide the Truth™\". A work seeking to explore risk perception over GMOs in Turkey identified a belief among the conservative political and religious figures who were opposed to GMOs that GMOs were \"a conspiracy by Jewish Multinational Companies and Israel for world domination\" while a Latvian study showed that a segment of the population of that country believed that GMOs were part of a greater conspiracy theory to poison the population of the country.\n\nA study of media rhetorical devices used in Hunan, China found that the news articles that were opposed to trials of golden rice promoted conspiracy theories \"including the view that the West was using genetic engineering to establish global control over agriculture and that GM products were instruments for genocide\". Likewise, a study of the rhetoric used in public policy debates about genetically modified food in Ghana showed that conspiracy theories were a feature of a civil society opposition to GMOs:Government and scientists were denying the claim that GMO was discriminatory and posed significant human health risk, as well as the call to action to do something about GMOs. Civil society adapted the counter rhetoric of \"insincerity\", claiming that scientists had some kind of “hidden agenda” behind their claim, such as eagerness to just earn money from their patents on GMOs. It is imperative that communication on GMOs includes the underlying assumptions, the uncertainties and the probabilities associated with both best and worst case scenarios. This is a necessary condition to minimise misinformation on GMOs but may be insufficient to completely erase conspiracy theories from the minds of the public especially when scientists and government are perceived to be biased towards multinational corporations that are ostensibly preoccupied with making profits.Social critic Margit Stange contextualized certain arguments adopted by GMO conspiracy theorists as being part of the larger controversy surrounding the subject:\n\nThe corporate push for genetically modified food arouses great suspicion. Critics charge that GM food (\"Frankenfood\") is profitable to industry not only because it can be patented but because crop uniformity will eventually drive up pesticide demand. The charge that big food interests take advantage of poverty to open new markets for GM food is restated by conspiracy theorists, who describe a deliberate macroeconomic creation of food shortages in impoverished nations in order to open the door to GM food. The food industry's opposition to GM food labeling and precautionary measures fuels such suspicions.\n\nThis view was echoed by bioethicist Michael Reiss and moral philosopher Roger Straughan who explain in their book \"Improving Nature?: The Science and Ethics of Genetic Engineering\" that fears about the consolidation of power by a few agrochemical companies over farmers is a main argument against new genetic engineering technology in agriculture: \"At its extreme, this fear belongs to the conspiracy-theory genre and, to caricature somewhat, envisages powerless farmers forced to pay ever increasing amounts to anonymous international companies who profit from the cost of the crop seed and from the cost of the herbicides used to spray them.\"\n\nPolitical science professors Joseph Uscinski and Joseph M. Parent in their book \"American Conspiracy Theories\" summarized the people that have adopted GMO conspiracy theories thusly:\n\nAnother prototypical conspiratorial movement involves those opposed to genetically modified organisms (GMO), in essence a protest against the genetic engineering of food. Not everyone who opposes GMOs is a conspiracy theorist: reasonable people can disagree about research and fail to see small groups of people covertly working against the common good. But most visible and vocal members of this movement, however, are conspiracy theorists. They believe that genetically modified foods are a corporate plot, led by the giant multinational Monsanto, to profit off unhealthy food.\n\nA major aspect of many conspiracy theories is the fear that large agribusinesses, especially Monsanto are working to undermine the health and safety of the general public by introducing and promoting GMOs in the food supply. One claim is that Monsanto is deliberately hiding scientific evidence that GMOs are harmful. Some anti-GMO activists claimed that Monsanto infiltrated both the American Food and Drug Administration and the American Association for the Advancement of Science which is why the two organizations have supported the scientific evidence for the safety of the genetically engineered food available for human consumption. Jeffrey M. Smith is identified in the book \"American Conspiracy Theories\" as arguing that Monsanto has captured the FDA and many other countries. In the compendium \"Agricultural and Food Controversies\", the authors who are social scientists and food scientists trace the conspiracy theory relating in particular to Monsanto back to events in the early 1990s:\nThere are some well-qualified dissenting scientists and a motivated group of food activists behind them, pushing back against GM food. They believe a GM crop is not substantially equivalent to traditional crops. Moreover, they believe that the FDA follows the substantial equivalence rule not because of the science, but because the FDA was corrupted by corporate influence. This is not a belief that the authors' share, but there are smart people of high character who do believe this conspiracy theory, and their side of the story deserves to be heard.\n\nIn \"The World According to Monsanto\", author Marie-Monique Robin describes how the substantial equivalence began with a 1992 policy statement by the FDA under the leadership of a former Monsanto lawyer, who, after working in the FDA, returned to Monsanto as vice president. Her story suggests that GM regulations were the product of a revolving-door system where regulators are former and/or future employees of the company being regulated (note that some argue Monsanto wanted excess regulations to keep out competitors, but that is not Robin's story). It is not hard to imagine a company rewarding lenient regulators with a nice job, and food activists have websites listing powerful government officials and their relation to Monsanto and other corporations. If this sounds like a conspiracy theory (a term not meant as a euphemism), it is.\n\nBelief that Monsanto is particularly problematic has inspired such actions as the March Against Monsanto and the singling out of Monsanto over other agribusinesses such as DuPont, Syngenta, Dow, BASF and Bayer, and has been identified as a salient feature of anti-GMO activism.\n\nAn example of Monsanto-based conspiracy theorizing were the claims by some anti-GMO activists that Monsanto banned GMOs from their cafeterias while promoting them for sale and consumption by the public. Anti-GMO/chemtrail blogger Barbara H. Peterson, a retired correctional officer and rancher, complained that Monsanto \"has painted those of us attempting to shed light on the dangers of genetically modified/engineered organisms (GMOs) as 'conspiracy theorists'...\" She went on to attack Monsanto's suggestion that sabotage could be a possible explanation for the discovery of a few plants of experimental genetically modified wheat found inexplicably growing on a farm in Oregon as being a conspiracy theory itself.\n\nIn January 2016, concerns over a Zika virus outbreak were accompanied by claims first published on Reddit that the virus was being spread by a genetically modified mosquito. The fears were based in part because of a new mosquito abatement initiative led by Oxitec—male mosquitoes (which do not bite) are genetically modified to be sterile, and released to mate with females, resulting in no offspring, thereby reducing the \"Aedes aegypti\" mosquito population that spreads tropical diseases such as Zika. The claims were identified as \"unproven\" by the debunking website snopes.com.\n\nIn the context of ongoing concerns over food safety at Chipotle Mexican Grill certain commentators have implied that the outbreaks of food-borne illnesses were intentional sabotage by the biotech industry in retaliation over Chipotle's removal of GMOs from their menu. The claims were identified as \"unproven\" by the debunking website snopes.com.\n\nIn \"Scholars & Rogues\", an online progressive political journal, David Lambert, a development program officer for the United Nations, compared the conspiracy theories supported by some in the anti-GMO movement to those supported in the anti-vaccination movement, \nLike preventable childhood diseases, malnutrition is another great moral failing of our time. GMOs such as golden rice—rice modified to contain high levels of beta carotene in order to compensate for the vitamin A deficiency which kills hundreds of thousands of children around the world and blinds many more every year—and drought resistant crops, which will become increasingly vital in the global south due to climate change, have vast potential to help those who don't shop at Whole Foods. But real progress has been stymied by the paranoid and misinformed, who clamor that GMOs, which are biologically no different than \"natural\" foods, are somehow poisonous. Behind it all is of course an evil corporation: Monsanto.\n\n"}
{"id": "55746545", "url": "https://en.wikipedia.org/wiki?curid=55746545", "title": "Germi weather station", "text": "Germi weather station\n\nSynoptic Meteorological Station of Germi County is one of the main international synoptic stations located in the Germi city of Ardabil province of Iran.\n\nSerious efforts have been made to establish a meteorological station in the city of Germi since 2000 and in the spring of 2001, succeeded.\nThe State Meteorological Organization has sent experts to locate and purchase place of Meteorological Station in the city of Germi. \nAfter going through the legal process, the construction of the station began and it began experimentation in February 2003.\n\nThe Synoptic Meteorological Station of Germi city, on May 20, 2004, with the presence of local authorities officially opened and joined the network of weather stations of the country.\n\nGermi Weather Station is a 12-hour synoptic station with an international code or ID 40714.\nThe activities of the synoptic meteorological station of Germi city include measurements of dry and wet temperatures, maximum and minimum air temperature, wind speed, wind speed and wind direction, amount of cloudy sky, evaporation rate, depth of depth of soil, minimum surface area, sunshine time (sun radiation), And measure the amount of rainfall (different forms of atmospheric precipitation), calculate the dew point and air humidity, and prepare timely reports of the parameters and transmit it to the provincial capital as well as provide various agricultural bulletins and monthly and weekly statistics.\n\n"}
{"id": "43581700", "url": "https://en.wikipedia.org/wiki?curid=43581700", "title": "Gesellschaft zur Erhaltung alter und gefährdeter Haustierrassen", "text": "Gesellschaft zur Erhaltung alter und gefährdeter Haustierrassen\n\nThe or GEH is a German national association for the conservation of historic and endangered domestic animal breeds.\n\nThe GEH was founded in Witzenhausen, in Hesse, central Germany, in 1981. It has about 2100 members. Since it was founded, no domestic livestock breed has become extinct in Germany.\n\nThe GEH co-operates with other national and international organisations for the conservation of biodiversity. It publishes an annual Rote Liste or red list of endangered breeds of livestock, which attributes one of four categories of conservation risk to domestic breeds of cattle, dogs, goats, horses, pigs, rabbits and sheep, of chickens, ducks, geese and turkeys, and of bees; listing of domestic pigeon breeds is in preparation. Some breeds from outside Germany are listed separately. The four levels of risk are:\n\n\nThe risk level is calculated using a formula that takes into account five criteria: the number of breeding animals or breeding females; the percentage of pure-bred matings; the five-year trend in breed numbers; the number of breeders or herds; and the interval between generations of the animal.\n\nThe GEH also publishes, in conjunction with the , the German national association of poultry breeders, a separate list of the historic poultry breeds and colour varieties that were raised in Germany before 1930. The same levels of conservation risk are assigned as in the main red list.\n\nSince 1984 the GEH has each year named one or more animal breeds as \"endangered breed of the year\". To date, these have been:\n"}
{"id": "21182020", "url": "https://en.wikipedia.org/wiki?curid=21182020", "title": "History of paper", "text": "History of paper\n\nPaper, a thin unwoven material made from milled plant fibers, is primarily used for writing, artwork, and packaging; it is commonly white. The first papermaking process was documented in China during the Eastern Han period (25–220 CE), traditionally attributed to the court official Cai Lun. During the 8th century, Chinese papermaking spread to the Islamic world, where pulp mills and paper mills were used for papermaking and money making. By the 11th century, papermaking was brought to Europe. By the 13th century, papermaking was refined with paper mills utilizing waterwheels in Spain. Later European improvements to the papermaking process came in the 19th century with the invention of wood-based papers.\n\nAlthough precursors such as papyrus and amate existed in the Mediterranean world and pre-Columbian Americas, respectively, these materials are not defined as true paper. Nor is true parchment considered paper; used principally for writing, parchment is heavily prepared animal skin that predates paper and possibly papyrus. In the twentieth century with the advent of plastic manufacture some plastic \"paper\" was introduced, as well as paper-plastic laminates, paper-metal laminates, and papers infused or coated with different products that give them special properties. \n\nThe word \"paper\" is etymologically derived from \"papyrus\", Ancient Greek for the \"Cyperus papyrus\" plant. Papyrus is a thick, paper-like material produced from the pith of the \"Cyperus papyrus\" plant which was used in ancient Egypt and other Mediterranean societies for writing long before paper was used in China.\n\nPapyrus is prepared by cutting off thin ribbon-like strips of the interior of the \"Cyperus papyrus\", and then laying out the strips side-by-side to make a sheet. A second layer is then placed on top, with the strips running at right angle to the first. The two layers are the pounded together into a sheet. The result is very strong, but has an uneven surface, especially at the edges of the strips. When used in scrolls, repeated rolling and unrolling causes the strips to come apart again, typically along vertical lines. This effect can be seen in many ancient papyrus documents.\n\nPaper contrasts with papyrus in that the plant material is broken down through maceration or disintegration before the paper is pressed. This produces a much more even surface, and no natural weak direction in the material which falls apart over time.\n\nArchaeological evidence of papermaking predates the traditional attribution given to Cai Lun, an imperial eunuch official of the Han dynasty (202 BCE – CE 220), thus the exact date or inventor of paper can not be deduced. The earliest extant paper fragment was unearthed at Fangmatan in Gansu province, and was likely part of a map, dated to 179–141 BCE. Fragments of paper have also been found at Dunhuang dated to 65 BCE and at Yumen pass, dated to 8 BCE.\n\n\"Cai Lun's\" invention, recorded hundreds of years after it took place, is dated to 105 CE. The innovation is a type of paper made of mulberry and other bast fibres along with fishing nets, old rags, and hemp waste which reduced the cost of paper production, which prior to this, and later, in the West, depended solely on rags.\n\nDuring the Shang (1600–1050 BCE) and Zhou (1050–256 BCE) dynasties of ancient China, documents were ordinarily written on bone or bamboo (on tablets or on bamboo strips sewn and rolled together into scrolls), making them very heavy, awkward, and hard to transport. The light material of silk was sometimes used as a recording medium, but was normally too expensive to consider. The Han dynasty Chinese court official Cai Lun (c. 50–121 CE) is credited as the inventor of a method of papermaking (inspired by wasps and bees) using rags and other plant fibers in 105 CE. However, the discovery of specimens bearing written Chinese characters in 2006 at Fangmatan in north-east China's Gansu Province suggests that paper was in use by the ancient Chinese military more than 100 years before Cai, in 8 BCE, and possibly much earlier as the map fragment found at the Fangmatan tomb site dates from the early 2nd century BCE. It therefore would appear that \"Cai Lun's contribution was to improve this skill systematically and scientifically, fix a recipe for papermaking\".\n\nThe record in the \"Twenty-Four Histories\" says\n\nThe production process may have originated from the practice of pounding and stirring rags in water, after which the matted fibres were collected on a mat. The bark of paper mulberry was particularly valued and high quality paper was developed in the late Han period using the bark of \"tan\" (檀; sandalwood). In the Eastern Jin period a fine bamboo screen-mould treated with insecticidal dye for permanence was used in papermaking. After printing was popularized during the Song dynasty the demand for paper grew substantially. In the year 1101, 1.5 million sheets of paper were sent to the capital.\n\nAmong the earliest known uses of paper was padding and wrapping delicate bronze mirrors according to archaeological evidence dating to the reign of Emperor Wu of Han from the 2nd century BCE. Padding doubled as both protection for the object as well as the user in cases where poisonous \"medicine\" were involved, as mentioned in the official history of the period. Although paper was used for writing by the 3rd century CE, paper continued to be used for wrapping (and other) purposes. Toilet paper was used in China from around the late 6th century. In 589, the Chinese scholar-official Yan Zhitui (531–591) wrote: \"Paper on which there are quotations or commentaries from Five Classics or the names of sages, I dare not use for toilet purposes\". An Arab traveler who visited China wrote of the curious Chinese tradition of toilet paper in 851, writing: \"... [the Chinese] do not wash themselves with water when they have done their necessities; but they only wipe themselves with paper\".\n\nDuring the Tang dynasty (618–907) paper was folded and sewn into square bags to preserve the flavor of tea. In the same period, it was written that tea was served from baskets with multi-colored paper cups and paper napkins of different size and shape. During the Song dynasty (960–1279) the government produced the world's first known paper-printed money, or banknote (\"see Jiaozi and Huizi\"). Paper money was bestowed as gifts to government officials in special paper envelopes. During the Yuan dynasty (1271–1368), the first well-documented Europeans in Medieval China, the Venetian merchant Marco Polo remarked how the Chinese burned paper effigies shaped as male and female servants, camels, horses, suits of clothing and armor while cremating the dead during funerary rites.\n\nAccording to Timothy Hugh Barrett, paper played a pivotal role in early Chinese written culture, and a \"strong reading culture seems to have developed quickly after its introduction, despite political fragmentation.\" Indeed the introduction of paper had immense consequences for the book world. It meant books would no longer have to be circulated in small sections or bundles, but in their entirety. Books could now be carried by hand rather than transported by cart. As a result individual collections of literary works increased in the following centuries.\n\nTextual culture seems to have been more developed in the south by the early 5th century, with individuals owning collections of several thousand scrolls. In the north an entire palace collection might have been only a few thousand scrolls in total. By the early 6th century, scholars in both the north and south were capable of citing upwards of 400 sources in commentaries on older works. A small compilation text from the 7th century included citations to over 1,400 works.\n\nThe personal nature of texts was remarked upon by a late 6th century imperial librarian. According to him, the possession of and familiarity with a few hundred scrolls was what it took to be socially accepted as an educated man. \n\nAccording to Endymion Wilkinson, one consequence of the rise of paper in China was that \"it rapidly began to surpass the Mediterranean empires in book production.\" During the Tang dynasty, China became the world leader in book production. In addition the gradual spread of woodblock printing from the late Tang and Song further boosted their lead ahead of the rest of the world.\n\nHowever despite the initial advantage afforded to China by the paper medium, by the 9th century its spread and development in the middle east had closed the gap between the two regions. Between the 9th to early 12th centuries, libraries in Cairo, Baghdad, and Cordoba held collections larger than even the ones in China, and dwarfed those in Europe. From about 1500 the maturation of paper making and printing in Southern Europe also had an effect in closing the gap with the Chinese. The Venetian Domenico Grimani's collection numbered 15,000 volumes by the time of his death in 1523. After 1600, European collections completely overtook those in China. The Bibliotheca Augusta numbered 60,000 volumes in 1649 and surged to 120,000 in 1666. In the 1720s the Bibliotheque du Roi numbered 80,000 books and the Cambridge University 40,000 in 1715. After 1700, libraries in North America also began to overtake those of China, and toward the end of the century, Thomas Jefferson's private collection numbered 4,889 titles in 6,487 volumes. The European advantage only increased further into the 19th century as national collections in Europe and America exceeded a million volumes while a few private collections, such as that of Lord Action, reached 70,000.\n\nPaper became central to the three arts of China – poetry, painting, and calligraphy. In later times paper constituted one of the 'Four Treasures of the Scholar's Studio,' alongside the brush, the ink, and the inkstone.\n\nAfter its origin in central China, the production and use of paper spread steadily. It is clear that paper was used at Dunhuang by 150 CE, in Loulan in the modern-day province of Xinjiang by 200, and in Turpan by 399. Paper was concurrently introduced in Japan sometime between the years 280 and 610.\n\nPaper spread to Vietnam in the 3rd century.\n\nPaper spread to Korea in the 4th century.\n\nPaper spread to Japan in the 5th century.\n\nPaper spread to India in the 7th century. However, the use of paper was not widespread there until the 12th century.\n\nAfter the defeat of the Chinese in the Battle of Talas in 751 (present day Kyrgyzstan), the invention spread to the Middle East.\n\nThe legend goes, the secret of papermaking was obtained from two Chinese prisoners from the Battle of Talas, which led to the first paper mill in the Islamic world being founded in Samarkand in Sogdia (modern-day Uzbekistan). There was a tradition that Muslims would release their prisoners if they could teach ten Muslims any valuable knowledge. There are records of paper being made at Gilgit in Pakistan by the sixth century, in Samarkand by 751, in Baghdad by 793, in Egypt by 900, and in Fes, Morocco around 1100.\n\nThe laborious process of paper making was refined and machinery was designed for bulk manufacturing of paper. Production began in Baghdad, where a method was invented to make a thicker sheet of paper, which helped transform papermaking from an art into a major industry. The use of water-powered pulp mills for preparing the pulp material used in papermaking, dates back to Samarkand in the 8th century, though this should not be confused with paper mills (see \"Paper mills\" section below). The Muslims also introduced the use of trip hammers (human- or animal-powered) in the production of paper, replacing the traditional Chinese mortar and pestle method. In turn, the trip hammer method was later employed by the Chinese. Historically, trip hammers were often powered by a water wheel, and are known to have been used in China as long ago as 40 BCE or maybe even as far back as the Zhou Dynasty (1050 BCE–221 BCE).\n\nBy the 9th century, Muslims were using paper regularly, although for important works like copies of the revered Qur'an, vellum was still preferred. Advances in book production and bookbinding were introduced.\nIn Muslim countries they made books lighter—sewn with silk and bound with leather-covered paste boards; they had a flap that wrapped the book up when not in use. As paper was less reactive to humidity, the heavy boards were not needed.\nBy the 12th century in Marrakech in Morocco a street was named \"Kutubiyyin\" or book sellers which contained more than 100 bookshops.\n\nIn 1035 a Persian traveler visiting markets in Cairo noted that vegetables, spices and hardware were wrapped in paper for the customers after they were sold.\nSince the First Crusade in 1096, paper manufacturing in Damascus had been interrupted by wars, but its production continued in two other centres. Egypt continued with the thicker paper, while Iran became the center of the thinner papers. Papermaking was diffused across the Islamic world, from where it was diffused further west into Europe. Paper manufacture was introduced to India in the 13th century by Muslim merchants, where it almost wholly replaced traditional writing materials.\n\nThe oldest known paper document in the West is the Mozarab Missal of Silos from the 11th century, probably using paper made in the Islamic part of the Iberian Peninsula. They used hemp and linen rags as a source of fiber. The first recorded paper mill in the Iberian Peninsula was in Xàtiva in 1056.\nPapermaking reached Europe as early as 1085 in Toledo and was firmly established in Xàtiva, Spain by 1150. It is clear that France had a paper mill by 1190, and by 1276 mills were established in Fabriano, Italy and in Treviso and other northern Italian towns by 1340. Papermaking then spread further northwards, with evidence of paper being made in Troyes, France by 1348, in Holland sometime around 1340–1350, in Mainz, Germany in 1320, and in Nuremberg by 1390 in a mill set up by Ulman Stromer. This was just about the time when the woodcut printmaking technique was transferred from fabric to paper in the old master print and popular prints. There was a paper mill in Switzerland by 1432 and the first mill in England was set up by John Tate in 1490 near Stevenage in Hertfordshire, but the first commercially successful paper mill in Britain did not occur before 1588 when John Spilman set up a mill near Dartford in Kent. During this time, paper making spread to Poland by 1491, to Austria by 1498, to Russia by 1576, to the Netherlands by 1586, to Denmark by 1596, and to Sweden by 1612.\n\nArab prisoners who settled in a town called Borgo Saraceno in the Italian Province of Ferrara introduced Fabriano artisans in the Province of Ancona the technique of making paper by hand. At the time they were renowned for their wool-weaving and manufacture of cloth. Fabriano papermakers considered the process of making paper by hand an art form and were able to refine the process to successfully compete with parchment which was the primary medium for writing at the time. They developed the application of stamping hammers to reduce rags to pulp for making paper, sizing paper by means of animal glue, and creating watermarks in the paper during its forming process. The Fabriano used glue obtained by boiling scrolls or scraps of animal skin to size the paper; it is suggested that this technique was recommended by the local tanneries. The introduction of the first European watermarks in Fabriano was linked to applying metal wires on a cover laid against the mould which was used for forming the paper.\n\nThey adapted the waterwheels from the fuller's mills to drive a series of three wooden hammers per trough. The hammers were raised by their heads by cams fixed to a waterwheel's axle made from a large tree trunk.\n\nIn the Americas, archaeological evidence indicates that a similar bark-paper writing material was used by the Mayans no later than the 5th century CE. Called \"amatl\" or \"amate\", it was in widespread use among Mesoamerican cultures until the Spanish conquest. The earliest sample of amate was found at Huitzilapa near the Magdalena Municipality, Jalisco, Mexico, belonging to the shaft tomb culture. It is dated to 75 BCE.\n\nThe production of amate is much more similar to paper than papyrus. The bark material is soaked in water, or in modern methods boiled, so that it breaks down into a mass of fibres. They are then laid out in a frame and pressed into sheets. It is a true paper product in that the material is not in its original form, but the base material has much larger fibres than those used in modern papers. As a result, amate has a rougher surface than modern paper, and may dry into a sheet with hills and valleys as the different length fibres shrink.\n\nEuropean papermaking spread to the Americas first in Mexico by 1575 and then in Philadelphia by 1690.\n\nThe use of human and animal powered mills was known to Chinese and Muslim papermakers. However, evidence for water-powered paper mills is elusive among both prior to the 11th century. The general absence of the use of water-powered paper mills in Muslim papermaking prior to the 11th century is suggested by the habit of Muslim authors at the time to call a production center not a \"mill\", but a \"paper manufactory\".\n\nDonald Hill has identified a possible reference to a water-powered paper mill in Samarkand, in the 11th-century work of the Persian scholar Abu Rayhan Biruni, but concludes that the passage is \"too brief to enable us to say with certainty\" that it refers to a water-powered paper mill. This is seen by Halevi as evidence of Samarkand first harnessing waterpower in the production of paper, but notes that it is not known if waterpower was applied to papermaking elsewhere across the Islamic world at the time. Burns remains sceptical, given the isolated occurrence of the reference and the prevalence of manual labour in Islamic papermaking elsewhere prior to the 13th century.\n\nClear evidence of a water-powered paper mill dates to 1282 in the Spanish Kingdom of Aragon. A decree by the Christian king Peter III addresses the establishment of a royal \"molendinum\", a proper hydraulic mill, in the paper manufacturing centre of Xàtiva. The crown innovation was operated by the Muslim Mudéjar community in the Moorish quarter of Xàtiva,though it appears to have been resented by sections of the local Muslim papermakering community; the document guarantees them the right to continue the way of traditional papermaking by beating the pulp manually and grants them the right to be exempted from work in the new mill. Paper making centers began to multiply in the late 13th century in Italy, reducing the price of paper to one sixth of parchment and then falling further; paper making centers reached Germany a century later.\n\nThe first paper mill north of the Alps was established in Nuremberg by Ulman Stromer in 1390; it is later depicted in the lavishly illustrated \"Nuremberg Chronicle\". From the mid-14th century onwards, European paper milling underwent a rapid improvement of many work processes.\n\nBefore the industrialisation of the paper production the most common fibre source was recycled fibres from used textiles, called rags. The rags were from hemp, linen and cotton. A process for removing printing inks from recycled paper was invented by German jurist Justus Claproth in 1774. Today this method is called deinking. It was not until the introduction of wood pulp in 1843 that paper production was not dependent on recycled materials from ragpickers.\n\nAlthough cheaper than vellum, paper remained expensive, at least in book-sized quantities, through the centuries, until the advent of steam-driven paper making machines in the 19th century, which could make paper with fibres from wood pulp. Although older machines predated it, the Fourdrinier papermaking machine became the basis for most modern papermaking. Nicholas Louis Robert of Essonnes, France, was granted a patent for a continuous paper making machine in 1799. At the time he was working for Leger Didot with whom he quarrelled over the ownership of the invention. Didot sent his brother-in-law, John Gamble, to meet Sealy and Henry Fourdrinier, stationers of London, who agreed to finance the project. Gamble was granted British patent 2487 on 20 October 1801. With the help particularly of Bryan Donkin, a skilled and ingenious mechanic, an improved version of the Robert original was installed at Frogmore Paper Mill, Hertfordshire, in 1803, followed by another in 1804. A third machine was installed at the Fourdriniers' own mill at Two Waters. The Fourdriniers also bought a mill at St Neots intending to install two machines there and the process and machines continued to develop.\n\nHowever, experiments with wood showed no real results in the late 18th century and at the start of the 19th century. By 1800, Matthias Koops (in London, England) further investigated the idea of using wood to make paper, and in 1801 he wrote and published a book titled \"Historical account of the substances which have been used to describe events, and to convey ideas, from the earliest date, to the invention of paper.\" His book was printed on paper made from wood shavings (and adhered together). No pages were fabricated using the pulping method (from either rags or wood). He received financial support from the royal family to make his printing machines and acquire the materials and infrastructure needed to start his printing business. But his enterprise was short lived. Only a few years following his first and only printed book (the one he wrote and printed), he went bankrupt. The book was very well done (strong and had a fine appearance), but it was very costly.\n\nThen in the 1830s and 1840s, two men on two different continents took up the challenge, but from a totally new perspective. Both Friedrich Gottlob Keller and Charles Fenerty began experiments with wood but using the same technique used in paper making; instead of pulping rags, they thought about pulping wood. And at about the same time, by mid-1844, they announced their findings. They invented a machine which extracted the fibres from wood (exactly as with rags) and made paper from it. Charles Fenerty also bleached the pulp so that the paper was white. This started a new era for paper making. By the end of the 19th-century almost all printers in the western world were using wood in lieu of rags to make paper.\n\nTogether with the invention of the practical fountain pen and the mass-produced pencil of the same period, and in conjunction with the advent of the steam driven rotary printing press, wood based paper caused a major transformation of the 19th century economy and society in industrialized countries. With the introduction of cheaper paper, schoolbooks, fiction, non-fiction, and newspapers became gradually available by 1900. Cheap wood based paper also meant that keeping personal diaries or writing letters became possible and so, by 1850, the clerk, or writer, ceased to be a high-status job.\n\nThe original wood-based paper was acidic due to the use of alum and more prone to disintegrate over time, through processes known as slow fires. Documents written on more expensive rag paper were more stable. Mass-market paperback books still use these cheaper mechanical papers (see below), but book publishers can now use acid-free paper for hardback and trade paperback books.\n\nDetermining the provenance of paper is a complex process that can be done in a variety of ways. The easiest way is using a known sheet of paper as an exemplar. Using known sheets can produce an exact identification. Next, comparing watermarks with those contained in catalogs or trade listings can yield useful results. Inspecting the surface can also determine age and location by looking for distinct marks from the production process. Chemical and fiber analysis can be used to establish date of creation and perhaps location.\n\n\n"}
{"id": "748800", "url": "https://en.wikipedia.org/wiki?curid=748800", "title": "Information: The New Language of Science", "text": "Information: The New Language of Science\n\nInformation: The New Language of Science is a 2003 book by Hans Christian von Baeyer, Chancellor Professor of Physics at the College of William and Mary, examining contemporary information science.\n"}
{"id": "44427249", "url": "https://en.wikipedia.org/wiki?curid=44427249", "title": "Ingredient-flavor network", "text": "Ingredient-flavor network\n\nIngredient-flavor networks are networks describing the sharing of flavor compounds of culinary ingredients. In the bipartite form, an ingredient-flavor network consist of two different types of nodes: the ingredients used in the recipes and the flavor compounds that contributes to the flavor of each ingredients. The links connecting different types of nodes are undirected, represent certain compound occur in each ingredients. The ingredient-flavor network can also be projected in the ingredient or compound space where nodes are ingredients or compounds, links represents the sharing of the same compounds to different ingredients or the coexistence in the same ingredient of different compounds.\n\nIn 2011, Yong-Yeol Ahn, Sebastian E. Ahnert, James P. Bagrow and Albert-László Barabási investigated the ingredient-flavor networks of North American, Latin American, Western European, Southern European and East Asian cuisines. Based on culinary repository epicurious.com, allrecipes.com and menupan.com, 56,498 recipes has been included in the survey.\n\nThe efforts to apply network analysis on foods also occurred in the work of Kinouchi and Chun-Yuen Teng, with the former examined the relationship between ingredients and recipes, and the latter derived the ingredient-ingredient networks of both compliments and substitutions. Yet Ahn's \ningredient-flavor network was constructed based on the molecular level understanding of culinary networks and received wide attention\n\nAccording to Ahn, in the total number of 56,498 recipes studied, 381 ingredients and 1021 flavor compounds were identified. Averagely each ingredients connect to 51 flavor compounds.\n\nIt was found that in comparison with random pairing of ingredients and flavor compounds, North American cuisines tend to share more compounds while East Asian cuisines tend to share fewer compounds. It was also shown that this tendency was mostly generated by the frequently used ingredients in each cuisines.\n\nAn important feature that the ingredient-flavor network showed is the principle of food pairing. A well known hypothesis states that ingredients sharing flavor compounds are more likely to taste well together than ingredients that do not. However, the sensory test by Miriam Kort, etc. claimed that the shared compound hypothesis can be debatable. \nAccording to Ahn, the food pairing pattern changes in different cuisines. North American recipes tends to obey the shared compound hypothesis while East Asian cuisines tend to avoid it. Besides the spatial variance, Kush R. Varshney, Lav R. Varshney, Jun Wang, and Daniel Myers also showed the time variance in food pairing by comparing the modern European recipes with the Medieval European recipes. They concluded that the Medieval cuisine tend to share more compounds than the cuisine today.\n\n"}
{"id": "26457540", "url": "https://en.wikipedia.org/wiki?curid=26457540", "title": "International Geography Olympiad", "text": "International Geography Olympiad\n\nThe International Geography Olympiad (iGeo) is an annual competition for 16- to 19-year-old geography students from all over the world. Students chosen to represent their countries are some of the best, chosen from thousands of students who participate enthusiastically in their own National Geography Olympiads. iGeo tests the abilities of every participants in spatial patterns and processes. The iGeo consists of three parts: a written test, a multimedia test and a substantial fieldwork exercise requiring observation, leading to cartographic representation and geographical analysis. The programme also includes poster presentations by teams, cultural exchanges, and time for students to get to know their fellow students and explore the host city.\n\nThe International Geography Olympiad is organised by the International Geographical Union (IGU) Olympiad Task Force, who produce tests with reference to the local organisers and the international board.\n\nAfter the first iGeo in 1996, it was recommended that the competition was held biennially. Due to the competition growing in popularity, since 2012 the competition has been held annually, rather than biennially, as is the case with the other large International Science Olympiads.\n\nDuring the 1994 Congress of the International Geographical Union (IGU) in Prague, people from Poland and the Netherlands launched the idea of an International Geography Competition (iGeo) or Olympiad for students between 15 and 19 years of age. The first one was held in 1996 in The Hague, Netherlands, with five participating countries. The participant count grew to 24 countries with the 2008 competition in Carthage, Tunisia.\n\nBefore 2012, the International Science Olympiads were held every two years, and some regional geography Olympiads were held during intervening years. These include the Asia Pacific Regional Geography Olympiads (APRGO), which were held in 2007 (Hsinchu, Taiwan), 2009 (Tsukuba, Japan), and 2011 (Merida, Mexico), and the Central European Regional Geography Olympiads (CERIGEO). Since 2013, the International Geography Olympiad, in concordance with the other Olympiads, has been held on a yearly basis.\n\nAt the most recent iGeo, held in Quebec City, Canada between July and August 2018, there were 43 participating countries.\n\nThe next Olympiad is the 2019 iGeo, which will be held in Hong Kong, China in August 2019.\n\nThe participating countries and regions in the 2018 International Geography Olympiad are:\n\n</div>\n\nThe names used are the standard names officially used by the International Geographical Union, based on the roster list for the 2017 International Geography Olympiad in Belgrade.\n"}
{"id": "43422602", "url": "https://en.wikipedia.org/wiki?curid=43422602", "title": "Julio Augusto Henriques", "text": "Julio Augusto Henriques\n\nJúlio Augusto Henriques (January 15, 1838 - January 15, 1928) was a Portuguese botanist and professor at the University of Coimbra. He developed the Herbarium of the University and Coimbra Botanical Garden. He also founded the 'Society Broterian', which brought together various scientists botanists, geologists and naturalists. He was a large admirer of the work of Charles Darwin. He also wrote many articles about the flora of Portugal.\n\nHenriques was born on 15 January 1838, in the parish of Arco de Baúlhe.\n\nIn 1853, he married 'Zulmira Angelina de Magalhães Lima'\n\nHe went to school in the city of Braga. In 1854, he trained to become a lawyer at College of Sao Bento in Coimbra, (following his fathers wishes). He completed his law course in 1859, but then enrolled on a second college course in Philosophy at the University of Coimbra. After gaining a bachelor's degree in Philosophy, he went on to obtain a Ph.D. doctorate.\n\nHenriques was a supporter of Darwinism. His final thesis in 1865, was entitled 'As especies sao mudaveis? (translated as 'Are species modifiable?) was a defence of Charles Darwin's theory of evolution.\n\nIn 1866, he became a tutor in Philosophy at the University of Coimbra. He soon became secretary of the Philosophy Faculty, which he held until 1873. In 1869, he became a substitute lecturer in botany, agriculture, zoology, chemistry and mineralogy.\n\nHe most favoured botany and the plant sciences, and then in 1873, this new found passion lead to him becoming head of the Botany and Agriculture. This also meant being in charge of the university botanical garden. Using other botanical gardens in Europe as his guide, he achieved new funding for the department. He acquired new laboratory resources, increased fieldwork opportunities for his botany students. He re-organised the books, to create a library, then he re-organised the botanical specimens to create a herbarium. This new plant collection attracted others, including a vast number of specimens from the German collector Heinrich Moritz Willkomm (who collected in Spain and Portugal). He also added to the Botanical Garden, by collaborating (via exchanges of plants and seeds) with other botanical gardens and plant organisations.\n\nThis invigoration within the botany department, turned Coimbra into a national centre for studying Portuguese flora.\n\nIn 1876, he wrote 'O Jardim Botanico', published by University Press.\n\nIn 1880, he was still in contact with Heinrich Moritz Willkomm, with several letters being sent between the two botanists.\n\nIn 1880, he founded the \"Sociedade Broteriana\", which was named in honour after Félix Avelar Brotero (a botanist and founder of the Botanical Garden. The botanical society brought together plant enthusiasts from all over Portugal, to write various articles in the society's bulletins. and disseminate information about plants throughout the country.\n\nHenriques became to travel all over the country to collect more specimens for the University and the herbarium. He also started classifying and identifying the herbarium collection (including Willkomm's and his) specimens.\n\nHe produced many articles and publications on the fungi, lichens, algae and vascular plants (such as Amaryllidaceae, Poaceae and Plantaginaceae) of Portugal and produced a regional flora of the Mondego Basin.\n\nIn 1882, Henriques published an article 'Carlos Darwin', which is devoted to Charles Darwin's life and works.\n\nIn 1883, he published 'Expedicao scientifica a Serra da Estrella', in 'Seccao de botanica', based on a botanical exploration taken of Serra da Estrella(Star Mountain range) in 1881. It listed 716 plants (including 600 vascular species).\n\nIn 1886, he published 'Uma excursao botanico na Serra do Caramullo' in 'Bol. Soc Broteriana', various geological, topographical and flora observations from explorations taken in June 1884.\n\nIn 1886, he published 'Contribuição para o estudo da flora d'Africa: Flora de S. Thome' about flora of the Portuguese commonwealth in Africa, he collected in São Tomé and Príncipe as well, and is responsible for publishing the first flora of these islands in 1886. He was also interested in the development of agriculture on São Tomé and Príncipe and in Angola, publishing instructions on successful cultivation of certain crop species (particularly quinine for its protective qualities against malaria), and sending over thousands of species of potential agricultural interest.\n\nIn 1889, he published Lecanora newtoniana (a species of Lichen) in Ins. Guin.: 17 (1889).\n\nHe studied specimens from plant collectors, Frenchman Jules Alexandre Daveau (during 1852-1929) and M. Ferreira. (during 1860-1920).\nIn 1901, he published 'De Macieira ate Castro Daire' in 'Bol. Soc Broteriana' various geological, topographical and flora details of various regions in articles.\n\nHenriques did not hold a large number of administrative positions, preferring instead to concentrate on his teaching and running the gardens.\n\nIn 1907, the University of Uppsala (in Sweden) awarded him an honorary doctorate but, as a particularly humble man, he often refused other recognition's.\n\nIn 1915, he wrote a letter on 5 January from Jardim Botanico, Coimbra to Sir William Abbott Herdman, the letter is now stored by the Natural History Museum Archives in London.\n\nIn 1917, he wrote 'A Ilha de S. Tomé, sob o ponto de vista histórico-natural e agrícola'.\n\nHe also wrote 'Plantas Da Borracha E Da Gutta-Percha' (Portuguese Edition). It was re-published in paperback form on 27 January 2012.\n\nHe is the botanical author of Iris boissieri.\n\nAfter his death in 1928, several articles 'Flora vascular de Transcow' were published in 'Anais Fac. Cien Porto in 1936 by Gonçalo Sampaio. This was a brief annotated list of 405 species of vascular plants collected by Henriques in July 1908.\n\nIn 1955, 'Portugaliae acta biologica: Sistemática, ecologia, biogeografia e paleontologia', Volumes 6-8 was also published.\n\nIn 1978, Sociedade Broteriana also published the letters sent between Julio Henriques and Heinrich Moritz Willkomm.\n\n"}
{"id": "54474403", "url": "https://en.wikipedia.org/wiki?curid=54474403", "title": "LEDA 83677", "text": "LEDA 83677\n\nLEDA 83677 is a lenticular galaxy located about 290 million light-years away in the constellation Coma Berenices. It is a member of the Coma cluster of galaxies. LEDA 83677 is also classified as a type 1 Seyfert galaxy. The core of the galaxy is emitting high-energy X-rays and ultraviolet light, probably caused by a massive black hole lurking in the core. \n"}
{"id": "9954641", "url": "https://en.wikipedia.org/wiki?curid=9954641", "title": "Line moiré", "text": "Line moiré\n\nLine moiré is one type of moiré pattern; a pattern that appears when superposing two transparent layers containing correlated opaque patterns. Line moiré is the case when the superposed patterns comprise straight or curved lines. When moving the layer patterns, the moiré patterns transform or move at a faster speed. This effect is called optical moiré speedup.\n\nSimple moiré patterns can be observed when superposing two transparent layers comprising periodically repeating opaque parallel lines as shown in Figure 1. The lines of one layer are parallel to the lines of the second layer.\n\nThe superposition image does not change if transparent layers with their opaque patterns are inverted. When considering printed samples, one of the layers is denoted as the base layer and the other one as the revealing layer. It is assumed that the revealing layer is printed on a transparency and is superimposed on top of the base layer, which can be printed either on a transparency or on an opaque paper. The periods of the two layer patterns are close. We denote the period of the base layer as \"p\" and the period of the revealing layer as \"p\".\n\nThe superposition image of Figure 1 outlines periodically repeating dark parallel bands, called moiré lines. Spacing between the moiré lines is much larger than the periods of lines in the two layers.\n\nLight bands of the superposition image correspond to the zones where the lines of both layers overlap. The dark bands of the superposition image forming the moiré lines correspond to the zones where the lines of the two layers interleave, hiding the white background. The labels of Figure 2 show the passages from light zones with overlapping layer lines to dark zones with interleaving layer lines. The light and dark zones are periodically interchanging.\n\nFigure 3 shows a detailed diagram of the superposition image between two adjacent zones with overlapping lines of the revealing and base layers (i.e., between two light bands).\n\nThe period \"p\" of moiré lines is the distance from one point where the lines of both layers overlap (at the bottom of the figure) to the next such point (at the top). Let us count the layer lines, starting from the bottom point. At the count 0 the lines of both layers overlap. Since in our case \"p\"<\"p\", for the same number of counted lines, the base layer lines with a long period advance faster than the revealing layer lines with a short period. At the halfway of the distance \"p\", the base layer lines are ahead the revealing layer lines by a half a period (\"p\"/2) of the revealing layer lines, due to which the lines are interleaving, forming a dark moiré band. At the full distance \"p\", the base layer lines are ahead of the revealing layer lines by a full period \"p\", so the lines of the layers again overlap. The base layer lines gain the distance \"p\" with as many lines (\"p\"/\"p\") as the number of the revealing layer lines (\"p\"/\"p\") for the same distance minus one: \"p\"/\"p\" = \"p\"/\"p\" + 1. From here we obtain the well known formula for the period \"p\" of the superposition image:\nFor the case when the revealing layer period is longer than the base layer period, the distance between moiré bands is the absolute value computed by the formula. The superposition of two layers comprising parallel lines forms an optical image comprising parallel moiré lines with a magnified period. According to the formula for computing \"p\", the closer the periods of the two layers, the stronger the magnification factor is.\n\nThe thicknesses of layer lines affect the overall darkness of the superposition image and the thickness of the moiré bands, but the period \"p\" does not depend on the layer lines’ thickness.\n\nThe moiré bands of Figure 1 will move if we displace the revealing layer. When the revealing layer moves perpendicularly to layer lines, the moiré bands move along the same axis, but several times faster than the movement of the revealing layer.\n\nThe GIF animation shown in Figure 4 corresponds to a slow movement of the revealing layer. The GIF file repeatedly animates an upward movement of the revealing layer (perpendicular to layer lines) across a distance equal to \"p\". The animation demonstrates that the moiré lines of the superposition image move up at a speed, much faster than the movement speed of the revealing layer.\n\nWhen the revealing layer is shifted up perpendicularly to the layer lines by one full period (\"p\") of its pattern, the superposition optical image must be the same as the initial one. It means that the moiré lines traverse a distance equal to the period of the superposition image \"p\" while the revealing layer traverses the distance equal to its period \"p\". Assuming that the base layer is immobile (\"v\"=0), the following equation represents the ratio of the optical speed to the revealing layer’s speed:\nBy replacing \"p\" with its formula, we have\nIn case the period of the revealing layer is longer than the period of the base layer, the optical image moves in the opposite direction. The negative value of the ratio computed according to this formula signifies a movement in the reverse direction.\n\nHere we present patterns with inclined lines. When we are interested in optical speedup we can represent the case of inclined patterns such that the formulas for computing moiré periods and optical speedups remain valid in their current simplest form. For this purpose, the values of periods \"p\", \"p\", and \"p\" correspond to the distances between the lines along the axis of movements (the vertical axis in the animated example of Figure 4). When the layer lines are perpendicular to the movement axis, the periods (\"p\") are equal to the distances (denoted as \"T\") between the lines (as in Figure 4). If the lines are inclined, the periods (\"p\") along the axis of the movement are not equal to the distances (\"T\") between the lines.\n\nThe superposition of two layers with identically inclined lines forms moiré lines inclined at the same angle. Figure 5 is obtained from Figure 1 with a vertical shearing. In Figure 5 the layer lines and the moiré lines are inclined by 10 degrees. Since the inclination is not a rotation, during the inclination the distance (\"p\") between the layer lines along the vertical axis is conserved, but the true distance (\"T\") between the lines (along an axis perpendicular to these lines) is changed. The difference between the vertical periods \"p\", \"p\", and the distances \"T\", \"T\" is shown in the diagram of Figure 8.\n\nThe inclination degree of layer lines may change along the horizontal axis forming curves. The superposition of two layers with identical inclination pattern forms moiré curves with the same inclination pattern. In Figure 6 the inclination degree of layer lines gradually changes according to the following sequence of degrees (+30, –30, +30, –30, +30). Layer periods \"p\" and \"p\" represent the distances between the curves along the vertical axis. The presented formulas for computing the period \"p\" (the vertical distance between the moiré curves) and the optical speedup (along the vertical axis) are valid for Figure 6.\n\nMore interesting is the case when the inclination degrees of layer lines are not the same for the base and revealing layers. Figure 7 shows an animation of a superposition images where the inclination degree of base layer lines is constant (10 degrees), but the inclination of the revealing layer lines oscillates between 5 and 15 degrees. The periods of layers along the vertical axis \"p\" and \"p\" are the same all the time. Correspondingly, the period \"p\" (along the vertical axis) computed with the basic formula also remains the same.\n\nFigure 8 helps to compute the inclination degree of moiré optical lines as a function of the inclination of the revealing and the base layer lines. We draw the layer lines schematically without showing their true thicknesses. The bold lines of the diagram inclined by \"α\" degrees are the base layer lines. The bold lines inclined by \"α\" degrees are the revealing layer lines. The base layer lines are vertically spaced by a distance equal to \"p\", and the revealing layer lines are vertically spaced by a distance equal to \"p\". The distances \"T\" and \"T\" represent the true space between the base layer and revealing layer lines, correspondingly. The intersections of the lines of the base and the revealing layers (marked in the figure by two arrows) lie on a central axis of a light moiré band. The dashed line of Figure 8 corresponds to the axis of the light moiré band. The inclination degree of moiré lines is therefore the inclination \"α\" of the dashed line.\n\nFrom Figure 8 we deduce the following two equations:\nFrom these equations we deduce the equation for computing the inclination of moiré lines as a function of the inclinations of the base layer and the revealing layer lines:\n\nThe true pattern periods \"T\", \"T\", and \"T\" (along the axes perpendicular to pattern lines) are computed as follows (see Figure 8):\nFrom here, using the formula for computing tan(\"α\") with periods \"p\", we deduce a well known formula for computing the moiré angle \"α\" with periods \"T\":\nFrom formula for computing \"p\" we deduce another well known formula for computing the period \"T\" of moiré pattern (along the axis perpendicular to moiré bands):\nIn the particular case when \"T\"=\"T\"=\"T\", the formula for the period \"T\" is reduced into well known formula:\nAnd the formula for computing α is reduced to:\n\nHere is the equation for computing the revealing layer line inclination \"α\" for a given base layer line inclination \"α\", and a desired moiré line inclination \"α\":\n\nFor any given base layer line inclination, this equation permits us to obtain a desired moiré line inclination by properly choosing the revealing layer inclination. In Figure 6 we showed an example where the curves of layers follow an identical inclination pattern forming a superposition image with the same inclination pattern. The inclination degrees of the layers’ and moiré lines change along the horizontal axis according to the following sequence of alternating degree values (+30, –30, +30, –30, +30). In Figure 9 we obtain the same superposition pattern as in Figure 6, but with a base layer comprising straight lines inclined by –10 degrees. The revealing pattern of Figure 9 is computed by interpolating the curves into connected straight lines, where for each position along the horizontal axis, the revealing line’s inclination angle \"α\" is computed as a function of \"α\" and \"α\" according to the equation above.\n\nFigure 9 demonstrates that the difference between the inclination angles of revealing and base layer lines has to be several times smaller than the difference between inclination angles of moiré and base layer lines.\n\nAnother example forming the same superposition patterns as in Figure 6 and Figure 9 is shown in Figure 10. In Figure 10 the desired inclination pattern (+30, –30, +30, –30, +30) is obtained using a base layer with an inverted inclination pattern (–30, +30, –30, +30, –30).\n\nFigure 11 shows an animation where we obtain a superposition image with a constant inclination pattern of moiré lines (+30, –30, +30, –30, +30) for continuously modifying pairs of base and revealing layers. The base layer inclination pattern gradually changes and the revealing layer inclination pattern correspondingly adapts such that the superposition image’s inclination pattern remains the same.\n\n"}
{"id": "12844133", "url": "https://en.wikipedia.org/wiki?curid=12844133", "title": "List of Nevada state symbols", "text": "List of Nevada state symbols\n\nThis is a list of symbols of the U.S. state of Nevada. The majority of the items in the list are officially recognized symbols created by an act of the Nevada Legislature and signed into law by the governor.\n\n\n"}
{"id": "58409529", "url": "https://en.wikipedia.org/wiki?curid=58409529", "title": "List of fossiliferous stratigraphic units in Guyana", "text": "List of fossiliferous stratigraphic units in Guyana\n\nThis is a list of fossiliferous stratigraphic units in Guyana.\n\n\n"}
{"id": "32770854", "url": "https://en.wikipedia.org/wiki?curid=32770854", "title": "List of inorganic reactions", "text": "List of inorganic reactions\n\nWell-known types of reactions that involve organic compounds include:\n\n\n"}
{"id": "46289556", "url": "https://en.wikipedia.org/wiki?curid=46289556", "title": "Martin Seeleib-Kaiser", "text": "Martin Seeleib-Kaiser\n\nMartin Seeleib-Kaiser (born 13 February 1964) is a social scientist. Since 2017 he has been a professor of comparative public policy at the Institute of Political Science of the Eberhard Karls Universität Tübingen, in Tübingen, Germany. He was previously a fellow of St Cross College, Oxford, and Barnett professor of comparative social policy and politics at the Department of Social Policy and Intervention of the University of Oxford.\n\n"}
{"id": "28095621", "url": "https://en.wikipedia.org/wiki?curid=28095621", "title": "Methodology of econometrics", "text": "Methodology of econometrics\n\nThe methodology of econometrics is the study of the range of differing approaches to undertaking econometric analysis.\n\nCommonly distinguished differing approaches that have been identified and studied include:\n\nIn addition to these more clearly defined approaches, Hoover identifies a range of \"heterogeneous\" or \"textbook approaches\" that those less, or even un-, concerned with methodology, tend to follow.\n\nEconometrics may use standard statistical models to study economic questions, but most often they are with observational data, rather than in controlled experiments. In this, the design of observational studies in econometrics is similar to the design of studies in other observational disciplines, such as astronomy, epidemiology, sociology and political science. Analysis of data from an observational study is guided by the study protocol, although exploratory data analysis may by useful for generating new hypotheses. Economics often analyzes systems of equations and inequalities, such as supply and demand hypothesized to be in equilibrium. Consequently, the field of econometrics has developed methods for identification and estimation of simultaneous-equation models. These methods are analogous to methods used in other areas of science, such as the field of system identification in systems analysis and control theory. Such methods may allow researchers to estimate models and investigate their empirical consequences, without directly manipulating the system.\n\nOne of the fundamental statistical methods used by econometricians is regression analysis. Regression methods are important in econometrics because economists typically cannot use controlled experiments. Econometricians often seek illuminating natural experiments in the absence of evidence from controlled experiments. Observational data may be subject to omitted-variable bias and a list of other problems that must be addressed using causal analysis of simultaneous-equation models.\n\nIn recent decades, econometricians have increasingly turned to use of experiments to evaluate the often-contradictory conclusions of observational studies. Here, controlled and randomized experiments provide statistical inferences that may yield better empirical performance than do purely observational studies.\n\nData sets to which econometric analyses are applied can be classified as time-series data, cross-sectional data, panel data, and multidimensional panel data. Time-series data sets contain observations over time; for example, inflation over the course of several years. Cross-sectional data sets contain observations at a single point in time; for example, many individuals' incomes in a given year. Panel data sets contain both time-series and cross-sectional observations. Multi-dimensional panel data sets contain observations across time, cross-sectionally, and across some third dimension. For example, the Survey of Professional Forecasters contains forecasts for many forecasters (cross-sectional observations), at many points in time (time series observations), and at multiple forecast horizons (a third dimension).\n\nIn many econometric contexts, the commonly used ordinary least squares method may not recover the theoretical relation desired or may produce estimates with poor statistical properties, because the assumptions for valid use of the method are violated. One widely used remedy is the method of instrumental variables (IV). For an economic model described by more than one equation, simultaneous-equation methods may be used to remedy similar problems, including two IV variants, Two-Stage Least Squares (2SLS), and Three-Stage Least Squares (3SLS).\n\nComputational concerns are important for evaluating econometric methods and for use in decision making. Such concerns include mathematical well-posedness: the existence, uniqueness, and stability of any solutions to econometric equations. Another concern is the numerical efficiency and accuracy of software. A third concern is also the usability of econometric software.\n\nStructural econometrics extends the ability of researchers to analyze data by using economic models as the lens through which to view the data. The benefit of this approach is that any policy recommendations are not subject to the Lucas critique since counter-factual analyses take an agent's re-optimization into account. Structural econometric analyses begin with an economic model that captures the salient features of the agents under investigation. The researcher then searches for parameters of the model that match the outputs of the model to the data. There are two ways of doing this. The first requires the researcher to completely solve the model and then use maximum likelihood. However, there have been many advances that can bypass the full solution of the model and that estimate models in two stages. Importantly, these methods allow the researcher to consider more complicated models with strategic interactions and multiple equilibria.\n\nA good example of structural econometrics is in the estimation of first price sealed bid auctions with independent private values. The key difficulty with bidding data from these auctions is that bids only partially reveal information on the underlying valuations, bids shade the underlying valuations. One would like to estimate these valuations in order to understand the magnitude of profits each bidder makes. More importantly, it is necessary to have the valuation distribution in hand to engage in mechanism design. In a first price sealed bid auction the expected payoff of a bidder is given by:\nwhere v is the bidder valuation, b is the bid. The optimal bid formula_2 solves a first order condition:\nwhich can be re-arranged to yield the following equation for formula_4\n\nNotice that the probability that a bid wins an auction can be estimated from a data set of completed auctions, where all bids are observed. This can be done using simple non-parametric estimators. If all bids are observed, it is then possible to use the above relation and the estimated probability function and its derivative to point wise estimate the underlying valuation. This will then allow the investigator to estimate the valuation distribution.\n\nSimilarly to the methodology of economics (cf.), the empirical branch of the discipline also was the topic of the debate that dates back to Milton Friedman's famous essay advising an instrumentalist interpretation of the discipline. According to the instrumentalist philosophy of science, models are interpreted as a tool and its truth or falsity is not considered. With the aim to save the viewpoint that econometric models depict (resemble, isolate, idealize, etc.) the realm of economy, various realist interpretations were put forth. Nancy Cartwright interpreted the coefficients of econometric models as probabilistic, causal laws. Kevin Hoover supported the standard scientific-realist interpretation of (econometric) models as isolations of reality. From the position of critical realism, Tony Lawson argued that the lack of robustness of econometric models indicates their falsity, and therefore rejected the scientific-realist interpretation (cf. the criticisms of econometrics). In response to such criticms, Hoover employed perspectival realism to defend the truth of econometric models. Considering that perspectivism is a pragmatically-flavored stance and does not presuppose the ontic commitment, Maziarz employed Uskali Mäki's model of modeling to show that econometric results differ because of their context-specificity, but pairs of contrary models can grasp different aspects of the same economic reality. In other words, purpose, audience, and context, in addition to target, influence the results of econometric modeling.\n\n"}
{"id": "48623758", "url": "https://en.wikipedia.org/wiki?curid=48623758", "title": "My Brief History", "text": "My Brief History\n\nMy Brief History is a memoir published in 2013 by the English physicist Stephen Hawking. The book recounts Hawking's journey from his post-war London boyhood to his years of international acclaim and celebrity.\n\n\"My Brief History\" has received modest praise from critics. Ian Sample of \"The Guardian\" wrote, \"Hawking's memoir, \"My Brief History\", is a skip across the surface of the Cambridge cosmologist's life, from his quirky upbringing in London and St Albans to his latest work on the beginning of time and the evolution of the universe. The details are sketched, but the brevity makes for a bold picture. Hawking's intellectual activity soars as his illness takes hold and eventually puts an intolerable burden on his marriages.\" Chuck Leddy of \"The Boston Globe\" similarly observed, \"It's clear, though, that Hawking is more comfortable looking up at the universe than into himself, more concerned with detailing the evolution of a career than the twists and turns of a life, though he does reveal some interesting details about his beginnings as a scientist. In clean, direct prose, Hawking leads us from his birth in Oxford in 1942 to the present.\"\n"}
{"id": "20963652", "url": "https://en.wikipedia.org/wiki?curid=20963652", "title": "Natural gas by country", "text": "Natural gas by country\n\nThis article includes a chart representing proven reserves, production, consumption, exports and imports of natural gas by country. Below the numbers there is specified which position a country holds by the corresponding parameter. Dependent territories, not fully recognized countries and supranational entities are not ranked. By default countries are ranked by their total proven natural gas reserves.\n\nAll data is taken from CIA World Factbook. Note that data related to one parameter may be more up to date than data related to some other.\n\nThe International Energy Agency top 10 natural gas producers in 2011 were (66.7 % of total) (bcm): 1) Russia 677 (20.0 %), 2) United States 651 (19.2 %), 3) Canada 160 (4.7 %), 4) Qatar 151 (4.5 %), 5) Iran 149 (4.4 %), 6) Norway 106 (3.1 %), 7) China 103 (3.0 %), 8) Saudi Arabia 92 (2.7 %), 9) Indonesia 92 (2.7 %), 10) Netherlands 81 (2.4 %) and World 3 388 (100 %).\n\n\nGeneral:\n\n<br>\n"}
{"id": "26773116", "url": "https://en.wikipedia.org/wiki?curid=26773116", "title": "Number bond", "text": "Number bond\n\nIn mathematics education at primary school level, a number bond (sometimes alternatively called an addition fact) is a simple addition sum which has become so familiar that a child can recognise it and complete it almost instantly, with recall as automatic as that of an entry from a multiplication table in multiplication.\n\nFor example,\n\nA child who \"knows\" this number bond should be able to immediately fill in any one of these three numbers if it were missing, given the other two, without having to \"work it out\".\n\nNumber bonds are often learned in sets for which the sum is a common round number such as 10 or 20. Having acquired some familiar number bonds, children should also soon learn how to use them to develop strategies to complete more complicated sums, for example by navigating from a new sum to an adjacent number bond they know, i.e. 5 + 2 and 4 + 3 are both number bonds that make 7; or by strategies like \"making ten\", for example recognising that 7 + 6 = (7 + 3) + 3 = 13.\n\nThe term \"number bond\" is also used to refer to a pictorial representation of part-part-whole relationships, often found in the Singapore mathematics curriculum. Number bonds consist of a minimum of 3 circles that are connected by lines. The “whole” is written in the first circle and its “parts” are written in the adjoining circles.\nNumber bonds are used to build deeper understanding of math facts.\n\nThe term \"number bond\" is sometimes derided as a piece of unnecessary new mathematical jargon, adding an element of pointless abstraction or incomprehensibility for those not familiar with it (such as children's parents) to a subject even as simple as primary school addition. The term has been used at least since the 1920s and formally entered the primary curriculum in Singapore in the early 1970s.\n\nIn the U.K. the phrase came into widespread classroom use from the late 1990s when the National Numeracy Strategy brought in an emphasis on in-classroom discussion of strategies for developing mental arithmetic in its \"numeracy hour\".\n\n\n"}
{"id": "30992262", "url": "https://en.wikipedia.org/wiki?curid=30992262", "title": "Partial cloning", "text": "Partial cloning\n\nIn the field of cell biology, the method of partial cloning (PCL) converts a fully differentiated \"old\" somatic cell into a partially reprogrammed \"young\" cell that retains all the specialised functions of the differentiated \"old\" cell but is simply younger. The method of PCL reverses characteristics associated with old cells. For example, old, senescent, cells \"rejuvenated\" by PCL are free of highly condensed senescence-associated heterochromatin foci (SAHF) and re-acquire the proliferation potential of young cells. The method of PCL thus rejuvenates old cells without de-differentiation and passage through an embryonic, pluripotent, stage.\n\nPCL consists in introducing a somatic adult or senescent cell nucleus or entire cell with enlarged membrane pores in an (activated) oocyte and to withdraw this treated cell before its de-differentiation and first cell division occurs. Thus, the progressive rejuvenation capability of the oocyte is used only temporarily in order to obtain a partial natural rejuvenation. PCL permits to envisage a chosen degree of partial rejuvenation in changing the duration of the introduction of the treated cell in the oocyte. Using PCL cell de-differentiation and its age reprogramming might be, at least partially, separable. Thus the existence of an isolated ageing clock would be confirmed at least during a certain part of the cellular evolution and involution.\n\nFirst experimental result shows a possible high efficiency in partial rejuvenation of senescent mouse cells. Notably PCL rejuvenates exclusively one single tissue or organ, in contrast to classical cloning PCL is therefore unable to reconstitute an entire organism. Furthermore, PCL is feasible in a few hours in opposition to classical cloning or induced pluripotent stem cells (iPS) which all need weeks or months.\n\nClassical cloning can rejuvenate old cells but the process demands that the old cells must artificially pass through an embryonic cell stage. Partial cloning affords the advantage that the old cells to be rejuvenated do not have to pass through the embryonic cell stage and are simply made younger.\n\nThe extension of human lifespan, in terms of useful, quality, years added to life, has been a goal for many since time immemorial. And while a goal whose attainment was thought improbable, or at least achievable only in the far distant future, the discovery that animals can be cloned has brought the goal of rejuvenation much closer. The remarkable discovery that animals can be cloned showed that the nucleus of an old cell can be used as a donor in so-called “nuclear transfer” experiments where an old nucleus is transferred into a recipient egg whose own nuclear material has been removed. The “reconstructed” egg is then prompted to engage development and develops through an embryonic stage that results, once the embryo is implanted into a surrogate mother, into a new born. Thus an old cell can give rise to a newborn, which has a typical lifespan: the age of the donor cell is “wiped clean” and returned to a youthful state. Notably, in classical animal cloning the rejuvenation process involves a return to an embryonic form. Thus the specialized functions of the adult cell are also “wiped clean” and returned to an embryonic cell type. And in classical cloning passage through this embryonic state is a must for the age of the cell to be “wiped clean”.\n\nThe key notion that exemplifies “partial” cloning from “classical” cloning is the separation of the mechanism(s) that “wipe clean” the specialization of a cell from those that “wipe-clean” the age of the cell. In short, partial cloning aims to retain the specialized functions of a cell and simply make it younger, e.g., a skin cell is rejuvenated without having to pass through the embryonic stage that is a must for rejuvenation via the classical cloning technique (see diagram).\n\nIn a new laboratory at the Forschungszentrum Borstel our work on partial cloning focuses, inter alia, on the restricted, temporary, incubation of an “old” cell within the egg. In this way only the age of the cell is “wiped clean” and its specialized, differentiated, state is retained. It is simply made younger – rejuvenated - without going through the embryonic state. The measure of Diagram showing the difference between “Classical” and “Partial” cloning: Classical cloning (the route given by the black arrows) can rejuvenate an old cell but requires passage through an embryonic stage. “Partial cloning” (given by the red arrow) rejuvenates old cells without passage through an embryonic stage.“Partial cloning” (given by the red arrow)rejuvenates old cells without passage through an embryonic stage.\nIn a new laboratory at the Forschungszentrum Borstel our work on partial cloning focuses, inter alia, on the restricted, temporary, incubation of an “old” cell within the egg. In this way only the age of the cell is “wiped clean” and its specialized, differentiated, state is retained. It is simply made younger – rejuvenated - without going through the embryonic state. The measure of rejuvenation in our system is, first, the re-acquisition of the ability of an old cell to divide, something that is lost in old cells and, second, the loss of characteristics that are associated with old cells.\n\nShould such rejuvenation be achievable the consequences for medicine would be profound. It would avoid the need to artificially pass through an embryonic stage – either by nuclear transfer or by the so-called iPS cells method - to rejuvenate cells. One would simply be able to take aged cells from a patient and then return to the patient their own, histocompatible, rejuvenated heart cells, liver cells etc. In sharp contrast to the cycle of artificial de-differentiation of somatic cells to stem cells and then the artificial re-differentiation of stem cells to the desired differentiated cell type, which is highly inefficient, time-consuming and results in unstable cell types. The process of partial cloning would be efficient and rapid and thus cheap both in terms of materials and manpower. In short, partial cloning has enormous potential to relieve human suffering and disease: it is the most rapid and cheap route to successful regenerative medicine. Partial cloning also avoids the ethical problems associated with “classical” cloning in that it does not result in live born – it mere uses the oocyte briefly as a means to condition and thereby rejuvenate the old cell exclusively.\n\n"}
{"id": "25733398", "url": "https://en.wikipedia.org/wiki?curid=25733398", "title": "Perceptual computing", "text": "Perceptual computing\n\nPerceptual computing is an application of Zadeh's theory of computing with words on the field of assisting people to make subjective judgments.\n\nThe \"perceptual computer\" – \"Per-C\" – an instantiation of perceptual computing – has the architecture that is depicted in Fig. 1 [2]–[6]. It consists of three components: encoder, CWW engine and decoder. Perceptions – words – activate the Per-C and are the Per-C output (along with data); so, it is possible for a human to interact with the Per-C using just a vocabulary.\n\nA vocabulary is application (context) dependent, and must be large enough so that it lets the end-user interact with the Per-C in a user-friendly manner. The encoder transforms words into fuzzy sets (FSs) and leads to a \"codebook\" – words with their associated FS models. The outputs of the encoder activate a Computing With Words (CWW) engine, whose output is one or more other FSs, which are then mapped by the decoder into a recommendation (subjective judgment) with supporting data. The recommendation may be in the form of a word, group of similar words, rank or class.\n\nAlthough there are lots of details needed in order to implement the Per-C’s three components – encoder, decoder and CWW engine – and they are covered in [5], it is when the Per-C is applied to specific applications, that the focus on the methodology becomes clear. Stepping back from those details, the \"methodology of perceptual computing\" is:\n\n\nTo-date a Per-C has been implemented for the following four applications: (1) investment decision-making, (2) social judgment making, (3) distributed decision making, and (4) hierarchical and distributed decision-making. A specific example of the fourth application is the so-called \"Journal Publication Judgment Advisor\" [5, Ch. 10] in which for the first time only words are used at every level of the following hierarchical and distributed decision making process:\n\"n\" reviewers have to provide a subjective recommendation about a journal article that has been sent to them by the Associate Editor, who then has to aggregate the independent recommendations into a final recommendation that is sent to the Editor-in-Chief of the journal. Because it is very problematic to ask reviewers to provide numerical scores for paper-evaluation sub-categories (the two major categories are \"Technical Merit\" and \"Presentation\"), such as importance, content, depth, style, organization, clarity, references, etc., each reviewer will only be asked to provide a linguistic score for each of these categories. They will not be asked for an overall recommendation about the paper because in the past it is quite common for reviewers who provide the same numerical scores for such categories to give very different publishing recommendations. By leaving a specific recommendation to the associate editor such inconsistencies can hope to be eliminated. \nHow words can be aggregated to reflect each reviewer’s recommendation as well as the expertise of each reviewer about the paper’s subject matter is done using a linguistic weighted average. Although the journal publication judgment advisor uses reviewers and an associate editor, the word “reviewer” could be replaced by judge, expert, low-level manager, commander, referee, etc., and the term “associate editor” could be replaced by control center, command center, higher-level manager, etc. So, this application has potential wide applicability to many other applications.\n\nRecently, a new Per-C based Failure mode and effects analysis (FMEA) methodology was developed, with its application to edible bird's nest farming, in Borneo, has been reported.\nIn summary, the Per-C (whose development has taken more than a decade) is the first complete implementation of Zadeh’s CWW paradigm, as applied to assisting people to make subjective judgments.\n\n\n\n"}
{"id": "16864949", "url": "https://en.wikipedia.org/wiki?curid=16864949", "title": "Richard Vogt (aircraft designer)", "text": "Richard Vogt (aircraft designer)\n\nRichard Vogt (19 December 1894 - January 1979) was a military German aircraft designer who was known for his original airframes, including the asymmetrical BV 141 during World War II. After the war, he moved the United States as part of Operation Paperclip, where he worked on American military aircraft design.\n\nRichard Vogt was born in Schwäbisch Gmünd, a town in the Kingdom of Württemberg, which at that time was a constituent state of the German Empire. He was the seventh child of twelve siblings.\n\nHe was admitted to a school of universal literacy education in . When he was a student at the school, he had an opportunity get to know Ernst Heinkel. In 1912, when he was 18 years old, Vogt built his first aeroplane. With the help of a friend and under the eye of Heinkel, he attempted unsuccessfully to fly it just outside Mutlangen, a neighboring town to Schwäbisch Gmünd.\n\nAfter school Vogt worked for a year at an engine factory in Ludwigshafen. With the outbreak of World War I, he was conscripted into the military ranks of the German Empire, with which he was wounded in action, and medically evacuated back to Germany. Vogt then trained as a pilot in Halberstadt. \n\nOn being discharged from military service in August 1916 and found work at the Zeppelin works in Friedrichshafen. While there, Vogt was impressed by Claudius Dornier and determined to become an aircraft designer. After the war, he completed a two-year study course at the Technical University in Stuttgart, and subsequently served as an assistant to Professor Baumann at the university's Institute of Aeronautical and Automobile Systems until 1922. During that period he was awarded his first patent and received a doctorate degree.\n\nOn behalf of Dornier, Vogt was briefly sent to Italy and then, in 1923, to the Kawasaki in Kobe, Japan, which was a licensed manufacturer of Dornier aircraft. In Japan he was appointed to the rank of chief designer, and he trained the young Japanese engineer Takeo Doi to be his successor. Doi would go on to design the Ki-61 \"Hien\" . During that period Vogt designed several types including the KDA-5 Army Type 92 biplane fighter plane, KDA-2 Army Type 88 biplane reconnaissance, KDA-3 single-seat fighter, and (in cooperation with Doi) a modified version of the KDA-5 Army Type 92-I biplane fighter. He stayed with Kawasaki until 1933.\n\nIn 1933 he was offered the position of Chief Designer at Hamburger Flugzeugbau, an aircraft manufacturer recently set up by Blohm & Voss shipbuilders. During his flight back from Japan he worked on the idea of a tubular steel main wing spar which could also double as an armoured fuel tank. Almost all of his subsequent designs would feature such a combined hollow steel spar and fuel tank.\n\nVogt's next major innovation was an asymmetric aircraft layout in which the thrust line was offset to one side, allowing the pilot a clear view on the other side. It appeared in the Ha 141 reconnaissance aircraft, of which about twenty were built.\n\nShortly before World War II broke out, Hamburger Flugzeugbau was reformed as the aircraft division of Blohm & Voss and changed its name accordingly. The designation of Vogt's aircraft changed from Ha to BV, with many of the types then under development changing their designations, for example the Ha 141 became the BV 141.\n\nOther important, more conventional designs included a series of ever-larger flying boats. The BV 238 was the largest and heaviest aircraft manufactured until the end of the war by any Axis power. Vogt also developed a series of gliding munitions, but the advanced control systems caused problems and although quite large quantities were manufactured, none ever saw operational service.\n\nHis design style was noted by the British journal \"Aeroplane\" in the caption to a cartoon:\n\nVogt's final innovation was a tailless \"pfeilflieger\" (swept wing) design, well suited to the new jet engines then under development. A series of designs culminated in the P 215 all-weather fighter, which received an order for three prototypes just weeks before the war ended.\n\nAfter World War II, Vogt was recruited by the US Air Force under \"Operation Paperclip\", and he moved to the United States. He worked as a civilian employee for the Research Laboratory of the US Air Force in Dayton, Ohio from the beginning of 1947 to 1954. Subsequently he became the chief designer of the Aerophysics Development Corporation and worked there until the parent company decided to close the business in 1960.\n\nFrom August 1960 to August 1966, he served as a staff member on the team of George S. Schairer, who was the chief aerodynamicist in the research and testing division of Boeing. At Boeing, Vogt was especially involved in the design of vertical takeoff systems and hydrofoils. He also investigated the effect of the length and shape of wings on the flying range, and he proved that small extensions attached to both tips of the wings improved the aerodynamics and increased the operation range of the aircraft. This finding has been widely used in the design of modern aircraft, where the extension parts are well known as the wing tips or winglets. His last assignment was the after-launch evaluation of the design of the Boeing 747.\n\nAfter retiring from Boeing, he enjoyed developing a safe sailboat that would not turn over, and he wrote his memoirs. In 1977 a fire totally destroyed his house, resulting in the loss of many personal and technical documents.\n\nIn January 1979 he died of myocardial infarction in Santa Barbara, California, at age 84. \n\nVogt was married and had two sons.\n\nThese types were all built and flown.\n\n\n\n\n"}
{"id": "28763746", "url": "https://en.wikipedia.org/wiki?curid=28763746", "title": "Satya N. Atluri ICCES Medal", "text": "Satya N. Atluri ICCES Medal\n\nSatya N. Atluri ICCES Medal is a Medal awarded annually by ICCES (International Conference on Computational & Experimental Engineering and Sciences) and the Tech Science Press to an individual who has had a significant impact on the world of engineering, the sciences, and commerce, and the well-being of the society at large as a result. This Medal is presented at the Awards Banquet of the annual ICCES Conference. The recipient of the Medal is invited to deliver a Plenary Lecture, on a topic of her/his choosing, at the ICCES conference. This Medal honors Professor Satya N. Atluri of UCI, who founded ICCES in 1986, and founded the journals, \"CMES: Computer Modeling in Engineering & Sciences\" (2000), \"CMC: Computers, Materials, & Continua\" (2004), \"MCB: Molecular & Cellular Biomechanics\" (2004), \"SL: Structural Longevity\" (2008), and \"ACM: Advances in Computational Mechanics\" (2008), all of which are published by Tech Science Press. All these journals are in the frontier disciplines of engineering and the sciences, and especially at the interfaces of engineering and the sciences. Previously, Professor Atluri founded, and was Editor-in-Chief of the international journal, \"Computational Mechanics\", during 1986-2000.\n\nThe ICCES conferences are held annually: Tokyo (1986); Atlanta (1988); Melbourne, Australia (1991); Hong Kong (1992); Big Island, Hawaii (1995); San Jose, Costa Rica (1997); Atlanta (1998); Los Angeles (2000); Puerto Vallarta, Mexico (2001); Reno, Nevada (2002); Corfu, Greece (2003); Madeira, Portugal (2004); Chennai, India (2005); Miami (2007); Honolulu (2008); Phuket, Thailand (2009), Las Vegas (2010), and Nanjing, China (2011). All these conferences bring together each year, about 500 of the world’s leading academic, industrial, and government researchers in multidisciplinary engineering, sciences, technologies, and pertinent policies. All aspects of theory, computation, and experimentation are emphasized at these conferences. It is in this spirit that the Satya N. Atluri ICCES Medal honors: 1. Either an individual for her or his multifaceted and exemplary contributions in the broadest sense, and for their impact on society, or 2. An engineering/scientific/technological achievement by a group of individuals, which by its global visibility, benefits the well-being of vast segments of people.\n\nThe recent recipients of the Satya N. Atluri Medal include:\n2011: Dr. Guangjing Cao, President, Three Gorges Dam Groups, China; ICCES 2011 Plenary Lecture.\n2010: Dr. Ratan Naval Tata, Chairman, Tata Sons, Mumbai, India\n2009: Dr. Subra Suresh, Director, National Science Foundation, formerly from MIT\n\n"}
{"id": "2503155", "url": "https://en.wikipedia.org/wiki?curid=2503155", "title": "Sea spray", "text": "Sea spray\n\nSea spray refers to aerosol particles that are formed directly from the ocean, mostly by ejection into the atmosphere by bursting bubbles at the air-sea interface. Sea spray contains both organic matter and inorganic salts that form sea salt aerosol (SSA). SSA has the ability to form cloud condensation nuclei (CCN) and remove anthropogenic aerosol pollutants from the atmosphere. Sea spray is directly (and indirectly, through SSA) responsible for a significant degree of the heat and moisture fluxes between the atmosphere and the ocean, affecting global climate patterns and tropical storm intensity.Sea spray also influences plant growth and species distribution in coastal ecosystems and increases corrosion of building materials in coastal areas.\n\nWhen wind, whitecaps, and breaking waves mix air into the sea surface, the air regroups to form bubbles, floats to the surface, and bursts at the air-sea interface . When they burst, they release up to a thousand particles of sea spray , which range in size from nanometers to micrometers and can be expelled up to 20 cm from the sea surface . Film droplets make up the majority of the smaller particles created by the initial burst, while jet droplets are generated by a collapse of the bubble cavity and are ejected from the sea surface in the form of a vertical jet . In windy conditions, water droplets are mechanically torn off from crests of breaking waves. Sea spray droplets generated via such a mechanism are called spume droplets \"\" and are typically larger in size and have less residence time in air. Impingement of plunging waves on sea surface also generates sea spray in the form of splash droplets \"\". The composition of the sea spray depends primarily on the composition of the water from which it is produced, but broadly speaking is a mixture of salts and organic matter.  Several factors determine the production flux of sea spray, especially wind speed, swell height, swell period, humidity, and temperature differential between the atmosphere and the surface water . Production and size distribution rate of SSAs are thus sensitive to the mixing state . A lesser studied area of sea spray generation is the formation of sea spray as a result of rain drop impact on the sea surface \".\n\nIn addition to the local conditions that influence sea spray formation, there are also consistent spatial patterns in sea spray production and composition. Because sea spray is generated when air is mixed into the ocean, formation gradients are established by turbulence of the surface water . Wave action along coastal shorelines is generally where turbulence is greatest, so this is where sea spray production is the highest. Particles generated in turbulent coastal areas can travel horizontally up to 25 km within the planetary boundary layer . As distance from shore decreases, sea spray production declines to a level sustained almost exclusively by white caps . The proportion of the ocean surface area that is turbulent enough to produce significant sea spray is called the white cap fraction . The only other production mechanism of sea spray in the open ocean is through direct wind action, where strong winds actually break the surface tension of the water and lift particles into the air . However, particles of seawater generated in this way are often too heavy to remain suspended in the atmosphere and usually are deposited back to the sea within a few dozen meters of transport .\n\nDuring winter months, the ocean typically experiences stormy, windy conditions that generate more air inundation into the sea and therefore more sea spray . Calmer summer months result in lower overall production of sea spray . During peak primary productivity in the summer, increased organic matter in the surface ocean drives subsequent increases in sea spray. Given that sea spray retains the properties of the water from which it was produced, the composition of sea spray experiences extreme seasonal variation. During the summer, dissolved organic carbon (DOC) can constitute 60-90% of sea spray mass . Even though much more sea spray is produced during the stormy winter season, the composition is nearly all salt because of the low primary production .\n\nThe organic matter in sea spray consists of dissolved organic carbon (DOC) and even microbes themselves, like bacteria and viruses. The amount of organic matter in sea spray depends on microbiological processes, though the total effect of these processes is still unknown . Biomass often enters sea spray through the death and lysis of algal cells, often caused by viral infections . Cells are broken apart into DOC that is propelled into the atmosphere when surface bubbles pop. When primary productivity peaks during the summer, algal blooms can generate an enormous amount of organic matter that is eventually incorporated into sea spray . In the right conditions, aggregation of the DOC can also form surfactant or sea foam.\n\nChlorophyll-a, which is used as a proxy for primary production in the photic zone of the ocean, is viewed by some researchers as a way to approximate the amount of organic matter present in sea spray. However, one study suggests that measurements of Chlorophyll-a and organic matter content in sea spray are only weakly correlated and that Chlorophyll-a is not an adequate proxy .\n\nAt high winds the droplet evaporation layer (DEL) influences the surface energy heat exchange of the ocean . The latent heat flux of sea spray generated at the DEL has been cited as an important addition to climate modeling efforts, particularly in simulations assessing air/sea heat balance as related to hurricanes and cyclones formed during high wind events . During the formation of whitecaps, sea spray droplets exhibit the same properties as the ocean surface, but rapidly adapt to surrounding air. Some sea spray droplets immediately reabsorb into the sea while others evaporate entirely and contribute salt particles like dimethyl sulfide (DMS) to the atmosphere where they can be transported via turbulence to cloud layers and serve as CCN . The formation of these CCN like DMS have climate implications as well, due to their influence on cloud formation and interaction with solar radiation . Additionally, the contribution of sea spray DMS to the atmosphere is linked to the global sulfur cycle. Understanding total forcing from natural sources like sea spray can illuminate critical constraints posed by anthropogenic influence and can be coupled with ocean chemistry, biology and physics to predict future ocean and atmospheric variability .\n\nThe proportion of organic matter in sea spray can impact reflectance, determine the overall cooling effect of SSAs , and slightly alter the capacity for SSAs to form CCN (17). Even small changes in SSA levels can affect the global radiation budget leading to implications for global climate . SSA has a low albedo, but its presence overlaid on the darker ocean surface affects absorption and reflectance of incoming solar radiation . \n\nThe influence of sea spray on the surface heat and moisture exchange peaks during times of greatest difference between air and sea temperatures . When air temperature is low, sea spray sensible heat flux can be nearly as great as the spray latent heat flux at high latitudes . In addition, sea spray enhances the air/sea enthalpy flux during high winds as a result of temperature and humidity redistribution in the marine boundary layer . Sea spray droplets injected into the air thermally equilibrate ~1% of their mass. This leads to the addition of sensible heat prior to ocean reentry, enhancing their potential for significant enthalpy input .\n\nThe effects of sea spray transport in the atmospheric boundary layer is not yet completely understood\" \". Sea spray droplets alter the air-sea momentum fluxes by being accelerated and decelerated by the winds\" \". In hurricane-force winds, it is observed that there is some reduction in the air/sea momentum flux\" \". This reduction in momentum flux manifests as saturation of air/sea drag coefficient. Some studies have identified spray effects as one of the potential reasons for the air/sea drag coefficient saturation . It has been shown through several numerical and theoretical studies that sea spray, if present in significant amounts in the atmospheric boundary layer, leads to saturation of air-sea drag coefficients .\n\nSalt deposition from sea spray is the primary factor influencing distribution of plant communities in coastal ecosystems . Ion concentrations of sea spray deposited on land generally mirror their concentrations in the ocean, except that potassium is often higher in sea spray . Deposition of salts on land generally decreases with distance from the ocean but increases with increasing wind speed . Salt deposition from sea spray is correlated with a decrease in plant height and significant scarring, shoot reduction, stem height decrease, and tissue death on the windward side of shrubs and trees . Variation in salt deposition also influences competition between plants and establishes gradients of salt tolerance .\n\nWhile the salts within sea spray can severely inhibit plant growth in coastal ecosystems, selecting for salt-tolerant species, sea spray can also bring vital nutrients to these habitats. For example, one study showed that sea spray in Wales, UK delivers roughly 32 kg of potassium per hectare to coastal sand dunes each year . Because dune soils leach nutrients very quickly, sea spray fertilization could be very influential to dune ecosystems, especially for plants that are less competitive in nutrient-limited environments.\n\nViruses, bacteria, and plankton are ubiquitous in sea water, and this biodiversity is reflected in the composition of sea spray . Generally speaking, sea spray has slightly lower concentrations of microbes than the water it is produced from. However, the microbial community in sea spray is often distinct from nearby water and sandy beaches, suggesting that some species are more biased towards SSA transportation than others. Sea spray from one beach can contain thousands of operational taxonomic units (OTUs). Nearly 10,000 different OTUs have been discovered in sea spray just between San Francisco, CA and Monterey, CA, with only 11% of them found ubiquitously . This suggests that sea spray in every coastal region likely has its own unique assemblage of microbial diversity, with thousands of new OTUs yet to be discovered. Many of the more common OTUs have been identified to the following taxa: Cryptophyta (order), Stramenopiles (order) and OM60 (family). Many have even been identified to genus: Persicirhabdus, Fluviicola, Synecococcus, Vibrio, and Enterococcus .\n\nScientists have conjectured a stream of airborne microorganisms circles the planet above weather systems but below commercial air lanes. Some of these peripatetic microorganisms are swept up from terrestrial dust storms, but most originate from the marine microorganisms in sea spray. In 2018 a team of scientists reported that hundreds of millions of viruses and tens of millions of bacteria are deposited daily on every square meter around the planet.\n\nSea spray is largely responsible for corrosion of metallic objects near the coastline, as the salts accelerate the corrosion process in the presence of abundant atmospheric oxygen and moisture. Salts do not dissolve in air directly, but are suspended as fine particulates, or dissolved in microscopic airborne water droplets.\n\nThe salt spray test is a measure of material endurance or resistance to corrosion, particularly if the material will be used outdoors and must perform in a mechanical load bearing or otherwise critical role. These results are often of great interest to marine industries, whose products may suffer extreme acceleration of corrosion and subsequent failure due to salt water exposure.\n"}
{"id": "11345653", "url": "https://en.wikipedia.org/wiki?curid=11345653", "title": "Sexing", "text": "Sexing\n\nThrough sexing, biologists and agricultural workers determine the mian of livestock and other animals they work with. The specialized trade of chicken sexing has a particular importance in the poultry industry.\n\nAssisted physical sexing is a relevant issue in vertebrates with cloacae (e.g. birds, reptiles or amphibians) when there is no external sexual dimorphism. In veterinary practice, fibroscopy is used under general anaesthesia in birds such as parrots.\n\nMolecular sexing is a set of techniques that use DNA for determining sex in wild or domestic species (population studies, farming, genetics) or humans (archaeology, forensic medicine). Markers commonly used include amelogenin, SRY and ZFX/ZFY. Various techniques have been developed using simple polymerase chain reaction product size dimorphism, presence/absence, restriction dimorphism, or even sequencing.\n"}
{"id": "4372153", "url": "https://en.wikipedia.org/wiki?curid=4372153", "title": "The Conquest of Space", "text": "The Conquest of Space\n\nThe Conquest of Space is a 1949 speculative science book written by Willy Ley and illustrated by Chesley Bonestell. The book contains a portfolio of paintings by Bonestell depicting the possible future exploration of the solar system, with explanatory text by Ley.\n\nOf the 58 illustrations by Bonestell in \"Conquest\", most had been published previously, in color, in magazines.\n\n\n"}
{"id": "37657014", "url": "https://en.wikipedia.org/wiki?curid=37657014", "title": "The Red Wall", "text": "The Red Wall\n\nThe Red Wall: A Woman in the RCMP is a non-fiction book, written by Canadian writer Jane Hall, first published in July 2007 by General Store Publishing. In the book, the author chronicles her personal experiences as the first woman accepted in the Royal Canadian Mounted Police (RCMP). Hall recalls that ever since becoming a Mountie in 1977, people have asked her \"What it was like\"? Hall says she always avoided answering the question because she knew the story couldn't be told with a few sentences. The book's 351 pages are apparently sufficient as the book has been well received for its historical significance. Hall says of her book, \"It was time to break the silence; time to acknowledge our successes and our failures. Time to move forward.\" Hall spent eight years writing her manuscript, and another two years copy-editing her work before presenting it for publication.\n\n\"The Red Wall\" received shortlist recognition for the 2008 \"Edna Staebler Award for Creative Non-Fiction\".\n\n\nAmazon, \"The Red Wall: A Woman in the RCMP\", Book Reviews, Retrieved 01/17/2013\n\n"}
{"id": "7132677", "url": "https://en.wikipedia.org/wiki?curid=7132677", "title": "The Story of Modern Science", "text": "The Story of Modern Science\n\nThe Story of Modern Science is a ten-volume book series by Henry Smith Williams, published by Funk and Wagnalls Co. The books, published in 1923, explained in detail the current technology and scientific methods of the Modern Era.\n\nThe 10 volumes include:\n\n"}
{"id": "16277069", "url": "https://en.wikipedia.org/wiki?curid=16277069", "title": "The Theory of Interstellar Trade", "text": "The Theory of Interstellar Trade\n\nThe Theory of Interstellar Trade is a paper written in 1978 by the economist Paul Krugman. The paper was first published in March 2010 in the journal \"Economic Inquiry\". He described the paper as something he wrote to cheer himself up when he was an \"oppressed assistant professor\" caught up in the academic rat race.\n\nKrugman analyzed the question of\nHow should interest rates on goods in transit be computed when the goods travel at close to the speed of light? This is a problem because the time taken in transit will appear less to an observer traveling with the goods than to a stationary observer.\nKrugman emphasized that in spite of its farcical subject matter, the economic analysis in the paper is correctly done. In his own words,\nwhile the subject of this paper is silly, the analysis actually does make sense. This paper, then, is a serious analysis of a ridiculous subject, which is of course the opposite of what is usual in economics.\nResponding to the paper, the economist Tyler Cowen speculated on how time travel affects time preference discounting.\nMy own puzzling focuses on the determinants of real interest rates, given how time dilation changes the meaning of time preference. As you approach the speed of light you move into the future relative to more stationary observers. So can you not leave a penny in a savings account, take a very rapid spaceflight, and come back to earth \"many years later\" as a billionaire? Hardly any time has passed for you. In essence we are abolishing time preference, or at least allowing people to lower their time preference by spending money on fuel. I believe that in such worlds the real interest rate cannot exceed the costs at which more fuel can \"propel you into the future through time dilation.\"\n\nAssume you have a spaceship that can approach lightspeed and thus you can make, say 1,000 years, seem like 10 years. If you can gain the time value of money 100 times faster, that effectively makes the interest rate 100x higher for you. Thus time preference is meaningless. This is a problem for economic theory. Krugman's solution is to force the interest rate to correspond to the cost of making the trip.\n\nHere is the standard compound interest formula:\n\nFuel costs for the above trip are $X. Let us say you have $1,000 to invest and wonder whether it is worth taking a near-lightspeed voyage to increase your return 10 subjective years from now.\n\nStay on earth:\nTake trip:\n\nKrugman says that it is necessarily true that A=B, for time preference theory to stay consistent; thus it will be driven by X. In the future, given a free market, spaceship fuel costs will be the primary determinant of interest rates. The less the fuel cost, the less the interest rate, and vice versa.\n\nIn 2004, Espen Gaarder Haug published a theory he titled \"SpaceTime-Finance\" in \"Wilmott\" magazine to show how a series of finance calculations had to be adjusted to avoid arbitrage when hypothetically traveling at very high velocities relative to other observers (traders). He illustrated how such necessary adjustments already could be measurable at the speed of the space shuttle, but also that such calculations and adjustments were of little or no practical relevance today since we all are moving at the nearly same speed relative to the enormous speed of light.\n\nIn 2008, John Hickman at Berry College in Georgia (USA) published in the \"Journal of Astro Politics\" an article related to the political and cultural obstacles that could be related to hypothetical interstellar trade. A 2009 article explored tax implications.\n\n"}
{"id": "41075299", "url": "https://en.wikipedia.org/wiki?curid=41075299", "title": "Theory and History", "text": "Theory and History\n\nTheory and History: An Interpretation of Social and Economic Evolution is a treatise by Austrian school economist and philosopher Ludwig von Mises. It can be thought of as a continuation in the development of the Misesian system of social science. In particular, it provides further epistemological support for his earlier works, esp. Human Action. Most notably, Mises elaborates on methodological dualism, develops the concept of thymology – a historical branch of the sciences of human action – and presents his critique of Marxist materialism.\n\nFurthermore, Mises puts forward a theory of knowledge and value. He later explores and critically analyzes paradigms of thought like determinism, materialism, dialectic materialism, historicism, scientism, positivism, behaviorism and psychology. He argues that these schools of thought – some politically motivated, others blinded by dogmatism – have committed epistemological and methodological blunders and are not conducive to a scientific understanding of human behavior.\n\nEconomist Murray Rothbard considered \"Theory and History\" to be Mises's most overlooked work.\n\nMises presents the book in four parts.\n\nIntroduction and Part One – Value:\n\nThe first part sets the overall theme of the book with Mises introducing the concept of methodological dualism. He then expounds a theory of value that is central throughout. Regarding his view on science – as systematic body of knowledge, of both natural and social phenomena – as a means to successful action in the world, Mises argues that in order to properly understand human behavior we must attribute – as a methodological resort – volition and purpose to human behaviour. Mises considers this the epistemological and methodological basis of the sciences of human action. The branch that deals with the logical implications of action as such is called praxeology.\n\nPart Two – Determinism and Materialism:\n\nIn the second part, Mises weighs in on the free will vs. determinism controversy and comments that the long historical debates did little to settle the problems at hand. He argues that while the natural sciences, in discovering scientific laws, must presuppose a strict regularity in the occurrence of causes and effects, i.e. determinism, such a presupposition cannot be held in the case of human action. He argues further that the social sciences must take thoughts, ideas, and judgments of value as ultimately given in the analysis of human action. Our ignorance of the origins and causes of these phenomena, Mises argues, forces us – at least for the time being – to adopt a dualistic approach. He contends that attempts to find the origins and causes of these phenomena are vain, as is the task of all varieties of materialism. Mises then turns his attention to the doctrine of materialism, more specifically, that of Marxist dialectical materialism.\n\nPart Three – Epistemological Problems of History:\n\nThe third part deals with the logical and epistemological problems of historical analysis. Mises explains the individualistic character of historical human events. He argues that the historian must ultimately face – when tracing back the causal factors behind past human action – a point at which no further reduction is possible, i.e. the ideas and actions of individuals. This, he claims, is the \"ultimate given of history\". In spite of the individuality of historical events, Mises still insists there are general laws of human behaviour but that they are praxeological laws, i.e. a priori, not historical laws, i.e. a posteriori. However, historicism, according to Mises, claimed there were no general laws – especially economic laws – of human behavior. Mises then enters into a critique of historicism.\n\nMises also addresses the challenges of scientism in the context of social science, namely the application of positivism and behaviorism in the realm of human action. However, more noteworthy is Mises's presentation of thymology, a historical branch of the sciences of human action. Mises argues that thymology is what everybody resorts to when trying to \"understand\" and anticipate the historical and future actions of their fellow men, and is particularly useful to the historian. He then expounds the scope of thymology and its relation to praxeology.\n\nPart Four – The Course of History:\n\nIn the final part of his treatise, Mises dissects and critiques various speculations and interpretations of history, including a common interpretation of modern Western civilization. He also comments on his observation regarding society's move away from classical liberalism, freedom, and capitalism towards socialism and totalitarianism. Moreover, Mises notes the rising ideology of wealth and income equality and speculates on its origins. He argues that rising anti-capitalistic ideology is fostering a present trend toward the impoverishment of society. He criticizes the notion of society's inevitable \"trend toward progress\", and argues that the evolution of society and civilization is predicated – not on an automatic and inevitable path – but on the underlying ideology which can, at any time, change. To this point, Mises closes with some remarks on the uncertainty of the future and the neglect of ideological factors that can give rise to civilization but also stamp it out.\n\n\n"}
{"id": "35835908", "url": "https://en.wikipedia.org/wiki?curid=35835908", "title": "WISE 0146+4234", "text": "WISE 0146+4234\n\nWISE J014656.66+423410.0 (designation abbreviated to WISE 0146+4234) is a binary brown dwarf of spectral classes T9 and Y0 located in the constellation Andromeda. It is approximately 60 light-years from Earth..\n\nWISE 0146+4234 was discovered in 2012 by J. Davy Kirkpatrick et al. from data, collected by Wide-field Infrared Survey Explorer (WISE) Earth-orbiting satellite — NASA infrared-wavelength 40 cm (16 in) space telescope, which mission lasted from December 2009 to February 2011. In 2012 Kirkpatrick et al. published a paper in The Astrophysical Journal, where they presented discovery of seven new found by WISE brown dwarfs of spectral type Y, among which also was WISE 0146+4234.\n\nThe distance of WISE 0146+4234 was initially estimated to be 20 light-years from earth. Later measurements of its stellar parallax showed that it was actually 60 light-years away.\n\nThe other six discoveries of brown dwarfs, published in Kirkpatrick et al. (2012):\n"}
{"id": "10377624", "url": "https://en.wikipedia.org/wiki?curid=10377624", "title": "WOSTEP", "text": "WOSTEP\n\nWOSTEP, the Watchmakers of Switzerland Training and Educational Program, is an internationally recognized professional qualification in the maintenance and care of fine-quality watches. It was devised by the Centre Suisse de Formation et de Perfectionnement Horloger and is sponsored by manufacturers and retailers within the horological industry in Switzerland.\n\nDuring the 1960s, and at the request of the U.S. Government, the Swiss government created what would eventually evolve into WOSTEP- Federation of the Swiss Watch Industry FH. It was originally designed to train American watchmakers in techniques of watchmaking that developed in Geneva and the Jura mountains as from the 16th Century.\n\nIt is important to understand that at the time of the founding of Wostep, America was losing its title as \"world's largest watch producer\" to the Soviet Union (mostly making poor-quality everyday watches). As American watch companies continued to slide into oblivion after the end of World War II, some were able to update by purchasing movements from Swiss companies, even establishing their own subsidiaries in Switzerland (e.g. Waltham Watch Company, Hamilton Watch Company, Benrus, Bulova), to keep them going another 10–20 years before folding completely in the U.S. It was this reason that the U.S. requested some sort of formalized training for its best watchmakers.\n\nThere had always been relatively small imports of ultra-fine Swiss watches, but post World War II the number of watches imported either as partial or complete watches grew exponentially. These \"modern\" watch movements were decidedly different from the American companies' products, which grew out of 100 years of manufacturing (starting mass production of quality watches). American manufacturers were not able to develop new products and new methods to compete and were destroyed in record time.\n\nThe Federation developed an 11-month training program in which a watchmaker was flown to Neuchâtel, Switzerland, and trained by any one of many talented instructors that worked at WOSTEP over the years.\n\nRecent changes in structure have assured the survival of WOSTEP as a foundation with a beautiful lakefront chateau converted to the school building. With the retirement of long-time director Antoine Simonin and his wife, the next generation has taken the reins and continues to develop courses for full training used throughout the world at their WOSTEP-Partnership Schools. The school also provides a variety of industry-specific training to companies and practicing watchmakers.\n\n"}
{"id": "36057709", "url": "https://en.wikipedia.org/wiki?curid=36057709", "title": "Warren Samuel Fisher", "text": "Warren Samuel Fisher\n\nWarren Samuel Fisher (1878–1971) was an American entomologist who specialised in Coleoptera.\n\nHe was employed by the National Museum of Natural History in Washington. Fisher was especially interested in Buprestidae and Cerambycidae.\n\n"}
{"id": "175271", "url": "https://en.wikipedia.org/wiki?curid=175271", "title": "Yorick (programming language)", "text": "Yorick (programming language)\n\nYorick is an interpreted programming language designed for numerics, graph plotting, and steering large scientific simulation codes. It is quite fast due to array syntax, and extensible via C or Fortran routines. It was created in 1996 by David H. Munro of Lawrence Livermore National Laboratory.\n\nYorick is good at manipulating elements in N-dimensional arrays conveniently with its powerful syntax.\n\nSeveral elements can be accessed all at once:\n\n\n\nLike \"theading\" in PDL (Perl Data Language) and \"broadcasting\" in Numpy (Numeric extension for Python), Yorick has a mechanism to do this:\n\n\n\"..\" is a rubber-index to represent zero or more dimensions of the array.\n\n\"*\" is a kind of rubber-index to reshape a slice(sub-array) of array to a vector.\n\n\nTensor multiplication is done as follows in Yorick:\n\nmeans formula_1\n\n"}
