{"id": "1909466", "url": "https://en.wikipedia.org/wiki?curid=1909466", "title": "Adaptive reuse", "text": "Adaptive reuse\n\nAdaptive reuse refers to the process of reusing an old site or building for a purpose other than which it was built or designed for. Along with brownfield reclamation, adaptive reuse is seen by many as a key factor in land conservation and the reduction of urban sprawl. However adaptive reuse can become controversial as there is sometimes a blurred line between renovation, facadism and adaptive reuse. It can be regarded as a compromise between historic preservation and demolition.\n\nAdaptive reuse deals with the issues of conservation and heritage policies. Whilst old buildings become unsuitable for their programmatic requirements, as progress in technology, politics and economics moves faster than the built environment, adaptive reuse comes in as a sustainable option for the reclamation of sites. In many situations, the types of buildings most likely to become subjects of adaptive reuse include: industrial buildings, as cities become gentrified and the process of manufacture moves away from city; political buildings, such as palaces and buildings which cannot support current and future visitors of the site; and community buildings such as churches or schools where the use has changed over time.\n\nAdaptive reuse is seen as an effective way of reducing urban sprawl and environmental impact. By reusing an existing structure within a site, the energy required to create these spaces is lessened, as is the material waste that comes from destroying old sites and rebuilding using new materials. Through adaptive reuse old, unoccupied buildings can become suitable sites for many different types of use.\n\nWhile the process of adaptive reuse is a decision often made purely by companies establishing a particular brand or presence, there are often criteria for deciding whether a building should be conserved and reused or just demolished for the area of land it occupies. Some of these determining criteria include:\n\nThere has been much debate on the economic possibilities and viability of adaptive reuse as different corporations and companies seek to find sustainable ways to approach their corporate or retail sites. There are many outcomes that affect the economic return of adaptive reuse as an avenue to reuse of a given site. Factors such as the reuse of materials and resources as well as a lesser need to involve energy, both in terms of labor and machine powered, can effectively decrease the monetary funds needed for companies to establish sites. However, there can be hidden costs in reusing old buildings such as the unknown contamination of older sites, decay and disuse affecting the usability of a building, and the possible need for modification of an older building to fit current and future building codes.\n\nThe economic costs differ from project to project and some professionals go as far as to assert that new build is always more economical and renovation is universally more expensive, due to their own involvement with adaptive reuse projects. Others claim that the return on investment is enhanced when using an older building because of the savings involved. One Canadian developer claims that reusing buildings generally represents a saving of between 10-12% over building new. In terms of profitability, there are also assertions that adaptive reuse projects often have an uncertainty to their profitably that newer developments lack. When looking for funding to build, these considerations must be addressed.\n\nWith many heritage sites on the agenda for government agencies, there are a number of financial incentives provided in order to increase the use of older sites in many countries. In the provinces of Canada, some municipalities offer financial encouragement for heritage development such as, the City of Waterloo who waived the development charges for the Seagram Lofts that have been estimated to be around $700,000. Governing bodies also benefit from the reuse of once abandoned sites as once occupied, they generate tax revenue and therefore often recover the initial investment.\n\nWith the debate of adaptive reuse as a sustainable avenue in the development of key sites, there are many advantages to using certain sites for redevelopment. One of these advantages is the site's location. In many cases, historical sites are often located in the centers of large cities. Due to the spatial development of a given area, these buildings can often be heritage-listed and therefore sold as an entity, rather than just for the land that they occupy, which the new tenants then have to retrofit the building for their particular purpose. Older buildings also often have a specific period character through the detailing and joinery of their constructed eras that newer or reconstructed developments lack. In certain cases such as the hospitality industry, the grand character of a site can influence the feel of their building and are used for maximum potential to enhance the site's physical attractiveness to a client.\n\nAs mentioned above, adaptive reuse sometimes isn’t the most viable option for all historic sites. For some sites that have been left alone to decay by neglect, the physical damage of the site can render the site unusable both in terms of the cost to repair the damage as well as unsafe by government standards. Adaptive reuse of sites contaminated by hazardous materials such as asbestos also require special precautions during construction, although these procedures are well-established. See, for example, http://www.labor.state.ny.us/workerprotection/safetyhealth/sh56.shtm\n\nAs a comparatively young country, adaptive reuse is not the norm in Canada, where redevelopment has typically meant demolition and building anew. Calgary and Edmonton are particularly known for their pro-demolition culture, but they are not unique in this regard. However, since the 1990s, adaptive use has gained traction. The conversion of former railway-centred warehouse districts to residential and commercial uses has occurred in Edmonton, Calgary, Saskatoon, Regina, and Winnipeg.\n\nIn Toronto, the Distillery District, a neighbourhood in the city's southeast side, was entirely adapted from the old Gooderham & Worts distillery. Other prominent re-uses include the Candy Factory on Queen Street West and the Toy Factory, in the city's Liberty Village district, both designed by Quadrangle Architects, a firm specializing in adaptive re-use in Toronto and elsewhere. Vancouver's Yaletown, an upscale neighbourhood established in the 1990s, features warehouses and other small-industrial structures and spaces converted into apartments and offices for the gentrification of the area. Vancouver's Granville Island also demonstrates a successful mix of adaptive reuse as well as retention of traditional uses in the same district. Montreal's Griffintown, Old Port, and Lachine Canal areas all feature ex-industrial areas that have been reused or will do so in the future on current plans.\n\nOther noted adaptive reuse projects in the 2010s have included the Laurentian School of Architecture in Sudbury, which is incorporating several historic buildings in the city's downtown core into its new campus, similar to the downtown campus of NSCAD University in Halifax, and Mill Square in Sault Ste. Marie, an ongoing project to convert the derelict St. Mary's Paper mill into a mixed-use cultural and tourism hub.\n\nA number of former military bases in Canada, declared surplus in the 1990s, have also proven to be opportune for adaptive reuse. An example is the former CFB Cornwallis in rural Nova Scotia which was largely converted, without demolitions, into a business park.\n\nGhirardelli Square in San Francisco was the first major adaptive reuse project in the United States, opening in 1964. Urban waterfronts, historically used as points for industrial production and transport, are now selling-points for home buyers and renters. In American city neighborhoods that have seen racial and ethnic demographic changes over the last century, some houses of worship have been converted for other religions, and some others have been converted into residences.\n\nA large number of brick mill buildings in the Northeast United States have undergone mill conversion projects. In the United States, especially in the Northeast and Midwest, loft housing is one prominent result of adaptive reuse projects. Formerly-industrial areas such as the Meatpacking District in New York City, Callowhill in Philadelphia and SoMa in San Francisco are being transformed into residential neighborhoods through this process. This transformation is sometimes associated with gentrification. Station Square in Pittsburgh Pennsylvania is an example of a mile-long former Pittsburgh and Lake Erie Railroad terminal and headquarters being converted into a retail, office, hotel, and tourist destination. The Pratt Street Power Plant in Baltimore was converted to offices, retail, and restaurants. An example of adaptive reuse conversion to office space are The Hilliard Mills. The adaptive reuse of Empire Stores will transform seven abandoned coffee warehouse in Brooklyn Bridge Park in New York City into office, retail, restaurant and a rooftop public park.\n\nOther museums adapted from old factories include \"MassMOCA\", the Massachusetts Museum of Contemporary Art, the Watermill Center in Long Island, New York, and The Dia Art Foundation Museum in upstate New York.\n\nIn San Diego, California, the historic brick structure of the Western Metal Supply Co. building at 7th Avenue (between K and L Streets) was preserved and incorporated into the design of Petco Park, the new baseball-only ballpark of the San Diego Padres, and can be prominently seen in the left-field corner of that ballpark. It now houses the team's flagship gift shop, luxury rental suites, a restaurant and rooftop bleachers, and its southeast corner serves as the ballpark's left field foul pole.\n\nIn Australia, there have been a number of adaptive reuse projects as the main cities have turned from industrial areas into areas of high value and business areas. In Sydney, sites such as the old Sydney Mint have been renovated and adapted into inner-city headquarters for the Historic Houses Trust of New South Wales. The movement of the city from an industrial, working class area into a gentrified area with high house prices has helped a number of adaptive reuse sites to exist within such an area, the old Hyde Park Barracks building has also been transformed from an old jail into a museum which documents and records the history of Australia's first settlers and convicts.\n\nThe industrial history of Australia has also been an influencing factor in determining the types of buildings and areas which have gone on to become adaptive reuse sites, especially in the realms of private residences and community based buildings. Some such sites include, Nonda Katsalidis’ Malthouse apartments in Richmond, a conversion of a former grain silo and the South Australian site of the Balhannah Mines which was adapted into a private residence and has received awards from the Housing Industry Association and the Design Institute of Australia.\n\nIn Adelaide four prominent, heritage listed 19th Century buildings in poor repair were restored, refurbished and given new roles by the South Australian Government during the Rann Government (2002 to 2011). The Torrens Building in Victoria Square, former headquarters of the Registrar-General, was restored and adapted to become the Australian campus for both Carnegie Mellon University and University College London. The former Adelaide Stock Exchange building was purchased, restored and adapted to become the Science Exchange for the Royal Institution Australia and the Australian Science Media Centre. The Torrens Parade Ground and building were restored for use as a headquarters for veterans' organisations. Nearly $50 million was committed to restore and adapt the large Glenside Psychiatric Hospital and precinct as the new Adelaide Studios of the South Australian Film Corporation opened by Premier Rann in October 2011. And the 62 hectare former Mitsubishi Motors plant is being adapted to become a clean manufacturing centre and education and training hub for Flinders University and TAFE.\n\nIn Europe, the main forms of adaptive reuse have been around former palaces and unused residences of the different European royal families into publicly accessible galleries and museums. Many of the spaces have been restored with period finishes and display different collections of art, and design. In Paris, France, the most famous example of adaptive reuse is the Musée du Louvre, a former palace built in the late 12th century under Philip II and opened to the public as a museum in 1793. Also, in London, England, the Queen's House, a former royal residence built around 1614, has become part of the National Maritime Museum and houses the museum's fine art collection.\n\nThe Tate Modern, also in London is another example of adaptive reuse in the European continent, unlike other adaptive reuse galleries in Europe, the Tate Modern takes full advantage of the site of the former Bankside Power Station, which involved the refurbishment of the old, abandoned power station. The wide industrial space has proven to be a worthy backdrop to modern art, with the famous turbine hall hosting artists including Olafur Eliasson, Rachel Whiteread and Ai Weiwei.\n\nOther famous adaptive reuse sites in Europe include the Maastricht branch of the Selexyz chain in the Netherlands. This project received 2007 Lensvelt de Architect interior design award for its innovative reuse and is number one on \"The Guardian\"'s worldwide top ten bookstores list.\n\nIn Łódź, Poland, the Izrael Poznański mills have been turned into the Manufaktura mixed-use development, including a mall, 3 museums, multi-cinema and restaurants.\n\n\n"}
{"id": "5810240", "url": "https://en.wikipedia.org/wiki?curid=5810240", "title": "Alfred John Jukes-Browne", "text": "Alfred John Jukes-Browne\n\nAlfred John Jukes-Browne, FRS FGS (16 April 1851 - 14 August 1914) was a British invertebrate palaeontologist and stratigrapher.\n\nHe was born Alfred John Browne near Wolverhampton in 1851 to Alfred Hall and Caroline Amelia (née Jukes) Browne. His uncle was the geologist Joseph Beete Jukes, well known for his work on the English and Irish geological surveys. Browne added his mother's maiden name of Jukes to his own as soon as he came of age. He was educated at Highgate School (1863–1868) and gained a BA at St John's College, Cambridge.\n\nHe secured a post in 1874 on the staff of the Geological Survey and was chiefly occupied in mapping parts of Suffolk, Cambridge, Rutland, and Lincoln up to 1883 and then entrusted with the preparation of a monograph on the British Upper Cretaceous rocks. He subsequently wrote a number of books on the subject. He retired from the Geological Survey in 1902 on account of ill-health.\n\nHe was elected a Fellow of the Royal Society in 1909.\n\nHe died in Devon in 1914. He had married Emma Jessie Smith in 1881, who died giving birth to their second child in 1892.\n\n"}
{"id": "2272886", "url": "https://en.wikipedia.org/wiki?curid=2272886", "title": "Antisolar point", "text": "Antisolar point\n\nThe antisolar point is the abstract point on the celestial sphere directly opposite of the Sun from an observer's perspective. This means that the antisolar point lies above the horizon when the Sun is below it, and vice versa. On a sunny day, the antisolar point can be easily found; it is located within the shadow of the observer's head. Like the zenith and nadir, the antisolar point is not fixed in three-dimensional space, but is defined relative to the observer. Each observer has their own antisolar point, which moves along with them as they change position.\n\nThe antisolar point forms the geometric center of several optical phenomena, including subhorizon haloes, rainbows, glories, and heiligenschein. Occasionally, around sunset or sunrise, anticrepuscular rays appear to converge toward the antisolar point near the horizon. However, this is an optical illusion caused by perspective; in reality, the \"rays\" (i.e. bands of shadow) run near-parallel to each other.\n\nAlso around the antisolar point, the gegenschein is often visible in a moonless night sky away from city lights, arising from the backscatter of sunlight by interplanetary dust. In astronomy, the full Moon or a planet in opposition lies around the antisolar point. During a total lunar eclipse, the full Moon enters the umbra of Earth's shadow, which the planet has been casting onto its atmosphere, into space, and toward the antisolar point.\n\nThe anthelic point is often used as a synonym for the antisolar point, but the two should be differentiated.\nWhile the antisolar point is directly opposite the sun, always below the horizon when the sun is up, the anthelic point is opposite but at the same elevation as the sun, and is therefore located on the parhelic circle. There are several halo phenomena that are centered on or converge on the anthelic point, such as the anthelion, Wegener arcs, Tricker arcs and the parhelic circle itself.\n"}
{"id": "2951953", "url": "https://en.wikipedia.org/wiki?curid=2951953", "title": "Argo (ROV)", "text": "Argo (ROV)\n\nArgo is an unmanned deep-towed undersea video camera sled developed by Dr. Robert Ballard through Woods Hole Oceanographic Institute's Deep Submergence Laboratory. \"Argo\" is most famous for its role in the discovery of the wreck of the RMS \"Titanic\" in 1985. Argo would also play the key role in Ballard's discovery of the wreck of the battleship \"Bismarck\" in 1989.\n\nThe towed sled, capable of operating depths of 6,000 meters (20,000 feet), meant 98% of the ocean floor was within reach. The original Argo, used to find Titanic, was 15 feet long, 3.5 feet tall, and 3.5 feet wide and weighed about 4,000 pounds in air. It had an array of cameras looking forward and down, as well as strobes and incandescent lighting to illuminate the ocean floor. It could acquire wide-angle film and television pictures while flying 50 to 100 feet above the sea floor, towed from a surface vessel, and could also zoom in for detailed views.\n\n"}
{"id": "1994152", "url": "https://en.wikipedia.org/wiki?curid=1994152", "title": "Aurostibite", "text": "Aurostibite\n\nAurostibite is an isometric gold antimonide mineral which is a member of the pyrite group. Aurostibite was discovered in 1952 and can be found in hydrothermal gold-quartz veins, in sulfur-deficient environments that contain other antimony minerals. The mineral can be found in Yellowknife in the Northwest Territories of Canada, and the Timiskaming District in Ontario, Canada. Antimonides are rare and are normally placed in the sulfide class by mineralogists.\n\n"}
{"id": "20951069", "url": "https://en.wikipedia.org/wiki?curid=20951069", "title": "Bhāskara's wheel", "text": "Bhāskara's wheel\n\nBhāskara's wheel was invented in 1150 by Bhāskara II, an Indian mathematician, in an attempt to create a hypothetical perpetual motion machine. The wheel consisted of curved or tilted spokes partially filled with mercury. Once in motion, the mercury would flow from one side of the spoke to another, thus forcing the wheel to continue motion, in constant dynamic equilibrium.\n"}
{"id": "29333313", "url": "https://en.wikipedia.org/wiki?curid=29333313", "title": "Boyd Glacier", "text": "Boyd Glacier\n\nBoyd Glacier () is a heavily crevassed glacier flowing west-northwest for about to the Sulzberger Ice Shelf between Bailey Ridge and Mount Douglass in the Ford Ranges of Marie Byrd Land. It was discovered on aerial flights of the Byrd Antarctic Expedition in 1934, and named for Vernon D. Boyd, an expedition machinist, and a member of West Base of the United States Antarctic Service (1939–41).\n"}
{"id": "219761", "url": "https://en.wikipedia.org/wiki?curid=219761", "title": "Calx", "text": "Calx\n\nCalx is a substance formed from an ore or mineral that has been heated.\n\nCalx, especially of a metal, is now known as an oxide. According to the obsolete phlogiston theory, the calx was the true elemental substance, having lost its phlogiston in the process of combustion. \n\n\"Calx\" is also sometimes used in older texts on artist's techniques to mean calcium oxide. \n\nCalx is Latin for chalk or limestone, from the Greek χάλιξ (\"khaliks\", “pebble”). It is not to be confused with the Latin homonym meaning heelbone (or calcaneus in modern medical Latin), which has an entirely separate derivation.\n\n"}
{"id": "5440635", "url": "https://en.wikipedia.org/wiki?curid=5440635", "title": "Carlos Frenk", "text": "Carlos Frenk\n\nCarlos Silvestre Frenk, (born 27 October 1951) is a Mexican-British cosmologist. His main interests lie in the field of cosmology, galaxy formation and computer simulations of cosmic structure formation. \n\nFrenk was educated at the University of Mexico and went on to study for a PhD in astronomy at the University of Cambridge where his thesis was supervised by Bernard J. T. Jones and awarded in 1981. \n\nFrenk was the inaugural Ogden Professor of Fundamental Physics at Durham University in 2001, following an endowment by Peter Ogden. He still holds this chair, and is Director of the Institute for Computational Cosmology at Durham, which is part of the Ogden Centre for Fundamental Physics. He is also Principal Investigator of the Virgo Consortium.\n\nHe was elected a Fellow of the Royal Society (FRS) in 2004 and is a member of the Royal Society's Council. He won the Gold Medal of the Royal Astronomical Society in 2014. Other awards and honours include:\n\n\nFrenk was appointed Commander of the Order of the British Empire (CBE) in the 2017 Birthday Honours for services to cosmology and the public dissemination of basic science. He was interviewed by Kirsty Young for \"Desert Island Discs\", first broadcast in 2018.\n"}
{"id": "18734771", "url": "https://en.wikipedia.org/wiki?curid=18734771", "title": "Classification of Instructional Programs", "text": "Classification of Instructional Programs\n\nThe Classification of Instructional Programs (CIP) is a taxonomy of academic disciplines at institutions of higher education in the United States and Canada.\n\nThe CIP was originally developed by the National Center for Education Statistics (NCES) of the United States Department of Education in 1980 and was revised in 1985, 1990, 2000 and 2010. The 2010 edition (CIP 2010) is the fourth and current revision of the taxonomy. Instructional programs are classified by a six-digit CIP at the most granular level and are classified according to the two-digit and four-digit prefixes of the code. For example, \"Forensic Science and Technology\" has the six-digit code 43.0106, which places it in \"Criminal Justice and Corrections\" (43.01) and \"Homeland Security, Law Enforcement, Firefighting and Related Protective Services\" (two-digit CIP 43).\n\n"}
{"id": "55085766", "url": "https://en.wikipedia.org/wiki?curid=55085766", "title": "Creepiness", "text": "Creepiness\n\nCreepiness is the state of being creepy, or causing an unpleasant feeling of fear or unease. A person who exhibits creepy behaviour is called a creep. Certain traits or hobbies may make people seem creepy to others. The internet has been described as increasingly creepy. Adam Kotsko has compared the modern conception of creepiness to the Freudian concept of \"unheimlich\". The term has also been used to describe paranormal or supernatural phenomena.\n\nThe concept of creepiness has only recently been formally addressed in social media marketing. In the abstract the feeling of \"creepiness\" is subjective: for example some dolls have been described as creepy. Part of the modern attribute of \"creepiness\" includes the feeling of social stalking and privacy invasions. For example, the practice of ride share service Uber monitoring the post-ride activity of its passengers has been described as creepy. In 2017 the company announced that they would be ending this practice. An example of purported creepy behavior is that by the purity pledgers of the purity movement. \n\nA 2016 study by Francis McAndrew and Sara Koehnke from Knox College (Illinois) surveyed 1,342 people of all ages (1,029 females and 312 male) about what kind of traits the subjects found creepy if a friend was describing meeting someone new to them. The study found that men were more likely to be seen as creepy than women. Females are more likely than males to perceive sexual threat from a creepy person. Unpredictability is an important component of creepiness. Also hobbies and occupations related to handling the dead, clowns, and bird watching were commonly thought as creepy. A post on Jezebel has said that there exists a blurriness and ill-defined difference between healthy forms of male sexual expression and being creepy. The term \"creep\" is typically applied to unattractive people, a facet that has resulted in some analysts describing the term as cacophobic.\n\n"}
{"id": "54552914", "url": "https://en.wikipedia.org/wiki?curid=54552914", "title": "Dave Rahm", "text": "Dave Rahm\n\nDavid \"Dave\" Rahm (1931–1976), nicknamed \"the Flying Professor\", was a Canadian geologist, professor and stunt pilot. He taught at Western Washington State College and was a visiting professor of geology at the University of Jordan. Rahm lived in Bellingham, Washington.\n\nRahm met King Hussein of Jordan in 1974 at the Abbotsford Air Show in British Columbia. Hussein asked him to come to Jordan and train the Royal Jordanian Falcons aerobatics team. Rahm was killed in a crash in the summer of 1976 while performing a stunt in Amman, Jordan.\n\nRahm died performing a combination of a Lomcevak, tailslide and stall turn in a Pitts Special. Rahm's wife, Katy Rahm, and King Hussein were both present. Hussein attempted to pull Rahm from his burning plane, but Rahm had already died. Writer Annie Dillard wrote an essay about Rahm called \"The Stunt Pilot\", reprinted as the last chapter of her collection, \"The Writing Life\". Rahm's widow later wrote a memoir, \"Flying High: Soaring Above the Tragedies of Life\".\n"}
{"id": "5586006", "url": "https://en.wikipedia.org/wiki?curid=5586006", "title": "Eight Little Piggies", "text": "Eight Little Piggies\n\nEight Little Piggies (1993) is the sixth volume of collected essays by the Harvard paleontologist Stephen Jay Gould. The essays were selected from his monthly column \"The View of Life\" in \"Natural History\" magazine, to which Gould contributed for 27 years. The book deals, in typically discursive fashion, with themes familiar to Gould's writing: evolution and its teaching, science biography, probabilities and common sense.\n\nThe title essay, \"Eight Little Piggies\", explores concepts such as archetypes and polydactyly via the anatomy of early tetrapods. Other essays discuss themes such as the scale of extinction, vertebrate anatomy, grand patterns of evolution, and human nature.\n\n\n"}
{"id": "36732828", "url": "https://en.wikipedia.org/wiki?curid=36732828", "title": "Flat pseudospectral method", "text": "Flat pseudospectral method\n\nThe flat pseudospectral method is part of the family of the Ross–Fahroo pseudospectral methods introduced by Ross and Fahroo. The method combines the concept of differential flatness with pseudospectral optimal control to generate outputs in the so-called flat space.\n\nBecause the differentiation matrix, formula_1, in a pseudospectral method is square, higher-order derivatives of any polynomial, formula_2, can be obtained by powers of formula_1,\n\nwhere formula_5 is the pseudospectral variable and formula_6 is a finite positive integer. \nBy differential flatness, there exists functions formula_7 and formula_8 such that the state and control variables can be written as,\n\nThe combination of these concepts generates the flat pseudospectral method; that is, x and u are written as,\nThus, an optimal control problem can be quickly and easily transformed to a problem with just the Y pseudospectral variable.\n\n"}
{"id": "6298225", "url": "https://en.wikipedia.org/wiki?curid=6298225", "title": "GME of Deutscher Wetterdienst", "text": "GME of Deutscher Wetterdienst\n\nGME is an operational global numerical weather prediction model run by Deutscher Wetterdienst, the German national meteorological service. The model is on almost uniform icosahedral-hexagonal grid. The GME grid point approach avoids the disadvantages of spectral techniques as well as the pole problem in latitude–longitude grids and provides a data structure well suited to high efficiency on distributed memory parallel computers.\n\nThe GME's approach to a global grid would later be utilized by the Flow-following, finite-volume Icosahedral Model (FIM), an experimental model currently in development in the United States.\n\n"}
{"id": "52745729", "url": "https://en.wikipedia.org/wiki?curid=52745729", "title": "Ghost (physics)", "text": "Ghost (physics)\n\nIn the terminology of quantum field theory, a ghost, ghost field, or gauge ghost is an unphysical state in a gauge theory. Ghosts are necessary to keep gauge invariance in theories where the local fields exceed a number of physical degrees of freedom.\n\nFaddeev–Popov ghosts are extraneous fields which are introduced to maintain the consistency of the path integral formulation. They are named after Ludvig Faddeev and Victor Popov.\n\nFaddeev–Popov ghosts are sometimes referred to as \"good ghosts\".\n\nGoldstone bosons are sometimes referred to as ghosts. Mainly, when speaking about the vanishing bosons of the spontaneous symmetry breaking of the electroweak symmetry through the Higgs mechanism. These \"good\" ghosts are artefacts of gauge fixing. The longitudinal polarization components of the W and Z bosons correspond to the Goldstone bosons of the spontaneously broken part of the electroweak symmetry SU(2)⊗U(1), which, however, are not observable. Because this symmetry is gauged, the three would-be Goldstone bosons, or ghosts, are \"eaten\" by the three gauge bosons (\"W\" and \"Z\") corresponding to the three broken generators; this gives these three gauge bosons a mass, and the associated necessary third polarization degree of freedom.\n\n\"Bad ghosts\" represent another, more general meaning of the word \"ghost\" in theoretical physics: states of negative norm, or fields with the wrong sign of the kinetic term, such as Pauli–Villars ghosts, whose existence allows the probabilities to be negative thus violating unitarity.\n\nGhost particles could obtain the symmetry or break it in gauge fields. The \"good ghost\" particles actually obtain the symmetry by unchanging the \"gauge fixing Lagrangian\" in a gauge transformation, while bad ghost particles break the symmetry by bringing in the non-abelian G-matrix which does change the symmetry, and this was the main reason to introduce the gauge covariant and contravariant derivatives.\n\nA ghost condensate is a speculative proposal in which a ghost, an excitation of a field with a wrong sign of the kinetic term, acquires a vacuum expectation value. This phenomenon breaks Lorentz invariance spontaneously. Around the new vacuum state, all excitations have a positive norm, and therefore the probabilities are positive definite.\n\nWe have a real scalar field φ with the following action\n\nwhere \"a\" and \"b\" are positive constants and\n\nusing the sign convention in the (+, −, −, −) metric signature.\n\nThe theories of ghost condensate predict specific non-Gaussianities of the cosmic microwave background. These theories have been proposed by Nima Arkani-Hamed, Markus Luty, and others.\n\nUnfortunately, this theory allows for superluminal propagation of information in some cases and has no lower bound on its energy. This model doesn't admit a Hamiltonian formulation (the Legendre transform is multi-valued because the momentum function isn't convex) because it is acausal. Quantizing this theory leads to problems.\n\nThe Landau pole is sometimes referred as the Landau ghost. Named after Lev Landau, this ghost is an inconsistency in the renormalization procedure in which there is no asymptotic freedom at large energy scales.\n\n"}
{"id": "42100545", "url": "https://en.wikipedia.org/wiki?curid=42100545", "title": "Glossary of operating systems terms", "text": "Glossary of operating systems terms\n\nThis page is a glossary of Operating systems terminology.\n\n\n\n\n\n\n\n\n\n\nWindows\n\n\n"}
{"id": "42963240", "url": "https://en.wikipedia.org/wiki?curid=42963240", "title": "Grapevine virus E", "text": "Grapevine virus E\n\nGrapevine virus E (GVE) is a plant virus species in the genus \"Vitivirus\".\n\n\n"}
{"id": "20041584", "url": "https://en.wikipedia.org/wiki?curid=20041584", "title": "H square", "text": "H square\n\nIn mathematics and control theory, H, or \"H-square\" is a Hardy space with square norm. It is a subspace of \"L\" space, and is thus a Hilbert space. In particular, it is a reproducing kernel Hilbert space.\n\nIn general, elements of \"L\" on the unit circle are given by\n\nwhereas elements of \"H\" are given by\n\nThe projection from \"L\" to \"H\" (by setting \"a\" = 0 when \"n\" < 0) is orthogonal.\n\nThe Laplace transform formula_3 given by\n\ncan be understood as a linear operator\n\nwhere formula_6 is the set of square-integrable functions on the positive real number line, and formula_7 is the right half of the complex plane. It is more; it is an isomorphism, in that it is invertible, and it isometric, in that it satisfies\n\nThe Laplace transform is \"half\" of a Fourier transform; from the decomposition\n\none then obtains an orthogonal decomposition of formula_10 into two Hardy spaces\n\nThis is essentially the Paley-Wiener theorem.\n\n\n"}
{"id": "51103519", "url": "https://en.wikipedia.org/wiki?curid=51103519", "title": "Human interactions with fungi", "text": "Human interactions with fungi\n\nHuman interactions with fungi include both beneficial uses, whether practical or symbolic, and harmful interactions such as when fungi damage crops, timber, or food.\n\nYeasts have been used since ancient times to leaven bread and to ferment beer and wine. More recently, mould fungi have been exploited to create a wide range of industrial products, including enzymes and drugs. Medicines based on fungi include antibiotics, immunosuppressants, statins and many anti-cancer drugs. The yeast species \"Saccharomyces cerevisiae\" is an important model organism in cell biology. The fruiting bodies of some larger fungi are collected as edible mushrooms, including delicacies like the chanterelle, cep, and truffle, while a few species are cultivated. Mould fungi provide the meaty (umami) flavour of fermented soybean products such as tempeh, miso and soy sauce, and contribute flavour and colour to blue cheeses including Roquefort and Stilton. Moulds also yield vegetarian meat substitutes like Quorn. Some fungi, especially the fly agaric and psilocybin mushrooms are used for the psychoactive drugs that they contain; these in particular are the focus of academic study in the field of ethnomycology. Fungi have appeared, too, from time to time, in literature and art.\n\nFungi create harm by spoiling food, destroying timber, and by causing diseases of crops, livestock, and humans. Fungi, mainly moulds like \"Penicillium\" and \"Aspergillus\", spoil many stored foods. Fungi cause the majority of plant diseases, which in turn cause serious economic losses. Sometimes, as in the Great Irish Famine of 1845–1849, fungal diseases of plants, in this case potato blight caused by \"Phytophthora\", result in large-scale human suffering. Fungi are similarly the main cause of economic losses of timber in buildings. Finally, fungi cause many diseases of humans and livestock; Aspergillosis kills some 600,000 people a year, mainly however those with already weakened immune systems.\n\nCulture consists of the social behaviour and norms found in human societies and transmitted through social learning. Cultural universals in all human societies include expressive forms like art, music, dance, ritual, religion, and technologies like tool usage, cooking, shelter, and clothing. The concept of material culture covers physical expressions such as technology, architecture and art, whereas immaterial culture includes principles of social organization, mythology, philosophy, literature, and science. This article describes the roles played by fungi in human culture.\n\nYeasts have been used since ancient times to leaven bread and to ferment beer and wine. More recently, fungi have been used for a wide variety of industrial fermentations, whether working directly for their effects on materials such as processing paper pulp or bioremediating industrial waste, or serving as the source of enzymes for many purposes, such as fading and softening denim for fashionable blue jeans. Fungi yield a wide variety of industrial enzymes including amylases, invertases, cellulases and hemicellulases, pectinases, proteases, laccases, phytases, alpha-glucuronidases, mannanases, and lipases.\n\nThe fruiting bodies of many larger fungi such as the chanterelle and the cep are collected as edible mushrooms.\nSome, such as truffles, are esteemed as costly delicacies. A few species such as \"Agaricus bisporus\" and oyster mushrooms (\"Pleurotus\" spp.) are cultivated.\n\nMould fungi are the source of the meaty (umami) flavour of the soybean products tempeh, miso and soy sauce. Tempeh has been produced in Java since the 13th century. Like tofu, it is made into protein-rich blocks, but these have a firm texture and earthy flavour, since (unlike tofu) the whole beans are retained, providing a higher content of dietary fibre and vitamins. Miso too is rich in protein, vitamins and minerals. It is fermented from a mixture of soybeans and cereals, forming a soft paste used to flavour soups and other Japanese dishes. Soy sauce has been used in China since the 2nd century AD, and is now widespread in Asia. Like miso, it is made by fermenting a mixture of soybeans and cereals with moulds such as \"Aspergillus oryzae\".\n\nThe mould \"Penicillium roqueforti\" contributes the blue coloration and much of the flavour in blue cheeses such as Roquefort and Stilton. Mould fungi are processed to produce vegetarian meat substitutes such as Quorn.\n\nFungi are the sources of many types of medicinal drug including antibiotics, immunosuppressants, and statins. Major classes of antibiotics, the penicillins and the cephalosporins, are derived from substances produced by fungi. So also are the immunosuppressant macrolides, the cyclosporins.\n\nThe cholesterol-lowering drugs, the statins, were initially produced by fungi including \"Penicillium\". The first commercial statin, lovastatin, was fermented by the mould \"Aspergillus terreus\".\n\nNumerous anti-cancer drugs such as the mitotic inhibitors vinblastine, vincristine, podophyllotoxin, griseofulvin, aurantiamine, oxaline, and neoxaline are produced by fungi.\n\nMany fungi have been used as folk medicines around the world, including in Europe and in India where traditions are well documented. Some have been found to have useful active ingredients, though these do not always correspond with traditional uses of the fungi concerned. Ergot and various cereal smuts, such as \"Ustilago tritici\" (wheat grain smut) were used for disorders of pregnancy. Yeasts, made into a boiled paste with wheat flour, were used in India to treat fevers and dysentery. Wounds were treated in Europe with moulds, using for example a slice of mouldy bread or mouldy wheat straw, with active ingredients patulin and other penicillin-like compounds.\n\nThe yeast species \"Saccharomyces cerevisiae\" has been an important model organism in modern cell biology for much of the twentieth century, and is one of the most thoroughly researched eukaryotic microorganisms. It was the first eukaryote whose genome was sequenced. In the twentyfirst century, the filamentous mould \"Aspergillus\" has been adopted for genome studies.\n\nEntomopathogenic fungi infect and kill insects, including a variety of pest species, so they have been investigated as possible biological control agents. A variety of Ascomycetes, including \"Beauveria\", \"Lecanicillium\", \"Metarhizium\", and \"Paecilomyces\" have promising features for use as biological insecticides. \"Metarhizium\" in particular can help to control outbreaks of locusts.\n\nSome species such as the fly agaric and psilocybin mushrooms are used for the psychoactive drugs that they contain. These are the focus of academic study and intense debate in the field of ethnomycology. In the 1950s, the American banker Robert Gordon Wasson participated in a Mazatec psilocybin mushroom ritual, and wrote an influential but controversial book claiming that the Soma mentioned in the Rigveda was \"Amanita muscaria\", the fly agaric. The mycologist John Ramsbottom however confirmed one element that Allegro later wove into his theory, stating in 1953 that the tree of the knowledge of good and evil fresco in the Plaincourault Chapel depicted \"Amanita muscaria\".\n\nThe ergot fungi whose sclerotia appear as \"black grain\" in rye and other cereals are implicated in the witch trials of 17th century Norway, where the hallucinations caused by ergotism, with visions of Satan as a black dog or cat, caused people to be accused of witchcraft. People in other cultures such as the Aztecs brewed drinks with ergot, which contains alkaloids based on lysergic acid.\n\nFungi have appeared from time to time in literature, both for children and for adults. In Lewis Carroll's 1865 \"Alice's Adventures in Wonderland\", Alice grows larger if she eats one side of the mushroom, and shrinks if she eats from the other side. Shakespeare has Prospero remark in \"The Tempest\" that elves \"make midnight mushrooms\". Poems and novels about or mentioning fungi have been written by Edmund Spenser, Percy Bysshe Shelley, Keats, Tennyson, Arthur Conan Doyle, D. H. Lawrence, and Emily Dickinson. Tennyson referred to the fairy ring mushroom (\"Marasmius\") with the phrase \"the fairy footings on the grass\".\n\nFungi sometimes feature in works of art, such as by Paolo Porpora in the late 17th century. The children's author Beatrix Potter painted hundreds of accurate watercolour illustrations of fungi. More recently, artists such as Martin Belou, Helen Downie (alias \"Unskilled Worker\"), and Steffen Dam have created installations and paintings of mushrooms.\n\nFungi, especially moulds but also yeasts, are important agents of food spoilage. \n\"Penicillium\" moulds cause soft rot such as of apples, while \"Aspergillus\" moulds create patches on the surface of old bread, yoghurt and many other foods. Yeasts spoil sugary foods such as plums and jams, fermenting the sugars to alcohol. Scientific understanding of spoilage began in the 19th century with works such as Louis Pasteur's 1879 \"Studies on Fermentation\", which investigated the spoilage of beer.\n\nSaprotrophic wood-decay fungi are the primary cause of decomposition of wood, causing billions of dollars of economic damage each year. Fungal decay, while useful in composting, is destructive of timber exposed to the weather, and in the case of dry rot caused by \"Serpula lacrymans\", also of timbers in largely dry houses. Some wood-decay fungi such as the honey fungi, species of \"Armillaria\", are parasites of living trees, attacking their roots and eventually killing them, and continuing to decompose the wood when they are dead. Honey fungus is a serious horticultural pest, as it can spread from tree to tree by long strap-shaped rhizomorphs in the soil.\n\nFungi are important crop pathogens, as they reproduce rapidly, affect a wide range of crops around the world, cause some 85% of plant diseases, and can create serious economic losses. The range of types of fungi involved is also wide, including Ascomycetes such as \"Fusarium\" causing wilt, Basidiomycetes such as \"Ustilago\" causing smuts and \"Puccinia\" causing cereal rusts, and Oomycetes such as \"Phytophthora\" causing potato late blight and the resulting Great Irish Famine of 1845-1849. Where crop diversity is low, and in particular where single varieties of major crops are nearly universal, fungal diseases can cause the loss of an entire crop, as with the potato in Ireland, and as with the monocultured crop of maize (corn) in the USA in 1970, where over a billion dollars' worth of production was lost. Similarly, the 'Gros Michel' seedless banana crop was essentially completely destroyed worldwide in the 1950s by the wild fungus, \"Fusarium oxysporum\". It was replaced by the Cavendish banana, which in turn was in 2015 facing total destruction by the same disease.\n\nPathogenic fungi cause a variety of diseases in humans and livestock. Aspergillosis, most commonly caused by \"Aspergillus fumigatus\", kills some 600,000 people per year, mostly those with already weakened immune systems. \"Pneumocystis\" causes pneumonia, again mainly in people with weakened immune systems. \"Candida\" yeasts are the agents of Candidiasis, causing infections of the mouth, throat, and genital tract, and more seriously of the blood. Ringworm is a skin infection that infects some 20% of the human population; it is caused by some 40 different fungi.\n"}
{"id": "24150477", "url": "https://en.wikipedia.org/wiki?curid=24150477", "title": "Investment theory of party competition", "text": "Investment theory of party competition\n\nThe Investment theory of party competition is a political theory developed by Thomas Ferguson, Emeritus Professor of Political Science at the University of Massachusetts Boston. The theory focuses on how business elites, not voters, play the leading part in political systems. The theory offers an alternative to the conventional, voter-focused, Voter Realignment theory and Median voter theorem, which has been criticized by Ferguson and others.\n\nThe \"Investment Theory of Party Competition\" was first outlined by Thomas Ferguson in his 1983 paper \"Party Realignment and American Industrial Structure: The Investment Theory of Political Parties in Historical Perspective\". The theory is detailed most extensively in Ferguson's 1995 book \"Golden Rule: The Investment Theory of Party Competition and the Logic of Money-driven Political Systems\", in which his earlier paper is republished as a chapter.\n\nFerguson frames his theory as being both inspired by and an alternative to the traditional median voter theories of democracy such as that posited by Anthony Downs in his 1957 work \"An Economic Theory of Democracy\". Quoting Downs, Ferguson accepts that 'the expense of political awareness is so great that no citizen can afford to bear it in every policy area, even if by doing so he could discover places where his intervention would reap large profits'. While Downs largely overlooked the implications of this insight, Ferguson makes it the foundation of the \"Investment Theory of Party Competition\", recognizing that if voters cannot bear the cost of becoming informed about public affairs they have little hope of successfully supervising government.\n\nThe central claim of the \"Investment Theory\" is that since ordinary citizens cannot afford to acquire the information required to invest in political parties, the political system will be dominated by those who can. As a result, the investment theory holds that rather than being seen as simple vote maximizers, political parties are best analyzed as blocs of investors who coalesce to advance candidates representing their interests.\n\nContrary to the median voter theorem where political parties have traditionally been seen as vote maximizers who will seek out the position of the 'median voter' on any particular issue, the \"Investment Theory\" holds the real area of competition for political parties is major investors who have an interest in investing to control the state.\n\nThis is because, in situations where money is important, political parties must take positions that enable them to attract the investment required to run successful campaigns. This is the case even if those positions are not supported by the majority of the population, since it is futile for a party to adopt even a popular position if it cannot afford the expense of communicating that position to the electorate in an election campaign. In fact the \"Investment theory\" predicts that in many cases political parties are more likely to try and change the position of the public to match those of its investors than vice versa.\n\nInstead political parties will try to assemble the votes they need through appeals to the electorate on issues that do not conflict with the interests of their investors. Vigorous debate may take place on issues where an opposing bloc of investors is able to mobilize and advertise their position. A further consequence of this theory is that in policy areas where large investors agree on policy, no party competition will take place. This is the case regardless of the views of the general population, unless ordinary citizens are able to become major investors in their own right through expenditure of time and income.\n\nThe \"Investment Theory of Party Competition\" does not deny the possibility that masses of voters can become major investors in an electoral system, and accepts that in cases where this does happen the effect may resemble classical voter competition models. For this to happen, however, generally requires channels that facilitate mass deliberation and expression, typically 'secondary' organizations capable of spreading the cost of acquiring information and concentrating contributions from many individuals to act politically. Such conditions may enable high information flows to the general population and make political debate and action a part of everyday life. Where these conditions do not exist, however, it is unlikely that ordinary citizens will be able to afford the costs required to control policy.\n\nA consequence of the \"Investment Theory\" is that it is not necessary to assume that the voting population is stupid or malevolent to explain why it will often vote for parties whose policies are opposed to their own interests. In fact, Ferguson suggests, the general population is far from ignorant or uninterested in the outcome of elections, and will often make considerable effort to understand the issues under discussion. Voting decisions ultimately, however, must be made on the basis of the information that is available, and if acquiring information is expensive in terms of time or money then most likely those decisions will be made on the basis of information subsidized by wealthy investors.\n\nAccording to Ferguson, who credits the insight to Downs, one of the reasons that wealthy investors are able to influence politics to their advantage is that much of the politically relevant information that is so expensive for ordinary citizens to acquire comes quite naturally to businesses in the course of their daily operations. An example might include international banks whose business contacts constitute 'a first-rate foreign policy network'.\n\nSimilarly, economies of scale give businesses an advantage over ordinary voters. For example, large investors will routinely consult with lawyers, public relations advisors, lobbyists, and political consultants before acting. The cost of this advice is prohibitively expensive for most citizens.\n\nSince investors cannot guarantee the outcome of an election or know exactly what policies a candidate will implement once in power, they must estimate the chances their investment will be successful. In some cases, this may lead to investors supporting more than one candidate, perhaps in more than one party. In other cases, it is expected that an investor will judge that one party will never accept its desired policies and so will become the 'core' of one party. Ferguson cites the support of labor-intensive industries such as textiles and steel supporting the Republican party after the New Deal as an example, owing to their labor policy.\n\nAlthough the \"Investment Theory\" recognizes the importance of financial contributions to political parties, Ferguson notes that direct cash contributions 'are probably not the most important way in which truly top business figures (\"major investors\") act politically'. Investors are also likely to act as sources of contacts, fundraisers, and as sources of legitimation for candidates, particularly through endorsements in the media. Similarly, the theory does not predict that elections will be won by the party that is able to spend the most money. Instead, it suggests that while parties will likely need to attract significant resources in order to be able to mount a successful campaign, they do not necessarily need to attract the \"most\" money.\n\nThe \"Investment Theory\" makes a number of novel predictions compared to other theories of party systems.\n\nFerguson uses the Investment Theory as the basis of an analysis of the New Deal in his 1984 paper \"From Normalcy to New Deal: Industrial structure, Party Competition, and American Public Policy in the Great Depression\", in which he argues that the New Deal policies became possible due to the changing nature of the American economy and the new coalitions of political investors that emerged as a result.\n\nFerguson argues that in the early years of the twentieth century American politics was dominated by a coalition of labor-intensive industries including steel, coal, and textiles, who opposed labor, and protectionist industries who supported the Republican Party. These industries were also joined by finance, who largely shared the support for trade tariffs and aggressive foreign policy. This coalition first began to split after the first world war as successful capital-intensive firms such as Standard Oil and General Electric began to emerge for whom labor issues were less pressing and who favored lower tariffs to stimulate world trade and open new markets. International banks also moved away from protectionist policies, as post-war recovery required European nations to export to America and required US banks to do so.\n\nThese firms went on to form the coalition that backed Franklin D. Roosevelt's New Deal policies, as their dominant position in the world economy made them the primary beneficiaries of the New Deal's free trade policies. While these new multinational corporations were better able to tolerate the pro-labour policies of the New Deal, they did not necessarily support it. Instead, Ferguson credits the rise of independent industrial unionism as the result of masses of American voters for the first time in US history successfully pooling their resources to become major investors in their own right.\n\nAlthough the 'Investment Theory' is largely concerned with financial donations to political parties, Ferguson argues that the theory implies reform must look beyond campaign finance alone. While acknowledging a need for reform of campaign finance, 'if only to prevent more and more of society's resources going down a black hole', Ferguson suggests, that no matter how diligent the regulators are, wealthy investors will doubtless find new ways to corrupt the political system.\n\nInstead, since the problem of money influencing politics stems from the cost of information, Ferguson argues the solution might come from finding ways for regular citizens to share these costs. Since it is inefficient for individuals or even groups of individuals to cope with this problem on an individual basis, Ferguson proposes that the cost must be subsidized by the state.\n\nWhile the United States (and other nations) already subsidize some of these costs, for example in providing public finance to political parties, franking mail or providing staff to politicians, this rarely takes place on a scale that actually does the public any good. Instead, this funding merely subsidizes parties that the rich control, with the effect that public money merely leverages the contributions of major investors. The solution, then, is to apply the 'Golden Rule' to ordinary citizens by providing enough public support to allow ordinary members of the public to run for office and have a reasonable chance of winning. This would not only allow ordinary citizens and (seemingly) heterodox opinions to be heard but would also have the effect of limiting the harm that private financing can have.\n\n\n"}
{"id": "54039909", "url": "https://en.wikipedia.org/wiki?curid=54039909", "title": "James K. Beattie", "text": "James K. Beattie\n\nJames K. Beattie is an Australian chemist from University of Sydney and an Elected Fellow of the American Association for the Advancement of Science and Royal Society of Chemistry.\n"}
{"id": "39776736", "url": "https://en.wikipedia.org/wiki?curid=39776736", "title": "Lewis and Clark Trail-Travois Road", "text": "Lewis and Clark Trail-Travois Road\n\nThe Lewis and Clark Trail–Travois Road is a historic site located east of Pomeroy, Washington, on U.S. Route 12 (US 12). It is a surviving stretch of Indian travois trail followed by Lewis and Clark in their 1805–06 expedition and mentioned in their writings. It was listed on the National Register of Historic Places in 1974.\n\nThe site is significant for the 1805–06 event, for including \"a trail that was very important in aboriginal times\", and for its information potential. It seems to have been described by Meriwether Lewis, who wrote on May 3, 1806: \"...we Continued Still up the Creek bottoms ... to the place at which the roade leaves the [Pataha] Creek and assends the hill up to the high plains: here we Encamped in a Small grove of Cotton trees...\".\n\nTravois were used by American Indians to transport possessions by means of two long poles slung with a hammock trailing behind a horse or dog. The deep, parallel tracks caused by the dragging poles are still visible today in a quarter-mile (0.4 km) section of the original trail, sometimes called the Nez Perce Trail, followed by the Lewis and Clark Expedition and preserved at this site.\n\n\n"}
{"id": "33468860", "url": "https://en.wikipedia.org/wiki?curid=33468860", "title": "Lionel Greenstreet", "text": "Lionel Greenstreet\n\nLionel Greenstreet (1889–1979) was the first officer of the \"Endurance\" and a member of the Imperial Trans-Antarctic Expedition of 1914–1917, for which he was awarded the Polar Medal. When he died on 13 January 1979, he was the last survivor of the Weddell Sea party within the expedition.\n\nGreenstreet was born into a family of officers in the merchant navy of the British Empire; his father Herbert Edward Greenstreet had been granted captain's papers by the New Zealand Shipping Company. At age 15, Greenstreet became a sea cadet, never returning to school. He gained his Master's certificate in 1911. As a young ship's officer, he wrote to the Captain of the \"Endurance\", Frank Worsley in 1914, asking to be considered for a berth. His request arrived just as the ship's named First Officer had thrown up his papers to accept service in World War I, which had just broken out. Greenstreet was told to report to the \"Endurance\" in Plymouth Sound for an interview; and upon arrival, after brief inspection by Worsley he was abruptly told that the position of First Officer was his and that he had twenty-four hours to prepare for the departure of the vessel to the Southern Ocean. The fledgling ship's officer recalled that after considerable effort he had settled his affairs and reported aboard the ship, which then sailed 30 minutes after his arrival.\n\nThe expedition's overall commander was the explorer Ernest Shackleton, and the goal of the \"Endurance\" was Vahsel Bay on the coast of Antarctica, from which Shackleton and the shore party hoped to cross the icy continent by dogsled; but on 18 January 1915, a few miles short of this destination, the ship was beset by ice and frozen into heavy pack from which she would not emerge. Greenstreet's duties changed with the new status of his ship; he kept deck watches in an attempt to find a lead of open water through which the ship could extricate itself, and joined a crew assigned to work below-decks in a futile attempt to stop the leaks that the ice was beginning to punch through the ship's hull. Despite the work of Greenstreet and his seamen and fellow ship's officers, Shackleton was forced to issue the order to abandon ship on 27 October. The expedition's 28 members and ship's company had to camp together as castaways on the frozen surface of the Weddell Sea.\n\nAfter the \"Endurance\" was abandoned, Greenstreet's duties again changed. He was given brief command of a team of sled dogs, and helped to hunt for fresh meat to supplement the castaways' inadequate supply of food. Shackleton later recalled with gratitude how Greenstreet and his hunting partner, Alexander Macklin, had killed and brought in a Weddell seal weighing 800 pounds.\n\nAt a slightly later stage of their self-rescue, after the icy campsite of the men of the \"Endurance\" had drifted northward into warmer water, the change in water temperature caused their refuge to melt and on 9 April 1916 they were forced to climb aboard the open boats that they had salvaged from their former vessel. Shackleton and his men had salvaged three lifeboats, and Greenstreet was the fourth-ranking member of the expedition. He accompanied his captain, Frank Worsley, in the \"Dudley Docker\" during the eight-day ordeal that marked the progress of the open-boat flotilla to a new and more secure campsite on Elephant Island in the archipelago of the South Shetlands.\n\nGreenstreet, although characterised by Worsley as \"a fine seaman\", was not chosen to accompany the six-man party that set out from Elephant Island to South Georgia Island. Instead the former first officer was detailed to a two-man party, working with William Bakewell, to alter bits and scraps of salvaged ship's canvas into a jury-rigged canvas deck to enable the sole remaining sail-worthy lifeboat of the ship's company, the \"James Caird\", to navigate in the open sea. Worsley, whose life would depend upon the success of this work, describes it as follows:\n\nFrozen like a board and caked with ice, the canvas was sewn, in painful circumstances, by two cheery optimists – Greenstreet, Chief Officer of the \"Endurance\", and Bakewell, a Canadian [sic] AB. The only way they could do it was by holding the frozen canvas in the blubber fire till it thawed, often burning their fingers, while the oily smoke got in their eyes and noses, half-blinding and choking them. Then they sewed, often getting frostbitten and having to use great care that the difficult sewing with cold, brittle sail needles did not break all of our now scanty supply. All the time, while repeating the unpleasant task of thawing a length, and sewing it, 'Horace' [Greenstreet] was irrepressibly cracking his sailor jokes and Bakewell replying.\nThe sails hoisted, Shackleton, Worsley and the \"James Caird\" set out into the Southern Ocean. Greenstreet and his 21 fellow castaways remained encamped on Elephant Island, and were rescued on 30 August 1916.\n\nAfter the end of the expedition and with World War I continuing, Greenstreet was commissioned as a second lieutenant in the Inland Water Transport arm of the Royal Engineers in 1917, serving in supply barges on the Tigris in British-occupied Mesopotamia. He married Millie Baddeley Muir in 1917. After the war, Greenstreet chose shore life, working in marine insurance.\n\nHe saw active service in World War II as an officer in the Royal Naval Reserve. Greenstreet left command of Admiralty tug \"Freebooter\" in January 1942 to become rescue tug equipment officer in charge of the US shipbuilding programme of rescue tugs for the Admiralty under lend-lease.\n\nAfter the war, he resumed insurance work prior to retirement. His wife Millie died in 1955 and he married Audrey Day.\n\nWorsley, who had chosen Greenstreet on very short notice to join the fateful expedition, repeatedly paid tribute to him in his memoirs. During the first open-boat journey, \"Greenstreet was splendid, never losing hope and always ready to crack some appalling sailor-joke.\" On Elephant Island Worsley's last sight of Greenstreet prior to the Southern Ocean trip was his former First Officer, \"cheerfully profane as ever\", helping to bag stones to be loaded onto the \"James Caird\" as ballast.\n\n"}
{"id": "38279", "url": "https://en.wikipedia.org/wiki?curid=38279", "title": "Lions' Commentary on UNIX 6th Edition, with Source Code", "text": "Lions' Commentary on UNIX 6th Edition, with Source Code\n\nLions' Commentary on UNIX 6th Edition, with Source Code by John Lions (1976) contains source code of the 6th Edition Unix kernel plus a commentary. It is commonly referred to as the Lions book. Despite its age, it is still considered an excellent commentary on simple but high quality code.\n\nFor many years, the Lions book was the \"only\" Unix kernel documentation available outside Bell Labs. Although the license of 6th Edition allowed classroom use of the source code, the license of 7th Edition specifically excluded such use, so the book spread through illegal copy machine reproductions (a kind of samizdat). It was commonly held to be the most copied book in computer science.\n\nThe book was reprinted in 1996 by Peer-To-Peer Communications.\n\n\"Unix Operating System Source Code Level Six\" is the kernel source code, lightly edited by Lions to better separate the functionality — system initialization and process management, interrupts and system calls, basic I/O, file systems and pipes and character devices. All procedures and symbols are listed alphabetically with a cross reference.\n\nThe code as presented will run on a PDP-11/40 with RK-05 disk drive, LP-11 line printer interface, PCL-11 paper tape writer and KL-11 terminal interface, or a suitable PDP-11 emulator, such as SIMH.\n\n\"A Commentary on the Unix Operating System\" starts with notes on Unix and other useful documentation (the Unix manual pages, DEC hardware manuals and so on), a section on the architecture of the PDP-11 and a chapter on how to read C programs. The source commentary follows, divided into the same sections as the code. The book ends with suggested exercises for the student.\n\nAs Lions explains, this commentary supplements the comments in the source. It is possible to understand the code without the extra commentary, and the reader is advised to do so and only read the notes as needed. The commentary also remarks on how the code might be improved.\n\nThe source code and commentary were originally produced in May 1976 as a set of lecture notes for Lions's computer science courses (6.602B and 6.657G) at the University of New South Wales Department of Computer Science.\n\n\"UNIX News\" March 1977 announced the availability of the book to Unix licensees. Lions had trouble keeping up with its popularity, and by 1978 it was available only from Bell Labs.\n\nWhen AT&T announced Unix Version 7 at USENIX in June 1979, the academic/research license no longer automatically permitted classroom use. Thus, licensees were no longer able to use the Lions notes for classes on operating systems.\n\nHowever, thousands of computer science students around the world spread photocopies. As they could not study it legally in class, they would sometimes meet after hours to discuss the book. Many pioneers of Unix and open source had a treasured multiple-generation photocopy.\n\nOther follow-on effects of the license change included Andrew S. Tanenbaum creating Minix. As Tanenbaum wrote in \"Operating Systems\" (1987):\n\nVarious Unix people, particularly Peter H. Salus, Dennis Ritchie and Berny Goodheart, lobbied Unix's various owners (AT&T, Novell, the Santa Cruz Operation) for many years to allow the book to be published officially. In 1996, the Santa Cruz Operation finally authorised the release of the twenty-year-old 6th Edition source code (along with the source code of other versions of \"Ancient UNIX\"), and the full code plus the 1977 version of the commentary was published by Peer-To-Peer Communications (). The reissue includes commentary from Michael Tilson (SCO), Peter Salus, Dennis Ritchie, Ken Thompson, Peter Collinson, Greg Rose, Mike O'Dell, Berny Goodheart and Peter Reintjes.\n\nThe infamous program comment \"You are not expected to understand this\" occurs on line 2238 of the source code (\"Lions' Commentary\", p. 22) at the end of a comment explaining the process exchange mechanism. It refers to line 325 of the file slp.c. The source code reads:\n\nA major reason why this piece of code was hard to understand was that it depended on a quirk of the way the C-compiler for the PDP-11 saved registers in procedure calls. This code failed when ported to other machines and had to be redesigned in Version 7 Unix. Dennis Ritchie later explained the meaning of this remark:\n\nxv6, a modern reimplementation of Sixth Edition Unix in ANSI C for multiprocessor x86 systems.\n\n\n"}
{"id": "9794219", "url": "https://en.wikipedia.org/wiki?curid=9794219", "title": "List of Intel codenames", "text": "List of Intel codenames\n\nIntel has historically named integrated circuit (IC) development projects after geographical names of towns, rivers or mountains near the location of the Intel facility responsible for the IC. Many of these are in the American West, particularly in Oregon (where most of Intel's CPU projects are designed; see famous codenames). As Intel's development activities have expanded, this nomenclature has expanded to Israel and India, and some older codenames refer to celestial bodies.\n\nThe following table lists known Intel codenames along with a brief explanation of their meaning and their likely namesake, and the year of their earliest known public appearance. Most processors after a certain date were named after cities that could be found on the map of the United States. This was done for trademark considerations. Banias was the last of the non-US city names. Gesher was renamed to Sandy Bridge to comply with the new rule. Dothan was a city both in Israel and in Alabama.\n\nSee also List of computer technology code names\n\nList of Intel microprocessors\n\n"}
{"id": "54715230", "url": "https://en.wikipedia.org/wiki?curid=54715230", "title": "List of Iranian mathematicians", "text": "List of Iranian mathematicians\n\nThe following is a list of Iranian mathematicians including ethnic Iranian mathematicians.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "35318832", "url": "https://en.wikipedia.org/wiki?curid=35318832", "title": "List of isomers of dodecane", "text": "List of isomers of dodecane\n\nThis is the list of 355 isomers of dodecane.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "10891653", "url": "https://en.wikipedia.org/wiki?curid=10891653", "title": "List of lakes in Arkansas", "text": "List of lakes in Arkansas\n\nThere are at least 2340 named lakes and reservoirs in Arkansas. The following list contains lists of lakes and reservoirs in Arkansas by county.\n\nA lake is a terrain feature (or physical feature), a body of liquid on the surface of a world that is localized to the bottom of basin (another type of landform or terrain feature; that is not global). Another definition is a body of fresh or salt water of considerable size that is surrounded by land. On Earth a body of water is considered a lake when it is inland, not part of the ocean, is larger and deeper than a pond. \n\nA reservoir (etymology from French \"réservoir\" a \"storehouse ) is an artificial lake used to store water. Reservoirs may be created in river valleys by the construction of a dam or may be built by excavation in the ground or by conventional construction techniques such a brickwork or cast concrete.\n\n"}
{"id": "14652668", "url": "https://en.wikipedia.org/wiki?curid=14652668", "title": "List of lakes in California", "text": "List of lakes in California\n\nThere are more than 3,000 named lakes, reservoirs, and dry lakes in the U.S. state of California.\n\nIn terms of area covered, the largest lake in California is the Salton Sea, a lake formed in 1905 which is now saline. It occupies in the southeast corner of the state, but because it is shallow it only holds about of water. Tulare Lake in the San Joaquin Valley was larger, at approximately 690 square miles (1,780 km), until it was drained during the later years of the nineteenth century.\n\nIn terms of volume, the largest lake on the list is Lake Tahoe, located on the California–Nevada border. It holds roughly of water. It is also the largest freshwater lake by area, at , and the deepest lake, with a maximum depth of .\n\nAmong freshwater lakes entirely contained within the state, the largest by area is Clear Lake, which covers .\n\nMany of California's large lakes are actually reservoirs: artificial bodies of fresh water. In terms of both area and volume, the largest of these is Lake Shasta, which formed behind Shasta Dam in the 1940s. The dam can impound of water over .\n\nLake Elsinore, which covers , is billed as the largest natural freshwater lake in Southern California.\n\nThe list is alphabetized by the name of the lake, with the words \"lake\", \"of\", and \"the\" ignored. To sort on a different column, click on the arrows in butt\n\nGeographic coordinates, approximate elevations, alternative names, and other details may be obtained by following the Geographic Names Information System links in the third column.\nNote: Lakes grow and shrink due to precipitation, evaporation, releases, and diversions.\nFor this reason, many of the surface areas tabulated below are very approximate. \nFor reservoirs, the areas at maximum water storage are indicated. Reservoirs used for flood control are seldom allowed to reach maximum storage.\n\n\n"}
{"id": "4340860", "url": "https://en.wikipedia.org/wiki?curid=4340860", "title": "List of schools of psychoanalysis", "text": "List of schools of psychoanalysis\n\nThis is a list of schools of psychoanalysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "21578825", "url": "https://en.wikipedia.org/wiki?curid=21578825", "title": "List of solar telescopes", "text": "List of solar telescopes\n\nThis is a list of solar telescopes built in various countries around the world. A solar telescope is a specialized telescope that is used to observe the Sun.\n\nThis list contains ground-based professional observatory telescopes at optical wavelengths in chronological order. Solar telescopes often have multiple focal lengths, and use a various combination of mirrors such as coelostats, lenses, and tubes for instruments including spectrographs, cameras, or coronagraphs. There are many types of instruments that have been designed to observe Earth's Sun, for example, in the 20th century solar towers were common.\n\nTelescopes for the sun have existed for hundreds of years, this list is not complete and only goes back to 1900.\n\nThere are much smaller commercial and/or amateur telescopes such as \"Coronado Filters\" from founder and designer David Lunt, bought by Meade Instruments in 2004 and sells SolarMax solar telescopes up to 8 cm\n\nMost solar observatories observe optically at visible, UV, and near infrared wavelengths, but other things can be observed.\n\n\n"}
{"id": "612519", "url": "https://en.wikipedia.org/wiki?curid=612519", "title": "Lunar Prospector", "text": "Lunar Prospector\n\nLunar Prospector was the third mission selected by NASA for full development and construction as part of the Discovery Program. At a cost of $62.8 million, the 19-month mission was designed for a low polar orbit investigation of the Moon, including mapping of surface composition including polar ice deposits, measurements of magnetic and gravity fields, and study of lunar outgassing events. The mission ended July 31, 1999, when the orbiter was deliberately crashed into a crater near the lunar south pole after the presence of water ice was successfully detected.\n\nData from the mission allowed the construction of a detailed map of the surface composition of the Moon, and helped to improve understanding of the origin, evolution, current state, and resources of the Moon. Several articles on the scientific results were published in the journal \"Science\".\n\n\"Lunar Prospector\" was managed by NASA Ames Research Center with the prime contractor Lockheed Martin. The Principal Investigator for the mission was Alan Binder. His personal account of the mission, \"Lunar Prospector: Against all Odds\", is highly critical of the bureaucracy of NASA overall, and of its contractors.\n\nThe spacecraft was a graphite-epoxy drum, in diameter and high with three radial instrument booms. A extension boom at the end of one of the 2.5 m booms held the magnetometer. Total initial mass (fully fueled) was . It was spin-stabilized (nominal spin rate 12 rpm) with its spin axis normal to the ecliptic plane. The spacecraft was controlled by six hydrazine monopropellant 22-newton thrusters (two aft, two forward, and two tangential). Three fuel tanks mounted inside the drum held of hydrazine pressurized by helium. The power system consisted of body-mounted solar cells which produced an average of 186 W and a 4.8 A·h rechargeable NiCd battery. \n\nCommunications were through two S band transponders, a slotted, phased-array medium-gain antenna for downlink, and an omnidirectional low-gain antenna for downlink and uplink. The on-board computer was a Harris 80C86 with 64 kilobytes of EEPROM and 64 kilobytes of static RAM. All control was from the ground, the computer echoing each command to the ground for verification there. Once the command was ground-verified, an \"execute\" command from the ground told the computer to proceed with execution of the command. The computer built telemetry data as a combination of immediate data and also read from a circular queue buffer which allowed the computer to repeat data it had read 53 minutes earlier. This simple solid-state recorder ensured that all data collected during communications blackout periods would be received, providing the blackout was not longer than 53 minutes.\n\nThe probe also carried a small amount of the remains of Eugene Shoemaker (April 28, 1928 – July 18, 1997), astronomer and co-discoverer of Comet Shoemaker-Levy 9, to the Moon for a space burial.\n\nFollowing launch on January 7, 1998 UT (6 January EST) aboard a four-stage Athena II rocket, \"Lunar Prospector\" had a 105-hour cruise to the Moon. During the cruise, the three instrument booms were deployed. The MAG and APS collected calibration data, while the GRS, NS, and ER outgassed for one day, after which they also collected calibration data in cislunar space. The craft was inserted into an 11.6-hour period capture orbit about the Moon at the end of the cruise phase. After 24 hours \"Lunar Prospector\" was inserted into a 3.5-hour period intermediate orbit, followed 24 hours later (on January 13, 1998) by transfer into a preliminary mapping orbit, and then on 16 January by insertion into the near-circular altitude nominal lunar polar mapping orbit with an inclination of 90 degrees and a period of 118 minutes. Lunar calibration data was collected during the 11.6- and 3.5-hour orbits. Lunar mapping data collection started shortly after the 118 minute orbit was achieved. The data collection was periodically interrupted during the mission as planned for orbital maintenance burns, which took place to recircularize the orbit whenever the periselene or aposelene was more than to from the 100 km nominal orbit; this occurred about once per month. On December 19, 1998, a maneuver lowered the orbit to to perform higher resolution studies. The orbit was altered again on 28 January to a orbit, ending the one year primary mission and beginning the extended mission.\n\nThe mission ended on July 31, 1999 at 9:52:02 UT (5:52:02 EDT) when \"Lunar Prospector\" was steered into a deliberate collision in a permanently shadowed area of the Shoemaker crater near the lunar south pole. It was hoped that the impact would liberate water vapor from the suspected ice deposits in the crater and that the plume would be detectable from Earth; however, no such plume was observed.\n\nThe \"Lunar Prospector\" mission was the third mission selected by NASA for full development and launch as part of NASA's Discovery Program. Total cost for the mission was $63 million including development ($34 million), launch vehicle (~$25 million) and operations (~$4 million).\n\nThe spacecraft carried six instruments: a Gamma Ray Spectrometer, a Neutron Spectrometer, a Magnetometer, an Electron Reflectometer, an Alpha Particle Spectrometer, and a Doppler Gravity Experiment. The instruments were omnidirectional and required no sequencing. The normal observation sequence was to record and downlink data continuously.\n\nThe \"Lunar Prospector\" GRS produced the first global measurements of gamma-ray spectra from the lunar surface, from which are derived the first \"direct\" measurements of the chemical composition for the entire lunar surface. This data effectively mapped the distribution of various important elements across the Moon. For example, the \"Lunar Prospector\" GRS identified several regions with high iron concentrations.\n\nThe fundamental purpose of the GRS experiment was to provide global maps of elemental abundances on the lunar surface. The GRS was designed to record the spectrum of gamma rays emitted by:\nThe most important elements detectable by the GRS were uranium (U), thorium (Th), and potassium (K), radioactive elements which generate gamma rays spontaneously, and iron (Fe), titanium (Ti), oxygen (O), silicon (Si), aluminum (Al), magnesium (Mg), and calcium (Ca), elements which emit gamma rays when hit by cosmic rays or solar wind particles. The uranium, thorium, and potassium in particular were used to map the location of KREEP (potassium, rare-earth element, and phosphorus containing material, which is thought to have developed late in the formation of the crust and upper mantle, and is therefore important to understanding lunar evolution). The GRS was also capable of detecting fast (epithermal) neutrons, which complemented the neutron spectrometer in the search for water on the Moon.\n\nThe Gamma Ray Spectrometer was a small cylinder which was mounted on the end of one of the three radial booms extending from \"Lunar Prospector\". It consisted of a bismuth germanate crystal surrounded by a shield of borated plastic. Gamma rays striking the bismuth atoms produced a flash of light with an intensity proportional to the energy of the gamma ray which was recorded by detectors. The energy of the gamma ray is associated with the element responsible for its emission. Due to a low signal-to-noise ratio, multiple passes were required to generate statistically significant results. At nine passes per month, it was expected to take about three months to confidently estimate abundances of thorium, potassium, and uranium, and 12 months for the other elements. The precision varies according to element measured. For U, Th, and K, the precision is 7% to 15%, for Fe 45%, for Ti 20%, and for the overall distribution of KREEP 15% to 30%. The borated plastic shield was used in the detection of fast neutrons. The GRS was designed to achieve global coverage from an altitude of approximately and with a surface resolution of .\n\nBased on the \"Lunar Prospector\" Neutron Spectrometer (NS) data, mission scientists have determined that there is enough evidence for lunar water ice in the polar craters of the Moon, an estimated 3 billion metric tons (800 billion US gallons).\n\nThe NS was designed to detect minute amounts of water ice which were believed to exist on the Moon. It was capable of detecting water ice at a level of less than 0.01%. the Moon has a number of permanently shadowed craters near the poles with continuous temperatures of . These craters may act as cold-traps of water from incoming comets and meteoroids. Any water from these bodies which found its way into these craters could become permanently frozen. The NS was also used to measure the abundance of hydrogen implanted by solar wind.\n\nThe neutron spectrometer was a thin cylinder colocated with the Alpha Particle Spectrometer at the end of one of the three radial \"Lunar Prospector\" science booms. The instrument had a surface resolution of . For the polar ice studies, the NS was slated to examine the poles to 80 degrees latitude, with a sensitivity of at least 10 ppm by volume of hydrogen. For the implanted hydrogen studies, the NS was intended to examine the entire globe with a sensitivity of 50 ppmv. The neutron spectrometer consisted of two canisters each containing helium-3 and an energy counter. Any neutrons colliding with the helium atoms give an energy signature which can be detected and counted. One of the canisters was wrapped in cadmium, and one in tin. The cadmium screens out thermal (low energy or slow-moving) neutrons, while the tin does not. Thermal neutrons are cosmic-ray-generated neutrons which have lost much of their energy in collisions with hydrogen atoms. Differences in the counts between the two canisters indicate the number of thermal neutrons detected, which in turn indicates the amount of hydrogen in the Moon's crust at a given location. Large quantities of hydrogen would likely be due to the presence of water.\n\nThe Alpha Particle Spectrometer (APS) was damaged during launch, ruining one of the five detecting faces. Additionally, due to sunspot activity peaking during the mission, the lunar data was obscured by solar interference. The information was eventually recovered by subtracting out the effects of the solar activity.\n\nThe APS was designed to detect radon outgassing events on the surface of the Moon. The APS recorded alpha particle signatures of radioactive decay of radon gas and its byproduct product, polonium. These putative outgassing events, in which radon, nitrogen, and carbon dioxide are vented, are hypothesized to be the source of the tenuous lunar atmosphere, and may be the result of the low-level volcanic/tectonic activity on the Moon. Information on the existence, timing, and sources of these events may help in a determination of the style and rate of lunar tectonics.\n\nThe APS was a cube approximately colocated with the neutron spectrometer on the end of one of the three radial \"Lunar Prospector\" science booms. It contained ten silicon detectors sandwiched between gold and aluminum disks arranged on five of six sides of the cube. Alpha particles, produced by the decay of radon and polonium, leave tracks of charge on the silicon wafers when they impact the silicon. A high voltage is applied to the silicon, and the current is amplified by being funneled along the tracks to the aluminum disk and is recorded for identification. The APS was designed to make a global examination of gas release events and polonium distribution with a surface resolution of and a precision of 10%.\n\nThe Doppler Gravity Experiment (DGE) was the first polar, low-altitude mapping of the lunar gravity field. The \"Clementine\" mission had previously produced a relatively low-resolution map, but the \"Prospector\" DGE obtained data approximately five times as detailed: the \"first truly operational gravity map of the Moon\". The practical benefits of this are more stable long-term orbits and better fuel efficiency. Additionally, the DGE data is hoped to help researchers learn more about lunar origins and the nature of the lunar core. The DGE identified three new near-side mass concentration regions.\n\nThe purpose of the \"Lunar Prospector\" DGE was to learn about the surface and internal mass distribution of the Moon. This is accomplished by measuring the Doppler shift in the S-band tracking signal as it reaches Earth, which can be converted to spacecraft accelerations. The accelerations can be processed to provide estimates of the lunar gravity field, from which the location and size of mass anomalies affecting the spacecraft orbit can be modeled. Estimates of the surface and internal mass distribution give information on the crust, lithosphere, and internal structure of the Moon.\n\nThis experiment provided the first lunar gravity data from a low polar orbit. Because line-of-sight tracking was required for this experiment, only the near-side gravity field could be estimated using this Doppler method. The experiment was a byproduct of the spacecraft S band tracking, and so has no listed weight or power requirements. The experiment was designed to give the near-side gravity field with a surface resolution of and precision of 5 mGal (0.05 mm/s²) in the form of spherical harmonic coefficients to degree and order 60. In the extended mission, in which the spacecraft descended to an orbit with an altitude of and then to , this resolution was expected to improve by a factor of 100 or more.\n\nThe downlink telemetry signal was transmitted at 2273 MHz, over a ±1 MHz bandwidth as a right-hand circularly polarized signal at a nominal power of 5 W and peak power of 7 W. Command uplinks were sent at 2093.0542 MHz over a ±1 MHz bandwidth. The transponder was a standard Loral/Conic S-Band transponder. An omnidirectional antenna can be used for uplink and downlink, or a medium gain helix antenna can be used (downlink only). Since the spacecraft was spin-stabilized, the spin resulted in a bias in the Doppler signal due to the spacecraft antenna pattern spinning with respect to the Earth station of 0.417 Hz (27.3 mm/s) for the omnidirectional antenna, and −0.0172 Hz (−1.12 mm/s) for the medium gain antenna. LOS data was sampled at 5 seconds to account for the approximately 5 second spin rate of the spacecraft, leaving a residual of less than 0.1 mm/s.\n\nThe Magnetometer and Electron Reflectometer (collectively, MAG/ER) detected anomalous surface magnetic fields on the Moon, which are in stark contrast to a global magnetosphere (which the Moon lacks). the Moon's overall magnetic field is too weak to deflect the solar wind, but MAG/ER discovered a small surface anomaly that can do so. This anomaly, about in diameter, has therefore been referred to as \"the smallest known magnetosphere, magnetosheath and bow shock system in the Solar System.\" Due to this and other magnetic features of the Moon's surface, hydrogen deposited by solar wind is non-uniformly distributed, being denser at the periphery of the magnetic features. Since hydrogen density is a desirable characteristic for hypothetical lunar bases, this information may be useful in choosing optimal sites for possible long-term Moon missions.\n\nThe electron reflectometer (ER) and magnetometer (MAG) were designed to collect information on the lunar magnetic fields. the Moon has no global magnetic field, but it does have weak localized magnetic fields at its surface. These may be paleomagnetic remnants of a former global magnetic field, or may be due to meteor impacts or other local phenomena. This experiment was to help map these fields and provide information on their origins, allow possible examination of distribution of minerals on the lunar surface, aid in a determination of the size and composition of the lunar core, and provide information on the lunar induced magnetic dipole.\n\nThe ER determined the location and strength of magnetic fields from the energy spectrum and direction of electrons. The instrument measured the pitch angles of solar wind electrons reflected from the Moon by lunar magnetic fields. Stronger local magnetic fields can reflect electrons with larger pitch angles. Field strengths as small as 0.01 nT could be measured with a spatial accuracy of about at the lunar surface. The MAG was a triaxial fluxgate magnetometer similar in design to the instrument used on Mars Global Surveyor. It could measure the magnetic field amplitude and direction at spacecraft altitude with a spatial resolution of about when ambient plasma disturbances are minimal.\n\nThe ER and the electronics package were located at the end of one of the three radial science booms on \"Lunar Prospector\". The MAG was in turn extended further on a boom—a combined from \"Lunar Prospector\" in order to isolate it from spacecraft generated magnetic fields. The ER and MAG instruments had a combined mass of and used 4.5 watts of power.\n\n\n"}
{"id": "49968788", "url": "https://en.wikipedia.org/wiki?curid=49968788", "title": "Masreliez's theorem", "text": "Masreliez's theorem\n\nMasreliez theorem describes a recursive algorithm within the technology of extended Kalman filter, named after the Swedish-American physicist John Masreliez, who is its author. The algorithm estimates the state of a dynamic system with the help of often incomplete measurements marred by distortion.\n\nMasreliez's theorem produces estimates that are quite good approximations to the exact conditional mean in non-Gaussian additive outlier (AO) situations. Some evidence for this can be had by Monte Carlo simulations.\n\nThe key approximation property used to construct these filters is that the state prediction density is approximately Gaussian. Masreliez discovered in 1975 that this approximation yields an intuitively appealing non-Gaussian filter recursions, with data dependent covariance (unlike the Gaussian case) this derivation also provides one of the nicest ways of establishing the standard Kalman filter recursions. Some theoretical justification for use of the Masreliez approximation is provided by the \"continuity of state prediction densities\" theorem in Martin (1979).\n\n"}
{"id": "26261786", "url": "https://en.wikipedia.org/wiki?curid=26261786", "title": "Microphotography (literature)", "text": "Microphotography (literature)\n\nMicrophotography is a writing style that emerged in the early 1990s in science journalism. The style is named after micrographs and is distinctive for its highly detailed, worm's eye or microscopic view of the macroscopic world.\n\nOne of the flagship works in this style was David Bodanis's commercially prominent \"The Secret House: 24 hours in the strange & wonderful world in which we spend our nights and days\". This book followed daily life in a typical enclosed human habitat in minute detail, featuring detailed physical and biological explanations.\n"}
{"id": "43330749", "url": "https://en.wikipedia.org/wiki?curid=43330749", "title": "Molly Woods", "text": "Molly Woods\n\nMolly Woods is a fictional character from the science fiction drama series \"Extant\", played by Halle Berry. She made her first screen appearance in the show's pilot episode \"Re-Entry\", which first aired on July 9, 2014 on the CBS network in the United States.\n\nMolly is an astronaut. She is married to John Woods and is the mother of Ethan Woods, a \"humanich\" android. While aboard a spaceflight to \"ISEA\" (the fictional space agency that replaced NASA) spacestation, she inexplicably becomes pregnant during the 13-month solo voyage. She returns to Earth amid chaotic happenings with her family life while in the space program and unexplained events of the early pregnancy. She goes through medical and psychological tests after returning to \"ISEA\".\n\nThe \"believed dead\" Harmon Kryger makes an attempt to contact Molly after she has an eventful day with her husband John and son Ethan. From hurried and missed breakfast with John, to the trip to the museum with Ethan which features Ethan with more questions than answers from Molly. Also, the revelation from Sam Barton that her brain scans are just like Krygers, (but he died) and she is 14 weeks pregnant. The night is just as mysterious as the Woods prepare for the next day's events.\n\nLater we see Ethan prepared for interaction with children at school, as Molly defends his place in a school setting to the worried parents. Molly reveals to John, she just signed up for another rotation on the \"Seraphim,\" while John was ready to add Ethan to the family. Molly objects saying, \"you just can't plug and play a family.\" Molly's boss interrupts her morning jog to let her in on the Covert program that allowed her to become pregnant by timed release Nano-technology. He mentions her brain scans that matched \"Kryger's\" and that's what led to his suicide.\n\nUnbeknownst to Molly \"Director Sparks\" is planning to covertly bring her in for further study with a medical team. Her doctor \"Dr. Barton,\" arrives that night to draw a blood sample to determine the DNA of Molly's unborn fetus. Later, Molly reveals to John she is pregnant and that she is afraid he wouldn't believe her.\n\nIn the opening, we see Molly in a darkened room reminiscing about earlier times with Marcus. She goes to the bookcase and removes an old photo album, and we see a picture of her with Marcus in his \"ISEA\" mission uniform with Molly and it's a moving scene. They begin to look at each other and tighten their embrace. She is interrupted when John appears after putting Ethan in rest mode for the night. Next we see her aboard the \"Seraphim\" initiating bio-sample tests on plants and worms, when \"Ben,\" her onboard computer (a disembodied voice) tells her of an incoming \"v-com,\" or video communication from John and Ethan. The \"v-com\" is interrupted by what \"Ben\" says is a solar flare. \"Ben\" then goes offline while Molly frantically tries to reboot.\n\nThat's when we first see \"Marcus\" appear to Molly's shock and surprise. She and Marcus interact briefly after she tries to burn her hand to attempt to see if the person she sees is real. Next we see Molly explaining away 13 lost hours of video from the space station. \"Gordon Kern,\" (the deputy director of \"ISEA\") mentions the backup cameras on board the \"Seraphim,\" and questions Molly about the lost footage which she explains away (not convincingly to Director Sparks) as a glitch.\n\nWe see Molly wake up in the lab and in a frantic scene we see her playback the security video footage to see if Marcus is there, and he's not, but she realizes she has to hide the footage from \"ISEA\" since \"Marcus\" died years earlier.\n\nThe next night John has the birthday party for Molly and they invite friends and close \"ISEA\" associates. Dr. Barton arrives to take the blood sample from Molly, after she leaves Molly has sharp pains in her stomach and then flashes back to the event on the \"Seraphim\" with Marcus saying \"it's ok.\" Then Molly returns to the party and the doorbell rings and it's \"Tim Dawkins,\" Marcus' brother. Molly hasn't seen \"Tim\" for a while. He tells her he's on furlough and decided to drop by. We learn she actually hasn't seen \"Tim\" in eight years since she lost the baby and Marcus. He curiously asks Molly if she has seen Marcus and tries to reassure Molly that it will be all right. Next we see Molly call Tim over when John wants a group party picture. She mingles in the party for a while. Then after revealing her secret to John they return and Molly asks John if he has seen Tim.\n\nMolly suddenly realizes she is the only one who has seen \"Tim\" and something is happening to her.\n"}
{"id": "18247348", "url": "https://en.wikipedia.org/wiki?curid=18247348", "title": "Morley Kare", "text": "Morley Kare\n\nDr. Morley Richard Kare (1922–1990) was a physiologist and biologist.\n\nMorley Richard Kare was born in 1922 in Winnipeg, Manitoba. He received his bachelor's degree in agriculture from the University of Manitoba in 1943, his master’s in nutrition from the University of British Columbia in 1948 and his Ph.D. in physiology from Cornell University in 1952.\n\nDr. Kare taught physiology at Cornell University, North Carolina State University and the University of Pennsylvania. Although his early research focused on muscle biochemistry and metabolism, he became increasingly interested in the senses of taste and smell and how these senses contribute to nutrition and food choice across species. Kare is best remembered for founding the Monell Chemical Senses Center, a multidisciplinary basic research institute devoted to the science of taste and smell, located in Philadelphia, Pennsylvania. He served as the Center’s first Director from 1968 until his death in 1990, at which point Dr. Gary Beauchamp took over.\n\nIn his memory, the Monell Center created the Morley R. Kare Fellows Fund in 1990. The Fund helps support scientists beginning careers in the chemical senses.\n\n"}
{"id": "11355257", "url": "https://en.wikipedia.org/wiki?curid=11355257", "title": "Moses Botarel Farissol", "text": "Moses Botarel Farissol\n\nMoses Botarel Farissol was a Jewish astronomer and mathematician of the second half of the 15th century.\n\nHe wrote a work on the calendar entitled \"Meleket ha-Ḳebi'ah,\" and compiled, under the title \"Nofet Ẓufim,\" calendric tables. Both these works, in manuscript, are preserved in the royal library at Munich.\n\n\n"}
{"id": "36172635", "url": "https://en.wikipedia.org/wiki?curid=36172635", "title": "Nikolai Cholodny", "text": "Nikolai Cholodny\n\nNikolai Grigoryevich Cholodny (; 22 June 1882 – 4 May 1953) was an influential microbiologist who worked at the University of Kiev, Ukraine in the USSR during the 1930s.\n\nHe is known for the Cholodny–Went model, which he developed independently with Frits Warmolt Went of the California Institute of Technology.\nDespite being associated with the same theory, the two men never actually met.\n\nCholodny worked in the A.V. Fomin Botanical Garden, attached to the University of Kiev.\nHe was one of the pioneers of the concept that microbes adhere to surfaces, \nusing the technique of first placing glass slides in earth for a measured time period,\nthen using a microscope to examine the slides.\nThe Prokaryote \"Leptothrix cholodnii\" is named after him. \nIn 1927 Cholodny proposed that the cells of the coleoptile are first polarized under the influence of uneven exposure to light, so growth hormone can diffuse more rapidly towards the side in the shade than in any other direction.\nWent reached the same conclusion in 1928, and the two scientists' names have been attached to the controversial Cholodny-Went theory.\n\nSelected works include:\n\nIn 1937 N. G. Cholodny and E. Ch. Sankewitsch published an article on \"Influence of weak electric currents upon the growth of the coleoptile\" in \"Plant Physiology\".\nThe same year he published an article on \"Charles Darwin and the modern theory of tropisms\" in \"Science\" magazine.\n"}
{"id": "34709979", "url": "https://en.wikipedia.org/wiki?curid=34709979", "title": "Orra White Hitchcock", "text": "Orra White Hitchcock\n\nOrra White Hitchcock (March 8, 1796 – May 26, 1863) was one of America's earliest women botanical and scientific illustrators and artists, best known for illustrating the scientific works of her husband, geologist Edward Hitchcock (1793–1864), but also notable for her own artistic and scientific work.\n\nHitchcock was born to a prosperous farming family (Jarib and Ruth Sherman White) in South Amherst, Massachusetts. She was educated by a tutor and at two “ladies” schools, proved herself a child prodigy in numerous scientific and classical subjects, and showed early promise in drawing and painting. From 1813 to 1818 she taught young girls natural sciences, and the fine and decorative arts at Deerfield Academy. Her early training grounded her in both science and art. Hitchcock has been called the Connecticut River Valley's “earliest and most often published woman artist.”\n\nHitchcock's art was integral to the work of her husband, geologist Edward Hitchcock, Principal of Deerfield Academy, minister, professor and third president of Amherst College (1845–1854). She made hundreds of illustrations for Edward Hitchcock's scientific publications, including detailed landscapes of the Connecticut River Valley for his Massachusetts geological survey volumes, and custom designed charts that illustrated his local discoveries and his classroom lectures. In addition, she made detailed drawings of native flowers and grasses and small precise watercolors of small local mushrooms. Her work is a time-focused chronicle of the scenic, botanically and geologically diverse Connecticut River Valley in western Massachusetts. Orra White Hitchocock, a scientist in her own right, had the contemporary reputation as one of the valley's “most distinguished naturalists.”\n\nBetween 1817 and 1821 the Hitchcock and her husband collected native plants for a conventional herbarium. At the same time, she created a 64-page album of watercolors of about 175 local flower and grass specimens for her Herbarium parvum, pictum. This painted herbarium is in the Deerfield Academy Archives.\n\nOrra White married Edward Hitchcock on May 31, 1821. In the summer and fall, she created a small watercolor album of native mushrooms and lichens, Fungi selecti picti. Edward Hitchcock labeled and catalogued the specimens. This painted album is in the Smith College Archives; a facsimile has been published by the Mortimer Rare Book Room, Smith College.\n\nOrra White Hitchcock made drawings for more than 200 plates and 1,000 wood-engraved or woodcut illustrations for Edward Hitchcock's professional publications. The subjects included landscapes, geologic strata, specimens, and more. The most well known appear in her husband's seminal works, the 1833 \"Report on the Geology, Mineralogy, Botany, and Zoology of Massachusetts\" and its successor, the 1841 \"Final Report\" produced when he was State Geologist. For the 1833 edition, Pendleton's Lithography (Boston) lithographed nine of Orra White Hitchcock's Connecticut River Valley drawings and printed them as plates for the work. In 1841, B. W. Thayer and Co., Lithographers (Boston) printed revised lithographs and an additional plate. The hand-colored plate \"Autumnal Scenery. View in Amherst\" is Orra White Hitchcock's most frequently seen work.\n\nBetween 1828 and the 1840s, Hitchcock made hundreds of large and dramatic classroom charts of geologic cross-sections, prehistoric beasts (like the Megatherium), fossils and ichnological (later called dinosaur) footprints. She copied scientific illustrations from contemporary works and made original illustrations of Edward Hitchcock's new ideas or discoveries, like “Ornithichnites, Hitch.” He considered them “indispensable aids” for his lectures. The Amherst College Archives and Special Collections holds an extensive collection of classroom charts.\n\nHitchcock's first documented published drawing is from an 1818 article by Edward_Hitchcock in the periodical \"Port Folio\". On rare occasions, she created illustrations for other scientists. Hitchcock's last documented work was her symbolic illustrations for her husband's \"Religious Lectures on Peculiar Phenomena in the Four Seasons\", including an emblematic representation of spring and a stylized rainbow.\n\nOrra White Hitchcock raised 6 surviving children, taught them art and science and was Edward Hitchcock's partner in his scientific undertakings. She travelled with her husband in the United States and to England and Europe (in 1850). She is the mother of geologist Charles Henry Hitchcock (1836–1919) and physical education and hygiene pioneer Edward Hitchcock, Jr. (1828–1911).\n\nEdward_Hitchcock acknowledged his wife's essential contributions to his work in the dedication of \"The Religion of Geology\", citing her drawings as more powerful than his pen.\n\nOrra White Hitchcock died at 67 on May 26, 1863 from consumption.\n\nThough she was not a trained professional, Hitchcock's scientific intellect and the artistic ability to visually transcribe key scientific principles and natural phenomena, flora and fauna, enabled her to make substantial contributions to the understanding of geology and botany in the first half of the nineteenth century in the United States.\nWhile published illustrations exist, only a small number of Hitchcock's original works survives. The Amherst College Archives and Special Collections has the most extensive documentation of her life and work, in the \"Edward and Orra White Hitchcock Papers\" and copies of all of Edward Hitchcock's scientific publications. The Mead Art Museum at Amherst College held the first major retrospective exhibition of her work in 2011, \"Orra White Hitchcock (1796-1863): An Amherst Woman of Art and Science,\" with a catalogue.\n\n\n\n\n"}
{"id": "16089297", "url": "https://en.wikipedia.org/wiki?curid=16089297", "title": "Otto Zdansky", "text": "Otto Zdansky\n\nOtto Karl Josef Zdansky (28 November 1894, Vienna – 26 December 1988, Uppsala) was an Austrian paleontologist.\n\nHe graduated from the Philosophical School at the University of Vienna in Paleontology on March 21, 1921 with the academic degree 'Dr. phil.' (dissertation: 'Über die Temporalregion des Schildkrötenschädels').\n\nHe is best known for his work in China, where he, as an assistant to Johan Gunnar Andersson, discovered a fossil tooth of the Peking Man in 1921 at the Dragon Bone Hill, although he did not disclose it until 1926 when he published it in \"Nature\" after an analysis by Davidson Black.\nHe is also famous for his excavations of mammal fossils in Baode County area (Pao Te Hsien), Shanxi Province. Zdansky in 1923 excavated the sauropod dinosaur \"Euhelopus zdanskyi\" named after him.\n"}
{"id": "22055657", "url": "https://en.wikipedia.org/wiki?curid=22055657", "title": "Psychological research", "text": "Psychological research\n\nPsychological research refers to research that psychologists conduct to systematic study and analyse of the experiences and behaviours of individuals or groups. Their research can have educational, occupational and clinical applications.\n\nPsychologists use many research methods, and categorical distinctions of these methods have emerged. Methods can be categorized by the kind of data they produce: qualitative or quantitative—and both these are used for pure or applied research.\n\nPsychology tends to be eclectic, applying knowledge from other fields. Some of its methods are used within other areas of research, especially in the social and behavioural sciences.\n\nThe field of psychology commonly uses experimental methods in what is known as experimental psychology. Researchers design experiments to test specific hypotheses (the deductive approach), or to evaluate functional relationships (the inductive approach).\n\nThe method of experimentation involves an experimenter changing some influence—the \"independent variable(IV)\"— on the research subjects, and studying the effects it produces on an expected aspect—the \"dependent variable (DV)\"— of the subjects behaviour or experience. Other variables researchers consider in experimentation are known as the \"extraneous variables\", and are either \"controllable\" or \"confounding\" (more than one variable at play).\n\nConfounding variables are external variables that are not taken into account when conducting an experiment. Because they are not controlled for, they can skew experiments results and provide a false or unreliable conclusion. For example, the psychologist Seymour Feshbach conducted an experiment to see how violence on television (the independent variable), affected aggression in adolescent boys (the dependent variable). He published his results in a paper called \"Television and Aggression\" in 1971. The paper showed that, in some cases, the lack of violence on television made the boys \"more\" aggressive. This was due to a confounding variable, which in this case was frustration. This means that extraneous variables are important to consider when designing experiments, and many methods have emerged to scientifically control them. For this reason, many experiments in psychology are conducted in laboratory conditions where they can be more strictly regulated.\n\nAlternatively, some experiments are less controlled. Quasi-experiment's are those that a researcher sets up in a controlled environment, but does not control the independent variable. For example, Michael R. Cunningham used a quasi-experiment to \"...measure the physical in physical attractiveness.\" On the other hand, in field experiments the experimenter controls an independent variable (making it the control variable), but does not control the environment where the experiment takes place. Experimenters sometimes apply fewer controls, as a way to lessen potential biases. In a \"true experiment\", participants are randomly chosen to remove the chance of experimenter's bias.\n\n\"Observational research,\" (a type of non-experimental, correlational research), involves the researcher observing the ongoing behaviour of their subjects. There are multiple methods of observational research such as \"participant observations\", \"non-participant observations\" and \"naturalistic observations\".\n\nParticipant observations are methods that involve a researcher joining the particular social group they are studying. For example, the social psychologist, Leon Festinger and his associates, joined a group called \"The Seekers\" in order to observe them. The Seekers believed they were in touch with aliens, and that the aliens had told them the world was about to end. When the foretasted event didn't happen, Festinger and his associates observed how the attitudes of the group members changed. They published their results in a 1956 book called \"When Prophecy Fails\". David Rosenhan in 1973 published a journal that involved research by participant observations.\" see: on being sane in insane places\".\n\nThe other method of observational research is non-participant observation. In particular naturalistic methods are methods that simply study behaviours that occur naturally in natural environments—with no manipulation by the observer. The events studied \"must\" be natural and not staged. This fact gives naturalistic observational research a high ecological validity. During naturalistic observations, researchers can avoid interfering with the behavior they are observing by using unobtrusive methods, if needed.\n\nBoth types of observational methods are designed to be as reliable as possible. Reliability can be estimated using \"inter-observer reliability\", that is, by comparing observations conducted by different researchers. \"Intra-observer reliability\" means estimating the reliability of an observation using a comparison of observations conducted by the same researcher. The reliability of conducted studies is important in any field of science.\n\n\"For a statistical perspective of reliability, see also\" Reliability (statistics).\n\nAll scientific processes begin with a description based on observation. Theories may develop later to explain these observations or classify associated phenomena. In scientific methodology, the conceptualizing of descriptive research precedes the hypotheses of \"explanatory research\".\n\nAn example of a descriptive device used in psychological research is the \"diary\", which is used to record observations. There is a history of use of diaries within clinical psychology. Examples of psychologists that used them include B.F. Skinner (1904–1990) and Virginia Axline (1911–1988). A special case of a diary in this context, that has particular importance in development psychology, is known as the \"baby biography\", and was used by psychologists such as Jean Piaget.\n\nOther recording methods can include video or audio. For example, forensic psychologists record custodial interrogations to aid law enforcement.\n\nA \"case study\"—or \"case report\"—is an intensive analysis of a person, group, or event that stresses developmental factors related to the context. Case studies may be descriptive or explanatory. Explanatory case studies explore causation to identify underlying principles. However, there is a debate to whether case studies count as a scientific research method. Clinical psychologists use case studies most often, especially to describe abnormal events and conditions, which are particularly important in clinical research. Sigmund Freud made extensive use of case studies to formulate his theory of psychoanalysis.\n\nFamous case studies include: Anna O. and Rat Man of Freud's Genie, who is one of the most severe cases of social isolation ever recorded, and Washoe, a chimpanzee who was the first non-human that had learned to communicate using American Sign Language.\n\nInterviews and questionnaires intrude as a foreign element into the\nsocial setting they would describe, they create as well as measure atti-\ntudes, they elicit atypical role and response, they are limited to those\nwho are accessible and who will cooperate, and the responses obtained\nare produced in part by dimensions of individual differences irrelevant\nto the topic at hand.\n\nBradburn et al. (1979) found a tendency for survey respondents to over-\nreport socially desirable behaviours when interviewed using less anonymous methods.\n\nPsychometrics is a field of study concerned with the theory and technique of psychological measurement. One part of the field is concerned with the objective measurement of skills and knowledge, abilities, attitudes, personality traits, and educational achievement.\n\nArchival research can be defined as the study of existing data. The existing data is collected to answer research questions. Existing data sources may include statistical records, survey archives, and written records.\n\nCross-sectional research is a research method often used in developmental psychology, but also utilized in many other areas including social science and education. This type of study utilizes different groups of people who differ in the variable of interest, but share other characteristics such as socioeconomic status, educational background, and ethnicity.\n\nFor example, researchers studying developmental psychology might select groups of people who are remarkably similar in most areas, but differ only in age.\n\nLongitudinal research is a type of research method used to discover relationships between variables that are not related to various background variables. This observational research technique involves studying the same group of individuals over an extended period of time.\n\nData is first collected at the outset of the study, and may then be gathered repeatedly throughout the length of the study. In some cases, longitudinal studies can last several decades.\n\nCross-cultural psychology is a branch of psychology that looks at how cultural factors influence human behavior.\n\nEssentially, cohort refers to people who are approximately the same age. When researchers conduct different types of studies (for example, developmental/cross sectional studies), they use cohorts to see how people of different ages compare on some topic at one point in time. For example, a researcher may compare the effects of a new study aid in three different cohorts: 10th graders, 11th graders, and 12th graders. In this way, you can examine the study aid across three different grade levels.\n\nA discipline lying on the border between artificial intelligence and psychology. It is concerned with building computer models of human cognitive processes and is based on an analogy between the human mind and computer programs. The brain and computer are viewed as general-purpose symbol-manipulation systems, capable of supporting software processes, but no analogy is drawn at a hardware level.\n\nThe term \"unobtrusive measures\" was first coined by Eugene Webb, Campbell, Schwartz, and Sechrest in a 1966 book, \"Unobtrusive methods: Nonreactive research in the social science\", in which they described methods that don't involve direct induction of data from research subjects. For example, the evidence people leave behind as they traverse their physical environment is unobtrusive. Unobtrusive methods get around biases, such as the selection bias and the experimenter's bias, that result from the researcher and his intrusion. Consequently, however, these methods reduce the researcher's control over the type of data collected.\n\nWeb and others regard these methods as an additional tools to use with the more common \"reactive\" and \"intrusive methods\".\n\n\n"}
{"id": "32015318", "url": "https://en.wikipedia.org/wiki?curid=32015318", "title": "Ranna an aeir", "text": "Ranna an aeir\n\nRanna an aeir (\"The Constellations\") is the title of a medieval Irish astronomical tract, thought to date from c.1500–1550? It was written in Early Modern Irish, with some words in English and Latin. \n\n\n\n"}
{"id": "18471115", "url": "https://en.wikipedia.org/wiki?curid=18471115", "title": "Risk Governance: Coping with Uncertainty in a Complex World", "text": "Risk Governance: Coping with Uncertainty in a Complex World\n\nRisk Governance: Coping with Uncertainty in a Complex World is part of \"The Earthscan Risk in Society Series\" of books. Published in 2008, the book brings together and updates the well-known work of risk theorist and researcher Ortwin Renn, integrating concepts of risk in the social, engineering and natural sciences. \"Risk Governance\" presents the context of risk handling before proceeding through the core topics of assessment, evaluation, perception, management and communication. The main focus is on systemic risks, such as genetically modified organisms, which have a high degree of complexity, uncertainty and ambiguity, and with major repercussions on financial, economic, and social impact areas.\n\nReviews of this book have appeared in the journals \"Natural Hazards\", and \"Acta Sociologica\".\n\nAuthor Ortwin Renn is Director of the nonprofit company DIALOGIK, a research institute for the investigation of communication and participation processes in environmental policymaking and Professor and Chair of Environmental Sociology of the State University in Stuttgart, Germany. \n"}
{"id": "13774285", "url": "https://en.wikipedia.org/wiki?curid=13774285", "title": "SciVee", "text": "SciVee\n\nSciVee was a science video sharing website where researchers could upload, view and share science video clips and connect them to scientific literature, posters and slides from 2007-2015. The SciVee website is partnered with three groups: The Public Library of Science (PLoS), a publisher of a series of open access (OA) journals who have added content to the website, the National Science Foundation (NSF), who provided seed funding to start the website, and the San Diego Supercomputer Center (SDSC), who houses SciVee's video servers and data for the website. The University of California, San Diego-based service uses Adobe Flash technology to display video combined with documents and imagery via SciVee's patent pending rich internet applications (RIA) or \"virtual studio\" WYSIWYG to combine, or “synchronize” them with a published scientific article from a scientific journal or poster from a scientific conference poster session. Any video synchronized with a published scientific article is called a “pubcast,” while a video that is synchronized with a scientific conference session poster is called a “postercast.” Science videos that are not synchronized with a scientific article or poster can be uploaded and linked with supplementary files.\n\nResearch scientists are the primary audience for the website, but students of all levels, educators and the general public also use the site. Video content ranges from dense and highly technical explanations of scientific publications to elementary school level science. Unregistered users can watch the videos and use the provided embed code to vlog to videos into external websites, while registered users are permitted to upload an unlimited number of videos, synchronize scientific documents, add commentary to the site, create public profiles, and join or create communities. Registration is free and provides access to a full social networking service that allows registered members to interact with other members through private messaging, blogging, and open community discussions.\n\nWith seed funding from NSF, two professors from the University of California, Phil Bourne and Leo Chalupa, founded the website. In early 2007, Phil and Leo put together a small team of people to create the Web 2.0 website allowing participation membership to a social science network with video and article upload. SciVee was named by combining the words \"science\" and \"video\". After initial development, the website began accepting its first video uploads August 1, 2007. Nineteen days after going online in a pre-alpha test state, Slashdot dubbed SciVee the “YouTube for Science.” The official alpha site launch took place September 1, 2007. Based on the feedback received from the initial boom of new members from being Slashdotted, the SciVee team made updates to the website to accommodate customer demands and launched its beta release December 3, 2007. That day, CNN and USA Today featured articles about a video created by four UC San Diego science and film students showing \"a typical recrystallization experiment straight out of Chemistry 101.\" Since then, SciVee has added more features to the site including discussions, blogging, extended profiles and other community options. On August 26, 2008, SciVee launched a new postercast capability. The same day, Paul Glazowski at Mashable referred to the release as \"a new option for users to upload feature material in the form of “postercasts” that enable users to complement their traditional video presentations with an interactive documentation component. The synchronous delivery of these is remarkably user-friendly.\". The site abruptly disappeared around the end of 2015.\n\nPhilip Bourne stated in his article in CTWatch Quarterly, Volume 3, Number 3, August 2007:\n\"We believe that the research community is ripe for a revolution in scientific communication and that the current generation of scientists will be the one to push it forward. These scientists, generally graduate students and new post-docs, have grown up with cyberinfrastructure as a part of their daily lives, not just a specialized aspect of their profession. They have a natural ability to do science in an electronic environment without the need for printed publications or static documents and, in fact, can feel quite limited by the traditional format of a publication. Perhaps most importantly, they appreciate that the sheer amount of data and the number of publications is prohibitive to the traditional methods of keeping current with the literature...To this end, we have developed SciVee, which allows authors to upload an article they have already published (open access, naturally) with a video or podcast presentation (about 10 minutes long) that they have made that describes the highlights of the paper. The author can then synchronize the video with the content of the article (text, figures, etc.) such that the relevant parts of the article appear as the author discusses them during the video presentation. We call the result a pubcast.\" \n\nLynn Fink, SciVee's scientific developer, stated in her article submitted to FEST, the International Science Media Fair in Trieste:\n\"Keeping current with the literature is a crucial part of doing science. It is, however, getting increasingly more difficult due to the growing number of articles that are published. SciVee aims to make this task easier and faster by delivering the key points of articles in an accessible and enjoyable medium – the pubcast: a short video of the author speaking about their published paper while the text of their paper is displayed next to the video. Prior to the open access movement, the creation of pubcasts would have been prohibitively difficult and readers would be forced to face a growing stack of articles to read. Fortunately, with pubcasts, readers can interact with several articles in the time it would take to read a single full article in the traditional way... We believe that the emergence of open access literature is the spur that will drive innovation in scientific communication. In contrast to closed access literature, where publishers require a subscription to access content, articles that are published as open access are available for immediate viewing, download, and distribution. Furthermore, the author retains the copyright, rather than the publisher, under a Creative Commons license (usually CCAL 2.5 or 3.0) which grants the author much more freedom in the use of their own work. This license also grants considerable freedom to a consumer of this article. Specifically, the CCAL licenses under which most open access articles are published allow a consumer to \"make and distribute derivative works, in any digital medium\". SciVee takes full advantage of this by integrating the full text of the articles with web-based video... The traditional article format no longer effectively supports the research in the current age. We believe that taking advantage of open access articles in this way will have a significant impact on the scientific community. SciVee modernizes scientific publishing and communication by taking advantage of the possibilities the information age has to offer, namely widespread use of cyberinfrastructure. SciVee makes the process of creating and consuming scientific literature more enjoyable and accessible. We hope that scientific community will embrace these efforts and help make scientific communication.\"\nmore effective.\n\n\n"}
{"id": "5975433", "url": "https://en.wikipedia.org/wiki?curid=5975433", "title": "Singular control", "text": "Singular control\n\nIn optimal control, problems of singular control are problems that are difficult to solve because a straightforward application of Pontryagin's minimum principle fails to yield a complete solution. Only a few such problems have been solved, such as Merton's portfolio problem in financial economics or trajectory optimization in aeronautics. A more technical explanation follows.\n\nThe most common difficulty in applying Pontryagin's principle arises when the Hamiltonian depends linearly on the control formula_1, i.e., is of the form: formula_2 and the control is restricted to being between an upper and a lower bound: formula_3. To minimize formula_4, we need to make formula_1 as big or as small as possible, depending on the sign of formula_6, specifically:\n\nIf formula_8 is positive at some times, negative at others and is only zero instantaneously, then the solution is straightforward and is a bang-bang control that switches from formula_9 to formula_10 at times when formula_8 switches from negative to positive. \n\nThe case when formula_8 remains at zero for a finite length of time formula_13 is called the singular control case. Between formula_14 and formula_15 the maximization of the Hamiltonian with respect to formula_1 gives us no useful information and the solution in that time interval is going to have to be found from other considerations. (One approach would be to repeatedly differentiate formula_17 with respect to time until the control u again explicitly appears, which is guaranteed to happen eventually. One can then set that expression to zero and solve for u. This amounts to saying that between formula_14 and formula_15 the control formula_1 is determined by the requirement that the singularity condition continues to hold. The resulting so-called singular arc will be optimal if it satisfies the Kelley condition:\n\n. This condition is also called the generalized Legendre-Clebsch condition).\n\nThe term bang-singular control refers to a control that has a bang-bang portion as well as a singular portion.\n"}
{"id": "2497774", "url": "https://en.wikipedia.org/wiki?curid=2497774", "title": "Spatial network analysis software", "text": "Spatial network analysis software\n\nSpatial network analysis software packages are computer tools used to prepare various graph-based analysis of spatial networks. They stem from various research fields in transportation, architecture and urban planning. The earliest examples of spatial network analysis using computers include the work of Garrison (1962), Kansky (1963), Levin (1964), Harary (1969), Rittel (1967), Tabor (1970) and others in the 1960s and 70s. Various fields of study have later developed specific spatial analysis software to suit their needs, including TransCAD among transportation researchers, GIS among planners and geographers, Axman among Space Syntax researchers, and various plugins for other software platforms. The list below gives and overview of some of the available software. Many of these were developed in academia and are freely available or freely available for academic research.\n\nIn historical order:\n\n\n"}
{"id": "40046265", "url": "https://en.wikipedia.org/wiki?curid=40046265", "title": "Timothy McCarthy (sailor)", "text": "Timothy McCarthy (sailor)\n\nTimothy 'Tim' McCarthy (15 July 1888 – 16 March 1917) was an Irish able seaman (AB). He is best known for his service in the Imperial Trans-Antarctic Expedition of 1914–1916, for which he was awarded the Bronze Polar Medal.\n\nMcCarthy was born on 15 July 1888 in Kinsale, Ireland. He signed on the \"Endurance\" as an able seaman, and participated fully in the dangers and privations of the Weddell Sea, particularly after the \"Endurance\" sank and the ship's company and shore party were marooned on a nearby ice floe. Later, the ship's company and shore party were forced to take to lifeboats, and the seamanship of the ABs became a decisive element in the survival of the entire company.\n\nExpedition commander Ernest Shackleton was impressed by McCarthy's skill during the survival journey from the northern Weddell Sea to Elephant Island, and so when the expedition leader decided to relaunch the best lifeboat into the open Southern Ocean, with the goal of contacting potential rescuers in South Georgia, McCarthy was one of the five men he chose to accompany him. The sixteen-day voyage of the \"James Caird\" in April–May 1916 became a classic story of human endurance, and the boat's navigator, Frank Worsley, offered fervent and repeated praise to McCarthy for his services in keeping the boat afloat and helping to ensure the survival of the party.\n\nWhen the \"James Caird\", in a sinking condition, made landfall on South Georgia's uncharted southern coast, two of the six men of the company were physically unfit for further service. Shackleton and his men turned over the boat and made it into a primitive shore shelter, which they called \"Peggotty Camp\". Three of the men—Tom Crean, Worsley, and Shackleton himself—crossed the icy island to finalise the rescue, while the other three, the ageing carpenter Henry McNish and incapacitated seaman John Vincent, briefly remained at the camp under McCarthy's informal leadership. The island crossing was successful and the party was reunited after less than forty-eight hours. Soon McCarthy found himself heading northward. Having successfully participated in his own rescue, his part in the expedition was over.\n\nUpon making landfall in the British Isles, McCarthy (who was not an Irish Nationalist and identified himself with the British Empire) found his country fighting World War I. He joined the Royal Navy Reserve as a leading seaman. In these duties he was assigned to man a deck gun on the S.S. \"Narragansett\", an oil tanker. On 16 March 1917, this vessel was torpedoed and sunk with all hands in the Western Approaches. McCarthy, aged 28, was the first member of the Weddell Sea party of the Imperial Trans-Antarctic Expedition to die.\n\nIn 1916–17, McCarthy was awarded the Polar Medal in bronze. McCarthy Island, a rocky island just off King Haakon Bay where the \"James Caird\" made landfall, was named in McCarthy's honour by the South Georgia Survey in the period 1951–1957. A joint bust of Tim McCarthy and his brother Morty McCarthy, also an Antarctic explorer, was unveiled in Kinsale in September 2000.\n"}
{"id": "1683651", "url": "https://en.wikipedia.org/wiki?curid=1683651", "title": "Vacuum solution", "text": "Vacuum solution\n\nA vacuum solution is a solution of a field equation in which the sources of the field are taken to be identically zero. That is, such field equations are written without matter interaction (i.e.- set to zero).\n\nIn Maxwell's theory of electromagnetism, a vacuum solution would represent the electromagnetic field in a region of space where there are no electromagnetic sources (charges and electric currents), i.e. where the current 4-vector vanishes:\n\nIn Einstein's theory of general relativity, a vacuum solution would represent the gravitational field in a region of spacetime where there are no gravitational sources (masses), i.e. where the energy–momentum tensor vanishes:\n\nKasner vacuum solution\n\nIn a Kaluza–Klein vacuum (static) field equations\n\n\n"}
{"id": "34784718", "url": "https://en.wikipedia.org/wiki?curid=34784718", "title": "Zeppelin (research station)", "text": "Zeppelin (research station)\n\nZeppelin Observatory (\"Zeppelinobservatoriet\") is a research station in Svalbard, Norway. It is located near the top of Zeppelinfjellet above Ny-Ålesund on the peninsula of Brøggerhalvøya. It is operated by the Norwegian Polar Institute.\n\nThe research station at Zeppelinfjellet was built between 1988-1989 and officially opened in 1990. After 10 years of use, it was determined that the building no longer covered the needs that were required to operate advanced instrumentation. In the second half of 1999, the old building was demolished and a new and improved station was built at the same site. The new station building was officially opened on 2 May 2000.\n"}
