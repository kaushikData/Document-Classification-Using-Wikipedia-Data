{"id": "30805061", "url": "https://en.wikipedia.org/wiki?curid=30805061", "title": "AHRC/ESRC Religion and Society Programme", "text": "AHRC/ESRC Religion and Society Programme\n\nThe AHRC/ESRC Religion and Society Programme is a five-year strategic research initiative funded by two UK research councils: the Arts and Humanities Research Council and the Economic and Social Research Council. running 2007-2012. It has funded 75 projects across UK universities investigating various aspects of the complex relationships between religion and society, both historical and contemporary.\n\nResearch supported includes the website \"British Religion in Numbers\", providing a one-stop shop for statistics on religion in Britain led by Professor David Voas at Manchester University, Professor Kim Knott at Leeds University's restudy of British media coverage of religion and spirituality, and an investigation of Hindu, Muslim and Sikh shrine practices across the Punjab led by Dr Tej Purewal, also at Manchester. Phase 2 of the Programme has focused on Youth and Religion specifically, supporting projects like Dr Basia Spalek at Birmingham University's research into police partnerships with young Muslims.\n\nThe Religion and Society Programme is hosted at Lancaster University, and directed by Professor Linda Woodhead helped by Dr Rebecca Catto (Research Associate) and Peta Ainsworth (Administrator). The Programme also helps to organize various relevant and topical events such as a day at the British Library asking ‘where next for religion in the public sphere’ in July 2010 and a closed seminar asking ‘Child abuse in the Catholic Church – what can be learned?’ at Heythrop College London in November 2010.\n\nSociologist have studied religion right from the beginning of the discipline, in the 19th century. It was a central concern of the first generation of sociological thinkers and those who influence them. Major influences in the history of the disciplines, such as Comte, Karl Marx, Durkheim, Weber, and Simmel all contributed in various ways to the sociology of religion. The French philosopher August Comte, coined the term sociology. Even so, religion as a replacement for existing religions as a new religion of positivism. Positivism was the view that only the methods of the natural sciences could provide knowledge of human nature and society (Kaelber, 2004). He thought positivism was valuable for its potential to solve social problems and to reorganize society. He developed a blueprint for a new social order which had a religion of humanity at its ethical basis. He did so to recognize the importance of religion in creating social bonds between people. However, his work has been largely overshadowed by the work of other scholars in sociology of religion, such as Max Weber and Durkheim's. Later work in the sociology of religion has often been developed with the work of these early scholars. Just to pick one example, one of the most famous works in the sociology of religion; Max Weber's \"the Protestant Ethic and the Spirit of Capitalism\". This was concerned with the question of the role played by religion. It's all to understand why a system of rational capitalism developing the west from about the 17th century onwards (Kaelber, 2004). Sociologist are interested in studying religion for three main reason. The first, is that religion is simply very important in the lives of many people around the world. Eighty five percent of the world's population affirmed some kind of religious belief with Christians, Muslims, and Hindus making up the three largest religious groups. Religious ideas help people to interpret their experiences. Religious values influence many people's actions and the organizations provide many people with fellowship, aid, and support. Sociologists just seeking to understand people's culture, can’t avoid than to understand the role that religion plays in their lives. Also, they must understand the consequences of religious beliefs and practices for the wider society, which might contain non-religious elements or a diversity. Second, revolves around the idea of how religion takes different forms. The difference in religious beliefs and practices between societies is something that is noticeable and needs explanation. Finally, the third reason for religious study in sociology is the fact that religion changes over time in response to different social conditions. The prime objective for this effect, is to understand why (Kaelber, 2002).\n\nReligion in America has had a relationship since the very beginning of the nation. This relationship has changed over time and has shaped the culture. It started with the Native Americans and their worship of the Great Spirits. They believed that their ancestors watched over them and that the spirits in nature blessed them. When colonist came to America, they brought Christianity along with them. They tried to integrate the religion into the lives of the Native people. Christianity branched out and separated into Protestant and Catholic (Trundle, 2012). There were many forms of Protestant like Methodist, Lutheran, and Quakers. There are other forms of Catholic as well, including the Roman Catholic and eastern Orthodox. Later other religions were introduced to America through immigration. Religions like Islam, Buddhism, and Hinduism, and many others. These began to influence the culture. America was known as the melting pot of different cultures and religions. It was seen as a safe haven for those seeking religious freedom. Religion has effected America in many ways over the years. It was the reason the colonist came to America, and was used as an excuse for dominating over the Native Americans (Trundle, 2012). It was also used as a foundation for the civil rights movement as Martin Luther King Jr. presented his speech through his own personal belief system. More recent was September 11 when the twin towers were destroyed through religious beliefs. The bombers claimed that it was for religious purposes on why they did what they did. In effect, all religions joined together in America to rebuild and grow stronger. Religion has brought people together in times of need, throughout history. It has also caused negative effects in society as explained with 9/11.\n\nFewer and fewer people believe nowadays. It's possible that in a generation, there simply won’t be religion across Europe and large sections of North America, Australia, and Asia. That's not necessarily a problem. However, it is worth thinking about why people made up religion in the first place, and what we’re doing with the needs and longings that led them to do so. At one level, religions are about asking us to believe in something. When people say, they can’t believe, they tend to stop right there with the whole religion business. They often then point out all the horrid things that religions have undoubtly done and continue to do. In this sense, belief is almost the least important and defiantly the least interesting side of religion. What's fascinating is all the other stuff religions get up to. For example, the way they regularly gather people around, and tell them to be nice to one another. Also, the way they create a sense of community, teaching others to see everyone else as human beings (Gruber, 2008). Religions use rituals to point stuff out to us and lodge it in our fickle minds. For example, that the seasons are changing or that it's the time to remember our ancestors. Religions know were not just intellectual creatures, so they carefully appeal to us via art and beauty. We think of beauty in one category, a frivolous and superficial thing, and truth and depth in another. Religions join them together. They build temples, cathedrals, and mosques that use beauty to lend depth to important ideas. They use the resources of art to remind us of what matters. Its directed at making us feel things, calm, pity, awe.\n\nWe may no longer believe, but the needs and longings that make up these stories go on. We long for beauty, wisdom, and purpose. We want to live for something more than just ourselves. Society tells us to direct our hopes in two areas. These are romantic love, and professional success, and it distracts us with news, movies, and consumption. It's not enough, as we know. We need reminders to be good, places to reawaken awe, and something to awaken our kinder, less selfish impulses. Universal things which need tending like delicate flowers and rituals that bring us together. The choice isn’t between religion and a secular world as it is now. The challenge is to learn from religions, so we can fill the secular world with replacements for the things we long ago made up religion to provide (Giovanni, 2003).\n\nGiovanni, G. D. (2003). Faith without religion, religion without faith: Kant and hegel on religion. \"Journal of the History of Philosophy, 41\"(3), 365.\n\nGruber, J., & Hungerman, D. M. (2008). The church versus the mall: What happens when religion faces increased secular competition? \"The Quarterly Journal of Economics, 123\"(2), 831.\n\nKaelber, L. (2004). Sociology of religion: A historical Introduction/Religion in society: A sociology of Religion/Religion: The social Context/Religion in sociological Perspective/Invitation to the sociology of religion. \"Teaching Sociology, 32\"(3), 329-332.\n\nKaelber, L. (2002). Sociology of religion: Theoretical and comparative perspectives / sociology of religion: Contemporary developments / A comparatice sociology of world religions: Virtuosos, priests, and popular religion. \"Teaching Sociology, 30\"(4), 496.\n\nTrundle, R. C. (2012). AMERICA'S RELIGION VERSUS RELIGION IN AMERICA: A PHILOSOPHIC PROFILE. \"Journal for the Study of Religions and Ideologies, 11\"(33), 3-20.\n"}
{"id": "26266522", "url": "https://en.wikipedia.org/wiki?curid=26266522", "title": "ARGUS-IS", "text": "ARGUS-IS\n\nThe ARGUS-IS, or the Autonomous Real-Time Ground Ubiquitous Surveillance Imaging System, is a Defense Advanced Research Projects Agency (DARPA) project contracted to BAE Systems.\n\nARGUS is an advanced camera system that uses hundreds of cellphone cameras in a mosaic to video and auto-track every moving object within a 36 square mile area.\n\nARGUS is a form of wide-area persistent surveillance system that allows for one camera to provide such detailed video that users can collect \"pattern-of-life\" data and track individual people inside the footage anywhere within the field of regard. This is accomplished by utilizing air assets (manned aircraft, drones, blimps, aerostats) to persistently loiter and record video of a 36 square mile area with enough detail to track individual pedestrians, vehicles or other objects of interest as long as the air asset remains circling above. Automatic object-tracking software called Persistics from the Lawrence Livermore labs allows users to auto-track every moving object within the field of regard (36 sq miles) and generate geolocation chronographs of each individual vehicle and pedestrian's movements, making them searchable via geolocation query.\n\nAs ARGUS floats overhead for months at a time, it dragnet tracks every moving person and vehicle and chronographs their movements, allowing forensic investigators to rewind the footage and watch the activities of anyone they select within the footage.\n\nARGUS is only one form of Wide Area Persistent Surveillance. Other WAPS systems are already being used for domestic law enforcement across the USA including Persistent Surveillance in Dayton, Ohio, Vigilant Stare as well as Pixia's Hiper Stare. While the government has not admitted to deploying ARGUS over the US, it has shown video in which ARGUS was used within the United States. A variety of privacy advocacy groups including the ACLU and Teame Zazzu have worked to bring the domestic deployments of ARGUS and other WAPS systems into the public debate.\n\nTraffic cameras, which were meant to help enforce traffic laws at intersections, have also sparked some controversy, due to their use by law enforcement agencies for purposes unrelated to traffic violations. These cameras also work as transit choke-points that allow individuals inside the vehicle to be positively identified and license plate data to be collected and time stamped for cross reference with airborne Wide Area Persistent Surveillance Systems used by police.\n\nA demonstration of the system was made available to the PBS NOVA program and used in a story on UAVs.\n\nThe contract was awarded in late 2007 with a budget of US$18.5 million and duration of 30 months.\n\nThe first test flight using a UH-60 Black Hawk was declared a success by BAE in February 2010.\n\nIn early 2014, the ARGUS-IS achieved initial operating capability (IOC) with the U.S. Air Force as part of Gorgon Stare Increment 2, giving the MQ-9 Reaper the ability to survey an area of .\n\nThe three principal components of the ARGUS-IS are a 1.8 Gigapixels video system plus two processing subsystems, one in the air and the other located on the ground.\n\nThe sensor uses four lenses and 368 cell phone cameras, 5 megapixels each.\n\nThe system could stream 1 exabyte of high definition video per day.\n\n"}
{"id": "9394772", "url": "https://en.wikipedia.org/wiki?curid=9394772", "title": "Barnes–Hut simulation", "text": "Barnes–Hut simulation\n\nThe Barnes–Hut simulation (Josh Barnes and Piet Hut) is an approximation algorithm for performing an \"n\"-body simulation. It is notable for having order O(\"n\" log \"n\") compared to a direct-sum algorithm which would be O(\"n\").\n\nThe simulation volume is usually divided up into cubic cells via an octree (in a three-dimensional space), so that only particles from nearby cells need to be treated individually, and particles in distant cells can be treated as a single large particle centered at the cell's center of mass (or as a low-order multipole expansion). This can dramatically reduce the number of particle pair interactions that must be computed.\n\nSome of the most demanding high-performance computing projects do computational astrophysics using the Barnes–Hut treecode algorithm,\nsuch as DEGIMA.\n\nIn a three-dimensional \"n\"-body simulation, the Barnes–Hut algorithm recursively divides the \"n\" bodies into groups by storing them in an octree (or a quad-tree in a 2D simulation). Each node in this tree represents a region of the three-dimensional space. \nThe topmost node represents the whole space, and its eight children represent the eight octants of the space. The space is recursively subdivided into octants until each subdivision contains 0 or 1 bodies (some regions do not have bodies in all of their octants). \nThere are two types of nodes in the octree: internal and external nodes. An external node has no children and is either empty or represents a single body. Each internal node represents the group of bodies beneath it, and stores the center of mass and the total mass of all its children bodies.\n\nTo calculate the net force on a particular body, the nodes of the tree are traversed, starting from the root. If the center of mass of an internal node is sufficiently far from the body, the bodies contained in that part of the tree are treated as a single particle whose position and mass is respectively the center of mass and total mass of the internal node. If the internal node is sufficiently close to the body, the process is repeated for each of its children.\n\nWhether a node is or isn't sufficiently far away from a body, depends on the quotient formula_1, where \"s\" is the width of the region represented by the internal node, and \"d\" is the distance between the body and the node’s center of mass. The node is sufficiently far away when this ratio is smaller than a threshold value \"θ\". The parameter \"θ\" determines the accuracy of the simulation; larger values of \"θ\" increase the speed of the simulation but decreases its accuracy. If \"θ\" = 0, no internal node is treated as a single body and the algorithm degenerates to a direct-sum algorithm.\n\n\n\n\n"}
{"id": "29333046", "url": "https://en.wikipedia.org/wiki?curid=29333046", "title": "Bowden Glacier", "text": "Bowden Glacier\n\nBowden Glacier () is a glacier lying on the southeast flank of Salient Ridge that flows northeast to Blue Glacier, Victoria Land. It was named by the New Zealand Geographic Board in 1994 for Charles Bowden, first chairman of the Ross Dependency Committee during Sir Edmund Hillary's South Pole Expedition, part of the Commonwealth Trans-Antarctic Expedition in 1957. Bowden also served as a member of the Parliament of New Zealand until 1955.\n"}
{"id": "4063055", "url": "https://en.wikipedia.org/wiki?curid=4063055", "title": "Bruce Maccabee", "text": "Bruce Maccabee\n\nBruce Maccabee (May 6, 1942) is an American optical physicist formerly employed by the U.S. Navy, and a leading ufologist.\n\nMaccabee received a B.S. in physics at Worcester Polytechnic Institute in Worcester, Mass., and then at American University, Washington, DC, (M.S. and Ph.D. in physics). In 1972 he began his career at the Naval Ordnance Laboratory, White Oak, Silver Spring, Maryland; which later became the Naval Surface Warfare Center Dahlgren Division. Dr. Maccabee retired from government service in 2008. He has worked on optical data processing, generation of underwater sound with lasers and various aspects of the Strategic Defense Initiative (SDI) and Ballistic Missile Defense (BMD) using high power lasers.\n\nMaccabee has been an active ufologist since the late 1960s when he joined the National Investigations Committee on Aerial Phenomena (NICAP) and was active in research and investigation for NICAP until its demise in 1980. He became a member of the Mutual UFO Network (MUFON) in 1975 and was subsequently appointed to the position of state Director for Maryland, a position he still holds. In 1979 he was instrumental in establishing the Fund for UFO Research (FUFOR) and was the chairman for about 13 years. He presently serves on the National Board of the Fund.\n\nHis UFO research and investigations (which, he often stresses, are completely unrelated to his Navy work) have included the Kenneth Arnold sighting (June 24, 1947), the McMinnville, Oregon (Trent) photos of 1950, the Gemini 11 astronaut photos of September, 1966, the Tehran UFO incident of September 1976, the New Zealand sightings of December 1978, the Japan Airlines (JAL1628) sighting of November 1986, the numerous sightings of Gulf Breeze UFO incident, 1987–1988, the \"red bubba\" sightings, 1990-1992 (including his own sighting in September, 1991), the Mexico City video of August, 1997 (which he deemed a hoax), the Phoenix lights sightings of March 13, 1997, 2004 Mexican UFO incident and many others.\n\nHe has also done historical research and was the first to obtain the secret \"flying disc file\" of the FBI (what he calls \"the REAL X-Files\"). In addition, he has collected documents from the CIA, the U.S. Air Force, the U.S. Army, and other government agencies.\n\nMaccabee is the author or coauthor of about three dozen technical articles and more than a hundred UFO articles over the last 30 years, including many which appeared in the \"MUFON UFO Journal\" and MUFON Symposium proceedings. Among his papers was a reanalysis of the statistics and results of the famed Battelle Memorial Institute Project Blue Book Special Report No. 14, a massive analysis of 3200 Air Force cases through the mid-1950s. (See Identified Flying Objects (IFOs)). Another was a reanalysis of the results of the Condon Committee UFO study from 1969. (Like many others, Maccabee concluded that Edward Condon lied about the results.)\n\nIn addition, he has also written or contributed to half a dozen books on the subject of UFOs and appeared on numerous radio and TV shows and documentaries (some given below) as an authority on the subject.\n\nMaccabee is also an accomplished pianist who performed at the 1997 and 1999 MUFON symposia. He lives in Allen County, Ohio and married to Jan Maccabee.\n\n\n\n\nDr. Maccabee has been interviewed by print, radio and TV media numerous times since 1978. He has also appeared in a number of documentaries. Here are some of the more recent: \n\n"}
{"id": "8837050", "url": "https://en.wikipedia.org/wiki?curid=8837050", "title": "Copernican heliocentrism", "text": "Copernican heliocentrism\n\nCopernican heliocentrism is the name given to the astronomical model developed by Nicolaus Copernicus and published in 1543. It positioned the Sun near the center of the Universe, motionless, with Earth and the other planets orbiting around it in circular paths modified by epicycles and at uniform speeds. The Copernican model displaced the geocentric model of the Ptolemy that had prevailed for centuries, placing Earth at the center of the Universe. It is often regarded as the launching point to modern astronomy and the Scientific Revolution.\n\nCopernicus was aware that the ancient Greek Aristarchus had already proposed a heliocentric theory, and cited him as a proponent of it in a reference that was deleted before publication, but there is no evidence that Copernicus had knowledge of, or access to, the specific details of Aristarchus' theory. Although he had circulated an outline of his own heliocentric theory to colleagues sometime before 1514, he did not decide to publish it until he was urged to do so late in his life by his pupil Rheticus. Copernicus's challenge was to present a practical alternative to the Ptolemaic model by more elegantly and accurately determining the length of a solar year while preserving the metaphysical implications of a mathematically ordered cosmos. Thus, his heliocentric model retained several of the Ptolemaic elements causing the inaccuracies, such as the planets' circular orbits, epicycles, and uniform speeds, while at the same time re-introducing such innovations as:\n\nPhilolaus (4th century BCE) was one of the first to hypothesize movement of the Earth, probably inspired by Pythagoras' theories about a spherical, moving globe. Aristarchus of Samos in the 3rd century BCE had developed some theories of Heraclides Ponticus (speaking of a revolution by Earth on its axis) to propose what was, so far as is known, the first serious model of a heliocentric solar system. Though his original text has been lost, a reference in Archimedes' book \"The Sand Reckoner\" (\"Archimedis Syracusani Arenarius & Dimensio Circuli\") describes a work by Aristarchus in which he advanced the heliocentric model. Archimedes wrote:\n\nIt is a common idea that the heliocentric view was rejected by the contemporaries of Aristarchus. This is due to Gilles Ménage's translation of a passage from Plutarch's \"On the Apparent Face in the Orb of the Moon\". Plutarch reported that Cleanthes (a contemporary of Aristarchus and head of the Stoics) as a worshipper of the Sun and opponent to the heliocentric model, was jokingly told by Aristarchus that he should be charged with impiety. Gilles Ménage, shortly after the trials of Galileo and Giordano Bruno, amended an accusative (identifying the object of the verb) with a nominative (the subject of the sentence), and vice versa, so that the impiety accusation fell over the heliocentric sustainer. The resulting misconception of an isolated and persecuted Aristarchus is still transmitted today.\n\nSeveral Islamic astronomers questioned the Earth's apparent immobility, and centrality within the universe. Some accepted that the earth rotates around its axis, such as Abu Sa'id al-Sijzi (d. circa 1020). who invented an astrolabe based on a belief held by some of his contemporaries \"that the motion we see is due to the Earth's movement and not to that of the sky.\" The prevalence of this view is further confirmed by a reference from an Arabic work in the 13th century which states: According to the geometers [or engineers] (\"muhandisīn\"), the earth is in constant circular motion, and what appears to be the motion of the heavens is actually due to the motion of the earth and not the stars. In the 12th century, Nur ad-Din al-Bitruji proposed a complete alternative to the Ptolemaic system (although not heliocentric). He declared the Ptolemaic system as an imaginary model, successful at predicting planetary positions, but not real or physical. Al-Btiruji's alternative system spread through most of Europe during the 13th century.\n\nCopernicus cited Aristarchus and Philolaus in an early manuscript of his book which survives, stating: \"Philolaus believed in the mobility of the earth, and some even say that Aristarchus of Samos was of that opinion.\" For reasons unknown (although possibly out of reluctance to quote pre-Christian sources), he did not include this passage in the publication of his book. Inspiration came to Copernicus not from observation of the planets, but from reading two authors. In Cicero he found an account of the theory of Hicetas. Plutarch provided an account of the Pythagoreans Heraclides Ponticus, Philolaus, and Ecphantes. These authors had proposed a moving Earth, which did not, however, revolve around a central sun. When Copernicus' book was published, it contained an unauthorized preface by the Lutheran theologian Andreas Osiander. This cleric stated that Copernicus wrote his heliocentric account of the Earth's movement as a mere mathematical hypothesis, not as an account that contained truth or even probability. Since Copernicus' hypothesis was believed to contradict the Old Testament account of the Sun's movement around the Earth (Joshua 10:12-13), this was apparently written to soften any religious backlash against the book. However, there is no evidence that Copernicus himself considered the heliocentric model as merely mathematically convenient, separate from reality.\n\nMathematical techniques developed in the 13th to 14th centuries by the Arab and Persian astronomers Mo'ayyeduddin al-Urdi, Nasir al-Din al-Tusi, and Ibn al-Shatir for geocentric models of planetary motions closely resemble some of those used later by Copernicus in his heliocentric models. Copernicus used what is now known as the Urdi lemma and the Tusi couple in the same planetary models as found in Arabic sources. Furthermore, the exact replacement of the equant by two epicycles used by Copernicus in the \"Commentariolus\" was found in an earlier work by Ibn al-Shatir (d. c. 1375) of Damascus. Ibn al-Shatir's lunar and Mercury models are also identical to those of Copernicus. This has led some scholars to argue that Copernicus must have had access to some yet to be identified work on the ideas of those earlier astronomers. \nHowever, no likely candidate for this conjectured work has yet come to light, and other scholars have argued that Copernicus could well have developed these ideas independently of the late Islamic tradition. Nevertheless, Copernicus cited some of the Islamic astronomers whose theories and observations he used in \"De Revolutionibus\", namely al-Battani, Thabit ibn Qurra, al-Zarqali, Averroes, and al-Bitruji.\n\nEuropean scholars were well aware of the problems with Ptolemaic astronomy since the 13th century. The debate was precipitated by the reception by Averroes' criticism of Ptolemy, and it was again revived by the recovery of Ptolemy's text and its translation into Latin in the mid-15th century. Otto E. Neugebauer in 1957 argued that the debate in 15th-century Latin scholarship must also have been informed by the criticism of Ptolemy produced after Averroes, by the Ilkhanid-era (13th to 14th centuries) Persian school of astronomy associated with the \nMaragheh observatory (especially the works of Al-Urdi, Al-Tusi and Ibn al-Shatir).\n\nThe state of the question as received by Copernicus is summarized in the \"Theoricae novae planetarum\" by Georg von Peuerbach, compiled from lecture notes by Peuerbach's student Regiomontanus in 1454 but printed only in 1472.\nPeuerbach attempts to give a new, mathematically more elegant presentation of Ptolemy's system, but he does not arrive at heliocentrism. \nRegiomontanus himself was the teacher of Domenico Maria Novara da Ferrara, who was in turn the teacher of Copernicus.\n\nThere is a possibility that Regiomontanus already arrived at a theory of heliocentrism before his death in 1476, as he paid particular attention to the heliocentric theory of Aristarchus in a late work, and mentions the \"motion of the Earth\" in a letter.\n\nThe prevailing astronomical model of the cosmos in Europe in the 1,400 years leading up to the 16th century was that created by the Roman citizen Claudius Ptolemy in his \"Almagest\", dating from about 150 A.D. Throughout the Middle Ages it was spoken of as the authoritative text on astronomy, although its author remained a little understood figure frequently mistaken as one of the Ptolemaic rulers of Egypt. The Ptolemaic system drew on many previous theories that viewed Earth as a stationary center of the universe. Stars were embedded in a large outer sphere which rotated relatively rapidly, while the planets dwelt in smaller spheres between—a separate one for each planet. To account for apparent anomalies in this view, such as the apparent retrograde motion of the planets, a system of deferents and epicycles was used. The planet was said to revolve in a small circle (the epicycle) about a center, which itself revolved in a larger circle (the deferent) about a center on or near the Earth.\n\nA complementary theory to Ptolemy's employed homocentric spheres: the spheres within which the planets rotated could themselves rotate somewhat. This theory predated Ptolemy (it was first devised by Eudoxus of Cnidus; by the time of Copernicus it was associated with Averroes). Also popular with astronomers were variations such as eccentrics—by which the rotational axis was offset and not completely at the center.\n\nPtolemy's unique contribution to this theory was the equant—a point about which the center of a planet's epicycle moved with uniform angular velocity, but which was offset from the center of its deferent. This violated one of the fundamental principles of Aristotelian cosmology—namely, that the motions of the planets should be explained in terms of uniform circular motion, and was considered a serious defect by many medieval astronomers. In Copernicus's day, the most up-to-date version of the Ptolemaic system was that of Peurbach (1423–1461) and Regiomontanus (1436–1476).\n\nCopernicus' major work, \"De revolutionibus orbium coelestium\" - \"On the Revolutions of the Heavenly Spheres\" (first edition 1543 in Nuremberg, second edition 1566 in Basel), was published during the year of his death, though he had arrived at his theory several decades earlier. The book marks the beginning of the shift away from a geocentric (and anthropocentric) universe with the Earth at its center. Copernicus held that the Earth is another planet revolving around the fixed sun once a year, and turning on its axis once a day. But while Copernicus put the Sun at the center of the celestial spheres, he did not put it at the exact center of the universe, but near it. Copernicus' system used only uniform circular motions, correcting what was seen by many as the chief inelegance in Ptolemy's system.\n\nThe Copernican model replaced Ptolemy's equant circles with more epicycles. This is the main reason that Copernicus' system had even more epicycles than Ptolemy's. The Copernican system can be summarized in several propositions, as Copernicus himself did in his early \"Commentariolus\" that he handed only to friends probably in the 1510s. The \"little commentary\" was never printed. Its existence was only known indirectly until a copy was discovered in Stockholm around 1880, and another in Vienna a few years later.\nThe major features of Copernican theory are:\n\nIt opened with an originally anonymous preface by Andreas Osiander, a theologian friend of Copernicus, who urged that the theory, which was considered a tool that allows simpler and more accurate calculations, did not necessarily have implications outside the limited realm of astronomy.\n\nCopernicus' actual book began with a letter from his (by then deceased) friend Nikolaus von Schönberg, Cardinal Archbishop of Capua, urging Copernicus to publish his theory. Then, in a lengthy introduction, Copernicus dedicated the book to Pope Paul III, explaining his ostensible motive in writing the book as relating to the inability of earlier astronomers to agree on an adequate theory of the planets, and noting that if his system increased the accuracy of astronomical predictions it would allow the Church to develop a more accurate calendar. At that time, a reform of the Julian Calendar was considered necessary and was one of the major reasons for the Church's interest in astronomy.\n\nThe work itself was then divided into six books:\n\n\nFrom publication until about 1700, few astronomers were convinced by the Copernican system, though the book was relatively widely circulated (around 500 copies of the first and second editions have survived, which is a large number by the scientific standards of the time). Few of Copernicus' contemporaries were ready to concede that the Earth actually moved, although Erasmus Reinhold used Copernicus' parameters to produce the Prutenic Tables. However, these tables translated Copernicus' mathematical methods back into a geocentric system, rejecting heliocentric cosmology on physical and theological grounds. The Prutenic tables came to be preferred by Prussian and German astronomers. The degree of improved accuracy of these tables remains an open question, but their usage of Copernican ideas led to more serious consideration of a heliocentric model. However, even forty-five years after the publication of \"De Revolutionibus\", the astronomer Tycho Brahe went so far as to construct a cosmology precisely equivalent to that of Copernicus, but with the Earth held fixed in the center of the celestial sphere instead of the Sun. It was another generation before a community of practicing astronomers appeared who accepted heliocentric cosmology.\n\nFrom a modern point of view, the Copernican model has a number of advantages. It accurately predicts the relative distances of the planets from the Sun, although this meant abandoning the cherished Aristotelian idea that there is no empty space between the planetary spheres. Copernicus also gave a clear account of the cause of the seasons: that the Earth's axis is not perpendicular to the plane of its orbit. In addition, Copernicus's theory provided a strikingly simple explanation for the apparent retrograde motions of the planets—namely as parallactic displacements resulting from the Earth's motion around the Sun—an important consideration in Johannes Kepler's conviction that the theory was substantially correct. In the heliocentric model the planets' apparent retrograde motions' occurring at opposition to the Sun are a natural consequence of their heliocentric orbits. In the geocentic model, however, these are explained by the ad hoc use of epicycles, whose revolutions are mysteriously tied to that of the Sun's.\n\nHowever, for his contemporaries, the ideas presented by Copernicus were not markedly easier to use than the geocentric theory and did not produce more accurate predictions of planetary positions. Copernicus was aware of this and could not present any observational \"proof\", relying instead on arguments about what would be a more complete and elegant system. The Copernican model appeared to be contrary to common sense and to contradict the Bible. Tycho Brahe's arguments against Copernicus are illustrative of the physical, theological, and even astronomical grounds on which heliocentric cosmology was rejected. Tycho, arguably the most accomplished astronomer of his time, appreciated the elegance of the Copernican system, but objected to the idea of a moving Earth on the basis of physics, astronomy, and religion. The Aristotelian physics of the time (modern Newtonian physics was still a century away) offered no physical explanation for the motion of a massive body like Earth, but could easily explain the motion of heavenly bodies by postulating that they were made of a different sort of substance called aether that moved naturally. So Tycho said that the Copernican system “... expertly and completely circumvents all that is superfluous or discordant in the system of Ptolemy. On no point does it offend the principle of mathematics. Yet it ascribes to the Earth, that hulking, lazy body, unfit for motion, a motion as quick as that of the aethereal torches, and a triple motion at that.” Likewise, Tycho took issue with the vast distances to the stars that Copernicus had assumed in order to explain why the Earth's motion produced no visible changes in the appearance of the fixed stars (known as annual stellar parallax). Tycho had measured the apparent sizes of stars (now known to be illusory – see stellar magnitude), and used geometry to calculate that in order to both have those apparent sizes and be as far away as heliocentrism required, stars would have to be huge (the size of Earth's orbit or larger, and thus much larger than the sun). Regarding this Tycho wrote, “Deduce these things geometrically if you like, and you will see how many absurdities (not to mention others) accompany this assumption [of the motion of the earth] by inference.” He said his Tychonic system, which incorporated Copernican features into a geocentric system, “offended neither the principles of physics nor Holy Scripture”. Thus many astronomers accepted some aspects of Copernicus's theory at the expense of others. His model did have a large influence on later scientists such as Galileo and Johannes Kepler, who adopted, championed and (especially in Kepler's case) sought to improve it. However, in the years following publication of \"de Revolutionibus\", for leading astronomers such as Erasmus Reinhold, the key attraction of Copernicus's ideas was that they reinstated the idea of uniform circular motion for the planets.\n\nDuring the 17th century, several further discoveries eventually led to the wider acceptance of heliocentrism:\n\nIn 1725, James Bradley discovered stellar aberration, an apparent annual motion of stars around small ellipses, and attributed it to the finite speed of light and the motion of Earth in its orbit around the Sun.\n\nIn 1838, Friedrich Bessel made the first successful measurements of annual parallax for the star 61 Cygni, of 0.314 arcseconds; which indicated that the star was 10.3 ly away, close to the currently accepted value of 11.4 ly. He narrowly beat Friedrich Georg Wilhelm Struve and Thomas Henderson, who measured the parallaxes of Vega and Alpha Centauri in the same year.\n\nWhether Copernicus' propositions were \"revolutionary\" or \"conservative\" has been a topic of debate in the historiography of science.\nIn his book \"The Sleepwalkers: A History of Man's Changing Vision of the Universe\" (1959), Arthur Koestler attempted to deconstruct the Copernican \"revolution\" by portraying Copernicus as a coward who was reluctant to publish his work due to a crippling fear of ridicule.\nThomas Kuhn argued that Copernicus only transferred \"some properties to the Sun's many astronomical functions previously attributed to the earth.\"\nOther historians have since argued that Kuhn underestimated what was \"revolutionary\" about Copernicus' work, and emphasized the difficulty Copernicus would have had in putting forward a new astronomical theory relying alone on simplicity in geometry, given that he had no experimental evidence.\n\n\n\n\n"}
{"id": "3791271", "url": "https://en.wikipedia.org/wiki?curid=3791271", "title": "DGP model", "text": "DGP model\n\nThe DGP model is a model of gravity proposed by Gia Dvali, Gregory Gabadadze, and Massimo Porrati in 2000. The model is popular among some model builders, but has resisted being embedded into string theory.\n\nThe DGP model assumes the existence of a 4+1-dimensional Minkowski space, within which ordinary 3+1-dimensional Minkowski space is embedded. The model assumes an action consisting of two terms: One term is the usual Einstein–Hilbert action, which involves only the 4-D spacetime dimensions. The other term is the equivalent of the Einstein–Hilbert action, as extended to all 5 dimensions. The 4-D term dominates at short distances, and the 5-D term dominates at long distances.\n\nThe model was proposed in part in order to reproduce the cosmic acceleration of dark energy without any need for a small but non-zero vacuum energy density. But critics argue that this branch of the theory is unstable. However, the theory remains interesting because of Dvali's claim that the unusual structure of the graviton propagator makes non-perturbative effects important in a seemingly linear regime, such as the solar system. Because there is no four-dimensional, linearized effective theory that reproduces the DGP model for weak-field gravity, the theory avoids the vDVZ discontinuity that otherwise plagues attempts to write down a theory of massive gravity.\n\nIn 2008, Fang \"et al.\" argued that recent cosmological observations (including measurements of baryon acoustic oscillations by the Sloan Digital Sky Survey, and measurements of the cosmic microwave background and type 1a supernovae) is in direct conflict with the DGP cosmology unless a cosmological constant or some other form of dark energy is added. However, this negates the appeal of the DGP cosmology, which accelerates without needing to add dark energy.\n\n"}
{"id": "5444229", "url": "https://en.wikipedia.org/wiki?curid=5444229", "title": "Dale Russell", "text": "Dale Russell\n\nDale Alan Russell (born 27 December 1937) is an American-Canadian geologist and palaeontologist. He is currently Research Professor at the Department of Marine Earth and Atmospheric Sciences (MEAS) at North Carolina State University. Dinosaurs he has described include \"Daspletosaurus\", and he was amongst the first paleontologists to consider an extraterrestrial cause (supernova, comet, asteroid) for the extinction of the dinosaurs.\n\nIn 1982, Russell created the \"dinosauroid\" thought experiment, which speculated an evolutionary path for \"Troodon\" if it had not gone extinct in the Cretaceous–Paleogene extinction event 65 million years ago, and had instead evolved into an intelligent being. Russell commissioned a model of his dinosauroid by artist Ron Sequin, and the concept became popular. Various later anthropologists have continued Russell's speculations about intelligent \"Troodon\"-like dinosaurs, though they often find his original idea too anthropomorphic.\n"}
{"id": "23005120", "url": "https://en.wikipedia.org/wiki?curid=23005120", "title": "David Blair (information technologist)", "text": "David Blair (information technologist)\n\nDavid Clark Blair (May 23, 1947- May 15, 2011). Born in Salem, Oregon was Professor of Business Information Technology at Stephen M. Ross School of Business, University of Michigan. He received Ph.D. from the University Of California, Berkeley, and M.S. from the University Of Washington.\n\nHis research focused on the linguistic representation and associative searching in document retrieval, using a relational database, evaluating information retrieval theories, the management of information in corporate lawsuits, and the document-based decision support.\n\n\n\n"}
{"id": "8040776", "url": "https://en.wikipedia.org/wiki?curid=8040776", "title": "Defensible space theory", "text": "Defensible space theory\n\nThe defensible space theory of architect and city planner Oscar Newman encompasses ideas about crime prevention and neighborhood safety. The theory developed in the early 1970s, and he wrote his first book on the topic, \"Defensible Space\" in 1972. The book contains a study from New York that pointed out that higher crime rate existed in high-rise apartment buildings than in lower housing projects. This, he concluded, was because residents felt no control or personal responsibility for an area occupied by so many people. Throughout his study, Newman focused on explaining his ideas on social control, crime prevention, and public health in relation to community design.\n\nAs defined in Newman's book \"Design Guidelines for Creating Defensible Space\", defensible space is \"a residential environment whose physical characteristics—building layout and site plan—function to allow inhabitants themselves to become key agents in ensuring their security.\" He goes on to explain that a housing development is only defensible if residents intend to adopt this role, which is defined by good design: \"Defensible space therefore is a sociophysical phenomenon,\" says Newman. Both society and physical elements are parts of a successful defensible space.\n\nThe theory argues that an area is safer when people feel a sense of ownership and responsibility for that piece of a community. Newman asserts that \"the criminal is isolated because his turf is removed\" when each space in an area is owned and cared for by a responsible party. If an intruder can sense a watchful community, he feels less secure committing his crime. The idea is that crime and delinquency can be controlled and mitigated through environmental design.\n\nThere are five factors that make a defensible space:\n\nThe concept of defensible space is controversial. A United States Department of Justice experiment in Hartford, Connecticut closed streets and assigned police teams to certain neighborhoods. New public housing projects were designed around ideas of limited access to the city, but Hartford did not show any dramatic drop in crime. Yet, the private places of St. Louis do have much lower crime than public streets. The reason appears to be that in St. Louis, people had the capacity and incentives to defend their defensible spaces. Residents had the right to ask an unwelcome individual (i.e. not a resident or guest) to leave their street, because they jointly owned it. On public streets, one cannot legally act against someone until they have committed a crime.\n\nThe intention of physical features is to create a sense of territorialism in community members which will ensure a safe living environment for those that care for it. Defensible space works with a hierarchy of living and community spaces. According to the theory, housing developments that evoke territorialism are “the strongest deterrents to criminal and vandal activity.” Housing should be grouped in such a way that members feel a mutual benefit. Also to deter crime, areas should be defined for function, paths should be defined for movement, outdoor areas should be juxtaposed with homes, and indoor spaces should visually provide for close watch of outside areas. \n\nNewman holds that through good design, people should not only feel comfortable questioning what is happening in their surroundings, but they should feel obligated to do so. Any intruder should be able to sense the existence of a watchful community and avoid the situation altogether. Criminals fear the likelihood that a resident, upon viewing the intrusion, would then question their actions. This is highly effective in neighborhoods that cannot afford a professional crime watch. \n\nThe defensible space theory is applicable to any type of planned space. From low density housing to high rises, the key is the development of a communal area in which residents can “extend the realm of their homes and the zone of felt responsibility.” Circulation paths and common entry are important aspects of defensible design as well. Residents must also feel a need to extend their protective attitudes to locations where property and urban streets and surroundings connect. The interfacing between private property and community space should be protected similarly.\nNewman’s intent in creating the principles of defensible space is to give the residents of a community control of public spaces that they formerly felt were out of reach. In effect, residents care enough for their area to protect it from crime as they would protect their own private property.\n\nOscar Newman’s basic five principles of designing defensible space as quoted in Design Guidelines for Creating Defensible Space are as follows:\n\n\nTo create a defensible space community, residential areas should be subdivided into smaller entities of similar families because control is enhanced. Responsibility for the area is more easily assumed in a smaller group of families as opposed to a larger community. Smaller groups more frequently use an area geared toward them. The number of activities in the space is increased; thus, a feeling of ownership and a need to protect the property follows. On the other hand, when larger groups use a community space, no one has control over the area, and an agreement over its acceptable uses is often in dispute.\n\nThe defensible space theory was largely popular in city design from its emergence until the 1980s. Some of his basic ideas are still taken into consideration presently, and all contemporary approaches and discussions of the relationship between crime and house design use Newman's theory as a critical point of reference. Although modifications were made to the original theory in the 1980s, Newman's basic principles still exist in design, and have been used by the United States Department of Housing and Urban Development as \"both a criminological concept and a proven strategy for enhancing our Nation's quality of life\".\n\nIn the HBO miniseries Show Me a Hero, Newman, a recurring character, articulates his theory of defensible space to Judge Sand as they are trying to plan where to place two hundred new units of desegregated public housing in the city of Yonkers, New York.\n\n\n"}
{"id": "4343217", "url": "https://en.wikipedia.org/wiki?curid=4343217", "title": "ELODIE spectrograph", "text": "ELODIE spectrograph\n\nELODIE was an echelle type spectrograph installed at the Observatoire de Haute-Provence 1.93m reflector in south-eastern France for the Northern Extrasolar Planet Search. Its optical instrumentation was developed by André Baranne from the Marseille Observatory. The purpose of this instrument was extrasolar planet detection by the radial velocity method. This instrument was also used for the M-Dwarf Programmes.\n\nELODIE first light was achieved in 1993. ELODIE was decommissioned in August 2006 and replaced in September 2006 by SOPHIE, a new instrument of the same type but with improved features.\n\nThe electromagnetic spectrum wavelength range is 389.5 nm to 681.5 nm in a single exposure, split into 67 spectral orders. The instrument, which was located in a temperature-controlled room, was fed with optical fibers from the Cassegrain focus. One of the unique features of ELODIE was an integrated data reduction pipeline which fully reduces the spectra immediately after acquisition and allows the user to measure highly accurate radial velocities through cross-correlation with a numerical mask. This accuracy can reach ±7 m/s.\n\nOver 34,000 spectra have been taken with this instrument so far, over 20,000 of which are publicly available through a dedicated on-line archive. ELODIE was the result of a collaboration between the observatories of Haute-Provence, Geneva and Marseille. A publication describing the instrument appeared in \"Astronomy & Astrophysics Supplements\".\n\nThe first extrasolar planet to be discovered orbiting a Sun-like star, 51 Pegasi b, was discovered in 1995 using this instrument. Over twenty such planets have been found with ELODIE. This instrument was also used to find a planet by the transit method.\n\n\n"}
{"id": "476351", "url": "https://en.wikipedia.org/wiki?curid=476351", "title": "Ekman number", "text": "Ekman number\n\nThe Ekman number (Ek) is a dimensionless number used in fluid dynamics to describe the ratio of viscous forces to Coriolis forces. It is frequently used in describing geophysical phenomena in the oceans and atmosphere in order to characterise the ratio of viscous forces to the Coriolis forces arising from planetary rotation. It is named after the Swedish oceanographer Vagn Walfrid Ekman.\n\nWhen the Ekman number is small, disturbances are able to propagate before decaying owing to low frictional effects. The Ekman number also describes the order of magnitude for the thickness of an Ekman layer, a boundary layer in which viscous diffusion is balanced by Coriolis effects, rather than the usual convective inertia.\n\nIt is defined as:\n\n- where \"D\" is a characteristic (usually vertical) length scale of a phenomenon; \"ν\", the kinematic eddy viscosity; Ω, the angular velocity of planetary rotation; and φ, the latitude. The term 2 Ω sin φ is the Coriolis frequency.\nIt is given in terms of the kinematic viscosity, \"ν\"; the angular velocity, Ω; and a characteristic length scale, \"L\".\n\nThere do appear to be some differing conventions in the literature.\n\nTritton gives:\n\nIn contrast, the NRL Plasma Formulary gives:\n\nwhere Ro is the Rossby number and Re is the Reynolds number.\n"}
{"id": "56087330", "url": "https://en.wikipedia.org/wiki?curid=56087330", "title": "Emily Willoughby", "text": "Emily Willoughby\n\nEmily Willoughby is an American paleoartist, illustrator, writer, and PhD student in behavior genetics living in Minneapolis, Minnesota. Willoughby is best known for specializing in illustrating maniraptoran dinosaurs, although she has also done illustrations of ceratopsian dinosaurs and ankylosaurians for the nonprofit Institute for the Study of Mongolian Dinosaurs.\n\nWilloughby is noted for her interest in birds, dromaeosaurids and other feathered animals. She has done illustrations of many feathered theropods for scientific papers, most notably of \"Dakotaraptor steini\" in 2015.\n\nWilloughby, along with Jonathan Kane, T. Michael Keesey, Glenn Morton and James Comer, also authored \"God's Word or Human Reason?\", a 2017 book detailing the relationship between religion and science, in which the authors argue that there is no need for a Christian lifestyle to be incompatible with scientific consensus. Many of the authors talk about their former creationist lifestyles and beliefs throughout the book.\n\nWilloughby's art is mentioned and featured commonly in various programs and dinosaur books, such as Paul Barrett and Darren Naish's \"Dinosaurs: How They Lived and Evolved 2016\".\n"}
{"id": "4178263", "url": "https://en.wikipedia.org/wiki?curid=4178263", "title": "Exogamous group", "text": "Exogamous group\n\nExogamous group is a section of society within which marriages are prohibited. A marriage within an exogamous group is regarded as incestuous.\n\nExamples of exogamous groups are: \n\n\n"}
{"id": "32759787", "url": "https://en.wikipedia.org/wiki?curid=32759787", "title": "Exploration of the Pacific", "text": "Exploration of the Pacific\n\nPolynesians reached nearly all the Pacific islands by about 1200 AD, followed by Asian navigation in Southeast Asia and West Pacific. Around the Middle Ages Muslim traders linked the Middle East and East Africa to the Asian Pacific coasts (to southern China and much of the Malay Archipelago). The direct contact of European fleets with the Pacific began in 1512, with the Portuguese, on its western edges, followed by the Spanish discovery of the Pacific from the American coast.\n\nIn 1521 a Spanish expedition led by the Portuguese navigator Ferdinand Magellan was the first known crossing of the Pacific Ocean, who then named it the \"peaceful sea\". Starting in 1565 with the voyage of Andres de Urdaneta and for the next 250 years, the Spanish controlled the transpacific trade with the Manila galleons that crossed from Mexico to the Philippines and vice versa, until 1815. Other expeditions from Mexico and Peru discovered various archipelagos in the North and South Pacific. In the 17th and 18th centuries, other European powers sent expeditions to the Pacific, namely the Dutch Republic, England, France, and Russia.\n\nHumans reached Australia by at least 40,000 BC which implies some degree of water crossing. People were in the Americas before 10,000 BC. One theory holds that they travelled along the coast by canoe.\n\nAbout 3000 BC speakers of the Austronesian languages, probably on the island of Taiwan, mastered the art of long-distance canoe travel and spread themselves, or their languages, south to the Philippines and Indonesia and east to the islands of Micronesia and Melanesia. The Polynesians branched off and occupied Polynesia to the east. Dates and routes are uncertain, but they seem to have started from the Bismarck Archipelago, went west past Fiji to Samoa and Tonga about 1500 BC. By 100 AD they were in the Marquesas Islands and 300-800 AD in Tahiti (Tahiti is west of the Marquesas.) 300-800 AD is also given for their arrival at Easter Island, their easternmost point and the same date range for Hawaii, which is far to the north and distant from other islands. Far to the southwest, New Zealand was reached about 1250 AD. The Chatham Islands, about 500 miles east of New Zealand were reached about 1500. The fact that some Polynesians possessed the South American Sweet potato implies that they may have reached the Americas or, conversely, that people from the Americas may have reached Polynesia. Thor Heyerdahl's \"Kon-Tiki\" expedition successfully demonstrated that the trip from the Americas to Polynesia using only materials and technology available at the time was at least possible.\n\nOn the Asian side long-distance trade developed all along the coast from Mozambique to Japan. Trade, and therefore knowledge, extended to the Indonesian Islands but apparently not Australia. By at the latest 878 when there was a significant Islamic settlement in Canton much of this trade was controlled by Arabs or Muslims. In 219 BC Xu Fu sailed out into the Pacific searching for the elixir of immortality. From 1404-33 Zheng He led expeditions into the Indian Ocean.\n\nAn interesting issue is Japanese fishing boats. If one was blown out to sea and lacked proper equipment it could be carried by the current all the way to North America. Japanese boats reached Acapulco in 1617, the Aleutians in 1782, Alaska in 1805, the mouth of the Columbia River in 1820, and Cape Flattery in 1833. Such trips may have taken place before Europeans were present in those areas to make detailed records of them.\n\nThe first contact of European navigators with the western edge of the Pacific Ocean was made by the Portuguese expeditions of António de Abreu and Francisco Serrão, via the Lesser Sunda Islands, to the Maluku Islands, in 1512, and with Jorge Álvares's expedition to southern China in 1513, both ordered by Afonso de Albuquerque from Malacca.\n\nSpanish explorer Balboa was the first European to sight the Pacific from America in 1513 after his expedition crossed the Isthmus of Panama and reached a new ocean. He named it \"Mar del Sur\" (literally, \"Sea of the South\" or \"South Sea\") because the ocean was to the south of the coast of the isthmus where he first observed the Pacific. Later, Portuguese explorer Ferdinand Magellan sailed the Pacific East to West on a Castilian (\"Spanish\") expedition of world circumnavigation starting in 1519. Magellan called the ocean \"Pacífico\" (or \"Pacific\" meaning, \"peaceful\") because, after sailing through the stormy seas off Cape Horn, the expedition found calm waters. The ocean was often called the \"Sea of Magellan\" in his honor until the eighteenth century.\n\nFrom 1565 to 1815, a Spanish transpacific route known as the Manila galleons regularly crossed from Mexico to the Philippines and back. On the Asian side the Portuguese and later the Dutch built a regular trade from the East Indies to Japan. On the American side Spanish power stretched thousands of miles from Mexico to Chile. The vast central Pacific was visited only by the Manila galleons and an occasional explorer. The south Pacific was first crossed by Spanish expeditions in the 16th century who discovered many islands including Tuvalu, the Marquesas, the Cook Islands, the Solomon Islands, and the Admiralty Islands, and later the Pitcairn and Vanuatu archipelagos.\n\nThe Pacific recognized: Europeans knew that there was a vast ocean to the west, and the Chinese knew that there was one to the east. Learned Europeans thought that the world was round and that the two oceans were one. In 1492 Columbus sailed west to what he thought was Asia. When Pedro Álvares Cabral, en route to Asia via the Atlantic and the Indian oceans, reached Brazil, in 1500, the true extent of the Americas began to become known. The Martin Waldseemüller map of 1507 was the first to show the Americas separating two distinct oceans. This guess was confirmed in 1513 when Balboa crossed Panama and found salt water. The Magellan expedition of 1519-22 proved that there was one continuous ocean from the Americas to Asia. The Diogo Ribeiro map of 1529 was the first to show the Pacific at about its proper size.\n\nThe coast of Asia: The first European to see the Pacific Ocean was probably Marco Polo about 1292. The Portuguese reached India in 1498, conquered Malacca in 1511 and in 1512 António de Abreu and Francisco Serrão reached the Spice Islands. In May 1513 Jorge Álvares reached southern China and in the same year Balboa crossed Panama. In 1525 Diogo da Rocha and Gomes de Sequeira reached the Caroline Islands, and Jorge de Menezes in 1526-27 landed on the \"Islands of Don Jorge de Menezes\", in the northwest coast of New Guinea (now part of Indonesia), and named the region \"Ilhas dos Papuas\" and is thus credited with the European \"discovery\" of Papua. In 1542 Fernão Mendes Pinto reached Japan. From about 1543 until 1614, the Portuguese monopolize the trade between China and Japan, through the nanban trade. In 1589, João da Gama reached Hokkaido and possibly sighted the Kuril islands, crossing the Pacific further north of the routes usually taken until then. The land that he eventually discovered northeast of Japan, has since become a matter of legend and controversy. \n\nOne hundred years after the Spanish and Portuguese the Dutch Republic began its remarkable expansion. The Dutch reached the East Indies in 1596, the Spice Islands in 1602 and in 1619 founded Batavia. In 1600 a Dutch fleet reached Japan from the Strait of Magellan. The Dutch had little success in China but established themselves at Hirado, Nagasaki in 1609 and monopolized the Japan trade from 1639. In 1639 Matthijs Quast and Abel Tasman searched the empty ocean east of Japan looking for two islands called 'Rica de Oro' and 'Rica de Plata'. In 1643 Maarten Gerritsz Vries reached and charted Sakhalin and the Kuril Islands. In 1653 Hendrick Hamel was shipwrecked in Korea. At about this time the Russians reached the Pacific overland via Siberia (see below). It is significant that the Russian and Dutch trades were never linked since Siberian furs might easily have been exported to China at great profit.\nMagellan and the Manila Galleons: In 1519 Ferdinand Magellan sailed down the east coast of South America, found and sailed through the strait that bears his name and on 28 November 1520 entered the Pacific. He then sailed north and caught the trade winds which carried him across the Pacific to the Philippines where he was killed. One surviving ship returned west across the Indian Ocean and the other went north in the hope of finding the westerlies and reaching Mexico. Unable to find the right winds, it was forced to return to the East Indies. In 1565 (44 years later) Andrés de Urdaneta found a wind system that would reliably blow a ship eastward back to the Americas. From then until 1815 the annual Manila Galleons crossed the Pacific from Mexico to the Philippines and back, exchanging Mexican silver for spices and porcelain. Until the time of Captain Cook these were the only large ships to regularly cross the Pacific. The route was purely commercial and there was no exploration of the areas to the north and south. In 1668 the Spanish founded a colony on Guam as a resting place for west-bound galleons. For a long time this was the only non-coastal European settlement in the Pacific.\n\nSouth America: In 1513, six years before Magellan, Spanish explorer Vasco Núñez de Balboa crossed the Isthmus of Panama and saw the Pacific Ocean. In 1517-18 two ships were built on the Pacific coast. In 1522 Pascual de Andagoya sailed the coast as far as Ecuador. In 1532 Francisco Pizarro conquered Peru. A regular trade developed that carried Peruvian silver up the coast to Panama where it was carried overland to the Caribbean and part to Spain. Spanish settlement extended as far south as central Chile. In 1557-8 Juan Fernández Ladrillero discovered the Juan Fernandez islands and explored the Chilean coast down to the Strait of Magellan.\n\nThe South Pacific: Several Spanish expeditions were sent from South America across the Pacific Ocean in the 16th and early 17th centuries. They all used the southern trade winds. In 1567/68 Álvaro de Mendaña de Neira sailed from Peru to the Solomon Islands. In 1595 he tried again and reached the Santa Cruz Islands (eastern Solomons toward Fiji). He died there and the survivors reached the Philippines. In 1606 Pedro Fernandes de Queirós reached Vanuatu south of the Solomons. He continued exploring and eventually sailed back to Mexico. One of his separated ships under Luis Vaz de Torres sailed west and discovered the strait that bears his name sighting the northern tip of Australia. Other Spanish expeditions discovered Tuvalu, the Marquesas, the Cook Islands, the Admiralty Islands and the Pitcairn. In 1722 the Dutchman Jacob Roggeveen sailed from Cape Horn to Batavia and discovered Easter Island and Samoa.\n\nCape Horn: Six years after Magellan, in 1526, one of the ships of the Loaísa Expedition sailed through the Strait of Magellan and followed the coast north to Mexico. In 1578 Francis Drake passed through the Strait, sailed north raiding Spanish ships and put in somewhere on the coast of California. In 1580 Pedro Sarmiento de Gamboa, who was hunting for Drake, was the first to sail from the Strait to Europe. In 1587 Thomas Cavendish followed Drake, captured a Manila galleon and returned via the Indian Ocean. In 1599 the first Dutch ships passed through the Strait of Magellan (Will Adams, the first Englishman to reach Japan, was on board). Olivier van Noort followed and became the first Dutch circumnavigator.\n\nIn 1525 Francisco de Hoces, while trying to enter the Strait as part of the Loaisa Expedition, was blown south by a storm and saw what he thought was land's end. In 1578 Drake was blown south on the west side and saw what he thought was open water. In 1616 Willem Schouten sought a more southerly passage and rounded Cape Horn. In 1619 the Garcia de Nodal expedition followed the Dutch and proved that Tierra del Fuego was an island by circumnavigating it. Since the Strait of Magellan is narrow and hard to navigate Cape Horn became the standard route until the opening of the Panama Canal. It is a measure of the difficulty of these seas that it was not until 1820 that anyone went as far south as Antarctica.\n\nNorth America: When the Spanish conquered Mexico in 1521 they gained a stretch of Pacific coast. In 1533, Fortún Ximénez reached Baja California and in 1539 Francisco de Ulloa showed that it was a peninsula, but the myth of an Island of California continued for many years. In 1542 Juan Rodriguez Cabrillo reached a point north of San Francisco. In 1578 Drake landed somewhere on the coast. In 1587 Pedro de Unamuno, coming from the Philippines, stopped at Morro Bay, California. In 1592, Juan de Fuca may have reached Puget Sound.\n\nIn 1595, Sebastian Rodriguez Cermeño (Sebastião Rodrigues Soromenho), commander of the Manila galleon \"San Agustín\", attempted an exploration of the California coast. He reached the continent between Point St. George and Trinidad Head in California, but the galleon was later wrecked in a storm off Drake's Bay and the survivors had to sail the rest of the way back to Mexico in a small launch. The smaller vessel, however, allowed Cermeño to sail closer to the coast and to make useful observations of coastal features. In 1602, Sebastián Vizcaíno re-explored the California coast, one of his ships reaching Oregon. His was the last northward exploration for the next 150 years.\n\nThe Portolà expedition of 1769 began the land exploration of Alta California, following the coast as far north as San Francisco Bay and using the reports of Cermeño and Vizcaíno for guidance.\n\nAfter conquering Mexico the Spanish occupied the southern two thirds of Mexico, all of Central America and the South American coast down to Chile. North of this the land was too dry to support a dense population that could be ruled and taxed. The only exception was the Pueblo peoples far to the north in New Mexico. People like Francisco Vásquez de Coronado penetrated far into the interior and found nothing that the Spanish valued. The Chichimeca country of northern Mexico was slowly absorbed and Baja California began to be settled in 1687. The returning Manila galleons followed the westerlies to the coast of California, but immediately turned south, making only a few attempts to explore the coast. For more see History of the West Coast of North America and Early knowledge of the Pacific Northwest.\n\nAustralia and the southwest: Australia is remarkable for the number of explorers who missed it. There seems to be no record of Indonesian sailors reaching Australia. Some think that the Portuguese reached Australia before 1600 but these theories are difficult to prove. The 1567–1606 Spanish voyages from South America stopped at islands to the east before reaching Australia. The first European to definitely see Australia was Willem Janszoon who in February 1606 reached the Cape York Peninsula and thought it was part of New Guinea. Also in 1606 (June to October) Luis Váez de Torres of the Quiros expedition from South America followed the south coast of New Guinea and passed through the Torres Strait without recognizing Australia. His voyage, and therefore the separation between Australia and New Guinea, was not generally known until 1765. From about 1611 the standard Dutch route to the East Indies was to follow the roaring forties as far east as possible and then turn sharply north to Batavia. Since it was difficult to know longitude some ships would reach the west coast or be wrecked on it. 1616 Dirk Hartog bumped into the west coast and did some exploring. Frederick de Houtman did the same in 1619. In 1623 Jan Carstenszoon followed the south coast of New Guinea, missed Torres Strait and went along the north coast of Australia. In 1643 Abel Tasman left Mauritius, missed Australia, found Tasmania, continued east and found New Zealand, missed the strait between the north and south islands, turned northwest, missed Australia again and sailed along the north coast of New Guinea. In 1644 he followed the south coast of New Guinea, missed the Torres Strait, turned south and mapped the north coast of Australia. In 1688 the English buccaneer William Dampier beached a ship on the northwest coast. In 1696 Willem de Vlamingh explored the southwest coast. In 1699 Dampier was sent to find the east coast of Australia. He sailed along the west coast, went north to Timor, followed the north coast of New Guinea to the Bismarck Archipelago and abandoned his search because his ship had become rotten. Until Captain Cook the east coast was completely unknown and New Zealand had only been seen once.\n\nPacific Islands: See also History of the Pacific Islands\n\nMythical Lands: Europeans had long believed in a Strait of Anian somewhere near Bering Strait. A large and distorted Hokkaido was called 'Ezo', 'Jesso' and many other spellings. One of the Kuril Islands named \"Companies Landt\" by Vries grew into a large mass attached to North America. Joao-da-Gama-Land was thought to be east of Japan. There was an overgrown Puget Sound called \"Grande Mer de l'Ouest\" possibly connected to Hudson Bay. In the far south was a Terra Australis. The map published in Diderot's \"Encyclopédie\" in 1755 is filled with nonsense. In 1875 no less than 123 mythical islands were removed from the Royal Navy chart of the North Pacific.\n\nAlaska and the Russians: The modern period begins with Russian expeditions. They crossed Siberia and reached the Pacific in 1639 (Ivan Moskvitin). In 1644 Vassili Poyarkov found the Amur River. In 1648 Semyon Dezhnyov (probably) entered the Pacific from the Arctic Ocean. In 1652 Mikhail Stadukhin followed the coast of the Sea of Okhotsk. In 1697 Vladimir Atlasov entered the Kamchatka Peninsula overland from the north. In 1716 the first seagoing boats were built to reach Kamchatka from the mainland. In 1728 Vitus Bering sailed from Kamchatka through the strait that bears his name without seeing America. In 1732 Mikhail Gvozdev and Ivan Fedorov (navigator) saw the tip of Alaska from the Bering Strait. In 1741 Vitus Bering and Alexei Chirikov sailed just south of the Aleutian Islands and reached the Alaska panhandle. Peter Kuzmich Krenitzin mapped the Aleutians before 1769. The myth of a land mass north of the Aleutians took a long time to dispel. Russians fur hunters island-hopped along the Aleutians and then along the south coast of Alaska looking mainly for sea otter (Attu at the west end of the Aleutians in 1745, Unalaska Island at the east end in 1759, Kodiak Island 1784, Kenai Peninsula 1785, Yakutat, 1795, Sitka 1799, Fort Ross 1812). North of the Aleutians posts appeared on the west coast after 1819. Spaniards from Mexico met the Russians in 1788. (see below). Russian America was sold to the United States in 1867.\nCaptain Cook: On his first voyage (1768–71) James Cook went to Tahiti from Cape Horn, circumnavigated New Zealand, followed the east coast of Australia for the first time and returned via the Torres Strait and the Cape of Good Hope. On his second voyage (1772–75) he sailed from west to east keeping as far south as possible and showed that there was probably no Terra Australis. On his third voyage (1776–80) he found the Hawaiian Islands and followed the North American coast from Oregon to the Bering Strait, mapping this coast for the first time and showing that there was probably no Northwest passage. Cook was killed in Hawaii in 1779. The expedition made a second attempt at the Bering Strait, stopped at Kamchatka and China and reached England in 1780. Cook set a high standard of scientific exploration, showed that there was no large land mass in the southern ocean, mapped the two largest island groups in the Pacific and by following the east coast of Australia and the west coast of North America closed the last gaps in European knowledge of the Pacific coasts. After Cook everything was detail.\n\nCook's rivals and successors: Several governments sponsored Pacific expeditions, often in rivalry or emulation of Captain Cook. At the time of Cook's first voyage, in 1766-69 Louis Antoine de Bougainville crossed the Pacific and publicized Tahiti and in 1767 Samuel Wallis and Philip Carteret separately crossed the Pacific. In 1785-88 Jean-François de Galaup, comte de Lapérouse followed the American coast from Chile to Alaska, crossed to China, explored northern Japan and Kamchatka, went south to Australia and lost his life in the Santa Cruz Islands. The Malaspina Expedition (1789–1794) visited the American coast, Manila, New Zealand and Australia. In 1792-93 George Vancouver more thoroughly mapped the west coast of Canada. In 1803/6 Adam Johann von Krusenstern led the first Russian circumnavigation and investigated both sides of the North Pacific. In 1820 Fabian Gottlieb von Bellingshausen saw Antarctica. A number of other voyages are listed in \"European and American voyages of scientific exploration.\"\n\nSpain on the west coast of North America: For Europeans in the Age of Exploration western North America was one of the most distant places on earth (9 to 12 months of sailing). Spain had long claimed the entire west coast of the Americas. The area north of Mexico however was given little attention in the early years. This changed when the Russians appeared in Alaska. The Spanish moved north to California and built a series of missions along the Pacific coast including: San Diego in 1767, Monterey, California in 1770 and San Francisco in 1776. San Francisco Bay was discovered in 1769 by Gaspar de Portolà from the landward side because its mouth is not obvious from the sea. The Spanish settlement of San Francisco remained the northern limit of land occupation. By sea, from 1774 to 1793 the Spanish expeditions to the Pacific Northwest tried to assert Spanish claims against the Russians and British. In 1774 Juan José Pérez Hernández reached what is now the south end of the Alaska panhandle. In 1778 Captain Cook sailed the west coast and spent a month at Nootka Sound on Vancouver Island. An expedition led by Juan Francisco de la Bodega y Quadra sailed north to Nootka and reached Prince William Sound. In 1788 Esteban José Martínez went north and met the Russians for the first time (Unalaska and Kodiak Island) and heard that the Russians were planning to occupy Nootka Sound. In 1789 Martinez went north to build a fort at Nootka and found British and American merchant ships already there. He seized a British ship which led to the Nootka Crisis and Spanish recognition of non-Spanish trade on the northwest coast. In 1791 the Malaspina expedition mapped the Alaska coast. In 1792 Dionisio Alcalá Galiano circumnavigated Vancouver Island. In 1792-93 George Vancouver also mapped the complex coast of British Columbia. Vancouver Island was originally named Quadra's and Vancouver's Island in commemoration of the friendly negotiations held by the Spanish commander of the Nootka Sound settlement, Juan Francisco de la Bodega y Quadra and British naval captain George Vancouver in Nootka Sound in 1792. In 1793 Alexander Mackenzie reached the Pacific overland from Canada. By this time Spain was becoming involved in the French wars and increasingly unable to assert its claims on the Pacific coast. In 1804 the Lewis and Clark expedition reached the Pacific overland from the Mississippi River. By the Adams–Onís Treaty of 1819 Spain gave up its claims north of California. Canadian fur traders, and later a smaller number of Americans, crossed the mountains and built posts on the coast. In 1846 the Oregon Treaty divided the Oregon country between Britain and the United States. The United States conquered California in 1848 and purchased Alaska in 1867.\n\nNortheast: The Russians moved south and the Japanese moved north and explored the Kuril Islands and Sakhalin. About 1805 Adam Johann von Krusenstern was apparently the first Russian to reach eastern Siberia by sea from European Russia. In 1808 Mamiya Rinzo explored the coast of Sakhalin. During the Crimean War a British fleet failed to capture Petropavlovsk-Kamchatsky. In 1860 Russia annexed the southeast corner of Siberia from China.\n\nThe Pacific opened to trade and imperialism: After Captain Cook large numbers of European merchant vessels began to enter the Pacific. The reasons for this are not completely clear. On Cook's third voyage furs bought at Nootka were sold in China at a 1,800 percent profit - enough to pay for a trading voyage. The first to do this was James Hanna from Macao in 1785. Robert Gray in 1787 was the first American. This Maritime fur trade reached its peak about 1810, drew many ships into the Pacific and drew Canadians and Americans to the coast. The first Pacific whaling ship left London in 1788 and by the nineteenth century there were hundreds of whaleships in the Pacific each year. Clipper ships cut the sailing time from Europe to the Pacific. England founded a colony in Australia in 1788 and New Zealand in 1840. After about 1800 England began to replace the Dutch Republic along the Asian coast. Hong Kong became a colony in 1839 during the First Opium War, which was also the first time that a large European military and naval force appeared in the Pacific. European ships and sailors disrupted life on the Pacific islands. Most of the Pacific islands were soon claimed by one European power or another.\n\n"}
{"id": "6874571", "url": "https://en.wikipedia.org/wiki?curid=6874571", "title": "Generative second-language acquisition", "text": "Generative second-language acquisition\n\nThe generative approach to second language (L2) acquisition applies theoretical insights developed from within generative linguistics to investigate how second languages are learned by adults and children. Research is conducted in syntax, phonology, morphology, phonetics and semantics.\n\nThe main questions in generative second language acquisition include whether Universal Grammar is available to the adult second language learner to guide acquisition, as it is to the first language learner; whether second language learners can reset linguistic parameters; whether second-language learners experience difficulties interfacing between different modules of the grammar; and whether child second language acquisition differs from that of adults. As generative second language research endeavors to explain the totality of L2 acquisition phenomena, it is also concerned with investigating the extent of linguistic transfer, maturational effects on acquisition, and why some learners fail to acquire a target-like L2 grammar even with abundant input.\n\nResearch in generative second-language acquisition is presented at a range of conferences, including: GASLA (Generative Approaches to Second Language Acquisition), GALANA (Generative Approaches to Language Acquisition - North America), and BUCLD (Boston University Conference on Language Development).\n\nProminent researchers of the topic include Suzanne Flynn of MIT, Bonnie Schwartz of University of Hawaii, Antonella Sorace of University of Edinburgh, and Lydia White of McGill University.\n"}
{"id": "36626070", "url": "https://en.wikipedia.org/wiki?curid=36626070", "title": "Glossary of physics", "text": "Glossary of physics\n\nThis glossary of physics is a list of definitions of terms and concepts relevant to physics, its sub-disciplines, and related fields, including mechanics, materials science, nuclear physics, particle physics, and thermodynamics.\n\nFor more inclusive glossaries concerning related fields of science and technology, see Glossary of chemistry terms, Glossary of astronomy, Glossary of areas of mathematics, and Glossary of engineering.\n\n \n\n \n\n \n\n"}
{"id": "40850094", "url": "https://en.wikipedia.org/wiki?curid=40850094", "title": "Gossip from the Forest (Maitland book)", "text": "Gossip from the Forest (Maitland book)\n\nGossip from the Forest: the Tangled Roots of our Forests and Fairytales is a 2012 book by Sara Maitland about the connections between forests and fairytales in Northern Europe. It is structured around accounts of walks through 12 forests in Scotland and England, one per month of the year, and 12 associated retellings of traditional fairytales, and was published by Granta ().\n\nMaitland has described the book as \"a hybrid book ... history and photographs and nature and politics and science and anthropology and fiction (my own retellings of 12 Grimm stories) and, indeed, gossip.\"\n\n"}
{"id": "33362364", "url": "https://en.wikipedia.org/wiki?curid=33362364", "title": "Graphetics", "text": "Graphetics\n\nGraphetics is a branch of linguistics concerned with the analysis of the physical properties of shapes used in writing. \n\nIt is an etic study, meaning that it has an outsider's perspective and is not concerned with any particular writing system. It is contrasted with the related emic field of graphemics, the study of the relation between different shapes in particular writing systems. Graphetics is analogous to phonetics; graphetics is to the study of writing as phonetics is to the study of spoken language. As such, it can be divided into two areas, \"visual graphetics\" and \"mechanical graphetics\", which are analogous to auditory and articulatory phonetics, respectively. Both printed and handwritten language can be the subject of graphetic study.\n"}
{"id": "50899261", "url": "https://en.wikipedia.org/wiki?curid=50899261", "title": "Ibrahim ibn Said al-Sahli", "text": "Ibrahim ibn Said al-Sahli\n\nIbrahim Ibn Saîd al-Sahlì (11th century) was an Andalusian globe-maker, active from 1050 to 1090.\n\nIbrahim Ibn Saîd al-Sahlì worked in Valencia and Toledo in what is now Spain, and was mentioned in a list of mathematics students in Andalusia in a book written in 1068. Ibrahim Ibn Saîd al-Sahlì created the \"astrolabe of Al-Sahli\", an instrument to determine the positions of the stars on the sky, in the city of Tulaytulah (now Toledo, Spain) in the year 1066. He built four more astrolabes between 1067 and 1086. His first astrolabe was characterized by the peculiarity of its operation, as other astrolabes made in his time were not similar.\n\n"}
{"id": "99965", "url": "https://en.wikipedia.org/wiki?curid=99965", "title": "Index of computing articles", "text": "Index of computing articles\n\nOriginally, the word computing was synonymous with counting and calculating, and the science and technology of mathematical calculations. Today, \"computing\" means using computers and other computing machines. It includes their operation and usage, the electrical processes carried out within the computing hardware itself, and the theoretical concepts governing them (computer science).\n\n\"See also:\" List of programmers, List of computing people, List of computer scientists, List of basic computer science topics, List of terms relating to algorithms and data structures.\n\nTopics on computing include:\n1.TR.6 –\n100BaseFX –\n100BaseTX –\n100BaseT –\n100BaseVG –\n100VG-AnyLAN –\n10base2 –\n10base5 –\n10baseT –\n120 reset –\n16-bit –\n16-bit application –\n16550 UART –\n1NF –\n1TBS –\n\n2.PAK –\n20-GATE –\n20-GATE –\n28-bit –\n2B1D –\n2B1Q –\n2D –\n2NF –\n\n3-tier (computing) –\n32-bit application –\n32-bit –\n320xx microprocessor –\n320xx –\n386BSD –\n386SPART.PAR –\n3Com Corporation –\n3DO –\n3D computer graphics –\n3GL –\n3NF –\n3Station –\n\n4.2BSD –\n404 error –\n431A –\n473L system –\n486SX –\n4GL –\n4NF –\n\n51-FORTH –\n56 kbit/s line –\n5ESS switch –\n5NF –\n5th Glove –\n\n6.001 –\n64-bit –\n680x0 –\n6x86 –\n\n8-bit clean –\n8.3 filename –\n80x86 –\n82430FX –\n82430HX –\n82430MX –\n82430VX –\n8514 (display standard) –\n8514-A –\n88open –\n8N1 –\n8x86 –\n\n90-90 Rule –\n9PAC\n\nABC ALGOL –\nABLE –\nABSET –\nABSYS –\nAccent –\nAcceptance, Test Or Launch Language –\nAccessible Computing –\nAda –\nAddressing mode –\nAIM alliance –\nAirPort –\nAIX –\nAlan –\nALGOL –\nAlgorithm –\nAltiVec –\nAmdahl's law –\nAmerica Online –\nAmiga –\nAmigaE –\nAnalysis of algorithms –\nAOL –\nAPL –\nApple Computer –\nApple II –\nApple Macintosh –\nAppleScript –\nArray programming –\nArithmetic and logical unit –\nASCII –\nActive Server Pages –\nASP.NET –\nAssembly language –\nAtari –\nAtlas Autocode –\nAutoLISP –\nAutomaton –\nAWK –\n\nB –\nBackus–Naur form –\nBasic Rate Interface (2B+D)--\nBASIC –\nBatch job –\nBCPL –\nBefunge –\nBeOS –\nBerkeley Software Distribution –\nBETA –\nBig Mac –\nBig O notation –\nBinary symmetric channel –\nBinary Synchronous Transmission –\nBinary numeral system –\nBit –\nBLISS –\nBlue –\nBlu-ray Disc –\nBlue screen of death –\nBourne shell (sh)\nBourne-Again shell (bash)\nBrainfuck –\nBtrieve –\nBurrows-Abadi-Needham logic –\nBusiness Computing –\n\nC++ –\nC# –\nC –\nCache –\nCanonical LR parser –\nCat (Unix) –\nCD-ROM –\nCentral processing unit –\nChimera –\nChomsky normal form –\nCIH virus –\nClassic Mac OS –\nCOBOL –\nCocoa (software) –\nCode and fix –\nCode Red worm –\nColdFusion –\nColouring algorithm –\nCOMAL –\nComm (Unix) –\nCommand line interface –\nCommand line interpreter –\nCOMMAND.COM –\nCommercial at (computing) –\nCommodore 1541 –\nCommodore 1581 –\nCommodore 64 –\nCommodore Amiga –\nCommon logarithm –\nCommon Unix Printing System –\nCompact disc –\nCompiler –\nComputability theory –\nComputational complexity theory –\nComputation –\nComputer-aided design –\nComputer-aided manufacturing –\nComputer architecture –\nComputer cluster –\nComputer hardware –\nComputer network –\nComputer numbering formats –\nComputer programming –\nComputer science –\nComputer security –\nComputer software –\nComputer system –\nComputer –\nComputing –\nContext-free grammar –\nContext-sensitive grammar –\nContext-sensitive language –\nControl flow –\nControl store –\nControl unit –\nCORAL66 –\nCP/M operating system –\nCPL –\nCracking (software) –\nCracking (passwords) –\nCryptanalysis –\nCryptography –\nCybersquatting –\nCYK algorithm –\nCyrix 6x86 –\n\nD –\nData compression –\nDatabase normalization –\nDecidable set –\nDeep Blue –\nDesktop environment –\nDesktop publishing –\nDeterministic finite automaton –\nDialer -\nDIBOL –\nDiff –\nDigital camera –\nDEC (Digital Equipment Corporation) –\nDigital signal processing –\nDigital visual interface –\nDirect manipulation interface –\nDisk storage –\nDistance transform –\nDistance map –\nDistance field –\nDVD –\nDVI (TeX) –\nDvorak Simplified Keyboard –\nDylan –\n\nEarth Simulator –\nEBCDIC –\nECMAScript (a.k.a. JavaScript) –\nElectronic data processing (EDP) –\nEnhanced Versatile Disc (EVD) –\nENIAC –\nEnterprise Java Beans (EJB) –\nEntscheidungsproblem –\nEquality (relational operator) –\nErlang –\nEnterprise resource planning (ERP) –\nES EVM –\nEthernet –\nEuclidean algorithm –\nEuphoria –\nExploit (computer science) –\n\nFederated Naming Service –\nField specification -\nFinal Cut Pro –\nFinite state automaton –\nFireWire –\nFirst-generation language –\nFloating point unit –\nFloppy disk –\nFormal language –\nForth –\nFortran –\nFourth-generation language –\nFragmentation –\nFree On-line Dictionary of Computing –\nFree Software Foundation –\nFree software movement –\nFree software –\nFreescale 68HC11 –\nFreeware –\nFunction-level programming –\nFunctional programming –\n\nG4 –\nG5 –\nGEM –\nGeneral Algebraic Modeling System –\nGenie –\nGNU -\nGNU bison –\nGnutella –\nGodiva –\nGraphical user interface –\nGraphics Device Interface –\nGreibach normal form –\nG.hn –\n\nhack (technology slang) –\nHacker (computer security) -\nHacker (hobbyist) -\nHacker (programmer subculture) –\nHacker (term) -\nHalting problem –\nHard Drive –\nHaskell –\nHD DVD –\nHistory of computing –\nHistory of computing hardware –\nHistory of Microsoft Windows –\nHistory of operating systems –\nHistory of the graphical user interface –\nHitachi 6309 –\nHome computer –\nHugo –\nHuman-computer interaction –\n\nIA-32 –\nIA-64 –\nIBM PC –\nInteractive computation -\nIBM –\niBook –\niCab –\niCal –\nIcon –\niDVD –\nIEEE 802.2 –\nIEEE 802.3 –\nIEEE floating-point standard –\niMac –\nimage processing –\niMovie –\nInform –\nInstruction register –\nIntel 8008 –\nIntel 80186 –\nIntel 80188 –\nIntel 80386 –\nIntel 80486SX –\nIntel 80486 –\nIntel 8048 –\nIntel 8051 –\nIntel 8080 –\nIntel 8086 –\nIntel 80x86 –\nIntel –\nINTERCAL –\nInternational Electrotechnical Commission –\nInternet Explorer –\nInternet –\niPhoto –\niPod –\niResQ -\nIrreversible circuit -\niSync –\niTunes –\n\nJ –\nJava Platform, Enterprise Edition –\nJava Platform, Micro Edition –\nJava Platform, Standard Edition –\nJava API –\nJava –\nJava Virtual Machine –\nJavaScript –\nJPEG –\n\nK&R –\nKDE –\nKid –\nKilobyte –\nKleene star –\nKlez –\nKRYPTON –\n\nLALR parser –\nLambda calculus –\nLasso –\nLaTeX –\nLeet –\nLegal aspects of computing –\nLex –\nLimbo –\nLinked list –\nLinux –\nLisp –\nList of coding terms –\nList of computing terms that end in \"ware\" –\nList of IBM products –\nList of information technology management topics –\nList of Intel microprocessors –\nList of programming languages –\nList of operating systems –\nList of Soviet computer systems –\nLL parser –\nLogical programming –\nLogo –\nLotus 1-2-3 –\nLR parser –\nLua –\nLynx language –\nLynx browser\n\nm4 –\nmacOS Server –\nmacOS –\nMAD –\nMainframe computer –\nMalware -\nMary –\nMealy machine –\nMegabyte –\nMelissa worm -\nMercury –\nMesa –\nMicrocode –\nMicroprocessor –\nMicroprogram –\nMicrosequencer –\nMicrosoft Windows –\nMicrosoft –\nMiranda –\nML –\nMMC –\nMMU –\nMMX –\nMobile Trin –\nModula –\nMOO –\nMoore's Law –\nMoore machine –\nMorris worm –\nMOS Technologies 6501 –\nMOS Technologies 6502 –\nMOS Technologies 650x –\nMOS Technologies 6510 –\nMotorola 68000 –\nMotorola 6800 –\nMotorola 68020 –\nMotorola 68030 –\nMotorola 68040 –\nMotorola 68060 –\nMotorola 6809 –\nMotorola 680x0 –\nMotorola 68LC040 –\nMotorola 88000 –\nMozilla –\nMPEG –\nMS-DOS –\nMultics –\nMultiprocessing –\nMUMPS\n\n.NET –\nNetBSD –\nNetlib –\nNetscape Navigator –\nNeXT –\nNial –\nNybble –\nNinety-Ninety Rule –\nNon-Uniform Memory Access –\nNondeterministic finite automaton\n\nOberon –\nObjective-C –\nobject –\nOCaml –\noccam –\nOmniWeb –\nOne True Brace Style –\nOpenBSD –\nOpenOffice.org –\nOpen source –\nOpen Source Initiative –\nOpera (web browser) –\nOperating system advocacy –\nOperating system –\n\nPA-RISC –\nPage description language –\nPancake sorting –\nParallax Propeller –\nParallel computing –\nParser (language) –\nParsing (technique) –\nPartial function –\nPascal –\nPDP –\nPeer-to-peer network –\nPerl –\nPersonal computer –\nPHP –\nPILOT –\nPL/I –\nPointer –\nPoplog –\nPortable Document Format (PDF) –\nPoser –\nPostScript –\nPowerBook –\nPowerPC –\nPrefix grammar –\nPreprocessor –\nPrimitive recursive function –\nProgramming language –\nProlog –\nPSPACE-complete –\nPulse code modulation (PCM) –\nPushdown automaton –\nPython –\n\nQuarkXPress –\nQuickTime –\nQWERTY –\n\nRam disk –\nRAM (random access memory) –\nRandom access –\nRascal –\nRatfor –\nRCA 1802 –\nRead only memory (ROM) –\nREBOL –\nRecovery-oriented computing –\nRecursive descent parser –\nRecursion (computer science) –\nRecursive set –\nRecursively enumerable language –\nRecursively enumerable set –\nReference (computer science) –\nReferential transparency –\nRegister –\nRegular expression –\nRegular grammar –\nRegular language –\nRPG –\nRetrocomputing –\nREXX –\nRFC –\nRISC –\nRS/6000 –\nRuby –\n\nSafari (web browser) –\nSAIL –\nScript kiddie –\nScripting language –\nSCSI –\nSecond-generation programming language –\nSecure Sockets Layer –\nsed –\nSelf (or SELF) –\nSemaphore (programming) –\nSequential access –\nSETL –\nShareware –\nShell script –\nShellcode –\nSIMD –\nSimula –\nSircam –\nSlide rule –\nSLIP –\nSLR parser –\nSmalltalk –\nServer Message Block –\nSMBus –\nSMIL (computer) –\nSmiley –\nSNOBOL –\nSoftware engineering –\nSONET –\nSpace-cadet keyboard –\nSPARC International –\nSpecialist computer –\nSPITBOL –\nSQL –\nSQL slammer worm –\nSR –\nSSL –\nService-oriented architecture -\nS/SL –\nStale pointer bug –\nStandard ML (or SML) –\nStateless server –\nStructured programming –\nSubject-oriented programming –\nsubnetwork –\nSupercomputer –\nSwap space –\nSymbolic mathematics –\nSymlink –\nSymmetric multiprocessing –\nSyntactic sugar –\nSyQuest Technology –\nSYSKEY –\nSystemboard –\nSystem programming language –\nIBM System R –\n\nTADS –\nTcl –\nteco –\nText editor –\nTeX –\nThird generation language –\nTimeline of computing 1950–1979 –\nTimeline of computing 1980–1989 –\nTimeline of computing 1990–1999 –\nTimeline of computing hardware 2400 BC–1949 –\nTimeline of computing –\nTk –\nTPU –\nTrac –\nTransparency (computing) –\nTrin II –\nTrin VX –\nTuring machine –\nTuring –\n2B1Q –\n\nUAT –\nUnicode –\nUnicon –\nUnix –\nUnix shell –\nUNIX System V –\nUnLambda –\nUSB –\nUnreachable memory –\n\nVar'aq –\nVAX –\nVBScript –\nVector processor –\nVentura Publisher –\nVery-large-scale integration –\nVideo editing –\nVirtual Memory System –\nVirtual memory –\nVisual Basic –\nVisual FoxPro –\nVon Neumann architecture –\n\nWeb browser –\nWestern Design Center –\nWestern Design Center 65C02 –\nWestern Design Center 65816 –\nWhitespace –\nwiki –\nwindow manager –\nWindows 1.0 –\nWindows 2000 –\nWindows 95 –\nWindows Me –\nWindows NT –\nWindows XP –\nWord processor –\nWorld wide web –\nWYSIWYG –\n\nX Window System –\nX86 –\nX-Mouse –\n\nYacc –\nYaST –\nYet Another –\nYorick –\n\nZ notation –\nZ shell –\nZilog Z80 –\nZooming User Interface –\nZX80 –\nZX81 –\nZX Spectrum\n"}
{"id": "5794146", "url": "https://en.wikipedia.org/wiki?curid=5794146", "title": "Index of structural engineering articles", "text": "Index of structural engineering articles\n\nThis is an alphabetical list of articles pertaining specifically to structural engineering. For a broad overview of engineering, please see List of engineering topics. For biographies please see List of engineers.\n\nA-frame –\nAerodynamics –\nAeroelasticity –\nAir-supported structure –\nAirframe –\nAluminium –\nAnalytical method –\nAngular frequency –\nAngular speed –\nArchitecture –\nArchitectural engineering –\nArch –\nArch bridge\n\nBase isolation –\nBeam –\nBeam axle –\nBending –\nBifurcation theory –\nBiomechanics –\nBoat Building –\nBody-on-frame –\nBox girder bridge –\nBox truss –\nBridge engineering –\nBuckling –\nBuilding –\nBuilding construction –\nBuilding engineering\n\nCable –\nCable-stayed bridge –\nCantilever –\nCantilever bridge –\nCarbon-fiber-reinforced polymer –\nCasing –\nCasting –\nCatastrophic failure –\nCenter of mass –\nChaos theory –\nChassis –\nChimneys –\nCoachwork –\nCoefficient of thermal expansion –\nCoil spring –\nColumns –\nComposite material –\nComposite structure –\nCompression –\nCompressive stress –\nConcrete –\nConcrete cover –\nConstruction –\nConstruction engineering –\nConstruction management –\nContinuum mechanics –\nCorrosion –\nCrane –\nCreep –\nCrumple zone –\nCurvature\n\nDam –\nDamper –\nDamping –\nDead and live loads –\nDeflection –\nDeformation –\nDirect stiffness method –\nDome –\nDouble wishbone suspension –\nDuhamel's integral –\nDynamical system –\nDynamics\n\nEarthquake--\nEarthquake engineering –\nEarthquake engineering research –\nEarthquake engineering structures –\nEarthquake loss –\nEarthquake performance evaluation –\nEarthquake simulation –\nElasticity theory –\nElasticity –\nEnergy principles in structural mechanics –\nEngineering mechanics –\nEuler method –\nEuler–Bernoulli beam equation\n\nFalsework –\nFatigue –\nFibre reinforced plastic –\nFinite element analysis –\nFinite element method –\nFinite element method in structural mechanics –\nFire safety –\nFire protection –\nFire protection engineering –\nFirst moment of area –\nFlexibility method –\nFloating raft system –\nFloor –\nFluid mechanics –\nFootbridges –\nForce –\nFormwork –\nFoundation engineering –\nFracture –\nFracture mechanics –\nFrame –\nFrequency –\nFuselage\n\nGirder –\nGrout\n\nHoist –\nHollow structural section –\nHooke's law –\nHull –\nHurricane-proof building –\nHyperboloid structure\n\nInstitution of Structural Engineers\n\nJoint\n\nLattice tower –\nLever –\nLeaf spring –\nLimit state design –\nLinear elasticity –\nLinear system –\nLinkage –\nLive axle –\nLoad –\nLoad factor\n\nMacPherson strut –\nMasonry –\nMast –\nMaterial science –\nModulus of elasticity –\nMohr-Coulomb theory –\nMonocoque –\nMoment –\nMoment distribution –\nMoment of inertia –\nMortar –\nMoulding\n\nNewton method –\nNewtonian mechanics –\nNon-linear system –\nNumerical analysis -\nNon-persistent joint\n\nOffshore engineering –\nOscillation\n\nPermissible stress design –\nPile –\nPlastic analysis –\nPlastic bending –\nplasticity –\nPoisson's ratio –\nPortland cement –\nPortal frame –\nPrecast concrete –\nPrestressed concrete –\nPressure vessel\n\nRadius of gyration –\nReady-mix concrete –\nRebar –\nReinforced concrete –\nResponse spectrum –\nRetaining wall –\nRigid frame –\nRotation\n\nSecond moment of area –\nSeismic analysis –\nSeismic loading –\nSeismic performance –\nSeismic retrofit –\nSeismic risk –\nShear –\nShear flow –\nShear modulus –\nShear strain –\nShear strength –\nShear stress –\nShear wall –\nShipbuilding –\nShip Construction –\nShock absorbers –\nShotcrete –\nShrinkage –\nSimple machine –\nSkyscraper –\nSlab –\nSolid mechanics –\nSpace frame –\nStatics –\nStatically determinate –\nStatically indeterminate –\nStatistical method –\nSteel –\nStiffness –\nStrand jack –\nStrength of materials –\nStress analysis –\nStress-strain curve –\nStrut –\nStrut bar –\nStructural analysis –\nStructural design –\nStructural dynamics –\nStructural failure –\nStructural health monitoring –\nStructural load –\nStructural mechanics –\nStructural steel –\nStructural system –\nSubframe –\nSuperleggera –\nSuspension (disambiguation page) –\nSuspension bridge\n\nTall building –\nTensile architecture –\nTensile strength –\nTensile stress –\nTensile structure –\nTension –\nTimber –\nTimber framing –\nThermal conductivity –\nThermal shock –\nThermodynamics –\nThermoplastic –\nTruss –\nTruss bridge –\nTorsion –\nTorsion beam suspension –\nTorsion box –\nTower –\nTubular bridge –\nTuned mass damper\n\nUnit dummy force method –\nUnsprung weight\n\nVehicle dynamics –\nVessel –\nVery large floating structures –\nVibration –\nVibration control –\nVirtual work\n\nWall –\nWear –\nWedge –\nWelding –\nWheel and axle\n\nYield strength –\nYoung's modulus\n"}
{"id": "49320014", "url": "https://en.wikipedia.org/wiki?curid=49320014", "title": "Information panspermia", "text": "Information panspermia\n\nInformation panspermia is a concept on the possibility of life travel in the universe by means of transmission of a compressed information on the bases of life e.g. genome coding, which then can enable the recovery of an intelligent life.\n\nThe concept is invented and coined by Vahe Gurzadyan, which then is attributed by Stephen Webb as Solution 23 to Fermi paradox:\n\n\"The Armenian mathematical physicist Vahe Gurzadyan has posited an interesting hypothesis: we might inhabit a Galaxy “full of traveling life streams” strings of bits beamed throughout space.\"\n\nKolmogorov complexity is defined as the length of the computer program which enables the complete recovery of an object. Gurzadyan showed that the complexity of the human genome is relatively low due to non-random parts in the genomic sequences. Moreover, he noticed that since the genomic information on the terrestrial life, starting from bacteria up to humans, contains essential common parts, the entire terrestrial life information can be compressed and transmitted, as he estimated, to over Galactic distances via Arecibo-type antenna. Von Neumann automata network or some mechanism can perform the decoding of the information package. Within this concept one can even assume that the terrestrial life itself might be a result of such an information package.\n\nInformation panspermia has been discussed by Gurzadyan and Roger Penrose within the scheme of Conformal Cyclic Cosmology, i.e. the possibility of transmission of information from pre-Big Bang aeon to ours via the cosmic microwave background radiation.\n\nThis concept assumes a different strategy of the study of the cosmic signals based on universal compressing and decoding principles.\n"}
{"id": "4448772", "url": "https://en.wikipedia.org/wiki?curid=4448772", "title": "International Standard Classification of Occupations", "text": "International Standard Classification of Occupations\n\nThe International Standard Classification of Occupations (ISCO) is an International Labour Organization (ILO) classification structure for organizing information on labour and jobs. It is part of the international family of economic and social classifications of the United Nations. The current version, known as ISCO-08, was published in 2008 and is the fourth iteration, following ISCO-58, ISCO-68 and ISCO-88.\n\nThe ILO describes the purpose of the ISCO classification as:\na tool for organizing jobs into a clearly defined set of groups according to the tasks and duties undertaken in the job. It is intended for use in statistical applications and in a variety of client oriented applications. Client oriented applications include the matching of job seekers with job vacancies, the management of short or long term migration of workers between countries and the development of vocational training programmes and guidance.\n\nThe ISCO is the basis for many national occupation classifications as well as applications in specific domains such as reporting of teaching, agricultural and healthcare workforce information. The ISCO-08 revision is expected to be the standard for labour information worldwide in the coming decade, for instance as applied to incoming data from the 2010 Global Round of National Population Censuses.\n\nThe ISCO-08 divides jobs into 10 major groups: \n\nEach major group is further organized into sub-major, minor and unit (not shown) groups. The basic criteria used to define the system are the skill level and specialization required to competently perform the tasks and duties of the occupations.\n\nManagers\n\n\nProfessionals\n\n\nTechnicians and associate professionals\n\n\nClerical support workers\n\n\nService and sales workers\n\n\nSkilled agricultural, forestry and fishery workers\n\n\nCraft and related trades workers\n\n\nPlant and machine operators and assemblers\n\n\nElementary occupations\n\n\nArmed forces occupations\n\n\n\n\n"}
{"id": "50852731", "url": "https://en.wikipedia.org/wiki?curid=50852731", "title": "Katrin Linse", "text": "Katrin Linse\n\nKatrin Linse is a German Antarctic marine biologist, best known for her work on discovering new Antarctic and deep sea species.\n\nLinse was born in Germany. She discovered marine biology at the age of six, thanks to undergraduates she encountered while on holiday with her family. She developed the goal of working in polar science at the age of twelve, when a national polar research vessel was being built in a shipyard near to her family home: forbidden from visiting the shipyard during its open day, a were all women and children, Linse announced that one day she would sail aboard the ship to study marine life in the Antarctic. At the age of twenty-six, she did so for the first time.\n\nLinse received her undergraduate degree with \"magna cum laude\" honors from Kiel University, where she also completed her master's degree. She completed her PhD at the University of Hamburg and the Zoology Museum, where she compared molluscs collected from the tip of South America with those found in Antarctica.\n\nLinse is a Senior Biodiversity Biologist at the British Antarctic Survey, where she began her career in 2001. She is a distinguished marine benthic biologist with over fifteen years’ research experience in the biodiversity, phylogeography and evolution of Antarctic marine invertebrates. She has participated in over 13 ship-based expeditions, including several that studied hydrothermal habitats in the Southern Ocean and numerous that have impacted global knowledge of the Southern Ocean and Antarctica. She has led or contributed to the discovery of dozens of new species, as she works toward what she has referred to as a “census of marine life in the Southern Ocean.”\n\nAmong Linse’s contributions to Antarctic science is her work leading the team that designed the first georeferenced Antarctic benthos database, which resulted in new Antarctic provinces and an updated biogeography for the Southern Ocean. Her research on phylogenetic relationships of current Antarctic species and their evolutionary histories contributed to the discovery of high biodiversity in the bathyal and abyssal Antarctic deep sea, and influenced the discovery of the first hydrothermal vents in the Southern Ocean. She also has led or contributed to numerous teams that discovered new species, including a new family of deep-sea starfish living in the warm waters around a hydrothermal vent in the East Scotia Ridge in the Southern Ocean; a new species of bug (\"Jaera tyleri)\", similar in appearance to the common woodlouse, which was found plastered all over a whale carcass on the floor of the deep Southern Ocean; and more than thirty new species of marine life discovered during an expedition to the Amundsen Sea off Pine Island Bay in Antarctica. She also was part of the BAS team that made the first comprehensive “inventory” of sea and land animals around a group of Antarctic islands, South Orkney Islands, near the tip of the Antarctic Peninsula.\n\n\n"}
{"id": "38790484", "url": "https://en.wikipedia.org/wiki?curid=38790484", "title": "List of Satish Dhawan Space Centre launches", "text": "List of Satish Dhawan Space Centre launches\n\nThe following list gives a detailed record of the launches taken place in Satish Dhawan Space Centre. It is the main satellite launch centre for the Indian Space Research Organisation (ISRO). It is located in Sriharikota, Andhra Pradesh, north of Chennai. Originally called Sriharikota Range (SHAR), an acronym that ISRO has retained to the present day. The centre was renamed in 2002 after the death of ISRO's former chairman Satish Dhawan.\n\nAs of 14 November 2018, there have been a total of 67 launches, including 53 successful launches, 4 partial successes, and 9 failed launches.\n\n\n\n"}
{"id": "43190276", "url": "https://en.wikipedia.org/wiki?curid=43190276", "title": "List of oil and gas fields of the Barents Sea", "text": "List of oil and gas fields of the Barents Sea\n\nThis list of oil and gas field of the Barents Sea contains links to the major oil and gas fields beneath the Barents Sea. The Barents Sea is bordered by the north Norwegian and Russian coasts, the Novaya Zemlya, Franz Josef Land and Svalbard archipelagos, and the eastern margin of the Norwegian Sea.\n\nIn terms of hydrocarbon exploration, the area is divided into the Russian side on the East and the Norwegian side on the West, with the two also showing quite different petroleum geology. The Russian side contains a number of very large gas fields contained in Upper Jurassic sandstone reservoirs, bounded by anticline structures, whereas the Norwegian side contains much smaller Middle-Lower Jurassic sandstone reservoirs bounded by fault blocks. Some of the largest discoveries were made in the 1980s but have still not been brought into production because of a combination of the extreme conditions, political instability and the historical low price of gas.\n\n\n\n"}
{"id": "15428814", "url": "https://en.wikipedia.org/wiki?curid=15428814", "title": "List of scientific equations named after people", "text": "List of scientific equations named after people\n\nThis is a list of scientific equations named after people (eponymous equations).\n"}
{"id": "4108833", "url": "https://en.wikipedia.org/wiki?curid=4108833", "title": "List of sociology journals", "text": "List of sociology journals\n\nThis list presents representative academic journals covering sociology and its various subfields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "3522475", "url": "https://en.wikipedia.org/wiki?curid=3522475", "title": "List of synchrotron radiation facilities", "text": "List of synchrotron radiation facilities\n\nThis is a table of synchrotrons and storage rings used as synchrotron radiation sources, and free electron lasers.\n\n"}
{"id": "7120093", "url": "https://en.wikipedia.org/wiki?curid=7120093", "title": "List of volcanoes in Italy", "text": "List of volcanoes in Italy\n\nThis is a list of active and extinct volcanoes in Italy.\n\n\n"}
{"id": "55163082", "url": "https://en.wikipedia.org/wiki?curid=55163082", "title": "Martin Gellert", "text": "Martin Gellert\n\nMartin Frank Gellert (born 1929) is a Czechoslovak-born American molecular biologist who is a past president of the American Society for Biochemistry and Molecular Biology. He graduated from Harvard University in 1950 with an A.B. His doctorate was completed at Columbia University in 1956.\nIn 1985 he won the Richard Lounsbery Award jointly with Thomas Maniatis for \"their seminal contributions to our understanding of the structure and function of DNA, which were essential and fundamental to the development of recombinant DNA techniques.\"\n"}
{"id": "31761777", "url": "https://en.wikipedia.org/wiki?curid=31761777", "title": "Neutron Time Of Flight", "text": "Neutron Time Of Flight\n\n\"This article is about one specific instrument. For generic information about the spectroscopic method, see Time of flight. For time-of-flight instruments in neutron scattering, see Neutron time-of-flight scattering.\"\n\nThe Neutron Time Of Flight (n-TOF) facility is a neutron spectrometer at CERN. It consists of a pulsed source, a flight path of 200 m length, and a detector systems. Neutron energies are deduced from the time of flight between source and detector; hence the name of the facility. The neutrons are produced by neutron spallation; by directing a pulsed beam of protons from the Proton Synchrotron (PS) towards a lead target about 300 neutrons expelled for each impact of a proton. The neutrons are slowed down after being emitted, first by the lead target and afterwards by a slab containing water. This results in a wide range of neutron energies since some neutrons will slow down more than others when passing through the targets. Finally, the neutrons are ejected through the 200m long flight path before they arrive at an experimental area.\n\n"}
{"id": "2627025", "url": "https://en.wikipedia.org/wiki?curid=2627025", "title": "Obligatory passage point", "text": "Obligatory passage point\n\nThe concept of Obligatory passage point (OPP) was developed by sociologist Michel Callon in a seminal contribution to actor–network theory: Callon, Michel (1986), \"Elements of a sociology of translation: Domestication of the Scallops and the Fishermen of St Brieuc Bay\". \"In\" John Law (Ed.), \"Power, Action and Belief: A New Sociology of Knowledge?\" London, Routledge: 196-233.\n\nObligatory passage points are a feature of actor-networks, usually associated with the initial (problematization) phase of a translation process. An OPP can be thought of as the narrow end of a funnel, that forces the actors to converge on a certain topic, purpose or question. The OPP thereby becomes a necessary element for the formation of a network and an action program. The OPP thereby mediates all interactions between actors in a network and defines the action program. Obligatory passage points allow for local networks to set up negotiation spaces that allow them a degree of autonomy from the global network of involved actors. \n\nIf a project is unable to impose itself as a strong OPP between the global and local networks, it has no control over global resources such as financial and political support, which can be misused or withdrawn. Additionally, a weak OPP is unable to take credit for the successes achieved within the local network, as outside actors are able to bypass its control and influence the local network directly. \n\nAn action program can comprise a number of different OPP's. An OPP can also be redefined as the problematization phase is revisited.\n\nIn Callon and Law's '\"Engineering and Sociology in a Military Aircraft Project\" the project management of a project to design a new strategic jet fighter for the British Military became an obligatory passage point between representatives of government and aerospace engineers.\n\nIn recent years, the notion of the Obligatory Passage Point has taken hold in the Information Systems Security and Information Privacy disciplines and journals. Backhouse et al. (2006) illustrated how practices and policies are standardized and institutionalized through OPP.\n"}
{"id": "649307", "url": "https://en.wikipedia.org/wiki?curid=649307", "title": "Operation Sandstone", "text": "Operation Sandstone\n\nOperation Sandstone was a series of nuclear weapon tests in 1948. It was the third series of American tests, following Trinity in 1945 and Crossroads in 1946, and preceding Ranger. Like the Crossroads tests, the Sandstone tests were carried out at the Pacific Proving Grounds, although at Enewetak Atoll rather than Bikini Atoll. They differed from \"Crossroads\" in that they were conducted by the Atomic Energy Commission, with the armed forces having only a supporting role. The purpose of the Sandstone tests was also different: they were primarily tests of new bomb designs rather than of the effects of nuclear weapons. Three tests were carried out in April and May 1948 by Joint Task Force 7, with a work force of 10,366 personnel, of whom 9,890 were military.\n\nThe successful testing of the new cores in the \"Operation Sandstone\" tests rendered every component of the old weapons obsolete. Even before the third test had been carried out, production of the old cores was halted, and all effort concentrated on the new Mark 4 nuclear bomb, which would become the first mass-produced nuclear weapon. More efficient use of fissionable material as a result of Operation Sandstone would increase the U.S. nuclear stockpile from 56 bombs in June 1948 to 169 in June 1949.\n\nNuclear weapons were developed during World War II by the Manhattan Project, which created a network of production facilities, and the weapons research and design laboratory at the Los Alamos National Laboratory. Two types of bombs were developed: the Mark 1 \"Little Boy\", a gun-type fission weapon using uranium-235, and the Mark 3 \"Fat Man\", an implosion-type nuclear weapon using plutonium.\n\nThese weapons were not far removed from their laboratory origins. A great deal of work remained to improve ease of assembly, safety, reliability and storage before they were ready for production. There were also many improvements to their performance that had been suggested or recommended during the war that had not been possible under the pressure of wartime development. Norris Bradbury, who replaced Robert Oppenheimer as director at Los Alamos, felt that \"we had, to put it bluntly, lousy bombs.\"\n\nPlutonium was produced by irradiating uranium-238 in three 250 MW nuclear reactors at the Hanford site. In theory they could produce of plutonium per megawatt-day, or about per month. In practice, production never approached such a level in 1945, when only between was produced per month. A \"Fat Man\" core required about of plutonium, of which 21% fissioned. Plutonium production fell off during 1946 due to swelling of the reactors' graphite neutron moderators. This is known as the Wigner effect, after its discoverer, the Manhattan Project scientist Eugene Wigner.\n\nThese reactors were also required for the production (by irradiation of bismuth-209) of polonium-210, which was used in the initiators, a critical component of the nuclear weapons. Some of bismuth-209 had to be irradiated for 100 days to produce 600 curies of polonium-210, a little over . Because polonium-210 has a half-life of only 138 days, at least one reactor had to be kept running. The oldest unit, B pile, was therefore closed down so that it would be available in the future. Investigation of the problem would take most of 1946 before a fix was found.\n\nUranium-235 was derived from enrichment of natural uranium at the Y-12 plant and K-25 site in Oak Ridge, Tennessee. Improvements in the processes and procedures of the electromagnetic and gaseous isotope separation between October 1945 and June 1946 led to an increase in production to around of uranium-235 per month, which was only enough for one of the very wasteful \"Little Boy\"s. A \"Fat Man\" was 17.5 times as efficient as a \"Little Boy\", but a ton of uranium ore could yield eight times as much uranium-235 as plutonium, and on a per-gram basis, plutonium cost somewhere between four and eight times as much to produce as uranium-235, which at this time cost around $26 per gram.\n\nThe objectives of the Sandstone series of tests were to:\n\nLevitation meant that instead of being immediately inside the tamper, there would be an air gap between the tamper and the core, which would be suspended inside on wires. This would allow the tamper to gain more momentum before striking the core. The principle was similar to swinging a hammer at a nail. In order for this to work outside the laboratory, the wires had to be strong enough to withstand being dropped from an aircraft, but thin enough to not disturb the spherical symmetry of the implosion. The Theoretical Division at Los Alamos, known as T Division, had run computer calculations on the levitated core as early March 1945. The use of the levitated core had been proposed during the planning for Operation Crossroads, but it had been decided instead to use the existing solid core \"Christy\" design. This was named after its designer, Robert Christy. For Sandstone, however, it was decided that at least two of the three tests would use levitated cores.\n\nThe motivation behind the composite core was to make better use of the available fissionable material. The use of uranium-235 in an implosion weapon instead of the inefficient gun type Little Boy was an obvious development. However, while plutonium was more expensive and harder to produce than uranium-235, it fissions faster, because it makes better use of the neutrons its fission produces. On the other hand, the slower reaction of uranium-235 permits the assembly of super-critical masses, making it theoretically possible to produce weapons with high yields. By July 1945, Oppenheimer and Groves were considering using both materials in a composite core containing of plutonium and of uranium-235. The composite cores became available in 1946. Los Alamos' priority then became the development of an all-uranium-235 core. By January 1948 the national stockpile contained 50 cores, of which 36 were composite Christy cores, nine were plutonium Christy cores, and five were composite levitated cores. Testing the new levitated, composite and uranium-235 cores would require at least three test firings.\n\nMore efficient weapons would require less efficient initiators. This meant that less polonium would be required. At the time of Sandstone, the national stockpile of polonium-beryllium initiators consisted of 50 A-Class initiators, with more than 25 curies of polonium, and 13 B-Class initiators with 12 to 25 curies. During Sandstone, at least one test would be conducted with a B-Class initiator.\n\nThe tests were authorized by President Harry S. Truman on June 27, 1947. The Atomic Energy Commission's Director of Military Applications, Brigadier General James McCormack and his deputy, Captain James S. Russell, met with Bradbury and John Manley at Los Alamos on July 9 to make arrangements for the tests. They readily agreed that they would be scientific in nature, with Los Alamos supplying the technical direction and the armed forces providing supplies and logistical support. The cost of the tests, around $20 million, was divided between the Department of Defense and the Atomic Energy Commission. Lieutenant General John E. Hull was designated as test commander. Rear Admiral William S. Parsons and Major General William E. Kepner reprised their Operation Crossroads roles as deputy commanders. Joint Task Force 7 was formally activated on October 18, 1947. As its commander, Hull was answerable to both the Joint Chiefs of Staff and the Atomic Energy Commission.\n\nJoint Task Force 7 consisted of 10,366 personnel, 9,890 of them military. Its headquarters consisted of about 175 men, of whom 96 were on board the . The rest were accommodated on the , and . A special division of the Los Alamos National Laboratory, known as J Division, was created specifically to manage nuclear testing. An Atomic Energy Commission group (Task Group 7.1) was responsible for preparing and detonating the nuclear weapons, and conducting the experiments. It consisted of some 283 scientists and technicians responsible for nuclear tests from J Division, the Armed Forces Special Weapons Project, the Naval Research Laboratory, the Naval Ordnance Laboratory, Argonne National Laboratory, the Aberdeen Proving Ground, the Atomic Energy Commission, Edgerton, Germeshausen & Grier, and other agencies. \n\nEach dealt with a different aspect of the tests. The Naval Ordnance Laboratory handled the blast measurement tests, while the Naval Research Laboratory conducted the radiation measurement experiments, and Argonne National Laboratory did gamma ray measurements. Edgerton, Germeshausen, and Grier were contractors hired to design and install the timing and firing systems. Seven experimental weapon assemblies and six cores were delivered to San Pedro, California, and loaded on the weapon assembly ship USS \"Curtiss\", in February 1948, but the Atomic Energy Commission only gave permission for the expenditure of three cores in the tests.\n\nThe naval forces were organized as Task Group 7.3. It consisted of:\n\n\n\nSource: Berkhouse \"et al\", \"Operation Sandstone\", p. 40\n\nIn September 1947, Hull, Russell, who was designated test director on October 14, and Joint Task Force 7's scientific director, Darol K. Froman from the Los Alamos Laboratories, set out with a group of scientists and military officers to examine various proposed test sites in the Pacific. Enewetak Atoll was chosen as the test site on October 11. The island was remote, but with a good harbor and an airstrip. It also had ocean currents and trade winds that would carry fallout out to sea, an important consideration in view of what had happened at Bikini Atoll during Operation Crossroads.\n\nAs the Trust Territory of the Pacific Islands was a United Nations trust territory administered by the United States, the United Nations Security Council was notified of the upcoming tests on December 2. The atoll was inhabited by the dri-Enewetak, who lived on Aomon, and the dri-Enjebi, who lived on Bijire. Their original homes had been on Enewetak and Enjebi, but they had been moved during the war to make way for military bases. The population, about 140 in number, had been temporarily relocated to Meck Island during Operation Crossroads. This time, Ujelang Atoll, an uninhabited atoll southwest of Enewetak, was selected as a relocation site. A Naval Construction Battalion group arrived there on November 22 to build accommodation and amenities. The military authorities met with the local chiefs on December 3, and they agreed to the relocation, which was carried out by by December 20. An LST and four Douglas C-54 Skymaster aircraft were placed on standby to evacuate Ujelan in case it was affected by fallout, but were not required.\n\nUnlike the Crossroads tests, which were conducted in the media spotlight, the \"Sandstone\" tests were carried out with minimal publicity. On April 15, there was still discussion in Washington about whether or not there should be any public announcement of the tests at all. Hull opposed making any announcement until after the series was completed, but the AEC commissioners felt that the news would leak out, and the United States would look secretive. It was therefore decided to make a last minute announcement. There was no announcement of the purpose of the tests, and only cursory press releases. On 18 May, after the series was over, Hull held a press conference in Hawaii, but only permitted the media to quote from written statements.\n\nEnjebi, Aomon, and Runit Islands were cleared of vegetation and graded level to make it easier to install the required instrumentation, and a causeway was built between Aomon and Bijire so the instrument cables could be run from the test tower on Aomon to the control station on Bijire. The detonations were ordered so that later test areas would suffer minimal fallout from the earlier shots. The Army component, Task Group 7.2, was responsible for construction work. It consisted of the 1220th Provisional Engineer Battalion, with the 1217th and 1218th Composite Service Platoons, the 18th Engineer Construction Company and 1219th Signal Service Platoon; Companies D and E of the 2nd Engineer Special Brigade's 532nd Engineer Boat and Shore Regiment; the 461st Transportation Amphibious Truck Company; 854th Transportation Port Company; 401st CIC Detachment; and the Naval Shore Base Detachment.\n\nAs in \"Operation Crossroads\", each detonation was given its own code name, taken from the Joint Army/Navy Phonetic Alphabet. All used modified Mark III assemblies, and were detonated from towers. The timing of the detonations was a matter of compromise. The gamma ray measurement experiments required darkness, but the Boeing B-17 Flying Fortress drones that would sample the clouds needed daylight to control them. As a compromise, the Sandstone detonations all took place shortly before dawn.\n\nThe detonations in the United States' \"Sandstone\" series are listed below:\n\nThe \"X-Ray\" nuclear device used a levitated composite core. It was detonated on Enjebi just before sunrise at 06:17 on April 15, 1948, with a yield of 37 kilotons. The efficiency of utilization of the plutonium was about 35%; that of the uranium-235 was 25% or more. This was somewhat higher than Los Alamos' prediction. Observers watching from ships in the lagoon saw a brilliant flash and felt the radiant heat. A condensation cloud in diameter quickly enveloped the fireball, which glowed within the cloud. It took 45 to 50 seconds for the thunderous roar of the explosion to reach the observers.\n\nAbout 20 minutes later, \"Bariko\" launched a helicopter to check on the cable winch which was to collect samples. It also lowered boats to test radioactivity levels in the lagoon. B-17 pilotless drone aircraft were flown through the clouds, and a drone light tank was used to recover soil samples from the crater. Unfortunately, it became bogged and had to be towed out ten days later.\n\nThe \"Yoke\" nuclear device used a levitated all-uranium-235 core. It was detonated on Aomon just before sunrise on May 1, 1948 at 06:09, a day late due to unfavorable winds. The observers saw a similar flash and felt the same heat as the X-Ray blast, but the wide condensation cloud was larger, and the sound of the explosion more forceful. One observer likened it to the sound of \"a paper bag which is forcefully burst in a small room\". They were correct: its yield of 49 kilotons made it the largest nuclear detonation up to that time, but it was considered inefficient and wasteful of the fissile material.\n\n\"Zebra\", the third test, and the last of the Sandstone series, was detonated on Runit just before sunrise at 06:04 on May 15, 1948. This test was characterized by AEC Chairman David Lilienthal as the \"hardest and most important\" test of the three. By using one of the B-class initiators, it demonstrated that these could still be used with confidence. The observers perceived the flash and blast as similar to the previous two tests, but this time the base of the condensation cloud was at , which gave the observers an unobstructed view of the fireball, which therefore appeared to be brighter and last longer than the other two. Looks were deceiving: its levitated uranium-235 core produced a yield of 18 kilotons.\n\nThe procedures used in the previous tests were repeated, but this time the winch cable snagged, and the test samples had to be retrieved by a jeep, exposing its crew to more radiation. The Los Alamos personnel assigned to remove the filters from the B-17 drones had apparently carried out the procedure on \"X-Ray\" and \"Yoke\" without problems, but this time three of them suffered radiation burns on their hands serious enough to be hospitalized and need skin grafting. One of the men who had carried out the procedure for Yoke was then also found to have burns on his hands and was hospitalized too, but was discharged on 28 May. Once again the drone tank gave trouble, and bogged in the crater, but the soil samples were retrieved by the backup drone tank. Both tanks were subsequently dumped in the ocean.\n\nThe successful testing of the new cores in the Sandstone tests had a profound effect. Practically every component of the old weapons was rendered obsolete. Even before the third test had been carried out, Bradbury had halted production of the old cores, and ordered that all effort was to be concentrated on the Mark 4 nuclear bomb, which would become the first mass-produced nuclear weapon. The more efficient use of fissionable material would increase the nuclear stockpile from 56 bombs in June 1948 to 169 in June 1949. The Mark III bombs were withdrawn from service in 1950. At the same time, new production plants were coming online and the Wigner effect problem had been solved. By May 1951, plutonium production was twelve times that of 1947, while uranium-235 production had increased eight-fold. The Chief of the Armed Forces Special Weapons Project, Major General Kenneth D. Nichols, saw clearly that the era of scarcity was over. He now \"recommended that we should be thinking in terms of thousands of weapons rather than hundreds.\"\n\n"}
{"id": "54077", "url": "https://en.wikipedia.org/wiki?curid=54077", "title": "Perpetual motion", "text": "Perpetual motion\n\nPerpetual motion is motion of bodies that continues indefinitely. A perpetual motion machine is a hypothetical machine that can do work indefinitely without an energy source. This kind of machine is impossible, as it would violate the first or second law of thermodynamics.\n\nThese laws of thermodynamics apply regardless of the size of the system. For example, the motions and rotations of celestial bodies such as planets may appear perpetual, but are actually subject to many processes that slowly dissipate their kinetic energy, such as solar wind, interstellar medium resistance, gravitational radiation and thermal radiation, so they will not keep moving forever.\n\nThus, machines that extract energy from finite sources will not operate indefinitely, because they are driven by the energy stored in the source, which will eventually be exhausted. A common example is devices powered by ocean currents, whose energy is ultimately derived from the Sun, which itself will eventually burn out. Machines powered by more obscure sources have been proposed, but are subject to the same inescapable laws, and will eventually wind down.\n\nIn 2017 new states of matter, time crystals, were discovered in which on a microscopic scale the component atoms are in continual repetitive motion, thus satisfying the literal definition of \"perpetual motion\". However, these do not constitute perpetual motion machines in the traditional sense or violate thermodynamic laws because they are in their quantum ground state, so no energy can be extracted from them; they have \"motion without energy\".\n\nThe history of perpetual motion machines dates back to the Middle Ages. For millennia, it was not clear whether perpetual motion devices were possible or not, but the development of modern theories of thermodynamics has shown that they are impossible. Despite this, many attempts have been made to construct such machines, continuing into modern times. Modern designers and proponents often use other terms, such as \"over unity\", to describe their inventions.\n\nThere is a scientific consensus that perpetual motion in an isolated system violates either the first law of thermodynamics, the second law of thermodynamics, or both. The first law of thermodynamics is a version of the law of conservation of energy. The second law can be phrased in several different ways, the most intuitive of which is that heat flows spontaneously from hotter to colder places; relevant here is that the law observes that in every macroscopic process, there is friction or something close to it; another statement is that no heat engine (an engine which produces work while moving heat from a high temperature to a low temperature) can be more efficient than a Carnot heat engine.\n\nIn other words:\n\n\nStatements 2 and 3 apply to heat engines. Other types of engines which convert e.g. mechanical into electromagnetic energy, cannot operate with 100% efficiency, because it is impossible to design any system that is free of energy dissipation.\n\nMachines which comply with both laws of thermodynamics by accessing energy from unconventional sources are sometimes referred to as perpetual motion machines, although they do not meet the standard criteria for the name. By way of example, clocks and other low-power machines, such as Cox's timepiece, have been designed to run on the differences in barometric pressure or temperature between night and day. These machines have a source of energy, albeit one which is not readily apparent so that they only seem to violate the laws of thermodynamics.\n\nEven machines which extract energy from long-lived sources - such as ocean currents - will run down when their energy sources inevitably do. They are not perpetual motion machines because they are consuming energy from an external source and are not isolated systems.\n\nOne classification of perpetual motion machines refers to the particular law of thermodynamics the machines purport to violate:\n\n\"Epistemic impossibility\" describes things which absolutely cannot occur within our \"current\" formulation of the physical laws. This interpretation of the word \"impossible\" is what is intended in discussions of the impossibility of perpetual motion in a closed system.\n\nThe conservation laws are particularly robust from a mathematical perspective. Noether's theorem, which was proven mathematically in 1915, states that any conservation law can be derived from a corresponding continuous symmetry of the action of a physical system. For example, if the true laws of physics remain invariant over time then the conservation of energy follows. On the other hand, if the conservation laws are invalid, then the foundations of physics would need to change.\n\nScientific investigations as to whether the laws of physics are invariant over time use telescopes to examine the universe in the distant past to discover, to the limits of our measurements, whether ancient stars were identical to stars today. Combining different measurements such as spectroscopy, direct measurement of the speed of light in the past and similar measurements demonstrates that physics has remained substantially the same, if not identical, for all of observable time spanning billions of years.\n\nThe principles of thermodynamics are so well established, both theoretically and experimentally, that proposals for perpetual motion machines are universally met with disbelief on the part of physicists. Any proposed perpetual motion design offers a potentially instructive challenge to physicists: one is certain that it cannot work, so one must explain \"how\" it fails to work. The difficulty (and the value) of such an exercise depends on the subtlety of the proposal; the best ones tend to arise from physicists' own thought experiments and often shed light upon certain aspects of physics. So, for example, the thought experiment of a Brownian ratchet as a perpetual motion machine was first discussed by Gabriel Lippmann in 1900 but it was not until 1912 that Marian Smoluchowski gave an adequate explanation for why it cannot work. However, during that twelve-year period scientists did not believe that the machine was possible. They were merely unaware of the exact mechanism by which it would inevitably fail.\n\nIn the mid 19th-century Henry Dircks investigated the history of perpetual motion experiments, writing a vitriolic attack on those who continued to attempt what he believed to be impossible:\n\nSome common ideas recur repeatedly in perpetual motion machine designs. Many ideas that continue to appear today were stated as early as 1670 by John Wilkins, Bishop of Chester and an official of the Royal Society. He outlined three potential sources of power for a perpetual motion machine, \" Extractions\", \"Magnetical Virtues\" and \"the Natural Affection of Gravity\".\n\nThe seemingly mysterious ability of magnets to influence motion at a distance without any apparent energy source has long appealed to inventors. One of the earliest examples of a magnetic motor was proposed by Wilkins and has been widely copied since: it consists of a ramp with a magnet at the top, which pulled a metal ball up the ramp. Near the magnet was a small hole that was supposed to allow the ball to drop under the ramp and return to the bottom, where a flap allowed it to return to the top again. The device simply could not work. Faced with this problem, more modern versions typically use a series of ramps and magnets, positioned so the ball is to be handed off from one magnet to another as it moves. The problem remains the same.\n\nGravity also acts at a distance, without an apparent energy source, but to get energy out of a gravitational field (for instance, by dropping a heavy object, producing kinetic energy as it falls) one has to put energy in (for instance, by lifting the object up), and some energy is always dissipated in the process. A typical application of gravity in a perpetual motion machine is Bhaskara's wheel in the 12th century, whose key idea is itself a recurring theme, often called the overbalanced wheel: moving weights are attached to a wheel in such a way that they fall to a position further from the wheel's center for one half of the wheel's rotation, and closer to the center for the other half. Since weights further from the center apply a greater torque, it was thought that the wheel would rotate for ever. However, since the side with weights further from the center has fewer weights than the other side, at that moment, the torque is balanced and perpetual movement is not achieved. The moving weights may be hammers on pivoted arms, or rolling balls, or mercury in tubes; the principle is the same.\nAnother theoretical machine involves a frictionless environment for motion. This involves the use of diamagnetic or electromagnetic levitation to float an object. This is done in a vacuum to eliminate air friction and friction from an axle. The levitated object is then free to rotate around its center of gravity without interference. However, this machine has no practical purpose because the rotated object cannot do any work as work requires the levitated object to cause motion in other objects, bringing friction into the problem. Furthermore, a \"perfect\" vacuum is an unattainable goal since both the container and the object itself would slowly vaporize, thereby degrading the vacuum.\n\nTo extract work from heat, thus producing a perpetual motion machine of the second kind, the most common approach (dating back at least to Maxwell's demon) is \"unidirectionality\". Only molecules moving fast enough and in the right direction are allowed through the demon's trap door. In a Brownian ratchet, forces tending to turn the ratchet one way are able to do so while forces in the other direction are not. A diode in a heat bath allows through currents in one direction and not the other. These schemes typically fail in two ways: either maintaining the unidirectionality costs energy (requiring Maxwell's demon to perform more thermodynamic work to gauge the speed of the molecules than the amount of energy gained by the difference of temperature caused) or the unidirectionality is an illusion and occasional big violations make up for the frequent small non-violations (the Brownian ratchet will be subject to internal Brownian forces and therefore will sometimes turn the wrong way).\nBuoyancy is another frequently misunderstood phenomenon. Some proposed perpetual-motion machines miss the fact that to push a volume of air down in a fluid takes the same work as to raise a corresponding volume of fluid up against gravity. These types of machines may involve two chambers with pistons, and a mechanism to squeeze the air out of the top chamber into the bottom one, which then becomes buoyant and floats to the top. The squeezing mechanism in these designs would not be able to do enough work to move the air down, or would leave no excess work available to be extracted.\n\nProposals for such inoperable machines have become so common that the United States Patent and Trademark Office (USPTO) has made an official policy of refusing to grant patents for perpetual motion machines without a working model. The USPTO Manual of Patent Examining Practice states:\n\nAnd, further, that:\nThe filing of a patent application is a clerical task, and the USPTO will not refuse filings for perpetual motion machines; the application will be filed and then most probably rejected by the patent examiner, after he has done a formal examination. Even if a patent is granted, it does not mean that the invention actually works, it just means that the examiner believes that it works, or was unable to figure out why it would not work.\n\nThe USPTO maintains a collection of Perpetual Motion Gimmicks.\n\nThe United Kingdom Patent Office has a specific practice on perpetual motion; Section 4.05 of the UKPO Manual of Patent Practice states:\n\nExamples of decisions by the UK Patent Office to refuse patent applications for perpetual motion machines include:\n\nThe European Patent Classification (ECLA) has classes including patent applications on perpetual motion systems: ECLA classes \"F03B17/04: Alleged perpetua mobilia ...\" and \"F03B17/00B: [... machines or engines] (with closed loop circulation or similar : ... Installations wherein the liquid circulates in a closed loop; Alleged perpetua mobilia of this or similar kind ...\".\n\nAs \"perpetual motion\" can exist only in isolated systems, and true isolated systems do not exist, there are not any real \"perpetual motion\" devices. However, there are concepts and technical drafts that propose \"perpetual motion\", but on closer analysis it is revealed that they actually \"consume\" some sort of natural resource or latent energy, such as the phase changes of water or other fluids or small natural temperature gradients, or simply cannot sustain indefinite operation. In general, extracting work from these devices is impossible.\n\nSome examples of such devices include:\n\n\nIn some cases a thought (or \"gedanken\") experiment appears to suggest that perpetual motion may be possible through accepted and understood physical processes. However, in all cases, a flaw has been found when all of the relevant physics is considered. Examples include:\n\n\n"}
{"id": "171541", "url": "https://en.wikipedia.org/wiki?curid=171541", "title": "Planck mass", "text": "Planck mass\n\nIn physics, the Planck mass, denoted by \"m\", is the unit of mass in the system of natural units known as Planck units. It is approximately 0.02 milligrams. Unlike some other Planck units, such as Planck length, Planck mass is not a fundamental lower or upper bound; instead, Planck mass is a unit of mass defined using only what Max Planck considered fundamental and universal units. One Planck mass is roughly the mass of a flea egg.\nFor comparison, this value is of the order of 10 (a quadrillion) times larger than the highest energy available to contemporary particle accelerators.\n\nIt is defined as:\n\nwhere \"c\" is the speed of light in a vacuum, \"G\" is the gravitational constant, and \"ħ\" is the reduced Planck constant.\n\nSubstituting values for the various components in this definition gives the approximate equivalent value of this unit in terms of other units of mass:\n\nFor the Planck mass formula_3, the Schwarzschild radius (formula_4) and the Compton wavelength (formula_5) are of the same order as the Planck length formula_6.\nParticle physicists and cosmologists often use an alternative normalization with the reduced Planck mass, which is\nThe factor of formula_8 simplifies a number of equations in general relativity.\n\nThe Planck mass was first suggested by Max Planck in 1899. He suggested that there existed some fundamental natural units for length, mass, time and energy. He derived these units using only dimensional analysis of what he considered the most fundamental universal constants: the speed of light, the Newton gravitational constant, and the Planck constant.\n\nThe formula for the Planck mass can be derived by dimensional analysis. In this approach, one starts with the three physical constants \"ħ\", \"c\", and \"G\", and attempts to combine them to get a quantity whose dimension is mass. The formula sought is of the form\nwhere formula_10 are constants to be determined by equating the dimensions of both sides. Using the symbols formula_11 for mass, formula_12 for length and formula_13 for time, and writing [] to denote the dimension of some physical quantity , we have the following:\nTherefore,\nIf one wants this to equal formula_11, the dimension of mass, using formula_19, the following equations need to hold:\nThe solution of this system is:\nThus, the Planck mass is:\n\nDimensional analysis can only determine a formula up to a dimensionless multiplicative factor. There is no \"a priori\" reason for starting with the reduced Planck constant \"ħ\" instead of the original Planck constant \"h\", which differs from it by a factor of 2π.\n\nEquivalently, the Planck mass is defined such that the gravitational potential energy between two masses \"m\" of separation \"r\" is equal to the energy of a photon (or the mass-energy of a graviton, if such a particle exists) of angular wavelength \"r\" (see the Planck relation), or that their ratio equals one.\nIsolating \"m\", we get that\nNote that if, instead of Planck masses, the electron mass were used, the equation would require a gravitational coupling constant, analogous to how the equation of the fine-structure constant relates the elementary charge and the Planck charge. Thus, the Planck mass can be viewed as resulting from absorbing the gravitational coupling constant into the unit of mass (and those of distance/time as well), like the Planck charge does for the fine-structure constant.\n\n\n"}
{"id": "6414536", "url": "https://en.wikipedia.org/wiki?curid=6414536", "title": "Post-orbital constriction", "text": "Post-orbital constriction\n\nIn physical anthropology, post-orbital constriction, often referred to as the post-orbital constriction index, is a narrowing of the cranium (skull) just behind the eye sockets (the orbits, hence the name), in primates — including primitive hominids. This constriction is very noticeable in non-human primates, slightly less so in Australopithecines, even less in Homo erectus and the most primitive Homo sapiens. The post-orbital constriction index of archaic \"Homo\" species begins to fall within the range of modern \"Homo sapiens\" during the Mid-Pleistocene era. In a departure from \"Homo erectus\", \"Homo sapiens\" manifests a reduced post-orbital constriction due to increase in cranial capacity (about 1,350 cc), accompanied by higher cranial vaults. It completely disappears in modern Homo sapiens. Thus, it is a useful, quantifiable measure of how far along the evolutionary path a hominid fossil might be placed.\n\nIn species such as baboons and African great apes, an increase in the available capacity of the infratemporal fossa is simultaneously accompanied by a constriction in the sagittal plane. As such, the anterior and posterior portions of the anterior temporalis muscle are inversely correlated in size, with the anterior being larger. Although the temporalis muscle is used for chewing, there is no evidence that the supraorbital structure of primates is dependent upon their respective chewing habits or dietary preferences.\n\nPost-orbital constriction is defined by either a ratio of minimum frontal breadth (MFB) behind the supraorbital torus divided by maximum upper facial breadth (bifrontomalare temporale, BFM) or as the maximum width behind the orbit of the skull.\n\n"}
{"id": "6414403", "url": "https://en.wikipedia.org/wiki?curid=6414403", "title": "Qualitative Inquiry", "text": "Qualitative Inquiry\n\nQualitative Inquiry is a peer-reviewed academic journal that covers methodological issues raised by qualitative research in the social sciences. The journal's editors-in-chief are Yvonna Lincoln (Texas A&M University) and Norman K. Denzin (University of Illinois at Urbana–Champaign). It was established in 1995 and is currently published by SAGE Publications.\n\n\"Qualitative Inquiry\" is abstracted and indexed in Scopus and the Social Sciences Citation Index. According to the \"Journal Citation Reports\", its 2017 impact factor is 1.207, ranking it 44 out of 98 journals in the category \"Social Sciences, Interdisciplinary\".\n"}
{"id": "6414154", "url": "https://en.wikipedia.org/wiki?curid=6414154", "title": "Qualitative Research (journal)", "text": "Qualitative Research (journal)\n\nQualitative Research is a bimonthly peer-reviewed academic journal covering qualitative research methods in the fields of sociology and other social sciences. It was established in 2001 and is published by Sage Publications. The founding editors were Sara Delamont and P. Atkinson. The current editors-in-chief are Bella Dicks, Karen Henwood, and William Housley (Cardiff University).\n\nThe journal is abstracted and indexed in Scopus, and the Social Sciences Citation Index. According to the \"Journal Citation Reports\", its 2017 impact factor was 2.951.\n"}
{"id": "54119209", "url": "https://en.wikipedia.org/wiki?curid=54119209", "title": "Relative hour (Jewish law)", "text": "Relative hour (Jewish law)\n\nRelative hour (Hebrew singular: / ; plural: / ), sometimes called halachic hour, seasonal hour and variable hour, is a term used in rabbinic Jewish law that assigns 12 hours to each day and 12 hours to each night, all throughout the year. A relative hour has no fixed radical, but changes with the length of each day - depending on summer (when the days are long and the nights are short), and on winter (when the days are short and the nights are long). Even so, in all seasons a day is always divided into 12 hours, and a night is always divided into 12 hours, which inevitably makes for a longer hour or a shorter hour. All of the hours mentioned by the Sages in either the Mishnah or Talmud, or in other rabbinic writings, refer strictly to relative hours.\n\nAnother feature of this ancient practice is that, unlike the standard modern 12-hour clock that assigns 12 o'clock pm for noon time, in the ancient Jewish tradition noon time was always the \"sixth hour\" of the day, whereas the \"first hour\" began with the break of dawn, by most exponents of Jewish law, and with sunrise by the Vilna Gaon and Rabbi Hai Gaon. 12:o'clock am (midnight) was also the \"sixth hour\" of the night, whereas the \"first hour\" of the night began when the first three stars appeared in the night sky.\nIn old times, the hour was detected by observation of the position of the sun, or when the first three stars appeared in the night sky. During the first six hours of the day, the sun is seen in the eastern sky. At the \"sixth hour\", the sun is always at its zenith in the sky, meaning, it is either directly overhead, or parallel (depending on the hemisphere). Those persons living in the Northern Hemisphere, the sun at noon time will appear overhead slightly towards the south, whereas for those living in the Southern Hemisphere, the sun at noon time will appear overhead slightly towards the north. From the 6th and a half hour to the 12th hour, the sun inclines towards the west, until it sets. The conclusion of a day at the end of twilight may slightly vary in minutes from place to place, depending on the elevation and the terrain. Typically, nightfall ushers in more quickly in the low-lying valleys, than it does on a high mountaintop.\nThe conventional Jewish way of calibrating the time of day is to reckon the \"first hour\" of the day with the rise of dawn (), that is to say, approximately 72 minutes before sunrise, and the end of the day commencing shortly after sunset when the first three medium-size stars have appeared in the night sky. From the moment of sunset when the sun is no longer visible until the appearance of the first three medium-size stars is a unit of time called evening twilight (). In the Talmud, twilight is estimated at being the time that it takes a person to walk three quarters of a biblical mile (i.e. 1,500 cubits, insofar that a biblical mile is equal to 2,000 cubits). According to Maran's \"Shulhan Arukh\", a man traverses a biblical mile in 18 minutes, meaning, one is able to walk three quarters of a mile in 13½ minutes. According to Maimonides, a man walks a biblical mile in 24 minutes, meaning, three quarters of a mile is done in 18 minutes. In Jewish law, the short period of dusk or twilight (from the moment the sun has disappeared over the horizon until the appearance of the first three stars) is a space of time whose designation is doubtful, partly considered day and partly considered night. When the first medium-size star appears in the night sky, it is still considered day; when the second star appears, it is an ambiguous case. When the third star appears, it is the beginning of the \"first hour\" of the night. Between the break of dawn and the first three medium-size stars that appear in the night sky there are always 12 hours.\n\nIn the Modern Age of astral science and of precise astronomical calculations, it is now possible to determine the length of the ever-changing hour by simple mathematics. To determine the length of each relative hour, one needs but simply know two variables: (a) the precise time of sunrise, and (b) the precise time of sunset. Since the actual day begins approximately 72 minutes before sunrise, and ends 13½ minutes after the sun has already set and can no longer be seen over the horizon (according to Maran), or 18 minutes (according to Maimonides), by collecting the total number of minutes in any given day and dividing the total number of minutes by 12, the dividend that one is left with is the number of minutes to each hour. In summer months, when the days are long, the length of each hour during daytime can be as much as 77 minutes or more, whereas the length of each hour during nighttime can be less than 42 minutes.\n\nIn Jewish Halacha, the practical bearing of this teaching is reflected in many \"halachic\" practices. For example, whenever observant Jews refer to the appointed time for reciting the verses of \"Kriyat Shema\", ideally, this recital must be made from the time of sunrise until the end of the \"third hour\" of the day, a time that actually fluctuates on the standard 12-hour clock, depending on summer and winter. Its application is also used in determining the time of the Morning Prayer, traditionally said, as a first resort, from sunrise until the end of the \"fourth hour\", but as a last resort can be said until noon time, and which times will vary if one were to rely solely on the dials of the standard 12-hour clock, depending on the Summer months and Winter months.\n\nOn the eve of Passover, Jews are only permitted to eat leavened bread-stuffs until the \"fourth-hour\" of the day.\n\nIn Jewish tradition, prayers were usually offered at the time of the daily whole-burnt offerings. The historian, Josephus, writing about the daily whole-burnt offering, says that it was offered twice each day, in the morning and about the \"ninth hour\". The Mishnah, a compendium of Jewish oral laws compiled in the late 2nd-century CE, says of the morning daily offering that it was offered in the \"fourth hour\", but says of the late afternoon offering: \"The daily whole-burnt offering was slaughtered at a half after the \"eighth hour\", and offered up at a half after the \"ninth hour\".\" Elsewhere, when describing the slaughter of the Passover offerings on the eve of Passover (the 14th day of the lunar month Nisan), Josephus writes: \"...their feast which is called the Passover, when they slay their sacrifices, from the \"ninth hour\" to the \"eleventh\", etc.\" (roughly corresponding to 3 o'clock pm to 5 o'clock pm). Conversely, the Mishnah states that on the eve of Passover the daily whole-burnt offering was slaughtered at a half past the \"seventh hour\", and offered up at a half past the \"eighth hour\".\n\n"}
{"id": "25394139", "url": "https://en.wikipedia.org/wiki?curid=25394139", "title": "Research Excellence Framework", "text": "Research Excellence Framework\n\nThe Research Excellence Framework is the successor to the Research Assessment Exercise. It is an impact evaluation which assesses the research of British higher education institutions. It was first used in 2014 to assess the period 2008–2013. REF is undertaken by the four UK higher education funding bodies: Research England, the Scottish Funding Council (SFC), the Higher Education Funding Council for Wales (HEFCW), and the Department for the Economy, Northern Ireland (DfE). \n\nIts stated aims are to provide accountability for public investment in research, establish \"reputational yardsticks\", and thereby to achieve an efficient allocation of resources. Critics argue, inter alia, that there is too much focus on the impact of research outside of the university system, and that impact has no real relevance to the quality of research. It is suggested that REF actually encourages mediocrity in published research, and discourages research which might have value in the long term. \n\nThe next iteration of the REF will be in 2021, continuing the previous assessment model of focusing on research outputs, research impact and research environment.\n\nIn June 2007 the Higher Education Funding Council for England (HEFCE) issued a circular letter announcing that a new framework for assessing research quality in UK universities would replace the Research Assessment Exercise (RAE), following the 2008 RAE. The following quote from the letter indicates some of the original motivation:\n\nThe letter also set out a timetable for the development of the REF. HEFCE undertook a consultation exercise during September–December 2009, soliciting responses from stakeholders on the proposals. These include for example the response from Universities UK, and the response from the University and College Union.\n\nIn July 2010 (following the May 2010 general election), the Universities and Science minister David Willetts announced that the REF will be delayed by a year in order to assess the efficacy of the impact measure.\n\nIn July 2016, Lord Nicholas Stern's review was published, drafting general guidelines for the next REF in 2021. In general, the review was supportive with the methodology used in 2014 to evaluate universities' research, however it emphasised the need for more engagement with the general public and the increase of number of case studies that undertook interdisciplinary approach. The Research-impact.org team at Loughborough University Business and Economic School have been experimenting with crowdfunding for research in order to increase the university's researchers' public engagement.\n\nREF's impact was defined as \"an effect on, change or benefit to the economy, society, culture, public policy or services, health, the environment or quality of life, beyond academia\".\n\nSubmissions are assessed according to the following criteria:\n\n\nTwo publishers, \"The Guardian\" and \"Times Higher Education\", produce overall rankings of multidisciplinary universities based on power and quality(GPA).\n\nPower rankings aim to show universities with a breadth of quality, while Quality rankings aim to show the depth of quality.\n\n\"The Guardian\" Power rankings only consider rankings graded at Four and Three star, while \"Times Higher Education\" Power rankings consider rankings across all gradings.\n\nAn additional Quality ranking is the one ranking institutions according to the proportion of their research graded as \"Four star\". That is, submitted researches graded as \"Quality that is world-leading in originality, significance and rigour\".\n\nA particular source of criticism has been the element of the REF that addresses the \"impact\" of research. The articles below raise two objections. The main one is that \"impact\" has been defined to mean impact outside the academy. If researchers were required to pursue this form of impact, it would undermine academic freedom. The other is that impact—as currently construed—is hard to measure in any way that would be regarded as fair and impartial.\n\nThe Higher Education Funding Council for England argue that their measure of \"impact\" is a broad one which will encompass impact upon the \"economy, society, public policy, culture and the quality of life\".. However, the assessment structure does make what impact practically can be claimed rather narrow (4 page limit, no method section, 10 impact references, 10 research references and only 1 page to summarize the research and the impact respectively). These strict discursive guidelines alongside the REF's dated notion of how research impact functions (teaching research impact excluded, linear model, etc.) does restrict what impact is suited practically more for the assessment. \n\nAnother area of criticism, which the REF inherited from the structure of the RAE, is that for most full-time staff members submission normally consists of four published 'research output items'. There is no recognition of the difference between a book and an article in terms of research value. Therefore, the REF system discourages long term projects that strive for excellence. This problem is particularly evident in the humanities, where most of the ground-breaking research is traditionally not published in articles. Therefore, many researchers are pushed towards a relatively mediocre activity, which will allow them to produce one or two books during the assessment period, but not the kind of monograph that normally would need four or five years of research and writing.\n\nMoreover, the system of the four published items discourages long-term projects with relatively high research risk in the sciences as well, since researchers are reluctant to engage in projects or experiments that may not be successful and may not lead to a publication. Since most of the ground-breaking research in the sciences takes place with precisely such risky and imaginative projects, the type of research activity that is encouraged by the REF structure is quite conservative. Also, in terms of the impact of the examined research, in the history of the sciences and the humanities it is not unusual to take some time until the full impact of a discovery is made. The present system has a vista of only four or five years.\n\nThe Times Higher Education also revealed that some universities appeared to be \"gaming\" the REF system. This included \"REF Poaching\", in which staff with established research records were headhunted from their universities immediately before the REF, giving the poaching institution full credit for their publications without having taken the risk of supporting the researcher. It also included employing large numbers of staff on 0.2 FTE contracts, the lowest level of employment that qualifies them for REF submission.\n\nIn addition to such concerns about what really can be measured by four research output items, and how impact may be measured, the whole system is often criticized as unnecessarily complex and expensive, whereas quality evaluation in the digital age could be much simpler and effective.\n\nThe system, with its associated financial implications, has also been criticised for diverting resources from teaching. As such, increases in student fees may often not have resulted in more staff time being spent on teaching.\n\nIn July 2016, Lord Nicholas Stern's review was published, drafting general guidelines for the next REF in 2021. One of the recommendation was to increase research public engagement. Research engagement means enhancing delivery of the benefits from research. It also means making the public more aware of the research findings and their implications. One mechanism for public engagement is crowdfunding for research, where dedicated platforms host crowdfunding campaigns for university research, in a range of topics. Crowdfunding for research has two advantages: one, it is a source for a relatively high guaranteed funding, with a rate of around 50%, second, it is a very effective tool to engage with the general public.\n\nOne problem that the Stern review did not address in relation to the research impact assessment, is that the structure of case study design template on which impact is assessed, does not contain a method section, and thereby making the assessment of what type of impact was claimed a rhetoric game of who can claim the most (cf. Brauer, 2018). Thereby, grand claims are incentivized by the assessment structure. The problem occurs, because qualitative judgments of the significance and reach of the impact (without an account of the underlying method) cement contemporary values into the assessment, as such; \"[…] \"call it socially constructed, mutual learning, social practice whatever, the key is that we can’t separate characteristics of Impact from the process imposed on value and recognise it as such.\"\" (Derrick, 2018:160) When checking the reference of current claims, these where either not accessible (e.g. the relevant websites were taken down), referenced in such a way that it didn't reflect self-authorship or testimonials of individuals connected to the researcher (Brauer, 2018:142-147). Similarly, Sayer (2014) criticizes the overall peer review of the REF process, describing it as poor simulacrum of standard academic quality and that the assessment process is further complicated by the sheer workload of the assessment (p. 35). On a similar note, a RAND study found that the majority of the references were never consulted, certain assessment panels were discouraged from using the internet and the reference help structure of the REF took sometimes two weeks to produce associated references . Thereby, the external impact focus disciplines the assessment into focusing on external values. This ossifyies contemporary societal values into academic culture, due to the missing method section where rigor in impact claim that challenge current social values potentially could have been shown. In an assessment of current impact submissions none of the reported impacts include such claims.\n\n\n"}
{"id": "7953419", "url": "https://en.wikipedia.org/wiki?curid=7953419", "title": "Restoring force", "text": "Restoring force\n\nRestoring force, in a physics context, is a force that gives rise to an equilibrium in a physical system. If the system is perturbed away from the equilibrium, the restoring force will tend to bring the system back toward equilibrium. The restoring force is a function only of position of the mass or particle. It is always directed back toward the equilibrium position of the system. The restoring force is often referred to in simple harmonic motion. The force which is responsible to restore original size and shape is called restoring force.\n\nAn example is the action of a spring. An idealized spring exerts a force that is proportional to the amount of deformation of the spring from its equilibrium length, exerted in a direction to oppose the deformation. Pulling the spring to a greater length causes it to exert a force that brings the spring back toward its equilibrium length. The amount of force can be determined by multiplying the spring constant of the spring by the amount of stretch.\n\nAnother example is of a pendulum. When the pendulum is not swinging all the forces acting on the pendulum are in equilibrium. The force due to gravity and the mass of the object at the end of the pendulum is equal to the tension in the string holding that object up. When a pendulum is put in motion the place of equilibrium is at the bottom of the swing, the place where the pendulum rests. When the pendulum is at the top of its swing the force bringing the pendulum back down to this midpoint is gravity. As a result gravity can be seen as the restoring force in this.\nRestoring force of a spring : ( f=-kx )\n\n"}
{"id": "2145977", "url": "https://en.wikipedia.org/wiki?curid=2145977", "title": "Slapper detonator", "text": "Slapper detonator\n\nA slapper detonator, also called exploding foil initiator (EFI), is a relatively recent kind of a detonator developed by Lawrence Livermore National Laboratory, US Patent No. 4,788,913. It is an improvement of the earlier exploding-bridgewire detonator; instead of directly coupling the shock wave from the exploding wire, the expanding plasma from an explosion of a metal foil drives another thin plastic or metal foil called a \"flyer\" or a \"slapper\" across a gap, and its high-velocity impact on the explosive (for example, PETN or hexanitrostilbene) then delivers the energy and shock needed to initiate a detonation. Normally all the slapper's kinetic energy is supplied only by the heating (and hence expansion) of the plasma (the former foil) by the current passing through it, though constructions with a \"back strap\" to further drive the plasma forward by magnetic field exist too. This assembly is quite efficient; up to 30% of the electrical energy can be converted to the slapper's kinetic energy.\n\nThe initial explosion is usually caused by explosive vaporization of a thin metal wire or strip, by driving several thousand amperes of electric current through it, usually from a capacitor charged to several thousand volts. The switching may be done by a spark gap or a krytron.\n\nUsually the construction consists of an explosive booster pellet, against which a disk with a hole in the center is set. Over the other side of the disk, there is a layer of an insulating film, for example, Kapton or PET film, with a thin strip of metal (typically aluminum or gold) foil deposited on its outer side. A narrowed section of the metal then explosively vaporizes when a current pulse passes through it, which shears the mylar foil and the plasma ball pushes it through the hole, accelerating it to very high speed. The impact then detonates the explosive pellet.\nAdvantages over explosive-bridgewire detonators include:\n\nIn a variant called laser detonator the vaporization can be caused by a high-power laser pulse delivered over-the-air or coupled by an optical fiber; this is reportedly used as a safety detonator in some mining operations and quarries. Typically a 1-watt solid-state laser is used.\n\nThe slapper detonators are frequently used in modern weapon designs and aerospace technology.\n\nFor the description of the required firing system, see Firing system for exploding-bridgewire detonator.\n\n\n\nCooper, Paul W., \"Explosives Engineering\", New York: Wiley-VCH, 1996. \n"}
{"id": "43044633", "url": "https://en.wikipedia.org/wiki?curid=43044633", "title": "SpaceNews", "text": "SpaceNews\n\nSpaceNews is a print and digital publication that covers business and political news in the space and satellite industry. \"SpaceNews\" provides news, commentary and analysis to an audience of government officials, politicians and executives within the space industry. \"SpaceNews\" details topics in civil, military and commercial space and the satellite communications business.\n\n\"SpaceNews\" covers important news in North America, Europe, Asia, Africa, the Middle East and South America from NASA, the European Space Agency, and private spaceflight firms such as Arianespace, International Launch Services, SpaceX and United Launch Alliance. The magazine regularly features profiles on relevant and important figures within the space industry. These profiles have featured numerous government leaders, corporate executives and other knowledgeable space experts, including NASA administrators Richard Truly, Daniel Goldin, Sean O’Keefe, Michael Griffin and Charles Boldin. \n\nFounded in 1989, \"SpaceNews\" publishes its flagship magazine 20 times per year. Brian Berger, a journalist who joined \"SpaceNews\" in 1989 to cover the NASA and reusable launch vehicle beats, was named editor-in-chief in January 2016. \n\n\"SpaceNews\" produces and publishes several electronic newsletters, including First Up, First Up Satcom, SN Military.Space and SpaceNews This Week. \n\n\"SpaceNews\" produces the official show daily of the Space Symposium, a civil, military and commercial space conference the Space Foundation holds annually in Colorado Springs, Colorado. The show daily is a tabloid-size print publication distributed to attendees during the conference. In 2018, \"SpaceNews\" began producing a print show daily for AIAA and Utah State University’s Conference on Small Satellites, held each August on the campus of Utah State University in Logan, Utah. \n\n\"SpaceNews\" was originally owned by the Army Times Publishing Company, which was acquired by Gannett in 1997. In 2000, Space.com (later renamed Imaginova) acquired \"SpaceNews\" from Gannett. \"SpaceNews\" is currently owned by Pocket Ventures, LLC., which acquired the publication from Imaginova in June 2012.\n"}
{"id": "47011713", "url": "https://en.wikipedia.org/wiki?curid=47011713", "title": "The Inflationary Universe", "text": "The Inflationary Universe\n\nThe Inflationary Universe is a popular science book by theoretical physicist Alan Guth, first published in 1997. The book explores the theory of inflation, which was first presented by the author in 1979.\n\nIn April 2015, physicist and Nobel laureate Steven Weinberg included \"The Inflationary Universe\" in a personal list of \"the 13 best science books for the general reader\".\n\n\n"}
{"id": "5452205", "url": "https://en.wikipedia.org/wiki?curid=5452205", "title": "Trading zones", "text": "Trading zones\n\nThe metaphor of a trading zone is being applied to collaborations in science and technology. The basis of the metaphor is anthropological studies of how different cultures are able to exchange goods, despite differences in language and culture. \n\nPeter Galison produced the \"trading zone\" metaphor in order to explain how physicists from different paradigms went about collaborating with each other and with engineers to develop particle detectors and radar.\n\nAccording to Galison, \"Two groups can agree on rules of exchange even if they ascribe utterly different significance to the objects being exchanged; they may even disagree on the meaning of the exchange process itself. Nonetheless, the trading partners can hammer out a local coordination, despite vast global differences. In an even more sophisticated way, cultures in interaction frequently establish contact languages, systems of discourse that can vary from the most function-specific jargons, through semispecific pidgins, to full-fledged creoles rich enough to support activities as complex as poetry and metalinguistic reflection\" (Galison 1997, p. 783)\n\nIn the case of radar, for example, the physicists and engineers had to gradually develop what was effectively a pidgin or creole language involving shared concepts like ‘equivalent circuits’ that the physicists represented symbolically in terms of field theory and the engineers saw as extensions of their radio toolkit. \n\nExchanges across disciplinary boundaries can also be carried out with the help of an agent: a person who is familiar enough with the language of two or more cultures to facilitate trade.\n\nAt one point in the development of MRI, surgeons saw a lesion where an engineer familiar with the device would have recognized an artifact produced by the way the device was being used. It took someone with expertise in both physics and surgery to see how each of the different disciplines viewed the device, and develop procedures for correcting the problem (Baird & Cohen, 1999). The ability to converse expertly in more than one discipline is called interactional expertise (Collins & Evans, 2002).\n\n\nA workshop at Arizona State University on Trading Zones, Interactional Expertise and Interdisciplinary Collaboration raised the possibility of applying these concepts to other applications like global health and service science, and also identified avenues for future research (https://archive.is/20121215123346/http://bart.tcc.virginia.edu/Tradzoneworkshop/index.htm).\n\n\n\n"}
{"id": "1058983", "url": "https://en.wikipedia.org/wiki?curid=1058983", "title": "Triangular theory of love", "text": "Triangular theory of love\n\nThe triangular theory of love is a theory of love developed by Robert Sternberg, a member of the Psychology Department at Yale University. During his time as a professor, Sternberg emphasized his research in the fields of intelligence, creativity, wisdom, leadership, thinking styles, ethical reasoning, love, and hate. In the context of interpersonal relationships, \"the three components of love, according to the triangular theory, are an intimacy component, a passion component, and a decision/commitment component.\"\n\nSternberg says that intimacy refers to \"feelings of closeness, connectedness, and bondedness in loving relationships\", passion refers to \"the drives that lead to romance, physical attraction, sexual consummation, and related phenomena in loving relationships\" and decision/commitment means different things in the short and long term. In the short-term, it refers to \"the decision that one loves a certain other\", and in the long-term, it refers to \"one's commitment to maintain that love.\"\n\nThe three components of love are as follows\n\nPassion: Passion can be associated with either physical arousal or emotional stimulation. Passion is defined in three ways:\n\n\nIntimacy: Intimacy is described as the feelings of closeness and attachment to one another. This tends to strengthen the tight bond that is shared between those two individuals. Additionally, having a sense of intimacy helps create the feeling of being at ease with one another, in the sense that the two parties are mutual in their feelings.\n\nIntimacy is primarily defined as something of a personal or private nature; familiarity.\n\nCommitment: Unlike the other two blocks, commitment involves a conscious decision to stick with one another. The decision to remain committed is mainly determined by the level of satisfaction that a partner derives from the relationship. There are three ways to define commitment:\n\n\n\"The amount of love one experiences depends on the absolute strength of these three components, and the type of love one experiences depends on their strengths relative to each other.\" Different stages and types of love can be explained as different combinations of these three elements; for example, the relative emphasis of each component changes over time as an adult romantic relationship develops. A relationship based on a single element is less likely to survive than one based on two or three elements.\n\nOne of the first theories of love was developed by Sigmund Freud. As Freud so frequently attributed human nature to unconscious desires, his theory of love centered around the need for an \"ego ideal\". His definition of an ego ideal is this: the image of the person that one wants to become, which is patterned after those whom one holds with great respect.\n\nAnother theory was introduced by Maslow. Maslow's hierarchy of needs places self-actualization at the peak. He maintains that those who have reached self-actualization are capable of love.\n\nYet another theory, one about being in love, was developed by Reik. Being in love was said to be attainable for those who could love for the sake of loving people, not just fixing one's own problem.\n\nWhen theories about love moved from being clinically based to being socially and personality based, they became focused on types of love, as opposed to becoming able to love.\n\nOf the multiple different early and later theories of love, there are two specific early theories that contribute to and influence Sternberg's theory.\n\nThe first is a theory presented by Zick Rubin named The Theory of Liking vs. Loving. In his theory, to define romantic love, Rubin concludes that attachment, caring, and intimacy are the three main principles that are key to the difference of liking one person and loving them. Rubin states that if a person simply enjoys another's presence and spending time with them, that person only likes the other. However, if a person shares a strong desire for intimacy and contact, as well as cares equally about the other's needs and their own, the person loves the other.\n\nIn Sternberg's theory, one of his main principles is intimacy. It is clear that intimacy is an important aspect of love, ultimately using it to help define the difference between compassionate and passionate love.\n\nThe second is a theory—The Color Wheel Model of Love—presented by John Lee. In his theory, using the analogy of primary colors to love, Lee defines the three different styles of love. These include Eros, Ludos, and Storge. Most importantly within his theory, he concludes that these three primary styles, like the making of complementary colors, can be combined to make secondary forms of love.\n\nIn Sternberg's theory, he presents, like Lee, that through the combination of his three main principles, different forms of love are created.\n\nSternberg also described three models of love, including the Spearmanian, Thomsonian, and Thurstonian models. According to the Spearmanian model, love is a single bundle of positive feelings. In the Thomsonian model, love is a mixture of multiple feeling that, when brought together, produce the feeling. The Spearmanian model is the closest to the triangular theory of love, and dictates that love is made up of equal parts that are more easily understood on their own than as a whole. In this model, the various factors are equal in their contribution to the feeling, and could be disconnected from each other.\n\nSternberg's triangular theory of love was developed after the identification of passionate love and companionate love. Passionate love and companionate love are different kinds of love but are connected in relationships.\n\nPassionate love is associated with strong feelings of love and desire for a specific person. This love is full of excitement and newness. Passionate love is important in the beginning of the relationship and typically lasts for about a year. There is a chemical component to passionate love. Those experiencing passionate love are also experiencing increased neurotransmitters, specifically phenylethylamine. These feelings are most commonly found in the most early stages of love.\n\nCompanionate love follows passionate love. Companionate love is also known as affectionate love. When a couple reaches this level of love, they feel mutual understanding and care for each other. This love is important for the survival of the relationship. This type of love comes later on in the relationship and requires a certain level of knowledge for each person in the relationship.\n\nSternberg created his triangle next. The triangle's points are intimacy, passion, and commitment.\n\nIntimate love is the corner of the triangle that encompasses the close bonds of loving relationships. Intimate love felt between two people means that they each feel a sense of high regard for each other. They wish to make each other happy, share with each other, be in communication with each other, help when one is in need. A couple with intimate love deeply values each other. Intimate love has been called the \"warm\" love because of the way it brings two people close together. Sternberg's prediction of this love was that it would diminish as the relationship became less interrupted, thus increasing predictability.\n\nPassionate love is based on drive. Couples in passionate love feel physically attracted to each other. Sexual desire is typically a component of passionate love. Passionate love is not limited to sexual attraction, however. It is a way for couples to express feelings of nurture, dominance, submission, self-actualization, etc. Passionate love is considered the \"hot\" component of love because of the strong presence of arousal between two people. Sternberg believed that passionate love will diminish as the positive force of the relationship is taken over by opposite forces. This idea comes from Solomon's opponent-force theory.\n\nCommitment, or committed love, is for lovers who are committed to being together for a long period of time. Something to note about commitment, however, is that one can be committed to someone without feeling love for him or her, and one can feel love for someone without being committed to him or her. Commitment is considered to be the \"cold\" love because it does not require either intimacy or passion. Sternberg believed that committed love increases in intensity as the relationship grows. Commitment can be considered for friends as well.\n\nSternberg believed love to progress and evolve in predictable ways; that all couples in love will experience intimate, passionate, and committed love in the same patterns.\n\nAlthough these types of love may contain qualities that exist in non-loving relationships, they are specific to loving relationships. A description of non-love is listed below, along with the other kinds of love. These kinds of love are combinations of one or two of the three corners of Sternberg's triangle of love.\n\nThe three components, pictorially labeled on the vertices of a triangle, interact with each other and with the actions they produce so as to form seven different kinds of love experiences (nonlove is not represented). The size of the triangle functions to represent the \"amount\" of love—the bigger the triangle, the greater the love. Each corner has its own type of love and provides different combinations to create different types of love and labels for them. The shape of the triangle functions to represent the \"style\" of love, which may vary over the course of the relationship:\n\nSternberg's triangular theory of love provides a strong foundation for his later theory of love, entitled Love as a Story. In this theory, he explains that the large numbers of unique and different love stories convey different ways of how love is understood. He believes, over time, this exposure helps a person determine what love is or what it should be to them. These two theories create Sternberg's duplex theory of love.\n\n\"Personal relationships that have the greatest longevity and satisfaction are those in which partners are constantly working on sustaining intimacy and reinforcing commitment to each other.\"\n\nIn a study done by Michele Acker and Mark Davis in 1992, Sternberg's triangular theory of love was tested for validity. By studying a population that extended outside the typically studied group of 18 to 20-year-old college students, Acker and Davis were able to study more accurately the stages of love in people. Some criticism of Sternberg's theory of love is that although he predicted the stages of a person's love for another person, he did not specify a time or point in the relationship when the stages would evolve. He does not specify whether the different parts of love are dependent on duration of relationship or on the particular stage that relationship has reached. Acker and Davis point out that the stage and duration of the relationship are potentially important to the love component and explore them.\n\nThey find that there are no exact answers because not only each couple, but each individual in the couple experiences love in a different way. There are three perceptions of the triangular theory of love, or \"the possibility of multiple triangles\". Multiple triangles can exist because individuals can experience each component of love (or point of the triangle) more intensely than another. These separate triangles, according to Acker and Davis and many others, are 'real' triangles, 'ideal' triangles, and 'perceived' triangles.\n\nThese 'real' triangles are indicative of how each individual views the progress and depth of his or her relationship. The 'ideal' triangles are indicative of each individual's ideal qualities of his or her partner/relationship. The 'perceived' triangles are indicative of each individual's ideas of how his or her partner is viewing the relationship. If any of these three separate triangles do not look the same as a person's partner's triangles, dissatisfaction is likely to increase.\n\nSternberg's triangular theory of love, may not be as simple as he initially laid it out to be. Sternberg measured his theory on couples who were roughly the same age (mean age of 28) and whose relationship duration was roughly the same (4 to 5 years). His sample size was limited in characteristic variety. Acker and Davis announced this issue as being one of three major problems with Sternberg's theory. Romantic love, in particular, is not often the same in undergraduate level couples as couples who are not undergrads. Acker and Davis studied a sample that was older than Sternberg's sample of undergraduates. Sternberg himself did this in 1997.\n\nThe two other most obvious problems with Sternberg's theory of love are as follows. The first is a question of the separate nature of the levels of love. The second is a question of the measures that have previously been used to assess the three levels of love. These problems with Sternberg's theory continued to be studied, for example Lomas (2018).\n\n"}
{"id": "43317048", "url": "https://en.wikipedia.org/wiki?curid=43317048", "title": "Vantablack", "text": "Vantablack\n\nVantablack is a material developed by Surrey NanoSystems in the United Kingdom and is one of the darkest substances known, absorbing up to 99.6% of radiation in the visible spectrum.\n\nThe name is a compound of the acronym (\"VANTAs\") \"vertically aligned carbon nanotube arrays\" and the shade \"black\".\n\nVantablack is composed of a forest of vertical tubes \"grown\" on a substrate using a modified chemical vapor deposition process (CVD). When light strikes Vantablack, instead of bouncing off from it, it becomes trapped and is continually deflected amongst the tubes, eventually becoming absorbed and dissipating into heat.\n\nVantablack was an improvement over similar substances developed at the time. Vantablack absorbs 99.965% of visible light and can be created at . NASA had previously developed a similar substance that was grown at , so it required materials to be more heat resistant than Vantablack.\n\nThe outgassing and particle fallout levels of Vantablack are low. The high levels in similar substances in the past had limited their commercial utility. Vantablack also has greater resistance to mechanical vibration, and has greater thermal stability.\n\nEarly development was carried out at the National Physical Laboratory in the UK, the term \"Vanta\" was coined sometime later. Vertically aligned nanotube arrays are sold by several firms, including NanoLab, Santa Barbara Infrared and others.\n\nThe Vantablack name is trademarked by Surrey NanoSystems Limited, and has been referenced in three patents registered in the United States Patent and Trademark Office.\n\nBeing the blackest material, this substance has many potential applications, including preventing stray light from entering telescopes, and improving the performance of infrared cameras both on Earth and in space. Ben Jensen, Chief Technology Officer, Surrey NanoSystems, has explained: \"For example, it reduces stray light, improving the ability of sensitive telescopes to see the faintest stars... Its ultra-low reflectance improves the sensitivity of terrestrial, space and air-borne instrumentation.\"\n\nVantablack may also increase the absorption of heat in materials used in concentrated solar power technology, as well as military applications such as thermal camouflage. Its emissivity and scalability support a wide range of applications.\n\nIn addition to directly growing aligned carbon nanotubes, Vantablack is made into two sprayable paints with randomly-oriented nanotubes, Vantablack S-VIS and Vantablack S-IR with better infrared absorption than the former. These paints require a special license, a temperature of 100–280 °C, and vacuum post-processing. Surrey NanoSystems also markets a line of non-nanotube sprayable paints known as Vantablack VBx that are even easier to apply.\n\nVantablack S-VIS, a sprayable paint that uses randomly-aligned carbon nanotubes and only has high absorption in the visible light band, has been exclusively licensed to Anish Kapoor's studio for artistic use. This has caused outrage among some other artists, including Christian Furr and Stuart Semple. In retaliation, Semple banned Kapoor from buying the strong shade of pink that Semple had developed. He later stated that the move was itself like performance art and that he did not anticipate the amount of attention it received. In December 2016, Kapoor posted an Instagram post of his middle finger dipped in Semple's pink. Semple later barred Kapoor from buying other products of his, including one sold as \"Black 2.0\", which has similar qualities to Vantablack VBx, despite being acrylic.\n\nNanolab, a Waltham, Massachusetts-based carbon nanotube manufacturer, partnered with Boston artist Jason Chase to release a nanotube-based black paint called Singularity Black. During the first showing of the colour, Chase, alluding to Vantablack, stated that \"its possibilities have been stunted by not being available to experiment with,\" and Singularity Black's release was important to create access.\n\nArtreport.com contributor Jazia Hammoudi opined that the controversy had been manufactured by the media, while the author of an article in \"Wired\" magazine suggested that the controversy between the artists and its online response was a spontaneous piece of collective performance art in itself. The manufacturer claims that Vantablack is subject to export controls by the UK, and due to its physical requirements and thermal characteristics, the original Vantablack is not practical for use in many types of art.\n\nVantablack VBx2, a variant of the non-nanotube Vantablack VBx that is optimized for large area spraying, was used in a \"Vantablack pavilion\" at the 2018 Winter Olympics.\n\nThe first orders were delivered in July 2014. In 2015, production was scaled up to satisfy the needs of buyers in the aerospace and defense sectors.\n\n"}
{"id": "2635677", "url": "https://en.wikipedia.org/wiki?curid=2635677", "title": "Young Inventors International", "text": "Young Inventors International\n\nYoung Inventors International or YII is a non-profit organization started in 2001 by Anne Swift, a university student at the time. The organization aims to educate innovators under the age of 35 and connect them with resources to commercialize their inventions.\n\nYoung Inventors International has a membership of approximately 1400 with the world-wide headquarters located in Toronto, Ontario, Canada. YII holds international conferences and workshops, maintains an online resource portal, and disseminates best practices for supporting young innovators.\n\n"}
