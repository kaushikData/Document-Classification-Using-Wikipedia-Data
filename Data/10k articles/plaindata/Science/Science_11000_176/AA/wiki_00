{"id": "1447145", "url": "https://en.wikipedia.org/wiki?curid=1447145", "title": "21st Century (Digital Boy)", "text": "21st Century (Digital Boy)\n\n\"21st Century (Digital Boy)\" is a song by the punk rock group Bad Religion. It was originally recorded in 1990 on their fifth full-length studio album \"Against the Grain\" and re-recorded on the 1994 album \"Stranger Than Fiction\". The following year it was included on the \"All Ages\" compilation release.\n\nAlthough the \"Against the Grain\" version was not released as a single, the \"Stranger Than Fiction\" version was a popular hit. The hit version was also featured on the 2002 compilation \"Punk Rock Songs\", which was not endorsed by the band.\n\nIn 1994, Bad Religion re-recorded the song for their eighth studio album \"Stranger Than Fiction\". Guitarist Brett Gurewitz claimed that Bad Religion re-recorded it because their then-label Atlantic Records said they did not \"hear a single\" in that album and thought the song was a hit so they asked the band to redo it.\n\nWhen also asked why \"21st Century (Digital Boy)\" would be re-recorded for \"Stranger Than Fiction\", bassist Jay Bentley replied:\n\nThe lyrics of the song could be interpreted as a rejection of modern consumerist culture, as exemplified in the lyrics \"I'm a 21st Century Digital Boy / I don't know how to live, but I've got a lot of toys\". This alienation and rejection of consumerism and mainstream culture is a common theme in the music of Bad Religion. The bridge includes references to the group's two previous records (as of the original recording), \"Suffer\" and \"No Control\". Contrary to rumor, \"21st Century (Digital Boy)\" was not written or performed live in 1988 nor was it going to appear on \"No Control\".\n\nThe song pays homage to King Crimson's \"21st Century Schizoid Man\", even incorporating some of its lyrics towards the end:\n\n\"Cat's foot iron claw\"<br>\n\"Neuro-surgeons scream for more\"<br>\n\"Innocents raped with napalm fire\"\n\nThe line \"everything I want I really need\" that follows is a play on \"21st Century Schizoid Man\"'s \"nothing he's got he really needs.\" The principal difference between the two versions is after that line. On original Against the Grain, as the song fades out, Graffin sings the title of the song four more times with a different word instead of \"digital\" (including \"21st Century Schizoid Boy\" in reference to King Crimson's song) backed with another guitar solo. Stranger Than Fiction version ends with one final \"Ain't life a mystery?\" line.\n\nExcerpt from a 2010 interview with Greg Graffin in Scientific American magazine:\n\nQ: \"Your most famous song is \"21st Century Digital Boy,\" which pokes fun at our gadget-laden era.\"\n\nA: \"Oh no, we love technology and gadgets. We use irony in 60 percent of our music. \"21st Century Digital Boy\" is an ironic twist characterizing the youth of today. The truth is that even though the song was written in 1990, it was clear that the youth were going to be affected for good and bad by digital technology. It's probably because we loved video games so much.\"\n\nThe \"Against the Grain\" version is available as downloadable content in both \"Rock Band 2\" and \"Guitar Hero World Tour\". \"Guitar Hero World Tour\" incorrectly notes 2004 instead of 1990 as its date of the song. The 2004 date could possibly be referring to \"Against the Grain\"'s remastered date.\n\n\"The Dylan Ratigan Show\", a television program on the news channel MSNBC, used the song as background music during a segment targeting \"Facebook addiction\".\n\n\n\"21st Century Digital Girl\" is the third and final single from the album \"21st Century\" by German trance group Groove Coverage. The song is an adaptation of Bad Religion's \"21st Century Digital Boy\".\n"}
{"id": "961605", "url": "https://en.wikipedia.org/wiki?curid=961605", "title": "ACES (computational chemistry)", "text": "ACES (computational chemistry)\n\nAces II (Advanced Concepts in Electronic Structure Theory) is an ab initio computational chemistry package for performing high-level quantum chemical ab initio calculations. Its major strength is the accurate calculation of atomic and molecular energies as well as properties using many-body techniques such as many-body perturbation theory (MBPT) and, in particular coupled cluster techniques to treat electron correlation. The development of ACES II began in early 1990 in the group of Professor Rodney J. Bartlett at the Quantum Theory Project (QTP) of the University of Florida in Gainesville. There, the need for more efficient codes had been realized and the idea of writing an entirely new program package emerged. During 1990 and 1991 John F. Stanton, Jürgen Gauß, and John D. Watts, all of them at that time postdoctoral researchers in the Bartlett group, supported by a few students, wrote the backbone of what is now known as the ACES II program package. The only parts which were not new coding efforts were the integral packages (the MOLECULE package of J. Almlöf, the VPROP package of P.R. Taylor, and the integral derivative package ABACUS of T. Helgaker, P. Jorgensen J. Olsen, and H.J. Aa. Jensen). The latter was modified extensively for adaptation with Aces II, while the others remained very much in their original forms.\n\nUltimately, two different versions of the program evolved. The first was maintained by the Bartlett group at the University of Florida, and the other (known as ACESII-MAB) was maintained by groups at the University of Texas, Universitaet Mainz in Germany, and ELTE in Budapest, Hungary. The latter has recently been renamed as CFOUR.\n\nAces III is a parallel implementation that was released in the fall of 2008. The effort led to definition of a new architecture for scalable parallel software called the super instruction architecture. The design and creation of software is divided into two parts:\n\n\nThe ACES III program consists of 580,000 lines of SIAL code of which 200,000 lines are comments, and 230,000 lines of C/C++ and Fortran of which 62,000 lines are comments.\n\n\n"}
{"id": "46727979", "url": "https://en.wikipedia.org/wiki?curid=46727979", "title": "ANVUR", "text": "ANVUR\n\nANVUR (Agenzia Nazionale di Valutazione del Sistema Universitario e della Ricerca) is the Italian National Agency for the Evaluation of the University and Research Systems. ANVUR was established by a 2006 law with the objective of improving meritocracy in Italian academic research. It was based on Aeres in France and the Research Excellence Framework (REF) in the United Kingdom.\n\nANVUR began to compile its assessment of Italian research, the \"VQR\" (eValuation of the Quality of Research) in November 2011, assessing 95 universities, 21 research agencies or institutes, and 17 inter-university consortia.\n\n"}
{"id": "6228251", "url": "https://en.wikipedia.org/wiki?curid=6228251", "title": "Abisko Scientific Research Station", "text": "Abisko Scientific Research Station\n\nThe Abisko Scientific Research Station (ANS) is a field research station managed by the Swedish Polar Research Secretariat. Situated on the south shore of Lake Torneträsk, it lies at the edge of the Abisko National Park. The station conducts ecological, geological, geomorphological and meteorological research in subarctic environments and each year about 500 scientists visit from all over the world. The varied geological, topographical and climatic conditions of the area allow it to be inhabited by a range of flora and fauna. These features, which have caused the area to be given National Park status, also make it an important place for scientific research, particularly of alpine and subalpine ecosystems.\n\nThe first proper field station to be established in the area dates back to 1903, however the official station has only been affiliated with the Royal Swedish Academy of Sciences since 1935. The station holds meteorological datasets extending back to 1913, including air and soil temperature, precipitation and UV radiation. The research station has been the site of many long-term experiments investigating climate change impacts and is part of the International Tundra Experiment (ITEX) which is an international collaboration investigating the effects of environmental change on plants in circumpolar ecosystems. Since December 2010 the station is managed by the Swedish Polar Research Secretariat.\n\nThe research station today consists of laboratories and general workrooms containing a variety of scientific equipment, and also of various 'outposts' where research is carried out \"in situ\". It is used for research, teaching and meetings, and has a large collection of scientific books and papers available on site.\n\nThough many research projects are carried out at the station regarding geography and biology in general, particular emphasis is placed on meteorology and plant ecology. Many of these projects overlap as the station hosts research into climate change in the region and the resulting changes to plant communities. In recent years, research has included work on permafrost degradation, the importance of winter climate change and tree-line dynamics.\n\nAbisko Scientific Research Station is a member of the international network University of the Arctic.\n\n\n"}
{"id": "18971312", "url": "https://en.wikipedia.org/wiki?curid=18971312", "title": "Alan S. Kornacki", "text": "Alan S. Kornacki\n\nAlan Stanley Kornacki (born May 4, 1952, in Bayonne, New Jersey) is an American geologist and retired Army colonel, currently the Senior Staff Geochemist at Shell International Exploration and Production Inc. He received a B.S. in Geology from University of Missouri–Rolla in 1974, before completing his M.S. and Ph.D. in Geology at Harvard University in 1984, on a Graduate Research Fellowship. His dissertation focused on refractory inclusions in Carbonaceous Chondrites. He began a career in the petroleum industry in 1985 when he joined Shell USA. In 1981, he was awarded the Nininger Meteorite Award, and in 2008 he was awarded a professional degree by University of Missouri–Rolla. Alan Kornacki is most known for his characterization of wax from deep water crude oil, an important obstacle in modern drilling and refining technology, and his research on new sources of hydrocarbons such as oil shale.\n\n\n"}
{"id": "970559", "url": "https://en.wikipedia.org/wiki?curid=970559", "title": "Albert H. Crews", "text": "Albert H. Crews\n\nAlbert Hanlin \"Al\" Crews Jr. (born March 23, 1929), (Col, USAF, Ret.), is a former American chemical and aeronautical engineer, and U.S. Air Force astronaut, who was briefly included in the X-20 Dyna-Soar program.\n\nHe was born on March 23, 1929, in El Dorado, Arkansas. He graduated in 1950 from the University of Louisiana at Lafayette (then named Southwestern Louisiana Institute) with a Bachelor of Science degree in Chemical Engineering. He earned a Master of Science degree in Aeronautical Engineering from the U.S. Air Force Institute of Technology in 1959.\n\nAs a USAF Test Pilot School graduate, he was selected as a military astronaut designee in the second group of X-20 Dyna-Soar astronauts on April 20, 1962, and assigned as a Dyna-Soar pilot on September 20, 1962. The Dyna-Soar program was cancelled in 1963. On November 12, 1965, he was selected as an astronaut in the first group for the Manned Orbiting Laboratory (MOL) program. He transferred to NASA Flight Crew Directorate at the Johnson Space Center, Houston, Texas, in June 1969 when the MOL program was cancelled. He remained a pilot for NASA, flying such aircraft as the \"Super Guppy\" outsize cargo transport, the WB-57F atmospheric research aircraft and the OV-095 SAIL Space Shuttle simulator until he retired at age 65.\n\nHe is married, with three children from his previous marriage: Gail, Marina and Kellee.\n\n"}
{"id": "48788602", "url": "https://en.wikipedia.org/wiki?curid=48788602", "title": "Allen Kent", "text": "Allen Kent\n\nAllen Kent (October 24, 1921 – May 1, 2014) was an information scientist.\n\nHe was born in New York City. At City College of New York he earned a degree in chemistry. During World War II, he served in the Army Air Corps. After the war, he worked on a classified project at MIT in mechanized document encoding and search.\n\nIn 1955, he helped found the Center for Documentation Communication Research at Western Reserve University. This was \"the first academic program in the field of mechanized information retrieval, first using cards, then utilizing new reel-to-reel tape technology.\" In the same year he introduce the measures of precision and recall in . In 1959, he wrote an article for Harper's magazine entitled, \"A Machine That Does Research\" which provided one of the first incites in mainstream media about how Americans lives can change due to electronic information technology. He joined the faculty of the University of Pittsburgh in 1963, where in 1970 he began the Department of Information Science. He retired from the university in 1992. At the time of his death, he was Distinguished Service Professor in the School of Information Sciences at the University of Pittsburgh The school named a scholarship after him.\n\n\n\n"}
{"id": "45234106", "url": "https://en.wikipedia.org/wiki?curid=45234106", "title": "Antelope Range and Livestock Research Station", "text": "Antelope Range and Livestock Research Station\n\nThe Antelope Range and Livestock Research Station is operated by South Dakota State University and its extension programs to improve ranching in South Dakota. The site is large, the largest of SDSU’s research stations. This land was owned by the state and operated as an pronghorn antelope preserve until 1947, when it was transferred to the agricultural university for research into \"the balance between cattle and sheep production and protection and renewal of range resources\". Current work focuses on sustainable beef and sheep production on rangeland. There are 120 head of beef cow and 400 sheep now on the range. North Dakota State University operates a similar research station at Hettinger, North Dakota, which often partners with SDSU's Antelope station for sheep research.\n\nThe land is east of Buffalo, South Dakota, south of South Dakota Highway 20. It lies in the middle of Harding County, South Dakota, in the far northwest corner of the state. The station is about west of Custer National Forest.\n\nAntelope Range and Livestock Research Station is one of seven South Dakota State University field stations. The others are:\n"}
{"id": "57940553", "url": "https://en.wikipedia.org/wiki?curid=57940553", "title": "Arctic Alaska-Chukotka terrane", "text": "Arctic Alaska-Chukotka terrane\n\nThe Arctic Alaska-Chukotka terrane (AAC) is a microcontinent that today encompasses the North Slope, Brooks Range, and Seward Peninsula of northern Alaska; the Chukotka Peninsula, New Siberia Islands, and Wrangel Island in eastern Siberia; and the continental shelves of the Bering, Beaufort, and Chukchi seas. \nComparable in size to Greenland, the AAC is the largest of the Neoproterozoic–Early Paleozoic continental fragments now dispersed around the Arctic Ocean; some of which possibly formed the continent Arctida.\n\nThe AAC originated on the shores of the Iapetus Ocean and is a composite terrane made of fragments from the Baltica, Laurentia, and Siberia continents, as well as the ocean floor of the Panthalassic ocean. The AAC has a complex geological history that includes the Grenville, Timanian, Caledonian–Appalachian, and Ellesmerian orogenies.\n\nThe Proterozoic–Carboniferous histories of Arctic Alaska and Chukotka are similar but their Triassic–Jurassic histories are apparently distinct. Whether or not they were separate blocks before the Mesozoic opening of the Amerasia Basin is disputed.\nThe age of the basement of the AAC remains enigmatic, hence also details about the microcontinent's ancient, tectonic history. It is, nevertheless, clear from Neoproterozoic igneous rocks that the AAC was not originally part of Laurentia, but most likely Baltica. The microcontinent was obviously involved in a series of magmatic events, beginning at 1.6–1.4 , and ending in the Avalonia–Cadomian orogeny.\n\n"}
{"id": "40378241", "url": "https://en.wikipedia.org/wiki?curid=40378241", "title": "Associated motion", "text": "Associated motion\n\nAssociated motion is a grammatical category whose main function is to associate a motion component to the event expressed by the verbal root.\n\nThis category is attested in Pama–Nyungan languages, where it was first discovered (Koch 1984, Wilkins 1991), in Tacanan (Guillaume 2006, 2008, 2009), and in Rgyalrong languages (Jacques 2013).\n\nLanguages with Associated Motion present a contrast between association motion and purposive motion verb constructions, as in the following examples from Jacques (2013:202-3):\n\nAlthough both examples have the same English translation, they differ in that (2) with the translocative associated motion prefix ɕ- implies that the buying did take place, while (1) with the motion verb does not. The distinction made by the translocative is similar to the distinction made in \"I went \"and\" bought things\".\n\n"}
{"id": "568209", "url": "https://en.wikipedia.org/wiki?curid=568209", "title": "Atkinson–Shiffrin memory model", "text": "Atkinson–Shiffrin memory model\n\nThe Atkinson–Shiffrin model (also known as the multi-store model or modal model) is a model of memory proposed in 1968 by Richard Atkinson and Richard Shiffrin. The model asserts that human memory has three separate components: \n\nSince its first publication this model has come under much scrutiny and has been criticized for various reasons (described below). However, it is notable for the significant influence it had in stimulating subsequent memory research.\n\nThe modal model of memories is an explanation of how memory processes work. The three-part multi-store model was first described by Atkinson and Shiffrin in 1968, though the idea of distinct memory stores was by no means a new idea at the time. William James described a distinction between primary and secondary memory in 1890, where primary memory consisted of thoughts held for a short time in consciousness and secondary memory consisted of a permanent, unconscious store. However, at the time the parsimony of separate memory stores was a contested notion. A summary of the evidence given for the distinction between long-term and short-term stores is given below. Additionally, Atkinson and Shiffrin included a sensory register alongside the previously theorized primary and secondary memory, as well as a variety of control processes which regulate the transfer of memory.\n\nFollowing its first publication, multiple extensions of the model have been put forth such as a precategorical acoustic store, the search of associative memory model, the perturbation model, and permastore. Additionally, alternative frameworks have been proposed, such as procedural reinstatement, a distinctiveness model, and Baddeley and Hitch's model of working memory, among others.\n\nWhen an environmental stimulus is detected by the senses it is briefly available in what Atkinson and Shiffrin called the \"sensory registers\" (also \"sensory buffers\" or \"sensory memory\"). Though this store is generally referred to as \"the sensory register\" or \"sensory memory\", it is actually composed of multiple registers, one for each sense. The sensory registers do not process the information carried by the stimulus, but rather detect and hold that information for use in short-term memory. For this reason Atkinson and Shiffrin also called the registers \"buffers\", as they prevent immense amounts of information from overwhelming higher-level cognitive processes. Information is only transferred to the short-term memory when attention is given to it, otherwise it decays rapidly and is forgotten.\n\nWhile it is generally agreed that there is a sensory register for each sense, most of the research in the area has focused on the visual and auditory systems.\n\nIconic memory, which is associated with the visual system, is perhaps the most researched of the sensory registers. The original evidence suggesting sensory stores which are separate to short-term and long-term memory was experimentally demonstrated for the visual system using a tachistoscope.\n\nIconic memory is only limited to field of vision. That is, as long as a stimulus has entered the field of vision there is no limit to the amount of visual information iconic memory can hold at any one time. As noted above, sensory registers do not allow for further processing of information, and as such iconic memory only holds information for visual stimuli such as shape, size, color and location (but not semantic meaning). As the higher-level processes are limited in their capacities, not all information from sensory memory can be conveyed. It has been argued that the momentary mental freezing of visual input allows for the selection of specific aspects which should be passed on for further memory processing. The biggest limitation of iconic memory is the rapid decay of the information stored there; items in iconic memory decay after only 0.5-1.0 seconds.\n\nEchoic memory, coined by Ulric Neisser, refers to information that is registered by the auditory system. As with iconic memory, echoic memory only holds superficial aspects of sound (e.g. pitch, tempo, or rhythm) and it has a nearly limitless capacity. Echoic memory is generally cited as having a duration of between 1.5 and 5 seconds depending on context but has been shown to last up to 20 seconds in the absence of competing information.\n\nWhile much of the information in sensory memory decays and is forgotten, some is attended to. The information that is attended is transferred to the \"short-term store\" (also \"short-term memory\", \"working memory\"; note that while these terms are often used interchangeably they were not originally intended to be used as such).\n\nAs with sensory memory, the information that enters short-term memory decays and is lost, but the information in the short-term store has a longer duration, approximately 18–20 seconds when the information is not being actively rehearsed, though it is possible that this depends on modality and could be as long as 30 seconds. Fortunately, the information can be held in the short-term store for much longer through what Atkinson and Shiffrin called rehearsal. For auditory information rehearsal can be taken in a literal sense: continually repeating the items. However, the term can be applied for any information that is attended to, such as when a visual image is intentionally held in mind. Finally, information in the short-term store does not have to be of the same modality as its sensory input. For example, written text which enters visually can be held as auditory information, and likewise auditory input can be visualized. On this model, rehearsal of information allows for it to be stored more permanently in the long-term store. Atkinson and Shiffrin discussed this at length for auditory and visual information but did not give much attention to the rehearsal/storage of other modalities due to the experimental difficulties of studying those modalities.\n\nThere is a limit to the amount of information that can be held in the short-term store: 7 ± 2 chunks. These chunks, which were noted by Miller in his seminal paper \"The Magical Number Seven, Plus or Minus Two\", are defined as independent items of information. It is important to note that some chunks are perceived as one unit though they could be broken down into multiple items, for example \"1066\" can be either the series of four digits \"1, 0, 6, 6\" or the semantically grouped item \"1066\" which is the year the Battle of Hastings was fought. Chunking allows for large amounts of information to be held in memory: 149283141066 is twelve individual items, well outside the limit of the short-term store, but it can be grouped semantically into the 4 chunks \"Columbus[1492] ate[8] pie[314→3.14→] at the Battle of Hastings[1066]\". Because short-term memory is limited in capacity, it severely limits the amount of information that can be attended to at any one time.\n\nThe \"long-term store\" (also \"long-term memory\") is a more or less permanent store. Information that is stored here can be \"copied\" and transferred to the short-term store where it can be attended to and manipulated.\n\nInformation is postulated to enter the long-term store from the short-term store more or less automatically. As Atkinson and Shiffrin model it, transfer from the short-term store to the long-term store is occurring for as long as the information is being attended to in the short-term store. In this way, varying amounts of attention result in varying amounts of time in short-term memory. Ostensibly, the longer an item is held in short-term memory, the stronger its memory trace will be in long-term memory. Some extraneous variables include: participant differences (Personal ability of an individual, these differentiate each participant and their capacity levels), demand characteristics (Which is the participants knowledge about experiment and what they bring to the experiment), experimenter effect (which is the effects the researchers have on participants through their expectations and possibility of bias behavior for self-fulfillment results), non-standardized instructions and procedures (The effect on participants if they receive different instructions, and if no consistency in procedure, also impacts from variations of conditions).\nAtkinson and Shiffrin cite evidence for this transfer mechanism in studies by Hebb (1961) and Melton (1963) which show that repeated rote repetition enhances long-term memory. One may also think to the original Ebbinghaus memory experiments showing that forgetting increases for items which are studied fewer times. Finally, the authors note that there are stronger encoding processes than simple rote rehearsal, namely relating the new information to information which has already made its way into the long-term store.\n\nIn this model, as with most models of memory, long-term memory is assumed to be nearly limitless in its duration and capacity. It is most often the case that brain structures begin to deteriorate and fail before any limit of learning is reached. This is not to assume that any item which is stored in long-term memory is accessible at any point in the lifetime. Rather, it is noted that the connections, cues, or associations to the memory deteriorate; the memory remains intact but unreachable.\n\nAt the time of the original publication there was a schism in the field of memory on the issue of a single process or dual-process model of memory, the two processes referring to short-term and long-term memory. Atkinson and Shiffrin cite hippocampal lesion studies as compelling evidence for a separation of the two stores. These studies showed that patients with bilateral damage to the hippocampal region had nearly no ability to form new long-term memories though their short-term memory remained intact. One may also be familiar with similar evidence found through the study of Henry Molaison, famously known as H.M., who underwent a severe bilateral medial temporal lobectomy which removed most of his hippocampal regions. These data suggest that there is indeed a clear separation between the short-term and long-term stores.\n\nOne of the early and central criticisms to the Atkinson-Shiffrin model was the inclusion of the sensory registers as part of memory. Specifically, the original model seemed to describe the sensory registers as both a structure and a control process. Parsimony would suggest that if the sensory registers are actually control processes, there is no need for a tri-partite system. Later revisions to the model addressed these claims and incorporated the sensory registers with the short-term store.\n\nBaddeley and Hitch have in turn called to question the specific structure of the short-term store, proposing that it is subdivided into multiple components. While the different components were not specifically addressed in the original Atkinson-Shiffrin model, the authors do note that little research has been done investigating the different ways sensory modalities may be represented in the short-term store. Thus the model of working memory given by Baddeley and Hitch should be viewed as a refinement of the original model.\n\nThe model has been further criticized as suggesting that rehearsal is the key process which initiates and facilitates transfer of information into LTM. There is very little evidence supporting this hypothesis, and long-term recall can in fact be better predicted by a levels-of-processing framework. In this framework, items which are encoded at a deeper, more semantic level are shown to have stronger traces in long-term memory. This criticism is somewhat unfounded as Atkinson and Shiffrin clearly state a difference between rehearsal and coding, where coding is akin to elaborative processes which levels-of-processing would call deep-processing. In this light, the levels-of-processing framework could be seen as more of an extension of the Atkinson-Shiffrin model rather than a refutation.\n\nIn the case of long-term memory, it is unlikely that different types of information, such as the motor skills to ride a bike, memory for vocabulary, and memory for personal life events are stored in the same fashion. Endel Tulving notes the importance of encoding specificity in long-term memory. To clarify, there are definite differences in the way information is stored depending on whether it is episodic (memories of events), procedural (knowledge of how to do things), or semantic (general knowledge). A short (non-inclusive) example comes from the study of Henry Molaison (H.M.): learning a simple motor task (tracing a star pattern in a mirror), which involves implicit and procedural long-term storage, is unaffected by bilateral lesioning of the hippocampal regions while other forms of long-term memory, like vocabulary learning (semantic) and memories for events, are severely impaired.\n\nFor more thorough and technical reviews of the main criticisms please refer to the following resources:\n\nDue to the above and other criticism through the 1970s, the original model underwent many revisions to account for phenomena it could not explain. The \"search of associative memory\" (SAM) model is the culmination of that work. The SAM model uses a two-phase memory system: short- and long-term stores. Unlike the original Atkinson–Shiffrin model, there is no sensory store in the SAM model.\n\nShort-term store takes on the form of a buffer, which has a limited capacity. The model assumes a buffer rehearsal system in which the buffer has a size, \"r\". Items enter the short-term store and accompany other items that are already present in the buffer, until size \"r\" has been reached. Once the buffer is at full capacity, when new items enter, they replace an item, \"r\", which already exists in the buffer. A probability of 1/\"r\" determines which already existing item will be replaced from the buffer. In general, items that have been in the buffer for longer are more likely to be replaced by new items.\n\nThe long-term store is responsible for storing relationships between different items and of items to their contexts. Context information refers to the situational and temporal factors present at the time when an item is in the short-term store, such as emotional feelings or environmental details. The amount of item-context information which is transferred to the long-term store is proportional to the amount of time that the item remains in the short-term store. On the other hand, the strength of the item-item associations is proportional to the amount of time that two items simultaneously existed in the short-term store.\n\nIt is best to show how items are recalled from the long-term store using an example. Assume a participant has just studied a list of word pairs and is now being tested on his memory of those pairs. If the prior list contained, \"blanket – ocean\", the test would be to recall \"ocean\" when prompted with \"blanket – ?\".\n\nMemories stored in long-term store are retrieved through a logical process involving the assembly of cues, sampling, recovery, and evaluation of recovery. According to the model, when an item needs to be recalled from memory the individual assembles the various cues for the item in the short-term store. In this case, the cues would be any cues surrounding the pair \"blanket – ocean\", like the words that preceded and followed it, what the participant was feeling at the time, how far into the list the words were, etc.\n\nUsing these cues the individual determines which area of the long-term store to search and then samples any items with associations to the cues. This search is automatic and unconscious, which is how the authors would explain how an answer \"pops\" into one's head. The items which are eventually recovered, or recalled, are those with the strongest associations to the cue item, here \"blanket\". Once an item has been recovered it is evaluated, here the participant would decide whether \"blanket - [recovered word]\" matches \"blanket – ocean\". If there is a match, or if the participant believes there is a match, the recovered word is output. Otherwise the search starts from the beginning using different cues or weighting cues differently if possible.\n\nThe usefulness of the SAM model and in particular its model of the short-term store is often demonstrated by its application to the recency effect in free recall. When serial-position curves are applied to SAM, a strong recency effect is observed, but this effect is strongly diminished when a distractor, usually arithmetic, is placed in between study and test trials. The recency effect occurs because items at the end of the test list are likely to still be present in short-term store and therefore retrieved first. However, when new information is processed, this item enters the short-term store and displaces other information from it. When a distracting task is given after the presentation of all items, information from this task displaces the last items from short-term store, resulting in a substantial reduction of recency.\n\nThe SAM model faces serious problems in accounting for long-term recency data and long-range contiguity data. While both of these effects are observed, the short-term store cannot account for the effects. Since a distracting task after the presentation of word pairs or large interpresentation intervals filled with distractors would be expected to displace the last few studied items from the short-term store, recency effects are still observed. According to the rules of the short-term store, recency and contiguity effects should be eliminated with these distractors as the most recently studied items would no longer be present in the short-term memory. Currently, the SAM model competes with single-store free recall models of memory, such as the Temporal Context Model. In this study the impact of age and level of maturation on participants ability to recall images from STM was investigated. It was hypothesised that students who fell in the age bracket of sixteen and 18 years would have greater recall ability and perform better on the STM recall imagery test than, eleven to thirteen year old. Additionally, the original model assumes that items in a particular list the only significant associations between items are those formed during the study portion of an experiment. In other words, it does not account for the effects of prior knowledge about to-be-studied items. A more recent extension of the model incorporates various features which allow the model to account for memory store for the effects of prior semantic knowledge and prior episodic knowledge. For years there has been investigation regarding the level and capacity of STM. past research that has influenced the experiment is Atkinson - Shiffrin memory model. The model asserts. The experiment is focusing on the 'short term store', which holds information for the duration of twelve seconds and has a capacity of 7 ± 2 items. this has influenced the researches amount of images presented and how long they are viewed for. The extension proposes a store for preexisting semantic associations; a contextual drift mechanism allowing for decontextualisation of knowledge, e.g. if you first learned a banana was a fruit because you put it in the same class as apple, you do not always have to think of apples to know bananas are fruits; a memory search mechanism that uses both episodic and semantic associations, as opposed to a unitary mechanism; and a large lexicon including both words from prior lists and unpresented words.\n\n"}
{"id": "34248218", "url": "https://en.wikipedia.org/wiki?curid=34248218", "title": "Buoyant density centrifugation", "text": "Buoyant density centrifugation\n\nBuoyant density centrifugation (also isopycnic centrifugation) uses the concept of buoyancy to separate molecules in solution.\n\nUsually a caesium chloride (CsCl) solution is used, but in the general case it's usually approximately the same density as the molecules that are to be centrifuged. The sample is put on top of the solution, and then the tube is spun at a very high speed for an extended time, at times lasting days. The CsCl molecules become densely packed toward the bottom, so even layers of different densities form. Since the original solution was approximately the same density, they go to a level where their density and the CsCl density are the same, to which they form a sharp, distinctive band.\n\nThis method very sharply separates molecules, and is so sharp that it can even separate different molecular isotopes from one another.\n\nBuoyant density of majority of DNA is 1.7g/cm3 which is equal to density of 6M CsCl solution. Buoyant density of DNA changes with its GC content.\n\n\n"}
{"id": "34366633", "url": "https://en.wikipedia.org/wiki?curid=34366633", "title": "Class kappa-ell function", "text": "Class kappa-ell function\n\nIn control theory, it is often required to check if a nonautonomous system is stable or not. To cope with this it is necessary to use some special comparison functions. Class formula_1 functions belong to this family:\n\nDefinition: A continuous function formula_2 is said to belong to class formula_1 if:\n\n"}
{"id": "37699639", "url": "https://en.wikipedia.org/wiki?curid=37699639", "title": "Cluster Lensing and Supernova survey with Hubble", "text": "Cluster Lensing and Supernova survey with Hubble\n\nThe Cluster Lensing And Supernova survey with Hubble (CLASH) is a multi-wavelength census of 25 massive galaxy clusters with Advanced Camera for Surveys (ACS) and Wide Field Camera 3 (WFC3) instruments of Hubble Space Telescope over a 3.5 year period (2010 - 2013).\n\nThe gravity of these massive clusters is powerful enough to visibly bend the path of light, somewhat like a magnifying glass and are thus useful tools for studying very distant objects. They also contribute to a range of topics in cosmology, as the precise nature of the lensed images encapsulates information about the properties of spacetime and the expansion of the cosmos.\n\nAs of November 2012, the CLASH has surveyed 20 clusters out of the 25. One of the galaxy clusters, MACS J0647+7015 was found to have gravitationally lensed the most distant galaxy (MACS0647-JD) then ever imaged, in 2012.\n\nIn 2013, one study utilizing CLASH data found that RX J1347.5-1145 had intense gravitational bending of light such that 8 images of the same object were detected. (See Gravitational lensing)\n\nThe Principal Investigator of the CLASH program is Marc Postman.\n\nList of clusters is:\n"}
{"id": "2906487", "url": "https://en.wikipedia.org/wiki?curid=2906487", "title": "Comet seeker", "text": "Comet seeker\n\nA comet seeker is a type of small telescope adapted especially to searching for comets: commonly of short focal length and large aperture, in order to secure the greatest brilliancy of light.\n\nFor example, in 1842 a 4-inch (10 cm) aperture Comet Seeker was added to Markree Observatory, which was used to discover 9 Metis in 1848.\n\n\n"}
{"id": "543428", "url": "https://en.wikipedia.org/wiki?curid=543428", "title": "Compton Gamma Ray Observatory", "text": "Compton Gamma Ray Observatory\n\nThe Compton Gamma Ray Observatory (CGRO) was a space observatory detecting photons with energies from 20 keV to 30 GeV, in Earth orbit from 1991 to 2000. It featured four main telescopes in one spacecraft, covering X-rays and gamma rays, including various specialized sub-instruments and detectors. Following 14 years of effort, the observatory was launched from Space Shuttle \"Atlantis\" during STS-37 on April 5, 1991, and operated until its deorbit on June 4, 2000. It was deployed in low earth orbit at to avoid the Van Allen radiation belt. It was the heaviest astrophysical payload ever flown at that time at .\n\nCosting $617 million, the CGRO was part of NASA's \"Great Observatories\" series, along with the Hubble Space Telescope, the Chandra X-ray Observatory, and the Spitzer Space Telescope. It was the second of the series to be launched into space, following the Hubble Space Telescope. CGRO was named after Arthur Holly Compton (Washington University in St. Louis), Nobel prize winner, for work involved with gamma ray physics. CGRO was built by TRW (now Northrop Grumman Aerospace Systems) in Redondo Beach, California. CGRO was an international collaboration and additional contributions came from the European Space Agency and various universities, as well as the U.S. Naval Research Laboratory.\n\nSuccessors to CGRO include the ESA INTEGRAL spacecraft (launched 2002), NASA's Swift Gamma-Ray Burst Mission (launched 2004) and NASA's Fermi Gamma-ray Space Telescope (launched 2008); all three remain operational as of 2017.\n\nCGRO carried a complement of four instruments that covered an unprecedented six decades of the electromagnetic spectrum, from 20 keV to 30 GeV (from 0.02 MeV to 30000 MeV). In order of increasing spectral energy coverage:\n\nThe Burst and Transient Source Experiment, (BATSE) by NASA's Marshall Space Flight Center searched the sky for gamma ray bursts (20 to >600 keV) and conducted full sky surveys for long-lived sources. It consisted of eight identical detector modules, one at each of the satellite's corners. Each module consisted of both a NaI(Tl) Large Area Detector (LAD) covering the 20 keV to ~2 MeV range, 50.48 cm in dia by 1.27 cm thick, and a 12.7 cm dia by 7.62 cm thick NaI Spectroscopy Detector, which extended the upper energy range to 8 MeV, all surrounded by a plastic scintillator in active anti-coincidence to veto the large background rates due to cosmic rays and trapped radiation. Sudden increases in the LAD rates triggered a high-speed data storage mode, the details of the burst being read out to telemetry later. Bursts were typically detected at rates of roughly one per day over the 9-year CGRO mission. A strong burst could result in the observation of many thousands of gamma rays within a time interval ranging from ~0.1 s up to about 100 s.\n\nThe Oriented Scintillation Spectrometer Experiment, (OSSE), by the Naval Research Laboratory detected gamma rays entering the field of view of any of four detector modules, which could be pointed individually, and were effective in the 0.05 to 10 MeV range. Each detector had a central scintillation spectrometer crystal of NaI(Tl) 12 in (303 mm) in diameter, by 4 in (102 mm) thick, optically coupled at the rear to a 3 in (76.2 mm) thick CsI(Na) crystal of similar diameter, viewed by seven photomultiplier tubes, operated as a phoswich: i.e., particle and gamma-ray events from the rear produced slow-rise time (~1 μs) pulses, which could be electronically distinguished from pure NaI events from the front, which produced faster (~0.25 μs) pulses. Thus the CsI backing crystal acted as an active anticoincidence shield, vetoing events from the rear. A further barrel-shaped CsI shield, also in electronic anticoincidence, surrounded the central detector on the sides and provided coarse collimation, rejecting gamma rays and charged particles from the sides or most of the forward field-of-view (FOV). A finer level of angular collimation was provided by a tungsten slat collimator grid within the outer CsI barrel, which collimated the response to a 3.8° x 11.4° FWHM rectangular FOV. A plastic scintillator across the front of each module vetoed charged particles entering from the front. The four detectors were typically operated in pairs of two. During a gamma-ray source observation, one detector would take observations of the source, while the other would slew slightly off source to measure the background levels. The two detectors would routinely switch roles, allowing for more accurate measurements of both the source and background. The instruments could slew with a speed of approximately 2 degrees per second.\n\nThe Imaging Compton Telescope, (COMPTEL) by the Max Planck Institute for Extraterrestrial Physics, the University of New Hampshire, Netherlands Institute for Space Research, and ESA's Astrophysics Division was tuned to the 0.75-30 MeV energy range and determined the angle of arrival of photons to within a degree and the energy to within five percent at higher energies. The instrument had a field of view of one steradian. For cosmic gamma-ray events, the experiment required two nearly simultaneous interactions, in a set of front and rear scintillators. Gamma rays would Compton scatter in a forward detector module, where the interaction energy \"E\", given to the recoil electron was measured, while the Compton scattered photon would then be caught in one of a second layer of scintillators to the rear, where its total energy, \"E\", would be measured. From these two energies, \"E\" and \"E\", the Compton scattering angle, angle θ, can be determined, along with the total energy, \"E + E\", of the incident photon. The positions of the interactions, in both the front and rear scintillators, was also measured. The vector, V, connecting the two interaction points determined a direction to the sky, and the angle θ about this direction, defined a cone about V on which the source of the photon must lie, and a corresponding \"event circle\" on the sky. Because of the requirement for a near coincidence between the two interactions, with the correct delay of a few nanoseconds, most modes of background production were strongly suppressed. From the collection of many event energies and event circles, a map of the positions of sources, along with their photon fluxes and spectra, could be determined.\n\nThe Energetic Gamma Ray Experiment Telescope, (EGRET) measured high energy (20 MeV to 30 GeV) gamma ray source positions to a fraction of a degree and photon energy to within 15 percent. EGRET was developed by NASA Goddard Space Flight Center, the Max Planck Institute for Extraterrestrial Physics, and Stanford University. Its detector operated on the principle of electron-positron pair production from high energy photons interacting in the detector. The tracks of the high-energy electron and positron created were measured within the detector volume, and the axis of the \"V\" of the two emerging particles projected to the sky. Finally, their total energy was measured in a large calorimeter scintillation detector at the rear of the instrument.\n\n\nGamma ray burst 990123 (23 January 1999) was one of the brightest bursts recorded at the time, and was the first GRB with an optical afterglow observed during the prompt gamma ray emission (a reverse shock flash). This allowed astronomers to measure a redshift of 1.6 and a distance of 3.2 Gpc. Combining the measured energy of the burst in gamma-rays and the distance, the total emitted energy assuming an isotropic explosion could be deduced and resulted in the direct conversion of approximately two solar masses into energy. This finally convinced the community that GRB afterglows resulted from highly collimated explosions, which strongly reduced the needed energy budget.\n\n\nIt was deployed to an altitude of 450 km on April 7, 1991 when it was first launched. Over time the orbit decayed and needed re-boosting to prevent atmospheric entry sooner than desired.\nRe-boosts:\n\nAfter one of its three gyroscopes failed in December 1999, the observatory was deliberately de-orbited. At the time, the observatory was still operational; however the failure of another gyroscope would have made de-orbiting much more difficult and dangerous. With some controversy, NASA decided in the interest of public safety that a controlled crash into an ocean was preferable to letting the craft come down on its own at random. Unlike the Hubble Space Telescope or the International Space Station, it was not designed for on-orbit repair and refurbishment. It entered the Earth's atmosphere on 4 June 2000, with the debris that did not burn up (\"six 1,800-pound aluminum I-beams and parts made of titanium, including more than 5,000 bolts\") falling into the Pacific Ocean.\n\nThis de-orbit was NASA's first intentional controlled de-orbit of a satellite.\n\n\n"}
{"id": "22645299", "url": "https://en.wikipedia.org/wiki?curid=22645299", "title": "Cooper Robertson", "text": "Cooper Robertson\n\nCooper Robertson is an international architecture and urban design firm headquartered in New York City.\n\nFounded as Alexander Cooper and Associates by Alex Cooper in 1979, the firm has designed a number of significant planned communities, urban infill, and transit-oriented developments, including Battery Park City in New York and the new communities of Celebration, Florida, Watercolor, Florida and Val d'Europe outside Paris, France. Also known for architecture, open space design, and university campus planning, the firm's work includes a plan for the expansion of Harvard University's campus into Allston, Massachusetts, MOMA QNS, (the Museum of Modern Art's temporary home in Queens, New York), the New Albany Country Club in New Albany, Ohio outside Columbus, the new Columbia University School of Social Work building in Upper Manhattan, the Visitor Center at the Lewis Ginter Botanical Garden in Richmond, Virginia, the Framework for Campus Planning for Yale University, Zuccotti Park (a one block park adjacent to the World Trade Center site on Liberty Street in Lower Manhattan), and numerous houses, many of which are in the Hamptons on the East End of Long Island and in the Caribbean.\n\nAlex Cooper and Jaquelin T. Robertson attended Yale College and Yale School of Architecture during the same period and also worked together in the New York City Department of City Planning. When Robertson joined the firm in 1988, the firm changed its name to Cooper, Robertson & Partners. In 2015, the firm rebranded and is currently known as Cooper Robertson.\n\nThe following is an incomplete list:\n\nThe following is an incomplete list:\n\n\n"}
{"id": "415929", "url": "https://en.wikipedia.org/wiki?curid=415929", "title": "Council for the Central Laboratory of the Research Councils", "text": "Council for the Central Laboratory of the Research Councils\n\nThe Council for the Central Laboratory of the Research Councils (CCLRC) was a UK government body that carried out civil research in science and engineering.\n\nThe CCLRC was created on 1 April 1995 as a non-departmental public body from the laboratories of the previous Science and Engineering Research Council including 1942 staff and an annual turnover of £106 million which had temporarily been controlled by the EPSRC. It operated at three locations:\n\n\nThe Diamond Light Source, was developed by the CCLRC at the Rutherford Appleton Laboratory and established as an independent company.\n\nThe CCLRC was established:\n\na) to promote high quality scientific and engineering research by providing facilities and technical expertise in support of basic, strategic and applied research programmes;\n\nb) to support the advancement of knowledge and technology, thereby contributing to the economic competitiveness of Our United Kingdom and the quality of life;\n\nc) to provide advice, disseminate knowledge, and promote public understanding in the fields of science, engineering and technology.\n\nWhich in practice meant that it administered the UK's large scale facilities for materials and biomolecular research, laser and space science and alternative energy exploration on behalf of the government, the other UK research funding bodies, universities, and corporate research and development.\n\nOver eighty per cent of the funding for the programmes carried out by the CCLRC came through partnership with other UK Research Councils, whilst the remaining twenty per cent came through partnership with industrial and overseas organisations. Some 10,000 researchers are estimated to have used CLRC's facilities and services each year - usually working in close collaboration with CLRC's support scientists and engineers to get the best from the facilities and expertise available.\n\nAs well as operating as a single entity, the Council (CCLRC) also operated its own wholly owned trading\nsubsidiary, Central Laboratory Innovation and Knowledge Transfer Limited (CLIK).\n\nBy 2006 annual expenditure had nearly doubled since CCLRC's foundation to £199.8 million as the Council's international role expanded to include the payment of the UK's subscriptions to facilities at the ILL and ESRF.\n\nOn 1 April 2007 CCLRC merged with PPARC to form the Science and Technology Facilities Council so that a single organisation was responsible for providing UK scientists with access to large scientific facilities in the UK and elsewhere in the world, including CERN, ESA and ESO.\n\nGiven the wide breadth of its mission there was plenty of room for the CCLRC staff to promote different activities of the Council. Consequently its branding was always a problem.\n\nBetween 1994 and 1995 while the laboratories were temporarily operated by the EPSRC, the abbreviation \"DRAL\" was used to brand them as distinct from the funding Council. When the CCLRC was first established in 1995 the Director General of Research Councils did not want it to be confused with the funding Councils, so he did not want the abbreviation to include the letters \"RC\" at the end, but to be simply \"CCL\" - Council of the Central Laboratories. This only lasted for a few months when the accepted abbreviation became \"CLRC\" - Central Laboratory of the Research Councils. Since the full name included an extra word a confusion was created as to why the abbreviation appeared to be that of a different name than the actual one. To end this confusion, in 2002 the abbreviation was changed to the complete acronym of \"CCLRC\".\n\n\n\n"}
{"id": "14672145", "url": "https://en.wikipedia.org/wiki?curid=14672145", "title": "Edmond Jean-Baptiste Fleutiaux", "text": "Edmond Jean-Baptiste Fleutiaux\n\nEdmond Jean Baptiste Fleutiaux (22 October 1858, Argenteuil, Val-d'Oise – 1951) was a French entomologist who specialised in Coleoptera.\n\nFleutiaux worked on the beetle fauna of Southeast Asia, particularly French Indochina and Africa.\nHe wrote \"Catalogue systématique des Cicindelidae décruits depuis Linné\" (1892)and \"Revision des Eucnemides africains\" (1945). His collection is conserved by the Muséum national d'histoire naturelle.Fleutiaux was a member of the Société entomologique de France.\n"}
{"id": "347832", "url": "https://en.wikipedia.org/wiki?curid=347832", "title": "Fifth generation computer", "text": "Fifth generation computer\n\nThe Fifth Generation Computer Systems [Present and Future] (FGCS) was an initiative by Japan's Ministry of International Trade and Industry, begun in 1982, to create a computer using massively parallel computing/processing. It was to be the result of a massive government/industry research project in Japan during the 1980s. It aimed to create an \"epoch-making computer\" with supercomputer-like performance and to provide a platform for future developments in artificial intelligence. There was also an unrelated Russian project also named as a fifth-generation computer (see Kronos (computer)).\n\nIn his \"Trip report\" paper, Prof. Ehud Shapiro (which focused the FGCS project on concurrent logic programming as the software foundation for the project) captured the rationale and motivations driving this huge project: \"As part of Japan's effort to become a leader in the computer industry, the Institute for New Generation Computer Technology has launched a revolutionary ten-year plan for the development of large computer systems which will be applicable to knowledge information processing systems. These Fifth Generation computers will be built around the concepts of logic programming. In order to refute the accusation that Japan exploits knowledge from abroad without contributing any of its own, this project will stimulate original research and will make its results available to the international research community.\"\n\nThe term \"fifth generation\" was intended to convey the system as being a leap beyond existing machines. In the history of computing hardware, computers using vacuum tubes were called the first generation; transistors and diodes, the second; integrated circuits, the third; and those using microprocessors, the fourth. Whereas previous computer generations had focused on increasing the number of logic elements in a single CPU, the fifth generation, it was widely believed at the time, would instead turn to massive numbers of CPUs for added performance.\n\nThe project was to create the computer over a ten-year period, after which it was considered ended and investment in a new \"sixth generation\" project would begin. Opinions about its outcome are divided: either it was a failure, or it was ahead of its time.\n\nIn the late 1960s till the early 1970s, there was much talk about \"generations\" of computer hardware — usually \"three generations\".\n\nOmitted from this taxonomy is the \"zeroth-generation\" computer based on metal gears (such as the IBM 407) or mechanical relays (such as the Mark I), and the post-third-generation computers based on Very Large Scale Integrated (VLSI) circuits.\n\nThere was also a parallel set of generations for software:\n\nThroughout these multiple generations up to the 1990s, Japan had largely been a follower in the computing arena, building computers following U.S. and British leads. The Ministry of International Trade and Industry (MITI) decided to attempt to break out of this follow-the-leader pattern, and in the mid-1970s started looking, on a small scale, into the future of computing. They asked the Japan Information Processing Development Center (JIPDEC) to indicate a number of future directions, and in 1979 offered a three-year contract to carry out more in-depth studies along with industry and academia. It was during this period that the term \"fifth-generation computer\" started to be used.\n\nPrior to the 1970s, MITI guidance had successes such as an improved steel industry, the creation of the oil supertanker, the automotive industry, consumer electronics, and computer memory. MITI decided that the future was going to be information technology. However, the Japanese language, in both written and spoken form, presented and still presents major obstacles for computers. These hurdles could not be taken lightly. So MITI held a conference and invited people around the world to help them.\n\nThe primary fields for investigation from this initial project were:\n\n\nThe project imagined an \"epoch-making computer\" with supercomputer-like performance using massively parallel computing/processing. The aim was to build parallel computers for artificial intelligence applications using concurrent logic programming. The FGCS project and its vast findings contributed greatly to the development of the concurrent logic programming field.\n\nThe target defined by the FGCS project was to develop \"Knowledge Information Processing systems\" (roughly meaning, applied Artificial Intelligence). The chosen tool to implement this goal was logic programming. Logic programming approach as was characterized by Maarten Van Emden – one of its founders – as:\n\n\nMore technically, it can be summed up in two equations:\n\n\nThe Axioms typically used are universal axioms of a restricted form, called Horn-clauses or definite-clauses. The statement proved in a computation is an existential statement. The proof is constructive, and provides values for the existentially quantified variables: these values constitute the output of the computation.\n\nLogic programming was thought as something that unified various gradients of computer science (software engineering, databases, computer architecture and artificial intelligence). It seemed that logic programming was the \"missing link\" between knowledge engineering and parallel computer architectures.\n\nIn 1982, during a visit to the ICOT, Ehud Shapiro invented Concurrent Prolog, a novel concurrent programming language that integrated logic programming and concurrent programming. Concurrent Prolog is a logic programming language designed for concurrent programming and parallel execution. It is a process oriented language, which embodies dataflow synchronization and guarded-command indeterminacy as its basic control mechanisms. Shapiro described the language in a Report marked as ICOT Technical Report 003, which presented a Concurrent Prolog interpreter written in Prolog. Shapiro's work on Concurrent Prolog inspired a change in the direction of the FGCS from focusing on parallel implementation of Prolog to the focus on concurrent logic programming as the software foundation for the project. It also inspired the concurrent logic programming language Guarded Horn Clauses (GHC) by Ueda, which was the basis of KL1, the programming language that was finally designed and implemented by the FGCS project as its core programming language.\n\nThe project imagined a parallel processing computer running on top of massive databases (as opposed to a traditional filesystem) using a logic programming language to define and access the data. They envisioned building a prototype machine with performance between 100M and 1G LIPS, where a LIPS is a \"Logical Inference Per Second.\" At the time typical workstation machines were capable of about 100k LIPS. They proposed to build this machine over a ten-year period, 3 years for initial R&D, 4 years for building various subsystems, and a final 3 years to complete a working prototype system. In 1982 the government decided to go ahead with the project, and established the Institute for New Generation Computer Technology (ICOT) through joint investment with various Japanese computer companies.\n\nSo ingrained was the belief that parallel computing was the future of all performance gains that the Fifth-Generation project generated a great deal of apprehension in the computer field. After having seen the Japanese take over the consumer electronics field during the 1970s and apparently doing the same in the automotive world during the 1980s, the Japanese in the 1980s had a reputation for invincibility. Soon parallel projects were set up in the US as the Strategic Computing Initiative and the Microelectronics and Computer Technology Corporation (MCC), in the UK as Alvey, and in Europe as the European Strategic Program on Research in Information Technology (ESPRIT), as well as ECRC (European Computer Research Centre) in Munich, a collaboration between ICL in Britain, Bull in France, and Siemens in Germany.\n\nFive running Parallel Inference Machines (PIM) were eventually produced: PIM/m, PIM/p, PIM/i, PIM/k, PIM/c. The project also produced applications to run on these systems, such as the parallel database management system Kappa, the legal reasoning system \"HELIC-II\", and the automated theorem prover \"MGTP\", as well as applications to bioinformatics.\n\nThe FGCS Project did not meet with commercial success for reasons similar to the Lisp machine companies and Thinking Machines. The highly parallel computer architecture was eventually surpassed in speed by less specialized hardware (for example, Sun workstations and Intel x86 machines). The project did produce a new generation of promising Japanese researchers. But after the FGCS Project, MITI stopped funding large-scale computer research projects, and the research momentum developed by the FGCS Project dissipated. However MITI/ICOT embarked on a Sixth Generation Project in the 1990s.\n\nA primary problem was the choice of concurrent logic programming as the bridge between the parallel computer architecture and the use of logic as a knowledge representation and problem solving language for AI applications. This never happened cleanly; a number of languages were developed, all with their own limitations. In particular, the committed choice feature of concurrent constraint logic programming interfered with the logical semantics of the languages.\n\nAnother problem was that existing CPU performance quickly pushed through the \"obvious\" barriers that experts perceived in the 1980s, and the value of parallel computing quickly dropped to the point where it was for some time used only in niche situations. Although a number of workstations of increasing capacity were designed and built over the project's lifespan, they generally found themselves soon outperformed by \"off the shelf\" units available commercially.\n\nThe project also suffered from being on the wrong side of the technology curve. During its lifespan, GUIs became mainstream in computers; the internet enabled locally stored databases to become distributed; and even simple research projects provided better real-world results in data mining. Moreover, the project found that the promises of logic programming were largely negated by the use of committed choice.\n\nAt the end of the ten-year period, the project had spent over ¥50 billion (about US$400 million at 1992 exchange rates) and was terminated without having met its goals. The workstations had no appeal in a market where general purpose systems could now take over their job and even outrun them. This is parallel to the Lisp machine market, where rule-based systems such as CLIPS could run on general-purpose computers, making expensive Lisp machines unnecessary.\n\nIn spite of the possibility of considering the project a failure, many of the approaches envisioned in the Fifth-Generation project, such as logic programming distributed over massive knowledge-bases, are now being re-interpreted in current technologies. For example, the Web Ontology Language (OWL) employs several layers of logic-based knowledge representation systems. It appears, however, that these new technologies reinvented rather than leveraged approaches investigated under the Fifth-Generation initiative.\n\nIn the early 21st century, many flavors of parallel computing began to proliferate, including multi-core architectures at the low-end and massively parallel processing at the high end. When clock speeds of CPUs began to move into the 3–5 GHz range, CPU power dissipation and other problems became more important. The ability of industry to produce ever-faster single CPU systems (linked to Moore's Law about the periodic doubling of transistor counts) began to be threatened. Ordinary consumer machines and game consoles began to have parallel processors like the Intel Core, AMD K10, and Cell (microprocessor). Graphics card companies like Nvidia and AMD began introducing large parallel systems like CUDA and open(CL). Again, however, it is not clear that these developments were facilitated in any significant way by the Fifth-Generation project.\n\nIn summary, a strong case can be made that the Fifth-Generation project was ahead of its time, but it is debatable whether this counters or justifies claims that it was a failure.\n\n"}
{"id": "715679", "url": "https://en.wikipedia.org/wiki?curid=715679", "title": "Gaugino", "text": "Gaugino\n\nIn supersymmetry theories of particle physics, a gaugino is the hypothetical fermionic supersymmetric field quantum (superpartner) of a gauge field, as predicted by gauge theory combined with supersymmetry. All gauginos have spin 1/2, except for gravitino (spin 3/2).\n\nIn the minimal supersymmetric extension of the standard model the following gauginos exist:\n\n\nSometimes the term \"electroweakinos\" is used to refer to winos and binos and on occasion also higgsinos.\n\nGauginos mix with higgsinos, the superpartners of the Higgs field's degrees of freedom, to form mass eigenstates called neutralinos, which are electrically neutral, and charginos, which are electrically charged.\n\nIn many supersymmetric models, the lightest supersymmetric particle (LSP), often a neutralino such as the photino, is stable. In that case it is a WIMP and a candidate for dark matter.\n\n"}
{"id": "3025648", "url": "https://en.wikipedia.org/wiki?curid=3025648", "title": "Georgi Bliznakov", "text": "Georgi Bliznakov\n\nGeorgi Bliznakov ( (14 November 1920 – April 2004) was an eminent Bulgarian chemist. He was head of the Department of Inorganic Chemistry and rector at Sofia University, director of the Institute of Inorganic Chemistry of the Bulgarian Academy of Sciences and vice-chairman of the academy.\n\nBliznakov was born in 1920 in Berkovitsa, Bulgaria. After graduating in chemistry in 1943 from Sofia University he worked in industry until 1946, when he joined the University of Varna as an assistant in inorganic and physical chemistry. In 1949 he joined the Department of Physical Chemistry at the Polytechnic Institute in Sofia (now the University of Chemical Technology and Metallurgy) as an assistant where he stayed until moving to the Department of Inorganic Chemistry at Sofia University in 1951, becoming full professor and head of department in 1960. He stayed in that post until 1989, serving as university rector from 1981 to 1985.\n\nBliznakov's main area of research was crystallization. He was the first to introduce adsorption as a thermodynamic factor in crystal growth, and studied catalysis, particular in relation to ammonia oxidation, the preparation of pure substances, radiochemical processes, and the effect of impurities on the linear crystallization rate.\n\nHe is the co-author of some of the most popular secondary school chemistry text books in Bulgaria.\n"}
{"id": "16068751", "url": "https://en.wikipedia.org/wiki?curid=16068751", "title": "Glossary of geology", "text": "Glossary of geology\n\nThis page is a glossary of geology.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "168651", "url": "https://en.wikipedia.org/wiki?curid=168651", "title": "High-performance liquid chromatography", "text": "High-performance liquid chromatography\n\nHigh-performance liquid chromatography (HPLC; formerly referred to as high-pressure liquid chromatography) is a technique in analytical chemistry used to separate, identify, and quantify each component in a mixture. It relies on pumps to pass a pressurized liquid solvent containing the sample mixture through a column filled with a solid adsorbent material. Each component in the sample interacts slightly differently with the adsorbent material, causing different flow rates for the different components and leading to the separation of the components as they flow out of the column.\n\nHPLC has been used for manufacturing (\"e.g.\", during the production process of pharmaceutical and biological products), legal (\"e.g.\", detecting performance enhancement drugs in urine), research (\"e.g.\", separating the components of a complex biological sample, or of similar synthetic chemicals from each other), and medical (\"e.g.\", detecting vitamin D levels in blood serum) purposes.\n\nChromatography can be described as a mass transfer process involving adsorption. HPLC relies on pumps to pass a pressurized liquid and a sample mixture through a column filled with adsorbent, leading to the separation of the sample components. The active component of the column, the adsorbent, is typically a granular material made of solid particles (\"e.g.\", silica, polymers, etc.), 2–50 μm in size. The components of the sample mixture are separated from each other due to their different degrees of interaction with the adsorbent particles. The pressurized liquid is typically a mixture of solvents (\"e.g.\", water, acetonitrile and/or methanol) and is referred to as a \"mobile phase\". Its composition and temperature play a major role in the separation process by influencing the interactions taking place between sample components and adsorbent. These interactions are physical in nature, such as hydrophobic (dispersive), dipole–dipole and ionic, most often a combination.\n\nHPLC is distinguished from traditional (\"low pressure\") liquid chromatography because operational pressures are significantly higher (50–350 bar), while ordinary liquid chromatography typically relies on the force of gravity to pass the mobile phase through the column. Due to the small sample amount separated in analytical HPLC, typical column dimensions are 2.1–4.6 mm diameter, and 30–250 mm length. Also HPLC columns are made with smaller adsorbent particles (2–50 μm in average particle size). This gives HPLC superior resolving power (the ability to distinguish between compounds) when separating mixtures, which makes it a popular chromatographic technique.\n\nThe schematic of a HPLC instrument typically includes a degasser, sampler, pumps, and a detector. The sampler brings the sample mixture into the mobile phase stream which carries it into the column. The pumps deliver the desired flow and composition of the mobile phase through the column. The detector generates a signal proportional to the amount of sample component emerging from the column, hence allowing for quantitative analysis of the sample components. A digital microprocessor and user software control the HPLC instrument and provide data analysis. Some models of mechanical pumps in a HPLC instrument can mix multiple solvents together in ratios changing in time, generating a composition gradient in the mobile phase. Various detectors are in common use, such as UV/Vis, photodiode array (PDA) or based on mass spectrometry. Most HPLC instruments also have a column oven that allows for adjusting the temperature at which the separation is performed.\n\nThe sample mixture to be separated and analyzed is introduced, in a discrete small volume (typically microliters), into the stream of mobile phase percolating through the column. The components of the sample move through the column at different velocities, which are a function of specific physical interactions with the adsorbent (also called stationary phase). The velocity of each component depends on its chemical nature, on the nature of the stationary phase (column) and on the composition of the mobile phase. The time at which a specific analyte elutes (emerges from the column) is called its retention time. The retention time measured under particular conditions is an identifying characteristic of a given analyte.\n\nMany different types of columns are available, filled with adsorbents varying in particle size, and in the nature of their surface (\"surface chemistry\"). The use of smaller particle size packing materials requires the use of higher operational pressure (\"backpressure\") and typically improves chromatographic resolution (\"i.e.\", the degree of separation between consecutive analytes emerging from the column). Sorbent particles may be hydrophobic or polar in nature.\n\nCommon mobile phases used include any miscible combination of water with various organic solvents (the most common are acetonitrile and methanol). Some HPLC techniques use water-free mobile phases (see Normal-phase chromatography below). The aqueous component of the mobile phase may contain acids (such as formic, phosphoric or trifluoroacetic acid) or salts to assist in the separation of the sample components. The composition of the mobile phase may be kept constant (\"isocratic elution mode\") or varied (\"gradient elution mode\") during the chromatographic analysis. Isocratic elution is typically effective in the separation of sample components that are very different in their affinity for the stationary phase. In gradient elution the composition of the mobile phase is varied typically from low to high eluting strength. The eluting strength of the mobile phase is reflected by analyte retention times with high eluting strength producing fast elution (=short retention times). A typical gradient profile in reversed phase chromatography might start at 5% acetonitrile (in water or aqueous buffer) and progress linearly to 95% acetonitrile over 5–25 minutes. Periods of constant mobile phase composition may be part of any gradient profile. For example, the mobile phase composition may be kept constant at 5% acetonitrile for 1–3 min, followed by a linear change up to 95% acetonitrile.\n\nThe chosen composition of the mobile phase (also called eluent) depends on the intensity of interactions between various sample components (\"analytes\") and stationary phase (\"e.g.\", hydrophobic interactions in reversed-phase HPLC). Depending on their affinity for the stationary and mobile phases analytes partition between the two during the separation process taking place in the column. This partitioning process is similar to that which occurs during a liquid–liquid extraction but is continuous, not step-wise. In this example, using a water/acetonitrile gradient, more hydrophobic components will elute (come off the column) late, once the mobile phase gets more concentrated in acetonitrile (\"i.e.\", in a mobile phase of higher eluting strength).\n\nThe choice of mobile phase components, additives (such as salts or acids) and gradient conditions depends on the nature of the column and sample components. Often a series of trial runs is performed with the sample in order to find the HPLC method which gives adequate separation.\n\nPrior to HPLC scientists used standard liquid chromatographic techniques. Liquid chromatographic systems were largely inefficient due to the flow rate of solvents being dependent on gravity. Separations took many hours, and sometimes days to complete. Gas chromatography (GC) at the time was more powerful than liquid chromatography (LC), however, it was believed that gas phase separation and analysis of very polar high molecular weight biopolymers was impossible. GC was ineffective for many biochemists because of the thermal instability of the solutes. As a result, alternative methods were hypothesized which would soon result in the development of HPLC.\n\nFollowing on the seminal work of Martin and Synge in 1941, it was predicted by Cal Giddings, Josef Huber, and others in the 1960s that LC could be operated in the high-efficiency mode by reducing the packing-particle diameter substantially below the typical LC (and GC) level of 150 μm and using pressure to increase the mobile phase velocity. These predictions underwent extensive experimentation and refinement throughout the 60s into the 70s. Early developmental research began to improve LC particles, and the invention of Zipax, a superficially porous particle, was promising for HPLC technology.\n\nThe 1970s brought about many developments in hardware and instrumentation. Researchers began using pumps and injectors to make a rudimentary design of an HPLC system. Gas amplifier pumps were ideal because they operated at constant pressure and did not require leak free seals or check valves for steady flow and good quantitation. Hardware milestones were made at Dupont IPD (Industrial Polymers Division) such as a low-dwell-volume gradient device being utilized as well as replacing the septum injector with a loop injection valve.\n\nWhile instrumentational developments were important, the history of HPLC is primarily about the history and evolution of particle technology. After the introduction of porous layer particles, there has been a steady trend to reduced particle size to improve efficiency. However, by decreasing particle size, new problems arose. The practical disadvantages stem from the excessive pressure drop needed to force mobile fluid through the column and the difficulty of preparing a uniform packing of extremely fine materials. Every time particle size is reduced significantly, another round of instrument development usually must occur to handle the pressure.\n\nPartition chromatography was one of the first kinds of chromatography that chemists developed. The partition coefficient principle has been applied in paper chromatography, thin layer chromatography, gas phase and liquid–liquid separation applications. The 1952 Nobel Prize in chemistry was earned by Archer John Porter Martin and Richard Laurence Millington Synge for their development of the technique, which was used for their separation of amino acids. Partition chromatography uses a retained solvent, on the surface or within the grains or fibers of an \"inert\" solid supporting matrix as with paper chromatography; or takes advantage of some coulombic and/or hydrogen donor interaction with the stationary phase. Analyte molecules partition between a liquid stationary phase and the eluent. Just as in Hydrophilic Interaction Chromatography (HILIC; a sub-technique within HPLC), this method separates analytes based on differences in their polarity. HILIC most often uses a bonded polar stationary phase and a mobile phase made primarily of acetonitrile with water as the strong component. Partition HPLC has been used historically on unbonded silica or alumina supports. Each works effectively for separating analytes by relative polar differences. HILIC bonded phases have the advantage of separating acidic, basic and neutral solutes in a single chromatographic run.\n\nThe polar analytes diffuse into a stationary water layer associated with the polar stationary phase and are thus retained. The stronger the interactions between the polar analyte and the polar stationary phase (relative to the mobile phase) the longer the elution time. The interaction strength depends on the functional groups part of the analyte molecular structure, with more polarized groups (\"e.g.\", hydroxyl-) and groups capable of hydrogen bonding inducing more retention. Coulombic (electrostatic) interactions can also increase retention. Use of more polar solvents in the mobile phase will decrease the retention time of the analytes, whereas more hydrophobic solvents tend to increase retention times.\n\nNormal–phase chromatography was one of the first kinds of HPLC that chemists developed. Also known as normal-phase HPLC (NP-HPLC) this method separates analytes based on their affinity for a polar stationary surface such as silica, hence it is based on analyte ability to engage in polar interactions (such as hydrogen-bonding or dipole-dipole type of interactions) with the sorbent surface. NP-HPLC uses a non-polar, non-aqueous mobile phase (\"e.g.\", Chloroform), and works effectively for separating analytes readily soluble in non-polar solvents. The analyte associates with and is retained by the polar stationary phase. Adsorption strengths increase with increased analyte polarity. The interaction strength depends not only on the functional groups present in the structure of the analyte molecule, but also on steric factors. The effect of steric hindrance on interaction strength allows this method to resolve (separate) structural isomers.\n\nThe use of more polar solvents in the mobile phase will decrease the retention time of analytes, whereas more hydrophobic solvents tend to induce slower elution (increased retention times). Very polar solvents such as traces of water in the mobile phase tend to adsorb to the solid surface of the stationary phase forming a stationary bound (water) layer which is considered to play an active role in retention. This behavior is somewhat peculiar to normal phase chromatography because it is governed almost exclusively by an adsorptive mechanism (\"i.e.\", analytes interact with a solid surface rather than with the solvated layer of a ligand attached to the sorbent surface; see also reversed-phase HPLC below). Adsorption chromatography is still widely used for structural isomer separations in both column and thin-layer chromatography formats on activated (dried) silica or alumina supports.\n\nPartition- and NP-HPLC fell out of favor in the 1970s with the development of reversed-phase HPLC because of poor reproducibility of retention times due to the presence of a water or protic organic solvent layer on the surface of the silica or alumina chromatographic media. This layer changes with any changes in the composition of the mobile phase (\"e.g.\", moisture level) causing drifting retention times.\n\nRecently, partition chromatography has become popular again with the development of Hilic bonded phases which demonstrate improved reproducibility, and due to a better understanding of the range of usefulness of the technique.\n\nThe basic principle of displacement chromatography is:\nA molecule with a high affinity for the chromatography matrix (the displacer) will compete effectively for binding sites, and thus displace all molecules with lesser affinities.\nThere are distinct differences between displacement and elution chromatography. In elution mode, substances typically emerge from a column in narrow, Gaussian peaks. Wide separation of peaks, preferably to baseline, is desired in order to achieve maximum purification. The speed at which any component of a mixture travels down the column in elution mode depends on many factors. But for two substances to travel at different speeds, and thereby be resolved, there must be substantial differences in some interaction between the biomolecules and the chromatography matrix. Operating parameters are adjusted to maximize the effect of this difference. In many cases, baseline separation of the peaks can be achieved only with gradient elution and low column loadings. Thus, two drawbacks to elution mode chromatography, especially at the preparative scale, are operational complexity, due to gradient solvent pumping, and low throughput, due to low column loadings. Displacement chromatography has advantages over elution chromatography in that components are resolved into consecutive zones of pure substances rather than “peaks”. Because the process takes advantage of the nonlinearity of the isotherms, a larger column feed can be separated on a given column with the purified components recovered at significantly higher concentration.\n\nReversed phase HPLC (RP-HPLC) has a non-polar stationary phase and an aqueous, moderately polar mobile phase. One common stationary phase is a silica which has been surface-modified with RMeSiCl, where R is a straight chain alkyl group such as CH or CH. With such stationary phases, retention time is longer for molecules which are less polar, while polar molecules elute more readily (early in the analysis). An investigator can increase retention times by adding more water to the mobile phase; thereby making the affinity of the hydrophobic analyte for the hydrophobic stationary phase stronger relative to the now more hydrophilic mobile phase. Similarly, an investigator can decrease retention time by adding more organic solvent to the eluent. RP-HPLC is so commonly used that it is often incorrectly referred to as \"HPLC\" without further specification. The pharmaceutical industry regularly employs RP-HPLC to qualify drugs before their release.\n\nRP-HPLC operates on the principle of hydrophobic interactions, which originates from the high symmetry in the dipolar water structure and plays the most important role in all processes in life science. RP-HPLC allows the measurement of these interactive forces. The binding of the analyte to the stationary phase is proportional to the contact surface area around the non-polar segment of the analyte molecule upon association with the ligand on the stationary phase. This solvophobic effect is dominated by the force of water for \"cavity-reduction\" around the analyte and the C-chain versus the complex of both. The energy released in this process is proportional to the surface tension of the eluent (water: 7.3 J/cm², methanol: 2.2 J/cm²) and to the hydrophobic surface of the analyte and the ligand respectively. The retention can be decreased by adding a less polar solvent (methanol, acetonitrile) into the mobile phase to reduce the surface tension of water. Gradient elution uses this effect by automatically reducing the polarity and the surface tension of the aqueous mobile phase during the course of the analysis.\n\nStructural properties of the analyte molecule play an important role in its retention characteristics. In general, an analyte with a larger hydrophobic surface area (C–H, C–C, and generally non-polar atomic bonds, such as S-S and others) is retained longer because it is non-interacting with the water structure. On the other hand, analytes with higher polar surface area (conferred by the presence of polar groups, such as -OH, -NH, COO or -NH in their structure) are less retained as they are better integrated into water. Such interactions are subject to steric effects in that very large molecules may have only restricted access to the pores of the stationary phase, where the interactions with surface ligands (alkyl chains) take place. Such surface hindrance typically results in less retention.\n\nRetention time increases with hydrophobic (non-polar) surface area. Branched chain compounds elute more rapidly than their corresponding linear isomers because the overall surface area is decreased. Similarly organic compounds with single C–C bonds elute later than those with a C=C or C–C triple bond, as the double or triple bond is shorter than a single C–C bond.\n\nAside from mobile phase surface tension (organizational strength in eluent structure), other mobile phase modifiers can affect analyte retention. For example, the addition of inorganic salts causes a moderate linear increase in the surface tension of aqueous solutions (ca. 1.5 J/cm² per Mol for NaCl, 2.5 J/cm² per Mol for (NH)SO), and because the entropy of the analyte-solvent interface is controlled by surface tension, the addition of salts tend to increase the retention time. This technique is used for mild separation and recovery of proteins and protection of their biological activity in protein analysis (hydrophobic interaction chromatography, HIC).\n\nAnother important factor is the mobile phase pH since it can change the hydrophobic character of the analyte. For this reason most methods use a buffering agent, such as sodium phosphate, to control the pH. Buffers serve multiple purposes: control of pH, neutralize the charge on the silica surface of the stationary phase and act as ion pairing agents to neutralize analyte charge. Ammonium formate is commonly added in mass spectrometry to improve detection of certain analytes by the formation of analyte-ammonium adducts. A volatile organic acid such as acetic acid, or most commonly formic acid, is often added to the mobile phase if mass spectrometry is used to analyze the column effluent. Trifluoroacetic acid is used infrequently in mass spectrometry applications due to its persistence in the detector and solvent delivery system, but can be effective in improving retention of analytes such as carboxylic acids in applications utilizing other detectors, as it is a fairly strong organic acid. The effects of acids and buffers vary by application but generally improve chromatographic resolution.\n\nReversed phase columns are quite difficult to damage compared with normal silica columns; however, many reversed phase columns consist of alkyl derivatized silica particles and should never be used with aqueous bases as these will destroy the underlying silica particle. They can be used with aqueous acid, but the column should not be exposed to the acid for too long, as it can corrode the metal parts of the HPLC equipment. RP-HPLC columns should be flushed with clean solvent after use to remove residual acids or buffers, and stored in an appropriate composition of solvent. The metal content of HPLC columns must be kept low if the best possible ability to separate substances is to be retained. A good test for the metal content of a column is to inject a sample which is a mixture of 2,2'- and 4,4'-bipyridine. Because the 2,2'-bipy can chelate the metal, the shape of the peak for the 2,2'-bipy will be distorted (tailed) when metal ions are present on the surface of the silica...\n\nSize-exclusion chromatography (SEC), also known as \"gel permeation chromatography\" or \"gel filtration chromatography\", separates particles on the basis of molecular size (actually by a particle's Stokes radius). It is generally a low resolution chromatography and thus it is often reserved for the final, \"polishing\" step of the purification. It is also useful for determining the tertiary structure and quaternary structure of purified proteins. SEC is used primarily for the analysis of large molecules such as proteins or polymers. SEC works by trapping these smaller molecules in the pores of a particle. The larger molecules simply pass by the pores as they are too large to enter the pores. Larger molecules therefore flow through the column quicker than smaller molecules, that is, the smaller the molecule, the longer the retention time.\n\nThis technique is widely used for the molecular weight determination of polysaccharides. SEC is the official technique (suggested by European pharmacopeia) for the molecular weight comparison of different commercially available low-molecular weight heparins.\n\nIn ion-exchange chromatography (IC), retention is based on the attraction between solute ions and charged sites bound to the stationary phase. Solute ions of the same charge as the charged sites on the column are excluded from binding, while solute ions of the opposite charge of the charged sites of the column are retained on the column. Solute ions that are retained on the column can be eluted from the column by changing the solvent conditions (\"e.g.\", increasing the ion effect of the solvent system by increasing the salt concentration of the solution, increasing the column temperature, changing the pH of the solvent, etc.).\n\nTypes of ion exchangers include polystyrene resins, cellulose and dextran ion exchangers (gels), and controlled-pore glass or porous silica. Polystyrene resins allow cross linkage which increases the stability of the chain. Higher cross linkage reduces swerving, which increases the equilibration time and ultimately improves selectivity. Cellulose and dextran ion exchangers possess larger pore sizes and low charge densities making them suitable for protein separation.\n\nIn general, ion exchangers favor the binding of ions of higher charge and smaller radius.\n\nAn increase in counter ion (with respect to the functional groups in resins) concentration reduces the retention time. A decrease in pH reduces the retention time in cation exchange while an increase in pH reduces the retention time in anion exchange. By lowering the pH of the solvent in a cation exchange column, for instance, more hydrogen ions are available to compete for positions on the anionic stationary phase, thereby eluting weakly bound cations.\n\nThis form of chromatography is widely used in the following applications: water purification, preconcentration of trace components, ligand-exchange chromatography, ion-exchange chromatography of proteins, high-pH anion-exchange chromatography of carbohydrates and oligosaccharides, and others.\n\nThis chromatographic process relies on the property of biologically active substances to form stable, specific, and reversible complexes. The formation of these complexes involves the participation of common molecular forces such as the Van der Waals interaction, electrostatic interaction, dipole-dipole interaction, hydrophobic interaction, and the hydrogen bond. An efficient, biospecific bond is formed by a simultaneous and concerted action of several of these forces in the complementary binding sites.\n\nAqueous normal-phase chromatography (ANP) is a chromatographic technique which encompasses the mobile phase region between reversed-phase chromatography (RP) and organic normal phase chromatography (ONP). This technique is used to achieve unique selectivity for hydrophilic compounds, showing normal phase elution using reversed-phase solvents. \n\nA separation in which the mobile phase composition remains constant throughout the procedure is termed \"isocratic\" (meaning \"constant composition\"). (The example of these the percentage of methanol throughout the procedure will remain constant i.e 10%) The word was coined by Csaba Horvath who was one of the pioneers of HPLC.,\n\nThe mobile phase composition does not have to remain constant. A separation in which the mobile phase composition is changed during the separation process is described as a \"gradient elution\". One example is a gradient starting at 10% methanol and ending at 90% methanol after 20 minutes. The two components of the mobile phase are typically termed \"A\" and \"B\"; \"A\" is the \"weak\" solvent which allows the solute to elute only slowly, while \"B\" is the \"strong\" solvent which rapidly elutes the solutes from the column. In reversed-phase chromatography, solvent \"A\" is often water or an aqueous buffer, while \"B\" is an organic solvent miscible with water, such as acetonitrile, methanol, THF, or isopropanol.\n\nIn isocratic elution, peak width increases with retention time linearly according to the equation for N, the number of theoretical plates. This leads to the disadvantage that late-eluting peaks get very flat and broad. Their shape and width may keep them from being recognized as peaks.\n\nGradient elution decreases the retention of the later-eluting components so that they elute faster, giving narrower (and taller) peaks for most components. This also improves the peak shape for tailed peaks, as the increasing concentration of the organic eluent pushes the tailing part of a peak forward. This also increases the peak height (the peak looks \"sharper\"), which is important in trace analysis. The gradient program may include sudden \"step\" increases in the percentage of the organic component, or different slopes at different times – all according to the desire for optimum separation in minimum time.\n\nIn isocratic elution, the selectivity does not change if the column dimensions (length and inner diameter) change – that is, the peaks elute in the same order. In gradient elution, the elution order may change as the dimensions or flow rate change.\n\nThe driving force in reversed phase chromatography originates in the high order of the water structure. The role of the \"organic component of the mobile phase\" is to reduce this high order and thus \"reduce the retarding strength of the aqueous component.\"\n\nHPLC separations have theoretical parameters and equations to describe the separation of components into signal peaks when detected by instrumentation such as by a UV detector or a mass spectrometer. The parameters are largely derived from two sets of chromatagraphic theory: plate theory (as part of Partition chromatography), and the rate theory of chromatography / \"Van Deemter equation\". Of course, they can be put in practice through analysis of HPLC chromatograms, although rate theory is considered the more accurate theory.\n\nThey are analogous to the calculation of retention factor for a paper chromatography separation, but describes how well HPLC separates a mixture into two or more components that are detected as peaks (bands) on a chromatogram. The HPLC parameters are the: efficiency factor(\"N\"), the retention factor (kappa prime), and the separation factor (alpha). Together the factors are variables in a resolution equation, which describes how well two components' peaks separated or overlapped each other. These parameters are mostly only used for describing HPLC reversed phase and HPLC normal phase separations, since those separations tend to be more subtle than other HPLC modes (\"e.g.\", ion exchange and size exclusion).\n\nVoid volume is the amount of space in a column that is occupied by solvent. It is the space within the column that is outside of the column's internal packing material. Void volume is measured on a chromatogram as the first component peak detected, which is usually the solvent that was present in the sample mixture; ideally the sample solvent flows through the column without interacting with the column, but is still detectable as distinct from the HPLC solvent. The void volume is used as a correction factor.\n\nEfficiency factor (\"N\") practically measures how sharp component peaks on the chromatogram are, as ratio of the component peak's area (\"retention time\") relative to the width of the peaks at their widest point (at the baseline). Peaks that are tall, sharp, and relatively narrow indicate that separation method efficiently removed a component from a mixture; high efficiency. Efficiency is very dependent upon the HPLC column and the HPLC method used. Efficiency factor is synonymous with plate number, and the 'number of theoretical plates'.\n\nRetention factor (\"kappa prime\") measures how long a component of the mixture stuck to the column, measured by the area under the curve of its peak in a chromatogram (since HPLC chromatograms are a function of time). Each chromatogram peak will have its own retention factor (\"e.g.\", \"kappa\" for the retention factor of the first peak). This factor may be corrected for by the void volume of the column.\n\nSeparation factor (\"alpha\") is a relative comparison on how well two neighboring components of the mixture were separated (\"i.e.\", two neighboring bands on a chromatogram). This factor is defined in terms of a ratio of the retention factors of a pair of neighboring chromatogram peaks, and may also be corrected for by the void volume of the column. The greater the separation factor value is over 1.0, the better the separation, until about 2.0 beyond which an HPLC method is probably not needed for separation.\nResolution equations relate the three factors such that high efficiency and separation factors improve the resolution of component peaks in a HPLC separation.\n\nThe internal diameter (ID) of an HPLC column is an important parameter that influences the detection sensitivity and separation selectivity in gradient elution. It also determines the quantity of analyte that can be loaded onto the column. Larger columns are usually seen in industrial applications, such as the purification of a drug product for later use. Low-ID columns have improved sensitivity and lower solvent consumption at the expense of loading capacity.\n\nLarger ID columns (over 10 mm) are used to purify usable amounts of material because of their large loading capacity.\n\nAnalytical scale columns (4.6 mm) have been the most common type of columns, though smaller columns are rapidly gaining in popularity. They are used in traditional quantitative analysis of samples and often use a UV-Vis absorbance detector.\n\nNarrow-bore columns (1–2 mm) are used for applications when more sensitivity is desired either with special UV-vis detectors, fluorescence detection or with other detection methods like liquid chromatography-mass spectrometry\n\nCapillary columns (under 0.3 mm) are used almost exclusively with alternative detection means such as mass spectrometry. They are usually made from fused silica capillaries, rather than the stainless steel tubing that larger columns employ.\n\nMost traditional HPLC is performed with the stationary phase attached to the outside of small spherical silica particles (very small beads). These particles come in a variety of sizes with 5 µm beads being the most common. Smaller particles generally provide more surface area and better separations, but the pressure required for optimum linear velocity increases by the inverse of the particle diameter squared.\n\nThis means that changing to particles that are half as big, keeping the size of the column the same, will double the performance, but increase the required pressure by a factor of four. Larger particles are used in preparative HPLC (column diameters 5 cm up to >30 cm) and for non-HPLC applications such as solid-phase extraction.\n\nMany stationary phases are porous to provide greater surface area. Small pores provide greater surface area while larger pore size has better kinetics, especially for larger analytes. For example, a protein which is only slightly smaller than a pore might enter the pore but does not easily leave once inside.\n\nPumps vary in pressure capacity, but their performance is measured on their ability to yield a consistent and reproducible volumetric flow rate. Pressure may reach as high as 60 MPa (6000 lbf/in), or about 600 atmospheres. Modern HPLC systems have been improved to work at much higher pressures, and therefore are able to use much smaller particle sizes in the columns (<2 μm). These ultra high performance liquid chromatography\" systems or UHPLCs can work at up to 120 MPa (17,405 lbf/in), or about 1200 atmospheres. The term \"UPLC\" is a trademark of the Waters Corporation, but is sometimes used to refer to the more general technique of UHPLC.\n\nHPLC detectors fall into two main categories: universal or selective. Universal detectors typically measure a bulk property (\"e.g.\", refractive index) by measuring a difference of a physical property between the mobile phase and mobile phase with solute while selective detectors measure a solute property (\"e.g.\", UV-Vis absorbance) by simply responding to the physical or chemical property of the solute. HPLC most commonly uses a UV-Vis absorbance detector, however, a wide range of other chromatography detectors can be used. A universal detector that complements UV-Vis absorbance detection is the Charged aerosol detector (CAD). A kind of commonly utilized detector includes refractive index detectors, which provide readings by measuring the changes in the refractive index of the effluent as it moves through the flow cell. In certain cases, it is possible to use multiple detectors, for example LCMS normally combines UV-Vis with a mass spectrometer.\n\nLarge numbers of samples can be automatically injected onto an HPLC system, by the use of HPLC autosamplers. In addition, HPLC autosamplers have an injection volume and technique which is exactly the same for each injection, consequently they provide a high degree of injection volume precision.\n\nHPLC has many applications in both laboratory and clinical science. It is a common technique used in pharmaceutical development, as it is a dependable way to obtain and ensure product purity. While HPLC can produce extremely high quality (pure) products, it is not always the primary method used in the production of bulk drug materials. According to the European pharmacopoeia, HPLC is used in only 15.5% of syntheses. However, it plays a role in 44% of syntheses in the United States pharmacopoeia. This could possibly be due to differences in monetary and time constraints, as HPLC on a large scale can be an expensive technique. An increase in specificity, precision, and accuracy that occurs with HPLC unfortunately corresponds to an increase in cost.\n\nThis technique is also used for detection of illicit drugs in urine. The most common method of drug detection is an immunoassay. This method is much more convenient. However, convenience comes at the cost of specificity and coverage of a wide range of drugs. As HPLC is a method of determining (and possibly increasing) purity, using HPLC alone in evaluating concentrations of drugs is somewhat insufficient. With this, HPLC in this context is often performed in conjunction with mass spectrometry. Using liquid chromatography instead of gas chromatography in conjunction with MS circumvents the necessity for derivitizing with acetylating or alkylation agents, which can be a burdensome extra step. This technique has been used to detect a variety of agents like doping agents, drug metabolites, glucuronide conjugates, amphetamines, opioids, cocaine, BZDs, ketamine, LSD, cannabis, and pesticides. Performing HPLC in conjunction with Mass spectrometry reduces the absolute need for standardizing HPLC experimental runs.\n\nSimilar assays can be performed for research purposes, detecting concentrations of potential clinical candidates like anti-fungal and asthma drugs. This technique is obviously useful in observing multiple species in collected samples, as well, but requires the use of standard solutions when information about species identity is sought out. It is used as a method to confirm results of synthesis reactions, as purity is essential in this type of research. However, mass spectrometry is still the more reliable way to identify species.\n\nMedical use of HPLC can include drug analysis, but falls more closely under the category of nutrient analysis. While urine is the most common medium for analyzing drug concentrations, blood serum is the sample collected for most medical analyses with HPLC. Other methods of detection of molecules that are useful for clinical studies have been tested against HPLC, namely immunoassays. In one example of this, competitive protein binding assays (CPBA) and HPLC were compared for sensitivity in detection of vitamin D. Useful for diagnosing vitamin D deficiencies in children, it was found that sensitivity and specificity of this CPBA reached only 40% and 60%, respectively, of the capacity of HPLC. While an expensive tool, the accuracy of HPLC is nearly unparalleled.\n\n\n\n"}
{"id": "26023042", "url": "https://en.wikipedia.org/wiki?curid=26023042", "title": "Hubble (film)", "text": "Hubble (film)\n\nHubble (also known as Hubble 3D, IMAX: Hubble, or IMAX: Hubble 3D) is a 2010 American documentary film about Space Shuttle missions to repair and upgrade the Hubble Space Telescope. It is narrated by the actor Leonardo DiCaprio.\n\n\"Through the power of IMAX 3D, \"Hubble 3D\" will enable movie-goers to journey through distant galaxies to explore the grandeur and mysteries of our celestial surroundings, and accompany space-walking astronauts as they attempt the most difficult and important tasks in NASA’s history. The film offers an inspiring and unique look into the Hubble Space Telescope's (HST's) legacy and highlight its profound impact on the way we view the universe and ourselves. \"Hubble 3D\" is an IMAX and Warner Bros. Pictures production, in cooperation with National Aeronautics and Space Administration (NASA). The film reunites the \"Space Station 3D\" film making team, led by producer/director Toni Myers. \"Hubble 3D\" blasted off exclusively to IMAX and IMAX 3D theaters on March 19, 2010.\"\n\nThe film's itinerary takes the viewer past Saturn's aurora, the Helix Nebula in the constellation of Aquarius, the \"Pillars of Creation\" in the Eagle Nebula, the Andromeda galaxy, and the beautiful Butterfly Nebula. The HST has provided data and imagery so detailed that scientists and film technicians have been able to put viewers \"inside\" the images during two extended CGI fly-throughs. In the most awesome sequence, gaseous clouds billow while million-mile-an-hour stellar winds blow through a cloud canyon in the Orion Nebula some 90 trillion miles across. These data-driven animations were created by the Office of Public Outreach at the Space Telescope Science Institute and the Advanced Visualization Laboratory at the National Center for Supercomputing Applications.\n\nFootage seen within the movie was compiled from multiple sources, including IMAX cameras taken into space on Hubble Space Telescope Servicing Missions by the Space Shuttle. This allowed the camera to shoot footage of the satellite close-up and during maintenance. IMAX cameras were taken to the Hubble Space Telescope on STS-61 (Servicing Mission 1 in December 1993) and most recently on STS-125 (Servicing Mission 4 in May 2009) which carried an IMAX 3D camera. The IMAX 3D camera contained a mile of film, though this allowed for only 8 minutes 30 seconds of footage to be recorded.\n\nThe film has received positive reviews. Review aggregate Rotten Tomatoes reports that 86% of critics have given the film a positive review, based on 37 reviews, giving the film a 'Fresh' rating. The site's consensus reads \"Offering a stunning, expansive viewing experience, \"Hubble 3D\" takes advantage of IMAX and 3-D technology like no other film.\"\n\nReview aggregate website Metacritic reports a score of 79 out of 100 from 13 critical reviews, indicating 'generally favorable reviews.' \"New York Times\" critic Neil Genzlinger largely concurred, but added \"... such ponderous, cliché-heavy narration. Leonardo DiCaprio delivers it, which might lead you to expect something moderately hip; instead it’s that same old leaden drone, which back when space flight was new perhaps conveyed suitable awe but these days just makes you feel as if you’re in a junior-high civics class.\"\n\n"}
{"id": "7322901", "url": "https://en.wikipedia.org/wiki?curid=7322901", "title": "Jeffrey H. Goodman", "text": "Jeffrey H. Goodman\n\nJeffrey H. Goodman is a neuroscientist at Helen Hayes Hospital in West Haverstraw, New York. His work studying treatments for Epilepsy has been published in many journals (H-index 30) and presented at conferences all over the world. One of his recent publications involves a procedure for delivering a low frequency sine wave stimulation as a possible treatment . This abstract, co-authored by Jane Schon, Sudarshan Phani, and Jared Zucker, was published in the 2006 Abstract publication of Epilepsia (published by the American Epilepsy Society).\n\nGoodman has two daughters.\n\n"}
{"id": "34678776", "url": "https://en.wikipedia.org/wiki?curid=34678776", "title": "John Arthur Bartrum", "text": "John Arthur Bartrum\n\nJohn Arthur Bartrum (24 May 1885 – 7 June 1949) was a New Zealand geologist and university professor. He was born in Geraldine, South Canterbury, New Zealand on 24 May 1885.\n"}
{"id": "14232709", "url": "https://en.wikipedia.org/wiki?curid=14232709", "title": "Kernphysikalische Forschungsberichte", "text": "Kernphysikalische Forschungsberichte\n\nKernphysikalische Forschungsberichte (\"Research Reports in Nuclear Physics\") was an internal publication of the German \"Uranverein\", which was initiated under the \"Heereswaffenamt\" (Army Ordnance Office) in 1939; in 1942, supervision of the \"Uranverein\" was turned over to the Reichsforschungsrat under the Reichserziehungsministerium. Reports in this publication were classified Top Secret, they had very limited distribution, and the authors were not allowed to keep copies. The reports were confiscated under the Allied Operation Alsos and sent to the United States Atomic Energy Commission for evaluation. In 1971, the reports were declassified and returned to Germany. Many of the reports are available at the Karlsruhe Nuclear Research Center and the Niels Bohr Library of the American Institute of Physics. Many of them are reprinted and transcribed in the book \n\"Collected Works / Gesammelte Werke\" listed below which is available in most libraries. There are reports numbered G-1 to G-395.\n\nProminent German scientists who published reports in \"Kernphysikalische Forschungsberichte\" as members of the \"Uranverein\" can be grouped as follows:\n\n\n\n"}
{"id": "774936", "url": "https://en.wikipedia.org/wiki?curid=774936", "title": "List of GNU packages", "text": "List of GNU packages\n\nA number of notable software packages were developed for, or are maintained by, the Free Software Foundation as part of the GNU Project.\n\nSummarising the situation in 2013, Richard Stallman identified nine aspects which generally apply to being a GNU package, but he noted that exceptions and flexibility are possible when there are good reasons:\n\nThere is no official \"base system\" of the GNU operating system. GNU was designed to be a replacement for Unix operating systems of the 1980s and used the POSIX standards as a guide, but either definition would give a much larger \"base system\". The following list is instead a small set of GNU packages which seem closer to being \"core\" packages than being in any of the further down sections. Inclusions (such as plotutils) and exclusions (such as the C standard library) are of course debatable.\n\nThe software listed below is generally useful to software developers and other computer programmers.\n\n\nThe following libraries and software frameworks are often used in combination with the basic toolchain tools above to build software. (For libraries specifically designed to implement GUI desktops, see Graphical desktop.)\n\n\nThe following packages provide compilers and interpreters for programming languages beyond those included in the GNU Compiler Collection.\n\n\nThe software listed below is generally useful to users not specifically engaged in software development.\n\nThe following packages provide GUI desktop environments, window managers, and associated graphics libraries.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "2675176", "url": "https://en.wikipedia.org/wiki?curid=2675176", "title": "List of compounds with carbon number 23", "text": "List of compounds with carbon number 23\n\nThis is a partial list of molecules that contain 23 carbon atoms.\n\n"}
{"id": "739282", "url": "https://en.wikipedia.org/wiki?curid=739282", "title": "List of fells in the Lake District", "text": "List of fells in the Lake District\n\nThis is a list of fells, hills, mountains, groups of mountains and subsidiary summits and tops in the Lake District, England.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese are the 214 fells selected by Alfred Wainwright for a chapter in his seven \"Pictorial Guides to the Lakeland Fells\". See List of Wainwrights for them sorted by book, and the other Lake District fells he listed in \"The Outlying Fells of Lakeland\".\n\n\nA Marilyn is a hill which has a relative height of at least 150 metres (approximately 500 feet), regardless of its absolute height above sea level. List of Marilyns in England gives a more detailed listing, including the relative height for each fell.\n\n\nThe Hewitts are hills which have a relative height of at least 30 metres (approximately 100 feet), and are over 2000 feet (approximately 610 metres) above sea level.\n\n\n\n"}
{"id": "54203260", "url": "https://en.wikipedia.org/wiki?curid=54203260", "title": "List of investigational sleep drugs", "text": "List of investigational sleep drugs\n\nThis is a list of investigational sleep drugs, or drugs for the treatment of sleep disorders that are currently under development for clinical use but are not yet approved. \"Chemical/generic names are listed first, with developmental code names, synonyms, and brand names in parentheses.\"\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "5971797", "url": "https://en.wikipedia.org/wiki?curid=5971797", "title": "List of mathematicians (B)", "text": "List of mathematicians (B)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "41509593", "url": "https://en.wikipedia.org/wiki?curid=41509593", "title": "List of things named after Niels Bohr", "text": "List of things named after Niels Bohr\n\nNiels Bohr (1885 – 1962), Danish physicist who made foundational contributions to understanding atomic structure and quantum theory, is the eponym of the topics listed below.\n\n\n\n"}
{"id": "14244580", "url": "https://en.wikipedia.org/wiki?curid=14244580", "title": "Lucien Berland", "text": "Lucien Berland\n\nLucien Berland (14 May 1888 in Ay, Marne – 18 August 1962 in Versailles ) was a French entomologist and an arachnologist\n\n\n"}
{"id": "18406", "url": "https://en.wikipedia.org/wiki?curid=18406", "title": "Luminiferous aether", "text": "Luminiferous aether\n\nLuminiferous aether or ether (\"luminiferous\", meaning \"light-bearing\"), was the postulated medium for the propagation of light. It was invoked to explain the ability of the apparently wave-based light to propagate through empty space, something that waves should not be able to do. The assumption of a spatial plenum of luminiferous aether, rather than a spatial vacuum, provided the theoretical medium that was required by wave theories of light.\n\nThe aether hypothesis was the topic of considerable debate throughout its history, as it required the existence of an invisible and infinite material with no interaction with physical objects. As the nature of light was explored, especially in the 19th century, the physical qualities required of an aether became increasingly contradictory. By the late 1800s, the existence of the aether was being questioned, although there was no physical theory to replace it.\n\nThe negative outcome of the Michelson–Morley experiment (1887) suggested that the aether did not exist, a finding that was confirmed in subsequent experiments through the 1920s. This led to considerable theoretical work to explain the propagation of light without an aether. A major breakthrough was the theory of relativity, which could explain why the experiment failed to see aether, but was more broadly interpreted to suggest that it was not needed. The Michelson-Morley experiment, along with the blackbody radiator and photoelectric effect, was a key experiment in the development of modern physics, which includes both relativity and quantum theory, the latter of which explains the wave-like nature of light.\n\nIn the 17th century, Robert Boyle was a proponent of an aether hypothesis. According to Boyle, the aether consists of subtle particles, one sort of which explains the absence of vacuum and the mechanical interactions between bodies, and the other sort of which explains phenomena such as magnetism (and possibly gravity) that are, otherwise, inexplicable on the basis of purely mechanical interactions of macroscopic bodies, \"though in the ether of the ancients there was nothing taken notice of but a diffused and very subtle substance; yet we are at present content to allow that there is always in the air a swarm of steams moving in a determinate course between the north pole and the south\".\n\nChristiaan Huygens hypothesized that light is a wave propagating through an aether. He and Isaac Newton could only envision light waves as being longitudinal, propagating like sound and other mechanical waves in fluids. However, longitudinal waves necessarily have only one form for a given propagation direction, rather than two polarizations like transverse wave. Thus, longitudinal waves can not explain birefringence, in which two polarizations of light are refracted differently by a crystal. In addition, Newton rejected light as waves in a medium because such a medium would have to extend everywhere in space, and would thereby \"disturb and retard the Motions of those great Bodies\" (the planets and comets) and thus \"as it is of no use, and hinders the Operation of Nature, and makes her languish, so there is no evidence for its Existence, and therefore it ought to be rejected\".\n\nIsaac Newton contended that light is made up of numerous small particles. This can explain such features as light's ability to travel in straight lines and reflect off surfaces. Newton imagined that light particles as non-spherical \"corpuscles\", with different \"sides\" that give rise to birefringence. But the particle theory of light can not satisfactorily explain refraction and diffraction. To explain refraction, Newton's \"Opticks\" (1704) postulated an \"Aethereal Medium\" transmitting vibrations faster than light, by which light, when overtaken, is put into \"Fits of easy Reflexion and easy Transmission\", which caused refraction and diffraction. Newton believed that these vibrations were related to heat radiation:\n\nIs not the Heat of the warm Room convey'd through the vacuum by the Vibrations of a much subtiler Medium than Air, which after the Air was drawn out remained in the Vacuum? And is not this Medium the same with that Medium by which Light is refracted and reflected, and by whose Vibrations Light communicates Heat to Bodies, and is put into Fits of easy Reflexion and easy Transmission?\n\nIn contrast to the modern understanding that heat radiation and light are both electromagnetic radiation, Newton viewed heat and light as two different phenomena. He believed heat vibrations to be excited \"when a Ray of Light falls upon the Surface of any pellucid Body\". He wrote, \"I do not know what this Aether is\", but that if it consists of particles then they must be exceedingly smaller than those of Air, or even than those of Light: The exceeding smallness of its Particles may contribute to the greatness of the force by which those Particles may recede from one another, and thereby make that Medium exceedingly more rare and elastic than Air, and by consequence exceedingly less able to resist the motions of Projectiles, and exceedingly more able to press upon gross Bodies, by endeavoring to expand itself.\n\nIn 1720, James Bradley carried out a series of experiments attempting to measure stellar parallax by taking measurements of stars at different times of the year. As the Earth moves around the sun, the apparent angle to a given distant spot changes. By measuring those angles the distance to the star can be calculated based on the known orbital circumference of the Earth around the sun. He failed to detect any parallax, thereby placing a lower limit on the distance to stars.\n\nDuring these experiments, Bradley also discovered a related effect; the apparent positions of the stars did change over the year, but not as expected. Instead of the apparent angle being maximized when the Earth was at either end of its orbit with respect to the star, the angle was maximized when the Earth was at its fastest sideways velocity with respect to the star. This effect is now known as stellar aberration.\n\nBradley explained this effect in the context of Newton's corpuscular theory of light, by showing that the aberration angle was given by simple vector addition of the Earth's orbital velocity and the velocity of the corpuscles of light, just as vertically falling raindrops strike a moving object at an angle. Knowing the Earth's velocity and the aberration angle, this enabled him to estimate the speed of light.\n\nExplaining stellar aberration in the context of an aether-based theory of light was regarded as more problematic. As the aberration relied on relative velocities, and the measured velocity was dependent on the motion of the Earth, the aether had to be remaining stationary with respect to the star as the Earth moved through it. This meant that the Earth could travel through the aether, a physical medium, with no apparent effect – precisely the problem that led Newton to reject a wave model in the first place.\n\nA century later, Thomas Young and Augustin-Jean Fresnel revived the wave theory of light when they pointed out that light could be a transverse wave rather than a longitudinal wave – the polarization of a transverse wave (like Newton's \"sides\" of light) could explain birefringence, and in the wake of a series of experiments on diffraction the particle model of Newton was finally abandoned. Physicists assumed, moreover, that like mechanical waves, light waves required a medium for propagation, and thus required Huygens's idea of an aether \"gas\" permeating all space.\n\nHowever, a transverse wave apparently required the propagating medium to behave as a solid, as opposed to a gas or fluid. The idea of a solid that did not interact with other matter seemed a bit odd, and Augustin-Louis Cauchy suggested that perhaps there was some sort of \"dragging\", or \"entrainment\", but this made the aberration measurements difficult to understand. He also suggested that the \"absence\" of longitudinal waves suggested that the aether had negative compressibility. George Green pointed out that such a fluid would be unstable. George Gabriel Stokes became a champion of the entrainment interpretation, developing a model in which the aether might be (by analogy with pine pitch) rigid at very high frequencies and fluid at lower speeds. Thus the Earth could move through it fairly freely, but it would be rigid enough to support light.\n\nIn 1856, Wilhelm Eduard Weber and Rudolf Kohlrausch measured the numerical value of the ratio of the electromagnetic unit of charge to the electrostatic unit of charge. They found that the ratio equals the product of the speed of light and the square root of two. The following year, Gustav Kirchhoff wrote a paper in which he showed that the speed of a signal along an electric wire was equal to the speed of light. These are the first recorded historical links between the speed of light and electromagnetic phenomena.\n\nJames Clerk Maxwell began working on Michael Faraday's lines of force. In his 1861 paper \"\" he modelled these magnetic lines of force using a sea of molecular vortices that he considered to be partly made of aether and partly made of ordinary matter. He derived expressions for the dielectric constant and the magnetic permeability in terms of the transverse elasticity and the density of this elastic medium. He then equated the ratio of the dielectric constant to the magnetic permeability with a suitably adapted version of Weber and Kohlrausch's result of 1856, and he substituted this result into Newton's equation for the speed of sound. On obtaining a value that was close to the speed of light as measured by Hippolyte Fizeau, Maxwell concluded that light consists in undulations of the same medium that is the cause of electric and magnetic phenomena.\n\nMaxwell had, however, expressed some uncertainties surrounding the precise nature of his molecular vortices and so he began to embark on a purely dynamical approach to the problem. He wrote another paper in 1864, entitled \"A Dynamical Theory of the Electromagnetic Field\", in which the details of the luminiferous medium were less explicit. Although Maxwell did not explicitly mention the sea of molecular vortices, his derivation of Ampère's circuital law was carried over from the 1861 paper and he used a dynamical approach involving rotational motion within the electromagnetic field which he likened to the action of flywheels. Using this approach to justify the electromotive force equation (the precursor of the Lorentz force equation), he derived a wave equation from a set of eight equations which appeared in the paper and which included the electromotive force equation and Ampère's circuital law. Maxwell once again used the experimental results of Weber and Kohlrausch to show that this wave equation represented an electromagnetic wave that propagates at the speed of light, hence supporting the view that light is a form of electromagnetic radiation.\n\nThe apparent need for a propagation medium for such Hertzian waves can be seen by the fact that they consist of orthogonal electric (E) and magnetic (B or H) waves. The E waves consist of undulating dipolar electric fields, and all such dipoles appeared to require separated and opposite electric charges. Electric charge is an inextricable property of matter, so it appeared that some form of matter was required to provide the alternating current that would seem to have to exist at any point along the propagation path of the wave. Propagation of waves in a true vacuum would imply the existence of electric fields without associated electric charge, or of electric charge without associated matter. Albeit compatible with Maxwell's equations, electromagnetic induction of electric fields could not be demonstrated in vacuum, because all methods of detecting electric fields required electrically charged matter.\n\nIn addition, Maxwell's equations required that all electromagnetic waves in vacuum propagate at a fixed speed, \"c\". As this can only occur in one reference frame in Newtonian physics (see Galilean-Newtonian relativity), the aether was hypothesized as the absolute and unique frame of reference in which Maxwell's equations hold. That is, the aether must be \"still\" universally, otherwise \"c\" would vary along with any variations that might occur in its supportive medium. Maxwell himself proposed several mechanical models of aether based on wheels and gears, and George Francis FitzGerald even constructed a working model of one of them. These models had to agree with the fact that the electromagnetic waves are transverse but never longitudinal.\n\nBy this point the mechanical qualities of the aether had become more and more magical: it had to be a fluid in order to fill space, but one that was millions of times more rigid than steel in order to support the high frequencies of light waves. It also had to be massless and without viscosity, otherwise it would visibly affect the orbits of planets. Additionally it appeared it had to be completely transparent, non-dispersive, incompressible, and continuous at a very small scale. Maxwell wrote in \"Encyclopædia Britannica\":\n\nAethers were invented for the planets to swim in, to constitute electric atmospheres and magnetic effluvia, to convey sensations from one part of our bodies to another, and so on, until all space had been filled three or four times over with aethers. ... The only aether which has survived is that which was invented by Huygens to explain the propagation of light.\n\nContemporary scientists were aware of the problems, but aether theory was so entrenched in physical law by this point that it was simply assumed to exist. In 1908 Oliver Lodge gave a speech on behalf of Lord Rayleigh to the Royal Institution on this topic, in which he outlined its physical properties, and then attempted to offer reasons why they were not impossible. Nevertheless, he was also aware of the criticisms, and quoted Lord Salisbury as saying that \"aether is little more than a nominative case of the verb \"to undulate\"\". Others criticized it as an \"English invention\", although Rayleigh jokingly stated it was actually an invention of the Royal Institution.\n\nBy the early 20th century, aether theory was in trouble. A series of increasingly complex experiments had been carried out in the late 19th century to try to detect the motion of the Earth through the aether, and had failed to do so. A range of proposed aether-dragging theories could explain the null result but these were more complex, and tended to use arbitrary-looking coefficients and physical assumptions. Lorentz and FitzGerald offered within the framework of Lorentz ether theory a more elegant solution to how the motion of an absolute aether could be undetectable (length contraction), but if their equations were correct, the new special theory of relativity (1905) could generate the same mathematics without referring to an aether at all. Aether fell to Occam's Razor.\n\nThe two most important models, which were aimed to describe the relative motion of the Earth and aether, were Augustin-Jean Fresnel's (1818) model of the (nearly) stationary aether including a partial aether drag determined by Fresnel's dragging coefficient,\nand George Gabriel Stokes' (1844)\nmodel of complete aether drag. The latter theory was not considered as correct, since it was not compatible with the aberration of light, and the auxiliary hypotheses developed to explain this problem were not convincing. Also, subsequent experiments as the Sagnac effect (1913) also showed that this model is untenable. However, the most important experiment supporting Fresnel's theory was Fizeau's 1851 experimental confirmation of Fresnel's 1818 prediction that a medium with refractive index \"n\" moving with a velocity \"v\" would increase the speed of light travelling through the medium in the same direction as \"v\" from \"c\"/\"n\" to:\n\nThat is, movement adds only a fraction of the medium's velocity to the light (predicted by Fresnel in order to make Snell's law work in all frames of reference, consistent with stellar aberration). This was initially interpreted to mean that the medium drags the aether along, with a \"portion\" of the medium's velocity, but that understanding became very problematic after Wilhelm Veltmann demonstrated that the index \"n\" in Fresnel's formula depended upon the wavelength of light, so that the aether could not be moving at a wavelength-independent speed. This implied that there must be a separate aether for each of the infinitely many frequencies.\n\nThe key difficulty with Fresnel's aether hypothesis arose from the juxtaposition of the two well-established theories of Newtonian dynamics and Maxwell's electromagnetism. Under a Galilean transformation the equations of Newtonian dynamics are invariant, whereas those of electromagnetism are not. Basically this means that while physics should remain the same in non-accelerated experiments, light would not follow the same rules because it is travelling in the universal \"aether frame\". Some effect caused by this difference should be detectable.\n\nA simple example concerns the model on which aether was originally built: sound. The speed of propagation for mechanical waves, the speed of sound, is defined by the mechanical properties of the medium. Sound travels 4.3 times faster in water than in air. This explains why a person hearing an explosion underwater and quickly surfacing can hear it again as the slower travelling sound arrives through the air. Similarly, a traveller on an airliner can still carry on a conversation with another traveller because the sound of words is travelling along with the air inside the aircraft. This effect is basic to all Newtonian dynamics, which says that everything from sound to the trajectory of a thrown baseball should all remain the same in the aircraft flying (at least at a constant speed) as if still sitting on the ground. This is the basis of the Galilean transformation, and the concept of frame of reference.\n\nBut the same was not supposed to be true for light, since Maxwell's mathematics demanded a single universal speed for the propagation of light, based, not on local conditions, but on two measured properties, the permittivity and permeability of free space, that were assumed to be the same throughout the universe. If these numbers did change, there should be noticeable effects in the sky; stars in different directions would have different colours, for instance.\n\nThus at any point there should be one special coordinate system, \"at rest relative to the aether\". Maxwell noted in the late 1870s that detecting motion relative to this aether should be easy enough—light travelling along with the motion of the Earth would have a different speed than light travelling backward, as they would both be moving against the unmoving aether. Even if the aether had an overall universal flow, changes in position during the day/night cycle, or over the span of seasons, should allow the drift to be detected.\n\nAlthough the aether is almost stationary according to Fresnel, his theory predicts a positive outcome of aether drift experiments only to \"second\" order in formula_2, because Fresnel's dragging coefficient would cause a negative outcome of all optical experiments capable of measuring effects to \"first\" order in formula_2. This was confirmed by the following first-order experiments, which all gave negative results. The following list is based on the description of Wilhelm Wien (1898), with changes and additional experiments according to the descriptions of Edmund Taylor Whittaker (1910) and Jakob Laub (1910):\n\n\nBesides those optical experiments, also electrodynamic first-order experiments were conducted, which should have led to positive results according to Fresnel. However, Hendrik Antoon Lorentz (1895) modified Fresnel's theory and showed that those experiments can be explained by a stationary aether as well:\n\n\nWhile the \"first\"-order experiments could be explained by a modified stationary aether, more precise \"second\"-order experiments were expected to give positive results, however, no such results could be found.\n\nThe famous Michelson–Morley experiment compared the source light with itself after being sent in different directions, looking for changes in phase in a manner that could be measured with extremely high accuracy. In this experiment, their goal was to determine the velocity of the Earth through the aether. The publication of their result in 1887, the null result, was the first clear demonstration that something was seriously wrong with the aether hypothesis (Michelson's first experiment in 1881 was not entirely conclusive). In this case the MM experiment yielded a shift of the fringing pattern of about 0.01 of a fringe, corresponding to a small velocity. However, it was incompatible with the expected aether wind effect due to the Earth's (seasonally varying) velocity which would have required a shift of 0.4 of a fringe, and the error was small enough that the value may have indeed been zero. Therefore, the null hypothesis, the hypothesis that there was no aether wind, could not be rejected. More modern experiments have since reduced the possible value to a number very close to zero, about 10.\n\nA series of experiments using similar but increasingly sophisticated apparatuses all returned the null result as well. Conceptually different experiments that also attempted to detect the motion of the aether were the Trouton–Noble experiment (1903), whose objective was to detect torsion effects caused by electrostatic fields, and the experiments of Rayleigh and Brace (1902, 1904), to detect double refraction in various media. However, all of them obtained a null result, like Michelson–Morley (MM) previously did.\n\nThese \"aether-wind\" experiments led to a flurry of efforts to \"save\" aether by assigning to it ever more complex properties, while only few scientists, like Emil Cohn or Alfred Bucherer, considered the possibility of the abandonment of the aether hypothesis. Of particular interest was the possibility of \"aether entrainment\" or \"aether drag\", which would lower the magnitude of the measurement, perhaps enough to explain the results of the Michelson-Morley experiment. However, as noted earlier, aether dragging already had problems of its own, notably aberration. In addition, the interference experiments of Lodge (1893, 1897) and Ludwig Zehnder (1895), aimed to show whether the aether is dragged by various, rotating masses, showed no aether drag. A more precise measurement was made in the Hammar experiment (1935), which ran a complete MM experiment with one of the \"legs\" placed between two massive lead blocks. If the aether was dragged by mass then this experiment would have been able to detect the drag caused by the lead, but again the null result was achieved. The theory was again modified, this time to suggest that the entrainment only worked for very large masses or those masses with large magnetic fields. This too was shown to be incorrect by the Michelson–Gale–Pearson experiment, which detected the Sagnac effect due to Earth's rotation (see Aether drag hypothesis).\n\nAnother, completely different attempt to save \"absolute\" aether was made in the Lorentz–FitzGerald contraction hypothesis, which posited that \"everything\" was affected by travel through the aether. In this theory the reason the Michelson–Morley experiment \"failed\" was that the apparatus contracted in length in the direction of travel. That is, the light was being affected in the \"natural\" manner by its travel though the aether as predicted, but so was the apparatus itself, cancelling out any difference when measured. FitzGerald had inferred this hypothesis from a paper by Oliver Heaviside. Without referral to an aether, this physical interpretation of relativistic effects was shared by Kennedy and Thorndike in 1932 as they concluded that the interferometer's arm contracts and also the frequency of its light source \"very nearly\" varies in the way required by relativity.\n\nSimilarly the Sagnac effect, observed by G. Sagnac in 1913, was immediately seen to be fully consistent with special relativity. In fact, the Michelson-Gale-Pearson experiment in 1925 was proposed specifically as a test to confirm the relativity theory, although it was also recognized that such tests, which merely measure absolute rotation, are also consistent with non-relativistic theories.\n\nDuring the 1920s, the experiments pioneered by Michelson were repeated by Dayton Miller, who publicly proclaimed positive results on several occasions, although they were not large enough to be consistent with any known aether theory. However, other researchers were unable to duplicate Miller's claimed results. Over the years the experimental accuracy of such measurements has been raised by many orders of magnitude, and no trace of any violations of Lorentz invariance has been seen. (A later re-analysis of Miller's results concluded that he had underestimated the variations due to temperature.)\n\nSince the Miller experiment and its unclear results there have been many more experimental attempts to detect the aether. Many experimenters have claimed positive results. These results have not gained much attention from mainstream science, since they contradict a large quantity of high-precision measurements, all the results of which were consistent with special relativity.\n\nBetween 1892 and 1904, Hendrik Lorentz developed an electron-aether theory, in which he introduced a strict separation between matter (electrons) and aether. In his model the aether is completely motionless, and won't be set in motion in the neighborhood of ponderable matter. Contrary to earlier electron models, the electromagnetic field of the aether appears as a mediator between the electrons, and changes in this field cannot propagate faster than the speed of light. A fundamental concept of Lorentz's theory in 1895 was the \"theorem of corresponding states\" for terms of order v/c. This theorem states that an observer moving relative to the aether makes the same observations as a resting observer, after a suitable change of variables. Lorentz noticed that it was necessary to change the space-time variables when changing frames and introduced concepts like physical length contraction (1892) to explain the Michelson–Morley experiment, and the mathematical concept of local time (1895) to explain the aberration of light and the Fizeau experiment. This resulted in the formulation of the so-called Lorentz transformation by Joseph Larmor (1897, 1900) and Lorentz (1899, 1904), whereby (it was noted by Larmor) the complete formulation of local time is accompanied by some sort of time dilation of electrons moving in the aether. As Lorentz later noted (1921, 1928), he considered the time indicated by clocks resting in the aether as \"true\" time, while local time was seen by him as a heuristic working hypothesis and a mathematical artifice. Therefore, Lorentz's theorem is seen by modern authors as being a mathematical transformation from a \"real\" system resting in the aether into a \"fictitious\" system in motion.\n\nThe work of Lorentz was mathematically perfected by Henri Poincaré, who formulated on many occasions the Principle of Relativity and tried to harmonize it with electrodynamics. He declared simultaneity only a convenient convention which depends on the speed of light, whereby the constancy of the speed of light would be a useful postulate for making the laws of nature as simple as possible. In 1900 and 1904 he physically interpreted Lorentz's local time as the result of clock synchronization by light signals. In June and July 1905 he declared the relativity principle a general law of nature, including gravitation. He corrected some mistakes of Lorentz and proved the Lorentz covariance of the electromagnetic equations. However, he used the notion of an aether as a perfectly undetectable medium and distinguished between apparent and real time, so most historians of science argue that he failed to invent special relativity.\n\nAether theory was dealt another blow when the Galilean transformation and Newtonian dynamics were both modified by Albert Einstein's special theory of relativity, giving the mathematics of Lorentzian electrodynamics a new, \"non-aether\" context. Unlike most major shifts in scientific thought, special relativity was adopted by the scientific community remarkably quickly, consistent with Einstein's later comment that the laws of physics described by the Special Theory were \"ripe for discovery\" in 1905. Max Planck's early advocacy of the special theory, along with the elegant formulation given to it by Hermann Minkowski, contributed much to the rapid acceptance of special relativity among working scientists.\n\nEinstein based his theory on Lorentz's earlier work. Instead of suggesting that the mechanical properties of objects changed with their constant-velocity motion through an undetectable aether, Einstein proposed to deduce the characteristics that any successful theory must possess in order to be consistent with the most basic and firmly established principles, independent of the existence of a hypothetical aether. He found that the Lorentz transformation must transcend its connection with Maxwell's equations, and must represent the fundamental relations between the space and time coordinates of inertial frames of reference. In this way he demonstrated that the laws of physics remained invariant as they had with the Galilean transformation, but that light was now invariant as well.\n\nWith the development of the special theory of relativity, the need to account for a single universal frame of reference had disappeared – and acceptance of the 19th century theory of a luminiferous aether disappeared with it. For Einstein, the Lorentz transformation implied a conceptual change: that the concept of position in space or time was not absolute, but could differ depending on the observer's location and velocity.\n\nMoreover, in another paper published the same month in 1905, Einstein made several observations on a then-thorny problem, the photoelectric effect. In this work he demonstrated that light can be considered as particles that have a \"wave-like nature\". Particles obviously do not need a medium to travel, and thus, neither did light. This was the first step that would lead to the full development of quantum mechanics, in which the wave-like nature \"and\" the particle-like nature of light are both considered as valid descriptions of light. A summary of Einstein's thinking about the aether hypothesis, relativity and light quanta may be found in his 1909 (originally German) lecture \"The Development of Our Views on the Composition and Essence of Radiation\".\n\nLorentz on his side continued to use the aether hypothesis. In his lectures of around 1911 he pointed out that what \"the theory of relativity has to say ... can be carried out independently of what one thinks of the aether and the time\". He commented that \"whether there is an aether or not, electromagnetic fields certainly exist, and so also does the energy of the electrical oscillations\" so that, \"if we do not like the name of 'aether', we must use another word as a peg to hang all these things upon\". He concluded that \"one cannot deny the bearer of these concepts a certain substantiality\".\n\nIn later years there have been a few individuals who advocated a neo-Lorentzian approach to physics, which is Lorentzian in the sense of positing an absolute true state of rest that is undetectable and which plays no role in the predictions of the theory. (No violations of Lorentz covariance have ever been detected, despite strenuous efforts.) Hence these theories resemble the 19th century aether theories in name only. For example, the founder of quantum field theory, Paul Dirac, stated in 1951 in an article in Nature, titled \"Is there an Aether?\" that \"we are rather forced to have an aether\". However, Dirac never formulated a complete theory, and so his speculations found no acceptance by the scientific community.\n\nWhen Einstein was still a student in the Zurich Polytechnic in 1900, he was very interested in the idea of aether. His initial proposal of research thesis was to do an experiment to measure how fast the Earth was moving through the aether. \"The velocity of a wave is proportional to the square root of the elastic forces which cause [its] propagation, and inversely proportional to the mass of the aether moved by these forces.\"\n\nIn 1916, after Einstein completed his foundational work on general relativity, Lorentz wrote a letter to him in which he speculated that within general relativity the aether was re-introduced. In his response Einstein wrote that one can actually speak about a \"new aether\", but one may not speak of motion in relation to that aether. This was further elaborated by Einstein in some semi-popular articles (1918, 1920, 1924, 1930).\n\nIn 1918 Einstein publicly alluded to that new definition for the first time. Then, in the early 1920s, in a lecture which he was invited to give at Lorentz's university in Leiden, Einstein sought to reconcile the theory of relativity with Lorentzian aether. In this lecture Einstein stressed that special relativity took away the last mechanical property of the aether: immobility. However, he continued that special relativity does not necessarily rule out the aether, because the latter can be used to give physical reality to acceleration and rotation. This concept was fully elaborated within general relativity, in which physical properties (which are partially determined by matter) are attributed to space, but no substance or state of motion can be attributed to that \"aether\" (by which he meant curved space-time).\n\nIn another paper of 1924, named \"Concerning the Aether\", Einstein argued that Newton's absolute space, in which acceleration is absolute, is the \"Aether of Mechanics\". And within the electromagnetic theory of Maxwell and Lorentz one can speak of the \"Aether of Electrodynamics\", in which the aether possesses an absolute state of motion. As regards special relativity, also in this theory acceleration is absolute as in Newton's mechanics. However, the difference from the electromagnetic aether of Maxwell and Lorentz lies in the fact, that \"because it was no longer possible to speak, in any absolute sense, of simultaneous states at different locations in the aether, the aether became, as it were, four dimensional, since there was no objective way of ordering its states by time alone\". Now the \"aether of special relativity\" is still \"absolute\", because matter is affected by the properties of the aether, but the aether is not affected by the presence of matter. This asymmetry was solved within general relativity. Einstein explained that the \"aether of general relativity\" is not absolute, because matter is influenced by the aether, just as matter influences the structure of the aether.\n\nThe only similarity of this relativistic aether concept with the classical aether models lies in the presence of physical properties in space, which can be identified through geodesics. As historians such as John Stachel argue, Einstein's views on the \"new aether\" are not in conflict with his abandonment of the aether in 1905. As Einstein himself pointed out, no \"substance\" and no state of motion can be attributed to that new aether. Einstein's use of the word \"aether\" found little support in the scientific community, and played no role in the continuing development of modern physics.\n\n\n\n \n"}
{"id": "9150392", "url": "https://en.wikipedia.org/wiki?curid=9150392", "title": "Malaia garnet", "text": "Malaia garnet\n\nMalaia garnet or Malaya garnet is a gemological varietal name for light to dark slightly pinkish orange, reddish orange, or yellowish orange garnet, that are of a mixture within the \"pyralspite\" series pyrope, almandine, and spessartine with a little calcium. The name Malaia is translated from Swahili to mean, \"one without a family\". It is found in east Africa, in the Umba Valley bordering Tanzania and Kenya. \n"}
{"id": "25106296", "url": "https://en.wikipedia.org/wiki?curid=25106296", "title": "Metropolitan Area Projects Plan", "text": "Metropolitan Area Projects Plan\n\nMetropolitan Area Projects Plan (MAPS) is a multi-year, municipal capital improvement program, consisting of a number of projects, originally conceived in the 1990s in Oklahoma City by its then mayor Ron Norick. A MAPS program features several interrelated and defined capital projects, funded by a temporary sales tax (allowing projects to be paid for in cash, without incurring debt), administered by a separate dedicated city staff funded by the sales tax, and supervised by a volunteer citizens oversight committee.\n\nIn some ways, a MAPS program is similar to a Local option sales tax. However, taxes collected by a MAPS program do not go to a city's general fund, but are instead deposited into a trust dedicated to the specific projects identified in the taxes' enabling ordinance. Additionally, MAPS programs are only indirectly controlled by a city's elected governance body; a citizens oversight committee provides direct oversight, which is also established by the enabling ordinance.\n\nThe key features of the original program were designed to provide accountability to the citizens of the community as well as provide a funding mechanism for capital projects without using a city's general revenue funds, and included:\n\nA common challenge of \"pay as you go\" programs such as MAPS is that, because the lead-up time while accumulating the needed funds can be lengthy, specific projects of the program \"are often are scuttled when administrations change and new leaders want their own signature projects... Oklahoma City was able to avoid this pitfall ‹because the city› changes mayors, but not strategies.\" Because the voters approved a multi-year temporary sales tax that was dedicated to multiple specific projects that together had significant public support, and because the infrastructure to support the program and its projects was also temporary with dedicated funding from the sales tax, a change in elected officials has not been sufficient to change the scope of the program.\n\nA key to success is the feature of multiple projects; while each project taken individually might not have sufficient support for individual funding, when taken as a group, the package has sufficient support for funding. Said another way, it is important to \"bundle projects to enhance community buy-in.\" Another benefit of multiple projects is that by staging the projects, citizens may have their confidence level increased by being shown staggered \"early results ‹with less expensive projects› ... by experiencing a string of groundbreakings.\"\n\nIn many ways the early 1980s recession in the United States began in Oklahoma City with the collapse of the Penn Square Bank. This subsequent collapse of the state's energy business and failure of additional financial institutions, lead to a significant out-migration and excess capacity in real estate. The resulting lack of infrastructure investment in the inner city proved to be a factor in the city's inability to attract new business. In the early 1990s some Oklahoma City interests were concerned about what they perceived as civic decline. In 1992, after the city lost a contract to house a new maintenance facility for United Airlines to Indianapolis, Indiana because the airline considered Indianapolis to have a better standard of living and quality of life, then-mayor Ron Norick and the Greater Oklahoma City Chamber of Commerce proposed MAPS as a measure to improve the city's economy and attractiveness as a tourist destination.\n\nCity residents were initially skeptical over funding the public projects through a sales tax increase, and as late as a month before the tax referendum opposed the plan by a 20% margin. However, the plan did pass by a slim margin in a vote in December, 1993. During the five year tax period the city raised nearly $310 million in direct taxes, plus $52 million of income on the tax money it had deposited. The tax was extended with voter approval for an additional six months to raise enough money to complete all of the projects, and construction continued until 2004.\n\nEncouraged by the success of MAPS, city leaders proposed and adopted \"MAPS for Kids\", a public school improvement program. In December 2009 the city approved a third program, \"MAPS 3\", which would build $777 million in further improvements paid for by a similar sales tax increase.\n\nThe MAPS initiatives in Oklahoma City, to date, consist of three major programs: The original MAPS program, MAPS for Kids, and MAPS 3.\n\nThe original Metropolitan Area Projects Plan, or MAPS, was a $350 million public works and redevelopment project in Oklahoma City, Oklahoma during the middle to late 1990s, funded by a temporary, five year, voter-approved sales tax increase. \"The various MAPS projects were believed to be capable of improving the economy and attractiveness of this ‹downtown› core and having a profound impact on proximal areas;\" the common theme of the projects was downtown redevelopment. The original MAPS program comprised nine projects that took 10 years to complete, and were chosen to appeal to a wide variety of city residents and also revitalize the city's downtown: \n\nThe second MAPS program, called MAPS for Kids, was a $700 million initiative to improve schools in the various school districts whose boundaries coincided with the City of Oklahoma City. Oklahoma has school districts whose boundaries have nothing to do with the boundaries of any other political subdivision. At the time the original MAPS tax was expiring, the Oklahoma City Public Schools system was struggling with little political capital. \"They could not pass bond issues, and the school buildings were falling apart as a result.\" Then mayor Kirk Humphreys proposed a second MAPS program to repair more than 100 area schools. With the addition of a $180-million bond issue and an eye on addressing childhood obesity issues, new gymnasiums were added to all of the Oklahoma City District's elementary schools. The program was approved by voters in 2001. This temporary sales tax was collected for seven years, with 70 percent disbursed to the Oklahoma City School District and 30 percent to 23 Suburban School Districts.\n\nMAPS 3 is a $777 million program, approved by voters in 2009 with 54% of the vote (the same percentage as the Original MAPS.) The one-cent sales tax initiative began in April 2010 and ends in December 2017. After a year of public meetings organized by Mayor Mick Cornett, there developed a consensus that a future MAPS program should focus on projects that improved the Quality of Life in Oklahoma City. Hundreds of citizens suggested projects to be considered; through a series of public meetings eight projects were eventually selected to be included in the MAPS 3 proposal.\nRecognizing that the city's economy may fluctuate during the time of the sales tax collection, and that there may be unforeseen project contingencies, the program also has a significant Infrastructure/Contingency component.\n\nThe public forum process used to identify projects to be included in the MAPS 3 program established enough public political support to overcome an effort by a newly elected city council person who attempted to derail the original MAPS 3 proposal. An initiative petition was filed that would have eliminated the Convention Center and ended the Sales tax earlier. This council person also ran for Mayor, with MAPS being a major factor in the election. Mayor Cornett was re-elected for an unprecedented 4th term with 65.7% of the vote.\n\nThe MAPS programs have had significant impact in Oklahoma City, both economically and from a quality of life standpoint. In the 20 years since its inception \"nearly $5 billion in economic impact can be attributed to the original MAPS program. This represents a nearly 10-fold return on the city's original investment.\" \n\nAs was recently reported in a Wichita newspaper, \n\"From quirky festivals and dozens of restaurants to outdoor recreation and romantic riverboat rides, a revitalized downtown Oklahoma City boasts so many attractions it is quickly becoming a favorite weekend destination for Oklahomans and Kansans alike.\" \n\nIt is arguable that in terms of associated investment, hotels segment represent the largest impact of the original MAPS. \"In 1992-93 when the city's leaders developed the plans for the MAPS investments, there was only one significant hotel operating in downtown Oklahoma City. What is now the Sheraton had nearly 400 rooms but was in need of upgrading. By 2008, there were seven significant hotels in the Downtown CBD and Bricktown with a total of about 1,600 rooms.\"\n\n"}
{"id": "50659821", "url": "https://en.wikipedia.org/wiki?curid=50659821", "title": "National Bureau of Classification (NBC)", "text": "National Bureau of Classification (NBC)\n\nThe National Bureau of Classification (NBC), previously the Film Censor Board of Maldives, is a government office founded on 21 May 1956 with the objective of presenting and promoting cinema and theatrical performances for the benefit of the Maldivian people.\n\nThe Government of Maldives first formed a Film Censor Board on 21 May 1956 under the President’s Office. This Board was formed with the objective of presenting and promoting cinema and theatrical performances for the benefit of the Maldivian people.\nOn 15 May 1983 the Film Censor Board was transferred under the mandate of the Ministry of Home Affairs and Housing. The mission of the five-member Board was to check films and theatrical dramas to see if they had issues that conflicted:\n\n1. The tenets of Islam.\n2. The Constitution, Laws and Regulations of the Maldives.\n3. The Maldivian Culture.\n\nOn 1 April 1998 the Film Censor Board became a subsidiary of the Ministry of Information, Arts and Culture and on 29 December 2005, the Film Censor Board was re-branded as the National Bureau of Classification, NBC. This change was brought as the government believed that classification and education was the key to manage and regulate different content so as to protect values of artistic productions and safeguard consumer interests at the same time. Presently, the National Bureau of Classification is under the Ministry of Youth and Sports.\n\nClassification Certificates issued by NBC are based on the following categories:\n\n"}
{"id": "15613454", "url": "https://en.wikipedia.org/wiki?curid=15613454", "title": "Nicolae Frolov", "text": "Nicolae Frolov\n\nNicolae Frolov (1876–1948) was a Romanian geologist and agronomist from Bessarabia. He was noted for his work in explicating the hydrology of Bessarabia and as director of the Chișinău National Museum.\n\nBorn in Corneşti, Ungheni, Bessarabia, he graduated from Chișinău Theological Seminary and then went to Estonia where in 1904 he graduated from the University of Dorpat (now Tartu). After which he taught at the universities of St, Petersburg, Kiev and Odessa. In 1921, he moved to Chişinău and was appointed as director of the Chișinău Museum. He taught agronmy and agricultural economics at the university there until 1940, when he retired and moved to Romania. He died in Iaşi.\n\n"}
{"id": "11397250", "url": "https://en.wikipedia.org/wiki?curid=11397250", "title": "Ordinal optimization", "text": "Ordinal optimization\n\nIn mathematical optimization, ordinal optimization is the maximization of functions taking values in a partially ordered set (\"poset\"). Ordinal optimization has applications in the theory of queuing networks.\n\nA partial order is a binary relation \"≤\" over a set \"P\" which is reflexive, antisymmetric, and transitive, i.e., for all \"a\", \"b\", and \"c\" in \"P\", we have that:\n\n\nIn other words, a partial order is an antisymmetric preorder.\n\nA set with a partial order is called a partially ordered set (also called a poset). The term \"ordered set\" is sometimes also used for posets, as long as it is clear from the context that no other kinds of orders are meant. In particular, totally ordered sets can also be referred to as \"ordered sets\", especially in areas where these structures are more common than posets.\n\nFor \"a, b\" distinct elements of a partially ordered set \"P\", if \"a ≤ b\" or \"b ≤ a\", then \"a\" and \"b\" are comparable. Otherwise they are incomparable. If every two elements of a poset are comparable, the poset is called a totally ordered set or chain (e.g. the natural numbers under order). A poset in which every two elements are incomparable is called an antichain.\n\nStandard examples of posets arising in mathematics include:\n\n\nThere are several notions of \"greatest\" and \"least\" element in a poset \"P\", notably:\n\n\nFor example, consider the natural numbers, ordered by divisibility: 1 is a least element, as it divides all other elements, but this set does not have a greatest element nor does it have any maximal elements: any \"g\" divides 2\"g\", so 2\"g\" is greater than \"g\" and \"g\" cannot be maximal. If instead we consider only the natural numbers that are greater than 1, then the resulting poset does not have a least element, but any prime number is a minimal element. In this poset, 60 is an upper bound (though not the least upper bound) of {2,3,5} and 2 is a lower bound of {4,6,8,12}.\n\nIn many such cases, the poset has additional structure: For example, the poset can be a lattice or a partially ordered algebraic structure.\n\nA poset (\"L\", ≤) is a lattice if it satisfies the following two axioms.\n\n\nThe join and meet of \"a\" and \"b\" are denoted by formula_1 and formula_2, respectively. This definition makes formula_5 and formula_6 binary operations. The first axiom says that \"L\" is a join-semilattice; the second says that \"L\" is a meet-semilattice. Both operations are monotone with respect to the order: \"a\" ≤ \"a\" and \"b\" ≤ \"b\" implies that aformula_5 b ≤ a formula_8 b and aformula_6b ≤ aformula_6b.\n\nIt follows by an induction argument that every non-empty finite subset of a lattice has a join (supremum) and a meet (infimum). With additional assumptions, further conclusions may be possible; \"see\" Completeness (order theory) for more discussion of this subject.\n\nA bounded lattice has a greatest (or maximum) and least (or minimum) element, denoted 1 and 0 by convention (also called top and bottom). Any lattice can be converted into a bounded lattice by adding a greatest and least element, and every non-empty finite lattice is bounded, by taking the join (resp., meet) of all elements, denoted by formula_11 (resp.formula_12) where formula_13.\n\nA poset is a bounded lattice if and only if every finite set of elements (including the empty set) has a join and a meet. Here, the join of an empty set of elements is defined to be the least element formula_14, and the meet of the empty set is defined to be the greatest element formula_15. This convention is consistent with the associativity and commutativity of meet and join: the join of a union of finite sets is equal to the join of the joins of the sets, and dually, the meet of a union of finite sets is equal to the meet of the meets of the sets, i.e., for finite subsets \"A\" and \"B\" of a poset \"L\",\n\nand\n\nhold. Taking \"B\" to be the empty set,\n\nand\n\nwhich is consistent with the fact that formula_20.\n\nThe poset can be a partially ordered algebraic structure.\n\nIn algebra, an \"ordered semigroup\" is a semigroup (\"S\",•) together with a partial order ≤ that is \"compatible\" with the semigroup operation, meaning that \"x\" ≤ \"y\" implies z•x ≤ z•y and x•z ≤ y•z for all \"x\", \"y\", \"z\" in \"S\". If S is a group and it is ordered as a semigroup, one obtains the notion of ordered group, and similarly if S is a monoid it may be called \"ordered monoid\". Partially ordered vector spaces and vector lattices are important in optimization with multiple objectives.\n\nProblems of ordinal optimization arise in many disciplines. Computer scientists study selection algorithms, which are simpler than sorting algorithms.\n\nStatistical decision theory studies \"selection problems\" that require the identification of a \"best\" subpopulation or of identifying a \"near best\" subpopulation.\n\nSince the 1960s, the field of ordinal optimization has expanded in theory and in applications. In particular, antimatroids and the \"max-plus algebra\" have found application in network analysis and queuing theory, particularly in queuing networks and discrete-event systems.\n\n\n\n"}
{"id": "32988300", "url": "https://en.wikipedia.org/wiki?curid=32988300", "title": "Outline of social science", "text": "Outline of social science\n\nThe following outline is provided as an overview of and topical guide to social science:\n\nSocial science – branch of science concerned with society and human behaviors.\n\nSocial science can be described as all of the following:\n\n\n\n\n\nAshishmedia\n\n\n\n"}
{"id": "774144", "url": "https://en.wikipedia.org/wiki?curid=774144", "title": "Pierre Joseph Bonnaterre", "text": "Pierre Joseph Bonnaterre\n\nAbbé Pierre Joseph Bonnaterre (1752, Aveyron – 20 September 1804, Saint-Geniez) was a French naturalist who contributed sections on cetaceans, mammals, birds, reptiles, amphibians, and insects to the \"Tableau encyclopédique et méthodique\". He is also notable as the first scientist to study the feral child Victor of Aveyron.\n\nBonnaterre is credited with identifying about 25 new species of fish, and assembled illustrations of about 400 in his encyclopedia work.\n\nHe was the first scientist to study Victor, the wild child of Aveyron, whose life inspired François Truffaut for his film \"The Wild Child\".\n\n"}
{"id": "20646400", "url": "https://en.wikipedia.org/wiki?curid=20646400", "title": "Strange matter", "text": "Strange matter\n\nStrange matter is a particular form of quark matter, usually thought of as a \"liquid\" of up, down and strange quarks. It is to be contrasted with nuclear matter, which is a liquid of neutrons and protons (which themselves are built out of up and down quarks), and with non-strange quark matter, which is a quark liquid containing only up and down quarks. At high enough density, strange matter is expected to be color superconducting. Strange matter is hypothesized to occur in the core of neutron stars, or, more speculatively, as isolated droplets that may vary in size from femtometers (strangelets) to kilometers (quark stars).\n\nIn particle physics and astrophysics, the term is used in two ways, one broader and the other more specific \n\n\nUnder the broader definition, strange matter might occur inside neutron stars, if the pressure at their core is high enough (i.e. above the critical pressure). At the sort of densities we expect in the center of a neutron star, the quark matter would probably be strange matter. It could conceivably be non-strange quark matter, if the effective mass of the strange quark were too high. Charm and heavier quarks would only occur at much higher densities.\n\nA neutron star with a quark matter core is often called a hybrid star. However, it is hard to know whether hybrid stars really exist in nature because physicists currently have little idea of the likely value of the critical pressure or density. It seems plausible that the transition to quark matter will already have occurred when the separation between the nucleons becomes much smaller than their size, so the critical density must be less than about 100 times nuclear saturation density. But a more precise estimate is not yet available, because the strong interaction that governs the behavior of quarks is mathematically intractable, and numerical calculations using lattice QCD are currently blocked by the fermion sign problem.\n\nOne major area of activity in neutron star physics is the attempt to find observable signatures by which we could tell, from earth based observations of neutron stars, whether they have quark matter (probably strange matter) in their core. \n\nIf the \"strange matter hypothesis\" is true then nuclear matter is metastable against decaying into strange matter. The lifetime for spontaneous decay is very long, so we do not see this decay process happening around us. However, under this hypothesis there should be strange matter in the universe:\n"}
{"id": "46694272", "url": "https://en.wikipedia.org/wiki?curid=46694272", "title": "The Devil's Teeth", "text": "The Devil's Teeth\n\nThe Devil's Teeth: A True Story of Obsession and Survival Among America's Great White Sharks is a non-fiction book about great white sharks by American journalist Susan Casey. The text was initially published by Henry Holt and Company on June 7, 2005. The book became a widely acclaimed bestseller.\n\nSusan Casey, a journalist and ocean lover, became infatuated with great white sharks of the Farallon Islands—dubbed by sailors in the 1850s the \"devil's teeth.\" The sharks there are the alphas among alphas, some longer than twenty feet, and they congregate just twenty-seven miles off the coast of San Francisco. After going through many restrictions and barriers, she manages to join a group of scientists studying predation patterns by great white sharks within the so-called Red Triangle.\n\n\n"}
{"id": "113442", "url": "https://en.wikipedia.org/wiki?curid=113442", "title": "The Right Stuff (film)", "text": "The Right Stuff (film)\n\nThe Right Stuff is a 1983 American epic historical drama film. It was adapted from Tom Wolfe's best-selling 1979 book of the same name about the Navy, Marine and Air Force test pilots who were involved in aeronautical research at Edwards Air Force Base, California, as well as the Mercury Seven, the seven military pilots who were selected to be the astronauts for Project Mercury, the first manned spaceflight by the United States. \"The Right Stuff\" was written and directed by Philip Kaufman and stars Ed Harris, Scott Glenn, Sam Shepard, Fred Ward, Dennis Quaid and Barbara Hershey. Levon Helm is the narrator in the introduction and elsewhere in the film, as well as having a co-starring role as Air Force test pilot Jack Ridley.\n\nThe film was a box-office failure, grossing approximately $21 million against a $27 million budget. Despite this, it received widespread critical acclaim and eight Oscar nominations at the 56th Academy Awards, four of which it won. In 2013 the film was selected for preservation in the United States National Film Registry by the Library of Congress as being \"culturally, historically, or aesthetically significant\".\n\nIn 1947, the Muroc Army Air Field in California has test pilots fly high-speed aircraft such as the rocket-powered Bell X-1, but some are killed as a result. After another pilot, Slick Goodlin, demands $150,000 () to attempt to break the sound barrier, war hero Captain Chuck Yeager receives the chance to fly the X-1. While on a horseback ride with his wife Glennis, Yeager collides with a tree branch and breaks his ribs, which inhibits him from leaning over and locking the door to the X-1. Worried that he might not fly the mission, Yeager confides in friend and fellow pilot Jack Ridley. Ridley cuts off part of a broomstick and tells Yeager to use it as a lever to help seal the hatch to the X-1, and Yeager becomes the first person to fly at supersonic speed, defeating the \"demon in the sky\".\n\nSix years later, Muroc, now Edwards Air Force Base, still attracts the best test pilots. Yeager (now a major) and friendly rival Scott Crossfield repeatedly break the other's speed records. They often visit the Happy Bottom Riding Club run by Pancho Barnes, who classifies the pilots at Edwards as either \"prime\" (such as Yeager and Crossfield) that fly the best equipment or newer \"pudknockers\" who only dream about it. Gordon \"Gordo\" Cooper, Virgil \"Gus\" Grissom and Donald \"Deke\" Slayton, captains of the United States Air Force, are among the \"pudknockers\" who hope to also prove that they have \"the Right Stuff\". The tests are no longer secret, as the military soon recognizes that it needs good publicity for funding, and with \"no bucks, no Buck Rogers\". Cooper's wife, Trudy, and other wives are afraid of becoming widows, but cannot change their husbands' ambitions and desire for success and fame.\n\nIn 1957, the launch of the Russian Sputnik satellite alarms the United States government. Politicians such as Senator Lyndon B. Johnson and military leaders demand that NASA help America defeat the Russians in the new Space Race. The search for the first Americans in space excludes Yeager because he lacks a college degree. Grueling physical and mental tests select the Mercury Seven astronauts, including John Glenn of the United States Marine Corps, Alan Shepard, Wally Schirra and Scott Carpenter of the United States Navy, as well as Cooper, Grissom and Slayton; they immediately become national heroes. Although many early NASA rockets explode during launch, the ambitious astronauts all hope to be the first in space as part of Project Mercury. Although engineers see the men as passengers, the pilots insist that the Mercury spacecraft have a window, a hatch with explosive bolts, and pitch-yaw-roll controls. However, Russia beats them into space on April 12, 1961 with the launch of Vostok 1 carrying Yuri Gagarin into space. The seven astronauts are determined to match and surpass the Russians.\n\nShepard is the first American to reach space on the 15-minute sub-orbital flight of Mercury-Redstone 3 on May 5. After Grissom's similar flight of Mercury-Redstone 4 on July 21, the capsule's hatch blows open and quickly fills with water. Grissom escapes, but the spacecraft, overweight with seawater, sinks. Many criticize Grissom for possibly panicking and opening the hatch prematurely. Glenn becomes the first American to orbit the Earth on Mercury-Atlas 6 on February 20, 1962, surviving a possibly loose heat shield, and receives a ticker-tape parade. He, his colleagues, and their families become celebrities, including a gigantic celebration in the Sam Houston Coliseum to announce the opening of the Manned Space Center in Houston, despite Glenn's wife Annie's fear of public speaking due to a stutter.\n\nAlthough test pilots at Edwards mock the Mercury program for sending \"spam in a can\" into space, they recognize that they are no longer the fastest men on Earth, and Yeager states that \"it takes a special kind of man to volunteer for a suicide mission, especially when it's on national TV.\" While testing the new Lockheed NF-104A, Yeager attempts to set a new altitude record at the edge of space but is nearly killed in a high-speed ejection after his aircraft went out of control in a flat spin. Though seriously burned, after reaching the ground Yeager gathers up his parachute and walks to the ambulance, proving that he still has the Right Stuff.\n\nOn May 15, 1963, Cooper has a successful launch on Mercury-Atlas 9, ending the Mercury program. As the last American to fly into space alone, he \"went higher, farther, and faster than any other American ... for a brief moment, Gordo Cooper became the greatest pilot anyone had ever seen.\"\n\n\n\nThe following real people also appeared in archive footage: Ed Sullivan with Bill Dana (playing his character José Jiménez); Yuri Gagarin and Nikita Khrushchev seen embracing at a review, along with Georgi Malenkov, Nikolai Bulganin, Kliment Voroshilov, and Anastas Mikoyan in attendance; and Lyndon B. Johnson, John F. Kennedy, and James E. Webb. The real Alan Shepard also makes a brief cameo in archival footage during Kennedy's archival scenes. The real Chuck Yeager also appears briefly as a bartender.\n\nIn 1979, independent producers Robert Chartoff and Irwin Winkler outbid Universal Pictures for the movie rights to Tom Wolfe's book, hiring William Goldman to write the screenplay. At Winkler's suggestion, Goldman's adaptation focused on the astronauts, entirely ignoring Chuck Yeager. Goldman was inspired to accept the job because he wanted to say something patriotic about America in the wake of the Iran hostage crisis.\n\nIn June 1980, United Artists agreed to finance the film up to $20 million, and the producers began looking for a director. Michael Ritchie was originally attached but fell through; so did John Avildsen who, four years prior, had won an Oscar for his work under Winkler and Chartoff on the original \"Rocky\". (\"The Right Stuff\" would have reunited Avildsen with both producers, and also with a fourth \"Rocky\" veteran, composer Bill Conti.) Ultimately, Chartoff and Winkler approached director Philip Kaufman, who agreed to make the film but did not like Goldman's script; Kaufman disliked the emphasis on patriotism, and wanted Yeager put back in the film. Eventually, Goldman quit the project in August 1980 and United Artists pulled out.\n\nWhen Wolfe showed no interest in adapting his own book, Kaufman wrote a draft in eight weeks. His draft restored Yeager to the story because \"if you're serious about tracing where the future - read: space travel - began, its roots lay with Yeager and the whole test pilot-subculture. Ultimately, astronautics descended from that point.\"\n\nAfter the financial failure of \"Heaven's Gate\", the studio put \"The Right Stuff\" in turnaround. Then the Ladd Company stepped in with an estimated $17 million.\n\nActor Ed Harris auditioned twice in 1981 for the role of John Glenn. Originally, Kaufman wanted to use a troupe of contortionists to portray the press corps, but settled on the improvisational comedy troupe Fratelli Bologna, known for its sponsorship of \"St. Stupid's Day\" in San Francisco. The director created a locust-like chatter to accompany the press corps whenever they appear, which was achieved through a sound combination of (among other things) motorized Nikon cameras and clicking beetles.\n\nShot between March and October 1982, with additional filming continuing into January 1983, most of the film was shot in and around San Francisco, where a waterfront warehouse was transformed into a studio. Location shooting took place primarily at the abandoned Hamilton Air Force Base north of San Francisco which was converted into a sound stage for the numerous interior sets. No location could substitute for the distinctive Edwards Air Force Base landscape which necessitated the entire production crew move to the Mojave Desert for the opening sequences that framed the story of the test pilots at Muroc Army Air Field, later Edwards AFB. Additional shooting took place in California City in early 1983. During the filming of a sequence which portrayed Chuck Yeager's ejection from an NF-104 stuck in a flat spin resulting in his smoldering helmet being filled with smoke, stuntman Joseph Svec, a former Green Beret, was killed when he failed to open his parachute.\n\nYeager was hired as a technical consultant on the film. He took the actors flying, studied the storyboards and special effects, and pointed out the errors. To prepare for their roles, Kaufman gave the actors playing the seven astronauts an extensive videotape collection to study.\n\nThe efforts at making an authentic feature led to the use of many full size aircraft, scale models and special effects to replicate the scenes at Edwards Air Force Base and Cape Canaveral Air Force Station. According to special visual effects supervisor Gary Gutierrez, the first special effects were too clean and they wanted a \"dirty, funky, early NASA look.\" Gutierrez and his team started from scratch, employing unconventional techniques—like going up a hill with model airplanes on wires and fog machines to create clouds, or shooting model F-104s from a crossbow device and capturing their flight with up to four cameras. Avant garde filmmaker Jordan Belson created the background of the Earth as seen from high-flying planes and from orbiting spacecraft.\n\nKaufman gave his five editors a list of documentary images the film required and they searched the country for film from NASA, the Air Force, and Bell Aircraft vaults. They also discovered Russian stock footage not viewed in 30 years. During the course of the production, Kaufman met with resistance from the Ladd Company and threatened to quit several times. In December 1982, 8,000 feet of film portraying John Glenn's trip in orbit and return to Earth disappeared or was stolen from Kaufman's editing facility in Berkeley, California. The missing footage was never found but the footage was reconstructed from copies.\n\nAlthough \"The Right Stuff\" was based on historical events and real people, as chronicled in Wolfe's book, some substantial dramatic liberties were taken. Neither Yeager's flight in the X-1 to break the sound barrier early in the film or his later, nearly fatal flight in the NF-104A were spur-of-moment, capriciously decided events, as the film seems to imply - they actually were part of the routine testing program for both aircraft. Yeager had already test-flown both aircraft a number of times previously and was very familiar with them. Jack Ridley had actually died in 1957, even though his character appears in several key scenes taking place after that, most notably including Yeager's 1963 flight of the NF-104A.\n\n\"The Right Stuff\" depicts Cooper arriving at Edwards in 1953, reminiscing with Grissom there about the two of them having supposedly flown together at the Langley Air Force Base and then hanging out with Grissom and Slayton, including all three supposedly being present at Edwards when Scott Crossfield flew at Mach 2 in November 1953. The film shows the three of them being recruited together there for the astronaut program in late 1957, with Grissom supposedly expressing keen interest in becoming a \"star-voyager\". According to their respective NASA biographies, none of the three was posted to Edwards before 1955 (Slayton in 1955 and Grissom and Cooper in 1956,) and neither of the latter two had previously trained at Langley. By the time astronaut recruitment began in late 1957 after the Soviets had orbited Sputnik, Grissom had already left Edwards and returned to Wright-Patterson Air Force Base, where he had served previously and was happy with his new assignment there. Grissom did not even know he was under consideration for the astronaut program until he received mysterious orders \"out of the blue\" to report to Washington in civilian clothing for what turned out to be a recruitment session for NASA. \n\nWhile the film took liberties with certain historical facts as part of \"dramatic license\", criticism focused on one: the portrayal of Gus Grissom panicking when his \"Liberty Bell 7\" spacecraft sank following splashdown. Most historians, as well as engineers working for or with NASA and many of the related contractor agencies within the aerospace industry, are now convinced that the premature detonation of the spacecraft hatch's explosive bolts was caused by mechanical failure not associated with direct human error or deliberate detonation by Grissom. This determination had been made long before the film was completed. Both Schirra and Gordon Cooper were critical of \"The Right Stuff\" for its treatment of Grissom, who was killed in the Apollo 1 launch pad fire in January 1967 and thus unable to defend himself when the film was being made. However, Kaufman was closely following Tom Wolfe's book, which focused not on how or why the hatch actually blew, but how NASA engineers and some of Grissom's colleagues (and even his own wife) believed he caused the accident; much of the dialogue in this sequence was taken directly from Wolfe's prose.\n\nThere were other inaccuracies as well, notably about the engineers who built the Mercury craft.\n\nA large number of film models were assembled for the production; for the more than 80 aircraft appearing in the film, static mock-ups and models were used as well as authentic aircraft of the period. Lieutenant Colonel Duncan Wilmore, USAF (Ret) acted as the United States Air Force liaison to the production, beginning his role as a technical consultant in 1980 when the pre-production planning had begun. The first draft of the script in 1980 had concentrated only on the Mercury 7 but as subsequent revisions developed the treatment into more of the original story that Wolfe had envisioned, the aircraft of late-1940s that would have been seen at Edwards AFB were required. Wilmore gathered World War II era \"prop\" aircraft including:\n\nThe first group were mainly \"set dressing\" on the ramp while the Confederate Air Force (now renamed the Commemorative Air Force) B-29 \"Fifi\" was modified to act as the B-29 \"mothership\" to carry the Bell X-1 and X-1A rocket-powered record-breakers.\n\nOther \"real\" aircraft included the early jet fighters and trainers as well as current USAF and United States Navy examples. These flying aircraft and helicopters included:\n\nA number of aircraft significant to the story had to be recreated. The first was an essentially static X-1 that had to at least roll and even realistically \"belch flame\" which was accomplished by a simulated rocket blast from the exhaust pipes. A series of wooden mock-up X-1s were used to depict interior shots of the cockpit, the mating up of the X-1 to a modified B-29 fuselage and bomb bay and ultimately to recreate flight in a combination of model work and live-action photography. The \"follow-up\" X-1A was also an all-wooden model.\n\nThe U.S. Navy's Douglas D-558-2 Skyrocket that Crossfield duelled with Yeager's X-1 and X-1A was recreated from a modified Hawker Hunter jet fighter. The climactic flight of Yeager in a Lockheed NF-104A was originally to be made with a modified Lockheed F-104 Starfighter but ultimately, Wilmore decided that the production had to make do with a repainted Luftwaffe F-104G, which lacks the rocket engine of the NF-104.\n\nWooden mock-ups of the Mercury space capsules also realistically depicted the NASA spacecraft and were built from the original mold.\n\nFor many of the flying sequences, scale models were produced by USFX Studios and filmed outdoors in natural sunlight against the sky. Even off-the-shelf plastic scale models were utilized for aerial scenes. The X-1, F-104 and B-29 models were built in large numbers as a number of the more than 40 scale models were destroyed in the process of filming. The blending together of miniatures, full-scale mock-ups and actual aircraft was seamlessly integrated into the live-action footage. The addition of original newsreel footage was used sparingly but to effect to provide another layer of authenticity.\n\n\"The Right Stuff\" had its world premiere on October 16, 1983, at the Kennedy Center in Washington, D.C., to benefit the American Film Institute. It was given a limited release on October 21, 1983, in 229 theaters, grossing $1.6 million on its opening weekend. It went into wide release on February 17, 1984, in 627 theaters where it grossed an additional $1.6 million on that weekend. But despite this, the movie bombed at the box office with $21.1 million. The failure of this and \"Twice Upon a Time\" caused The Ladd Company to shut down.\n\nAs part of the promotion for the film, Veronica Cartwright, Chuck Yeager, Gordon Cooper, Scott Glenn and Dennis Quaid appeared in 1983 at ConStellation, the 41st World Science Fiction Convention in Baltimore.\n\n\"The Right Stuff\" received overwhelming acclaim from critics. The film holds a 98% approval rating on Rotten Tomatoes based on 44 reviews. Film critic Roger Ebert named \"The Right Stuff\" best film of 1983, and wrote, \"it joins a short list of recent American movies that might be called experimental epics: movies that have an ambitious reach through time and subject matter, that spend freely for locations or special effects, but that consider each scene as intently as an art film\". He later named it one of the best films of the decade and wrote, \"\"The Right Stuff\" is a greater film because it is not a straightforward historical account but pulls back to chronicle the transition from Yeager and other test pilots to a mighty public relations enterprise\". He later put it at #2 on his 10 best of the 1980s, behind Martin Scorsese's \"Raging Bull\". Gene Siskel, Ebert's co-host of \"At the Movies\", also named \"The Right Stuff\" the best film of 1983, and said \"It's a great film, and I hope everyone sees it.\" Siskel also went on to include \"The Right Stuff\" at #3 on his list of the best films of the 1980s, behind \"Shoah\" and \"Raging Bull\".\n\nIn his review for \"Newsweek\", David Ansen wrote, \"When \"The Right Stuff\" takes to the skies, it can't be compared with any other movie, old or new: it's simply the most thrilling flight footage ever put on film\". Gary Arnold in his review for the \"Washington Post\", wrote, \"The movie is obviously so solid and appealing that it's bound to go through the roof commercially and keep on soaring for the next year or so\". In his review for \"The New York Times\", Vincent Canby praised Shepard's performance: \"Both as the character he plays and as an iconic screen presence, Mr. Shepard gives the film much well-needed heft. He is the center of gravity\". Pauline Kael wrote, \"The movie has the happy, excited spirit of a fanfare, and it's astonishingly entertaining, considering what a screw-up it is\".\n\nYeager said of the film: \"Sam [Shepard] is not a real flamboyant actor, and I'm not a real flamboyant-type individual ... he played his role the way I fly airplanes\". Deke Slayton said that none of the film \"was all that accurate, but it was well done\". Slayton later described the film as being \"as bad as the book was good, just a joke\". Wally Schirra said, \"They insulted the lovely people who talked us through the program - the NASA engineers. They made them like bumbling Germans\". Scott Carpenter felt that it was a \"great movie in all regards\".\n\nRobert Osborne, who introduced showings of the film on Turner Classic Movies, was quite enthusiastic about the film. The cameo appearance by the real Chuck Yeager in the film was a particular \"treat\" which Osborne cited. The recounting of many of the legendary aspects of Yeager's life was left in place, including the naming of the X-1, \"Glamorous Glennis\" after his wife and his superstitious preflight ritual of asking for a stick of Beemans chewing gum from his best friend, Jack Ridley.\n\nWhen the film came out, the former (and future) astronaut Senator John Glenn of Ohio was running for the Democratic nomination for President of the United States. In the weeks before the movie's premiere, media pundits as well as people in the Democratic Party wondered that if the film became a big success, it could give Glenn an advantage in the upcoming primaries, a speculation that eventually proved groundless.\n\n\"The Right Stuff\" won four Academy Awards: for Best Sound Effects Editing (Jay Boekelheide), for Best Film Editing, for Best Original Score and for Best Sound (Mark Berger, Tom Scott, Randy Thom and David MacMillan).\n\nThe film was also nominated for Best Actor in a Supporting Role (Sam Shepard), Best Art Direction (Art Direction: Geoffrey Kirkland, Richard Lawrence, W. Stewart Campbell and Peter R. Romero; Set Decoration: George R. Nelson), Best Cinematography (Caleb Deschanel) and Best Picture. The movie was also nominated for the Hugo Award for Best Dramatic Presentation in 1984. Scott Glenn was also nominated for the New York Film Critics' Award for Best Supporting Actor.\n\nOn June 23, 2003, Warner Bros. released a two-disc DVD Special Edition that featured scene-specific commentaries with key cast and crew members, deleted scenes, three documentaries on the making of \"The Right Stuff\" including interviews with Mercury astronauts and Chuck Yeager, and a feature-length PBS documentary, \"John Glenn: American Hero\". These extras are also included in the November 5, 2013 release of the 30th Anniversary edition, which also includes a 40-page book binding case, with the film in Blu-ray format. The extras are in standard DVD format.\n\nIn addition, the British Film Institute published a book on \"The Right Stuff\" by Tom Charity in October 1997 that offered a detailed analysis and behind-the-scenes anecdotes.\n\nAlthough an album mix had been prepared by Bill Conti in 1983 (and indeed the poster contains the credit \"Original Soundtrack Available On Geffen Records\"), the soundtrack album release was cancelled following the film's disappointing box office. In 1986, Conti conducted a re-recording of selections from the score and from his music for \"North and South\", performed by the London Symphony Orchestra and released by Varèse Sarabande The original soundtrack was released by Varèse Sarabande on September 20, 2013, prepared from the 1983 album mix (as the original masters of the complete score were lost).\n\n\n"}
{"id": "30529991", "url": "https://en.wikipedia.org/wiki?curid=30529991", "title": "The Sibling Society", "text": "The Sibling Society\n\nThe Sibling Society is a book by poet, activist and author Robert Bly, published in 1996. Bly argues that modern men face difficulties caused by an inability to reach full maturity, and discusses the consequences this has for the societies in which they live. The core of Bly's thesis seems derived from Alexander Mitscherlich's 1963 monograph, Society without the Father (Auf dem Weg zur vaterlosen Gesellschaft), for which Bly wrote an introduction to the American edition ().\n"}
{"id": "15326711", "url": "https://en.wikipedia.org/wiki?curid=15326711", "title": "Thomas Brown Jordan", "text": "Thomas Brown Jordan\n\nThomas Brown Jordan (24 October 1807 – 31 May 1890) was a British inventor and engineer.\n\nBorn at Bristol on 24 October 1807, he was the son of Thomas Jordan (a Quaker engineer), and began life as an artist.\n\nWhen barely twenty he moved to Falmouth. While painting there and at Penzance he made the acquaintance of Robert Were Fox the Younger, in whose physical researches he took the greatest interest, as well as becoming drawing master to Fox's son, Barclay.\n\nR.W.Fox's influence led him to relinquish painting and to set up as a mathematical instrument maker in Falmouth, where he effected improvements in the miners' dial, and had some share in the construction of Fox's improved dipping-needle. In 1838 Jordan devised an instrument for recording by photography the variations of the barometric column, and he shortly afterwards invented a declination magnetograph and a self-recording actinometer. His pioneering use of photography in meteorology was acknowledged by both Charles Wheatstone and Sir John Herschel. For some years subsequent to 1839 he held the post of secretary of the Royal Cornwall Polytechnic Society.\n\nSir Henry de la Beche, when engaged on the geological survey of Cornwall, made Jordan's acquaintance, and secured his appointment in 1840 as first keeper of mining records, with charge of plans, sections, and models. Jordan took a great interest in electro-metallurgy during the early years of its development, and in 1841 he made an egg-cup of electro-deposited copper, plated with silver outside and gold inside, which was considered a model of workmanship, and is now deposited in the Museum of Practical Geology, Jermyn Street, London.\n\nUpon resigning his appointment as keeper of mining records in 1845, Jordan invented a highly ingenious process of carving by machinery, and set up works at Lambeth for carrying into effect the invention, for which in 1847 he received the gold Isis medal from the Society of Arts, and in the same year he was elected a Fellow of the Society. The wood-carving machinery was subsequently exhibited at the Great Exhibition of 1851, and the products were extensively used in the decoration of the House of Lords.\n\nLater on Jordan started work as a mechanical engineer, first at Manchester, then at Glasgow, where he devised a series of machines for the production of school slates. Shortly after 1870, however, he returned to London, and established himself as a mining engineer in conjunction with his son, Mr. Thomas Rowland Jordan. Jordan's last invention, patented in 1877, was a portable machine for boring blast-holes in rock. He died in Bournemouth on 31 May 1890.\n\nJordan married, in 1837, Sarah Dunn. They had eleven children. He died 1890-05-31. Mrs. Jordan survived him.\n\n\n"}
{"id": "56301525", "url": "https://en.wikipedia.org/wiki?curid=56301525", "title": "Wilhelm Eichenberg", "text": "Wilhelm Eichenberg\n\nWilhelm Eichenberg (fl. 1930s) was a geologist and zoologist known for having described the class Conodonta of prehistoric jawless fish in 1930.\n\n"}
{"id": "7778265", "url": "https://en.wikipedia.org/wiki?curid=7778265", "title": "WomensHub", "text": "WomensHub\n\nWomensHub is a Philippines-based non-government organisation that supports \"women struggling for self-determination\" in using ICTs, or information and communication technologies.\n\nThis organisation recently completed three years as a collective. It joined the Asia-Pacific programme of the Association for Progressive Communications's Women Network Support Programme.\n\nIts focus, inter alia, has been to increase the visibility of gender and ICT issues during the Asia-Pacific NGO Conference for the Beijing Platform for Action held in mid-2004 in Bangkok, the capital of Thailand. WomensHub members there ran the internet access facility in the conference site, and provided user-support to conference participants.\n\nWomensHub was created in December 2000 \"to advance women and social movement issues in the field of ICTs.\"\n\nIt works on promoting gender equality through ICTs, and also supports the Women's Electronic Network Training (WENT) of the Asian Women's Resource Exchange (AWORC). WENT offers an annual workshop on electronic networking, open to women and organizations, which helps to build skills in women's organizations and networks in Asia and the Pacific.\n\n"}
