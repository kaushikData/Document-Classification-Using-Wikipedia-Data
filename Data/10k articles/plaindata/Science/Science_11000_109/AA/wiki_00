{"id": "26306022", "url": "https://en.wikipedia.org/wiki?curid=26306022", "title": "A.C. Redfield Lifetime Achievement Award", "text": "A.C. Redfield Lifetime Achievement Award\n\nThe Lifetime Achievement Award was first presented in 1994 to honor major long-term achievements in the fields of limnology and oceanography, including research, education and service to the community and society. In 2004, the American Society of Limnology and Oceanography Board renamed the award in honor of Alfred C. Redfield. \n\nA. The information in the table is according to the \"ASLO Awards and Nominations\" web page at the official website of the American Society of Limnology and Oceanography unless otherwise specified by additional citations.\n\n"}
{"id": "2941992", "url": "https://en.wikipedia.org/wiki?curid=2941992", "title": "Adrenal fatigue", "text": "Adrenal fatigue\n\nAdrenal fatigue or hypoadrenia is a pseudoscientific diagnosis believed in alternative medicine to be the state when adrenal glands are exhausted and unable to produce adequate quantities of hormones, primarily the glucocorticoid cortisol, due to chronic stress or infections. Adrenal fatigue should not be confused with a number of actual forms of adrenal dysfunction such as adrenal insufficiency or Addison's disease.\n\nThe term \"adrenal fatigue\", which was invented in 1998 by James Wilson, a chiropractor, may be applied to a collection of mostly nonspecific symptoms. There is no scientific evidence supporting the concept of adrenal fatigue and it is not recognized as a diagnosis by any scientific or medical community. A systematic review found no evidence for the term adrenal fatigue, confirming the consensus among endocrinological societies that it is a myth.\n\nBlood or salivary testing is sometimes offered but there is no evidence that adrenal fatigue exists or can be tested for. The concept of adrenal fatigue has given rise to an industry of dietary supplements marketed to treat this condition. These supplements are largely unregulated in the U.S., are ineffective, and in some cases may be dangerous.\n\n\n"}
{"id": "2647662", "url": "https://en.wikipedia.org/wiki?curid=2647662", "title": "André Cailleux", "text": "André Cailleux\n\nAndré de Cayeux de Senarpont (known as André Cailleux, 24 December 1907 – 27 December 1986) was a French paleontologist and geologist.\n\nHe was born in Paris, France. After earning his Ph.D. in 1942, he became a specialist in glacial and periglacial morphology. He worked as a professor of geology at the Sorbonne in Paris. \n\nCailleux was a Christian and proponent of theistic evolution.\n\nHe died at Saint-Maur-des-Fossés.\n\n\nHe was also the author of many papers.\n\n\n\n"}
{"id": "40232536", "url": "https://en.wikipedia.org/wiki?curid=40232536", "title": "Bacillus phage G", "text": "Bacillus phage G\n\n\"Bacillus\" phage G is a bacteriophage (phage) that infects \"Bacillus\" bacteria. The phage has been reported to have the largest genome of all discovered \"Myoviridae\" with nearly 700 protein-coding genes.\n\n"}
{"id": "12853423", "url": "https://en.wikipedia.org/wiki?curid=12853423", "title": "Brand implementation", "text": "Brand implementation\n\nIn marketing, brand implementation refers to the physical representation and consistent application of brand identity across visual and verbal media. In visual terms, this can include signage, uniforms, liveries, interior design and branded merchandise. Brand implementation encompasses facets of architecture, product design, industrial design, quantity surveying, engineering, procurement, project management and retail design.\n\nBrand implementation is an integrated part of a branding cycle and needs to be initiated during the brand design and development phase. Brand implementation is the continuous and consistent application of the brand's image in all business units, communication channels and media.\n\nThis refers to marketing and branding as a unified whole. In that respect, brand implementation is a continuous process, which requires controlling the brand's image and presence despite changes in markets and company structure.\n\nBrand implementation emerged as a discipline in the 1990s when brand owners recognized the need for consistency across branded estates. Traditionally, brand implementation was handled by various parties, including shop-fitters, interior designers and sign companies. Lack of centralized project management led to inconsistencies, while information dissymmetry meant suppliers had too much control over brand issues. Brand implementation was consequently coined as an umbrella term for all aspects of the application and maintenance of physical brand assets.\n\nThe experience of more than 80 companies operating worldwide shows that a lack of planning before the rebrand, consistent implementation and complete control are the key problems of brand implementation. Eight large stumbling blocks stand in the way of effective implementation: \n\nBrand implementation does not involve the design or creation of brand identity; brand implementation agencies work closely with branding agencies to ensure that their work is applied accurately and consistently. This relationship is referred to as Magic and Logic (RTM of Marketing Supply Chain International). Branding agencies look after the Magic (creative) and brand implementation agencies look after the Logic (implementation).\n\n\n"}
{"id": "6956", "url": "https://en.wikipedia.org/wiki?curid=6956", "title": "Conservation law", "text": "Conservation law\n\nIn physics, a conservation law states that a particular measurable property of an isolated physical system does not change as the system evolves over time. Exact conservation laws include conservation of energy, conservation of linear momentum, conservation of angular momentum, and conservation of electric charge. There are also many approximate conservation laws, which apply to such quantities as mass, parity, lepton number, baryon number, strangeness, hypercharge, etc. These quantities are conserved in certain classes of physics processes, but not in all.\n\nA local conservation law is usually expressed mathematically as a continuity equation, a partial differential equation which gives a relation between the amount of the quantity and the \"transport\" of that quantity. It states that the amount of the conserved quantity at a point or within a volume can only change by the amount of the quantity which flows in or out of the volume.\n\nFrom Noether's theorem, each conservation law is associated with a symmetry in the underlying physics.\n\nConservation laws are fundamental to our understanding of the physical world, in that they describe which processes can or cannot occur in nature. For example, the conservation law of energy states that the total quantity of energy in an isolated system does not change, though it may change form. In general, the total quantity of the property governed by that law remains unchanged during physical processes. With respect to classical physics, conservation laws include conservation of energy, mass (or matter), linear momentum, angular momentum, and electric charge. With respect to particle physics, particles cannot be created or destroyed except in pairs, where one is ordinary and the other is an antiparticle. With respect to symmetries and invariance principles, three special conservation laws have been described, associated with inversion or reversal of space, time, and charge.\n\nConservation laws are considered to be fundamental laws of nature, with broad application in physics, as well as in other fields such as chemistry, biology, geology, and engineering.\n\nMost conservation laws are exact, or absolute, in the sense that they apply to all possible processes. Some conservation laws are partial, in that they hold for some processes but not for others.\n\nOne particularly important result concerning conservation laws is Noether's theorem, which states that there is a one-to-one correspondence between each one of them and a differentiable symmetry of nature. For example, the conservation of energy follows from the time-invariance of physical systems, and the conservation of angular momentum arises from the fact that physical systems behave the same regardless of how they are oriented in space.\n\nA partial listing of physical conservation equations due to symmetry that are said to be exact laws, or more precisely \"have never been proven to be violated:\"\n\nThere are also approximate conservation laws. These are approximately true in particular situations, such as low speeds, short time scales, or certain interactions.\n\n\nThe total amount of some conserved quantity in the universe could remain unchanged if an equal amount were to appear at one point \"A\" and simultaneously disappear from another separate point \"B\". For example, an amount of energy could appear on Earth without changing the total amount in the Universe if the same amount of energy were to disappear from a remote region of the Universe. This weak form of \"global\" conservation is really not a conservation law because it is not Lorentz invariant, so phenomena like the above do not occur in nature. Due to Special Relativity, if the appearance of the energy at \"A\" and disappearance of the energy at \"B\" are simultaneous in one inertial reference frame, they will not be simultaneous in other inertial reference frames moving with respect to the first. In a moving frame one will occur before the other; either the energy at \"A\" will appear \"before\" or \"after\" the energy at \"B\" disappears. In both cases, during the interval energy will not be conserved. \n\nA stronger form of conservation law requires that, for the amount of a conserved quantity at a point to change, there must be a flow, or \"flux\" of the quantity into or out of the point. For example, the amount of electric charge in a volume is never found to change without an electric current into or out of the volume that carries the difference in charge. Since it only involves continuous \"local\" changes, this stronger type of conservation law is Lorentz invariant; a quantity conserved in one reference frame is conserved in all moving reference frames. This is called a \"local conservation\" law. Local conservation also implies global conservation; that the total amount of the conserved quantity in the Universe remains constant. All of the conservation laws listed above are local conservation laws. A local conservation law is expressed mathematically by a \"continuity equation\", which states that the change in the quantity in a volume is equal to the total net \"flux\" of the quantity through the surface of the volume. The following sections discuss continuity equations in general.\n\nIn continuum mechanics, the most general form of an exact conservation law is given by a continuity equation. For example, conservation of electric charge \"q\" is\n\nwhere ∇⋅ is the divergence operator, \"ρ\" is the density of \"q\" (amount per unit volume), j is the flux of \"q\" (amount crossing a unit area in unit time), and \"t\" is time.\n\nIf we assume that the motion u of the charge is a continuous function of position and time, then\n\nIn one space dimension this can be put into the form of a homogeneous first-order quasilinear hyperbolic equation:\n\nwhere the dependent variable \"y\" is called the \"density\" of a \"conserved quantity\", and \"A(y)\" is called the \"current jacobian\", and the subscript notation for partial derivatives has been employed. The more general inhomogeneous case:\n\nis not a conservation equation but the general kind of balance equation describing a dissipative system. The dependent variable \"y\" is called a \"nonconserved quantity\", and the inhomogeneous term \"s(y,x,t)\" is the-\"source\", or dissipation. For example, balance equations of this kind are the momentum and energy Navier-Stokes equations, or the entropy balance for a general isolated system.\n\nIn the one-dimensional space a conservation equation is a first-order quasilinear hyperbolic equation that can be put into the \"advection\" form:\n\nwhere the dependent variable \"y(x,t)\" is called the density of the \"conserved\" (scalar) quantity (c.q.(d.) = conserved quantity (density)), and \"a(y)\" is called the current coefficient, usually corresponding to the partial derivative in the conserved quantity of a current density (c.d.) of the conserved quantity \"j(y)\":\n\nIn this case since the chain rule applies:\n\nthe conservation equation can be put into the current density form:\n\nIn a space with more than one dimension the former definition can be extended to an equation that can be put into the form:\n\nwhere the \"conserved quantity\" is \"y(r,t)\", \"formula_11\" denotes the scalar product, \"∇\" is the nabla operator, here indicating a gradient, and \"a(y)\" is a vector of current coefficients, analogously corresponding to the divergence of a vector c.d. associated to the c.q. j(y):\n\nThis is the case for the continuity equation:\n\nHere the conserved quantity is the mass, with density \"ρ\"(r,t) and current density \"ρ\"u, identical to the momentum density, while u(r,t) is the flow velocity.\n\nIn the general case a conservation equation can be also a system of this kind of equations (a vector equation) in the form:\n\nwhere y is called the \"conserved\" (vector) quantity, ∇ y is its gradient, 0 is the zero vector, and A(y) is called the Jacobian of the current density. In fact as in the former scalar case, also in the vector case A(y) usually corresponding to the Jacobian of a current density matrix J(y):\n\nand the conservation equation can be put into the form:\n\nFor example, this the case for Euler equations (fluid dynamics). In the simple incompressible case they are:\n\nwhere:\n\nIt can be shown that the conserved (vector) quantity and the c.d. matrix for these equations are respectively:\n\nwhere \"formula_19\" denotes the outer product.\n\nConservation equations can be also expressed in integral form: the advantage of the latter is substantially that it requires less smoothness of the solution, which paves the way to weak form, extending the class of admissible solutions to include discontinuous solutions. By integrating in any space-time domain the current density form in 1-D space:\n\nand by using Green's theorem, the integral form is:\n\nIn a similar fashion, for the scalar multidimensional space, the integral form is:\n\nwhere the line integration is performed along the boundary of the domain, in an anticlock-wise manner.\n\nMoreover, by defining a test function \"φ\"(r,\"t\") continuously differentiable both in time and space with compact support, the weak form can be obtained pivoting on the initial condition. In 1-D space it is:\n\nNote that in the weak form all the partial derivatives of the density and current density have been passed on to the test function, which with the former hypothesis is sufficiently smooth to admit these derivatives.\n\n\n\n\n"}
{"id": "1728077", "url": "https://en.wikipedia.org/wiki?curid=1728077", "title": "Copywriting", "text": "Copywriting\n\nCopywriting is the act of writing text for the purpose of advertising or other forms of marketing. The product, called copy, is written content that aims to increase brand awareness and ultimately persuade a person or group to take a particular action.\n\nCopywriters help create billboards, brochures, catalogs, jingle lyrics, magazine and newspaper advertisements, sales letters and other direct mail, scripts for television or radio commercials, taglines, white papers, social media posts, and other marketing communications.\n\nThey are generally known as website content writers or copywriters if their work appears mostly on the Internet. A content writer helps create online advertisements, web pages, email newsletters, blog posts and social media posts.\n\nCross discipline copywriters who look at the wider context of their work are called digital copywriters. The distinction is that these individuals consider the mechanics of the user journey, the external links that are included in the copy for search engine optimization and are highly focused towards creating online sales and dealing with technical issues such as bounce rate.\n\nMany copywriters are employed in advertising agencies, public relations firms, or copywriting agencies.\n\n\nCopywriters also work in-house for retail chains, book publishers or other big firms which advertise frequently. They can also be employed to write advertorials for newspapers, magazines, broadcasters and cable providers.\n\nSome copywriters work as independent contractors, doing freelance writing for a variety of clients. They may work at a client's office, a coworking office, a coffeehouse, or from home.\n\nCopywriters are similar to technical writers and the careers may overlap. Broadly speaking, however, technical writing is dedicated to informing readers rather than persuading them. For example, a copywriter writes an advertisement to sell a car, while a technical writer writes the operator's manual explaining how to use it.\n\nJohn Emory Powers (1837-1919) was the world's first full-time copywriter. Other famous copywriters who worked in advertising throughout their careers include William Bernbach, Leo Burnett, Robert Collier, Claude C. Hopkins, David Ogilvy and Lester Wunderman.\n\nMany creative artists worked as copywriters before becoming famous, including:\n\nHerschell Gordon Lewis, on the other hand, became famous for directing exploitation films before becoming a successful copywriter.\n\nIn book publishing, \"flap copy\" or \"jacket flap copy\" is the summary of a book which appears on the inside of a hardcover dust jacket; \"back cover copy\" is similar text, usually briefer, on the outside back cover; and \"catalog copy\" is a summary written for a publisher's catalog.\n\nThe Internet has expanded the range of copywriting opportunities to include landing pages and other web content, online advertisements, emails, blogs, social media and other forms of electronic communications.\n\nThe Internet has brought new opportunities for copywriters to learn their craft, do research and view others' work. Clients, copywriters and art directors can more readily find each other, making freelancing a viable job option.\n\nWeb copy may include among its objectives the achievement of higher rankings in search engines. Known as \"organic\" search engine optimization (SEO), this involves the strategic placement and repetition of keywords and keyword phrases on web pages, writing in a manner that human readers would consider normal.\n\n"}
{"id": "43791967", "url": "https://en.wikipedia.org/wiki?curid=43791967", "title": "Discrete Global Grid", "text": "Discrete Global Grid\n\nA Discrete Global Grid (DGG) is a mosaic which covers the entire Earth's surface.\nMathematically it is a space partitioning: it consists of a set of non-empty regions that form a partition of the Earth's surface. In a usual grid-modeling strategy, to simplify position calculations, each region is represented by a point, abstracting the grid as a set of region-points. Each region or region-point in the grid is called a cell.\n\nWhen each cell of a grid is subject to a recursive partition, resulting in a \"series of discrete global grids with progressively finer resolution\", forming a hierarchical grid, it is named Hierarchical DGG (sometimes \"DGG system\").\n\nDiscrete Global Grids are used as the geometric basis for the building of geospatial data structures. Each cell is related with data objects or values, or (in the hierarchical case) may be associated with other cells. DGGs have been proposed for use in a wide range of geospatial applications, including vector and raster location representation, data fusion, and spatial databases.\n\nThe most usual grids are for horizontal position representation, using a standard datum, like WGS84. In this context is commom also to use a specific DGG as foundation for geocoding standardization.\n\nIn the context of a spatial index, a DGG can assign unique identifiers to each grid cell, using it for spatial indexing purposes, in geodatabases or for geocoding.\n\nThe \"globe\", in the DGG concept, have no strict semantic; but in Geodesy a so-called \"Grid Reference System\" is a grid that divides space with precise positions relative to a datum, that is an approximated a \"standard model of the Geoid\". So, in the role of Geoid, the \"globe\" covered by a DGG can be any of the following objects:\n\n\nAs a global modeling process, modern DGGs, when including projection process, tend to avoid surfaces like cylinder or a conic solids that result in discontinuities and indexing problems. Regular polyhedra and other topological equivalents of sphere led to the most promising known options to be covered by DGGs, because \"spherical projections preserve the \"correct topology\" of the Earth – there are no singularities or discontinuities to deal with\".\n\nWhen working with a DGG it is important to specify which of these options was adopted. So, the characterization of the \"reference model of the globe\" of a DGG can be summarized by:\n\n\nNOTE: when the DGG is covering a projection surface, in a context of data provenance, the metadata about reference-Goid is also important — typical informing its ISO 19111's CRS value, with no confusion with the projection surface.\n\nThe main distinguishing feature to classify or compare DGGs is the use or not of an hierarchical grid structures:\n\n\nOther usual criteria to classify a DGG are tile-shape and granularity (\"grid resolution\"):\n\n\nThe most common class of Discrete Global Grids are those that place cell center points on longitude/latitude meridians and parallels, or which use the longitude/latitude meridians and parallels to form the boundaries of rectangular cells. Examples of such grids, all based on Latitude/Longitude:\n\nThe right aside illustration show 3 boundary maps of the coast of Great Britain. The first map was covered by a grid-level-0 with 150 km size cells. Only a grey cell in the center, with no need of zoom for detail, remains level-0; all other cells of the second map was partitioned into four-cells-grid (grid-level-1), each with 75 km. In the third map 12 cells level-1 remains as grey, all other was partitioned again, each level-1-cell transformed into a level-2-grid. Examples of DGGs that use such recursive process, generating hierarchical grids, include:\n\nThere are many DGGs because there are many representational, optimization and modeling alternatives. All DGG grid is a composition of its cells, and, in the Hierarchical DGG each cell use a new grid over its local region.\n\nThe illustration is not adequate to TIN DEM cases and similar \"raw data\" structures, where the database not use the cell concept (that geometrically is the triangular region), but nodes and edges: each node is an elevation and each edge is the distance between to nodes.\n\nIn general each cell of the DGG is identified by the coordinates of its region-point (illustrated as the <samp>centralPoint</samp> of a database representation). It is also possible, with loss of functionality, to use a \"free identifier\", that is, any unique number or unique symbolic label per cell, the cell ID. The ID is usually used as spatial index (such as internal Quadtree or k-d tree), but is also possible to transform ID into a human-readable label for geocoding applications.\n\nModern databases (e.g. using S2 grid) use also multiple representations for the same data, offering both, a grid (or cell region) based in the Geoid and a grid based in the projection.\n\nDiscrete Global Grids with cell regions defined by parallels and meridians of latitude/longitude have been used since the earliest days of global geospatial computing. Before it, the discretization of continuous coordinates for practical purposes, with paper maps, occurred only with low granularity. Perhaps the most representative and main example of DGG of this pre-digital era was the 1940s military UTM DGGs, with finner granulaed cell identification for geocoding purposes. Similarly some \"hierarchical grid\" exists before geospatial computing, but only in coarse granulation.\n\nA global surface is not required for use on daily geographical maps, and the memory was very expansive before the 2000s, to put all planetary data into the same computer. The first digital global grids was used for data processing of the satellite images and global (climatic and oceanographic) fluid dynamics modeling.\n\nThe first published references to \"Hierarchical Geodesic DGG\" systems are to systems developed for atmospheric modeling and published in 1968. These systems have hexagonal cell regions created on the surface of a spherical icosahedron.\n\nThe spatial hierarchical grids was subject to more intensive studies in the 1980s, when main structures, as Quadtree, was adapted in image indexing and databases.\n\nWhile specific instances of these grids have been in use for decades, the term \"Discrete Global Grids\" were coined by researchers at Oregon State University in 1997 to describe the class of all such entities.\n\nThe evaluation Discrete Global Grid consists of many aspects, including area, shape, compactness, etc.\nEvaluation methods for map projection, such as Tissot's indicatrix, are also suitable for evaluating map projection based Discrete Global Grid.\n\nIn addition, averaged ratio between complementary profiles (AveRaComp) gives a good evaluation of shape distortions for quadrilateral-shaped Discrete Global Grid.\n\nDatabase development-choices and adaptations are oriented by practical demands, for greater performance, reliability or precision. The best choices are being selected and adapted to necessities, propitiating the evolution of the DGG architectures. Examples of this evolution process: from non-hierarchical to hierarchical DGGs; from the use of Z-curve indexes (a \"naive algorithm\" based in digits-interlacing), used by Geohash, to Hilbert-curve indexes, used in modern optimizations, like S2.\n\nIn general each cell of the grid is identified by the coordinates of its region-point, but it is also possible to simplify the coordinate syntax and semantics, to obtain an identifier, as in a classic alphanumeric grids — and find the coordinates of a region-point from its identifier. \nSmall and fast coordinate representations is a goal in the cell-ID implementations, for any DGG solutions.\n\nThere are no loss of functionality when using a \"free identifier\" instead a coodinate, that is, any unique number or unique symbolic label per region-point, the cell ID. So, to transform a coordinate into a human-readable label, and/or compressing the length of the label, is an additional step in the grid representation.\n\nSome popular \"global place codes\" as ISO 3166-1 alpha-2 for administrative regions or Longhurst code for ecological regions of the globe, are \"partial\" in globe's coverage. By other hand, any set of cell-identifiers of a specific DGG can be used as \"full-coverage place codes\". Each different set of IDs, when used as a standard for data interchange purposes, are named \"geocoding system\".\nThere are many ways to represent the value of a cell identifier (\"cell-ID\") of a grid: structured or monolithic, binary or not, human-readable or not. Supposing a map feature, like the (~5m scale feature), represented by its minimum bounding cell or a center-point-cell, the \"cell ID\" will be:\n\nAll these geocodes represents the same position in the globe, with similar precision, but differ in string-length, separators-use and alphabet (non-separator characters). \nIn some cases the \"original DGG\" representation can be used. The variants are minor changes, affecting only final representation, for example the base of the numeric representation, or interlacing parts of the structured into only one number or code representation. The most popular variants are used for geocoding applications.\n\nDGGs and its variants, with human-readable cell-identifiers, has been used as \"de facto\" standard for alphanumeric grids. It is not limited to alphanumeric symbols, but \"alphanumeric\" is the most usual term.\n\nGeocodes are notations for locations, and in a DGG context, notations to express grid cell IDs. There are a continuous evolution in digital standards and DGGs, so a continuous change in the popularity of each geocoding covention in the last years. Broader adoption also depends on country's government adoption, use in popular mapping platforms, and many other factors.\n\nExamples used in the following list are about \"minor grid cell\" containing the , codice_1.\n\nOther documented variants, but supposed not in use, or to be \"never popular\":\n\n\n"}
{"id": "3754006", "url": "https://en.wikipedia.org/wiki?curid=3754006", "title": "Doctor of Engineering", "text": "Doctor of Engineering\n\nThe Doctor of Engineering, or Engineering Doctorate, (abbreviated Eng.D., D.Eng., D.Engr., Dr.Eng., or Dr.-Ing.) is a doctoral degree awarded on the basis of advanced study and research in engineering and applied sciences. In most of the countries it is a terminal research doctorate; in the United Kingdom it can be a higher doctorate. An EngD degree is essentially an engineering PhD with a solid industrial base and an additional taught element. \n\nAlong with the PhD, it represents the highest academic qualification in engineering. Successful completion of a EngD or PhD in engineering is required to gain employment as a full-time, tenure-track university professor or postdoctoral researcher in the field. As with other earned research doctorates, individuals with the degree are awarded the academic title doctor, which is often represented via the English honorific \"Dr.\" \n\nEngD candidates submit a significant project, typically referred to as a thesis, dissertation, or praxis, consisting of a body of original academic research that is in principle worthy of publication in a peer-reviewed journal. Candidates must defend this work before a panel of expert examiners called a thesis, dissertation, or doctoral committee. \n\nCountries following the German/US model of education usually have similar requirements for awarding Ph.D.(Eng.) and doctor of engineering degrees. The common degree abbreviations in the USA are EngD, D.Eng., D.Eng.Sc./Eng.Sc.D, whereas in the German-speaking world it is more commonly known as Dr.-Ing. The common degree abbreviation in the Netherlands is Professional Doctorate in Engineering (PDEng).\n\nTo be admitted as a doctoral student, one must hold a Master's degree in engineering or related science subject and pass a comprehensive entrance exam. The student must complete necessary course work, be taught examinable courses, perform independent research under supervision of a qualified doctoral advisor, and pass the thesis defense. The degree requires a high level of expertise in the theoretical aspects of relevant scientific principles and experience with details of the implementation of theory on realistic problems. The D.Eng. takes three to six years (full-time) to complete and has compulsory taught components and coursework/projects and is granted in recognition of high achievement in scholarship and an ability to apply engineering fundamentals to the solution of complex technical problems.\n\nA Doctor of Engineering degree awarded by universities in China, Japan, and South Korea is equivalent to a PhD degree. To be admitted as a doctoral student, one must hold a master's degree in the same or related subject and pass a comprehensive entrance exam. The student must complete necessary course work, perform independent research under supervision of a qualified Doctoral Advisor, and pass the thesis defense. It usually takes more than three years for a student with an M.S. Degree to complete his/her doctoral study. However, there are few areas of study(such as Materials Science, Polymer Technology, and Biomedical Engineering) where both Doctor of Science and Doctor of Engineering can be awarded depending upon the graduate school which houses the department.\n\nIn Germany the doctoral degree in engineering is called \"Doktoringenieur\" (Doktor der Ingenieurwissenschaften, Dr.-Ing.) and is usually earned after four to six years of research and completing a dissertation. A researcher pursuing a doctorate needs to hold a master's degree or the \"Diplom-Ingenieur\" degree (Dipl.-Ing.).\n\nIn France the degree of \"Doctor-Engineer\" (\"docteur-ingénieur\") was a former applied science research degree. It was discontinued after 1984 and engineers wishing to go further as researchers now seek a PhD.\n\nThe Engineering Doctorate scheme is a British postgraduate education programme promoted by the UK's Engineering and Physical Sciences Research Council (EPSRC). The programme is undertaken over four years. Students conduct PhD-equivalent research and undertake taught business and technical courses whilst working closely with an industrial sponsor. Successful candidates are awarded the degree of Doctor of Engineering (EngD) and are addressed as \"doctor\".\n\nThe first programmes began in 1992. In 2009, Engineering Doctorate schemes were offered by 45 UK universities, both singly or in partnership with other universities as industrial doctorate centres. Students on the scheme are encouraged to describe themselves as 'research engineers' rather than 'research students' and as of 2009 the minimum funding level was £1,500 higher than the minimum funding level for PhD students. Advocates of the scheme like to draw attention to the fact that EngD students share some courses with MBA students.\n\nIn the UK a similar formation to doctorate is the NVQ 8 or QCF 8. However, a doctoral degree typically incorporates a research project which must offer an original contribution to knowledge within an academic subject area; an element which NVQs lack.\n\nIn the United Kingdom, the D.Eng. degree was traditionally awarded as a higher doctorate on the basis of a significant contribution to some field of engineering over the course of a career. However, since 1992 some British universities have introduced the Engineering Doctorate, abbreviated as \"EngD\", which is instead a research doctorate and regarded in the UK as equivalent to a PhD.\n\nThe Engineering Doctorate (EngD) scheme was established by the EPSRC in 1992 following the recommendations of the 1990 Engineering Doctorate Report, produced by a working group chaired by Professor John Parnaby. The scheme was launched with five centres - at Warwick, UMIST and Manchester universities and a Welsh consortium led by University College Swansea. After a 1997 review, a further tranche of five centres was established, and further centres were added in 2001 and 2006 following calls by EPSRC in particular areas of identified national need.\n\nIn a 2006 stakeholder survey of the scheme conducted on behalf of EPSRC it was found that the quality of output of research engineers was perceived to match or exceed that of a PhD. However, the majority of respondents disagreed with claims that EngDs were recruited to higher-paid posts than PhDs or that EngDs were more desirable to employers than PhDs. Observations were made that the EngD was not widely known, and that universities may offer EngD degrees that were not necessarily of the format promoted by the EPSRC.\n\nA March 2007 \"Review of the EPSRC Engineering Doctorate Centres\" noted that since 1992, some 1230 research engineers had been enrolled, sponsored by over 510 different companies (28 had sponsored at least six REs), at 22 centres based at 14 universities (some jointly run by several collaborating universities). The panel remained convinced of the value and performance of the EngD scheme, and made six key recommendations including clearer brand definition, academic study of the longer term impacts of the scheme, promotion of the scheme to potential new sponsors, business sectors and REs, work with the Engineering Council UK to develop a career path for REs to Chartered Engineer status, creation of a virtual \"EngD Academy\", and increased resources for the scheme.\n\nWork on establishing an Association of Engineering Doctorates began in 2010.\n\nThe Doctor of Engineering and the PhD in Engineering are equivalent degrees. Both doctorates are research doctorates representing the highest academic qualification in engineering. As such, both EngD and PhD programs require students to develop original research leading to a dissertation defense. Furthermore, both doctorates enable holders to become faculty members at academic institutions. The EngD and PhD in Engineering are terminal degrees, allowing the recipient to obtain a tenure-track position.\n\nIn other cases, the distinction is one of orientation and intended outcomes. The Doctor of Engineering degree is designed for practitioners who wish to apply the knowledge they gain in a business or technical environment. Unlike a Doctor of Philosophy (PhD) degree program, wherein research leads to foundational work that is published in industry journals, the EngD demands that research be applied to solving a real-world problem using the latest engineering concepts and tools. The program culminates in the production of a thesis, dissertation, or praxis, for use by practicing engineers to address a common concern or challenge. Research toward the EngD is “applied” rather than basic. \n\nThe PhD is highly focused on developing theoretical knowledge, while the EngD emphasizes applied research. Upon completion, graduates of PhD programs generally migrate to full-time faculty positions in academia, while those of EngD programs re-emerge in industry as applied researchers or Executives. If working full-time in industry, graduates of EngD and PhD programs often become adjunct professors in top undergraduate and graduate degree programs.\n\nThe following ABET accredited universities offer Doctor of Engineering degrees:\n\nThe following EPSRC-funded centres have offered EngDs:\n\n\n"}
{"id": "34436412", "url": "https://en.wikipedia.org/wiki?curid=34436412", "title": "Don Poldermans", "text": "Don Poldermans\n\nDon Poldermans is a Dutch former cardiovascular medicine researcher who was fired for scientific misconduct and ethics concerns over informed consent. He was employed by Erasmus Medical Center in Rotterdam, Netherlands, where he was the head of the perioperative cardiac care unit. In addition, he was a member of the European Society of Cardiology Committee for Practice Guidelines and he acted as the Chairperson of the Task Force for the European Society of Cardiology.\n\nDon Poldermans was conducting research for Erasmus when accusations regarding the integrity of Poldermans’ work were brought forward. In order to investigate this case, Erasmus appointed a Committee for the Investigation of Scientific Integrity. The Committee found Poldermans to have committed misconduct on several counts. The primary studies that have been brought into question are four of the Dutch Echocardiographic Cardiac Risk Evaluation Applying Stress Echocardiography (DECREASE) studies, specifically DECREASE VI, IV, III, and II. DECREASE I was too far in the past to be investigated.\n\nFirst, some of the randomized controlled trials did not obtain written informed consent from the participants before randomly allocating them to different strategies. These actions were a serious breach of medical research conduct.\n\nSecond, the committee determined that the data were not collected according to the protocol described beforehand and reported in the publications. For example, events such as myocardial infarction were not diagnosed by a panel of independent researchers, but by a single person who made no documentation for the reason for the categorisations, which were later found to be contradictory to the patients' own medical records.\n\nThird, the committee determined that, in several cases, these trials had fabricated data.\n\nLast, the committee found that untrustworthy data had been knowingly submitted for publication, another breach of proper scientific conduct.\n\nA project running at the time of the investigation, DECREASE VI, was abandoned, because patients had not given consent to take part. The manner in which previous data was collected, reported, and occasionally fabricated indicates academic misconduct, bringing the legitimacy of the current data into question.\n\nThe enquiry decided that DECREASE II need not be retracted because it believed that Dobutamine Stress Echo, the subject of the study, was no longer used in hospitals. This belief appears to be incorrect. DECREASE VI led to two publications, which were also not retracted. No other researchers were disciplined in this inquiry. The Committee informed all parties involved in the funding of the project.\n\nPoldermans was dismissed from his position at Erasmus Medical Center. Two professors will supervise the research done by Poldermans’ students and will establish whether or not the projects in question can be completed successfully. If this is not possible, then the professors will find new research projects for the students affected. Don Poldermans acknowledged the committee’s decision but he claimed that his misconduct was unintended.\n\nWithout Poldermans' trials, the remaining credible trials suggest that the recommendation for initiation of a perioperative course of beta blocker seems to increase mortality by 27%.\n\nErasmus university issued a third investigation report in 2014. This covered DECREASE I which had not been covered in the previous investigations. It stated that the individual patients enrolled in DECREASE I could not be identified for cross-checking against the medical records. It therefore could not conclude on the accuracy or otherwise of the DECREASE I publications. An independent analysis by British researchers listed inconsistencies in the published DECREASE I reports that put their accuracy into doubt.\n"}
{"id": "57319424", "url": "https://en.wikipedia.org/wiki?curid=57319424", "title": "Down to fuck", "text": "Down to fuck\n\nThe term \"down to fuck\" (DTF) is slang for a person willing to have sexual intercourse (or fuck) with another person, often in the form of casual sex. The term has been in use since 2007 having been mentioned in the film \"Superbad\" and has been widely used in the media, movies, and music. It was a catchphrase used by Mike \"The Situation\" Sorrentino on the \"Jersey Shore\".\n"}
{"id": "26932691", "url": "https://en.wikipedia.org/wiki?curid=26932691", "title": "Durma Melhor", "text": "Durma Melhor\n\nDurma Melhor (\"Sleep Better\", translated into English) is a Brazilian blog featuring relevant information on issues related to sleep. Published in Portuguese, the blog content includes articles on nutrition, beauty, exercise, relaxation, environment, technology, scientific studies and frequently asked questions regarding sleep.\n\nIt also features a selection of videos, podcasts, books, websites, soothing songs available for free download, as well as online tests to assess one's sleep quality and a comprehensive list of sleep labs in Brazil.\n\nDurma Melhor contributors include Dr. Flávio Alóe, neurologist and neurophysiologist of the Brazilian Society of Clinical Neurophysiology and a certified physician in Sleep Medicine by the Brazilian Society of Sleep, who's been widely interviewed by Brazilian magazines, newspapers and TV shows, such as Programa do Jô.\n\nIn April 2010, Durma Melhor launched a channel that allows the general public to send questions and have them answered by experts in sleep medicine.\n\nThe blog was launched in January 2010 by Marcos Miguel, a Brazilian journalist who's graduated at the Universidade Federal de Juiz de Fora, in Minas Gerais state.\n\n"}
{"id": "12891588", "url": "https://en.wikipedia.org/wiki?curid=12891588", "title": "Experimental design diagram", "text": "Experimental design diagram\n\nExperimental Design Diagram (EDD) is a diagram used in science classrooms to design an experiment. This diagram helps to identify the essential components of an experiment. It includes a title, the research hypothesis and null hypothesis, the independent variable, the levels of the independent variable, the number of trials, the dependent variable, the operational definition of the dependent variable and the constants.\n\n[science way of controlling the data]\n"}
{"id": "51992248", "url": "https://en.wikipedia.org/wiki?curid=51992248", "title": "FESOM", "text": "FESOM\n\nFESOM (Finite-Element/volumE Sea ice-Ocean Model) is a\nmulti-resolution ocean general circulation model that solves the equations \nof motion describing the ocean and sea ice using finite-element and \nfinite-volume methods on unstructured computational grids. The model \nis developed and supported by researchers at the Alfred Wegener\nInstitute, Helmholtz Centre for Polar and Marine Research (AWI), in Bremerhaven, \nGermany.\n\nFESOM implements the idea of using meshes with variable resolution\nto simulate the circulation of the global ocean with regional focus. Because\nof the broad range of scales characterizing the ocean circulation, downscaling\nis commonly needed to describe processes on regional scales. FESOM allows global\nmulti-resolution cross-scale simulations without traditional nesting.\n\nThe dynamical core of the new version (FESOM2) switches from the finite-element \nmethod used in the original version of FESOM to the finite-volume method for the sake of better computational efficiency. Both versions include the Finite-Element Sea Ice Model (FESIM). FESOM is also used as the ocean component of the AWI-CM, the coupled atmosphere-ocean climate model developed at AWI.\n\nThe prototype version of FESOM appeared in 2004 due to work of Sergey Danilov,\nGennady Kivman and Jens Schröter. Ralph Timmermann extended it to a full global ocean - sea ice configuration in 2009. Qiang Wang rewrote its numerical algorithm and parameterizations from 2008 through 2014, which led to essentially improved numerical and physical performance. The last release of FESOM with the finite-element dynamical core is FESOM1.4 (Wang et al., 2014). \nThe release of AWI-CM using FESOM is by Sidorenko et al. in 2015.\n\n"}
{"id": "4905404", "url": "https://en.wikipedia.org/wiki?curid=4905404", "title": "Faculties of Medicines in Hong Kong", "text": "Faculties of Medicines in Hong Kong\n\nBefore the handover of Hong Kong to the People's Republic of China in 1997, medical education in this former British colony traditionally and exclusively followed the path of western medicine. Faculties of Medicine were modelled after those in the United Kingdom, and only doctors trained in western medicine were considered \"formal\" and \"reliable.\" Chinese medicine practitioners had no formal status at that time. However, after the return of the territory to China, the practice of traditional Chinese medicine was further regulated and schools of Chinese Medicine were set up within some of the government funded tertiary institutions in Hong Kong. The first school of its kind, the School of Chinese Medicine at Hong Kong Baptist University, was established in 1998. Currently, there are two faculties with academic programmes in western medicine and three schools of Chinese Medicine in the city.\n\nThere are two schools that teach Western Medicine in Hong Kong, namely, the Li Ka Shing Faculty of Medicine at the University of Hong Kong and the Faculty of Medicine at the Chinese University of Hong Kong. The medium of instruction is English.\n\nThere are three public, government funded schools of Traditional Chinese Medicine / Pharmacy in Hong Kong, including the School of Chinese Medicine of Hong Kong Baptist University, the School of Chinese Medicine at the University of Hong Kong, and the School of Chinese Medicine at the Chinese University of Hong Kong.\n\n\n\n"}
{"id": "12212921", "url": "https://en.wikipedia.org/wiki?curid=12212921", "title": "Galaxy Zoo", "text": "Galaxy Zoo\n\nGalaxy Zoo is a crowdsourced astronomy project which invites people to assist in the morphological classification of large numbers of galaxies. It is an example of citizen science as it enlists the help of members of the public to help in scientific research.\n\nThere have been 15 versions as of July 2017, many of which are outlined in this article. Galaxy Zoo is part of the Zooniverse, a group of citizen science projects. An outcome of the project is to better determine the different aspects of objects and to separate them into classifications.\n\nA key factor leading to the creation of the project was the problem of what has been referred to as data deluge, where research produces vast sets of information to the extent that research teams are not able to analyse and process much of it. Kevin Schawinski, previously an astrophysicist at Oxford University and co-founder of Galaxy Zoo, described the problem that led to Galaxy Zoo's creation when he was set the task of classifying the morphology of more than 900,000 galaxies by eye that had been imaged by the Sloan Digital Sky Survey at the Apache Point Observatory in New Mexico, USA. \"I classified 50,000 galaxies myself in a week, it was mind-numbing.\" Chris Lintott, also a co-founder of the project, stated: \"In many parts of science, we're not constrained by what data we can get, we're constrained by what we can do with the data we have. Citizen science is a very powerful way of solving that problem.\"\n\nThe Galaxy Zoo concept was inspired by others such as Stardust@home, where the public was asked by NASA to search images obtained from a mission to a comet for interstellar dust impacts. Unlike earlier internet-based citizen science projects such as SETI@home, which used spare computer processing power to analyse data (also known as distributed or volunteer computing), Stardust@home involved the active participation of human volunteers to complete the research task. In August 2014, the Stardust team reported the discovery of first potential interstellar space particles after citizen scientists had looked through more than a million images.\n\nWhen Galaxy Zoo first started, the science team hoped that 20–30,000 people would take part in classifying the 900,000 galaxies that made up the sample. It had been estimated that a perfect graduate student working 24 hours a day 7 days a week would take 3–5 years to classify all the galaxies in the sample once. However, in the first Galaxy Zoo, more than 40 million classifications were made in approximately 175 days by more than 100,000 volunteers, providing an average of 38 classifications per galaxy.\n\nChris Lintott commented that: \"One advantage is that you get to see parts of space that have never been seen before. These images were taken by a robotic telescope and processed automatically, so the odds are that when you log on, that first galaxy you see will be one that no human has seen before.\" This was confirmed by Kevin Schawinski: \"Most of these galaxies have been photographed by a robotic telescope, and then processed by computer. So this is the first time they will have been seen by human eyes.\".\n\nGalaxy Zoo recruited volunteers to help with the largest galaxy census ever carried out. Opening the project to the general public saved the professional astronomers the task of studying all the galaxies themselves, resulting in classification of a large number of galaxies undertaken in a shorter time than what smaller research teams would be able to do, classifying 900,000 galaxies in months rather than years if done by smaller research teams. Computer programs had been unable to reliably classify galaxies: several groups had attempted to develop image-analysis programs. Kevin Schawinski stated: \"The human brain is actually much better than a computer at these pattern recognition tasks.\" However, volunteers astonished the project’s organizers by classifying the entire catalog years ahead of schedule. An online forum was later set up two weeks after the initial start, partially due to a large volume of emails being sent around, to the point that it was troublesome for those receiving them to process and respond to them. This led volunteers to point out anomalies that on closer inspection have turned out to be new astronomical objects such as 'Hanny's Voorwerp' and 'the Green Pea galaxies'. “I’m incredibly impressed by what they’ve managed to achieve,” says University of Oxford astronomer Roger Davies, former president of the Royal Astronomical Society.“They’ve made it possible to do things with a huge survey.”\n\nThe Galaxy Zoo forum became a hotbed for the discussion of the SDSS images and more general science questions. Its 'global moderator', UK teacher Alice Sheppard, said of it: \"I don't quite know what it is, but Galaxy Zoo does something to people. The contributions, both creative and academic, that people have made to the forum are as stunning as the sight of any spiral, and never fail to move me.\" Author Michael Nielsen wrote in his book Reinventing Discovery: \"But Galaxy Zoo can go beyond computers, because it can also apply human intelligence in the analysis, the kind of intelligence that recognizes that Hanny's Voorwerp or a Pea galaxy is out of the ordinary, and deserves further investigation. Galaxy Zoo is thus a hybrid, able to do deep analyses of large data sets that are impossible in any other way.\" A community feeling was also created. Roger Davies stated: \"The community of Galaxy Zoo gives them the opportunity to participate that they’re looking for.” This community became known as the 'Zooites'. Aida Berges, a homemaker living in Puerto Rico who has classified hundreds of thousands of galaxies, stated: \"Every galaxy has a story to tell. They are beautiful, mysterious, and show how amazing our universe is. It was love at first sight when I started in Galaxy Zoo ... It is a magical place, and it feels like coming home at last.\" The Galaxy Zoo Forum became a read-only archive in July 2014. After seven years online and over 650,000 posts, it continues to generate science.\n\nAs of July 2017, 60 scientific papers have been published as a direct result of Galaxy Zoo and hundreds of thousands of volunteers. In previous studies though, it was found that data produced by volunteers was more likely to contain bias or mistakes. However Chris Lintott says that crowdsourced results are reliable, as proven by the fact that they are being used and published in peer-reviewed science papers. Indeed, other scientists have questioned crowdsourcing and crowdsourced studies. Steven Bamford, a Galaxy Zoo research scientist, stated: \"As a professional researcher you take pride in the work that you do. And the idea that anybody off the street could come and do something better sounds threatening but also implausible.\" David Anderson, the founder of BOINC, stated: [For many sceptical scientists] \"There's this idea that they're giving up control somehow, and that their importance would be diminished\". The continuing goodwill of citizen scientists is also questioned. Chris Lintott stated: \"Rather than letting anyone pitch for volunteers, we'd like to be a place where people can come and expect a certain level of commitment\".\n\nTenth anniversary. A conference was held between July 10-12th 2017 at St. Catherine's College, Oxford, to recognise the tenth anniversary of the start of Galaxy Zoo in July 2007. Co-founder Chris Lintott stated: \"What started as a small project has been completely transformed by the enthusiasm and efforts of the volunteers... It has had a real impact on our understanding of galaxy evolution.\" 125 million galaxy classifications resulting in 60 peer reviewed academic papers from at least 15 different projects have been made since July 2007. Discoveries include:Hanny's Voorwerp, Green pea galaxies and more recently objects known as 'Yellow Balls'. On the conference Twitter feed, #GZ10, it states that 10 of the 60 papers have over 100 citations [within the Astrophysics Data System] in 10 years. Karen Masters, an astrophysicist at Portsmouth University and project scientist for GZ stated: \"We're genuinely asking for help with something we cannot do ourselves and the results have made a big contribution to the field.\" As a result of GZ's success, the citizen science web portal Zooniverse was started, which has since hosted a 100 projects.\n\nThe original Galaxy Zoo consisted of a data set made up of ≈900,000 galaxies imaged by the Sloan Digital Sky Survey. With so many galaxies, it had been assumed that it would take years for visitors to the site to work through them all, but within 24 hours of launch, the website was receiving almost 70,000 classifications an hour. In the end, more than 50 million classifications were received by the project during its first year, contributed by more than 150,000 people. This was started in July 2007 and retired in 2009.\n\nThis consisted of some 250,000 of the brightest galaxies from the Sloan Digital Sky Survey. Galaxy Zoo 2 allowed for a much more detailed classification, by shape and by the intensity or dimness of the galactic core, and with a special section for oddities like mergers or ring galaxies. The sample also contained fewer optical oddities. The project closed with some 60 million classifications.\n\nThis studied the role of interacting galaxies. Interacting galaxies are galaxies that exhibit a gravitational influence on one another. This influence is exhibited over the course of millions or even billions of years as two or more galaxies pass nearby one another. The near passage of two massive structures can cause the galaxies to be distorted and possibly merge. The Galaxy Zoo Mergers aimed to provide a set of tools that allowed users to randomly sample various sets of simulation parameters in rapid succession by showing 8 simulation outputs at a time. This started in November 2009 and was retired in June 2012.\n\nGalaxy Zoo used images partner from the Palomar Transient Factory to find Supernovae. The task in this Galaxy Zoo project was to help catch exploding stars – supernovae. Data for the site was provided by an automatic survey in California at the Palomar Observatory. Astronomers followed up on the best candidates at telescopes around the world. This started in August 2009 and was retired in August 2012.\n\nThe site’s third incarnation, Galaxy Zoo Hubble drew from surveys conducted by the Hubble Space Telescope to view earlier epochs of galaxy formation. In these surveys, which involve many days of dedicated observing time, we can see light from galaxies which has taken billions of years to reach us. The idea behind Galaxy Zoo Hubble was to be able to compare galaxies then to galaxies now, giving us a clear understanding of what factors influence their growth, whether through mergers, active black holes or simply star formation. This started in April 2010 and was retired in September 2012.\n\nIn October 2016, a study titled: \"Galaxy Zoo: Morphological Classifications for 120,000 Galaxies in HST Legacy Imaging\" was accepted for publication by the journal Monthly Notices of the Royal Astronomical Society. The abstract begins: \"We present the data release paper for the Galaxy Zoo: Hubble project. This is the third phase in a large effort to measure reliable, detailed morphologies of galaxies by using crowdsourced visual classifications of colour composite images. Images in Galaxy Zoo Hubble were selected from various publicly-released Hubble Space Telescope Legacy programs conducted with the Advanced Camera for Surveys, with filters that probe the rest- frame optical emission from galaxies out to z ≈1.\"\n\nThe present Galaxy Zoo (4) combines new imaging from the Sloan Digital Sky Survey with the most distant images yet from the Hubble Space Telescope CANDELS survey. The CANDELS survey makes use of the new Wide Field Camera 3 to take ultra-deep images of the universe. The project also includes images taken with the United Kingdom Infrared Telescope in Hawaii, for the recently completed UKIDSS project. UKIDSS is the largest, deepest survey of the sky at infrared wavelengths. Kevin Schawinski explained that: \"The two sources of data work together perfectly: the new images from Sloan give us our most detailed view of the local universe, while the CANDELS survey from the Hubble telescope allows us to look deeper into the universe’s past than ever before.\"\n\nIn October 2016, a paper was accepted for publishing in MNRAS titled: \"Galaxy Zoo: Quantitative Visual Morphological Classifications for 48,000 galaxies from CANDELS\". Quoting: \"We present quantified visual morphologies of approximately 48,000 galaxies observed in three Hubble Space Telescope legacy fields by the Cosmic And Near-infrared Deep Extragalactic Legacy Survey (CANDELS) and classified by participants in the Galaxy Zoo project. 90% of galaxies have z < 3 and are observed in rest-frame optical wavelengths by CANDELS. Each galaxy received an average of 40 independent classifications, which we combine into detailed morphological information on galaxy features such as clumpiness, bar instabilities, spiral structure, and merger and tidal signatures.\"\n\nOn December 17, 2013, Galaxy Zoo opened a project called Radio Galaxy Zoo. It uses observations from the Australia Telescope Large Area Survey in Radio, and compares them to the Spitzer Space Telescope's infrared data. There are about 6000 images to look through. The CSIRO press release states that Radio Galaxy Zoo is a new citizen science project that lets anyone become a cosmic explorer. It continues that by matching galaxy images with radio images from CSIRO’s Australia Telescope, a participant can work out if a galaxy has a supermassive black hole.\n\nAnother project that uses data from volunteer classifications is Galaxy Zoo Quench, which studies the interactions between galaxies and the effect it has on starbursts (among others). This has yet to be completed.\n\nAs of July 2017, the full list of Galaxy Zoo projects (15) is: Galaxy Zoo 1, Galaxy Zoo 2, Galaxy Zoo Mergers, Galaxy Zoo Supernovae, Galaxy Zoo Hubble, Galaxy Zoo CANDELS, Radio Galaxy Zoo, Galaxy Zoo Quench, Galaxy Zoo DECALS 1, Galaxy Zoo DECALS2 + SDSS, Illustris, UKIDSS, Galaxy Zoo Bar Lengths and two more.\n\nOne of the original aims for Galaxy Zoo was to explore which way galaxies rotated. Cosmologist Kate Land stated: \"Some people have argued that galaxies are rotating all in agreement with each other, not randomly as we'd expect. We want people to classify the galaxies according to which way they're rotating and I'll be able to go and see if there's anything bizarre going on. If there are any patterns that we're not expecting, it could really turn up some surprises.\" In Galaxy Zoo 1, volunteers were asked to judge from the SDSS images whether the galaxies were elliptical or spiral and, if spiral, whether they were rotating in a clockwise or anti-clockwise direction. The rotation, also called the Chirality, of galaxies has been examined in several Galaxy Zoo related papers.\n\nAmong the results a psychological bias was demonstrated. Galaxy Zoo scientists had wanted to check whether spiral galaxies were evenly distributed or whether there was some intrinsic property of the Universe that caused galaxies to rotate one way or the other. When the Science team came to analyse the results they found an excess of anti-clockwise spinning spiral galaxies. But when the team checked this bias by asking volunteers to classify the same image which had then been reversed, there was still an excess of anti-clockwise classifications. This demonstrated that the human brain has real difficultly discerning between something rotating clockwise or anti-clockwise. Having measured this effect the team could adjust for it, and went on to establish that spirals which were near each other tended to rotate in the same direction.\n\nMainstream astronomical theory before Galaxy Zoo held that elliptical (or 'early type') galaxies were red in color and spiral (or 'late type') galaxies were blue in color: several papers published as a result of Galaxy Zoo have proved otherwise. A population of blue ellipticals was found. These are galaxies which have changed their shape from spiral to oval, but still have young stars in them. Indeed, Galaxy Zoo came about through Schawinski's searching for blue elliptical galaxies, as near the end of 2006, he had spent most of his waking hours trying to find these rare galaxies. Blueness in galaxies means that new stars are forming. However ellipticals are almost always red, indicating that they are full of old and dead stars. Thus, blue ellipticals are paradoxical, but give clues to star-formation in different types of galaxies.\n\nAlso, a population of red spirals mean was found. These have a different evolutionary path from normal spiral galaxies, showing red spiral galaxies can stop making new stars without changing their shape. Using Galaxy Zoo data for their sample, Tojeiro et al. 2013 find (pg.5): 13959 red ellipticals, 381 blue ellipticals, 5139 blue late-type spirals, 294 red late-type spirals, 1144 blue early-type spirals, and 1265 red early-type spirals. Chris Lintott stated: \"These red spiral galaxies had been lurking in the data and no-one had spotted them. They were staring us in the face. Now we know that a third of spirals around the edges of some clusters of galaxies are red.\" He also stated: \"These results are possible thanks to a major scientific contribution from our many volunteer armchair astronomers. No group of professionals could have classified this many galaxies alone.\" A team using the Hubble Space telescope has independently verified the existence of red spirals. Meghan Gray stated: \"Our two projects have approached the problem from very different directions. It is gratifying to see that we each provide independent pieces of the puzzle pointing to the same conclusion.\"\n\nIt is thought that Red Spirals are galaxies in the process of transition from young to old. They are more massive than blue spirals and are found on the outskirts of large clusters of galaxies. Chris Lintott stated: \"We think what we’re seeing is galaxies that have been gently strangled, so to speak, where somehow the gas supply for star formation has been cut off, but that they’ve been strangled so gently that the arms are still there.\" The cause might be the Red Spiral's gentle interaction with a galaxy cluster. He further explained: \"The kind of thing we’re imagining [is that] as the galaxy moves into a denser environment, there’s lot of gas in clusters as well as galaxies, and it’s possible the gas from the galaxy just gets stripped off by the denser medium it’s plowing into.\"\n\nThe properties of Galactic Dust have been examined in several Galaxy Zoo papers. The interstellar medium of spiral galaxies is filled by gas and small solid particles called dust grains. Despite constituting only a minor fraction of the galactic mass (between 0.1% and 0.01% for the Milky Way), dust grains have a major role in shaping the appearance of a galaxy. Because of their dimension (typically smaller than a few tenths of a micron), they are very effective in absorbing and scattering the radiation emitted by stars in the ultraviolet, optical and near-infrared. Although the interstellar regions are more devoid of matter than any vacuum artificially created on earth, there is matter in space. These regions have very low densities and consist mainly of gas (99%) and dust. In total, approximately 15% of the visible matter in the Milky Way is composed of interstellar gas and dust.\n\nThe study of dust in galaxies is interesting for many reasons. For example, the dimming effects of dust need to be corrected for to estimate the total mass of a galaxy from measurements of its light. Standard candles used to measure the expansion history of the Universe also need to be corrected for dust extinction.\n\nA catalogue of 1,990 overlapping galaxies was published in 2013, which had been collected by volunteers on the Galaxy Zoo forum using SDSS images. The abstract states: 'Analysis of galaxies with overlapping images offers a direct way to probe the distribution of dust extinction and its effects on the background light.' This catalogue was also used in a study of ultraviolet attenuation laws.\n\nSome spiral galaxies have central bar-shaped structures composed of stars. These galaxies are called 'barred spirals' and have been investigated by Galaxy Zoo in several studies. It is unclear why some spiral galaxies have bars and some do not. Galaxy Zoo research has shown that red spirals are about twice as likely to host bars as blue spirals. These colours are significant. Blue galaxies get their hue from the hot young stars they contain, implying that they are forming stars in large numbers. In red galaxies, this star formation has stopped, leaving behind the cooler, long-lived stars that give them their red colour. Karen Masters, a scientist involved in the studies, stated: \"For some time data have hinted that spirals with more old stars are more likely to have bars, but with such a large number of bar classifications we're much more confident about our results. It's not yet clear whether the bars are some side effect of an external process that turns spiral galaxies red, or if they alone can cause this transformation.\"\n\nSpiral galaxies usually have 'bulges' at their centers. These bulges are huge, tightly packed groups of stars. However, using Galaxy Zoo volunteer classifications, it has been found that some spiral galaxies do not have bulges. Many galactic bulges are thought to host a supermassive black hole at their centers: however pure disk galaxies with no bulges but with growing central black holes were found. That pure disk galaxies and their central black holes may be consistent with a relation derived from elliptical and bulge-dominated galaxies with very different formation histories implies the details of stellar galaxy evolution and dynamics may not be fundamental to the co-evolution of galaxies and black holes. It seems that these bulgeless galaxies have formed in environments isolated from other galaxies. It is hypothesised that the black hole mass may be more tightly tied to the overall gravitational potential of a galaxy and therefore its dark matter halo, rather than to the dynamical bulge component.\n\nIn September 2014, a paper titled: \"Galaxy Zoo: CANDELS Barred Disks and Bar Fractions\" was accepted for publication by the MNRAS. This was the first set of results from the Hubble Space Telescope CANDELS survey that was part of Galaxy Zoo 4. The study reports \"the discovery of strong barred structures in massive disk galaxies at z ≈1.5 in deep rest-frame optical images from CANDELS\". From within a sample of 876 disk galaxies identified by visual classification in Galaxy Zoo 4, 123 barred galaxies are examined. It is found that the bar fraction across the redshift range 0.5 < z < 2 does not significantly evolve.\n\nGalaxy Zoo Mergers was a Galaxy Zoo project started in November 2009 and retired in June 2012. There have also been a number of studies on galaxy mergers, among which was a survey of ≈3000, which presented \"the largest, most homogeneous catalogue of merging galaxies in the nearby universe\". This catalogue was spread over two papers and was a result of volunteers selecting likely candidates from Galaxy Zoo 1 and posting them on the Galaxy Zoo forum. Other papers that have used Galaxy Zoo data resulted in observations that include those taken by the Chandra X-ray Observatory.\n\nA 2018 study published in the online journal \"Citizen Science: Theory and Practice\" compares two citizen science projects, Foldit and GZ, to explore the difference between projects that do or don't use gaming to obtain data. Over a one-year period, the authors conducted a virtual ethnographic study of the public forums of both projects by using web scraping techniques. The abstract ends: \"We conclude that ideals of science embraced by citizen scientists appear to influence the reasons why they participate, either emphasizing equality, like in Galazy Zoo, or meritocracy, like in Foldit.\"\n\nExamples of crowdsourced astronomy projects include:\n\n\nZooniverse projects:\n"}
{"id": "48624055", "url": "https://en.wikipedia.org/wiki?curid=48624055", "title": "George and the Unbreakable Code", "text": "George and the Unbreakable Code\n\nGeorge and the Unbreakable Code is a 2014 children's book written by Stephen and Lucy Hawking. The book is the fourth book in the George series, following \"George's Secret Key to the Universe\", \"George's Cosmic Treasure Hunt\", and \"George and the Big Bang\", and preceding \"George and the Blue Moon\".\n\n"}
{"id": "51365411", "url": "https://en.wikipedia.org/wiki?curid=51365411", "title": "Germania Sacra", "text": "Germania Sacra\n\nGermania Sacra (Latin for \"Sacred/Holy Germania/Germany\") is a long-term research project into German church history from its beginnings through the Reformation in the 16th century to the period of secularisation in the early 19th century.\n\nThe first attempt to collect and publish the history of the German dioceses in reference books was made by Martin Gerbert, the prince-abbot of the monastery St. Blasien in the late 18 century, but his works were never completed.\n\nFollowing into Gerberts footsteps, Paul Fridolin Kehr established a new \"Germania Sacra\" under the patronage of the \"Kaiser-Wilhelm-Society\" at the \"Kaiser-Wilhelm-Institute of German History\" in Berlin in 1917. He tried to connect the nationwide research projects and combine them under \"Germania Sacra\" to create an archival collection of monasteries, convents, cathedral chapters and religious dignitaries. After multiple financial problems, the first book was published on 11 of June in 1929. It is part of the \"“Alte Folge”\", which comprises seven volumes published between 1929 and 1972.\n\nAfter the 2 World War and the death of Kehr, the \"Kaiser-Wilhelm-Society\" wasn’t interested in supporting the institute any longer and the newly founded \"Max-Planck-Society\" took over the patronage of the institute and its project. Hermann Heimpel, the first head of the \"Max-Planck-Institute of History\" in Göttingen, continued work on the \"Germania Sacra\" in 1956, cooperating with an academic director, who coordinated the work of external researchers. While the last books of the \"Alte Folge\" were published between 1966 and 1972, the researchers under Heimpel began writing and publishing the \"Neue Folge\", which were released between 1962 and 2007 in 50 volumes.\nIn 2007, the \"Max-Planck-Institute of History\" in Göttingen was rededicated and the work on the \"Germania Sacra\" was continued by the\"Academy of Sciences and Humanities\" under the supervision of Prof. Dr. Hedwig Röckelein in Göttingen in 2008. The volumes of the \"Germania Sacra\" published after 2008 belong to the \"Dritte Folge\" and currently comprise 14 volumes (as of: June 2018). \n\nIn 2015 the first supplementary volume was published, which contains edited preparation work to the main works of \"Germania Sacra\". \n\nThe main objective of \"Germania Sacra\" is a statistical description of the ecclesiastical institutions which existed between the \"Holy Roman Empire\" and the period of secularisation in the early 19 century. In achieving that objective, the entirety of sources and secondary literature concerning Medieval and Early Modern ecclesiastical institutions is to be portrayed. These institutions include the dioceses (with focus on the bishops), the cathedral chapters and the monasteries and convents up to and including their end during the reformation or the secularisation. At the beginning, the collection and the editing of the historical sources was mainly conducted by archivists at the request of Paul Kehr. In the present, the work on \"Germania Sacra\" is divided between historical research and the editorial department. The research is conducted by archivists, historians, members of ecclesiastical orders, theologians etc. \n\n\"Germania Sacras\" publications are divided into the \"Alte Folge\", the \"Neue Folge,\" the \"Dritte Folge,\" the \"Studien zur Germania Sacra\" and the \"Supplementbände zur Germania Sacra\".\n\nIn addition to the volumes of \"Germania Sacra,\" the project provides two databases. A comprehensive inventory of medieval and early modern clerics and the \"Database of monasteries, convents and collegiate churches of the Old Empire \"\n\n\n"}
{"id": "6012335", "url": "https://en.wikipedia.org/wiki?curid=6012335", "title": "History of biotechnology", "text": "History of biotechnology\n\nBiotechnology is the application of scientific and engineering principles to the processing of materials by biological agents to provide goods and services. From its inception, biotechnology has maintained a close relationship with society. Although now most often associated with the development of drugs, historically biotechnology has been principally associated with food, addressing such issues as malnutrition and famine. The history of biotechnology begins with zymotechnology, which commenced with a focus on brewing techniques for beer. By World War I, however, zymotechnology would expand to tackle larger industrial issues, and the potential of industrial fermentation gave rise to biotechnology. However, both the single-cell protein and gasohol projects failed to progress due to varying issues including public resistance, a changing economic scene, and shifts in political power.\n\nYet the formation of a new field, genetic engineering, would soon bring biotechnology to the forefront of science in society, and the intimate relationship between the scientific community, the public, and the government would ensue. These debates gained exposure in 1975 at the Asilomar Conference, where Joshua Lederberg was the most outspoken supporter for this emerging field in biotechnology. By as early as 1978, with the development of synthetic human insulin, Lederberg's claims would prove valid, and the biotechnology industry grew rapidly. Each new scientific advance became a media event designed to capture public support, and by the 1980s, biotechnology grew into a promising real industry. In 1988, only five proteins from genetically engineered cells had been approved as drugs by the United States Food and Drug Administration (FDA), but this number would skyrocket to over 125 by the end of the 1990s.\n\nThe field of genetic engineering remains a heated topic of discussion in today's society with the advent of gene therapy, stem cell research, cloning, and genetically modified food. While it seems only natural nowadays to link pharmaceutical drugs as solutions to health and societal problems, this relationship of biotechnology serving social needs began centuries ago.\n\nBiotechnology arose from the field of zymotechnology or zymurgy, which began as a search for a better understanding of industrial fermentation, particularly beer. Beer was an important industrial, and not just social, commodity. In late 19th-century Germany, brewing contributed as much to the gross national product as steel, and taxes on alcohol proved to be significant sources of revenue to the government. In the 1860s, institutes and remunerative consultancies were dedicated to the technology of brewing. The most famous was the private Carlsberg Institute, founded in 1875, which employed Emil Christian Hansen, who pioneered the pure yeast process for the reliable production of consistent beer. Less well known were private consultancies that advised the brewing industry. One of these, the Zymotechnic Institute, was established in Chicago by the German-born chemist John Ewald Siebel.\n\nThe heyday and expansion of zymotechnology came in World War I in response to industrial needs to support the war. Max Delbrück grew yeast on an immense scale during the war to meet 60 percent of Germany's animal feed needs. Compounds of another fermentation product, lactic acid, made up for a lack of hydraulic fluid, glycerol. On the Allied side the Russian chemist Chaim Weizmann used starch to eliminate Britain's shortage of acetone, a key raw material for cordite, by fermenting maize to acetone. The industrial potential of fermentation was outgrowing its traditional home in brewing, and \"zymotechnology\" soon gave way to \"biotechnology.\"\n\nWith food shortages spreading and resources fading, some dreamed of a new industrial solution. The Hungarian Károly Ereky coined the word \"biotechnology\" in Hungary during 1919 to describe a technology based on converting raw materials into a more useful product. He built a slaughterhouse for a thousand pigs and also a fattening farm with space for 50,000 pigs, raising over 100,000 pigs a year. The enterprise was enormous, becoming one of the largest and most profitable meat and fat operations in the world. In a book entitled \"Biotechnologie\", Ereky further developed a theme that would be reiterated through the 20th century: biotechnology could provide solutions to societal crises, such as food and energy shortages. For Ereky, the term \"biotechnologie\" indicated the process by which raw materials could be biologically upgraded into socially useful products.\n\nThis catchword spread quickly after the First World War, as \"biotechnology\" entered German dictionaries and was taken up abroad by business-hungry private consultancies as far away as the United States. In Chicago, for example, the coming of prohibition at the end of World War I encouraged biological industries to create opportunities for new fermentation products, in particular a market for nonalcoholic drinks. Emil Siebel, the son of the founder of the Zymotechnic Institute, broke away from his father's company to establish his own called the \"Bureau of Biotechnology,\" which specifically offered expertise in fermented nonalcoholic drinks.\n\nThe belief that the needs of an industrial society could be met by fermenting agricultural waste was an important ingredient of the \"chemurgic movement.\" Fermentation-based processes generated products of ever-growing utility. In the 1940s, penicillin was the most dramatic. While it was discovered in England, it was produced industrially in the U.S. using a deep fermentation process originally developed in Peoria, Illinois. The enormous profits and the public expectations penicillin engendered caused a radical shift in the standing of the pharmaceutical industry. Doctors used the phrase \"miracle drug\", and the historian of its wartime use, David Adams, has suggested that to the public penicillin represented the perfect health that went together with the car and the dream house of wartime American advertising. Beginning in the 1950s, fermentation technology also became advanced enough to produce steroids on industrially significant scales. Of particular importance was the improved semisynthesis of cortisone which simplified the old 31 step synthesis to 11 steps. This advance was estimated to reduce the cost of the drug by 70%, making the medicine inexpensive and available. Today biotechnology still plays a central role in the production of these compounds and likely will for years to come.\n\nEven greater expectations of biotechnology were raised during the 1960s by a process that grew single-cell protein. When the so-called protein gap threatened world hunger, producing food locally by growing it from waste seemed to offer a solution. It was the possibilities of growing microorganisms on oil that captured the imagination of scientists, policy makers, and commerce. Major companies such as British Petroleum (BP) staked their futures on it. In 1962, BP built a pilot plant at Cap de Lavera in Southern France to publicize its product, Toprina. Initial research work at Lavera was done by Alfred Champagnat, In 1963, construction started on BP's second pilot plant at Grangemouth Oil Refinery in Britain.\n\nAs there was no well-accepted term to describe the new foods, in 1966 the term \"single-cell protein\" (SCP) was coined at MIT to provide an acceptable and exciting new title, avoiding the unpleasant connotations of microbial or bacterial.\n\nThe \"food from oil\" idea became quite popular by the 1970s, when facilities for growing yeast fed by n-paraffins were built in a number of countries. The Soviets were particularly enthusiastic, opening large \"BVK\" (\"belkovo-vitaminny kontsentrat\", i.e., \"protein-vitamin concentrate\") plants next to their oil refineries in Kstovo (1973) and Kirishi (1974).\n\nBy the late 1970s, however, the cultural climate had completely changed, as the growth in SCP interest had taken place against a shifting economic and cultural scene (136). First, the price of oil rose catastrophically in 1974, so that its cost per barrel was five times greater than it had been two years earlier. Second, despite continuing hunger around the world, anticipated demand also began to shift from humans to animals. The program had begun with the vision of growing food for Third World people, yet the product was instead launched as an animal food for the developed world. The rapidly rising demand for animal feed made that market appear economically more attractive. The ultimate downfall of the SCP project, however, came from public resistance.\n\nThis was particularly vocal in Japan, where production came closest to fruition. For all their enthusiasm for innovation and traditional interest in microbiologically produced foods, the Japanese were the first to ban the production of single-cell proteins. The Japanese ultimately were unable to separate the idea of their new \"natural\" foods from the far from natural connotation of oil. These arguments were made against a background of suspicion of heavy industry in which anxiety over minute traces of petroleum was expressed. Thus, public resistance to an unnatural product led to the end of the SCP project as an attempt to solve world hunger.\n\nAlso, in 1989 in the USSR, the public environmental concerns made the government decide to close down (or convert to different technologies) all 8 paraffin-fed-yeast plants that the Soviet Ministry of Microbiological Industry had by that time.\n\nIn the late 1970s, biotechnology offered another possible solution to a societal crisis. The escalation in the price of oil in 1974 increased the cost of the Western world's energy tenfold. In response, the U.S. government promoted the production of gasohol, gasoline with 10 percent alcohol added, as an answer to the energy crisis. In 1979, when the Soviet Union sent troops to Afghanistan, the Carter administration cut off its supplies to agricultural produce in retaliation, creating a surplus of agriculture in the U.S. As a result, fermenting the agricultural surpluses to synthesize fuel seemed to be an economical solution to the shortage of oil threatened by the Iran–Iraq War. Before the new direction could be taken, however, the political wind changed again: the Reagan administration came to power in January 1981 and, with the declining oil prices of the 1980s, ended support for the gasohol industry before it was born.\n\nBiotechnology seemed to be the solution for major social problems, including world hunger and energy crises. In the 1960s, radical measures would be needed to meet world starvation, and biotechnology seemed to provide an answer. However, the solutions proved to be too expensive and socially unacceptable, and solving world hunger through SCP food was dismissed. In the 1970s, the food crisis was succeeded by the energy crisis, and here too, biotechnology seemed to provide an answer. But once again, costs proved prohibitive as oil prices slumped in the 1980s. Thus, in practice, the implications of biotechnology were not fully realized in these situations. But this would soon change with the rise of genetic engineering.\n\nThe origins of biotechnology culminated with the birth of genetic engineering. There were two key events that have come to be seen as scientific breakthroughs beginning the era that would unite genetics with biotechnology. One was the 1953 discovery of the structure of DNA, by Watson and Crick, and the other was the 1973 discovery by Cohen and Boyer of a recombinant DNA technique by which a section of DNA was cut from the plasmid of an E. coli bacterium and transferred into the DNA of another. This approach could, in principle, enable bacteria to adopt the genes and produce proteins of other organisms, including humans. Popularly referred to as \"genetic engineering,\" it came to be defined as the basis of new biotechnology.\n\nGenetic engineering proved to be a topic that thrust biotechnology into the public scene, and the interaction between scientists, politicians, and the public defined the work that was accomplished in this area. Technical developments during this time were revolutionary and at times frightening. In December 1967, the first heart transplant by Christian Barnard reminded the public that the physical identity of a person was becoming increasingly problematic. While poetic imagination had always seen the heart at the center of the soul, now there was the prospect of individuals being defined by other people's hearts. During the same month, Arthur Kornberg announced that he had managed to biochemically replicate a viral gene. \"Life had been synthesized,\" said the head of the National Institutes of Health. Genetic engineering was now on the scientific agenda, as it was becoming possible to identify genetic characteristics with diseases such as beta thalassemia and sickle-cell anemia.\n\nResponses to scientific achievements were colored by cultural skepticism. Scientists and their expertise were looked upon with suspicion. In 1968, an immensely popular work, \"The Biological Time Bomb\", was written by the British journalist Gordon Rattray Taylor. The author's preface saw Kornberg's discovery of replicating a viral gene as a route to lethal doomsday bugs. The publisher's blurb for the book warned that within ten years, \"You may marry a semi-artificial man or woman…choose your children's sex…tune out pain…change your memories…and live to be 150 if the scientific revolution doesn’t destroy us first.\" The book ended with a chapter called \"The Future – If Any.\" While it is rare for current science to be represented in the movies, in this period of \"Star Trek\", science fiction and science fact seemed to be converging. \"Cloning\" became a popular word in the media. Woody Allen satirized the cloning of a person from a nose in his 1973 movie \"Sleeper\", and cloning Adolf Hitler from surviving cells was the theme of the 1976 novel by Ira Levin, \"The Boys from Brazil\".\n\nIn response to these public concerns, scientists, industry, and governments increasingly linked the power of recombinant DNA to the immensely practical functions that biotechnology promised. One of the key scientific figures that attempted to highlight the promising aspects of genetic engineering was Joshua Lederberg, a Stanford professor and Nobel laureate. While in the 1960s \"genetic engineering\" described eugenics and work involving the manipulation of the human genome, Lederberg stressed research that would involve microbes instead. Lederberg emphasized the importance of focusing on curing living people. Lederberg's 1963 paper, \"Biological Future of Man\" suggested that, while molecular biology might one day make it possible to change the human genotype, \"what we have overlooked is euphenics, the engineering of human development.\" Lederberg constructed the word \"euphenics\" to emphasize changing the phenotype after conception rather than the genotype which would affect future generations.\n\nWith the discovery of recombinant DNA by Cohen and Boyer in 1973, the idea that genetic engineering would have major human and societal consequences was born. In July 1974, a group of eminent molecular biologists headed by Paul Berg wrote to \"Science\" suggesting that the consequences of this work were so potentially destructive that there should be a pause until its implications had been thought through. This suggestion was explored at a meeting in February 1975 at California's Monterey Peninsula, forever immortalized by the location, Asilomar. Its historic outcome was an unprecedented call for a halt in research until it could be regulated in such a way that the public need not be anxious, and it led to a 16-month moratorium until National Institutes of Health (NIH) guidelines were established.\n\nJoshua Lederberg was the leading exception in emphasizing, as he had for years, the potential benefits. At Asilomar, in an atmosphere favoring control and regulation, he circulated a paper countering the pessimism and fears of misuses with the benefits conferred by successful use. He described \"an early chance for a technology of untold importance for diagnostic and therapeutic medicine: the ready production of an unlimited variety of human proteins. Analogous applications may be foreseen in fermentation process for cheaply manufacturing essential nutrients, and in the improvement of microbes for the production of antibiotics and of special industrial chemicals.\" In June 1976, the 16-month moratorium on research expired with the Director's Advisory Committee (DAC) publication of the NIH guidelines of good practice. They defined the risks of certain kinds of experiments and the appropriate physical conditions for their pursuit, as well as a list of things too dangerous to perform at all. Moreover, modified organisms were not to be tested outside the confines of a laboratory or allowed into the environment.\n\nAtypical as Lederberg was at Asilomar, his optimistic vision of genetic engineering would soon lead to the development of the biotechnology industry. Over the next two years, as public concern over the dangers of recombinant DNA research grew, so too did interest in its technical and practical applications. Curing genetic diseases remained in the realms of science fiction, but it appeared that producing human simple proteins could be good business. Insulin, one of the smaller, best characterized and understood proteins, had been used in treating type 1 diabetes for a half century. It had been extracted from animals in a chemically slightly different form from the human product. Yet, if one could produce synthetic human insulin, one could meet an existing demand with a product whose approval would be relatively easy to obtain from regulators. In the period 1975 to 1977, synthetic \"human\" insulin represented the aspirations for new products that could be made with the new biotechnology. Microbial production of synthetic human insulin was finally announced in September 1978 and was produced by a startup company, Genentech. Although that company did not commercialize the product themselves, instead, it licensed the production method to Eli Lilly and Company. 1978 also saw the first application for a patent on a gene, the gene which produces human growth hormone, by the University of California, thus introducing the legal principle that genes could be patented. Since that filing, almost 20% of the more than 20,000 genes in the human DNA have been patented.\n\nThe radical shift in the connotation of \"genetic engineering\" from an emphasis on the inherited characteristics of people to the commercial production of proteins and therapeutic drugs was nurtured by Joshua Lederberg. His broad concerns since the 1960s had been stimulated by enthusiasm for science and its potential medical benefits. Countering calls for strict regulation, he expressed a vision of potential utility. Against a belief that new techniques would entail unmentionable and uncontrollable consequences for humanity and the environment, a growing consensus on the economic value of recombinant DNA emerged.\n\nWith ancestral roots in industrial microbiology that date back centuries, the new biotechnology industry grew rapidly beginning in the mid-1970s. Each new scientific advance became a media event designed to capture investment confidence and public support. Although market expectations and social benefits of new products were frequently overstated, many people were prepared to see genetic engineering as the next great advance in technological progress. By the 1980s, biotechnology characterized a nascent real industry, providing titles for emerging trade organizations such as the Biotechnology Industry Organization (BIO).\n\nThe main focus of attention after insulin were the potential profit makers in the pharmaceutical industry: human growth hormone and what promised to be a miraculous cure for viral diseases, interferon. Cancer was a central target in the 1970s because increasingly the disease was linked to viruses. By 1980, a new company, Biogen, had produced interferon through recombinant DNA. The emergence of interferon and the possibility of curing cancer raised money in the community for research and increased the enthusiasm of an otherwise uncertain and tentative society. Moreover, to the 1970s plight of cancer was added AIDS in the 1980s, offering an enormous potential market for a successful therapy, and more immediately, a market for diagnostic tests based on monoclonal antibodies. By 1988, only five proteins from genetically engineered cells had been approved as drugs by the United States Food and Drug Administration (FDA): synthetic insulin, human growth hormone, hepatitis B vaccine, alpha-interferon, and tissue plasminogen activator (TPa), for lysis of blood clots. By the end of the 1990s, however, 125 more genetically engineered drugs would be approved.\n\nThe 2007–2008 global financial crisis led to several changes in the way the biotechnology industry was financed and organized. First, it led to a decline in overall financial investment in the sector, globally; and second, in some countries like the UK it led to a shift from business strategies focused on going for an initial public offering (IPO) to seeking a trade sale instead. By 2011, financial investment in the biotechnology industry started to improve again and by 2014 the global market capitalization reached $1 trillion.\n\nGenetic engineering also reached the agricultural front as well. There was tremendous progress since the market introduction of the genetically engineered Flavr Savr tomato in 1994. Ernst and Young reported that in 1998, 30% of the U.S. soybean crop was expected to be from genetically engineered seeds. In 1998, about 30% of the US cotton and corn crops were also expected to be products of genetic engineering.\n\nGenetic engineering in biotechnology stimulated hopes for both therapeutic proteins, drugs and biological organisms themselves, such as seeds, pesticides, engineered yeasts, and modified human cells for treating genetic diseases. From the perspective of its commercial promoters, scientific breakthroughs, industrial commitment, and official support were finally coming together, and biotechnology became a normal part of business. No longer were the proponents for the economic and technological significance of biotechnology the iconoclasts. Their message had finally become accepted and incorporated into the policies of governments and industry.\n\nAccording to Burrill and Company, an industry investment bank, over $350 billion has been invested in biotech since the emergence of the industry, and global revenues rose from $23 billion in 2000 to more than $50 billion in 2005. The greatest growth has been in Latin America but all regions of the world have shown strong growth trends. By 2007 and into 2008, though, a downturn in the fortunes of biotech emerged, at least in the United Kingdom, as the result of declining investment in the face of failure of biotech pipelines to deliver and a consequent downturn in return on investment.\n\n\n\n"}
{"id": "10271359", "url": "https://en.wikipedia.org/wiki?curid=10271359", "title": "Implicit cognition", "text": "Implicit cognition\n\nImplicit cognition refers to unconscious influences such as knowledge, perception, or memory, that influence a person's behavior, even though they themselves have no conscious awareness whatsoever of those influences.\n\nImplicit cognition is everything one does and learns unconsciously or without any awareness that one is doing it. An example of implicit cognition could be when a person first learns to ride a bike: at first they are aware that they are learning the required skills. After having stopped for many years, when the person starts to ride the bike again they do not have to relearn the motor skills required, as their implicit knowledge of the motor skills takes over and they can just start riding the bike as if they had never stopped. In other words, they do not have to think about the actions that they are performing in order to ride the bike. It can be seen with this example that implicit cognition is involved with many of the different mental activities and everyday situations of people's daily lives. There are many processes in which implicit memory works, which include learning, our social cognition, and our problem solving skills.\n\nImplicit cognition was first discovered in the year of 1649 by Descartes in his \"Passions of the Soul\". He said in one of his writings that he saw that unpleasant childhood experiences remain imprinted in a child's brain until its death without any conscious memory of it remaining. Even though this idea was never accepted by any of his peers, in 1704 Gottfried Wilhelm Leibniz in his \"New Essays Concerning Human Understanding\" stressed the importance of unconscious perceptions which he said were the ideas that we are not consciously aware of yet still influence people's behavior. He claimed that people have residual effects of prior impressions without any remembrance of them. In 1802 French philosopher Maine de Biran in his \"The Influence of Habit on the Faculty of Thinking\" was the first person after Leibniz to systematically discuss implicit memory stating that after enough repetition, a habit can become automatic or completed without any conscious awareness. In 1870 Ewald Hering said that it was essential to consider unconscious memory, which is involved in involuntary recall, and the development of automatic and unconscious habitual actions.\n\nImplicit learning starts in our early childhood, this means that people are not able to learn the proper grammar and rules to speaking a language until the age of seven. So if this is the case then how do we learn to talk by the age of four? One of the ways that this is possible is through implicit learning and association. Children learn their first language from what they hear when they are listening to the adults and through their own talking activities. This goes to show that the way children learn language involves implicit learning.\n\nA study was conducted with amnesiac patients in an attempt to demonstrate that amnesiac patients that were unable to learn a list of words or pictures when their performance was tested were able to complete or put together fragmented words and incomplete pictures. This was found to be true as the patients were able to perform better when asked to complete words or pictures. A possible explanation for this could be that implicit memory should be less susceptible to damage that may happen to the brain than explicit memory. There was a case where a 54-year-old man that had bitemporal damage worse on the right side had a hard time remembering things from his own life as well as famous events, names and even; yet he was able to perform within the normal limits with a word completion task involving famous names and with judgments of famous faces. This is a prime example that implicit memory can be less vulnerable to brain damage.\n\nA famous study investigated the Identification blindsight effect with individuals who had suffered damage to one half of the visual cortex and were blind in the opposite half of the visual field. It was discovered that when objects or pictures were shown to these blind areas, the participants said that they saw no object or picture, but a certain number were able to identify the stimulus as either a cross or circle when asked to guess at a considerably higher rate than would have been expected by chance. The reason that this happened is because the information was able to be process through the first three stages of selection, organization, and interpretation or comprehension of the perceptual cycle but failed at only the last stage of retention and memory where the identified image is entered into their awareness. Thus stimuli can enter implicit memory even when people are unable to consciously perceive them.\n\nImplicit cognition also plays a role in social cognition. People tend to see objects and individuals as more encouraging or acceptable the more often that people are exposed to them. An example includes the False-fame Effect. Graf and Masson (1993) conducted a study where they showed participants a list with both famous and non-famous names. When it was shown around people were able to recall the famous names more than the non-famous names initially, but after about a 24-hour delay participants began to associate the non-famous names with famous people. This supports implicit cognition because the participants began to unconsciously associate the non-famous names with famous people.\n\nAlthough the process is unconscious, implicit cognition influences how people view each other as well as their interactions with one another. People tend to view those who look alike as belonging together or to similar groups and associate them with the social groups that existed in their high school years. These groups represented different relations between the students and were made up of students who were perceived as having similarities among each other. \nA study was conducted to see the amount of distance that participants put between individuals given certain circumstances. The participants were asked to place figures of individuals where they thought the figures should be standing given certain circumstances. It was found that people typically place men and women close to each other, to make little families formed with the figures of a woman, a man and of children. The participants did the same when asked to show friends and/or acquaintances, the two figures were placed relatively close to one another rather than if they were asked to represent strangers. When asked to represent strangers the participants placed the figures far apart. There are two parts to the social relations view that is liking relations were the ultimate goal is to be together, then there is the disliking relation view which is separation from the person. An example of this could be when someone is walking down a hall way and see someone whom they know and like that person is more likely to wave and say hello to them. On the other hand, say the person they see is someone whom they dislike, their response will be the opposite as they try to either avoid them or get away from them as quickly as possible showing the separation between the two of them. There are two views to the social relations theory, one of them is that people are out to mainly seek dominance of those around them, while the other view is that people mainly see the relations as either belonging or not belonging or liking and disliking on another. It is seen that males mainly seek dominance against one another as they are competitive and looking to outdo one another. For females on the other hand it is seen that women perceive their social views and values as more of the belonging or liking scale in terms of their closeness to one another. Implicit cognition not only involved how people view each other but also how they view themselves. This means that our own image is constructed from what others see of us rather than our own views. The way that we view ourselves is from what others see us as, or from the times that we compare ourselves to other people. The way that this plays a role in implicit cognition is because all of these actions people do unconsciously, or they are unaware that they are making these decision. Men do not consciously seek to be dominant over one another as women do not consciously arrange their social views or values in terms of their closeness. These are each things that people do without their conscious knowledge of these actions, which ties in with implicit cognition.\n\nThere are scenarios when we act on something and then think back about handling it in different situation or manner. That is implicit cognition coming into play, the mind will then go based off ethical and similar situations when interacting with a certain thought. Implicit cognition and its automated thought process allows a person to decide something out of impulse. It is often defined as an involuntary process where tasks are easily absent of consciousness. There are plenty of factors that influence behaviors and thought processes. Such as social learning, to stigmas, and two major aspects of implicit and explicit cognition. Implicit on one hand is obtained through social aspects and association, while explicit cognition is gained through propositional attitudes or beliefs of certain thoughts, Implicit cognition can be incorporated with a mixture of attention, goals, self-association, and at times even motivational processes. Researchers have used different methods to test these theories of behavior correlation with implicit cognition. Using Implicit Association Tests (ITA's) is a method that is significantly used, according to Fazio & Olsen (2003) and Richetin & Richardson (2008). Since published, approximately ten years or so, it has been widely used influencing research on implicit attitudes. Implicit cognition is a process based off automatic mental interpretations. It's what a person really thinks, yet is not consciously aware of. Behavior is then affected, usually causing negative influences, both theoretically and empirical reasons presume that automatic cognitive processes are contributed to aggressive behaviors.\n\nImpulse behaviors are often created without awareness. Negativity is a characteristic of implicit cognition, since it is an automated response. Explicit cognition is rarely used when trying to discover behavior of ones thought process. Researchers again use IAT's to determine ones thoughts and how a person incorporates these automatic processes, findings consider that implicit cognition may direct what behaviors a person may choose when facing extreme stimuli. For example, death can be perceived as positive, negative, or a combination of the two. Depending on the attributes of death, it can include a general perspective or a \"me\" attribute. implied that implicit association with death and or suicide initiates a final process when deciding how to cope to these extreme measures. Self- harm is another characteristic associated with implicit cognition. Because although we may think of it, it is controlled subconsciously. IAT's showed that there was a stronger correlation of implicit cognition and death/suicide than self-harm. The idea of pain may influence a person to think twice, while suicidal may seem quick, thus the automatic process can show how effective this negative behavior and implicit cognition come hand in hand. Automated processes doesn't allow a person to thoroughly create a conscious choice, therefore creating negative influence to behavior. Another negative behavior that can be associated with implicit cognition is depression. Whether a person takes a positive or negative outlook on the certain situation can produce if a person will be associated with depression. It is easier to determine an implicit mindset simple because it is outside of awareness. Implicit processes are considered critical when determining a person's reactions to a certain schema. Implicit cognition is often immediately affective towards a person's reaction. Implicit cognitions also consisted of negative schemas that included hidden cognitive frameworks, and activation of stress. Awareness was often misinterpreted and implicit cognition emerged because of these negative schemas. Behaviors merged through implicit cognition involve a variety of addictive behaviors, problematic thinking, depression, aggression, suicide, death, and other negative factors. Certain life situations add to this schema, whether it be stressful situations, sudden, or anything along these lines, aspects of implicit cognition are used and evaluated.\n\nImplicit cognition can also be associated with mental illness and the way thoughts are processed. Automatic stigmas and attitudes may anticipate other cognitive and behavior tendencies. A person with mental illness may be correlated with a guilt-related, self-associated personality. Because of these associations it may be managed outside one's own control and awareness, showing how implicit cognition is affected. However a dual process can be assessed within implicit and explicit cognition. An agreement between the two thought processes may be an issue, explicit may not be in contact with implicit, therefore causing more of a problem. Mental illness can include both implicit and explicit attitudes, however implicit self-concepts gave more negative consequence when dealing with mental illness. Much of implicit problems happened to be associated with alcohol, however this isn't the goal in order to describe a mental process and implicit cognition. The most widely influenced mental illness in association with implicit cognition would be Schizophrenia. Since a person of this illness has a problem of detecting what is real and what is not, implicit memory is often used with these patients. However, since it cannot really be detected if it is emotionally, mentally, or a combination of both some aspects of this illness are usually exercised uninterrupted, and unconsciously. Since schizophrenia is widely varied and has different characteristics, we cannot quite measure the outcome of implicit cognition.\n\nImplicit cognition refers to perceptual, memory, comprehension, and performance processes that occur through unconscious awareness. For example, when a patient is discharged after a surgery, the effects of the anesthesia can cause abnormal behaviors without any conscious awareness. According to Wiers et al. (2006) some scholars argue implicit cognition is misinterpreted and could be used to improve behaviors while others highlight the dangers of it. Research studies have shown implicit cognition is a strong predictor for several issues like substance abuse, misconduct, and mental disorders. These inherent thoughts are influenced from early adolescent experiences primarily a negative impact from culture. Adolescents who experience a rough childhood early on develop low levels of self-esteem. Therefore, the cognition to act dangerously is an oblivious development. Research for implicit cognition has started to grow especially within mental disorders.\n\nSchemas are used to interpret the functions involved when individuals would make sense of their surroundings. This cognition happens through an explicit process of recalling an item routinely or implicit process that is outside conscious awareness control. A recent study suggests individuals who have experienced a difficult upbringing develop schemata of fear as well as anxiety and will react almost immediately when they feel threatened. People who are anxious predominantly focus on any peril-related stimuli since they are hyper vigilant. For example, an anxious individual who is about to cross the street at the same time a car drives to a stop sign. The anxious person will automatically assume the driver will not stop. This is recognition of threat through a semantic process that instantaneously occurs. Ambiguous cues are viewed as a threat since there is no relevant knowledge to make sense of. People will have a difficult time to understand and will respond negatively. This kind of behavior can explain how implicit cognition may be an influence for pathological anxiety.\n\nThe ideas of psychotic patients who have low self-esteem are prone to more serious illnesses. This concept was examined through both implicit and explicit perspectives by measuring the self-esteem of paranoia and depression patients. Previous research suggests that negative implicit cognition is not the symptoms for depression and paranoia, but it is an antecedent for the onset. Current research proposes that high implicit self-esteem is linked to less paranoia. It is imperative for patients who have low self-esteem to become more overt about these situations. Another study found a substantial association between adverse self-questioning in implicit cognition and depression. People who do not think highly of themselves are more likely to be depressed because of this involuntary implicit learning.\n\nImplicit cognition is another influential predictor for bipolar disorder and unipolar disorder. Research proposes patients with bipolar disorder show more common implicit and depressive self-referencing than unipolar patients. Implicit cognition plays a strong role for patients with both bipolar and unipolar disorder. These patients have dysfunctional self-schemata, which is viewed as vulnerability for potential illnesses. Patients who have this vulnerability usually do not seek mental assistance which can later become more problematic to treat. Bipolar disorder patients with low implicit self-esteem are more defensive. This is an unconscious reaction to be manic protective when they feel threatened in any way.\nSince the growing research of implicit cognition is associated with abnormalities, researchers attempted to find a connection between implicit neuroticism and schizophrenia. Indeed, there was a correlation; participants with schizophrenia were high in implicit neuroticism and low in implicit extraversion when compared to people who were mentally healthy. Participants were given questionnaires that ask personality questions such as \"I enjoy being the center of attention\". Implicit cognition constitutes low levels of extraversion because these participants are known to avoid any coping. Schizophrenia patients and healthy individuals differ in associative representation pertaining to themselves in neuroticism features. People who are schizophrenic develop an implicit learning, meaning they have an error free learning style so they never take feedback from anyone else.\n\nResearch on suicide can be a difficult process because suicidal patients are commonly covert about their intentions to avoid hospitalization. An implicit cognition associated self-task was applied in one experiment to unveil any suspicious behavior of people who might attempt suicide. This study found patients who were released from mental hospitals showed significant implicit association to attempt suicide. The Implicit Association Task would predict whether a patient was likely to attempt suicide depending on they respond. An individual's implicit cognition may lead to a behavior to best cope with stress. This behavior may be suicide, substance abuse, or even violence. However, implicit association with death will show those most at risk for attempting suicide because this individual is looking for the best solution for ending their stress.\n\nImplicit cognition is measured in different ways to find the most accurate outcomes. The task used for patients with anxiety disorders was a modified Stroop task to observe attention biases in anxiety. A participant's reaction time was measured by the reported color of each word. Participants would name the color for each risk relevant words and risk irrelevant words and depending on the color of each word might slow the reaction time. Colors like red were used to see if anxious participants would have a slower reaction time. These colors are known as aposematic implying a threat warning. Most studies used the Implicit Association Test which varies for each study. For example, implicit self-esteem can be tested by giving participants questionnaires that are self-referent. These ask questions like \"I am known to be suicidal\". Depending on the response of the participants, the researcher can assessed the current state for each participant. Patients that were rated high in suicide immediately received psychiatric treatment. According to these experiments implicit cognition may be a strong predictor for mental disorders.\n\n"}
{"id": "36257670", "url": "https://en.wikipedia.org/wiki?curid=36257670", "title": "Inventing Iron Man", "text": "Inventing Iron Man\n\nInventing Iron Man: The Possibility of a Human Machine is a popular science book published in 2011 by neuroscience professor, martial arts master, and long-time comic-book reader E. Paul Zehr. By looking at current technology, as well as how the human body and nervous system would have to adapt, Zehr applies scientific principles and creativity to explore the feasibility of Iron Man as a reality. \n\nIn the book, Zehr physically deconstructs Iron Man, analyzing how we could use modern-day technology to create a suit of armor similar to the one made by Tony Stark. The book makes a direct, parallel comparison of comic book science fiction with modern science. Zehr expresses that while science may be nearing a point where a suit like Iron Man’s could be made, it is important to consider limitations of the human body itself. Inventing Iron Man scientifically examines the brain-machine interface, bringing together concepts in neuroscience and neuroplasticity. \n\n\n"}
{"id": "54267696", "url": "https://en.wikipedia.org/wiki?curid=54267696", "title": "John Talbot Robinson", "text": "John Talbot Robinson\n\nJohn Talbot Robinson (10 January 1923 – 12 October 2001) was a distinguished South African hominin paleontologist. His most famous discovery (with Robert Broom) was the nearly complete fossil skull of the hominin species \"Australopithecus africanus\", known as Mrs. Ples.\n\nRobinson was born in Elliot, South Africa. After training in Cape Town as a zoologist, Robinson moved to Pretoria in 1946 to take up a post at the Transvaal Museum. In Pretoria, he worked with Robert Broom. They focused on excavations at the caves of Sterkfontein, Kromdraai and Swartkrans. Between 1946 and 1952 they jointly published twenty-three books and articles.\n\nAfter Broom's death in 1951, Robinson took over as head of the Department of Vertebrate Paleontology and Physical Anthropology. In 1956 he published what is arguably his most important work, a monograph titled \"The Dentition of the Australopithecinae\" after which the University of Cape Town awarded him a Doctor of Science degree.\n\nIn 1963 Robinson began a Professorship at the University of Wisconsin–Madison and eventually held joint appointments in the Zoology and Anthropology Departments. He was a gifted and empathetic teacher who spent many after-class hours mentoring his undergraduate and graduate students. He taught courses in evolutionary theory and human origins, zoology and anthropology. Robinson continued to make trips back to South Africa to carry out research.\n\nHe died in Madison, Wisconsin in 2001.\n"}
{"id": "14326716", "url": "https://en.wikipedia.org/wiki?curid=14326716", "title": "Joseph T. Gregory", "text": "Joseph T. Gregory\n\nDr. Joseph Tracy Gregory (July 28, 1914 – November 18, 2007) was an American paleontologist and professor.\n\nJoseph Tracy Gregory was born in Eureka, California, the only child of Frank Gregory, a civil engineer, and Edith Tracy, a high school teacher. He grew up in Berkeley, California and continued with his college education there, graduating from the University of California with an A.B. in 1935, and receiving his doctorate in 1938. During World War II, he served as a lieutenant in the Army Air Forces in the weather service.\n\nAfter the war, he became Assistant Professor of Geology at Yale University, as well as Curator of Vertebrate Paleontology at the Peabody Museum of Natural History. In 1960, he moved to the University of California at Berkeley, where he was Professor of Vertebrate Paleontology, eventually retiring in 1979 as Emeritus Professor.\n\nHe was especially active in researching the paleontological record of the Western United States. In his later career, he gained notice as the primary editor of the annual \"Bibliography of Vertebrate Paleontology.\"\n\nThe Society of Vertebrate Paleontology’s Joseph T. Gregory Award is given annually since 1992 for “contributions to the welfare of Vertebrate Paleontology.”\n\nHe married Jane Everest in 1949. They had two children, Carl Douglas Gregory (1950- ) and Sarah Jane Gregory (1953-2006).\n\n"}
{"id": "7408707", "url": "https://en.wikipedia.org/wiki?curid=7408707", "title": "List of BPEL engines", "text": "List of BPEL engines\n\nThis is a list of notable Business Process Execution Language (BPEL) and Business Process Model and Notation (BPMN) engines.\n\n"}
{"id": "47968007", "url": "https://en.wikipedia.org/wiki?curid=47968007", "title": "List of Cambridge University Press journals", "text": "List of Cambridge University Press journals\n\nThis list of Cambridge University Press journals includes all academic journals published by Cambridge Journals , including journals no longer published or no longer published by Cambridge, but for which they still maintain archives. Several journals in this list are published by Cambridge in cooperation with or on behalf of other entities such as learned and professional societies. In these cases Cambridge provides publishing and printing, distribution, online archives, and other services on behalf of the original publisher.\n\nCambridge Journals publishes journals under three main access policies: closed access, open access, and a hybrid model in which individual articles in an otherwise closed access journal is available under open access terms. Such articles are designated as open access by its author or publishing organisation at time of acceptance, and Cambridge Journals charges an article processing fee to cover their associated costs like peer-review, copy-editing, and typesetting. For articles published as open access in hybrid journals, Cambridge Journals applies what it terms its \"double-dipping policy\": for journals with more than a minimum share of open access articles (5%) and article processing fees (£5000), subscription rates are lowered for renewing subscribers the following year. In 2015, under this policy and using data from 2014, subscription renewal rates for 2016 were reduced for six journals by 2.6–7.7%. Articles published under open access terms may be licensed under one of several available Creative Commons licenses.\n\n\n\n"}
{"id": "6017625", "url": "https://en.wikipedia.org/wiki?curid=6017625", "title": "List of environmental books", "text": "List of environmental books\n\nThis page is a list of environmental books.\n\nIn this context they are notable books that describe, as a major theme, the effects of human activity on the environment.\n\nNon-fiction is an account or representation of a subject which is presented as fact. This presentation may be accurate or not; that is, it can give either a true or a false account of the subject in question. However, it is generally assumed that the authors of such accounts believe them to be truthful at the time of their composition.\n\n\n"}
{"id": "42245467", "url": "https://en.wikipedia.org/wiki?curid=42245467", "title": "List of medical schools in Iran", "text": "List of medical schools in Iran\n\nThis list of medical schools in Iran includes major academic institutions in Iran that award Doctor of Medicine (MD) degrees.\n"}
{"id": "1000167", "url": "https://en.wikipedia.org/wiki?curid=1000167", "title": "List of rock formations", "text": "List of rock formations\n\nA rock formation is an isolated, scenic, or spectacular surface rock outcrop. Rock formations are usually the result of weathering and erosion sculpting the existing rock. The term 'rock formation' can also refer to specific sedimentary strata or other rock unit in stratigraphic and petrologic studies.\n\nA rock structure can be created in any rock type or combination:\n\nGeologists have created a number of terms to describe different rock structures in the landscape that can be formed by natural processes:\n\nHere is a list of rock formations by continent.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "41513895", "url": "https://en.wikipedia.org/wiki?curid=41513895", "title": "List of things named after Hermann von Helmholtz", "text": "List of things named after Hermann von Helmholtz\n\nHermann von Helmholtz (1821 – 1894), German physician and physicist who made significant contributions to several widely varied areas of modern science, is the eponym of the topics listed below.\n\n\n\n\n\n"}
{"id": "7120027", "url": "https://en.wikipedia.org/wiki?curid=7120027", "title": "List of volcanoes in Grenada", "text": "List of volcanoes in Grenada\n\nThis is a list of active and extinct volcanoes in Grenada.\n\n"}
{"id": "42397902", "url": "https://en.wikipedia.org/wiki?curid=42397902", "title": "Microscale and macroscale models", "text": "Microscale and macroscale models\n\nMicroscale models form a broad class of computational models that simulate fine-scale details, in contrast with macroscale models, which amalgamate details into select categories. Microscale and macroscale models can be used together to understand different aspects of the same problem.\n\nMacroscale models can include ordinary, partial, and integro-differential equations, where categories and flows between the categories determine the dynamics, or may involve only algebraic equations. An abstract macroscale model may be combined with more detailed microscale models. Connections between the two scales are related to multiscale modeling. One mathematical technique for multiscale modeling of nanomaterials is based upon the use of Multiscale Green's function.\n\nIn contrast, microscale models can simulate a variety of details, such as individual bacteria in biofilms, individual pedestrians in simulated neighborhoods, individual light beams in ray-tracing imagery, individual houses in cities, fine-scale pores and fluid flow in batteries, fine-scale compartments in meteorology, fine-scale structures in particulate systems, and other models where interactions among individuals and background conditions determine the dynamics.\n\nDiscrete-event models, individual-based models, and agent-based models are special cases of microscale models. However, microscale models do not require discrete individuals or discrete events. Fine details on topography, buildings, and trees can add microscale detail to meteorological simulations and can connect to what are called mesoscale models in that discipline. Square-meter-sized landscape resolution available from images allow waterflow across land surfaces to be modeled, for example rivulets and water pockets, using gigabyte-sized arrays of detail. Models of neural networks may include individual neurons but may run in continuous time and thereby lack precise discrete events.\n\nIdeas for computational microscale models arose in the earliest days of computing and were applied to complex systems that could not accurately be described by standard mathematical forms.\n\nTwo themes emerged in the work of two founders of modern computation around the middle of the 20th century. First, pioneer Alan Turing used simplified macroscale models to understand the chemical basis of morphogenesis, but then proposed and used computational microscale models to understand the nonlinearities and other conditions that would arise in actual biological systems. Second, pioneer John von Neumann created a cellular automaton to understand the possibilities for self-replication of arbitrarily complex entities, which had a microscale representation in the cellular automaton but no simplified macroscale form. This second theme is taken to be part of agent-based models, where the entities ultimately can be artificially intelligent agents operating autonomously.\n\nBy the last quarter of the 20th century, computational capacity had grown so far that up to tens of thousands of individuals or more could be included in microscale models, and that sparse arrays could be applied to also achieve high performance. Continued increases in computing capacity allowed hundreds of millions of individuals to be simulated on ordinary computers with microscale models by the early 21st century.\n\nThe term \"microscale model\" arose later in the 20th century and now appears in the literature of many branches of physical and biological science.\n\nFigure 1 represents a fundamental macroscale model: population growth in an unlimited environment. Its equation is relevant elsewhere, such as compounding growth of capital in economics or exponential decay in physics. It has one amalgamated variable, formula_1, the number of individuals in the population at some time formula_2. It has an amalgamated parameter formula_3, the annual growth rate of the population, calculated as the difference between the annual birth rate formula_4 and the annual death rate formula_5. Time formula_2 can measured in years, as shown here for illustration, or in any other suitable unit.\n\nThe macroscale model of Figure 1 amalgamates parameters and incorporates a number of simplifying approximations: \nThese approximations of the macroscale model can all be refined in analogous microscale models.\n\nOn the first approximation listed above—that birth and death rates are constant—the macroscale model of the Figure 1 is exactly the mean of a large number of stochastic trials with the growth rate fluctuating randomly in each instance of time. Microscale stochastic details are subsumed into a partial differential diffusion equation and that equation is used to establish the equivalence.\n\nTo relax other assumptions, researchers have applied computational methods. Figure 2 is a sample computational microscale algorithm that corresponds to the macroscale model of Figure 1. When all individuals are identical and mutations in birth and death rates are disabled, the microscale dynamics closely parallel the macroscale dynamics (Figures 3A and 3B). The slight differences between the two models arise from stochastic variations in the microscale version not present in the deterministic macroscale model. These variations will be different each time the algorithm is carried out, arising from intentional variations in random number sequences.\n\nWhen not all individuals are identical, the microscale dynamics can differ significantly from the macroscale dynamics, simulating more realistic situations than can be modeled at the macroscale (Figures 3C and 3D). The microscale model does not explicitly incorporate the differential equation, though for large populations it simulates it closely. When individuals differ from one another, the system has a well defined behavior but the differential equations governing that behavior are difficult to codify. The algorithm of Figure 2 is a basic example of what is called an equation-free model.\n\nWhen mutations are enabled in the microscale model (formula_7), the population grows more rapidly than in the macroscale model (Figures 3C and 3D). Mutations in parameters allow some individuals to have higher birth rates and others to have lower death rates, and those individuals contribute proportionally more to the population. All else being equal, the average birth rate drifts to higher values and the average death rate drifts to lower values as the simulation progresses. This drift is tracked in the data structures named \"beta\" and \"delta\" of the microscale algorithm of Figure 2.\n\nThe algorithm of Figure 2 is a simplified a microscale model using the Euler method. Other algorithms such as the Gillespie method and the discrete event method are also used in practice. Versions of the algorithm in practical use include efficiencies such as removing individuals from consideration once they die (to reduce memory requirements and increase speed), and scheduling stochastic events into the future (to provide a continuous time scale and to further improve speed). Such approaches can be orders of magnitude faster.\n\nThe complexity of systems addressed by microscale models leads to complexity in the models themselves, and the specification of a microscale model can be tens or hundreds of times larger than its corresponding macroscale model. (The simplified example of Figure 2 has 25 times as many lines in its specification as does Figure 1.) Since bugs occur in computer software and cannot completely be removed by standard methods such as testing, and since complex models often are neither published in detail nor peer-reviewed, their validity has been called into question. Guidelines on best practices for microscale models exist but no papers on the topic claim a full resolution of the problem of validating complex models.\n\nComputing capacity is reaching levels where populations of entire countries or even the entire world are within the reach of microscale models, and improvements in census and travel data allow further improvements in parameterizing such models. Remote sensors from Earth-observing satellites and from ground-based observatories such as the National Ecological Observatory Network (NEON) provide large amounts of data for calibration. Potential applications range from predicting and reducing the spread of disease to helping understand the dynamics of the earth.\n\nFigure 1. \"One of the simplest of macroscale models: an ordinary differential equation describing continuous exponential growth. formula_1 is the size of the population at time formula_2, formula_10 is the rate of change through time in the single dimension formula_11. formula_12 is the initial population at formula_13, formula_4 is a birth rate per time unit, and formula_5 is a death rate per time unit. At the left is the differential form; at the right is the explicit solution in terms of standard mathematical functions, which follows in this case from the differential form. Almost all macroscale models are more complex than this example, in that they have multiple dimensions, lack explicit solutions in terms of standard mathematical functions, and must be understood from their differential forms.\"\n\nFigure 2. \"A basic algorithm applying the Euler method to an individual-based model. See text for discussion. The algorithm, represented in pseudocode, begins with invocation of procedure formula_16, which uses the data structures to carry out the simulation according to the numbered steps described at the right. It repeatedly invokes function formula_17, which returns its parameter perturbed by a random number drawn from a uniform distribution with standard deviation defined by the variable formula_18. (The square root of 12 appears because the standard deviation of a uniform distribution includes that factor.) Function formula_19 in the algorithm is assumed to return a uniformly distributed random number formula_20. The data are assumed to be reset to their initial values on each invocation of formula_16.\"\nFigure 3. \"Graphical comparison of the dynamics of macroscale and microscale simulations of Figures 1 and 2, respectively.\"\n"}
{"id": "32499653", "url": "https://en.wikipedia.org/wiki?curid=32499653", "title": "MonsterTalk", "text": "MonsterTalk\n\nMonsterTalk is an audio podcast presented by the Skeptics Society's \"Skeptic\" magazine. The show critically examines the science behind cryptozoological creatures, such as Bigfoot, the Loch Ness Monster, and werewolves. It is hosted by Blake Smith and Karen Stollznow, and produced by Blake Smith. In 2012, MonsterTalk was awarded the Parsec Award for the \"Best Fact Behind the Fiction Podcast\".\n\nMonstertalk was founded by Benjamin Radford, Blake Smith and Karen Stollznow. The first episode was released on July 2, 2009, and featured an interview with NYU Professor Todd Disotell about the attempted use of DNA evidence in cryptozoologists' search for Bigfoot.\n\nMonsterTalk interviews scientists and investigators that research cryptozoological claims. The show has covered an extensive array of monsters and cryptids, including zombies, demons, ninjas, the Mothman, Cthulhu, the Skookum cast, the Patterson–Gimlin film., the Minnesota Iceman, the Chupacabra; and extinct animals such as pterosaurs, plesiosaurs, and the thylacine.\n\nGuests of the show have included Phil Plait, Professor PZ Myers, Steven Novella, magician James Randi, author Brian Regal, writer and illustrator Daniel Loxton, and investigator Joe Nickell.\n\nIn 2011, Curt Holman conducted an interview with MonsterTalk's producer, Blake Smith, for an article in \"Creative Loafing\". Ghostly Talk radio interviewed the Monster Talk crew in 2009. Blake Smith and Karen Stollznow were also interviewed on the podcast/radio show \"Skeptically Speaking\" on May 7, 2010. MonsterTalk is rebroadcast on WPRR in Grand Rapids, Michigan.\n\nMonsterTalk was nominated for a Parsec Award in 2010 and again in 2011.\n\nMonsterTalk won the 2012 Parsec Award for \"Best Fact Behind the Fiction Podcast\", for programs that \"explore the facts that influence the fictions—the science, history, culture, and mythology that inspire these stories.\"\n\n"}
{"id": "56666421", "url": "https://en.wikipedia.org/wiki?curid=56666421", "title": "Occult fracture", "text": "Occult fracture\n\nAn occult fracture is a fracture that is not readily visible, generally in regard to projectional radiography (\"X-ray\"). Radiographically occult and subtle fractures are a diagnostic challenge. They may be divided into (1) \"high energy trauma fracture,\" (2) \"fatigue fracture\" from cyclical and sustained mechanical stress, and (3) \"insufficiency fracture\" occurring in weakened bone (e.g., in osteoporosis and postradiotherapy). Independently of the cause, the initial radiographic examination can be negative either because the findings seem normal or are too subtle. Advanced imaging tools such as computed tomography, magnetic resonance imaging, and scintigraphy are highly valuable in the early detection of these fractures.\n\nFractures represent up to 80% of the missed diagnoses in the emergency department. Failure to recognize the subtle signs of osseous injury is one of the reasons behind this major diagnostic challenge. While occult fractures present no radiographic findings, radiographically subtle fractures are easily overlooked on initial radiographs. In both cases, a negative radiographic diagnosis with prominent clinical suspicion of osseous injury will prompt advanced imaging examination such as computed tomography (CT), magnetic resonance imaging (MRI), ultrasound, and nuclear medicine to confirm or exclude the clinically suspected diagnosis. The burden entailed in missing these fractures includes prolonged pain with a loss of function, and disability. Early detection, on the other hand, enables more effective treatment, a shorter hospitalization period if necessary, and decreased medical costs in the long run. It will also prevent inherent complications such as nonunion, malunion, premature osteoarthritis, and avascular osteonecrosis (as in scaphoid fracture). Occult and subtle fractures may be divided into: (1) fractures associated with high energy trauma; fatigue fracture secondary to repetitive and unusual stress being applied to bone with normal elastic resistance; and insufficiency fracture resulting from normal or minimal stress on a bone with decreased elastic resistance. The term \"stress fracture\" is more general and encompasses both of the latter two entities.\n\nRadiographically occult and subtle fractures are often a challenging diagnostic problem in daily clinical practice. Radiologists should be aware of the different situations and mechanisms of these injuries as well as the subtle radiographic signs that can be encountered in each situation. The knowledge of normal images and the consideration of the clinical context are of great value in improving the detection of these fractures either on conventional radiographs or with more advanced imaging tools.\n\nThanks to rapid technological advancement, new and more efficient imaging hardware is constantly released for all imaging modalities including CT, MRI, nuclear medicine, and ultrasound. Nonetheless, not every department can afford all new technologies, and radiologists sometimes have to face the challenge of providing the highest diagnostic performance with basic imaging tools. This can only be achieved by ensuring high quality of examination with the available imaging tools.\n\nRadiography is the first step for detection of fractures. The detection of subtle signs of fracture requires a high standard for the acquisition technique and a thorough and systematic interpretation of radiographic images. Correct diagnosis primarily relies on the reader's experience. Awareness of normal anatomic features is crucial for the interpreter to be able to detect subtle signs of fracture. Fat pads should be carefully examined for convexity, which implies joint effusion (e.g., in the hip and elbow joints). However, the radiographic technique (positioning in particular) must be optimal for this evaluation to be valid. Osseous lines should be checked for integrity (e.g., acetabular rim in the hip). Trabecular angulation, impaction lines, and sclerotic bands also suggest fracture in osseous structures with a significant proportion of cancellous bone such as proximal femur.\n\nThe general rule is to perform two orthogonal views, but more specific views should be added if there is any suspicion of fracture. Moreover, one should be aware of the commonly encountered lesions and their locations. In wrist trauma, for instance, the interpreter should pay close attention to the scaphoid and triquetrum, which are the two most commonly injured carpal bones. The mechanism of trauma may also be helpful to locate the potential fracture. A fall on an outstretched hand suggests scaphoid fracture. Although the classical presentation consists of a radiolucent line and cortical disruption, the radiographic signs will depend upon the time elapsed between the first clinical symptoms and the time of radiographic examination, the location of the fracture within the bone, and the ratio of cortical to cancellous bone. Particular attention should be paid when analysing the subchondral plate, which may be disrupted or deformed. In metaphyseal areas, delayed signs of fracture include a band of sclerosis perpendicular to the trabeculae, while diaphyseal fractures may present as periosteal thickening.\n\nDigital radiography known as tomosynthesis has been shown to be superior to conventional radiographs in the detection of occult fracture of the scaphoid. Tomosynthesis has the ability to demonstrate cortical, as well as moderately displaced trabecular fractures. Thus, the performance of tomosynthesis in detecting radiographically occult fractures is considered as comparable to CT.\n\nMultidetector computed tomography (MDCT) is a highly valuable imaging tool for the diagnosis of occult fractures. CT has several advantages including short acquisition time (compared to MRI), the ability to acquire volumetric and isotropic image data sets, the opportunity to reconstruct multiplanar reformations in any arbitrary plane, and excellent spatial resolution. Moreover, the image quality for multiplanar reconstruction may be increased by reducing slice thickness and acquisition pitch. In general, bony structures are best demonstrated by using a small focal spot and using a \"bone\" algorithm. CT contributes much to the diagnosis of occult fractures by depicting subtle fracture lines, depressed, or distracted articular surfaces and by assessing bone loss It also detects late bony changes such as increased medullary density, endosteal sclerosis, sclerotic lines in trabecular bone, and periosteal thickening. Moreover, CT aids in excluding other differential diagnoses, especially in case of isolated bone marrow edema, by confirming the normal appearance of the remaining trabeculae and excluding space-occupying lesions such as malignancy and osteomyelitis.\n\nNewest generation of CT, such as dedicated cone-beam CT (CBCT) system for musculoskeletal extremities, may be beneficial in various conditions, such as arthritis and occult fractures. Although dedicated CBCT for musculoskeletal extremities is still a matter of research, it has been shown to be of potential benefit as an adjunct for CT and MRI. It offers the possibility of volumetric imaging, which may be helpful in case of suspected occult fractures. It also provides higher spatial resolution and a potentially reduced dose compared to CT.\n\nThe diagnostic performance of MRI in the detection of occult fractures has been shown to be comparable, or better than MDCT. Indeed, while the specificity of both CT and MRI for the diagnosis of fracture can be as high as 100%, the sensitivity was reported to be higher for MRI. The superiority of MRI over any other imaging modality including MDCT for the detection of occult hip fractures is now recognised. For instance, an occult intertrochanteric extension of a greater trochanter fracture can be most effectively appreciated on MRI Moreover, MRI is extremely helpful in detecting associated soft tissue abnormalities, especially ligamentous lesions. MRI is now considered as the standard in this context. However, because of its relative unavailability in emergency settings and high costs, MRI may only be performed in \"high risk patients\" with negative X-rays. For example, when a hip occult fracture is suspected, patients with reduced baseline mobility and pain on axial compression are considered at risk and, therefore, should be examined by MRI. MRI signs of occult fractures are evident several weeks before radiographic signs appear. In the hip, a limited and cost effective MR protocol, with only T1 weighted () coronal images, may enable a reliable diagnosis or exclusion of occult fracture in very little time, for example, 7 minutes. Typically, a linear hypointensity is observed on T1 W images. MRI is also highly sensitive to marrow abnormalities surrounding the fracture line, which appear as hypointensity on T1 W images and hyperintensity on fluid-sensitive sequences. Such signal changes are thought to be a combination of bone marrow edema, intraosseous haemorrhage, and/or granulation tissue and help to identify even undisplaced fractures. However, in the absence of a history of trauma and linear hypointensities on T1 W images, isolated bone marrow edema may represent other pathologies such as osteoid osteoma and sclerosing osteomyelitis.\n\nAlthough 1.5 T and 3 T MR is considered as the current gold standard for the detection of radiographically occult fractures, ultra-high field MR provide higher signal-to-noise ratio and, therefore, is expected to be superior to 1.5 T and 3 T. Ultra-high field MR seems to be promising in the diagnosis of a variety of musculoskeletal conditions including trauma, but it is not used in daily routine yet.\n\nThe most traditional method is bone scintigraphy. Although scintigraphy is highly sensitive for the detection of occult fracture, its lack of specificity limits its diagnostic utility. However, when MRI is not available, scintigraphy may be of value, especially in the absence of trauma history, for example, for detection of insufficiency and fatigue fractures. While radiography may show only late signs of bone reaction (such as periosteal thickening and band of sclerosis), scintigraphic examination allows for earlier detection of bony changes. Regarding Fluorine-18 2-deoxy-D-glucose (FDG) positron emission tomography (PET), it is critical to be aware that occult fractures may be responsible of marked metabolic uptake, and, thus, represent a potential false positive of metastatic disease. Integrated hybrid single-photon emission computerized tomography (SPECT)/CT combines the detection of abnormal bone metabolism with SPECT, to the precise anatomical detail provided by high resolution CT. For instance, SPECT/CT may be interesting in the detection of radiographic occult fractures of the wrist and other sport-related injuries.\n\nHigh frequency ultrasound has been shown to be of value, particularly in the pediatric population. In this case, and in an emergency setting, ultrasonography can be more accessible and less time consuming than radiographs and has high specificity and sensitivity in the evaluation of suspected long bone fractures. The utility of ultrasonography has also been shown for adults with suspicion of wrist trauma or fatigue/stress fracture. Recently, it has been suggested that therapeutic ultrasound may be beneficial as a primary evaluation of bone stress injury; however, its benefit seems to be more evident in selected high risk patients rather than general population.\n\nOccult osseous injuries may result from a direct blow to the bone by compressive forces of adjacent bones against one another or by traction forces during an avulsion injury. Lesions in the tibial plateau, hip, ankle, and wrist are often missed. In a tibial plateau fracture, any disruption of the posterior and anterior cortical rims of the plateau should be sought. Impaction of subchondral bone will appear as an increased sclerosis of the subchondral bone (Figure 1). In the hip, posterior acetabular fractures also present subtle radiographic findings. The acetabular lines should then be carefully examined keeping in mind that the posterior rim, which is harder to see on X-rays, is more frequently fractured than the anterior rim (Figure 2). In the wrist, detection of carpal bone fractures is often challenging, with up to 18% of scaphoid fractures radiographically occult. Carpal fractures, especially the scaphoid, are associated with the risk of avascular necrosis. In apparently normal wrist radiographs from symptomatic patients, if there is history of a fall on an outstretched hand with pain in the anatomic snuffbox, suggesting scaphoid injury, the initial examination with posteroanterior, lateral, and pronation oblique views must be complemented by other specific views such as supination oblique and the \"scaphoid\" view A careful examination of cortices for evidence of discontinuity or offset and cancellous bone for lucency is necessary (Figure 3).\n\nFigure 1: A 56-year-old woman presenting with left knee pain after a fall. (a) Initial anteroposterior radiograph was considered normal, however, subtle cortical disruption of the anterior rim of the medial tibial plateau, medial to the tibial spine, is noted (arrow). (b) Coronal T1-weighted MRI confirms the cortical disruption (arrow) and shows extensive fracture through the proximal tibia. (c) Coronal proton density-weighted image with fat saturation shows extensive edema in the subchondral bone. Note also hypersignal adjacent to the medial collateral ligament corresponding to a grade I sprain (arrowheads).\n\nFigure 3: A 26-year-old man presenting with wrist pain after being assaulted. (a) Initial anteroposterior radiograph shows a subtle linear lucency within the scaphoid extending to the scaphocapitate articular surface that was overlooked (arrow). (b) Initial \"scaphoid\" view was negative. (c) Followup anteroposterior radiographs, 12 days later, shows obvious scaphoid fracture (arrows).\n\nTriquetral fracture usually occurs on the dorsal aspect by impingement from the ulnar styloid or avulsion of strong ligamentous attachment. The dorsal avulsion fracture or \"chip fracture\" appears as a small bony fragment on the dorsal aspect of the triquetrum and is best detected on the lateral view(Figure 4). When radiography is negative in patients with high suspicion of a fracture, both MRI and MDCT will be of value. However, it has been shown that MRI is superior for detecting trabecular fractures in carpal bones.\n\nFigure 4: Dorsal triquetral fracture of the left wrist in a 30-year-old man after a trauma. (a) Anteroposterior radiograph shows a normal appearance. (b) Lateral radiograph of the same wrist demonstrates a chip fracture off the dorsal aspect of the triquetrum (arrow).\n\nThe greater tuberosity of the humerus is also an illustrative location of occult fractures. The osseous injury may follow seizures, glenohumeral dislocation, forced abduction, or direct impaction. They are commonly discovered on MRI in symptomatic patients with suspicion of rotator cuff tear. Coronal images are best suited for detection. They appear as crescentic oblique lines surrounded by a bone marrow edema pattern (Figure 5). The rotator cuff must be inspected since associated ligamentous lesions are common. In the ankle, malleoli and tarsal bones should be checked carefully for any cortical disruptions and radiolucent lines that may reveal a fracture. Awareness of the exact location of the pain will help direct the attention of the interpreter when searching for very subtle signs of fracture (Figure 6).\n\nFigure 6: Subtle anterior talar fracture in a 39-year-old man presenting with ankle pain after a fall. (a) Anteroposterior radiograph shows a subtle oblique radiolucent line through the talus (white arrows). (b) Sagittal CT reformation confirms the presence of an anterior talar fracture with cortical offset (black arrow).\nAvulsion fractures, which consist of a detached bone fragment resulting from a ligament or tendon pulling away from the bone, may also present with subtle radiographic signs. Tiny osseous fragments near the presumed attachment site of a ligament suggest this diagnosis. Common sites are the lateral tibial plateau (the Segond fracture), the spinal tuberosity of the tibia resulting from anterior cruciate ligament avulsion, and the ischial tuberosity.\n\nFatigue fractures occur when healthy bone is exposed to repeated stress. The bone is a living tissue, with the capacity to repair itself; fatigue fractures occur when repetitive injuries exceed the repair capacity of the bone. This type of fracture does not occur as a single event but rather incrementally as a sequence of cellular events that begin with increased osteoclastic activity. Microfractures occur later and are accompanied by bone marrow edema, which can be detected on MRI. This stage appears on MRI as an isolated bone marrow edema pattern without a fracture line and is called stress reaction. Then, periosteal new bone forms and may be visible on radiography. Full cortical fractures occur if the repetitive stress continues. Only timely detection and appropriate management can interrupt this sequence.\n\nFatigue fractures are more frequent in women which may be due to the relatively smaller bones of women. Moreover, pregnancy is a well-recognized risk factor for femoral neck fatigue fracture. While fibular and metatarsal fractures have a low risk of complications, other sites including the femoral neck, midanterior tibia, navicular, talar, and other intraarticular fractures are prone to complications such as delayed union, nonunion, and displacement. The site of the insufficiency fracture may be specific to the activity: for example, rugby and basketball players are more prone to navicular fractures, while gymnasts have a higher risk for talar fractures (Figure 7). Long distance runners are at increased risk for pelvic, tibial (Figures 8 and 9), and fibular fractures. In the military, calcaneus (Figure 10) and metatarsals are the most commonly cited injuries, especially in new recruits. Billiard players are at risk for upper limb fractures (Figure 11).\nFigure 7: Fatigue fracture of the talus in a 25-year-old male basketball player with right hind foot and ankle pain, without history of trauma, and a normal initial radiograph (not shown). (a) One-month followup lateral radiograph shows normal appearance. (b) Sagittal T1-weighted MRI shows an irregular fracture line (arrow) within an ill-defined area of hypointensity corresponding to bone marrow edema.\nFigure 8: Proximal diaphyseal fatigue fracture of the tibia in a 20-year-old man with a history of regular jogging. (a) Lateral radiograph shows no obvious fracture lines but a subtle localized medial tibial cortex periosteal reaction (arrows). (b) Sagittal reformatted CT image acquired 1-month after the radiograph shows a linear hypoattenuation in the tibial cortex (arrowhead), as well as obvious periosteal thickening (arrows). (c) Sagittal T2-weighted fat-saturated image acquired the same day shows an area of hyperintensity spreading over the proximal tibia (arrows), which is consistent with the presence of proximal tibial fracture.\nFigure 9: Proximal metaphyseal fatigue fracture of the tibia in a 27-year-old recent male military recruit. (a) Anteroposterior radiograph is within normal limits. (b) Coronal T1-weighted MR image shows a marked linear hypoattenuation along the medial tibial metaphysis (arrow) surrounded by diffuse hypointensity in keeping with posttraumatic edema.\nFigure 10: Calcaneal fatigue fracture in a 30-year-old male runner. Radiographs were normal (not shown). (a) Sagittal T1-weighted and (b) short tau inversion recovery images show a linear hypointensity (arrows) of calcaneal tuberosity within diffuse bone marrow edema, which appears as an ill-defined area of hyperintensity on a fluid sensitive pulse sequence (arrowheads).\nFigure 11: Stress fracture of the right radius in a 40-year-old man, a semiprofessional billiard player, with no history of trauma and complaining of pain of the right forearm for one month. (a) Anteroposterior radiograph shows medial radial cortex periosteal reaction (arrow) but no fracture line is seen. (b) Coronal reformatted CT depicts monocortical fracture line through the periosteal thickening (arrowheads). (c) Coronal T2-weighted fat-suppressed MRI shows intramedullary hyperintensity within the bone marrow (arrow) corresponding to bone marrow edema.\n\nRadiographic examination usually shows delayed signs of fracture up to 2 to 3 months after initial injury. In a bony region with a high proportion of cancellous bone (e.g., femoral neck), a fatigue fracture appears as an ill-defined transverse sclerotic band (in contact or close to the medial cortex), with a periosteal thickening appearing at a later stage. In case of continued stress, a fracture line through the thickened cortex and a region of sclerosis may be observed. MRI is of great value for early diagnosis and displaying bone marrow edema, while scintigraphy is useful for showing increased metabolic activity within the bone. However, MRI is preferred since scintigraphy lacks specificity. In case of isolated bone marrow edema in MRI without a fracture line, the diagnosis of fatigue fracture may be more complicated, and other conditions such as transient edema and osteoid osteoma need to be excluded. Additional imaging by CT is warranted in such cases.\n\nInsufficiency fractures occur in weakened bones. Although osteoporosis is a classic cause, other conditions resulting in bone demineralization are well-recognized risk factors. These include previous radiation therapy and chemotherapy, especially in a context of gynaecologic malignancy, chronic renal failure, chronic rheumatological diseases, and corticosteroid therapy. In long bones, chronic joint diseases such as rheumatoid arthritis are associated with angular deformity and flexion contraction, increasing the stress on the bone around the joints and, therefore, the risk of insufficiency fracture. Pelvic, sacral, and proximal femoral fractures are of increasing significance especially with the aging of the population.\nThe sacrum is usually masked by overlapping bowel gas in conventional radiographs, and the subtle radiographic findings are usually nondiagnostic and even misleading. The characteristic \"H\" pattern has been correlated with biomechanical models of patient activities. The vertical parasagittal planes correspond to the region of maximal stress during walking, while the horizontal fracture develops later, secondary to the loss of lateral support by parasagittal fractures. MRI is the primary imaging technique in this case, with the most common MRI pattern showing bone marrow edema and a fracture line (Figure 12). Coronal views are quite contributive in sacral fractures, allowing the detection of the horizontal component, especially with fluid-sensitive sequences. Although the sacrum is the most commonly involved, pelvic insufficiency fractures are often multiple, and other typical locations should be mentioned.\n\nProximal femoral fractures usually occur in osteoporotic patients, and their signs include subtle neck angulation, trabecular angulation, and subcapital impaction line. A frog-leg lateral view may be helpful if the greater trochanter is short enough. However, positioning can be difficult because of hip pain. In patients with strong suspicion of proximal femoral fracture and negative radiographs, MRI limited to coronal T1 W images and scintigraphy can be highly valuable (Figures 13 and 14). Such an option, with limited examination time, is cost-effective and allows reliable exclusion or confirmation of the diagnosis, preventing an unnecessary stay at the hospital or delayed treatment. Moreover, MRI helps to detect soft tissue abnormalities which are more frequently seen in femoral, acetabular, and pubic injuries than sacral lesions. Concomitant fractures are also frequently seen in typical pelvic sites.\nFigure 13: Partial osseous avulsion of the gluteal muscles at the greater trochanter in a 59-year-old man who presented with the right hip pain without a history of trauma. Lauenstein view and anteroposterior and radiographs (not shown) did not show an obvious fracture line or disruption of bony contours in the acetabulum or the right femoral neck. (a) Coronal T1-weighted MRI displays an incomplete fracture line extending partially from the greater trochanter (arrow). (b) Coronal short tau inversion recovery MRI shows heterogeneous hyperintensity in the same region (arrow) as well as hyperintensity within the gluteus medius and minimus muscles (arrowheads) consistent with tissue edema and hematoma.\nFigure 14: Subcapital insufficiency fracture in a 55-year-old man with a left hip pain without a history of trauma. Anteroposterior and Lauenstein view radiographs centered on the left hip do not show an obvious fracture line, but mild acetabular osteophytosis was noted consistent with hip osteoarthritis (not shown). (a) Coronal T1-weighted MRI shows a linear low-signal band through the femoral neck corresponding to a fracture line (arrowheads). (b) Bone scintigraphy shows focal uptake (arrow) corresponding to the fracture.\n"}
{"id": "254958", "url": "https://en.wikipedia.org/wiki?curid=254958", "title": "Operation Storax", "text": "Operation Storax\n\nOperation Storax was a series of 47 nuclear tests conducted by the United States in 1962-1963 at the Nevada Test Site. These tests followed the \"Operation Fishbowl\" series and preceded the \"Operation Roller Coaster\" series.\n"}
{"id": "29293", "url": "https://en.wikipedia.org/wiki?curid=29293", "title": "Optical spectrometer", "text": "Optical spectrometer\n\nAn optical spectrometer (spectrophotometer, spectrograph or spectroscope) is an instrument used to measure properties of light over a specific portion of the electromagnetic spectrum, typically used in spectroscopic analysis to identify materials. The variable measured is most often the light's intensity but could also, for instance, be the polarization state. The independent variable is usually the wavelength of the light or a unit directly proportional to the photon energy, such as reciprocal centimeters or electron volts, which has a reciprocal relationship to wavelength.\n\nA spectrometer is used in spectroscopy for producing spectral lines and measuring their wavelengths and intensities. Spectrometers may also operate over a wide range of non-optical wavelengths, from gamma rays and X-rays into the far infrared. If the instrument is designed to measure the spectrum in absolute units rather than relative units, then it is typically called a spectrophotometer. The majority of spectrophotometers are used in spectral regions near the visible spectrum.\n\nIn general, any particular instrument will operate over a small portion of this total range because of the different techniques used to measure different portions of the spectrum. Below optical frequencies (that is, at microwave and radio frequencies), the spectrum analyzer is a closely related electronic device.\n\nSpectrometers are used in many fields. For example, they are used in astronomy to analyze the radiation from astronomical objects and deduce chemical composition. The spectrometer uses a prism or a grating to spread the light from a distant object into a spectrum. This allows astronomers to detect many of the chemical elements by their characteristic spectral fingerprints. If the object is glowing by itself, it will show spectral lines caused by the glowing gas itself. These lines are named for the elements which cause them, such as the hydrogen alpha, beta, and gamma lines. Chemical compounds may also be identified by absorption. Typically these are dark bands in specific locations in the spectrum caused by energy being absorbed as light from other objects passes through a gas cloud. Much of our knowledge of the chemical makeup of the universe comes from spectra.\n\nSpectroscopes are often used in astronomy and some branches of chemistry. Early spectroscopes were simply prisms with graduations marking wavelengths of light. Modern spectroscopes generally use a diffraction grating, a movable slit, and some kind of photodetector, all automated and controlled by a computer.\n\nJoseph von Fraunhofer developed the first modern spectroscope by combining a prism, diffraction slit and telescope in a manner that increased the spectral resolution and was reproducible in other laboratories. Fraunhofer also went on to invent the first diffraction spectroscope. Gustav Robert Kirchhoff and Robert Bunsen discovered the application of spectroscopes to chemical analysis and used this approach to discover caesium and rubidium. Kirchhoff and Bunsen's analysis also enabled a chemical explanation of stellar spectra, including Fraunhofer lines.\n\nWhen a material is heated to incandescence it emits light that is characteristic of the atomic makeup of the material.\nParticular light frequencies give rise to sharply defined bands on the scale which can be thought of as fingerprints. For example, the element sodium has a very characteristic double yellow band known as the Sodium D-lines at 588.9950 and 589.5924 nanometers, the color of which will be familiar to anyone who has seen a low pressure sodium vapor lamp.\n\nIn the original spectroscope design in the early 19th century, light entered a slit and a collimating lens transformed the light into a thin beam of parallel rays. The light then passed through a prism (in hand-held spectroscopes, usually an Amici prism) that refracted the beam into a spectrum because different wavelengths were refracted different amounts due to dispersion. This image was then viewed through a tube with a scale that was transposed upon the spectral image, enabling its direct measurement.\n\nWith the development of photographic film, the more accurate spectrograph was created. It was based on the same principle as the spectroscope, but it had a camera in place of the viewing tube. In recent years, the electronic circuits built around the photomultiplier tube have replaced the camera, allowing real-time spectrographic analysis with far greater accuracy. Arrays of photosensors are also used in place of film in spectrographic systems. Such spectral analysis, or spectroscopy, has become an important scientific tool for analyzing the composition of unknown material and for studying astronomical phenomena and testing astronomical theories.\n\nIn modern spectrographs in the UV, visible, and near-IR spectral ranges, the spectrum is generally given in the form of photon number per unit wavelength (nm or μm), wavenumber (μm, cm), frequency (THz), or energy (eV), with the units indicated by the abscissa. In the mid- to far-IR, spectra are typically expressed in units of Watts per unit wavelength (μm) or wavenumber (cm). In many cases, the spectrum is displayed with the units left implied (such as \"digital counts\" per spectral channel).\n\nA spectrograph is an instrument that separates an incoming wave into a frequency spectrum. There are several kinds of machines referred to as \"spectrographs\", depending on the precise nature of the waves. The first spectrographs used photographic paper as the detector. The star spectral classification and discovery of the main sequence, Hubble's law and the Hubble sequence were all made with spectrographs that used photographic paper. The plant pigment phytochrome was discovered using a spectrograph that used living plants as the detector. More recent spectrographs use electronic detectors, such as CCDs which can be used for both visible and UV light. The exact choice of detector depends on the wavelengths of light to be recorded.\n\nA spectrograph is sometimes called polychromator, as an analogy to monochromator.\n\n"}
{"id": "16623268", "url": "https://en.wikipedia.org/wiki?curid=16623268", "title": "Parallel state", "text": "Parallel state\n\nThe \"parallel state\" is a term coined by American historian Robert Paxton to describe a collection of organizations or institutions that are state-like in their organization, management and structure, but they are not officially part of the legitimate state or government. They serve primarily to promote the prevailing political and social ideology of the state.\n\nThe parallel state differs from the more commonly used \"state within a state\" in that they are usually endorsed by the prevailing political elite of a country, while the \"state within a state\" is a pejorative term to describe state-like institutions that operate without the consent of and even to the detriment to the authority of an established state (such as churches and religious institutions or secret societies with their own laws and court systems). \n\nHowever, Turkey's former Prime Minister Recep Tayyip Erdoğan has used the term \"parallel state\" (or \"parallel structure\") to describe followers of Fethullah Gülen who occupy senior bureaucratic and judicial positions, which have been accused of attempting to bring down Erdoğan's government. This is in contrast to the meaning of the term initially coined by Paxton, and instead resembles the term \"state within a state.\" Gülen's Cemaat movement, which has a large presence within Turkey, has allegedly been involved in limiting the power of the Turkish Armed Forces through the Ergenekon trials and the Sledgehammer case while allied with Erdoğan. Following the 2013–14 protests against Erdoğan's government, the Cemaat Movement turned against Erdoğan, who thus labelled them as a \"parallel state.\" Erdoğan has blamed Gülen's followers on orchestrating the 2013 government corruption scandal, as well as the 2016 coup attempt.\n\nParallel states are common in totalitarian societies, such as Nazi Germany, Fascist Italy, the Soviet Union, and North Korea and are parties, youth organizations, leisure organizations, work/labor collectives, unions, and militias.\n\n\"Parallel States\" is also a study into the possibility of uniting one country while giving them two states parallel to each other in power and representation, both those states would be compliant however to one central-authority. This study was also suggested as a corner stone for possible peace scenarios in war torn countries.\n\n"}
{"id": "58502876", "url": "https://en.wikipedia.org/wiki?curid=58502876", "title": "Paula Arnold", "text": "Paula Arnold\n\nPaula (Kellner) Arnold (; 1885-1968) was an Austrian-born Israeli journalist, botanist, and naturalist noted for her works on the flora and birds of Israel.\n\n"}
{"id": "5618550", "url": "https://en.wikipedia.org/wiki?curid=5618550", "title": "Personal Rescue Enclosure", "text": "Personal Rescue Enclosure\n\nThe personal rescue enclosure (PRE) or \"rescue ball\" was a device for transporting astronauts from one Space Shuttle to another in case of an emergency. It was produced as a prototype but never flew on any missions.\n\nThe ball was 36 inches (86 cm) in diameter and had a volume of 0.33 cubic meters. The structure comprised three fabric layers and incorporated a window and a zipper to allow the astronaut to enter and exit the ball. The ball enabled one crew member to curl up inside and don an oxygen mask and hold a carbon dioxide scrubber/oxygen supply device with one hour worth of oxygen. The ball would have been connected by an umbilical to the shuttle to supply air until the airlock depressurized. The rescue ball containing the crew member would have been carried to the rescue shuttle by a space suited astronaut.\n\nThe PRE was designed to protect humans in space in the event of an emergency where not enough full space suits were available. It was developed in the 1970s and 1980s to support the Space Shuttle program. The PRE was designed to be used in conjunction with a fully suited astronaut that would provide mobility to the person in the ball. The ball's life-support systems consisting of oxygen and a CO scrubber could support a person for about an hour.\n\nThe life support system that supplied oxygen was called the Personal Oxygen Supply, or alternatively it could be supplied with oxygen from an external source after being sealed. The ball was made of fabric, and was sealed by way of zippers, with a small circular window to allow the occupant to see out.\n\nNASA evaluated three methods of transporting the balls:\n\nDimensions:\n\n"}
{"id": "23963", "url": "https://en.wikipedia.org/wiki?curid=23963", "title": "Phenetics", "text": "Phenetics\n\nIn biology, phenetics ( - to appear) , also known as taximetrics, is an attempt to classify organisms based on overall similarity, usually in morphology or other observable traits, regardless of their phylogeny or evolutionary relation. It is closely related to numerical taxonomy which is concerned with the use of numerical methods for taxonomic classification. Many people contributed to the development of phenetics, but the most influential were Peter Sneath and Robert R. Sokal. Their books are still primary references for this sub-discipline, although now out of print.\n\nPhenetics has largely been superseded by cladistics for research into evolutionary relationships among species. However, certain phenetic methods, such as neighbor-joining, have found their way into phylogenetics, as a reasonable approximation of phylogeny when more advanced methods (such as Bayesian inference) are too computationally expensive.\n\nPhenetic techniques include various forms of clustering and ordination. These are sophisticated ways of reducing the variation displayed by organisms to a manageable level. In practice this means measuring dozens of variables, and then presenting them as two- or three-dimensional graphs. Much of the technical challenge in phenetics revolves around balancing the loss of information in such a reduction against the ease of interpreting the resulting graphs.\n\nThe method can be traced back to 1763 and Michel Adanson (in his \"Familles des plantes\") because of two shared basic principles — overall similarity and equal weighting — and modern pheneticists are sometimes called neo-Adansonians.\n\nPhenetic analyses are unrooted, that is, they do not distinguish between plesiomorphies, traits that are inherited from an ancestor, and apomorphies, traits that evolved anew in one or several lineages. A common problem with phenetic analysis is that basal evolutionary grades, which retain many plesiomorphies compared to more advanced lineages, appear to be monophyletic. Phenetic analyses are also liable to be misled by convergent evolution and adaptive radiation. Cladistic methods have attempted to solve those problems.\n\nConsider for example songbirds. These can be divided into two groups – Corvida, which retains ancient characters in phenotype and genotype, and Passerida, which has more modern traits. But only the latter are a group of closest relatives; the former are numerous independent and ancient lineages which are about as distantly related to each other as each single one of them is to the Passerida. In a phenetic analysis, the large degree of overall similarity found among the Corvida will make them appear to be monophyletic too, but their shared traits were present in the ancestors of \"all\" songbirds already. It is the loss of these ancestral traits rather than their presence that signifies which songbirds are more closely related to each other than to other songbirds. However, the requirement that taxa be monophyletic – rather than paraphyletic as in the case of the Corvida – is itself part of the cladistic view of Taxonomy, not necessarily followed to an absolute degree by other schools.\n\nThe two methodologies are not mutually exclusive. There is no reason why, e.g., species identified using phenetics cannot subsequently be subjected to cladistic analysis, to determine their evolutionary relationships. Phenetic methods can also be superior to cladistics when only the \"distinctness\" of related taxa is important, as the computational requirements are lower.\n\nThe history of pheneticism and cladism as rival taxonomic systems is analysed in David Hull's 1988 book \"Science as a Process\".\n\nTraditionally there was a great deal of heated debate between pheneticists and cladists, as both methods were initially proposed to resolve evolutionary relationships. Perhaps the \"high-water mark\" of phenetics were the DNA-DNA hybridization studies by Charles G. Sibley, Jon E. Ahlquist and Burt L. Monroe Jr., from which resulted the 1990 Sibley-Ahlquist taxonomy for birds. Highly controversial at its time, some of its findings (e.g. the Galloanserae) have been vindicated, while others (e.g. the all-inclusive \"Ciconiiformes\" or the \"Corvida\") have been rejected. However, with computers growing increasingly powerful and widespread, more refined cladistic algorithms became available and could put the suggestions of Willi Hennig to the test; as it turned out, the results of cladistic analyses turned out to be superior to those of phenetic methods – at least when it came to resolving phylogenies.\n\nMany systematists continue to use phenetic methods, particularly in addressing species-level questions. While a major goal of taxonomy remains describing the 'tree of life' – the evolutionary path connecting all species – in fieldwork one needs to be able to separate one taxon from another. Classifying diverse groups of closely related organisms that differ very subtly is difficult using a cladistic approach. Phenetics provides numerical tools for examining overall patterns of variation, allowing researchers to identify discrete groups that can be classified as species.\n\nModern applications of phenetics are common in botany, and some examples can be found in most issues of the journal \"Systematic Botany\". Indeed, due to the effects of horizontal gene transfer, polyploid complexes and other peculiarities of plant genomics, phenetic techniques in botany – though less informative altogether – may, in these special cases, be less prone to errors compared with cladistic analysis of DNA sequences.\n\nIn addition, many of the techniques developed by phenetic taxonomists have been adopted and extended by community ecologists, due to a similar need to deal with large amounts of data.\n\n"}
{"id": "1548441", "url": "https://en.wikipedia.org/wiki?curid=1548441", "title": "Plastics engineering", "text": "Plastics engineering\n\nPlastics engineering encompasses the processing, design, development, and manufacture of plastics products. A plastic is a polymeric material that is in a semi-liquid state, having the property of plasticity and exhibiting flow. Plastics engineering encompasses plastics material and plastic machinery. Plastic Machinery is the general term for all types of machinery and devices used in the plastics processing industry. The nature of plastic materials poses unique challenges to an engineer. Mechanical properties of plastics are often difficult to quantify, and the plastics engineer has to design a product that meets certain specifications while keeping costs to a minimum. Other properties that the plastics engineer has to address include: outdoor weatherability, thermal properties such as upper use temperature, electrical properties, barrier properties, and resistance to chemical attack.\n\nIn plastics engineering, as in most engineering disciplines, the economics of a product plays an important role. The cost of plastic materials ranges from the cheapest commodity plastics used in mass-produced consumer products to the very expensive, specialty plastics. The cost of a plastic product is measured in different ways, and the absolute cost of a plastic material is difficult to ascertain. Cost is often measured in price per pound of material, or price per unit volume of material. In many cases however, it is important for a product to meet certain specifications, and cost could then be measured in price per unit of a property. Price with respect to processibility is often important, as some materials need to be processed at very high temperatures, increasing the amount of cooling time a part needs. In a large production run cooling time is very expensive.\n\nSome plastics are manufactured from re-cycled materials but their use in engineering tends to be limited because the consistency of formulation and their physical properties tend to be less consistent. Electrical and electronic equipment and motor vehicle markets together accounted for 58 percent of engineered plastics demand in 2003. Engineered plastics demand in the US was estimated at $9,702 million in 2007.\n\nA big challenge for plastics engineers is the reduction of the ecological footprints of their products. First attempts like the Vinyloop process can guarantee that a product's primary energy demand is 46 percent lower than conventional produced PVC. The global warming potential is 39 percent lower. \n\n\n"}
{"id": "34447124", "url": "https://en.wikipedia.org/wiki?curid=34447124", "title": "Quantum ESPRESSO", "text": "Quantum ESPRESSO\n\nQuantum ESPRESSO is a software suite for ab initio quantum chemistry methods of electronic-structure calculation and materials modeling, distributed for free under the GNU General Public License. It is based on Density Functional Theory, plane wave basis sets, and pseudopotentials (both norm-conserving and ultrasoft). ESPRESSO is an acronym for opEn-Source Package for Research in Electronic Structure, Simulation, and Optimization.\nThe core plane wave DFT functions of QE are provided by the PWscf component, PWscf previously existed as an independent project. PWscf (Plane-Wave Self-Consistent Field) is a set of programs for electronic structure calculations within density functional theory and density functional perturbation theory, using plane wave basis sets and pseudopotentials. The software is released under the GNU General Public License.\n\nThe latest version QE-6.3.0 was released on 5 July 2018.\n\nQuantum ESPRESSO is an open initiative, of the CNR-IOM DEMOCRITOS National Simulation Center in Trieste (Italy) and its partners, in collaboration with different centers worldwide such as MIT, Princeton University, the University of Minnesota or the Ecole Polytechnique Fédérale de Lausanne. The project is coordinated by the QUANTUM ESPRESSO foundation which is formed by many research centers and groups all over the world. The first version called \"pw.1.0.0\", was released on 15-06-2001.\n\nThe program, written mainly in fortran-90 with some parts in C or in Fortran-77, was built out of the merging and re-engineering of different independently-developed core packages, plus a set of packages, designed to be inter-operable with the core components, which allow the performance of more advanced tasks. \n\nThe basic packages include \"Pwscf\" which solves the self-consistent Kohn and Sham equations, obtained for a periodic solid, \"CP\" to carry out Car-Parrinello molecular dynamics, and \"PostProc\", which allows data analysis and plotting. Regarding the additional packages, is noteworthy to point out \"atomic\" for the pseudopotential generation, \"PHonon package, w\"hich implements density-functional perturbation theory (DFPT) for the calculation of second- and third-order derivatives of the energy with respect to atomic displacements and \"NEB\": for the calculation of reaction pathways and energy barriers.\n\nThe different tasks that can be performed include\n\nThe main components of the QUANTUM ESPRESSO distribution are designed to exploit the architecture of today’s supercomputers characterized by multiple levels and layers of inter-processor communication . The parallelization is achieved using both MPI and OpenMP parallelization, allowing the main codes of the distribution to run in parallel on most or all parallel machines with very good performance.\n\n\n"}
{"id": "4282259", "url": "https://en.wikipedia.org/wiki?curid=4282259", "title": "Secrets and Lies (book)", "text": "Secrets and Lies (book)\n\nSecrets and Lies, subtitled \"the anatomy of an anti-environmental PR campaign\", is a 1999 book by Nicky Hager and Bob Burton.\n\nThe book documents the public relations information put out by Timberlands West Coast Limited in order to win public support for logging of native forests on the West Coast of New Zealand.\n\nThe material is based on a large amount of documentation leaked by a staff member from the local branch of Shandwick (now Weber Shandwick Worldwide), a global public relations company, which had been hired by Timberlands to run a secret campaign against environmental groups such as Native Forest Action between 1997 and 1999.\n\nThe book describes its tactics of surveillance of meetings, monitoring the press and responding to every letter to the editor, greenwashing, the use of SLAPPs, cleaning anti-logging graffiti and blotting out campaign posters in public places, and managing to install its pro-logging educational materials into schools.\n\nThe book alleges that almost every pro-logging letter or article was organized by this campaign.\n\nIn 2000, a press council complaint was made against a letter to the editor in \"The Press\", which argued that Hager had lied in the book. The complaint was not upheld, because the Press Council ruled that it was responsible for vetting robust debate in the letter pages.\n\nDuring a general Parliamentary debate in November 2006, when the book \"The Hollow Men\" had an injunction against its publication, the MP Gerry Brownlee said of the author and the book:\n\n\"The Hollow Men\" documents behind the scenes activities of the National Party, of which Brownlee was deputy leader at the time.\n\nIn 2009, Kerry Tankard looked back at the book in a review for \"Salient\". She concluded that: \"As a study of how PR firms help corporations to spin and manipulate public opinion, I’ve seen none better.\"\n\n"}
{"id": "14682521", "url": "https://en.wikipedia.org/wiki?curid=14682521", "title": "Somatic antigen", "text": "Somatic antigen\n\nA somatic antigen is an antigen located in the cell wall of a gram-positive or gram-negative bacterium.\n"}
{"id": "14848284", "url": "https://en.wikipedia.org/wiki?curid=14848284", "title": "Table of volume of distribution for drugs", "text": "Table of volume of distribution for drugs\n\nThis is a table of volume of distribution (V) for various medication. For comparison, those with a V L/kg body weight of less than 0.2 are mainly distributed in blood plasma, 0.2-0.7 mostly in the extracellular fluid and those with more than 0.7 are distributed throughout total body water.\n"}
{"id": "29990850", "url": "https://en.wikipedia.org/wiki?curid=29990850", "title": "Technological and industrial history of 20th-century Canada", "text": "Technological and industrial history of 20th-century Canada\n\nThe technological and industrial history of Canada encompasses the country's development in the areas of transportation, communication, energy, materials, public works, public services (health care), domestic/consumer and defence technologies.\n\nThe terms chosen for the \"age\" described below are both literal and metaphorical. They describe the technology that dominated the period of time in question but are also representative of a large number of other technologies introduced during the same period. Also of note is the fact that the period of diffusion of a technology can begin modestly and can extend well beyond the \"age\" of its introduction. To maintain continuity, the treatment of its diffusion is dealt with in the context of its dominant \"age\".\n\nTechnology is a major cultural determinant, no less important in shaping human lives than philosophy, religion, social organization, or political systems. In the broadest sense, these forces are also aspects of technology. The French sociologist Jacques Ellul defined \"la technique\" as the totality of all rational methods in every field of human activity so that, for example, education, law, sports, propaganda, and the social sciences are all technologies in that sense. At the other end of the scale, common parlance limits the term's meaning to specific industrial arts.\n\nMetal mining also became significant industry during this period. The International Nickel Company (Inco) was established in 1902 through the fusion of two companies. A refinery using the Orford process was built in Port Colborne, Ontario in 1918 and then moved to Copper Cliff, Ontario, where that technique was replaced by the matte flotation process in 1948. Hard rock gold mining became practical in 1887, with the development of the potassium cyanidation process, by Scott MacArthur, which was used to separate the gold from the ore. This technique was first used in Canada at the Mikado Mine in the Lake-of-the-Woods Region again made accessible by the CPR. The CPR also provided access to the B.C. interior, where lead, copper, silver and gold ores had been discovered in the Rossland area in 1891. The ores were transported to Trail, B.C., where they were roasted. After CPR built the Crowsnest Pass it purchased the Trail roasting facility and in 1899 built a blast furnace to smelt lead ore. In 1902 the first electrolytic lead refining plant using the Betts Cell Process began operation in Trail. The Consolidated Mining and Smelting Company of Canada Ltd. was founded as a CPR subsidiary and began to develop the Sullivan Mine with its lead, zinc and silver ores, in Kimberley in 1909.\n\nBy 1912 the Dominion Coal Company produced 40% of Canada’s total coal output.\n\nIn 1904 a company in Bowmanville, Ontario, began Canada’s first powdered milk production operation. The large-scale home delivery of milk began in Toronto, Ottawa and Montreal in 1900.\n\nRailway and locomotive construction in the latter 19th century created a huge demand for steel. The Bessemer furnace at the Algom steel mill in Sault Ste. Marie, Ontario went into operation in 1902. The Montreal Rolling Mills Co, The Hamilton Steel and Iron Company, the Canada Screw Company, the Canada Bolt and Nut Company, and the Dominion Wire Manufacturing Company were consolidated in 1910 to form The Steel Company of Canada headquartered in Toronto. With mills located in Hamilton and other cities, it was the largest producer of steel in Canada for most of the century. Its competitor, the Dominion Steel Castings Company Limited founded in 1912, renamed the Dominion Foundries and Steel Company in 1917 and Dofasco in 1980, had its Hamilton facilities located next to those of Stelco.\n\nAt the turn of the 20th century, a number of rim-jobs exceeded at an alarming rate, although it is concerning the government had much to worry about. Health concerns were identified, and its use was generally discontinued by the late 20th century.\n\nThe modern version of plywood was invented in the US in 1905 in Portland, Oregon. In 1913, the Fraser Mills in New Westminster, British Columbia, produced the first Canadian plywood, primarily from Douglas fir. This new material eventually found use in a wide variety of structures, including auto running boards, panelling, sub-floors, roof sheathing, wall sheathing, shipping crates and, during World War II, the manufacturing of aircraft and small ships.\nThe pulp and paper industry also developed during these years. The closely related sulphate pulp process was introduced in Canada in 1907, when the Brompton Pulp & Paper Company began operation in East Angus, Quebec. This process dominates the industry to this day. The pulp slurry was fed in a continuous stream into a paper-making machine that flattened, pressed and dried it into newsprint on huge rolls many metres wide and containing thousands of meters of paper.\n\nBusiness and public administration was improved and simplified with the introduction of the typewriter, which acquired a familiar standardized form by about 1910, which features the \"qwerty\" keyboard, the typebar, ribbon, cylinder and carriage return lever. Popular models in Canada were manufactured by the US Remington and Underwood companies, among others. The introduction of the mechanical desk calculator complemented that of the typewriter. Most machines used in Canada were manufactured in the US by companies such as Friden, Monroe, and SCM/Marchant. The Gestetner copy machine, which used the stencil technique to reproduce copies of documents, was invented in England in 1881 by David Gestetner and quickly became popular in offices around the world, including those in Canada.\n\nNotable works of civil engineering realized the completion of: the New Westminster Bridge, Vancouver 1904, the Lethbridge Viaduct, Lethbridge, Alberta, 1909, the Spiral Tunnels, Hector to Field BC, 1909, the St. Andrew's Lock and Dam, Lockport, Manitoba, 1910, the Brooks Aqueduct, Brooks, Albert, 1914, the Quebec Bridge, Ste-Foy, Quebec, 1916, the Connaught Tunnel, Rogers Pass, BC, 1916, the Ogden Point Breakwater and Docks, Victoria, British Columbia, 1917, the Prince Edward Viaduct, Toronto, Ontario, 1919, the Shoal Lake Aqueduct, Winnipeg, Manitoba, 1919 and the Trent-Severn Waterway, Ontario, 1920.\n\nIn the 1930s diesel-powered excavation shovels replaced steam shovels for the excavation of railway right-of-ways and the digging of basements and foundations for skyscrapers and domestic housing, in the late 19th century.\n\nIt was the age of the skyscraper and the race to build the tallest structure in the British Empire set off a competition among cities across Canada. Successive record holders included the Traders Bank of Canada, 15 floors, Yonge St, Toronto, 1905, the Dominion Building, 13 floors, Vancouver, 1910, World (Sun) Tower, 17 floors, Vancouver, 1912, the Canadian Pacific Building, 16 floors, Toronto, 1913, the Royal Bank, 20 floors, Toronto, 1915, the Royal Bank, Montreal, 1928, the Royal York Hotel, Toronto, 1929 and the Canadian Imperial Bank of Commerce, Toronto, in 1931.\n\nThe construction of skyscrapers, grand hotels and other large buildings led to the development of central heating, an essential feature in Canada's cold climate. In the 20th century such systems were used to provide heat to small communities such as university campuses, northern industrial towns or military bases. Smaller systems were used in private homes. Another technique, the convection method, was introduced to domestic dwellings at this time. A metal furnace in the basement, using wood or coal as fuel, would heat air in a plenum which would rise by convection through a series of metal ducts into the rooms of the house above. When the air cooled it would fall to the floor and return to the plenum through another series of metal return ducts. In later years an electric fan was used to force the hot air from the plenum through the ducts.\n\nThe introduction of the flush toilet in the US and Canada in the 1880s created a market that inspired the invention of rolled toilet paper. The product was first produced in the US by the Albany Perforated Wrapping Paper Company in 1877. The US Scott Paper Company began manufacturing toilette paper in 1902 and by 1925 Scott Paper was the largest manufacturer of toilette paper in the world. As early as 1926 the Purex brand had been established in Canada and with the arrival of Scott Paper Canada in 1927 the White Swan brand was introduced.\n\nThe introduction of medical x-rays during this period dramatically improved medical diagnostics. Discovered by Roentgen in Germany in 1895, x-ray units were in operation at the Toronto and Montreal General Hospitals by the turn of the century. The sphygmomanometer or blood pressure meter, that familiar device employing a cuff placed around the patients arm, found its way into the office of most Canadian doctors in the early 20th century. The spread of bovine tuberculosis, a crippling childhood disease, was curbed through the introduction of pasteurized milk in Montreal and Toronto at the turn of the century. This practice was soon followed by the dairy industry across Canada.\n\nBayer began marketing the wonder drug of the age, Aspirin, in 1899. It was an instant success and quickly became popular in Canada. Originally sold as a powder, the tablet was introduced in 1914. A very important step in the mass production of medical products was taken that same year when Dr. John Fitzgerald founded an institution that would be named the Connaught Laboratories in 1917, at the University of Toronto. Initially the laboratories produced vaccines and antitoxins for smallpox, tetanus, diphtheria and rabies. In 1922 after the Nobel Prize winning work on Dr. Banting and Dr. Best the facility began to manufacture insulin.\n\nIn 1914 Dr. John Fitzgerald established laboratories in Toronto to produce vaccines for smallpox, rabies, diphtheria and tetanus. The facility was named the Connaught laboratories in 1917 in honour of Prince Albert, the Duke of Connaught the recently retired Governor General. Beginning in 1922 the laboratories began to mass-produce the newly discovered hormone insulin.\n\nThe chlorination of municipal drinking water, a technique known to kill bacteria and thus make the water safer for human consumption, was introduced in Toronto in 1910. It became widely used across Canada in the years that followed.\n\nThe 13- and 18-pound muzzle-loading gun field gun with modern recoil and sighting systems were acquired at the turn of the century. A notable acquisition was the first breech-loading gun, in Canadian use, the 13-pound quick-firing (Q.F.) and 18-pound Q.F. firing shrapnel and high-explosive rounds, in 1905. The Royal Canadian Navy founded in 1910, took possession of its first ships, two tired steel-hulled former Royal Navy cruisers, the \"Rainbow\", in 1910, stationed at Esquimalt on the west coast and the \"Niobe\" at Halifax on the east coast.\n\nDuring the post-World War I era, a plethora of technologies were introduced, including the car, air service, air navigation, paved roads, radio, the telephone, refrigeration, wonder drugs and powered farming, mining and forestry equipment.\n\nThe Ford Motor Company of Canada, founded in Windsor, Ontario in 1904, was the first major company to introduce the automobile to Canada. It manufactured cars in that city and was the first company to use the assembly line manufacturing technique in Canada. Facilities were established in the McGregor wagon factory in Walkerville (now part of Windsor), where the first vehicle off the line was a Model C. Production of the Model T was introduced in 1909 and by 1913 the company was manufacturing motors, the first internal combustion engines built in Canada, at the Windsor plant. A number of different types, all based on US designs, were manufactured including, the Ford Model-A, Ford Model-C, Ford Model-K, Ford Model-N and Ford Model-T. During WWII, the Canadian Military Pattern Truck, was built there. Following the war, the Ford Meteor was assembled until production moved to the new Ford plant in Oakville in 1953, where production has continued until this day.\n\nIn 1918 the McLaughlin Motor Company, Ltd. of Oshawa, Ontario and the Chevrolet Motor Company of Canada Ltd. merged to form General Motors of Canada and became a subsidiary of the US-owned General Motors Corporation. The company manufactured Buicks, Oldsmobiles and Oaklands on its assembly line in Oshawa.\nChrysler Canada, established in Windsor, began vehicle assembly in that city in 1936. Studebaker Canada Ltd. manufactured cars and trucks at a plant in Hamilton, Ontario from 1947 to 1966.\n\nThe automobile was a hit with Canadians. In 1904 there were 535 cars in Ontario, by 1913 there were 50,000 in Canada, by 1916, 123,000, by 1922, 513,000 and by 1930, 1,076,000. Of note was Thomas Wilby's Trans-Canada road trip, the first by automobile across Canada, from Halifax to Victoria, in 1912, on a series of highways that became known as the All Red Route. As the car gained in popularity local automobile clubs were founded. In 1913 nine of these clubs from across the country got together to form the Canadian Automobile Association.\n\nCars required gasoline, and the first service station in Canada was built in Vancouver on Smythe Street in 1907. Most early stations were informal curb-side affairs, and it was not until the twenties that the filling station as we know it began to appear, with Imperial Oil building architect-designed stations for its customers. By 1928 Imperial had evolved three standard filling station designs for different locations: business district, urban residential and small town/leased property. In the 1920s, gasoline itself was modified by the addition of tetraethyllead to reduce premature detonation of the gas-air mixture in the cylinder, commonly described as knock, in internal combustion engines. Both health and environmental problems would later become associated with leaded gasoline.\n\nThe popularity of the car also had a dramatic impact on urban infrastructure and roads in particular. The dirt, gravel, tar and occasionally cobblestone that characterized most city roads was inadequate for the automobile and towns and cities and provinces across Canada began paving projects creating roads of asphalt and concrete that were more suitable. The traffic light was also introduced to help regulate the congestion that began to arise in the twenties especially in larger cities like Toronto, Montreal and Vancouver. The first in Canada was installed at the corner of Bloor St. and Yonge St. in Toronto in 1925. The operation of cars on the newly paved roads in a snowy climate necessitated the use of a technology for keeping roads clear for cars in winter. Municipalities and provinces acquired snowplows, fleets of trucks with steel blades attached to the front bumper to clear city and provincial roads. The use of crushed rock salt for melting snow and ice on roads was also introduced during this period. The technique was effective but unfortunately proved to be very corrosive to steel and concrete. This had serious consequences for the undercarriage of the steel vehicles that used the roads in winter as well as the roads themselves along with bridges and parking lots.\n\nThe popularity of the car in urban areas also lead to the introduction of the parking meter in the centre of most urban areas. Invented in the US in 1935 it progressively found its way onto city streets across Canada in the years that followed.\n\nThe car began to compete with the streetcar in the thirties and forties and many cities reduced or abandoned this service. New suburbs were built without streetcar lines and urban diesel powered buses were used to provide public transport. Only a handful of cities continued to maintain streetcar service into the fifties and beyond, most notably Toronto which to this day has a very elaborate public streetcar network.\n\nThe auto-craze gave rise to a booming do-it-yourself car maintenance and repair movement with businesses specializing in car parts and tools becoming popular. One of the notable firm in this field, the familiar, Canadian Tire, began operations in Toronto in 1922 and has become one of Canada's largest retailers.\n\nLong distance travel by aircraft became increasingly important and practical in the postwar years. Taking off in a Vickers Vimy IV bomber from Lester's Field in St. Johns, Newfoundland on 14 June 1919, John Alcock and Arthur Whitten Brown, (Alcock and Brown) made the first trans-Atlantic flight, crash landing in Ireland 16 hours later. The first cross-Canada flight began in Halifax on 7 October 1920 and ended in Vancouver ten days later.\n\nIn the twenties and thirties the Canadian north was developed with the help of hundreds of small float equipped \"bush planes\" used to fly men and supplies to mining, forestry, trapping and fishing camps. The first commercial air passenger flight in Canada was made in 1920, when two bush pilots flew a fur buyer from Winnipeg to The Pas, Manitoba. National passenger air service was introduced by Trans-Canada Airlines beginning in 1937 and Canadian Pacific Airlines starting in 1942.\n\nOf note was the attempt by Britain to establish an airship service between that country and Canada and a related test flight by the British built dirigible the R-100 was made in July 1930. After a successful crossing of the Atlantic the giant craft moored at a mast especially constructed for that purpose at St. Hubert near Montreal. The ship flew on to Toronto before finally returning to Britain. However, technical problems with the craft prevented further flights and the idea of a Trans-Atlantic lighter-than-air passenger service was abandoned.\n\nTo facilitate the development of a national aviation service the Government of Canada created a kind of national highway in the sky called the Trans-Canada Airway consisting of airports, radio and weather services and lighting for night flying, at various locations across Canada. Construction started in 1929 but was slowed by the depression. The western leg from Vancouver to Winnipeg was completed in 1938. The section from Winnipeg to Toronto and Montreal was inaugurated in 1939 and the extensions to Moncton, Halifax and St. John's completed in 1940, 1941 and 1942 respectively.\n\nIn 1901, Guglielmo Marconi sent radio signals across the Atlantic Ocean. He established a machine to produce electromagnetic waves at Cornwall in England and a machine to detect these waves at Signal Hill in St. John's Newfoundland. On 12 December 1901 he announced that he had received the transmission of waves sent by the transmitter in England at the station in St John’s. \nIn Montreal, in 1920, XWA (CINW (AM)) became the first commercial AM radio broadcaster in the world. Both the AM transmitter and receiver used analog technology. The following year CKAC became the first French- language AM radio broadcaster in Canada. State operated national radio broadcasting chains were established beginning in the late 1920s, including the CNR National Radio Network, 1927, the Canadian Radio Broadcasting Commission Radio Network, 1932 and the Canadian Broadcasting Corporation Radio Network, 1936. Private independent AM broadcast operations sprouted like mushrooms in cities large and small across Canada during the thirties and forties. Canadian Marconi Company (CMC Electronics) formed in Montreal in 1903 and Northern Electric, manufactured radios for home use, the first mass-produced electronic equipment in Canada. The circuits of these devises were based on analog technology.\n\nThe teleprinter became a popular technology with telegraph companies beginning in 1922. When used with the telegraph system it permitted the automated printing of thousands of telegraph messages and became the backbone of the telegram service offered by the Canadian National Telegraph Company formed in 1920 and the Canadian Pacific Telegraph Company.\n\nThe wirephoto, was introduced in the US by Associated Press in 1935. This technology permitted the transmission of a photograph by use of telephone wires and became widely used by newspapers for reporting purposes. The technology was quickly introduced to Canada by Canadian Press (1917), which provided the service to newspapers across the country. Canadian Press also became the exclusive provider of Canadian wirephotos for Associated Press.\n\nThe Canadian film industry experienced mixed success during the twenties and thirties. Film maker Ernest Shipman produced five features between 1920 and 1923 before meeting with financial failure. The successful Canadian-owned Allen Theatre chain attained an important place in the exhibition market before being taken over by Famous Players Canadian Corporation (Cineplex Entertainment) in 1923. The technology of the talking cinema or \"talkies\" was introduced to Canada in 1927 by that company to take advantage of the arrival of talking films produced in Hollywood. The first Canadian \"talkie\" was \"The Viking\", an adventure story about sealing off the coast of Newfoundland, produced in 1931.\n\nAssociated Screen News of Canada in Montreal produced two notable newsreel series, \"Kinograms\" in the twenties and \"Canadian Cameo\" from 1932 to 1953. The regular production of short films by the newly created Canadian Government Motion Picture Bureau began in the 1930s. British law encouraging filmmaking in the Commonwealth led Hollywood to circumvent the spirit of the concept by establishing film production companies to make American films in Calgary, Toronto, Montreal and Victoria. These companies produced a small number of features but closed operations when the British law was changed to exclude their films. In 1941, Odeon Theatres of Canada opened a new cinema chain to compete with Famous Players.\n\nThe making of documentary films grew tremendously during World War II with the creation of the National Film Board of Canada in 1939. By 1945 it was one of the major film production studios in the world with a staff of nearly 800 and over 500 films to its credit, including the very popular \"The World in Action\" and \"Canada Carries On\" series of monthly propaganda films.\n\nPlastics were also introduced during these years. In Toronto, Plastics Ltd. began to produce Bakelite soon after its invention in 1909. Another firm, Canadian Electro Products of Shawinigan, Quebec, invented polyvinyl acetate which was used in copolymer resins and water-based paints. The wartime production of nitrocellulose naturally led to the manufacture at Shawinigan in 1932, of transparent cellulose film used for packaging. What is now called fibreglass was invented in the US in 1938 at Owens-Corning by Russell Games Slayter and introduced to Canada shortly thereafter.\nAluminum also became popular. In 1902, attracted by the availability of cheap hydro power, the Aluminum Company of America established a Canadian subsidiary, the Northern Aluminum Company (Alcan) at Shawinigan Falls, Quebec to produce that metal using the electrolysis technique. Corporate changes led to the creation of the Aluminum Company of Canada (Alcan) in 1925 and in 1926 the company constructed a giant aluminum smelter at a place it named Arvida, Quebec. Once again the site was chosen for the availability of cheap hydro electricity and the proximity of a deep-water port at Bagotville for large ships carrying bauxite or aluminum ore. World War II accelerated the demand for aluminum, which was the principal material in aircraft production and the Arvida facility was greatly expanded. In 1958 another huge Alcan smelter was built at Kitimat, British Columbia.\n\nThe growth in popularity of the car also created a need for rubber for automobile tires. Accelerated by the emergency of World War II, a substantial synthetic rubber production industry was established at Sarnia, Ontario in the early forties. The oil refineries there provided a ready source of raw materials. In particular, the Suspensiod crackers operated there by Imperial Oil produced large quantities of hydrocarbon gases. These were used by a new Crown enterprise, Polymer Corporation created in 1942, and associated private companies, St. Clair Processing Corporation Ltd., Dow Chemical of Canada Ltd., and Canadian Synthetic Rubber Ltd., itself a subsidiary of four Canadian rubber companies, Dominion, Firestone, Goodyear and Goodrich, to produce both GR-S and butyl type synthetic rubber. Initially production was destined for wartime use on military vehicles but in postwar years output was quickly redirected to civilian automobile production.\n\nThe closely related synthetic textile industry appeared in the years just after the First War. The production of artificial silk, more properly known as viscose rayon, made from bleached wood pulp, began in Cornwall, Ontario in 1925, in a factory built by Courtaulds (Canada). A year later Celanese Canada began making acetate yarn in a new plant in Drummondville, Quebec. DuPont Canada was the first to manufacture nylon yarn in Canada at its factory in Kingston, Ontario in 1942. This secret material was initially used for parachutes but following the war was used to make nylon stockings.\n\nAsbestos has long been known for its fibrous and heat resistant properties. With the rise of the automobile, asbestos became an important material, being used to make brakes. The world's largest asbestos mine, the Jeffrey Mine in Asbestos, Quebec had its beginnings in the 1878 when a local farmer W. H. Jeffrey began to mine the substance there. Original mining methods were primitive and involved blasting, the use of chisels to remove the mineral from the rock by hand and a crane powered by one horse. By 1895, 2300 tons of asbestos were being removed from the open pit mine per year. The mine was purchased by the Johns-Manville Company of the US in 1918 and has since become the largest asbestos mine in the world, over two kilometres in diameter and 350 metres deep. The material was used for insulation in buildings and ships and, of course, for automobile brakes. However, serious health problems, including lung cancer, have been associated with its mining and use, and in recent years mining activity there has diminished.\n\nWith the rail building era coming to an end, the rise of the automotive industry in southern Ontario provided the Hamilton steel mills of the Steel Company of Canada and the Dominion Foundries and Steel Company with a new market. Dofasco introduced the basic oxygen steelmaking at its mills in Hamilton in 1954. In the latter part of the century, Algoma, in Sault Ste. Marie, built coke oven batteries and blast furnaces, while phasing out the open-hearth and Bessemer steel-making process in favour of the basic oxygen steel-making.\n\nThe industrial production of bread became notable during these years. At the beginning of the 20th century it is estimated that only about 8% of Canadian wives bought bread commercially. However, the industrial production of bread grew impressively and by the 1960s, 95% of homemakers purchased bread commercially. One bakery of note, The Canada Bread Company Limited, was founded in 1911 as the result of the amalgamation of five smaller companies. Industrial bakeries such as this were characterized by the use of large machines for the mixing of dough, which was placed in pans on slow moving conveyor belts that transported them through giant ovens, where they were baked. Large automated packaging machines wrapped the finished loaves at great speed. Improvements in transportation and packaging technology throughout the decades allowed a shrinking number of bakeries to serve every larger markets. In 1939 there were about 3200 commercial bakeries across the country but by 1973 the figure stood at 1700, while in 1981 there were 1400.\n\nMeat packing grew to become Canada's most important food processing industry during this period. In Calgary, Alberta, in 1890, Pat Burns established P. Burns and Company, which became the largest meat processor in western Canada. In Toronto in 1896 the innovative Harris Abbatoir was established to export chilled sides of beef to the British market. The industry grew rapidly during the war, supplying meat to Canadian and British troops overseas. However, it underwent a period of consolidation in the twenties due to a loss of markets. This led to the merger of two major players, William Davies and the Harris Abattoir, to form Canada Packers in Toronto. By 1930, \"The Big Three\", meat packers in Canada were Canada Packers, Swift Canadian and P.Burns and Company in Calgary, Alberta.\n\nThe increasing popularity of the electric refrigerator in Canadian restaurants and homes made it practical for manufacturers to make available various frozen foods. The first such offering, a frozen strawberry pack was produced in Montreal and Ottawa beginning in 1932 by Heeney Frosted Foods Ltd.\n\nCold breakfast cereal became increasingly popular during these years. Wheat and later corn flakes were developed in the US by the Kellogg brothers in 1894 and the Kellogg Company was formed in 1906. In London, Ontario the Canadian Corn Company purchased the rights to manufacture and distribute Toasted Corn Flakes for Canadian distribution. In 1924 the American Kellogg Company purchased the London operation and formed Kellogg Canada Inc. Since that time the company has manufactured and distributed in Canada a wide variety of breakfast cereals including Corn Flakes, 1907, Bran Flakes, 1915, All Bran, 1916 and Rice Krispies, 1928.\n\nAlthough neither the tin can nor soups were remarkable in any way in the thirties, the combination of the two in the form of the well known Campbell’s soup was very popular. The Campbell Soup Company introduced its soup products to Canada in 1930, making them at its factory in Toronto on the lake shore.\n\nInstant coffee was another tasty innovation introduced during these years. The inventor and world leader in the manufacture of instant coffee, the Swiss-based Nestlé Company began operations in Canada with the production of canned condensed milk at its plant, The Maple Leaf Condensed Milk Company, in Chesterville, Ontario in 1918. Head office research invented instant coffee and began selling it around the world including Canada, as Nescafe in 1938. It became hugely popular with allied troops during World War II. In 1952 the instant chocolate drink, Nestle Quik, was introduced to Canada.\n\nThe sanitary napkin and Kleenex brand facial tissue were introduced in the 1920s. Kimberly, Clark and Co. (Kimberly Clark) formed in the US in 1872, invented cellucotton in 1914. It used this material as the basis for a sanitary napkin and marketed the product as Kotex beginning in 1920. Kleenex, initially intended for the removal of face cream, was introduced in 1924. In 1925 the company formed what would become, Canadian Cellucotton Products Limited, for the marketing of these and other products in Canada and internationally. The first practical electric razor, the Sunbeam \"Shavemaster\" and the Remington \"Close Shaver\" made available in the US in 1937 and in Canada shortly thereafter.\n\nWith a base of caustic soda, the world's first oven cleaner, Easy-Off, was invented in Regina in 1932 by Herbert McCool and manufactured in his home in that city until 1946, when production shifted to Iberville, Quebec. The product has since become the most popular oven cleaner in the world.\n\nThe grand hotel continued to make a mark with new structures, including the Bigwinn Inn, Muskoka, Ontario, 1920, the Jasper Park Lodge, Jasper, Alberta, 1922, the Hotel Newfoundland, St. John's, Newfoundland, 1926, the Hotel Saskatchewan, Regina, Saskatchewan, 1927, the Prince of Wales Hotel, Waterton Lakes National Park, Alberta, 1927, the Lord Nelson Hotel, Halifax, Nova Scotia, 1928, The Pines, Digby, Nova Scotia, 1929, the Royal York Hotel, Toronto, 1929, the Chateau Montebello, Montebello, Quebec, 1930, the Nova Scotian Hotel, Halifax, Nova Scotia, 1930, the Charlottetown Hotel, Charlottetown, P.E.I. and the Bessborough Hotel, Saskatoon, Saskatchewan, 1935.\n\nIn 1875 in Montreal, a McGill student, J. Creighton, established the basic rules for hockey as we know it today. The world's first facility dedicated to hockey, the Westmount Arena was built in Montreal in 1898 while the first industrial refrigeration equipment for making artificial ice in Canada was installed in 1911 by Frank and Lester Patrick for their new arenas in Vancouver and Victoria. The Mutual Street Arena, with its artificial ice surface, was built in Toronto in 1912. With the development of wide span roof structures the construction of large indoor ice rink stadiums became possible. These two technologies were used to build the Montreal Forum, home of the legendary Montreal Canadiens hockey team, in Montreal in 1924 and Maple Leaf Gardens home of the Toronto Maple Leafs, in that city in 1931. Baseball's facilities were upgraded with construction of the new Maple Leaf Stadium on Lake Shore Boulevard in Toronto in 1926 and the De Lormier Downs Stadium (Hector Racine Stadium), in Montreal in 1927. Civic Stadium, now Ivor Wynne Stadium, was built in Hamilton, Ontario in 1930, to host the British Empire Games held there that year.\n\nThe construction of the very large, Basilica of Sainte-Anne-de-Beaupré, near Quebec city was completed in 1926.\n\nThere were also advances in the lighting of public, commercial and industrial buildings. In 1938, after decades of development in the US and Europe, General Electric in the US, and shortly thereafter Westinghouse, began to sell the fluorescent lamp. Because of its lower power consumption its use was quickly adopted for large-scale applications. These lights were quickly made available to the Canadian market through the Canadian subsidiaries of these two companies.\n\nNotable engineering works of the period included the Second Narrows Bridge, Vancouver, 1925, the R.C. Harris Filtration Plant, Toronto, Ontario, 1926, the Peace Bridge, Fort Erie, Ontario, 1927, the Champlain Bridge (Ottawa), 1928, the Ocean Terminals, Halifax, Nova Scotia, 1928, Sea Island Airport (Vancouver International Airport), Vancouver, 1929, the Ambassador Bridge, Windsor-Detroit, 1929, the Windsor-Detroit Tunnel, 1930, the Broadway Bridge, Saskatoon, Saskatchewan, 1932, the Île d'Orléans Bridge, near Quebec City, 1934, the Thousand Islands Bridge, Ontario, 1937, the Pattullo Bridge, Vancouver, 1937, the Lion's Gate Bridge, Vancouver, British Columbia, 1938, Malton Airport (Toronto Pearson International Airport), Toronto, 1938, the Blue Water Bridge, Sarnia, Ontario, 1938, the Queen Elizabeth Way, Ontario, 1939, the Rainbow Bridge (Niagara Falls), 1941, Dorval Airport, (Montréal-Pierre Elliott Trudeau International Airport), Montreal, 1941 and the Alaska Highway, Dawson Creek, British Columbia, 1942.\n\nCanada’s first major roller coaster the Crystal Beach Cyclone was built at the Crystal Beach Amusement park in 1927. It quickly gained a reputation for its wild and even violent ride and one passenger, Amos Wiedrich was killed in 1938 when he stood up to take off his coat while the coaster was in motion.\n\nThe dump truck and bulldozer were introduced during these years for a variety of earth moving tasks including road building and canal construction. The dump truck was invented in Saint John, New Brunswick in 1920 by Robert T. Mawhinney and the bulldozer was developed in the US in 1923. Both quickly became popular worldwide.\n\nMedical treatment benefited from the introduction of the electrocardiograph, used to diagnose heart problems, in large hospitals in the late twenties. There were also important innovations with respect to the treatment of epilepsy during this period. In Montreal, Dr. Wilder Penfield, with a grant from the US Rockefeller Foundation, founded the Montreal Neurological Institute at the Royal Victoria Hospital (Montreal), in 1934 to study and treat epilepsy and other neurological diseases.\n\nThe military suffered a huge decline in the 1920s and 1930s. The Royal Canadian Air Force founded in 1924, was largely a bush and float plane operation. Only in the 1930s did it acquire a modest combat capability with a handful of British Armstrong Whitworth Siskin fighters and a squadron of Hawker Hurricane fighters as the clouds of war grew menacing. The Royal Canadian Navy, perpetually starved for equipment acquired its first custom-built ships, the destroyers HMCS Saguenay (D79) and HMCS Skeena (D59) on May 22, 1931. In 1929 the army began to retire its horses and was issued four 6-wheeled Leyland tractors in 1929, to tow its 60-pound guns. Four 3-inch 20-cwt. anti-aircraft guns were taken on strength in 1937. As a reflection of this intense and diverse engineering activity, the Canadian Council of Professional Engineers was established in 1936. This organization was renamed Engineers Canada in 2007.\n\nCanada was involved in the wartime Manhattan Project to build an atomic bomb, including the Montreal Laboratory for nuclear research by scientists from Britain, and American contracts with Canadian firms Eldorado Gold Mines for mining and processing uranium and a heavy water plant built by Consolidated Mining and Smelting (CMS) at Trail, British Columbia.\n\nThe years following World War II introduced even more innovations, including television, the transistor radio, synthetic fabrics, plastic, computers, super highways, shopping centres, atomic energy, nuclear weapons, transcontinental energy pipelines, long range electric transmission, transcontinental microwave networks, fast food, chemical fertilizer, insecticides, the birth control pill, jet aircraft, cable TV, colour TV, the instant replay, the audio cartridge and audio cassette, satellite communications and continental air defence systems.\n\nIn the early 1980s Canadian Satellite Communications (Cancom) assembled a package of Canadian and American television channels which it offered to remote communities throughout the northern regions of Canada. The signals were distributed by Anik satellite and made available to the local populace through cable. By the later part of the decade several hundred communities were using this service.\n\nThere was also technological innovation in the telephone system. The first trans-Atlantic telephone cable, jointly owned by the Canadian Overseas Telecommunication Corporation, British Post Office and AT&T, was brought into service in 1956, paving the way for telephone calls from Canada to Britain and Europe. An improved cable, CANTAT was installed in 1961. A similar service on the west coast, COMPAC, the Commonwealth Pacific Cable System was inaugurated in 1963, linking Canada by undersea telephone cable with New Zealand and Australia. CN/CP Telecommunications introduced the well known Telex service to Canada in 1956. Direct distance dialing was initiated in Canada in 1958, beginning with customers in Toronto and on 1 July of that year the Trans-Canada Microwave system, known as the Trans-Canada Skyway, went into service. The concept and operation of a dedicated emergency telephone number originated in Canada, where the City of Winnipeg established the world's first 9-1-1 service in 1959. The service eventually spread and was offered continent wide.\n\nThe Anik (satellite) series of communications satellites initially built by Hughes Aircraft and operated by Telesat Canada starting in 1972 formed the basis of the world's first domestic satellite communications service. Telesat has launched a large number of satellites including, Anik A1 - 1972, Anik A2 - 1973, Anik A3 - 1975, Anik B - 1978, Anik D1 - 1982, Anik C3 - 1982, Anik C2 - 1983, Anik D2 - 1984, Anik C1 - 1985, Anik E2 - 1991, Anik E1 - 1991, MSAT - 1996, Nimiq 1 - 1999, Anik F1 - 2000, Nimiq 2 - 2002, Anik F2 - 2004, Anik F1R - 2005, and Anik F3 - 2007.\n\n\"Dataroute\", the world's first national digital data system was introduced by CN/CP in 1973.\n\nAfter considerable political turmoil Canada acquired nuclear weapons from the Americans under a \"dual key\" arrangement on 1 January 1963. Genie air-to-air rockets armed with atomic warheads were based at RCAF Stations Comox, British Columbia, Bagotville, Quebec, and Chatham, New Brunswick, as the primary weapon for the newly acquired CF-101 interceptor. The nuclear armed BOMARC (Boeing Michigan Air Research Corporation) anti-aircraft missile was based at RCAF Stations North Bay, Ontario, and Lamacaza, Quebec. In Germany, as part of Canada's NATO commitment, nuclear free-fall bombs were acquired for the RCAF CF-104 strike fighter and the Canadian Army in Germany took possession of a battery of Honest John surface-to-surface battlefield rockets armed with nuclear warheads. By 1984 all these atomic weapons had been returned to the United States.\n\nWhile there were no accidents involving nuclear weapons in Canadian hands, there were at least two involving USAF aircraft flying in Canadian airspace. On 14 February 1950 a USAF B-36 heavy bomber, serial 44-92075, carrying one Mark 4 (Fat Man type) atomic bomb experienced multiple engine failures while flying south off the coast of British Columbia and jettisoned the bomb over Squally Channel. The crew bailed out and the plane flew on autopilot for another 330 km before crashing on a mountainside in the Kispiox Valley. In eastern Canada on 10 November 1950, a USAF B-50 heavy bomber, serial 46-038, flying from Goose Bay, Labrador, to the United States, experienced engine trouble and in accordance with standard operating procedures, jettisoned the Mark 4 atomic bomb it was carrying over the St. Lawrence River, near Rivière-du-Loup. The bomb's own 2200 kg conventional explosives blew it apart before it hit the water. The plane flew on to a base in the US.\n\nComputers were introduced in a variety of areas at this time. The National Research Council Canada experimented with fire-control computers towards the end of the war. The University of Toronto Computer Centre, established in 1947, developed Canada’s first operational computer the University of Toronto Electronic Computer (UTEC) in 1951. This was followed by the purchase of FERUT (Ferranti University of Toronto) computer, by the Computer Centre in 1952. The NRC used this computer in the early fifties for the hydrographic modeling of the St. Lawrence Seaway then under construction. Computers were also developed by other organizations, including the National Research Council, the NRC Computer (1954–1960), Ferranti Canada, mail sorter (1955), Computing Devices of Canada, the \"Kicksorter\" (1957–1963), the Defence Research Board, the DRTE (1960) and Sperry Canada, UMAC-5 (1962). Avro Canada in Toronto worked unsuccessfully to develop the fire-control computer for the Velvet Glove air-to-air missile for the highly advanced but ill-fated Avro Canada CF-105 Arrow interceptor. Avro Canada made extensive use of computers in calculations for aircraft design and manufacturing processes, including CNC. Other military developers included the Royal Canadian Navy with its DATAR system for the command and control of warships.\n\nIn the 1950s the Pinetree, Mid-Canada and DEW Line air-defence radar chains built across Canada relied heavily on computers. Certainly the largest and most powerful computer in Canada at the time was the IBM/USAF developed AN/FSQ-7, installed in the late 1950s, underground at RCAF Station North Bay, as the \"brain\" of the DEW Line System. The machine contained 55,000 vacuum tubes, weighed 275 tons and occupied a half-acre of floor space. It could perform 75,000 instructions per second.\nBy 1958 there were 41 computers in operation in Canada with big business, universities or the military. The most popular was the IBM 650, which was used by 19 organizations, including Canadair Limited, Canadian General Electric, Ford Motor Company of Canada, Great West Life Assurance Company, Imperial Oil Limited, Orenda Engines Limited and the University of Toronto. Other models in use included the Bendix G-15, 4, the ALWAC III-E, 3, the IBM 705, 3, the UNIVAC II, 3, the Datatron 205, 2, the LGP-30, 2, the Borroughs E 101, 1, the IBM 704, 1, and the NRC 102A/D.\n\nOne of the first commercial users of computers was Trans-Canada Airlines (TCA). In 1961 Ferranti-Packard developed the ReserVec computer reservation system for TCA (now Air Canada). This formed the basis for the Ferranti-Packard 6000 computer, and in 1963 two were sold in Canada, one to the Defence Research Establishment Atlantic, in Dartmouth, Nova Scotia and the other to the Toronto Stock Exchange.\n\nIn 1961 the Royal Bank of Canada was the first bank in Canada to introduce computers for its operations, followed by the Toronto-Dominion Bank in 1962. It was soon followed by the other large Canadian banks, including the Canadian Imperial Bank of Commerce and Bank of Nova Scotia. When they introduced the credit card about the same time these records were kept on large central computers as well. It was this experience with large computer systems linking hundreds of branch offices across the country that enabled the banks to introduce the ATM and the debit card, across Canada in the 1980s. Computers were also introduced to control complex industrial processes. Interprovincial Pipe Line Limited of Edmonton was one of the first Canadian companies to employ computers as a means of controlling the flow of gas in its very large pipeline system. Atomic Energy of Canada Limited used computers to control atomic fission in its power reactors. In 1977 the Toronto Stock Exchange became the first stock market in the world convert to electronic trading with the introduction of the its Computer Assisted Trading System. Twenty years later, in 1997, the exchange closed its trading floor and converted to a fully automated, computer-driven trading system.\n\nComputers were also recognized as a tool for policing. The Canadian Police Information Centre which was established in 1966 under the auspices of the RCMP, has operated, since that date, a national computer data base that provides information relating to criminal activity in Canada.\n\nDuring this period Canada Post applied computer readable codes to speed the delivery of mail. On 1 April 1971 the postal code system was introduced. The technique involved the use of a six-character geographic code placed on the envelope or parcel by the sender. The code was in turn machine scanned by a computer-driven optical reader that signaled the sorting equipment to direct the item to the proper destination. While technically effective, the introduction of the system lead to serious labour trouble at Canada Post for several years by unionized workers who were afraid of pay cuts or job loss (Postal codes in Canada).\n\nIn transportation, several significant works were completed, including the Toronto Subway, 1954, the Trans-Canada Gas Pipeline, 1958, the St. Lawrence Seaway, 1959, Trans-Canada Highway, completed in 1962, the Montreal Subway, 1966, GO Transit, Toronto area, 1967 and Highway 401, Ontario, completed in 1968.\n\nOn August 10, 1949, the Avro Canada C102 Jetliner, a mid-range four-engine passenger jet, made its first flight, just 13 days after the world's first and eight years before the US's first, the Boeing 707. Trans-Canada Airlines (later Air Canada) and Canadian Pacific Airlines introduced jet passenger service with the de Havilland Comet, DC-8, DC9, B727 and B-737. The B-747 was introduced by these companies in the early seventies. In the sixties and early seventies De Havilland Aircraft of Canada in Toronto developed the DHC-7 Dash 7 and DHC-8 Dash 8 STOL aircraft. These were used to provide passenger service to small city centre airports in Toronto, Ottawa and Montreal. A number of international carriers also acquired these aircraft to provide similar services elsewhere in the world. The first Canadian owned helicopter began operation in Canada on 12 March 1947. On that date Photographic Survey Corporation took possession of a Bell Bell 47B-3, registration CF-FJA.\n\nThe development of trans-oceanic aviation in the postwar years created a need for accurate weather information over the Atlantic and Pacific Oceans. An international agreement in 1946 established 13 Ocean Weather Stations with Canada being responsible for two, ocean station Baker several hundred miles off the east coast from 1947 to 1950 and for ocean station Papa off the coast of Vancouver from 1950 to 1981. A number of ships, both converted RCN vessels and purpose built CCG weather ships were stationed at these points to gather weather information during these years. Ships involved included, HMCS St. Stephen, HMCS St. Catherines, HMCS Stone Town, CCGS Vancouver, and CCGS Quadra.\n\nOf note was the transit of the Northwest Passage in 1954 by HMCS Labrador, Canada's first purpose built icebreaker, which was acquired that same year, in service with the Royal Canadian Navy.\n\nOf particular significance was the conversion from steam to diesel by Canada's two great railways. Beginning in the mid fifties the CPR and Canadian National Railways began replacing their steam locomotives with diesel locomotives. By 1960 the conversion was mostly complete.\n\nThe Volkswagen was introduced to Canadians in 1952 and became very popular with drivers who wanted greater fuel economy than that provided by the larger cars then on the market. It was sold in Canada until 1977. The seat belt became a standard feature of domestic passenger cars in the late sixties. The catalytic converter was also introduced during these years. The first devices, designed to reduce air pollution from automobile exhaust, were installed in the 1975 model year for US cars manufactured in Canada. Because of environmental concerns and the fact that it was not compatible with these converters, the major gasoline companies in Canada began to eliminate the sale of leaded gasoline that same year.\n\nAlthough Armand Bombardier invented the snowmobile, the initial production model, the B-7 dating from 1937 was a large 7 passenger vehicle. It was not until 1959 with the development of the small gas engine that the individual snowmobile or Ski-doo was produced by Bombardier (Bombardier Recreational Products) in the company factory at Valcourt, Quebec. A number of competitors in Canada and elsewhere entered the market and sales of snowmobiles skyrocketed with 2 million being sold worldwide between 1970 and 1973. To this day, snowmobiles remain popular in Canada and other regions with snowy winters.\n\nPedestrian walkways have become important features of some Canadian cities. Climate controlled underground passageways and shopping malls have been features of the downtown cores of Toronto (PATH (Toronto)) and Montreal (Underground City, Montreal) since the mid-sixties. Arguable the most unusual, is the Plus 15 system in downtown Calgary. Initiated in 1970 it presently consists of 57 bridges and 16 km of enclosed climate controlled passageways suspended above ground level which permit pedestrians to walk anywhere in the downtown core summer or winter without ever going outside.\n\nBeginning in the mid-1950s nuclear-generated electricity was developed under a partnership of industry and government at both the federal and provincial levels. A demonstration power reactor, the NPD was built at Rolphton, Ontario in 1962, followed by a commercial-scale CANDU prototype at Douglas Point in 1968. In 1971 electricity became commercially available from the large (ultimately 8-unit) Pickering station near Toronto, Ontario and, starting in 1977, the Bruce station (ultimately 8-units as well), near Kinkardine, Ontario. These were followed by the Gentilly-2 Atomic Electric Plant, Trois-Rivières, Quebec and the Point Lepreau Atomic Electric Plant, Point Lepreau, New Brunswick both in 1982. The electric current supplied by commercial hydro companies to consumers was changed and organizations like Hydro Ontario converted from 25 cycles to 60 cycles during the ten-year period from 1949 to 1959.\n\nThe introduction of this technology was not without mishap. On 12 December 1952 the experimental NRX reactor at Chalk River suffered a failure of its cooling system and underwent a partial meltdown. On May 24, 1958, the newly commissioned NRU reactor also a Chalk River experienced a major accident when one of the uranium filled fuel rods caught fire and seriously contaminated the reactor building with radioactive debris.\n\nThe modern era of oil production in Canada began in 1947 when Imperial made its major discovery at Leduc, Alberta. The availability of oil and gas in Alberta, a half continent away from central Canada provided the impetus for the construction of two huge transcontinental pipelines to the eastern Canadian market. The crude oil pipeline was the first to be built. The construction of first section of the Interprovincial Pipeline from Edmonton to Regina to Superior Wisconsin in the US began in 1950 and was completed in an astonishing 150 days. In 1953 the pipeline was extended through the US to Sarnia, Ontario and from Sarnia to Toronto in 1957. At the time of its completion it was the longest pipeline in the world. The oil and gasoline industry has grown tremendously since then, mainly to meet the demand for gasoline created by the popularity of the car and for home heating oil. Major oil refineries have been built in Vancouver, British Columbia, Edmonton, Alberta, Sarnia, Ontario, Montreal, Quebec and Saint John, New Brunswick.\n\nThe construction of the transcontinental oil pipeline was followed by that of the gas carrying Trans-Canada pipeline. Work began in 1956 at the Alberta/Saskatchewan border advancing to Regina, Winnipeg and Port Arthur, Ontario in 1957. Construction was not without mishap as during a pressure test in 1957 five kilometres of pipe blew up near Dryden, Ontario. The line crossed the very technically difficult Canadian Shield north of Lake Superior and reached Toronto and Montreal in 1958. At the time of its completion it was the longest pipeline in the world. Political controversies related to the construction of the pipeline contributed to the fall of the St. Laurent Liberal government in 1957. \nGas pipelines were also built in Alberta, the largest being that of Alberta Gas Trunk Lines, incorporated in 1954 and British Columbia, where the Westcoast Transmission Co. system was completed in 1957. While large pipelines carried natural gas across the continent smaller distribution systems were necessary to carry gas into factories and individual homes, where it was used as a source of heat. Very complex local understreet pipeline networks were constructed in cities across Canada to meet this requirement.\n\nOther energy projects of the period included the Lakeview Generating Station, Mississauga, Ontario, 1962, the W.A.C.Bennett (hydro) Dam, British Columbia, 1967, the Gardiner (hydro) Dam, Saskatchewan, 1968, the Churchill Falls Hydro Dam, Labrador, 1971, the Nanticoke Generating Station (largest coal-fired plant in North America), Nanticoke, Ontario, 1978 and La Grande 2 Hydro Dam, Quebec, 1979. The energy crisis of 1973 had domestic repercussions with many consumers taking steps to reduce energy costs through the installation of improved home insulation and wood burning stoves.\n\nThe existence in Alberta of large quantities of surface bitumen (oil) mixed with sand has been known for many years. In the late 1970s the commercial production of synthetic crude oil from this bitumen began near Fort McMurray. Construction at this site, by a company known as Syncrude, began in 1973 and the first crude oil was produced there in 1978. The complex and costly production process involves scraping the sticky bitumen-laden sand from the surface, transporting it to a processing facility, removing the sand from the bitumen and upgrading the bitumen to a product known as light sweet crude. The technical scale of the operation is very large. Initially the sandy tar-like bitumen was scrapped from the ground using gigantic powered rotating mechanical wheels equipped with scraping buckets and the oil sand was placed on conveyor belts for transport to the processing plant. However, the severely cold Albert winters caused the continuous breakdown of the machinery and a new technique was developed. This involves the use of gigantic power shovels and dumptrucks to deliver the bitumen laden sand to the processing plant. Once at the plant the bitumen is removed from the sand with a process that involves the use of hot water. The bitumen is then subjected to fluid coking, hydroprocessing, hydrotreating and reblending. Syncrude is the largest producer of synthetic crude oil from bitumen sand in the world and the largest producer of oil from a single site in Canada.\n\nThe forestry industry underwent a notable process of mechanization in the postwar years. The most visible change was the introduction of the chain saw. When originally developed for modern use in the 1920s, this heavy, gasoline engine-driven machine required two men for its operation. However, improvements in engine technology eventually made the saw small and light enough to be operated easily by one person. In 1944 one of the first industrial users, Bloedel Stewart and Welch Ltd. in British Columbia had 112 chain saws in operation, but their use accounted for only a small part of total forestry tree cutting. In 1950 less than one percent of pulpwood in Canada was cut with chain saws. However, by 1955 this figure had grown to more than 50%.\n\nOther machines were also introduced during this period. The first feller buncher was used by the Quebec North Shore Paper Company in 1957. Hydraulic tree shears were first used in 1966 by the Abitibi Pulp and Paper Company Limited (Abitibi-Consolidated). Snowmobiles and tracked machines replaced animals for the hauling of logs. In 1948 several Bombardier machines were employed to this end by the Ste. Anne Power Company Limited in Quebec. In 1959 Timberland Machines of Woodstock, Ontario developed the Timberbuncher, a self-propelled machine which could move through the forest, cut a whole tree at its base (a process known as full tree harvesting) and, using a hydraulic arm, place it into a pile for hauling. Machines that stripped the branches from felled trees, a process known as delimbing, were also introduced at this time.\n\nWith the help of these technologies, the Canadian pulp and paper industry grew to become one of the major suppliers of newsprint in the world through the operations of companies such as MacMillan Bloedel Limited, Repap Enterprises Inc., Kruger Inc., Great Lakes Forest Products Ltd, British Columbia Forest Products Ltd., Consolidated-Bathurst Inc., Canadian Forest Products Ltd., CIP Inc., Domtar Pulp & Paper Products Group and Abitibi Consolidated.\n\nThe use of pesticides was a prominent feature of postwar agriculture across Canada. Insecticides based on fluorine, arsenic, rotenone, nicotine pyrethrum as well as herbicides using sulphiric acid, arsenites and salt and finally fungicides based on sulphur, mercury or copper have been very effective in controlling life forms that degrade agricultural output. At the same time these compounds have also had a negative effect beyond their intended sphere of use. DDT was registered for use in Canada from 1946 until 1985, when its use was banned. The product was never manufactured in Canada. Food irradiation, in particular the irradiation of potatoes to prevent sprouting while in storage, was approved for use in Canada in 1960.\n\nPotash-based chemical fertilizer became an important element of increased agricultural production in Canada and around the world in the postwar era. In Saskatchewan techniques were introduced for the mining of the huge potash deposits found there. These involve both \"dry\" and \"wet\", methods of mining. The dry method involves the sinking of a vertical shaft and the use of large powered cutting machines to cut into the potash horizontally. The wet technique known as solution mining is used to access potash at greater depths. This involves drilling a vertical hole into the deposit into which is pumped hot water. The liquid dissolves the potash underground and then returns to the surface, where another process separates the mineral from the water.\n\nBusiness administration underwent technological change. The ball point pen was marketed in the US in October 1945 and in Canada shortly thereafter. The IBM Selectric typewriter, introduced in 1961, quickly became popular with businesses in Canada, as did the Xerox photocopier in the 1960s.\n\nThere was important progress in medical technology during this period. In 1945 Dr. Stuart Stanbury established a National Blood Transfusion Programme for the Canadian Red Cross Society, thus making available to those in need, a dependable source of blood for medical purposes. The associated test for blood typing was introduced at the same time. Blood tests would become increasingly sophisticated in the coming years. The electroencephalograph, used for the diagnosis of neurological disorders was introduced in major Canadian medical institutions in the late forties.\n\nThe techniques for the mass production and distribution of vaccines and for the mass public inoculation were introduced during these years. Polio was a disease that affected large numbers of Canadian children during the first part of the 20th century. In the US, research by Dr. Jonas Salk in the late 1940s led to the discovery of vaccine for the prevention of this disease. However, there was no technique for volume manufacture of the drug. Connaught Laboratories of Toronto developed a synthetic culture known as \"medium 199\", which enabled the mass production of this polio vaccine beginning in 1952. A successful all-Canadian mass inoculation of children using the new vaccine was undertaken in the spring of 1955, the first such mass public health campaign of its type in Canada. Antibiotics such as penicillin were quickly made available to the general public in the postwar years.\n\nThere was also progress with respect to the treatment of heart disease. The pacemaker invented with significant Canadian participation was used to treat patients with arrhythmia. For more serious problems open heart surgery became an option for patients and permitted the repair of faulty heart valves, the clearing of blocked coronary arteries and the resolution of other problems. Canada's first heart transplant was performed on 31 May 1968, by Dr. Pierre Godin the Chief Surgeon at the Montreal Heart Institute, on patient Albert Murphy of Chomedy, Quebec a 59-year-old retired butcher suffering from degenerative heart disease. The operation took place about six months after the world's first, by Dr. Christian Barnard. Neurosurgery was introduced in a substantive way in the 1960s.\n\nCancer patients were provided with a new option, radiation therapy, through what was popularly known as the \"Cobalt Bomb\", again developed with important Canadian input. The use of radio isotopes for diagnostics was also introduced. Chemotherapy also became a treatment option. In 1960 the use of a subcutaneous arteriovenous shunt along with the artificial kidney machine allowed hemodialysis for patients with chronic renal failure.\n\nDuring these years the Montreal Neurological Institute pioneered the development of medical imaging technologies introducing Canada's first CAT scan in 1973, PET scan in 1975 and MRI in 1982. The technique of medical ultrasonography also became widely available beginning in the late 1960s and was especially popular with expectant mothers interested in the health and sex of their fetus. The number of these machines in use has grown greatly over the years. In 2004 there were about 150 MRI units and about 350 CAT units in use in Canada.\n\nThe corneal contact lenses first developed in 1949 gained mass appeal in Canada and elsewhere in the 1960s. Made of polymethyl methacrylate (PMMA) they could be worn up to 16 hours a day.\n\nDevelopments in orthodontics made the straightening of the teeth of children with \"braces\" commonplace. Children were also often on the receiving end of the tonsillectomy a fashionable surgical procedure during these years.\n\nThe surgical replacement of body parts also became possible and was used to treat ailing kidneys and joints such as knees and hips. The availability of cosmetic implants became popular during these years. In 1962, in the US, Dow Corning developed the silicone gel-filled breast implant which was used by women for surgical breast augmentation. The procedure was common in Canada. In recent years implants containing saline solution have also become popular.\n\nPharmaceuticals attained a high-profile. The availability of the birth control pill in 1960 made it possible for women to protect themselves from unwanted pregnancy. Stress could be treated with tranquilizers, such as valium, introduced in 1963. The consumption of vitamins became widespread and supplements were added to staple foods such as milk and bread and were taken in pill form. While most of these drugs were safe, one, thalidomide, had horrific consequences for its users. Thalidomide was invented in West Germany in 1954 by Chemie Grunenthal as a sedative. It was noted that the drug was particularly effective in treating the symptoms of morning sickness associated with pregnancy. The drug was made available under prescription to Canadians beginning 1 April 1961. Tragically it was discovered that the drug caused miscarriage and severe birth defects. As a result, the drug was withdrawn from the Canadian market on 2 March 1962.\n\nThe \"recreational\" use of \"soft drugs\" such a marijuana, LSD and hashish became part of the 1960s counter culture. Marijuana was often produced locally using the hydroponic method.\n\nThe car, cheap gasoline and postwar affluence created boom conditions for the expansion of suburbia. Several standard designs for the single family home on a standard lot were reproduced cookie-cutter style row-upon-row in cities across Canada as subdivision after subdivision sprang up radiating from the central core. The designs were thoroughly modern, reflecting the optimism of the era, usually with a peaked roof, asphalt shingles and a brick or wood siding exterior and included a living room, kitchen and occasionally dining room and two, three or four bedrooms and a full basement made of poured concrete or cinder block. Floors were usually made of varnished hardwood planks and the walls and ceilings of gyprock. Copper piping brought running water from the serviced street and copper wiring electricity from the rear lot line. Clay tile pipe carried the sewage from the flush, sit toilet to the main sewer line running under the street. There was usually a driveway beside the house for the family car, and less frequently a carport or garage.\n\nMost homes were equipped with a telephone often with a \"party\" line but these became rare by the 1960s. A television set was also common in almost all homes by the end of the 1950s and the record player gave way to the hi-fi stereo. Almost all kitchens were equipped with electric refrigerators and electric or less commonly gas, stoves. Where there was gas it was usually piped to the home through a main line running under the street. There were a variety of electrical \"labour saving\" devices including electrical mixers can openers and carving knives. Central heating was a standard feature and coal, delivered to the home by a diesel powered truck, was the dominant fuel source in the early postwar years. However, as the 1950s progressed coal gave way to oil and gas heating. Home furnishings were almost all mass-produced and made from wood, fabric and various types of stuffing for cushions. In the kitchen metal chrome tube chairs and formica topped tables were popular. The small front and back yard were maintained with the help of a gasoline-powered lawn mower, and the hedge and bushes were trimmed with electric clippers. In the early 1960s the high-rise apartment building began to make its appearance in large cities. The self-supporting steel structures were usually seven stories or more, and large buildings contained hundreds of dwelling units. Initially they were especially visible along Highway 401 in Toronto, Metropolitan Boulevard in Montreal and the north shore of English Bay in Vancouver. Their construction was possible due to the introduction of the high-rise crane, which to this day remains a common feature of city skylines.\n\nThe arrival of television had an effect on eating habits. In 1953, C.A. Swanson & Sons introduced the TV dinner to the US market. The pre-cooked food items, including, meat, potatoes and a vegetable were placed in the segments of an aluminum tray and frozen. The consumer purchased the frozen product and heated it in the oven for about 25 minutes. It could be eaten out of the tray. In 1960, Swanson, a subsidiary of the Campbell Soup Company, built a factory in Listowel, Ontario to manufacture TV dinners and other Campbell frozen products for the Canadian market. The electric toothbrush introduced in 1959 has become very popular in Canada since the 1990s.\n\nThe steel aerosol spray can with the gas propellant, and \"crimp on nozzle\" was developed in the US in 1949. It quickly became a favored type of packaging in Canada for a number of products including whipped cream, deodorant, bug spray and hair spray. The gas propellant, usually a chlorofluorocarbon (CFC), became a target for environmental concern in the 1970s when research demonstrated that it had a harmful effect on the ozone layer in the atmosphere. The international Montreal Protocol of 1989 banned the use of these substances and they were subsequently replaced with volatile hydrocarbons. The disposable diaper was introduced to the Canadian market in 1972 by Procter & Gamble Canada Ltd. The chemistry of hair colouring was introduced to the domestic scene in the US and Canada in 1956, when Clairol began marketing a home hair colouring kit for women that could be purchased for a modest price at a local pharmacy or grocery store. Invented in Canada, the green plastic garbage bag (Bin bag) was introduced to Canadians in the late 1960s. Patented in 1955 by Swiss engineer, George de Mestral, velcro, a two sided fastening technology based on hooks and loops, was introduced to a number of countries, including Canada, in the late 1950s. In Montreal, Velek Ltd. acquired the exclusive marketing rights for velcro in North and South America and Japan.\n\nThe format for sound recordings changed in the years just following the war. In the US, Columbia Records introduced the long playing (LP) 33 format in 1948. Columbia made an agreement with Sparton Records, of London, Ontario, established in 1930, for the manufacture and distribution of its LP records in Canada. Not to be caught short, RCA Victor in the US responded in 1949 with its own technological innovation the 45 rpm record (with the big hole in the centre) and manufactured and distributed this new format for the Canadian market through its Canadian subsidiary, RCA Victor of Canada, established in Montreal in 1929. The video home system (VHS) released in 1976 by Victor Company of Japan, Limited (JVC), quickly became popular in Canada and was used to record TV programmes or to play VHS tapes of Hollywood movies that could be rented in neighborhood video stores that soon became a common feature of suburban strip malls. In 2003 the popularity of DVD surpassed VHS and by 2006 the technology had become obsolete.\n\nThe introduction of the credit card complimented the appearance of the shopping mall. In 1968 a number of Canadian banks including the Bank of Nova Scotia, the Royal Bank of Canada, the Toronto-Dominion Bank and the Canadian Imperial Bank of Commerce began issuing the Chargex credit card to customers. In 1977 these cards were reissued by the same banks under the VISA brand name. The MasterCard credit card became available to Canadians in 1973.\n\nThe booming growth of the suburbs led to the appearance of the shopping mall, a low-rise steel frame, commercial structure housing a number of retail outlets and surrounded by acres of asphalt parking lot for large numbers of cars. The first in Canada included the Norgate Shopping Centre, Saint-Laurent, Quebec, 1949, the Dorval Shopping Centre, Dorval, Quebec, 1950, the Park Royal Shopping Centre, West Vancouver, British Columbia, 1950, the Sunnybrook Plaza, Toronto, 1951 and York Mills, Toronto, 1952.\n\nThe hospitality industry was similarly affected and fast food drive-in restaurants began to appear. In 1951 the first St. Hubert BBQ restaurant opened its doors on St-Hubert street in Montreal. A&W opened its first Canadian operation in Winnipeg, Manitoba in 1957. In 1959 Harvey's opened its first eatery on Yonge Street in Richmond Hill. In Hamilton, Ontario, the first Tim Hortons restaurant opened in 1964. The first McDonald's restaurant outside the United States was opened in Richmond, British Columbia in 1967 and Pizza Delight was founded in Shediac, New Brunswick, in 1968.\n\nCinema attendance boomed after the war and with it innovations in cinema design. The first double-screen cinema, The Elgin, opened its doors in Ottawa in 1946 and the drive-in cinema became popular after the war. However, the long cold Canadian winters discouraged the widespread diffusion of this type of film exhibition. The dramatic IMAX large-scale cinema format was invented as the result of developments in cinematic technology during Expo '67 in Montreal. The world's first permanent Imax cinema, Cinesphere, was built at Ontario Place in Toronto in 1971. Others were built in Vancouver for Expo '86 and at the Canadian Museum of Civilization in Gatineau, Quebec, in 1989. By 1995 there were 129 Imax cinemas entertaining audiences around the world. The audio cartridge and audio cassette became popular in the early 1970s with the cassette eventually winning the battle of the formats. This compact medium led to the appearance of high quality in-car sound systems.\n\nThe New Woodbine Racetrack for thoroughbred horse racing opened to the public in Toronto in 1956 (simply Woodbine after 1963) replacing the original Woodbine which was built in 1874. Canada’s first purpose-built auto racing track, the Westwood Motorsport Park was built in Coquitlam, British Columbia in 1959. The Mosport International Raceway, north of Bowmanville, Ontario opened to the public in 1961 and hosted the Canadian Grand Prix Formula 1 races from 1967 to 1977. La Ronde became Canada’s largest amusement park when it opened in 1967 as part of Expo '67 in Montreal. It is popular to this day for a number of roller coasters, including The Boomerang, Cobra, Goliath, Le Monstre and Vampire.\n\nDetergent, a replacement for soap, introduced in the postwar years, was used to keep clothes and dishes clean through the action of its active ingredient, tetrapropylene, a derivative of petroleum. The popular Tide brand became available in 1948. In 1964 permanent press fabrics were invented in the US by Ruth Rogan Benerito, a scientist at the Physical Chemistry Research Group of the Cotton Chemical Reactions Laboratory and introduced to Canada shortly thereafter. The press resulted from the treatment of the fabric with formaldehyde. Invented by DuPont scientist Dr. Roy J. Plunkett in 1938, polytetrafluoroethylene, a polymer considered the world’s most slippery substance, was introduced commercially as Teflon, in 1946 in the US. It is used in a wide variety of applications, including as a non-stick coating on the cooking surface of pots and pans and is manufactured in Canada by DuPont in Mississauga, Ontario. Krazy Glue (ethyl cyanoacrylate) was introduced to Canada in 1973.\n\nIn the postwar years Canadian municipalities began treating raw sewage, which up to that time, with a few notable exceptions, had been allowed to flow directly from their sewer systems into nearby streams, rivers, lakes and oceans. New facilities were added in Toronto including the Highland Creek Wastewater Treatment Plant in 1956 and the Humber Wastewater Treatment Plant in 1960. Vancouver built a number of sewage treatment facilities including the Lions Gate Wastewater Treatment Plant in 1961, the Iona Island Wastewater Treatment Plant in 1963 and the Lulu Island Wastewater Treatment Plant in 1973. The City of Ottawa built the Green's Creek Pollution Control Center (now Robert O. Pickard Environmental Centre) in 1961. In 1970 the City of Montreal began the construction of a large sewer network which channeled all effluent to the treatment plant at Rivière-des-Prairies on the east end of the island and became operational in 1996. By 1980, 64% of Canadians were served by sewage treatment, with the figure rising to 78% in 1997.\n\nPublic and industrial concern with air pollution and acid rain led to measures being taken by a number of companies to cut back on harmful atmospheric emissions. In 1972, Inco undertook steps to reduce emissions of SO2 and other gases by installing scrubbers and a chimney at its Copper Cliff smelter in Ontario.\n\nBridges of note included the Angus L. Macdonald Bridge in Halifax 1954, the Oak Street Bridge, Vancouver, 1957, the Burlington Bay James N. Allan Skyway, 1958, Ogdensburg-Prescott International Bridge, 1960, the Queensborough Bridge, Vancouver 1960, the Sault Ste. Marie International Bridge, Sault Ste. Marie, Ontario, 1962, the Champlain Bridge, Montreal, 1962, the Lewiston-Queenston Bridge, Niagara Falls, Ontario, 1962, the Port Mann Bridge, Vancouver, 1964, the Macdonald-Cartier Bridge, Ottawa, 1965, the Pont de la Concorde (Montreal), 1966, the Prince Edward Viaduct, Toronto 1966, the Laviolette Bridge, Trois-Rivières, Quebec, 1967, the Saint John Harbour Bridge, Saint John, New Brunswick, 1968, the Dinsmore Bridge, Vancouver, 1969, the A. Murray McKay Bridge, Halifax, 1970, the Pierre Laporte Bridge, Quebec City, 1970, the Portage Bridge, Ottawa, 1973 and the Arthur Laing Bridge, Vancouver, 1976.\n\nThis was also an era of gigantism, and there were both successes and failures. Beginning in 1963, massive civil engineering works were undertaken in the St. Lawrence River in Montreal to build the site for the 1967 World's Fair, known as Expo 67. The gigantic Red River Floodway, which opened in 1968, was designed to carry flood water from a rising Red River around the heart of the City of Winnipeg. It was completed in 1968 and proved successful when used for the first time in 1969. At the time of its completion it was the second largest earth-moving project in the world, after the Panama Canal.\n\nNorthwest of Montreal thousands of acres of fertile farmland were expropriated to build the huge new Mirabel International Airport, which opened in 1975. The facility was to be linked to the heart of Montreal with a fast train. The train was never built and both passengers and air carriers stayed away in droves. The site eventually became a quiet industrial airport, home to the production facilities for Bombardier regional jets. On the other hand, the James Bay Project undertaken in Quebec at the same time was a booming success. Several large dams on the La Grande River with their associated long distance transmission lines provide Hydro-Québec with an important source of electricity.\n\nThe CN Tower, the world's tallest free-standing structure, was constructed in Toronto in 1975.\n\nImportant skyscrapers, including Place Ville Marie (Royal Bank), Montreal, 1962, the Canadian Imperial Bank of Commerce Tower, Montreal, 1962, the Edifice Trust Royal (C.I.L. House), Montreal, 1962, the Toronto Dominion Bank Tower, Toronto, 1967, The Simpson Tower, Toronto, 1968, the Hôtel Château Champlain, Montreal, 1967, the Royal Trust Tower, Toronto, 1969, Royal Centre, Vancouver, 1972, Inco Superstack, Sudbury, Ontario, 1972, First Canadian Place, Toronto, 1975, Harbour Centre, Vancouver, 1976, the Complexe Desjardins, la Tour du Sud, Montreal, 1976, the Scotia Tower, Calgary, 1976, the Scotia Tower, Vancouver, 1977, Royal Bank Plaza, South Tower, Toronto, 1977 and the First Bank Tower, Toronto, 1979, represented significant architectural achievements during this period.\n\nThe massive Saint Joseph's Oratory, the largest church in Canada, the construction of which began in 1924, was completed in 1967 on the north slope of Mont Royal in Montreal.\n\nNotable large sports facilities included, Empire Stadium, Vancouver, 1954, McMahon Stadium, Calgary, Alberta, 1960, the Montreal Automobile Stadium (Autostad) 1966, the Olympic Stadium, Montreal, 1976 and Commonwealth Stadium (Edmonton), 1978.\n\nMicroelectronics became a part of everyday life during this period. The personal computer became a feature of most homes, and the microchip found its way into a bewildering variety of products from cars to washing machines.\n\nIn 1977 the first commercially produced personal computers were invented in the US: the Apple II, the PET 2001 and the TRS-80. They were quickly made available in Canada. In 1980 IBM introduced the IBM PC. Microsoft provided the operating system, through IBM, where it was referred to as PC DOS and as a stand-alone product known as MS-DOS. This created a rivalry for personal computer operating systems, Apple and Microsoft, which endures to this day. A large variety of special-use software and applications have been developed for use with these operating systems. There have also been a multiplicity of hardware manufacturers which have produced a wide variety of personal computers, and the heart of these machines, the central processing unit, has increased in speed and capacity by leaps and bounds. There were 1,560,000 personal computers in Canada by 1987, of which 650,000 were in homes, 610,000 in businesses and 300,000 in educational institutions. Canadian producers of micro-computers included Sidus Systems, 3D Microcomputers, Seanix Technology and MDG Computers. Of note is the fact that these machines were based on digital technology, and their widespread and rapid introduction to Canada at the same time that the telephone system was undergoing a similar transformation would herald an era of rapid technological advance in the field of communication and computing.\n\nThe laptop computer also appeared during these years and achieved notable popularity in Canada beginning in the 1990s. In 1981 the first commercially available portable computer, the Osborne 1, became available. Other models followed, including the Kaypro II in 1982, the popular Compaq Portable and Tandy Corporation TRS-80 Model 100 both in 1983, the IBM PC Convertible, 1986, the Macintosh Portable, 1989 and Power Book, 1991. The latter models in particular were popular with both professionals and consumers.\n\nIn the 1970s and 1980s word processing, a method for \"typing\" documents using a keyboard linked to a computer and a video screen, was developed. Early machines were dedicated exclusively to this function and a notable Canadian contribution, the Superplus IV, produced by AES Data in Montreal in 1981, became widely popular. However, the rise of the personal computer and the invention of PC-compatible word processing software, such as WordPerfect in 1982 and Microsoft Word in 1983, made stand-alone word processors obsolete. Spreadsheet software also became popular for accounting purposes, notably Microsoft Excel, which was also introduced to the world and Canadian market in 1983. These new machines with their new software quickly dominated the market and became an almost universal feature of any Canadian office.\n\nIn 1987 there were considerable numbers of larger computers in Canada, including 25,000 mainframe and mini-computers. But the most powerful of all were the supercomputers. The Meteorological Service of Canada has been a noted user of large computers and has pioneered the Canadian use of supercomputers. Machines used have included the Bendix G20, 1962, an IBM 360-95 scientific mainframe computer, 1967, its first supercomputer a CDC 7600 from Control Data Corporation, 1973, a Cray 1S supercomputer, 1983, a NEC supercomputer, 1993 and an IBM supercomputer in 2003. At the time of its installation this latter machine was the most powerful computer in Canada.\n\nThe Communications Security Establishment (CSE), Canada's \"electronic spy\" agency has been a notable user of supercomputers. CSE code breaking capabilities degraded substantially in the 1960s and 1970s but were upgraded with the acquisition of a Cray X-MP/11 (modified) supercomputer delivered to the Sir Leonard Tilley Building in Ottawa, in March 1985. It was, at the time, the most powerful computer in Canada. In the early 1990s, the Establishment purchased a Floating Point Systems FPS 522-EA supercomputer at a cost of $1,620,371. This machine was upgraded to a Cray S-MP superserver after Cray acquired Floating Point Systems in December 1991 and used the Folklore Operating System supplied by the NSA in the US. These machines are now retired. \nLittle information is available on the types of computers used by the CSEC since then. However, Cray in the US has produced a number of improved supercomputers since then. These include the Cray SX-6, early 2000s, the Cray X1, 2003 (development funded in part by the NSA), Cray XD1, 2004, Cray XT3, Cray XT4, 2006, Cray XMt, 2006 and Cray CX1, 2008. It is possible that some of these models have been used by the CSEC and are in use today.\n\nIn 2008, Canada’s most powerful research computer, an IBM supercomputer, was installed in Toronto. The $20 million machine, about the size of an SUV, can make 12.5 trillion computations per second and will be used for proteomics research by the Ontario Cancer Institute, the Princess Margaret Hospital (specializing in cancer) and the University Health Network. An IBM System x iData Plex supercomputer began operation at the University of Toronto in 2009. However, the supercomputer used by Environment Canada for weather forecasting remains the largest in Canada.\n\nCanada's major telephone companies introduced digital technology and fibre optics during this period paving the way for more advanced business and customer telecommunications services.\n\nIn 1976 Nortel developed the first digital private branch exchange (PBX) in the world. That same year Nortel announced its \"Digital World\" project, which foresaw the development and market introduction of a complete family of digital switching, transmission and business communications systems.\n\nIn 1977 Bell Canada began to conduct a fibre optic field trial to residential customers in Montreal. The Manitoba Telephone System began to introduce fibre optics in that province beginning with customers in Elie, in 1981. In 1984 Sasktel, the provincially owned telephone company in Saskatchewan, completed the construction of a 3,268 km. long commercial fibre optics network to fifty-two communities in that province. In 1984 CNCP Telecommunications began the construction of a trans-Canada fibre optic network. By 2009, the national fibre optic system in Canada stretched 7000 kilometers across the country and included underwater fibre links to PEI (1985) and Newfoundland. Eight fibres are reserved for Trans-Canada traffic.\n\nOn 1 July 1985, Cantel and Bell Cellular began to offer cell phone service in Canada. They used a technical standard known as CDMA, which was compatible with mobile phone systems in the US but not elsewhere in the world. The fax began to make its presence felt in offices across Canada in the early 1980s.\n\nThe \"Globe and Mail\" began to produce its contents in electronic form in 1979. A year later in 1980, in order to enable the daily distribution of the Toronto-based paper across the country, in a bid to become Canada's \"national newspaper\", it began the transmission of its contents via Anik Satellite to regional offices, where it was printed and distributed.\n\nBluetooth technology, developed in 1994 by Ericsson, was introduced to Canadian consumers at that time. The technique permits the very short range radio communication of Bluetooth equipped electronic devices with each other for the purposes of information transfer. It is designed to eliminate the need for wires and cables to connect such machines.\n\nThe use of lasers became common throughout Canada during these years. The devices are usually found as components of larger systems.\n\nLasers are used in the field of telecommunications, where they are act as the modulated light sources for fibre optic systems. The high frequency of the pulsed beams of light they produce enables the transmission of large quantities of information and the absence of an electromagnetic field around the fiber optic cable lessens transmission loss and increases the security of the data. Lasers are also used in many mechanical manufacturing systems to start and stop processes, measure component size and monitor and maintain quality.\n\nIn public they are commonly found at the retail checkout counter, where they scan bar codes. They are also used to open and close doors for people and cars and are common in public washrooms, where they control the flow of water for taps and the flushing of urinals and toilettes.\n\nTheir use in medicine has been growing. They are used to \"burn\" plaque from clogged arteries, to remove the decayed portion of teeth in dental treatment and to treat vision problems related to retinal detachment and near sightedness (lasik surgery).\n\nThe Lidar, a laser-based instrument, is used by meteorologists to determine the height of the cloud base.\n\nSmaller vehicles became popular in response to the oil crisis of 1973. In 1981 the Chrysler K platform introduced by that company formed the basis for the compact K-car. Front wheel drive was widely introduced to North America by the Big Three US automakers beginning in 1978, when the Plymouth Horizon and Dodge Omni, both with transverse mounted engines, became available. A wide variety of front wheel-drive-models were quickly offered to Canadians by other car makers. \nThe air bag safety feature was introduced during these years. In the US, the 1973 model Oldsmobile Toronado was the first passenger car equipped with an air bag. Ford introduced air bags as an option in the Tempo in the US in 1984 and Crysler made them a standard feature in 1988. Air bags were made available in the Canadian market as this feature became available in US models.\n\nThe semi-trailer truck (18 wheeler), became the dominant vehicle on the heavily used Highway 401 (Ontario). Containerization, which had made headway in ocean shipping with the construction of terminals in Halifax, Montreal and Vancouver also led to the eventual elimination of the railway box car and began to make inroads in the trucking industry. Light rail systems were built in Edmonton, Alberta in 1978, Calgary, Alberta, in 1981 and Vancouver, British Columbia in 1986.\n\nThe Ontario Highway 407 Express Toll Road (ETR), opened in Toronto in 1997 to ease the burden of traffic on Highway 401 which passes through the heart of the city to the south. Special technology is used to collect tolls without the use to toll booths. Regular users can equip their cars with a transponder that sends a signal to highway sensors when the vehicle enters and leaves the road. For those vehicles without a transponder, special electro-optical sensors read the number plate and a bill for the toll is sent to the vehicle owner in the mail.\n\nIn 1996 GM Canada introduced the OnStar service to Canadian and US customers who chose this option when purchasing a new car. Considered a safety feature, the service provides emergency services, vehicle diagnostics and directions to drivers on the road. It is based on GPS technology as well as CDMA mobile phone technology provided in Canada by Bell Mobility. There is a 24-hour emergency response centre in Oshawa for vehicles located in Canada.\n\nIn the 1990s, a national weather radar surveillance system for aviation and general use was established. The initial system, the Canadian weather radar network, established by Environment Canada, became operational in 1997 and consisted of 18 weather radars using the 5 centimetre wavelength (C-Band) and one using the ten-centimeter wavelength (S-Band). In 1998 that organization received approval to add another 12 radars and to upgrade the system to the Pulse-Doppler radar standard.\n\nAlso in 1997 the responsibility for air traffic control in Canada was transferred from Transport Canada to Nav Canada. The very large and complex control system operated by that organization uses a number of technologies, including 1400 ground-based navigation aids, 46 radars and six Automatic dependent surveillance-broadcast systems, which are based on global positioning technology.\n\nTechnological improvements have also enhanced rail safety during these years. These include a large number of incremental changes to elements of rolling stock including wheels, axles, trucks, couplers and brakes. There have also been improvements to tracks including continuous weld, concrete sleepers and switching techniques. Techniques of traffic control and communication have also been improved.\n\nBombardier's invention of a new class of aircraft, the regional jet or RJ, allowed airlines to introduce jet passenger service to smaller centres. The design of this machine was facilitated through the use of computer-aided design software. In 2009, Bombardier Aerospace of Montreal, in cooperation with the National Research Council Canada began using robots for the assembly of its aircraft.\n\nFactory farming of pigs and chickens in particular became a prominent feature of agriculture during these years. Large numbers of these animals are crowded into very large barns with controlled environments in an effort to maximize their growth and hence profit for the farmer. The use of antibiotics to fight infection, as well as the use of growth hormones is common. The growth in the number of these farms has been dramatic. The Fraser Valley in British Columbia is home to the highest concentration of such farms in Canada and the number of farms there increased from 56 in 1991 to 146 in 2001. The growth of genetically modified crops also became common. One of the most notable in this regard is canola. Developed in Canada from rapeseed during the 1970s by Keith Downey and Baldur Stefansson it is used to produce oil that is low in erucic acid and glucosinolate and has become a major cash crop in North America. A strain of canola with additional modification that made it resistant to herbicide was introduced in Canada in 1996.\n\nThe use of computer-controlled robots in manufacturing (computer-aided manufacturing) as well as the closely associated, just-in-time inventory management technique, were pioneered in Canada by the auto manufacturers, who introduced them to improve efficiency. They were put to use in new auto manufacturing plants built, by Honda Canada in Alliston, Ontario and Toyota Canada in Cambridge, Ontario (1988). The techniques of robotic assembly-line manufacturing have improved over the years. As of 2009 robots form the basis for automobile production in Canada with a number of facilities and companies using this technology, including (city, company, model):\n\nGeomatics, a term that originated in Canada, is a technique of computer-based mapping that integrates information from a variety of sources including, cartography, remote sensing (including images from RADARSAT), surveying, global satellite navigation systems, geodesy and photogrammetry. In the new century it has become an important tool used by Canadians in a variety of endeavours including, commerce, the environment, search and rescue, urban planning, defence and natural resource management among other things. The Geomatics Industry Association of Canada, founded in 1988, presently has a membership of more than 100 organizations.\n\nDuring these years Canada's unmanned space programme included the first launching of a Canadian earth observation satellite, RADARSAT-1 in 1995 and an improved version RADARSAT-2 in 2007. Placed in polar orbits each of these satellites images almost the Earth's entire surface, every 24 days using a powerful synthetic aperture radar, SAR. The images have both operational and scientific applications and their data is of use in geology, hydrology, agriculture, cartography, forestry, climatology, urbanology, environmental studies, meteorology, oceanography and other fields. In 2009 the Canadian Space Agency announced a follow-up programme, RADARSAT Constellation, which will see the launching of three earth observation satellites, in 2014, 2015 and 2016 respectively, working as a trio to provide complete coverage of Canada's land and ocean surfaces as well as 95% of the surface of the world every 24 hours.\n\nThe technology for offshore oil and gas extraction was introduced to Canada during these years. The first of several projects off of Canada’s east coast was the massive Hibernia platform, a gravity base structure (GBS), built in Bull Arm, Newfoundland in the early 1990s. The 1.2 million ton structure was towed into place, 315 km to the south east of St. John's, Newfoundland, over the Hibernia off-shore oil reservoir, where it was positioned resting on the ocean floor in 80 meters of water with its superstructure rising 50 meters above the surface of the sea. In 1997 the facility began to pump oil from the sea bottom. The oil is stored in giant on-board tanks and continuously off-loaded by a fleet of dedicated shuttle tankers which transport it to the shore-based oil refinery at Come-by-Chance, Newfoundland.\n\nThe nearby Terra Nova, 350 km off shore, began to produce oil in 2002. The platform itself rests on the ocean floor and pumps oil from the sea bottom. However, unlike the Hibernia facility, the oil flows directly into a Floating Production Storage and Offloading (FPSO) vessel, the Terra Nova FPSO, where it is processed and stored. The oil in the storage tanks is then removed by a shuttle tanker. A third oil production facility in the same area, the White Rose (oil field), operated by Husky Petroleum, which began operation in 2005 also uses a Floating Production Storage and Offloading (FPSO) vessel, the SeaRose FPSO.\n\nUndersea pipelines, the first in Canada and part of the Sable Offshore Energy Project have been introduced to transport gas from undersea wells off the coast of Nova Scotia since 2000. Gas was discovered near Sable Island in 1979 with the first platform and well head installed in the Thebaud field in 1999. An on-shore gas treatment facility was built at Goldboro and connected to the wellhead with a 225 km long undersea pipeline and production began in 2000. Other fields have been connected via undersea pipeline including the North Triumph, Venture, Alma and South Venture.\n\nOther notable energy works included, the ill-fated east coast Ocean Ranger drilling platform, which sank in a storm in 1982 with the loss of all aboard, the Nova Scotia Power Corporation tidal generating station, Annapolis, 1984 and the Darlington Nuclear Generating Station, Darlington, Ontario, 1990. The construction of large-scale hydro-electric plants far from electric markets lead to the introduction of techniques for long distance electric power transmission. These techniques were used at a number of sites, including the W.A.C Bennett hydro station in British Columbia in 1968, Churchill Falls, Labrador, 1971 and the Robert-Bourassa generating station, 1981, the La Grande-3 generating station, 1984, the La Grande-4 generating station, 1986 and the La Grande-2-A generating station, 1992, all in Quebec.\n\nBiotechnology involves modifying living organisms to serve human goals. Biological techniques are derived from a number of sciences including, biology, chemistry, organic chemistry, biochemistry, genetics, botany, zoology, microbiology and embryology to name a few. In 1987, a number of Canadian companies and university research organizations operating in the field of biotechnology established the Industrial Biotechnology Association of Canada, also known as BIOTECanada. In 2005, the industry consisted of companies offering biotechnologically based services and products in the following sectors: human health (262 firms), agriculture (89 firms), food processing (54 firms), environment (33 firms), bioinformatics (16 firms), natural resources (21 firms) and aquaculture (15 firms). As of 2010 BIOTECanada had more than 250 members.\n\nIn 2005 noted bio-medical spenders on biotechnological research (research spending in millions of C$) included, Apotex Inc. (151.1), Pfizer Canada Inc. (147.5), GlaxoSmithKline Inc. (111.8), Merck Frosst Canada Ltd. (96.6), Biovail Corporation (88.9), AstraZeneca Canada Inc. (79.8) and Sanofi Pasteur Limited (76.6).\n\nAnother product of biotechnology, genetically modified crops, are grown throughout Canada. One of the most widely known, canola was developed in Canada through selective breeding in 1974. Canola oil is used in food products and in non-food items such as lipstick, candles, bio-fuels and newspaper ink. One of the largest suppliers of genetically modified seeds for food crops and animal feed in Canada is Monsanto Canada, established in Winnipeg in 1901.\n\nMedical treatment advanced during these years. The use of lasers and computers became important parts of medical treatment. Computers were essential in the development of new medical imaging devices such as the CAT scan, positron emission tomography and the MRI. Minimally invasive surgery, also known as laparoscopic surgery, reduced surgical damage to patients. Lasers were used with catheters for clearing blocked arteries and catheters with small cameras provided images of conditions inside the body. Coronary bypass surgery became commonplace. Laser eye surgery became popular in the 1990s and was used to improve visual acuity for the near-sighted. New chemical chemotherapy combinations helped prolong the lives of cancer patients. Techniques for the long-term application of medication through the use of a skin patch or implants appeared during these years.\n\nA large number of medical drugs (List of bestselling drugs) for treating a wide variety of ailments such as high blood pressure, high cholesterol levels, arthritis, allergies, anemia, depression, asthma, osteoporosis and diabetes, became available to Canadians during this period.\n\nThe techniques for blood collection, processing and transfusion came under severe criticism in Canada during the 1980s, and led to Canada's worst-ever public health crisis. Between 1980 and 1985, 2000 recipients of tainted blood provided by the Canadian Red Cross were infected with the HI virus. Between 1980 and 1990, 30,000 Canadian transfusion recipients were infected with hepatitis C from tainted blood. About 8000 of those who received bad blood have died or are expected to die as a result. An investigation known as the Royal Commission of Inquiry on the Blood System in Canada was launched in 1993 and issued its final report in 1997.\n\nA private company, IVF Canada of Scarborough, was the first to begin offering in vitro fertilization (IVF) in Canada beginning in 1983. Since that date the company has recorded a number of Canadian \"firsts\" in this field, including the first IVF pregnancy, first IVF twins, the first IVF triplets and the first baby born from a frozen embryo. Beginning in 1998 male erectile difficulties could be treated with the use of Viagra and other medications.\n\nThere were innovations in home design and construction during this period. Houses generally became bigger. New materials such as vinyl siding became common and often replaced more expensive brick for home exteriors. The car port and garage became widespread features and the latter was often located close to the curb, creating a rather crowded streetscape. The home dishwasher and the microwave oven were introduced. Large-screen televisions usually of the cathode ray or projection type were found in many homes. The Sony Walkman, introduced in 1979, quickly gained popularity as a means for listening to music on the go. There were innovations in the field of domestic cuisine including the introduction of microwave popcorn. In 1981, the development of the susceptor bag (a paper bag impregnated with an aluminum-coated polyester film), allowed popcorn therein to be popped in a microwave oven without scorching.\n\nThe compact disc (CD) and the digital video disc (DVD) were introduced at this time. The CD, which appeared in 1982, became a favourite format for musical recordings. By 1986 most music stores in Canada had phased out the LP and replaced it with the CD. It was also used for other purposes including data storage in the form of the CD-ROM. A closely related format, the DVD, with greater storage capacity than the CD, was introduced in 1997. In 1986, Americ Disc of Drummondville, Quebec began to manufacture CDs and after 1997, DVDs and has become one of the largest suppliers of this product in North America. The infrared-based TV remote control became popular with Canadians in the early 1980s.\n\nVideo games have become a wildly popular form of entertainment especially for youth, since the 1980s. The earliest video game dated from 1947 and a number of devices were produced in the 1950s and 1960s. However, it was the development of the computer chip that led to their popularization. The coin-operated arcade game \"Pong\" introduced by Atari in 1972 was the first to become widely available. The next phase of development included the introduction in the mid-1970s of the home console, first with a hardwired game, but then complemented in 1977 by “plug and play”, which allowed the use of game cartridges for variety. Beginning in 1985 PC gaming became popular, exploiting the flexibility and increasing popularity of the personal computer. In 1989 Nintendo released its Game Boy, the first of the hand-held electronic games. Game imagery became more elaborate with the introduction of the 32-bit chip that was featured in the Sony PlayStation released in 1994. The 128-bit, sixth generation of video games was born with the introduction of the Sega Dreamcast in 1998. These technologies found a place in the Canadian consumer market from the moment of their introduction and Canadian companies have played a role in their development, with hardware makers like ATI Technologies developing high-powered video chips for game imagery and software companies developing a number of games.\n\nGore-Tex, a breathable, waterproof textile, was patented in the US in 1980. Clothing made from this product and designed for outdoor all-weather, sporting, athletic and recreational activity became available in Canada shortly thereafter.\n\nThe theme park became popular in the 1980s and the technology of thrill is the main attraction. In Toronto, Canada's Wonderland, Canada’s largest, opened its doors to the public in 1981 and is now tied for second place in North America as the theme park with the most roller coasters (List of roller coasters at Canada's Wonderland). Galaxyland the world's largest indoor amusement park located in the West Edmonton Mall which opened in 1981 has garnered continent-wide attention. The most popular thrill ride, the Mindbender (Galaxyland) is the largest indoor triple-loop roller coaster in North America. The Drop of Doom was another featured attraction until closed in the early 2000s. The Mall's World Waterpark, which opened in 1985 offers bathers a chance to cavort in the world’s largest indoor wave pool. The technology of auto racetrack design and construction has been put to good use in Montreal at the Circuit Gilles Villeneuve, Canada’s premier auto race track and home of the Canadian Grand Prix Formula 1 motor car races since 1978. The slot machine, so dear to gamblers, was introduced during this period. Casinos have been built in Windsor, Caesars Windsor 1994, Niagara Falls, Niagara Fallsview Casino Resort 1996 and Orillia, Casino Rama 1996, Ontario, Montreal, Montreal Casino, Gatineau, Casino du Lac Leamy 1996 and Baie St. Paul, Casino de Charlevoix 1994, Quebec, Halifax, Casino Nova Scotia 1995, Nova Scotia and in Vancouver, the River Rock Casino Resort 2006, British Columbia.\n\nThe world’s first automated teller machine (ATM) service was developed by the Sherwood Credit Union in Regina in 1977, at that institution’s North Albert Branch. Other Canadian financial institutions followed this lead and by the 1980s the ATM was available throughout Canada.\n\nDuring the 1980s the bar code became a familiar feature on consumer products ranging from food to clothes as did the bar code scanner at the retail checkout counter. These two technologies greatly improved the effectiveness of the check-out procedure and improved inventory management as well, through the associated computer accounting of stock. This was one of the factors leading to the technique of just-in-time inventory management for retail, commercial and industrial undertakings.\n\nThe payment of consumer purchases at the retail checkout counter through the use of an electronic debit card was introduced across Canada in 1994. Known as Interac, the system allows the consumer to swipe his personal card and with the use of a personal identification number have the amount of the purchase electronically deducted from his or her bank account. The service has since become very popular.\n\nAlthough the concept of recycling waste materials was not new, the Blue Box Recycling System for domestic refuse collection made the idea highly visible. Initially developed by Laidlaw Waste Systems for the Kitchener, Ontario, in 1983, it was introduced to Ontario municipalities in 1986, by Ontario Multi-Material Recycling Incorporated (OMMRI), and promoted by Nyle Ludolph, who became known as the Father of the Blue Box. The concept involved the use of blue plastic boxes which were distributed to home owners, who in turn filled them with recyclable refuse and placed them at curbside for weekly pickup. The refuse was taken to specially designed plants, where materials were sorted and recycled. The technique became popular in municipalities across Canada in the years that followed.\n\nDuring these years the municipal garbage dump evolved to become the sanitary landfill site. A number of technologies, including clay and plastic liners were used to contain the smell and leachate. The largest in Canada, the Keele Valley Landfill was operated by the City of Toronto from 1983 until 2002, when it was closed because it was full.\n\nChlorofluorocarbons (CFCs), the gas propellants used in aerosol spray cans, became a target for environmental concern in the 1970s and 1980s when research demonstrated that they had a harmful effect on the ozone layer in the atmosphere. The international Montreal Protocol of 1989 banned the use of these substances and they were subsequently replaced with volatile hydrocarbons.\n\nThe problem with choice to reuse is still not available. There are many products that don't need to be recycled for a hundred years, but are put out monthly.\n\nLarge architectural works of note included BC Place, Vancouver, 1983, Petro-Canada Centre, West Tower, Calgary, 1984, the West Edmonton Mall, Edmonton, Alberta, 1986, Scotia Plaza, Toronto 1988, the Canterra Tower, Calgary, 1988, the Sky Dome, Toronto, 1989, Bankers Hall, Calgary, 1989, BCE Place–Canada Trust Tower, Toronto, 1990, the Bay Wellington Tower, Toronto, 1990, Tour du 1000 de la Gauchetière, Montreal, 1991, Tour IBM-Marathon, Montreal, 1992 and GM Place, Vancouver, 1995.\n\nNew arenas for Canada's National Hockey League teams were built, including GM Place, Vancouver, home of the Vancouver Canucks in 1995, the Corel Centre in Ottawa, home of the Ottawa Senators and Molson Centre in Montreal, new home of the Montreal Canadiens, both in 1996.\n\nSignificant new bridges included the Alex Fraser Bridge, Vancouver, 1986, the Skybridge (TransLink), Vancouver, 1989 and the Confederation Bridge, NB-PEI, 1997.\n\nIn the earlier parts of Canada's history, the state often played a crucial role in the diffusion of these technologies, in some cases through a monopoly enterprise, in others with a private \"partner\". In more recent times the need for the role of the state has diminished in the presence of a larger private sector.\n\nIn the latter part of the 20th century there is evidence that Canadian values prefer public expenditures on social programmes at the expense of public spending on the maintenance and expansion of public technical infrastructure. This can be seen in the fact that in 2008 the Federation of Canadian Municipalities estimated that it would take $123 billion to restore and repair aging urban infrastructure across Canada.\n\n"}
{"id": "51130935", "url": "https://en.wikipedia.org/wiki?curid=51130935", "title": "The End of Night (book)", "text": "The End of Night (book)\n\nThe End of Night: Searching for Natural Darkness in an Age of Artificial Light is a 2013 non-fiction book by Paul Bogard on the gradual disappearance, due to light pollution, of true darkness from the night skies of most people on the planet. Bogard examines the effects of this loss on human physical and mental health, society, and ecosystems, and how it might be mitigated.\n\nThe book has been translated into Chinese, German, Japanese, Korean, and Spanish.\n\nBogard's book is structured into nine chapters, roughly corresponding to the nine levels of the Bortle scale, which attempts to quantify the subjective brightness and suitability for astronomy of the sky in different environments. On his use of the scale, which was invented in 2001, Bogard has said, \"one of the reasons why identifying different depths of darkness is so important is that we don’t recognize that we’re losing it, unless we have a name to recognize it by.\"\n\nBogard begins at a Bortle level 9 environment, by the Luxor Sky Beam, the brightest spotlight on Earth, located on the Las Vegas Strip. He explores the nighttime landscapes of London and Paris, and examines the planning, or lack thereof, in each city's lighting. He visits locations throughout the continental US, as well as Florence, the Canary Islands, and the isle of Sark, in his quest to understand the nature of light pollution. He experiences firsthand the deleterious effects of night shift work, talks with a former prison inmate about the psychological effects of uninterrupted light, and shares his own fear of the dark. Bogard ultimately finds a Bortle level 1 environment: an environment so perfectly free of stray light that the Milky Way casts noticeable shadows.\n\nBogard argues against the long-held assumption of a correlation between bright light and reduced crime, citing research that finds no such link. Rather than suggesting a return to the completely unlit nights of centuries past, however, he argues for a careful consideration of where and how light is deployed, in order to provide sufficient nighttime illumination for safety, without creating glare and other unwanted effects.\n\n\"Telegraph\" reviewer Stephanie Cross wrote that \"the appeal of Bogard’s book derives not just from his often wide-eyed enthusiasm for his subject, but also from the constellation of characters he encounters on his journeys into the night.\" In \"The Guardian\", novelist Salley Vickers wrote that \"Bogard sets about his investigations with an energetic purposiveness and enterprise,\" but complained that \"the book comes to seem a little thin, moving too rapidly from one chatty anecdotal meeting to another.\" \"The Wall Street Journal\" questioned Bogard's statements on the relationship between light and safety, and concluded ambivalently: \"\"The End of Night\" delivers a forceful, if incomplete, critique of our overexposed world.\"\n\nThe book was awarded the 2014 Nautilus Silver Award. It was named an Amazon Best Book of the Month and Nonfiction Editor's Pick for July 2013, and \"Gizmodo\" selected it as one of its Best Books of 2013. The book was shortlisted for the PEN/E. O. Wilson Literary Science Writing Award, and was a finalist for the Sigurd F. Olson Nature Writing Award.\n\nBorn in northern Minnesota, Bogard is an assistant professor of English at James Madison University.\n\n"}
{"id": "13854139", "url": "https://en.wikipedia.org/wiki?curid=13854139", "title": "The Journal of Men's Studies", "text": "The Journal of Men's Studies\n\nThe Journal of Men's Studies (abbreviated JMS) is a peer-reviewed journal established in 1992 as the first published by Men's Studies Press. As of 2015 the journal is published by Sage Publications.\n\n"}
{"id": "23875920", "url": "https://en.wikipedia.org/wiki?curid=23875920", "title": "Trees of Pakistan", "text": "Trees of Pakistan\n\nIn Pakistan, more than 430 tree species are distributed over 82 families and 226 genera. Out of these 22 species from 5 families and 11 genera belong to softwood trees of gymnosperms. For all plant families found in Pakistan, see Flora of Pakistan.\n\n\n\n"}
{"id": "5439710", "url": "https://en.wikipedia.org/wiki?curid=5439710", "title": "Water hole (radio)", "text": "Water hole (radio)\n\nThe waterhole, or water hole, is an especially quiet band of the electromagnetic spectrum between 1.42 and 1.67 gigahertz, corresponding to wavelengths of 21 and 18 centimeters respectively. It is a popular observing frequency used by radio telescopes in radio astronomy. The term was coined by Bernard Oliver in 1971. The strongest hydroxyl radical spectral line radiates at 18 centimeters, and hydrogen at 21 centimeters. These two molecules, which combined form water, are widespread in interstellar gas, and their presence absorbs radio noise at these frequencies. Therefore, the spectrum between these frequencies form a \"quiet\" channel in the interstellar radio noise background. Bernard M. Oliver theorized that the waterhole would be an obvious band for communication with extraterrestrial intelligence, hence the name, which is a form of pun: in English, a watering hole is a vernacular reference to a common place to meet and talk. Several programs involved in the search for extraterrestrial intelligence, including SETI@home, search in the waterhole radio frequencies. \n\n"}
