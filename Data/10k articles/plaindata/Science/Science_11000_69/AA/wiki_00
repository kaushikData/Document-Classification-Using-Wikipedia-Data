{"id": "2492046", "url": "https://en.wikipedia.org/wiki?curid=2492046", "title": "Aerugite", "text": "Aerugite\n\nAerugite is a rare complex nickel arsenate mineral with a variably reported formula: Ni(AsO)AsO. It forms green to deep blue-green trigonal crystals. It has a Mohs hardness of 4 and a specific gravity of 5.85 to 5.95.\n\nIt was first described in 1858 in either the South Terres mine of Cornwall, England or in Erzgebirge, Saxony, Germany. The origin is disputed. The most common occurrence is as an incrustation on furnace walls in which ores are \"roasted\". Its name comes from the Latin word \"aerugo\" for \"copper rust\". \n"}
{"id": "12009413", "url": "https://en.wikipedia.org/wiki?curid=12009413", "title": "Ames process", "text": "Ames process\n\nThe Ames process is a process by which pure uranium metal is obtained. It can be achieved by mixing any of the uranium halides (commonly uranium tetrafluoride) with magnesium metal powder or aluminium metal powder.\n\nThe Ames process was used on August 3, 1942 by a group of chemists led by Frank Spedding and Harley Wilhelm at the Ames Laboratory as part of the Manhattan Project. It is a type of thermite-based purification, which was patented in 1895 by German chemist Hans Goldschmidt. Development of the Ames process came at a time of increased research into mass uranium-metal production. The desire for increased production was motivated by a fear of Nazi Germany developing nuclear weapons before the Allies. The process originally involved mixing powdered uranium tetrafluoride and powdered magnesium together. This mixture was placed inside an iron pipe that was welded shut on one side and capped shut on another side. This container, called a \"bomb\" by Spedding, was placed into a furnace. When heated to a temperature of , the contents of the container reacted violently, leaving a 35-gram ingot of pure uranium metal. The process was quickly scaled up; by October 1942 the \"Ames Project\" was producing metal at a rate of per week. The uranium tetrafluoride and magnesium were sealed in a refractory-lined reactor vessel, still referred to as a \"bomb\". The thermite reaction was initiated by furnace heating the assembly to ; the large difference in density between slag and metal allowed complete separation in the liquid state, yielding slag-free metal. By July 1943, the production rate exceeded of uranium metal per month. Approximately 1000 tons of uranium ingots were produced at Ames before the process was transferred to industry.\n\nThe Ames project received the Army-Navy \"E\" Award for Excellence in Production on October 12, 1945, signifying 2.5 years of excellence in industrial production of metallic uranium as a vital war material. Iowa State University is unique among educational institutions to have received this award for outstanding service, an honor normally given to industry.\n\nThe metallothermic reduction of anhydrous rare-earth fluorides to rare-earth metals is also referred to as the Ames process.\n\nThe study of rare earths was also advanced during World War II: Synthetic plutonium was believed to be rare-earth-like, it was assumed that knowledge of rare earths would assist in planning for and the study of transuranic elements; ion-exchange methods developed for actinide processing were forerunners to processing methods for rare-earth oxides; methods used for uranium were modified for plutonium, which were subsequently the basis for rare-earth metal preparation.\n"}
{"id": "19278728", "url": "https://en.wikipedia.org/wiki?curid=19278728", "title": "Architectural engineering", "text": "Architectural engineering\n\nArchitectural Engineering, also known as Building Engineering or Architecture Engineering, is the application of engineering principles and technology to building design and construction. Definitions of an architectural engineer may refer to:\n\nStructural engineering involves the analysis and design of the built environment (buildings, bridges, equipment supports, towers and walls). Those concentrating on buildings are sometimes informally referred to as \"building engineers\". Structural engineers require expertise in strength of materials, structural analysis, and in predicting structural load such as from weight of the building, occupants and contents, and extreme events such as wind, rain, ice, and seismic design of structures which is referred to as earthquake engineering. Architectural Engineers sometimes incorporate structural as one aspect of their designs; the structural discipline when practiced as a specialty works closely with architects and other engineering specialists.\n\nMechanical engineering and electrical engineering engineers are specialists, commonly referred to as (Mechanical, Electrical and Plumbing) when engaged in the building design fields. Also known as \"building services engineering\" in the United Kingdom, Canada and Australia. Mechanical engineers often design and oversee the heating, ventilation and air conditioning (HVAC), plumbing and rain gutter systems. Plumbing designers often include design specifications for simple active fire protection systems, but for more complicated projects, fire protection engineers are often separately retained. Electrical engineers are responsible for the building's power distribution, telecommunication, fire alarm, signalization, lightning protection and control systems, as well as lighting systems.\n\nIn many jurisdictions of the United States, the architectural engineer is a licensed engineering professional. Usually a graduate of an architectural engineering university program preparing students to perform whole-building design in competition with architect-engineer teams; or for practice in one of structural, mechanical or electrical fields of building design, but with an appreciation of integrated architectural requirements. \n\nFormal architectural engineering education, following the engineering model of earlier disciplines, developed in the late 19th century, and became widespread in the United States by the mid-20th century. With the establishment of a specific \"architectural engineering\" NCEES Professional Engineering registration examination in the 1990s, and first offering in April 2003, architectural engineering became recognized as a distinct engineering discipline in the United States. \n\nIn most license-regulated jurisdictions, architectural engineers are not entitled to practice architecture unless they are also licensed as architects, and may be restricted from the practice of structural engineering on specific types of higher importance buildings such as hospitals. Regulations and customary practice vary widely by region or city.\n\nIn some countries, the practice of architecture includes planning, designing and overseeing the building's construction, and architecture, as a profession providing architectural services, is referred to as \"architectural engineering\". In Japan, a \"first-class architect\" plays the dual role of architect and building engineer, although the services of a licensed \"structural design first-class architect\" are required for buildings over a certain scale.\n\nIn some languages, such as Korean and Arabic, \"architect\" is literally translated as \"architectural engineer\". In some countries, an \"architectural engineer\" (such as the \"ingegnere edile\" in Italy) is entitled to practice architecture and is often referred to as an architect. These individuals are often also structural engineers. In other countries, such as Germany, Austria, Iran, and most of the Arab countries, architecture graduates receive an engineering degree (\"Dipl.-Ing. – Diplom-Ingenieur\").\n\nIn Spain, an \"architect\" has a technical university education and legal powers to carry out building structure and facility projects.\n\nIn Brazil, architects and engineers used to share the same accreditation process (Conselho Federal de Engenheiros, Arquitetos e Agrônomos (CONFEA) – Federal Council of Engineering, Architecture and Agronomy). Now the Brazilian architects and urbanists have their own accreditation process (CAU – Architecture and Urbanism Council). Besides traditional architecture design training, Brazilian architecture courses also offer complementary training in engineering disciplines such as structural, electrical, hydraulic and mechanical engineering. After graduation, architects focus in architectural planning, yet they can be responsible to the whole building, when it concerns to small buildings (except in electric wiring, where the architect autonomy is limited to systems up to 30kVA, and it has to be done by an Electrical Engineer), applied to buildings, urban environment, built cultural heritage, landscape planning, interiorscape planning and regional planning.\n\nIn Greece licensed architectural engineers are graduates from architecture faculties that belong to the Polytechnic University, obtaining an \"Engineering Diploma\". They graduate after 5 years of studies and are fully entitled architects once they become members of the Technical Chamber of Greece (TEE – Τεχνικό Επιμελητήριο Ελλάδος).\nThe Technical Chamber of Greece has more than 100,000 members encompassing all the engineering disciplines as well as architecture.\nA prerequisite for being a member is to be licensed as a qualified engineer or architect and to be a graduate of an engineering and architecture schools of a Greek university, or of an equivalent school from abroad.\nThe Technical Chamber of Greece is the authorized body to provide work licenses to engineers of all disciplines as well as architects, graduated in Greece or abroad. The license is awarded after examinations. The examinations take place three to four times a year. The Engineering Diploma equals a master's degree in ECTS units (300) according to the Bologna Accords.\n\nThe architectural, structural, mechanical and electrical engineering branches each have well established educational requirements that are usually fulfilled by completion of a university program.\nWhat differentiates architectural engineering from architecture (architect) as a separate and single, integrated field of study, compared to other engineering disciplines, is its multi-disciplinary engineering approach. Through training in and appreciation of architecture, the field seeks integration of building systems within its overall building design. Architectural engineering includes the design of building systems including heating, ventilation and air conditioning (HVAC), plumbing, fire protection, electrical, lighting, architectural acoustics, and structural systems. In some university programs, students are required to concentrate on one of the systems; in others, they can receive a generalist architectural or building engineering degree.\n\n"}
{"id": "7872324", "url": "https://en.wikipedia.org/wiki?curid=7872324", "title": "Artificial intelligence systems integration", "text": "Artificial intelligence systems integration\n\nThe core idea of Artificial Intelligence systems integration is making individual software components, such as speech synthesizers, interoperable with other components, such as common sense knowledgebases, in order to create larger, broader and more capable A.I. systems. The main methods that have been proposed for integration are message routing, or communication protocols that the software components use to communicate with each other, often through a middleware blackboard system.\n\nMost artificial intelligence systems involve some sort of integrated technologies, for example the integration of speech synthesis technologies with that of speech recognition. However, in recent years there has been an increasing discussion on the importance of systems integration as a field in its own right. Proponents of this approach are researchers such as Marvin Minsky, Aaron Sloman, Deb Roy, Kristinn R. Thórisson and Michael A. Arbib. A reason for the recent attention A.I. integration is attracting is that there have already been created a number of (relatively) simple A.I. systems for specific problem domains (such as computer vision, speech synthesis, etc.), and that integrating what's already available is a more logical approach to broader A.I. than building monolithic systems from scratch.\n\nThe focus on systems integration, especially with regard to modular approaches, derive from the fact that most intelligences of significant scales are composed of a multitude of processes and/or utilize multi-modal input and output. For example, a humanoid-type of intelligence would preferably have to be able to talk using speech synthesis, hear using speech recognition, understand using a logical (or some other undefined) mechanism, and so forth. In order to produce artificially intelligent software of broader intelligence, integration of these modalities is necessary.\n\nCollaboration is an integral part of software development as evidenced by the size of software companies and the size of their software departments. Among the tools to ease software collaboration are various procedures and standards that developers can follow to ensure quality, reliability and that their software is compatible with software created by others (such as W3C standards for webpage development). However, collaboration in fields of A.I. has been lacking, for the most part not seen outside of the respected schools, departments or research institutes (and sometimes not within them either). This presents practitioners of A.I. systems integration with a substantial problem and often causes A.I. researchers to have to 're-invent the wheel' each time they want a specific functionality to work with their software. Even more damaging is the \"not invented here\" syndrome, which manifests itself in a strong reluctance of A.I. researchers to build on the work of others.\n\nThe outcome of this in A.I. is a large set of \"solution islands\": A.I. research has produced numerous isolated software components and mechanisms that deal with various parts of intelligence separately. To take some examples:\n\n\nWith the increased popularity of the free software movement, a lot of the software being created, including A.I. systems, that is available for public exploit. The next natural step is to merge these individual software components into coherent, intelligent systems of a broader nature. As a multitude of components (that often serve the same purpose) have already been created by the community, the most accessible way of integration is giving each of these components an easy way to communicate with each other. By doing so, each component by itself becomes a module which can then be tried in various settings and configurations of larger architectures.\n\nMany online communities for A.I. developers exist where tutorials, examples and forums aim at helping both beginners and experts build intelligent systems (for example the AI Depot, Generation 5). However, few communities have succeeded in making a certain standard or a code of conduct popular to allow the large collection of miscellaneous systems to be integrated with any ease. Recently, however, there have been focused attempts at producing standards for A.I. research collaboration, Mindmakers.org is an online community specifically created to harbor collaboration in the development of A.I. systems. The community has proposed the OpenAIR message and routing protocol for communication between software components, making it easier for individual developers to make modules instantly integrateble into other peoples' projects.\n\nThe Constructionist design methodology (CDM, or 'Constructionist A.I.') is a formal methodology proposed in 2004, for use in the development of cognitive robotics, communicative humanoids and broad AI systems. The creation of such systems requires integration of a large number of functionalities that must be carefully coordinated to achieve coherent system behavior. CDM is based on iterative design steps that lead to the creation of a network of named interacting modules, communicating via explicitly typed streams and discrete messages. The OpenAIR message protocol (see below) was inspired by the CDM, and has frequently been used to aid in development of intelligent systems using CDM.\n\nOne of the first projects to use CDM was Mirage, an embodied, graphical agent visualized through augmented reality which could communicate with human users and talk about objects present in the user's physical room. Mirage was created by Kristinn R. Thórisson, the creator of CDM, and a number of students at Columbia University in 2004. The methodology is actively being developed at Reykjavik University.\n\nOpenAIR is a message routing and communication protocol that has been gaining in popularity over the past two years. The protocol is managed by Mindmakers.org, and is described on their site in the following manner:\n\n\"\"OpenAIR is a routing and communication protocol based on a publish-subscribe architecture. It is intended to be the \"glue\" that allows numerous A.I. researchers to share code more effectively — \"AIR to share\". It is a definition or a blueprint of the \"post office and mail delivery system\" for distributed, multi-module systems. OpenAIR provides a core foundation upon which subsequent markup languages and semantics can be based, e.g. gesture recognition and generation, computer vision, hardware-software interfacing etc.; for a recent example see CVML\".\"\n\nOpenAIR was created to allow software components that serve their own purpose to communicate with each other to produce large scale, overall behavior of an intelligent systems. A simple example would be to have a speech recognition system, and a speech synthesizer communicate with an expert system through OpenAIR messages, to create a system that can hear and answer various questions through spoken dialogue. CORBA (see below) is an older but similar architecture that can be used for comparison, but OpenAIR was specifically created for A.I. research, while CORBA is a more general standard.\n\nThe OpenAIR protocol has been used for collaboration on a number of A.I. systems, a list can be found on the Mindmakers project pages. Psyclone is a popular platform to pair with the OpenAIR protocol (see below).\n\nPsyclone is a software platform, or an AI operating system (AIOS), developed by Communicative Machines Laboratories for use in creating large, multi modal A.I. systems. The system is an implementation of a blackboard system that supports the OpenAIR message protocol. Psyclone is available for free for non-commercial purposes and has therefore often been used by research institutes on low budgets and novice A.I. developers.\n\nElvin is a content-based router with a central routing station, similar to the Psyclone AIOS (see above).\n\nThe OOA is a hybrid architecture that relies on a special inter-agent communication language (ICL) – a logic-based declarative language which is good for expressing high-level, complex tasks and natural language expressions.\n\nThe Common Object Request Broker Architecture (CORBA) is a standard that enables software components written in multiple computer languages and running on multiple computers to interoperate. CORBA is defined by the Object Management Group (OMG). CORBA follows similar principles as the OpenAIR protocol (see above), and can be used for A.I. systems integration.\n\nThe Messaging Open Service Interface Definition (OSID) is an O.K.I. specification which provides a means of sending, subscribing and receiving messages. OSIDs are programmatic interfaces which comprise a Service Oriented Architecture for designing and building reusable and interoperable software.\n\n\n\n\n"}
{"id": "1940", "url": "https://en.wikipedia.org/wiki?curid=1940", "title": "Astronomer Royal", "text": "Astronomer Royal\n\nAstronomer Royal is a senior post in the Royal Households of the United Kingdom. There are two officers, the senior being the Astronomer Royal dating from 22 June 1675; the second is the Astronomer Royal for Scotland dating from 1834.\n\nThe post was created by King Charles II in 1675, at the same time as he founded the Royal Observatory Greenwich. He appointed John Flamsteed, instructing him \".\"\n\nThe Astronomer Royal was director of the Royal Observatory Greenwich from the establishment of the post in 1675 until 1972. The Astronomer Royal became an honorary title in 1972 without executive responsibilities and a separate post of Director of the Royal Greenwich Observatory was created to manage the institution.\n\nThe Astronomer Royal today receives a stipend of 100 GBP per year and is a member of the Royal Household, under the general authority of the Lord Chamberlain. After the separation of the two offices, the position of Astronomer Royal has been largely honorary, though he remains available to advise the Sovereign on astronomical and related scientific matters, and the office is of great prestige.\n\nThere was also formerly a Royal Astronomer of Ireland.\n\n\n"}
{"id": "9989464", "url": "https://en.wikipedia.org/wiki?curid=9989464", "title": "Australian National Commission for UNESCO", "text": "Australian National Commission for UNESCO\n\nThe Australian National Commission for UNESCO was established under Section 7 of the \"United Nations Educational, Scientific and Cultural Organization Act,\" 1947 (Cwlth) and is the Australian government organisation responsible for advising on the implementation of UNESCO policies and programmes in Australia and for advising on Australia's involvement with UNESCO. The Commission comprises a Secretariat and a total of eighteen (18) members, and operates under the Charter of the Australian National Commission for UNESCO, a statutory instrument pursuant to Section 7 the above legislation. As peace education is fundamental to the mission of UNESCO, so too advising on the promotion and advancement of peace education in Australia is central to the role of the Australian National Commission for UNESCO.\n\n\n"}
{"id": "15984030", "url": "https://en.wikipedia.org/wiki?curid=15984030", "title": "Avanti (project)", "text": "Avanti (project)\n\nAvanti was established by the UK Department of Trade and Industry in 2002 to formulate an approach to collaborative working in order to enable construction project partners to work together effectively. The project was promoted by the Department of Trade and Industry with the support of most of the largest UK firms in the construction industry. Avanti also involved the International Alliance for Interoperability (IAI), Loughborough University and Co-Construct, a network of five construction research and information organizations.\n\nThe Avanti programme aimed to help overcome problems caused by incomplete,\ninaccurate and ambiguous information.\n\nThe Tavistock Institute report (1965) printed an extract from an RICS meeting of 1910 which stated \"Architectural information is invariably inaccurate, ambiguous and incomplete\". By the 1940s, the impact of this was valued at an additional 10% to the construction cost. By 1994 the Latham Report \"Constructing the Team\" suggested waste in the industry accounted for 25-30% of project costs. This figure is supported by earlier publications from the Building Research Establishment (BRE) and the Building Services Research and Information Association (BSRIA), which led to reports in 1987 from the Coordinating Committee for Project Information (now CPIC) that recommended a common format and disciplined approach to reduce the problem. These suggestions were proven on DTI-funded projects and case studies at Endeavour House at London Stansted Airport and the BAA Heathrow Express, which showed that significant savings in project cost and drawing production could be made by addressing people and process issues.\n\nMost construction projects are complex and involve co-ordination between many project participants both on- and off-site. As projects have increased in size and complexity, the number of participants involved in a single project has also increased. To improve collaboration between participants, the industry needs tools that work well, appropriate processes to implement them, and the right approach from the people involved. Tackling these points was at the heart of the Avanti action research programme.\n\nIts purpose was to support live construction project teams, as they collaborate to deliver projects, by helping them apply the appropriate technologies. It worked through a team of partners representing each stage of the project process, with the objective of improving project and business performance through the use of information and communication technologies (ICT) to support collaborative working. It set out to show the advantages of CAD systems over conventional drawings, with information managed as a database rather than a drawing cabinet, and aimed to demonstrate that 3D Computer Aided Design systems could revolutionise projects, provided they were used for more than visualisation.\n\nTo reduce the risks involved in adopting new working methods, Avanti brought together areas of current best practice (such as CPIC protocols) that had previously gained too little market penetration to have significant impact on the sector. The objective was to deliver improved project and business performance by using ICT to support collaborative working.\n\nAvanti focused on early access to all project information by all partners, on early involvement of the supply chain, and on sharing of information, drawings and schedules, in an agreed and consistent manner. The Avanti approach was supported by handbooks, toolkits and on-site mentoring and was reliant on advice and materials provided by CPIC.\n\nAvanti mobilised existing technologies to improve business performance by increasing the quality of information and predictability of outcomes and by reducing risk and waste. The core of the Avanti approach to a project's whole life cycle was based on team-working and access to a common information model. All CAD information was generated with the same origin, orientation and scale, and organised in layers that could be shared. All layers and CAD model files were named consistently within a specific Avanti convention to allow others to find the relevant CAD data.\n\nLearning from early projects, Avanti produced practical working documentation and tested the methods on live projects. This information supplemented the CPIC document Code of Procedure for the Construction Industry and addressed problems of incomplete, inaccurate and ambiguous information. Avanti guidance included:\n\nThe advice also covers areas such as:\n\nFor most projects, using this guidance consisted of three phases:\n\n\nIn July 2006, the Avanti DTI Project documentation and brand ownership was transferred to Constructing Excellence. Since the handover, Constructing Excellence endeavoured to promote the savings demonstrated on live projects. Further work was also carried out to make Avanti part of the update of BS 1192. The BS 5555 committee coded the methods.\n\nEvaluation of the impacts of the Avanti project showed: \n\nConstructing Excellence is developing a self-sustaining business to support roll out of the Avanti methodology to the UK construction industry. It will also draw on the results of other DTI-supported collaborative research such as \"Building Down Barriers\".\n\n\n"}
{"id": "56285516", "url": "https://en.wikipedia.org/wiki?curid=56285516", "title": "Axel Gavelin", "text": "Axel Gavelin\n\nAxel Olof Gavelin (1875–1947) was Swedish geologist active at the Geological Survey of Sweden of which he was director from 1916 to 1941. He studied both ice-dammed lakes and Precambrian rocks across Sweden.\n"}
{"id": "4629929", "url": "https://en.wikipedia.org/wiki?curid=4629929", "title": "Bibliographic database", "text": "Bibliographic database\n\nA bibliographic database is a database of bibliographic records, an organized digital collection of references to published literature, including journal and newspaper articles, conference proceedings, reports, government and legal publications, patents, books, etc. In contrast to library catalogue entries, a large proportion of the bibliographic records in bibliographic databases describe articles, conference papers, etc., rather than complete monographs, and they generally contain very rich subject descriptions in the form of keywords, subject classification terms, or abstracts.\n\nA bibliographic database may be general in scope or cover a specific academic discipline like computer science. A significant number of bibliographic databases are proprietary, available by licensing agreement from vendors, or directly from the indexing and abstracting services that create them.\n\nMany bibliographic databases have evolved into digital libraries, providing the full text of the indexed contents. Others converge with non-bibliographic scholarly databases to create more complete disciplinary search engine systems, such as Chemical Abstracts or Entrez.\n\nPrior to the mid-20th century, individuals searching for published literature had to rely on printed bibliographic indexes, generated manually from index cards. \"During the early 1960s computers were used to digitize text for the first time; the purpose was to reduce the cost and time required to publish two American abstracting journals, the \"Index Medicus\" of the National Library of Medicine and the \"Scientific and Technical Aerospace Reports\" of the National Aeronautics and Space Administration (NASA). By the late 1960s such bodies of digitized alphanumeric information, known as bibliographic and numeric databases, constituted a new type of information resource. Online interactive retrieval became commercially viable in the early 1970s over private telecommunications networks. The first services offered a few databases of indexes and abstracts of scholarly literature. These databases contained bibliographic descriptions of journal articles that were searchable by keywords in author and title, and sometimes by journal name or subject heading. The user interfaces were crude, the access was expensive, and searching was done by librarians on behalf of 'end users'.\n\n"}
{"id": "43640928", "url": "https://en.wikipedia.org/wiki?curid=43640928", "title": "Blanca Renée Arrillaga", "text": "Blanca Renée Arrillaga\n\nBlanca Renée Arrillaga Oronoz de Maffei (Artigas, 1917 - 2011) was a Uruguayan chemist, botanist, professor and agrostologist. Originally from the Uruguayan Department of Artigas, she began her studies there then moved to Montevideo where she earned a degree in Pharmaceutical Chemistry from the Faculty of Chemistry at the University of the Republic (Uruguay). Her most important publications include 'Nuevas especies y notas taxonómicas en Uruguay y Paraguay' (1968) in collaboration; Plantas Medicinales (1969) and Gramíneas Uruguayas (1970) with B. Rosengurtt and P. Izaguirre de Artucio.\n"}
{"id": "51494119", "url": "https://en.wikipedia.org/wiki?curid=51494119", "title": "Charles Green (cook)", "text": "Charles Green (cook)\n\nCharles Green (24 November 1888 – 26 September 1974), also known as Charlie Green, was a British ship's cook, who took part in Sir Ernest Shackleton's Imperial Trans-Antarctic Expedition as the cook for the Weddell sea party on board the \"Endurance\". The son of a master baker, Charles learnt to bake, but ran away at the age of 22 to join the Merchant Navy. Whilst in Buenos Aires on board the \"Andes\" in October 1914, he heard word that Shackleton had fired the expedition's cook, for drunkenness, and was subsequently hired.\n\nDuring the expedition, Green was assisted by able seaman Perce Blackborow, who had come on board as a stowaway. When the ship sank after being trapped in the Antarctic ice, he continued to cook for the crew during their camps on the Antarctic ice, with much more limited equipment and the use of a blubber stove. A few days after the crew arrived on Elephant Island, Green collapsed from exhaustion, and was ordered to rest until he recovered.\n\nAfter the crew were rescued, Green returned to England, where he joined the Royal Navy as a cook, taking part in the war. He was wounded in August 1918 whilst serving on the Destroyer H.M.S. Wakeful, and re-joined the Merchant Navy in 1919.\n\nGreen was invited to re-join Shackleton on another expedition to Antarctica, the Shackleton–Rowett Expedition, along with many other crew members from \"Endurance\". During this trip, Shackleton gave Green a set of lantern-slides, before his death shortly after arrival at South Georgia. The expedition continued without Shackleton, but was not a great success.\n\nAfter returning to England in 1922, Green once again rejoined the Merchant Navy, working on board a variety of ships. Using his lantern slides, he gave many talks about the Endurance mission at ports around the World, and continued to give these lectures after retiring from the Navy in 1931.\n\nHe became a Fire Watcher during World War II within the city of Hull.\n\nGreen was one of the last surviving members of the \"Endurance\" crew, and attended the 50th Anniversary reunion in 1964 with the two remaining survivors.\n"}
{"id": "3536648", "url": "https://en.wikipedia.org/wiki?curid=3536648", "title": "Decoy effect", "text": "Decoy effect\n\nIn marketing, the decoy effect (or attraction effect or asymmetric dominance effect) is the phenomenon whereby consumers will tend to have a specific change in preference between two options when also presented with a third option that is \"asymmetrically dominated\". An option is asymmetrically dominated when it is inferior in all respects to one option; but, in comparison to the other option, it is inferior in some respects and superior in others. In other words, in terms of specific attributes determining preferences, it is completely dominated by (i.e., inferior to) one option and only partially dominated by the other. When the asymmetrically dominated option is present, a higher percentage of consumers will prefer the dominating option than when the asymmetrically dominated option is absent. The asymmetrically dominated option is therefore a decoy serving to increase preference for the dominating option. The decoy effect is also an example of the violation of the independence of irrelevant alternatives axiom of decision theory.\n\nSuppose there is a consideration set (options to choose from in a menu) that involves MP3 players. Consumers will generally see higher storage capacity (number of GB) and lower price as positive attributes; while some consumers may want a player that can store more songs, other consumers will want a player that costs less. In Consideration Set 1, two devices are available:\n\nIn this case, some consumers will prefer \"A\" for its greater storage capacity, while others will prefer \"B\" for its lower price.\n\nNow suppose that a new player, \"C\", the \"decoy\", is added to the market; it is more expensive than both \"A\", the \"target\", and \"B\", the \"competitor\", and has more storage than \"B\" but less than \"A\":\n\nThe addition of decoy \"C\" — which consumers would presumably avoid, given that a lower price can be paid for a model with more storage—causes \"A\", the dominating option, to be chosen more often than if only the two choices in Consideration Set 1 existed; \"C\" affects consumer preferences by acting as a basis of comparison for \"A\" and \"B\". Because \"A\" is better than \"C\" in both respects, while \"B\" is only partially better than \"C\", more consumers will prefer \"A\" now than did before. \"C\" is therefore a decoy whose sole purpose is to increase sales of \"A\".\n\nConversely, suppose that instead of \"C\", a player \"D\" is introduced that has less storage than both \"A\" and \"B\", and that is more expensive than \"B\" but not as expensive as \"A\":\n\nThe result here is similar: consumers will not prefer \"D\", because it is not as good as \"B\" in any respect. However, whereas \"C\" increased preference for \"A\", \"D\" has the opposite effect, increasing preference for \"B\".\n\nAnother example showed in Dan Ariely's Predictably Irrational was a true case used by \"The Economist\" magazine. In a subscription screen is presented 3 options like below:\n\n\nWhen presented like above,16% of the students in the experiment conducted by Ariely chose the first option, 0% chose the middle option, and 84% chose the third option. Even though nobody picked the second option, when he removed that option the result was the inverse; 68% of the students picked the online only option, and 32% chose the print and web option.\n\nThe decoy effect is usually measured by comparing the frequency of choice of the target, \"A\" in the absence of the decoy, \"C\", compared with when the decoy is present in the consideration set. The decoy effect can also be measured as how much more a consumer is ready to pay to choose the target rather than the competitor.\n\nIn 2018, researchers from the University of Colorado at Denver and the Chinese Academy of Sciences published a study of 168 workers in three food factories in China. For the first 20 days of the study, workers were provided with spray bottles of sanitizer to clean their hands and workstations, which they were meant to do hourly. Then the researchers offered workers the choice of their usual spray bottle or a less convenient option, specifically a squeeze bottle of sanitizer or a wash basin. The workers increased their use of the original spray bottles from their baseline 60% to over 90%.\n\nThe debate on the existence and relevance of the attraction effect was recently renewed. New research points out that the attraction effect does not appear in realistic purchasing scenarios, for example when options are presented graphically, or when the target and the competitor are not exactly of the same value.\n\nThe original authors had to underline again that the attraction effect occurs only if the consumer is close to indifference between the target and the competitor, if both dimensions of the products (in our example, price and storage capacity) are about as important as each other to the consumer, if the decoy is not too undesirable, and if the dominance relation is easy to identify. A recent study has indeed confirmed that the attraction effect persists when options are presented graphically, i.e., as scatter plots.\n\n"}
{"id": "59195982", "url": "https://en.wikipedia.org/wiki?curid=59195982", "title": "Emily Shuckburgh", "text": "Emily Shuckburgh\n\nEmily Fleur Shuckburgh, is a climate scientist and science communicator. She leads the Data Science Group and is Deputy head of the Polar Oceans Team at the British Antarctic Survey and is also a fellow of Darwin College Cambridge. Her research interests include the dynamics of the atmosphere, oceans and climate. She is a theoretician, numerical modeller and observational scientist.\n\nFollowing a BS in Mathematics at the University of Oxford, Emily Shuckburgh completed a PhD at the University of Cambridge in Applied mathematics. After post doctoral research at Ecole Normal Superieure in Paris and at MIT, she joined the British Antarctic Survey where she leads the Natural Environment Research Council Ocean Regulation of Climate by Heat and Carbon Sequestration and Transports (ORCHESTRA) project. As a fellow of Darwin College, Cambridge she holds several positions within the University of Cambridge; she is an associate fellow of the Centre for Science and Policy and fellow of the Cambridge Institute for Sustainability Leadership. Her research interests include the dynamics of the atmosphere, oceans and climate. She is a theoretician, numerical modeller and observational scientist.\n\nDr Shuckburgh is a fellow of the Royal Meteorological Society, where she is co-Chair of their Climate Science Communications Group and a former Chair of their Scientific Publications Committee. She has also acted as an advisor to the UK Government on behalf of the Natural Environment Research Council . \n\nDr Shuckburgh has written on climate science, sustainability and women in science for publications including the Financial Times, The New Statesman and the Sunday Times. She serves on the board of the Campaign for Science and Engineering. \n\nIn 2016 she was awarded an OBE for services to science and the public communication of science. She co-authored Climate Change (A Ladybird Expert Book) with HRH The Prince of Wales and Tony Juniper\n\n"}
{"id": "21154613", "url": "https://en.wikipedia.org/wiki?curid=21154613", "title": "Evelyn O'Rourke", "text": "Evelyn O'Rourke\n\nEvelyn O'Rourke is an Irish broadcaster employed by RTÉ\n\nO'Rourke joined RTÉ in 1998 as a researcher on \"The Gay Byrne Show\" and \"The Arts Show\" on RTÉ Radio 1 before moving on to become roving reporter on \"Today with Pat Kenny\" on RTÉ Radio 1 and \"The Gerry Ryan Show\" on RTÉ 2fm. She has served as a presenter on several RTÉ programmes, including \"Drivetime\" \"Liveline\" and \"Morning Ireland\" on RTÉ Radio 1 as well as an annual \"Countdown to the Leaving Cert\" series on RTÉ 2fm on which she offered advice to Leaving Certificate students. She also was a regular contributor to the \"It Says in the Papers\" slot on Morning Ireland. Other television work includes Crimecall, The Daily Show, Seoige and as a judge on TV talent shows \"Glas Vegas\" and \"The All Ireland Talent Show\".\n\nO'Rourke was born in Dublin. She attended Coláiste Iosagáin on Dublin's Southside. Having completed her school studies she attended Trinity College, Dublin to study Drama and English from 1990 – 1994. After this she moved to the West of Ireland to work in film and television. In 1998 O'Rourke returned to Dublin, joining RTÉ Radio, working with the veteran broadcaster Gay Byrne during his final season on radio. She was then transferred to \"The Arts Show\" with Mike Murphy, and later to \"Today with Pat Kenny\" where she was employed as a reporter. She then worked in radio as Ryan Tubridy's first \"newspaper lady\" on the now-defunct RTÉ 2fm breakfast show \"The Full Irish\". In September 2002, she began reporting for \"The Gerry Ryan Show\".\n\nO'Rourke began working for RTÉ in 1998 as a researcher on \"The Gay Byrne Show\". Her next job was on \"The Arts Show\" and she then joined \"Today with Pat Kenny\" as a reporter. She was then promoted to the position of \"The Gerry Ryan Show\"s roving reporter on RTÉ 2fm.\n\nShe has presented many programmes on radio and TV including 'Liveline', 'Today with Pat Kenny', 'The Gerry Ryan Show' and she presented a tribute show to Gerry Ryan the day after his death in 2010, her first show since having a baby.Head of 2FM John McMahon, hugged his tearful wife Evelyn O’Rourke, a Gerry Ryan Show reporter for eight years, after she signed the condolence book. She presented a special tribute to Ryan on Saturday morning, in her first radio show since returning from maternity leave. The couple regarded Ryan as a friend as well as a colleague. \"Both Gerry and John are gadget freaks,\" said O’Rourke. \"We keep saying ‘are’, we keep talking about him in the present tense,\" her husband responded.\n\nO'Rourke took on the role of radio reporter and television presenter for the RTÉ cross-media event \"Operation Transformation\" in 2008. She performed the role of \"cheerleader\" to her companion Gerry Ryan's \"ringmaster\". She was also helpful in the choosing of the 2010 participants despite her first pregnancy and childbirth occurring around this time.<\n\nAmongst her other television appearances are \"Seoige\", \"The Cafe\" and \"The Lucy Kennedy Show\".\n\nO'Rourke appeared on \"The Panel\" on 29 January 2009.\n\nO'Rourke was a judge on the January 10 edition of series two of \"The All Ireland Talent Show\" after Shane Lynch's flight from London was cancelled, despite having given birth to her first son just six weeks earlier. She also served as an advisor to Shane Lynch on \"The All Ireland Talent Show'. She was also a judge on TG4 variety show \"Glas Vegas\"\n\nO'Rourke is married to John McMahon (b. 10 December),and they have two sons, Oisin and Ross. O'Rourke holidays in West Kerry each year with her family and friends.\n\nOn December 2, 2009, she gave birth to her first son, Oisin Brian McMahon and her second son Ross Pascal on February 2, 2011.\nShe returned to RTÉ in November 2011 where she joined the Radio 1 Arts and Culture programme 'Arena' as a reporter. Since her return she has also appeared on a number of programmes ranging from 'Crime Call' as stand in presenter, 'The Today programme' as contributor on RTÉ television, and 'The Colm Hayes Show' on 2fm. She is now the reporter on the popular RTÉ Radio 1 arts and popular culture programme Arena.\n\n"}
{"id": "56174514", "url": "https://en.wikipedia.org/wiki?curid=56174514", "title": "Explorer 26", "text": "Explorer 26\n\nExplorer 26 was an American satellite launched on December 21, 1964, as part of NASA's Explorers program. Its primary mission was to study the Earth's magnetic field.\n\nExplorer 26 was a spin-stabilized satellite weighing 45.8 kg. It carried 5 experiments: Solid-State Electron Detector, Omnidirectional and Unidirectional Electron and Proton Fluxes, Fluxgate Magnetometers, Proton-Electron Scintillation Detector, and Solar Cell Damage. The Solar Cell Damage experiment was intended to quantify the degradation of solar cell performance due to radiation, and evaluate the effectiveness of glass shields at preventing this degradation. \n"}
{"id": "63452", "url": "https://en.wikipedia.org/wiki?curid=63452", "title": "Heuristic", "text": "Heuristic\n\nA heuristic technique (; , \"find\" or \"discover\"), often called simply a heuristic, is any approach to problem solving, learning, or discovery that employs a practical method, not guaranteed to be optimal, perfect, logical, or rational, but instead sufficient for reaching an immediate goal. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. Heuristics can be mental shortcuts that ease the cognitive load of making a decision. Examples that employ heuristics include using a rule of thumb, an educated guess, an intuitive judgment, a guesstimate, stereotyping, profiling, or common sense.\n\nHeuristics are the strategies derived from previous experiences with similar problems. These strategies rely on using readily accessible, though loosely applicable, information to control problem solving in human beings, machines, and abstract issues.\n\nThe most fundamental heuristic is trial and error, which can be used in everything from matching nuts and bolts to finding the values of variables in algebra problems.\n\nHere are a few other commonly used heuristics, from George Pólya's 1945 book, \"How to Solve It\":\n\nIn psychology, heuristics are simple, efficient rules, learned or hard-coded by evolutionary processes, that have been proposed to explain how people make decisions, come to judgements, and solve problems typically when facing complex problems or incomplete information. Researchers test if people use those rules with various methods. These rules work well under most circumstances, but in certain cases lead to systematic errors or cognitive biases.\n\nThe study of heuristics in human decision-making was developed in the 1970s and 80s by psychologists Amos Tversky and Daniel Kahneman, although the concept was originally introduced by Nobel laureate Herbert A. Simon. Simon's original, primary object of research was problem solving which showed that we operate within what he calls bounded rationality. He coined the term \"satisficing\", which denotes the situation where people seek solutions or accept choices or judgments that are \"good enough\" for their purposes, but could be optimized.\n\nRudolf Groner analyzed the history of heuristics from its roots in ancient Greece up to contemporary work in cognitive psychology and artificial intelligence, and proposed a cognitive style \"heuristic versus algorithmic thinking\" which can be assessed by means of a validated questionnaire.\n\nGerd Gigerenzer focused on the \"fast and frugal\" properties of heuristics, i.e., using heuristics in a way that is principally accurate and thus eliminating most cognitive bias. Heuristics – like the recognition heuristic or the take-the-best heuristic – are viewed as special tools that tackle specific tasks (e.g., binary choice) under conditions of uncertainty and are organized in an \"adaptive toolbox\". From one particular batch of research, Gigerenzer and Wolfgang Gaissmaier found that both individuals and organizations rely on heuristics in an adaptive way. They also found that ignoring part of the information [with a decision], rather than weighing all the options, can actually lead to more accurate decisions.\n\nHeuristics, through greater refinement and research, have begun to be applied to other theories, or be explained by them. For example: the cognitive-experiential self-theory (CEST) also is an adaptive view of heuristic processing. CEST breaks down two systems that process information. At some times, roughly speaking, individuals consider issues rationally, systematically, logically, deliberately, effortfully, and verbally. On other occasions, individuals consider issues intuitively, effortlessly, globally, and emotionally. From this perspective, heuristics are part of a larger experiential processing system that is often adaptive, but vulnerable to error in situations that require logical analysis.\n\nIn 2002, Daniel Kahneman and Shane Frederick proposed that cognitive heuristics work by a process called \"attribute substitution\", which happens without conscious awareness. According to this theory, when somebody makes a judgment (of a \"target attribute\") that is computationally complex, a rather easier calculated \"heuristic attribute\" is substituted. In effect, a cognitively difficult problem is dealt with by answering a rather simpler problem, without being aware of this happening. This theory explains cases where judgments fail to show regression toward the mean. Heuristics can be considered to reduce the complexity of clinical judgements in healthcare.\n\n\n\nHeuristics were also found to be used in the manipulation and creation of cognitive maps. Cognitive maps are internal representations of our physical environment, particularly associated with spatial relationships. These internal representations of our environment are used as memory as a guide in our external environment. It was found that when questioned about maps imaging, distancing, etc., people commonly made distortions to images. These distortions took shape in the regularization of images (i.e., images are represented as more like pure abstract geometric images, though they are irregular in shape).\n\nThere are several ways that humans form and use cognitive maps. Visual intake is a key part of mapping. The first is by using \"landmarks\". This is where a person uses a mental image to estimate a relationship, usually distance, between two objects. The second is \"route-road\" knowledge, and is generally developed after a person has performed a task and is relaying the information of that task to another person. The third is a \"survey\". A person estimates a distance based on a mental image that, to them, might appear like an actual map. This image is generally created when a person's brain begins making image corrections. These are presented in five ways: 1. \"Right-angle bias\" is when a person straightens out an image, like mapping an intersection, and begins to give everything 90-degree angles, when in reality it may not be that way. 2. \"Symmetry heuristic\" is when people tend to think of shapes, or buildings, as being more symmetrical than they really are. 3. \"Rotation heuristic\" is when a person takes a naturally (realistically) distorted image and straightens it out for their mental image. 4. \"Alignment heuristic\" is similar to the previous, where people align objects mentally to make them straighter than they really are. 5. \"Relative-position heuristic\": people do not accurately distance landmarks in their mental image based on how well they remember that particular item.\n\nAnother method of creating cognitive maps is by means of auditory intake based on verbal descriptions. Using the mapping based from a person's visual intake, another person can create a mental image, such as directions to a certain location.\n\n\"Heuristic device\" is used when an entity X exists to enable understanding of, or knowledge concerning, some other entity Y. A good example is a model that, as it is never identical with what it models, is a heuristic device to enable understanding of what it models. Stories, metaphors, etc., can also be termed heuristic in that sense. A classic example is the notion of utopia as described in Plato's best-known work, \"The Republic\". This means that the \"ideal city\" as depicted in \"The Republic\" is not given as something to be pursued, or to present an orientation-point for development; rather, it shows how things would have to be connected, and how one thing would lead to another (often with highly problematic results), if one would opt for certain principles and carry them through rigorously.\n\n\"Heuristic\" is also often used as a noun to describe a rule-of-thumb, procedure, or method. Philosophers of science have emphasized the importance of heuristics in creative thought and constructing scientific theories. (See \"The Logic of Scientific Discovery\" by Karl Popper; and philosophers such as Imre Lakatos, Lindley Darden, William C. Wimsatt, and others.)\n\nIn legal theory, especially in the theory of law and economics, heuristics are used in the law when case-by-case analysis would be impractical, insofar as \"practicality\" is defined by the interests of a governing body.\n\nThe present securities regulation regime largely assumes that all investors act as perfectly rational persons.\nIn truth, actual investors face cognitive limitations from biases, heuristics, and framing effects.\n\nFor instance, in all states in the United States the legal drinking age for unsupervised persons is 21 years, because it is argued that people need to be mature enough to make decisions involving the risks of alcohol consumption. However, assuming people mature at different rates, the specific age of 21 would be too late for some and too early for others. In this case, the somewhat arbitrary deadline is used because it is impossible or impractical to tell whether an individual is sufficiently mature for society to trust them with that kind of responsibility. Some proposed changes, however, have included the completion of an alcohol education course rather than the attainment of 21 years of age as the criterion for legal alcohol possession. This would put youth alcohol policy more on a case-by-case basis and less on a heuristic one, since the completion of such a course would presumably be voluntary and not uniform across the population.\n\nThe same reasoning applies to patent law. Patents are justified on the grounds that inventors must be protected so they have incentive to invent. It is therefore argued that it is in society's best interest that inventors receive a temporary government-granted monopoly on their idea, so that they can recoup investment costs and make economic profit for a limited period. In the United States, the length of this temporary monopoly is 20 years from the date the application for patent was filed, though the monopoly does not actually begin until the application has matured into a patent. However, like the drinking-age problem above, the specific length of time would need to be different for every product to be efficient. A 20-year term is used because it is difficult to tell what the number should be for any individual patent. More recently, some, including University of North Dakota law professor Eric E. Johnson, have argued that patents in different kinds of industries – such as software patents – should be protected for different lengths of time.\n\nStereotyping is a type of heuristic that all people use to form opinions or make judgments about things they have never seen or experienced. They work as a mental shortcut to assess everything from the social status of a person based on their actions to assumptions that a plant that is tall, has a trunk, and has leaves is a tree even though the person making the evaluation has never seen that particular type of tree before.\n\nStereotypes, as first described by journalist Walter Lippmann in his book \"Public Opinion\" (1922), are the pictures we have in our heads that are built around experiences as well as what we are told about the world.\n\nA heuristic can be used in artificial intelligence systems while searching a solution space. The heuristic is derived by using some function that is put into the system by the designer or by adjusting the weight of branches based on how likely each branch is to lead to a goal node.\n\nThe concept of heuristics has critiques and controversies. The \"We Cannot Be That Dumb\" critique shows that the average person has low ability to make sound and effective judgments.\n\n\n"}
{"id": "4055891", "url": "https://en.wikipedia.org/wiki?curid=4055891", "title": "High-energy X-rays", "text": "High-energy X-rays\n\nHigh-energy X-rays or HEX-rays are very hard X-rays, with typical energies of 80–1000 keV (1 MeV), about one order of magnitude higher than conventional X-rays (and well into gamma-ray energies over 120 keV). They are produced at modern synchrotron radiation sources such as the beamline ID15 at the European Synchrotron Radiation Facility (ESRF). The main benefit is the deep penetration into matter which makes them a probe for thick samples in physics and materials science and permits an in-air sample environment and operation. Scattering angles are small and diffraction directed forward allows for simple detector setups.\n\nHigh-energy X-rays (HEX-rays) between 100 and 300 keV bear unique advantage over conventional hard X-rays, which lie in the range of 5–20 keV They can be listed as follows:\n\n\nWith these advantages, HEX-rays can be applied for a wide range of investigations. An overview, which is far from complete:\n\n\n\n"}
{"id": "3030749", "url": "https://en.wikipedia.org/wiki?curid=3030749", "title": "It Came from Outer Space (book)", "text": "It Came from Outer Space (book)\n\nIt Came From Outer Space was a 2003 publication of four versions of Ray Bradbury's screen treatments written in 1952 for the movie of the same name that was released in 1953. The treatments range from a 37-page outline to a 119-page story. Bradbury did not write the final screenplay.\nBradbury's previously unpublished short story \"A Matter of Taste\" was the inspiration for his treatments, and was included in this edition. The collection was edited by Donn Albright, who also edited Gauntlet Press' 2005 version of Bradbury's \"The Halloween Tree\".\n"}
{"id": "54577445", "url": "https://en.wikipedia.org/wiki?curid=54577445", "title": "James William Newton", "text": "James William Newton\n\nCaptain James William Newton (1831–1906) claimed to have invented the foghorn technique by using loud and low notes.\n"}
{"id": "56022757", "url": "https://en.wikipedia.org/wiki?curid=56022757", "title": "Jan Lexell", "text": "Jan Lexell\n\nJan Lexell (born March 13, 1958) is a Swedish physician and academic, who is a specialist in rehabilitation medicine and neurology. He is head of the rehabilitation medicine research group in the Department of Health Sciences at Lund University, Lund. One of his research areas is the effect of physical activity on the aging process. Lexell is also senior consultant in the Department of Neurology and Rehabilitation Medicine at Skane University Hospital, Lund. Since 1 January 2018, Lexell has been a professor of rehabilitation medicine in the Department of Neuroscience at Uppsala University and Senior consultant at Uppsala University Hospital. Lexell's research is frequently cited; in particular, his work during the 1980s on examining the vastus lateralis muscle immediately post-mortem, which helped identify an atrophy in type-2 fiber area in older people, Some of his most important research was carried out in 1989-90, while working at the University of Liverpool, UK, with grants from several major Swedish medical organisations.\n\nJan Lexell was born in Luleå, Sweden, where he attended elementary school and high school. As a junior, he was active in sports. He was a junior national team player in table tennis and took 5th place at the European Table Tennis Championships for under-20s in 1975 as his best result. After completing high school, he continued his education at Umeå University. He has said that it was his great interest in training and the human body that led him to medical education. Lexell studied as a doctor and graduated in 1984. A year later, in 1985, he obtained a medical degree, also at Umeå University, with the thesis \"Distribution of different fiber types in human skeletal muscles\". After his dissertation, he continued his muscle research and specialist education to become a neurologist in Umeå. This took him to Liverpool University in the UK. In 1995 he moved to Skåne to study rehabilitation medicine. In parallel with his clinical work and research in neurological rehabilitation in Lund, he helped set up the Department of Health Sciences at Sunderby Hospital, Sweden, in 2000. His work there has led to almost 50 doctoral dissertations. Until 2010, Lexell was also adjunct professor of clinical neuroscience at Luleå University of Technology. In 2013 he was appointed Professor of physical medicine and rehabilitation at Lund University and in 2016 as a visiting professor at the Department of Health Sciences, Luleå University of Technology.\n\nLexell is actively engaged in sports medicine, in particular paralympic sports medicine. Lexell is a standing member of the Medical committee in the International Paralympic Committee (IPC), and was directly involved in preparing Sweden's paralympic athletes for the 2016 Summer Paralympics in Rio. He was a contributor to the \"Textbook of Sports Medicine: Basic Science and Clinical Aspects of Sports Injury and Physical Activity\", published by Wiley in 2003, revised 2008.\n\n\n"}
{"id": "25154846", "url": "https://en.wikipedia.org/wiki?curid=25154846", "title": "Jiří Baum", "text": "Jiří Baum\n\nJiří Baum (20 September 1900 – 1944, Warsaw) was a Czech zoologist, museum curator, explorer and writer. He served as the director of the zoological department of the National Museum in Prague and is best known in his field for his 1933 book \"Through the African Wilderness\" and his 1935 zoological expedition in the Australian outback. In Australia, Baum teamed with Walter Drowley Filmer due to his local expertise with spiders.\n\n"}
{"id": "20308233", "url": "https://en.wikipedia.org/wiki?curid=20308233", "title": "Large Optical Test and Integration Site", "text": "Large Optical Test and Integration Site\n\nThe Large Optical Test and Integration Site, or LOTIS, is a facility at the Lockheed Martin Space Systems Company in Sunnyvale, CA for testing large optical components under realistic conditions. LOTIS has large, thermally stabilized vacuum chamber, vibration-isolated optical benches, and a large 6.5 meter telescope (run backwards as a collimator) to create images for the test optics to view. It can generate images from the visible through the mid-wavelength IR.\n\nThe fabrication error in the Hubble Space Telescope main mirror is the classic example that shows the need for such testing. In this case, a test facility such as LOTIS was not easily available, so the completed optics were not tested as a unit, and instead relied on careful testing of the individual components. Unfortunately, due to a mis-assembled null corrector, one of the tests was wrong, and a mis-figured mirror was not discovered until the telescope was placed into service. This is exactly the type of problem that LOTIS can detect, since it is designed to test large optical systems as a unit by creating realistic inputs and operating conditions for them.\n\n"}
{"id": "39319146", "url": "https://en.wikipedia.org/wiki?curid=39319146", "title": "Lentiviral vector in gene therapy", "text": "Lentiviral vector in gene therapy\n\nLentiviral vectors in gene therapy is a method by which genes can be inserted, modified, or deleted in organisms using lentivirus.\n\nLentivirus are a family of viruses that are responsible for notable diseases like HIV. The lentivirus is unique in that it has been the basis of research using viruses in gene therapy. To be effective in gene therapy, there must be insertion, alteration and/or removal of host cell genes. To do this scientists use the lentivirus' mechanisms of infection to achieve a desired outcome to gene therapy.\n\nTo understand the capabilities of a lentiviral vector, one has to consider the biology of the infection process. The lentivirus is a retrovirus, meaning it has a single stranded RNA genome with a reverse transcriptase enzyme. Lentiviruses also have a viral envelope with protruding glycoproteins that aid in attachment to the host cell's outer membrane. The virus contains a reverse transcriptase molecule found to perform transcription of the viral genetic material upon entering the cell. Within the viral genome are RNA sequences that code for specific proteins that facilitate the incorporation of the viral sequences into the host cell genome. The \"gag\" domain codes for the structural components of the virus like the capsid, the matrix, nucleoproteins. The \"pol\" domain codes for the reverse transcriptase and integrase enzymes. Lastly, the \"env\" domain of the viral genome encodes for the glycoproteins and envelope on the surface of the virus.\n\nThere are multiple steps involved in the infection and replication of a lentivirus in a host cell. In the first step the virus uses its surface glycoproteins for attachment to the outer surface of a cell. More specifically, lentiviruses attach to the CD40 ligand glycoproteins on the surface of a hosts target cell. The viral material is then injected into the host cell's cytoplasm. Within the cytoplasm the viral reverse transcriptase enzyme performs reverse transcription of the viral RNA genome to create a viral DNA genome. The viral DNA is then sent into the nucleus of the host cell where it is incorporated into the host cell's genome with the help of the viral enzyme integrase. From now on, the host cell starts to transcribe the entire viral RNA and express the structural viral proteins, in particular those that form the viral capsid and the envelop. The lentiviral RNA and the viral proteins than assemble and the newly formed virions burst from the host cell when enough are made.\n\nSome experimental applications of lentiviral vectors have been done in gene therapy in order to cure diseases like Diabetes mellitus, Murine haemophilia A, prostate cancer, chronic granulomatous disease, and vascular diseases. Therapy requires manipulation of the lentivirus genes and structure for delivery of specific genes to alter the course of the disease. Researchers and scientists do this through the use of transinfection, which is the action of infecting a cell with a microbe to gain different genes or desired traits. HIV-derived lentiviral vectors have been used for introducing libraries of complementary DNAs, short hairpin RNAs, and cis-regulatory elements into many targets, including embryonic stem cells.\n\nIn a study designed to enhance the outcomes of vascular transplant through vascular endothelial cell gene therapy, the third generation of Lentivirus showed to be effective in the delivery of genes to moderate venous grafts and transplants in procedures like coronary artery bypass. Because the virus has been adapted to lose most of its genome, the virus becomes safer and more effective in transplanting the required genes into the host cell. A draw back to this therapy is explained in the study that long-term gene expression may require the use of promoters and can aid in a greater trans-gene expression. The researchers accomplished this by the addition of self-inactivating plasmids and creating a more universal tropism by pseudotyping a vesicular stomatis virus glycoprotein.\n\nIn chronic granulomatous disease immune functioning is deficient as a result of the loss of nicotinomide adenine dinucleotide phosphate oxidase (NADP) in phagocyte cells, which aids in lipid and nucleic acid synthesis. If this becomes deficient, the bodies immune responses become weakened. Study performed in mice emphasizes the use of lineage-specific lentiviral vectors for the production of NADP. Scientists developed this strain of lentivirus by transinfecting 293T cells with pseudotyped virus with the vesicular stomatitis G protein. The viral vector's responsibility was to increase the gene synthesis and production of NADP in these phagocytic cells. They did this to create an affinity for myeloid cells.\n\nWith prostate cancer, the Lentivirus is transformed by being bound to trastuzumab to attach to androgen-sensitive LNCaP and castration-resistant C4-2 human prostate cancer cell lines. These two cells are primarily responsible for secretion of excess human epidermal growth factor receptor 2 (HER-2), which is a hormone linked to prostate cancer. By attaching to these cells and changing their genomes, the Lentivirus can slow down, and even kill, the cancer causing cells. Researchers caused the specificity of the vector by manipulating the Fab region of the viral genome and pseudotyped it with the Sindbis virus.\n\nHaemophilia A has also been studied in gene therapy with a lentiviral vector in mice. The vector targets the haematopoietic cells in order to increase the amount of factor VIII, which is affected in haemophilia A. But this continues to be a subject of study as the lentivirus vector was not completely successful in achieving this goal. They did this by trans-infecting the virus in a 293T cell, creating a virus known as 2bF8 expressing generation of viral vectors.\n\nStudies have also found that injection of a lentiviral vector with IL-10 expressing genes in utero in mice can suppress, and prevent, rheumatoid arthritis and create new cells with constant gene expression. This contributes to the data on stem cells and in utero inoculation of viral vectors for gene therapy. The target for the viral vector in this study, were the synovial cells. Normally functioning synovial cells produce TNFα and IL-1.\n\nLike many of the in utero studies, the lentiviral vector gene therapy for diabetes mellitus is more effective in utero as the stem cells that become affected by the gene therapy create new cells with the new gene created by the actual viral intervention. The vector targets the cells within the pancreas to add insulin secreting genes to help control diabetes mellitus. Vectors were cloned using a cytomegalovirus promoter and then cotransinfected in the 293T cell.\n\n"}
{"id": "56523460", "url": "https://en.wikipedia.org/wiki?curid=56523460", "title": "List of British Standards", "text": "List of British Standards\n\nBritish Standards are the standards produced by BSI Group which is incorporated under a Royal Charter (and which is formally designated as the National Standards Body (NSB) for the UK). The BSI Group produces British Standards under the authority of the Charter, which lays down as one of the BSI's objectives to:\n\n"}
{"id": "21318116", "url": "https://en.wikipedia.org/wiki?curid=21318116", "title": "List of English inventors and designers", "text": "List of English inventors and designers\n\nThis is a list of English inventors and designers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "3648390", "url": "https://en.wikipedia.org/wiki?curid=3648390", "title": "List of astronomy journals", "text": "List of astronomy journals\n\nThis is a list of scientific journals publishing articles in astronomy, astrophysics, and space sciences.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "2680902", "url": "https://en.wikipedia.org/wiki?curid=2680902", "title": "List of compounds with carbon number 1", "text": "List of compounds with carbon number 1\n\nThis is a partial list of molecules that contain 1 carbon atom.\n\nEach hydrogen added to a molecule can be considered as a proton plus a one-electron reduction of the redox state, while each oxygen counts as a two-electron oxidation. Thus a net addition of HO is a simple hydration with no net change in redox state and frequently occurs reversibly in aqueous solution. These relationships may be viewed in the following table (peroxides and radicals are excluded; unstable or hypothetical compounds are italicized).\n\n"}
{"id": "16368217", "url": "https://en.wikipedia.org/wiki?curid=16368217", "title": "List of countries by number of broadband Internet subscriptions", "text": "List of countries by number of broadband Internet subscriptions\n\nThis article contains a sortable list of countries by number of broadband Internet subscriptions and penetration rates, using data compiled by the International Telecommunication Union.\n\nThe list includes figures for both fixed wired broadband subscriptions and mobile cellular subscriptions:\n\n\nPenetration rate is the percentage (%) of a country's population that are subscribers. A dash (—) is shown when data for 2012 is not available. Non-country and disputed areas are shown in \"italics\". Taiwan is listed as a sovereign country.\n\nNote: Because a single Internet subscription may be shared by many people and a single person my have more than one subscription, the penetration rate will not reflect the actual level of access to broadband Internet of the population and penetration rates larger than 100% are possible.\n\n\n"}
{"id": "48695181", "url": "https://en.wikipedia.org/wiki?curid=48695181", "title": "List of female scientists in the 20th century", "text": "List of female scientists in the 20th century\n\nThis is a \"historical\" list, intended to deal with the time period when women working in science were rare. For this reason, this list deals only with the 20th century. Some women who primarily worked in the 19th or 21st centuries may appear in a different list.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "799398", "url": "https://en.wikipedia.org/wiki?curid=799398", "title": "List of important publications in economics", "text": "List of important publications in economics\n\nThis is a list of important publications in economics, organized by field.\n\nSome reasons why a particular publication might be regarded as important:\n\n\nDescription: The book is usually considered to be the beginning of modern economics. It begins with a discussion of the Industrial Revolution. Later it critiques the mercantilism and a synthesis of the emerging economic thinking of his time. It is best known for the idea of the invisible hand , although this idea is only mentioned once in the book. Smith was critical of the 'vile maxim' of the 'masters of mankind', all for themselves and nothing for other people. The Butcher, the Baker, and the Brewer provide goods and services to each other out of self-interest; the unplanned result of this division of labor is a better standard of living for all three.\n\nImportance: Topic creator, Breakthrough, Influence, Introduction\n\n\nDescription: Elaborates, clarifies and corrects previous theories, and adds important new concepts\n\nImportance: Breakthrough, influence (esp on Marx), broadened scientific foundations of economics\n\n\nDescription: Das Kapital is a political-economic treatise by Karl Marx. Marx wrote this critical analysis of capitalism and of the political economy from the perspective of historical materialism, the view that history can be understood as a sequence of modes of production in which exploiting classes extract an economic surplus from exploited classes.\n\nImportance: Breakthrough, Influence\n\n\nDescription: Describes how poverty in the midst of plenty results from unequal rights to use natural resources, and declining wages in the face of increasing labor productivity results from the Law of Rent. Advocated Georgism, specifically a land value tax.\n\nImportance: Influence, Breakthrough...\n\n\n\"Influence\": Credited with co-founding of marginal utility analysis and the Austrian School of economics.\n\n\n\"Influence\": Standard text for generations of economics students.\n\n\nImportance:: Influential multi-level, best-selling principles textbook that popularized neoclassical synthesis of Keynesian economics and neoclassical economics.\n\n\nDescription: See Importance.\n\nImportance: The book \"built\" on ordinal utility and mainstreamed the now-standard distinction between the substitution effect and the income effect for an individual in demand theory in the 2-good case. It generalized analysis to the case of one good and all other goods, that is, the \"composite good\". It aggregated individuals and businesses through demand and supply across the economy. It anticipated the aggregation problem, most acutely for the stock of capital goods. It introduced general equilibrium theory to an English-speaking audience, refined the theory, and for the first time attempted a rigorous statement of stability conditions for general equilibrium.\n\nAmong the most important list of publication in macroeconomics are:\n\n\nDescription: In this book, Keynes put forward a theory based upon the notion of aggregate demand to explain variations in the overall level of economic activity, such as were observed in the Great Depression. The total income in a society is defined by the sum of consumption and investment; and in a state of unemployment and unused production capacity, one can only enhance employment and total income by first increasing expenditures for either consumption or investment.\n\nImportance: Topic creator, Breakthrough, Influence\n\n\nDescription: Friedman and Schwartz used changes in monetary aggregates to explain business cycle fluctuations in the United States economy.\n\nImportance: Influence\n\n\nDescription: The book by the mathematician John von Neumann and economist Oskar Morgenstern. It contained a mathematical theory of economic and social organization, based on a theory of games of strategy.\n\nThis is now a classic work, upon which modern-day game theory is based. Game theory has since been widely used to analyze real-world phenomena from arms races to optimal policy choices of presidential candidates, from vaccination policy to major league baseball salary negotiations. It is today established, both throughout the social sciences and in a wide range of other sciences.\n\nImportance: Topic creator, Influence\n\n\nThe book showed how operationally meaningful theorems can be described with a small number of analogous methods, thus providing \"a general theory of economic theories.\" It moved mathematics out of the appendices (as in John R. Hicks's Value and Capital) and helped change how standard economic analysis across subjects could be done with the same mathematical methods.\n\nImportance and Influence: Accelerated change in standard methods\n\n\nDescription:\n\nImportance:\n\n\nDescription:\n\nImportance:\n\n\nDescription:\n\nImportance :\n\n\nDescription:\n\nImportance:\n\n\nDescription: Describes the Dickey–Fuller test.\n\nImportance:\n\nDescription: Emphasizes the difference between statistical significance and economic significance, and shows that the understanding is not clear in a review of papers from \"The American Economic Review\".\n\nImportance: Raised the caution against \"asterisk economics\" in econometrics to another level. See McCloskey critique.\n\n\nDescription:\n\nImportance:\n\nDescription: Extensive study about the theoretical inclusion and empirical importance of education in production.\n\nImportance: Classic study of how investment in an individual's education and training is similar to business investments.\n\nDescription: Empirical investigation of the labor market returns to education.\n\nImportance: Popularizing the empirical research in that subfield. Coining the so-called \"Mincer equation\".\n\n\nDescription: Definitive one-volume resource on the field.\n\nImportance: Introduction\n\n\nDescription:\n\nImportance:\n\n\nDescription: In this article, Prospect theory, a descriptive theory of choices under uncertainty, is introduced, bringing together ideas from psychology (framing and probability weighting) and economics (expected utility).\n\nImportance: Topic creator, Breakthrough\n\n\nDescription: A first structured and methodical survey of economic methods, with a focus on methodology.\n\nImportance: Consolidation of the field, methodological issues.\n\n\nDescription: A new and insightful handbook for advanced experimental and behavioral economics students.\n\nImportance: Introduction\n\n\nDescription: The experimental economics handbook.\n\nImportance: Introduction, Influence\n\n\nDescription: Development of the utility framework which shows an optimum can be reached using a portfolio of investments. In effect the first real proof that you should not put all your eggs in one basket.\n\nImportance: Precursor to most modern portfolio theory work in finance.\n\n\nDescription: Development of the Capital asset pricing model used to determine appropriate prices for assets.\n\nImportance: Topic creator, Influence\n\n\nDescription: It developed the Black–Scholes model for determining the price of options, in particular stock options. The use of the Black–Scholes formula has become pervasive in financial markets, and has been extended by numerous refinements.\n\nImportance: Breakthrough, Influence\n\nThe Entropy Law and the Economic Process (1971, Harvard University Press) by Nicholas Georgescu-Roegen.\n\nSteady-State Economics (2nd edition, 1991, Island Press) by Herman Daly\n\nNatural Capitalism, Paul Hawken\n\nSmall Is Beautiful, E.F. Schumacher\n\n\"Economics and Consumer Behavior\", Deaton & Muellbauer, Cambridge.\n\n\nDescription:\n\nImportance:\n\n\nDescription:\n\nThis is an advanced undergraduate text that does not require knowledge of calculus (although some prior knowledge would be an advantage) or of game theory. The text covers many of the basic ideas and theorems of game theory and industrial organisation, with some more advanced applications at the end of the text.\n\nImportance:\n\n\nDescription:\n\nImportance:\n\n\nDescription:\n\nImportance:\n\n\nDescription:\n\nImportance:\n\n\n\nDescription: First modern development economics textbook\n\nImportance: Introduction\n\n\nDescription: Widely used textbook.\n\nImportance: Introduction\n\n\nDescription: Widely used textbook.\n\nImportance: Introduction\n\nDescription: examines the last 30 years of development economics, viewed through the World Bank's World Development Reports.\n\n\nDescription: Pigou was the one of the most influential economists that dealt with Welfare economics. He developed the idea of Pigovian tax.\n\nImportance: Topic creator, Breakthrough, Influence\n\n\nDescription: Inspired renewed interest in basic welfare issues, mentioned in Sen's Nobel citation\n\nImportance: Influence\n\nDescription: Explores the \"specific differentia of medical care as the object of normative economics\", demonstrating that the consideration of uncertainty is key to understanding markets in health care.\n\nImportance: Generally considered a seminal work of enduring significance; key to the foundation of health economics as a field of study.\n\nDescription: The standard health economics textbook in most leading universities. It assumes some background knowledge in economics.\n\nImportance: Introduction.\n\n\nDescription: The most comprehensive available collection of essays on contemporary health economics. Advanced readers will appreciate its mathematical rigor. Those who are seeking research or dissertation topics should find this two-volume set to be an invaluable resource.\n\n\n"}
{"id": "10393997", "url": "https://en.wikipedia.org/wiki?curid=10393997", "title": "List of naturally occurring tryptamines", "text": "List of naturally occurring tryptamines\n\nThe following table lists tryptamine alkaloids endogenous to living organisms:\n"}
{"id": "40778748", "url": "https://en.wikipedia.org/wiki?curid=40778748", "title": "List of people with ankylosing spondylitis", "text": "List of people with ankylosing spondylitis\n\nThis is a list of people, living or dead, accompanied by verifiable source citations associating them with ankylosing spondylitis, either based on their own public statements, or (in the case of dead people only) reported contemporary or posthumous diagnoses. Ankylosing spondylitis (AS) is a type of arthritis in which there is long term inflammation of the joints of the spine. Typically the joints where the spine joins the pelvis are also affected. Occasionally other joints such as the shoulders or hips are involved. \n"}
{"id": "49143122", "url": "https://en.wikipedia.org/wiki?curid=49143122", "title": "List of women anthropologists", "text": "List of women anthropologists\n\nThis is a list of women anthropologists.\n\n"}
{"id": "1518042", "url": "https://en.wikipedia.org/wiki?curid=1518042", "title": "MOATA", "text": "MOATA\n\nMOATA was a 100 kW thermal Argonaut class reactor built at the Australian Atomic Energy Commission (later ANSTO) Research Establishment at Lucas Heights, Sydney. MOATA went critical at 5:50am on 10 April 1961 and ended operations on 31 May 1995. MOATA was the first reactor to be decommissioned in Australia in 2009.\n\nThe design of university training reactor MOATA was based on the Argonaut research reactor developed by the Argonne National Laboratory in the mid-1950s, in the United States. Moata is an Aboriginal name meaning \"gentle-fire\" or \"fire-stick\". \n\nMOATA was designed and built by the Advanced Technology Laboratories and first went critical on 10 April 1961.\n\nThe purpose of the reactor was for training scientists, however in the mid-1970s it was expanded to include activation analysis and neutron radiography. MOATA initially offered training in reactor control and neutron physics, later neutron activation analysis, neutron radiography, soil analysis and nuclear medicine research.\n\nThe reactor was shut down in 1995 as it was no longer possible, after 34 years, to economically justify its continued operations. Experimental data on nuclear fuel and moderator systems was also accumulated during the operation of the reactor. With the dismantling of the reactor complete in 2009, the site has been completely restored. It was the first reactor to be decommissioned in Australia. \n\nIn 1995 the used fuel from the reactor was unloaded and in 2006, it was shipped to the United States under the US DoE Foreign Research Reactor Spent Nuclear Fuel Acceptance.\n"}
{"id": "12820028", "url": "https://en.wikipedia.org/wiki?curid=12820028", "title": "Mandelbaum effect", "text": "Mandelbaum effect\n\nThe Mandelbaum effect is the tendency for the eye to focus nearby in conditions of poor visibility. It was first codified by J. Mandelbaum in 1960. Because dirty glass can aggravate the effect, potentially causing a pilot or driver to miss seeing an obstacle or hazard, it is a strong safety argument for keeping windows clean.\n\nWhen visibility is poor, as at night during rainstorms or fog, the eye tends to relax and focus on its best distance, technically known as \"empty field\" or \"dark focus\". This distance is usually just under one meter (one yard), but varies considerably among people. The tendency is aggravated by objects close to the eye, drawing focus closer.\n\nIt has been shown that the Mandelbaum effect is not a refractive error in the usual sense: it is not a structural characteristic of the eyes, but the effect arises from normal variations of perception in the brain. As in the aviation condition known as spatial disorientation, it is posited that some people are severely affected by the effect, some mildly, and some not at all.\n\nIn aviation and automobile safety research, the Mandelbaum effect is a useful tool in determining bias in stressful conditions. There seem to be consistent patterns in pilots' and drivers' perceptions during poor visibility. Methods of compensating for the Mandelbaum effect are still subject to research.\n\n"}
{"id": "20202587", "url": "https://en.wikipedia.org/wiki?curid=20202587", "title": "Metallurgy Analysis", "text": "Metallurgy Analysis\n\nMetallurgical analysis (冶金分析, in Chinese) is a science magazine based in Beijing, China. It is concerned in analysis of metal products. It is organized by Central Research Institute of Iron and Steel of China (中国钢铁研究总院).\n\nIn determining the suitability of submitted articles for publication, particular scrutiny will be placed on the degree of novelty and significance of the research and the extent to which it adds to existing knowledge in material analysis.\n"}
{"id": "14679356", "url": "https://en.wikipedia.org/wiki?curid=14679356", "title": "Minimal effects hypothesis", "text": "Minimal effects hypothesis\n\nIn political science, the minimal effects hypothesis states that political campaigns only marginally persuade and convert voters. The hypothesis was formulated during early research into voting behavior between the 1940s and the 1960s, and this period formed the initial \"minimum effects\" era in the United States. The hypothesis seemed solid and was associated with the general assumption that voters had clear positions on issues and knew where candidates stood on these issues. Since then the minimal effects hypothesis has been criticized and empirical research since the 1980s has suggested that voters do have uncertainties about candidates' positions and these uncertainties do influence voters' decisions. These findings have led to renewed interest in research into the effects of campaigns, with recent published research appearing both for and against the minimal effects hypothesis.\n\n"}
{"id": "640820", "url": "https://en.wikipedia.org/wiki?curid=640820", "title": "Morphological analysis (problem-solving)", "text": "Morphological analysis (problem-solving)\n\nMorphological analysis or general morphological analysis is a method developed by Fritz Zwicky (1967, 1969) for exploring all the possible solutions to a multi-dimensional, non-quantified complex problem.\n\nGeneral morphology was developed by Fritz Zwicky, the Bulgarian-born, Swiss-national astrophysicist based at the California Institute of Technology. Among others, Zwicky applied morphological analysis (MA) to astronomical studies and the development of jet and rocket propulsion systems. As a problem-structuring and problem-solving technique, MA was designed for multi-dimensional, non-quantifiable problems where causal modeling and simulation do not function well, or at all. Zwicky developed this approach to address seemingly non-reducible complexity: using the technique of cross-consistency assessment (CCA) (Ritchey, 1998), the system allows for reduction by identifying the possible solutions that actually exist, eliminating the illogical solution combinations in a grid box rather than reducing the number of variables involved. A detailed introduction to morphological modeling is given in Ritchey (2002, 2006). A summary of some 80 published articles exemplifying the various applications of general morphology, including engineering design, technological forecasting, organizational development and policy analysis, is available in Álvarez & Ritchey (2015).\n\nConsider a complex, real-world problem, like those of marketing or making policies for a nation, where there are many governing factors, and most of them cannot be expressed as numerical time series data, as one would like to have for building mathematical models. \n\nThe conventional approach here would be to break the system down into parts, isolate the vital parts (dropping the 'trivial' components) for their contributions to the output and solve the simplified system for creating desired models or scenarios. The disadvantage of this method is that real-world scenarios do not behave rationally: more often than not, a simplified model will break down when the contribution of the 'trivial' components becomes significant. Also, importantly, the behaviour of many components will be governed by the states of, and their relations with, other components – ones that may be seen to be minor before the analysis.\n\nMorphological analysis, on the other hand, does not drop any of the components from the system itself, but works backwards from the output towards the system internals. Again, the interactions and relations get to play their parts in MA and their effects are accounted for in the analysis.\n\n\n"}
{"id": "16597072", "url": "https://en.wikipedia.org/wiki?curid=16597072", "title": "Operation Sculpin", "text": "Operation Sculpin\n\nThe United States's Sculpin nuclear test series was a group of 7 nuclear tests conducted in 1990-1991. These tests followed the \"Operation Aqueduct\" series and preceded the \"Operation Julin\" series.\n"}
{"id": "3145455", "url": "https://en.wikipedia.org/wiki?curid=3145455", "title": "Otto Struve Telescope", "text": "Otto Struve Telescope\n\nThe Otto Struve Telescope was the first major telescope to be built at McDonald Observatory. Located in the Davis Mountains in West Texas, the Otto Struve Telescope was designed by Warner & Swasey Company and constructed between 1933 and 1939 by the Paterson-Leitch Company. Its mirror was the second largest in the world at the time. It was named after the Russian-American astronomer Otto Struve in 1966, three years after his death.\n\nThe Davis Mountains is an excellent location for astronomical research because of the clear dry air and moderately high elevation. The remote nature of the facility proved to be a significant challenge in transporting such a large mirror. It was a very precarious journey for the Otto Struve Telescope's mirror to this remote part of Texas and up to the top of Mount Locke. The mirror was transported from the local town of Fort Davis up the mountain by Carleton D. Wilson, owner of a local trucking company, while locals cheered as they looked on.\n\nThe Otto Struve telescope is still in use today. It is updated with modern imaging detectors allowing astronomers to conduct many types of research.\n\n\nThe Otto Struve telescope saw first light in 1939, behind the 100-inch Hooker telescope and ahead of two large British Commonwealth telescopes, both in Canada. Many competing projects were delayed due to the war in the early 1940s.\n\nFour largest telescopes in 1939:\n\n\n"}
{"id": "25384553", "url": "https://en.wikipedia.org/wiki?curid=25384553", "title": "Pea galaxy", "text": "Pea galaxy\n\nA Pea galaxy, also referred to as a Pea or Green Pea, might be a type of Luminous Blue Compact Galaxy which is undergoing very high rates of star formation. Pea galaxies are so-named because of their small size and greenish appearance in the images taken by the Sloan Digital Sky Survey (SDSS).\n\nPea Galaxies were first discovered in 2007 by the volunteer citizen scientists within the forum section of the online astronomy project Galaxy Zoo (GZ), part of the Zooniverse web portal.\n\nThe Pea galaxies, also known as Green Peas (GPs), are compact oxygen-rich emission line galaxy that were discovered at redshift between \"z\" = 0.112 and 0.360. These low-mass galaxies have an upper size limit generally no bigger than across, and typically they reside in environments less than two-thirds the density of normal galaxy environments. An average GP has a redshift of \"z\" = 0.258, a mass of ~3,200 million (~3,200 million solar masses), a star formation rate of /yr (~10 solar masses a year), an [O III] equivalent width of 69.4 nm and a low metallicity. A GP is purely star-forming, rather than having an Active galactic nucleus. They have a strong emission line at the [OIII] wavelength of 500.7 nm. [OIII], O or doubly ionized oxygen, is a Forbidden mechanism of the visible spectrum and is only possible at very low density. When the entire photometric SDSS catalogue was searched, 40,222 objects were returned, which leads to the conclusion the GPs are rare objects.\n\nGPs are the least massive and most actively star-forming galaxies in the local universe. \"These galaxies would have been normal in the early Universe, but we just don’t see such active galaxies today\", said Kevin Schawinski. \"Understanding the Green Peas may tell us something about how stars were formed in the early Universe and how galaxies evolve\".\n\nGPs exist at a time when the Universe was three-quarters of its present age and so are clues as to how Galaxy formation and evolution took place in the early Universe. With the publication of Amorin's GTC paper in February 2012, it is now thought that GPs might be old galaxies having formed most of their stellar mass several billion years ago. Old stars have been spectroscopically confirmed in one of the three galaxies in the study by the presence of Magnesium.\nIn January 2016, a study was published in the journal Nature identifying J0925+1403 as a Lyman continuum photons (LyC) 'leaker' with an escape fraction of ~8% (see section below). A follow-up study using the same Hubble Space Telescope (HST) data identifies four more LyC leakers, described as GPs. In 2014-15, two separate sources identified two other GPs to be likely LyC leaking candidates (J1219 and J0815), suggesting that these two GPs are also low-redshift analogs of high-redshift Lyman-alpha and LyC leakers. Finding local LyC leakers is crucial to theories about the early universe and reionization. More details here:Izotov et al. 2016\n\nThe image to the right shows Pea galaxy GP_J1219. This was observed in 2014 by a HST team whose Principal Investigator was Alaina Henry, using the Cosmic Origins Spectrograph and the Near Ultraviolet channel. The scale bar in the image shows 1 arc second (1\"), which corresponds to ~10,750 light years at the distance of 2.69 billion light years for GP_J1219. When using the COS Multi-Anode Micro-channel Array, in NUV imaging mode, the detector plate scale is ~40 pixels per arcsecond (0.0235 arcseconds per pixel).\n\nGalaxy Zoo (GZ) is a project online since July 2007 which seeks to classify up to one million galaxies. On July 28, 2007, two days after the start of the Galaxy Zoo Internet forum, citizen scientist 'Nightblizzard' posted two green objects thought to be galaxies. A discussion, or thread, was started on this forum by Hanny Van Arkel on the 12th of August 2007 called \"Give peas a chance\" in which various green objects were posted. This thread started humorously, as the name is a word play of the title of the John Lennon song \"Give Peace a Chance\", but by December 2007, it had become clear that some of these unusual objects were a distinct group of galaxies. These \"Pea galaxies\" appear in the SDSS as unresolved green images. This is because the Peas have a very bright, or powerful, Spectral line in their spectra for highly-ionized oxygen, which in SDSS color composites increases the luminosity, or brightness, of the \"r\" color band with respect to the two other color bands \"g\" and \"i\". The \"r\" color band shows as green in SDSS images. Enthusiasts, calling themselves the \"Peas Corps\" (another humorous play on the Peace Corps), collected over a hundred of these Peas, which were eventually placed together into a dedicated discussion thread started by Carolin Cardamone in July 2008. The collection, once refined, provided values that could be used in a systematic computer search of the GZ database of one million objects, which eventually resulted in a sample of 251 Pea galaxies, also known as Green Peas (GPs).\n\nIn November 2009, authors C. Cardamone, Kevin Schawinski, M. Sarzi, S. Bamford, N. Bennert, C. Urry, Chris Lintott, W. Keel and 9 others published a paper in the Monthly Notices of the Royal Astronomical Society titled \"Galaxy Zoo Green Peas: Discovery of A Class of Compact Extremely Star-Forming Galaxies\". Within this paper, 10 Galaxy Zoo volunteers are acknowledged as having made a particularly significant contribution. They are: Elisabeth Baeten, Gemma Coughlin, Dan Goldstein, Brian Legg, Mark McCallum, Christian Manteuffel, Richard Nowell, Richard Proctor, Alice Sheppard and Hanny Van Arkel. They are thanked for \"giving Peas a chance\". Citations for 2009MNRAS.399.1191C are available from the SAO/NASA Astrophysics Data System. More details here:Cardamone 2009 Physics\n\nIt would be wrong to assume that the 80 GPs were all new discoveries. Out of the 80 original, 46 GPs have previous citations dated before November 2009 in the NASA/IPAC Extragalactic Database. The original 80 GPs were part of a sample from SDSS data-release 7 (DR7), but did not include galaxies from other sources. Some of these other sources did include objects that might well have been classed as GPs if they were in the SDSS sample. One example of a paper that demonstrates this is: In April 2009, authors J. J. Salzer, A. L. Williams and C. Gronwall published a paper in the Astrophysical Journal Letters titled \"A Population of Metal-Poor Galaxies with ~L* Luminosities at Intermediate Redshifts\". In this paper, \"new spectroscopy and metallicity estimates for a sample of 15 star-forming galaxies with redshifts in the range 0.29 – 0.42\" were presented. These objects were selected using the KPNO International Spectroscopic Survey (KISS). Certainly 3 of these 15 when viewed as objects in SDSS are green (KISSR 1516, KISSR 2042 and KISSRx 467). Indeed, quoting from Salzer et al. 2009, section 4.1, it reads \"A New Class of Galaxy? Given the large number of studies of metal abundances in galaxies with intermediate and high redshift mentioned in the Introduction, it may seem odd that systems similar to those described here have not been recognized previously.\"\n\nIn June 2010, authors R. Amorin, E. Perez-Montero and J. Vilchez published a paper in The Astrophysical Journal Letters titled \"On the oxygen and nitrogen chemical abundances and the evolution of the \"green pea\" galaxies\". In it they explore issues concerning the metallicity of 79 GPs, disputing the original findings in Cardamone et al. They conclude, \"arguing that recent interaction-induced inflow of gas, possibly coupled with a selective metal-rich gas loss drive by supernova winds may explain our findings and the known galaxy properties\". More details here:Two papers by Amorin\n\nIn February 2011, authors Y. Izotov, N. Guseva and T. Thuan published a paper in the Astrophysical Journal titled \"Green Pea Galaxies and Cohorts: Luminous Compact Emission-line Galaxies in the Sloan Digital Sky Survey\". They find that the 80 GPs are not a rare class of galaxies on their own, but rather a subset of a class known as 'Luminous Compact Galaxies' (LCGs), of which there are 803. More details here:Luminous Compact Galaxies\n\nIn November 2011, authors Y. Izotov, N. Guseva, K. Fricke and C. Henkel published a paper in Astronomy and Astrophysics titled 'Star-forming galaxies with hot dust emission in the SDSS discovered by the Wide-field Infrared Survey Explorer (WISE)'. In this paper, they find four galaxies that have very red colours in the wavelength range 3.4 micrometres (W1) and 4.6 micrometres (W2). This implies that the dust in these galaxies is at temperatures up to 1000K. These four galaxies are GPs and more than double the number of known galaxies with these characteristics.\n\nIn January 2012, authors R. Amorin, R. Perez-Montero and J.Vilchez published a 'Conference proceeding' titled \"Unveiling the Nature of the \"Green Pea\" galaxies\". In this publication, they announce that they have conducted a set of observations using the Optical System for Imaging and low Resolution Integrated Spectroscopy (OSIRIS) at the Gran Telescopio Canarias, and that there is a forthcoming paper about their research. These observations \"will provide new insights on the evolutionary state of the Green Peas. In particular, we will be able to see whether the Green Peas show an extended, old stellar population underlying the young starbursts, like those typically dominant in terms of stellar mass in most Blue Compact Galaxies\". More details here: Two papers by Amorin\n\nIn January 2012, authors L. Pilyugin, J. Vilchez, L. Mattsson and T. Thuan published a paper in the MNRAS titled: \"Abundance determination from global emission-line SDSS spectra: exploring objects with high N/O ratios\". In it they compare the oxygen and nitrogen abundances derived from global emission-line SDSS spectra of galaxies using (1) the electron temperature method and (2) two recent strong line calibrations: the O/N and N/S calibrations. Three sets of objects were compared: composite hydrogen-rich nebula, 281 SDSS galaxies and a sample of GPs with detectable [OIII]-4363 auroral lines. Among the questions surrounding the GPs are how much nebulae influence their spectra and results. Through comparisons of the three objects using proven methodology and analysis of metallicity, they conclude that \"the high nitrogen-to-oxygen ratios derived in some Green Pea galaxies may be caused by the fact that their SDSS spectra are spectra of composite nebulae made up of several components with different physical properties (such as metallicity). However, for the hottest Green Pea galaxies, which appear to be dwarf galaxies, this explanation does not seem to be plausible.\"\n\nIn January 2012, author S. Hawley published a paper in the Publications of the Astronomical Society of the Pacific titled \"Abundances in \"Green Pea\" Star-forming Galaxies\". In this paper, former NASA astronaut Steven Hawley compares the results from previous GP papers regarding their metallicities. Hawley compares different ways of calibrating and interpreting the various results, mainly from Cardamone et al. and Amorin et al. but some from Izotov et al., and suggests why the various discrepancies between these papers' findings might be. He also considers such details as the contribution of Wolf–Rayet stars to the gas ionization, and which sets of emission lines give the most accurate results for these galaxies. He ends by writing: \"The calibrations derived from the Green Peas differ from those commonly utilized and would be useful if star-forming galaxies like the Green Peas with extremely hot ionizing sources are found to be more common.\"\n\nIn February 2012, authors S. Chakraborti, N. Yadav, C. Cardamone and A. Ray published a paper in The Astrophysical Journal Letters titled 'Radio Detection of Green Peas: Implications for Magnetic Fields in Young Galaxies'. In this paper, magnetism studies using new data from the Giant Metrewave Radio Telescope describe various observations based around the GPs. They show that the three \"very young\" starburst galaxies that were studied have magnetic fields larger than the Milky Way. This is at odds with the current understanding that galaxies build up their magnetic properties over time. More details here:Radio detection\n\nIn April 2012, authors R. Amorin, E. Perez-Montero, J. Vilchez and P. Papaderos published a paper in the Astrophysical Journal titled \"The Star Formation History and Metal Content of the 'Green Peas'. New Detailed GTC-OSIRIS spectrophotometry of Three Galaxies\". They give the results for the deep broad-band imaging and long-slit spectroscopy for 3 GPs that had been observed using the OSIRIS instrument, mounted on the 10.4m Gran Telescopio Canarias at the Roque de los Muchachos Observatory. More details here:GTC-OSIRIS\n\nIn August 2012, authors R. Amorín, J. Vílchez, G. Hägele, V. Firpo, E. Pérez-Montero and P. Papaderos published a paper in the Astrophysical Journal Letters titled \"Complex gas kinematics in compact, rapidly assembling star-forming galaxies\". Using the ISIS spectrograph on the William Herschel Telescope, they publish results of the high-quality spectra that they took of six galaxies, five of which are GPs. After studying the Hydrogen alpha emission lines (ELs) in the spectra of all six, it is shown that these ELs are made up of multiple lines, meaning that the GPs have several chunks of gas and stars moving at large velocities relative to each other. These ELs also show that the GPs are effectively a 'turbulent mess', with parts (or clumps) moving at speeds of over 500 km/s (five hundred km/s) relative to each other.\n\nIn January 2013, authors S. Parnovsky, I. Izotova and Y. Izotov published a paper in Astrophysics and Space Science titled \"H alpha and UV luminosities and star formation rates in a large sample of luminous compact galaxies\". In it, they present a statistical study of the star formation rates (SFR) derived from the GALEX observations in the Ultraviolet continuum and in the H alpha emission line for a sample of ~800 luminous compact galaxies (LCGs). Within the larger set of LCGs, including the GPs, SFR of up to /yr (~110 solar masses a year) are found, as well as estimates of the ages of the starbursts.\n\nIn April 2013, authors A. Jaskot and M. Oey published a paper in the Astrophysical Journal titled \"The Origin and Optical Depth of Ionizing Radiation in the \"Green Pea\" Galaxies\". Six \"extreme\" GPs are studied. Using these, the authors endeavour to narrow down the list of possibilities about what is producing the radiation and the substantial amounts of high-energy photon that might be escaping from the GPs. Following on from this paper, observations on the Hubble Space Telescope, totalling 24 orbits, were taken in December 2013. The Cosmic Origins Spectrograph and the Advanced Camera for Surveys were used on four of the \"extreme\" GPs. More details here:Two papers by Jaskot and Oey\n\nIn January 2014, authors Y. Izotov, N. Guseva, K. Fricke and C. Henkel published a paper in Astronomy & Astrophysics entitled\n\"Multi-wavelength study of 14000 star-forming galaxies from the Sloan Digital Sky Survey\". In it, they use a variety of sources to demonstrate: \"that the emission emerging from young star-forming regions is the dominant dust-heating source for temperatures to several hundred degrees in the sample star-forming galaxies\". The first source of data is SDSS from which 14,610 spectra with strong emission lines are selected. These 14,610 spectra were then cross-identified with sources from photometric sky surveys in other wavelength ranges. Those are: 1)GALEX for the ultraviolet; 2)the 2MASS survey for the near-infrared; 3)the Wide-field Infrared Survey Explorer All-Sky Source Catalog for infrared at differing wavelengths; 4)the IRAS survey for the far-infrared and the 5)NVSS Survey at radio-wavelengths. Only a small fraction of the SDSS objects were detected in the last two surveys. Among the results is a list of twenty galaxies with the highest magnitudes which have hot dust of several hundred degrees. Of these twenty, all could be classified as GPs and/or LCGs. Also among the results, the luminosity is obtained in the sample galaxies in a wide wavelength range. At the highest luminosities, the sample galaxies had luminosites approaching those of high-redshift Lyman-break galaxy.\n\nIn January 2014, authors A. Jaskot, M. Oey, J. Salzer, A. Van Sistine and M. Haynes gave a presentation titled \"Neutral Gas and Low-Redshift Starbursts: From Infall to Ionization\" to the American Astronomical Society at their meeting #223. The presentation included data from The Arecibo Observatory Legacy Fast ALFA Survey (ALFALFA). The authors analyzed the optical spectra of the GPs and concluded \"While the ALFALFA survey demonstrates the role of external processes in triggering starbursts, the Green Peas show that starbursts' radiation can escape to affect their external environment\", finding \"that the Peas are likely optically thin to Lyman continuum (LyC) radiation.\"\n\nIn June 2014, authors A. Jaskot and M. Oey published a conference report titled \"The Origin and Optical Depth of Ionizing Photons in the Green Pea Galaxies\". This appears in \"Massive Young Star Clusters Near and Far: From the Milky Way to Reionization\", based on the 2013 Guillermo Haro Conference. More details here:Two papers by Jaskot and Oey\n\nIn May 2015, authors A. Henry, C. Scarlata, C. L. Martin and D. Erb published a paper in the Astrophysical Journal entitled, \"Lyα Emission from Green Peas: The Role of Circumgalactic Gas Density, Covering, and Kinematics\". In this paper, ten Green Peas were studied in the ultraviolet, using high resolution spectroscopy with the Hubble Space Telescope using the Cosmic Origins Spectrograph. This study showed, for the first time, that Green Peas have strong Lyα emission much like distant, high-redshift galaxies observed in a younger universe. Henry et al. explored the physical mechanisms that determine how Lyα escapes from the Green Peas, and concluded that variations in the neutral hydrogen column density were the most important factor. More details here: Lyman Alpha Emission from Green Peas.\n\nIn May 2016, author Miranda C. P. Straub published a research paper in the open access journal Citizen Science: Theory and Practice called 'Giving Citizen Scientists a Chance: A Study of Volunteer-led Scientific Discovery'. The abstract states: \"The discovery of a class of galaxies called Green Peas provides an example of scientific work done by volunteers. This unique situation arose out of a science crowdsourcing website called Galaxy Zoo.\"\n\nIn April 2016, Yang et al. published \"Green Pea Galaxies Reveal Secrets of Lyα Escape.\" Archival Lyman-alpha spectra of 12 GPs that have been observed with the HST/COS were analysed and modelled with radiative transfer models. The dependence of Lyman-alpha (LyA) escape fractions on various properties were explored. All 12 GPs show LyA lines in emission, with a LyA equivalent width distribution similar to high-redshift emitters. Among the findings are that the LyA escape fraction depends strongly on metallicity and moderately on dust extinction. The papers results suggest that low H1 column density and low metallicity are essential for LyA escape. \"In conclusion, GPs provide an unmatched opportunity to study LyA escape in LyA Emitters.\"\n\nIn a presentation to the American Astronomical Society Meeting #229 in January 2017, Matt Brorby and Philip Kaaret describe the observations of two GPs and their x-ray emission. Using both space telescope programs Chandra GO: 16400764 and Hubble GO: 13940, they examine Luminous Compact Galaxies, both GPs, J0842+1150 and SHOC 486. They conclude: 1) These are the first x-ray observations of GPs. 2) The two GPs studied are the first test of Lx-SFR-Z planar relation and that they are consistent with this. 3) Low metallicity galaxies exhibit enhanced x-ray emission relative to normal metallicity starforming galaxies. 4) GPs are useful for predictions of X-ray output in the early universe.\n\nIn March 2017, Yang et al. published a paper in the Astrophysical Journal called: \"Lyα and UV Sizes of Green Pea Galaxies\". The authors studied the Lyman-alpha (LyA) escape in a statistical sample of 43 GPs with HST/COS LyA spectra, taken from 6 HST programs. Their conclusions include: 1) Using GPs that cover the whole ranges of dust extinction and metallicity, they find about two-thirds are strong LyA emitters. This confirms that GPs generally are \"the best analogs of high-z (redshift) Lyman-alpha Emitters (LAEs) in the nearby universe.\" The LyA escape fractions show anti-correlations with a few LyA kinematic features. 3) The authors find many correlations regarding the dependence of LyA escape on galactic properties, such as dust extinction and metallicity.) The single shell radiative transfer model can reproduce most LyA profiles of GPs.) An empirical linear relation between the LyA escape fraction, dust extinction and the LyA red peak velocity.\n\nIn August 2017, Yang et al. published a study in the Astrophysical Journal called: \"Lyα profile, dust, and prediction of Lyα escape fraction in Green Pea Galaxies\". The authors state that GPs are nearby analogues of high redshift Lyman-alpha (LyA)- emitting galaxies. Using spectral data from the HST-COS MAST archive, 24 GPs were studied for their LyA escape and the spatial profiles of LyA and UV continuum emissions. Results include: 1) Having compared LyA and UV sizes from the 2D spectra and 1D spatial profiles, it is found that most GPs show more extended LyA emission than the UV continuum. 2) 8 GPs had their spatial profiles of LyA photons at blueshifted and redshifted velocities compared. 3) The LyA escape fraction was compared with the size ration of LyA to UV. It was found that GPs that have LyA escape fractions greater than 10% \"tend to have more compact LyA morphology\".\n\nIn October 2017, Lofthouse et al. published a study in the Monthly Notices of the Royal Astronomical Society named: The authors used integral field spectroscopy, from the SWIFT and Palm 3K instruments, to perform a spatially-resolved spectroscopic analysis of four GPs, numbered 1,2,4 and 5. Among the results are that GPs 1 & 2 are rotationally-supported (they have a rotating centre), while GPs 4 & 5 are dispersion-dominated systems. GPs 1 & 2 show morphologies indicative of ongoing or mergers. However, GPs 4 & 5 show no signs of recent interactions and have similar star-forming rates. This indicates mergers are not \"a necessary requirement for driving the high star formation in these types of galaxies\".\n\nIn December 2017, authors Jaskot, Oey, Scarlata and Dowd published a paper in the Astrophysical Journal Letters titled:\"Kinematics and Optical Depth in the Green Peas: Suppressed Superwinds in Candidate LyC Emitters\". Within the paper, they say that current thinking describes how superwinds clear neutral gas away from young starburst galaxies, which in turn regulates the escape of Lyman Continuum photons from star-forming galaxies. Models predict however that in the most extreme compact starbursts, those superwinds may not launch. The authors explore the role of outflows in generating low optical depth in GPs, using observations from the Hubble Space Telescope. They compare the kinematics of ultraviolet absorption and with Lyman alpha escape fraction, Lyman alpha peak separation or low-ionisation absorption. The most extreme GPs show the slowest velocities, which \"are consistent with models for suppressed superwinds, which suggests that outflows may not be the only cause of LyC escape from galaxies.\"\n\nIn January 2016, a letter was published in the journal Nature called: \"Eight per cent leakage of Lyman continuum photons from a compact, star-forming dwarf galaxy\" by authors: Y.I. Izotov, I. Orlitová, D. Schaerer, T.X. Thuan, A. Verhamme, N.G. Guseva & G. Worseck. The abstract states: \"One of the key questions in observational cosmology is the identification of the sources responsible for ionisation of the Universe after the cosmic Dark Ages\". It also states: \"Here we present far-ultraviolet observations of a nearby low-mass star-forming galaxy, J0925+1403, selected for its compactness and high excitation... The galaxy is 'leaking' ionising radiation, with an escape fraction of 7.8%.\" These levels of radiation are thought to be similar to those of the first galaxies in the universe, which emerged in a time known as reionization. These findings have led the research team to conclude that J0925 can ionise intergalactic material up to 40 times its own stellar mass. The study was a result of observations carried out using the Cosmic Origins Spectrograph aboard the Hubble Space Telescope.\n\nGP J0925 is thought to be similar to the most distant, and thus earliest, galaxies in the universe and has been shown to 'leak' LyC. It is about 3 billion light years away (redshift z=0.301), or approximately 75% of the current age of the universe. Co-author Trinh Thuan said in a statement: \"The finding is significant because it gives us a good place to look for probing the reionization phenomenon, which took place early in the formation of the universe that became the universe we have today\". He also stated: \"As we make additional observations using Hubble, we expect to gain a much better understanding of the way photons are ejected from this type of galaxy, and the specific galaxy types driving cosmic reionization.\" He concludes: \"These are crucial observations in the process of stepping back in time to the early universe.\"\n\nIn October 2016, a study was published in the MNRAS entitled: \"Detection of high Lyman continuum leakage from four low-redshift compact star-forming galaxies\". Its authors are Y. I. Izotov, D. Schaerer, T. X. Thuan, G. Worseck, N. G. Guseva, I. Orlitova, A. Verhamme. The abstract states: \"Following our first detection reported in Izotov et al. (2016) [as above], we present the detection of Lyman continuum (LyC) radiation of four other compact star-forming galaxies observed with the Cosmic Origins Spectrograph (COS) onboard the Hubble Space Telescope (HST)\".\n\nThis study contains the methods and findings from Izotov et al. 2016 (a) which concentrated on one galaxy, whereas the above paper, Izotov et al. 2016 (b) has findings for four galaxies, all of which have LyC leakage. When compared with other known local galaxies that leak LyC, as listed in this article, Izotov et al. 2016 (a & b) doubles the numbers of known leakers.\n\n In May 2015, authors Alaina Henry, Claudia Scarlata, Crystal Martin, and Dawn Erb published a paper titled: \"Lyα Emission from Green Peas: The Role of Circumgalactic Gas Density, Covering, and Kinematics\". The motivation of this work was to understand why some galaxies have Lyα emission, while others don't. A host of physical conditions in galaxies regulate the output of this spectral feature; hence, understanding its emission is fundamentally important for understanding how galaxies form and how they impact their intergalactic surroundings.\n\nHenry et al. hypothesized that, since the GPs seem more like galaxies at redshift=z>2, and Lyα is common at these redshifts, that Lyα would be common in the GPs as well. Observations with the HST using the COS, as in 'Description', proved this to be true for a sample of 10 GPs. The spectra, shown here to the right, indicate resonant scattering of Lyα photons that are emitted near zero velocity. The wealth of data existing on the GPs, combined with the COS spectra, allowed Henry et al. to explore the physical mechanisms that regulate the Lyα output. These authors concluded that variations in the amount of neutral hydrogen gas, which scatters Lyα photons, are the cause of a factor of 10 difference in Lyα output in their sample.\n\nThe spectrum of GP_J1219 (an image of which is in 'Description') shows its very strong flux measurements when compared to other 9 GPs. Indeed, only GP_J1214 has a value approaching that of J1219. Note also the double peaks in some GPs and the velocity values of the emissions, indicating the inflow and outflow of matter in the GPs.\n\nIn April 2013, authors A. Jaskot and M. Oey published a paper in The Astrophysical Journal titled \"The Origin and Optical Depth of Ionizing Radiation in the \"Green Pea\" Galaxies\". Six \"extreme\" GPs are studied. Using these, they endeavour to narrow down the list of possibilities about what is producing the UV-radiation and the substantial amounts of high-energy photon that might be escaping from the GPs. Through trying to observe these photons in nearby galaxies such as the GPs, our understanding of how galaxies behaved in the early Universe might well be revolutionised. It is reported that the GPs are exciting candidates to help astronomers understand a major milestone in the development of the cosmos 13 billion years ago, during the epoch of reionization.\n\nIn February 2014, authors A. Jaskot and M. Oey published a conference report titled \"The Origin and Optical Depth of Ionizing Photons in the Green Pea Galaxies\". This will appear in \"Massive Young Star Clusters Near and Far: From the Milky Way to Reionization\", based on the 2013 Guillermo Haro Conference. In the publication, Jaskot and Oey write: \"We are currently analyzing observations from IMACS and MagE on the Magellan Telescopes and COS and ACS on Hubble Space Telescope (HST) to distinguish between WR (Wolf-Rayet star) and the shock ionization scenarios and confirm the GPs’ optical depths. The absence of WR features in the deeper IMACS spectra tentatively supports the shock scenario, although the detection limits do not yet definitively rule out the WR photoionization hypothesis.\"\n\nAt the time this paper was published, only five Green Peas (GPs) had been imaged by the Hubble Space Telescope (HST). Three of these images reveal GPs to be made up of bright clumps of star formation and low surface density features indicative of recent or ongoing galaxy mergers. These three HST images were imaged as part of a study of local ultraviolet (UV-luminous) galaxies in 2005. Major mergers are frequently sites of active star-formation and to the right a graph is shown that plots specific star formation rate (SFR / Galaxy Mass) against galaxy mass. In this graph, the GPs are compared to the 3003 mergers from the Galaxy Zoo Merger Sample (GZMS). It shows that the GPs have low masses typical of dwarf galaxy and much higher star-forming rates (SFR) when compared to the GZMS. The black, dashed line shows a constant SFR of /yr (~10 solar masses). Most GPs have a SFR between 3 and /yr (between ~3 and ~30 solar masses).\n\nGPs are rare. Of the one million objects that make up GZ's image bank, only 251 GPs were found. After having to discard 148 of these 251 because of atmospheric contamination of their Stellar spectra, the 103 that were left, with the highest signal-to-noise ratio, were analyzed further using the classic emission line diagnostic by Baldwin, Phillips and Terlevich which separates starbursts and active galactic nuclei. 80 were found to be starburst galaxies. The graph to the left classifies 103 narrow-line GPs (all with SNR ≥ 3 in the emission lines) as 10 active galactic nuclei (blue diamonds), 13 transition objects (green crosses) and 80 starbursts (red stars). The solid line is: Kewley et al. (2001) maximal starburst contribution (labelled Ke01). The dashed line is: Kauffmann et al. (2003) separating purely star-forming objects from AGN (labelled Ka03).\n\nGPs have a strong [OIII] emission line when compared to the rest of their spectral continuum. In a SDSS spectrum, this shows up as a large peak with [OIII] at the top. The wavelength of [OIII] (500.7 nm) was chosen to determine the luminosities of the GPs using Equivalent width (Eq.Wth.). The histogram on the right shows on the horizontal scale the Eq.Wth. of a comparison of 10,000 normal galaxies (marked red), UV-luminous Galaxies (marked blue) and GPs (marked green). As can be seen from the histogram, the Eq.Wth. of the GPs is much larger than normal for even prolific starburst galaxies such as UV-luminous Galaxies.\n\nWithin the Cardamone et al. paper, comparisons are made with other compact galaxies, namely Blue Compact Dwarfs Galaxies and UV-luminous Galaxies, at local and much higher distances. The findings show that GPs form a different class of galaxies than Ultra Blue Compact Dwarfs, but may be similar to the most luminous members of the Blue Compact Dwarf Galaxy category. The GPs are also similar to UV-luminous high redshift galaxies such as Lyman-break Galaxies and Lyman-alpha emitter. It is concluded that if the underlying processes occurring in the GPs are similar to that found in the UV-luminous high redshift galaxies, the GPs may be the last remnants of a mode of star formation common in the early Universe.\n\nGPs have low interstellar reddening values, as shown in the histogram on the right, with nearly all GPs having \"E\"(\"B\"-\"V\") ≤ 0.25. The distribution shown indicates that the line-emitting regions of star-forming GPs are not highly reddened, particularly when compared to more typical star-forming or starburst galaxies. This low reddening combined with very high UV luminosity is rare in galaxies in the local Universe and is more typically found in galaxies at higher redshifts.\n\nCardamone et al. describe GPs as having a low metallicity, but that the oxygen present is highly ionized. The average GP has a metallicity of log[O/H]+12~8.69, which is solar \"or\" sub-solar, depending on which set of standard values is used. Although the GPs are in general consistent with the mass-metallicity relation, they depart from it at the highest mass end and thus do not follow the trend. GPs have a range of masses, but a more uniform metallicity than the sample compared against. These metallicities are common in low mass galaxies such as Peas.\n\nAs well as the optical images from the SDSS, measurements from the GALEX survey were used to determine the ultraviolet values. This survey is well matched in depth and area, and 139 of the sampled 251 GPs are found in GALEX Release 4 (G.R.4). For the 56 of the 80 star-forming GPs with GALEX detections, the median luminosity is ~30,000 million formula_1 (~30,000 million solar luminosities).\n\nWhen compiling the Cardamone paper, spectral classification was made using Gas And Absorption Line Fitting (GANDALF). This sophisticated computer software was programmed by Marc Sarzi, who helped analyze the SDSS spectra.\n\nThese values are from Table 4, pages 16–17 of Cardamone 2009 et al., which shows the 80 GPs that have been analysed here. The long 18-digit numbers are the SDSS DR7 reference numbers.\n\nColor selection was by using the difference in the levels of three Optical filters, in order to capture these color limits: u-r ≤ 2.5 (1), r-i ≤ -0.2 (2), r-z ≤ 0.5 (3), g-r ≥ r-i + 0.5 (4), u-r ≥ 2.5 (r-z) (5). If the diagram on the right (one of two in the paper) is looked at, the effectiveness of this color selection can be seen. The Color-color diagram shows ~100 GPs (green crosses), 10,000 comparison galaxies (red points) and 9,500 comparison quasar (purple stars) at similar redshifts to the GPs. The black lines show how these figures are on the diagram.\n\nComparing a GP to the Milky Way can be useful when trying to visualize these star-forming rates. An average GP has a mass of ~3,200 million (~3,200 million solar masses). The Milky Way (MW) is a spiral galaxy and has a mass of ~1,125,000, million (~1,125,000 million solar masses). So the MW has the mass of ~390 GPs.\n\nResearch has shown that the MW converts /yr (~2 solar masses per year) worth of interstellar medium into stars. An average GP converts /yr (~10 solar masses) of interstellar gas into stars, which is ~5 times the rate of the MW.\n\nOne of the original ways of recognizing GPs, before SQL programming was involved, was because of a discrepancy about how the SDSS labels them within Skyserver. Out of the 251 of the original GP sample that were identified by the SDSS spectroscopic pipeline as having galaxy spectra, only 7 were targeted by the SDSS spectral fibre allocation as galaxies i.e. 244 were not.\n\nIn June 2010, authors R. Amorín, E. Pérez-Montero and J.M. Vílchez published a paper in The Astrophysical Journal letters titled \"On the Oxygen and Nitrogen Chemical Abundances and the Evolution of the \"Green Pea\" Galaxies\", which disputes the metallicities calculated in the original Cardamone et al. GPs paper Amorin et al. use a different methodology from Cardamone et al. to produce metallicity values more than one fifth (20%) of the previous values (about 20% solar or one fifth solar) for the 80 'starburst' GPs. These mean values are log[O/H]+12~8.05, which shows a clear offset of 0.65dex between the two papers' values. For these 80 GPs, Amorin et al., using a direct method, rather than strong-line methods as used in Cardamone et al., calculate physical properties, as well as oxygen and nitrogen ionic abundances. These metals pollute hydrogen and helium, which make up the majority of the substances present in galaxies. As these metals are produced in Supernova, the older a galaxy is, the more metals it would have. As GPs are in the nearby, or older, Universe, they should have more metals than galaxies at an earlier time.\nAmorin et al. find that the amount of metals, including the abundance of nitrogen, are different from normal values and that GPs are not consistent with the mass-metallicity relation, as concluded by Cardamone et al. This analysis indicates that GPs can be considered as genuine metal-poor galaxies. They then argue that this oxygen under-abundance is due to a recent interaction-induced inflow of gas, possibly coupled with a selective metal-rich gas loss driven by Supernovae winds and that this can explain their findings. This further suggests that GPs are likely very short-lived as the intense star formation in them would quickly enrich the gas.\nIn May 2011, R.Amorin, J.M.Vilchez and E.Perez-Montero published a conference proceeding paper titled \"Unveiling the Nature of the \"Green Pea\" galaxies\". In it they review recent scientific results and announcing a forthcoming paper on their recent observations at the Gran Telescopio Canarias. This paper is also a modified report of a presentation at the Joint European and National Astronomy Meeting (JENAM) 2010. They conclude that GPs are a genuine population of metal-poor, luminous and very compact starburst galaxies. Amongst the data, five graphs illustrate the findings they have made. Amorin et al. use masses calculated by Izotov, rather than by Cardamone. The metallicities that Amorin et al. use agree with Izotov's findings, or vice versa, rather than Cardamone's.\n\nThe first graph (on the left; fig.1 in paper) plots the nitrogen/oxygen vs. oxygen/hydrogen abundance ratio. The 2D histogram of SDSS star forming galaxies is shown in logarithmic scale while the GPs are indicated by circles. This shows that GPs are metal-poor.\nThe second graph (on the right; fig.2 in paper) plots O/H vs. stellar mass. The 2D histogram of SDSS SFGs is shown in logarithmic scale and their best likelihood fit is shown by a black solid line. The subset of 62 GPs are indicated by circles and their best linear fit is shown by a dashed line. For comparison we also show the quadratic fit presented in Amorin \"et al.\" 2010 for the full sample of 80 GPs. SFGs at z ≥ 2 by Erb et al. are also shown by asterisks for comparison.\nThe third graph (on the left; fig.3 in paper) plots N/O vs. stellar mass. Symbols as in fig.1.\nThe fourth graph (on the right; fig.4 in paper) plots O/H vs. B-band (rest-frame) absolute magnitude. The meaning of symbols is indicated. Distances used in computing (extinction corrected) absolute magnitudes were, in all cases, calculated using spectroscopic redshifts and the same cosmological parameters. The dashed line indicates the fit to the HII galaxies in the Luminosity-Metallicity Relationship (MZR) given by Lee et al. 2004.\n\nThe fifth graph (on the left; fig.5 in paper) plots gas mass fraction vs. metallicity. Different lines correspond to closed-box models at different yields, as indicated in the legend. Open and filled circles are GPs which are above and below the fit to their MZR. Diamonds are values for the same Wolf-Rayet galaxies as in Fig. 4.\n\nIn February 2012, authors R. Amorin, E. Perez-Montero, J. Vilchez and P. Papaderos published a paper titled \"The star formation history and metal content of the \"Green Peas\". New detailed GTC-OSIRIS spectrophotometry of three galaxies\" in which they presented the findings of observations carried out using the Gran Telescopio Canarias at the Roque de los Muchachos Observatory. They gather deep broad-band imaging and long-slit spectroscopy of 3 GPs using high precision equipment.\n\nTheir findings show that the three GPs display relatively low Extinction (astronomy), low oxygen abundances and high nitrogen-to-oxygen ratios. Also reported are the clear signatures of Wolf–Rayet stars, of which a population are found (between ~800 and ~1200). A combination of population and evolutionary synthesis models strongly suggest a formation history dominated by starbursts. These models show that these three GPs currently undergo a major starburst producing between ~4% and ~20% of their stellar mass. However, as these models imply, they are old galaxies having formed most of their stellar mass several billion years ago. The presence of old stars has been spectroscopically verified in one of the three galaxies by the detection of Magnesium. Surface photometry, using data from the Hubble Space Telescope archive, indicates that the three GPs possess an exponential low surface brightness envelope (see Low-surface-brightness galaxy). This suggests that GPs are identifiable with major episodes in the assembly history of local Blue Compact Dwarf galaxies.\n\nThe three galaxies are (using SDSS references):\n\nIn February 2011, Yuri Izotov, Natalia Guseva and Trinh Thuan published a paper titled \"Green Pea Galaxies and Cohorts: Luminous Compact Emission-line Galaxies in the Sloan Digital Sky Survey\", examining the GPs and comparing these to a larger set of 803 Luminous Compact Galaxies (LCGs). They use a different set of selection criteria from Cardamone et al. These are: a) a high extinction-corrected luminosity > 3x10^40 Ergs s^-1 of the hydrogen beta emission line; (see Hydrogen spectral series) b) a high equivalent width greater than 5 nm; c) a strong [OIII] wavelength at the 436.3 nm emission line allowing accurate abundance determination; d) a compact structure on SDSS images; and e) an absence of obvious active galactic nuclei spectroscopic features.\n\nIts conclusions (shortened) are:\n\n\nIn February 2012, authors Sayan Chakraborti, Naveen Yadav, Alak Ray and Carolin Cardamone published a paper titled \"Radio Detection of Green Peas: Implications for Magnetic Fields in Young Galaxies\" which deals with the magnetic properties of the GPs. In it, they describe observations which have produced some unexpected results raising puzzling questions about the origin and evolution of magnetism in young galaxies. The ages are estimated from looking at the star formation that the GPs currently have ongoing and then estimating the age of the most recent starburst. GPs are very young galaxies, with models of the observed stellar populations indicating that they are around 10^8 (one hundred million) years old (1/100th the age of the Milky Way). There is some question as to whether the GPs all started from the same starburst or if multiple starbursts went on (much older stellar populations are hidden as we can't see the light from these).\n\nUsing data from the Giant Metrewave Radio Telescope (GMRT) and archive observations from the Karl G. Jansky Very Large Array (VLA), Chakraborti et al. produced a set of results which are based around the VLA FIRST detection of stacked flux from 32 GPs and three 3-hour low frequency observations from the GMRT which targeted the three most promising candidates which had expected fluxes at the milli-Jansky (mJy) level.\n\nChakraborti et al. find that the three GPs observed by the GMRT have a magnetic field of B~39 μG, and more generally a figure of greater than B~30μG for all the GPs. This is compared to a figure of B~5μG for the Milky Way. The present understanding is of magnetic field growth based on the amplification of seed fields by dynamo theory and its action over a galaxy's lifetime. The observations of GPs challenge that thinking.\n\nGiven the high star-forming rates of GPs generally, they are expected to host a large number of Supernovae. Supernovae accelerate electrons to high energies, near to the speed of light, which may then emit synchrotron radiation in radio spectrum frequencies.\n\n\n"}
{"id": "10047126", "url": "https://en.wikipedia.org/wiki?curid=10047126", "title": "Phytosulfokine", "text": "Phytosulfokine\n\nPhytosulfokines are plant hormones that belong to the growing class of plant peptide hormones. Phytosulfokines are sulfated growth factors strongly promoting proliferation of plant cells in cultures.\n"}
{"id": "37997242", "url": "https://en.wikipedia.org/wiki?curid=37997242", "title": "Road Weather Information System", "text": "Road Weather Information System\n\nA Road Weather Information System (RWIS) comprises automatic weather stations (technically referred to as Environmental Sensor Stations (ESS)) in the field, a communication system for data transfer, and central systems to collect field data from numerous ESS. These stations measure real-time atmospheric parameters, pavement conditions, water level conditions, and visibility. Central RWIS hardware and software are used to process observations from ESS to develop nowcasts or forecasts, and display or disseminate road weather information in a format that can be easily interpreted by a manager. RWIS data are used by road operators and maintainers to support decision making. Real-time RWIS data is also used by Automated Warning Systems (AWS). The spatial and temporal resolution can be that of a mesonet. The data is often considered proprietary although it is often ingested into numerical weather prediction models.\n\n"}
{"id": "19408628", "url": "https://en.wikipedia.org/wiki?curid=19408628", "title": "Scanning confocal electron microscopy", "text": "Scanning confocal electron microscopy\n\nScanning confocal electron microscopy (SCEM) is an electron microscopy technique analogous to scanning confocal optical microscopy (SCOM). In this technique, the studied sample is illuminated by a focussed electron beam, as in other scanning microscopy techniques, such as scanning transmission electron microscopy or scanning electron microscopy. However, in SCEM, the collection optics is arranged symmetrically to the illumination optics to gather only the electrons that pass the beam focus. This results in superior depth resolution of the imaging. The technique is relatively new and is being actively developed.\n\nThe idea of SCEM logically follows from SCOM and thus is rather old. However, practical design and construction of scanning confocal electron microscope is a complex problem first solved by Nestor J. Zaluzec. His first scanning confocal electron microscope demonstrated the 3D properties of the SCEM, but have not realized the sub-nanometer lateral spatial resolution achievable with high-energy electrons (lateral resolution of only ~80 nm has been demonstrated). Several groups are currently working on construction of atomic resolution SCEM. In particular, atomically resolved SCEM images have already been obtained \n\nThe sample is illuminated by a focused electron beam, and the beam is re-focused on the detector, thus collecting only electrons passing through the focus. In order to produce an image, the beam should be laterally scanned. In the original design, this was achieved by placing synchronized scanning and descanning deflectors. Such design is complex and only a few custom-built setups exist. Another approach is to use stationary illumination and collection, but perform scan by moving the sample with a high-precision piezo-controlled holder. Such holders are readily available and can fit into most commercial electron microscopes thereby realizing the SCEM mode. As a practical demonstration, atomically resolved SCEM images have been recorded.\n\nHigh energies of incident particles (200 keV electrons vs. 2 eV photons) result in much higher spatial resolution of SCEM as compared to SCOM (lateral resolution <1 nm vs. >400 nm).\n\nAs compared to conventional electron microscopy (TEM, STEM, SEM), SCEM offers 3-dimensional imaging. 3D imaging in SCEM was expected from the confocal geometry of SCEM, and it has recently been confirmed by theoretical modeling. In particular, it is predicted that a heavy layer (gold) can be identified in light matrix (aluminum) with ~10 nm precision in depth; this depth resolution is limited by the convergence angle of the electron beam and could be improved to a few nanometers in next-generation electron microscopes equipped with two fifth-order spherical aberration correctors.\n\n"}
{"id": "274810", "url": "https://en.wikipedia.org/wiki?curid=274810", "title": "Scientific instrument", "text": "Scientific instrument\n\nA scientific instrument is, broadly speaking, a device or tool used for scientific purposes, including the study of both natural phenomena and theoretical research.\n\nHistorically, the definition of a scientific instrument has varied, based on usage, laws, and historical time period. Before the mid-nineteenth century such tools were referred to as \"natural philosophical\" or \"philosophical\" apparatus and instruments, and older tools from antiquity to the Middle Ages (such as the astrolabe and pendulum clock) defy a more modern definition of \"a tool developed to investigate nature qualitatively or quantitatively.\" Scientific instruments were made by instrument makers living near a center of learning or research, such as a university or research laboratory. Instrument makers designed, constructed, and refined instruments for specific purposes, but if demand was sufficient, an instrument would go into production as a commercial product. By World War II, the demand for improved analyses of wartime products such as medicines, fuels, and weaponized agents pushed instrumentation to new heights. Today, changes to instruments used in scientific endeavors — particularly analytical instruments — are occurring rapidly, with interconnections to computers and data management systems becoming increasingly necessary.\n\nScientific instruments vary greatly in size, shape, purpose, complication and complexity. This includes relatively simple laboratory equipment like scales, rulers, chronometers, thermometers, etc. Other simple tools developed in late 20th century or early 21st century are Foldscope (an optical microscope), KAS Periodic Table (SCALE), MasSpec Pen (pen that detects cancer), glucose meter, etc. However, some scientific instruments can be quite large in size and significant in complexity, like particle colliders or radio-telescope antennas. Conversely microscale and nanoscale technologies are advancing to the point where instrument sizes are shifting towards the tiny, including nanoscale surgical instruments, biological nanobots, and bioelectronics.\n\nInstruments are increasingly based upon integration with computers to improve and simplify control; enhance and extend instrumental functions, conditions, and parameter adjustments; and streamline data sampling, collection, resolution, analysis (both during and post-process), and storage and retrieval. Advanced instruments can be connected as a local area network (LAN) directly of via middleware and can be further integrated as part of an information management application such as a laboratory information management system (LIMS). Instrument connectivity can be furthered even more using internet of things (IoT) technologies, allowing for example laboratories separated by great distances to connect their instruments to a network that can be monitored from a workstation or mobile device elsewhere.\n\n\n\n\n\n"}
{"id": "936181", "url": "https://en.wikipedia.org/wiki?curid=936181", "title": "Summer Science Program", "text": "Summer Science Program\n\nThe Summer Science Program (SSP) is an academic summer program where high school students experience college-level education and do research in celestial mechanics by studying the orbits of asteroids or biochemistry by studying the kinetic properties of enzymes. The program was established in 1959 at The Thacher School in Ojai, California. It now takes place on two astrophysics campuses, New Mexico Tech in Socorro, New Mexico and University of Colorado, Boulder in Boulder, Colorado, and one biochemistry campus, Purdue University in West Lafayette, Indiana.\n\nIn the Astrophysics program, each team takes a series of images of a near-earth asteroid, then writes software to calculate its orbit and predict its future path. In the Biochemistry program, each team isolates and models an enzyme from a fungal crop pathogen, then designs a molecule to inhibit that enzyme. \n\nEach Summer Science Program campus hosts 36 participants and 7 faculty for 39 days. Faculty have integrated academic and residential roles. The participant experience is designed to be similar at all campuses. For the research, participants are organized into teams of three. The schedule includes classroom time, lab time, field trips (scientific and recreational) and guest lectures from working scientists and other professionals. Past guest speakers have included Maarten Schmidt, who has done pioneering work in quasars; Richard Feynman, a Nobel laureate in physics; James Randi, magician and debunker of pseudoscience; Mitch Kapor, founder of Lotus Development; Paul MacCready, creator of the Gossamer Condor and Gossamer Albatross; and Eric Allin Cornell, a Nobel laureate in physics.\n\nThe application process and admission criteria are similar to that of selective colleges. Primarily juniors (rising seniors) are admitted, with a few sophomores each year. There is a program fee, inclusive of tuition, room & board, and supplies. Need-based financial aid grants are available to cover part or all of the fee, with SSP guaranteeing to meet all demonstrated need. Donations to the nonprofit fund financial aid.\n\nIn 1991, the National Academies' Commission on Physical Sciences, Mathematics, and Applications observed that \"All participants go on to college. About 37 percent of the pre-1985 graduates are now working in science and medicine, and 34 percent in engineering, mathematics, and computer science (including the founder of Lotus Development Corporation).\"\n\nThe program is co-sponsored by the Massachusetts Institute of Technology and the California Institute of Technology which are affiliate universities, and by Harvey Mudd College, which is an academic partner to the program. \n\nThe program was established in 1959 at The Thacher School in Ojai, California, as a response to the launch of Sputnik 1 and the start of the Space Race. The Headmaster at Thacher was concerned that the country's top high school students were not being adequately informed and inspired about careers in the physical sciences. He decided to create an intense summer program to challenge such students and give them a taste of \"doing real science,\" with assistance from Caltech, UCLA, Claremont Colleges, and Stanford. Financial support came from Hughes Aircraft.\n\nThe first SSP was led by Dr. Paul Routly and Dr. Foster Strong. In 1960, Dr. George Abell joined the program for his first of more than 20 summers at SSP.\n\nThe first year, SSP had 26 students. The students used data from the \"Russian ephemeris\" (\"Ephemyeredi Mahlikh Planyet\") to find asteroids to photograph, measured the positions, and submitted the data to the Minor Planet Center at the Harvard-Smithsonian Center for Astrophysics. The students were excited to find that when they calculated the orbit of 9 Metis, their data resulted in a significant correction to the Russian ephemeris.\n\nWomen were admitted starting in 1969, and reached 50% of enrollment in 2010.\n\nAfter 41 summers at Thacher, a significant threat to the continuation of SSP came in 2000, when Thacher School decided to use its entire campus for a different purpose. SSP alumni incorporated Summer Science Program, Inc., solicited funding largely from the alumni community, and found a new host campus. Beginning in 2000, SSP was held at the Happy Valley School, located just across the Ojai Valley from The Thacher School. In 2007, Happy Valley School was renamed Besant Hill School.\n\nWith the alumni rescue complete, they soon began looking to expand the program. In 2003 a second campus opened at New Mexico Tech in Socorro with the support of New Mexico Tech, Los Alamos and Sandia national laboratories, and others. In 2010, the California campus moved to Westmont College in Santa Barbara, then in 2015 to University of Colorado Boulder.\nIn 2016, following three years of planning and preparation, a pilot of the first SSP in Biochemistry was held at Purdue University in West Lafayette, Indiana. Six alumni from the previous summer successfully ran through the new experiment. In 2017, the first SSP in Biochemistry was successfully held with 24 participants. \n\nThrough 2017, over 2,500 participants have attended 75 programs. All alumni and former faculty are \"members\" of the nonprofit for life, along with other members named by the Board. The Members elect Trustees annually, have access to an online database for networking, and are invited to an Annual Dinner held each fall. \n\nFor the first 50 years of the program, students took photographic images of main-belt asteroids (between the orbits of Mars and Jupiter). Starting in 2009 students took digital images of (much fainter) near-Earth asteroids (inside the orbit of Mars). The process of orbit determination is conceptually the same in both cases. First students take a series of images of asteroids. After identifying the asteroid, its position on the image relative to known stars is carefully calculated. That relative position is then used to determine the position of the asteroid in celestial coordinates (right ascension and declination) at the exact time the image was taken. The series of positions as the asteroid moves across the sky allows the student to fit an approximate orbit to the asteroid. The measured asteroid coordinates (not the calculated orbital elements) are submitted to the Harvard-Smithsonian Center for Astrophysics.\n\nOver the decades SSP students have done their orbit determination calculations on mechanical calculators (1960s), then electronic calculators (1970s), then \"mini-computers\" (1980s), then personal computers (1990s and 2000s). In recent years they write their orbit determination programs in the Python programming language, employing the Gaussian method.\n\nThe following is a list of notable alumni.\n\n"}
{"id": "36618507", "url": "https://en.wikipedia.org/wiki?curid=36618507", "title": "Tech Nation", "text": "Tech Nation\n\nTech Nation and its regular segment BioTech Nation is a weekly interview program talk show that airs four times each weekend on the \"Tech Nation\" is hosted by award-winning science journalist Moira Gunn and is produced by Tech Nation Media at the studios of KQED in San Francisco. The program opens with a 5-minute commentary on life in modern times, and typically a major one-on-one interview with major figures in science and technology. The final segment of each show is BioTech Nation which presents interviews with leading biotech scientists, CEOs, venture capitalists, government officials, bio-ethicists and others who make up the biotech community. \n\n\"Tech Nation\" and \"BioTech Nation\" are also available in a separate podcasts at IT Conversations (IT Conversations has ceased to operate, only the back-archive is available). The podcasts are downloaded over 23 million times per year and over 1.3 million listeners tune in each week to hear the program. It is also carried on the NPR Now channel via Sirius/XM satellite radio. \n\nTech Nation is funded by donations and a professional volunteer base. \n\nTech Nation and BioTech Nation was conceived and created by Dr. Moira Gunn, with the steady belief that the impact of technology and science on society is ever growing and that everyone needs to understand its impact. The programs are provided at no cost to public radio stations and other non-profit organizations, including schools and universities.\n\n"}
{"id": "3255573", "url": "https://en.wikipedia.org/wiki?curid=3255573", "title": "The Lattice Project", "text": "The Lattice Project\n\nThe Lattice Project was once a distributed computing project that combined computing resources, Grid middleware, specialized scientific application software and web services into a comprehensive Grid computing system for scientific analysis. It ran the Genetic Algorithm for Rapid Likelihood Inference (GARLI) software to determine the relationships between different genetic samples.\n\nA major aspect of the project makes use of the Berkeley Open Infrastructure for Network Computing (BOINC) platform. The Lattice Project maintained a separate BOINC web site, but the site is dead as of this writing, causing this project to shut down because BOINC depends on this website to get setup information to set this project up on its clients.\n\n"}
{"id": "58778", "url": "https://en.wikipedia.org/wiki?curid=58778", "title": "Timeline of states of matter and phase transitions", "text": "Timeline of states of matter and phase transitions\n\nTimeline of states of matter and phase transitions\n\n"}
