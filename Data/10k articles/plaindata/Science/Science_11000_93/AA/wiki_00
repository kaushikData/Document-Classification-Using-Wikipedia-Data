{"id": "36426504", "url": "https://en.wikipedia.org/wiki?curid=36426504", "title": "Abell 223", "text": "Abell 223\n\nAbell 223 is a galaxy cluster. It is located at a distance of 2.4 billion light-years from Earth.\nThe cluster is connected to nearby cluster Abell 222 by a filament of matter. Research has shown that only 20% of that matter is normal. The rest is thought to be dark matter.\n\n"}
{"id": "779866", "url": "https://en.wikipedia.org/wiki?curid=779866", "title": "Amguid crater", "text": "Amguid crater\n\nAmguid is a meteorite crater in Algeria.\n\nIt is approximately in diameter, approximately 65 m deep and the age is estimated to be less than 100,000 years and is probably Pleistocene. The crater is exposed at the surface.\n\nCrater was discovered by Europeans in 1948, first scientific description was made by Jean-Phillippe Lefranc in 1969.\n\n\n\n"}
{"id": "27545563", "url": "https://en.wikipedia.org/wiki?curid=27545563", "title": "Bary Glacier", "text": "Bary Glacier\n\nBary Glacier () is a glacier flowing west into Jacobsen Bight, South Georgia, south of Christophersen Glacier. The glacier cuts through the longest sedimentary sequence on the island, from Christophersen Glacier to Cape Darnley. It was named by the UK Antarctic Place-Names Committee in 1982 after Thomas de Bary, one of the first directors of the Compañía Argentina de Pesca from 1904.\n\n"}
{"id": "6936288", "url": "https://en.wikipedia.org/wiki?curid=6936288", "title": "Baseball telecasts technology", "text": "Baseball telecasts technology\n\nThe following is a chronological list of the technological advancements of Major League Baseball television broadcasts:\n\nOn August 26, the first ever Major League Baseball telecast (the Brooklyn Dodgers vs. Cincinnati Reds from Ebbets Field) aired by W2XBS, an experimental station in New York City which would ultimately become WNBC-TV.\n\nRed Barber called the game without the benefit of a monitor and with only two cameras capturing the game. One camera was on Barber and the other was behind the plate. Barber had to guess from which light was on and where it pointed.\n\nIn 1939, baseball games were usually covered by one camera providing a point-of-view along the third base line.\n\n\nOn August 11, 1951, WCBS-TV in New York City televised the first baseball game (in which the Boston Braves beat the Brooklyn Dodgers by the score of 8-1) in color.\n\nOn October 3 of that year, NBC aired the first coast-to-coast baseball telecast as the Brooklyn Dodgers were beaten by the New York Giants in the final game of a playoff series by the score of 5-4 (off Bobby Thomson's now-legendary home run).\n\n\n1955 marked the first time that the World Series was televised in color.\n\n\n\nOn July 23, 1962, Major League Baseball had its first satellite telecast (via Telstar Communications). The telecast included portion of a contest between the Chicago Cubs vs. the Philadelphia Phillies from Wrigley Field with Jack Brickhouse commentating on WGN-TV.\n\nBy 1969, the usage of chroma key (in which the commentators would open a telecast by standing in front of a greenscreen composite of the stadiums' crowds) became a common practice for baseball telecasts.\n\n\n\nIn the bottom of the 12th inning of Game 6 of the 1975 World Series at Boston's Fenway Park, Red Sox catcher Carlton Fisk was facing Cincinnati Reds pitcher Pat Darcy. Fisk then hit a pitch down the left field line that appeared to be heading to foul territory. The enduring image of Fisk jumping and waving the ball fair as he made his way to first base is arguably one of baseball's greatest moments. The ball struck the foul pole, giving the Red Sox a 7-6 win and forcing a seventh and deciding game of the Fall Classic. During this time, cameramen covering baseball games were instructed to follow the flight of the ball; reportedly, Fisk's reaction was only being recorded because NBC cameraman Lou Gerard, positioned inside Fenway's scoreboard at the base of the left-field Green Monster wall, had become distracted by a large rat. This play was perhaps the most important catalyst in getting camera operators to focus most of their attention on the players themselves.\n\nOn July 6, 1983, NBC televised the All-Star Game out of Chicago's Comiskey Park. During the telecast, special guest analyst, Don Sutton helped introduce NBC's new pitching tracking device dubbed The NBC Tracer. \"The NBC Tracer\" was a stroboscopic comet tail showing the path of a pitch to the catcher's glove. For instance, \"The NBC Tracer\" helped track a Dave Stieb curveball among others.\n\nIn 1985, NBC's telecast of the All-Star Game out of the Metrodome in Minnesota was the first program to be broadcast in stereo by a television network.\n\n\nFor the 1987 World Series between the Minnesota Twins and St. Louis Cardinals, ABC utilized 12 cameras and nine tape machines. This includes cameras positioned down the left field line, on the roof of the Metrodome, and high above third base.\n\nIn 1990, CBS took over from both ABC and NBC as Major League Baseball's national, over-the-air television provider. They in the process brought along their telestration technology that they dubbed CBS Chalkboard. \"CBS Chalkboard\" made its debut eight years earlier during CBS' coverage of Super Bowl XVI.\n\nFor CBS' coverage of the 1992 All-Star Game, they introduced Basecam, a lipstick-size camera, inside first base.\n\nDuring CBS' coverage of the 1993 World Series, umpires were upset with the overhead replays being televised by CBS. Dave Phillips, the crew chief, said just prior to Game 2 that the umpires want \"CBS to be fair with their approach.\"\n\nRick Gentile, the senior vice president for production of CBS Sports, said that Richie Phillips, the lawyer for the Major League Umpires Association, tried to call the broadcast booth during Saturday's game, but the call was not put through. Richie Phillips apparently was upset when Dave Phillips called the Philadelphia Phillies' Ricky Jordan out on strikes in the fourth inning, and a replay showed the pitch to be about 6 inches outside.\n\nNational League President Bill White, while using a CBS headset in the broadcast booth during Game 1, was overheard telling Gentile and the producer Bob Dekas: \n\n\n\nOn July 8, 1997, Fox televised its first ever All-Star Game (out of Jacobs Field in Cleveland). For this particular game, Fox introduced \"Catcher-Cam\" in which a camera was affixed to the catchers' masks in order to provide unique perspectives of the action around home plate. Catcher-Cam soon would become a regular fixture in Fox's baseball broadcasts.\n\nIn addition to Catcher-Cam, other innovations (some of which have received more acclaim than others) that Fox has provided for baseball telecasts have been:\n\nFor a Saturday afternoon telecast of a Los Angeles Dodgers/Chicago Cubs game at Wrigley Field on August 26, 2000, Fox aired a special \"Turn Back the Clock\" broadcast to commemorate the 61st anniversary of the first televised baseball game. The broadcast started with a re-creation of the television technology of 1939, with play-by-play announcer Joe Buck working alone with a single microphone, a single black-and-white camera, and no graphics; then, each subsequent half-inning would see the broadcast \"jump ahead in time\" to a later era, showing the evolving technologies and presentation of network baseball coverage through the years.\n\n\n\nIn October 2002, Fox televised the first ever World Series to be shown in high definition.\n\n\n\nStarting in 2004, some TBS telecasts (mostly Fridays or Saturdays) became more enhanced. The network decided to call it Braves TBS Xtra. Enhancements included catcher cam, \"Xtra Motion\", which featured the type of pitch and movement, also \"leadOff Line\". It would also show features with inside access to players.\n\nIn October 2004, Fox started airing all Major League Baseball postseason broadcasts (including the League Championship Series and World Series) in high definition. Fox also started airing the Major League Baseball All-Star Game in HD the following year. At the same time, the FoxBox and graphics are upgraded.\n\n\n\nFor their 2007 Division Series coverage, TBS debuted various new looks, such as the first live online views from cameras in dugouts and ones focused on pitchers. TBS also introduced a graphic that creates sort of a rainbow to trace the arc of pitches on game replays. The graphic was superimposed in the studio so analysts like Cal Ripken, Jr. for instance, could take virtual cuts at pitches thrown in games.\n\nDuring their 2009 playoff coverage, TBS displays their \"PitchTrax\" graphic full-time during at-bats (with the center field camera only) during the high-definition version of the broadcast in the extreme right-hand corner of the screen.\n\nMeanwhile, for their own 2009 playoff coverage, Fox announced that they would occasionally include this stat on replays: Speed of pitches as they leave pitchers' hands as well as their speed when they cross home plate.\n\n\nWith the start of the 2011 postseason, TBS planned to introduce the following\n\nThe screen on TBS's standard definition feed now airs a letterboxed version of the native HD feed to match Fox's default widescreen SD presentation, allowing the right side pitch tracking graphic to be seen by SD viewers.\n\nFor the 2011 World Series, Fox debuted infrared technology that's designed to pinpoint heat made by a ball making contact — with, say, bats, face masks, players' bodies — and mark the spot for viewers by making it glow. During Game 1, Fox used \"Hot Spot\" to show that a batted ball was fouled off Texas Rangers batter Adrián Beltré's foot.\n\nFox's 2012 World Series coverage would include a camera whose replays could generate as many as 20,000 frames per second, the most ever seen on Fox—and up from about 60 frames per second on regular replays. The camera would allow viewers \"to see the ball compress\" when batted, similar to how cameras now show golf balls getting compressed when struck. The technology for the camera originated with the U.S. military looking at replays of missile impacts.\n\n\n"}
{"id": "14497677", "url": "https://en.wikipedia.org/wiki?curid=14497677", "title": "Bernhard Eitel", "text": "Bernhard Eitel\n\nBernhard Eitel (born 31 August 1959) is a German earth scientist and geographer. \n\nEitel was born in Baden. Since October 2007, he has been the Rector of Heidelberg University. \n\n"}
{"id": "206687", "url": "https://en.wikipedia.org/wiki?curid=206687", "title": "Brett J. Gladman", "text": "Brett J. Gladman\n\nBrett James Gladman (born 1966) is a Canadian astronomer, discoverer of moons and minor planets, and a full professor at the University of British Columbia's Department of Physics and Astronomy in Vancouver, British Columbia. He holds the Canada Research Chair in planetary astronomy.\n\nGladman is best known for his work in dynamical astronomy in the Solar System. He has studied the\ntransport of meteorites between planets, the delivery of meteoroids from the main asteroid belt, and\nthe possibility of the transport of life via this mechanism, known as panspermia. He also studies planet formation, especially the puzzle of how the giant planets came to be.\n\nHe is discoverer or co-discoverer of many astronomical bodies in the Solar System, asteroids, Kuiper Belt comets, and many moons of the giant planets:\n\nGladman is a member of the Canada–France Ecliptic Plane Survey (CFEPS), and the Outer Solar System Origins Survey (OSSOS) which has detected and tracked the world's largest sample of well-understood Kuiper belt comets, including unusual objects like 2004 XR190 (\"Buffy\") and 2008 KV42 (\"Drac\"), the first trans-Neptunian object on a retrograde orbit around the Sun.\n\nGladman was awarded the H. C. Urey Prize by the Division of Planetary Sciences of the American Astronomical Society in 2002. The main-belt asteroid 7638 Gladman is named in his honor. During 2008–2011 he served as member and chair of the Science Advisory Council of the Canada-France-Hawaii Telescope on Mauna Kea in Hawaii. He was awarded a Killam Research Fellowship in 2015.\n\n\n"}
{"id": "2762666", "url": "https://en.wikipedia.org/wiki?curid=2762666", "title": "Buy-side analyst", "text": "Buy-side analyst\n\nBuy-side analysts (\"buy-siders\") work for buy side money management firms such as mutual funds, pension funds, trusts, and hedge funds. They are incentivised to identify investment opportunities that will improve the net worth of the portfolio they work for.\n\nA buy-side analyst typically works in a mutual fund, pension fund, or other non-brokerage firm, and provides research and recommendations exclusively for the benefit of the company's own money managers (as opposed to individual investors). Unlike sell-side recommendations and reports—which are meant for the analyst's brokerage firm's clients, and the broad outlines of which the press often widely disseminates—buy-side recommendations are not available to anyone outside the firm. If the buy-side analyst stumbles upon a formula, vision, or approach that proves effective, it is kept secret.\n\n\n"}
{"id": "24671268", "url": "https://en.wikipedia.org/wiki?curid=24671268", "title": "Comparison of HTC devices", "text": "Comparison of HTC devices\n\nHTC is the original design manufacturer for many Android and Windows Phone-based smartphones and PDAs. Brands that market or previously marketed HTC-manufactured products include Dell, Fujitsu Siemens, HP/Compaq, i-mate, Krome, O, Palm, Sharp Corporation, and UTStarcom. HTC also manufactures ultra-mobile PCs, and is also the manufacturer of the Nexus One and Nexus 9, a smartphone and tablet designed and branded by Google, respectively.\n\nHTC 10 evo (also known as bolt) needs to be added to the above figure reference is here: https://www.htc.com/us/smartphones/htc-bolt/buy/\n\nThese devices were created under contract for a single non-carrier manufacturer and not branded by HTC, any carriers, or any other manufacturer. For this reason, none of them feature any custom HTC UI.\n\n"}
{"id": "4748226", "url": "https://en.wikipedia.org/wiki?curid=4748226", "title": "Conference of Socialist Economists", "text": "Conference of Socialist Economists\n\nThe Conference of Socialist Economists (CSE) describes itself as an international, democratic membership organisation committed to developing a materialist critique of capitalism, unconstrained by conventional academic divisions between subjects.\n\nCSE's origins lie in the general upsurge in socialist politics in the United Kingdom in the 1960s spurred by disillusion with the Labour government of Harold Wilson, and more specifically in a corresponding dissatisfaction with orthodox economic theory.\n\nA first conference in January 1970 was attended by 75 people, mainly economists, who discussed papers on the capital controversy, the state of development economics, and the internationalisation of capital. A second conference in October of the same year attracted 125 participants (including 20 from abroad) and considered the economic role of the state in modern capitalism.\n\nThis event proved to be the founding conference, deciding to set up CSE as a permanent organisation, to organise a further conference on Britain and the EEC, and to investigate launching a journal. This further conference (December 1971) saw the launch of the \"Bulletin of the CSE\", with the first issue containing four of the conference papers. The \"Bulletin\" was succeeded in 1977 by a refereed journal, \"Capital & Class\", which continues to be published.\n\nNotwithstanding its name and history, both CSE and \"Capital & Class\" live up to the declared aim of being unconstrained by conventional academic subject divisions. Probably only a minority of CSE members are professional economists, and the journal's contents range over the whole of the social and human sciences.\n\nCommon Sense was Journal of the Edinburgh Conference of Socialist Economists.\n\n"}
{"id": "2490676", "url": "https://en.wikipedia.org/wiki?curid=2490676", "title": "Coulomb's constant", "text": "Coulomb's constant\n\nCoulomb's constant, the electric force constant, or the electrostatic constant (denoted , or ) is a proportionality constant in electrodynamics equations. In SI units, it is exactly equal to , or roughly equaling . It was named after the French physicist Charles-Augustin de Coulomb (1736–1806) who introduced Coulomb's law.\n\nCoulomb's constant is the constant of proportionality in Coulomb's law,\n\nwhere is a unit vector in the -direction and\n\nwhere is the fine-structure constant, is the speed of light, is the reduced Planck constant, and is elementary charge. In SI: \nwhere formula_4 is the vacuum permittivity.\nThis formula can be derived from Gauss' law,\n\nTaking this integral for a sphere, radius , around a point charge, we note that the electric field points radially outwards at all times and is normal to a differential surface element on the sphere, and is constant for all points equidistant from the point charge.\n\nNoting that for some test charge ,\n\nIn modern systems of units Coulomb's constant is an exact constant, in Gaussian units , in Lorentz–Heaviside units (also called \"rationalized\") and in SI , where the vacuum permittivity , the speed of light in vacuum is , the vacuum permeability is , \nso that\n\nCoulomb's constant is used in many electric equations, although it is sometimes expressed as the following product of the vacuum permittivity constant:\n\nCoulomb's constant appears in many expressions including the following:\n\nCoulomb's law:\n\nElectric potential energy:\n\nElectric field:\n\n"}
{"id": "38657573", "url": "https://en.wikipedia.org/wiki?curid=38657573", "title": "Cryotank", "text": "Cryotank\n\nCryotank or cryogenic tank is a tank that is used to store frozen biological material. \n\nThe term “cryotank” refers to storage of super-cold fuels, such as liquid oxygen and liquid hydrogen. Cryotanks and cryogenics can be seen in many sci-fi movies, but they are still currently undeveloped. All that needs to be done is for a human to be loaded into the tank and then they can be frozen until a time comes when any diseases they have can be cured and they can live an even longer life. This could also be used in space travel and just preserving human life in general. The problem with this is when the human body is frozen, ice crystals form in the cells. The ice crystals then continue to expand rupturing the cell wall and destroying the integrity of the cell, or killing it.\n\nThis means in order for humans to undergo the cryogenic process a way to significantly raise the levels of glucose produced in the human body is needed.\n\nCryogenic tanks are used to store natural gases such as oxygen, argon, nitrogen, helium, and other materials. Tanks can store the materials at the correct temperature and pressure for transportation.\n\nIn science fiction, cryogenic tanks are used to freeze people. \"cry-\" is a Greek prefix which means \"cold or freezing\", hence humans are stored in the tank frozen until a future date.\nCryotanks are found in some science fiction films such as \"Prometheus\" (2012) and \"The Host\" (2013).\n\n\n"}
{"id": "127511", "url": "https://en.wikipedia.org/wiki?curid=127511", "title": "DNA sequencer", "text": "DNA sequencer\n\nA DNA sequencer is a scientific instrument used to automate the DNA sequencing process. Given a sample of DNA, a DNA sequencer is used to determine the order of the four bases: G (guanine), C (cytosine), A (adenine) and T (thymine). This is then reported as a text string, called a read. Some DNA sequencers can be also considered optical instruments as they analyze light signals originating from fluorochromes attached to nucleotides.\n\nThe first automated DNA sequencer, invented by Lloyd M. Smith, was introduced by Applied Biosystems in 1987. It used the Sanger sequencing method, a technology which formed the basis of the “first generation” of DNA sequencers and enabled the completion of the human genome project in 2001. This first generation of DNA sequencers are essentially automated electrophoresis systems that detect the migration of labelled DNA fragments. Therefore, these sequencers can also be used in the genotyping of genetic markers where only the length of a DNA fragment(s) needs to be determined (e.g. microsatellites, AFLPs).\n\nThe Human Genome Project catalysed the development of cheaper, high throughput and more accurate platforms known as Next Generation Sequencers (NGS) to sequence the human genome. These include the 454, SOLiD and Illumina DNA sequencing platforms. Next generation sequencing machines have increased the rate of DNA sequence substantially compared with previous Sanger methods. DNA samples can be prepared automatically in as little as 90 mins, while a human genome can be sequenced at 15 times coverage in a matter of days.\n\nMore recent, third-generation DNA sequencers such as SMRT and Oxford Nanopore measure the addition of nucleotides to a single DNA molecule in real time.\n\nBecause of limitations in DNA sequencer technology these reads are short compared to the length of a genome therefore the reads must be assembled into longer contigs. The data may also contain errors, caused by limitations in the DNA sequencing technique or by errors during PCR amplification. DNA sequencer manufacturers use a number of different methods to detect which DNA bases are present. The specific protocols applied in different sequencing platforms have an impact in the final data that is generated. Therefore, comparing data quality and cost across different technologies can be a daunting task. Each manufacturer provides their own ways to inform sequencing errors and scores. However, errors and scores between different platforms cannot always be compared directly. Since these systems rely on different DNA sequencing approaches, choosing the best DNA sequencer and method will typically depend on the experiment objectives and available budget.\n\nThe first DNA sequencing methods were developed by Gilbert (1973) and Sanger (1975). Gilbert introduced a sequencing method based on chemical modification of DNA followed by cleavage at specific bases whereas Sanger’s technique is based on dideoxynucleotide chain termination. The Sanger method became popular due to its increased efficiency and low radioactivity. The first automated DNA sequencer was the AB370A, introduced in 1986 by Applied Biosystems. The AB370A was able to sequence 96 samples simultaneously, 500 kilobases per day, and reaching read lengths up to 600 bases. This was the beginning of the “first generation” of DNA sequencers, which implemented Sanger sequencing, fluorescent dideoxy nucleotides and polyacrylamide gel sandwiched between glass plates - slab gels. The next major advance was the release in 1995 of the AB310 which utilized a linear polymer in a capillary in place of the slab gel for DNA strand separation by electrophoresis. These techniques formed the base for the completion of the human genome project in 2001. The human genome project catalysed the development of cheaper, high throughput and more accurate platforms known as Next Generation Sequencers (NGS). In 2005, 454 Life Sciences released the 454 sequencer, followed by Solexa Genome Analyzer and SOLiD (Supported Oligo Ligation Detection) by Agencourt in 2006. Applied Biosystems acquired Agencourt in 2006, and in 2007, Roche bought 454 Life Sciences, while Illumina purchased Solexa. Ion Torrent entered the market in 2010 and was acquired by Life Technologies (now Thermo Fisher Scientific). These are still the most common NGS systems due to their competitive cost, accuracy, and performance.\n\nMore recently, a third generation of DNA sequencers was introduced. The sequencing methods applied by these sequencers do not require DNA amplification (polymerase chain reaction – PCR), which speeds up the sample preparation before sequencing and reduces errors. In addition, sequencing data is collected from the reactions caused by the addition of nucleotides in the complementary strand in real time. Two companies introduced different approaches in their third-generation sequencers. Pacific Biosciences sequencers utilize a method called Single-molecule real-time (SMRT), where sequencing data is produced by light (captured by a camera) emitted when a nucleotide is added to the complementary strand by enzymes containing fluorescent dyes. Oxford Nanopore Technologies is another company developing third-generation sequencers using electronic systems based on nanopore sensing technologies.\n\nDNA sequencers have been developed, manufactured, and sold by the following companies, among others.\n\nThe 454 DNA sequencer was the first next-generation sequencer to become commercially successful. It was developed by 454 Life Sciences and purchased by Roche in 2007. 454 utilizes the detection of pyrophosphate released by the DNA polymerase reaction when adding a nucleotide to the template strain.\n\nRoche currently manufactures two systems based on their pyrosequencing technology: the GS FLX+ and the GS Junior System. The GS FLX+ System promises read lengths of approximately 1000 base pairs while the GS Junior System promises 400 base pair reads. A predecessor to GS FLX+, the 454 GS FLX Titanium system was released in 2008, achieving an output of 0.7G of data per run, with 99.9% accuracy after quality filter, and a read length of up to 700bp. In 2009, Roche launched the GS Junior, a bench top version of the 454 sequencer with read length up to 400bp, and simplified library preparation and data processing.\n\nOne of the advantages of 454 systems is their running speed, Manpower can be reduced with automation of library preparation and semi-automation of emulsion PCR. A disadvantage of the 454 system is that it is prone to errors when estimating the number of bases in a long string of identical nucleotides. This is referred to as a homopolymer error and occurs when there are 6 or more identical bases in row. Another disadvantage is that the price of reagents is relatively more expensive compared with other next-generation sequencers.\n\nIn 2013 Roche announced that they would be shutting down development of 454 technology and phasing out 454 machines completely in 2016.\n\nRoche produces a number of software tools which are optimised for the analysis of 454 sequencing data. GS Run Processor converts raw images generated by a sequencing run into intensity values. The process consists of two main steps: image processing and signal processing. The software also applies normalization, signal correction, base-calling and quality scores for individual reads. The software outputs data in Standard Flowgram Format (or SFF) files to be used in data analysis applications (GS De Novo Assembler, GS Reference Mapper or GS Amplicon Variant Analyzer). GS De Novo Assembler is a tool for \"de novo\" assembly of whole-genomes up to 3GB in size from shotgun reads alone or combined with paired end data generated by 454 sequencers. It also supports de novo assembly of transcripts (including analysis), and also isoform variant detection. GS Reference Mapper maps short reads to a reference genome, generating a consensus sequence. The software is able to generate output files for assessment, indicating insertions, deletions and SNPs. Can handle large and complex genomes of any size. Finally, the GS Amplicon Variant Analyzer aligns reads from amplicon samples against a reference, identifying variants (linked or not) and their frequencies. It can also be used to detect unknown and low-frequency variants. It includes graphical tools for analysis of alignments.\n\nIllumina produces a number of next-generation sequencing machines using technology acquired from Manteia Predictive Medicine and developed by Solexa. Illumina makes a number of next generation sequencing machines using this technology including the HiSeq, Genome Analyzer IIx, MiSeq and the HiScanSQ, which can also process microarrays.\n\nThe technology leading to these DNA sequencers was first released by Solexa in 2006 as the Genome Analyzer. Illumina purchased Solexa in 2007. The Genome Analyzer uses a sequencing by synthesis method. The first model produced 1G per run. During the year 2009 the output was increased from 20G per run in August to 50G per run in December. In 2010 Illumina released the HiSeq 2000 with an output of 200 and then 600G per run which would take 8 days. At its release the HiSeq 2000 provided one of the cheapest sequencing platforms at $0.02 per million bases as costed by the Beijing Genomics Institute.\n\nIn 2011 Illumina released a benchtop sequencer called the MiSeq. At its release the MiSeq could generate 1.5G per run with paired end 150bp reads. A sequencing run can be performed in 10 hours when using automated DNA sample preparation.\n\nThe Illumina HiSeq uses two software tools to calculate the number and position of DNA clusters to assess the sequencing quality: the HiSeq control system and the real-time analyzer. These methods help to assess if nearby clusters are interfering with each other.\n\nLife Technologies (now Thermo Fisher Scientific) produces DNA sequencers under the Applied Biosystems and Ion Torrent brands. Applied Biosystems makes the SOLiD next-generation sequencing platform, and Sanger-based DNA sequencers such as the 3500 Genetic Analyzer. Under the Ion Torrent brand, Applied Biosystems produces four next-generation sequencers: the Ion PGM System, Ion Proton System, Ion S5 and Ion S5xl systems.. The company is also believed to be developing their new capillary DNA sequencer called SeqStudio that will be released early 2018. \n\nSOLiD systems was acquired by Applied Biosystems in 2006. SOLiD applies sequencing by ligation and dual base encoding. The first SOLiD system was launched in 2007, generating reading lengths of 35bp and 3G data per run. After five upgrades, the 5500xl sequencing system was released in 2010, considerably increasing read length to 85bp, improving accuracy up to 99.99% and producing 30G per 7-day run.\n\nThe limited read length of the SOLiD has remained a significant shortcoming and has to some extent limited its use to experiments where read length is less vital such as resequencing and transcriptome analysis and more recently ChIP-Seq and methylation experiments. The DNA sample preparation time for SOLiD systems has become much quicker with the automation of sequencing library preparations such as the Tecan system.\n\nThe colour space data produced by the SOLiD platform can be decoded into DNA bases for further analysis, however software that considers the original colour space information can give more accurate results. Life Technologies has released BioScope, a data analysis package for resequencing, ChiP-Seq and transcriptome analysis. It uses the MaxMapper algorithm to map the colour space reads.\n\nBeckman Coulter (now Danaher) has previously manufactured chain termination and capillary electrophoresis-based DNA sequencers under the model name CEQ, including the CEQ 8000. The company now produces the GeXP Genetic Analysis System, which uses dye terminator sequencing. This method uses a thermocycler in much the same way as PCR to denature, anneal, and extend DNA fragments, amplifying the sequenced fragments.\n\nPacific Biosciences produces the PacBio RS and Sequel sequencing systems using a single molecule real time sequencing, or SMRT, method. This system can produce read lengths of multiple thousands of base pairs. Higher raw read errors are corrected using either circular consensus - where the same strand is read over and over again - or using optimized assembly strategies. Scientists have reported 99.9999% accuracy with these strategies. The Sequel system was launched in 2015 with an increased capacity and a lower price.\n\nOxford Nanopore Technologies has begun shipping early versions of its nanopore sequencing MinION sequencer to selected labs. The device is four inches long and gets power from a USB port. MinION decodes DNA directly as the molecule is drawn at the rate of 450 bases/second through a nanopore suspended in a membrane. Changes in electric current indicate which base is present. It is 60 to 85 percent accurate, compared with 99.9 percent in conventional machines. Even inaccurate results may prove useful because it produces long read lengths. GridION is a slightly larger sequencer that processes up to five MinION flow cells at once. PromethION is another (unreleased) product that will use as many as 100,000 pores in parallel, more suitable for high volume sequencing.\n"}
{"id": "15452499", "url": "https://en.wikipedia.org/wiki?curid=15452499", "title": "David Suzuki: The Autobiography", "text": "David Suzuki: The Autobiography\n\nDavid Suzuki: The Autobiography is the 2006 autobiography of Canadian science writer and broadcaster David Suzuki. The book focuses mostly on his life since the 1987 publication of his first autobiography, \"Metamorphosis: Stages in a Life\". It begins with a chronological account of his childhood, academic years, and broadcasting career. In later chapters, Suzuki adopts a memoir style, writing about themes such as his relationship with Australia, his experiences in Brazil and Papua New Guinea, the founding of the David Suzuki Foundation, and his thoughts on climate change, celebrity status, technology, and death. Throughout, Suzuki highlights the continuing impact of events from his childhood.\n\nThis is Suzuki's forty-third book and, he says, his last. Critics have called the book candid, sincere, and charming, with insightful commentary if occasionally flat stories. Suzuki's scientific background is reflected in the writing's rational and analytic style.\n\nSuzuki's autobiography spent four weeks at 1 on the \"Maclean's\" list of non-fiction best-sellers and six weeks at No. 6 on the \"Globe and Mail's\" list. The book won two awards in 2007: the Canadian Booksellers' Association's Libris Award for Non-Fiction Book of the Year and the British Columbia Booksellers' Choice Award. The publishers, Greystone Books and Douglas & McIntyre, won the CBA Libris Award for Marketing Achievement of the Year.\n\nVancouver-based David Suzuki, 70 years old at the time of this book's publication, is best known as an environmental activist and host of the television show \"The Nature of Things\". He has also worked as a geneticist, nature writer, and university professor. His previous book, written in 2002, was \"Good News for a Change.\" His 1987 book, \"Metamorphosis: Stages in a Life\", unintentionally became his first autobiography. \"Metamorphosis\" was originally drafted as a collection of essays, but following the prompting of his publisher, Suzuki rewrote it in a more autobiographical style.\n\nSuzuki's working title for this second autobiography was \"The Outsider\", a title intended to express the author's view of his own role in society. The origin of this outsider feeling comes from isolation suffered at a Japanese Canadian internment camp during World War II. He was imprisoned there for being Japanese but shunned by other Japanese for being a third generation Canadian, speaking only English. His feeling of isolation continued during his early school years when the only other student of Japanese heritage was his twin sister. Suzuki's daughters acknowledged this perception of himself as an outsider but insist that the public views him very differently, as one of their own, leading to the simple \"The Autobiography\" title.\n\nSuzuki's objective in writing the book was to document his experiences of personal rewards gained from the environmental movement and to illustrate, specifically for young people, opportunities in environmentalism. Suzuki believes that he has been unfairly labelled as \"the master of doom and gloom\" by conservative media outlets and that this book will help balance that view. He intends this autobiography to be his final book. Following its publication he planned to reduce his work week from seven to four days to spend more time with family and personal pursuits.\nThe book has eighteen chapters with a two-page preface, which explains his experience with \"Metamorphosis\" and how this book complements it. The thesis of this book is identified by one reviewer as: \"the importance of childhood's formative years for the development of the person. In Suzuki's case, it is the effects of racism, notably time spent in BC's internment camps during the Second World War, that still haunt him.\" In an interview, Suzuki said, \"my drive to do well has been motivated by the desire to demonstrate to my fellow Canadians that my family and I had not deserved to be treated as we were\". Suzuki identifies a turning point of his life as winning his high school's student presidential election. He initially refused to run believing he was not popular enough. His father encouraged him, saying: \"There's no disgrace in losing ... The important thing is trying.\" Suzuki ran and unexpectedly won with an \"outsider\" platform.\n\nSuzuki recounts his youth and academic years as a student, professor, and genetics researcher. On his broadcasting career, Suzuki recalls early interviews that demonstrated an affinity for public speaking and the jobs that allowed him to travel the world. Regarding his personal life, he describes his relationships with his five children and the development of his two marriages. In a review in the \"New Zealand Listener,\" David Larsen observes: \"Step by step, you see him thinking his way into full-fledged environmentalism: not because he's a natural zealot, but because he's an intellectually honest man brought face to face with evidence that our current economic and energy policies are digging our grandchildren's graves.\"\n\nLater chapters tell of events since \"Metamorphosis.\" In British Columbia, Suzuki spends time on the Queen Charlotte Islands and in Stein Valley advocating against logging. He describes his travels in Brazil while shooting an episode of \"The Nature of Things\" in 1988 and the relationship he developed with the Kayapo people. One of their leaders returned to Canada with him to advocate the protection of his homeland in the Amazon. His tour of Papua New Guinea and how Australia became his second home are explained. He describes the founding and early years of the David Suzuki Foundation, a non-profit organization based on environmental protection and developing sustainability. In the final four chapters Suzuki elaborates on his thoughts about climate change, celebrity status, technology, and death. He laments the lack of global action on climate change, scientific illiteracy on the part of politicians, and the lack of media attention to science. In the final chapter he accepts death as an inevitability and expects his works to be forgotten quickly, leaving his grandchildren as his only true legacy.\n\nSuzuki's tone is relaxed and understated. Robert Wiersema notes that Suzuki's style has \"an analytic quality ... probably rooted in his scientific training\". Suzuki shows a humble, dry humour and instances of blurting out surprising statements. One reviewer describes the style as a \"fusion of by-the-numbers personal narrative and passionate, insightful commentary\".\n\nThe book begins as a chronological narrative of Suzuki's life with photographs of his family and friends. The first five chapters cover the same time period as the first autobiography, from childhood to age fifty. Later chapters use a memoir style with personal thoughts developed around themes. Suzuki recounts his experiences with indigenous groups and his personal relationships with individual members. A travelogue of his journeys in Brazil, Papua New Guinea, Australia, and some places in Canada is presented. Scientific concepts and explanations occur throughout the book.\n\nTwo weeks before its release on April 22, 2006, an excerpt was printed in the national daily newspaper \"The Globe and Mail.\" Greystone Books, the Vancouver division of Douglas & McIntyre, published the book. The book tour included more than 35 stops over two months throughout Canada. Promoted by the publishers as his \"final book tour\" and labelled by Suzuki as his \"thank-you book tour\", it began in Victoria, British Columbia, and included stops from coast-to-coast, from Whitehorse, Yukon, to New Glasgow, Nova Scotia. Attended by nearly 500 people at each event, a multimedia slideshow with personal photos and videos was presented by Suzuki. The publishers estimated that Suzuki signed 5000 books and conducted 137 media interviews. For their efforts Douglas & McIntyre and Greystone Books were awarded the 2007 Canadian Booksellers Association's Libris Award for Marketing Achievement of the Year. In July, the book was published by Allen & Unwin in Australia. Suzuki conducted a promotional tour of both Australia and New Zealand in October and November. The same publishers released paperback editions in April 2007.\n\n\"The Autobiography\" was No. 1 on \"Maclean's\" list of nonfiction bestsellers in Canada for four weeks and spent fifteen weeks in the top ten. The book was on \"The Globe and Mail's\" non-fiction bestsellers' list for five weeks and peaked at No. 6. The book won the 2007 Canadian Booksellers Association's Libris Award for Non-Fiction Book of the Year and the 2007 British Columbia Booksellers' Choice Award.\n\nCritics variously described his writing as \"forthright,\" \"chatty\", and \"charming\". In a review in \"The Globe and Mail\" Brian Brett admires \"Suzuki's disarming candour\" and labels it \"a strange, fascinating book\". While Brett's review is positive, he calls it \"clunkily written\" and sometimes repetitive. The \"Edmonton Journal\" review notes that Suzuki could \"charm the socks off the most hardened soul\", but that many of his stories fall flat. The review in the \"Quill & Quire\" notes Suzuki \"has not written an indulgent autobiography\" and that he \"is too polite to dish on his enemies\". Writing for \"The Vancouver Sun\", Robert Wiersema states that while \"his life is an open book ... [y]ou get the sense of meeting the real Suzuki for the first time.\" Wiersema calls him \"a natural storyteller\". The \"New Zealand Listener\" review states, \"as a writer, he has the charm of a high-school geek desperately trying to get a date ... but ultimately it's what allows his story to convince\". Several critics find Suzuki's writing on death to be particularly well-done.\n\nA number of reviewers compared this book with the earlier one, \"Metamorphosis\". The \"Edmonton Journal\" considers \"David Suzuki: The Autobiography\" to be more candid and insightful than the previous book. On the other hand, Peter Desbarats, writing in Literary Review of Canada, suggests that \"Metamorphosis\" had more personal charm. Desbarats is disappointed that \"The Autobiography\" does not provide a better reflection on the themes of \"Metamorphosis.\" He points out that the best parts, Suzuki's early years, are condensed from one third of \"Metamorphosis\" to a single chapter in \"The Autobiography.\" Desbarats states that neither book ends with a \"satisfying final word\" and concludes that Suzuki \"is his own worst and most frustrating biographer\".\n\n"}
{"id": "56104271", "url": "https://en.wikipedia.org/wiki?curid=56104271", "title": "Degg's Model", "text": "Degg's Model\n\nThe Degg's Model shows that a disaster only occurs if a vulnerable population is exposed to a hazard. It was devised in 1992 by Dr. Martin Degg, Head of Geography at the University of Chester.\n"}
{"id": "13085570", "url": "https://en.wikipedia.org/wiki?curid=13085570", "title": "Don't Ask Me (TV programme)", "text": "Don't Ask Me (TV programme)\n\nDon't Ask Me was a popular British television science show made by Yorkshire Television for the ITV network and ran from 1974 to 1978. It attempted to answer science-based questions and contributors included Magnus Pyke (natural sciences), Rob Buckman (medicine), David Bellamy (biology), Miriam Stoppard (medicine), and Derek Griffiths. Those behind the scenes included Adam Hart-Davis, who later became a well-known science presenter in his own right.\nThe theme music was \"House of the King\" by the contemporary Dutch progressive rock band Focus.\n\nThe series was rebroadcast for a time on TVOntario.\n\nA follow up called \"Don't Just Sit There\" ran for 19 episodes from 1979 to 1980. It was also produced for Yorkshire TV and featured the same panel.\n\n"}
{"id": "1283229", "url": "https://en.wikipedia.org/wiki?curid=1283229", "title": "Eugene Garfield", "text": "Eugene Garfield\n\nEugene Eli Garfield (September 16, 1925 – February 26, 2017) was an American linguist and businessman, one of the founders of bibliometrics and scientometrics. He helped to create \"Current Contents\", \"Science Citation Index\" (SCI), \"Journal Citation Reports\", and \"Index Chemicus\", among others, and founded the magazine \"The Scientist\".\n\nGarfield was born in 1925 in New York City as Eugene Eli Garfinkle, and was raised in a Lithuanian-Italian Jewish family. He studied at the University of Colorado and University of California, Berkeley before getting a Bachelor of Science degree in chemistry from Columbia University in 1948. Garfield also received a degree in Library Science from Columbia University in 1953 He went on to do his PhD in the Department of Linguistics at the University of Pennsylvania, which he completed in 1961 for developing an algorithm for translating chemical nomenclature into chemical formulas.\n\nGarfield founded the Institute for Scientific Information (ISI), which was located in Philadelphia, Pennsylvania. ISI formed a major part of the science division of Thomson Reuters. In October 2016 Thomson Reuters completed the sale of its intellectual property and science division; it is now known as Clarivate Analytics.\n\nGarfield is responsible for many innovative bibliographic products, including \"Current Contents\", the \"Science Citation Index\" (SCI), and other citation databases, the \"Journal Citation Reports\", and \"Index Chemicus\". He is the founding editor and publisher of \"The Scientist\", a news magazine for life scientists. In 2003, the University of South Florida School of Information was honored to have him as lecturer for the Alice G. Smith Lecture. In 2007, he launched HistCite, a bibliometric analysis and visualization software package.\n\nFollowing ideas inspired by Vannevar Bush's highly cited 1945 article \"As We May Think\", Garfield undertook the development of a comprehensive citation index showing the propagation of scientific thinking; he started the Institute for Scientific Information in 1955 (it was sold to the Thomson Corporation in 1992). According to Garfield, \"the citation index ... may help a historian to measure the influence of an article — that is, its 'impact factor'\". The creation of the \"Science Citation Index\" made it possible to calculate impact factor, which ostensibly measures the importance of scientific journals. It led to the unexpected discovery that a few journals like \"Nature\" and \"Science\" were core for all of hard science. The same pattern does not happen with the humanities or the social sciences.\n\nHis entrepreneurial flair in having turned what was, at least at the time, an obscure and specialist metric into a highly profitable business has been noted.\n\nGarfield's work led to the development of several information retrieval algorithms, like the HITS algorithm and PageRank. Both use the structured citation between websites through hyperlinks. Google co-founders Larry Page and Sergey Brin acknowledged Gene in their development of PageRank, the algorithm that powers their company's search engine.\n\nGarfield was awarded the John Price Wetherill Medal in 1984 and the Richard J. Bolte Sr. Award in 2007. The Association for Library and Information Science Education has a fund for doctoral research through an award named after Garfield.\n\nWriting in \"Physiology News\", No. 69, Winter 2007, David Colquhoun of the Department of Pharmacology, University College London, described the \"impact factor,\" a method for comparing scholarly journals, as \"the invention of Eugene Garfield, a man who has done enormous harm to true science.\" Colquhoun ridiculed C. Hoeffel's assertion that Garfield's impact factor \"has the advantage of already being in existence and is therefore a good technique for scientific evaluation\" by saying, \"you can't get much dumber than that. It is a 'good technique' because it is already in existence? There is something better. Read the papers.\"\n\nGarfield is survived by a wife, three sons, a daughter, a step-daughter, two granddaughters, and two great-grandchildren.\n\n"}
{"id": "49062577", "url": "https://en.wikipedia.org/wiki?curid=49062577", "title": "Explorer 19", "text": "Explorer 19\n\nExplorer 19 was an American satellite launched on December 19, 1963, as part of NASA's Explorers program. It was the third of six identical Explorer satellites launched to study air density and composition, and the second to reach orbit. It was identical to Explorer 9.\n\nThe second of six identical air density research satellites to be launched, Explorer 9 was the first to successfully reach orbit. It was still operational when the next satellite, Explorer 19, was launched, allowing simultaneous readings to be taken and compared. The spacecraft consisted of alternating layers of aluminium foil and Mylar polyester film. Uniformly distributed over the aluminium surface were 5.1 cm-diameter dots of white paint for thermal control. The sphere was packed in a tube 21.6 cm in diameter and 48.3 cm long and mounted in the nose of the fourth stage of the launch vehicle. Upon separation of the fourth stage, the sphere was inflated by a nitrogen gas bottle, and a separation spring ejected it out into its own orbit. The two hemispheres of aluminium foil were separated with a gap of Mylar at the spacecraft's equator and served as the antenna. A 136 MHz, 15 mW beacon was carried for tracking purposes, but the beacon failed on the first orbit and the SAO Baker-Nunn camera network had to be relied upon for tracking. Power was supplied by solar cells and rechargeable batteries.\n\nExplorer 19's launch vehicle, a Scout X-4, placed it into a slightly lower than planned orbit.\n\n"}
{"id": "60133", "url": "https://en.wikipedia.org/wiki?curid=60133", "title": "Foundation series", "text": "Foundation series\n\nThe \"Foundation\" series is a science fiction book series written by American author Isaac Asimov. For nearly thirty years, the series was a trilogy: \"Foundation\", \"Foundation and Empire\", and \"Second Foundation\". It won the one-time Hugo Award for \"Best All-Time Series\" in 1966. Asimov began adding to the series in 1981, with two sequels: \"Foundation's Edge\", \"Foundation and Earth\", and two prequels: \"Prelude to Foundation\", \"Forward the Foundation\". The additions made reference to events in Asimov's \"Robot\" and \"Empire\" series, indicating that they were also set in the same fictional universe.\n\nThe premise of the series is that the mathematician Hari Seldon spent his life developing a branch of mathematics known as psychohistory, a concept of mathematical sociology. Using the laws of mass action, it can predict the future, but only on a large scale. Seldon foresees the imminent fall of the Galactic Empire, which encompasses the entire Milky Way, and a dark age lasting 30,000 years before a second great empire arises. Seldon's calculations also show there is a way to limit this interregnum to just one thousand years. To ensure the more favorable outcome and reduce human misery during the intervening period, Seldon creates the Foundation – a group of talented artisans and engineers positioned at the twinned extreme ends of the galaxy – to preserve and expand on humanity's collective knowledge, and thus become the foundation for the accelerated resurgence of this new galactic empire.\n\nThe original trilogy of novels were originally a series of eight short stories published in \"Astounding Magazine\" between May 1942 and January 1950. According to Asimov, the premise was based on ideas set forth in Edward Gibbon's \"History of the Decline and Fall of the Roman Empire\", and was invented spontaneously on his way to meet with editor John W. Campbell, with whom he developed the concepts of the collapse of the Galactic Empire, the civilization-preserving Foundations, and psychohistory. Asimov wrote these early stories in his West Philadelphia apartment when he worked at the Philadelphia Naval Yard.\n\nThe first four stories were collected, along with a new story taking place before the others, as a fixup published by Gnome Press in 1951 as \"Foundation\". The remainder of the stories were published in pairs by Gnome as \"Foundation and Empire\" (1952) and \"Second Foundation\" (1953), resulting in the \"Foundation Trilogy\", as the series is still known.\n\nIn 1981, Asimov was persuaded by his publishers to write a fourth book, which became \"Foundation's Edge\" (1982).\n\nFour years later, Asimov followed up with yet another sequel, \"Foundation and Earth\" (1986), which was followed by the prequels \"Prelude to Foundation\" (1988) and \"Forward the Foundation\" (1993), published after his death in 1992. During the two-year lapse between writing the sequels and prequels, Asimov had tied in his \"Foundation\" series with his various other series, creating a single unified universe. The basic link is mentioned in \"Foundation's Edge\": an obscure tradition about a first wave of space settlements with robots and then a second without. The idea is the one developed in \"Robots of Dawn\", which, in addition to showing the way that the second wave of settlements were to be allowed, illustrates the benefits and shortcomings of the first wave of settlements and their so-called \"C/Fe\" (carbon/iron, signifying humans and robots together) culture. In this same book, the word \"psychohistory\" is used to describe the nascent idea of Seldon's work. Some of the drawbacks to this style of colonization, also called \"Spacer\" culture, are also exemplified by the events described in \"The Naked Sun\".\n\n\"Note: This plot is listed in the fictional chronological order of the stories in the series, which is not the order of publication. The series itself was left as a trilogy for many years, comprising \"Foundation\", \"Foundation and Empire\" and \"Second Foundation\". The two novels set chronologically earlier than the original trilogy, and the two which follow it, were later added to the series.\"\n\n\"Prelude to Foundation\" opens on the planet Trantor, the empire's capital planet, the day after Hari Seldon has given a speech at a mathematics conference. Several parties become aware of the content of his speech (that using mathematical formulas, it may be possible to predict the future course of human history). Seldon is hounded by the Emperor and various employed thugs who are working surreptitiously, which forces him into exile. Over the course of the book, Seldon and Dors Venabili, a female companion and professor of history, are taken from location to location by Chetter Hummin who, under the guise of a reporter, introduces them to various Trantorian walks of life in his attempts to keep Seldon hidden from the Emperor.\nThroughout their adventures all over Trantor, Seldon continuously denies that psychohistory is a realistic science. Even if feasible, it may take several decades to develop. Hummin, however, is convinced that Seldon knows something, so he continuously presses him to work out a starting point to develop psychohistory.\nEventually, after much traveling and introductions to various, diverse cultures on Trantor, Seldon realizes that using the entire known galaxy as a starting point is too overwhelming; he then decides to use Trantor as a model to work out the science, with a goal of later using the applied knowledge on the rest of the galaxy.\n\nEight years after the events of \"Prelude\", Seldon has worked out the science of psychohistory and has applied it on a galactic scale. His notability and fame increase and he is eventually promoted to First Minister to the Emperor. As the book progresses, Seldon loses those closest to him, including his wife, Dors Venabili, as his own health deteriorates into old age. Having worked his entire adult life to understand psychohistory, Seldon instructs his granddaughter, Wanda, to set up the Second Foundation.\n\nCalled forth to stand trial on Trantor for allegations of treason (for foreshadowing the decline of the Galactic Empire), Seldon explains that his science of psychohistory foresees many alternatives, all of which result in the Galactic Empire eventually falling. If humanity follows its current path, the Empire will fall and 30,000 years of turmoil will overcome humanity before a second Empire arises. However, an alternative path allows for the intervening years to be only one thousand, if Seldon is allowed to collect the most intelligent minds and create a compendium of all human knowledge, entitled \"Encyclopedia Galactica\". The board is still wary but allows Seldon to assemble whomever he needs, provided he and the \"Encyclopedists\" be exiled to a remote planet, Terminus. Seldon agrees to these terms – and also secretly establishes a second Foundation of which almost nothing is known, which he says is at the \"opposite end\" of the galaxy.\n\nAfter fifty years on Terminus, and with Seldon now deceased, the inhabitants find themselves in a crisis. With four powerful planets surrounding their own, the Encyclopedists have no defenses but their own intelligence. At the same time, a vault left by Seldon is due to automatically open. The vault reveals a pre-recorded hologram of Seldon, who informs the Encyclopedists that their entire reason for being on Terminus is a fraud, insofar as Seldon did not actually care whether or not an encyclopedia was created, only that the population was placed on Terminus and the events needed by his calculations were set in motion. In reality, the recording discloses, Terminus was set up to reduce the dark ages from 30,000 years to just one millennium, based on following his calculations. It will develop by facing intermittent and extreme \"crises\" – known as \"Seldon Crises\" – which the laws governing psychohistory show will inevitably be overcome, simply because human nature will cause events to fall in particular ways which lead to the intended goal. The recording reveals that the present events are the first such crisis, reminds them that a second foundation was also formed at the \"opposite end\" of the galaxy, and then falls silent.\n\nThe Mayor of Terminus City, Salvor Hardin, proposes to play the planets against each other. His plan is a success; the Foundation remains untouched, and he is promoted to Mayor of Terminus (the planet). Meanwhile, the minds of the Foundation continue to develop newer and greater technologies which are smaller and more powerful than the Empire's equivalents. Using its scientific advantage, Terminus develops trade routes with nearby planets, eventually taking them over when its technology becomes a much-needed commodity. The interplanetary traders effectively become the new diplomats to other planets. One such trader, Hober Mallow, becomes powerful enough to challenge and win the seat of Mayor and, by cutting off supplies to a nearby region, also succeeds in adding more planets to the Foundation's reach.\n\nAn ambitious general of the current Emperor of the Galaxy perceives the Foundation as a growing threat and orders an attack on it, using the Empire's still-mighty fleet of war vessels. The Emperor, initially supportive, becomes suspicious of his general's long-term motive for the attack, and recalls the fleet despite being close to victory. In spite of its undoubted inferiority in purely military terms, the Foundation emerges as the victor and the Empire itself is defeated. Seldon's hologram reappears in the vault on Terminus, and explains to the Foundation that this opening of the vault follows a conflict whose result was inevitable whatever might have been done – a weak Imperial navy could not have attacked them, while a strong navy would have shown itself by its successes to be a direct threat to the Emperor himself and been recalled.\n\nA century later, an unknown outsider called the Mule has begun taking over planets belonging to the Foundation at a rapid pace. The Foundation comes to realize the Mule is a mutant, unforeseen in Seldon's plan, and that the plan cannot have predicted any certainty of defeating him. Toran and Bayta Darell, accompanied by Ebling Mis – the galaxy's current greatest psychologist – and a court jester familiar with the Mule named Magnifico (whom they agree to protect, as his life is under threat from the Mule himself), set out to find the Second Foundation, hoping to bring an end to the Mule's reign. Mis studies furiously in the Great Library of Trantor to decipher the Second Foundation's location in order to visit it and seek their help. He is successful and also deduces that the Mule's success stems from his mutation; he is able to change the emotions of others, a power he used to first instill fear in the inhabitants of his conquered planets, then to make his enemies devoutly loyal to him. Mis is killed by Bayta Darell before he can reveal the location, having realised that Magnifico is in fact the Mule and has been using his gifts to drive Mis forward in his research, so that he can learn the location himself and subjugate the Second Foundation also. Dismayed at having made a mistake which allowed Bayta to see through his disguise, the Mule leaves Trantor to rule over his conquered planets while continuing his search.\n\nAs the Mule comes closer to finding it, the mysterious Second Foundation comes briefly out of hiding to face the threat directly. It consists of the descendants of Seldon's psychohistorians. While the first Foundation has developed the physical sciences, the Second Foundation has been developing Seldon's mathematics and the Seldon Plan, along with their own use of mental sciences. The Second Foundation ultimately wears down the Mule, who returns to rule over his kingdom peacefully for the rest of his life, without any further thought of conquering the Second Foundation.\n\nHowever, as a result, the first Foundation has learned something of the Second Foundation beyond the simple fact that it exists, and has some understanding of its role. This means their behavior will now be chosen in light of that knowledge, and not based on uninformed natural human behavior, which means their behavior will no longer be the natural responses required by the mathematics of the Seldon Plan. This places the Plan itself at great risk. Additionally, the first Foundation instead starts to resentfully consider the other as a rival, and begins to develop equipment related to detecting and blocking mental influence, in order to detect members of the Second Foundation. After many attempts to unravel the Second Foundation's whereabouts from the minimal clues available, the Foundation is led to believe the Second Foundation is located on Terminus (being the \"opposite end\" of a galaxy, for a galaxy with a circular shape). The Foundation uncovers and destroys a group of fifty members of the Second Foundation and is left believing they have destroyed the Second Foundation. No longer concerned at the perceived threat, their behaviors as a society will tend to be those anticipated by the Plan.\n\nIn fact the group of fifty were volunteers on Terminus whose role was to be captured and give the impression that they composed the whole of the Second Foundation, so that the Seldon Plan would be able to continue unimpeded. The Second Foundation, itself, is finally revealed to be located on the former Imperial Homeworld of Trantor. The clue \"at Star's End\" was not a physical clue, but was instead based on an old saying, \"All roads lead to Trantor, and that is where all stars end.\" Seldon, being a social scientist and not a physical one, placed the two Foundations at \"opposite ends\" of the galaxy, but in a sociological rather than physical sense. The first Foundation was located on the Periphery of the galaxy, where the Empire's influence was minimal; the Second Foundation was on Trantor, where, even in its dying days, the Empire's power and culture was strongest.\n\nBelieving the Second Foundation still exists (despite the common belief that it has been extinguished), young politician Golan Trevize is sent into exile by the current Mayor of the Foundation, Harla Branno, to uncover the Second Foundation; Trevize is accompanied by a scholar named Janov Pelorat. The reason for their belief is that, despite the unforeseeable impact of the Mule, the Seldon Plan still appears to be proceeding in accordance with the statements of Seldon's hologram, suggesting that the Second Foundation still exists and is secretly intervening to bring the plan back on course. After a few conversations with Pelorat, Trevize comes to believe that a mythical planet called Earth may hold the secret to the location. No such planet exists in any database, yet several myths and legends all refer to it, and it is Trevize's belief that the planet is deliberately being kept hidden. Unknown to Trevize and Pelorat, Branno is tracking their ship so that, in the event they find the Second Foundation, the first Foundation can take military or other action.\n\nMeanwhile, Stor Gendibal, a prominent member of the Second Foundation, discovers a simple local on Trantor who has had a very subtle alteration made to her mind, far more delicate than anything the Second Foundation can do. He concludes that a greater force of Mentalics must be active in the Galaxy. Following the events on Terminus, Gendibal endeavors to follow Trevize, reasoning that by doing so, he may find out who has altered the mind of the Trantor native.\n\nUsing the few scraps of reliable information within the various myths, Trevize and Pelorat discover a planet called Gaia which is inhabited solely by Mentalics, to such an extent that every organism and inanimate object on the planet shares a common mind. Both Branno and Gendibal, who have separately followed Trevize, also reach Gaia at the same time. Gaia reveals that it has engineered this situation because it wishes to do what is best for humanity but cannot be sure what is best. Trevize's purpose, faced with the leaders of both the First and Second Foundations and Gaia itself, is to be trusted to make the best decision among the three main alternatives for the future of the human race: the First Foundation's path, based on mastery of the physical world and its traditional political organization (i.e., Empire); the Second Foundation's path, based on mentalics and probable rule by an elite using mind control; or Gaia's path of absorption of the entire Galaxy into one shared, harmonious living entity in which all beings, and the galaxy itself, would be a part.\n\nAfter Trevize makes his decision for Gaia's path, the intellect of Gaia adjusts both Branno's and Gendibal's minds so that each believes he or she has succeeded in a significant task. (Branno believes she has successfully negotiated a treaty tying Sayshell to the Foundation, and Gendibal – now leader (and First Speaker) of the Second Foundation – believes that the Second Foundation is victorious and should continue as normal.) Trevize remains, but is uncertain as to why he has intuited (is \"sure\") that Gaia is the correct outcome for the future.\n\nStill uncertain about his decision, Trevize continues on with the search for Earth along with Pelorat and a local of Gaia, advanced in Mentalics, known as Blissenobiarella (usually referred to simply as Bliss). Eventually, Trevize finds three sets of coordinates which are very old. Adjusting them for time, he realizes that his ship's computer does not list any planet in the vicinity of the coordinates. When he physically visits the locations, he rediscovers the forgotten worlds of Aurora, Solaria, and finally Melpomenia. After searching and facing different dilemmas on each planet, Trevize still has not discovered any answers.\n\nAurora and Melpomenia are long deserted, but Solaria contains a small population which is extremely advanced in the field of Mentalics. When the lives of the group are threatened, Bliss uses her abilities (and the shared intellect of Gaia) to destroy the Solarian who is about to kill them. This leaves behind a small child who will be put to death if left alone, so Bliss makes the decision to keep the child as they quickly escape the planet.\n\nEventually, Trevize discovers Earth, but it, again, contains no satisfactory answers for him (it is also long-since deserted). However, it dawns on Trevize that the answer may not be on Earth, but on Earth's satellite – the Moon. Upon approaching the planet, they are drawn inside the Moon's core, where they meet a robot named R. Daneel Olivaw.\n\nOlivaw explains that he has been instrumental in guiding human history for thousands of years, having provided the impetus for Seldon to create psychohistory and also the creation of Gaia, but is now close to the end of his ability to maintain himself and will cease to function. Despite replacing his positronic brain (which contain 20,000 years of memories), he is going to die shortly. He explains that no further robotic brain can be devised to replace his current one, or which will let him continue assisting for the benefit of humanity. However, some additional time can be won to ensure the long term benefit of humanity by merging R. Daniel Olivaw's mind with the organic intellect of a human – in this case, the intellect of the child that the group rescued on Solaria.\n\nOnce again, Trevize is put in the position of deciding if having Olivaw meld with the child's superior intellect would be in the best interests of the galaxy. The decision is left ambiguous (though likely a \"yes\") as it is implied that the melding of the minds may be to the child's benefit, but that she may have sinister intentions about it.\n\nThe early stories were inspired by Edward Gibbon's \"The History of the Decline and Fall of the Roman Empire\". The plot of the series focuses on the growth and reach of the Foundation, against a backdrop of the \"decline and fall of the Galactic Empire.\" The themes of Asimov's stories were also influenced by the political tendency in SF fandom, associated with the Futurians, known as Michelism.\n\nThe focus of the books is the trends through which a civilization might progress, specifically seeking to analyze their progress, using history as a precedent. Although many science fiction novels such as \"Nineteen Eighty-Four\" or \"Fahrenheit 451\" do this, their focus is upon how current trends in society might come to fruition, and act as a moral allegory on the modern world. The \"Foundation\" series, on the other hand, looks at the trends in a wider scope, dealing with societal evolution and adaptation rather than the human and cultural qualities at one point in time.\n\nFurthermore, the concept of psychohistory, which gives the events in the story a sense of rational fatalism, leaves little room for moralization. Hari Seldon himself hopes that his Plan will \"reduce 30,000 years of Dark Ages and barbarism to a single millennium,\" a goal of exceptional moral gravity. Yet events within it are often treated as inevitable and necessary, rather than deviations from the greater good. For example, the Foundation slides gradually into oligarchy and dictatorship prior to the appearance of the galactic conqueror, known as the Mule, who was able to succeed through the random chance of a telepathic mutation. But, for the most part, the book treats the purpose of Seldon's plan as unquestionable, and that slide as being necessary in it, rather than mulling over whether the slide is, on the whole, positive or negative.\n\nThe books also wrestle with the idea of individualism. Hari Seldon's plan is often treated as an inevitable mechanism of society, a vast mindless mob mentality of quadrillions of humans across the galaxy. Many in the series struggle against it, only to fail. However, the plan itself is reliant upon the cunning of individuals such as Salvor Hardin and Hober Mallow to make wise decisions that capitalize on the trends. On the other hand, the Mule, a single individual with mental powers, topples the Foundation and nearly destroys the Seldon plan with his special, unforeseen abilities. To repair the damage the Mule inflicts, the Second Foundation deploys a plan which turns upon individual reactions. Psychohistory is based on group trends and cannot predict with sufficient accuracy the effects of extraordinary, unforeseeable individuals, and as originally presented, the Second Foundation's purpose was to counter this flaw. Later novels would identify the Plan's uncertainties that remained at Seldon's death as the primary reason for the existence of the Second Foundation, which (unlike the First) had retained the capacity to research and further develop psychohistory.\n\nAsimov tried unsuccessfully to end the series with \"Second Foundation\". However, because of the predicted thousand years until the rise of the next Empire (of which only a few hundred had elapsed), the series lacked a sense of closure. For decades, fans pressured him to write a sequel. In 1982, after a 30-year hiatus, Asimov gave in and wrote what was at the time a fourth volume: \"Foundation's Edge\". This was followed shortly thereafter by \"Foundation and Earth\". The story of this volume (which takes place some 500 years after Seldon) ties up all the loose ends and brings together all of his Robot, Empire, and Foundation novels into a single story. He also opens a brand new line of thought in the last dozen pages regarding Galaxia, a galaxy inhabited by a single collective mind. This concept was never explored further. According to his widow Janet Asimov (in her biography of Isaac, \"It's Been a Good Life\"), he had no idea how to continue after \"Foundation and Earth\", so he started writing the prequels.\n\nThe series is set in the same universe as Asimov's first published novel, \"Pebble in the Sky\", although \"Foundation\" takes place about 10,000 years later. \"Pebble in the Sky\" became the basis for the \"Empire\" series. Then, at some unknown date (prior to writing \"Foundation's Edge\") Asimov decided to merge the \"Foundation\"/\"Empire\" series with his \"Robot\" series. Thus, all three series are set in the same universe, giving them a combined length of 15 novels, and a total of about 1,500,000 words (see the List of books below). The merge also created a time-span of the series of around 20,000 years.\n\nThe stand-alone story \"Nemesis\" is also in the same continuity; being referenced in \"Forward the Foundation\", where Hari Seldon refers to a twenty-thousand-year-old story of \"a young woman that could communicate with an entire planet that circled a sun named Nemesis.\" Commentators noted that \"Nemesis\" contains barely disguised references to the Spacers and their calendar system, the Galactic Empire and even to Hari Seldon which seem to have been deliberately placed for the purpose of later integration into the \"Foundation\" universe.\n\nEarly on during Asimov's original world-building of the \"Foundation\" universe, he established within the first published stories a chronology placing the tales about 50,000 years into the future from the time they were written (\"circa\" 1940). This precept was maintained in the pages of his first novel \"Pebble in the Sky\", wherein Imperial archaeologist Bel Arvardan refers to ancient human strata discovered in the Sirius sector dating back \"some 50,000 years\". However, when Asimov decided decades later to retroactively integrate the universe of his \"Foundation\" and \"Galactic Empire\" novels with that of his \"Robot\" stories, a number of changes and minor discrepancies surfaced – the character R. Daneel Olivaw was established as having existed for some 20,000 years, with the original \"Robot\" novels featuring the character occurring not more than a couple of millennia after the early-21st century Susan Calvin short stories. Also, in \"Foundation's Edge\", mankind was referred to as having possessed interstellar space travel for only 22,000 years, a far cry from the 50 millennia of earlier works.\n\nIn the spring of 1955, Asimov published an early timeline in the pages of \"Thrilling Wonder Stories\" magazine based upon his thought processes concerning the \"Foundation\" universe's history at that point in his life, which vastly differs from its modern-era counterpart. Many included stories would later be either jettisoned from the later chronology or temporally relocated by the author. Also, the aforementioned lengthier scope of time was changed. For example, in the original 1950s timeline, humanity does not discover the hyperspatial drive until around 5000 AD, whereas in the reincorporated \"Robot\" universe chronology, the first interstellar jump occurs in 2029 AD, during the events of \"I, Robot\".\n\nBelow is a summarized timeline for events detailed in the series. All dates are quoted in Galactic Era (GE) and Foundation Era (FE) which starts in 12,068 GE.\nAsimov's novels covered only 500 of the expected 1,000 years it would take for the Foundation to become a galactic empire. The novels written after Asimov did not continue the timeline but rather sought to fill in gaps in the earlier stories. The \"Foundation\" universe was once again revisited in 1989's \"Foundation's Friends\", a collection of short stories written by many prominent science fiction authors of that time. Orson Scott Card's \"The Originist\" clarifies the founding of the Second Foundation shortly after Seldon's death; Harry Turtledove's \"Trantor Falls\" tells of the efforts by the Second Foundation to survive during the sacking of Trantor, the imperial capital and Second Foundation's home; and George Zebrowski's \"Foundation's Conscience\" is about the efforts of a historian to document Seldon's work following the rise of the Second Galactic Empire.\n\nAlso, shortly before his death in 1992, Asimov approved an outline for three novels, known as the Caliban trilogy by Roger MacBride Allen, set between \"Robots and Empire\" and the Empire series. The Caliban trilogy describes the terraforming of the Spacer world Inferno, a planet where an ecological crisis forces the Spacers to abandon many long-cherished parts of their culture. Allen's novels echo the uncertainties that Asimov's later books express about the Three Laws of Robotics, and in particular the way a thoroughly roboticized culture can degrade human initiative.\n\nAfter Asimov's death and at the request of Janet Asimov and the Asimov estate's representative, Ralph Vicinanza approached Gregory Benford, and asked him to write another \"Foundation\" story. He eventually agreed, and with Vicinanza and after speaking \"to several authors about [the] project\", formed a plan for a trilogy with \"two hard SF writers broadly influenced by Asimov and of unchallenged technical ability: Greg Bear and David Brin.\" \"Foundation's Fear\" (1997) takes place chronologically between part one and part two of Asimov's second prequel novel, \"Forward the Foundation\"; \"Foundation and Chaos\" (1998) is set at the same time as the first chapter of \"Foundation\", filling in background; \"Foundation's Triumph\" (1999) covers ground following the recording of the holographic messages to the Foundation, and ties together a number of loose ends. These books are now claimed by some to collectively be a \"\"Second Foundation\" trilogy\", although they are inserts into pre-existing prequels and some of the earlier Foundation storylines and not generally recognized as a new Trilogy.\n\nIn an epilogue to \"Foundation's Triumph\", Brin noted he could imagine himself or a different author writing another sequel to add to \"Foundation's Triumph\", feeling that Hari Seldon's story was not yet necessarily finished. He later published a possible start of such a book on his website.\n\nMore recently, the Asimov estate authorized publication of another trilogy of robot mysteries by Mark W. Tiedemann. These novels, which take place several years before Asimov's \"Robots and Empire\", are \"Mirage\" (2000), \"Chimera\" (2001), and \"Aurora\" (2002). These were followed by yet another robot mystery, Alexander C. Irvine's \"Have Robot, Will Travel\" (2004), set five years after the Tiedemann trilogy.\n\nIn 2001, Donald Kingsbury published the novel \"Psychohistorical Crisis\", set in the Foundation universe after the start of the Second Empire.\n\nNovels by various authors (\"Isaac Asimov's\" \"Robot City\", \"Robots and Aliens\" and \"Robots in Time\" series) are loosely connected to the \"Robot\" series, but contain many inconsistencies with Asimov's books, and are not generally considered part of the \"Foundation\" series.\n\nIn November 2009, the Isaac Asimov estate announced the upcoming publication of\" Robots and Chaos\", the first volume in a trilogy featuring Susan Calvin by fantasy author Mickey Zucker Reichert. The book was published in November 2011 under the title \"I, Robot: To Protect\".\n\nIn \"Learned Optimism\", psychologist Martin Seligman identifies the \"Foundation\" series as one of the most important influences in his professional life, because of the possibility of predictive sociology based on psychological principles. He also lays claim to the first successful prediction of a major historical (sociological) event, in the 1988 US elections, and he specifically attributes this to a psychological principle.\n\nIn his 1996 book \"To Renew America\", U. S. House Speaker Newt Gingrich wrote how he was influenced by reading the \"Foundation\" trilogy in high school.\n\nPaul Krugman, winner of the 2008 Nobel Memorial Prize in Economic Sciences, credits the \"Foundation\" series with turning his mind to economics, as the closest existing science to psychohistory.\n\nBusinessman and entrepreneur Elon Musk counts the series among the inspirations for his career. When Musk's Tesla Roadster was launched into space on the maiden flight of the Falcon Heavy rocket in February 2018, amongst other items it carried a copy of the Foundation series.\n\nIn the nonfiction PBS series \"\", Carl Sagan referred to an \"Encyclopedia Galactica\" in the episodes \"Encyclopaedia Galactica\" and \"Who Speaks for Earth\".\n\nIn 1966, the \"Foundation\" trilogy beat several other science fiction and fantasy series to receive a special Hugo Award for \"Best All-Time Series\". The runners-up for the award were \"Barsoom series\" by Edgar Rice Burroughs, \"Future History series\" by Robert A. Heinlein, \"Lensman series\" by Edward E. Smith and \"The Lord of the Rings\" by J. R. R. Tolkien. The Foundation series is still the only series so honored. Asimov himself wrote that he assumed the one-time award had been created to honor \"The Lord of the Rings\", and he was amazed when his work won.\n\nThe series has won three other Hugo Awards. \"Foundation's Edge\" won Best Novel in 1983, and was a bestseller for almost a year. Retrospective Hugo Awards were given in 1996 and 2018 for, respectively, \"The Mule\" (the major part of \"Foundation and Empire\") for Best Novel (1946) and \"Foundation\" (the first story written for the series, and second chapter of the first novel) for Best Short Story (1943).\n\nScience fiction parodies, such as Douglas Adams' \"The Hitchhiker's Guide to the Galaxy\" and Harry Harrison's \"Bill, the Galactic Hero\", often display clear \"Foundation\" influences. For instance, \"The Guide\" of the former is a spoof of the \"Encyclopedia Galactica\", and the series actually mentions the encyclopedia by name, remarking that it is rather \"dry\", and consequently sells fewer copies than the guide; the latter also features the ultra-urbanized Imperial planet Helior, often parodying the logistics such a planet-city would require, but that Asimov's novel downplays when describing Trantor.\n\nIn 1995, Donald Kingsbury wrote \"Historical Crisis\", which he later expanded into a novel, \"Psychohistorical Crisis\". It takes place about 2,000 years after \"Foundation\", after the founding of the Second Galactic Empire. It is set in the same fictional universe as the Foundation series, in considerable detail, but with virtually all \"Foundation\"-specific names either changed (e.g., Kalgan becomes Lakgan), or avoided (psychohistory is created by an unnamed, but often-referenced Founder). The novel explores the ideas of psychohistory in a number of new directions, inspired by more recent developments in mathematics and computer science, as well as by new ideas in science fiction itself.\n\nIn 1998, the novel Spectre (Star Trek) (part of the Shatnerverse series) by William Shatner and Judith and Garfield Reeves-Stevens states that the Mirror Universe divergent path has been studied by the \"Seldon Psychohistory Institute\".\n\nThe oboe-like holophonor in Matt Groening's animated television series \"Futurama\" is based directly upon the \"Visi-Sonor\" which Magnifico plays in \"Foundation and Empire.\" The \"Visi-Sonor\" is also mirrored in an episode of \"Special Unit 2\", where a child's television character plays an instrument that induces mind control over children.\n\nDuring the 2006–2007 Marvel Comics Civil War crossover storyline, in Fantastic Four #542 Mister Fantastic revealed his own attempt to develop psychohistory, saying he was inspired after reading the Foundation series.\n\nAccording to lead singer Ian Gillan, the hard rock band Deep Purple's song \"The Mule\" is based on the Foundation character: \"Yes, The Mule was inspired by Asimov. It's been a while but I'm sure you've made the right connection...Asimov was required reading in the 60's.\"\n\nAn eight-part radio adaptation of the original trilogy, with sound design by the BBC Radiophonic Workshop, was broadcast on BBC Radio 3 in 1973 – one of the first BBC radio drama serials to be made in stereo. A BBC 7 rerun commenced in July 2003.\n\nAdapted by Patrick Tull (episodes 1 to 4) and Mike Stott (episodes 5 to 8), the dramatisation was directed by David Cain and starred William Eedle as Hari Seldon, with Geoffrey Beevers as Gaal Dornick, Lee Montague as Salvor Hardin, Julian Glover as Hober Mallow, Dinsdale Landen as Bel Riose, Maurice Denham as Ebling Mis and Prunella Scales as Lady Callia.\n\nBy 1998, New Line Cinema had spent $1.5 million developing a film version of the \"Foundation Trilogy\". The failure to develop a new franchise was partly a reason the studio signed on to produce \"The Lord of the Rings\" film trilogy.\n\nOn July 29, 2008, New Line Cinema co-founders Bob Shaye and Michael Lynne were reported to have been signed on to produce an adaptation of the trilogy by their company Unique Pictures for Warner Brothers.\nHowever, Columbia Pictures (Sony) successfully bid for the screen rights on January 15, 2009, and then contracted Roland Emmerich to direct and produce. Michael Wimer was named as co-producer.\nTwo years later, the studio hired Dante Harper to adapt the books. This project failed to materialize and HBO acquired the rights when they became available in 2014.\n\nIn November 2014, \"TheWrap\" reported that Jonathan Nolan was writing and producing a TV series based on the \"Foundation Trilogy\" for HBO. Nolan confirmed his involvement at a Paley Center event on April 13, 2015. In June 2017, \"Deadline\" reported that Skydance Media will produce a TV series. In August 2018 it was announced that Apple has commissioned a 10 episode straight-to-series order.\n\nThe \"Author's Note\" of \"Prelude to Foundation\" contains the chronological ordering of Asimov's science fiction books, in which he also said, \"they were not written in the order in which (perhaps) they should be read\". \"Forward the Foundation\" does not appear in Asimov's list, as it was not yet published at the time, and the order of the Empire novels in Asimov's list is not entirely consistent with other lists. For example, the 1983 Ballantine Books printing of \"The Robots of Dawn\" lists the Empire novels as: \"The Stars, Like Dust, The Currents of Space\", and \"Pebble in the Sky\". Given that \"The Currents of Space\" includes Trantor and that \"The Stars, Like Dust\" does not, these two books possibly were accidentally reversed in Asimov's list.\n\n\nAnother alternative is to read the books in their original order of publication, since reading the \"Foundation\" prequels prior to reading the \"Foundation Trilogy\" fundamentally alters the original narrative structure of the trilogy by spoiling what were originally presented as plot surprises. In that same \"Author's Note\", Asimov noted that there is room for a book between \"Robots and Empire (5)\" and \"The Currents of Space (6)\", and that he could follow \"Foundation and Earth (15)\" with additional volumes.\n\nWhile not mentioned in the above list, the books \"The End of Eternity\" (1955) and \"Nemesis\" (1989) are also referenced in the series.\n\n\"The End of Eternity\" is vaguely referenced in \"Foundation's Edge\", where a character mentions the Eternals, whose \"task it was to choose a reality that would be most suitable to Humanity\". (\"The End of Eternity\" also refers to a \"Galactic Empire\" within its story.) In \"Forward the Foundation\", Hari Seldon refers to a 20-thousand-year-old story of \"a young woman that could communicate with an entire planet that circled a sun named Nemesis\", a reference to \"Nemesis\". In \"Nemesis\", the main colony is one of the Fifty Settlements, a collection of orbital colonies that form a state. The Fifty Settlements possibly were the basis for the fifty Spacer worlds in the \"Robot\" stories. The implication at the end of \"Nemesis\" that the inhabitants of the off-Earth colonies are splitting off from Earthbound humans could also be connected to a similar implication about the Spacers in Mark W. Tiedemann's \"Robot\" books.\n\nOn the other hand, these references might be just jokes by Asimov, and the stories mentioned could be just those really written by himself (as seen in \"The Robots of Dawn\", where Fastolfe makes a reference to Asimov's \"Liar!\") and still being read twenty thousand years later. Furthermore, Asimov himself did not mention \"The End of Eternity\" in the series listing from \"Prelude to Foundation\". As for \"Nemesis\", it was written after \"Prelude to Foundation\", but in the author's note Asimov explicitly states that the book is not part of the \"Foundation\" or \"Empire\" series, but that some day he might tie it to the others.\n\n\n\n"}
{"id": "19148519", "url": "https://en.wikipedia.org/wiki?curid=19148519", "title": "Glossary of robotics", "text": "Glossary of robotics\n\nRobotics is the branch of technology that deals with the design, construction, operation, structural disposition, manufacture and application of robots. Robotics is related to the sciences of electronics, engineering, mechanics, and software.\n\nThe following is a list of common definitions related to the Robotics field.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnline Robotics glossary repositories:\n"}
{"id": "6905556", "url": "https://en.wikipedia.org/wiki?curid=6905556", "title": "Infrared cirrus", "text": "Infrared cirrus\n\nInfrared cirrus are filamentary structures seen in space that emit infrared light. The name is given because the structures are cloud-like in appearance. These structures were first detected by the Infrared Astronomy Satellite at wavelengths of 60 and 100 micrometres.\n\n\n"}
{"id": "17113304", "url": "https://en.wikipedia.org/wiki?curid=17113304", "title": "Injun (satellite)", "text": "Injun (satellite)\n\nThe Injun program was a series of six satellites designed and built by researchers at the University of Iowa. They were intended to observe various radiation and magnetic phenomena in the ionosphere and beyond.\n\nThe design specifics of the satellites had little in common, though all were solar-powered and the first five used magnetic stabilization to control spacecraft attitude. (The last in the series was spin-stabilized.) Instruments included particle detectors of varying types, magnetometers, and photometers for observing auroras. The last three satellites were launched as part of the Explorer program.\n\nIn spite of various hardware difficulties and the loss of Injun 2 due to an upper stage failure, the program was generally successful. In particular they produced data on the Van Allen radiation belts including electrical convection in the magnetosphere and the radiation after effects of the Starfish Prime high-altitude nuclear test.\n\n"}
{"id": "53837245", "url": "https://en.wikipedia.org/wiki?curid=53837245", "title": "Jillian Buriak", "text": "Jillian Buriak\n\nJillian M. Buriak FRSC is a Canadian chemist, currently a Canada Research Chair in Nanomaterials at University of Alberta and a Fellow of the Royal Society of Canada, American Association for the Advancement of Science (AAAS) and Royal Society of Chemistry. She is known for her work develoing flexible, lightweight solar cells made from nanoparticles. By spraying a plastic surface with nanomaterials, she's able to fabricate a transparent layer of electrode that act as solar cells. Due the flexibility, they could be incorporated onto different surfaces.\n\nShe has an A.B. degree from Harvard University (1990) and a Ph.D. from Université Louis Pasteur (1995), Strasbourg, working on organometallic chemistry and catalysis. She held a postdoctoral appointment at the Scripps Research Institute at La Jolla, California, working on self-assembly of nanostructures on surfaces. Buriak started her independent faculty career at Purdue University in 1997, was promoted to associate professor, with tenure, in 2001. In 2003, She joined the University of Alberta as a full professor.\n\nFrom 2003 to 2008, Buriak was on the Board of Reviewing Editors (BoRE) at \"Science\" (handling 7-10 papers per week). She was an associate editor at \"ACS Nano\" from 2009 to 2013 (handling >500 papers per year). In 2014, she was appointed as the editor-in-chief of \"Chemistry of Materials\"handling ~5000 papers per year.\n\n\n"}
{"id": "3139692", "url": "https://en.wikipedia.org/wiki?curid=3139692", "title": "Lamb–Oseen vortex", "text": "Lamb–Oseen vortex\n\nIn fluid dynamics, the Lamb–Oseen vortex models a line vortex that decays due to viscosity. This vortex is named after Horace Lamb and Carl Wilhelm Oseen.\n\nThe mathematical model for the flow velocity in the circumferential formula_1–direction in the Lamb–Oseen vortex is:\n\nwith\nThe radial velocity is equal to zero.\n\nThe associated vorticity distribution in the vortex-filament-direction (here formula_7) can be found with the curl: \n\nAn alternative definition is to use the peak tangential velocity of the vortex rather than the total circulation\n\nwhere formula_10 is the radius at which formula_11 is\nattained, and the number \"α\" = 1.25643, see Devenport et al.\n\nThe pressure field simply ensures the vortex rotates in the circumferential direction, providing the centripetal force\nwhere \"ρ\" is the constant density\n"}
{"id": "10205016", "url": "https://en.wikipedia.org/wiki?curid=10205016", "title": "List of Internet entrepreneurs", "text": "List of Internet entrepreneurs\n\nAn Internet entrepreneur is an entrepreneur, an owner, founder or manager of an Internet based business. This list includes Internet company founders, and people brought on to companies for their entrepreneurship skills, not simply for their general business or accounting acumen, as is the case with some CEOs hired by companies started by entrepreneurs.\n\n\n"}
{"id": "15750850", "url": "https://en.wikipedia.org/wiki?curid=15750850", "title": "List of New Zealand inventors", "text": "List of New Zealand inventors\n\nThe following is a list of New Zealand inventors and inventions.\n\n"}
{"id": "30104339", "url": "https://en.wikipedia.org/wiki?curid=30104339", "title": "List of Russian manned space missions", "text": "List of Russian manned space missions\n\nThis is a list of the manned space missions conducted by the Russian Federal Space Agency since 1992. All Russian manned space missions thus far have been carried out using the Soyuz vehicle, and all visited either Mir or the International Space Station.\n\nThe Russian Federal Space Agency was the successor to the Soviet space program. Numeration of the Soyuz flights therefore continues from previous Soviet Soyuz launches. For previous flights of the Soyuz and other manned space vehicles, see List of Soviet manned space missions.\n\n Commercially funded cosmonaut or other \"spaceflight participant\".\n\n"}
{"id": "29230632", "url": "https://en.wikipedia.org/wiki?curid=29230632", "title": "List of Russian physicists", "text": "List of Russian physicists\n\nThis list of Russian physicists includes the famous physicists from the Russian Empire, the Soviet Union and the Russian Federation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "28006061", "url": "https://en.wikipedia.org/wiki?curid=28006061", "title": "List of neurosteroids", "text": "List of neurosteroids\n\nThis is a list of neurosteroids, or natural and synthetic steroids that are active on the mammalian nervous system through receptors other than steroid hormone receptors. It includes inhibitory, excitatory, and neurotrophic neurosteroids as well as pheromones and vomeropherines. In contrast to steroid hormones, neurosteroids have rapid, non-genomic effects through interactions with membrane steroid receptors and can quickly influence central nervous system function.\n\n\n\nThe following are proneurosteroids:\n\n\n\nThe following are proneurosteroids:\n\n\n\n\nThe following are proneurosteroids:\n\n\n\n\nThe following are proneurosteroids:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "24325591", "url": "https://en.wikipedia.org/wiki?curid=24325591", "title": "List of phenyltropanes", "text": "List of phenyltropanes\n\nPhenyltropanes (PTs) are a family of chemical compounds originally derived from structural modification of cocaine. The main feature differentiating phenyltropanes from cocaine is that they lack the ester functionality at the 3-position terminating in the benzene; and thusly the phenyl is attached direct to the tropane skeleton with no further spacer (therefore the name \"phenyl\"-tropane) that the cocaine benzoyloxy provided. The original purpose of which was to extirpate the cardiotoxicity inherent in the local anesthetic \"numbing\" capability of cocaine (since the methylated benzoate ester is essential to cocaine's blockage of sodium channels which cause topical anesthesia) while retaining stimulant function. These compounds present many different avenues of research into therapeutic applications, particularly in addiction treatment. Uses vary depending on their construction and structure-activity relationship ranging from the treating of cocaine dependency to understanding the dopamine reward system in the human brain to treating Alzheimer's & Parkinson's diseases. (Since 2008 there have been continual additions to the list and enumerations of the plethora of types of chemicals that fall into the category of this substance profile.) Certain phenyltropanes can even be used as a smoking cessation aid (\"c.f.\" RTI-29). Many of the compounds were first elucidated in published material by the Research Triangle Institute and are thus named with \"RTI\" serial-numbers (in this case the long form is either RTI-COC-\"n\", for 'cocaine' \"analog\", or specifically RTI-4229-\"n\" of the subsequent numbers given below in this article) Similarly, a number of others are named for Sterling-Winthrop pharmaceuticals (\"WIN\" serial-numbers) and Wake Forest University (\"WF\" serial-numbers). The following includes many of the phenyltropane class of drugs that have been made and studied.\n\n \nLike cocaine, phenyltropanes are considered a 'typical' or 'classical' (i.e. \"cocaine-like\") DAT re-uptake pump ligands in that they stabilize an \"open-to-out\" conformation on the dopamine transporter; despite the extreme similarity to phenyltropanes, benztropine and others are in suchwise not considered \"cocaine-like\" and are instead considered atypical inhibitors insofar as they stabilize what is considered a more inward-facing (closed-to-out) conformational state.\n\nConsidering the differences between PTs and cocaine: the difference in the length of the benzoyloxy and the phenyl linkage contrasted between cocaine and phenyltropanes makes for a shorter distance between the centroid of the aromatic benzene and the bridge nitrogen of the tropane in the latter PTs. This distance being on a scale of 5.6 Å for phenyltropanes and 7.7 Å for cocaine or analogs with the benzoyloxy intact. The manner in which this sets phenyltropanes into the binding pocket at MAT is postulated as one possible explanation to account for PTs increased behavioral stimulation profile over cocaine.\n\nBlank spacings within tables for omitted data use \"no data\", \"?\", \"-\" or \"—\" interchangeably.\n\n\nUse of a cyclopropyl ester appears to enable better MAT retention than does the choice of isopropyl ester.\n\nUse of a Bu resulted in greater DAT selectivity than did the Pr homologue.\n\nSee the \"N\"-desmethyl Paroxetine homologues\n\n<br>\nDimers of phenyltropanes, connected in their dual form using the C2 locant as altered toward a carboxamide structural configuring (in contrast and away from the usual inherent ecgonine carbmethoxy), as per Frank Ivy Carroll's patent inclusive of such chemical compounds, possibly so patented due to being actively delayed pro-drugs \"in vivo\".\n\nThese heterocycles are sometimes referred to as the \"bioisosteric equivalent\" of the simpler esters from which they are derived. A potential disadvantage of leaving the ββ-ester unreacted is that in addition to being hydrolyzable, it can also epimerize to the energetically more favorable trans configuration. This can happen to cocaine also.\n\nSeveral of the oxadiazoles contain the same number and types of heteroatoms, while their respective binding potencies display 8×-15× difference. A finding that would not be accounted for by their affinity originating from hydrogen bonding.\n\nTo explore the possibility of electrostatic interactions, the use of molecular electrostatic potentials (MEP) were employed with model compound 34 (replacing the phenyltropane moiety with a methyl group). Focusing on the vicinity of the atoms @ positions A—C, the \"minima\" of electrostatic potential near atom position A (Δ\"V\"(A)), calculated with semi-empirical (AM1) quantum mechanics computations (superimposing the heterocyclic and phenyl rings to ascertain the least in the way of steric and conformational discrepancies) found a correlation between affinity @ DAT and Δ\"V\"(A): wherein the values for the latter for 32c = 0, 32g = -4, 32h = -50 & 32i = -63 kcal/mol.\n\nIn contrast to this trend, it is understood that an increasingly negative Δ\"V\" is correlated with an increase of strength in hydrogen bonding, which is the opposing trend for the above; this indicates that the 2\"β\"-substituents (at least for the heterocyclic class) are dominated by electrostatic factors for binding in-the-stead of the presumptive hydrogen bonding model for this substituent of the cocaine-like binding ligand.\n\nN.B There are some alternative ways of making the tetrazole ring however; C.f. the sartan drugs synthesis schemes. BuSnN is a milder choice of reagent than hydrogen azide (c.f. Irbesartan).\n\nNote: \"p\"-fluorophenyl is weaker than the others. RTI-145 is not peroxy, it is a methyl carbonate.\n\n\"K\" value for displacement of WIN 35428.\nIC value.\n\nIrreversible (phenylisothiocyanate) binding ligand () RTI-76: \"4′-isothiocyanatophenyl (1R,2S,3S,5S)-3-(4-chlorophenyl)-8-methyl-8-azabicyclo[3.2.1]octane-2-carboxylate\". Also known as: \"3β-(p-chlorophenyl)tropan-2β-carboxylic acid p-isothiocyanatophenylmethyl ester\".\n\n<br>\nHD-205 (Murthy et al., 2007)\n\n<br>\nOne patent claims a series of compounds with biotin-related sidechains are pesticides.\n\n \n\nUnlike metal complexed PTs created with the intention of making useful radioligands, 21a & 21b were produced seeing as their \"η\"-coordinated moiety dramatically altered the electronic character and reactivity of the benzene ring, as well as such a change adding asymmetrical molecular volume to the otherwise planar arene ring unit of the molecule. (\"cf.\" the Dewar–Chatt–Duncanson model). In addition the planar dimension of the transition metal stacked arene becomes delocalized (\"cf.\" Bloom and Wheeler.).\n\n21a was twice as potent as both cocaine and troparil in displacement of \"β\"-CFT, as well as displaying high & low affinity \"K\" values in the same manner as those two compounds. Whereas its inhibition of DA uptake showed it as comparably equipotent to cocaine & troparil. 21b by contrast had a one hundredfold decrease in high-affinity site binding compared to cocaine and a potency 10× less for inhibiting DA uptake. Attesting these as true examples relating useful effective applications for bioorganometallic chemistry.\n\nThe discrepancy in binding for the two benzene metal chelates is assumed to be due to electrostatic differences rather than their respective size difference. The solid cone angles, measured by the steric parameter (\"i.e.\" θ) is θ=131° for Cr(CO) whereas Cp*Ru was θ=187° or only 30% larger. The tricarbonyl moiety being considered equivalent to the cyclopenta dienyl (Cp) ligand.\n\nNS2359 (GSK-372,475)\n\nIt is well established that electrostatic potential around the \"para\" position tends to improve MAT binding. This is believed to also be the case for the \"meta\" position, although it is less studied. N-demethylation dramatically potentiates NET and SERT affinity, but the effects of this on DAT binding are insignificant. Of course, this is not always the case. For an interesting exception to this trend, see the Taxil document. There is ample evidence suggesting that N-demethylation of alkaloids occurs naturally \"in vivo\" via a biological enzyme. The fact that hydrolysis of the ester leads to inactive metabolites means that this is still the main mode of deactivation for analogues that have an easily metabolised 2-ester substituent. The attached table provides good illustration of the effect of this chemical transformation on MAT binding affinities. N.B. In the case of both nocaine and pethidine, N-demethyl compounds are more toxic and have a decreased seizure threshold.\n\nThe \"N\"-demethylated variant of (\"i.e.\" compound code-name after dash)\n\n\"Interest in NET selective drugs continues as evidenced by the development of atomoxetine, manifaxine, and reboxetine as new NET selective compounds for treating ADHD and other CNS disorders such as depression\" (FIC, et al. 2005).\n\nSee the \"N\"-methyl paroxetine homologues\n\"cf.\" di-aryl phenyltropanes for another SSRI approximated hybrid: the fluoxetine based homologue of the phenyltropane class.\nThe eight position nitrogen has been found to not be an exclusively necessary functional anchor for binding at the MAT for phenyltropanes and related compounds. Sulfurs, oxygens, and even the removal of any heteroatom, leaving only the carbon skeleton of the structure at the bridged position, still show distinct affinity for the monoamine transporter cocaine-target site and continue to form an ionic bond with a measurable degree of reasonable efficacy.\n\nBi- and tri-cyclic aza compounds and their uses \n\nSee: Bridged cocaine derivatives & \"N\"8 Tricyclic (2\"β\"—crossed-over) \"N\"8—to—3\"β\" replaced aryl linked (expansive front-bridged) cocaine analogues\n\nFused tropane-derivatives as neurotransmitter reuptake inhibitors. Singh notes that all bridged derivatives tested displayed 2.5—104 fold higher DAT affinity than cocaine. The ones 2.8—190 fold more potent at DAT also had increased potency at the other two MAT sites (NET & SERT); NET having 1.6—78× increased activity. \"(+)-128\" additionally exhibited 100× greater potency @ SERT, whereas \"132a\" & \"133a\" had 4—5.2× weaker 5-HTT (\"i.e.\" SERT) activity. Front-bridged (\"e.g.\" 128 & 129) had a better 5-HT/DA reuptake ratio in favor of SERT, while the back-bridged (\"e.g.\" 130—133) preferred placement with DAT interaction.\n\n\nTo make a different type of analog (see Kozikowski patent above)\n\n<br>\n(1R,2S,10R,12S)-15-methyl-15-azatetracyclo(10.2.1.0²,¹⁰.0⁴,⁹)pentadeca-4(9),5,7-trien-3-one\n\n<br>\nParent compound of a series of spirocyclic cocaine benzoyl linkage modification analogs created by Suzuki coupling method of \"ortho\"-substituted arylboronic acids and an enol-triflate derived from cocaine; which technically has the three methylene length of cocaine analogues as well as the single length which defines the phenyltropane series. Note that the carbomethoxyl group is (due to constraints in synthetic processes used in the creation of this compound) alpha configured; which is not the usual, most prevalent, conformation favored for the PT cocaine-receptor binding pocket of most such sub-type of chemicals. The above and below depictions show attested compounds synthesized, additionally with variations upon the \"Endo–exo\" isomerism of their structures.<br>\n\n3-Phenyl-9-azabicyclo[3.3.1]nonane derivatives\n\nTo better elucidate the binding requirements at MAT, the methylene unit on the tropane was extended by one to create the azanonane analogs. Which are the beginning of classes of modifications that start to become effected by the concerns & influences of macrocyclic stereocontrol.\n\nDespite the loosened flexibility of the ring system, nitrogen constrained variants (such as were created to make the bridged class of phenyltropanes) which might better fit the rigid placement necessary to suit the spatial requirements needed in the binding pocket were not synthesized. Though front-bridged types were synthesized for the piperidine homologues: the trend of equal values for either isomers of that type followed the opposing trend of a smaller and lessened plasticity of the molecule to contend with a rationale for further constraining the pharmacophore within that scope. Instead such findings lend credence to the potential for the efficacy of fusing the nitrogen on an enlarged tropane, as like upon the compounds given below.\n3-Phenyl-7-azabicyclo[2.2.1]heptane derivatives\n\nRing-contracted analogs of phenyltropanes did not permit sufficient penetration of the phenyl into the target binding site on MAT for an affinity in the efficacious range. The distance from the nitrogen to the phenyl centroid for 155a was 4.2 and 155c was 5.0 Å, respectively. (Whereas troparil was 5.6 & compound 20a 5.5 angstroms). However piperidine homologues (discussed below) had comparable potencies.\nAzabornanes with longer substitutions at the 3\"β\"-position (benzoyloxys alkylphenyls, carbamoyls etc.) or with the nitrogen in the position it would be on the piperidine homologues (\"i.e.\" arrangements of differing locations for the nitrogens being either distal or proximal within the terms required to facilitate the framework of the compound to a correlative proportion, functional for the given moiety), were not synthesized, despite conclusions that the nitrogen to phenyl length was the issue at variance enough to be the interfering factor for the proper binding of the compressed topology of the azabornane. Carroll, however, has listed benzoyloxy azabornanes in patents.\nPiperidine homologues had comparable affinity & potency spreads to their respective phenyltropane analogues. Without as much of a discrepancy between the differing isomers of the piperidine class with respect to affinity and binding values as had in the phenyltropanes.\n\nHeterocyclic \"N\"-Desmethyl<br>\n<br>\n\"cf.\" Fencamfamine\n\nThese compounds include transition metals in their heteroatomic conformation, unlike non-radiolabel intended chelates where their element is chosen for intrinsic affectation to binding and function, these are tagged on by a \"tail\" (or similar) with a sufficient spacer to remain separated from known binding properties and instead are meant to add radioactivity enough to be easily tracked via observation methods that utilize radioactivity. As for anomalies of binding within the spectrum of the under-written kinds just mentioned: other factors not otherwise considered to account for its relatively lower potency, \"compound 89c\" is posited to protrude forward at the aryl place on its moiety toward the MAT ligand acceptor site in a manner detrimental to its efficacy. That is considered due to the steric bulk of the eight-position \"tail\" chelate substituted constituent, overreaching the means by which it was intended to be isolated from binding factors upon a tail, and ultimately nonetheless, interfering with its ability to bind. However, to broach this discrepancy, decreasing of the nitrogen tether at the eight position by a single methylene unit (89d) was shown to bring the potency of the analogous compound to the expected, substantially higher, potency: The \"N\"-methyl analog of 89c having an IC of 1.09 ± 0.02 @ DAT & 2.47 ± 0.14 nM @ SERT; making 89c upwards of thirty-three times weaker at those MAT uptake sites.\nPhenyltropanes can be grouped by \"N substitution\" \"Stereochemistry\" \"2-substitution\" & by the nature of the 3-phenyl group substituent X.\nOften this has dramatic effects on selectivity, potency, and duration, also toxicity, since phenyltropanes are highly versatile. For more examples of interesting phenyltropanes, see some of the more recent patents, e.g. , , , and .\n\nPotency \"in vitro\" should not be confused with the actual dosage, as [[pharmacokinetic]] factors can have a dramatic influence on what proportion of an administered dose actually gets to the target binding sites in the brain, and so a drug that is very potent at binding to the target may nevertheless have only moderate potency \"in vivo\". For example, [[RTI-336]] requires a higher dosage than cocaine. Accordingly, the active dosage of [[RTI-386]] is exceedingly poor despite the relatively high ex vivo [[Dopamine transporter|DAT]] binding affinity.\n\nMany molecular drug structures have exceedingly similar pharmarcology to phenyltropanes, yet by certain technicalities do not fit the phenyltropane moniker. These are namely classes of dopaminergic cocaine analogues that are in the [[piperidine]] class (a category that includes [[methylphenidate]]) or [[benztropine]] class (such as [[Difluoropine]]: which is extremely close to fitting the criteria of being a phenyltropane.) Whereas other potent [[Dopamine reuptake inhibitor|DRIs]] are far removed from being in the phenyltropane structural family, such as [[Benocyclidine]] or [[Vanoxerine]].\n[[File:DextroMPH-overlays-betaCPT.png|left|x200px]]\n\n\n[[Category:Cocaine]]\n[[Category:Dopamine reuptake inhibitors]]\n[[Category:Stimulants]]\n[[Category:Tropanes]]\n[[Category:Chemistry-related lists|Phenyltropanes]]\n[[Category:Designer drugs]]"}
{"id": "20723111", "url": "https://en.wikipedia.org/wiki?curid=20723111", "title": "Macrocyst", "text": "Macrocyst\n\nA macrocyst is an aggregate of cells of Dictyostelids formed during sexual reproduction enclosed in a cellulose wall.\n\nIf two amoebae of different mating types are present in a dark and wet environment, they can fuse during aggregation to form a giant cell. The giant cell will then engulf the other cells in the aggregate and encase the whole aggregate in a thick, cellulose wall to protect it. This is known as a macrocyst. Inside the macrocyst, the giant cell divides first through meiosis, then through mitosis to produce many haploid amoebae that will be released to feed as normal amoebae would.\n\n"}
{"id": "15450528", "url": "https://en.wikipedia.org/wiki?curid=15450528", "title": "Marshall Sklare Award", "text": "Marshall Sklare Award\n\nThe Marshall Sklare Award is an annual honor of the Association for the Social Scientific Study of Jewry (ASSJ). The ASSJ seeks to recognize \"a senior scholar who has made a significant scholarly contribution to the social scientific study of Jewry.\" In most cases, the recipient has given a scholarly address. In recent years, the honored scholar has presented the address at the annual meeting of the Association for Jewish Studies.\n\nThe award is named in memory of the \"founding father of American Jewish sociology\" Marshall Sklare (1921-1992), who had been Klutznick Family Professor of Contemporary Jewish Studies and Sociology at Brandeis University.\n\nPast recipients, field of study, and the title of their scholarly paper have been:\n\n"}
{"id": "9949565", "url": "https://en.wikipedia.org/wiki?curid=9949565", "title": "Medical microbiology", "text": "Medical microbiology\n\nMedical microbiology , the large subset of microbiology that is applied to medicine, is a branch of medical science concerned with the prevention, diagnosis and treatment of infectious diseases. In addition, this field of science studies various clinical applications of microbes for the improvement of health. There are four kinds of microorganisms that cause infectious disease: bacteria, fungi, parasites and viruses, and one type of infectious protein called prion.\n\nA medical microbiologist studies the characteristics of pathogens, their modes of transmission, mechanisms of infection and growth. Using this information, a treatment can be devised. Medical microbiologists often serve as consultants for physicians, providing identification of pathogens and suggesting treatment options.\nOther tasks may include the identification of potential health risks to the community or monitoring the evolution of potentially virulent or resistant strains of microbes, educating the community and assisting in the design of health practices. They may also assist in preventing or controlling epidemics and outbreaks of disease.\nNot all medical microbiologists study microbial pathology; some study common, non-pathogenic species to determine whether their properties can be used to develop antibiotics or other treatment methods.\n\nEpidemiology, the study of the patterns, causes, and effects of health and disease conditions in populations, is an important part of medical microbiology, although the clinical aspect of the field primarily focuses on the presence and growth of microbial infections in individuals, their effects on the human body, and the methods of treating those infections. In this respect the entire field, as an applied science, can be conceptually subdivided into academic and clinical subspecialties, although in reality there is a fluid continuum between public health microbiology and clinical microbiology, just as the state of the art in clinical laboratories depends on continual improvements in academic medicine and research laboratories. \n\nIn 1676, Anton van Leeuwenhoek observed bacteria and other microorganisms, using a single-lens microscope of his own design.\n\nIn 1796, Edward Jenner developed a method using cowpox to successfully immunize a child against smallpox. The same principles are used for developing vaccines today.\n\nFollowing on from this, in 1857 Louis Pasteur also designed vaccines against several diseases such as anthrax, fowl cholera and rabies as well as pasteurization for food preservation.\n\nIn 1867 Joseph Lister is considered to be the father of antiseptic surgery. By sterilizing the instruments with diluted carbolic acid and using it to clean wounds, post-operative infections were reduced, making surgery safer for patients.\n\nIn the years between 1876 and 1884 Robert Koch provided much insight into infectious diseases. He was one of the first scientists to focus on the isolation of bacteria in pure culture. This gave rise to the germ theory, a certain microorganism being responsible for a certain disease. He developed a series of criteria around this that have become known as the Koch's postulates.\n\nA major milestone in medical microbiology is the Gram stain. In 1884 Hans Christian Gram developed the method of staining bacteria to make them more visible and differentiable under a microscope. This technique is widely used today.\n\nIn 1929 Alexander Fleming developed the most commonly used antibiotic substance both at the time and now: penicillin.\n\nDNA sequencing, a method developed by Walter Gilbert and Frederick Sanger in 1977, caused a rapid change the development of vaccines, medical treatments and diagnostic methods. Some of these include synthetic insulin which was produced in 1979 using recombinant DNA and the first genetically engineered vaccine was created in 1986 for hepatitis B.\n\nIn 1995 a team at The Institute for Genomic Research sequenced the first bacterial genome; \"Haemophilus influenzae\". A few months later, the first eukaryotic genome was completed. This would prove invaluable for diagnostic techniques.\n\n\n\n\n\nInfections may be caused by bacteria, viruses, fungi, and parasites. The pathogen that causes the disease may be exogenous (acquired from an external source; environmental, animal or other people, e.g. Influenza) or endogenous (from normal flora e.g. candidiasis).\n\nThe site at which a microbe enters the body is referred to as the portal of entry. These include the respiratory tract, gastrointestinal tract, genitourinary tract, skin, and mucous membranes. The portal of entry for a specific microbe is normally dependent on how it travels from its natural habitat to the host.\n\nThere are various ways in which disease can be transmitted between individuals. \nThese include:\n\nLike other pathogens, viruses use these methods of transmission to enter the body, but viruses differ in that they must also enter into the host's actual cells. Once the virus has gained access to the host's cells, the virus' genetic material (RNA or DNA) must be introduced to the cell. Replication between viruses is greatly varied and depends on the type of genes involved in them. Most DNA viruses assemble in the nucleus while most RNA viruses develop solely in cytoplasm.\n\nThe mechanisms for infection, proliferation, and persistence of a virus in cells of the host are crucial for its survival. For example, some diseases such as measles employ a strategy whereby it must spread to a series of hosts. In these forms of viral infection, the illness is often treated by the body's own immune response, and therefore the virus is required to disperse to new hosts before it is destroyed by immunological resistance or host death. In contrast, some infectious agents such as the Feline leukemia virus, are able to withstand immune responses and are capable of achieving long-term residence within an individual host, whilst also retaining the ability to spread into successive hosts.\n\nIdentification of an infectious agent for a minor illness can be as simple as clinical presentation; such as gastrointestinal disease and skin infections. In order to make an educated estimate as to which microbe could be causing the disease, epidemiological factors need to be considered; such as the patient's likelihood of exposure to the suspected organism and the presence and prevalence of a microbial strain in a community.\n\nDiagnosis of infectious disease is nearly always initiated by consulting the patient's medical history and conducting a physical examination. More detailed identification techniques involve microbial culture, microscopy, biochemical tests and genotyping. Other less common techniques (such as X-rays, CAT scans, PET scans or NMR) are used to produce images of internal abnormalities resulting from the growth of an infectious agent.\n\nMicrobiological culture is the primary method used for isolating infectious disease for study in the laboratory. Tissue or fluid samples are tested for the presence of a specific pathogen, which is determined by growth in a selective or differential medium.\n\nThe 3 main types of media used for testing are:\n\nCulture techniques will often use a microscopic examination to help in the identification of the microbe. Instruments such as compound light microscopes can be used to assess critical aspects of the organism. This can be performed immediately after the sample is taken from the patient and is used in conjunction with biochemical staining techniques, allowing for resolution of cellular features. Electron microscopes and fluorescence microscopes are also used for observing microbes in greater detail for research.\n\nFast and relatively simple biochemical tests can be used to identify infectious agents. For bacterial identification, the use of metabolic or enzymatic characteristics are common due to their ability to ferment carbohydrates in patterns characteristic of their genus and species. Acids, alcohols and gases are usually detected in these tests when bacteria are grown in selective liquid or solid media, as mentioned above. In order to perform these tests en masse, automated machines are used. These machines perform multiple biochemical tests simultaneously, using cards with several wells containing different dehydrated chemicals. The microbe of interest will react with each chemical in a specific way, aiding in its identification.\n\nSerological methods are highly sensitive, specific and often extremely rapid laboratory tests used to identify different types of microorganisms. The tests are based upon the ability of an antibody to bind specifically to an antigen. The antigen (usually a protein or carbohydrate made by an infectious agent) is bound by the antibody, allowing this type of test to be used for organisms other than bacteria. This binding then sets off a chain of events that can be easily and definitively observed, depending on the test. More complex serological techniques are known as immunoassays. Using a similar basis as described above, immunoassays can detect or measure antigens from either infectious agents or the proteins generated by an infected host in response to the infection.\n\nPolymerase chain reaction (PCR) assays are the most commonly used molecular technique to detect and study microbes. As compared to other methods, sequencing and analysis is definitive, reliable, accurate, and fast. Today, quantitative PCR is the primary technique used, as this method provides faster data compared to a standard PCR assay. For instance, traditional PCR techniques require the use of gel electrophoresis to visualize amplified DNA molecules after the reaction has finished. quantitative PCR does not require this, as the detection system uses fluorescence and probes to detect the DNA molecules as they are being amplified. In addition to this, quantitative PCR also removes the risk of contamination that can occur during standard PCR procedures (carrying over PCR product into subsequent PCRs). Another advantage of using PCR to detect and study microbes is that the DNA sequences of newly discovered infectious microbes or strains can be compared to those already listed in databases, which in turn helps to increase understanding of which organism is causing the infectious disease and thus what possible methods of treatment could be used. This technique is the current standard for detecting viral infections such as AIDS and hepatitis.\n\nOnce an infection has been diagnosed and identified, suitable treatment options must be assessed by the physician and consulting medical microbiologists. Some infections can be dealt with by the body's own immune system, but more serious infections are treated with antimicrobial drugs. Bacterial infections are treated with antibacterials (often called antibiotics) whereas fungal and viral infections are treated with antifungals and antivirals respectively. A broad class of drugs known as antiparasitics are used to treat parasitic diseases.\n\nMedical microbiologists often make treatment recommendations to the patient's physician based on the strain of microbe and its antibiotic resistances, the site of infection, the potential toxicity of antimicrobial drugs and any drug allergies the patient has.\nIn addition to drugs being specific to a certain kind of organism (bacteria, fungi, etc.), some drugs are specific to a certain genus or species of organism, and will not work on other organisms. Because of this specificity, medical microbiologists must consider the effectiveness of certain antimicrobial drugs when making recommendations. Additionally, strains of an organism may be resistant to a certain drug or class of drug, even when it is typically effective against the species. These strains, termed resistant strains, present a serious public health concern of growing importance to the medical industry as the spread of antibiotic resistance worsens. Antimicrobial resistance is an increasingly problematic issue that leads to millions of deaths every year.\n\nWhilst drug resistance typically involves microbes chemically inactivating an antimicrobial drug or a cell mechanically stopping the uptake of a drug, another form of drug resistance can arise from the formation of biofilms. Some bacteria are able to form biofilms by adhering to surfaces on implanted devices such as catheters and prostheses and creating an extracellular matrix for other cells to adhere to. This provides them with a stable environment from which the bacteria can disperse and infect other parts of the host. Additionally, the extracellular matrix and dense outer layer of bacterial cells can protect the inner bacteria cells from antimicrobial drugs.\n\nMedical microbiology is not only about diagnosing and treating disease, it also involves the study of beneficial microbes. Microbes have been shown to be helpful in combating infectious disease and promoting health. Treatments can be developed from microbes, as demonstrated by Alexander Fleming's discovery of penicillin as well as the development of new antibiotics from the bacterial genus Streptomyces among many others. Not only are microorganisms a source of antibiotics but some may also act as probiotics to provide health benefits to the host, such as providing better gastrointestinal health or inhibiting pathogens.\n\n"}
{"id": "2081599", "url": "https://en.wikipedia.org/wiki?curid=2081599", "title": "Moral high ground", "text": "Moral high ground\n\nThe moral high ground, in ethical or political parlance, refers to the status of being respected for remaining moral, and adhering to and upholding a universally recognized standard of justice or goodness.\n\nHolding the moral high ground can be used to legitimize political movements, notably nonviolent resistance, especially in the face of violent opposition, and has been used by civil disobedience movements around the world to garner sympathy and support from society.\n\nSimilarly, 21st century states may refrain from declaring war in order to retain the moral high ground – though the cynic will observe that realpolitik still leads to wars being \"fought\", only without declarations.\n\nEconomist and social critic Robert H. Frank challenged the idea that prosocial behavior was necessarily deleterious in business in his book \"What Price the Moral High Ground?\"\n\nHe argued that socially responsible firms often reap unexpected benefits even in highly competitive environments, because their commitment to principle makes them more attractive as partners to do business with.\n\nIn everyday use a person may take the perspective of the 'moral high ground' in order to produce a critique of something, or merely to win an argument. This perspective is sometimes associated to snobbery but may also be a legitimate way of taking up a stance.\n\nSocial sciences or philosophies are sometimes accused of taking the 'moral high ground' because they are often inherently interested in the project of human freedom and justice. The traditional project of education itself may be seen as defending a type of moral high ground from popular culture, perhaps by using critical pedagogy: its proponents may themselves be accused (rightly or wrongly) of seeking a false and unjustified sense of superiority thereby.\n\nKate Fillion considered that in the wake of second-wave feminism \"assumptions about female moral superiority pervade the public discourse\", placing women as \"comrades on the moral high ground, pitted against a common adversary who dwells in the shadowy depths far below\".\n\nRobert Lowell took the moral high ground, not once, but twice, with separate American presidents, in protesting US militarism.\n\nPeter Mandelson considered that Tony Blair was \"good at taking the high ground and throwing himself off it\".\n\n"}
{"id": "14697873", "url": "https://en.wikipedia.org/wiki?curid=14697873", "title": "Natural stress", "text": "Natural stress\n\nIn regard to agriculture, Abiotic stress is stress produced by natural environment factors such as extreme temperatures, wind, drought, and salinity. Humankind doesn’t have much control over abiotic stresses. It is very important for humans to understand how stress factors affect plants and other living things so that we can take some preventative measures. \n\nPreventative measures are the only way that humans can protect themselves and their possessions from abiotic stress. There are many different types of abiotic stressors, and several methods that humans can use to reduce the negative effects of stress on living things.\n\nOne of the types of Abiotic Stress is cold. This has a huge impact on farmers. Cold impacts crop growers all over the world in every single country. Yields suffer and farmers also suffer huge losses because the weather is just too cold to produce crops (Xiong & Zhu, 2001). \n\nHumans have planned the planting of our crops around the seasons. Even though the seasons are fairly predictable, there are always unexpected storms, heat waves, or cold snaps that can ruin our growing seasons.(Suzuki & Mittler, 2006)\n\nROS stands for reactive oxygen species. ROS plays a large role in mediating events through transduction. Cold stress was shown to enhance the transcript, protein, and activity of different ROS-scavenging enzymes. Low temperature stress has also been shown to increase the H2 O2 accumulation in cells.(Suzuki & Mittler, 2006)\n\nPlants can be acclimated to low or even freezing temperatures. If a plant can go through a mild cold spell this activates the cold-responsive genes in the plant. Then if the temperature drops again, the genes will have conditioned the plant to cope with the low temperature. Even below freezing temperatures can be survived if the proper genes are activated (Suzuki & Mittler, 2006).\n\nHeat stress has been shown to cause problems in mitochondrial functions and can result in oxidative damage. Activators of heat stress receptors and defenses are thought to be related to ROS. Heat is another thing that plants can deal with if they have the proper pretreatment. This means that if the temperature gradually warms up the plants are going to be better able to cope with the change. A sudden long temperature increase could cause damage to the plant because their cells and receptors haven’t had enough time to prepare for a major temperature change.\n\nHeat stress can also have a detrimental effect on plant reproduction. Temperatures 10 degrees Celsius or more above normal growing temperatures can have a bad effect on several plant reproductive functions. Pollen meiosis, pollen germination, ovule development, ovule viability, development of the embryo, and seedling growth are all aspects of plant reproduction that are affected by heat.(Cross, McKay, McHughen, & Bonham-Smith, 2003)\n\nThere have been many studies on the effects of heat on plant reproduction. One study on plants was conducted on Canola plants at 28 degrees Celsius, the result was decreased plant size, but the plants were still fertile. Another experiment was conducted on Canola plants at 32 degrees Celsius, this resulted in the production of sterile plants. Plants seem to be more easily damaged by extreme temperatures during the late flower to early seed development stage (Cross, McKay, McHughen, & Bonham-Smith, 2003).\n\nWind is a huge part of abiotic stress. There is simply no way to stop the wind from blowing. This is definitely a bigger problem in some parts of the world than in others. Barren areas such as deserts are very susceptible to natural wind erosion. These types of areas don’t have any vegetation to hold the soil particles in place. Once the wind starts to blow the soil around, there is nothing to stop the process. The only chance for the soil to stay in place is if the wind doesn’t blow. This is usually not an option.\n\nPlant growth in windblown areas is very limited. Because the soil is constantly moving, there is no opportunity for plants to develop a root system. Soil that blows a lot usually is very dry also. This leaves little nutrients to promote plant growth.\n\nFarmland is typically very susceptible to wind erosion. Most farmers do not plant cover crops during the seasons when their main crops are not in the fields. They simply leave the ground open and uncovered. When the soil is dry, the top layer becomes similar to powder. When the wind blows, the powdery top layer of the farmland is picked up and carried for miles. This is the exact scenario that occurred during the “Dust Bowl” in the 30’s. The combination of drought and poor farming practices allowed the wind to moves thousands of tons of dirt from one area to the next.\n\nWind is one of the factors that humans can really have some control over. Simply practice good farming practices. Don’t leave ground bare and without any type of vegetation. During dry seasons it is especially important to have the land covered because dry soil moves much easier than wet soil in the wind.\n\nWhen soil is not blowing due to the wind, conditions are much better for plant growth. Plants cannot grow in a soil that is constantly blowing. Their root systems do not have time to be established. Also, when soil particles are blowing they wear away at the plants that they run into. Plants are essentially “sand blasted.”\n\nDrought is very detrimental to all types of plant growth. When there is no water in the soil there are not very many nutrients to support plant growth. Drought also enhances the effects of wind. When drought occurs the soil becomes very dry and light. The wind picks up this dry dirt and carries it away. This action severely degrades the soil and creates a poor condition for growing plants.\n\nPlants have been exposed to the elements for thousands of years. During this time they have evolved in order to lessen the effects of abiotic stress. Signal transduction is the mechanism in plants that is responsible for the adaptation of plants (Xiong & Zhu, 2001). Many signaling transduction networks have been discovered and studied in microbial and animal systems. There is limited knowledge in the plant field because it is very difficult to find exactly which phenotypes in the plant are affected by stressors. These phenotypes are very valuable to the researchers. They need to know the phenotypes so that they can create a method to screen for mutant genes. Mutants are the key to finding signaling pathways in living creatures.\n\n<nowiki>Animals and microbes easier to run tests on because they show a reaction fairly quickly when a stress factor is put on them, this leads to the isolation of the specific gene. There have been decades of research on the effects of temperature, drought, and salinity, but not very many answers.</nowiki>\n\nThe part of the plants, animal, or microbe that first senses an abiotic stress factor is a receptor. Once a signal is picked up by a receptor, signals are transmitted intercellularly and then they activate nuclear transcription to get the effects of a certain set of genes. These activated genes allow the plant to respond to the stress that it is experiencing. Even though none of the receptors for cold, drought, salinity or the stress hormone abscisic acid in plants is known for sure, the knowledge that we have today shows that receptor-like protein kinases, two-component histidine kinases, as well as G-protein receptors may be the possible sensors of these different signals.\n\nReceptor like kinases can be found in plants as well as animals. There are many more RLKs in plants than there are in animals. They are also a little bit different. Unlike animal RLKs that usually possess tyrosine signature sequences, plant RLKs have serine or threonine signature sequences (Xiong & Zhu, 2001).\n\nPlants are most commonly modified to be resistant to specific herbicides or pathogens, but we have the technology to modify plants in order to make them resistant to specific abiotic stressors. Cold, heat, drought, or salt are all factors that could possibly be defended against by genetically modified plants.\n\nSome plants could have genes added to them from other species of plants that have a resistance to a specific stress. Plants implanted with these genes would then become transgenic plants because they have the genes from another species of plant in them. Scientists first have to isolate the specific gene in a plant that is responsible for its resistance. The gene would then be taken out of the plant and put into another plant. The plant that is injected with the new resistant gene would have a resistance to an abiotic stressor and be able to tolerate a wider range of conditions (Weil, 2005).\n\nThis process of creating transgenic plants could have a huge impact on our nation’s economy. If plants could be genetically engineered to be resistant to a wider variety of stress, crop yields would skyrocket. With the expansion of town and cities there is a decreasing number of farm acres. Although the farm acres are being built on, the number of people consuming agriculture products is going up. Ethanol is also responsible for using much more of the corn that is grown here in the U.S. The production of this fuel has put a strain on the corn market. Prices of corn have gone up and this price is having a negative impact on the people who feed animals using corn. The combination of reduced acres of farmland and a higher demand on crops have left producers and consumers in a severe dilemma. The only solution to this problem is to keep getting higher and higher yields from the cropland that we have left.\n\nGenetically modified plants are a good answer to the problem of not enough crops to go around. These plants can be engineered to be resistant to all types of abiotic stress. This would eliminate crop yield loss due to extreme temperatures, drought, wind, or salinity. The consumers of crops would enjoy a little bit lower prices because the demand on them would be a little lower.\n\nThe Midwestern U.S. is experiencing a severe drought. Farmers are being limited on how much they can irrigate due to the shortage of water. There is also very little rain during the growing season so the crops do not yield very well. This problem could be solved by genetically modifying plants to become more drought resistant. If plants could use less water and produce yields that are superior or equal to current ones, it would be better for the people and also the environment. People would enjoy an abundance of crops to consume and export for a profit. The environment would be able to have more water in its aquifers and rivers throughout the country.\n\nAnother environmental factor that would be improved would be the amount of land left for wildlife. Crops modified to be resistant to abiotic stress and other factors that decrease yields would require less land use. Producers would be able to grow enough crops on less acres if the plants were modified to produce very high yields. This would allow some of the cropland that is in use today to be set aside for wildlife. Instead of farming “fence line to fence line” farmers would be able to create large buffers in their fields. These buffers would provide a great habitat for plants and animals.\n\nA lot of people do not like genetically modified organisms. People opposed to these modified plants often claim that they are not safe for the environment or for human consumption. There are many videos and reports in circulation that discredit the safety of genetically modified organisms. King Corn is one video that claims that corn is bad for humans to consume.\n\nThere are strict regulations and protocols that go along with genetically modifying plants. A company that specializes in producing genetically modifying organisms must put their plants through a huge variety of tests to ensure the safety of their product. Each of these tests must be passed by the product in order to produce more of the plant seeds.\n\nWhen seeds are mass-produced, the fields that they are grown in have to meet specific criteria. They must have no vegetation zones around them to prevent the spread of the modified plants into the native population. The plots must be carefully labeled and marked so that the company knows exactly what is planted in the field. All of these protocols are in place to ensure the safety of the consumers and also of the environment.\nBecause genetically modified plants are given stress resistant genes or high yielding genes they are better for the environment. They only help create more land to be put back into natural habitats for plants and animals.\n\nAbiotic stress is a naturally occurring factor that cannot be controlled by humans. One example of two stressors that are complimentary to each other is wind and drought. Drought dries out the soil and kills the plants that are growing in the soil. After this occurs, the soil is left barren and dry. Wind can pick up the soil and carry for miles. Irrigation can keep this from happening, but it is not possible to irrigate some areas.\n\nGenetically modified plants can be implemented to slow down the effects of the abiotic stressors. This allows more crops to be grown on a smaller amount of land. Less need for farmland allows some of it to be set aside for natural wildlife habitat.\n\nAbiotic stress only poses a problem to people or the environment if they are not prepared for it. There can be steps taken by humans to lessen the effects. Plants and animals have the ability to adapt to abiotic stress over time.\n\n"}
{"id": "19835580", "url": "https://en.wikipedia.org/wiki?curid=19835580", "title": "Nice model", "text": "Nice model\n\nThe Nice () model is a scenario for the dynamical evolution of the Solar System. It is named for the location of the Observatoire de la Côte d'Azur, where it was initially developed, in 2005 in Nice, France. It proposes the migration of the giant planets from an initial compact configuration into their present positions, long after the dissipation of the initial protoplanetary disk. In this way, it differs from earlier models of the Solar System's formation. This planetary migration is used in dynamical simulations of the Solar System to explain historical events including the Late Heavy Bombardment of the inner Solar System, the formation of the Oort cloud, and the existence of populations of small Solar System bodies including the Kuiper belt, the Neptune and Jupiter trojans, and the numerous resonant trans-Neptunian objects dominated by Neptune. Its success at reproducing many of the observed features of the Solar System means that it is widely accepted as the current most realistic model of the Solar System's early evolution, although it is not universally favoured among planetary scientists. Later research revealed a number of differences between the original Nice model's predictions and observations of the current Solar System, for example the orbits of the terrestrial planets and the asteroids, leading to its modification.\n\nThe original core of the Nice model is a triplet of papers published in the general science journal \"Nature\" in 2005 by an international collaboration of scientists: Rodney Gomes, Hal Levison, Alessandro Morbidelli, and Kleomenis Tsiganis. In these publications, the four authors proposed that after the dissipation of the gas and dust of the primordial Solar System disk, the four giant planets (Jupiter, Saturn, Uranus, and Neptune) were originally found on near-circular orbits between ~5.5 and ~17 astronomical units (AU), much more closely spaced and compact than in the present. A large, dense disk of small rock and ice planetesimals, their total about 35 Earth masses, extended from the orbit of the outermost giant planet to some 35 AU.\n\nScientists understand so little about the formation of Uranus and Neptune that Levison states, \"...the possibilities concerning the formation of Uranus and Neptune are almost endless.\"\nHowever, it is suggested that this planetary system evolved in the following manner. Planetesimals at the disk's inner edge occasionally pass through gravitational encounters with the outermost giant planet, which change the planetesimals' orbits. The planets scatter the majority of the small icy bodies that they encounter inward, exchanging angular momentum with the scattered objects so that the planets move outwards in response, preserving the angular momentum of the system. These planetesimals then similarly scatter off the next planet they encounter, successively moving the orbits of Uranus, Neptune, and Saturn outwards. Despite the minute movement each exchange of momentum can produce, cumulatively these planetesimal encounters shift (migrate) the orbits of the planets by significant amounts. This process continues until the planetesimals interact with the innermost and most massive giant planet, Jupiter, whose immense gravity sends them into highly elliptical orbits or even ejects them outright from the Solar System. This, in contrast, causes Jupiter to move slightly inward.\n\nThe low rate of orbital encounters governs the rate at which planetesimals are lost from the disk, and the corresponding rate of migration. After several hundreds of millions of years of slow, gradual migration, Jupiter and Saturn, the two inmost giant planets, cross their mutual 1:2 mean-motion resonance. This resonance increases their orbital eccentricities, destabilizing the entire planetary system. The arrangement of the giant planets alters quickly and dramatically. Jupiter shifts Saturn out towards its present position, and this relocation causes mutual gravitational encounters between Saturn and the two ice giants, which propel Neptune and Uranus onto much more eccentric orbits. These ice giants then plough into the planetesimal disk, scattering tens of thousands of planetesimals from their formerly stable orbits in the outer Solar System. This disruption almost entirely scatters the primordial disk, removing 99% of its mass, a scenario which explains the modern-day absence of a dense trans-Neptunian population. Some of the planetesimals are thrown into the inner Solar System, producing a sudden influx of impacts on the terrestrial planets: the Late Heavy Bombardment.\n\nEventually, the giant planets reach their current orbital semi-major axes, and dynamical friction with the remaining planetesimal disc damps their eccentricities and makes the orbits of Uranus and Neptune circular again.\n\nIn some 50% of the initial models of Tsiganis and colleagues, Neptune and Uranus also exchange places. An exchange of Uranus and Neptune would be consistent with models of their formation in a disk that had a surface density that declined with distance from the Sun, which predicts that the masses of the planets should also decline with distance from the Sun.\n\nRunning dynamical models of the Solar System with different initial conditions for the simulated length of the history of the Solar System will produce the various populations of objects within the Solar System. As the initial conditions of the model are allowed to vary, each population will be more or less numerous, and will have particular orbital properties. Proving a model of the evolution of the early Solar System is difficult, since the evolution cannot be directly observed. However, the success of any dynamical model can be judged by comparing the population predictions from the simulations to astronomical observations of these populations. At the present time, computer models of the Solar System that are begun with the initial conditions of the Nice scenario best match many aspects of the observed Solar System.\n\nThe crater record on the Moon and on the terrestrial planets is part of the main evidence for the Late Heavy Bombardment (LHB): an intensification in the number of impactors, at about 600 million years after the Solar System's formation. In the Nice model icy planetesimals are scattered onto planet-crossing orbits when the outer disc is disrupted by Uranus and Neptune causing a sharp spike of impacts by icy objects. The migration of outer planets also causes mean-motion and secular resonances to sweep through the inner Solar System. In the asteroid belt these excite the eccentricities of the asteroids driving them onto orbits that intersect those of the terrestrial planets causing a more extended period of impacts by stony objects and removing roughly 90% of its mass. The number of planetesimals that would reach the Moon is consistent with the crater record from the LHB. However, the orbital distribution of the remaining asteroids does not match observations. In the outer Solar System the impacts onto Jupiter's moons are sufficient to trigger Ganymede's differentiation but not Callisto's. The impacts of icy planetesimals onto Saturn's inner moons are excessive, however, resulting in the vaporization of their ice.\n\nAfter Jupiter and Saturn cross the 2:1 resonance their combined gravitational influence destabilizes the Trojan co-orbital region allowing existing Trojan groups in the L and L Lagrange points of Jupiter and Neptune to escape and new objects from the outer planetesimal disk to be captured. Objects in the trojan co-orbital region undergo libration, drifting cyclically relative to the L and L points. When Jupiter and Saturn are near but not in resonance the location where Jupiter passes Saturn relative to their perihelia circulates slowly. If the period of this circulation falls into resonance with the period that the trojans librate the range of their librations can increase until they escape. When this occurs the trojan co-orbital region is \"dynamically open\" and objects can both escape and enter the region. Primordial trojans escape and a fraction of the numerous objects from the disrupted planetesimal disk temporarily inhabit it. Later when Jupiter and Saturn orbits are farther apart the Trojan region becomes \"dynamically closed\", and the planetesimals in the trojan region are captured, with many remaining today. The captured trojans have a wide range of inclinations, which had not previously been understood, due to their repeated encounters with the giant planets. The libration angle and eccentricity of the simulated population also matches observations of the orbits of the Jupiter trojans. This mechanism of the Nice model similarly generates the Neptune trojans.\n\nA large number of planetesimals would have also been captured in Jupiter's mean motion resonances as Jupiter migrated inward. Those that remained in a 3:2 resonance with Jupiter form the Hilda family. The eccentricity of other objects declined while they were in a resonance and escaped onto stable orbits in the outer asteroid belt, at distances greater than 2.6 AU as the resonances moved inward. These captured objects would then have undergone collisional erosion, grinding the population away into smaller fragments that can then be acted on by the Yarkovsky effect, causing small objects to drift into unstable resonances, and Poynting–Robertson drag causing smaller grains to drift toward the sun. These processes remove more than 90% of the origin mass implanted into the asteroid belt according to Bottke and colleagues. The size frequency distribution of this simulated population following this erosion are in excellent agreement with observations. This suggests that the Jupiter Trojans, Hildas, and some of the outer asteroid belt, all spectral D-type asteroids, are the remnant planetesimals from this capture and erosion process. It has also been suggested that the dwarf planet was captured via this process. A few D-type asteroids have been recently discovered with semi-major axes less than 2.5 AU, closer than those that would be captured in the original Nice model.\n\nAny original populations of irregular satellites captured by traditional mechanisms, such as drag or impacts from the accretion disks, would be lost during the encounters between the planets at the time of global system instability. In the Nice model, the outer planets encounter large numbers of planetesimals after Uranus and Neptune enter and disrupt the planetesimal disk. A fraction of these planetesimals are captured by these planets via three-way interactions during encounters between planets. The probability for any planetesimal to be captured by an ice giant is relatively high, a few 10. These new satellites could be captured at almost any angle, so unlike the regular satellites of Saturn, Uranus, and Neptune, they do not necessarily orbit in the planets' equatorial planes. Some irregulars may have even been exchanged between planets. The resulting irregular orbits match well with the observed populations' semimajor axes, inclinations, and eccentricities. Subsequent collisions between these captured satellites may have created the suspected collisional families seen today. These collisions are also required to erode the population to the present size distribution.\n\nTriton, the largest moon of Neptune, can be explained if it was captured in a three-body interaction involving the disruption of a binary planetoid. Such binary disruption would be more likely if Triton was the smaller member of the binary. However, Triton's capture would be more likely in the early Solar System when the gas disk would damp relative velocities, and binary exchange reactions would not in general have supplied the large number of small irregulars.\n\nThere were not enough interactions between Jupiter and the other planets to explain Jupiter's retinue of irregulars in the initial Nice model simulations that reproduced other aspects of the outer Solar System. This suggests either that a second mechanism was at work for that planet, or that the early simulations did not reproduce the evolution of the giant planets orbits.\n\nThe migration of the outer planets is also necessary to account for the existence and properties of the Solar System's outermost regions. Originally, the Kuiper belt was much denser and closer to the Sun, with an outer edge at approximately 30 AU. Its inner edge would have been just beyond the orbits of Uranus and Neptune, which were in turn far closer to the Sun when they formed (most likely in the range of 15–20 AU), and in opposite locations, with Uranus farther from the Sun than Neptune.\n\nGravitational encounters between the planets scatter Neptune outward into the planetesimal disk with a semi-major axis of ~28 AU and an eccentricity as high as 0.4. Neptune's high eccentricity causes its mean-motion resonances to overlap and orbits in the region between Neptune and its 2:1 mean motion resonances to become chaotic. The orbits of objects between Neptune and the edge of the planetesimal disk at this time can evolve outward onto stable low-eccentricity orbits within this region. When Neptune's eccentricity is damped by dynamical friction they become trapped on these orbits. These objects form a dynamically-cold belt, since their inclinations remain small during the short time they interact with Neptune. Later, as Neptune migrates outward on a low eccentricity orbit, objects that have been scattered outward are captured into its resonances and can have their eccentricities decline and their inclinations increase due to the Kozai mechanism, allowing them to escape onto stable higher-inclination orbits. Other objects remain captured in resonance, forming the plutinos and other resonant populations. These two population are dynamically hot, with higher inclinations and eccentricities; due to their being scattered outward and the longer period these objects interact with Neptune.\n\nThis evolution of Neptune's orbit produces both resonant and non-resonant populations, an outer edge at Neptune's 2:1 resonance, and a small mass relative to the original planetesimal disk. The excess of low-inclination plutinos in other models is avoided due to Neptune being scattered outward, leaving its 3:2 resonance beyond the original edge of the planetesimal disk. The differing initial locations, with the cold classical objects originating primarily from the outer disk, and capture processes, offer explanations for the bi-modal inclination distribution and its correlation with compositions. However, this evolution of Neptune's orbit fails to account for some of the characteristics of the orbital distribution. It predicts a greater average eccentricity in classical Kuiper belt object orbits than is observed (0.10–0.13 versus 0.07) and it does not produce enough higher-inclination objects. It also cannot explain the apparent complete absence of gray objects in the cold population, although it has been suggested that color differences arise in part from surface evolution processes rather than entirely from differences in primordial composition.\n\nThe shortage of the lowest-eccentricity objects predicted in the Nice model may indicate that the cold population formed in situ. In addition to their differing orbits the hot and cold populations have but differing colors. The cold population is markedly redder than the hot, suggesting it has a different composition and formed in a different region. The cold population also includes a large number of binary objects with loosely bound orbits that would be unlikely to survive close encounter with Neptune. If the cold population formed at its current location, preserving it would require that Neptune's eccentricity remained small, or that its perihelion precessed rapidly due to a strong interaction between it and Uranus.\n\nObjects scattered outward by Neptune onto orbits with semi-major axis greater than 50 AU can be captured in resonances forming the resonant population of the scattered disc, or if their eccentricities are reduced while in resonance they can escape from the resonance onto stable orbits in the scattered disc while Neptune is migrating. When Neptune's eccentricity is large its aphelion can reach well beyond its current orbit. Objects that attain perihelia close to or larger than Neptune's at this time can become detached from Neptune when its eccentricity is damped reducing its aphelion, leaving them on stable orbits in the scattered disc.\n\nObjects scattered outward by Uranus and Neptune onto larger orbits (roughly 5,000 AU) can have their perihelion raised by the galactic tide detaching them from the influence of the planets forming the inner Oort cloud with moderate inclinations. Others that reach even larger orbits can be perturbed by nearby stars forming the outer Oort cloud with isotropic inclinations. Objects scattered by Jupiter and Saturn are typically ejected from the Solar System Several percent of the initial planetesimal disc can be deposited in these reservoirs.\n\nThe Nice model has undergone a number of modifications since its initial publication as the understanding of the formation of the Solar System has advanced and significant differences between its predictions and observations have been identified. Hydrodynamical models of the early Solar System indicate that the orbits of the giant planets converge resulting in their capture into a series of resonances. The slow approach of Jupiter and Saturn to the 2:1 resonance during their later planetesimal-driven migration can allow Mars to captured in a secular resonance that excites its eccentricity to a level that destabilizes the inner Solar System. The eccentricities of the other terrestrial planets can also be excited beyond current levels by sweeping secular resonances after the instability. The orbital distribution of the asteroid belt is also left with an excess of high inclination objects due to secular resonances exciting inclinations and removing low inclination objects. Other differences between predictions and observations included the capture of few irregular satellites by Jupiter, the vaporization of the ice from Saturn's inner moons, a shortage of high inclination objects captured in the Kuiper belt, and the recent discovery of D-type asteroids in the inner asteroid belt.\n\nThe first modifications to the Nice model were the initial positions of the giant planets. Investigations of the behavior of planets orbiting in a gas disk using hydrodynamical models reveal that the giant planets would migrate toward the Sun. If the migration continued it would have resulted in Jupiter orbiting close to the Sun like recently discovered exoplanets known as hot Jupiters. Saturn's capture in a resonance with Jupiter prevents this, however, and the later capture of the other planets results in a quadruple resonant configuration with Jupiter and Saturn in their 3:2 resonance. A late instability beginning from this configuration is possible if the outer disk contains Pluto-massed objects. The gravitational stirring of the outer planetesimal disk by these Pluto-massed objects increases their eccentricities and also results in the inward migration of the giant planets. The quadruple resonance of the giant planets is broken when secular resonances are crossed during the inward migration. A late instability similar to the original Nice model then follows. Unlike the original Nice model the timing of this instability is not sensitive to the distance between the outer planet and the planetesimal disk. The combination of resonant planetary orbits and the late instability triggered by these long distant interactions has been referred to as the Nice 2 model.\n\nThe second modification was the requirement that one of the ice giants encounters Jupiter, causing its semi-major axis to jump. In this jumping-Jupiter scenario an ice giant encounters Saturn and is scattered inward onto a Jupiter-crossing orbit, causing Saturn's orbit to expand; then encounters Jupiter and is scattered outward, causing Jupiter's orbit to shrink. This results in a step-wise separation of Jupiter's and Saturn's orbits instead of a smooth divergent migration. The step-wise separation of the orbits of Jupiter and Saturn avoids the slow sweeping of secular resonances across the inner solar System that resulted in the excitation of the eccentricities of the terrestrial planets and an asteroid belt with an excessive ratio of high- to low-inclination objects The encounters between the ice giant and Jupiter in this model allow Jupiter to acquire its own irregular satellites. Jupiter trojans are also captured following these encounters when Jupiter's semi-major axis jumps and, if the ice giant passes through one of the libration points scattering trojans, one population is depleted relative to the other. The faster traverse of the secular resonances across the asteroid belt limits the loss of asteroids from its core. Most of the rocky impactors of the Late Heavy Bombardment instead originate from an inner extension that is disrupted when the giant planets reach their current positions, with a remnant remaining as the Hungaria asteroids. Some D-type asteroids are embedded in inner asteroid belt, within 2.5 AU, during encounters with the ice giant when it is crossing the asteroid belt.\n\nThe frequent ejection of the ice giant encountering Jupiter has led David Nesvorný and others to hypothesize an early Solar System with five giant planets, one of which was ejected during the instability. This five-planet Nice model begins with the giant planets in a 3:2, 3:2, 2:1, 3:2 resonant chain with a planetesimal disk orbiting beyond them. Following the breaking of the resonant chain Neptune first migrates outward into the planetesimal disk reaching 28 AU before encounters between planets begin. This initial migration reduces the mass of the outer disk enabling Jupiter's eccentricity to be preserved and produces a Kuiper belt with an inclination distribution that matches observations if 20 Earth-masses remained in the planetesimal disk when that migration began. Neptune's eccentricity can remain small during the instability since it only encounters the ejected ice giant, allowing an in situ cold-classical belt to be preserved. The lower mass planetesimal belt in combination with the excitation of inclinations and eccentricities by the Pluto-massed objects also significantly reduce the loss of ice by Saturn's inner moons. The combination of a late breaking of the resonance chain and a migration of Neptune to 28 AU before the instability is unlikely with the Nice 2 model. This gap may be bridged by a slow dust-driven migration over several million years following an early escape from resonance.\nA recent study found that the five-planet Nice model has a statistically small likelihood of reproducing the orbits of the terrestrial planets. Although this implies that the instability occurred before the formation of the terrestrial planets and could not be the source of the Late Heavy Bombardment, the advantage of an early instability is reduced by the sizable jumps in the semi-major axis of Jupiter and Saturn required to preserve the asteroid belt.\n\n\n"}
{"id": "12101011", "url": "https://en.wikipedia.org/wiki?curid=12101011", "title": "Nunatak (band)", "text": "Nunatak (band)\n\nNunatak was the British Antarctic Survey’s (BAS) Rothera Research Station’s house band. The five person indie rock band was part of a science team investigating climate change and evolutionary biology on the Antarctic Peninsula. They are chiefly known for their participation in Live Earth in 2007, where they were the only band to play in the event's Antarctica concert.\n\nThe band's name is the Greenlandic word for a mountain top protruding from an ice sheet. Originally, the band had named itself after a disease previously common to Punta Arenas roughly translated to \"Rat Shit Death\" but felt that the pronunciation of that name was less than politically correct.\n\nThe band disbanded in 2007, as its members returned to the United Kingdom, but they reunited to perform in the Sanday, Orkney Soulka festival in 2012.\n\nNunatak played the Live Earth Antarctica concert on July 7, 2007, to a \"sell out\" crowd of seventeen, the entire population of the Rothera Research Station. Their participation fulfilled the event's promise to hold a concert on all seven continents. Lead singer Matt Balmer, 22, said of the event that the band \"expected to spend our Antarctic winter here at Rothera quietly getting on with our work and maybe performing at the occasional Saturday night party. We could never have imagined taking part in a global concert.\"\n\nIn the buildup to the event, Director of BAS Professor Chris Rapley said:\n\nBritish Antarctic Survey cameraman Pete Bucktrout made videos of the band's performances that were transmitted back from Antarctica for later inclusion in Live Earth. The videos are available on the BAS website and the BAS YouTube channel.\n\n\n"}
{"id": "35681165", "url": "https://en.wikipedia.org/wiki?curid=35681165", "title": "Orlando Mendes", "text": "Orlando Mendes\n\nOrlando Marques de Almeida Mendes (Island of Mozambique, August 4, 1916 – Maputo, January 11, 1990) was a Mozambican biologist and writer.\n\nHe lived the Portuguese decolonisation of Mozambique. In 1944, he moved with his wife and daughter to Coimbra, where he studied biology at the University of Coimbra.\n\nHe worked as a biologist in Lourenço Marques and wrote for several publications such as: \"Tempo\", \"Itinerário\", \"Vértice\" and \"África\". In spite of being European, he strongly criticized colonial treatment towards black people and Salazar's administration. During the Portuguese Colonial War, he was with FRELIMO nationalist party.\n\n\n\n"}
{"id": "2902830", "url": "https://en.wikipedia.org/wiki?curid=2902830", "title": "Overlapping consensus", "text": "Overlapping consensus\n\nOverlapping consensus is a term coined by John Rawls in \"A Theory of Justice\" and developed in \"Political Liberalism\".\n\nThe term \"overlapping consensus\" refers to how supporters of different comprehensive normative doctrines—that entail apparently inconsistent conceptions of justice—can agree on particular principles of justice that underwrite a political community's basic social institutions. Comprehensive doctrines can include systems of religion, political ideology, or morality. \n\nRawls explains that an overlapping consensus on principles of justice can occur despite \"considerable differences in citizens' conceptions of justice provided that these conceptions lead to similar political judgements.\" The groups are able to achieve this consensus in part by refraining from political/public disputes over fundamental (e.g. metaphysical) arguments regarding religion and philosophy. Rawls elaborates that the existence of an overlapping consensus on conceptions of justice among major social groups holding differing—yet reasonable—comprehensive doctrines is a necessary and distinctive characteristic of political liberalism. Rawls also explains that the overlapping consensus on principles of justice is itself a moral conception and is supported by moral reasoning—although the fundamental grounds of this support may differ for each of the various groups holding disparate comprehensive doctrines, and these lines of reasoning may also differ from the public reasons provided for supporting the principles. These latter features distinguish his idea of an overlapping consensus from a mere modus vivendi, which is a strategic agreement entered into for pragmatic purposes, and therefore potentially unprincipled and unstable. The overlapping consensus could in sum be said to “depend, in effect, on there being a morally significant core of commitments common to the ‘reasonable’ fragment of each of the main comprehensive doctrines in the community”\n\nIt has been argued that reasonable forms of religious and moral public education may be agreed by considering which common values and principles may be determined through overlapping consensus between those of otherwise incommensurable comprehensive doctrines (e.g. those of a given religion and secularists) .\n\n\n"}
{"id": "55502054", "url": "https://en.wikipedia.org/wiki?curid=55502054", "title": "Parahydrogen induced polarization", "text": "Parahydrogen induced polarization\n\nParahydrogen Induced Polarization (PHIP) is a technique used in magnetic resonance imaging. The technique relies on the incorporation of hyperpolarized H into molecules, usually by hydrogenation.\n"}
{"id": "8543156", "url": "https://en.wikipedia.org/wiki?curid=8543156", "title": "Public engagement", "text": "Public engagement\n\nPublic engagement is a term that has recently been used, particularly in the UK, to describe \"the involvement of specialists listening to, developing their understanding of, and interacting with, non-specialists\" (as defined by England's university funding agency, HEFCE, in 2006).\n\nThe tradition of a decision-making body getting inputs from those with less power is generally known as \"consultation\". This became popular with UK governments during the 1980s and 1990s. Even though most governments that carry out consultations are democratically elected, many people who became involved in these processes were surprised that conduct of such \"consultations\" was unsatisfactory in at least three respects.\n\n\nAs early as 1979, science analyst Dorothy Nelkin pointed out that much of what passed for participation in governance could best be understood as attempts by the powerful to co-opt the public.\n\nPublic engagement is a relatively new term, hardly used before the late 1990s. The existing term it shares most in common with is participatory democracy, discussed by thinkers such as Jean-Jacques Rousseau, John Stuart Mill and G D H Cole.\n\nMany see participatory democracy as complementing representative democratic systems, in that it puts decision-making powers more directly in the hands of ordinary people. Rousseau suggested that participatory approaches to democracy had the advantage of demonstrating that \"no citizen is a master of another\" and that, in society, \"all of us are equally dependent on our fellow citizens\". Rousseau suggested that participation in decision – making increases feeling among individual citizens that they belong in their community. Perhaps the most long-standing institution of participatory democracy is the system of trial by jury.\n\nWhilst elected governments make the laws, it is therefore juries that are able to decide the innocence or guilt of anyone charged with breaking many of those laws, making it a key instrument of participatory democracy. Over the centuries they have achieved an importance to many democracies that have had to be fiercely defended. One senior judge surveying the limiting of a government's power provided by the jury over the centuries compared the jury to: \"a little parliament... No tyrant could afford to leave a subject's freedom in the hands of twelve of his countrymen... Trial by jury is more than an instrument of justice and more than one wheel of the constitution: it is the lamp that shows that freedom lives\". (Patrick Devlin 1956). Today, jury trials are practised in the UK, US, and many other democracies around the world including Russia, Spain, Brazil and Australia. Perhaps no other institution of government rivals the jury in placing power so directly in the hands of citizens, or wagers more on the truth of democracy's core claim that the people make their own best governors. Juries are therefore argued to be the most widespread form of genuine consultation at work in society today.\n\nThe tension between the state and civil society as underscored by Public Engagement within Newly Industrialized Economies (NIE) such as Singapore is illustrated by Kenneth Paul Tan of the Lee Kuan Yew School of Public Policy\n\n\"But speaking about public engagement is, of course, quite a different thing from carrying out public engagement. And this is where there seems to be a gap between rhetoric and practice in Singapore. For instance, government officials recently met selectively with concerned members of the public to discuss a controversial decision to build a road through a historically significant graveyard. When criticised for not taking the public's views seriously, the Government explained that the meeting was never meant to be a \"consultation\". So it is important to ask why such a gap exists and why it might be difficult to close it, assuming of course that closing it is what we want to do.\"\n\n\"As a neo-liberal global city, Singapore has been witnessing rising popular pressure. Politics has come to the fore again, prompting the policy establishment to pay greater heed to the demands of a new and more variegated citizenry, with political leaders now more sensitive to the real prospect of losing elections. At the same time, the cultural, ideological, practical and institutional legacies of the earlier survivalist and development stages continue to be a source of tension in the evolution of Singapore's political culture. By no means has this been a simple and linear story of liberalisation.\nHowever, are these recent developments enough to shift the deeply entrenched public sector mindsets that have been formed out of historically shaped ways of thinking and reasoning? Will a new generation of leaders in the public sector, whose horizons of experience may differ from the survivalist and developmental preoccupations of a previous generation, lead to fresh opportunities for new terms of engagement?\"\n\n\"The elitist proclivities of the public sector, reinforced by top-level salaries that are comparable to the private sector, are unlikely to incentivise real public engagement, since they reinforce the sense that public sector leaders, possessing superior intellect, knowledge and insight, must defend the public interest against irrational and dangerous mass populism. The public, according to this mindset, needs to be educated to think correctly rather than present themselves as equal participants in policy formulation and implementation.\n\nTaking participatory democracy as an ideal for public engagement has significant consequences for how we apply the concept to issues with a scientific or technical element. Instead of merely receiving inputs from various interested parties, a participatory model of consultation forces decision-makers to recognise the democratic accountability of their actions not merely every few years at elections, but in a more systematic, direct sense to citizens.\n\nA common misconception is that there is a particular \"methodology\" that can be devised to facilitate all public engagement. Effective participation, by contrast, is conducted on the assumption that each different situation will require a different design, using a new combination of tools as part of an evolving cycle of action and reflection by the institution involved.\n\nEach \"experiment\" in participatory democracy contains a unique mix of people and institutions. Each method must therefore select elements from a range of different approaches. Participation is also overtly \"political\" in that it is about humans, power and knowledge – all of which are inherently complex and which together make for a potent mix that requires sensitivity and careful planning. So while participatory processes can be replicated in the same way as scientific protocols, their human ingredients can differ so much that a concentration on replicating what happened elsewhere often hinders the practical application of a technique.\n\nBefore describing a scientific experiment, scientists know that it is vital to explain the context in which that experiment takes place. Was the plant in a test tube or in a farmer's field? Was the rat well fed or starving? This logic also applies in the case of a participatory process, in which the each consultation event is analogous to an experimental subject. Each needs to proceed from an understanding of its political, scientific, institutional and practical constraints.\n\nSo instead of recommending a perfect method of public engagement, Table 1 summarises some working principles for such processes, based on those used by PEALS at Newcastle University.\n\n\nThe movement for public engagement in science and technology grows out of a paradox: the steadily increasing number of ways citizens can learn about science has not always been matched by any increased level of scientific knowledge or sophistication among citizens. There are nearly one hundred science and technology museums in North America alone, numerous science blogs (the aggregation site, ScienceBlogs, reported 152 thousand posts and 3.3 million comments for 61 blogs alone before it closed in October 2017), and a proliferating number of science magazines. \n\nHowever, surveys of scientific literacy show a long term pattern in which Americans have only a moderate understanding of basic scientific facts and concepts. In 1992, only 59% of adults sampled could give correct answers to a series of scientific terms and concepts; in 2008 the number was 64%. However, in 2010 the presentation of these same measures of scientific literacy became controversial. Americans performed much worse on questions about evolution and the big bang theory than respondents from different countries. These differences disappeared when short caveats like, \"According to the theory of Evolution...\" were added to the questions – pointing to a larger conflict between scientific knowledge and personal beliefs in the U.S. Another survey found widening gaps in knowledge of nanotechnology between the most and least educated. Knowledge gaps also exist between different levels of education different media use. These gaps between education levels and knowledge make public engagement with science additionally complex.\n\nTo address this disconnect and complexity, there have been calls for new ways of connecting citizens with science in hopes that citizens can do more than respond passively to choices made by experts, and instead actually contribute to shaping science policy as it is made. This engagement of different publics in the policymaking process happens through the flow of information between the relevant publics and the sponsor of the engagement (e.g., policy makers, experts, scientists). Mechanisms for public engagement generally fall within one of the following types of information flow; public communication, public consultation, and public participation. \n\nPublic communication is characterized by the one-way transfer of information or resources to the public. This includes mechanisms like information broadcasts and static website resources. Traditional media functions in this way by influencing the public agenda, termed agenda-setting theory. Public consultation is the collection of information about or from the public by the sponsor. Potential mechanisms are opinion polls, referendum, surveys, focus groups, or interactive websites. These mechanisms gather information from the public to shape what sponsors focus on or invest their resources into. Public consultation is also a one-way flow of information, but in the reverse direction of public communication.\n\nPublic participation is the two-way dialogue between the public and the sponsor. This two-way dialogue can be either short-term, with brief exchanges of information, or long-term, lasting decades. Mechanisms for public participation include action planning workshops, citizens' jury, consensus conferences, and task forces. \n\nSocial media has become an increasingly prominent mechanism of public participation as well. Social allows for instant and on-going dialogue between sponsors and the public. Additionally, social is flexible and can be implemented in a variety of ways, but not all social media platforms function identically. For example, three major health organizations implemented social media campaigns during the Ebola epidemic of 2013: Centers for Disease Control and Prevention (CDC), World Health Organization (WHO), and Médecins Sans Frontières (MSF, also known as Doctors without Borders). All three organizations used both Twitter and Instagram to communicate with the public, but the public engaged with Instagram posts more often than Twitter. This finding is consistent with Visual communication theory. Additionally, each organization fostered a different level of engagement with the public (MSF garnered the highest engagement).\n\nDeliberative democracy\n\nDeliberative democracy is also a mechanism for public participation with potential implications for public engagement with science. It provides a structure for public participation about pending policy developments via public hearings, the mainstream media and the internet, consulting with different interest groups. This way, policy in the making is informed by the knowledge and experiences of those who will be affected by it, works to engage the public before final decisions are made, and sometimes gives affected groups a share of power in policy developments.\n\nStages 1 and 2 can use tools like public opinion surveys, media campaigns and public hearings. Stages 3 and 4 involve public or online deliberation or multi-stakeholder negotiation aimed at consensus building. Government decision makers at the agencies involved are expected to know their objectives and rationales, as well as key challenges to engaging the public.\n\nIn such situations, agencies must be on guard to see that all the important views are represented without raising expectations so high that all participants think their views will automatically be adopted as policy. Moreover, evaluation to ensure the effectiveness of public engagement is also important.\n\nKey examples of public engagement include:\n\n\nPublic engagement with science was formally called for in the Third Report of the UK House of Lords Committee on Science and Technology, which argued that \"public confidence in science and policy based on science has been eroded in recent year...there is a new humility on the part of science in the face of public attitudes, and a new assertiveness on the part of the public.\" One consultation, on the regulation of biotechnology in 1998, involved six two-day workshops as well as a large-scale survey. Asked who should be involved in regulating biotechnology, between 40 and 50 percent of respondents said regulatory groups should include a mixed advisory body, an expert body, scientists themselves, the general public, government, and environmental groups. One advisor to the Office of Science and Technology said the process was time-consuming and expensive, and workshops were open to the charge of being run by their organizers rather than their participants, but he still felt the participants dealt with the issues and came to understand them.\n\nHowever, there are important obstacles facing public engagement strategies. Selective exposure theory postulates that individuals favor information that agrees with their beliefs over information that contradicts those beliefs. Experimental evidence supports this theory. Individuals also form a filter bubble of like-minded people who hold a similar belief structure, decreasing the amount of dissonant information they are likely to encounter. Sponsors and communicators need to address or counter these obstacles when trying to engage with the public.\n\nThe following intrinsic and extrinsic constraints of public meetings can lead to unexpected a misrepresentation of the overall public's opinions:\n\n1. Attendance in public meetings is low and highly selective\n\nAlthough citizens express their intention to participate in public engagement activities, in real world, they are less likely to show up. For example, the average turnout at annual town meetings in Massachusetts in 1996 was 7.6 percent which was much lower than the average municipal election turnout of 31.1 percent. Low turnout rate in public meetings can lead serious sampling biases when attendees and non-attendees significantly differ in their interests. For example, attendees can be more interested in politics and involved in more personal discourses than non-attendees. In this case, their opinions can be slanted to one side.\n\n2. Group dynamics and personality traits of participants\n\nDepending the makeup of participants, group dynamics and personality characteristics of participants can considerably affect the outcomes of discussions. A small number of outspoken participants can make more than half of the comments during the discussions while least outspoken members make a very small portion of the comments.\n\n3. Moderated/controlled settings of public meetings\nIn order to minimize the potential effects of participants' demographic and cognitive characteristics on conversations, public meetings or consensus conferences tend to be carefully moderated and guided by facilitators. In such artificial setting, participants may behave in different ways that may differ from what is likely to occur in real-world discussions.\n\n4. Spillovers from public meetings to real-world discussion\n\nThe social implication effect of follow-up media coverage of public meetings or other engaging events may help transfer issues from these small group discussions to the broader community. However, in the case of the U.S., a spillover effect from public meetings into media discourse are minimal at best.\n\n5. Knowledge gap issues\n\nPublic meetings and consensus conferences may create knowledge gaps between high SES and less SES participants. The demographic, prepositional and cognitive differences between two groups in public meeting may lead to differing outcomes of public engagement. For example, highly educated participants may learn more from discussions and dominate the conversation while less educated members listen to their arguments. Furthermore, only small proportions of the population who may be already informed attend public meetings while the majority of the population who may need information the most do not. In such case, any public engagement effort may widen existing gaps further.\n\nFor examples of public engagement, see also:\n"}
{"id": "41968585", "url": "https://en.wikipedia.org/wiki?curid=41968585", "title": "RDS-3", "text": "RDS-3\n\nRDS-3 was the third atomic bomb developed by the Soviet Union in 1951, after the famous RDS-1 and RDS-2. It was called \"Marya\" in the military. The bomb had a composite design with a plutonium core inside a uranium shell, providing an explosive power of 41.2 kilotons. The RDS-3T was a modernized version and the first mass-produced nuclear weapon by the Soviet Union. It was assigned to Long Range Aviation in 1953.\n\nRDS-3 was tested on October 18, 1951, being air-dropped. It was the first such test of a nuclear device by the Soviets, known as Joe-3 in the West. It was detonated at an altitude of four-hundred meters.\n\n"}
{"id": "21108304", "url": "https://en.wikipedia.org/wiki?curid=21108304", "title": "ROSE (compiler framework)", "text": "ROSE (compiler framework)\n\nThe ROSE compiler framework, developed at Lawrence Livermore National Laboratory (LLNL), is an open-source software compiler infrastructure to generate source-to-source analyzers and translators for multiple source languages including C (C89, C98, Unified Parallel C (UPC)), C++ (C++98, C++11), Fortran (77, 95, 2003), OpenMP, Java, Python, and PHP.\n\nIt also supports certain binary files, and auto-parallelizing compilers by generating source code annotated with OpenMP directives. Unlike most other research compilers, ROSE is aimed at enabling non-experts to leverage compiler technologies to build their own custom software analyzers and optimizers.\n\nROSE consists of multiple front-ends, a midend operating on its internal intermediate representation (IR), and backends regenerating (unparse) source code from IR. Optionally, vendor compilers can be used to compile the unparsed source code into final executables.\n\nTo parse C and C++ applications, ROSE uses the Edison Design Group's C++ front-end. Fortran support, including F2003 and earlier 1977, 1990, and 1995 versions, is based on the Open Fortran Parser (OFP) developed at Los Alamos National Laboratory.\n\nThe ROSE IR consists of an abstract syntax tree, symbol tables, control flow graph, etc. It is an object-oriented IR with several levels of interfaces for quickly building source-to-source translators. All information from the input source code is carefully preserved in the ROSE IR, including C preprocessor control structure, source comments, source position information, and C++ template information, e.g., template arguments.\n\nROSE is released under a BSD-style license. It targets Linux and OS X on both IA-32 and x86-64 platforms. Its Edison Design Group (EDG) parts are proprietary and distributed in binary form. Source files of the EDG parts can be obtained if users have a commercial or research license from EDG.\n\nThe ROSE compiler infrastructure received one of the 2009 R&D 100 Awards. The R&D 100 Awards are presented annually by \"R&D Magazine\" to recognize the 100 most significant proven research and development advances introduced over the past year. An independent expert panel selects the winners.\n\n\n"}
{"id": "19742887", "url": "https://en.wikipedia.org/wiki?curid=19742887", "title": "Research center", "text": "Research center\n\nA research center (also known as research group) is a facility or building dedicated to research, commonly with the focus on a specific area. There are over 14,000 research centers in the United States. Centers apply varied disciplines including basic research and applied research in addition to non traditional techniques. However, a research center should not be confused with a research institute. Additionally, today many universities are establishing research centers to conduct a specific research or education activity. Over a hundred of research centers can be established in one university. This number certainly differs from a university to a university, but most of the research centers there do bring something to the scientific table.\n\n"}
{"id": "29998932", "url": "https://en.wikipedia.org/wiki?curid=29998932", "title": "Smart Village, Egypt", "text": "Smart Village, Egypt\n\nSmart Village () is a high-technology business district in the city of 6th of October, Egypt, established by Presidential Decree no.355 in 2000, with activities starting in 2001. It is located on the Cairo-Alexandria Desert Road, slightly west of Cairo. Smart Village occupies an area of 450 feddans. It's a business district with office buildings, retail shops, entertainment, factories and green spaces.\n\nHosni Mubarak signed the decree and provided the land for the building of Smart Village, in order for Egypt to build its IT economy and IT industry. Mubarak worked to get Microsoft, IBM and Cisco on-board. The 2001 decree gave companies in the Smart Village a ten-year tax break. The plan was to create several \"smart villages\" in Egypt.\n\nThe Smart Village contains government and ministry buildings, and many private companies. The Ministry of Communications and Information Technology is located here. In 2012, the Egyptian Stock Exchange moved its administrative functions to Smart Village and has one of the largest buildings in the district, with a skyway connecting the two. In 2012, Research In Motion, makers of Blackberry moved their offices here and looked to hire Egyptian engineers. The Bibliotheca Alexandrina opened an Egyptology research centre in Smart Villages, in May of 2018 and named it the Hawass Saqqara Training Centre. In September 2018, Raya announced it would lease a large space in Smart Villages. Raya specializes in business process outsourcing. Xceed, a subsidiary of Telecom Egypt has operations in the 6th of October Smart Village.\n\n\n"}
{"id": "13128936", "url": "https://en.wikipedia.org/wiki?curid=13128936", "title": "Stanko Karaman", "text": "Stanko Karaman\n\nStanko Luka Karaman (8 December 1889 – 17 May 1959) was a Yugoslav biologist of Bosnian Serb ancestry, researcher on amphipod and isopod crustaceans.\n\nIn 1926 he founded the Museum of South Serbia (later - Macedonian Museum of Natural History) in Skopje and in 1928, the Zoological Garden of Skopje.\n\nSeveral species are named after him, for example \"Delamarella karamani\" (Harpacticoida), \"Stygophalangium karamani\" (Arachnida), or \"Macedonethes stankoi\" (Isopoda).\n\nOther taxa named \"karamani\" are labeled after his son Gordan S. Karaman, also a carcinologist.\n\n\n"}
{"id": "11009430", "url": "https://en.wikipedia.org/wiki?curid=11009430", "title": "Systems simulation", "text": "Systems simulation\n\nSystems simulation is a set of techniques that use computers to imitate the operations of various real-world tasks or processes through simulation. Computers are used to generate numeric models for the purpose of describing or displaying complex interaction among multiple variables within a system. The complexity of the system arises from the stochastic (probabilistic) nature of the events, rules for the interaction of the elements and the difficulty in perceiving the behavior of the systems as a whole with the passing of time.\n\nOne of the most notable video games to incorporate systems simulation is Sim City, which simulates the multiple systems of a functioning city including but not limited to: electricity, water, sewage, public transportation, population growth, social interactions (including, but not limited to jobs, education and emergency response).\n\n\n"}
{"id": "4523848", "url": "https://en.wikipedia.org/wiki?curid=4523848", "title": "The Status Civilization", "text": "The Status Civilization\n\nThe Status Civilization is a science fiction novel by American writer Robert Sheckley, first published in 1960.\n\n\"The Status Civilization\" concerns Will Barrent, a man who finds himself, without memory of any crime or, indeed, of his previous life, being shipped across space to the planet Omega.\n\nOmega, used to imprison extreme offenders, has a hierarchical society of extreme brutality, where the only way to advance (and avoid dying) is to commit an endless series of crimes. The average life expectancy from time of arrival on Omega is three years. The story concerns Barrent's attempt to survive, escape, and return to Earth to clear himself of the accusations against him.\n\nEarth is a uniform, weakly structured, utopian society based on the mutual trust and conformity of its citizens. It is sleepy and stagnant, developing neither socially nor technologically. Its striking social stability is maintained by robots brainwashing children in \"closed classes.\" The ideologies of both Earth and Omega resemble one another, differing only in words. On Omega, the citizens worship Evil (always capitalized) in a cult dedicated to an entity called The Black One. On Earth, the world religion is an amalgam of all the \"good\" aspects of previous Earth religions. Its institution is the Church of the Spirit of Mankind Incarnate.\n\nAs Barrent comes closer to the truth about the reasons for his incarceration, his Omegan consciousness conflicts with his subconsciousness which was programmed in the closed classes by the robots when he was a child. The subsequent psychological struggle is played out by repeating all of the previous fights and battles which Barrent experienced throughout the book, eventually making clear the vision (or \"skrenning\") which the mutant girl on Omega foresaw of Barrent's death.\n\n"}
{"id": "58658728", "url": "https://en.wikipedia.org/wiki?curid=58658728", "title": "Vincenzo Barone", "text": "Vincenzo Barone\n\nVincenzo Barone (b. 8 November 1952, Ancona) is an Italian chemist; he is active in the field of theoretical and computational chemistry; full professor of physical chemistry (University of Naples, 1994), professor of theoretical and computational chemistry at the Scuola Normale Superiore di Pisa (SNS, 2009); since 2016 he is a director of SNS. From 2011 to 2013 he was a chairperson of the Italian Chemical Society (SCI); he is also a member of the International Academy of Quantum Molecular Science (IAQMS), the European Academy of Sciences, and a fellow of the Royal Society of Chemistry (RSC).\n\n"}
{"id": "10386451", "url": "https://en.wikipedia.org/wiki?curid=10386451", "title": "Zelinsky Model", "text": "Zelinsky Model\n\nThe \"Zelinsky Model of Migration Transition\", also known as the Migration Transition Model, claims that the type of migration that occurs within a country depends on how developed it is or what type of society it is. A connection is drawn from migration to the stages of within the Demographic Transition Model (DTM). It was developed by Wilbur Zelinsky (1921–2013), longtime professor of geography at the Pennsylvania State University.\n\nStage one (“Premodern traditional society”): This is before the onset of the\nurbanization, and it is very little to no migration and natural increase rates are about zero. There are very high levels of mobility (nomadism), but very little migration.\n\nStage two (“Early transitional society”): During stage two a “massive movement from countryside to cities\" occurs. And as a \"community experiences the process of modernization”.\nThere is a “rapid rate of natural increase”. And Internationally there is a high rate of emigration, although the total population number is still rising.\n\nStage three (“Late transitional society”): Stage three corresponds to the “critical rung...of the mobility transition” where urban-to-urban migration surpasses the rural-to-urban migration, where rural-to-urban migration “continues but at waning absolute or relative rates”, and “a complex migrational and circular movements within the urban network, from city to city or within a single metropolitan region”increased, circulation and non-economic migration starts to emerge. Then the net-out migration trend shifts to a net-in migration trend as more people immigrate than emigrate. That is, more people move in rather than out.\n\nStage four (“Advanced society”): During stage four the “movement from countryside to city continues but is further reduced in absolute and relative terms, vigorous movement of\nmigrants from city to city and within individual urban agglomerations...especially\nwithin a highly elaborated lattice of major and minor metropolises” is observed. A large increase of urban to suburban migration can also occur.\nThere is a “slight to moderate rate of natural increase or none at all”.\n\nStage five (“Future super advanced society”): During stage five “Nearly all residential migration may be of the interurban and interurban variety….No plausible predictions of fertility\nbehavior...a stable mortality pattern slightly below present levels”.\n\n"}
