{"id": "1949268", "url": "https://en.wikipedia.org/wiki?curid=1949268", "title": "Aether (classical element)", "text": "Aether (classical element)\n\nAccording to ancient and medieval science, aether (, \"aither\"), also spelled æther or ether and also called quintessence, is the material that fills the region of the universe above the terrestrial sphere. The concept of aether was used in several theories to explain several natural phenomena, such as the traveling of light and gravity. In the late 19th century, physicists postulated that aether permeated all throughout space, providing a medium through which light could travel in a vacuum, but evidence for the presence of such a medium was not found in the Michelson–Morley experiment, and this result has been interpreted as meaning that no such luminiferous aether exists.\n\nThe word (\"aithēr\") in Homeric Greek means \"pure, fresh air\" or \"clear sky\". In Greek mythology, it was thought to be the pure essence that the gods breathed, filling the space where they lived, analogous to the \"air\" breathed by mortals. It is also personified as a deity, Aether, the son of Erebus and Nyx in traditional Greek mythology. Aether is related to \"to incinerate\", and intransitive \"to burn, to shine\" (related is the name \"Aithiopes\" (Ethiopians; see Aethiopia), meaning \"people with a burnt (black) visage\").\n\nIn Plato's \"Timaeus\" (58d) speaking about air, Plato mentions that \"there is the most translucent kind which is called by the name of aether (αίθηρ)\". but otherwise he adopted the classical system of four elements. Aristotle, who had been Plato's student at the Akademia, agreed on this point with his former mentor, emphasizing additionally that fire sometimes has been mistaken for aether. However, in his Book \"On the Heavens\" he introduced a new \"first\" element to the system of the classical elements of Ionian philosophy. He noted that the four terrestrial classical elements were subject to change and naturally moved linearly. The first element however, located in the celestial regions and heavenly bodies, moved circularly and had none of the qualities the terrestrial classical elements had. It was neither hot nor cold, neither wet nor dry. With this addition the system of elements was extended to five and later commentators started referring to the new first one as the fifth and also called it \"aether\", a word that Aristotle had not used.\n\nAether did not follow Aristotelian physics either. Aether was also incapable of motion of quality or motion of quantity. Aether was only capable of local motion. Aether naturally moved in circles, and had no contrary, or unnatural, motion. Aristotle also noted that crystalline spheres made of aether held the celestial bodies. The idea of crystalline spheres and natural circular motion of aether led to Aristotle's explanation of the observed orbits of stars and planets in perfectly circular motion in crystalline aether.\n\nMedieval scholastic philosophers granted \"aether\" changes of density, in which the bodies of the planets were considered to be more dense than the medium which filled the rest of the universe. Robert Fludd stated that the aether was of the character that it was \"subtler than light\". Fludd cites the 3rd-century view of Plotinus, concerning the aether as penetrative and non-material. See also Arche.\n\nQuintessence is the Latinate name of the fifth element used by medieval alchemists for a medium similar or identical to that thought to make up the heavenly bodies. It was noted that there was very little presence of quintessence within the terrestrial sphere. Due to the low presence of quintessence, earth could be affected by what takes place within the heavenly bodies. This theory was developed in the 14th century text \"The testament of Lullius\", attributed to Ramon Llull. The use of quintessence became popular within medieval alchemy. Quintessence stemmed from the medieval elemental system, which consisted of the four classical elements, and aether, or quintessence, in addition to two chemical elements representing metals: sulphur, \"the stone which burns\", which characterized the principle of combustibility, and mercury, which contained the idealized principle of metallic properties.\n\nThis elemental system spread rapidly throughout all of Europe and became popular with alchemists, especially in medicinal alchemy. Medicinal alchemy then sought to isolate quintessence and incorporate it within medicine and elixirs. Due to quintessence's pure and heavenly quality, it was thought that through consumption one may rid oneself of any impurities or illnesses. In \"The book of Quintessence\", a 15th-century English translation of a continental text, quintessence was used as a medicine for many of man's illnesses. A process given for the creation of quintessence is distillation of alcohol seven times. Over the years, the term quintessence has become synonymous with elixirs, medicinal alchemy, and the philosopher's stone itself.\n\nWith the 18th century physics developments, physical models known as \"aether theories\" made use of a similar concept for the explanation of the propagation of electromagnetic and gravitational forces. As early as the 1670s, Newton used the idea of aether to help match observations to strict mechanical rules of his physics. However, the early modern aether had little in common with the aether of classical elements from which the name was borrowed. These aether theories are considered to be scientifically obsolete, as the development of special relativity showed that Maxwell's equations do not require the aether for the transmission of these forces. However, Einstein himself noted that his own model which replaced these theories could itself be thought of as an aether, as it implied that the empty space between objects had its own physical properties.\n\nDespite the early modern aether models being superseded by general relativity, occasionally some physicists have attempted to reintroduce the concept of aether in an attempt to address perceived deficiencies in current physical models. One proposed model of dark energy has been named \"quintessence\" by its proponents, in honor of the classical element. This idea relates to the hypothecial form of dark energy postulated as an explanation of observations of an accelerating universe. It has also been called a fifth fundamental force.\n\nThe motion of light was a long-standing investigation in physics for hundreds of years before the 20th century. The use of aether to describe this motion was popular during the 17th and 18th centuries, including a theory proposed by Johann II Bernoulli, who was recognized in 1736 with the prize of the French Academy. In his theory, all space is permeated by aether containing \"excessively small whirlpools\". These whirlpools allow for aether to have a certain elasticity, transmitting vibrations from the corpuscular packets of light as they travel through.\n\nThis theory of luminiferous aether would influence the wave theory of light proposed by Christiaan Huygens, in which light traveled in the form of longitudinal waves via an \"omnipresent, perfectly elastic medium having zero density, called aether\". At the time, it was thought that in order for light to travel through a vacuum, there must have been a medium filling the void through which it could propagate, as sound through air or ripples in a pool. Later, when it was proved that the nature of light wave is transverse instead of longitudinal, Huygens' theory was replaced by subsequent theories proposed by Maxwell, Einstein and de Broglie, which rejected the existence and necessity of aether to explain the various optical phenomena. These theories were supported by the results of the Michelson–Morley experiment in which evidence for the motion of aether was conclusively absent. The results of the experiment influenced many physicists of the time and contributed to the eventual development of Einstein's theory of special relativity.\n\nAether has been used in various gravitational theories as a medium to help explain gravitation and what causes it. It was used in one of Sir Isaac Newton's first published theories of gravitation, \"Philosophiæ Naturalis Principia Mathematica\" (the \"Principia\"). He based the whole description of planetary motions on a theoretical law of dynamic interactions. He renounced standing attempts at accounting for this particular form of interaction between distant bodies by introducing a mechanism of propagation through an intervening medium. He calls this intervening medium aether. In his aether model, Newton describes aether as a medium that \"flows\" continually downward toward the Earth's surface and is partially absorbed and partially diffused. This \"circulation\" of aether is what he associated the force of gravity with to help explain the action of gravity in a non-mechanical fashion. This theory described different aether densities, creating an aether density gradient. His theory also explains that aether was dense within objects and rare without them. As particles of denser aether interacted with the rare aether they were attracted back to the dense aether much like cooling vapors of water are attracted back to each other to form water. In the \"Principia\" he attempts to explain the elasticity and movement of aether by relating aether to his static model of fluids. This elastic interaction is what caused the pull of gravity to take place, according to this early theory, and allowed an explanation for action at a distance instead of action through direct contact. Newton also explained this changing rarity and density of aether in his letter to Robert Boyle in 1679. He illustrated aether and its field around objects in this letter as well and used this as a way to inform Robert Boyle about his theory. Although Newton eventually changed his theory of gravitation to one involving force and the laws of motion, his starting point for the modern understanding and explanation of gravity came from his original aether model on gravitation.\n\n"}
{"id": "1018300", "url": "https://en.wikipedia.org/wiki?curid=1018300", "title": "Anglo-Australian Near-Earth Asteroid Survey", "text": "Anglo-Australian Near-Earth Asteroid Survey\n\nThe Anglo-Australian Near-Earth Asteroid Survey (AANEAS) operated from 1990-96, becoming one of the most prolific programs of its type in the world. Apart from leading to the discovery of 38 near-Earth asteroids, 9 comets, 63 supernovae, several other astronomical phenomena and the delivery of a substantial proportion of all NEA astrometry obtained worldwide (e.g., 30% in 1994-95), AANEAS also led to many other scientific advances which were reported in the refereed literature.\n\n\n"}
{"id": "804218", "url": "https://en.wikipedia.org/wiki?curid=804218", "title": "Astronomical clock", "text": "Astronomical clock\n\nAn astronomical clock is a clock with special mechanisms and dials to display astronomical information, such as the relative positions of the sun, moon, zodiacal constellations, and sometimes major planets.\n\nThe term is loosely used to refer to any clock that shows, in addition to the time of day, astronomical information. This could include the location of the sun and moon in the sky, the age and Lunar phases, the position of the sun on the ecliptic and the current zodiac sign, the sidereal time, and other astronomical data such as the moon's nodes (for indicating eclipses) or a rotating star map. The term should not be confused with \"astronomical regulator\", a high precision but otherwise ordinary pendulum clock used in observatories.\n\nAstronomical clocks usually represent the solar system using the geocentric model. The center of the dial is often marked with a disc or sphere representing the earth, located at the center of the solar system. The sun is often represented by a golden sphere (as it initially appeared in the Antikythera Mechanism, back in the 2nd century BC), shown rotating around the earth once a day around a 24-hour analog dial. This view accorded both with the daily experience and with the philosophical world view of pre-Copernican Europe.\n\nResearch in 2011 and 2012 led an expert group of researchers to posit that European astronomical clocks are descended from the technology of the Antikythera mechanism.\n\nIn the 11th century, the Song dynasty Chinese horologist, mechanical engineer, and astronomer Su Song created a water-driven astronomical clock for his clock-tower of Kaifeng City. Su Song is noted for having incorporated an escapement mechanism and earliest known endless power-transmitting chain drive for his clock-tower and armillary sphere to function. Contemporary Muslim astronomers and engineers also constructed a variety of highly accurate astronomical clocks for use in their observatories, such as the castle clock (a water-powered astronomical clock) by Al-Jazari in 1206, and the astrolabic clock by Ibn al-Shatir in the early 14th century.\n\nThe early development of mechanical clocks in Europe is not fully understood, but there is general agreement that by 1300–1330 there existed mechanical clocks (powered by weights rather than by water and using an escapement) which were intended for two main purposes: for signalling and notification (e.g. the timing of services and public events), and for modelling the solar system. The latter is an inevitable development, because the astrolabe was used both by astronomers and astrologers, and it was natural to apply a clockwork drive to the rotating plate to produce a working model of the solar system. American historian Lynn White Jr. of Princeton University wrote: \nThe astronomical clocks developed by the English mathematician and cleric Richard of Wallingford in St Albans during the 1330s, and by medieval Italian physician and astronomer Giovanni de Dondi in Padua between 1348 and 1364 are masterpieces of their type. They no longer exist, but detailed descriptions of their design and construction survive, and modern reproductions have been made. Wallingford's clock may have shown the sun, moon (age, phase, and node), stars and planets, and had, in addition, a wheel of fortune and an indicator of the state of the tide at London Bridge. De Dondi's clock was a seven-faced construction with 107 moving parts, showing the positions of the sun, moon, and five planets, as well as religious feast days.\n\nBoth these clocks, and others like them, were probably less accurate than their designers would have wished. The gear ratios may have been exquisitely calculated, but their manufacture was somewhat beyond the mechanical abilities of the time, and they never worked reliably. Furthermore, in contrast to the intricate advanced wheelwork, the timekeeping mechanism in nearly all these clocks until the 16th century was the simple verge and foliot escapement, which had errors of at least half an hour a day.\n\nAstronomical clocks were built as demonstration or exhibition pieces, to impress as much as to educate or inform. The challenge of building these masterpieces meant that clockmakers would continue to produce them, to demonstrate their technical skill and their patrons' wealth. The philosophical message of an ordered, heavenly-ordained universe, which accorded with the Gothic era view of the world, helps explain their popularity.\n\nThe growing interest in astronomy during the 18th century revived interest in astronomical clocks, less for the philosophical message, more for the accurate astronomical information that pendulum-regulated clocks could display.\n\n\"Le Gros Horloge\" in Rouen is one of the earliest known astronomical clocks. The clock is installed in a Renaissance arch crossing the Rue du Gros-Horloge. The mechanism is one of the oldest in France; the movement was made in 1389. Construction of the clock was started by Jourdain del Leche, who lacked the necessary expertise to finish the task, so the work was completed by Jean de Felain, who became the first to hold the position of governor of the clock. The clock was originally constructed without a dial, with one revolution of the hour-hand representing twenty-four hours. The movement is cast in wrought iron, and at approximately twice the size of the Wells Cathedral clock, it is perhaps the largest such mechanism still extant. A facade was added in 1529 when the clock was moved to its current position. The Renaissance facade represents a golden sun with 24 rays on a starry blue background. The dial measures 2.5 metres in diameter.\n\nThe phases of the moon are shown in the oculus of the upper part of the dial. It completes a full rotation in 29 days. The week days are shown in an opening at the base of the dial with allegorical subjects for each day of the week.\n\nThe Science Museum (London) has a scale model of the 'Cosmic Engine', which Su Sung, a Chinese polymath, designed and constructed in China in 1092. This great astronomical hydromechanical clock tower was about ten metres high (about 30 feet) and featured a clock escapement and was indirectly powered by rotating wheel either with falling water and liquid mercury, which freezes at a much lower temperature than water, allowing operation of the clock during colder weather. A full-sized working replica of Su Sung's clock exists in the Republic of China (Taiwan)'s National Museum of Natural Science, Taichung city. This full-scale, fully functional replica, approximately in height, was constructed from Su Sung's original descriptions and mechanical drawings.\n\nThe most sophisticated water-powered astronomical clock was Al-Jazari's castle clock, considered to be an early example of a programmable analog computer, in 1206. It was a complex device that was about high, and had multiple functions alongside timekeeping. It included a display of the zodiac and the solar and lunar orbits, and a pointer in the shape of the crescent moon which travelled across the top of a gateway, moved by a hidden cart and causing automatic doors to open, each revealing a mannequin, every hour.\n\nIt was possible to re-program the length of day and night every day in order to account for the changing lengths of day and night throughout the year, and it also featured five musician automata who automatically played music when moved by levers operated by a hidden camshaft attached to a water wheel. Other components of the castle clock included a main reservoir with a float, a float chamber and flow regulator, plate and valve trough, two pulleys, crescent disc displaying the zodiac, and two falcon automata dropping balls into vases.\n\nThe Strasbourg Cathedral has housed three different astronomical clocks since the 14th century.\nThe first clock was built between 1352 and 1354 and stopped working sometime at the beginning of the 16th century. A second clock was then built by Herlin, Conrad Dasypodius, the Habrecht brothers, and others, between 1547 and 1574. This clock stopped working in 1788 or 1789 (as it apparently stopped working gradually, each component being disconnected\none after the other). After a lapse of 50 years, a new clock was built by Jean-Baptiste Schwilgué (1776–1856) and about 30 workers. This clock is housed\nin the case of the 2nd clock. It shows many astronomical and calendrical functions (including what is thought to be the first complete mechanization of the part of the computus needed to compute Easter) as well as several automata.\n\nOne of the most famous of this type of clock is the Old-Town Hall clock in Prague, Czech Republic. It is also known as the Prague orloj. The central portion was completed in 1410. The four figures are set in motion at the hour, with Death (represented by a skeleton) striking the time. On the hour there is a presentation of statues of the Apostles at the doorways above the clock, with all twelve presented at noon. In 1490 a calendar display was added below the clock along with decorative Gothic sculptures.\n\nDuring World War II the clock was nearly destroyed by Nazi fire. The townspeople are credited with heroic efforts in saving most of the parts. It was gradually renovated until 1948. In 1979 the clock was once more cleaned and renovated. According to local legend the city will suffer if the clock is neglected and its good operation is placed in jeopardy.\n\nOlomouc, the former capital of Moravia in the eastern part of the Czech Republic, also has an impressive exterior astronomical clock on the main town square. It is a rare example of a heliocentric astronomical clock.\n\nA legend dates its construction to the year 1422; however, in historic sources it is first mentioned in 1517. The clock was remodelled approximately once every century; in 1898 the astrolabe was replaced with a heliocentric model of the solar system. When the retreating Nazi German army passed through Olomouc in the final days of the war in May 1945, they opened fire on the old astronomical clock, leaving only a few pieces (that can now be seen in the local museum). As a result of the serious damage the clock was reconstructed in the style of socialist-realism in the first years of communist rule in Czechoslovakia (1948-early 1950s). The religious and royal figures were replaced with athletes, workers, farmers, scientists and other members of the proletariat, while the glockenspiel was altered to play three pieces of traditional local music.\n\nIn 2009 an astronomical clock was built in the municipality of Stará Bystrica in Northern Slovakia during the reconstruction of the town square.\n\nThe astronomical clock has the shape of a stylized form of Our Lady of Sorrows, patron of Slovakia; it has been described as the largest wooden statue of Slovakia. Its exterior is decorated by statues of important figures from Slovakia's history: Prince Pribina, King Svatopluk, Anton Bernolák, Ľudovít Štúr, Milan Rastislav Štefánik, Andrej Hlinka. Each hour, statuettes of saints connected with Slovakia appear: Cyril, Methodius, Andrew-Zorard, Benedict, Gorazd, Bystrík and Adalbert. The bells of the clock carry the names Sv. Juraj (St. George) and Riečnická Madona (Our Lady of Riečnica); the first is rung to indicate the time, the second accompanies the saints. This astronomical clock is the only one in Slovakia.\n\nThe astronomical part of the clock consists of an astrolabe displaying the astrological signs, positions of the Sun and Moon, and the lunar phases. The clock is controlled by computer using DCF77 signals.\n\nThe Ottoman engineer Taqi al-Din described a weight-driven clock with a verge-and-foliot escapement, a striking train of gears, an alarm, and a representation of the moon's phases in his book \"The Brightest Stars for the Construction of Mechanical Clocks\" (\"Al-Kawākib al-durriyya fī wadh' al-bankāmat al-dawriyya\"), written around 1565. The clock also displayed the zodiac.\n\nThe astronomical clock in Lund Cathedral in Sweden, \"Horologium mirabile Lundense\" was made around 1425, probably by the clockmaker Nicolaus Lilienveld in Rostock. After it had been in storage since 1837, it was restored and put back in place in 1923. Only the upper, astronomical part is original, while some of the other remaining medieval parts can be seen at the Cathedral museum. When it plays, one can hear \"In Dulci Jubilo\" from the smallest organ in the church, while seven wooden figures, representing the three magi and their servants, pass by.\n\nMade in 1860 by of Beauvais, the Astronomical clock (Besançon) expresses the theological concept that during each second of the day the Resurrection of Christ transforms the existence of man and of the world. The clock stands 5.8 metres high and 2.5 metres wide, and has 30,000 mechanical parts. Seventy dials provide 122 indications including seconds, hours, days and years. The clock is perpetual and can register up to 10,000 years, including adjustments for leap year cycles. Immediately after he finished the Besançon commission, Vérité built a larger more elaborate clock for Beauvais Cathedral in his home town.\n\nBeauvais astronomical clock in Beauvais Cathedral was built by Auguste-Lucien Vérité over a period of four years, from 1865 to 1868 as a follow-up to his elaborate clock at Besancon. It is 12 metres high, 6 metres wide, and contains over 90,000 individual parts. It has 52 dials that display the times of sunrise, sunset, moonrise, moonset, the phases of the moon, the solstices, the position of the planets, the current time in 18 cities around the world, and the tidal hours. The clock has a case that blends Romanesque and Byzantine styles and is crowned by a multi-tiered Celestial City with 68 automata that animate, at the striking of each hour, to enact the Last Judgement.\n\nThe city hall in Copenhagen has a complete astronomical clock, set in an interior glass cabinet.\nThe clock was designed over a period of 50 years by amateur astronomer and professional clockmaker Jens Olsen.\nSome of the components (such as the computus) were inspired by the Strasbourg clock, which was studied\nby Olsen. It was assembled from 1948 to 1955. Between 1995 and 1997 the clock underwent a complete restoration by the Danish watchmaker and conservator Søren Andersen.\n\nArguably the most complicated of its kind ever constructed, the last of a total of four astronomical clocks designed and made by Norwegian Rasmus Sørnes (1893–1967), is characterized by its superior complexity compactly housed in a casing with the modest measurements of 0.70 x 0.60 x 2.10 m. Features include locations of the sun and moon in the zodiac, Julian calendar, Gregorian calendar, sidereal time, GMT, local time with daylight saving time and leap year, solar and lunar cycle corrections, eclipses, local sunset and sunrise, moonphase, tides, sunspot cycles and a planetarium including Pluto's 248-year orbit and the 25 800-year period of the polar ecliptics (precession of the Earth's axis). All wheels are in brass and gold-plated. Dials are silver-plated.\n\nSørnes also made the necessary tools and based his work on his own observations of the firmament. This remarkable timepiece will probably be the last one ever to be designed and made by hand by one single person as true craftsmanship and a work of art. The result, outstanding for its performance and accuracy, remains a symbol of the transition from the mechanical age, Sørnes' electromechanical pendulum system pointing forward into the age of digital clocks. Having been exhibited at the Time Museum in Rockford, Illinois, and at The Chicago Museum of Science and Industry, the clock was sold in 2002 and its current location is not known. The Rasmus Sørnes Astronomical Clock no.3, the precursor to the Chicago Clock, his tools, patents, drawings, telescope and other items, are exhibited at the Borgarsyssel Museum in Sarpsborg, Norway.\n\nThere are many examples of astronomical table clocks, due to their popularity as showpieces. To become a master clockmaker in 17th century Augsburg candidates had to design and build a 'masterpiece' clock, an astronomical table top clock of formidable complexity. Examples can be found in museums, such as London's British Museum.\n\nCurrently Edmund Scientific among other retailers offer a mechanical Tellurium clock, perhaps the first mechanical astronomical clock to be mass marketed.\n\nIn Japan, Tanaka Hisashige made Myriad year clock in 1851.\n\nMore recently, independent clockmaker created a wristwatch astrolabe, the \"Astrolabium\" in addition to the \"Planetarium 2000\", the \"Eclipse 2001\" and the \"Real Moon.\" Ulysse Nardin also sells several astronomical wristwatches, the \"Astrolabium,\" \"Planetarium\", and the \"Tellurium J. Kepler.\"\n\nMany European countries have examples of astronomical clocks, including:\n\nBelgium\n\nCroatia\n\nDenmark\n\nFrance\n\nGermany\n\nMalta\n\nNorway\n\nItaly\n\nPoland\n\nSwitzerland\n\nUnited Kingdom\n\nAlthough each astronomical clock is different, they share some common features.\n\nMost astronomical clocks have a 24-hour analog dial around the outside edge, numbered from I to XII then from I to XII again. The current time is indicated by a golden ball or a picture of the sun at the end of a pointer. Local noon is usually at the top of the dial, and midnight at the bottom. Minute hands are rarely used.\n\nThe sun indicator or hand gives an approximate indication of both the sun's azimuth and altitude. For azimuth (bearing from North), the top of the dial indicates South, and the two VI points of the dial East and West. For altitude, the top is the zenith and the two VI and VI points define the horizon. (This is for the astronomical clocks designed for use in the northern hemisphere.) This interpretation is most accurate at the equinoxes, of course.\n\nIf XII is not at the top of the dial, or if the numbers are Arabic rather than Roman, then the time may be shown in Italian hours (also called Bohemian, or Old Czech, hours). In this system, 1 o'clock occurs at sunset, and counting continues through the night and into the next afternoon, reaching 24 an hour before sunset.\n\nIn the photograph of the Prague clock shown above, the time indicated by the sun hand is about noon (XII in Roman numerals), or about the 17th hour (Italian time in Arabic numerals).\n\nThe year is usually represented by the 12 signs of the zodiac, arranged either as a concentric circle inside the 24-hour dial, or drawn onto a displaced smaller circle, which is a projection of the ecliptic, the path of the sun and planets through the sky, and the plane of the Earth's orbit.\n\nThe ecliptic plane is projected onto the face of the clock, and, because of the Earth's tilted angle of rotation relative to its orbital plane, it is displaced from the center and appears to be distorted. The projection point for the stereographic projection is the North pole; on astrolabes the South pole is more common.\n\nThe ecliptic dial makes one complete revolution in 23 hours 56 minutes (a sidereal day), and will therefore gradually get out of phase with the hour hand, drifting slowly further apart during the year.\n\nTo find the date, find the place where the hour hand or sun disk intersects the ecliptic dial: this indicates the current star sign, the sun's current location on the ecliptic. The intersection point slowly moves round the ecliptic dial during the year, as the sun moves out of one astrological sign into another.\n\nIn the photograph of the Prague clock shown above, the sun's disk has recently moved into Aries (the stylized ram's horns), having left Pisces. The date is therefore late March or early April.\n\nIf the zodiac signs run around inside the hour hands, either this ring rotates to align itself with the hour hand, or there's another hand, revolving once per year, which points to the sun's current zodiac sign.\n\nA dial or ring indicating the numbers 1 to 29 or 30 indicates the moon's age: a new moon is 0, waxes and become full around day 15, and then wanes up to 29 or 30. The phase is sometimes shown by a rotating globe or black hemisphere, or a window that reveals part of a wavy black shape beneath.\n\nUnequal hours were the result of dividing up the period of daylight into 12 equal hours, and night time into another 12. In Europe, there is more daylight in the summer, and less night, so each of the 12 daylight hours is longer than a night hour. Similarly in winter, daylight hours are shorter, and night hours are longer. These unequal hours are shown by the curved lines radiating from the center. The longer daylight hours in summer can usually be seen at the outer edge of the dial, and the time in unequal hours is read by noting the intersection of the sun hand with the appropriate curved line.\n\nAstrologers placed importance on how the sun, moon, and planets were arranged and aligned in the sky. If certain planets appeared at the points of a triangle, hexagon, or square, or if they were opposite or next to each other, the appropriate aspect was used to determine the event's significance. On some clocks you can see the common aspects—triangle, square, and hexagon—drawn inside the central disk, with each line marked by the symbol for that aspect, and you may also see the signs for conjunction and opposition. On an astrolabe, the corners of the different aspects could be lined up on any of the planets. On a clock, though, the disk containing the aspect lines can't be rotated at will, so they usually show only the aspects of the sun or moon.\n\nIn the photograph of the Brescia clock above, the triangle, square, and star in the center of the dial show these aspects (the third, fourth, and sixth phases) of (presumably) the moon.\n\nThe moon's orbit is not in the same plane as the Earth's orbit around the sun, but crosses it in two places. The moon crosses the ecliptic plane twice a month, once when it goes up above the plane, and again 15 or so days later when it goes back down below the ecliptic. These two locations are the ascending and descending lunar nodes. Solar and lunar eclipses will occur only when the moon is positioned near one of these nodes, because at other times the moon is either too high or too low for an eclipse to be noticed from earth. Some astronomical clocks keep track of the position of the lunar nodes with a long pointer that crosses the dial. This so-called dragon hand makes one complete rotation around the ecliptic dial every 19 years. When the dragon hand and the new moon coincide, the moon is on the same plane as the earth and sun, and so there is every chance that an eclipse will be visible from somewhere on earth.\n\n\n\n"}
{"id": "53650798", "url": "https://en.wikipedia.org/wiki?curid=53650798", "title": "Axiom of Cumulative Inertia", "text": "Axiom of Cumulative Inertia\n\nThe Axiom of Cumulative Inertia is a sociological phenomenon rooted in observations of human migratory patterns. Introduced in 1966 by Myers, McGinnis and Masnick, the most widely known form of the axiom is encapsulated by the statement:\n\"The probability of remaining in any state of nature increases as a strict monotone function of duration of prior residence in that state.\"\nThere is thus a clear negative correlation between the amount of time a person has lived in a certain location, and the likelihood they will move in future. Suggested reasons for this change include loss aversion (resulting from possible unknown problems in the further place of residence), financial stability linked to a certain place of employment or the presence of an extended social network in the form of familial and other obligations.\nThe large number of factors which affect human migratory patterns make it difficult to assess the exact degree of impact of the various postulated variables, and debate also exists as to the appropriate relation between residence time and likelihood of moving.\n\nSince it was first coined from human residence patterns, cumulative inertia has since been observed in a variety of other social behaviours. It has been used to model entry to and recovery from addiction, modern tribalism effects, and the rehabilitation chances of repeat offenders (recidivism). Identifying the role and strength of inertia in such behaviours can be critical in designing the appropriate treatment or intervention strategy, and is an important tool in identifying individuals or groups which are most likely to change behaviour. \n\nIn addition to being relevant in specific demographic targeting, such as members of a political party who are more likely to be swayed by different ideology, or individuals who pose a higher risk of relapsing into additive behaviours, cumulative inertia can also play an important role outside of political campaigns or public policy making. Geographical areas presenting strong cumulative inertia are indicative of attractive regions wherein property prices or demand is likely to rise. Cumulative inertia can thus also have a strong financial impact.\n\nThe study of cumulative inertia remains an active area of current research in complex systems or quantitative sociology.\n\n"}
{"id": "28836751", "url": "https://en.wikipedia.org/wiki?curid=28836751", "title": "Bala Fault", "text": "Bala Fault\n\nThe Bala Fault is a SW-NE trending geological fault in Wales that extends offshore into Cardigan Bay. In the offshore area it is a major normal fault and forms the bounding structure to the Cardigan Bay Basin, with a fill including about 2500 m of Lias Group. At its northeastern end it links to the similarly orientated Llanelidan Fault.\n\n"}
{"id": "25479804", "url": "https://en.wikipedia.org/wiki?curid=25479804", "title": "Centre of Research in Theories and Practices that Overcome Inequalities", "text": "Centre of Research in Theories and Practices that Overcome Inequalities\n\nThe Centre of Research in Theories and Practices that Overcome Inequalities (CREA) was founded in 1991 by a current professor of Sociology at the University of Barcelona, Doctor Honoris Causa of West University of Timisoara and also a recognized researcher in Europe in the Social Science area, Ramon Flecha. After Ramon Flecha’s resignation as the Director of CREA, in 2006; Marta Soler, Doctor by Harvard, a current Professor of Sociological Theory, assumed the post. \nCREA, one of the centres that first joined the Scientific Park of Barcelona (University of Barcelona); is indisciplinary; multicultural and open accepting different ideologies, religions, lifestyles, sexual orientations ; transparent, since its knowledge is at everyone’s disposal; and it is a centre where the validity of arguments prevails over the positions of power of their members, creating, in this way, an environment of an egalitarian dialogue. This centre is formed by University research professors, researchers and professional collaborators of diverse disciplines (sociology, pedagogy, economy, mathematics, communication, biology, etc.).\n\nAll the research CREA carries out is done with the direct collaboration of the subjects researched, using the critical communicative methodology. The subject of research is directly included providing its interpretations, experiences and opinions, enriching, in this way, the research. It is about facilitating the participation of the researched in the research through an egalitarian dialogue with the researcher where validity claims prevail over power ones.\n\nThe Research Centre carries out both, international and national projects developing the following lines of research:\n\n\nAnd from the lines of research described above, five groups were created by CREA:\n\nCREA is, today, internationally well- known for its international research oriented to overcoming inequalities; already present in Europe and in various countries such as Brazil, United States, Korea or Australia.\nConcretely, from its origins, CREA has collaborated with international research groups, as well as with different authors of the Scientific International Community. Some of the seminars were held with important authors such as: Paulo Freire (1994), Ulrich Beck and Elisabeth Beck Gernsheim(1998), Alain Touraine (1999), Jon Elster (2001), Judith Butler ( 2001), Alejandro Portes (2002), Gordon Wells(2003), John Searle (2003) or Gary Orfield (2003).\n\nDifferent members of CREA have given lectures and seminars at several universities in Brazil, United States, Germany, Australia, Korea and others. Ramon Flecha participated in The Harvard Education Forum celebrated on February 27, 1998; in honor of Paulo Freire, alongside important international referents such as: Noam Chomsky (Institute Professor and Professor of Linguistics, MIT), Eileen de los Reyes (Moderator, Assistant Professor HGSE), Carolyn Higgins (Earlhm College), Yamila Hussein (HGSE master’s degree candidate), Donaldo Macedo (UMass Boston), Nancy Richardson (associate dean for ministry, Harvard Divinity School), Ira Shor (City College of New York).\n\nIn addition, and as another example, in 2008, professor of Sociological Theory and director of CREA, Marta Soler, gave two lectures about literary gatherings and Community involvement for social change and about the role of critical communicative methodology in Overcoming social exclusion ; and led a seminar at the Havens Center for Study of Social of Structure and Social Change, in the University of Wisconsin- Madison.\n\n\n\n"}
{"id": "2362408", "url": "https://en.wikipedia.org/wiki?curid=2362408", "title": "Collective action clause", "text": "Collective action clause\n\nA collective action clause (CAC) allows a supermajority of bondholders to agree to a debt restructuring that is legally binding on all holders of the bond, including those who vote against the restructuring. Bondholders generally opposed such clauses in the 1980s and 1990s, fearing that it gave debtors too much power. However, following Argentina's December 2001 default on its debts in which its bonds lost 70% of their value, CACs have become much more common, as they are now seen as potentially warding off more drastic action, but enabling easier coordination of bondholders.\n\nDuring the financial crisis of 2011-12, the Greek government imposed, with the support of the IMF and ECB, a retroactive CAC with a threshold of 75%. That impacted 90% of the bonds, which issued under the jurisdiction of Greek courts. On 9 May 2012, the required supermajority was obtained, with 85.8% of domestic-law bonds tendered in favour. An investor named Bill Gross said, \"The sanctity of their contracts is certainly lessened. Bondholders have that to look forward to going into the future,\" but the French Minister of Finance celebrated the deal. In the exchange, investors will receive new bonds with a face value of 31.5 percent of the old ones, together with notes from the EFSF. The new debt is governed by English law and comes with warrants that may provide extra income in years if Greek economic growth exceeds thresholds. \n\nIn accordance with the treaty establishing the European Stability Mechanism, all bonds issued by Eurozone member states with maturities exceeding one year, issued after January 1, 2013, have a mandatory collective action clause. \n\n"}
{"id": "2906541", "url": "https://en.wikipedia.org/wiki?curid=2906541", "title": "Cystolith", "text": "Cystolith\n\nCystolith (Gr. \"cavity\" and \"stone\") is a botanical term for outgrowths of the epidermal cell wall, usually of calcium carbonate, formed in a cellulose matrix in special cells called lithocysts, generally in the leaf of plants. \n\nCystoliths are present in certain families, including in many genera of Acanthaceae. Plants in the family Urticaceae, known as stinging nettles, also form leaf cystoliths, but only during their later flowering and seed setting stages. Other examples include Cannabis and other plants in the family Cannabaceae, which produce leaf and flower cystoliths, and \"Ficus elastica\", the Indian rubber plant of the family Moraceae. \n\nFrom a 1987 article on cystolith development and structure:\n"}
{"id": "9628193", "url": "https://en.wikipedia.org/wiki?curid=9628193", "title": "Darwin–Radau equation", "text": "Darwin–Radau equation\n\nIn astrophysics, the Darwin–Radau equation gives an approximate relation between the moment of inertia factor of a planetary body and its rotational speed and shape. The moment of inertia factor is directly related to the largest principal moment of inertia, \"C\". It is assumed that the rotating body is in hydrostatic equilibrium and is an ellipsoid of revolution. The Darwin–Radau equation states\n\nwhere \"M\" and \"R\" represent the mass and mean equatorial radius of the body. Here λ is the d'Alembert parameter and the Radau parameter η is defined as\n\nwhere \"q\" is the geodynamical constant\n\nand ε is the geometrical flattening\n\nwhere \"R\" is the mean polar radius and \"R\" is the mean equatorial radius.\n\nFor Earth, formula_5 and formula_6, which yields formula_7, a good approximation to the measured value of 0.3307.\n"}
{"id": "39186711", "url": "https://en.wikipedia.org/wiki?curid=39186711", "title": "Data-informed decision-making", "text": "Data-informed decision-making\n\nData-informed decision-making (DIDM) gives reference to the collection and analysis of data to guide decisions that improve success. DIDM is used in education communities (where data is used with the goal of helping students and improving curricula) but is also applicable to (and thus also used in) other fields in which data is used to inform decisions. While data based decision making is a more common term, \"data-informed\" decision-making is a preferable term since decisions should not be based solely on quantitative data. Most educators have access to a data system for the purpose of analyzing student data. These data systems present data to educators in an over-the-counter data format (embedding labels, supplemental documentation, and a help system, making key package/display and content decisions) to improve the success of educators’ data-informed decision-making. In Business, fostering and actively supporting DIDM in their firm and among their colleagues could be the main rôle of CIOs (Chief Information Officers) or CDOs (Chief Data Officers).\n\nAssessment in higher education is a form of DIDM aimed at using evidence of what students learn to improve curriculum, student learning, and teaching. Standardized tests, grades, and student work scored by rubrics are forms of student learning outcomes assessment. There are numerous organizations aimed at promoting the assessment of student learning through DIDM including the National Institute for Learning Outcomes Assessment, the Association for the Assessment of Student Learning in Higher Education, and, to an extent, the Association of American Colleges and Universities.\n\nSimilarly, the idea of data-informed decision making is becoming more prevalent within international development is similarly being emphasized. For instance, in a literature review focused on development, an integrated, participatory, structured and enpowering approach to using evidence and data in decision-making to inform development decisions was tied to improved results. \n"}
{"id": "46671358", "url": "https://en.wikipedia.org/wiki?curid=46671358", "title": "Dorothea Maria Graff", "text": "Dorothea Maria Graff\n\nDorothea Maria Graff (1678–1743) was an 18th-century painter from Germany, who lived and worked in Amsterdam, and Saint Petersburg.\n\nDorothea Maria Graff was born in Nuremberg as the daughter of the painters Maria Sibylla Merian and Johann Andreas Graff, and learned to paint from them and her sister Johanna Helena Herolt who was ten years older. In 1681 her mother returned to Frankfurt without her father, in order to live with her mother after her stepfather Jacob Marrel's death. Though Johann Graff joined his family later, in 1686 Merian left her husband and moved with her two daughters and her mother to a religious community of Labadists in Wieuwerd, Friesland. Johann Graff made various attempts at reconciliation but eventually returned to Germany. In 1691 the four women moved to Amsterdam, where they set up a studio painting flowers and botanical subjects, continuing Merian's work on \"The Caterpillar Book\". In 1699 Dorothea accompanied her mother to Surinam and they returned in September 1701.\n\nShe married a surgeon from Heidelberg, Philip Hendriks, a few months later on 2 December 1701. Dorothea moved in with her mother where they ran a business selling her mother's prints and paintings. The couple had a child who died young. Johanna moved with her husband to Surinam in 1711 and in the same year Dorothea's husband died and Dorothea took on her mother's name Merian, possibly for business reasons. In 1713 mother and daughter published \"Der rupsen begin, voedsel en wonderbaare veranderingen\", followed by the second volume in 1714. Though her daughters are not mentioned by name, both Johanna and Dorothea probably contributed to the plates. In 1714 mother Maria had a stroke that partially paralysed her and Johanna returned for a visit from Surinam, painting under her mother's name and working on volume three. In 1715 Dorothea remarried, this time to the widowed Swiss painter Georg Gsell, who had recently divorced his second wife and had previously lodged with Dorothea and her mother. Gsell had been living in Amsterdam since 1704 and had five daughters from his first marriage, including Katharina, the later wife of the mathematician Leonhard Euler. From Surinam, Johanna continued to supply her sister with insects and other supplies.\n\nIn 1717, after her mother died in January, Dorothea published the third volume of her mother's \"Der Rupsen Begin\". Works by the house of Merian were purchased by Zacharias Conrad von Uffenbach, Pieter Teyler van der Hulst, and Robert Areskin. Pieter the Great asked the Gsell-Merian couple to come work for him, and they did, first selling all of mother Maria's work that they could. In October 1717 Georg Gsell became court painter in Saint Petersburg and Dorothea became a teacher at the Petrus Academie of Science, and curator of the natural history collection Kunstkamera (which included her own work). In 1736 she returned to Amsterdam to purchase works by her mother for the collection. She died in Saint Petersburg.\n\nTodd, Kim. \"Maria Sibylla Merian (1647-1717): An Early Investigator Of Parasitoids And Phenotypic Plasticity.\" \"Terrestrial Arthropod Reviews\" 4.2 (2011): 131-144. \"Academic Search Complete\". Web. 24 Apr. 2016.\n"}
{"id": "25661541", "url": "https://en.wikipedia.org/wiki?curid=25661541", "title": "Electronic Colloquium on Computational Complexity", "text": "Electronic Colloquium on Computational Complexity\n\nThe Electronic Colloquium on Computational Complexity (ECCC) is an electronic archive of research papers in computational complexity theory, a branch of computer science.\n\nThe intention of the ECCC is to provide a fast publication service intermediate in its level of peer review between preprint servers such as authors' web sites or arXiv (which release papers with little or no delay and filtering) and journals (which subject papers to a heavy editing process but, in computer science, may take months or years to publish a paper). Papers submitted to ECCC are screened by a board of experts, who review the submissions to ensure that they are on-topic, novel, interesting, and written according to the standards of the field. Any panelist may accept or reject any of the submissions; if no decision is made within two months, the submission is automatically rejected.\n\nIn order to ensure the long-term stability of the archive, its contents are backed up by electronic media that are sent to multiple libraries and to the ECCC board members and by printouts that are stored in multiple locations.\nWorks in the ECCC remain the copyright of the authors, who may request their removal at any time.\n\nThe ECCC was founded in 1994 at the University of Trier in Trier, Germany. In 2004 its founding editor Christoph Meinel moved to the Hasso Plattner Institute at the University of Potsdam and moved some of the ECCC offices with him to Potsdam. In January 2017 the ECCC moved to the Weizmann Institute of Science.\n\nAfter the first ten years of the project, it had accepted more than 900 papers, and had nearly 500 registered users.\n"}
{"id": "25433344", "url": "https://en.wikipedia.org/wiki?curid=25433344", "title": "Elementary effects method", "text": "Elementary effects method\n\nThe elementary effects (EE) method is the most used screening method in sensitivity analysis. It is applied to identify non-influential inputs for a computationally costly mathematical model or for a model with a large number of inputs, where the costs of estimating other sensitivity analysis measures such as the variance-based measures is not affordable. Like all screening, the EE method provides qualitative sensitivity analysis measures, i.e. measures which allow the identification of non-influential inputs or which allow to rank the input factors in order of importance, but do not quantify exactly the relative importance of the inputs.\n\nTo exemplify the EE method, let us assume to consider a mathematical model with formula_1 input factors. Let formula_2 be the output of interest (a scalar for simplicity):\n\nThe original EE method of Morris provides two sensitivity measures for each input factor:\n\n\nThese two measures are obtained through a design based on the construction of a series of trajectories in the space of the inputs, where inputs are randomly moved One-At-a-Time (OAT).\nIn this design, each model input is assumed to vary across formula_6 selected levels in the space of the input factors. The region of experimentation formula_7 is thus a formula_8-dimensional formula_6-level grid.\n\nEach trajectory is composed of formula_10 points since input factors move one by one of a step formula_11 in formula_12 while all the others remain fixed.\n\nAlong each trajectory the so-called \"elementary effect\" for each input factor is defined as:\n\nwhere formula_14 is any selected value in formula_15 such that the transformed point is still in formula_15 for each index formula_17\n\nformula_18 elementary effects are estimated for each input formula_19 by randomly sampling formula_18 points formula_21. \n\nUsually formula_18 ~ 4-10, depending on the number of input factors, on the computational cost of the model and on the choice of the number of levels formula_23, since a high number of levels to be explored needs to be balanced by a high number of trajectories, in order to obtain an exploratory sample. It is demonstrated that a convenient choice for the parameters formula_23 and formula_11 is formula_23 even and formula_11 equal to formula_28, as this ensures equal probability of sampling in the input space.\n\nIn case input factors are not uniformly distributed, the best practice is to sample in the space of the quantiles and to obtain the inputs values using inverse cumulative distribution functions. Note that in this case formula_11 equals the step taken by the inputs in the space of the quantiles.\n\nThe two measures formula_4 and formula_5 are defined as the mean and the standard deviation of the distribution of the elementary effects of each input:\n\nThese two measures need to be read together (e.g. on a two-dimensional graph) in order to rank input factors in order of importance and identify those inputs which do not influence the output variability. Low values of both formula_4 and formula_5 correspond to a non-influent input.\n\nAn improvement of this method was developed by Campolongo et al. who proposed a revised measure formula_36, which on its own is sufficient to provide a reliable ranking of the input factors. The revised measure is the mean of the distribution of the absolute values of the elementary effects of the input factors:\nThe use of formula_36 solves the problem of the effects of opposite signs which occurs when the model is non-monotonic and which can cancel each other out, thus resulting in a low value for formula_4.\n\nAn efficient technical scheme to construct the trajectories used in the EE method is presented in the original paper by Morris while an improvement strategy aimed at better exploring the input space is proposed by Campolongo et al..\n"}
{"id": "70432", "url": "https://en.wikipedia.org/wiki?curid=70432", "title": "Emic and etic", "text": "Emic and etic\n\nIn anthropology, folkloristics, and the social and behavioral sciences, emic and etic refer to two kinds of field research done and viewpoints obtained: \"emic,\" from within the social group (from the perspective of the subject) and \"etic,\" from outside (from the perspective of the observer).\n\n\"The emic approach investigates how local people think\" (Kottak, 2006): How they perceive and categorize the world, their rules for behavior, what has meaning for them, and how they imagine and explain things. \"The etic (scientist-oriented) approach shifts the focus from local observations, categories, explanations, and interpretations to those of the anthropologist. The etic approach realizes that members of a culture often are too involved in what they are doing... to interpret their cultures impartially. When using the etic approach, the ethnographer emphasizes what he or she considers important.\"\n\nAlthough emics and etics are sometimes regarded as inherently in conflict and one can be preferred to the exclusion of the other, the complementarity of emic and etic approaches to anthropological research has been widely recognized, especially in the areas of interest concerning the characteristics of human nature as well as the form and function of human social systems.\nEmic and etic approaches of understanding behavior and personality fall under the study of cultural anthropology. Cultural anthropology states that people are shaped by their cultures and their subcultures, and we must account for this in the study of personality. One way is looking at things through an emic approach. This approach \"is culture specific because it focuses on a single culture and it is understood on its own terms.\" As explained below, the term \"emic\" originated from the specific linguistic term \"phonemic\", from \"phoneme\", which is a language-specific way of abstracting speech sounds.\nWhen these two approaches are combined, the “richest” view of a culture or society can be understood. On its own, an emic approach would struggle with applying overarching values to a single culture. The etic approach is helpful in enabling researchers to see more than one aspect of one culture, and in applying observations to cultures around the world.\n\nThe terms were coined in 1954 by linguist Kenneth Pike, who argued that the tools developed for describing linguistic behaviors could be adapted to the description of any human social behavior. As Pike noted, social scientists have long debated whether their knowledge is objective or subjective. Pike's innovation was to turn away from an epistemological debate, and turn instead to a methodological solution. \"Emic\" and \"etic\" are derived from the linguistic terms phonemic and phonetic respectively, which are in turn derived from Greek roots. The possibility of a truly objective description was discounted by Pike himself in his original work; he proposed the emic/etic dichotomy in anthropology as a way around philosophic issues about the very nature of objectivity.\n\nThe terms were also championed by anthropologists Ward Goodenough and Marvin Harris with slightly different connotations from those used by Pike. Goodenough was primarily interested in understanding the culturally specific meaning of specific beliefs and practices; Harris was primarily interested in explaining human behavior.\n\nPike, Harris, and others have argued that cultural \"insiders\" and \"outsiders\" are equally capable of producing emic \"and\" etic accounts of their culture. Some researchers use \"etic\" to refer to objective or outsider accounts, and \"emic\" to refer to subjective or insider accounts.\n\nMargaret Mead was an anthropologist who studied the patterns of adolescence in Samoa. She discovered that the difficulties and the transitions that adolescents faced are culturally influenced. The hormones that are released during puberty can be defined using an etic framework, because adolescents globally have the same hormones being secreted. However, Mead concluded that how adolescents respond to these hormones is greatly influenced by their cultural norms. Through her studies, Mead found that simple classifications about behaviors and personality could not be used because peoples’ cultures influenced their behaviors in such a radical way. Her studies helped create an emic approach of understanding behaviors and personality. Her research deduced that culture has a significant impact in shaping an individual’s personality.\nCarl Jung, a Swiss psychoanalyst, is a researcher who took an etic approach in his studies. Jung studied mythology, religion, ancient rituals, and dreams leading him to believe that there are archetypes that can be identified and used to categorize people’s behaviors. Archetypes are universal structures of the collective unconscious that refer to the inherent way people are predisposed to perceive and process information. The main archetypes that Jung studied were the persona (how people choose to present themselves to the world), the animus/ anima (part of people experiencing the world in viewing the opposite sex, that guides how they select their romantic partner), and the shadow (dark side of personalities because people have a concept of evil. Well-adjusted people must integrate both good and bad parts of themselves). Jung looked at the role of the mother and deduced that all people have mothers and see their mothers in a similar way; they offer nurture and comfort. His studies also suggest that “infants have evolved to suck milk from the breast, it is also the case that all children have inborn tendencies to react in certain ways.” This way of looking at the mother is an etic way of applying a concept cross-culturally and universally.\n\nEmic and etic approaches are important to understanding personality because problems can arise “when concepts, measures, and methods are carelessly transferred to other cultures in attempts to make cross- cultural generalizations about personality.” It is hard to apply certain generalizations of behavior to people who are so diverse and culturally different. One example of this is the F-scale (Macleod). The F-scale, which was created by Theodor Adorno, is used to measure Authoritarian Personality, which can, in turn, be used to predict prejudiced behaviors. This test, when applied to Americans accurately depicts prejudices towards black individuals. However, when a study was conducted in South Africa using the F-Scale, (Pettigrew and Friedman) results did not predict any anti-Black prejudices. This study used emic approaches of study by conducting interviews with the locals and etic approaches by giving participants generalized personality tests.\n\nOther explorations of the differences between reality and humans' models of it:\n\n\n"}
{"id": "48919235", "url": "https://en.wikipedia.org/wiki?curid=48919235", "title": "Fire urgency estimator in geosynchronous orbit", "text": "Fire urgency estimator in geosynchronous orbit\n\nFire urgency estimator in geosynchronous orbit (FUEGO) is a proposed method for early detection and evaluation of wildfires using a system of drones and satellites in geosyncronous orbit equipped with infrared sensors. Use of drones has been described as a potential problem due to Federal Aviation Administration's policy concerning use of airspace during fires.\n\nThe concept was published in the journal Remote Sensing. The research is led by Carlton Pennypacker who is an astrophysicist at UC Berkeley.\n\n"}
{"id": "41286907", "url": "https://en.wikipedia.org/wiki?curid=41286907", "title": "Food grading", "text": "Food grading\n\nFood grading involves the inspection, assessment and sorting of various foods regarding quality, freshness, legal conformity and market value. Food grading often occurs by hand, in which foods are assessed and sorted. Machinery is also used to grade foods, and may involve sorting products by size, shape and quality. For example, machinery can be used to remove spoiled food from fresh product.\n\nBeef grading in the United States is performed by the United States Department of Agriculture's (USDA) Agricultural and Marketing Service. There are eight beef quality grades, with U.S. Prime being the highest grade and U.S. Canner being the lowest grade. Beef grading is a complex process.\n\nIn beer grading, the letter \"X\" is used on some beers, and was traditionally a mark of beer strength, with the more Xs the greater the strength. Some sources suggest that the origin of the mark was in the breweries of medieval monasteries Another plausible explanation is contained in a treatise entitled \"The Art of Brewing\" published in London in 1829. It says; \"\"The duties on ale and beer, which were first imposed in 1643... at a certain period, in distinguishing between small beer and strong, all ale or beer, sold at or above ten shillings per barrel, was reckoned to be\" strong \"and was, therefore, subjected to a higher duty. The cask which contained this strong beer was then first marked with an X signifying ten; and hence the present quack-like denominations of XX (double X) and XXX (treble X) on the casks and accounts of the strong-ale brewers\".\n\nIn mid-19th century England, the use of \"X\" and other letters had evolved into a standardised grading system for the strength of beer. Today, it is used as a trade mark by a number of brewers in the United Kingdom, the Commonwealth and the United States.\n\nEuropean Bitterness Units scale, often abbreviated as EBU, is a scale for measuring the perceived bitterness of beer, with lower values being generally \"less bitter\" and higher values \"more bitter\". The scale and method are defined by the European Brewery Convention, and the numerical value should be the same as of the International Bitterness Units scale (IBU), defined in co-operation with the American Society of Brewing Chemists. However, the exact process of determining EBU and IBU values differs slightly, which may in theory result with slightly smaller values for EBU than IBU.\n\nThe International Bittering Units scale, or simply IBU scale, provides a measure of the bitterness of beer, which is provided by the hops used during brewing. Bittering units are measured through the use of a spectrophotometer and solvent extraction.\n\nSeveral grades of coconut milk exist: from thick at 20-22% fat to thin at 5-7% fat level.\n\nCoffee growers, traders, and roasters grade beans based on size, color, and a variety of other characteristics. Coffees of exceptional quality are traded as \"specialty coffees\" and fetch a higher price in the international market.\n\nAfter the roast, Coffee grading involves assessment of roasted coffee seed colorization and then labeling as light, medium light, medium, medium dark, dark, or very dark. A more accurate method of discerning the degree of roast involves measuring the reflected light from roasted seeds illuminated with a light source in the near infrared spectrum. This elaborate light meter uses a process known as spectroscopy to return a number that consistently indicates the roasted coffee's relative degree of roast or flavor development.\n\nIn the United States, egg grading is performed by the USDA, and is based upon the interior quality of the egg (see Haugh unit) and the appearance and condition of the egg shell. Eggs of any quality grade may differ in weight (size). Egg grading is performed by candling, which involves observing the interior of eggs by placing them in front of a bright light.\n\nGuar gum grading involves analysis for coloration, viscosity, moisture, granulation, protein content and insolubles ash.\n\nHoney grading in the United States is performed voluntarily based upon USDA standards (USDA does offer inspection and grading \"as on-line (in-plant) or lot inspection...upon application, on a fee-for-service basis.\") . Honey is graded based upon a number of factors, including water content, flavor and aroma, absence of defects and clarity. Honey is also classified by color though it is not a factor in the grading scale. U.S. honey grade scales are Grade A, Grade B, Grade C and Grade substandard.\n\nIn the U.S., lobster grading involves denoting lobsters as new-shell, hard-shell or old-shell, and because lobsters which have recently shed their shells are the most delicate, there is an inverse relationship between the price of American lobster and its flavour. New-shell lobsters have paper-thin shells and a worse meat-to-shell ratio, but the meat is very sweet. However, the lobsters are so delicate that even transport to Boston almost kills them, making the market for new-shell lobsters strictly local to the fishing towns where they are offloaded. Hard-shell lobsters with firm shells, but with less sweet meat, can survive shipping to Boston, New York and even Los Angeles, so they command a higher price than new-shell lobsters. Meanwhile, old-shell lobsters, which have not shed since the previous season and have a coarser flavour, can be air-shipped anywhere in the world and arrive alive, making them the most expensive. One seafood guide notes that an eight-dollar lobster dinner at a restaurant overlooking fishing piers in Maine is consistently delicious, while \"the eighty-dollar lobster in a three-star Paris restaurant is apt to be as much about presentation as flavor\".\n\nFollowing an effort from the International Maple Syrup Institute (IMSI) and many maple syrup producer associations, both Canada and the United States have altered their laws regarding the classification of maple syrup to be uniform. Whereas in the past each state or province had their own laws on the classification of maple syrup, now those laws define a unified grading system. This had been a work in progress for several years, and most of the finalization of the new grading system was made in 2014. The Canadian Food Inspection Agency announced in the Canada Gazette on 28 June 2014 that rules for the sale of maple syrup would be amended to include new descriptors, at the request of the IMSI.\n\nAs of December 31, 2014, the Canadian Food Inspection Agency (CFIA) and as of March 2, 2015, the United States Department of Agriculture (USDA) Agricultural Marketing Service (AMS) issued revised standards on the classification of maple syrup as follows:\n\n\nAs long as maple syrup does not have an off-flavor and is of a uniform color and clean and free from turbidity and sediment, it can be labelled as one of the A grades. If it exhibits any of these problems, it does not meet Grade A requirements and must be labelled as Processing Grade maple syrup and may not be sold in containers smaller than 5 gallons. If maple syrup does not meet the requirements of Processing Grade maple syrup (including a fairly characteristic maple taste), it is classified as Substandard.\nAs of February 2015, this grading system has been accepted and made law by most maple-producing states and provinces, other than Ontario, Quebec, and Ohio. Vermont, in an effort to \"jump-start\" the new grading regulations, adopted the new grading system as of January 1, 2014, after the grade changes passed the Senate and House in 2013. Maine passed a bill to take effect as soon as both Canada and the United States adopted the new grades. They are allowing a one-year grace period. In New York, the new grade changes became law on January 1, 2015, with a one-year grace period. New Hampshire did not require legislative approval and so the new grade laws became effective as of December 16, 2014, and producer compliance was required as of January 1, 2016.\n\nGolden and Amber grades typically have a milder flavor than Dark and Very dark, which are both dark and have an intense maple flavor. The darker grades of syrup are used primarily for cooking and baking, although some specialty dark syrups are produced for table use. Syrup harvested earlier in the season tends to yield a lighter color. With the new grading system, the classification of maple syrup depends ultimately on its internal transmittance at 560 nm wavelength through a 10 mm sample. Golden has to have more than 75 percent transmittance, Amber has to have 50.0 to 74.9 percent transmittance, Dark has to have 25.0 to 49.9 percent transmittance, and Very Dark is any product less than 25.0 percent transmittance.\n\nIn Canada, maple syrup was classified prior to December 31, 2014, by the Canadian Food Inspection Agency (CFIA) as one of three grades, each with several color classes: Canada No. 1, including Extra Light, Light, and Medium; No. 2 Amber; and No. 3 Dark or any other ungraded category. Producers in Ontario or Québec may have followed either federal or provincial grading guidelines. Québec's and Ontario's guidelines differed slightly from the federal: there were two \"number\" categories in Québec (Number 1, with four color classes, and 2, with five color classes). As in Québec, Ontario's producers had two \"number\" grades: 1, with three color classes; and 2, with one color class, which was typically referred to as \"Ontario Amber\" when produced and sold in that province only. A typical year's yield for a maple syrup producer will be about 25 to 30 percent of each of the #1 colors, 10 percent #2 Amber, and 2 percent #3 Dark. Producers in Quebec and Ontario may follow either federal or provincial grading guidelines, which differ slightly.\n\nThe United States used (some states still do, as they await state regulation) different grading standards. Maple syrup was divided into two major grades: Grade A and Grade B. Grade A was further divided into three subgrades: Light Amber (sometimes known as Fancy), Medium Amber, and Dark Amber. The Vermont Agency of Agriculture Food and Markets used a similar grading system of color, and is roughly equivalent, especially for lighter syrups, but using letters: \"AA\", \"A\", etc. The Vermont grading system differed from the US system in maintaining a slightly higher standard of product density (measured on the Baumé scale). New Hampshire maintained a similar standard, but not a separate state grading scale. The Vermont-graded product had 0.9 percent more sugar and less water in its composition than US-graded. One grade of syrup not for table use, called commercial or Grade C, was also produced under the Vermont system. Vermont inspectors enforce strict syrup grading regulations, and can fine producers up to US$1000 for labelling syrup incorrectly.\n\nIn the United States, there are two grades of milk, with Grade A primarily used for direct sales and consumption in stores, and Grade B used for indirect consumption, such as in cheese making or other processing.\n\nThe differences between the two grades are defined in the Wisconsin administrative code for Agriculture, Trade, and Consumer Protection, chapter 60. Grade B generally refers to milk that is cooled in milk cans, which are immersed in a bath of cold flowing water that typically is drawn up from an underground water well rather than using mechanical refrigeration.\n\nThe USDA has established the following grades for Florida oranges, which primarily apply to oranges sold as fresh fruit: US Fancy, US No. 1 Bright, US No. 1, US No. 1 Golden, US No. 1 Bronze, US No. 1 Russet, US No. 2 Bright, US No. 2, US No. 2 Russet, and US No. 3. The general characteristics graded are color (both hue and uniformity), firmness, maturity, varietal characteristics, texture, and shape. Fancy, the highest grade, requires the highest grade of color and an absence of blemishes, while the terms Bright, Golden, Bronze, and Russet concern solely discoloration.\n\nPea grading involves sorting peas by size, in which smallest peas are graded as the highest quality for their tenderness. Brines may be used, in which peas are floated in them, from which their density can be determined.\n\nIn the U.S., potato grading for Idaho potatoes is performed in which No. 1 potatoes are the highest quality and No. 2 are rated as lower in quality due to their appearance (e.g. blemishes or bruises, pointy ends). Density assessment can be performed by floating them in brines. High density potatoes are desirable in the production of dehydrated mashed potatoes, potato crisps and french fries.\n\nThe main criteria used by many countries and millers in rice grading are degree of milling, appearance (color), damaged (broken) and percentage of chalky kernels. In the United States rice is marketed according to three main properties size, color and condition (kernels damage), these properties are directly related to quality, milling percentage and other processing conditions. All properties are considered important in grading. For instance, chalky kernels are not desirable because they give lower milling yields after processing and easily break during handling.\n\nIn Sri Lanka, cinnamon grading is performed by dividing cinnamon quills into four groups, which are then further divided into specific grades.\n\nSeveral vanilla fruit grading systems are in use. Each country which produces vanilla has its own grading system, and individual vendors, in turn, sometimes use their own criteria for describing the quality of the fruits they offer for sale.\n\nIn the western black tea industry, tea leaf grading is the process of evaluating products based on the quality and condition of the tea leaves themselves. The highest grades are referred to as \"orange pekoe\", and the lowest as \"fannings\" or \"dust\". This grading system is based upon the size of processed and dried black tea leaves. Despite a purported Chinese origin, these grading terms are typically used for teas from Sri Lanka, India and countries other than China; they are not generally known within Chinese-speaking countries.\n\nBlack tea grading is usually based upon one of four scales of quality. Whole-leaf teas are the highest quality, followed by broken leaves, fannings, and dusts. Whole-leaf teas are produced with little or no alteration to the tea leaf. This results in a finished product with a coarser texture than that of bagged teas. Whole-leaf teas are widely considered the most valuable, especially if they contain leaf tips. Broken leaves are commonly sold as medium-grade loose teas. Smaller broken varieties may be included in tea bags.\n\nRooibos grades are largely related to the proportion of \"needle\" or leaf to stem content in the mix. A higher leaf content will result in a darker liquor, richer flavour and less \"dusty\" aftertaste. The high-grade rooibos is exported and does not reach local markets, with major consumers being in the EU, particularly Germany, where it is used in creating flavoured blends for loose-leaf tea markets. In development within South Africa are a small number of specialty tea companies producing similar blends.\n\nThere are two basic grades of carrageenan, refined carrageenan (RC) and semi-refined carrageenan (SRC). In the United States, RC and SRC are both labeled as carrageenan. In the European Union, RC is designated by the E number E-407, and SRC is E-407a. RC has a 2% maximum for acid-insoluble material and is produced through an alcohol precipitation process or potassium chloride gel press process. SRC contains a much higher level of cellulosic content and is produced in a less complex process. Indonesia, the Philippines, and Chile are three main sources of raw material and extracted carrageenan.\n\nLye is used to cure foods such as lutefisk, olives (making them less bitter), canned mandarin oranges, hominy, lye rolls, century eggs, and pretzels. It is also used as a tenderizer in the crust of baked Cantonese mooncakes, and in lye-water \"zongzi\" (glutinous rice dumplings wrapped in bamboo leaves), chewy southern Chinese noodles popular in Hong Kong and southern China, and Japanese ramen noodles. In the United States, food-grade lye must meet the requirements outlined in the Food Chemicals Codex (FCC), as prescribed by the U.S. Food and Drug Administration (FDA). Lower grades of lye are commonly used as drain or oven cleaner. Such grades should not be used for food preparation, as they may contain impurities harmful to human health.\n\nSodium bisulphate is used as a food additive to leaven cake mixes (make them rise) as well as being used in meat and poultry processing and most recently in browning prevention of fresh-cut produce. The food-grade product meets the requirements set out in the Food Chemicals Codex. It is denoted by E number E514ii in the EU and is approved for use in Australia and New Zealand where it is listed as additive 514. Food-grade sodium bisulfate is used in a variety of food products, including beverages, dressings, sauces, and fillings.\n\n\n"}
{"id": "550948", "url": "https://en.wikipedia.org/wiki?curid=550948", "title": "Genetic sexual attraction", "text": "Genetic sexual attraction\n\nGenetic sexual attraction is a theorised concept in which a strong sexual attraction may develop between close blood relatives who first meet as adults. There is no direct evidence for \"genetic attraction\" being an actual phenomenon and the theory has been criticized as pseudoscience.\n\nThe term was coined in the US in the late 1980s by Barbara Gonyo, the founder of \"Truth Seekers In Adoption\", a Chicago-based support group for adoptees and their new-found relatives. She developed sexual feelings for her son when she met him after he was adopted away, but he did not want to be part of any such contact.\n\nPeople tend to select mates who are like themselves. This holds for both physical appearance and mental traits. People commonly rank faces similar to their own as more attractive, trustworthy, etc. than average. However, Bereczkei (2004) attributes this in part to childhood imprinting on the opposite-sex parent. The study also reported a correlation of 0.233 for extraversion and 0.235 for inconsistency (using Eysenck's Personality Inventory). A review of many previous studies found these numbers to be quite common.\n\nBecause many traits are at least partially determined by genetics, genetic sexual attraction is presumed according to the theory to occur as a consequence of genetic relatives meeting as adults, typically as a consequence of adoption. However this is a very rare consequence of adoptive reunions.\n\nIncest is extremely rare between people raised together in early childhood due to a reverse sexual imprinting known as the Westermarck effect, which desensitizes them to later close sexual attraction. It is hypothesized that this effect evolved to prevent inbreeding.\n\nAlthough reported frequently as anecdote in the field of psychology, there are no studies showing that people are sexually attracted to those genetically similar to them. Studies of MHC genes show that unrelated people are less attracted to those genetically similar to them. However, in mice, this lack of attraction can be reversed by adoption. Whilst it has been documented that sexual attraction can occur between related individuals in some cases, it is not clear that calling this attraction GSA is appropriate.\n\nCritics of the theory have called it pseudoscience. Amanda Marcotte of \"Salon\" has stated that the term is nothing but a an attempt at sounding scientific while trying to minimize the taboo of incest. She also expressed that many news outlets have handled reports of the subject poorly by repeating what the defenders of the theory have said as opposed to actually looking into the research on the supossed phenomenon. She states that most of the publications which have choosen to run stories of couples speaking about \"genetic sexual attraction\" are not legetimate news sources and that one of the blogs which were written by a woman in an incestious relationship simply reads like a story of a young girl who's been groomed by her father.\n"}
{"id": "26895032", "url": "https://en.wikipedia.org/wiki?curid=26895032", "title": "Gustilo open fracture classification", "text": "Gustilo open fracture classification\n\nThe Gustilo open fracture classification system is the most commonly used classification system for open fractures. It was created by Ramón Gustilo and Anderson, and then further expanded by Gustilo, Mendoza, and Williams.\n\nThis system uses the amount of energy, the extent of soft-tissue injury and the extent of contamination for determination of fracture severity. Progression from grade 1 to 3C implies a higher degree of energy involved in the injury, higher soft tissue and bone damage and higher potential for complications. It is important to recognize that a Gustilo score of grade 3C implies vascular injury as well as bone and connective-tissue damage.\n\nThere are many discussions regarding the inter-observer reliability of this classification system. Different studies have shown inter-observer reliability of approximately 60% (ranging from 42% to 92%), representing poor-to-moderate agreement of scale grading between health-care professionals. This is due to much of the criteria being at risk of observer errors, and is a known liability of this scaling system. However, this classification is simple and hence easy to use, and is generally able to predict prognostic outcomes and guide treatment regimes. Generally, the higher the grading of Gustillo classification, the higher the rate of infection and complications; any Guistilo classification rating should still be interpreted with caution due to observer errors before any definite therapeutic plans are made. \n\nAlthough this classification system has a fairly good ability to predict fracture outcomes, it is not perfect. The Gustillo classification does not take into account the viability and death of soft tissues over time which can affect the outcome of the injury. Besides, the number of the underlying medical illnesses of the patient also affects the outcome. Whether the timing of wound debridement, soft tissue coverage, and bone have any benefits on the outcome is also questionable. Besides, different types of bones have different rates of infection because they are covered by different amounts of soft tissues. Gustilo initially does not recommend early wound closure and early fixation for Grade III fractures. However newer studies have shown that early wound closure and early fixation reduces infection rates, promotes fracture healing and early restoration of function. Therefore, assessment of all open fractures should include the mechanism of injury, the appearance of soft tissues, the likely levels of bacterial contamination and the specific characteristics of the fractures. Accurate assessment of the fracture can only be performed inside an operating theatre. \n\nFor more comprehensive prognosis purposes other classification systems, such as the Sickness Impact Profile (as a health status measure), Mangled Extremity Severity Score (MESS) and Limb Salvage Index (LSI) (decision to amputate or salvage a limb), have been devised.\n\nIn 1976, Gustilo and Anderson refined the early classification system proposed by Veliskasis in 1959. An early study conducted by Gustilo in 1976 showed that primary closures with prophylactic antibiotics of Type I and type II fractures reduced the risk of infection by 84.4%. Meanwhile, early internal fixation and primary closure of the wound in Type III fractures have a greater risk of getting osteomyelitis. However, Type III fractures occur in 60% of all the open fracture cases. Infection of the Type III fractures is observed in 10% to 50% of the time. Therefore, in 1984, Gustilo subclassified Type III fractures into A, B, and C with the aim of guiding the treatment of open fractures, communication and research, and to predict outcomes. Based of the results of the previous studies, Gustilo initially recommended therapeutic irrigation and surgical debridement for all fractures with primary closure for Type I and II fractures; secondary closure without internal fixation for Type III fractures. However, soon after that, he recommended internal fixation devices for Type III fractures.\n"}
{"id": "41322236", "url": "https://en.wikipedia.org/wiki?curid=41322236", "title": "Hui Liu", "text": "Hui Liu\n\nDr. Hui Liu () is a Chinese American professor and an entrepreneur in the field of wireless and satellite communications. He is a prolific researcher with more than 200 scholarly articles and 2 textbooks, and a creative innovator with 67 awarded patents in areas ranging from wireless systems, signal processing, satellite networks, to machine learning . He has more than 12,000 paper citations and an H-index of 56 as of 2018. Dr. Liu is also one of the principal designers of three industrial standards on cellular networks, terrestrial broadcasting, and satellite communications, respectively., \n\nLiu graduated from Fudan University of Shanghai, China with B.Sc degree in electrical engineering in 1988 and obtained master's degree at Portland State University. By 1995 he completed his Ph.D. in the same field as well from the University of Texas at Austin. From September of the same year till July 1998 Liu worked at the University of Virginia as a tenure-track Assistant Professor. After that, he joined the Electrical Engineering department of University of Washington where he became a tenure Full Professor and the Associate Chair of Research. From 2013-2016, he was the ZhiYuan Chair Professor and the Associate Dean at School of Electronic, Information & Electrical Engineering (SEIEE) at Shanghai Jiao Tong University.\n. He worked as a chief scientist at Cwill Telecom and was one of the principal designers of the 3G TD-SCDMA technologies. He founded Adaptix in 2000 and pioneered the development of OFDMA-based mobile broadband cellular networks (mobile WiMAX and 4G LTE) . He is the President and CTO of Silkwave Holdings Limited .\n\n\n\n\nIn 1997 the National Science Foundation awarded Liu with the \"NSF Early Faculty CAREER Award\"; In 2000 he became a recipient of the \"Young Investigator Award\" from the Office of Naval Research. During the same year, with the help of Artech House he published his \"Signal Processing Applications in CDMA Communications\" book and five years later published another one which was called \"OFDM-Based Broadband Wireless Networks – Design and Optimization\". Before he published his second book through Wiley he was twice nominated for the IEEE \"Best Paper Award\" and became the recipient of the \"Gold Prize Patent Award\" for his patent on 3G TD-SCDMA . Liu has received multiple IEEE \"Best Conference Paper Award\" and in 2008 he was honored to be an IEEE Fellow \"for contributions to global standards for broadband cellular and mobile broadcasting\".\n"}
{"id": "52744", "url": "https://en.wikipedia.org/wiki?curid=52744", "title": "Jean-Charles de Borda", "text": "Jean-Charles de Borda\n\nJean-Charles, chevalier de Borda (4 May 1733 – 19 February 1799) was a French mathematician, physicist, and sailor.\n\nBorda was born in the city of Dax to Jean‐Antoine de Borda and Jeanne‐Marie Thérèse de Lacroix. In 1756, Borda wrote \"Mémoire sur le mouvement des projectiles\", a product of his work as a military engineer. For that, he was elected to the French Academy of Sciences in 1764.\n\nBorda was a mariner and a scientist, spending time in the Caribbean testing out advances in chronometers. Between 1777 and 1778, he participated in the American Revolutionary War. In 1781, he was put in charge of several vessels in the French Navy. In 1782, he was captured by the English, and was returned to France shortly after. He returned as an engineer in the French Navy, making improvements to waterwheels and pumps. He was appointed as France's Inspector of Naval Shipbuilding in 1784, and with the assistance of the naval architect Jacques-Noël Sané in 1786 introduced a massive construction programme to revitalise the French navy based on the standard designs of Sané.\n\nIn 1770, Borda formulated a ranked preferential voting system that is referred to as the Borda count. The French Academy of Sciences used Borda's method to elect its members for about two decades until it was quashed by Napoleon Bonaparte who insisted that his own method be used after he became president of the Académie in 1801. The Borda count is in use today in some academic institutions, competitions and several political jurisdictions. The Borda count has also served as a basis for other methods such as the Quota Borda system and Nanson's method.\n\nIn 1778, he published his method of reducing Lunar Distances for computing the longitude, still regarded as the best of several similar mathematical procedures for navigation and position-fixing in pre-chronometer days; and used, for example, by Lewis and Clarke to measure their latitude and longitude during their exploration of the North-western United States.\n\nAnother of his contributions is his construction of the standard \"metre\", basis of the metric system to correspond to the measurements of Delambre.\nAs an instrument maker, he improved the reflecting circle (invented by Tobias Mayer) and the repeating circle (invented by his assistant, Etienne Lenoir), the latter used to measure the meridian arc from Dunkirk to Barcelona by Delambre and Méchain.\n\nWith the advent of the metric system after the French Revolution it was decided that the quarter circle should be divided into 100 degrees instead of 90 degrees, and the degree into 100 seconds instead of 60 seconds. This required the calculation of trigonometric tables and logarithms corresponding to the new size of the degree and instruments for measuring angles in the new system.\n\nBorda constructed instruments for measuring angles in the new units (the instrument could no longer be called a \"sextant\") which was later used in the measurement of the meridian between Dunkirk and Barcelona by Delambre to determine the length of the metre. The tables of logarithms of sines, secants, and tangents were also required for the purposes of navigation. Borda was an enthusiast for the metric system and constructed tables of these logarithms starting in 1792 but their publication was delayed until after his death and only published in the Year 9 (1801) as \"Tables of Logarithms of sines, secants, and tangents, co-secants, co-sines, and co-tangents for the Quarter of the Circle divided into 100 degrees, the degree into 100 minutes, and the minute into 100 seconds\" to ten decimals, and including his tables of logarithms to 7 decimals from 10,000 to 100,000 with tables for obtaining results to 10 decimals.\n\nThe division of the degree into hundredths was accompanied by the division of the day into 10 hours of 100 minutes and maps were required to show the new degrees of latitude and longitude. The Republican Calendar was abolished by Napoleon in 1806, but the 400-degree circle lived on as the Gradian.\n\n\n\n"}
{"id": "54556438", "url": "https://en.wikipedia.org/wiki?curid=54556438", "title": "Komar superpotential", "text": "Komar superpotential\n\nIn general relativity, the Komar superpotential, corresponding to the invariance of the Hilbert-Einstein Lagrangian formula_1, is the tensor density:\n\nassociated with a vector field formula_3, and where formula_4 denotes covariant derivative with respect to the Levi-Civita connection.\n\nThe Komar two-form:\n\nwhere formula_6 denotes interior product, generalizes to an arbitrary vector field formula_7 the so-called above Komar superpotential, which was originally derived for timelike Killing vector fields.\n\n"}
{"id": "365303", "url": "https://en.wikipedia.org/wiki?curid=365303", "title": "Korabl-Sputnik 2", "text": "Korabl-Sputnik 2\n\nKorabl-Sputnik 2 ( meaning \"Ship-Satellite 2\"), also known incorrectly as Sputnik 5 in the West, was a Soviet artificial satellite, and the third test flight of the Vostok spacecraft. It was the first spaceflight to send animals into orbit and return them safely back to Earth. Launched on 19 August 1960, it paved the way for the first human orbital flight, Vostok 1, which was launched less than eight months later.\n\nKorabl-Sputnik 2 was the second attempt to launch a Vostok capsule with dogs on board. The first try on 28 July, carrying a pair named Bars (Snow Leopard aka. Chaika (Seagull)) and Lisichka (Foxie)), had been unsuccessful after the Blok G strap-on suffered a fire and breakdown in one of the combustion chambers, followed by its breaking off of the booster 19 seconds after launch. Around 30 seconds, the launch vehicle disintegrated, the core and strap-ons flying in random directions and crashing into the steppe. Flight controllers sent a command to jettison the payload shroud and separate the descent module, but due to the low altitude, the parachutes only deployed partially, and the dogs were killed on impact with the ground. It was believed that the combustion chamber disintegration was due to longitudinal vibrations. This created a considerable uproar, as the problem, which had plagued earlier 8K72 launches, had supposedly been corrected. It was ultimately traced to deficient manufacturing practices at the R-7 assembly plant. The accident also encouraged the development of an ejector seat for the cosmonaut to escape from the capsule in the event of a launch failure, since the parachutes in the descent module would not be able to open properly until around 40 seconds into launch. This occurred, ironically, one day before the US program suffered a serious setback with the loss of a Mercury capsule.\n\nA commonly circulated film clip depicting a Vostok booster lifting followed by the movement of its shadow on the ground is often assumed to be from Vostok 1's launch; however, it was actually the ill-fated flight of 28 July 1960.\n\nThe launch of Korabl-Sputnik 2 occurred on 19 August 1960, using a Vostok-L carrier rocket. Official sources reported the launch time to have been 08:44:06 UTC; however, Sergei Voevodin gave it as 08:38:24. A radio station in Bonn, West Germany, was among the first to pick up signals from the spacecraft, which were confirmed on the third orbit by a Swedish radio station.\n\nThe spacecraft carried two dogs, Belka and Strelka, 40 mice, two rats and a variety of plants, as well as a television camera, which took images of the dogs. One of its objectives was to intercept the US Echo 1A satellite, which was a passive, nitrogen-filled balloon covered in reflective foil and thus highly visible as it passed overhead. Korabl-Sputnik 2 passed near Echo on the second orbit, and the radio system returned audio of the dogs barking as they saw it out the window. The spacecraft returned to Earth at 06:00:00 UTC on 20 August, the day after its launch. Telemetry revealed that one dog had suffered seizures during the fourth orbit, and it was decided to limit the first manned flight to three orbits. All of the animals were recovered safely, and a year later Strelka had a litter of puppies, one of which was sent to First Lady of the US Jacqueline Kennedy as a goodwill present from the Soviet Union. President Kennedy's advisers initially opposed taking the dog for fear that the Soviets might have planted microphones in its body to listen in on national defense meetings.\n\nStrelka and Belka were both taxidermied after their deaths and placed on display in the Moscow Museum of Space and Aeronautics.\n\n"}
{"id": "28285943", "url": "https://en.wikipedia.org/wiki?curid=28285943", "title": "LLNL RISE process", "text": "LLNL RISE process\n\nThe LLNL RISE process was an experimental shale oil extraction technology developed by the Lawrence Livermore National Laboratory. The name comes from the abbreviation of the Lawrence Livermore National Laboratory and words 'rubble in situ extraction'.\n\nLLNL RISE is a modified \"in situ\" extraction technology originally proposed by Rio Blanco Oil Shale Co. and developed by the Lawrence Livermore National Laboratory. It is classified as an internal combustion technology. The process was described in 1975 by Lewis A. E. and A. J. Rothman.\n\nIn the LLNL RISE process a part of the oil shale deposit (roughly 20% of the total deposit) is removed by the conventional mining technique. The remaining deposit is then broken up with explosives to increase porosity of the deposit. As a result, a large underground retort chamber by square and high is created. The retort chamber is ignited at the top. The combustion zone moves downward as an oxygen gas provided, similar to the process developed by the Occidental Petroleum. The heat causes retorting process converting kerogen in oil shale to oil shale gas and shale oil vapors. Some oil is collected at the bottom of the retort, other collected at the surface as vapors.\n\nThe process was never used commercially. It was tested by using experimental simulated retort with capacity of 6 tonnes of oil shale per day.\n"}
{"id": "9361141", "url": "https://en.wikipedia.org/wiki?curid=9361141", "title": "Lin Chao", "text": "Lin Chao\n\nProfessor Lin Chao is a Chinese Brazilian American evolutionary biologist and geneticist. Professor Chao gained his PhD in 1977 from the University of Massachusetts Amherst, as a student of Bruce R. Levin (now at Emory University), and was a NIH postdoctoral fellow at Princeton University in the laboratory of Edward C. Cox. He spent most of his career in the Department of Biology of the University of Maryland, College Park and is currently at the Ecology, Behavior and Evolution Section of the University of California, San Diego.\n\nProfessor Chao is best known for his early work on the evolution of bacteriocins, his demonstration of Muller’s ratchet in the RNA Virus Phi-6 and his work on sex in viruses. More recently, he was instrumental in the demonstration of the evolution of parasitic genetic elements in co-infecting bacteriophages and experimental tests of Fisher's geometric model. He argued that \"life is evolution by natural selection\". The approach generally used in his laboratory is called microbial experimental evolution.\n\nProfessor Chao is married to Camilla Rang, a fellow scientist, T-shirt designer and Swedish children's book author.\n\n"}
{"id": "27965789", "url": "https://en.wikipedia.org/wiki?curid=27965789", "title": "List of Fritz London Memorial Lectures", "text": "List of Fritz London Memorial Lectures\n\nThe Fritz London Memorial Lectures at Duke University invites scientists who impinge at one or more points upon the various fields of physics and chemistry to which Fritz London contributed. The series is partially supported by an endowment fund established by John Bardeen \"to perpetuate the memory of Fritz London, distinguished scientist and member of the Duke faculty from 1939 to the time of his death in 1954, and to promote research and understanding of Physics at Duke University and in the wider scientific community. \"\n\n"}
{"id": "24129448", "url": "https://en.wikipedia.org/wiki?curid=24129448", "title": "List of accelerator mass spectrometry facilities", "text": "List of accelerator mass spectrometry facilities\n\nThe following list of accelerator mass spectrometry facilities sets out the research centres which employ accelerator mass spectrometry (AMS).\n\nAccelerator mass spectrometry is an analytical technique that uses a full-sized particle accelerator as a big mass spectrometer.\n\n\n\n\n\n\n"}
{"id": "61794", "url": "https://en.wikipedia.org/wiki?curid=61794", "title": "List of diseases (X)", "text": "List of diseases (X)\n\nThis is a list of diseases starting with the letter \"X\".\n\n\n\n\n"}
{"id": "12983971", "url": "https://en.wikipedia.org/wiki?curid=12983971", "title": "List of lakes in Texas", "text": "List of lakes in Texas\n\nThe following is a list of reservoirs and lakes in the U.S. state of Texas.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "23615296", "url": "https://en.wikipedia.org/wiki?curid=23615296", "title": "List of prehistoric ostracod genera", "text": "List of prehistoric ostracod genera\n\nThis list of prehistoric ostracods is an attempt to create a comprehensive listing of all genera from the fossil record that have ever been considered to be members of the Ostracoda, excluding purely vernacular terms. The list includes all commonly accepted genera, but also genera that are now considered invalid, doubtful (\"nomina dubia\"), or were not formally published (\"nomina nuda\"), as well as junior synonyms of more established names, and genera that are no longer considered ostracods. \n\n"}
{"id": "14613702", "url": "https://en.wikipedia.org/wiki?curid=14613702", "title": "List of railway industry occupations", "text": "List of railway industry occupations\n\nThis is a list of railway industry occupations, but it also includes transient functional job titles according to activity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "676910", "url": "https://en.wikipedia.org/wiki?curid=676910", "title": "Lithophyte", "text": "Lithophyte\n\nLithophytes are plants that grow in or on rocks. Those that grow on rocks are also known as epipetric or epilithic plants. Lithophytes that grow on land feed off nutrients from rain water and nearby decaying plants, including their own dead tissue. Chasmophytes grow in fissures in rocks where soil or organic matter has accumulated.\n\nExamples of lithophytes include several \"Paphiopedilum\" orchids, ferns, many algae and liverworts. Species that only grow on rock or gravel are obligate lithophytes. Species that grow on rocky substrate and elsewhere are facultative lithophytes.\n\nAs nutrients tend to be rarely available to lithophytes or chasmophytes, many species of carnivorous plants can be viewed as being pre-adapted to life on rocks. By consuming prey, these plants can gather more nutrients than non-carnivorous lithophtytes. Examples include the pitcher plants \"Nepenthes campanulata\" and \"Heliamphora exappendiculata\", many \"Pinguicula\" and several \"Utricularia\" species.\n\n"}
{"id": "1405792", "url": "https://en.wikipedia.org/wiki?curid=1405792", "title": "Neuromantic (philosophy)", "text": "Neuromantic (philosophy)\n\nAccording to the anthropologist Bradd Shore, the neuromantic refers to the cybernetic frame of mind among excited computer enthusiasts as they experience what Michael Heim called \"the all-at-once simultaneity of totalizing presentness\". It is part of Shore's discourse on the embodied cognitive dimensions of the cultural transformation produced by the emergence of word processing, which he maintained has overcome the spatial limitations of the written text since \"text modules can be combined, reconfigured, reduced, expanded, appended, or deleted from other units at the touch of a finger.\" This phenomenon is said to elicit passion that is also identified as an impulse unique to the modern world. Shore explains that \"the sense of mastery over language resources that word processing bestows on the experienced user is intimately related to Martin Heidegger's notion of enframing \"(Bestellen\"), a subjection of the world to human will that Heidegger saw as a characteristic of all modern technology.\" \n\nThere are sources that claim the concept of neuromantic is not only confined to computer enthusiasts. For instance, there is the so-called neuromantic imagination that inspire architects. Here, the cybernetic frame of mind is fired by the concept of the \"third nature,\" which is occupied by those who inhabit not the actual terrain in which we live, work, and play but the virtual space of media flows that enter the \"unconscious\". See also Flowstates. \n"}
{"id": "17383315", "url": "https://en.wikipedia.org/wiki?curid=17383315", "title": "New England Hospital for Women and Children", "text": "New England Hospital for Women and Children\n\nMarie Zakrzewska was born on September 9, 1829 in Berlin. In one of her memoirs she wrote “I prefer to be remembered only as a woman who was willing to work for the elevation of woman.” She did just that by starting the hospital. As a child she followed her mother (a midwife) around the school of midwifery where she worked. Once she was 18 she applied to study midwifery, which was the only part of medicine women were allowed to work in, at the Royal Charité Hospital in Berlin. She was rejected, however she continued to apply at ages 19 and 20, but was still rejected. It was only once Dr. Joseph Hermann Schmidt, who worked at the school, used his influence to get her in that she finally was able to study medicine. Later, she moved to America, where women were allowed to be doctors.\n\nOnce in New York, she met Dr. Elizabeth Blackwell, who helped her to become a doctor. The two of them opened the New York Infirmary for Women and Children on May 1, 1857. This lead her to go to Boston to meet with the board at the New England Female College, where she was offered, and accepted, a position as a professor of Obstetrics, and Diseases of Women and Children and the head of the clinical program. She eventually resigned. With the help of members of the board of the New England Female College as well as Ednah D. Cheney (legal sponsor), Lucy Goddard (legal sponsor), Mrs. George G. Lee (donated $3000), and Samuel E. Sewall (donated $1000), she opened the New England Hospital for Women and Children on July 1, 1862.\n\nThe goal of the hospital was “1) to provide for women medical aid of competent physicians of their own sex; 2) to assist educated women in the practical study of medicine; and 3) to train nurses for the care of the sick.” Dr. Zakrzewska was devoted to providing obstetric care to all who needed it, regardless of race or economic status.\n\nAt the start of the hospital, the staff consisted of Dr. Zakrzewska, 2 interns, and 2 consulting physicians. By 1900 there was a residient physician, 54 attending, assisting, and advisory physicians, and 13 consulting physicians.\n\nAll staff and doctors that worked there were women until 1950, when a lack of money led the board to reverse the policy. The policy was again reversed in 1952 to only allow women to be on the staff. This was reversed a final time in 1962 when the by-laws were rewritten.\n\nIn 1864, the Massachusetts state legislature gave the hospital a grant of $5000, allowing it to expand down Pleasant street. The hospital also acquired property behind the hospital at the end of 1864. This allowed the hospital to split into 3 parts. A hospital, a dispensary, and an inpatient facility.\n\nIn 1865, Dr. Anita Tyng was hired as a surgeon. She was the first woman in the US to ever be listed as specializing in surgery by a hospital. She is one of the women who sent a letter MIT founder in January 1867 requesting to \"continue the study of Chemistry in the Technological Institute.\"\n\nIn 1872, the hospital moved to Codman Avenue in suburban Roxbury, and the dispensary stayed in the center of Boston. This new hospital opened a nursing school, the first in America. The first American trained nurse, Linda Richards (graduated 1873) and the first black nurse, Mary Eliza Mahoney (graduated 1879) were both trained at the nursing school. Unfortunately the nursing school was closed in 1951.\n\nThe hospital remained dedicated to women and children until 1951 when it was renamed the New England Hospital. Part of this decision was to reflect that it was now open to men as well as women and children. The hospital decided to open it to men because of financial difficulties.\n\nIn 1969 the hospital was renamed again. This time it was named The Dimock Community Health Center. It was named after Susan Dimock, a resident doctor (surgeon) at the hospital who drowned in the shipwreck of the SS Schiller on May 7, 1875 when she was just 28. Because of its history, the clinic's buildings are listed as a National Historic Landmark. All of the buildings at the Dimock Center are named after women.\n\nFrom the Center’s historic nine-acre campus located in the Egleston Square section of Roxbury, MA, and several satellite locations, The Dimock Center provides access to high-quality healthcare and human services that include: Adult & Pediatric Primary Care, Women’s Healthcare, Eye and Dental Care, HIV/AIDS Specialty Care, Outpatient Mental Health services, Residential Programs, The Mary Eliza Mahoney House shelter for families, pre-school, Head Start programs, after-school programs and Adult Basic Education & Workforce Training programs.\n\nThe Dimock Center has been recognized nationally as a model for the delivery of integrated care in an urban community.\n\n\n\n"}
{"id": "8054393", "url": "https://en.wikipedia.org/wiki?curid=8054393", "title": "Personnel recovery", "text": "Personnel recovery\n\nThe United States Armed Forces defines personnel recovery as \"The sum of military, DOD civilian, DOD contractor personnel, or other personnel designated by the President or Secretary of Defense, who are isolated, missing, detained, or captured (IMDC) in an operational environment. Also called PR.\" \nThe Joint Personnel Recovery Agency is the Chairman's Controlled Activity and is designated as DoD's office of primary responsibility for DoD-wide personnel recovery (PR) matters, less policy. \n\nThe European Personnel Recovery Centre facilitates the harmonisation of personnel recovery policy, doctrine and standards through clear lines of communications with partners stakeholders (nations and international organizations).\n\n\n\n"}
{"id": "21387879", "url": "https://en.wikipedia.org/wiki?curid=21387879", "title": "Pierre Le Roy", "text": "Pierre Le Roy\n\nPierre Le Roy (1717–1785) was a French clockmaker. He was the inventor of the detent escapement, the temperature-compensated balance and the isochronous balance spring. His developments are considered as the foundation of the modern precision clock. Le Roy was born in Paris, eldest son of Julien Le Roy, a clockmaker to Louis XV who had worked with Henry Sully, in which place Pierre Le Roy succeeded his father. He had three brothers: Julien Le Roy (1686-1759) a clockmaker and watchmaker, Julien-David Le Roy (1724–1803) an architect, and Charles Le Roy (1726–1779), a physician and Encyclopédiste.\n\nIn 1748, he invented a pivoted detent type of escapement, or detached escapement, which makes him the inventor of the detent escapement: \"The invention of the detached escapement belongs to P. Le Roy\".\n\nHe was distinguished principally in his mastery and improvement of the clock and chronograph, above all of the marine chronometer, in which he carried forward the pioneering work of John Harrison. He took a different approach from that of Harrison, believing that the way to achieve seaworthiness was to detach the escapement from the balance. He also differed from Harrison regarding his temperature compensation method, which used the variation of the rotation radius of the balance by modifying the diameter of the balance through bi-metallic components, a method which would become a standard in chronometers. His technique for temperature compensation was highly efficient in that it worked without changing the length of the spiral balance spring, which he had discovered to be isochronous only at a precise given length (i.e. when frequency is independent of amplitude, so that a mechanical clock or watch runs at the same rate regardless of changes in its drive force, so it keeps correct time as the mainspring unwinds).\n\nAfter having designed plans in 1754, he constructed his first chronometers by 1756, and accomplished his masterpiece in 1766. This remarkable chronometer incorporated a detached escapement, a temperature-compensated balance and an isochronous balance spring, innovations which would be adopted in subsequent chronometers. Harrison demonstrated a reliable chronometer at sea, but these developments by Le Roy are considered by Rupert Gould to be the foundation of the modern chronometer. Pierre Le Roy's chronometer had a performance equivalent to that of the Harrison H4 chronometer.\n\nIn 1769 he was awarded the double prize offered by the Académie française for the best method of measuring time at sea. He succeeded in giving his instruments the greatest possible regularity by the discovery of the isochronous spiral spring, in which he was in competition with Ferdinand Berthoud, but which he published first.\n\nHe was the author of several valuable publications on the art and science of clock-making and chronography, among them the \"Étrennes chronométriques\" of 1760. He also became \"Horloger du Roi\" in 1760.\n\nThe work of Le Roy was not fully recognized in France however, and his contemporary Ferdinand Berthoud became more famous, obtaining the prestigious title of \"Horloger de Marine\", which left Le Roy disillusioned and led him to retire. He died in Vitry in 1785.\n\n\n"}
{"id": "59068118", "url": "https://en.wikipedia.org/wiki?curid=59068118", "title": "Planctopirus", "text": "Planctopirus\n\nPlanctopirus is a genus of bacteria from the family of Planctomycetaceae\n"}
{"id": "12004853", "url": "https://en.wikipedia.org/wiki?curid=12004853", "title": "Raphaël Blanchard", "text": "Raphaël Blanchard\n\nRaphaël Anatole Émile Blanchard (28 February 1857, St.Christoph, Indre et Loire – 7 February 1919, Paris) was a French doctor and entomologist.\n\n\n\n"}
{"id": "1780263", "url": "https://en.wikipedia.org/wiki?curid=1780263", "title": "Relaxation (physics)", "text": "Relaxation (physics)\n\nIn the physical sciences, relaxation usually means the return of a perturbed system into equilibrium. \nEach relaxation process can be categorized \nby a relaxation time τ. The simplest theoretical description of relaxation as function of time \"t\" is an exponential law exp(-\"t\"/τ).\n\nLet the homogeneous differential equation:\nmodel damped unforced oscillations of a weight on a spring.\n\nThe displacement will then be of the form formula_2. The constant T is called the relaxation time of the system and the constant μ is the quasi-frequency.\n\nIn an RC circuit containing a charged capacitor and a resistor, the voltage decays exponentially:\nThe constant formula_4 is called the \"relaxation time\" of the circuit. A nonlinear oscillator circuit which generates a repeating waveform by the repetitive discharge of a capacitor through a resistance is called a \"relaxation oscillator\".\n\nIn condensed matter physics, relaxation is usually studied as a linear response to a small external perturbation. Since the underlying microscopic processes are active even in the absence of external perturbations, one can also study \"relaxation \"in\" equilibrium\" instead of the usual \"relaxation \"into\" equilibrium\" (see fluctuation-dissipation theorem).\n\nIn continuum mechanics, \"stress relaxation\" is the gradual disappearance of stresses from a viscoelastic medium after it has been deformed.\n\nIn dielectric materials, the dielectric polarization \"P\" depends on the electric field \"E\". If \"E\" changes, \"P(t)\" reacts: the polarization \"relaxes\" towards a new equilibrium.\n\nThe dielectric relaxation time is closely related to the electrical conductivity. In a semiconductor it is a measure of how long it takes to become neutralized by conduction process. This relaxation time is small in metals and can be large in semiconductors and insulators.\n\nAn amorphous solid, such as amorphous indomethacin displays a temperature dependence of molecular motion, which can be quantified as the average relaxation time for the solid in a metastable supercooled liquid or glass to approach the molecular motion characteristic of a crystal. Differential scanning calorimetry can be used to quantify enthalpy change due to molecular structural relaxation.\n\nThe term \"structural relaxation\" was introduced in the scientific literature in 1947/48 without any explanation, applied to NMR, and meaning the same as \"thermal relaxation\".\n\nIn nuclear magnetic resonance, relaxation is of prime importance. See Relaxation (NMR).\n\nIn chemical kinetics, relaxation methods are used for the measurement of very fast reaction rates. A system initially at equilibrium is perturbed by a rapid change in a parameter such as the temperature (most commonly), the pressure, the electric field or the pH of the solvent. The return to equilibrium is then observed, usually by spectroscopic means, and the relaxation time measured. In combination with the chemical equilibrium constant of the system, this enables the determination of the rate constants for the forward and reverse reactions.\n\nConsider a supersaturated portion of a cloud. Then shut off the updrafts, entrainment, or any other vapor sources/sinks and things that would induce the growth of the particles (ice or water). Then wait for this supersaturation to reduce and become just saturation (relative humidity = 100%), which is the equilibrium state. The time it takes for the supersaturation to dissipate is called relaxation time. It will happen as ice crystals or liquid water content grow within the cloud and will thus consume the contained moisture. The dynamics of relaxation are very important in cloud physics for accurate mathematical modelling.\n\nIn water clouds where the concentrations are larger (hundreds per cm) and the temperatures are warmer (thus allowing for much lower supersaturation rates as compared to ice clouds), the relaxation times will be very low (seconds to minutes).\n\nIn ice clouds the concentrations are lower (just a few per liter) and the temperatures are colder (very high supersaturation rates) and so the relaxation times can be as long as several hours. Relaxation time is given as\n\nIn astronomy, relaxation time relates to clusters of gravitationally interacting bodies, for instance, stars in a galaxy. The relaxation time is a measure of the time it takes for one object in the system (the \"test star\") to be significantly perturbed by other objects in the system (the \"field stars\"). It is most commonly defined as the time for the test star's velocity to change by of order itself.\n\nSuppose that the test star has velocity \"v\". As the star moves along its orbit, its motion will be randomly perturbed by the gravitational field of nearby stars. The relaxation time can be shown to be \nwhere \"ρ\" is the mean density, \"m\" is the test-star mass, \"σ\" is the 1d velocity dispersion of the field stars, and ln \"Λ\" is the Coulomb logarithm.\n\nVarious events occur on timescales relating to the relaxation time, including core collapse, energy equipartition, and formation of a Bahcall-Wolf cusp around a supermassive black hole.\n\n"}
{"id": "7055324", "url": "https://en.wikipedia.org/wiki?curid=7055324", "title": "Reproductive synchrony", "text": "Reproductive synchrony\n\nReproductive synchrony is a term used in evolutionary biology and behavioural ecology. Reproductive synchrony—sometimes termed 'ovulatory synchrony'—may manifest itself as 'breeding seasonality'. Where females undergo regular menstruation, 'menstrual synchrony' is another possible term.\n\nReproduction is said to be synchronised when fertile matings across a population are temporally clustered, resulting in multiple conceptions (and consequent births) within a restricted time window. In marine and other aquatic contexts, the phenomenon may be referred to as mass spawning. Mass spawning has been observed and recorded in a large number of phyla, including in coral communities within the Great Barrier Reef.\n\nIn primates, reproductive synchrony usually takes the form of conception and birth seasonality. The regulatory 'clock', in this case, is the sun's position in relation to the tilt of the earth. In nocturnal or partly nocturnal primates—for example, owl monkeys—the periodicity of the moon may also come into play. Synchrony in general is for primates an important variable determining the extent of 'paternity skew'—defined as the extent to which fertile matings can be monopolised by a fraction of the population of males. The greater the precision of female reproductive synchrony—the greater the number of ovulating females who must be guarded simultaneously—the harder it is for any dominant male to succeed in monopolising a harem all to himself. This is simply because, by attending to any one fertile female, the male unavoidably leaves the others at liberty to mate with his rivals. The outcome is to distribute paternity more widely across the total male population, reducing paternity skew (figures a, b).\n\nReproductive synchrony can never be perfect. On the other hand, theoretical models predict that group-living species will tend to synchronise wherever females can benefit by maximising the number of males offered chances of paternity, minimising reproductive skew. For example, the cichlid fish \"V. moorii\" spawns in the days leading up to each full moon (lunar synchrony), and broods often exhibit multiple paternity. The same models predict that female primates, including evolving humans, will tend to synchronise wherever fitness benefits can be gained by securing access to multiple males. Conversely, group-living females who need to restrict paternity to a single dominant harem-holder should assist him by avoiding synchrony.\n\nIn the human case, evolving females with increasingly heavy childcare burdens would have done best by \"resisting\" attempts at harem-holding by locally dominant males. No human female needs a partner who will get her pregnant only to disappear, abandoning her in favour of his next sexual partner. To any local group of females, the more such philandering can be successfully resisted—and the greater the proportion of previously excluded males who can be included in the breeding system and persuaded to invest effort—the better. Hence scientists would expect reproductive synchrony—whether seasonal, lunar or a combination of the two—to be central to evolving human strategies of reproductive levelling, reducing paternity skew and culminating in the predominantly monogamous egalitarian norms illustrated by extant hunter-gatherers. Divergent climate regimes differentiating Neanderthal reproductive strategies from those of modern \"Homo sapiens\" have recently been analysed in these terms.\n\n"}
{"id": "53848877", "url": "https://en.wikipedia.org/wiki?curid=53848877", "title": "Robert Hengehold", "text": "Robert Hengehold\n\nRobert Hengehold from the Air Force Institute of Technology, was awarded the status of Fellow in the American Physical Society, after they were nominated by their Forum on Industrial and Applied Physics in 2008, for \"pioneering contributions to semiconductor material characterization, over 30 years of distinguished and dedicated leadership in the development of graduate applied physics programs for military officers, and service to the physics community through APS sectional meetings specifically on applied and industrial phy\"\n"}
{"id": "27206499", "url": "https://en.wikipedia.org/wiki?curid=27206499", "title": "Robustness (morphology)", "text": "Robustness (morphology)\n\nIn biology, robustness is used to describe a species with a morphology based on strength and a heavy build. The alternative morphology is the gracile body type. \n\nFor example, comparing similar species, rats have robust body types while mice are gracile. Male and female members of the same species may display sexual dimorphism and have robust and gracile morphologies. \n\nThe terms \"robust\" vs. \"gracile\" are used relatively in the context of human evolution, to distinguish:\n"}
{"id": "27754607", "url": "https://en.wikipedia.org/wiki?curid=27754607", "title": "Scientastic!", "text": "Scientastic!\n\nScientastic! is an elementary to middle school age family television show that explores science, health and social issues through the eyes of today's youth. The show was created by John A. Pollock, Leo Eaton and Mike Erskine-Kellie. The show's format blends live-action and animation and is a hybrid of mystery and reality genres, incorporating a fictional plot with interviews from actual doctors and scientists in and around Pittsburgh, Pennsylvania, where the show is filmed. To date, two episodes have been released. The episode “Are You Sleeping?” was the recipient of two Emmy Awards in 2015.\n\nThe show's concept originated from Duquesne University biology professor John A. Pollock, with the goal of improving science literacy among school students ages 8 to 13 years old . Pollock partnered with television production company Planet Earth Television to create the pilot episode, \"Sticks and Stones\" (2010) and the subsequent episode “Are You Sleeping?” (2014). Both episodes were directed by Emmy award-winning producer and director Leo Eaton (\"Zoboomafoo\") and written by Mike Erskine-Kellie.\n\nThe show’s storyline follows the character of a young girl driven to explore topics in science out of an interest to learn and a desire to help others and the world around her. In the pilot episode, the character is a 12-year-old girl named “Leah” (played by actress Lili Reinhart), while in the following episode “Are You Sleeping?” she is a 14-year old named “Cassie” (played by actress Gabrielle Phillips). Her younger brother (played by actors Joseph Serafini and Justin Bees) joins along as Cassie researches and finds leads at her local library and then visits different museums, hospitals and institutions to conduct interviews with real-life specialists. The episode is supplemented with 2-D and 3-D animations that render anatomy and biology, as well as short interstitial episodes of music and dance.\n\nProduction of “Sticks and Stones” and “Are You Sleeping?” involved the efforts of many groups and individuals, including the Regenerative Medicine Partnership in Education, where Pollock serves as Principal Investigator, and the Pittsburgh Creative and Performing Arts School (CAPA).\n\nThe \"Scientastic!\" pilot episode, \"Sticks and Stones,\" focuses on the biology of bones, bone nutrition, bone healing and the use of stem cells in bone repair. It follows the story of Leah (actress Lili Reinhart) and her best friend Habiba (actress Habiba Hopson) who breaks her arm during a soccer practice after being roughhoused by a group of bullies on the opposing team. To help Habiba regain the confidence she needs to continue playing soccer, she investigates what bones are and how they heal, visiting hospitals, museums and research laboratories in Pittsburgh. Included are interviews with doctors, scientists and researchers at UPMC Children's Hospital of Pittsburgh, the Carnegie Museum of Natural History, the National Aviary and the McGowan Institute for Regenerative Medicine.\n\nLeah’s younger brother Axel (played by actor Joseph Serafini) films her investigations. At the end, Leah compiles Axel's footage and creates a webcast, which is featured on the show's website.\n\nThe pilot was released in May 2010, first airing September 2, 2010 on WQED-TV, Pittsburgh's PBS member station.\n\nThe \"Scientastic!\" episode “Are You Sleeping” explores issues surrounding lack of sleep, particularly its impact on the capacity to learn and make decisions. In the episode, lead character Cassie fails a math test that she stayed up all night studying for, prompting her to embark on a quest to learn about the science of sleep in order to help her sleep-deprived family and classmates. She and her younger brother Dean track down and interview professionals from the Pittsburgh Zoo, Phipps Conservatory, Meadowcroft Rockshelter and a sleep lab at University of Pittsburgh Medical Center, among others.\nThe episode touches on a long-running debate in the U.S. surrounding sleep deprivation and school start times, which was addressed in 2013 by U.S. Secretary of Education Arne Duncan. Duncan called for shifting to later start times for students, stating that doing so is not only common sense but is also backed by scientific research.\n\nThe hour-long episode was aired in April 2014 on WQED-TV and was distributed by American Public Television to more than 100 public television stations across the U.S.\n\nIn 2015, \"Scientastic!\" “Are You Sleeping?” won two Mid-Atlantic Region Emmy Awards from the National Academy of Television Arts and Sciences for in the Children/Youth/Teen Program or Special and Music Composition/Arrangement categories.\n\n\"Scientastic!\" received funding from UPMC Health Plan, National Center for Research Resources of the National Institutes of Health, US Department of Education and The Pittsburgh Foundation.\n\n\n"}
{"id": "145698", "url": "https://en.wikipedia.org/wiki?curid=145698", "title": "Seafloor spreading", "text": "Seafloor spreading\n\nSeafloor spreading is a process that occurs at mid-ocean ridges, where new oceanic crust is formed through volcanic activity and then gradually moves away from the ridge.\n\nEarlier theories (e.g. by Alfred Wegener and Alexander du Toit) of continental drift postulated that continents \"ploughed\" through the sea. The idea that the seafloor itself moves (and also carries the continents with it) as it expands from a central axis was proposed by Harry Hess from Princeton University in the 1960s. The theory is well accepted now, and the phenomenon is known to be caused by convection currents in the asthenosphere, which is ductile, or plastic, and the brittle lithosphere (crust and upper mantle).\n\nSeafloor spreading helps explain continental drift in the theory of plate tectonics. When oceanic plates diverge, tensional stress causes fractures to occur in the lithosphere. The motivating force for seafloor spreading ridges is tectonic plate pull rather than magma pressure, although there is typically significant magma activity at spreading ridges. At a spreading center, basaltic magma rises up the fractures and cools on the ocean floor to form new seabed. Hydrothermal vents are common at spreading centers. Older rocks will be found farther away from the spreading zone while younger rocks will be found nearer to the spreading zone. Additionally spreading rates determine if the ridge is a fast, intermediate, or slow. As a general rule, fast ridges have spreading (opening) rates of more than 9 cm/year. Intermediate ridges have a spreading rate of 5–9 cm/year while slow spreading ridges have a rate less than 5 cm/year.\n\nSeafloor spreading occurs at spreading centers, distributed along the crests of mid-ocean ridges. Spreading centers end in transform faults or in overlapping spreading center offsets. A spreading center includes a seismically active plate boundary zone a few kilometers to tens of kilometers wide, a crustal accretion zone within the boundary zone where the ocean crust is youngest, and an instantaneous plate boundary - a line within the crustal accretion zone demarcating the two separating plates. Within the crustal accretion zone is a 1-2 km-wide neovolcanic zone where active volcanism occurs. \n\nIn the general case, sea floor spreading starts as a rift in a continental land mass, similar to the Red Sea-East Africa Rift System today. The process starts by heating at the base of the continental crust which causes it to become more plastic and less dense. Because less dense objects rise in relation to denser objects, the area being heated becomes a broad dome (see isostasy). As the crust bows upward, fractures occur that gradually grow into rifts. The typical rift system consists of three rift arms at approximately 120 degree angles. These areas are named triple junctions and can be found in several places across the world today. The separated margins of the continents evolve to form passive margins.\nHess' theory was that new seafloor is formed when magma is forced upward toward the surface at a mid-ocean ridge.\n\nIf spreading continues past the incipient stage described above, two of the rift arms will open while the third arm stops opening and becomes a 'failed rift'. As the two active rifts continue to open, eventually the continental crust is attenuated as far as it will stretch. At this point basaltic oceanic crust begins to form between the separating continental fragments. When one of the rifts opens into the existing ocean, the rift system is flooded with seawater and becomes a new sea. The Red Sea is an example of a new arm of the sea. The East African rift was thought to be a \"failed\" arm that was opening somewhat more slowly than the other two arms, but in 2005 the Ethiopian Afar Geophysical Lithospheric Experiment reported that in the Afar region, September 2005, a 60 km fissure opened as wide as eight meters. During this period of initial flooding the new sea is sensitive to changes in climate and eustasy. As a result, the new sea will evaporate (partially or completely) several times before the elevation of the rift valley has been lowered to the point that the sea becomes stable. During this period of evaporation large evaporite deposits will be made in the rift valley. Later these deposits have the potential to become hydrocarbon seals and are of particular interest to petroleum geologists.\n\nSea floor spreading can stop during the process, but if it continues to the point that the continent is completely severed, then a new ocean basin is created. The Red Sea has not yet completely split Arabia from Africa, but a similar feature can be found on the other side of Africa that has broken completely free. South America once fit into the area of the Niger Delta. The Niger River has formed in the failed rift arm of the triple junction.\n\nAs new seafloor forms and spreads apart from the mid-ocean ridge it slowly cools over time. Older seafloor is therefore colder than new seafloor, and older oceanic basins deeper than new oceanic basins due to isostasy. If the diameter of the earth remains relatively constant despite the production of new crust, a mechanism must exist by which crust is also destroyed. The destruction of oceanic crust occurs at subduction zones where oceanic crust is forced under either continental crust or oceanic crust. Today, the Atlantic basin is actively spreading at the Mid-Atlantic Ridge. Only a small portion of the oceanic crust produced in the Atlantic is subducted. However, the plates making up the Pacific Ocean are experiencing subduction along many of their boundaries which causes the volcanic activity in what has been termed the Ring of Fire of the Pacific Ocean. The Pacific is also home to one of the world's most active spreading centers (the East Pacific Rise) with spreading rates of up to 13 cm/yr. The Mid-Atlantic Ridge is a \"textbook\" slow-spreading center, while the East Pacific Rise is used as an example of fast spreading. Spreading centers at slow and intermediate rates exhibit a rift valley while at fast rates an axial high is found within the crustal accretion zone. The differences in spreading rates affect not only the geometries of the ridges but also the geochemistry of the basalts that are produced.\n\nSince the new oceanic basins are shallower than the old oceanic basins, the total capacity of the world's ocean basins decreases during times of active sea floor spreading. During the opening of the Atlantic Ocean, sea level was so high that a Western Interior Seaway formed across North America from the Gulf of Mexico to the Arctic Ocean.\n\nAt the Mid-Atlantic Ridge (and in other areas), material from the upper mantle rises through the faults between oceanic plates to form new crust as the plates move away from each other, a phenomenon first observed as continental drift. When Alfred Wegener first presented a hypothesis of continental drift in 1912, he suggested that continents ploughed through the ocean crust. This was impossible: oceanic crust is both more dense and more rigid than continental crust. Accordingly, Wegener's theory wasn't taken very seriously, especially in the United States.\n\nSince then, it has been shown that the motion of the continents is linked to seafloor spreading by the theory of plate tectonics. In the 1960s, the past record of geomagnetic reversals was noticed by observing the magnetic stripe \"anomalies\" on the ocean floor. This results in broadly evident \"stripes\" from which the past magnetic field polarity can be inferred by looking at the data gathered from simply towing a magnetometer on the sea surface or from an aircraft. The stripes on one side of the mid-ocean ridge were the mirror image of those on the other side. The seafloor must have originated on the Earth's great fiery welts, like the Mid-Atlantic Ridge and the East Pacific Rise.\n\nThe driver for seafloor spreading in plates with active margins is the weight of the cool, dense, subducting slabs that pull them along. The magmatism at the ridge is considered to be \"passive upswelling\", which is caused by the plates being pulled apart under the weight of their own slabs. This can be thought of as analogous to a rug on a table with little friction: when part of the rug is off of the table, its weight pulls the rest of the rug down with it.\n\nThe depth of the seafloor (or the height of a location on a mid-ocean ridge above a baselevel) is closely correlated with its age (age of the lithosphere where depth is measured). The age-depth relation can be modeled by the cooling of a lithosphere plate or mantle half-space in areas without significant subduction. \n\nIn the half-space model, the seabed height is determined by the oceanic lithosphere temperature, due to thermal expansion.\nOceanic lithosphere is continuously formed at a constant rate at the mid-ocean ridges. The source of the lithosphere has a half-plane shape (x = 0, z < 0) and a constant temperature T.\nDue to its continuous creation, the lithosphere at x > 0 is moving away from the ridge at a constant velocity v, which is assumed large compared to other typical scales in the problem.\nThe temperature at the upper boundary of the lithosphere (z=0) is a constant T = 0. Thus at x = 0 the temperature is the Heaviside step function formula_1.\nFinally, we assume the system is at a quasi-steady state, so that the temperature distribution is constant in time, i.e. T=T(x,z).\n\nBy calculating in the frame of reference of the moving lithosphere (velocity v), which have spatial coordinate x' = x-vt, we may write T = T(x',z,t) and use the heat equation:\nformula_2\nwhere formula_3 is the thermal diffusivity of the mantle lithosphere.\n\nSince T depends on x' and t only through the combination formula_4, we have:\nformula_5\n\nThus:\nformula_6\n\nWe now use the assumption that formula_7 is large compared to other scales in the problem; we therefore neglect the last term in the equation, and get a 1-dimensional diffusion equation:\nformula_8\nwith the initial conditions \nformula_9.\n\nThe solution for formula_10 is given by the error function formula_11:\n\nDue to the large velocity, the temperature dependence on the horizontal direction is negligible, and the height at time t (i.e. of sea floor of age t) can be calculated by integrating the thermal expansion over z:\n\nwhere formula_14 is the effective volumetric thermal expansion coefficient, and h is the mid-ocean ridge height (compared to some reference).\n\nNote that the assumption the v is relatively large is equivalently to the assumption that the thermal diffusivity formula_3 is small compared to formula_16, where L is the ocean width (from mid-ocean ridges to continental shelf) and T is its age.\n\nThe effective thermal expansion coefficient formula_14 is different from the usual thermal expansion coefficient formula_18 due to isostasic effect of the change in water column height above the lithosphere as it expands or retracts. Both coefficients are related by:\n\nwhere formula_20 is the rock density and formula_21 is the density of water.\n\nBy substituting the parameters by their rough estimates: formula_22 m/s, formula_23 °C and T ~1220 °C (for the Atlantic and Indian oceans) or ~1120 °C (for the eastern Pacific), we have:\n\nfor the eastern Pacific Ocean, and:\n\nfor the Atlantic and the Indian Ocean, where the height is in meters and time is in millions of years.\nTo get the dependence on x, one must substitute t = x/v ~ Tx/L, where L is the distance between the ridge to the continental shelf (roughly half the ocean width), and T is the ocean age.\n\n\n"}
{"id": "13301535", "url": "https://en.wikipedia.org/wiki?curid=13301535", "title": "Seed nucleus", "text": "Seed nucleus\n\nA seed nucleus is an isotope that is the starting point for any of a variety of fusion chain reactions. The mix of nuclei produced at the conclusion of the chain reaction generally depends strongly on the relative availability of the seed nucleus or nuclei and the component being fused--whether neutrons as in the r-process and s-process or protons as in the rp-process. A smaller proportion of seed nuclei will generally result in products of larger mass, whereas a larger seed-to-neutron or seed-to-proton ratio will tend to produce comparatively lighter masses.\n"}
{"id": "33026044", "url": "https://en.wikipedia.org/wiki?curid=33026044", "title": "Social Movement Studies", "text": "Social Movement Studies\n\nSocial Movement Studies is a bimonthly peer-reviewed academic journal covering social science research on protests, social movements, and collective behavior, including reviews of books on these topics. It was established in 2002 as a biannual journal by founding co-editors Tim Jordan, Adam Lent, and George McKay, soon to be joined by Anne Mische, and is published by Routledge. The current editors-in-chief are Cristina Flesher Fominaya (Loughborough University) and Kevin Gillan (University of Manchester). Previous editors-in-chief include Graeme Hayes (Aston University) and Brian Doherty (Keele University). \n\nThe journal is abstracted and indexed in Current Contents/Social & Behavioral Sciences, Social Sciences Citation Index, and Scopus. According to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 1.581.\n"}
{"id": "1058631", "url": "https://en.wikipedia.org/wiki?curid=1058631", "title": "Sociocultural anthropology", "text": "Sociocultural anthropology\n\nSociocultural anthropology is a portmanteau used to refer to social anthropology and cultural anthropology together. Some universities, such as Boston University and New York University, link them together into one major of study.\n\nThe rubric \"cultural\" anthropology is generally applied to ethnographic works that are holistic in spirit, oriented to the ways in which culture affects individual experience, or aim to provide a rounded view of the knowledge, customs, and institutions of a people. \"Social\" anthropology is a term applied to ethnographic works that attempt to isolate a particular system of social relations such as those that comprise domestic life, economy, law, politics, or religion, give analytical priority to the organizational bases of social life, and attend to cultural phenomena as somewhat secondary to the main issues of social scientific inquiry.\n\nSociocultural anthropology, which we understand to include linguistic anthropology, is concerned with the problem of difference and similarity within and between human populations. The discipline arose concomitantly with the expansion of European colonial empires, and its practices and theories have been questioned and reformulated along with processes of decolonization. Such issues have re-emerged as transnational processes have challenged the centrality of the nation-state to theorizations about culture and power. New challenges have emerged as public debates about multiculturalism, and the increasing use of the culture concept outside of the academy and among peoples studied by anthropology. These are not \"business-as-usual\" times in the academy, in anthropology, or in the world, if ever there were such times.\n\nQuestions about cultural processes and theorizing about \"human nature\" escape the boundaries of anthropology as a discipline. The major paradigms framing cultural difference and human universals are profoundly contested; migrations, political collapses and social reorganizations transform the context in which the production of cultural meanings and theories of culture have been embedded and reproduced. For many of us, this is a moment in which it is necessary to take up the sort of broad challenges with which our disciplinary predecessors struggled – to redefine the field of inquiry and research in relation to debates that have enormous significance in our own lives and those of the people we study.\n\n"}
{"id": "4769321", "url": "https://en.wikipedia.org/wiki?curid=4769321", "title": "Timeline of fundamental physics discoveries", "text": "Timeline of fundamental physics discoveries\n\n"}
{"id": "44896214", "url": "https://en.wikipedia.org/wiki?curid=44896214", "title": "Tscherne classification", "text": "Tscherne classification\n\nThe Tscherne classification is a system of categorization of soft tissue injuries\n\nThe intraobserver (observations at two different times by the same person) agreement for Tscherne classification is 85%; while for inter-observer agreement is 65%.\n\nThis classification system was developed by Harald Tscherne and Hans-Jörg Oestern in 1982 at the Hannover Medical School (Hanover, Germany) to classify both open and closed fractures. This classification system is based on the physiological concept that the higher the kinetic energy imparted on the bone, the higher the kinetic energy imparted on the soft tissue. It also serves as a tool to guide management and to predict clinical outcomes. It also serves as a communication tool in research and in clinical presentations.\n"}
{"id": "50582259", "url": "https://en.wikipedia.org/wiki?curid=50582259", "title": "Vladimír Socha", "text": "Vladimír Socha\n\nMgr. Vladimír Socha (born January 1, 1982 in Hradec Králové) is a Czech writer, publisher, public lecturer and science promoter from the city of Hradec Králové (north-eastern Czech Republic). His main interest lies in dinosaur paleontology and the history of science in general. So far, he published eight books about dinosaurs and primeval life in Czech language, four smaller brochures (booklets) and many magazine articles about nature, history and prehistory. In 2009 he volunteered in the summer paleontological field dig in the Hell Creek formation in eastern Montana (for the \"Museum of the Rockies\" in Bozeman). A year later, he published a book about the experience called \"Dinosauři od Pekelného potoka\" (Dinosaurs from the Hell Creek). He also provides interviews for the Czech Radio and sometimes organises paleoart exhibitions.\n\n\n"}
{"id": "13573288", "url": "https://en.wikipedia.org/wiki?curid=13573288", "title": "Étienne Lenoir (instrument maker)", "text": "Étienne Lenoir (instrument maker)\n\nÉtienne Lenoir (1744–1832) was a French scientific instrument maker and inventor of the \"repeating circle\".\n\nWhen hired by Jean-Charles de Borda around 1772 to work on the \"reflecting circle\", he was about thirty years old and nearly illiterate. However, his intelligence and mechanical genius allowed him to perform work that few others could perform. He played a significant role in the improvements to the reflecting circle and later used this experience in inventing the repeating circle. As a result of this work, he became known as the pre-eminent maker of instruments for astronomy, navigation and surveying in France.\n\nIn 1787, the king of France appointed him \"certificated engineer to the king\". He created the instruments used on all the major French geodetic surveying projects and major naval expeditions of the time.\n\nHe worked primarily for the \"Commission des Poids et Mesures\" (Weights and Measures Commission) after 1792 and for the Commission du Metre (Metre Commission - for the determination of the metric unit of length, the metre). For these commissions, he produced the instruments used to measure the meridian arc and created the platinum rules for baseline measurements. In 1793, he made and signed the provisional standard brass metre. He also invented a comparator for the measurement of the definitive standard metre.\n\nLenoir was also a member of the \"Commission temporaire des Arts\" (1793–1794). He was made a member of the \"Bureau des Longitudes\" in 1814 and received the Legion of Honour.\n\nHe son, Paul-Etienne Lenoir, followed on in his footsteps, taking over his workshop around 1815 and continuing his work after Lenoir senior's death.\n\n"}
