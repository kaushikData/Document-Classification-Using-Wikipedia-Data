{"id": "28928557", "url": "https://en.wikipedia.org/wiki?curid=28928557", "title": "Bayly Glacier", "text": "Bayly Glacier\n\nBayly Glacier () is a glacier flowing into the head of Bancroft Bay, on the west coast of Graham Land. It was mapped by the Falkland Islands Dependencies Survey (FIDS) from photos taken by Hunting Aerosurveys Ltd in 1956–57, and named by the UK Antarctic Place-Names Committee in 1960 for Maurice B. Bayly, FIDS geologist at the Danco Island station in 1956 who, together with L. Harris, pioneered the route from the Portal Point hut (on nearby Reclus Peninsula) to the plateau in February 1957.\n\n"}
{"id": "11820034", "url": "https://en.wikipedia.org/wiki?curid=11820034", "title": "Berthelot's reagent", "text": "Berthelot's reagent\n\nBerthelot's reagent is an alkaline solution of phenol and hypochlorite, used in analytical chemistry. It is named after its inventor, Marcellin Berthelot. Ammonia reacts with Berthelot's reagent to form a blue product which is used in a colorimetric method for determining ammonia. The reagent can also be used for determining urea. In this case the enzyme urease is used to catalyze the hydrolysis of urea into carbon dioxide and ammonia. The ammonia is then determined with Berthelot's reagent.\n\nPhenol in the Berthelot reagent can be replaced by a variety of phenolic reagents, the most common being sodium salicylate, which is significantly less toxic. This has been used for blood urea nitrogen (BUN) determinations and commonly is used to determine water and soil total and ammonia-N. Replacement of phenol by 2-phenylphenol reduces interferences by a variety of soil and water constituents and improves color stability at slightly lower pH.\n\nBerthelot's reagent has been used in a range of situations. It is often used in colorimetric methods, through an AutoAnalyzer, spectrophotometer, or multiwell plate spectrophotometer. The reagent lacks sensitivity in situations where there may be amines as well as ammonia, however this can be overcome in part by the use of 2-pheylphenol to replace phenol. An ion selective electrode, or distillation/titration method can often be used in cases where Berthelot chemistry is ineffective.\n\nBerthelot chemistry has also been adapted for the analysis of nitrite and nitrate in soil and water after conversion, typically by reduction with Devarda's alloy, of these species to ammonium. \n\n<br>\n"}
{"id": "25643285", "url": "https://en.wikipedia.org/wiki?curid=25643285", "title": "Climate pattern", "text": "Climate pattern\n\nA climate pattern is any recurring characteristic of the climate. Climate patterns can last tens of thousands of years, like the glacial and interglacial periods within ice ages, or repeat each year, like monsoons.\n\nA climate pattern may come in the form of a regular cycle, like the diurnal cycle or the seasonal cycle; a quasi periodic event, like El Niño; or a highly irregular event, such as a volcanic winter. The regular cycles are generally well understood and may be removed by normalization. For example, graphs which show trends of temperature change will usually have the effects of seasonal variation removed.\n\nA \"mode of variability\" is a climate pattern with identifiable characteristics, specific regional effects, and often oscillatory behavior. Many modes of variability are used by climatologists as indices to represent the general climatic state of a region affected by a given climate pattern.\n\nMeasured via an empirical orthogonal function analysis, the mode of variability with the greatest effect on climates worldwide is the seasonal cycle, followed by El Niño-Southern Oscillation, followed by thermohaline circulation.\n\nOther well-known modes of variability include:\n\n\n"}
{"id": "23233259", "url": "https://en.wikipedia.org/wiki?curid=23233259", "title": "Combined Federated Battle Laboratories Network", "text": "Combined Federated Battle Laboratories Network\n\nThe Combined Federated Battle Laboratories Network (CFBLNet) is a laboratory environment which utilizes a distributed Wide Area Network (WAN) as the vehicle to simulate training environments and to de-risk command and control (C2) and intelligence capabilities by conducting Research and Development, Training, Trials and Assessment (RDTT&A) on command, control, communication, computer, intelligence, surveillance and reconnaissance (C4ISR) initiatives and training events. Since 2001, membership has been established and represented by three core parties: the U.S. Joint Staff, the Combined Communications and Electronics Board (Australia, Canada, New Zealand, United Kingdom and United States), and the North Atlantic Treaty Organization (including NATO agencies and 29 member nations, not all of which actively participate). Besides the core parties to the CFBLNet Technical Arrangement, four nations (Austria, Finland, Sweden and Switzerland) have become Guest Mission Partners under rules contained in CFBLNet governance documentation referred to as Publication 1.\n\nCFBLNet provides the main platform for conducting Coalition Interoperability Assurance and Validation (CIAV) events in the context of Federated Mission Networking.\n\nThe CFBLNet consists of a distributed and integrated network architecture of combined, joint, and military service infrastructure components (networks, database servers, application servers, client workstations, etc.). These strings of network equipment and services are located within the confines of the various national and international battle laboratories and experimentation sites of the participants, which provide the applications, analytic tools, and communications necessary to conduct initiatives or experiments.\n\nNo single nation owns the CFBLNet infrastructure; each member nation is responsible for the funding and maintenance of its own systems and CFBL network segments, which hook into the backbone at a defined Point-of-Presence (POP). All CFBLNet members must respect the sovereignty and intellectual property of the other nations. Also, each country is responsible for funding its own experiments. The Multinational Information Sharing (MNIS) in Fort Meade, Maryland (USA) maintains day-to-day control of the network and coordinates activities on the network. \n\nThe U.S. CFBLNet infrastructure is extensive and reaches to international demarcation points for the Southern Hemisphere and Europe. Nations and organizations within nations which are not a part of the Technical Agreement must be sponsored to become a Guest CFBLNet Mission Partner (GMP) by a charter member, Core CFBLNet Mission Partner (CMP), to sponsor initiatives and to connect to the CFBLNet.\n\nThe CFBLNet grew out the network designed to support the U.S. Joint Warfighter Interoperability Demonstrations (JWID), which used to build a support network for the period of the demonstrations and tear it down each year after the demonstrations. In 1999, the Coalition Warrior Interoperability Demonstration/Joint Warrior Interoperability Demonstration (CWID/JWID) exercise used, for the first time, a permanent infrastructure that became what is now called the Combined Federated Battle Laboratories Network (CFBLNet), as established by the NATO Consultation, Command and Control Board (NC3B) in 2001. The formal Technical Agreement (or charter) was signed in August 2002.\n\n"}
{"id": "23693319", "url": "https://en.wikipedia.org/wiki?curid=23693319", "title": "Counting the Eons", "text": "Counting the Eons\n\nCounting the Eons is a collection of seventeen nonfiction science essays written by Isaac Asimov. It was the sixteenth of a series of books collecting essays from \"The Magazine of Fantasy and Science Fiction\", these being first published between August 1980 and December 1981. It was first published by Doubleday & Company in 1983.\n\n\n"}
{"id": "4518269", "url": "https://en.wikipedia.org/wiki?curid=4518269", "title": "Defocus aberration", "text": "Defocus aberration\n\nIn optics, defocus is the aberration in which an image is simply out of focus. This aberration is familiar to anyone who has used a camera, videocamera, microscope, telescope, or binoculars. Optically, defocus refers to a translation of the focus along the optical axis away from the detection surface. In general, defocus reduces the sharpness and contrast of the image. What should be sharp, high-contrast edges in a scene become gradual transitions. Fine detail in the scene is blurred or even becomes invisible. Nearly all image-forming optical devices incorporate some form of focus adjustment to minimize defocus and maximize image quality.\n\nThe degree of image blurring for a given amount of focus shift depends inversely on the lens f-number. Low f-numbers, such as to 2.8, are very sensitive to defocus and have very shallow depths of focus. High f-numbers, in the 16 to 32 range, are highly tolerant of defocus, and consequently have large depths of focus. The limiting case in f-number is the pinhole camera, operating at perhaps 100 to 1000, in which case all objects are in focus almost regardless of their distance from the pinhole aperture. The penalty for achieving this extreme depth of focus is very dim illumination at the imaging film or sensor, limited resolution due to diffraction, and very long exposure time, which introduces the potential for image degradation due to motion blur.\n\nThe amount of allowable defocus is related to the resolution of the imaging medium. A lower-resolution imaging chip or film is more tolerant of defocus and other aberrations. To take full advantage of a higher resolution medium, defocus and other aberrations must be minimized.\n\nDefocus is modeled in Zernike polynomial format as formula_1, where formula_2 is the defocus coefficient in wavelengths of light. This corresponds to the parabola-shaped optical path difference between two spherical wavefronts that are tangent at their vertices and have different radii of curvature.\n\nFor some applications, such as phase contrast electron microscopy, defocused images can contain useful information. Multiple images recorded with various values of defocus can be used to examine how the intensity of the electron wave varies in three-dimensional space, and from this information the phase of the wave can be inferred. This is the basis of non-interferometric phase retrieval. Examples of phase retrieval algorithms that use defocused images include the Gerchberg–Saxton algorithm and various methods based on the transport of intensity equation.\n\nIn casual conversation, the term \"blur\" can be used to describe any reduction in vision. However, in a clinical setting blurry vision means the subjective experience or perception of optical defocus within the eye, called refractive error. Blur may appear differently depending on the amount and type of refractive error. The following are some examples of blurred images that may result from refractive errors:The extent of blurry vision can be assessed by measuring visual acuity with an eye chart. Blurry vision is often corrected by focusing light on the retina with corrective lenses. These corrections sometimes have unwanted effects including magnification or reduction, distortion, color fringes, and altered depth perception. During an eye exam, the patient's acuity is measured without correction, with their current correction, and after refraction. This allows the optometrist or ophthalmologist (\"eye doctor\") to determine the extent refractive errors play in limiting the quality of the patient's vision. A Snellen acuity of 6/6 or 20/20, or as decimal value 1.0, is considered to be sharp vision for an average human (young adults may have nearly twice that value). Best-corrected acuity lower than that is an indication that there is another limitation to vision beyond the correction of refractive error.\n\nOptical defocus can result from incorrect corrective lenses or insufficient accommodation, as, e.g., in presbyopia from the aging eye. As said above, light rays from a point source are then not focused to a single point on the retina but are distributed in a little disk of light, called the \"blur disk\". Its size depends on pupil size and amount of defocus, and is calculated by the equation \n\nformula_3\n\n(\"d\" = diameter in degrees visual angle, \"p\" = pupil size in mm, \"D\" = defocus in diopters).\n\nIn linear systems theory, the point image (i.e. the blur disk) is referred to as the point spread function (PSF). The retinal image is given by the convolution of the in-focus image with the PSF.\n\n\n"}
{"id": "23173262", "url": "https://en.wikipedia.org/wiki?curid=23173262", "title": "Degreeting", "text": "Degreeting\n\nIn pragmatics, a degreeting refers to the conversational procedure by which two participants of a conversation agree to discontinue the conversation. It is so named because a degreeting concludes a conversation in a similar way that a greeting engages one.\n\nThe following conversation is an example of degreeting:\n\n"}
{"id": "7376169", "url": "https://en.wikipedia.org/wiki?curid=7376169", "title": "Distributed republic", "text": "Distributed republic\n\nThe distributed republic is a concept of fluid republic consisting of land and citizens scattered around the globe, changing far more frequently than conventional nation-states. In fiction, many of these republics are corporate entities, while others are more loosely connected anarchist communities. The concept is rooted in the anarcho-capitalist, dystopian cyberpunk subgenre of science fiction, and was used extensively by novelist Neal Stephenson in his books \"Snow Crash\" and \"The Diamond Age\".\n\n"}
{"id": "20097989", "url": "https://en.wikipedia.org/wiki?curid=20097989", "title": "Economic repression", "text": "Economic repression\n\nEconomic repression comprises various actions to restrain certain economical activities or social groups involved in economic activities. It contrasts with economic liberalization. Economists note widespread economic repression in developing countries.\n\nThe main goal of economic repression is protectionism, the instruments for which include fines and ceilings on interest rates or exchange rates.\n\nA common type of economic repression against individuals is blacklisting.\n"}
{"id": "1765418", "url": "https://en.wikipedia.org/wiki?curid=1765418", "title": "Ecosystem ecology", "text": "Ecosystem ecology\n\nEcosystem ecology is the integrated study of living (biotic) and non-living (abiotic) components of ecosystems and their interactions within an ecosystem framework. This science examines how ecosystems work and relates this to their components such as chemicals, bedrock, soil, plants, and animals.\n\nEcosystem ecology examines physical and biological structures and examines how these ecosystem characteristics interact with each other. Ultimately, this helps us understand how to maintain high quality water and economically viable commodity production. A major focus of ecosystem ecology is on functional processes, ecological mechanisms that maintain the structure and services produced by ecosystems. These include primary productivity (production of biomass), decomposition, and trophic interactions.\n\nStudies of ecosystem function have greatly improved human understanding of sustainable production of forage, fiber, fuel, and provision of water. Functional processes are mediated by regional-to-local level climate, disturbance, and management. Thus ecosystem ecology provides a powerful framework for identifying ecological mechanisms that interact with global environmental problems, especially global warming and degradation of surface water.\n\nThis example demonstrates several important aspects of ecosystems:\n\nThese characteristics also introduce practical problems into natural resource management. Who will manage which ecosystem? Will timber cutting in the forest degrade recreational fishing in the stream? These questions are difficult for land managers to address while the boundary between ecosystems remains unclear; even though decisions in one ecosystem will affect the other. We need better understanding of the interactions and interdependencies of these ecosystems and the processes that maintain them before we can begin to address these questions.\n\nEcosystem ecology is an inherently interdisciplinary field of study. An individual ecosystem is composed of populations of organisms, interacting within communities, and contributing to the cycling of nutrients and the flow of energy. The ecosystem is the principal unit of study in ecosystem ecology.\n\nPopulation, community, and physiological ecology provide many of the underlying biological mechanisms influencing ecosystems and the processes they maintain. Flowing of energy and cycling of matter at the ecosystem level are often examined in ecosystem ecology, but, as a whole, this science is defined more by subject matter than by scale. Ecosystem ecology approaches organisms and abiotic pools of energy and nutrients as an integrated system which distinguishes it from associated sciences such as biogeochemistry.\n\nBiogeochemistry and hydrology focus on several fundamental ecosystem processes such as biologically mediated chemical cycling of nutrients and physical-biological cycling of water. Ecosystem ecology forms the mechanistic basis for regional or global processes encompassed by landscape-to-regional hydrology, global biogeochemistry, and earth system science.\n\nEcosystem ecology is philosophically and historically rooted in terrestrial ecology. The ecosystem concept has evolved rapidly during the last 100 years with important ideas developed by Frederic Clements, a botanist who argued for specific definitions of ecosystems and that physiological processes were responsible for their development and persistence. Although most of Clements ecosystem definitions have been greatly revised, initially by Henry Gleason and Arthur Tansley, and later by contemporary ecologists, the idea that physiological processes are fundamental to ecosystem structure and function remains central to ecology.\n\nIn this model, energy flows through the whole system were dependent on biotic and abiotic interactions of each individual component (species, inorganic pools of nutrients, etc.). Later work demonstrated that these interactions and flows applied to nutrient cycles, changed over the course of succession, and held powerful controls over ecosystem productivity. Transfers of energy and nutrients are innate to ecological systems regardless of whether they are aquatic or terrestrial. Thus, ecosystem ecology has emerged from important biological studies of plants, animals, terrestrial, aquatic, and marine ecosystems.\n\nEcosystem services are ecologically mediated functional processes essential to sustaining healthy human societies. Water provision and filtration, production of biomass in forestry, agriculture, and fisheries, and removal of greenhouse gases such as carbon dioxide (CO) from the atmosphere are examples of ecosystem services essential to public health and economic opportunity. Nutrient cycling is a process fundamental to agricultural and forest production.\n\nHowever, like most ecosystem processes, nutrient cycling is not an ecosystem characteristic which can be “dialed” to the most desirable level. Maximizing production in degraded systems is an overly simplistic solution to the complex problems of hunger and economic security. For instance, intensive fertilizer use in the midwestern United States has resulted in degraded fisheries in the Gulf of Mexico. Regrettably, a “Green Revolution” of intensive chemical fertilization has been recommended for agriculture in developed and developing countries. These strategies risk alteration of ecosystem processes that may be difficult to restore, especially when applied at broad scales without adequate assessment of impacts. Ecosystem processes may take many years to recover from significant disturbance.\n\nFor instance, large-scale forest clearance in the northeastern United States during the 18th and 19th centuries has altered soil texture, dominant vegetation, and nutrient cycling in ways that impact forest productivity in the present day. An appreciation of the importance of ecosystem function in maintenance of productivity, whether in agriculture or forestry, is needed in conjunction with plans for restoration of essential processes. Improved knowledge of ecosystem function will help to achieve long-term sustainability and stability in the poorest parts of the world.\n\nBiomass productivity is one of the most apparent and economically important ecosystem functions. Biomass accumulation begins at the cellular level via photosynthesis. Photosynthesis requires water and consequently global patterns of annual biomass production are correlated with annual precipitation. Amounts of productivity are also dependent on the overall capacity of plants to capture sunlight which is directly correlated with plant leaf area and N content.\n\nNet primary productivity (NPP) is the primary measure of biomass accumulation within an ecosystem. Net primary productivity can be calculated by a simple formula where the total amount of productivity is adjusted for total productivity losses through maintenance of biological processes:\n\nNPP is difficult to measure but a new technique known as eddy co-variance has shed light on how natural ecosystems influence the atmosphere. Figure 4 shows seasonal and annual changes in CO concentration measured at Mauna Loa, Hawaii from 1987 to 1990. CO concentration steadily increased, but within-year variation has been greater than the annual increase since measurements began in 1957.\n\nThese variations were thought to be due to seasonal uptake of CO during summer months. A newly developed technique for assessing ecosystem NPP has confirmed seasonal variation are driven by seasonal changes in CO uptake by vegetation. This has led many scientists and policy makers to speculate that ecosystems can be managed to ameliorate problems with global warming. This type of management may include reforesting or altering forest harvest schedules for many parts of the world.\n\nDecomposition and nutrient cycling are fundamental to ecosystem biomass production. Most natural ecosystems are nitrogen (N) limited and biomass production is closely correlated with N turnover.\nTypically external input of nutrients is very low and efficient recycling of nutrients maintains productivity. Decomposition of plant litter accounts for the majority of nutrients recycled through ecosystems (Figure 3). Rates of plant litter decomposition are highly dependent on litter quality; high concentration of phenolic compounds, especially lignin, in plant litter has a retarding effect on litter decomposition. More complex C compounds are decomposed more slowly and may take many years to completely breakdown. Decomposition is typically described with exponential decay and has been related to the mineral concentrations, especially manganese, in the leaf litter.\nGlobally, rates of decomposition are mediated by litter quality and climate. Ecosystems dominated by plants with low-lignin concentration often have rapid rates of decomposition and nutrient cycling (Chapin et al. 1982). Simple carbon (C) containing compounds are preferentially metabolized by decomposer microorganisms which results in rapid initial rates of decomposition, see Figure 5A, models that depend on constant rates of decay; so called “k” values, see Figure 5B. In addition to litter quality and climate, the activity of soil fauna is very important \n\nHowever, these models do not reflect simultaneous linear and non-linear decay processes which likely occur during decomposition. For instance, proteins, sugars and lipids decompose exponentially, but lignin decays at a more linear rate Thus, litter decay is inaccurately predicted by simplistic models.\n\nA simple alternative model presented in Figure 5C shows significantly more rapid decomposition that the standard model of figure 4B. Better understanding of decomposition models is an important research area of ecosystem ecology because this process is closely tied to nutrient supply and the overall capacity of ecosystems to sequester CO from the atmosphere.\n\nTrophic dynamics refers to process of energy and nutrient transfer between organisms. Trophic dynamics is an important part of the structure and function of ecosystems. Figure 3 shows energy transferred for an ecosystem at Silver Springs, Florida. Energy gained by primary producers (plants, P) is consumed by herbivores (H), which are consumed by carnivores (C), which are themselves consumed by “top- carnivores”(TC).\n\nOne of the most obvious patterns in Figure 3 is that as one moves up to higher trophic levels (i.e. from plants to top-carnivores) the total amount of energy decreases. Plants exert a “bottom-up” control on the energy structure of ecosystems by determining the total amount of energy that enters the system.\n\nHowever, predators can also influence the structure of lower trophic levels from the top-down. These influences can dramatically shift dominant species in terrestrial and marine systems The interplay and relative strength of top-down vs. bottom-up controls on ecosystem structure and function is an important area of research in the greater field of ecology.\n\nTrophic dynamics can strongly influence rates of decomposition and nutrient cycling in time and in space. For example, herbivory can increase litter decomposition and nutrient cycling via direct changes in litter quality and altered dominant vegetation. Insect herbivory has been shown to increase rates of decomposition and nutrient turnover due to changes in litter quality and increased frass inputs. \nHowever, insect outbreak does not always increase nutrient cycling. Stadler showed that C rich honeydew produced during aphid outbreak can result in increased N immobilization by soil microbes thus slowing down nutrient cycling and potentially limiting biomass production. North atlantic marine ecosystems have been greatly altered by overfishing of cod. Cod stocks crashed in the 1990s which resulted in increases in their prey such as shrimp and snow crab Human intervention in ecosystems has resulted in dramatic changes to ecosystem structure and function. These changes are occurring rapidly and have unknown consequences for economic security and human well-being.\n\nThe biosphere has been greatly altered by the demands of human societies. Ecosystem ecology plays an important role in understanding and adapting to the most pressing current environmental problems. Restoration ecology and ecosystem management are closely associated with ecosystem ecology. Restoring highly degraded resources depends on integration of functional mechanisms of ecosystems.\n\nWithout these functions intact, economic value of ecosystems is greatly reduced and potentially dangerous conditions may develop in the field. For example, areas within the mountainous western highlands of Guatemala are more susceptible to catastrophic landslides and crippling seasonal water shortages due to loss of forest resources. In contrast, cities such as Totonicapán that have preserved forests through strong social institutions have greater local economic stability and overall greater human well-being.\n\nThis situation is striking considering that these areas are close to each other, the majority of inhabitants are of Mayan descent, and the topography and overall resources are similar. This is a case of two groups of people managing resources in fundamentally different ways. Ecosystem ecology provides the basic science needed to avoid degradation and to restore ecosystem processes that provide for basic human needs.\n"}
{"id": "15228679", "url": "https://en.wikipedia.org/wiki?curid=15228679", "title": "Elkanah Billings", "text": "Elkanah Billings\n\nElkanah Billings (May 5, 1820 – June 14, 1876) is often referred to as Canada's first paleontologist. Billings was born on a farm by the Rideau River outside Bytown (Ottawa), now known as Billings Estate. His parents were named Lamira and Braddish Billings. His family included an older sister named Sabra and an older brother Maj Braddish Billings Jr, who practised as an architect and served in the North-West Rebellion. His brother W. Ross Billings also practised as an architect. His younger siblings were Samuel, Sarah and Charles. He was originally educated in law and in 1845, he was called to the Canadian bar. In 1852, he founded the journal the \"Canadian Naturalist (and Geologist)\". He continued to practise law until 1856, when he was hired to be the first paleontologist for Geological Survey of Canada (GSC). In his lifetime, he identified 1065 new species and 61 new genera, including \"Aspidella\", the first documented fossil of the Ediacaran biota.\n\nHe married Helen Walker Wilson in 1845. However, he was childless.\n\n\n\n"}
{"id": "38501582", "url": "https://en.wikipedia.org/wiki?curid=38501582", "title": "Emily Graslie", "text": "Emily Graslie\n\nEmily Graslie is an American science communicator and YouTube educator. She started volunteering at the Philip L. Wright Zoological Museum at the University of Montana in 2011. After appearing in a VlogBrothers video by Hank Green in 2012, she was asked to join the Nerdfighter network. She currently stars in her own educational YouTube channel called \"The Brain Scoop.\" Graslie is now employed by the Field Museum as their first-ever Chief Curiosity Correspondent.\n\nGraslie earned her bachelor's degree in Studio Art from the University of Montana in 2011. As a part of that program, she interned at the Philip L. Wright Zoological Museum in her senior year. Graslie became a full-time curatorial volunteer after she graduated, while working on her master's degree in museum studies. She cleaned new specimens, gave tours, trained new interns, and acted as a teaching assistant for a class at the University of Montana.\n\nIn June 2013, Graslie was hired by Chicago's Field Museum of Natural History to become their 'Chief Curiosity Correspondent'. She continues to host \"The Brain Scoop\" from this new location. She was the keynote speaker at the Chicago March for Science on April 22, 2017.\n\nGraslie first appeared on YouTube in Hank Green's December 7, 2012 VlogBrothers video. In the video, she showed Green a wide variety of the specimens in the lab. Because of her ease in front of the camera, enthusiasm, and fan comments, Graslie was asked to create her own YouTube channel, \"The Brain Scoop\", as a part of the Nerdfighter family. The series debuted in January 2013. Her work on the series has been described by journalists as \"articulate and hilarious\" as well as enthusiastic.\n\nHer November 27, 2013 video, which addressed the situation of women in STEM fields and inappropriate comments received to her own postings, received a high level of media attention. In January 2014, Amy Wallace, another science journalist, wrote an article about how science journalists can find themselves the target of ugly personal attacks, and the attacks on female journalists include criticisms of their sexual attractiveness, and their sexual morality. Wallace included Graslie when she listed half a dozen fellow female science journalists whose reasonable, science-based articles on controversial topics had triggered crude abusive backlashes.\n\nIn 2014, her channel \"The Brain Scoop\" was listed on New Media Rockstars Top 100 Channels, ranked at #96.\n\nIn 2016, she documented the efforts to help the recovery of the Kankakee mallow, an endangered species that is endemic to Illinois. As the only floral species that is only found in that state, she has started an effort to make it the official Illinois state flower, proposing that it replace the more generic Violet.\n\nIn recognition of her science education efforts, a butterfly species, \"Wahydra graslieae\", was named in her honor. The species is a grass skipper discovered in Ecuador.\n\nAndy Warren, senior collections manager of the Florida Museum of Natural History’s McGuire Center for Lepidoptera and Biodiversity, said \"We thought that after spending years explaining why specimens are important and bringing natural history collections to the attention of the public, Emily was definitely someone who should have a bug named after her.\"\n\n\n"}
{"id": "38974906", "url": "https://en.wikipedia.org/wiki?curid=38974906", "title": "Finkbeiner test", "text": "Finkbeiner test\n\nThe Finkbeiner test is a checklist proposed to help journalists avoid gender bias in media articles about women in science.\n\nThe Finkbeiner test is a checklist proposed by journalist Christie Aschwanden to help journalists avoid gender bias in media articles about women in science. To pass the test, an article about a female scientist must not mention:\n\nAschwanden formulated the test in an article in \"Double X Science\", an online science magazine for women, on 5 March 2013. She did so in response to what she considered was a type of media coverage of women scientists that:\nAschwanden created the test in the spirit of the Bechdel test, which is used to indicate gender bias in fiction. She named the test after fellow journalist Ann Finkbeiner, who had written a story for the science blog \"The Last Word on Nothing\" about her decision not to write about the subject of her latest article, an astronomer, \"as a woman\". \n\nThe test was mentioned in the media criticism of the \"New York Times\"s obituary of rocket scientist Yvonne Brill. That obituary, published on 30 March 2013, by Douglas Martin, began with the words: \"She made a mean beef stroganoff, followed her husband from job to job and took eight years off from work to raise three children\". A few hours after publication the \"New York Times\" revised the obituary to address some of the criticisms; the revised version begins \"She was a brilliant rocket scientist who followed her husband from job to job...\" \n\nAnother \"New York Times\" article, on Jennifer Doudna, published on 11 May 2015, drew similar criticism with reference to the Finkbeiner test. An article in \"The Globe and Mail\" on astrophysicist Victoria Kaspi, published on 16 February 2016, drew the same criticism, as did David Quammen's book \"A Tangled Tree\", for giving women scientists, especially Lynn Margulis, short shrift.\n\nSusan Gelman, Professor of Psychology at the University of Michigan, applauded the move to report on female scientists without emphasising their gender, but questions whether the Finkbeiner test should seek to eliminate all references to personal life, suggesting that the move should be towards asking male scientists about personal issues too. This view is shared by other writers. In addition, Vasudevan Mukunth points out in \"The Wire\", countries in which women are drastically under-represented in science might want to bend the test's rules in hopes of highlighting any systemic barriers: \"The test's usefulness rests on the myth of a level playing field – there is none in India.\" \n\nThe \"Reversed Finkbeiner\" approach is an exercise in which students are asked to write an article about a male scientist that would fail the Finkbeiner test if it were about a woman.\n"}
{"id": "55407740", "url": "https://en.wikipedia.org/wiki?curid=55407740", "title": "Gas immersion laser doping", "text": "Gas immersion laser doping\n\nGas immersion laser doping (GILD) is a method of doping a semiconductor material such as silicon.\n\nIn the case of doping silicon with boron to create a P-type semiconductor material, a thin wafer of silicon is placed in a containment chamber and is immersed in boron gas. A pulsed laser is directed at the silicon wafer and this results in localised melting and subsequent recrystalisation of the silicon wafer material, allowing boron atoms in the gas to diffuse into the molten sections of the silicon wafer. The end result of this process is a silicon wafer with boron impurities, creating a P-type semiconductor.\n\n"}
{"id": "15275923", "url": "https://en.wikipedia.org/wiki?curid=15275923", "title": "Glycoinformatics", "text": "Glycoinformatics\n\nGlycoinformatics is a field of bioinformatics that pertains to the study of carbohydrates involved in protein post-translational modification. It broadly includes (but is not restricted to) database, software, and algorithm development for the study of carbohydrate structures, glycoconjugates, enzymatic carbohydrate synthesis and degradation, as well as carbohydrate interactions. Conventional usage of the term does not currently include the treatment of carbohydrates from the more well-known nutritive aspect.\n\nEven though glycosylation is the most common form of protein modification, with highly complex carbohydrate structures, the bioinformatics on glycome is still very poor. \n\nUnlike proteins and nucleic acids which are linear, carbohydrates are often branched and extremely complex. For instance, just four sugars can be strung together to form more than 5 million different types of carbohydrates or nine different sugars may be assembled into 15 million possible four-sugar-chains. \n\nAlso, the number of simple sugars that make up glycans is more than the number of nucleotides that make up DNA or RNA. Therefore, it is more computationally expensive to evaluate their structures. \n\nOne of the main constrains in the glycoinformatics is the difficulty of representing sugars in the sequence form especially due to their branching nature. Owing to the lack of a genetic blue print, carbohydrates do not have a \"fixed\" sequence. Instead, the sequence is largely determined by the presence of a variety of enzymes, their kinetic differences and variations in the biosynthetic micro-environment of the cells. This increases the complexity of analysis and experimental reproducibility of the carbohydrate structure of interest. It is for this reason that carbohydrates are often considered as the \"information poor\" molecules.\n\nKeeping an up to date database is not easy given that there are differences in naming, graphical representations, and what information is provided pertaining to a given structure. \n"}
{"id": "57104312", "url": "https://en.wikipedia.org/wiki?curid=57104312", "title": "Haplogroup A-P305", "text": "Haplogroup A-P305\n\nHaplogroup A-P305 also known as A1 is a Human Y-chromosome DNA haplogroup. Like its parent haplogroup haplogroup A0-T (A-L1085), A1 includes the vast majority of living human males. It emerged in Africa approximately 161,300 years ago. By comparison, members of its sole sibling subclade, haplogroup A0 – the only other primary subclade of haplogroup A0-T – are found mostly in \n\nBasal, undivergent A-P305* is largely restricted to populations native to Africa, though a handful of cases have been reported in Europe and Western Asia. A-P305* is found at its highest rates in Bakola Pygmies (South Cameroon) at 8.3% and Berbers from Algeria at 1.5% and in Ghana. The clade also achieves high frequencies in the Bushmen hunter-gatherer populations of Southern Africa, followed closely by many Nilotic groups in Eastern Africa. However, haplogroup A's oldest sub-clades are exclusively found in Central-Northwest Africa, where it, and consequently Y-chromosomal Adam, is believed to have originated about 140,000 years ago. The clade has also been observed at notable frequencies in certain populations in Ethiopia, as well as some Pygmy groups in Central Africa.\n"}
{"id": "2433416", "url": "https://en.wikipedia.org/wiki?curid=2433416", "title": "Home Army and V-1 and V-2", "text": "Home Army and V-1 and V-2\n\nDuring World War II, the Polish resistance Home Army (\"Armia Krajowa\"), which conducted military operations against occupying German forces, was also heavily involved in intelligence work. This included operations investigating the German \"Wunderwaffe\": the V-1 flying bomb and the V-2 rocket. British intelligence received their first Polish report regarding the development of these weapons at Peenemünde in 1943.\n\nBy the summer of 1941 Home Army intelligence began receiving reports from its field units regarding some kind of secret tests being carried out by the Germans on the island of Usedom in the Baltic Sea. A special \"Bureau\" was formed within intelligence group \"Lombard\", charged with espionage inside the 3rd Reich and the Polish areas incorporated into it after 1939, to investigate the matter and to coordinate future actions. Specialized scientific expertise was provided to the group by the engineer Antoni Kocjan, \"Korona\", a renowned pre-war glider constructor. Furthermore, as part of their operations the \"Bureau\" managed to recruit an Austrian Anti-Nazi, Roman Traeger (T-As2), who was serving as an NCO in the Wehrmacht and was stationed on Usedom. Trager provided the AK with more detailed information regarding the \"flying torpedoes\" and pinpointed Peenemünde on Usedom as the site of the tests. The information obtained led to the first report from the AK to the British which was purportedly written by Jerzy Chmielewski, \"Rafal\", who was in charge of processing economic reports the \"Lombard\" group obtained.\n\nAfter V-2 flight testing began near the village of Blizna, south of Mielec (the first launch from there was on November 5, 1943), the AK had a unique opportunity to gather more information and to intercept parts of test rockets (most of which did not explode).\nThe AK quickly located the new testing ground at Blizna thanks to reports from local farmers and AK field units, who managed to obtain on their own pieces of the fired rockets, by arriving on the scene before German patrols. In late 1943 in cooperation with British intelligence, a plan was formed to make an attempt to capture a whole unexploded V-2 rocket and transport it to Britain.\n\nAt the time, opinion within British intelligence was divided. One group tended to believe the AK accounts and reports, while another was highly sceptical and argued that it was impossible to launch a rocket of the size reported by the AK using any known fuel.\nThen in early March 1944, British Intelligence Headquarters received a report of a Polish Underground worker (code name \"Makary\") who had crawled up to the Blizna railway line and saw on a flatcar heavily guarded by SS troops \"an object which, though covered by a tarpaulin, bore every resemblance to a monstrous torpedo.\" The Polish intelligence also informed the British about usage of liquid oxygen in a radio report from June 12, 1944. Some experts within both British and Polish intelligence communities quickly realized that learning the nature of the fuel utilized by the rockets was crucial, and hence, the need to obtain a working example.\n\nFrom April 1944, numerous test rockets were falling near Sarnaki village, in the vicinity of the Bug River, south of Siemiatycze. The number of parts collected by the Polish intelligence increased. They were then analyzed by the Polish scientists in Warsaw. According to some reports, around May 20, 1944, a relatively undamaged V-2 rocket fell on the swampy bank of the Bug near Sarnaki and local Poles managed to hide it before German arrival. Subsequently, the rocket was dismantled and smuggled across Poland. Operation Most III (Bridge III) secretly transported parts of the rocket out of Poland for analysis by British intelligence.\n\nWhile the early knowledge on a rocket by AK was quite a feat in pure intelligence terms, it did not necessarily translate into significant results on the ground. On the other hand, the AK did alert the British as to the dangers posed by both missile designs, which led them to allocate more resources to bombing production and launching sites and thus lessened the eventual devastation caused by them. Also, the Operation Hydra bombing raid on Peenemünde, purportedly carried out on the basis of Home Army intelligence, did delay the V-2 by six to eight weeks.\n\n\n"}
{"id": "317467", "url": "https://en.wikipedia.org/wiki?curid=317467", "title": "Homunculus", "text": "Homunculus\n\nA homunculus (; Latin for \"little person\") is a representation of a small human being. Popularized in sixteenth-century alchemy and nineteenth-century fiction, it has historically referred to the creation of a miniature, fully formed human. The concept has roots in preformationism as well as earlier folklore and alchemic traditions.\n\nThe homunculus first appears by name in alchemical writings attributed to Paracelsus (1493–1541). \"De natura rerum\" (1537) outlines his method for creating homunculi:\n\nComparisons have been made with several similar concepts in the writings of earlier alchemists. Although the actual word \"homunculus\" was never used, Carl Jung believed that the concept first appeared in the \"Visions of Zosimos\", written in the third century AD. In the visions, Zosimos encounters a priest who changes into \"the opposite of himself, into a mutilated \"anthroparion\"\". The Greek word \"anthroparion\" is similar to \"homunculus\" – a diminutive form of \"person\". Zosimos subsequently encounters other anthroparion in his dream but there is no mention of the creation of artificial life. In his commentary, Jung equates the homunculus with the Philosopher's Stone, and the \"inner person\" in parallel with Christ.\n\nIn Islamic alchemy, \"Takwin\" () was a goal of certain Muslim alchemists, a notable one being Jābir ibn Hayyān. In the alchemical context, \"Takwin\" refers to the artificial creation of life in the laboratory, up to and including human life.\n\nThe homunculus continued to appear in alchemical writings after Paracelsus' time. The \"Chymical Wedding of Christian Rosenkreutz\" (1616) for example, concludes with the creation of a male and female form identified as \"Homunculi duo\". The allegorical text suggests to the reader that the ultimate goal of alchemy is not chrysopoeia, but it is instead the artificial generation of humans. Here, the creation of homunculi symbolically represents spiritual regeneration and Christian soteriology.\n\nIn 1775, Count Johann Ferdinand von Kufstein, together with Abbé Geloni, an Italian cleric, are reputed to have created ten homunculi with the ability to foresee the future, which von Kufstein kept in glass containers at his Masonic lodge in Vienna. Dr. Emil Besetzny's Masonic handbook, \"Die Sphinx\", devoted an entire chapter to the \"wahrsagenden Geister\" (scrying ghosts). These are reputed to have been seen by several people, including local dignitaries.\n\nReferences to the homunculus do not appear prior to sixteenth-century alchemical writings; however, alchemists may have been influenced by earlier folk traditions. The mandragora, known in German as \"Alreona\", \"Alraun\" or \"Alraune\" is one example.\n\nIn \"Liber de imaginibus\", Paracelsus however denies that roots shaped like men grow naturally. He attacks dishonest people who carve roots to look like men and sell them as Alraun. He clarifies that the homunculus’ origins are in sperm, and that it is falsely confused with these ideas from necromancy and natural philosophy.\n\nThe homunculus has also been compared to the golem of Jewish folklore. Though the specifics outlining the creation of the golem and homunculus are very different, the concepts both metaphorically relate man to the divine, in his construction of life in his own image.\n\nPreformationism is the formerly-popular theory that animals developed from miniature versions of themselves. Sperm were believed to contain complete preformed individuals called \"animalcules\". Development was therefore a matter of enlarging this into a fully formed being. The term homunculus was later used in the discussion of conception and birth.\n\nNicolas Hartsoeker postulated the existence of animalcules in the semen of humans and other animals. This was the beginning of spermists' theory, who held the belief that the sperm was in fact a \"little man\" that was placed inside a woman for growth into a child. This seemed to them to neatly explain many of the mysteries of conception. It was later pointed out that if the sperm was a homunculus, identical in all but size to an adult, then the homunculus may have sperm of its own. This led to a \"reductio ad absurdum\" with a chain of homunculi \"all the way down\". This was not necessarily considered by spermists a fatal objection however, as it neatly explained how it was that \"in Adam\" all had sinned: the whole of humanity was already contained in his loins. The spermists' theory also failed to explain why children tend to resemble their mothers as well as their fathers, though some spermists believed that the growing homunculus assimilated maternal characteristics from the womb environment in which they grew.\n\nThe homunculus is commonly used today in scientific disciplines such as psychology as a teaching or memory tool to describe the distorted scale model of a human drawn or sculpted to reflect the relative space human body parts occupy on the somatosensory cortex (the \"sensory homunculus\") and the motor cortex (the \"motor homunculus\"). Both the motor and sensory homunculi usually appear as small men superimposed over the top of precentral or postcentral gyri for motor and sensory cortices, respectively. The homunculus is oriented with feet medial and shoulders lateral on top of both the precentral and the postcentral gyrus (for both motor and sensory). The man's head is depicted upside down in relation to the rest of the body such that the forehead is closest to the shoulders. The lips, hands, feet and sex organs have more sensory neurons than other parts of the body, so the homunculus has correspondingly large lips, hands, feet, and genitals. The motor homunculus is very similar to the sensory homunculus, but differs in several ways. Specifically, the motor homunculus has a portion for the tongue most lateral while the sensory homunculus has an area for genitalia most medial and an area for visceral organs most lateral. Well known in the field of neurology, this is also commonly called \"the little man inside the brain.\" This scientific model is known as the cortical homunculus.\n\nIn medical science, the term homunculus is sometimes applied to certain fetus-like ovarian cystic teratomae. These will sometimes contain hair, sebaceous material and in some cases cartilagous or bony structures.\n\nHomunculi can be found in centuries worth of literature. These fictions are primarily centred around imaginative speculations on the quest for artificial life associated with Paracelsian alchemy. One of the very earliest literary references occurs in Thomas Browne's \"Religio Medici\" (1643), in which the author states:\n\nThe fable of the alchemically-created homunculus may have been central in Mary Shelley's novel \"Frankenstein\" (1818). Professor Radu Florescu suggests that Johann Conrad Dippel, an alchemist born in Castle Frankenstein, might have been the inspiration for Victor Frankenstein. German playwright Johann Wolfgang von Goethe's \"Faust, Part Two\" (1832) famously features an alchemically-created homunculus. Here, the character of Homunculus embodies the quest of a pure spirit to be born into mortal form, contrasting Faust's desire to shed his mortal body to become pure spirit. The alchemical idea that the soul is not imprisoned in the body, but instead may find its brightest state as it passes through the material plane is central to the character.\nWilliam Makepeace Thackeray wrote under the pen name of Homunculus.\n\nThe homunculus legend, \"Frankenstein\" and \"Faust\" have continued to influence works in the twentieth and twenty-first century. The theme has been used not only in fantasy literature, but also to illuminate social topics. For instance, the British children's writers Mary Norton and Rumer Godden used homunculus motifs in their work, expressing various post-war anxieties about refugees, persecution of minorities in war, and the adaptation of these minorities to a \"big\" world. W. Somerset Maugham's 1908 novel \"The Magician\" utilises the concept of the homunculus as an important plot element. David H. Keller’s short story \"A Twentieth-Century Homunculus\" (1930) describes the creation of homunculi on an industrial scale by a pair of misogynists. Likewise, Sven Delblanc’s \"The Homunculus: A Magic Tale\" (1965) addresses misogyny and the Cold War industrial-military complexes of the Soviet Union and NATO.\n\nHomunculi appear in fantasy based television, film, and games in a manner consistent with literature. Examples can be found in numerous mediums, such as the films \"Homunculus\" (1916), \"Bride of Frankenstein\" (1935), \"The Golden Voyage of Sinbad\" (1973), the television movie \"Don't Be Afraid of the Dark\" (1973), \"Being John Malkovich\" (1999), Guillermo del Toro's \"The Devil's Backbone\" (2001), J.K. Rowling’s \"\" (2018), and the big-screen remake of \"Don't Be Afraid of the Dark\" (2011), fantasy role-playing games (such as \"Dungeons & Dragons\"), video games (such as \"Ragnarok Online\", \"Valkyrie Profile\", \" Shadow of Memories\", \"The Legend of Heroes series\", \"\"), books (such as \"The Secret Series\"), graphic novels (such as \"Bureau for Paranormal Research and Defense\") and manga (such as \"Homunculus\", \"Fullmetal Alchemist\", Fate/Zero, and \"Gosick\").\n\n\n"}
{"id": "23781390", "url": "https://en.wikipedia.org/wiki?curid=23781390", "title": "Index of pesticide articles", "text": "Index of pesticide articles\n\nThis is an index of articles relating to pesticides.\n\n"}
{"id": "38448468", "url": "https://en.wikipedia.org/wiki?curid=38448468", "title": "Index of physics articles (R)", "text": "Index of physics articles (R)\n\nThe index of physics articles is split into multiple pages due to its size.\n\nTo navigate by individual letter use the table of contents below.\n\n"}
{"id": "19129716", "url": "https://en.wikipedia.org/wiki?curid=19129716", "title": "Inspection time", "text": "Inspection time\n\nInspection time refers to the exposure duration required for a human subject to reliably identify a simple stimulus. Typically a stimulus made up of two parallel lines differing in length and joined at the tops by a cross bar is presented (similar to the Greek letter Pi). The ability to quickly detect the identity of a stimulus is moderately heritable and correlates with the subject's IQ.\n\nIf asked which of the two lines in the figure below is longer; the left or the right, almost all non-visually impaired subjects can answer correctly 100% of the time. If, however, the stimulus is backward masked after a short period of time, the proportion of correct responses declines as the exposure duration reduces, and reliable individual differences emerge in the percent identified correctly at different intervals.\n\nThe task itself was proposed by Doug Vickers as a measure of the rate of accumulation of information. Ted Nettelbeck, Chris Brand and others demonstrated it related quite strongly to psychometric intelligence, especially across the lower part of the IQ range \nsuggesting that differences in intelligence may reflect, in part, differences in the rate of information processing - a theory proposed by Arthur Jensen.\nOne version of the inspection time stimuli is shown below (1) with the stimulus (short left given as an example), which is replaced by a mask (2). (3) indicates the opportunity for the subject to report which stimulus they saw at their leisure.\n\nInspection time is moderately heritable. This work on the heritability of IT from Nick Martin's group also demonstrated, however, that perceptual speed does not have a causal role in intelligence, but rather that IQ and IT are distinct reflections of some shared biological processes.\n"}
{"id": "15761387", "url": "https://en.wikipedia.org/wiki?curid=15761387", "title": "International Futures", "text": "International Futures\n\nInternational Futures (IFs) is a global integrated assessment model designed to help in thinking strategically and systematically about key global systems (economic, demographic, education, health, environment, technology, domestic governance, infrastructure, agriculture, energy and environment) housed at the Frederick S. Pardee Center for International Futures. Initially created by Barry B. Hughes of the Josef Korbel School of International Studies at the University of Denver in Colorado, the model is free for public use in both its online and downloadable forms. \n\nThe Pardee Center for International Futures has partnered with many organizations to produce forecasts and data analysis. IFs has been utilized in the National Intelligence Council's \"Global Trends 2020\", \"Global Trends 2025\", and \"Global Trends 2030\" report. The International Futures model has also contributed to the United Nations \"Human Development Report\" and the \"Global Environmental Outlook\".\n\nIFs is hosted free for public use by Google Public Data Explorer, the Atlantic Council, and the Institute for Security Studies.\n\nThe model incorporates dynamically linked sub-models. They include: population, economic, agricultural, educational, energy, socio-political, international political, environmental, health, infrastructure and technology. IFs is a unique modeling tool because it endogenizes the impact of such a wide range of global systems for 183 countries.\n\nThe help system that accompanies the software provides an extensive overview of the model structure and computer code used to write the model. IFs has three main functions, all connected to its conceptual treatment of integrated assessment forecasts: data analysis, scenario analysis, and display.\n\nThe data analysis section of IFs represents a collection of over 2,000 data series from all major international data gatherers. It is constantly updated with new data series. This data forms the foundation of the model structure. Users can analyze historic data cross-sectionally, longitudinally or on a world map. Using cross-sectional analysis, users can select a variables and plot this against up to 5 independent variables. It is then possible to animate the map to see how the cross-sectional relationship changes across the 40+ years of data in the database. Longitudinally, users can plot the relationship between a dependent variable and time, from 1960 (for most data series) through the most recent data year available. A world map allows users to display data from any of these series using GIS options.\n\nThe software allows users to access and change the parameters and variables that are used in the model. The Scenario Analysis display lets users create their own global scenario or load a pre-run global scenario in their field of interest. For example, to analyze the effects of a policy intervention on different sub-models and variables within the model, make the changes to the appropriate variable and then analyze the results in comparison to the base-case. Many pre-run scenarios come packaged with the model, including work that has been completed for the United Nations Environment Programme and the National Intelligence Council.\n\nThis portion of the software allows users to display the forecast results of their scenario analyses for different provinces, countries and groups across different issue areas. Some of the specialized displays include: population, educational attainment, mortality rate, World Values Survey, Gini coefficient, the Millennium Development Goals, social accounting matrix, advanced sustainability analysis, and World Bank financial flows.\n\nThe project received a gift from Frederick S. Pardee, formerly of RAND, to construct the Pardee Center for International Futures at the University of Denver. It is responsible for the further institutionalization of the software, training sessions, and the continued work on the \"Patterns of Potential Human Progress\" (PPHP) volumes. The first PPHP volume discusses reduction in global poverty; the second, global education; the third, health care systems; the fourth, global infrastructure; the fifth, domestic governance.\n\nThe journal Poverty and Public Policy reviewed the first PPHP volume, and concluded the following:\n\nThe African Futures Project is a collaboration between the Institute for Security Studies and the Pardee Center for International Futures to promote long term strategic planning for African development. This collaboration has led to the publication of various African Futures Project Policy Briefs, monographs on long-term African Development and a website where the IFs model can be used specifically for exploring African development.\n\n"}
{"id": "56122872", "url": "https://en.wikipedia.org/wiki?curid=56122872", "title": "Journal of School Violence", "text": "Journal of School Violence\n\nThe Journal of School Violence is a quarterly peer-reviewed scientific journal covering the study of school violence. It was established in 2002 and was originally published by Haworth Press, but is now published by Taylor & Francis' subsidiary Routledge. The editors-in-chief are Ryan W. Randa (Sam Houston State University) and Brad W. Reyns (Weber State University). According to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 2.421.\n"}
{"id": "57769738", "url": "https://en.wikipedia.org/wiki?curid=57769738", "title": "Journal of Traditional Chinese Medicine", "text": "Journal of Traditional Chinese Medicine\n\nThe Journal of Traditional Chinese Medicine was the first English-language journal on the subject of traditional Chinese medicine, including acupuncture, herbal medicine, homeopathy, massotherapy, mind-body therapies, palliative care and other topics in complementary and alternative medicine.\n\nThe journal was original established in Chinese as \"Chung i tsa chih ying wen pan\" in 1955. The English edition of the \"Journal of Traditional Chinese Medicine\" was first published in 1981. It is jointly sponsored and published by the China Association of Chinese Medicine and the China Academy of Chinese Medical Sciences. Its headquarters are in Beijing. The journal is also published in German, Italian, Spanish, French and Portuguese editions.\n\nThe journal is abstracted and indexed in the following bibliographic databases:\n\n"}
{"id": "53839763", "url": "https://en.wikipedia.org/wiki?curid=53839763", "title": "Kenneth Ivan Golden", "text": "Kenneth Ivan Golden\n\nKenneth Ivan Golden from the University of Vermont, was awarded the status of Fellow in the American Physical Society, after they were nominated by their Division of Plasma Physics in 1991, for \"pioneering work in the theory of dynamical processes in strongly coupled plasmas; for extending the theory to the analysis of binary ion mixtures and of two dimensional electron systems; for contributions to the theory of the structure of shock waves in magnetized plasmas.\"\n"}
{"id": "17994", "url": "https://en.wikipedia.org/wiki?curid=17994", "title": "Learning theory (education)", "text": "Learning theory (education)\n\nLearning theories are conceptual frameworks that describe how students absorb, process, and retain knowledge during learning. Cognitive, emotional, and environmental influences, as well as prior experience, all play a part in how understanding, or a world view, is acquired or changed and knowledge and skills retained.\n\nBehaviorists look at learning as an aspect of conditioning and advocate a system of rewards and targets in education. Educators who embrace cognitive theory believe that the definition of learning as a change in behavior is too narrow, and study the learner rather than their environment—and in particular the complexities of human memory. Those who advocate constructivism believe that a learner's ability to learn relies largely on what they already know and understand, and the acquisition of knowledge should be an individually tailored process of construction. Transformative learning theory focuses on the often-necessary change required in a learner's preconceptions and world view. Geographical learning theory focuses on the ways that contexts and environments shape the learning process.\n\nOutside the realm of educational psychology, techniques to directly observe the functioning of the brain during the learning process, such as event-related potential and functional magnetic resonance imaging, are used in educational neuroscience. The theory of multiple intelligences, where learning is seen as the interaction between dozens of different functional areas in the brain each with their own individual strengths and weaknesses in any particular human learner, has also been proposed, but empirical research has found the theory to be unsupported by evidence.\n\nPlato (428 BC–347 BC) proposed the question: How does an individual learn something new when the topic is brand new to that person? This question may seem trivial; however, think of a human like a computer. The question would then become: How does a computer take in any factual information without previous programming? Plato answered his own question by stating that knowledge is present at birth and all information learned by a person is merely a recollection of something the soul has already learned previously, which is called the Theory of Recollection or Platonic epistemology. This answer could be further justified by a paradox: If a person knows something, they don't need to question it, and if a person does not know something, they don't know to question it. Plato says that if one did not previously know something, then they cannot learn it. He describes learning as a passive process, where information and knowledge are ironed into the soul over time. However, Plato's theory elicits even more questions about knowledge: If we can only learn something when we already had the knowledge impressed onto our souls, then how did our souls gain that knowledge in the first place? Plato's theory can seem convoluted; however, his classical theory can still help us understand knowledge today.\n\nJohn Locke (1632–1704) offered an answer to Plato's question as well. John Locke offered the \"blank slate\" theory where humans are born into the world with no innate knowledge. He recognized that something had to be present, however. This something, to John Locke, seemed to be \"mental powers\". Locke viewed these powers as a biological ability the baby is born with, similar to how a baby knows how to biologically function when born. So as soon as the baby enters the world, it immediately has experiences with its surroundings and all of those experiences are being transcribed to the baby's \"slate\". All of the experiences then eventually culminate into complex and abstract ideas. This theory can still help teachers understand their students' learning today.\n\nThe term \"behaviorism\" was coined by John Watson (1878–1959). Watson believed the behaviorist view is a purely objective experimental branch of natural science with a goal to predict and control behavior. In an article in the \"Psychological Review\", he stated that, \"Its theoretical goal is the prediction and control of behavior. Introspection forms no essential part of its methods, nor is the scientific value of its data dependent upon the readiness with which they lend themselves to interpretation in terms of consciousness.\"\n\nMethodological behaviorism is based on the theory of only explaining public events, or observable behavior. B.F. Skinner introduced another type of behaviorism called radical behaviorism, or the conceptual analysis of behavior, which is based on the theory of also explaining private events; particularly, thinking and feelings. Radical behaviorism forms the conceptual piece of behavior analysis.\n\nIn behavior analysis, learning is the acquisition of a new behavior through conditioning and social learning.\n\nThere are three types of conditioning and learning:\n\nIvan Pavlov discovered classical conditioning. He observed that if dogs come to associate the delivery of food with a white lab coat or the ringing of a bell, they produce saliva, even when there is no sight or smell of food. Classical conditioning considers this form of learning the same, whether in dogs or in humans. Operant conditioning reinforces this behavior with a reward or a punishment. A reward increases the likelihood of the behavior recurring, a punishment decreases its likelihood. Social learning theory observes behavior and is followed with modeling.\n\nThese three learning theories form the basis of applied behavior analysis, the application of behavior analysis, which uses analyzed antecedents, functional analysis, replacement behavior strategies, and often data collection and reinforcement to change behavior. The old practice was called behavior modification, which only used \"assumed\" antecedents and consequences to change behavior without acknowledging the conceptual analysis; analyzing the function of behavior and teaching of new behaviors that would serve the same function was never relevant in behavior modification.\n\nBehaviorists view the learning process as a change in behavior, and arrange the environment to elicit desired responses through such devices as behavioral objectives, Competency-based learning, and skill development and training. Educational approaches such as Early Intensive Behavioral Intervention, curriculum-based measurement, and direct instruction have emerged from this model.\n\nTransfer of learning is the idea that what one learns in school somehow carries over to situations different from that particular time and that particular setting. Transfer was amongst the first phenomena tested in educational psychology. Edward Lee Thorndike was a pioneer in transfer research. He found that though transfer is extremely important for learning, it is a rarely occurring phenomenon. In fact, he held an experiment where he had the subjects estimate the size of a specific shape and then he would switch the shape. He found that the prior information did not help the subjects; instead it impeded their learning.\n\nOne explanation of why transfer does not occur often involves surface structure and deep structure. The surface structure is the way a problem is framed. The deep structure is the steps for the solution. For example, when a math story problem changes contexts from asking how much it costs to reseed a lawn to how much it costs to varnish a table, they have different surface structures, but the steps for getting the answers are the same. However, many people are more influenced by the surface structure. In reality, the surface structure is unimportant. Nonetheless, people are concerned with it because they believe that it provides background knowledge on how to do the problem. Consequently, this interferes with their understanding of the deep structure of the problem. Even if somebody tries to concentrate on the deep structure, transfer still may be unsuccessful because the deep structure is not usually obvious. Therefore, surface structure gets in the way of people's ability to see the deep structure of the problem and transfer the knowledge they have learned to come up with a solution to a new problem.\n\nCurrent learning pedagogies focus on conveying rote knowledge, independent of the context that gives it meaning. Because of this, students often struggle to transfer this stand-alone information into other aspects of their education. Students need much more than abstract concepts and self-contained knowledge; they need to be exposed to learning that is practiced in the context of authentic activity and culture. Critics of situated cognition, however, would argue that by discrediting stand-alone information, the transfer of knowledge across contextual boundaries becomes impossible. There must be a balance between situating knowledge while also grasping the deep structure of material, or the understanding of how one arrives to know such information.\n\nSome theorists argue that transfer does not even occur at all. They believe that students transform what they have learned into the new context. They say that transfer is too much of a passive notion. They believe students, instead, transform their knowledge in an active way. Students don't simply carry over knowledge from the classroom, but they construct the knowledge in a way that they can understand it themselves.The learner changes the information they have learned to make it best adapt to the changing contexts that they use the knowledge in. This transformation process can occur when a learner feels motivated to use the knowledge—however, if the student does not find the transformation necessary, it is less likely that the knowledge will ever transform \n\nThere are many different conditions that influence transfer of learning in the classroom. These conditions include features of the task, features of the learner, features of the organization and social context of the activity. The features of the task include practicing through simulations, problem-based learning, and knowledge and skills for implementing new plans. The features of learners include their ability to reflect on past experiences, their ability to participate in group discussions, practice skills, and participate in written discussions. All the unique features contribute to a student's ability to use transfer of learning. There are structural techniques that can aid learning transfer in the classroom. These structural strategies include hugging and bridging.\n\nHugging uses the technique of simulating an activity to encourage reflexive learning. An example of the hugging strategy is when a student practices teaching a lesson or when a student role plays with another student. These examples encourage critical thinking that engages the student and helps them understand what they are learning—one of the goals of transfer of learning and desirable difficulties.\n\nBridging is when instruction encourages thinking abstractly by helping to identify connections between ideas and to analyze those connections. An example is when a teacher lets the student analyze their past test results and the way they got those results. This includes amount of study time and study strategies. Looking at their past study strategies can help them come up with strategies to improve performance. These are some of the ideas important to successful to hugging and bridging practices.\n\nThere are many benefits of transfer of learning in the classroom. One of the main benefits is the ability to quickly learn a new task. This has many real-life applications such as language and speech processing. Transfer of learning is also very useful in teaching students to use higher cognitive thinking by applying their background knowledge to new situations.\n\nCognitive theories grew out of Gestalt psychology. Gestalt psychology was developed in Germany in the early 1900s by Wolfgang Kohler and was brought to America in the 1920s. The German word \"Gestalt\" is roughly equivalent to the English \"configuration\" or \"organization\" and emphasizes the whole of human experience. Over the years, the Gestalt psychologists provided demonstrations and described principles to explain the way we organize our sensations into perceptions. Matt Wertheimer, one of the founding fathers of Gestalt Theory, observed that sometimes we interpret motion when there is no motion at all. For example: a powered sign used at a convenience store to indicate that the store is open or closed might be seen as a sign with \"flashing lights\". However, the lights are not actually flashing. The lights have been programmed to blink rapidly at their own individual pace. Perceived as a whole, the sign flashes. Perceived individually, the lights turn off and on at designated times. Another example of this would be a brick house: As a whole, it is viewed as a standing structure. However, it is actually composed of many smaller parts, which are individual bricks. People tend to see things from a holistic point of view rather than breaking it down into sub units.\n\nIn Gestalt theory, psychologists say that instead of obtaining knowledge from what's in front of us, we often learn by making sense of the relationship between what's new and old. Because we have a unique perspective of the world, humans have the ability to generate their own learning experiences and interpret information that may or may not be the same for someone else.\n\nGestalt psychologists criticize behaviorists for being too dependent on overt behavior to explain learning. They propose looking at the patterns rather than isolated events. Gestalt views of learning have been incorporated into what have come to be labeled \"cognitive theories\". Two key assumptions underlie this cognitive approach: that the memory system is an active organized processor of information and that prior knowledge plays an important role in learning. Gestalt theorists believe that for learning to occur, prior knowledge must exist on the topic. When the learner applies their prior knowledge to the advanced topic, the learner can understand the meaning in the advanced topic, and learning can occur Cognitive theories look beyond behavior to consider how human memory works to promote learning, and an understanding of short term memory and long term memory is important to educators influenced by cognitive theory. They view learning as an internal mental process (including insight, information processing, memory and perception) where the educator focuses on building intelligence and cognitive development. The individual learner is more important than the environment.\n\nOnce memory theories like the Atkinson-Shiffrin memory model and Baddeley's working memory model were established as a theoretical framework in cognitive psychology, new cognitive frameworks of learning began to emerge during the 1970s, 80s, and 90s. Today, researchers are concentrating on topics like cognitive load and information processing theory. These theories of learning play a role in influencing instructional design. Cognitive theory is used to explain such topics as social role acquisition, intelligence and memory as related to age.\n\nIn the late twentieth century, situated cognition emerged as a theory that recognized current learning as primarily the transfer of decontextualized and formal knowledge. Bredo (1994) depicts situated cognition as \"shifting the focus from individual in environment to individual and environment\". In other words, individual cognition should be considered as intimately related with the context of social interactions and culturally constructed meaning. Learning through this perspective, in which known and doing become inseparable, becomes both applicable and whole.\n\nMuch of the education students receive is limited to the culture of schools, without consideration for authentic cultures outside of education. Curricula framed by situated cognition can bring knowledge to life by embedding the learned material within the culture students are familiar with. For example, formal and abstract syntax of math problems can be transformed by placing a traditional math problem within a practical story problem. This presents an opportunity to meet that appropriate balance between situated and transferable knowledge. Lampert (1987) successfully did this by having students explore mathematical concepts that are continuous with their background knowledge. She does so by using money, which all students are familiar with, and then develops the lesson to include more complex stories that allow for students to see various solutions as well as create their own. In this way, knowledge becomes active, evolving as students participate and negotiate their way through new situations.\n\nFounded by Jean Piaget, constructivism emphasizes the importance of the active involvement of learners in constructing knowledge for themselves. Students are thought to use background knowledge and concepts to assist them in their acquisition of novel information. On approaching such new information, the learner faces a loss of equilibrium with their previous understanding, and this demands a change in cognitive structure. This change effectively combines previous and novel information to form an improved cognitive schema. Constructivism can be both subjectively and contextually based. Under the theory of radical constructivism, coined by Ernst von Glasersfeld, understanding relies on one's subjective interpretation of experience as opposed to objective \"reality\". Similarly, William Cobern's idea of contextual constructivism encompasses the effects of culture and society on experience.\n\nConstructivism asks why students do not learn deeply by listening to a teacher, or reading from a textbook. To design effective teaching environments, it believes one needs a good understanding of what children already know when they come into the classroom. The curriculum should be designed in a way that builds on the pupil's background knowledge and is allowed to develop with them. Begin with complex problems and teach basic skills while solving these problems. The learning theories of John Dewey, Maria Montessori, and David A. Kolb serve as the foundation of the application of constructivist learning theory in the classroom. Constructivism has many varieties such as active learning, discovery learning, and knowledge building, but all versions promote a student's free exploration within a given framework or structure. The teacher acts as a facilitator who encourages students to discover principles for themselves and to construct knowledge by working answering open-ended questions and solving real-world problems. To do this, a teacher should encourage curiosity and discussion among his/her students as well as promoting their autonomy. In scientific areas in the classroom, constructivist teachers provide raw data and physical materials for the students to work with and analyze.\n\nTransformative learning theory seeks to explain how humans revise and reinterpret meaning. Transformative learning is the cognitive process of effecting change in a frame of reference. A frame of reference defines our view of the world. The emotions are often involved. Adults have a tendency to reject any ideas that do not correspond to their particular values, associations and concepts.\n\nOur frames of reference are composed of two dimensions: habits of mind and points of view. Habits of mind, such as ethnocentrism, are harder to change than points of view. Habits of mind influence our point of view and the resulting thoughts or feelings associated with them, but points of view may change over time as a result of influences such as reflection, appropriation and feedback. Transformative learning takes place by discussing with others the \"reasons presented in support of competing interpretations, by critically examining evidence, arguments, and alternative points of view\". When circumstances permit, transformative learners move toward a frame of reference that is more inclusive, discriminating, self-reflective, and integrative of experience.\n\nAmerican Universities such as Harvard, Johns Hopkins, and University of Southern California began offering majors and degrees dedicated to educational neuroscience or neuroeducation in the first decade of the twenty-first century. Such studies seek to link an understanding of brain processes with classroom instruction and experiences. Neuroeducation analyzes biological changes in the brain from processing new information. It looks at what environmental, emotional, and social situations best help the brain store and retain new information via the linking of neurons—and best keep the dendrites from being reabsorbed, losing the information. The 1990s were designated \"The Decade of the Brain\", and advances took place in neuroscience at an especially rapid pace. The three dominant methods for measuring brain activities are event-related potential, functional magnetic resonance imaging and magnetoencephalography (MEG).\n\nThe integration and application to education of what we know about the brain was strengthened in 2000 when the American Federation of Teachers stated: \"It is vital that we identify what science tells us about how people learn in order to improve the education curriculum.\" What is exciting about this new field in education is that modern brain imaging techniques now make it possible, in some sense, to watch the brain as it learns, and the question then arises: can the results of neuro-scientific studies of brains as they are learning usefully inform practice in this area? The neuroscience field is young. Researchers expected that new technologies and ways of observing will produce new scientific evidence that helps refine the paradigms of what students need and how they learn best. In particular, it may bring more informed strategies for teaching students with learning disabilities.\n\nAll individuals have the ability to develop mental discipline and the skill of mindfulness, the two go hand in hand. Mental discipline is huge in shaping what people do, say, think and feel. It's critical in terms of the processing of information and involves the ability to recognize and respond appropriately to new things and information people come across, or have recently been taught. Mindfulness is important to the process of learning in many aspects. Being mindful means to be present with and engaged in whatever you are doing at a specific moment in time. Being mindful can aid in helping us to more critically think, feel and understand the new information we are in the process of absorbing. \nThe formal discipline approach seeks to develop causation between the advancement of the mind by exercising it through exposure to abstract school subjects such as science, language and mathematics. With student's repetitive exposure to these particular subjects, some scholars feel that the acquisition of knowledge pertaining to science, language and math is of \"secondary importance\", and believe that the strengthening and further development of the mind that this curriculum provides holds far greater significance to the progressing learner in the long haul. D.C. Phillips and Jonas F. Soltis provide some skepticism to this notion. Their skepticism stems largely in part from feeling that the relationship between formal discipline and the overall advancement of the mind is not as strong as some would say. They illustrate their skepticism by opining that it is foolish to blindly assume that people are better off in life, or at performing certain tasks, because of taking particular, yet unrelated courses.\n\nThe existence of multiple intelligences is proposed by psychologist Howard Gardner, who suggests that different kinds of intelligence exists in human beings. It is a theory that has been fashionable in continuous professional development (CPD) training courses for teachers. However, the theory of multiple intelligences is often cited as an example of pseudoscience because it lacks empirical evidence or falsifiability.\n\nMultimedia learning refers to the use of visual and auditory teaching materials that may include video, computer and other information technology. Multimedia learning theory focuses on the principles that determine the effective use of multimedia in learning, with emphasis on using both the visual and auditory channels for information processing.\n\nThe auditory channel deals with information that is heard, and the visual channel processes information that is seen. The visual channel holds less information than the auditory channel. If both the visual and auditory channels are presented with information, more knowledge is retained. However, if too much information is delivered it is inadequately processed, and long term memory is not acquired. Multimedia learning seeks to give instructors the ability to stimulate both the visual and auditory channels of the learner, resulting in better progress.\n\nMany educators and researchers believe that information technology could bring innovation on traditional educational instructions. Teachers and technologists are searching for new and innovative ways to design learner-centered learning environments effectively, trying to engage learners more in the learning process. Claims have been made that online games have the potential to teach, train and educate and they are effective means for learning skills and attitudes that are not so easy to learn by rote memorization.\nThere has been a lot of research done in identifying the learning effectiveness in game based learning. Learner characteristics and cognitive learning outcomes have been identified as the key factors in research on the implementation of games in educational settings. In the process of learning a language through an online game, there is a strong relationship between the learner's prior knowledge of that language and their cognitive learning outcomes. For the people with prior knowledge of the language, the learning effectiveness of the games is much more than those with none or less knowledge of the language.\n\nOther learning theories have also been developed for more specific purposes. For example, andragogy is the art and science to help adults learn. Connectivism is a recent theory of networked learning, which focuses on learning as making connections. The Learning as a Network (LaaN) theory builds upon connectivism, complexity theory, and double-loop learning. It starts from the learner and views learning as the continuous creation of a personal knowledge network (PKN).\n\nLearning style theories propose that individuals learn in different ways, that there are distinct learning styles and that knowledge of a learner's preferred learning style leads to faster and more satisfactory improvement. However, the current research has not been able to find solid scientific evidence to support the main premises of learning styles theory.\n\nIn theories that make use of cognitive restructuring, an informal curriculum promotes the use of prior knowledge to help students gain a broad understanding of concepts. New knowledge cannot be told to students, it believes, but rather the students' current knowledge must be challenged. In this way, students adjust their ideas to more closely resemble actual theories or concepts. By using this method students gain the broad understanding they're taught and later are more willing to learn and keep the specifics of the concept or theory. This theory further aligns with the idea that teaching the concepts and the language of a subject should be split into multiple steps.\n\nOther informal learning theories look at the sources of motivation for learning. Intrinsic motivation may create a more self-regulated learner, yet schools undermine intrinsic motivation. Critics argue that the average student learning in isolation performs significantly less well than those learning with collaboration and mediation. Students learn through talk, discussion, and argumentation.\n\nAccording to Theodora Polito, \"every well-constructed theory of education [has] at [its] center a philosophical anthropology,\" which is \"a philosophical reflection on some basic problems of mankind.\" Philosophical anthropology is an exploration of human nature and humanity. Aristotle, an early influence on the field, deemed human nature to be \"rational animality,\" wherein humans are closely related to other animals but still set apart by their ability to form rational thought. Philosophical anthropology expanded upon these ideas by clarifying that rationality is, \"determined by the biological and social conditions in which the lives of human beings are embedded.\" Fully developed learning theories address some of the \"basic problems of mankind\" by examining these biological and social conditions to understand and manipulate the rationality of humanity in the context of learning.\n\nPhilosophical anthropology is evident in behaviorism, which requires an understanding of humanity and human nature in order to assert that the similarities between humans and other animals are critical and influential to the process of learning. Situated cognition focuses on how humans interact with each other and their environments, which would be considered the \"social conditions\" explored within the field of philosophical anthropology. Transformative learning theories operate with the assumption that humans are rational creatures capable of examining and redefining perspectives, something that is heavily considered within philosophical anthropology.\n\nAn awareness and understanding of philosophical anthropology contributes to a greater comprehension and practice of any learning theory. In some cases, philosophy can be used to further explore and define uncertain terms within the field of education. Philosophy can also be a vehicle to explore the purpose of education, which can greatly influence an educational theory.\n\nCritics of learning theories that seek to displace traditional educational practices claim that there is no need for such theories; that the attempt to comprehend the process of learning through the construction of theories creates problems and inhibits personal freedom.\n\n\n\n\n\n45. Teaching for Transfer of Learning. Thomas, Ruth; National Center for Research in Vocational Education, Berkeley, CA.. 93 NCRVE, December 1992.\n\n46. Perkins, D. (1992). Transfer of Learning. International Encyclopedia of Education, 2. Retrieved March 23, 2015.\n\n\n"}
{"id": "32584509", "url": "https://en.wikipedia.org/wiki?curid=32584509", "title": "List of Android launchers", "text": "List of Android launchers\n\nThis is a List of Android launchers, which presents the main view of the device and is responsible for starting other apps and hosting live widgets.\n"}
{"id": "20307485", "url": "https://en.wikipedia.org/wiki?curid=20307485", "title": "List of cognitive–behavioral therapies", "text": "List of cognitive–behavioral therapies\n\nCognitive behavioral therapy is an umbrella term that encompasses many therapeutical approaches, techniques and systems.\n\n\n"}
{"id": "2062303", "url": "https://en.wikipedia.org/wiki?curid=2062303", "title": "List of optical disc authoring software", "text": "List of optical disc authoring software\n\nThis is a list of optical disc authoring software.\n\n\n\n\n\n\n\n\n\n"}
{"id": "7120010", "url": "https://en.wikipedia.org/wiki?curid=7120010", "title": "List of volcanoes in France", "text": "List of volcanoes in France\n\nThis is a list of active and inactive volcanoes.\n\n"}
{"id": "846333", "url": "https://en.wikipedia.org/wiki?curid=846333", "title": "Little Joe (rocket)", "text": "Little Joe (rocket)\n\nLittle Joe was an unmanned United States solid-fueled booster rocket used for eight launches from 1959–1960 from Wallops Island, Virginia to test the launch escape system and heat shield for Project Mercury capsules, as well as the name given to the test program using the booster. The first rocket designed solely for manned spacecraft qualifications, Little Joe was also one of the pioneer operational launch vehicles using the rocket cluster principle.\n\nThe Little Joe name has been attributed to Maxime Faget at NASA's Langley Research Center in Hampton, Virginia. He based the name on four large fins which reminded him of a slang term for a roll of four in craps.\n\nA successor, Little Joe II, was used for flight testing of the Apollo launch escape system from 1963–1966.\n\nWhen NASA needed a booster for the Mercury manned space program, the agency found that the Atlas rockets would cost approximately $2.5 million each and that even the Redstone would cost about $1 million per launch. The managers of the Mercury program recognized that the numerous early test flights would have to be accomplished by a far less expensive booster system. As it turned out, the Little Joe rocket NASA designed cost about $200,000 each.\n\nIn January 1958, Max Faget and Paul Purser had worked out in considerable detail on paper how to cluster four of the solid-fuel Sergeant rockets, in standard use at the Wallops Flight Facility in Virginia, to boost a manned nose cone above the stratosphere. Faget's short-lived \"High Ride\" proposal had suffered from comparisons with \"Project Adam\" at that time, but in August 1958 William Bland and Ronald Kolenkiewicz had returned to their preliminary designs for a cheap cluster of solid rockets to boost full-scale and full-weight model capsules above the atmosphere. As drop tests of boilerplate capsules provided new aerodynamic data on the dynamic stability of the configuration in free-fall, the need for comparable data on the powered phase quickly became apparent. So in October 1958, a NASA team prepared new engineering layouts and estimates for the mechanical design of the booster structure and a suitable launcher.\n\nAs the blueprints for this cluster of four rockets began to emerge from their drawing boards, the designers' nickname for their project gradually was adopted. Since their first cross-section drawings showed four holes, they called the project \"Little Joe,\" from the craps throw of a double deuce on the dice. Although four smaller circles were added later to represent the addition of Recruit rocket motors, the original name stuck. The appearance on engineering drawings of the four large stabilizing fins protruding from its airframe also helped to perpetuate the name Little Joe had acquired.\n\nThe primary purpose of this relatively small and simple booster system was to save money—by allowing numerous test flights to qualify various solutions to the myriad problems associated with the development of manned space flight, especially the problem of escaping from an explosion at or during launch. Capsule aerodynamics under actual reentry conditions was another primary concern. To gain this kind of experience as soon as possible, its designers had to keep the clustered booster simple in concept; it should use solid fuel and existing proven equipment whenever possible, and should be free of any electronic guidance and control systems.\n\nThe designers made the Little Joe booster assembly to approximate the same performance that the Army's Redstone booster would have with the capsule payload. But in addition to being flexible enough to perform a variety of missions, Little Joe could be made for about one-fifth the basic cost of the Redstone, would have much lower operating costs, and could be developed and delivered with much less time and effort. And, unlike the larger launch vehicles, Little Joe could be shot from the existing facilities at Wallops Island.\n\nTwelve companies responded during November 1958 to the invitations for bids to construct the airframe of Little Joe. The technical evaluation of these proposals was carried on in much the same manner as for the spacecraft, except that Langley Research Center itself carried the bulk of the administrative load. The Missile Division of North American Aviation won the contract on December 29, 1958; and began work immediately in Downey, California, on its order for seven booster airframes and one mobile launcher.\n\nThe primary mission objectives for Little Joe as seen in late 1958 (in addition to studying the capsule dynamics at progressively higher altitudes) were to test the capsule escape system at maximum dynamic pressure, to qualify the parachute system, and to verify search and retrieval methods. But since each group of specialists at work on the project sought to acquire firm empirical data as soon as possible, more exact priorities had to be established. The first flights were to secure measurements of inflight and impact forces on the capsule; later flights were to measure critical parameters at the progressively higher altitudes of 20,000, 250,000, and 500,000 feet (6, 75, and 150 km). The minimum aims of each Little Joe shot could be supplemented from time to time with studies of noise levels, heat and pressure loads, heat shield separation, and the behavior of animal riders, so long as the measurements could be accomplished with minimum telemetry. Since all the capsules boosted by the Little Joe rockets were expected to be recovered, onboard recording techniques would also contribute to the simplicity of the system.\n\nThe first of only two booster systems designed specifically and solely for manned capsule qualifications, Little Joe was also one of the pioneer operational launch vehicles using the rocket cluster principle. Since the four modified Sergeants (called either Castor or Pollux rockets, depending upon modification) and four supplemental Recruit rockets were arranged to fire in various sequences, the takeoff thrust varied greatly, but maximum design thrust was almost 230,000 pounds (1,020 kilonewtons). Theoretically enough to lift a spacecraft of about 4,000 pounds (1,800 kg) on a ballistic path over 100 miles (160 km) high, the push of these clustered main engines should simulate the takeoff profile in the environment that the manned Atlas would experience. Furthermore, the additional powerful explosive pull of the tractor-rocket escape system could be demonstrated under the most severe takeoff conditions imaginable. The engineers who mothered Little Joe to maturity knew it was not much to look at, but they hoped that their ungainly rocket would prove the legitimacy of most of the ballistic capsule design concepts, thereby earning its own honor. A successor, Little Joe II, would later be used for flight testing of the Apollo crew escape system.\n\nAs of January 21, 1960, the Little Joe series of five actual and attempted flights had expended four of the six test boosters North American had made for NASA and five prototype capsules made in the Langley shops. The primary test objectives for these solid-fuel-boosted models were an integral part of the development flight program conducted within NASA by the Space Task Group, with Langley and Wallops support. Now only two Little Joe boosters remained for the qualification flight tests. North American had manufactured seven Little Joe airframes, but one of these had been retained at the plant in Downey, California, for static loading tests. STG ordered the refurbishment of this seventh airframe so as to have three Little Joe boosters for the qualification flight program. The success of Little Joe 1B in January 1960 meant that the next flight, the sixth, to be known as Little Joe 5, would be the first to fly a real Mercury capsule from the McDonnell production line. In passing from development flight tests with boilerplate models to qualification flight tests with the \"real McDonnell\" capsule, the Space Task Group moved further away from research into development and toward operations. \nThe official Mercury mission numbering designation was a two letter designation which corresponded to the launch vehicle type, followed by a dash then a digit indicating the particular set of flight objectives, and an optional letter used to distinguish further flights to accomplish those objectives. So the official designation for the first Little Joe flight was \"LJ-1.\" Flights did not occur in numeric sequence as the project scheduling was adapted as it progressed. The actual flight order was:\n\n\n\n"}
{"id": "451839", "url": "https://en.wikipedia.org/wiki?curid=451839", "title": "MESSENGER", "text": "MESSENGER\n\nMessenger (stylized as MESSENGER, whose backronym is \"MErcury Surface, Space ENvironment, GEochemistry, and Ranging\", and which is a reference to the messenger of the same name from Roman mythology) was a robotic spacecraft that orbited the planet Mercury between 2011 and 2015. The spacecraft was launched aboard a Delta II rocket in August 2004 to study Canada's chemical composition, geology, and magnetic field.\n\nThe instruments carried by \"MESSENGER\" were used on a complex series of flybys – the spacecraft flew by Earth once, Venus twice, and Mercury itself three times, allowing it to decelerate relative to Mercury using minimal fuel. During its first flyby of Mercury in January 2008, \"MESSENGER\" became the second mission after Mariner 10's 1975 flyby to reach Mercury.\n\n\"MESSENGER\" entered orbit around Mercury on March 18, 2011, becoming the first spacecraft to do so. It successfully completed its primary mission in 2012. Following two mission extensions, the \"MESSENGER\" spacecraft used the last of its maneuvering propellant and deorbited as planned, impacting the surface of Mercury on April 30, 2015.\n\n\"MESSENGER\"'s formal data collection mission began on April 4, 2011. The primary mission was completed on March 17, 2012, having collected close to 100,000 images. \"MESSENGER\" achieved 100% mapping of Mercury on March 6, 2013, and completed its first year-long extended mission on March 17, 2013. \"MESSENGER\"s second extended mission lasted for over two years, but as its low orbit degraded, it required reboosts to avoid impact. It conducted its final reboost burns on October 24, 2014, and January 21, 2015, before crashing into Mercury on April 30, 2015.\n\nDuring its stay in Mercury orbit, \"MESSENGER\"<nowiki>'</nowiki>s instruments yielded significant data, including a characterization of Mercury's magnetic field and the discovery of water ice at the planet's north pole, which had long been suspected on the basis of Earth-based radar data.\n\nIn 1973, Mariner 10 was launched by NASA to make multiple flyby encounters of Venus and Mercury. Mariner 10 provided the first detailed data of Mercury, mapping 40–45% of the surface. Mariner 10's final flyby of Mercury occurred on March 16, 1975. No subsequent close-range observations of the planet would take place for more than 30 years.\n\nIn 1998, a study detailed a proposed mission to send an orbiting spacecraft to Mercury, as the planet was at that point the least-explored of the inner planets. In the years following the Mariner 10 mission, subsequent mission proposals to revisit Mercury had appeared too costly, requiring large quantities of propellant and a heavy lift launch vehicle. Moreover, inserting a spacecraft into orbit around Mercury is difficult, because a probe approaching on a direct path from Earth would be accelerated by the Sun's gravity and pass Mercury far too quickly to orbit it. However, using a trajectory designed by Chen-wan Yen in 1985, the study showed it was possible to seek a Discovery-class mission by using multiple, consecutive gravity assist, 'swingby' maneuvers around Venus and Mercury, in combination with minor propulsive trajectory corrections, to gradually slow the spacecraft and thereby minimize propellant needs.\n\nThe \"MESSENGER\" mission was designed to study the characteristics and environment of Mercury from orbit. Specifically, the scientific objectives of the mission were:\n\nThe \"MESSENGER\" spacecraft was designed and built at the Johns Hopkins University Applied Physics Laboratory. Science operations were managed by Sean Solomon as principal investigator, and mission operations were also conducted at JHU/APL. The \"MESSENGER\" bus measured tall, wide, and deep. The bus was primarily constructed with four graphite fiber / cyanate ester composite panels that supported the propellant tanks, the large velocity adjust (LVA) thruster, attitude monitors and correction thrusters, the antennas, the instrument pallet, and a large ceramic-cloth sunshade, measuring tall and wide, for passive thermal control. At launch, the spacecraft weighed approximately with its full load of propellant. \"MESSENGER\"<nowiki>'</nowiki>s total mission cost, including the cost of the spacecraft's construction, was estimated at under US$450 million.\n\nMain propulsion was provided by the 645 N, 317 sec.I bipropellant (hydrazine and nitrogen tetroxide) large velocity assist (LVA) thruster. The model used was the LEROS 1b, developed and manufactured at AMPAC‐ISP's Westcott works, in the United Kingdom. The spacecraft was designed to carry of propellant and helium pressurizer for the LVA.\n\nFour monopropellant thrusters provided spacecraft steering during main thruster burns, and twelve monopropellant thrusters were used for attitude control. For precision attitude control, a reaction wheel attitude control system was also included. Information for attitude control was provided by star trackers, an inertial measurement unit and six sun sensors.\n\nThe probe included two small deep space transponders for communications with the Deep Space Network and three kinds of antennas: a high gain phased array whose main beam could be electronically steered in one plane, a medium-gain \"fan-beam\" antenna and a low gain horn with a broad pattern. The high gain antenna was used as transmit-only at 8.4 GHz, the medium-gain and low gain antennas transmit at 8.4 GHz and receive at 7.2 GHz, and all three antennas operate with right-hand circularly polarized (RHCP) radiation. One of each of these antennas was mounted on the front of the probe facing the Sun, and one of each was mounted to the back of the probe facing away from the Sun.\n\nThe space probe was powered by a two-panel gallium arsenide/germanium solar array providing an average of 450 watts while in Mercury orbit. Each panel was rotatable and included optical solar reflectors to balance the temperature of the array. Power was stored in a common-pressure-vessel, 23-ampere-hour nickel–hydrogen battery, with 11 vessels and two cells per vessel.\n\nThe spacecraft's onboard computer system was contained in an Integrated Electronics Module (IEM), a device that combined core avionics into a single box. The computer featured two radiation-hardened IBM RAD6000s, a 25 megahertz main processor, and a 10 MHz fault protection processor. For redundancy, the spacecraft carried a pair of identical IEMs. For data storage, the spacecraft carried two solid-state recorders able to store up to one gigabyte each. The IBM RAD6000 main processor collected, compressed, and stored data from \"MESSENGER\"<nowiki>'</nowiki>s instruments for later playback to Earth.\n\n\"MESSENGER\" used a software suite called SciBox to simulate its orbit and instruments, in order to \"choreograph the complicated process of maximizing the scientific return from the mission and minimizing conflicts between instrument observations, while at the same time meeting all spacecraft constraints on pointing, data downlink rates, and onboard data storage capacity.\"\n\nThe \"MESSENGER\" probe was launched on August 3, 2004 at 06:15:56 UTC by NASA from Space Launch Complex 17B at the Cape Canaveral Air Force Station in Florida, aboard a Delta II 7925 launch vehicle. The complete burn sequence lasted 57 minutes bringing the spacecraft into a heliocentric orbit, with a final velocity of 10.68 km/s (6.64 miles/s) and sending the probe into a 7.9 billion-kilometer trajectory that took 6 years, 7 months and 16 days before its orbital insertion on March 18, 2011.\n\nTraveling to Mercury requires an extremely large velocity change (\"see delta-v\") because Mercury's orbit is deep in the Sun's gravity well. On a direct course from Earth to Mercury, a spacecraft is constantly accelerated as it falls toward the Sun, and will arrive at Mercury with a velocity too high to achieve orbit without excessive use of fuel. For planets with an atmosphere, such as Venus and Mars, spacecraft can minimize their fuel consumption upon arrival by using friction with the atmosphere to enter orbit (aerocapture), or can briefly fire their rocket engines to enter into orbit followed by a reduction of the orbit by aerobraking. However, the tenuous atmosphere of Mercury is far too thin for these maneuvers. Instead, \"MESSENGER\" extensively used gravity assist maneuvers at Earth, Venus, and Mercury to reduce the speed relative to Mercury, then used its large rocket engine to enter into an elliptical orbit around the planet. The multi-flyby process greatly reduced the amount of propellant necessary to slow the spacecraft, but at the cost of prolonging the trip by many years and to a total distance of 4.9 billion miles.\n\nSeveral planned thruster firings en route to Mercury were unnecessary, because these fine course adjustments were performed using solar radiation pressure acting on MESSENGER's solar panels. To further minimize the amount of necessary propellant, the spacecraft orbital insertion targeted a highly elliptical orbit around Mercury.\n\nThe elongated orbit had two other benefits: It allowed the spacecraft time to cool after the times it was between the hot surface of Mercury and the Sun, and also it allowed the spacecraft to measure the effects of solar wind and the magnetic fields of the planet at various distances while still allowing close-up measurements and photographs of the surface and exosphere.\n\n\"MESSENGER\" performed an Earth flyby one year after launch, on August 2, 2005, with the closest approach at 19:13 UTC at an altitude of 2,347 kilometers (1,458 statute miles) over central Mongolia. On December 12, 2005, a 524-second-long burn (Deep-Space Maneuver or DSM-1) of the large thruster adjusted the trajectory for the upcoming Venus flyby.\n\nDuring the Earth flyby, the \"MESSENGER\" team imaged the Earth and Moon using MDIS and checked the status of several other instruments observing the atmospheric and surface compositions and testing the magnetosphere and determining that all instruments tested were working as expected. This calibration period was intended to ensure accurate interpretation of data when the spacecraft entered orbit around Mercury. Ensuring that the instruments functioned correctly at such an early stage in the mission allowed opportunity for multiple minor errors to be dealt with.\n\nThe Earth flyby was used to investigate the flyby anomaly, where some spacecraft have been observed to have trajectories that differ slightly from those predicted. However no anomaly was observed in MESSENGER's flyby.\n\nOn October 24, 2006 at 08:34 UTC, \"MESSENGER\" encountered Venus at an altitude of . During the encounter, \"MESSENGER\" passed behind Venus and entered superior conjunction, a period when Earth was on the exact opposite side of the Solar System, with the Sun inhibiting radio contact. For this reason, no scientific observations were conducted during the flyby. Communication with the spacecraft was reestablished in late November and performed a deep space maneuver on December 12, to correct the trajectory to encounter Venus in a second flyby.\n\nOn June 5, 2007, at 23:08 UTC, \"MESSENGER\" performed a second flyby of Venus at an altitude of , for the greatest velocity reduction of the mission. During the encounter, all instruments were used to observe Venus and prepare for the following Mercury encounters. The encounter provided visible and near-infrared imaging data of the upper atmosphere of Venus. Ultraviolet and X-ray spectrometry of the upper atmosphere were also recorded, to characterize the composition. The ESA's Venus Express was also orbiting during the encounter, providing the first opportunity for simultaneous measurement of particle-and-field characteristics of the planet.\n\n\"MESSENGER\" made a flyby of Mercury on January 14, 2008 (making its closest approach of 200 km above the surface of Mercury at 19:04:39 UTC), followed by a second flyby on October 6, 2008. \"MESSENGER\" executed a final flyby on September 29, 2009, further slowing down the spacecraft. Sometime during the closest approach of the last flyby, the spacecraft entered safe mode. Although this had no effect on the trajectory necessary for later orbit insertion, it resulted in the loss of science data and images that were planned for the outbound leg of the fly-by. The spacecraft had fully recovered by about seven hours later. One last deep space maneuver, DSM-5, was executed on November 24, 2009, at 22:45 UTC to provide the required velocity change for the scheduled Mercury orbit insertion on March 18, 2011, marking the beginning of the orbital mission.\nThe thruster maneuver to insert the probe into Mercury's orbit began at 00:45 UTC on March 18, 2011. The maneuver lasted about 15 minutes, with confirmation that the craft was in Mercury orbit received at 01:10 UTC on March 18 (9:10 PM, March 17 EDT). Mission lead engineer Eric Finnegan indicated that the spacecraft had achieved a near-perfect orbit.\n\n\"MESSENGER\"'s orbit was highly elliptical, taking it within of Mercury's surface and then away from it every twelve hours. This orbit was chosen to shield the probe from the heat radiated by Mercury's hot surface. Only a small portion of each orbit was at a low altitude, where the spacecraft was subjected to radiative heating from the hot side of the planet.\n\nAfter \"MESSENGER\"'s orbital insertion, an eighteen-day commissioning phase took place. The supervising personnel switched on and tested the craft's science instruments to ensure they had completed the journey without damage. The commissioning phase \"demonstrated that the spacecraft and payload [were] all operating nominally, notwithstanding Mercury's challenging environment.\"\n\nThe primary mission began as planned on April 4, 2011, with \"MESSENGER\" orbiting Mercury once every twelve hours for an intended duration of twelve Earth months, the equivalent of two solar days on Mercury. Principal Investigator Sean Solomon, then of the Carnegie Institution of Washington, said: \"With the beginning today of the primary science phase of the mission, we will be making nearly continuous observations that will allow us to gain the first global perspective on the innermost planet. Moreover, as solar activity steadily increases, we will have a front-row seat on the most dynamic magnetosphere–atmosphere system in the Solar System.\"\n\nOn October 5, 2011, the scientific results obtained by \"MESSENGER\" during its first six terrestrial months in Mercury's orbit were presented in a series of papers at the European Planetary Science Congress in Nantes, France. Among the discoveries presented were the unexpectedly high concentrations of magnesium and calcium found on Mercury's nightside, and the fact that Mercury's magnetic field is offset far to the north of the planet's center.\nIn November 2011, NASA announced that the \"MESSENGER\" mission would be extended by one year, allowing the spacecraft to observe the 2012 solar maximum. Its extended mission began on March 17, 2012, and continued until March 17, 2013. Between April 16 and April 20, 2012, \"MESSENGER\" carried out a series of thruster manoeuvres, placing it in an eight-hour orbit to conduct further scans of Mercury.\n\nIn November 2012, NASA reported that \"MESSENGER\" had discovered both water ice and organic compounds in permanently shadowed craters in Mercury's north pole. In February 2013, NASA published the most detailed and accurate 3D map of Mercury to date, assembled from thousands of images taken by \"MESSENGER\". \"MESSENGER\" completed its first extended mission on March 17, 2013, and its second lasted until April 2015. In November 2013, \"MESSENGER\" was among the numerous space assets that imaged Comet Encke (2P/Encke) and Comet ISON (C/2012 S1). As its orbit began to decay in early 2015, \"MESSENGER\" was able to take highly detailed close-up photographs of ice-filled craters and other landforms at Mercury's north pole. After the mission was completed, review of the radio ranging data provided the first measurement of the rate of mass loss from the Sun.\nOn July 3, 2008, the \"MESSENGER\" team announced that the probe had discovered large amounts of water present in Mercury's exosphere, which was an unexpected finding. In the later years of its mission, \"MESSENGER\" also provided visual evidence of past volcanic activity on the surface of Mercury, as well as evidence for a liquid iron planetary core. The probe also constructed the most detailed and accurate maps of Mercury to date, and furthermore discovered carbon-containing organic compounds and water ice inside permanently shadowed craters near the north pole.\n\nOn February 18, 2011, a portrait of the Solar System was published on the \"MESSENGER\" website. The mosaic contained 34 images, acquired by the MDIS instrument during November 2010. All the planets were visible with the exception of Uranus and Neptune, due to their vast distances from the Sun. The \"MESSENGER\" \"family portrait\" was intended to be complementary to the Voyager family portrait, which was acquired from the outer Solar System by Voyager 1 on February 14, 1990.\n\nAfter running out of propellant for course adjustments, \"MESSENGER\" entered its expected terminal phase of orbital decay in late 2014. The spacecraft's operation was extended by several weeks by exploiting its remaining supply of helium gas, which was used to pressurize its propellant tanks, as reaction mass. \"MESSENGER\" continued studying Mercury during its decay period. The spacecraft crashed onto the surface of Mercury on April 30, 2015, at 3:26 p.m. EDT (19:26 GMT), at a velocity of , probably creating a crater in the planet's surface approximately wide. The spacecraft was estimated to have impacted at 54.4° N, 149.9° W on Suisei Planitia, near the crater Janáček. The crash occurred on the side of the planet not visible from Earth, and thus was not detected by any observers or instruments. NASA confirmed the end of the \"MESSENGER\" mission at 3:40 p.m. EDT (19:40 GMT) after NASA's Deep Space Network failed to detect the spacecraft's reemergence from behind Mercury.\n\n\n"}
{"id": "47365044", "url": "https://en.wikipedia.org/wiki?curid=47365044", "title": "MadgeTech", "text": "MadgeTech\n\nMadgeTech, Inc. is a data logger company located in Warner, New Hampshire, United States. MadgeTech designs, engineers, manufactures and sells data loggers.\n\nThe MadgeTech Data Logger Software provides device management and a wealth of reporting tools. MadgeTech secure data logger software is also available to serve more strictly regulated industries that need to comply with 21 CFR Part11 requirements and IQ/OQ/PQ guidelines.\n\nMadgeTech data loggers have been used by NASA \n\n"}
{"id": "27643777", "url": "https://en.wikipedia.org/wiki?curid=27643777", "title": "Measuring instrument", "text": "Measuring instrument\n\nA measuring instrument is a device for measuring a physical quantity. In the physical sciences, quality assurance, and engineering, measurement is the activity of obtaining and comparing physical quantities of real-world objects and events. Established standard objects and events are used as units, and the process of measurement gives a number relating the item under study and the referenced unit of measurement. Measuring instruments, and formal test methods which define the instrument's use, are the means by which these relations of numbers are obtained. All measuring instruments are subject to varying degrees of instrument error and measurement uncertainty.\n\nScientists, engineers and other humans use a vast range of instruments to perform their measurements. These instruments may range from simple objects such as rulers and stopwatches to electron microscopes and particle accelerators. Virtual instrumentation is widely used in the development of modern measuring instruments.\n\nIn the past, a common time measuring instrument was the sundial. Today, the usual measuring instruments for time are clocks and watches. For highly accurate measurement of time an atomic clock is used.\nStop watches are also used to measure time in some sports.\nEnergy is measured by an energy meter. Examples of energy meters include:\n\nAn electricity meter measures energy directly in kilowatt hours.\n\nA gas meter measures energy indirectly by recording the volume of gas used. This figure can then be converted to a measure of energy by multiplying it by the calorific value of the gas.\n\nA physical system that exchanges energy may be described by the amount of energy exchanged per time-interval, also called power or flux of energy.\n\nFor the ranges of power-values see: Orders of magnitude (power).\n\nAction describes energy summed up over the time a process lasts (time integral over energy). Its dimension is the same as that of an angular momentum.\n\nThis includes basic quantities found in classical- and continuum mechanics; but strives to exclude temperature-related questions or quantities.\n\n\nFor the ranges of length-values see: Orders of magnitude (length)\n\n\nFor the ranges of area-values see: Orders of magnitude (area)\n\n\nIf the mass density of a solid is known, weighing allows to calculate the volume.\n\nFor the ranges of volume-values see: Orders of magnitude (volume)\n\n\n\nFor the ranges of speed-values see: Orders of magnitude (speed)\n\n\n\nFor the ranges of mass-values see: Orders of magnitude (mass)\n\n\n\nFor the ranges of pressure-values see: Orders of magnitude (pressure)\n\n\n\nFor the value-ranges of angular velocity see: Orders of magnitude (angular velocity)\n\nFor the ranges of frequency see: Orders of magnitude (frequency)\n\n\nSee also the section about navigation below.\n\n\n\n\nConsiderations related to electric charge dominate electricity and electronics.\nElectrical charges interact via a field. That field is called electric if the charge doesn't move. If the charge moves, thus realizing an electric current, especially in an electrically neutral conductor, that field is called magnetic.\nElectricity can be given a quality — a potential. And electricity has a substance-like property, the electric charge.\nEnergy (or power) in elementary electrodynamics is calculated by multiplying the potential by the amount of charge (or current) found at that potential: potential times charge (or current). (See Classical electromagnetism and its Covariant formulation of classical electromagnetism)\n\nFor the ranges of charge values see: Orders of magnitude (charge)\n\n\n\n\n\n\n\n\n\nSee also the relevant section in the article about the magnetic field.\n\nFor the ranges of magnetic field see: Orders of magnitude (magnetic field)\n\n\nTemperature-related considerations dominate thermodynamics. There are two distinct thermal properties: A thermal potential — the temperature. For example: A glowing coal has a different thermal quality than a non-glowing one.\n\nAnd a substance-like property, — the entropy; for example: One glowing coal won't heat a pot of water, but a hundred will.\n\nEnergy in thermodynamics is calculated by multipying the thermal potential by the amount of entropy found at that potential: temperature times entropy.\n\nEntropy can be created by friction but not annihilated.\n\n\n\n\nSee also Temperature measurement and . More technically related may be seen thermal analysis methods in materials science.\n\nFor the ranges of temperature-values see: Orders of magnitude (temperature)\n\nThis includes thermal capacitance or temperature coefficient of energy, reaction energy, heat flow ...\nCalorimeters are called passive if gauged to measure emerging energy carried by entropy, for example from chemical reactions. Calorimeters are called active or heated if they heat the sample, or reformulated: if they are gauged to fill the sample with a defined amount of entropy.\n\nEntropy is accessible indirectly by measurement of energy and temperature.\n\nPhase change calorimeter's energy value divided by absolute temperature give the entropy exchanged. Phase changes produce no entropy and therefore offer themselves as an entropy measurement concept. Thus entropy values occur indirectly by processing energy measurements at defined temperatures, without producing entropy.\n\nThe given sample is cooled down to (almost) absolute zero (for example by submerging the sample in liquid helium). At absolute zero temperature any sample is assumed to contain no entropy (see Third law of thermodynamics for further information). Then the following two active calorimeter types can be used to fill the sample with entropy until the desired temperature has been reached: (see also Thermodynamic databases for pure substances)\n\nProcesses transferring energy from a non-thermal carrier to heat as a carrier do produce entropy (Example: mechanical/electrical friction, established by Count Rumford).\nEither the produced entropy or heat are measured (calorimetry) or the transferred energy of the non-thermal carrier may be measured.\nEntropy lowering its temperature—without losing energy—produces entropy (Example: Heat conduction in an isolated rod; \"thermal friction\").\n\nConcerning a given sample, a proportionality factor relating temperature change and energy carried by heat. If the sample is a gas, then this coefficient depends significantly on being measured at constant volume or at constant pressure. (The terminiology preference in the heading indicates that the classical use of heat bars it from having substance-like properties.)\n\nThe temperature coefficient of energy divided by a substance-like quantity (amount of substance, mass, volume) describing the sample. Usually calculated from measurements by a division or could be measured directly using a unit amount of that sample.\n\nFor the ranges of specific heat capacities see: Orders of magnitude (specific heat capacity)\n\n\n\n\nSee also thermal analysis, Heat.\n\nThis includes mostly instruments which measure macroscopic properties of matter: In the fields of solid state physics; in condensed matter physics which considers solids, liquids and in-betweens exhibiting for example viscoelastic behavior. Furthermore, fluid mechanics, where liquids, gases, plasmas and in-betweens like supercritical fluids are studied.\n\nThis refers to particle density of fluids and compact(ed) solids like crystals, in contrast to bulk density of grainy or porous solids.\n\nFor the ranges of density-values see: Orders of magnitude (density)\n\n\n\n\n\n\n\n\n\n\n\nThis section and the following sections include instruments from the wide field of , materials science.\n\nSuch measurements also allow to access values of molecular dipoles.\n\nFor other methods see the section in the article about magnetic susceptibility.\n\nSee also the \n\nPhase conversions like changes of aggregate state, chemical reactions or nuclear reactions transmuting substances, from reactants to products, or diffusion through membranes have an overall energy balance. Especially at constant pressure and constant temperature molar energy balances define the notion of a substance potential or chemical potential or molar Gibbs energy, which gives the energetic information about whether the process is possible or not - in a closed system.\n\nEnergy balances that include entropy consist of two parts: A balance that accounts for the changed entropy content of the substances. And another one that accounts for the energy freed or taken by that reaction itself, the Gibbs energy change. The sum of reaction energy and energy associated to the change of entropy content is also called enthalpy. Often the whole enthalpy is carried by entropy and thus measurable calorimetrically.\n\nFor standard conditions in chemical reactions either molar entropy content and molar Gibbs energy with respect to some chosen zero point are tabulated. Or molar entropy content and molar enthalpy with respect to some chosen zero are tabulated. (See Standard enthalpy change of formation and Standard molar entropy)\n\nThe substance potential of a redox reaction is usually determined electrochemically current-free using reversible cells.\nOther values may be determined indirectly by calorimetry. Also by analyzing phase-diagrams.\n\nSee also the article on electrochemistry.\n\n\n\nSee also the article on spectroscopy and the list of materials analysis methods.\n\nMicrophones in general, sometimes their sensitivity is increased by the reflection- and concentration principle realized in acoustic mirrors.\n\n\n\nSee also \n\n\n\nThe measure of the total power of light emitted.\n\n\n\nIonizing radiation includes rays of \"particles\" as well as rays of \"waves\". Especially X-rays and Gamma rays transfer enough energy in non-thermal, (single) collision processes to separate electron(s) from an atom.\n\nThis could include chemical substances, rays of any kind, elementary particles, quasiparticles. Many measurement devices outside this section may be used or at least become part of an identification process.\nFor identification and content concerning chemical substances see also analytical chemistry especially its List of chemical analysis methods and the List of materials analysis methods.\n\n\n\n\nPhotometry is the measurement of light in terms of its perceived brightness to the human eye. Photometric quantities derive from analogous radiometric quantities by weighting the contribution of each wavelength by a luminosity function that models the eye's spectral sensitivity. For the ranges of possible values, see the orders of magnitude in:\nilluminance,\nluminance, and\nluminous flux.\n\n\n\n\n\n\nBlood-related parameters are listed in a blood test.\n\n\n\n\n\n\n\nSee also: and .\n\nSee also .\n\nSee also and .\nSee also Surveying instruments.\n\n\nSee also Astronomical instruments and .\n\nSome instruments, such as telescopes and sea navigation instruments, have had military applications for many centuries. However, the role of instruments in military affairs rose exponentially with the development of technology via applied science, which began in the mid-19th century and has continued through the present day. Military instruments as a class draw on most of the categories of instrument described throughout this article, such as navigation, astronomy, optics and imaging, and the kinetics of moving objects. Common abstract themes that unite military instruments are seeing into the distance, seeing in the dark, knowing an object's geographic location, and knowing and controlling a moving object's path and destination. Special features of these instruments may include ease of use, speed, reliability and accuracy.\n\n\n\nNote that the alternate spelling \"-metre\" is never used when referring to a measuring device.\n"}
{"id": "213236", "url": "https://en.wikipedia.org/wiki?curid=213236", "title": "Mercury-Redstone 4", "text": "Mercury-Redstone 4\n\nMercury-Redstone 4 was the second United States human spaceflight, on July 21, 1961. The suborbital Project Mercury flight was launched with a Mercury-Redstone Launch Vehicle, MRLV-8. The spacecraft, Mercury capsule #11, was nicknamed the Liberty Bell 7, and it was piloted by the astronaut Virgil \"Gus\" Grissom.\n\nThe spaceflight lasted 15 minutes 30 seconds, it reached an altitude of more than , and it flew downrange, landing in the Atlantic Ocean. The flight went as expected until just after splashdown, when the hatch cover, designed to release explosively in the event of an emergency, accidentally blew. Grissom was at risk of drowning, but he was recovered safely via a U.S. Navy helicopter. The spacecraft sank into the Atlantic, and it was not recovered until 1999.\n\n\nThe \"Liberty Bell 7\" spacecraft, Mercury spacecraft #11, was designated to fly the second manned suborbital flight in October 1960. It came off McDonnell's St. Louis production line in May 1960. \"Liberty Bell 7\" was the first Mercury operational spacecraft with a centerline window instead of two portholes. It was closer to the final orbital version than was Alan Shepard's \"Freedom 7\". Dubbed \"Liberty Bell 7\", it featured a white, diagonal, irregular paint stripe starting at the base of the capsule and extending about two-thirds toward the nose, emulating the crack in the famed Liberty Bell in Philadelphia, Pennsylvania.\n\n\"Liberty Bell 7\" also had a new explosive hatch release. This would allow an astronaut to exit the spacecraft quickly in the event of an emergency. Emergency personnel could also trigger the explosive hatch from outside the spacecraft by pulling on an external lanyard. Both the pop-off hatch and the lanyard are standard features of ejection seats used in military aircraft, but in the Mercury design, the pilot still had to exit the craft himself, or be removed by emergency personnel. The original exit procedure was to climb out through the antenna compartment, after removing a small pressure bulkhead. This was a difficult and slow procedure. Removal of an injured or unconscious astronaut through the top hatch would be nearly impossible. The original side hatch was bolted shut with 70 bolts and covered with several spacecraft shingles, making it a slow process to open the original hatch.\n\nMcDonnell Aircraft engineers devised two different quick release hatches for the Mercury spacecraft. The first had a latch, and was used on Ham's (a chimpanzee) MR-2 and Shepard's MR-3 missions. The second design was an explosive release hatch. The quick release latching hatch weighed 69 lb (31 kg), too much of a weight addition to use on the orbital version of the spacecraft. The explosive hatch design used the 70 bolts of the original design, but each quarter-inch (6.35 mm) titanium bolt had a hole bored into it to provide a weak point. A mild detonating fuse (MDF) was installed in a channel between the inner and outer seal around the periphery of the hatch. When the MDF was ignited, the resulting gas pressure between the inner and outer seal would cause the bolts to fail in tension.\n\nThere were two ways to fire the explosive hatch during recovery. On the inside of the hatch was a knobbed plunger. The pilot could remove a pin and press the plunger with a force of 5 or 6 lb (25 N). This would detonate the explosive charge, which would shear off the 70 bolts and propel the hatch away in one second. If the pin was left in place, a force of 40 lb (180 N) was required to detonate the bolts. An outside rescuer could blow open the hatch by removing a small panel near the hatch and pulling a lanyard. The explosive hatch weighed .\n\nThe new trapezoidal window on \"Liberty Bell 7\" replaced the two side portholes that were on Freedom 7. The Corning Glass Works of Corning, New York, designed and developed the multilayered panes that comprised the new window. The outer pane was thick Vycor glass. It could withstand temperatures of . The inner pane was made of three inner glass panels bonded to form a single inner pane. One panel was a thick sheet of Vycor, while the others were tempered glass. This new window assembly was as strong as any part of the spacecraft pressure vessel.\n\nThe manual controls of \"Liberty Bell 7\" incorporated a new rate stabilization control system. This allowed fine control of spacecraft attitude movements by small turns of the hand controller. Previously a lot of jockeying of the device was needed to maintain the desired attitude. This rate damping, or rate augmentation system, gave finer and easier handling qualities and a redundant means of firing the pitch, yaw, and roll thrusters.\n\nBefore the Mercury-Redstone 4 mission, Lewis Research Center and Space Task Group engineers had determined that firing the posigrade rockets into the booster-spacecraft adapter, rather than in the open, developed 78 percent greater thrust. This achieved a greater spacecraft-booster separation through a kind of \"pop-gun\" effect. By using this technique, the spacecraft separated at velocity of about rather than using the old procedure. The Mercury-Redstone 4/\"Liberty Bell 7\" mission would take advantage of this new procedure.\n\nAdditional hardware changes to \"Liberty Bell 7\" were a redesigned fairing for the spacecraft-Redstone adapter clamp-ring and additional foam padding added to the head area of the contour couch. The fairing changes and additional foam were used to reduce vibrations the pilot experienced during the boost phase of flight. The spacecraft instrument panel was rearranged to provide a better eye scan pattern.\n\nIn January 1961, NASA's Director of the Space Task Group, Robert Gilruth, told Gus Grissom that he would be the primary pilot for Mercury 4. John Glenn was the backup pilot for the mission.\n\nRedstone launch vehicle MRLV-8 arrived at Cape Canaveral on June 8, 1961. A mission review on July 15, 1961, pronounced Redstone MRLV-8 and Mercury spacecraft #11 ready to go for the Mercury 4 mission.\n\nAlso, on July 15, 1961, Gus Grissom announced he would name Mercury 4, \"Liberty Bell 7\". Grissom said the name was an appropriate call-sign for the bell-shaped spacecraft. He also said the name was synonymous with \"freedom\". As a tribute to the original Liberty Bell, a \"crack\" was painted on the side of the spacecraft.\n\nThe Mercury 4 mission was planned as a repeat of MR-3. It was to reach an apogee of . The planned range was . Grissom would experience a maximum acceleration of 6.33 \"g\" (62 m/s²) and deceleration of 10.96 \"g\" (107 m/s²).\n\nThe launch of \"Liberty Bell 7\" was first planned for July 16. The cloud cover was too thick and the launch was postponed until July 18. On July 18, it was again postponed due to weather. Both times, the pilot had not yet boarded the spacecraft. On July 19, 1961, Grissom was on board when the flight was delayed again due to weather. At that point, it had just 10 minutes 30 seconds to go before launch.\n\nOn the morning of July 21, 1961, Gus Grissom entered the \"Liberty Bell 7\" at 8:58 UTC and the 70 hatch bolts were put in place. At 45 minutes prior to the scheduled launch, a pad technician discovered that one of the hatch bolts was misaligned. During a 30-minute hold that was called, McDonnell and NASA Space Task Group engineers decided that the 69 remaining bolts should be sufficient to hold the hatch in place and blow it at the appropriate time. The misaligned bolt was not replaced.\n\n\"Liberty Bell 7\" was launched at 12:20:36 UTC, July 21, 1961.\n\nGrissom later admitted at the postflight debriefing that he was \"a bit scared\" at liftoff, but he added that he soon gained confidence along with the acceleration increase. Hearing the engine roar at the pedestal, he thought that his elapsed-time clock had started late. Like Shepard, he was amazed at the smooth quality of the liftoff, but then he noticed gradually more severe vibrations. These were never violent enough to impair his vision.\n\nGrissom's cabin pressure sealed off at the proper altitude, about , and he felt elated that the environmental control system was in good working order. The suit and cabin temperatures, respectively about were quite comfortable. Watching his instruments for the pitch rate of the Redstone, Grissom saw it follow directions as programmed, tilting over at about one deg/s.\n\nDuring a three \"g\" (29 m/s²) acceleration on the up-leg of his flight, Grissom noticed a sudden change in the color of the horizon from light blue to jet black. His attention was distracted by the noise of the tower-jettison rocket firing on schedule. Grissom felt the separation, and he watched the tower through the window as it drifted off, trailing smoke, to his right. At two minutes and 22 seconds after launch, the Redstone's Rocketdyne engine cut off after building a speed of 6,561 ft/s (1 969 m/s). Grissom had a strong sensation of tumbling during the transition from high to zero acceleration, and, while he had become familiar with this sensation in centrifuge training, for a moment he lost his bearings.\n\nThe Redstone coasted for 10 seconds after its engine cut off; then a sharp report signaled that the posigrade rockets were popping the spacecraft loose from the booster. Although Grissom peered out his window throughout his ship's turnaround maneuver, he never caught sight of his booster.\n\nWith turnaround accomplished, the Air Force jet pilot for the first time became a space pilot, assuming manual-proportional control. A constant urge to look out the window made concentrating on his control tasks difficult. He told Shepard back in Mercury Control that the panorama of Earth's horizon, presenting an 800 mi (1 300 km) arc at peak altitude, was fascinating. His instruments rated a poor second to the spectacle below.\n\nTurning reluctantly to his dials and control stick, Grissom made a pitch movement change, but was past his desired mark. He jockeyed the handcontroller stick for position, trying to damp out all oscillations, then made a yaw movement and went too far in that direction. By the time the proper attitude was attained, the short time allocated for these maneuvers had been used, so he omitted the roll movement altogether. Grissom found the manual controls very sluggish when compared to the Mercury procedures trainer. He then switched to the new rate command control system and found perfect response, although fuel consumption was high.\n\nAfter the pitch and yaw maneuvers, Grissom made a roll-over movement so he could see the ground from his window. Some land beneath the clouds (later determined to be western Florida around the Apalachicola area) appeared in the hazy distance, but the pilot was unable to identify it. Suddenly Cape Canaveral came into view so clearly that Grissom found it hard to believe that his slant-range was over .\n\nHe saw Merritt Island, the Banana River, the Indian River, and what appeared to be a large airport runway. South of Cape Canaveral, he saw what he believed to be West Palm Beach.\n\nWith \"Liberty Bell 7\" at an altitude of , it was now time to position the spacecraft in its reentry attitude. Grissom had initiated the retrorocket sequence and the spacecraft was arcing downward. His pulse reached 171 beats per minute. Retrofire gave him the distinct and peculiar feeling that he had reversed his backward flight through space and was actually moving face forward. As he plummeted downward, he saw what appeared to be two of the spent retrorockets pass across the periscope view after the retrorocket package had been jettisoned.\n\nPitching the spacecraft over into a reentry attitude of 14 degrees from Earth-vertical, the pilot tried to see the stars out his observation window. Instead the glare of sunlight filled his cabin, making it difficult to read the panel dials, particularly those with blue lights. Grissom thought that he would not have noticed the 0.05 \"g\" (0.5 m/s²) light if he had not known it was about to flash on.\n\nReentry presented no problem. Grissom could not feel the oscillations following the acceleration buildup; he could only read them on the rate indicators. Meanwhile, he continued to report to the Mercury Control Center on his electric current reading, fuel quantity, acceleration, and other instrument indications. Condensation and smoke trailed off the heatshield at about as \"Liberty Bell 7\" plunged back into the atmosphere.\n\nThe drogue parachute deployed on schedule at . Grissom said he saw the deployment and felt some resulting pulsating motion, but not enough to worry him. Main parachute deployment occurred at , which was about higher than the design nominal altitude. Watching the main chute unfurl, Grissom spotted a L-shaped tear and another puncture in the canopy. Although he worried about them, the holes grew no bigger and his rate of descent soon slowed to about . Dumping his peroxide control fuel, the pilot began transmitting his panel readings.\n\nA \"clunk\" confirmed that the landing bag had dropped in preparation for impact. Grissom then removed his oxygen hose and opened his visor, but deliberately left the suit ventilation hose attached. Impact was milder than he had expected, although the spacecraft heeled over in the water until Grissom was lying on his left side. He thought he was facing downward. The spacecraft gradually righted itself, and, as the window cleared the water, Grissom jettisoned the reserve parachute and activated the rescue aids switch. \"Liberty Bell 7\" still appeared to be watertight, although it was rolling badly with the swells.\n\nPreparing for recovery, he disconnected his helmet and checked himself for debarkation. The neck dam did not unroll easily; Grissom tinkered with his suit collar to ensure his buoyancy in the event that he had to get out of the spacecraft quickly. When the recovery helicopters, which had taken to the air at launch time and visually followed the contrails and parachute descent, were still about from the impact point, which was only beyond the bullseye, Lieutenant James L. Lewis, the pilot of the primary recovery helicopter, radioed Grissom to ask if he was ready for pickup. He replied that he wanted them to wait five minutes while he recorded his cockpit panel data. Using a grease pencil with the pressure suit gloves was awkward, and several times the suit ventilation caused the neck dam to balloon, but Grissom simply placed his finger between neck and dam to allow the air to escape.\n\nAfter logging the panel data, Grissom asked the helicopters to begin the approach for pickup. He removed the pin from the hatch-cover detonator and lay back in the couch. \"I was lying there, minding my own business,\" he said afterward, \"when I heard a dull thud.\" The hatch cover blew away, and salt water swished into the spacecraft as it bobbed in the ocean. The capsule began taking on water and was sinking fast.\n\nGrissom had difficulty recollecting his actions at this point, but he was certain that he had not touched the hatch-activation plunger. He had earlier unbuckled himself from most of his harness; he now removed his helmet, grasped the instrument panel with his right hand, and climbed though the hatchway.\n\nThe copilot of the nearest recovery helicopter said that as he was preparing, per procedure, to cut off the spacecraft's antenna whip with a squib-actuated cutter at the end of a pole, the hatch cover flew off, struck the water about away, then skipped over the waves. Next he saw Grissom climb through the hatch and swim away.\n\nLeaving aside the swimming astronaut, Lewis completed his approach to the sinking spacecraft, as both he and Reinhard were intent on spacecraft recovery. This action was a conditioned reflex based on past training experience. While training off the Virginia beaches the helicopter pilots had noted that the astronauts seemed at home in and to enjoy the water. So Reinhard quickly cut the high-frequency antenna as soon as the helicopter reached \"Liberty Bell 7\". Throwing aside the antenna cutting device, Reinhard picked up the shepherd's-hook recovery pole and carefully threaded the crook through the recovery loop on top of the spacecraft. By this time Lewis had lowered the helicopter to assist Reinhard in his task to a point that the helicopter's three wheels were in the water. The capsule sank out of sight, but the pickup pole tangled as the attached cable went taut, indicating to the helicopter pilots that they had made the catch.\n\nReinhard immediately prepared to pass the floating astronaut the personnel hoist, but at that moment Lewis called a warning that a detector light had flashed on the instrument panel, indicating that metal chips were in the oil sump because of engine strain. Considering the implication of impending engine failure, Lewis told Reinhard to retract the personnel hoist while he called the second helicopter to retrieve Grissom.\n\nMeanwhile, Grissom, having made certain that he was not snared by any lines, noticed that the primary helicopter was having trouble raising the submerged spacecraft. He swam back to the spacecraft to see if he could assist, but found the cable properly attached. When he looked up for the personnel line, he saw the helicopter start to move away.\n\nSuddenly, Grissom realized that he was not riding as high in the water as he had been. All the time he had been in the water he kept feeling air escape through the neck dam. The more air he lost, the less buoyancy he had. Moreover, he had forgotten to secure his suit inlet valve. Swimming was becoming difficult, and now with the second helicopter moving in he found the rotor wash between the two aircraft was making swimming more difficult. Bobbing under the waves, Grissom was scared, angry, and looking for a swimmer from one of the helicopters to help him tread water. Then he caught sight of a familiar face, that of George Cox, aboard the second helicopter. Cox was the copilot who had retrieved both the chimpanzee Ham and Shepard on the first Mercury flight. With his head barely above water, Grissom found the sight of Cox heartening.\n\nCox tossed the \"horse-collar\" lifeline straight to Grissom, who immediately wrapped himself into the sling backwards. Lack of orthodoxy mattered little to Grissom now, for he was on his way to the safety of the helicopter, even though swells dunked him twice more before he got aboard. His first thought was to get a life preserver on. Grissom had been either swimming or floating for a period of only four or five minutes, \"although it seemed like an eternity to me,\" as he said afterward.\n\nAs the first helicopter moved away from Grissom, it struggled to raise the spacecraft high enough to drain the water from the impact bag. At one point the spacecraft was almost clear of the water, but like an anchor it prevented the helicopter from moving forward. The flooded capsule weighed over , beyond the helicopter's lifting capacity. The pilot, watching his insistent red warning light, decided not to chance losing two craft in one day. He finally cast loose, allowing the spacecraft to sink swiftly. Martin Byrnes, aboard the carrier, suggested that a marker be placed at the point so that the spacecraft might be recovered later. Rear Admiral J. E. Clark advised Byrnes that in that area the depth was about .\n\nSubstantial controversy ensued, as Grissom reported that the hatch had blown prematurely without his authorization. An independent technical review of the incident between August and October 1961 raised doubts regarding the theory that Grissom had blown the hatch and was responsible for the loss of the spacecraft. There is strong evidence that the Astronaut Office did not accept Grissom's guilt in the fact that he was maintained in the prime rotation spot for future flights, commanding the first Gemini flight, and the first planned Apollo flight.\n\nThree Mercury flights later, Astronaut Wally Schirra manually blew \"Sigma 7\"'s hatch after recovery when his spacecraft was on the deck of the recovery ship, in a deliberate attempt to dispel the rumor that Grissom might have blown the capsule's hatch deliberately. As anticipated, the kickback from the manual trigger left Schirra with a visible injury to his right hand. Grissom was uninjured when he exited the spacecraft, as documented by his postflight physical. This strongly supports his assertion that he did not \"accidentally\" hit the trigger, since in that case he would have been even more likely to injure himself than with intentional activation.\n\nIn a 1965 interview, Grissom said that he believed the external release lanyard came loose, triggering the hatch release. On the \"Liberty Bell 7\", this release lanyard was held in place by only one screw. This theory was accepted by Guenter Wendt, the Pad Leader for most early American manned spaceflights.\n\nDuring a launch simulation on Apollo 1 in 1967, the combination of a cabin fire and an inward opening hatch contributed to the death of Grissom, as well as that of the astronauts Ed White and Roger B. Chaffee in a launch-pad fire. Use of an explosive hatch had been rejected following the discovery by engineers that, in fact, an explosive egress system on a spacecraft could inadvertently fire without being triggered. Following the Apollo fire, Block II Apollo spacecraft were equipped with rapid-opening systems.\n\nAfter several unsuccessful attempts in 1992 and 1993, Oceaneering International, Inc. lifted the \"Liberty Bell 7\" off the floor of the Atlantic Ocean and onto the deck of a recovery ship on July 20, 1999, the 30th anniversary of the Apollo 11 lunar landing. The team was led by Curt Newport and financed by the Discovery Channel.\nThe spacecraft was found after a 14-year effort by Newport at a depth of nearly , east-southeast of Cape Canaveral.. Among the items found within were part of the flight gear and several Mercury head dimes which were taken to space to be souvenirs of the flight.\n\nAfter \"Liberty Bell 7\" was secured on the deck of the recovery ship, the \"Ocean Project\", experts removed and disposed of an explosive device (SOFAR bomb) that was supposed to detonate in the event of the spacecraft's sinking, but which failed to explode. The spacecraft was then placed in a container filled with seawater to prevent further corrosion. The Cosmosphere, in Hutchinson, Kansas, disassembled and cleaned the spacecraft, and it was released for a national tour through September 15, 2006. The spacecraft was then returned to the Cosmosphere where it is on permanent display. In 2016, it was temporarily lent to The Children's Museum of Indianapolis.\n\nPhilip Kaufman's 1983 film \"The Right Stuff\" includes a dramatization of the \"Liberty Bell 7\" mission in which Fred Ward played Gus Grissom.\n\n\n"}
{"id": "10897780", "url": "https://en.wikipedia.org/wiki?curid=10897780", "title": "Mersenne's laws", "text": "Mersenne's laws\n\nMersenne's laws are laws describing the frequency of oscillation of a stretched string or monochord, useful in musical tuning and musical instrument construction. The equation was first proposed by French mathematician and music theorist Marin Mersenne in his 1637 work \"Traité de l'harmonie universelle\". Mersenne's laws govern the construction and operation of string instruments, such as pianos and harps, which must accommodate the total tension force required to keep the strings at the proper pitch. Lower strings are thicker, thus having a greater mass per unit length. They typically have lower tension. Guitars are a familiar exception to this - string tensions are similar, for playability, so lower string pitch is largely achieved with increased mass per length. Higher-pitched strings typically are thinner, have higher tension, and may be shorter. \"This result does not differ substantially from Galileo's, yet it is rightly known as Mersenne's law,\" because Mersenne physically proved their truth through experiments (while Galileo considered their proof impossible). \"Mersenne investigated and refined these relationships by experiment but did not himself originate them\". Though his theories are correct, his measurements are not very exact, and his calculations were greatly improved by Joseph Sauveur (1653–1716) through the use of acoustic beats and metronomes.\n\nThe fundamental frequency is:\n\n\nThus, for example, all other properties of the string being equal, to make the note one octave higher (2/1) one would need either to decrease its length by half (1/2), to increase the tension to the square (4), or to decrease its mass per unit length by the inverse square (1/4).\nThese laws are derived from Mersenne's equation 22:\n\nThe formula for the fundamental frequency is:\n\nwhere \"f\" is the frequency, \"L\" is the length, \"F\" is the force and \"μ\" is the mass per unit length.\n\nSimilar laws were not developed for pipes and wind instruments at the same time since Mersenne's laws predate the conception of wind instrument pitch being dependent on longitudinal waves rather than \"percussion\".\n\n"}
{"id": "50311973", "url": "https://en.wikipedia.org/wiki?curid=50311973", "title": "Microfluidic cell culture", "text": "Microfluidic cell culture\n\nMicrofluidic cell culture integrates knowledge from biology, biochemistry, engineering, and physics to develop devices and techniques for culturing, maintaining, analyzing, and experimenting with cells at the microscale. It merges microfluidics, a set of technologies used for the manipulation of small fluid volumes (μL, nL, pL) within artificially fabricated microsystems, and cell culture, which involves the maintenance and growth of cells in a controlled laboratory environment. Microfluidics has been used for cell biology studies as the dimensions of the microfluidic channels are well suited for the physical scale of cells. For example, eukaryotic cells have linear dimensions between 10-100 μm which falls within the range of microfluidic dimensions. A key component of microfluidic cell culture is being able to mimic the cell microenvironment which includes soluble factors that regulate cell structure, function, behavior, and growth. Another important component for the devices is the ability to produce stable gradients that are present \"in vivo\" as these gradients play a significant role in understanding chemotactic, durotactic, and haptotactic effects on cells.\n\nSome considerations for microfluidic devices relating to cell culture include:\nFabrication material is crucial as not all polymers are biocompatible, with some materials such as PDMS causing undesirable adsorption or absorption of small molecules. Additionally, uncured PDMS oligomers can leach into the cell culture media, which can harm the microenvironment. As an alternative to commonly used PDMS, there have been advances in the use of thermoplastics (e.g., polystyrene) as a replacement material.\n\nSpatial organization of cells in microscale devices largely depends on the culture region geometry for cells to perform functions \"in vivo\". For example, long, narrow channels may be desired to culture neurons. The perfusion system chosen might also affect the geometry chosen. For example, in a system that incorporates syringe pumps, channels for perfusion inlet, perfusion outlet, waste, and cell loading would need to be added for the cell culture maintenance. Perfusion in microfluidic cell culture is important to enable long culture periods on-chip and cell differentiation.\n\nOther critical aspects for controlling the microenvironment include: cell seeding density, reduction of air bubbles as they can rupture cell membranes, evaporation of media due to an insufficiently humid environment, and cell culture maintenance (i.e. regular, timely media changes).\n\nSome of the major advantages of microfluidic cell culture include reduced sample volumes (especially important when using primary cells, which are often limited) and the flexibility to customize and study multiple microenvironments within the same device. A reduced cell population can also be used in a microscale system (e.g., a few hundred cells) in comparison to macroscale culture systems (which often require 10 – 10 cells); this can make studying certain cell-cell interactions more accessible. These reduced cell numbers make studying non-dividing or slow dividing cells (e.g., stem cells) easier than traditional culture methods (e.g., flasks, petri dishes, or well plates) due to the smaller sample volumes. Given the small dimensions in microfluidics, laminar flow can be achieved, allowing manipulations with the culture system to be done easily without affecting other culture chambers. Laminar flow is also useful as is it mimics \"in vivo\" fluid dynamics more accurately, often making microscale culture more relevant than traditional culture methods.\n\nTwo-dimensional (2D) cell culture is cell culture that takes place on a flat surface, e.g. the bottom of a well-plate, and is known as the conventional method. While these platforms are useful for growing and passaging cells to be used in subsequent experiments, they are not ideal environments to monitor cell responses to stimuli as cells cannot freely move or perform functions as observed \"in vivo\" that are dependent on cell-extracellular matrix material interactions.\n\nThree-dimensional (3D) cell culture is cell culture that takes place in a biologically relevant matrix, usually this involves cells being embedded in a hydrogel containing extracellular molecules (e.g., collagen). By adding an additional dimension, more advanced cell architectures can be achieved, and cell behavior is more representative of \"in vivo\" dynamics; cells can engage in enhanced communication with neighboring cells and cell-extracellular matrix interactions can be modeled. These simplified 3D cell culture models can be combined in a manner that recapitulates tissue- and organ-level functions in devices known as organ-on-a-chip. In these devices, chambers or collagen layers containing different cell types can interact with one another for multiple days while various channels deliver nutrients to the cells. An advantage of these devices is that tissue function can be characterized and observed under controlled conditions (e.g., effect of shear stress on cells, effect of cyclic strain or other forces) to better understand the overall function of the organ. While these 3D models ofter better model organ function on a cellular level compared with 2D models, there are still challenges. Some of the challenges include: imaging of the cells, control of gradients in static models (i.e., without a perfusion system), and difficulty recreating vasculature. Despite these challenges, 3D models are still used as tools for studying and testing drug responses in pharmacological studies. In recent years, there are microfluidic devices reproducing the complex \"in vivo\" microvascular network. The device is able to create a physiologically realistic 3D environment, which is desirable as a tool for drug screening, drug delivery, cell-cell interactions, tumor metastasis etc.\n"}
{"id": "56425279", "url": "https://en.wikipedia.org/wiki?curid=56425279", "title": "Mitutanka", "text": "Mitutanka\n\nMitutanka (Matootonah) was the lower Mandan village at the time of the Lewis and Clark expedition.At the time that Lewis and Clark visited the main chief was Sheheke\n\n. After a catastrophic smallpox epidemic, the Nuitadi Mandans of Good Boy moved north and later built Mitutanka at the confluence of the Knife River with the Missouri River. Mitutanka was on the west Bank while the Ruptare town of Ruptare was on the east bank of the Missouri.\n"}
{"id": "51501092", "url": "https://en.wikipedia.org/wiki?curid=51501092", "title": "NGC 171", "text": "NGC 171\n\nNGC 171 is a barred spiral galaxy with an apparent magnitude of 12, located around 3 million light-years away in the constellation Cetus. The galaxy has 2 main medium-wound arms, with a few minor arms, and a fairly bright nucleus and bulge. It was discovered on 20 October 1784 by William Herschel. It is also known as NGC 175.\n\n\n"}
{"id": "390905", "url": "https://en.wikipedia.org/wiki?curid=390905", "title": "New Horizons", "text": "New Horizons\n\nNew Horizons is an interplanetary space probe that was launched as a part of NASA's New Frontiers program. Engineered by the Johns Hopkins University Applied Physics Laboratory (APL) and the Southwest Research Institute (SwRI), with a team led by S. Alan Stern, the spacecraft was launched in 2006 with the primary mission to perform a flyby study of the Pluto system in 2015, and a secondary mission to fly by and study one or more other Kuiper belt objects (KBOs) in the decade to follow. It is the fifth artificial object to achieve the escape velocity needed to leave the Solar System.\n\nOn January 19, 2006, \"New Horizons\" was launched from Cape Canaveral Air Force Station by an Atlas V rocket directly into an Earth-and-solar escape trajectory with a speed of about . At launch, it was the fastest probe ever launched from Earth, but was beaten by the Parker Solar Probe on 12 August 2018. After a brief encounter with asteroid 132524 APL, \"New Horizons\" proceeded to Jupiter, making its closest approach on February 28, 2007, at a distance of . The Jupiter flyby provided a gravity assist that increased \"New Horizons\" speed; the flyby also enabled a general test of \"New Horizons\" scientific capabilities, returning data about the planet's atmosphere, moons, and magnetosphere.\n\nMost of the post-Jupiter voyage was spent in hibernation mode to preserve on-board systems, except for brief annual checkouts. On December 6, 2014, \"New Horizons\" was brought back online for the Pluto encounter, and instrument check-out began. On January 15, 2015, the \"New Horizons\" spacecraft began its approach phase to Pluto.\n\nOn July 14, 2015, at 11:49 UTC, it flew above the surface of Pluto, making it the first spacecraft to explore the dwarf planet. On October 25, 2016, at 21:48 UTC, the last of the recorded data from the Pluto flyby was received from \"New Horizons\". Having completed its flyby of Pluto, \"New Horizons\" has maneuvered for a flyby of Kuiper belt object , expected to take place on January 1, 2019, when it will be 43.4 AU from the Sun. In August 2018, NASA cited results by \"Alice\" on \"New Horizons\" to confirm the existence of a \"hydrogen wall\" at the outer edges of the Solar System, that was first detected in 1992 by the two Voyager spacecraft.\n\nIn August 1992, JPL scientist Robert Staehle called Pluto discoverer Clyde Tombaugh, requesting permission to visit his planet. \"I told him he was welcome to it,\" Tombaugh later remembered, \"though he's got to go one long, cold trip.\" The call eventually led to a series of proposed Pluto missions, leading up to \"New Horizons\".\n\nStamatios \"Tom\" Krimigis, head of the Applied Physics Laboratory's space division, one of many entrants in the New Frontiers Program competition, formed the \"New Horizons\" team with Alan Stern in December 2000. Appointed as the project's principal investigator, Stern was described by Krimigis as \"the personification of the Pluto mission\". \"New Horizons\" was based largely on Stern's work since \"Pluto 350\" and involved most of the team from \"Pluto Kuiper Express\". The \"New Horizons\" proposal was one of five that were officially submitted to NASA. It was later selected as one of two finalists to be subject to a three-month concept study, in June 2001. The other finalist, POSSE (Pluto and Outer Solar System Explorer), was a separate, but similar Pluto mission concept by the University of Colorado Boulder, led by principal investigator Larry W. Esposito, and supported by the JPL, Lockheed Martin and the University of California. However, the APL, in addition to being supported by \"Pluto Kuiper Express\" developers at the Goddard Space Flight Center and Stanford University, were at an advantage; they had recently developed \"NEAR Shoemaker\" for NASA, which had successfully entered orbit around 433 Eros earlier in the year, and would later land on the asteroid to scientific and engineering fanfare.\n\nIn November 2001, \"New Horizons\" was officially selected for funding as part of the New Frontiers program. However, the new NASA Administrator appointed by the Bush Administration, Sean O'Keefe, was not supportive of \"New Horizons\", and effectively cancelled it by not including it in NASA's budget for 2003. NASA's Associate Administrator for the Science Mission Directorate Ed Weiler prompted Stern to lobby for the funding of \"New Horizons\" in hopes of the mission appearing in the Planetary Science Decadal Survey; a prioritized \"wish list\", compiled by the United States National Research Council, that reflects the opinions of the scientific community. After an intense campaign to gain support for \"New Horizons\", the Planetary Science Decadal Survey of 2003–2013 was published in the summer of 2002. \"New Horizons\" topped the list of projects considered the highest priority among the scientific community in the medium-size category; ahead of missions to the Moon, and even Jupiter. Weiler stated that it was a result that \"[his] administration was not going to fight\". Funding for the mission was finally secured following the publication of the report, and Stern's team were finally able to start building the spacecraft and its instruments, with a planned launch in January 2006 and arrival at Pluto in 2015. Alice Bowman became Mission Operations Manager.\n\n\"New Horizons\" is the first mission in NASA's New Frontiers mission category, larger and more expensive than the Discovery missions but smaller than the Flagship Program. The cost of the mission (including spacecraft and instrument development, launch vehicle, mission operations, data analysis, and education/public outreach) is approximately $700 million over 15 years (2001–2016). The spacecraft was built primarily by Southwest Research Institute (SwRI) and the Johns Hopkins Applied Physics Laboratory. The mission's principal investigator is Alan Stern of the Southwest Research Institute (formerly NASA Associate Administrator).\n\nAfter separation from the launch vehicle, overall control was taken by Mission Operations Center (MOC) at the Applied Physics Laboratory in Howard County, Maryland. The science instruments are operated at Clyde Tombaugh Science Operations Center (T-SOC) in Boulder, Colorado. Navigation is performed at various contractor facilities, whereas the navigational positional data and related celestial reference frames are provided by the Naval Observatory Flagstaff Station through Headquarters NASA and JPL; KinetX is the lead on the \"New Horizons\" navigation team and is responsible for planning trajectory adjustments as the spacecraft speeds toward the outer Solar System. Coincidentally the Naval Observatory Flagstaff Station was where the photographic plates were taken for the discovery of Pluto's moon Charon; and the Naval Observatory is itself not far from the Lowell Observatory where Pluto was discovered.\n\n\"New Horizons\" was originally planned as a voyage to the only unexplored planet in the SolarSystem. When the spacecraft was launched, Pluto was still classified as a planet, later to be reclassified as a dwarf planet by the International Astronomical Union (IAU). Some members of the \"New Horizons\" team, including Alan Stern, disagree with the IAU definition and still describe Pluto as the ninth planet. Pluto's satellites Nix and Hydra also have a connection with the spacecraft: the first letters of their names (N and H) are the initials of \"New Horizons\". The moons' discoverers chose these names for this reason, plus Nix and Hydra's relationship to the mythological Pluto.\n\nIn addition to the science equipment, there are several cultural artifacts traveling with the spacecraft. These include a collection of 434,738 names stored on a compact disc, a piece of Scaled Composites's \"SpaceShipOne\", a \"Not Yet Explored\" USPS stamp, and a Flag of the United States, along with other mementos.\n\nAbout of Clyde Tombaugh's ashes are aboard the spacecraft, to commemorate his discovery of Pluto in 1930. A Florida-state quarter coin, whose design commemorates human exploration, is included, officially as a trim weight. One of the science packages (a dust counter) is named after Venetia Burney, who, as a child, suggested the name \"Pluto\" after its discovery.\n\nThe goal of the mission is to understand the formation of the Plutonian system, the Kuiper belt, and the transformation of the early Solar System. The spacecraft collected data on the atmospheres, surfaces, interiors, and environments of Pluto and its moons. It will also study other objects in the Kuiper belt. \"By way of comparison, \"New Horizons\" gathered 5,000 times as much data at Pluto as Mariner did at the Red Planet.\"\n\nSome of the questions the mission attempts to answer are: What is Pluto's atmosphere made of and how does it behave? What does its surface look like? Are there large geological structures? How do solar wind particles interact with Pluto's atmosphere?\n\nSpecifically, the mission's science objectives are to:\n\n\nThe spacecraft is comparable in size and general shape to a grand piano and has been compared to a piano glued to a cocktail bar-sized satellite dish. As a point of departure, the team took inspiration from the Ulysses spacecraft, which also carried a radioisotope thermoelectric generator (RTG) and dish on a box-in-box structure through the outer Solar System. Many subsystems and components have flight heritage from APL's CONTOUR spacecraft, which in turn had heritage from APL's TIMED spacecraft.\n\n\"New Horizons\" body forms a triangle, almost thick. (The \"Pioneers\" have hexagonal bodies, whereas the \"Voyagers\", \"Galileo\", and \"Cassini–Huygens\" have decagonal, hollow bodies.) A 7075 aluminium alloy tube forms the main structural column, between the launch vehicle adapter ring at the \"rear,\" and the radio dish antenna affixed to the \"front\" flat side. The titanium fuel tank is in this tube. The RTG attaches with a 4-sided titanium mount resembling a gray pyramid or stepstool. Titanium provides strength and thermal isolation. The rest of the triangle is primarily sandwich panels of thin aluminium facesheet (less than ) bonded to aluminium honeycomb core. The structure is larger than strictly necessary, with empty space inside. The structure is designed to act as shielding, reducing electronics errors caused by radiation from the RTG. Also, the mass distribution required for a spinning spacecraft demands a wider triangle.\n\nThe interior structure is painted black to equalize temperature by radiative heat transfer. Overall, the spacecraft is thoroughly blanketed to retain heat. Unlike the \"Pioneers\" and \"Voyagers\", the radio dish is also enclosed in blankets that extend to the body. The heat from the RTG adds warmth to the spacecraft while it is in the outer Solar System. While in the inner Solar System, the spacecraft must prevent overheating, hence electronic activity is limited, power is diverted to shunts with attached radiators, and louvers are opened to radiate excess heat. While the spacecraft is cruising inactively in the cold outer Solar System, the louvers are closed, and the shunt regulator reroutes power to electric heaters.\n\n\"New Horizons\" has both spin-stabilized (cruise) and three-axis stabilized (science) modes controlled entirely with hydrazine monopropellant. Additional post launch delta-v of over is provided by a internal tank. Helium is used as a pressurant, with an elastomeric diaphragm assisting expulsion. The spacecraft's on-orbit mass including fuel is over on the Jupiter flyby trajectory, but would have been only for the backup direct flight option to Pluto. Significantly, had the backup option been taken, this would have meant less fuel for later Kuiper belt operations.\n\nThere are 16 thrusters on \"New Horizons\": four and twelve plumbed into redundant branches. The larger thrusters are used primarily for trajectory corrections, and the small ones (previously used on \"Cassini\" and the \"Voyager\" spacecraft) are used primarily for attitude control and spinup/spindown maneuvers. Two star cameras are used to measure the spacecraft attitude. They are mounted on the face of the spacecraft and provide attitude information while in spin-stabilized or 3-axis mode. In between the time of star camera readings, spacecraft orientation is provided by dual redundant miniature inertial measurement units. Each unit contains three solid-state gyroscopes and three accelerometers. Two Adcole Sun sensors provide attitude determination. One detects the angle to the Sun, whereas the other measures spin rate and clocking.\n\nA cylindrical radioisotope thermoelectric generator (RTG) protrudes in the plane of the triangle from one vertex of the triangle. The RTG provided of power at launch, and was predicted to drop approximately 5% every 4years, decaying to by the time of its encounter with the Plutonian system in 2015 and will decay too far to power the transmitters in the 2030s. There are no onboard batteries. RTG output is relatively predictable; load transients are handled by a capacitor bank and fast circuit breakers.\n\nThe RTG, model \"GPHS-RTG,\" was originally a spare from the \"Cassini\" mission. The RTG contains of plutonium-238 oxide pellets. Each pellet is clad in iridium, then encased in a graphite shell. It was developed by the U.S. Department of Energy at the Materials and Fuels Complex, a part of the Idaho National Laboratory.\nThe original RTG design called for of plutonium, but a unit less powerful than the original design goal was produced because of delays at the United States Department of Energy, including security activities, that delayed plutonium production. The mission parameters and observation sequence had to be modified for the reduced wattage; still, not all instruments can operate simultaneously. The Department of Energy transferred the space battery program from Ohio to Argonne in 2002 because of security concerns.\n\nThe amount of radioactive plutonium in the RTG is about one-third the amount on board the Cassini–Huygens probe when it launched in 1997. That Cassini launch was protested by some. The United States Department of Energy estimated the chances of a New Horizons launch accident that would release radiation into the atmosphere at 1 in 350, and monitored the launch as it always does when RTGs are involved. It was estimated that a worst-case scenario of total dispersal of on-board plutonium would spread the equivalent radiation of 80% the average annual dosage in North America from background radiation over an area with a radius of .\n\nThe spacecraft carries two computer systems: the Command and Data Handling system and the Guidance and Control processor. Each of the two systems is duplicated for redundancy, for a total of four computers. The processor used for its flight computers is the Mongoose-V, a 12 MHz radiation-hardened version of the MIPS R3000 CPU. Multiple redundant clocks and timing routines are implemented in hardware and software to help prevent faults and downtime. To conserve heat and mass, spacecraft and instrument electronics are housed together in IEMs (integrated electronics modules). There are two redundant IEMs. Including other functions such as instrument and radio electronics, each IEM contains 9boards. The software of the probe runs on Nucleus RTOS operating system.\n\nThere have been two \"safing\" events, that sent the spacecraft into safe mode:\n\nCommunication with the spacecraft is via X band. The craft had a communication rate of at Jupiter; at Pluto's distance, a rate of approximately per transmitter is expected. Besides the low data rate, Pluto's distance also causes a latency of about 4.5 hours (one-way). The NASA Deep Space Network (DSN) dishes are used to relay commands once it is beyond Jupiter. The spacecraft uses dual modular redundancy transmitters and receivers, and either right- or left-hand circular polarization. The downlink signal is amplified by dual redundant 12-watt traveling-wave tube amplifiers (TWTAs) mounted on the body under the dish. The receivers are new, low-power designs. The system can be controlled to power both TWTAs at the same time, and transmit a dual-polarized downlink signal to the DSN that nearly doubles the downlink rate. DSN tests early in the mission with this dual polarization combining technique were successful, and the capability is now considered operational (when the spacecraft power budget permits both TWTAs to be powered).\n\nIn addition to the high-gain antenna, there are two backup low-gain antennas and a medium-gain dish. The high-gain dish has a Cassegrain reflector layout, composite construction, and a diameter providing over of gain, has a half-power beam width of about a degree. The prime-focus, medium-gain antenna, with a aperture and 10° half-power beam width, is mounted to the back of the high-gain antenna's secondary reflector. The forward low-gain antenna is stacked atop the feed of the medium-gain antenna. The aft low-gain antenna is mounted within the launch adapter at the rear of the spacecraft. This antenna was used only for early mission phases near Earth, just after launch and for emergencies if the spacecraft had lost attitude control.\n\n\"New Horizons\" recorded scientific instrument data to its solid-state memory buffer at each encounter, then transmitted the data to Earth. Data storage is done on two low-power solid-state recorders (one primary, one backup) holding up to s each. Because of the extreme distance from Pluto and the Kuiper belt, only one buffer load at those encounters can be saved. This is because \"New Horizons\" would require approximately 16 months after leaving the vicinity of Pluto to transmit the buffer load back to Earth. At Pluto's distance, radio signals from the space probe back to Earth took four hours and 25 minutes to traverse 4.7 billion km of space.\n\nPart of the reason for the delay between the gathering of and transmission of data is that all of the \"New Horizons\" instrumentation is body-mounted. In order for the cameras to record data, the entire probe must turn, and the one-degree-wide beam of the high-gain antenna was not pointing toward Earth. Previous spacecraft, such as the \"Voyager\" program probes, had a rotatable instrumentation platform (a \"scan platform\") that could take measurements from virtually any angle without losing radio contact with Earth. \"New Horizons\" was mechanically simplified to save weight, shorten the schedule, and improve reliability during its 15-year lifetime.\n\nThe \"Voyager 2\" scan platform jammed at Saturn, and the demands of long time exposures at outer planets led to a change of plans such that the entire probe was rotated to make photos at Uranus and Neptune, similar to how \"New Horizons\" rotated.\n\n\"New Horizons\" carries seven instruments: three optical instruments, two plasma instruments, a dust sensor and a radio science receiver/radiometer. The instruments are to be used to investigate the global geology, surface composition, surface temperature, atmospheric pressure, atmospheric temperature and escape rate of Pluto and its moons. The rated power is , though not all instruments operate simultaneously. In addition, \"New Horizons\" has an Ultrastable Oscillator subsystem, which may be used to study and test the Pioneer anomaly towards the end of the spacecraft's life.\n\nThe Long-Range Reconnaissance Imager (LORRI) is a long-focal-length imager designed for high resolution and responsivity at visible wavelengths. The instrument is equipped with a 1024×1024 pixel by 12-bits-per-pixel monochromatic CCD imager giving a resolution of 5μrad (~1arcsec). The CCD is chilled far below freezing by a passive radiator on the antisolar face of the spacecraft. This temperature differential requires insulation, and isolation from the rest of the structure. The aperture Ritchey–Chretien mirrors and metering structure are made of silicon carbide, to boost stiffness, reduce weight, and prevent warping at low temperatures. The optical elements sit in a composite light shield, and mount with titanium and fiberglass for thermal isolation. Overall mass is , with the optical tube assembly (OTA) weighing about , for one of the largest silicon-carbide telescopes flown at the time (now surpassed by Herschel). For viewing on public web sites the 12-bit per pixel LORRI images are converted to 8-bit per pixel JPEG images. These public images do not contain the full dynamic range of brightness information available from the raw LORRI images files.\n\nSolar Wind Around Pluto (SWAP) is a toroidal electrostatic analyzer and retarding potential analyzer (RPA), that makes up one of the two instruments comprising \"New Horizons\" Plasma and high-energy particle spectrometer suite (PAM), the other being PEPSSI. SWAP measures particles of up to 6.5keV and, because of the tenuous solar wind at Pluto's distance, the instrument is designed with the largest aperture of any such instrument ever flown.\n\nPluto Energetic Particle Spectrometer Science Investigation (PEPSSI) is a time of flight ion and electron sensor that makes up one of the two instruments comprising \"New Horizons\" plasma and high-energy particle spectrometer suite (PAM), the other being SWAP. Unlike SWAP, which measures particles of up to 6.5keV, PEPSSI goes up to 1MeV.\n\n\"Alice\" is an ultraviolet imaging spectrometer that is one of two photographic instruments comprising \"New Horizons\" Pluto Exploration Remote Sensing Investigation (PERSI); the other being the \"Ralph\" telescope. It resolves 1,024wavelength bands in the far and extreme ultraviolet (from 50–), over 32view fields. Its goal is to determine the composition of Pluto's atmosphere. This Alice instrument is derived from another Alice aboard ESA's Rosetta spacecraft.\nIn August 2018, NASA confirmed, based on results by \"Alice\" on the \"New Horizons\" spacecraft, of a \"hydrogen wall\" at the outer edges of the Solar System that was first detected in 1992 by the two Voyager spacecraft.\n\nThe \"Ralph\" telescope, 75 mm in aperture, is one of two photographic instruments that make up \"New Horizons\" Pluto Exploration Remote Sensing Investigation (PERSI), with the other being the Alice instrument. \"Ralph\" has two separate channels: MVIC (Multispectral Visible Imaging Camera), a visible-light CCD imager with broadband and color channels; and LEISA (Linear Etalon Imaging Spectral Array), a near-infrared imaging spectrometer. LEISA is derived from a similar instrument on the Earth Observing-1 spacecraft. \"Ralph\" was named after Alice's husband on \"The Honeymooners\", and was designed after Alice.\n\nOn June 23, 2017, NASA announced that it has renamed the LEISA instrument to the \"Lisa Hardaway Infrared Mapping Spectrometer\" in honor of Lisa Hardaway, the \"Ralph\" program manager at Ball Aerospace, who died in January 2017 at age 50.\n\nThe Venetia Burney Student Dust Counter (VBSDC), built by students at the University of Colorado Boulder, is operating periodically to make dust measurements. It consists of a detector panel, about , mounted on the anti-solar face of the spacecraft (the ram direction), and an electronics box within the spacecraft. The detector contains fourteen polyvinylidene difluoride (PVDF) panels, twelve science and two reference, which generate voltage when impacted. Effective collecting area is . No dust counter has operated past the orbit of Uranus; models of dust in the outer Solar System, especially the Kuiper belt, are speculative. The VBSDC is always turned on measuring the masses of the interplanetary and interstellar dust particles (in the range of nano- and picograms) as they collide with the PVDF panels mounted on the \"New Horizons\" spacecraft. The measured data is expected to greatly contribute to the understanding of the dust spectra of the Solar System. The dust spectra can then be compared with those from observations of other stars, giving new clues as to where Earth-like planets can be found in the universe. The dust counter is named for Venetia Burney, who first suggested the name \"Pluto\" at the age of 11. A thirteen-minute short film about the VBSDC garnered an Emmy Award for student achievement in 2006.\n\nThe Radio Science Experiment (REX) used an ultrastable crystal oscillator (essentially a calibrated crystal in a miniature oven) and some additional electronics to conduct radio science investigations using the communications channels. These are small enough to fit on a single card. Because there are two redundant communications subsystems, there are two, identical REX circuit boards.\n\nOn September 24, 2005, the spacecraft arrived at the Kennedy Space Center on board a C-17 Globemaster III for launch preparations. The launch of \"New Horizons\" was originally scheduled for January 11, 2006, but was initially delayed until January 17, 2006, to allow for borescope inspections of the Atlas V's kerosene tank. Further delays related to low cloud ceiling conditions downrange, and high winds and technical difficulties—unrelated to the rocket itself—prevented launch for a further two days.\n\nThe probe finally lifted off from Pad 41 at Cape Canaveral Air Force Station, Florida, directly south of Space Shuttle Launch Complex 39, at 19:00 UTC on January 19, 2006. The Centaur second stage ignited at 19:04:43 UTC and burned for 5 minutes 25 seconds. It reignited at 19:32 UTC and burned for 9 minutes 47 seconds. The ATK Star 48B third stage ignited at 19:42:37 UTC and burned for 1 minute 28 seconds. Combined, these burns successfully sent the probe on a solar-escape trajectory at . \"New Horizons\" took only nine hours to pass the Moon's orbit. Although there were backup launch opportunities in February 2006 and February 2007, only the first twenty-three days of the 2006 window permitted the Jupiter flyby. Any launch outside that period would have forced the spacecraft to fly a slower trajectory directly to Pluto, delaying its encounter by five to six years.\n\nThe probe was launched by a Lockheed Martin Atlas V 551 rocket, with a third stage added to increase the heliocentric (escape) speed. This was the first launch of the Atlas V 551 configuration, which uses five solid rocket boosters, and the first Atlas V with a third stage. Previous flights had used zero, two, or three solid boosters, but never five. The vehicle, AV-010, weighed at lift-off, and had earlier been slightly damaged when Hurricane Wilma swept across Florida on October 24, 2005. One of the solid rocket boosters was hit by a door. The booster was replaced with an identical unit, rather than inspecting and requalifying the original.\n\nThe launch was dedicated to the memory of launch conductor Daniel Sarokon, who was described by space program officials as one of the most influential people in the history of space travel.\n\nOn January 28 and 30, 2006, mission controllers guided the probe through its first trajectory-correction maneuver (TCM), which was divided into two parts (TCM-1A and TCM-1B). The total velocity change of these two corrections was about . TCM-1 was accurate enough to permit the cancellation of TCM-2, the second of three originally scheduled corrections. On March 9, 2006, controllers performed TCM-3, the last of three scheduled course corrections. The engines burned for 76 seconds, adjusting the spacecraft's velocity by about . Further trajectory maneuvers were not needed until September 25, 2007 (seven months after the Jupiter flyby), when the engines were fired for 15 minutes and 37 seconds, changing the spacecraft's velocity by , followed by another TCM, almost three years later on June 30, 2010, that lasted 35.6 seconds, when \"New Horizons\" had already reached the halfway point (in time traveled) to Pluto.\n\nDuring the week of February 20, 2006, controllers conducted initial in-flight tests of three onboard science instruments, the Alice ultraviolet imaging spectrometer, the PEPSSI plasma-sensor, and the LORRI long-range visible-spectrum camera. No scientific measurements or images were taken, but instrument electronics, and in the case of Alice, some electromechanical systems were shown to be functioning correctly.\n\nOn April 7, 2006, the spacecraft passed the orbit of Mars, moving at roughly away from the Sun at a solar distance of 243 million kilometers.\n\nBecause of the need to conserve fuel for possible encounters with Kuiper belt objects subsequent to the Pluto flyby, intentional encounters with objects in the asteroid belt were not planned. After launch, the \"New Horizons\" team scanned the spacecraft's trajectory to determine if any asteroids would, by chance, be close enough for observation. In May 2006 it was discovered that \"New Horizons\" would pass close to the tiny asteroid 132524 APL on June 13, 2006. Closest approach occurred at 4:05 UTC at a distance of . The asteroid was imaged by \"Ralph\" (use of LORRI was not possible because of proximity to the Sun), which gave the team a chance to test \"Ralph\" capabilities, and make observations of the asteroid's composition as well as light and phase curves. The asteroid was estimated to be in diameter. The spacecraft successfully tracked the rapidly moving asteroid over June 10–12, 2006.\n\nThe first images of Pluto from \"New Horizons\" were acquired September 21–24, 2006, during a test of LORRI. They were released on November 28, 2006. The images, taken from a distance of approximately , confirmed the spacecraft's ability to track distant targets, critical for maneuvering toward Pluto and other Kuiper belt objects.\n\n\"New Horizons\" used LORRI to take its first photographs of Jupiter on September 4, 2006, from a distance of . More detailed exploration of the system began in January 2007 with an infrared image of the moon Callisto, as well as several black-and-white images of Jupiter itself. \"New Horizons\" received a gravity assist from Jupiter, with its closest approach at 05:43:40 UTC on February 28, 2007, when it was from Jupiter. The flyby increased \"New Horizons\" speed by , accelerating the probe to a velocity of relative to the Sun and shortening its voyage to Pluto by three years.\n\nThe flyby was the center of a four-month intensive observation campaign lasting from January to June. Being an ever-changing scientific target, Jupiter has been observed intermittently since the end of the \"Galileo\" mission in September 2003. Knowledge about Jupiter benefited from the fact that \"New Horizons\" instruments were built using the latest technology, especially in the area of cameras, representing a significant improvement over \"Galileo\" cameras, which were modified versions of \"Voyager\" cameras, which, in turn, were modified \"Mariner\" cameras. The Jupiter encounter also served as a shakedown and dress rehearsal for the Pluto encounter. Because Jupiter is much closer to Earth than Pluto, the communications link can transmit multiple loadings of the memory buffer; thus the mission returned more data from the Jovian system than it was expected to transmit from Pluto.\n\nOne of the main goals during the Jupiter encounter was observing its atmospheric conditions and analyzing the structure and composition of its clouds. Heat-induced lightning strikes in the polar regions and \"waves\" that indicate violent storm activity were observed and measured. The Little Red Spot, spanning up to 70% of Earth's diameter, was imaged from up close for the first time. Recording from different angles and illumination conditions, \"New Horizons\" took detailed images of Jupiter's faint ring system, discovering debris left over from recent collisions within the rings or from other unexplained phenomena. The search for undiscovered moons within the rings showed no results. Travelling through Jupiter's magnetosphere, \"New Horizons\" collected valuable particle readings. \"Bubbles\" of plasma that are thought to be formed from material ejected by the moon Io, were noticed in the magnetotail.\n\nThe four largest moons of Jupiter were in poor positions for observation; the necessary path of the gravity-assist maneuver meant that \"New Horizons\" passed millions of kilometers from any of the Galilean moons. Still, its instruments were intended for small, dim targets, so they were scientifically useful on large, distant moons. Emphasis was put on Jupiter's innermost Galilean moon, Io, whose active volcanoes shoot out tons of material into Jupiter's magnetosphere, and further. Out of eleven observed eruptions, three were seen for the first time. That of Tvashtar reached an altitude of up to . The event gave scientists an unprecedented look into the structure and motion of the rising plume and its subsequent fall back to the surface. Infrared signatures of a further 36 volcanoes were noticed. Callisto's surface was analyzed with LEISA, revealing how lighting and viewing conditions affect infrared spectrum readings of its surface water ice. Minor moons such as Amalthea had their orbit solutions refined. The cameras determined their positions, acting as \"reverse optical navigation\".\n\nAfter passing Jupiter, \"New Horizons\" spent most of its journey towards Pluto in hibernation mode: redundant components as well as guidance and control systems were shut down to extend their life cycle, decrease operation costs and free the Deep Space Network for other missions. During hibernation mode, the onboard computer monitored the probe's systems and transmitted a signal back to Earth: a \"green\" code if everything was functioning as expected or a \"red\" code if mission control's assistance was needed. The probe was activated for about two months a year so that the instruments could be calibrated and the systems checked. The first hibernation mode cycle started on June 28, 2007, the second cycle began on December 16, 2008, the third cycle on August 27, 2009, and the fourth cycle on August 29, 2014 after a 10-week test.\n\n\"New Horizons\" crossed the orbit of Saturn on June 8, 2008, and Uranus on March 18, 2011. After astronomers announced the discovery of two new moons in the Pluto system, Kerberos and Styx, mission planners started contemplating the possibility of the probe running into unseen debris and dust left over from ancient collisions between the moons. A study based on 18 months of computer simulations, Earth-based telescope observations and occultations of the Pluto system revealed that the possibility of a catastrophic collision with debris or dust was less than 0.3% on the probe's scheduled course. If the hazard increased, \"New Horizons\" could have used one of two possible contingency plans, the so-called SHBOTs (Safe Haven by Other Trajectories): the probe could have continued on its present trajectory with the antenna facing the incoming particles so the more vital systems would be protected, or, it could have positioned its antenna to make a course correction that would take it just 3000 km from the surface of Pluto where it was expected that the atmospheric drag would clean the surrounding space of possible debris.\n\nWhile in hibernation mode in July 2012, \"New Horizons\" started gathering scientific data with SWAP, PEPSSI and VBSDC. Although it was originally planned to activate just the VBSDC, other instruments were powered on the initiative of principal investigator Alan Stern who decided they could use the opportunity to collect valuable heliospheric data. Before activating the other two instruments, ground tests were conducted to make sure that the expanded data gathering in this phase of the mission would not limit available energy, memory and fuel in the future and that all systems are functioning during the flyby. The first set of data was transmitted in January 2013 during a three-week activation from hibernation. The command and data handling software was updated to address the problem of computer resets.\n\nOther possible targets were Neptune trojans. The probe's trajectory to Pluto passed near Neptune's trailing Lagrange point (\"\"), which may host hundreds of bodies in 1:1 resonance. In late 2013, \"New Horizons\" passed within of the high-inclination L5 Neptune trojan , which was identified shortly before by the New Horizons KBO Search survey team while searching for more distant objects for \"New Horizons\" to fly by after its 2015 Pluto encounter. At that range, would have been bright enough to be detectable by \"New Horizons\" LORRI instrument; however, the \"New Horizons\" team eventually decided that they would not target for observations because the preparations for the Pluto approach took precedence.\n\nImages from July 1 to 3, 2013 by LORRI were the first by the probe to resolve Pluto and Charon as separate objects. On July 14, 2014, mission controllers performed a sixth trajectory-correction maneuver (TCM) since its launch to enable the craft to reach Pluto. Between July 19–24, 2014, \"New Horizons\" LORRI snapped 12 images of Charon revolving around Pluto, covering almost one full rotation at distances ranging from about . In August 2014, astronomers made high-precision measurements of Pluto's location and orbit around the Sun using the Atacama Large Millimeter/submillimeter Array (ALMA) to help NASA's \"New Horizons\" spacecraft accurately home in on Pluto. On December 6, 2014, mission controllers sent a signal for the craft to \"wake up\" from its final Pluto-approach hibernation and begin regular operations. The craft's response that it was \"awake\" arrived to Earth on December 7, 2014, at 02:30 UTC.\n\nDistant-encounter operations at Pluto began on January 4, 2015. At this date images of the targets with the onboard LORRI imager plus \"Ralph\" telescope would only be a few pixels in width. Investigators began taking Pluto and background starfield images to assist mission navigators in the design of course-correcting engine maneuvers that would precisely modify the trajectory of \"New Horizons\" to aim the approach. On January 15, 2015, NASA gave a brief update of the timeline of the approach and departure phases.\n\nOn February 12, 2015, NASA released new images of Pluto (taken from January 25 to 31) from the approaching probe. \"New Horizons\" was more than away from Pluto when it began taking the photos, which showed Pluto and its largest moon, Charon. The exposure time was too short to see Pluto's smaller, much fainter, moons.\n\nInvestigators compiled a series of images of the moons Nix and Hydra taken from January 27 through February 8, 2015, beginning at a range of . Pluto and Charon appear as a single overexposed object at the center. The right side image has been processed to remove the background starfield. The yet smaller two moons, Kerberos and Styx were seen on photos taken on April 25. Starting May 11 a hazard search was performed, by looking for unknown objects that could be a danger to the spacecraft, such as rings or more moons, which were possible to avoid by a course change.\n\nAlso in regards to the approach phase during January 2015, on August 21, 2012, the team announced that they would spend mission time attempting long-range observations of the Kuiper belt object temporarily designated VNH0004 (now designated ), when the object was at a distance from \"New Horizons\" of . The object would be too distant to resolve surface features or take spectroscopy, but it would be able to make observations that cannot be made from Earth, namely a phase curve and a search for small moons. A second object was planned to be observed in June 2015, and a third in September after the flyby; the team hoped to observe a dozen such objects through 2018. On April 15, 2015, Pluto was imaged showing a possible polar cap.\n\nOn July 4, 2015, \"New Horizons\" experienced a software anomaly and went into safe mode, preventing the spacecraft from performing scientific observations until engineers could resolve the problem. On July 5, NASA announced that the problem was determined to be a timing flaw in a command sequence used to prepare the spacecraft for its flyby, and the spacecraft would resume scheduled science operations on July 7. The science observations lost because of the anomaly were judged to have no impact on the mission's main objectives and minimal impact on other objectives.\n\nThe timing flaw consisted of performing two tasks simultaneously—compressing previously acquired data to release space for more data, and making a second copy of the approach command sequence—that together overloaded the spacecraft's primary computer. After the overload was detected, the spacecraft performed as designed: it switched from the primary computer to the backup computer, entered safe mode, and sent a distress call back to Earth. The distress call was received the afternoon of July 4, which alerted engineers that they needed to contact the spacecraft to get more information and resolve the issue. The resolution was that the problem happened as part of preparations for the approach, and was not expected to happen again because no similar tasks were planned for the remainder of the encounter.\n\nThe closest approach of the \"New Horizons\" spacecraft to Pluto occurred at 11:49 UTC on July 14, 2015 at a range of from the surface and from the center of Pluto. Telemetry data confirming a successful flyby and a healthy spacecraft were received on Earth from the vicinity of the Pluto system on July 15, 2015, 00:52:37 UTC, after 22 hours of planned radio silence due to the spacecraft being pointed toward the Pluto system. Mission managers estimated a one in 10,000 chance that debris could have destroyed it during the flyby, preventing it from sending data to Earth. The first details of the encounter were received the next day, but the download of the complete data set took just over 15 months, and analysis of the data will take longer.\n\nThe mission's science objectives are grouped in three distinct priorities. The \"primary objectives\" are required; the \"secondary objectives\" are expected to be met but are not demanded. The \"tertiary objectives\" are desired. These objectives may be attempted, though they may be skipped in favor of the above objectives. An objective to measure any magnetic field of Pluto was dropped. A magnetometer instrument could not be implemented within a reasonable mass budget and schedule, and SWAP and PEPSSI could do an indirect job detecting some magnetic field around Pluto.\n\n\n\"New Horizons\" passed within of Pluto, with this closest approach on July 14, 2015 at 11:50 UTC. \"New Horizons\" had a relative velocity of at its closest approach, and came as close as to Charon. Starting 3.2 days before the closest approach, long-range imaging included the mapping of Pluto and Charon to resolution. This is half the rotation period of the Pluto–Charon system and allowed imaging of all sides of both bodies. Close range imaging was repeated twice per day in order to search for surface changes caused by localized snow fall or surface cryovolcanism. Because of Pluto's tilt, a portion of the northern hemisphere would be in shadow at all times. During the flyby, engineers expected LORRI to be able to obtain select images with resolution as high as if closest distance were around 12,500 km, and MVIC was expected to obtain four-color global dayside maps at resolution. LORRI and MVIC attempted to overlap their respective coverage areas to form stereo pairs. LEISA obtained hyperspectral near-infrared maps at globally and for selected areas.\n\nMeanwhile, Alice characterized the atmosphere, both by emissions of atmospheric molecules (airglow), and by dimming of background stars as they pass behind Pluto (occultation). During and after closest approach, SWAP and PEPSSI sampled the high atmosphere and its effects on the solar wind. VBSDC searched for dust, inferring meteoroid collision rates and any invisible rings. REX performed active and passive radio science. The communications dish on Earth measured the disappearance and reappearance of the radio occultation signal as the probe flew by behind Pluto. The results resolved Pluto's diameter (by their timing) and atmospheric density and composition (by their weakening and strengthening pattern). (Alice can perform similar occultations, using sunlight instead of radio beacons.) Previous missions had the spacecraft transmit through the atmosphere, to Earth (\"downlink\"). Pluto's mass and mass distribution were evaluated by the gravitational tug on the spacecraft. As the spacecraft speeds up and slows down, the radio signal exhibited a Doppler shift. The Doppler shift was measured by comparison with the ultrastable oscillator in the communications electronics.\n\nReflected sunlight from Charon allowed some imaging observations of the nightside. Backlighting by the Sun gave an opportunity to highlight any rings or atmospheric hazes. REX performed radiometry of the nightside.\n\"New Horizons\" best spatial resolution of the small satellites is at Nix, at Hydra, and approximately at Kerberos and Styx. Estimates for the diameters of these bodies are: Nix at ; Hydra at ; Kerberos at ; and Styx at . This translates to a resolution of 164/124/109, 55/42/?, 7/3/?, and 4/3/? pixels in width for Nix, Hydra, Kerberos, and Styx, respectively.\n\nInitial predictions envisioned Kerberos as a relatively large and massive object whose dark surface led to it having a faint reflection. This proved to be wrong as images obtained by \"New Horizons\" on July 14 and sent back to Earth in October 2015 revealed an object just across with a highly reflective surface suggesting the presence of relatively clean water ice.\n\nSoon after the Pluto flyby, in July 2015, \"New Horizons\" reported that the spacecraft was healthy, its flight path was within the margins, and science data of the Pluto–Charon system had been recorded. The spacecraft's immediate task was to begin returning the 6.25 gigabytes of information collected. The free-space path loss at its distance of 4.5 light-hours (3,017,768,400 km) is approximately 303 dB at 7 GHz. Using the high gain antenna and transmitting at full power, New Horizons' EIRP is +83 dBm, and at this distance the signal reaching Earth is −220 dBm. The received signal level (RSL) using one, un-arrayed Deep Space Network antenna with 72 dBi of forward gain equals −148 dBm. Because of the extremely low RSL, it could only transmit data at 1 to 2 kilobits per second.\n\nBy March 30, 2016, \"New Horizons\" had reached the halfway point of transmitting this data. The transfer was completed on October 25, 2016 at 21:48 UTC, when the last piece of data—part of a Pluto–Charon observation sequence by the Ralph/LEISA imager—was received by the Johns Hopkins University Applied Physics Laboratory.\n\nAt a distance of from the Sun and from as of November 2018, \"New Horizons\" is of the constellation Sagittarius at relative to the Sun. The brightness of the Sun from the spacecraft is magnitude −18.5.\n\nThe \"New Horizons\" team requested, and received, a mission extension through 2021 to explore additional Kuiper belt objects (KBOs). During this Kuiper Belt Extended Mission (KEM), the spacecraft will perform a close fly-by of and conduct more distant observations on an additional two dozen objects.\n\nMission planners searched for one or more additional Kuiper belt objects (KBOs) of the order of in diameter as targets for flybys similar to the spacecraft's Plutonian encounter, but, despite the large population of KBOs, many factors limit the number of possible targets. Because the flight path is determined by the Pluto flyby, and the probe only has 33 kilograms of hydrazine remaining, the object to be visited needs to be within a cone, extending from Pluto, of less than a degree's width. This ruled out any possibility for a flyby of Eris, a trans-Neptunian object comparable in size to Pluto. It will also need to be within 55AU, because beyond 55AU, the communications link will become too weak, and the RTG power output will have decayed significantly enough to hinder observations. Desirable KBOs would be well over in diameter, neutral in color (to compare with the reddish Pluto), and, if possible, have a moon that imparts a wobble. After a search along \"New Horizon\" flight path using the Hubble Space Telescope, only three KBOs were found in range, and one of those objects was later dropped from consideration.\n\nIn 2011 a dedicated search for suitable KBOs using ground telescopes was started by mission scientists. Large ground telescopes with wide-field cameras, notably the twin 6.5-meter Magellan Telescopes in Chile, the 8.2-meter Subaru Observatory in Hawaii, and the Canada–France–Hawaii Telescope were used to search for potential targets. Through the citizen-science project, the public helped to scan telescopic images for possible suitable mission candidates by participating in the Ice Hunters project. The ground-based search resulted in the discovery of about 143 KBOs of potential interest, but none of these were close enough to the flight path of \"New Horizons\". Only the Hubble Space Telescope was deemed likely to find a suitable target in time for a successful KBO mission. On June 16, 2014, time on Hubble was granted. Hubble has a much greater ability to find suitable KBOs than ground telescopes. The probability that a target for \"New Horizons\" would be found was estimated beforehand at about 95%.\n\nOn October 15, 2014, it was revealed that Hubble's search had uncovered three potential targets, temporarily designated PT1 (\"potential target 1\"), PT2 and PT3 by the \"New Horizons\" team. All are objects with estimated diameters in the range, too small to be seen by ground telescopes, at distances from the Sun of 43–44 AU, which would put the encounters in the 2018–2019 period. The initial estimated probabilities that these objects are reachable within \"New Horizons\" fuel budget are 100%, 7%, and 97%, respectively. All are members of the \"cold\" (low-inclination, low-eccentricity) classical Kuiper belt, and thus very different from Pluto. PT1 (given the temporary designation \"1110113Y\" on the HST web site), the most favorably situated object, is magnitude 26.8, in diameter, and will be encountered around January 2019. A course change to reach it required about 35% of \"New Horizons\" available trajectory-adjustment fuel supply. A mission to PT3 was in some ways preferable, in that it is brighter and therefore probably larger than PT1, but the greater fuel requirements to reach it would have left less for maneuvering and unforeseen events. Once sufficient orbital information was provided, the Minor Planet Center gave provisional designations to the three target KBOs: (PT1), (PT2), and (PT3). By the fall of 2014, a possible fourth target, , had been eliminated by follow-up observations. PT2 was out of the running before the Pluto flyby. The spacecraft will also study almost 20 KBOs from afar.\n\nOn August 28, 2015, (PT1) was chosen as the flyby target. The necessary course adjustment was performed with four engine firings between October 22 and November 4, 2015. The flyby is scheduled for January 1, 2019. Funding was secured on July 1, 2016.\n\nAside from its flyby of , the extended mission for \"New Horizons\" calls for the spacecraft to conduct observations of, and look for ring systems around, between 25 and 35 different KBOs. In addition, it will continue to study the gas, dust and plasma composition of the Kuiper belt before the mission extension ends in 2021.\n\nOn November 2, 2015, \"New Horizons\" imaged KBO 15810 Arawn with the LORRI instrument from , showing the shape of the object and one or two details. This KBO was again imaged by the LORRI instrument on April 7–8, 2016, from a distance of . The new images allowed the science team to further refine the location of 15810 Arawn to within and to determine its rotational period of 5.47 hours.\n\nIn July 2016, the LORRI camera captured some distant images of Quaoar from ; the oblique view will complement Earth-based observations to study the object's light-scattering properties.\n\nOn December 5, 2017, when \"New Horizons\" was 40.9 AU from Earth, a calibration image of the Wishing Well cluster marked the most distant image from Earth ever taken by a spacecraft (breaking the 27-year record set by Voyager 1's famous Pale Blue Dot). Two hours later, New Horizons surpassed its own record, imaging the Kuiper belt objects and from a distance of 0.50 and 0.34 AU, respectively. These are the closest images of a Kuiper belt object besides Pluto .\n\nScience objectives of the 2018-2019 flyby include characterizing the geology and morphology of (nicknamed Ultima Thule), mapping the surface composition (searching for ammonia, carbon monoxide, methane, and water ice). Searches will be conducted for orbiting moonlets, a coma, rings, and the surrounding environment. Additional objectives include:\n\n\n is the first object to be targeted for a flyby that was discovered after the spacecraft was launched. \"New Horizons\" is planned to come within of , three times closer than the spacecraft's earlier encounter with Pluto. Images with a resolution of up to are expected.\n\nThe new mission began on October 22, 2015, when \"New Horizons\" carried out the first in a series of four initial targeting maneuvers designed to send it toward . The maneuver, which started at approximately 19:50 UTC and used two of the spacecraft's small hydrazine-fueled thrusters, lasted approximately 16 minutes and changed the spacecraft's trajectory by about . The remaining three targeting maneuvers took place on October 25, October 28, and November 4, 2015.\n\nThe craft was brought out of its hibernation at approximately 00:33 UTC on June 5, 2018 (06:12 UTC ), in order to prepare for the approach phase. After verifying its health status, the spacecraft transitioned from a spin-stabilized mode to a 3-axis-stabilized mode on August 13, 2018. The official approach phase began on August 16, 2018, and continues through December 24, 2018. The first images from \"New Horizons\" were scheduled to be taken in early September 2018. If obstacles are detected, mission planners may opt to divert the spacecraft's trajectory as late as mid-December 2018.\n\n\"New Horizons\" made its first detection of on August 16, 2018, from a distance of . At that time, was visible at magnitude 20, against a crowded stellar background in the direction of the constellation Sagittarius.\n\nThe Core phase begins a week before the encounter, and continues for two days after the encounter. The majority of the science data will be collected within 48 hours of the closest approach in a phase called the Inner Core. Closest approach will occur January 1, 2019, at 05:33 UTC, at which point it will be from the Sun. At this distance, the one-way transit time for radio signals between Earth and \"New Horizons\" will be 6 hours.\n\nAfter the encounter, preliminary data will be sent to Earth on January 1 and 2, 2019, to transmit high priority data. On January 9, \"New Horizons\" will return to a spin-stabilized mode, to prepare to send the remainder of its data back to Earth. This downlink is expected to continue through most of 2019.\n\n\"New Horizons\" has been called \"the fastest spacecraft ever launched\" because it left Earth at , faster than any other spacecraft to date. It is also the first spacecraft launched directly into a solar escape trajectory, which requires an approximate speed while near Earth of , plus additional delta-v to cover air and gravity drag, all to be provided by the launch vehicle.\n\nHowever, it is not the fastest spacecraft to leave the Solar System. , this record is held by \"Voyager 1\", traveling at relative to the Sun. \"Voyager 1\" attained greater hyperbolic excess velocity than \"New Horizons\" thanks to gravity assists by Jupiter and Saturn. When \"New Horizons\" reaches the distance of , it will be travelling at about , around slower than \"Voyager 1\" at that distance. Other spacecraft, such as the \"Helios\" probes, can also be measured as the fastest objects, because of their orbital speed relative to the Sun at perihelion: for \"Helios-B\". Because they remain in solar orbit, their specific orbital energy relative to the Sun is lower than \"New Horizons\" and other artificial objects escaping the Solar System.\n\n\"New Horizons\" Star 48B third stage is also on a hyperbolic escape trajectory from the Solar System, and reached Jupiter before the \"New Horizons\" spacecraft. The Star 48B was expected to cross Pluto's orbit on October 15, 2015. Because it is not in controlled flight, it did not receive the correct gravity assist, and passed within of Pluto. The Centaur second stage did not achieve solar escape velocity, and remains in a heliocentric orbit.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "244611", "url": "https://en.wikipedia.org/wiki?curid=244611", "title": "Newton's law of universal gravitation", "text": "Newton's law of universal gravitation\n\nNewton's law of universal gravitation states that every particle attracts every other particle in the universe with a force which is directly proportional to the product of their masses and inversely proportional to the square of the distance between their centers. This is a general physical law derived from empirical observations by what Isaac Newton called inductive reasoning. It is a part of classical mechanics and was formulated in Newton's work \"Philosophiæ Naturalis Principia Mathematica\" (\"the \"Principia\"\"), first published on 5 July 1686. When Newton's book was presented in 1686 to the Royal Society, Robert Hooke made a claim that Newton had obtained the inverse square law from him.\n\nIn today's language, the law states that every point mass attracts every other point mass by a force acting along the line intersecting the two points. The force is proportional to the product of the two masses, and inversely proportional to the square of the distance between them.\n\nThe equation for universal gravitation thus takes the form:\nwhere \"F\" is the gravitational force acting between two objects, \"m\" and \"m\" are the masses of the objects, \"r\" is the distance between the centers of their masses, and \"G\" is the gravitational constant.\n\nThe first test of Newton's theory of gravitation between masses in the laboratory was the Cavendish experiment conducted by the British scientist Henry Cavendish in 1798. It took place 111 years after the publication of Newton's \"Principia\" and approximately 71 years after his death.\n\nNewton's law of gravitation resembles Coulomb's law of electrical forces, which is used to calculate the magnitude of the electrical force arising between two charged bodies. Both are inverse-square laws, where force is inversely proportional to the square of the distance between the bodies. Coulomb's law has the product of two charges in place of the product of the masses, and the electrostatic constant in place of the gravitational constant.\n\nNewton's law has since been superseded by Albert Einstein's theory of general relativity, but it continues to be used as an excellent approximation of the effects of gravity in most applications. Relativity is required only when there is a need for extreme precision, or when dealing with very strong gravitational fields, such as those found near extremely massive and dense objects, or at very close distances (such as Mercury's orbit around the Sun).\n\nThe relation of the distance of objects in free fall to the square of the time taken had recently been confirmed by Grimaldi and Riccioli between 1640 and 1650. They had also made a calculation of the gravitational constant by recording the oscillations of a pendulum.\n\nA modern assessment about the early history of the inverse square law is that \"by the late 1670s\", the assumption of an \"inverse proportion between gravity and the square of distance was rather common and had been advanced by a number of different people for different reasons\". The same author credits Robert Hooke with a significant and seminal contribution, but treats Hooke's claim of priority on the inverse square point as irrelevant, as several individuals besides Newton and Hooke had suggested it. He points instead to the idea of \"compounding the celestial motions\" and the conversion of Newton's thinking away from \"centrifugal\" and towards \"centripetal\" force as Hooke's significant contributions.\n\nNewton gave credit in his \"Principia\" to two people: Bullialdus (who wrote without proof that there was a force on the Earth towards the Sun), and Borelli (who wrote that all planets were attracted towards the Sun). The main influence may have been Borelli, whose book Newton had a copy of.\n\nRobert Hooke published his ideas about the \"System of the World\" in the 1660s, when he read to the Royal Society on March 21, 1666, a paper \"concerning the inflection of a direct motion into a curve by a supervening attractive principle\", and he published them again in somewhat developed form in 1674, as an addition to \"An Attempt to Prove the Motion of the Earth from Observations\". Hooke announced in 1674 that he planned to \"explain a System of the World differing in many particulars from any yet known\", based on three \"Suppositions\": that \"all Celestial Bodies whatsoever, have an attraction or gravitating power towards their own Centers\" and \"they do also attract all the other Celestial Bodies that are within the sphere of their activity\"; that \"all bodies whatsoever that are put into a direct and simple motion, will so continue to move forward in a straight line, till they are by some other effectual powers deflected and bent...\"; and that \"these attractive powers are so much the more powerful in operating, by how much the nearer the body wrought upon is to their own Centers\". Thus Hooke clearly postulated mutual attractions between the Sun and planets, in a way that increased with nearness to the attracting body, together with a principle of linear inertia.\n\nHooke's statements up to 1674 made no mention, however, that an inverse square law applies or might apply to these attractions. Hooke's gravitation was also not yet universal, though it approached universality more closely than previous hypotheses. He also did not provide accompanying evidence or mathematical demonstration. On the latter two aspects, Hooke himself stated in 1674: \"Now what these several degrees [of attraction] are I have not yet experimentally verified\"; and as to his whole proposal: \"This I only hint at present\", \"having my self many other things in hand which I would first compleat, and therefore cannot so well attend it\" (i.e. \"prosecuting this Inquiry\"). It was later on, in writing on 6 January 1679|80 to Newton, that Hooke communicated his \"supposition ... that the Attraction always is in a duplicate proportion to the Distance from the Center Reciprocall, and Consequently that the Velocity will be in a subduplicate proportion to the Attraction and Consequently as Kepler Supposes Reciprocall to the Distance.\" (The inference about the velocity was incorrect.)\n\nHooke's correspondence with Newton during 1679–1680 not only mentioned this inverse square supposition for the decline of attraction with increasing distance, but also, in Hooke's opening letter to Newton, of 24 November 1679, an approach of \"compounding the celestial motions of the planets of a direct motion by the tangent & an attractive motion towards the central body\".\n\nNewton, faced in May 1686 with Hooke's claim on the inverse square law, denied that Hooke was to be credited as author of the idea. Among the reasons, Newton recalled that the idea had been discussed with Sir Christopher Wren previous to Hooke's 1679 letter. Newton also pointed out and acknowledged prior work of others, including Bullialdus, (who suggested, but without demonstration, that there was an attractive force from the Sun in the inverse square proportion to the distance), and Borelli (who suggested, also without demonstration, that there was a centrifugal tendency in counterbalance with a gravitational attraction towards the Sun so as to make the planets move in ellipses). D T Whiteside has described the contribution to Newton's thinking that came from Borelli's book, a copy of which was in Newton's library at his death.\n\nNewton further defended his work by saying that had he first heard of the inverse square proportion from Hooke, he would still have some rights to it in view of his demonstrations of its accuracy. Hooke, without evidence in favor of the supposition, could only guess that the inverse square law was approximately valid at great distances from the center. According to Newton, while the 'Principia' was still at pre-publication stage, there were so many a-priori reasons to doubt the accuracy of the inverse-square law (especially close to an attracting sphere) that \"without my (Newton's) Demonstrations, to which Mr Hooke is yet a stranger, it cannot believed by a judicious Philosopher to be any where accurate.\"\n\nThis remark refers among other things to Newton's finding, supported by mathematical demonstration, that if the inverse square law applies to tiny particles, then even a large spherically symmetrical mass also attracts masses external to its surface, even close up, exactly as if all its own mass were concentrated at its center. Thus Newton gave a justification, otherwise lacking, for applying the inverse square law to large spherical planetary masses as if they were tiny particles. In addition, Newton had formulated, in Propositions 43-45 of Book 1 and associated sections of Book 3, a sensitive test of the accuracy of the inverse square law, in which he showed that only where the law of force is calculated as the inverse square of the distance will the directions of orientation of the planets' orbital ellipses stay constant as they are observed to do apart from small effects attributable to inter-planetary perturbations.\n\nIn regard to evidence that still survives of the earlier history, manuscripts written by Newton in the 1660s show that Newton himself had, by 1669, arrived at proofs that in a circular case of planetary motion, \"endeavour to recede\" (what was later called centrifugal force) had an inverse-square relation with distance from the center. After his 1679-1680 correspondence with Hooke, Newton adopted the language of inward or centripetal force. According to Newton scholar J. Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The lesson offered by Hooke to Newton here, although significant, was one of perspective and did not change the analysis. This background shows there was basis for Newton to deny deriving the inverse square law from Hooke.\n\nOn the other hand, Newton did accept and acknowledge, in all editions of the \"Principia\", that Hooke (but not exclusively Hooke) had separately appreciated the inverse square law in the solar system. Newton acknowledged Wren, Hooke and Halley in this connection in the Scholium to Proposition 4 in Book 1. Newton also acknowledged to Halley that his correspondence with Hooke in 1679-80 had reawakened his dormant interest in astronomical matters, but that did not mean, according to Newton, that Hooke had told Newton anything new or original: \"yet am I not beholden to him for any light into that business but only for the diversion he gave me from my other studies to think on these things & for his dogmaticalness in writing as if he had found the motion in the Ellipsis, which inclined me to try it ...\"\n\nSince the time of Newton and Hooke, scholarly discussion has also touched on the question of whether Hooke's 1679 mention of 'compounding the motions' provided Newton with something new and valuable, even though that was not a claim actually voiced by Hooke at the time. As described above, Newton's manuscripts of the 1660s do show him actually combining tangential motion with the effects of radially directed force or endeavour, for example in his derivation of the inverse square relation for the circular case. They also show Newton clearly expressing the concept of linear inertia—for which he was indebted to Descartes' work, published in 1644 (as Hooke probably was). These matters do not appear to have been learned by Newton from Hooke.\n\nNevertheless, a number of authors have had more to say about what Newton gained from Hooke and some aspects remain controversial. The fact that most of Hooke's private papers had been destroyed or have disappeared does not help to establish the truth.\n\nNewton's role in relation to the inverse square law was not as it has sometimes been represented. He did not claim to think it up as a bare idea. What Newton did was to show how the inverse-square law of attraction had many necessary mathematical connections with observable features of the motions of bodies in the solar system; and that they were related in such a way that the observational evidence and the mathematical demonstrations, taken together, gave reason to believe that the inverse square law was not just approximately true but exactly true (to the accuracy achievable in Newton's time and for about two centuries afterwards – and with some loose ends of points that could not yet be certainly examined, where the implications of the theory had not yet been adequately identified or calculated).\n\nIn 1686, when the first book of Newton's \"Principia\" was presented to the Royal Society, Robert Hooke accused Newton of plagiarism by claiming that he had taken from him the \"notion\" of \"the rule of the decrease of Gravity, being reciprocally as the squares of the distances from the Center\". At the same time (according to Edmond Halley's contemporary report) Hooke agreed that \"the Demonstration of the Curves generated thereby\" was wholly Newton's.\n\nIn this way, the question arose as to what, if anything, Newton owed to Hooke. This is a subject extensively discussed since that time and on which some points, outlined below, continue to excite controversy.\n\nAbout thirty years after Newton's death in 1727, Alexis Clairaut, a mathematical astronomer eminent in his own right in the field of gravitational studies, wrote after reviewing what Hooke published, that \"One must not think that this idea ... of Hooke diminishes Newton's glory\"; and that \"the example of Hooke\" serves \"to show what a distance there is between a truth that is glimpsed and a truth that is demonstrated\".\n\nIn modern language, the law states the following:\n\nAssuming SI units, \"F\" is measured in newtons (N), \"m\" and \"m\" in kilograms (kg), \"r\" in meters (m), and the constant \"G\" is approximately equal to .\nThe value of the constant \"G\" was first accurately determined from the results of the Cavendish experiment conducted by the British scientist Henry Cavendish in 1798, although Cavendish did not himself calculate a numerical value for \"G\". This experiment was also the first test of Newton's theory of gravitation between masses in the laboratory. It took place 111 years after the publication of Newton's \"Principia\" and 71 years after Newton's death, so none of Newton's calculations could use the value of \"G\"; instead he could only calculate a force relative to another force.\n\nIf the bodies in question have spatial extent (as opposed to being point masses), then the gravitational force between them is calculated by summing the contributions of the notional point masses which constitute the bodies. In the limit, as the component point masses become \"infinitely small\", this entails integrating the force (in vector form, see below) over the extents of the two bodies.\n\nIn this way, it can be shown that an object with a spherically-symmetric distribution of mass exerts the same gravitational attraction on external bodies as if all the object's mass were concentrated at a point at its centre. (This is not generally true for non-spherically-symmetrical bodies.)\n\nFor points \"inside\" a spherically-symmetric distribution of matter, Newton's Shell theorem can be used to find the gravitational force. The theorem tells us how different parts of the mass distribution affect the gravitational force measured at a point located a distance r from the center of the mass distribution:\n\n\nAs a consequence, for example, within a shell of uniform thickness and density there is \"no net\" gravitational acceleration anywhere within the hollow sphere.\n\nFurthermore, inside a uniform sphere the gravity increases linearly with the distance from the center; the increase due to the additional mass is 1.5 times the decrease due to the larger distance from the center. Thus, if a spherically symmetric body has a uniform core and a uniform mantle with a density that is less than 2/3 of that of the core, then the gravity initially decreases outwardly beyond the boundary, and if the sphere is large enough, further outward the gravity increases again, and eventually it exceeds the gravity at the core/mantle boundary. The gravity of the Earth may be highest at the core/mantle boundary.\nNewton's law of universal gravitation can be written as a vector equation to account for the direction of the gravitational force as well as its magnitude. In this formula, quantities in bold represent vectors.\n\nwhere\n\nIt can be seen that the vector form of the equation is the same as the scalar form given earlier, except that F is now a vector quantity, and the right hand side is multiplied by the appropriate unit vector. Also, it can be seen that F = −F.\n\nThe gravitational field is a vector field that describes the gravitational force which would be applied on an object in any given point in space, per unit mass. It is actually equal to the gravitational acceleration at that point.\n\nIt is a generalisation of the vector form, which becomes particularly useful if more than 2 objects are involved (such as a rocket between the Earth and the Moon). For 2 objects (e.g. object 2 is a rocket, object 1 the Earth), we simply write r instead of r and \"m\" instead of \"m\" and define the gravitational field g(r) as:\n\nso that we can write:\n\nThis formulation is dependent on the objects causing the field. The field has units of acceleration; in SI, this is m/s.\n\nGravitational fields are also conservative; that is, the work done by gravity from one position to another is path-independent. This has the consequence that there exists a gravitational potential field \"V\"(r) such that\n\nIf \"m\" is a point mass or the mass of a sphere with homogeneous mass distribution, the force field g(r) outside the sphere is isotropic, i.e., depends only on the distance \"r\" from the center of the sphere. In that case\n\nthe gravitational field is on, inside and outside of symmetric masses.\n\nAs per Gauss Law, field in a symmetric body can be found by the mathematical equation:\n\nwhere formula_8 is a closed surface and formula_9 is the mass enclosed by the surface.\n\nHence, for a hollow sphere of radius formula_10 and total mass formula_11,\n\nFor a uniform solid sphere of radius formula_10 and total mass formula_11,\n\nNewton's description of gravity is sufficiently accurate for many practical purposes and is therefore widely used. Deviations from it are small when the dimensionless quantities \"φ\"/\"c\" and \"(v/c)\" are both much less than one, where \"φ\" is the gravitational potential, \"v\" is the velocity of the objects being studied, and \"c\" is the speed of light.\nFor example, Newtonian gravity provides an accurate description of the Earth/Sun system, since\n\nwhere \"r\" is the radius of the Earth's orbit around the Sun.\n\nIn situations where either dimensionless parameter is large, then\ngeneral relativity must be used to describe the system. General relativity reduces to Newtonian gravity in the limit of small potential and low velocities, so Newton's law of gravitation is often said to be the low-gravity limit of general relativity.\n\n\n\nWhile Newton was able to formulate his law of gravity in his monumental work, he was deeply uncomfortable with the notion of \"action at a distance\" that his equations implied. In 1692, in his third letter to Bentley, he wrote: \"That one body may act upon another at a distance through a vacuum without the mediation of anything else, by and through which their action and force may be conveyed from one another, is to me so great an absurdity that, I believe, no man who has in philosophic matters a competent faculty of thinking could ever fall into it.\"\n\nHe never, in his words, \"assigned the cause of this power\". In all other cases, he used the phenomenon of motion to explain the origin of various forces acting on bodies, but in the case of gravity, he was unable to experimentally identify the motion that produces the force of gravity (although he invented two mechanical hypotheses in 1675 and 1717). Moreover, he refused to even offer a hypothesis as to the cause of this force on grounds that to do so was contrary to sound science. He lamented that \"philosophers have hitherto attempted the search of nature in vain\" for the source of the gravitational force, as he was convinced \"by many reasons\" that there were \"causes hitherto unknown\" that were fundamental to all the \"phenomena of nature\". These fundamental phenomena are still under investigation and, though hypotheses abound, the definitive answer has yet to be found. And in Newton's 1713 \"General Scholium\" in the second edition of \"Principia\": \"I have not yet been able to discover the cause of these properties of gravity from phenomena and I feign no hypotheses... It is enough that gravity does really exist and acts according to the laws I have explained, and that it abundantly serves to account for all the motions of celestial bodies.\"\n\nThese objections were explained by Einstein's theory of general relativity, in which gravitation is an attribute of curved spacetime instead of being due to a force propagated between bodies. In Einstein's theory, energy and momentum distort spacetime in their vicinity, and other particles move in trajectories determined by the geometry of spacetime. This allowed a description of the motions of light and mass that was consistent with all available observations. In general relativity, the gravitational force is a fictitious force due to the curvature of spacetime, because the gravitational acceleration of a body in free fall is due to its world line being a geodesic of spacetime.\n\nNewton was the first to consider in his Principia an extended expression of his law of gravity including an inverse-cube term of the form\nattempting to explain the Moon's apsidal motion. Other extensions were proposed by Laplace (around 1790) and Decombes (1913):\n\nIn recent years, quests for non-inverse square terms in the law of gravity have been carried out by neutron interferometry.\n\nThe \"n\"-body problem is an ancient, classical problem of predicting the individual motions of a group of celestial objects interacting with each other gravitationally. Solving this problem — from the time of the Greeks and on — has been motivated by the desire to understand the motions of the Sun, planets and the visible stars. In the 20th century, understanding the dynamics of globular cluster star systems became an important \"n\"-body problem too. The \"n\"-body problem in general relativity is considerably more difficult to solve.\n\nThe classical physical problem can be informally stated as: \"given the quasi-steady orbital properties\" (\"instantaneous position, velocity and time\") \"of a group of celestial bodies, predict their interactive forces; and consequently, predict their true orbital motions for all future times\".\n\nThe two-body problem has been completely solved, as has the restricted three-body problem.\n\n\n"}
{"id": "50035369", "url": "https://en.wikipedia.org/wiki?curid=50035369", "title": "Strimvelis", "text": "Strimvelis\n\nStrimvelis is the first \"ex-vivo\" stem cell gene therapy to treat patients with a very rare disease called ADA-SCID (Severe Combined Immunodeficiency due to Adenosine Deaminase deficiency). ADA-SCID is estimated to occur in approximately 15 patients per year in Europe.\n\nThe treatment is personalized for each patient; hematopoietic stem cell (HSCs) are extracted from the patient and purified so that only CD34-expressing cells remain. Those cells are cultured with cytokines and growth factors and then transduced with a gammaretrovirus containing the human adenosine deaminase gene and then reinfused into the patient. These cells take root in the person's bone marrow, replicating and creating cells that mature and create normally functioning adenosine deaminase protein, resolving the problem. As of April 2016, the transduced cells had a shelf life of about six hours.\n\nPrior to extraction, the person is treated with granulocyte colony-stimulating factor in order to increase the number of stem cells and improve the harvest; after that but prior to reinfusion, the person is treated with busulfan or melphalan to kill as many of the person's existing HSCs to increase the chances of the new cells' survival.\n\nThe most common side effects in clinical trials were pyrexia, increased liver enzyme levels, anemia, neutropenia, hemolytic anaemia, aplastic anaemia and thrombocytopenia.\n\nThe treatment was developed at San Raffaele Telethon Institute for Gene Therapy and developed by GlaxoSmithKline (GSK) through a 2010 collaboration with Fondazione Telethon and Ospedale San Raffaele. GSK, working with the biotechnology company MolMed S.p.A, developed a manufacturing process that was previously only suitable for clinical trials into one demonstrated to be robust and suitable for commercial supply.\n\nIn April 2016, a committee at the European Medicines Agency recommended marketing approval for its use in children with adenosine deaminase deficiency, for whom no matched HSC donor is available, on the basis of a clinical trial that produced a 100% survival rate; the median follow-up time was 7 years after the treatment was administered. 75% of people who received the treatment needed no further enzyme replacement therapy. Efforts had begun 14 years before. The total number of children treated was reported as 22 and 18. Around 80% of patients have no matched donor. Strimvelis was approved by the European Commission on 27 May 2016.\n\nAs of 2016, the only site approved to manufacture the treatment was MolMed.\n\nIn 2017 GSK announced it was looking to sell off Strimvelis, and in March 2018 GSK sold Strimvelis to Orchard Therapeutics Ltd.; as of that time there had been only five sales of the product.\n\nThe condition affects about 14 people per year in Europe and 12 in the U.S.\n\nThe price for the treatment was set at €594k, 2 times the annual cost of enzyme replacement therapy injections. enzyme replacement therapy for ADA requires weekly injections and costs about $4.25 million for one patient over 10 years.\n"}
{"id": "6863181", "url": "https://en.wikipedia.org/wiki?curid=6863181", "title": "Tantrasamgraha", "text": "Tantrasamgraha\n\nTantrasamgraha, or Tantrasangraha, (literally, \"A Compilation of the System\") is an important astronomical treatise written by Nilakantha Somayaji, an astronomer/mathematician belonging to the Kerala school of astronomy and mathematics. \nThe treatise was completed in 1501 CE. It consists of 432 verses in Sanskrit divided into eight chapters. Tantrasamgraha had spawned a few commentaries: \"Tantrasamgraha-vyakhya\" of anonymous authorship and \"Yuktibhāṣā\" authored by Jyeshtadeva in about 1550 CE.\nTantrasangraha, together with its commentaries, bring forth the depths of the mathematical accomplishments the Kerala school of astronomy and mathematics, in particular the achievements of the remarkable mathematician of the school Sangamagrama Madhava. \nIn his \"Tantrasangraha\", Nilakantha revised Aryabhata's model for the planets Mercury and Venus. His equation of the centre for these planets remained the most accurate until the time of Johannes Kepler in the 17th century.\n\nIt was C.M. Whish, a civil servant of East India Company, who brought to the attention of the western scholarship the existence of Tantrasamgraha through a paper published in 1835. The other books mentioned by C.M. Whish in his paper were Yuktibhāṣā of Jyeshtadeva, Karanapaddhati of Puthumana Somayaji and Sadratnamala of Sankara Varman.\n\nNilakantha Somayaji, the author of Tantrasamgraha, was a Nambudiri belonging to the Gargya gotra and a resident of Trikkantiyur, near Tirur in central Kerala. The name of his Illam was Kelallur. He studied under Damodara, son of Paramesvara. The first and the last verses in Tantrasamgraha contain chronograms specifying the dates, in the form Kali days, of the commencement and of the completion of book. These work out to dates in 1500-01.\n\nA brief account of the contents of Tantrasamgraha is presented below. A descriptive account of the contents is available in Bharatheeya Vijnana/Sastra Dhara. Full details of the contents are available in an edition of Tantrasamgraha published in the Indian Journal of History of Science.\n\n\"A remarkable synthesis of Indian spherical astronomical knowledge occurs in a passage in Tantrasamgraha.\" \nIn astronomy, the spherical triangle formed by the zenith, the celestial north pole and the Sun is called the \"astronomical triangle\". Its sides and two of its angles are important astronomical quantities. The sides are 90° - φ where φ is the observer's terrestrial latitude, 90° - δ where δ is the Sun's declination and 90° - \"a \" where \"a\" is the Sun's altitude above the horizon. The important angles are the angle at the zenith which is the Sun's azimuth and the angle at the north pole which is the Sun's hour angle. The problem is to compute two of these elements when the other three elements are specified. There are precisely ten different possibilities and Tantrasamgraha contains discussions of all these possibilities with complete solutions one by one in \"one place\". \"The spherical triangle is handled as systematically here as in any modern textbook.\"\n\nThe terrestrial latitude of an observer's position is equal to the zenith distance of the Sun at noon on the equinctial day. The effect of solar parallax on zenith distance was known to Indian astronomers right from Aryabhata. But it was Nilakantha Somayaji who first discussed the effect of solar parallax on the observer's latitude. Tantrasamgraha gives the magnitude of this correction and also a correction due to the finite size of the Sun.\nTantrasamgraha contains a major revision of the older Indian planetary model for the interior planets Mercury and Venus and, in the history of astronomy, the first accurate formulation of the equation of centre for these planets. His planetary system was a partially heliocentric model in which Mercury, Venus, Mars, Jupiter and Saturn orbit the Sun, which in turn orbits the Earth, similar to the Tychonic system later proposed by Tycho Brahe in the late 16th century. Nilakantha's system was more accurate at predicting the heliocentric motions of the interior than the later Tychonic and Copernican models, and remained the most accurate until the 17th century when Johannes Kepler reformed the computation for the interior planets in much the same way Nilakantha did. Most astronomers of the Kerala school who followed him accepted his planetary model.\n\nA Conference to celebrate the 500th Anniversary of Tantrasangraha was organised by the Department of Theoretical Physics, University of Madras, in collaboration with the Inter-University Centre of the Indian Institute of Advanced Study, Shimla, during 11–13 March 2000, at Chennai. \nThe Conference turned out to be an important occasion for highlighting and reviewing the recent work on the achievements in Mathematics and Astronomy of the Kerala school and the new perspectives in History of Science, which are emerging from these studies. A compilation of the important papers presented at this Conference has also been published.\nThe following is a brief description of the other works by Nilakantha Somayaji.\n\n\n"}
{"id": "867515", "url": "https://en.wikipedia.org/wiki?curid=867515", "title": "Tired light", "text": "Tired light\n\nTired light is a class of hypothetical redshift mechanisms that was proposed as an alternative explanation for the redshift-distance relationship. These models have been proposed as alternatives to the models that require metric expansion of space of which the Big Bang and the Steady State cosmologies are the most famous examples. The concept was first proposed in 1929 by Fritz Zwicky, who suggested that if photons lost energy over time through collisions with other particles in a regular way, the more distant objects would appear redder than more nearby ones. Zwicky himself acknowledged that any sort of scattering of light would blur the images of distant objects more than what is seen. Additionally, the surface brightness of galaxies evolving with time, time dilation of cosmological sources, and a thermal spectrum of the cosmic microwave background have been observed — these effects should not be present if the cosmological redshift was due to any tired light scattering mechanism. Despite periodic re-examination of the concept, tired light has not been supported by observational tests and has lately been consigned to consideration only in the fringes of astrophysics.\n\nTired light was an idea that came about due to the observation made by Edwin Hubble that distant galaxies have redshifts proportional to their distance. Redshift is a shift in the spectrum of the emitted electromagnetic radiation from an object toward lower energies and frequencies, associated with the phenomenon of the Doppler effect. Observers of spiral nebulae such as Vesto Slipher observed that these objects (now known to be separate galaxies) generally exhibited redshift rather than blueshifts independent of where they were located. Since the relation holds in all directions it cannot be attributed to normal movement with respect to a background which would show an assortment of redshifts and blueshifts. Everything is moving \"away\" from the Milky Way galaxy. Hubble's contribution was to show that the magnitude of the redshift correlated strongly with the distance to the galaxies.\n\nBasing on Slipher's and Hubble's data, in 1927 Georges Lemaître realized that this correlation fit non-static solutions to the equations of Einstein's theory of gravity, the Friedmann–Lemaître solutions. However Lemaître's article was appreciated only after Hubble's publication of 1929. The universal redshift-distance relation in this solution is attributable to the effect an expanding universe has on a photon traveling on a null spacetime interval (also known as a \"light-like\" geodesic). In this formulation, there was still an analogous effect to the Doppler effect, though relative velocities need to be handled with more care since distances can be defined in different ways in expanding metrics.\n\nAt the same time, other explanations were proposed that did not concord with general relativity. Edward Milne proposed an explanation compatible with special relativity but not general relativity that there was a giant explosion that could explain redshifts (see Milne universe). Others proposed that systematic effects could explain the redshift-distance correlation. Along this line, Fritz Zwicky proposed a \"tired light\" mechanism in 1929. Zwicky suggested that photons might slowly lose energy as they travel vast distances through a static universe by interaction with matter or other photons, or by some novel physical mechanism. Since a decrease in energy corresponds to an increase in light's wavelength, this effect would produce a redshift in spectral lines that increase proportionally with the distance of the source. The term \"tired light\" was coined by Richard Tolman in the early 1930s as a way to refer to this idea.\n\nTired light mechanisms were among the proposed alternatives to the Big Bang and the Steady State cosmologies, both of which relied on the general relativistic expansion of the universe of the FRW metric. Through the middle of the twentieth century, most cosmologists supported one of these two paradigms, but there were a few scientists, especially those who were working on alternatives to general relativity, who worked with the tired light alternative. As the discipline of observational cosmology developed in the late twentieth century and the associated data became more numerous and accurate, the Big Bang emerged as the cosmological theory most supported by the observational evidence, and it remains the accepted consensus model with a current parametrization that precisely specifies the state and evolution of the universe. Although the proposals of \"tired light cosmologies\" are now more-or-less relegated to the dustbin of history, as a completely alternative proposal tired-light cosmologies were considered a remote possibility worthy of some consideration in cosmology texts well into the 1980s, though it was dismissed as an unlikely and \"ad hoc\" proposal by mainstream astrophysicists.\nBy the 1990s and on into the twenty-first century, a number of falsifying observations have shown that \"tired light\" hypotheses are not viable explanations for cosmological redshifts. For example, in a static universe with tired light mechanisms, the surface brightness of stars and galaxies should be constant, that is, the farther an object is, the less light we receive, but its apparent area diminishes as well, so the light received divided by the apparent area should be constant. In an expanding universe, the surface brightness diminishes with distance. As the observed object recedes, photons are emitted at a reduced rate because each photon has to travel a distance that is a little longer than the previous one, while its energy is reduced a little because of increasing redshift at a larger distance. On the other hand, in an expanding universe, the object appears to be larger than it really is, because it was closer to us when the photons started their travel. This causes a difference in surface brilliance of objects between a static and an expanding Universe. This is known as the Tolman surface brightness test that in those studies favors the expanding universe hypothesis and rules out static tired light models.\n\nRedshift is directly observable and used by cosmologists as a direct measure of lookback time. They often refer to age and distance to objects in terms of redshift rather than years or light-years. In such a scale, the Big Bang corresponds to a redshift of infinity. Alternative theories of gravity that do not have an expanding universe in them need an alternative to explain the correspondence between redshift and distance that is \"sui generis\" to the expanding metrics of general relativity. Such theories are sometimes referred to as \"tired-light cosmologies\", though not all authors are necessarily aware of the historical antecedents.\n\nIn general, any \"tired light\" mechanism must solve some basic problems, in that the observed redshift must:\n\nA number of tired light mechanisms have been suggested over the years. Fritz Zwicky, in his paper proposing these models investigated a number of redshift explanations, ruling out some himself. The simplest form of a tired light theory assumes an exponential decrease in photon energy with distance traveled:\n\nwhere formula_2 is the energy of the photon at distance formula_3 from the source of light, formula_4 is the energy of the photon at the source of light, and formula_5 is a large constant characterizing the \"resistance of the space\". To correspond to Hubble's law, the constant formula_5 must be several gigaparsecs. For example, Zwicky considered whether an integrated Compton effect could account for the scale normalization of the above model:\n\nThis expected \"blurring\" of cosmologically distant objects is not seen in the observational evidence, though it would take much larger telescopes than those available at that time to show this with certainty. Alternatively, Zwicky proposed a kind of Sachs–Wolfe effect explanation for the redshift distance relation:\n\nZwicky's proposals were carefully presented as falsifiable according to later observations:\n\nSuch broadening of absorption lines is not seen in high-redshift objects, thus falsifying this particular hypothesis.\n\nZwicky also notes, in the same paper, that according to a tired light model a distance-redshift relationship would necessarily be present in the light from sources within our own galaxy (even if the redshift would be so small that it would be hard to measure), that do not appear under a recessional-velocity based theory. He writes, referring to sources of light within our galaxy: \"It is especially desirable to determine the redshift independent of the proper velocities of the objects observed\". Subsequent to this, astronomers have patiently mapped out the three-dimensional velocity-position phase space for the galaxy and found the redshifts and blueshifts of galactic objects to accord well with the statistical distribution of a spiral galaxy, eliminating the intrinsic redshift component as an effect.\n\nFollowing after Zwicky in 1935, Edwin Hubble and Richard Tolman compared recessional redshift with a non-recessional one, writing that they:\n\nIn the early 1950s, Erwin Finlay-Freundlich proposed a redshift as \"the result of loss of energy by observed photons traversing a radiation field.\" which was cited and argued for as an explanation for the redshift-distance relation in a 1962 astrophysics theory \"Nature\" paper by University of Manchester physics professor P. F. Browne. The pre-eminent cosmologist Ralph Asher Alpher wrote a letter to \"Nature\" three months later in response to this suggestion heavily criticizing the approach, \"No generally accepted physical mechanism has been proposed for this loss.\" Still, until the so-called \"Age of Precision Cosmology\" was ushered in with results from the WMAP space probe and modern redshift surveys, tired light models could occasionally get published in the mainstream journals, including one that was published in the February 1979 edition of \"Nature\" proposing \"photon decay\" in a curved spacetime that was five months later criticized in the same journal as being wholly inconsistent with observations of the gravitational redshift observed in the solar limb. In 1986 a paper claiming tired light theories explained redshift better than cosmic expansion was published in the \"Astrophysical Journal\", but ten months later, in the same journal, such tired light models were shown to be inconsistent with extant observations. As cosmological measurements became more precise and the statistics in cosmological data sets improved, tired light proposals ended up being falsified, to the extent that the theory was described in 2001 by science writer Charles Seife as being \"firmly on the fringe of physics 30 years ago\".\n\n"}
{"id": "1681194", "url": "https://en.wikipedia.org/wiki?curid=1681194", "title": "Treatise on Invertebrate Paleontology", "text": "Treatise on Invertebrate Paleontology\n\nThe Treatise on Invertebrate Paleontology (or TIP) published by the Geological Society of America and the University of Kansas Press, is a definitive multi-authored work of some 50 volumes, written by more than 300 paleontologists, and covering every phylum, class, order, family, and genus of fossil and extant (still living) invertebrate animals. The prehistoric invertebrates are described as to their taxonomy, morphology, paleoecology, stratigraphic and paleogeographic range. However, genera with no fossil record whatsoever have just a very brief listing.\n\nPublication of the decades-long \"Treatise on Invertebrate Paleontology\" is a work-in-progress; and therefore it is not yet complete: For example, there is no volume yet published regarding the post-Paleozoic era caenogastropods (a molluscan group including the whelk and periwinkle). Furthermore, every so often, previously published volumes of the \"Treatise\" are revised.\n\nRaymond C. Moore, the project's founder and first editor, originally envisioned this \"Treatise\" in invertebrate paleontology as comprising just three large volumes, and totaling only three thousand pages.\n\nThe project began with work on a few, mostly slim volumes in which a single senior specialist in a distinct field of invertebrate paleozoology would summarize one particular group. As a result, each publication became a comprehensive compilation of \"everything known\" at that time for each group. Examples of this stage of the project are \"Part G. Bryozoa\", by Ray S. Bassler (the first volume, published in 1953), and \"Part P. Arthropoda Part 2, the Chelicerata\" by Alexander Petrunkevitch (1955/1956).\n\nAround 1959 or 1960, as more and larger invertebrate groups were being addressed, the incompleteness of the then-current state of affairs became apparent. So several senior editors of the \"Treatise\" started major research programs to fill in the evident gaps. Consequently, the succeeding volumes, while still maintaining the original format, began to change from being a set of single-authored compilations into being major research projects in their own right. Newer volumes had a committee and a chief editor for each volume, with yet other authors and researchers assigned particular sections. Museum collections that had not been previously described were studied; and sometimes new major taxonomic families—and even orders—had to be described. More attention was given to transitional fossils and evolutionary radiation—eventually producing a much-more complete encyclopedia of invertebrate paleontology.\n\nBut even in the second set of volumes, the various taxa were still described and organized in a classical Linnaean sense. The more-recent volumes began to introduce phylogenetic and cladistic ideas, along with new developments and discoveries in fields such as biogeography, molecular phylogeny, paleobiology, and organic chemistry, so that the current edition of \"Brachiopoda\" (1997 to 2002) is classified according to a cladistic arrangement, with three subphyla and a large number of classes replacing the original two classes of Articulata and Inarticulata.\n\nAll these discoveries led to revisions and additional volumes. Even those taxa already covered were expanded: Books such as those regarding the \"Cnidaria\" (vol. F), the \"Brachiopoda\" (vol. H) and the \"Trilobita\" (vol. O) each went from one modest publication to three large volumes. And yet another volume regarding the brachiopods (number five) was published in 2006.\n\nUntil 2007, the editor of the \"Treatise\" was Roger L. Kaesler at The Paleontological Institute at the University of Kansas in Lawrence, Kansas.\n\nFrom the beginning, the character of the \"Treatise\" volumes has followed and further developed the pattern of the classic \"Invertebrate Paleontology\" written by Moore, Lalicker and Fischer (1953).\n\nFollowing their lead, the \"Treatise\" includes in a typical article (a) a description of the basic anatomy of the modern members of each invertebrate group, (b) distinctive features of the fossils, (c) a comprehensive illustrated glossary of terms, (d) a short discussion of the evolutionary history of the group, (e) a stratigraphic range chart, done at the level of the major subdivision (lower, middle and upper) of each Geologic period.\n\nThis is followed by (f) a listing and technical description of every known genus, along with (g) geographic distribution (usually by continent only, but occasionally by country) and (h) stratigraphic range.\n\nNext come (i) one or two representative species illustrated by line drawings (in the early volumes) or by black-and-white photographs (in subsequent volumes), each accompanied by an appropriate reference for that genus. Furthermore, each \"Treatise\" article includes (j) the date, authorship, and scientific history of the taxa.\n\nFinally, there is (k) a comprehensive bibliography and list of references. Not only that, but the more recent volumes and revisions also include (l) new fossil and phylogenetic discoveries, (m) advances in numerical and cladistic methods, (n) analysis of the group's genome, (o) its molecular phylogeny, and so on.\n\nThe following is an annotated list of the volumes already published (1953 to 2007) or volumes currently being prepared:\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "25962276", "url": "https://en.wikipedia.org/wiki?curid=25962276", "title": "Uncertain inference", "text": "Uncertain inference\n\nUncertain inference was first described by C. J. van Rijsbergen as a way to formally define a query and document relationship in Information retrieval. This formalization is a logical implication with an attached measure of uncertainty.\n\nRijsbergen proposes that the measure of uncertainty of a document \"d\" to a query \"q\" be the probability of its logical implication, i.e.:\n\nA user's query can be interpreted as a set of assertions about the desired document. It is the system's task to infer, given a particular document, if the query assertions are true. If they are, the document is retrieved.\nIn many cases the contents of documents are not sufficient to assert the queries. A knowledge base of facts and rules is needed, but some of them may be uncertain because there may be a probability associated to using them for inference. Therefore, we can also refer to this as \"plausible inference\". The plausibility of an inference formula_2 is a function of the plausibility of each query assertion. Rather than retrieving a document that exactly matches the query we should rank the documents based on their plausibility in regards to that query.\nSince \"d\" and \"q\" are both generated by users, they are error prone; thus formula_2 is uncertain. This will affect the plausibility of a given query.\n\nBy doing this it accomplishes two things:\n\nMultimedia documents, like images or videos, have different inference properties for each datatype. They are also different from text document properties. The framework of plausible inference allows us to measure and combine the probabilities coming from these different properties.\n\nUncertain inference generalizes the notions of autoepistemic logic, where truth values are either known or unknown, and when known, they are true or false.\n\nIf we have a query of the form:\n\nwhere A, B and C are query assertions, then for a document D we want the probability:\n\nIf we transform this into the conditional probability formula_6 and if the query assertions are independent we can calculate the overall probability of the implication as the product of the individual assertions probabilities.\n\nCroft and Krovetz applied uncertain inference to an information retrieval system for office documents they called \"OFFICER\". In office documents the independence assumption is valid since the query will focus on their individual attributes. Besides analysing the content of documents one can also query about the author, size, topic or collection for example. They devised methods to compare document and query attributes, infer their plausibility and combine it into an overall rating for each document. Besides that uncertainty of document and query contents also had to be addressed.\n\nProbabilistic logic networks is a system for performing uncertain inference; crisp true/false truth values are replaced not only by a probability, but also by a confidence level, indicating the certitude of the probability.\n\nMarkov logic networks allow uncertain inference to be performed; uncertainties are computed using the maximum entropy principle, in analogy to the way that Markov chains describe the uncertainty of finite state machines.\n\n"}
{"id": "5273329", "url": "https://en.wikipedia.org/wiki?curid=5273329", "title": "Utinahica", "text": "Utinahica\n\nThe Utinahica were a Timucua tribe and chiefdom in the 17th century. They lived in what is now the southeastern part of the U.S. state of Georgia. Their descendants may include the Creek Indians. \n\nA Spanish mission, Santa Isabel de Utinahica, was established in the chief town of the Utinahica on the Altamaha River, near the present site of Jacksonville, Georgia, in the first half of the 17th century. \n\n"}
{"id": "29900112", "url": "https://en.wikipedia.org/wiki?curid=29900112", "title": "Wau Holland Foundation", "text": "Wau Holland Foundation\n\nThe Wau Holland Foundation (German: Wau Holland Stiftung; WHS) is a nonprofit foundation based in Hamburg, Germany.\n\nIt was established in 2003 in memory of Wau Holland, co-founder of the Chaos Computer Club. Loosely connected with the Chaos Computer Club, the foundation aims to preserve and further Holland's ideas in fields such as technology assessment, the history of technology and freedom of information. Specifically, it promotes the use of electronic media for educational purposes, as well as events on the social aspects of new technologies.\n\nFoundation projects include the \"Archive of Contemporary History of Technology (Hacker archive)\", which documents the history of the hacker scene, and a campaign against voting machines (both in collaboration with the Chaos Computer Club). The foundation also processes donations in Europe to support the WikiLeaks organization.\n\nAs of December 2010, their endowment was about 62,000 €. It also owns land (valued at about 1500 €), currently leased to a public institution.\n\nThe foundation has collected over $1.2 million USD for WikiLeaks since it began accepting donations on the organization's behalf in October 2009. On 4 December 2010, PayPal turned off donations in response to the foundation's connection to WikiLeaks, alleging that the account was being used for \"activities that encourage, promote, facilitate or instruct others to engage in illegal activity.\" On 8 December 2010 the foundation released a press statement, saying it has filed legal action against PayPal for blocking its account and for libel due to PayPal's allegations of \"illegal activity.\"\n\nAs a consequence of this activity of collecting donations for Wikileaks, its charitable status has been challenged and later revoked by the German authorities. Its charitable status has been reinstated on 12 December 2012, applied retroactively for 2011 and 2012.\n\n"}
{"id": "31541848", "url": "https://en.wikipedia.org/wiki?curid=31541848", "title": "Willard Lamb Velie", "text": "Willard Lamb Velie\n\nWillard Lamb Velie (1866 – October 24, 1928) was a businessman based in Moline, Illinois. He was an executive at Deere & Company before starting his own companies, which grew to become Velie Motor Company. He developed advanced engines for automobiles and airplanes.\n\nW. L. Velie was born in Moline, Illinois. He was the third of five children born to Stephen H. Velie and Emma Deere, the daughter of John Deere. Stephen Velie had moved to Rock Island, Illinois to work for the C. C. Webber & Company. In 1863 he entered into a partnership with his father-in-law, and when the company was incorporated he was elected to the offices of secretary and treasurer. W. L. Velie had two older brothers, a younger brother who died as an infant and a sister. He graduated from Phillips Academy in Andover, Massachusetts in 1885 and Yale University in 1888. After graduation he set out for Montana.\n\nIn 1890 Velie returned to Moline and began working at Deere & Company as a clerk. After a year he became a sales manager. When his father died in 1895 W.L. replaced him as the corporate secretary and a member of the board. In 1902 Velie founded his first company, the Velie Carriage Company of Moline. It manufactured buggies, carriages, surreys, driving wagons, and spring wagons called the \"Wrought Iron Line\" of vehicles. In 1907 alone the company made 21,000 buggies and surreys. When his cousin William Butterworth became president of Deere & Company in 1908, Velie was elected vice-president. Three years later he became the first chairman of the executive committee when it was formed. At the same time he founded the Velie Engineering Company, which produced gas, steam and electric motors and engines, plus automobile accessories and motor trucks. In 1916 he merged his two companies and began making tractors. Their first was the Velie Biltwel 12-24, a four-cylinder tractor powered by a Velie-built engine. While this appeared to be a conflict of interest Velie was a strong supporter of Deere & Company’s acquisition or development of a tractor. For the most part, Velie’s product lines and Deere’s lines remained separate. He and Butterworth were in disagreement, however, about the company’s operation and so Velie resigned his executive committee positions in 1918 and severed all ties with the company in 1921.\n\nWhile still at Deere & Company, W.L Velie incorporated the Velie Motor Vehicle Company in 1908. While he kept his tractor lines separate from the Deere line, his automobiles were marketed through Deere’s branch houses. When he merged his companies in 1916 he formed the Velie Motor Company. The products produced by the company came to be known for their quality at a reasonable price. By 1912 the company’s stock was valued at $1.5 million and four years later it was worth $2 million. Between 1913 and 1915 the Velie factory was turning out an average of 30 cars a day. By 1920 the buggy business was phased out and automobile production peaked at 9,000. Estimates of how many motor vehicles Velie produced in the two decades the factory was in operation range between 75,000 to 300,000. Velie was not the only person in the Tri-Cities, now known as the Quad Cities, who was building automobiles. At one point in the early 20th century there were ten different makes being produced in the area. Velie was the most successful. The U.S. Navy chose the Velie engine along with seven others as the best automobile motors for adaptation to military use. Other winners included the Brewster, Duesenberg, Fiat, Hispania Suiza, Issota Franchina, Marmon and Packard. Between 1916 and 1920 the company also produced the Biltwel 12-25\" tractor, which was powered by a Velie engine.\nWillard Velie, Jr. was named the company’s vice-president in 1927. He was the one who introduced the production of airplanes into the family business. They developed the first six-cylinder valve-in-head airplane motor in 1919. They also developed a five-cylinder radial aircraft that powered their Velie Monocoupe. In 1926 Don Luscombe of Davenport, Iowa and Frank Wallace of Bettendorf, Iowa formed the Central States Aero Company and began building monocoupe airplanes. The Velie’s started working with the company and then bought it in 1928. The result was Mono Aircraft, Inc., a subsidiary of Velie Motor Corporation.\n\nThe United States Department of Commerce awarded the Velie Monocoupe its highest rating, and plans were developed to build a four seat monocoach. The plane would never be built. W.L. Velie, Sr. died in October 1928. A month later Willard Jr. stopped the production of automobiles and sold the company's interests to an Indianapolis firm. Four months later Willard, Jr. died and the airplane interests were sold to a St. Louis, Missouri firm. The Velie Motor Corporation came to an end.\n\nW. L. Velie married Annie Flowerree in Helena, Montana on May 21, 1890. She was the daughter of Daniel Augustus Greene Flowerree, who was a millionaire cattle rancher. She was also a sister to his college roommate William Flowerree. The Velie’s raised two children Willard Jr. and Marjorie.\n\nVelie built a palatial home, named Villa Velie, for his family on a site on the south side of Moline, overlooking the Rock River Valley. He and his wife were inspired by the many Italian Villas they saw while traveling through Europe. The house was built from 1912-1913 in the Italian Villa style by artisans from Italy and Greece. It contained 46 rooms, of which 14 were bedrooms and 12 bathrooms. The floors on the ground floor and the first floor were covered in marble. Built-in bookcases lined the library walls, which were also frescoed. The slopes toward the river were planted with 21 grape varieties from southern France, and the family produced their own private label wine. The grounds also featured a conservatory that housed banana trees, a putting green, sculptured gardens and a ski run that extended to the river. The family moved to a more modest dwelling during the Great Depression. For a time the house sat empty. In 1941 the home was bought by Stanley Wiedner and it became the Plantation Restaurant. It was succeeded by a restaurant called W.L. Velie’s in 1982. Today the home is the home of QCR Holdings and a branch location of Quad City Bank and Trust.\n\nThe Velie’s spent their winters in Fort Myers, Florida. They built a home there next to Annie’s parents along the Caloosahatchee River. \nW.L. Velie died of an embolism complicated by a heart problem on October 10, 1928. He was buried in Riverside Cemetery in Moline.\n\n"}
