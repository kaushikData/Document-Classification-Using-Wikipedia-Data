{"id": "23035356", "url": "https://en.wikipedia.org/wiki?curid=23035356", "title": "Academic job market", "text": "Academic job market\n\nAcademic job market refers to the pool of vacant teaching and administrative positions in Academia, i.e. in institutions of Higher Education such as universities and colleges, and also to the competition for these positions, and the mechanisms for advertising and filling them. This job market differs somewhat from other job markets because of such institutions as tenure. It is frequently a subject of debate relating to questions of openness, discrimination and reverse discrimination, and political interference.\n\nThe Academic job market, like the structure of academic careers, operates somewhat differently in different countries.\n"}
{"id": "26710029", "url": "https://en.wikipedia.org/wiki?curid=26710029", "title": "Agricultural Science and Technology Indicators", "text": "Agricultural Science and Technology Indicators\n\nThe Agricultural Science and Technology Indicators (ASTI) is a comprehensive source of information on agricultural research and development (R&D) statistics.\n\nASTI compiles, analyzes, and publicizes data on institutional developments, investments, and capacity trends in agricultural R&D in low- and middle-income countries worldwide. ASTI has published a broad set of country briefs and regional synthesis reports that describe general human and financial capacity trends in agricultural R&D at national, regional, and global levels. \n\nASTI comprises a network of national, regional, and international agricultural R&D agencies and is hosted and facilitated by the International Food Policy Research Institute (IFPRI). ASTI is currently funded by the Bill & Melinda Gates Foundation.\n\nGreater investment in agricultural research could make a significant contribution to increasing agricultural production to the levels required to feed the world’s growing population. Furthermore, additional investments in agricultural research are required to address emerging challenges, such as increasing weather variability, adaptation to climate change, water scarcity, and increased price volatility in global markets. Despite this growing attention to the agricultural sector and the role of agricultural research, many low- and middle-income countries continue to struggle with serious and deepening capacity and funding constraints in their agricultural research and higher education systems. \n\nQuantitative information is fundamental to understanding the contribution of agricultural science and technology (S&T) to agricultural growth. Indicators derived from such information allow the performance, inputs, and outcomes of agricultural S&T systems to be measured, monitored, and benchmarked. These indicators assist S&T stakeholders in formulating policy, setting priorities, and undertaking strategic planning, monitoring, and evaluation. They also provide information to governments, policy research institutes, universities, and private-sector organizations involved in public debate on the state of agricultural S&T at national, regional, and international levels.\n\nASTI’s recent work has primarily focused on the following activities:\n\nIn 2009 ASTI launched a web application that allows its users to display different ASTI indicators by country and plot two indicators against each other.\n\nASTI’s current indicators include\n\nPublications include regional and global analyses of agricultural R&D investments, and country briefs and fact sheets presenting national data.\n\nASTI’s methodology is unique in that it combines first-hand data from a wide range of agricultural R&D agencies in low- and middle-income countries with relevant, secondary data on high-income countries for comparative purposes. \n\nASTI datasets are collected and processed using internationally accepted definitions and statistical procedures developed by the Organisation for Economic Co-operation (OECD) and the United Nations Educational, Science, and Cultural Organization (UNESCO). ASTI relies on its in-country partners to identify all agencies involved in agricultural R&D, to disseminate ASTI survey forms to them, and to coordinate the necessary follow-up.\n\nIn 2000, the world spent 39.5 billion dollars (in 2005 PPP prices, that is in inflation-adjusted terms) on agricultural R&D. The private sector was estimated to account for 41 percent of this total, the vast majority of which was performed in industrialized countries (96 percent). In contrast, only 6 percent of total investments in the developing world were derived from private firms. \nIn 2000, global public agricultural research investments totaled $23 billion in 2005 PPP dollars. This sum represents a 47 percent increase over the 1981 total of $16 billion. Although spending by the high-income countries as a whole continued to grow in absolute terms, their share of global spending decreased from 62 to 57 percent over the 1981-2000 period. In contrast, the share of spending by low-income countries increased from 9 to 11 percent and the share of middle-income countries increased from 29 to 32 percent over the same timeframe. Of the 2000 global total, the developing countries in the Asia-Pacific region combined invested $4.8 billion in 2000, compared to $2.7 billion for Latin America and the Caribbean, $1.2 billion for West Asia and North Africa, and $1.2 billion for Sub-Saharan Africa. Agricultural R&D spending for China and the Asia-Pacific region as a whole has grown considerably since 2000. After a period of declining investments in public agricultural R&D, the Latin America and the Caribbean region also experienced an increase in total agricultural R&D spending in 2006, comparable to the mid-1990s level.\n\nThe government sector is still providing most of the funding to agricultural research in the developing world, but funding sources can differ tremendously at the country level. Donor funding still plays an important role in most Sub-Saharan African countries and a handful of countries in Asia. In 2000/01 the main government agricultural research agencies in 23 Sub-Saharan African countries for which data were available obtained 35 percent of their funding through donor loans and contributions, which was considerably higher than the corresponding shares in the other regions. Funding generated through internally generated funds, including contractual arrangements with private and public enterprises, as well as funding by producer organizations have gained prominence in recent years across the developing world.\n\nThe institutional composition of agricultural R&D has become increasingly diversified over the past few decades. Although the government sector continues to dominate the execution of public agricultural research, the higher-education sector has gained prominence in a large number of developing countries. Despite the increasing share of the higher-education sector as a whole, the individual capacity of many individual higher-education agencies remains small. Nonprofit organizations such as producer organizations, marketing boards, foundations, and nongovernmental organizations (NGOs) are increasingly relevant elements of national and global agricultural research. Although in absolute numbers—total FTE researchers more than doubled in LAC and SSA during their respective periods—they continue to account for a small share of public agricultural research.\n\nBeintema, N.M. and G. J. Stads. 2011. African Agricultural R&D in the New Millennium: Progress for Some, Challenges for Many. IFPRI Food Policy Report. Washington, DC: International Food Policy Research Institute. (PDF-File, 1.1 MB - Accessed on May 20, 2011)\n\nBeintema, N.M. and Stads, G.J. 2008. Measuring Agricultural Research Investments: A Revised Global Picture. ASTI Background Note. Washington, D.C.: IFPRI. (PDF-File, 657K - Accessed on March 18, 2010)\n\nBeintema, N.M.and Di Marcantonio, F. 2009. Women's Participation in Agricultural Research and Higher Education: Key Trends in Sub-Saharan Africa. Washington, D.C.: IFPRI and Nairobi: G&D program. (PDF-File 334K - Accessed on March 18, 2010)\n\nStads, G.J. and Beintema, N.M. 2009. Public Agricultural Research in Latin America And The Caribbean: Investment and Capacity Trends. ASTI Synthesis Report. Washington, D.C.: International Food Policy Research Institute and Inter-American Development Bank. (PDF-File, 1.8 MB - Accessed on March 18, 2010)\n\nBeintema, N.M. and Stads, G.J. 2008. Diversity in Agricultural Research Resources in the Asia-Pacific Region. Synthesis. Washington, D.C.: IFPRI and Bangkok, Thailand: APAARI. (PDF-File, 837K - Accessed on March 18, 2010)\n\nBeintema, N.M. and Stads, G.J. 2008. Agricultural R&D Capacity and Investments in the Asia–Pacific Region. Research Brief No. 11. Washington, D.C.: IFPRI. (PDF-File, 211K - Accessed on March 18, 2010)\n\nThe ASTI website offers a wide set of country briefs, reports, and notes as well as datasets: (Accessed on March 18, 2010)\n\nASTI Data tool - (Accessed on March 18, 2010)\n"}
{"id": "19694222", "url": "https://en.wikipedia.org/wiki?curid=19694222", "title": "Akiyoshi Kitaoka", "text": "Akiyoshi Kitaoka\n\nIn 1984, he received a BSc from the Department of Biology, University of Tsukuba, Tsukuba, Japan, where he studied animal psychology (burrowing behavior in rats) and (at the Tokyo Metropolitan Institute for Neuroscience) neuronal activity of the inferotemporal cortex in macaque monkeys.\n\nAfter his 1991 PhD from the Institute of Psychology, University of Tsukuba, he specialized in visual perception and visual illusions of geometrical shape, brightness, color, in motion illusions and other visual phenomena like Gestalt completion and perceptual transparency, based on a modern conception of Gestalt Psychology.\nHe became renowned through his \"Rotating Snakes\" peripheral drift illusion (see below).\n\nIn 2006, he received the Gold Prize of the 9th L'ORÉAL Art and Science of Color contest.\n\nIn 2007, he received the Award for Original Studies from the Japanese Society of Cognitive Psychology.\n\nIn 2008, his designs were the inspiration for the critically acclaimed indie rock band Animal Collective's album \"Merriweather Post Pavilion\", which featured a leaf covered optical illusion.\n\n"}
{"id": "21121933", "url": "https://en.wikipedia.org/wiki?curid=21121933", "title": "Angle-resolved low-coherence interferometry", "text": "Angle-resolved low-coherence interferometry\n\nAngle-resolved low-coherence interferometry (a/LCI) is an emerging biomedical imaging technology which uses the properties of scattered light to measure the average size of cell structures, including cell nuclei. The technology shows promise as a clinical tool for \"in situ\" detection of dysplastic, or precancerous tissue.\n\nA/LCI combines low-coherence interferometry with angle-resolved scattering to solve the inverse problem of determining scatterer geometry based on far field diffraction patterns. Similar to optical coherence domain reflectometry (OCDR) and optical coherence tomography (OCT), a/LCI uses a broadband light source in an interferometry scheme in order to achieve optical sectioning with a depth resolution set by the coherence length of the source. Angle-resolved scattering measurements capture light as a function of the scattering angle, and invert the angles to deduce the average size of the scattering objects via a computational light scattering model such as Mie theory, which predicts angles based on the size of the scattering sphere. Combining these techniques allows construction of a system that can measure average scatter size at various depths within a tissue sample.\n\nAt present the most significant medical application of the technology is determining the state of tissue health based on measurements of average cell nuclei size. It has been found that as tissue changes from normal to cancerous, the average cell nuclei size increases. Several recent studies have shown that via cell nuclei measurements, a/LCI can detect the presence of low- and high-grade dysplasia with 91% sensitivity and distinguish between normal and dysplastic with 97% specificity.\n\nSince 2000, light scattering systems have been used for biomedical applications such as the study of cellular morphology as well as the diagnosis of dysplasia. Variations in scattering distributions as a function of angle or wavelength have been used to deduce information regarding the size of cells and subcellular objects such as nuclei and organelles. These size measurements can then be used diagnostically to detect tissue changes—including neoplastic changes (those leading to cancer).\n\nLight scattering spectroscopy has been used to detect dysplasia in the colon, bladder, cervix, and esophagus of human patients. Light scattering has also been used to detect Barrett’s esophagus, a metaplastic condition with a high probability of leading to dysplasia.\n\nHowever, in contrast with a/LCI, these techniques all rely on total intensity based measurements, which lack the ability to provide results as a function of depth in the tissue.\n\nThe first implementation of a/LCI used a Michelson interferometer, the same model used in the famous Michelson-Morley experiment. The Michelson interferometer splits one beam of light into two paths, one reference path and one sampling path, and recombines them again to produce a waveform resulting from interference. The difference between the reference beam and the sampling beam thus reveal the properties of the sample in the way it scatters light.\n\nThe early a/LCI device used a movable mirror and lens in the reference arm so that researchers could replicate different angles and depths in the reference beam as they occurred in the collected backscattered light. This allowed isolation of the backscattered light at varying depths of reflection in the sample. \nIn order to transform the data into measurements of cell structure, angular scattering distributions are then compared to the predictions of Mie theory—which calculates the size of spheres relative to their light scattering patterns.\n\nThe a/LCI technique was first validated in studies of polystyrene microspheres, the sizes of which were known and relatively homogeneous. A later study expanded the signal processing method to compensate for the nonspherical and inhomogeneous nature of cell nuclei.\n\nThis early system required up to 40 minutes to acquire the data for a 1 mm² point in a sample, but proved the feasibility of the idea.\n\nLike OCT, the early implementations of a/LCI relied on physically changing the optical path length (OPL) to control the depth in the sample from which data are acquired. However, it has been demonstrated that it is possible to use a Fourier domain implementation to yield depth resolution in a single data acquisition. A broadband light source is used to produce a spectrum of wavelengths at once, and the backscattered light is collected by a coherent optical fiber in the return path to capture different scattering angles simultaneously. Intensity is then measured via a spectrometer: a single frame from the spectrometer contains scattering intensity as a function of wavelength and angle. Finally the data is Fourier transformed on a line-by-line basis to generate scattering intensity as a function of OPL and angle. In the resulting image, the x axis represents the OPL and the y axis the angle of reflection, thus yielding a 2D map of reflection intensities.\n\nUsing this method, the acquisition speed is limited only by the integration time of the spectrometer and can be as short at 20 ms. The same data that initially required tens of minutes to acquire can be acquired ~10 times faster.\n\nThe Fourier-domain version of the a/LCI system uses a superluminescent diode (SLD) with a fiber-coupled output as the light source. A fiber splitter separates the signal path at 90% intensity and the reference path at 10%.\n\nThe light from the SLD passes through an optical isolator and subsequently a polarization controller. It has been shown that control of light polarization is important for maximizing optical signal and comparing angular scattering with the Mie scattering model. A polarization-maintaining fiber is used to carry the illumination light to the sample. A second polarization controller is similarly used to control the polarization of the light passing through the reference path.\n\nThe output of the fiber on the right is collimated using lens L1 and illuminates the tissue. But because the delivery fiber is offset from the optical axis of the lens, the beam is delivered to the sample at an oblique angle. Backscattered light is then collimated by the same lens and collected by the fiber bundle. The fibers are one focal length from the lens, and the sample is one focal length on the other side. This configuration captures light from the maximum range of angles and minimizes light noise due to specular reflections.\n\nAt the distal end of the fiber bundle, light from each fiber is imaged onto the spectrometer. Light from the sample and reference arms are mixed by a beamsplitting cube (BS), and are incident on the entrance slit of an imaging spectrometer. Data from the imaging spectrometer are transferred to a computer via universal serial bus interface for signal processing and display of results. The computer also provides control of the imaging spectrometer.\n\nThe a/LCI system has recently been enhanced to allow operation in a clinical setting with the addition of a handheld wand. By carefully controlling the polarization in the delivery fiber, using polarization-maintaining fibers and inline polarizers, the new system allows manipulation of the handheld wand without signal degradation due to birefringence effects. In addition, the new system employed an anti-reflection coated ball lens in the probe tip, which reduces reflections that otherwise limit the depth range of the system.\n\nThe portable system uses a 2 ft by 2 ft optical breadboard as the base, with the source, fiber optic components, lens, beamsplitter, and imaging spectrometer mounted to the breadboard. An aluminum cover protects the optics. A fiber probe with a handheld probe enables easy access to tissue samples for testing. On the left side sits a white sample platform, where tissue is placed for testing. The handheld probe is used by the operator to select specific sites on the tissue from which a/LCI readings are acquired.\n\n"}
{"id": "45284504", "url": "https://en.wikipedia.org/wiki?curid=45284504", "title": "Baldet (Martian crater)", "text": "Baldet (Martian crater)\n\nBaldet Crater is an impact crater in the Syrtis Major quadrangle of Mars, located at 23.0°N latitude and 294.6°W longitude. It is 180.0  km in diameter and was named after Fernand Baldet, and the name was approved in 1973 by the International Astronomical Union (IAU) Working Group for Planetary System Nomenclature (WGPSN).\n"}
{"id": "64221", "url": "https://en.wikipedia.org/wiki?curid=64221", "title": "Biorhythm", "text": "Biorhythm\n\nA biorhythm (from Greek βίος - \"bios\", \"life\" and ῥυθμός - \"rhuthmos\", \"any regular recurring motion, rhythm\") is an attempt to predict various aspects of a person's life through simple mathematical cycles. The theory was developed by Wilhelm Fliess in the late 19th century, and was popularized in the United States in late 1970s. Most scientists believe that the idea has no more predictive power than chance. \"The theory of biorhythms is a theory that claims our daily lives are significantly affected by rhythmic cycles.\"\n\nAccording to the theory of biorhythms, a person's life is influenced by rhythmic biological cycles that affect his or her ability in various domains, such as mental, physical and emotional activity. These cycles begin at birth and oscillate in a steady (sine wave) fashion throughout life, and by modeling them mathematically, it is suggested that a person's level of ability in each of these domains can be predicted from day to day. The theory is built on the idea that the biofeedback chemical and hormonal secretion functions within the body could show a sinusoidal behavior over time.\n\nMost biorhythm models use three cycles: a 23-day physical cycle, a 28-day emotional cycle, and a 33-day intellectual cycle. Although the 28-day cycle is the same length as the average woman's menstrual cycle and was originally described as a \"female\" cycle (see below), the two are not necessarily in synchronization. Each of these cycles varies between high and low extremes sinusoidally, with days where the cycle crosses the zero line described as \"critical days\" of greater risk or uncertainty.\n\nThe numbers from +100% (maximum) to -100% (minimum) indicate where on each cycle the rhythms are on a particular day. In general, a rhythm at 0% is crossing the midpoint and is thought to have no real impact on your life, whereas a rhythm at +100% (at the peak of that cycle) would give you an edge in that area, and a rhythm at -100% (at the bottom of that cycle) would make life more difficult in that area. There is no particular meaning to a day on which your rhythms are all high or all low, except the obvious benefits or hindrances that these rare extremes are thought to have on your life.\n\nIn addition to the three popular cycles, various other cycles have been proposed, based on linear combination of the three, or on longer or shorter rhythms.\n\nTheories published state the equations for the cycles as:\nwhere formula_4 indicates the number of days since birth. Basic arithmetic shows that the combination of the simpler 23- and 28-day cycles repeats every 644 days (or 1-3/4 years), while the triple combination of 23-, 28-, and 33-day cycles repeats every 21,252 days (or 58.18+ years).\n\nThe notion of periodic cycles in human fortunes is ancient; for instance, it is found in natal astrology and in folk beliefs about \"lucky days\". The 23- and 28-day rhythms used by biorhythmists, however, were first devised in the late 19th century by Wilhelm Fliess, a Berlin physician and patient of Sigmund Freud. Fliess believed that he observed regularities at 23- and 28-day intervals in a number of phenomena, including births and deaths. He labeled the 23-day rhythm \"male\" and the 28-day rhythm \"female\", matching the menstrual cycle.\n\nIn 1904, Viennese psychology professor Hermann Swoboda came to similar conclusions. Alfred Teltscher, professor of engineering at the University of Innsbruck, developed Swoboda's work and suggested that his students' good and bad days followed a rhythmic pattern; he believed that the brain's ability to absorb, mental ability, and alertness ran in 33-day cycles. One of the first academic researchers of biorhythms was Estonian-born Nikolai Pärna, who published a book in German called \"Rhythm, Life and Creation\" in 1923.\n\nThe practice of consulting biorhythms was popularized in the 1970s by a series of books by Bernard Gittelson, including \"Biorhythm — A Personal Science\", \"Biorhythm Charts of the Famous and Infamous\", and \"Biorhythm Sports Forecasting\". Gittelson's company, Biorhythm Computers, Inc., made a business selling personal biorhythm charts and calculators, but his ability to predict sporting events was not substantiated.\n\nCharting biorhythms for personal use was popular in the United States during the 1970s; many places (especially video arcades and amusement areas) had a biorhythm machine that provided charts upon entry of date of birth. Biorhythm programs were a common application on personal computers; and in the late 1970s, there were also handheld biorhythm calculators on the market, the \"Kosmos 1\" and the Casio \"Biolator\". Biorhythm charts appeared in the \"Chicago Tribune\" from 1977 to 1979, and Gittelson wrote daily biorhythm charts for the \"Toronto Star\" from 1981 to 1985.\n\nAlthough biorhythms have declined in popularity (pop culture magazine \"Vice\" considered them \"dead\" by the mid 2010s), there are free and proprietary apps and computer programs which have charting and analysis capabilities, as well as numerous websites that offer free biorhythm readings.\n\nThere have been some three dozen studies supporting biorhythm theory, but according to a study by Terence Hines, all of those had methodological and statistical errors. Hines rejected 134 biorhythm studies and concluded that the theory is not valid.\n\nSupporters continued to defend the theory after Hines' review, causing other scientists to consider the field as pseudoscience:\nThe physiologist Gordon Stein in the book \"Encyclopedia of Hoaxes\" (1993) has written: \"Both the theoretical underpinning and the practical scientific verification of biorhythm theory are lacking. Without those, biorhythms became just another pseudoscientific claim that people are willing to accept without required evidence. Those pushing biorhythm calculators and books on a gullible public are guilty of making fraudulent claims. They are hoaxers of the public if they know what they are saying has no factual justification.\"\n\nA 1978 study of the incidence of industrial accidents found neither empirical nor theoretical support for the biorhythm model.\n\n\n\n"}
{"id": "37354678", "url": "https://en.wikipedia.org/wiki?curid=37354678", "title": "Burkholderia phage phiE12-2", "text": "Burkholderia phage phiE12-2\n\nBurkholderia phage phiE12-2 is a bacteriophage (a virus that infects bacteria) of the family Myoviridae, genus \"P2-like viruses\". Its genetic structure corresponds to the class I of the Baltimore classification: dsDNA, being a DNA virus.\n"}
{"id": "31167341", "url": "https://en.wikipedia.org/wiki?curid=31167341", "title": "Charles Alexander Shain", "text": "Charles Alexander Shain\n\nCharles Alexander Shain (6 February 1922 – 11 February 1960) was an Australian pioneer in the field of radio astronomy.\n\nShain entered the University of Melbourne in 1940, studying physics, where he won a non-resident Exhibition to Trinity College. In 1942, for his final year, he moved into residence at Trinity on a Council Minor Scholarship, graduating at the end of the year with a BSc. \n\nWith Australia at war, Shain joined the Second Australian Imperial Force, but was discharged in 1943 on medical grounds. He then worked on radio countermeasures at the Australian Commonwealth Scientific and Industrial Research Organisation (CSIRO) Radiophysics Laboratory. Following the war, he entered the field of decametre radio astronomy. He was a pioneer in the study of absorption in H II regions as well as the effects of the ionosphere on radio signals. He died of cancer, leaving behind a wife and three children.\n"}
{"id": "3999072", "url": "https://en.wikipedia.org/wiki?curid=3999072", "title": "Congruence bias", "text": "Congruence bias\n\nCongruence bias is a type of cognitive bias similar to confirmation bias. Congruence bias occurs due to people's overreliance on directly testing a given hypothesis as well as neglecting indirect testing.\n\nSuppose that, in an experimental setting, a subject is presented with two buttons and told that pressing one of those buttons, but not the other, will open a door. The subject adopts the hypothesis that the button on the left opens the door in question. A direct test of this hypothesis would be pressing the button on the left; an indirect test would be pressing the button on the right. The latter is still a valid test because once the result of the door's remaining closed is found, the left button is proven to be the desired button. (This example is parallel to Bruner, Goodnow, and Austin's example in the psychology classic, \"A Study of Thinking\".)\n\nIt is possible to take this idea of direct and indirect testing and apply it to more complicated experiments in order to explain the presence of a congruence bias in people. In an experiment, a subject will test his own usually naive hypothesis again and again instead of trying to disprove it.\n\nThe classic example of subjects' congruence bias was discovered by Peter Wason (1960, 1968). Here, the experimenter gave subjects the number sequence \"2, 4, 6\", telling the subjects that this sequence followed a particular rule and instructing subjects to find the rule underlying the sequence logic. Subjects provided their own number sequences as tests to see if they could ascertain the rule dictating which numbers could be included in the sequence and which could not. Most subjects respond to the task by quickly deciding that the underlying rule is \"numbers ascending by 2\", and provide as tests only sequences concordant with this rule, such as \"3, 5, 7,\" or even \"pi plus 2, plus 4, plus 6\". Each of these sequences follows the underlying rule the experimenter is thinking of, though \"numbers ascending by 2\" is not the actual criterion being used. However, because subjects succeed at repeatedly testing the same singular principle, they naively believe their chosen hypothesis is correct. When a subject offers up to the experimenter the hypothesis \"numbers ascending by 2\" only to be told he is wrong, much confusion usually ensues. At this point, many subjects attempt to change the wording of the rule without changing its meaning, and even those who switch to indirect testing have trouble letting go of the \"+ 2\" convention, producing potential rules as idiosyncratic as \"the first two numbers in the sequence are random, and the third number is the second number plus two\". Many subjects never realize that the actual rule the experimenter was using was simply just to list ascending numbers, because of the subjects' inability to consider indirect tests of their hypotheses.\n\nWason attributed this failure of subjects to an inability to consider alternative hypotheses, which is the root of the congruence bias. Jonathan Baron explains that subjects could be said to be using a \"congruence heuristic\", wherein a hypothesis is tested only by thinking of results that would be found if that hypothesis is true. This heuristic, which many people seem to use, ignores alternative hypotheses.\n\nBaron suggests the following heuristics to avoid falling into the congruence bias trap:\n\n\n"}
{"id": "9721524", "url": "https://en.wikipedia.org/wiki?curid=9721524", "title": "Cunningham correction factor", "text": "Cunningham correction factor\n\nIn fluid dynamics, the Cunningham correction factor or Cunningham slip correction factor is used to account for noncontinuum effects when calculating the drag on small particles. The derivation of Stokes Law, which is used to calculate the drag force on small particles, assumes a No-slip condition which is no longer correct at high Knudsen number. The Cunningham slip correction factor allows predicting the drag force on a particle moving a fluid with Knudsen number between the continuum regime and free molecular flow.\n\nThe drag coefficient calculated with standard correlations is divided by the Cunningham correction factor, C given below.\n\nEbenezer Cunningham derived the correction factor in 1910 and with Robert Andrews Millikan, verified the correction in the same year.\n\nwhere\n\nThe Cunningham correction factor becomes significant when particles become smaller than 15 micrometers, for air at ambient conditions.\n\nFor sub-micrometer particles, Brownian motion must be taken into account.\n"}
{"id": "41633679", "url": "https://en.wikipedia.org/wiki?curid=41633679", "title": "DSSAT", "text": "DSSAT\n\nThe Decision Support System for Agrotechnology Transfer (DSSAT) is a set of computer programs for simulating agricultural crop growth. It has been used in over 100 countries by agronomists for evaluating farming methods. One application has been assessing the possible impacts on agriculture of climate change and testing adaptation methods.\n\nDSSAT is built with a modular approach, with different options available to represent such processes as evapotranspiration and soil organic matter accumulation, which facilitates testing different representations of processes important in crop growth. The functionality of DSSAT has also been extended through interfaces with other software such as GIS. DSSAT typically requires input parameters related to soil condition, weather, any management practices such as fertilizer use and irrigation, and characteristics of the crop variety being grown. Many common crops have their characteristics already implemented as DSSAT modules.\n\nDSSAT grew out of the International Benchmark Sites Network for Agrotechnological Transfer (IBSNAT) in the 1980s, with the first official release in 1989. Version 4, released in 2003, introduced a more modular structure and added tools for agricultural economic analysis and risk assessment. Development has continued in affiliation with the International Consortium for Agricultural Systems Applications (ICASA).\n\n"}
{"id": "1980733", "url": "https://en.wikipedia.org/wiki?curid=1980733", "title": "Deep-level trap", "text": "Deep-level trap\n\nDeep-level traps or deep-level defects are a generally undesirable type of electronic defect in semiconductors. They are \"deep\" in the sense that the energy required to remove an electron or hole from the trap to the valence or conduction band is much larger than the characteristic thermal energy \"kT\", where \"k\" is the Boltzmann constant and \"T\" is temperature. Deep traps interfere with more useful types of doping by \"compensating\" the dominant charge carrier type, annihilating either free electrons or electron holes depending on which is more prevalent. They also directly interfere with the operation of transistors, light-emitting diodes and other electronic and opto-electronic devices, by offering an intermediate state inside the band gap. Deep-level traps shorten the non-radiative life time of charge carriers, and—through the Shockley–Read–Hall (SRH) process—facilitate recombination of minority carriers, having adverse effects on the semiconductor device performance.\n\nCommon chemical elements that produce deep-level defects in silicon include iron, nickel, copper, gold, and silver. In general, transition metals produce this effect, while light metals such as aluminium do not.\n\nSurface states and crystallographic defects in the crystal lattice can also play role of deep-level traps.\n"}
{"id": "34641290", "url": "https://en.wikipedia.org/wiki?curid=34641290", "title": "Emotional geography", "text": "Emotional geography\n\nEmotional geography is a subtopic within human geography, dealing with the relationships between emotions and geographic places and their contextual environments.\n\nEmotional geography specifically focuses on how human emotions relate to, or affect, the environment around them.\n\n"}
{"id": "6818394", "url": "https://en.wikipedia.org/wiki?curid=6818394", "title": "Ernst Stuhlinger", "text": "Ernst Stuhlinger\n\nErnst Stuhlinger (December 19, 1913 Niederrimbach, Germany – May 25, 2008) was a German-American atomic, electrical, and rocket scientist. After being brought to the United States as part of Operation Paperclip, he developed guidance systems with Wernher von Braun's team for the US Army, and later was a scientist with NASA. He was also instrumental in the development of the ion engine for long-endurance space flight, and a wide variety of scientific experiments.\n\nStuhlinger was born in Niederrimbach (now part of Creglingen), Württemberg, Germany. At age 23, he earned his doctorate in physics at the University of Tübingen in 1936, working with Otto Haxel, Hans Bethe and his advisor Hans Geiger.\nIn 1939 to 1941, he worked in Berlin, on cosmic rays and nuclear physics as an assistant professor at the Berlin Institute of Technology developing innovative nuclear detector instrumentation.\n\nDespite showing promise as a scientist, in 1941 Stuhlinger was drafted as a private in the German Army and sent to the Russian front, where he was wounded during the Battle of Moscow. Following this, he was in the Battle of Stalingrad and was one of the few members of his unit to survive and make the long, on-foot retreat out of Russia in the cold of winter. Upon reaching German territory in 1943, Stuhingler was ordered to the rocket development center in Peenemunde where he joined Dr. Wernher von Braun's team. For the remainder of the war, he worked in the field of guidance systems. In 1954, Stuhlinger assisted in the founding of the Rocket City Astronomical Association (Renamed to the Von Braun Astronomical Society following von Braun's death) where he served as one of the five original directors for the observatory built inside Monte Sano State Park.\n\nStuhlinger was one of the first group of 126 scientists who emigrated to the United States with von Braun after World War II as part of Operation Paperclip. In the 1945–50 years, he primarily worked on guidance systems in US Army missile programs at Fort Bliss, Texas. In 1950, von Braun's team and the missile programs were transferred to Redstone Arsenal at Huntsville, Alabama. For the next decade, Stuhlinger and other von Braun team members worked on Army missiles, but they also devoted efforts in building an unofficial space capability. He eventually served as director of the Advanced Research Projects Division of the Army Ballistic Missile Agency (ABMA). On April 14, 1955, together with many other Paperclip members, he became a naturalized United States citizen. \nIn the 1950s, Stuhlinger, along with von Braun, collaborated with Walt Disney Pictures. Together, they produced three films, Man in Space and Man and the Moon in 1955, and Mars and Beyond in 1957. Stuhlinger worked as a technical consultant for these films.\n\nStuhlinger played a small but important role in the race to launch a US satellite after the success of Sputnik 1. There was little time to develop and test automated guidance or staging systems, so Stuhlinger developed a simple spring-powered staging timer that was triggered from the ground. On the night of January 31, 1958, Stuhlinger was at the controls of the timer when the Explorer 1 was launched, triggering the device right on time. He became known as \"the man with the golden finger.\" This satellite discovered the Van Allen radiation belt through a cosmic ray sensor, a felicitous intersection with his early physics expertise, included in a science package supervised by Stuhlinger.\n\nIn 1960, the major part of ABMA was transferred to NASA, forming the Marshall Space Flight Center (MSFC) in Huntsville, Alabama. Stuhlinger served as director of the MSFC Space Science Laboratory from its formation in 1960 until 1968, and then was MSFC's associate director for science from 1968 to 1975. Among his many other works at Marshall, he directed early planning for lunar exploration, worked on the Apollo Telescope Mount that produced a wealth of information about the Sun, led planning for the three High Energy Astronomical Observatories, and worked on the initial phases of what would become the Hubble Space Telescope.\n\nIn 1970, shortly after the first lunar landing, Stuhlinger received a letter from Sister Mary Jucunda in Zambia, Africa, asking how billions of dollars could be spent for space research when so many children on the Earth were starving to death. Stuhlinger's thoughtful response is often cited to justify such expenditures.\n\nStuhlinger spent much of his spare time developing designs for solar-powered spacecraft. The most popular of those designs relied on ion thrusters, which ionize either caesium or rubidium vapor and accelerate the positively charged ions through gridded electrodes. The spacecraft would be powered by one kilowatt of solar energy. He referred to the concept as a \"sunship\". He is considered as one of the pioneers of electric propulsion having, among many contributions, authored the classic textbook \"Ion Propulsion for Space Flight\" (McGraw-Hill, New York, 1964). In 2005, he was honored by the Electric Rocket Propulsion Society, and awarded its highest honor \"The Medal for Outstanding Achievement in Electric Propulsion\", which was renamed the Stuhlinger Medal shortly following his death.\n\nAfter retiring from NASA in January 1976, Stuhlinger became an adjunct professor and senior research scientist at the University of Alabama in Huntsville (UAH), holding this position for the next 20 years. In 1978, he was at the University of Munich for six months on a Humboldt Fellowship. Ernst was especially proud of winning this award as an American scientist. During 1984-89, he was also a senior research associate with Teledyne Brown Engineering.\n\nStarting in 1990, Stuhlinger and Frederick I. Ordway III collaborated on the two-volume biography \"Werhner von Braun: Crusader for Space\" (Krieger Publishing, 1994). In it, Stuhlinger downplayed claims that von Braun had mistreated prisoners working on the V-2 program during the war. Michael J. Neufeld has questioned this version, maintaining that knowledge of V-2 production using forced labor is an established fact. Stuhlinger reiterated the point that their aim was ultimately peaceful. In a newspaper article he wrote:\nYes, we did work on improved guidance systems, but in late 1944 we were convinced that the war would soon be over before new systems could be used on military rockets. However, we were convinced that somehow our work would find application in the future rockets that would not aim at London, but at the moon.\n\nStuhlinger was interviewed in 1984 by fellow Operation Paperclip scientist Konrad Dannenberg and UAH professor Donald Tarter for an oral history series. This hour-long review of their experiences has information on early space programs.\n\nIn 2004, when he was 90, Stuhlinger helped to raise funds to preserve a Saturn V rocket display at Huntsville, Alabama.\nErnst Stuhlinger died in Huntsville at age 94.\n\n\n\n"}
{"id": "57126594", "url": "https://en.wikipedia.org/wiki?curid=57126594", "title": "Explorer 28", "text": "Explorer 28\n\nExplorer 28 (or IMP-C) was a satellite launched in May 1965 to study Space physics. It was powered by chemical batteries and Solar panels. There were 7 experiments onboard, all devoted to particle studies. Performance was normal until mid-April 1967, when intermittent problems began. It stayed in contact until May 12, 1967, when contact was lost. The orbit kept going lower and lower until it re-entered on July 4 1968.\n"}
{"id": "3476318", "url": "https://en.wikipedia.org/wiki?curid=3476318", "title": "Fernando de Buen y Lozano", "text": "Fernando de Buen y Lozano\n\nFernando de Buen y Lozano (10 October 1895 – 6 May 1962) was a Spanish ichthyologist and oceanographer. He lived in Mexico, Uruguay, and Chile. In Uruguay, he was the director of the Department of Science at the Oceanography and Fisheries Service as well as Professor of Hydrobiology and Protozoology in the Faculty of Arts and Sciences. He was an honorary foreign member of the American Society of Ichthyologists and Herpetologists.\n\n \n"}
{"id": "38742706", "url": "https://en.wikipedia.org/wiki?curid=38742706", "title": "Ferrimolybdite", "text": "Ferrimolybdite\n\nFerrimolybdite is a hydrous iron molybdate mineral with formula: Fe(MoO)·8(HO) or Fe(MoO)·n(HO). It forms coatings and radial aggregates of soft yellow needles which crystallize in the orthorhombic system.\n\nIt was first described in 1914 for an occurrence in the Alekseevskii Mine in the Karysh River Basin, Khakassia Republic, Siberia, Russia. It was named for its composition (ferric iron and molybdenum).\n\nIt occurs as an oxidation product of molybdenum bearing ore deposits. \nAssociated minerals include: molybdenite, pyrite and chalcopyrite.\n"}
{"id": "2828298", "url": "https://en.wikipedia.org/wiki?curid=2828298", "title": "Generation–recombination noise", "text": "Generation–recombination noise\n\nGeneration–recombination noise, or g–r noise, is a type of electrical signal noise caused statistically by the fluctuation of the generation and recombination of electrons in semiconductor-based photon detectors.\n\n"}
{"id": "45426554", "url": "https://en.wikipedia.org/wiki?curid=45426554", "title": "Graphene quantum dot", "text": "Graphene quantum dot\n\nGraphene quantum dots (GQDs) represent single-layer to tens of layers of graphene of a size less than 30 nm. Due to its exceptional properties such as low toxicity, stable photoluminescence, chemical stability and pronounced quantum confinement effect, GQDs are considered as a novel material for biological, opto-electronics, energy and environmental applications.\n\nThe graphene quantum dot (GQD) is becoming an advanced multifunctional material for its unique optical, electronic, spin, and photoelectric properties induced by the quantum confinement effect and edge effect. GQDs are fragments limited in size, or domains, of a single-layer two-dimensional graphene crystal. Spectral studies have found that in almost all cases, GQDs are not single-layer graphene domains, but multi-layer formations containing up to 10 layers of reduced graphene oxide (rGO) from 10 to 60 nm in size.\n\nPresently, several techniques have been developed to prepare GQDs; these techniques mainly include electron beam lithography, chemical synthesis, electrochemical preparation, graphene oxide (GO) reduction, C60 catalytic transformation, the microwave assisted hydrothermal method (MAH), the Soft-Template method, the hydrothermal method, and the ultrasonic exfoliation method.\n\nGQDs have various important applications in bioimaging, cancer therapeutics, temperature sensing, drug delivery, surfactants, LEDs lighter converters, photodetectors, OPV solar cells, and photoluminescent material, biosensors fabrication.\n"}
{"id": "873197", "url": "https://en.wikipedia.org/wiki?curid=873197", "title": "Hale Telescope", "text": "Hale Telescope\n\nThe Hale telescope is a , 3.3 reflecting telescope at the Palomar Observatory in California, US, named after astronomer George Ellery Hale. With funding from the Rockefeller Foundation in 1928, he orchestrated the planning, design, and construction of the observatory, but with the project ending up taking 20 years he did not live to see its commissioning. The Hale was groundbreaking for its time, with double the diameter of the next largest telescope, and pioneered many new technologies in telescope mount design and in the design and fabrication of its large aluminum coated \"honeycomb\" low thermal expansion Pyrex mirror.. It was completed in 1948 and is still in active use.\n\nThe Hale Telescope represented the technological limit in building large optical telescopes for over 30 years. It was the largest telescope in the world until the Russian BTA-6 was built in 1976, and the second largest until the construction of the Keck Observatory Keck 1 in 1993.\n\nHale supervised the building of the telescopes at the Mount Wilson Observatory with grants from the Carnegie Institution of Washington: the telescope in 1908 and the telescope in 1917. These telescopes were very successful, leading to the rapid advance in understanding of the scale of the Universe through the 1920s, and demonstrating to visionaries like Hale the need for even larger collectors.\n\nThe chief optical designer for Hale's previous 100-inch telescope was George Willis Ritchey, who intended the new telescope to be of Ritchey–Chrétien design. Compared to the usual parabolic primary, this design would have provided sharper images over a larger usable field of view. However, Ritchey and Hale had a falling-out. With the project already late and over budget, Hale refused to adopt the new design, with its complex curvatures, and Ritchey left the project. The Mount Palomar Hale telescope turned out to be the last world-leading telescope to have a parabolic primary mirror.\n\nIn 1928 Hale secured a grant of $6 million from the Rockefeller Foundation for \"the construction of an observatory, including a 200-inch reflecting telescope\" to be administered by the California Institute of Technology (Caltech), of which Hale was a founding member. In the early 1930s, Hale selected a site at on Palomar Mountain in San Diego County, California, US, as the best site, and less likely to be affected by the growing light pollution problem in urban centers like Los Angeles. The Corning Glass Works was assigned the task of making a primary mirror. Construction of the observatory facilities and dome started in 1936, but because of interruptions caused by World War II, the telescope was not completed until 1948 when it was dedicated. Due to slight distortions of images, corrections were made to the telescope throughout 1949. It became available for research in 1950.\n\nThe telescope saw first light on January 26, 1949 at 10:06pm PST under the direction of American astronomer Edwin Powell Hubble, targeting NGC 2261, an object also known as Hubble's Variable Nebula. The photographs made then were published in the astronomical literature and in the May 7, 1949 issue of \"Collier's Magazine\".\n\nThe telescope continues to be used every clear night for scientific research by astronomers from Caltech and their operating partners, Cornell University, the University of California, and the Jet Propulsion Laboratory. It is equipped with modern optical and infrared array imagers, spectrographs, and an adaptive optics system. It has also used lucky cam imaging, which in combination with adaptive optics pushed the mirror close to its theoretical resolution for certain types of viewing.\n\nOne of the Corning Labs' glass test blanks for the Hale was used for the C. Donald Shane telescope's primary mirror.\n\nThe collecting area of the mirror is about 31,000 square inches (20 square meters).\n\nThe Hale telescope uses a special type of equatorial mount called a \"horseshoe mount\", a modified yoke mount that replaces the polar bearing with an open \"horseshoe\" structure that gives the telescope full access to the entire sky, including Polaris and stars near it. The optical tube assembly (OTA) uses a Serrurier truss, then newly invented by Mark U. Serrurier of Caltech in Pasadena in 1935, designed to flex in such a way as to keep all of the optics in alignment. Theodore von Karman designed the lubrication system to avoid potential issues with turbulence during tracking.\n\nOriginally, the Hale telescope was going to use a primary mirror of fused quartz manufactured by General Electric, but instead the primary mirror was cast in 1934 at Corning Glass Works in New York State using Corning's then new material called Pyrex (borosilicate glass). Pyrex was chosen for its low expansion qualities so the large mirror would not distort the images produced when it changed shape due to temperature variations (a problem that plagued earlier large telescopes).\n\nThe mirror was cast in a mold with 36 raised mold blocks (similar in shape to a waffle iron). This created a honeycomb mirror that cut the amount of Pyrex needed down from over to just , making a mirror that would cool faster in use and have multiple \"mounting points\" on the back to evenly distribute its weight (note – see external links 1934 article for drawings). The shape of a central hole was also part of the mold so light could pass through the finished mirror when it was used in a Cassegrain configuration (a Pyrex plug for this hole was also made to be used during the grinding and polishing process). While the glass was being poured into the mold during the first attempt to cast the 200-inch mirror, the intense heat caused several of the molding blocks to break loose and float to the top, ruining the mirror. The defective mirror was used to test the annealing process. After the mold was re-engineered, a second mirror was successfully cast.\n\nAfter cooling several months, the finished mirror blank was transported by rail to Pasadena, California. Once in Pasadena the mirror was transferred from the rail flat car to a specially designed semi-trailer for road transport to where it would be polished. In the optical shop in Pasadena (now the Synchrotron building at Caltech) standard telescope mirror making techniques were used to turn the flat blank into a precise concave parabolic shape, although they had to be executed on a grand scale. A special \"mirror cell\" jig was constructed which could employ five different motions when the mirror was ground and polished. Over 13 years almost of glass was ground and polished away reducing the weight of the mirror to . The mirror was coated (and still is re-coated every 18–24 months) with a reflective aluminum surface using the same aluminum vacuum-deposition process invented in 1930 by Caltech physicist and astronomer John Strong.\n\nThe Hale's mirror was near the technological limit of a primary mirror made of a single rigid piece of glass. Using a monolithic mirror much larger than the 5-meter Hale or 6-meter BTA-6 is prohibitively expensive due to the cost of both the mirror, and the massive structure needed to support it. A mirror beyond that size would also sag slightly under its own weight as the telescope is rotated to different positions, changing the precision shape of the surface, which must be accurate to within 2 millionths of an inch (50 nm). Modern telescopes over 9 meters use a different mirror design to solve this problem, with either a single thin flexible mirror or a cluster of smaller segmented mirrors, whose shape is continuously adjusted by a computer-controlled active optics system using actuators built into the mirror support cell.\n\nThe \"Cornell Mid-Infrared Asteroid Spectroscopy\" (MIDAS) survey used the Hale telescope with a spectrograpgh to study spectra from 29 asteroids.\n\nUp until the year 2010, telescopes could only directly image exoplanets under exceptional circumstances. Specifically, it is easier to obtain images when the planet is especially large (considerably larger than Jupiter), widely separated from its parent star, and hot so that it emits intense infrared radiation. However, in 2010 a team from NASA's Jet Propulsion Laboratory demonstrated that a vortex coronagraph could enable small scopes to directly image planets. They did this by imaging the previously imaged HR 8799 planets using just a 1.5 m portion of the Hale telescope.\n\nThe Hale had four times the light-collecting area of the next largest scope when it was commissioned in 1949. Other contemporary telescopes were the Hooker Telescope at the Mount Wilson Observatory and the Otto Struve Telescope at the McDonald Observatory.\n\n\n\n"}
{"id": "11592003", "url": "https://en.wikipedia.org/wiki?curid=11592003", "title": "Health systems engineering", "text": "Health systems engineering\n\nHealth Systems Engineering or Health Engineering (often known as \"Health Care Systems Engineering (HCSE)\") is an academic and a pragmatic discipline that approaches the health care industry, and other industries connected with health care delivery, as complex adaptive systems, and identifies and applies engineering design and analysis principles in such areas. This can overlap with biomedical engineering which focuses on design and development of various medical products; industrial engineering and operations management which involve improving organizational operations; and various health care practice fields like medicine, pharmacy, dentistry, nursing, etc. Other fields participating in this interdisciplinary area include public health, information technology, management studies, and regulatory law.\n\nPeople whose work implicates this field in some capacity can include members of all the above-noted fields, many of which have sub-fields targeted toward health care matters even if health or health care is not a principal focus of the overall field (e.g. management, law). Areas of biomedical engineering (BME) in this area often include clinical engineering (sometimes also called \"hospital engineering\") as well as those BMEs developing medical devices and pharmaceutical drugs. The industrial engineering (IE) principles employed tend to include optimization, decision analysis, human factors engineering, quality engineering, and value engineering.\n\n"}
{"id": "45145", "url": "https://en.wikipedia.org/wiki?curid=45145", "title": "Hubble sequence", "text": "Hubble sequence\n\nThe Hubble sequence is a morphological classification scheme for galaxies invented by Edwin Hubble in 1926. It is often known colloquially as the Hubble tuning fork diagram because of the shape in which it is traditionally represented.\n\nHubble’s scheme divides regular galaxies into three broad classes – ellipticals, lenticulars and spirals – based on their visual appearance (originally on photographic plates). A fourth class contains galaxies with an irregular appearance. To this day, the Hubble sequence is the most commonly used system for classifying galaxies, both in professional astronomical research and in amateur astronomy.\n\nOn the left (in the sense that the sequence is usually drawn) lie the ellipticals. Elliptical galaxies have relatively smooth, featureless light distributions and appear as ellipses in photographic images. They are denoted by the letter E, followed by an integer \"n\" representing their degree of ellipticity in the sky. By convention, \"n\" is ten times the ellipticity of the galaxy, rounded to the nearest integer, where the ellipticity is defined as for an ellipse with semi-major and semi-minor axes of lengths \"a\" and \"b\" respectively. The ellipticity increases from left to right on the Hubble diagram, with near-circular (E0) galaxies situated on the very left of the diagram. It is important to note that the ellipticity of a galaxy on the sky is only indirectly related to the true 3-dimensional shape (for example, a flattened, discus-shaped galaxy can appear almost round if viewed face-on or highly elliptical if viewed edge-on). Observationally, the most flattened \"elliptical\" galaxies have ellipticities \"e\" = 0.7 (denoted E7). However, from studying the light profiles and the ellipticity profiles, rather than just looking at the images, it was realised in the 1960s that the E5–E7 galaxies are probably misclassified lenticular galaxies with large-scale disks seen at various inclinations to our line-of-sight. Observations of the kinematics of early-type galaxies further confirmed this.\n\nExamples of elliptical galaxies: M49, M59, M60, M87, NGC 4125.\n\nAt the centre of the Hubble tuning fork, where the two spiral-galaxy branches and the elliptical branch join, lies an intermediate class of galaxies known as lenticulars and given the symbol S0. These galaxies consist of a bright central bulge, similar in appearance to an elliptical galaxy, surrounded by an extended, disk-like structure. Unlike spiral galaxies, the disks of lenticular galaxies have no visible spiral structure and are not actively forming stars in any significant quantity. \n\nWhen simply looking at a galaxy's image, lenticular galaxies with relatively face-on disks are difficult to distinguish from ellipticals of type E0–E3, making the classification of many such galaxies uncertain. When viewed edge-on, the disk becomes more apparent and prominent dust-lanes are sometimes visible in absorption at optical wavelengths.\n\nAt the time of the initial publication of Hubble's galaxy classification scheme, the existence of lenticular galaxies was purely hypothetical. Hubble believed that they were necessary as an intermediate stage between the highly flattened \"ellipticals\" and spirals. Later observations (by Hubble himself, among others) showed Hubble's belief to be correct and the S0 class was included in the definitive exposition of the Hubble sequence by Allan Sandage. Missing from the Hubble sequence are the early-type galaxies with intermediate-scale disks, in between the E and S0 type, Martha Liller denoted them ES galaxies in 1966.\n\nLenticular and spiral galaxies, taken together, are often referred to as disk galaxies. The bulge-to-disk flux ratio in lenticular galaxies can take on a range of values, just as it does for each of the spiral galaxy morphological types (Sa, Sb, etc.).\n\nExamples of lenticular galaxies: M85, M86, NGC 1316, NGC 2787, NGC 5866, Centaurus A.\n\nOn the right of the Hubble sequence diagram are two parallel branches encompassing the spiral galaxies. A spiral galaxy consists of a flattened disk, with stars forming a (usually two-armed) spiral structure, and a central concentration of stars known as the bulge. Roughly half of all spirals are also observed to have a bar-like structure, extending from the central bulge, at the ends of which the spiral arms begin. In the tuning-fork diagram, the regular spirals occupy the upper branch and are denoted by the letter S, while the lower branch contains the barred spirals, given the symbol SB. Both type of spirals are further subdivided according to the detailed appearance of their spiral structures. Membership of one of these subdivisions is indicated by adding a lower-case letter to the morphological type, as follows:\n\nHubble originally described three classes of spiral galaxy. This was extended by Gérard de Vaucouleurs to include a fourth class:\n\nAlthough strictly part of the de Vaucouleurs system of classification, the Sd class is often included in the Hubble sequence. The basic spiral types can be extended to enable finer distinctions of appearance. For example, spiral galaxies whose appearance is intermediate between two of the above classes are often identified by appending two lower-case letters to the main galaxy type (for example, Sbc for a galaxy that is intermediate between an Sb and an Sc).\n\nOur own Milky Way is generally classed as SBb or SBc, making it a barred spiral with well-defined arms. \n\nExamples of regular spiral galaxies: M31 (Andromeda Galaxy), M74, M81, M104 (Sombrero Galaxy), M51a (Whirlpool Galaxy), NGC 300, NGC 772.\n\nExamples of barred spiral galaxies: M91, M95, NGC 1097, NGC 1300, NGC1672, NGC 2536, NGC 2903.\n\nGalaxies that do not fit into the Hubble sequence, because they have no regular structure (either disk-like or ellipsoidal), are termed irregular galaxies. Hubble defined two classes of irregular galaxy:\nIn his extension to the Hubble sequence, de Vaucouleurs called the Irr I galaxies 'Magellanic irregulars', after the Magellanic Clouds – two satellites of the Milky Way which Hubble classified as Irr I. The discovery of a faint spiral structure in the Large Magellanic Cloud led de Vaucouleurs to further divide the irregular galaxies into those that, like the LMC, show some evidence for spiral structure (these are given the symbol Sm) and those that have no obvious structure, such as the Small Magellanic Cloud (denoted Im). In the extended Hubble sequence, the Magellanic irregulars are usually placed at the end of the spiral branch of the Hubble tuning fork.\n\nExamples of irregular galaxies: M82, NGC 1427A, Large Magellanic Cloud, Small Magellanic Cloud.\n\nElliptical and lenticular galaxies are commonly referred to together as “early-type” galaxies, while spirals and irregular galaxies are referred to as “late types”. This nomenclature is the source of the common, but erroneous, belief that the Hubble sequence was intended to reflect a supposed evolutionary sequence, from elliptical galaxies through lenticulars to either barred or regular spirals. In fact, Hubble was clear from the beginning that no such interpretation was implied:\n\nThe nomenclature, it is emphasized, refers to position in the sequence, and temporal connotations are made at one's peril. The entire classification is purely empirical and without prejudice to theories of evolution...\n\nThe evolutionary picture appears to be lent weight by the fact that the disks of spiral galaxies are observed to be home to many young stars and regions of active star formation, while elliptical galaxies are composed of predominantly old stellar populations. In fact, current evidence suggests the opposite: the early Universe appears to be dominated by spiral and irregular galaxies. In the currently favored picture of galaxy formation, present-day ellipticals formed as a result of mergers between these earlier building blocks; while some lenticular galaxies may have formed this way, others may have accreted their disks around pre-existing spheroids. Some lenticular galaxies may also be evolved spiral galaxies, whose gas has been stripped away leaving no fuel for continued star formation, although the galaxy LEDA 2108986 opens the debate on this.\n\nA common criticism of the Hubble scheme is that the criteria for assigning galaxies to classes are subjective, leading to different observers assigning galaxies to different classes (although experienced observers usually agree to within less than a single Hubble type). Although not really a short-coming, since the 1961 Hubble Atlas of Galaxies, the primary criteria used to assign the morphological type (a, b, c, etc.) has been the nature of the spiral arms, rather than the bulge-to-disk flux ratio, and thus a range of flux ratios exist for each morphological type, as with the lenticular galaxies.\n\nAnother criticism of the Hubble classification scheme is that, being based on the appearance of a galaxy in a two-dimensional image, the classes are only indirectly related to the true physical properties of galaxies. In particular, problems arise because of orientation effects. The same galaxy would look very different, if viewed edge-on, as opposed to a face-on or ‘broadside’ viewpoint. As such, the early-type sequence is poorly represented: the ES galaxies are missing from the Hubble sequence, and the E5–E7 galaxies are actually S0 galaxies. Furthermore, the barred ES and barred S0 galaxies are also absent.\n\nVisual classifications are also less reliable for faint or distant galaxies, and the appearance of galaxies can change depending on the wavelength of light in which they are observed.\n\nNonetheless, the Hubble sequence is still commonly used in the field of extragalactic astronomy and Hubble types are known to correlate with many physically relevant properties of galaxies, such as luminosities, colours, masses (of stars and gas) and star formation rates.\n\n\n"}
{"id": "5668886", "url": "https://en.wikipedia.org/wiki?curid=5668886", "title": "Hugh B. Cott", "text": "Hugh B. Cott\n\nHugh Bamford Cott (6 July 1900 – 18 April 1987) was a British zoologist, an authority on both natural and military camouflage, and a scientific illustrator and photographer. Many of his field studies took place in Africa, where he was especially interested in the Nile crocodile, the evolution of pattern and colour in animals. During the Second World War, Cott worked as a camouflage expert for the British Army and helped to influence War Office policy on camouflage. His book \"Adaptive Coloration in Animals\" (1940), popular among serving soldiers, was the major textbook on camouflage in zoology of the twentieth century. After the war, he became a Fellow of Selwyn College, Cambridge. As a Fellow of the Zoological Society of London, he undertook expeditions to Africa and the Amazon to collect specimens, mainly reptiles and amphibians.\n\nCott was born in Ashby Magna, Leicestershire, England, on 6 July 1900; his father was the rector there. He was schooled at Rugby. In 1919, he graduated from the Royal Military College, Sandhurst, and was commissioned into the Leicestershire Regiment. Between 1922 and 1925, he studied at Selwyn College, Cambridge.\n\nHe had intended to become a priest, and went to Cambridge to read theology, but after his first year he went on the university expedition to South America, where he studied natural forms in eastern Brazil in 1923, led by the entomologist Frank Balfour Browne, where he became fascinated by natural history, and changed his studies to zoology on his return. He then went on an expedition to the lower Amazon (1925–1926), and on research trips to the Zambesi river area in Africa (1927), including Mozambique, Zambia and East Africa, and Lanzarote (1930). He married Joyce Radford in 1928. He was a lecturer in Zoology at Bristol University from 1928 until 1932, when he moved to Glasgow University. He studied under another advocate of military camouflage, John Graham Kerr. His thesis, which he completed in 1935 under a Carnegie Fellowship, was on 'adaptive coloration' – both camouflage and warning coloration – in the Anura (frogs). In 1938 he was made a Doctor of Science at Glasgow, and he became a Zoology lecturer at Cambridge University and Strickland Curator of Birds at the university's Museum of Zoology.\n\nCott served in the Leicestershire Regiment of the British Army as a camouflage expert from 1919–1922, and, during the Second World War, with the Royal Engineers as a camouflage instructor from 1939–1945. Cott was chief instructor at the Camouflage Development and Training Camp at Helwan, Egypt, under filmmaker Geoffrey Barkas from its inception in November 1941.\n\nAfter the war, Cott returned to Cambridge, becoming a Fellow of Selwyn College in 1945; he worked there until he retired in 1967. He gave the Fison Memorial Lecture of 1958 on 'Protective Coloration in Animals'. He continued to work from time to time after his retirement, for instance conducting a survey of crocodile nests on the Victoria Nile for the Uganda National Parks in 1972. He died at the age of 86 on 18 April 1987.\n\nWhile trying to photograph a hen partridge on her nest, Cott waited for hours for the bird to return, finally taking some pictures of the empty nest before giving up. On developing the photographs, he realized the bird had been there all along, perfectly camouflaged.\n\nAs a camouflage expert during the Second World War, Cott likened the functions of military camouflage to those of protective coloration in nature. The three main categories of coloration in his book \"Adaptive Coloration in Animals\" are concealment, disguise, and advertisement. He studied, described and presented examples of such diverse camouflage effects as obliterative shading, disruption, differential blending, high contrast, coincident disruption, concealment of the eye, contour obliteration, shadow elimination, and mimicry. In his wartime lectures at Farnham Castle, he described nine categories of visual deception:\n\n\nCott's account of all this (illustrated by his own pen and ink drawings) is the 550-page book \"Adaptive Coloration in Animals\" (1940). It was proof-read by Kerr, who commented on its publication 'It is by far the finest thing of the kind in existence'. His co-workers' first-hand accounts of his work in military camouflage can be found in the memoirs of two of his fellow camoufleurs: Julian Trevelyan and Roland Penrose.\n\nPeter Forbes wrote of Cott's book:\n\nThe book was written as war loomed, and published in wartime. Cott makes use of his knowledge of natural history to draw parallels between survival in nature and in war, and to advise on military camouflage, for example commenting:\n\nForbes notes that \"Adaptive Coloration in Animals\" is a narrative, short on the experimentation that followed after the war, but Forbes continues:\n\nCott attempted to persuade the British army to use more effective camouflage techniques, including countershading. For example, in August 1940, with the Battle of Britain imminent, he painted two rail-mounted coastal guns, one in conventional style, one countershaded. In aerial photographs, the countershaded gun is essentially invisible. Cott was triumphant, announcing:\n\nHowever (like Kerr before him in the First World War), Cott did not succeed in influencing policy on camouflage, and he resigned from the Camouflage Advisory Panel in 1940.\n\nCott was a founding member of the Society of Wildlife Artists, and a fellow of the Royal Photographic Society. From material gathered in field expeditions, he made contributions to the Cambridge University zoological museum.\n\nCott possessed considerable artistic skill. Like Abbott Thayer, he used his artistry in his scientific work, including in \"Adaptive Coloration in Animals\", to help argue the case he was making. For example, his black-and-white potoo shows this rainforest bird sitting motionless on a mottled tree trunk, its behaviour and disruptive pattern combining to provide effective camouflage. The philosopher and jazz musician David Rothenberg wrote of Cott's art:\n\nIn addition to \"Adaptive Coloration in Animals\", Cott wrote two essays on camouflage: “Camouflage in nature and in war” in the \"Royal Engineers Journal\" (December 1938), pp501–517; and ”Animal form in relation to appearance” in Lancelot Law Whyte, ed. \"Aspects of form: a symposium on form in nature and art\" (London: Percy Lund Humphries, 1951). As a scientific illustrator and photographer, he also wrote three other books: \"Zoological photography in practice\" (1956); \"Uganda in black and white\" (1959); and \"Looking at animals: a zoologist in Africa\" (1975). He became interested in the relationship of bird colours with their role as warning colours, an idea that arose when he observed hornets attracted to some birds being skinned while ignoring others. This led him to study the palatability of birds and their eggs. Among his papers were several studies on the relative palatibility of the eggs based initially on the preferences of ferrets, rats and hedgehogs and later on the use of a panel of expert egg tasters. In one study he found that of 123 species of bird, the kittiwake eggs scored highly with 8.2 out of 10.\n\nThe journalist and author Peter Forbes praised Cott's balance of science and artistry:\n\nOver 60 years after its publication, \"Adaptive Coloration in Animals\" remains a core reference on the subject; the\nevolutionary biologists Graeme Ruxton, Thomas N. Sherratt and Michael Speed conclude their book on animal coloration by writing\n\nThe biologist Steven Vogel commented that:\n\nAn exhibition of his art, writing, and photographs, 'Life, Lines & Illusion', was held at the Nature in Art gallery in Gloucester in September and October 2018.\n\n\n\n\n"}
{"id": "45355308", "url": "https://en.wikipedia.org/wiki?curid=45355308", "title": "Implicit leadership theory", "text": "Implicit leadership theory\n\nImplicit leadership theory (ILT) is a cognitive theory of leadership developed by Robert Lord and colleagues. It is based on the idea that individuals create cognitive representations of the world, and use these preconceived notions to interpret their surroundings and control their behaviors. ILT suggests that group members have implicit expectations and assumptions about the personal characteristics, traits, and qualities that are inherent in a leader. These assumptions, termed implicit leadership theories or leader prototypes, guide an individual's perceptions and responses to leaders. The term implicit is used because they are not outwardly stated and the term theory is used because it involves the generalization of past experiences to new experiences. ILTs allow individuals to identify leaders and aid them in responding appropriately to leaders in order to avoid conflict.\n\nAlthough ILTs vary between individuals, many overlap in terms of the task skills and relationship skills that leaders should possess in order to be successful. In terms of task skills, most people seek a leader that is in control, determined, influential, and continuously involved in the group activities. When considering relationship skills, most people tend to prefer a leader who is caring, honest, open to new ideas, and interested in the group work. In a study by Offermann, Kennedy, and Wirtz (1994), the content and factor structure variation of male and female ILTs were compared across three stimuli, being leaders, effective leaders, and supervisors. They found that eight factors of the ILTs were rather stable across all participants, between males and females, and across the three stimuli. These eight factors were charisma, sensitivity, dedication, intelligence, attractiveness, masculinity, tyranny, and strength. The results from this study suggest that, although ILTs differ depending on the individual, this variation may be systematic and, at times, predictable.\n\nThe social world is solely understood in terms of perceptions, thus people use these perceptions intuitively to effectively organize and guide social interactions. We observe the actions of other people, take note of their personal characteristics, compare them to our own ILTs, and make decisions regarding whether they make an appropriate or inappropriate leader. Additionally, we use ILTs to evaluate the suitability and effectiveness of a group's leader. For example, if you believe that a good leader exerts control over the group, you may focus on this specific characteristic. Consequently, bias can result from noting only the instances when the leader was or was not in control. In a study by Foti and Lord (1987), participants were shown a videotape of a leader-group interaction. The participants were told to report the behaviors that the leader had or had not performed. The results of this study indicated that people are quicker to respond, more accurate, and more confident when they are judging behaviors that are both part of their ILTs and performed by the leader, in comparison to behaviors that were part of their ILTs and were not performed by the leader. Since ILTs are implicit theories, meaning the individual is likely unaware of their biases, it is difficult for ineffective ILTs to be recognized and discarded. For example, Offermann et al. (1994) found that masculinity was a stable ILT across participants, sex, and stimuli. However, males tend to be more autocratic and task-oriented in leadership style, while females tend to adopt a more participative and relationship-oriented style. Consequently, females generally tend to make better leaders as they have a more collaborative approach. This bias would be difficult to correct, as people are typically not aware of their implicit assumptions.\n\n\n\n"}
{"id": "50900233", "url": "https://en.wikipedia.org/wiki?curid=50900233", "title": "James William Benson", "text": "James William Benson\n\nJames William Benson (12 April 1826 – 7 October 1878) was an English scientific instrument maker, gold/silversmith, and watchmaker.\n\nJames William Benson was born in Reading, Berkshire, England. He was the son of William Benson and Phoebe Suckley.\n\nThe Benson family had been watchmakers since 1749. A company, trading as S.S. & J.W. Benson, was founded in 1847 by James William Benson (born in 1826 in Reading) and his older brother Samuel Suckley Benson (born in 1822 in London). The partnership was dissolved on 27 January 1855 and James William continued the business under the name, ‘J. W. Benson’. James William Benson died on 7 October 1878, aged 52, and his sons James, Alfred and Arthur took over the running of the business.\n\nThroughout its history, J.W. Benson Ltd was official watchmaker to the Admiralty & the War Department and also held a number of royal warrants, being watchmakers to Queen Victoria, the Prince of Wales, the Tsar of Russia and several other royal families.\n\nThe company's premises were: Cornhill (1847–64), Ludgate Hill (1854-1937), Old Bond Street (1872-3), Royal Exchange (1892-1937) and their original workshop was at 4-5 Horseshoe Court (at the rear of their Ludgate Hill premises). In 1892 it became a limited company and moved to their new 'steam' factory at 38 Belle Sauvage Yard.\n\nDuring W.W.II. the factory was bombed, destroying thousands of timepieces and from this point on the company no longer manufactured its own watches, but still continued as a retailer. The timepieces bearing the company name used high quality Swiss movements supplied by manufacturers such as, Vertex (Revue), Cyma/Tavannes, Longines and by the English maker, S. Smith & Sons.\n\nJ. W. Benson Ltd continued until 1973 at which time the name was sold to the Royal jewellers, Garrards.\n"}
{"id": "50000095", "url": "https://en.wikipedia.org/wiki?curid=50000095", "title": "Journal of Biological Education", "text": "Journal of Biological Education\n\nThe Journal of Biological Education is a quarterly peer-reviewed academic journal covering biology education. It was established in 1967, is owned by the Royal Society of Biology, and is published by Routledge. The editor-in-chief is Ian Kinchin (University of Surrey). According to the \"Journal Citation Reports\", the journal has a 2014 impact factor of 0.324, ranking it 32nd out of 37 journals in the category \"Education, Scientific Disciplines\".\n"}
{"id": "16977033", "url": "https://en.wikipedia.org/wiki?curid=16977033", "title": "Kugel–Khomskii coupling", "text": "Kugel–Khomskii coupling\n\nKugel–Khomskii coupling describes a coupling between the spin and orbital degrees of freedom in a solid; it is named after the Russian physicists Kliment I. Kugel (Климент Ильич Кугель) and Daniel I. Khomskii (Daniil I. Khomskii, Даниил Ильич Хомский). The Hamiltonian used is:\n\nformula_1\n\n"}
{"id": "29473124", "url": "https://en.wikipedia.org/wiki?curid=29473124", "title": "Lentisphaerae", "text": "Lentisphaerae\n\nLentisphaerae is a phylum of bacteria closely related to Chlamydiae and Verrucomicrobia.\n\nIt includes two monotypic orders Lentisphaerales and Victivallales. Phylum members can be aerobic or anaerobic and fall under two distinct phenotypes. One consists of terrestrial gut microbiota from mammals and birds. The other phenotype includes marine micro-organisms: sequences from fish and coral microbiomes and marine sediment.\n\nThe phylogeny based on the work of the All-Species Living Tree Project.\n\nThe currently accepted taxonomy is based on the List of Prokaryotic names with Standing in Nomenclature (LSPN) and the National Center for Biotechnology Information (NCBI).\n\n\nNotes:\n♠ Strain found at the National Center for Biotechnology Information (NCBI) but not listed in the List of Prokaryotic names with Standing in Nomenclature (LPSN)\n\nList of bacterial orders\n"}
{"id": "873934", "url": "https://en.wikipedia.org/wiki?curid=873934", "title": "Leonid Popov", "text": "Leonid Popov\n\nLeonid Ivanovich Popov (; born August 31, 1945) is a former Soviet cosmonaut.\n\nPopov was born in Oleksandriia, Kirovohrad Oblast, Ukrainian SSR. He was selected as a cosmonaut on April 27, 1970, and flew as Commander on Soyuz 35, Soyuz 40 and Soyuz T-7, logging 200 days, 14 hours, and 45 minutes in space before his retirement on June 13, 1987. Popov is married with two children.\n\nHe was awarded:\nForeign awards:\n"}
{"id": "4045298", "url": "https://en.wikipedia.org/wiki?curid=4045298", "title": "Lieuwe Dirk Boonstra", "text": "Lieuwe Dirk Boonstra\n\nLieuwe Dirk Boonstra (1905 – 1975) was a South African paleontologist whose work focused on the mammal-like reptiles of the Middle (\"Tapinocephalus\" Assemblage Zone) and Late Permian, whose fossil remains are common in the South African Karroo. He was the author of a large number of papers on Therapsids and Pareiasaurs, and described and revised a number of species.\n\n"}
{"id": "6377470", "url": "https://en.wikipedia.org/wiki?curid=6377470", "title": "List of disk partitioning software", "text": "List of disk partitioning software\n\nThis is a list of utilities for performing disk partitioning.\n"}
{"id": "38192079", "url": "https://en.wikipedia.org/wiki?curid=38192079", "title": "List of members of the National Academy of Engineering (Chemical)", "text": "List of members of the National Academy of Engineering (Chemical)\n\nThis list is a subsection of the List of members of the National Academy of Engineering, which includes over 2,000 current members of the United States National Academy of Engineering, each of whom is affiliated with one of 12 disciplinary sections. Each person's name, primary institution, and election year are given. This list does not include deceased members.\n"}
{"id": "7120025", "url": "https://en.wikipedia.org/wiki?curid=7120025", "title": "List of volcanoes in Germany", "text": "List of volcanoes in Germany\n\nThis is a list of active and extinct volcanoes.\n"}
{"id": "39541969", "url": "https://en.wikipedia.org/wiki?curid=39541969", "title": "Maximum acceptable toxicant concentration", "text": "Maximum acceptable toxicant concentration\n\nThe maximum acceptable toxicant concentration (MATC) is a value that is calculated through aquatic toxicity tests to help set water quality regulations for the protection of aquatic life. Using the results of a partial life-cycle chronic toxicity test, the MATC is reported as the geometric mean between the No Observed Effect Concentration (NOEC) and the lowest observed effect concentration (LOEC).\n\nThe MATC is used to set regulatory standards for priority pollutants under the US federal Clean Water Act. Regulatory guidelines give two acceptable concentrations of pollutants to protect against effects: chronic or acute. Since the MATC should only be reported in chronic toxicity tests, there is a widely accepted method to convert the chronic MATC to a concentration that protects against acute effects.\n\nThe MATC is calculated and reported from the results of a number of standard procedures designed by the United States Environmental Protection Agency (US EPA) and other organizations to maintain high accuracy and precision among all toxicity tests for regulatory purposes.\n\nIn a toxicity test, the NOEC and LOEC are derived as a comparison from the negative control, or the experimental group that does not contain the chemical in question. The NOEC is the highest concentration that does not cause a statistically different effect than the negative control through statistical hypothesis testing. Likewise, the LOEC is the lowest concentration tested that does cause a statistically different effect than the negative control. The MATC is the geometric mean between these two values, such that: MATC=\n\nThe MATC is calculated to protect against chronic effects on overall function or health of an organism, not death. A partial life cycle test must be used. This type of toxicity test uses organisms in their most sensitive life stages, usually during times of early reproduction and growth, but not juveniles. The MATC is the highest concentration that should not cause chronic effects, however, for regulatory purposes, a maximum concentration to protect against acute effects must exist as well.\n\nThe MATC can be applied to the results of an acute toxicity test to obtain a concentration that would protect against adverse effects during an acute exposure. An LC, or the concentration at which 50% of the organisms die during an acute toxicity test is used to derive a value called the acute to chronic ratio (ACR).\n\nThe MATC can be used to calculate the ACR as follows: formula_1\n\nThe ACR is useful for estimating an MATC for species in which only acute toxicity data exists, or for setting regulatory guidelines for the protection of aquatic life through water quality criteria by the US EPA.\n\nThe US EPA is the governmental organization responsible for writing and enforcing environmental regulations passed by Congress. The Clean Water Act was passed in 1972.\n\nSection 304(a)(1) of the Clean Water Act is the Water Quality Criteria (WQC) developed for the protection of aquatic life and human health. The MATC and ACR are used in a sequence of calculations to obtain the Criterion Maximum Concentration and Criterion Continuous Concentration (CMC and CCC, respectively) for the chemicals being regulated.\n\nThe CMC and CCC are two of the six parts of the aquatic life criterion under the WQC, and are the actual regulatory values for all priority pollutants tested. The CMC is the highest concentration of a chemical in water that aquatic organisms can be exposed to acutely without causing an adverse effect. Likewise, the CCC is the highest concentration of a chemical in water that aquatic organisms can be exposed to indefinitely without resulting in an adverse effect. Typically, the CMC is higher than the CCC.\n\nEnvironment Canada is the regulatory agency for environmental protection in Canada. Under the Canada Water Act of 1970, the Canadian Water Quality Guidelines (CWQG) for the Protection of Aquatic Life give regulatory guidelines of maximum concentrations of pollutants that are acceptable in freshwater and marine environments.\n\nThe CWQG's long and short-term exposure concentrations are derived in a similar way as the methods used by the US EPA, and are the CMC or CCC equivalents. When an MATC is reported with toxicity tests, it has sometimes been called a threshold-observed-effect-concentration (TOEC). The MATC and TOEC are both calculated as the geometric mean of the NOEC and LOEC, and are often used interchangeably.\n\nStandard methods are designed and used widely to maximize precision and accuracy for all toxicity tests. Values derived from toxicity tests such as the MATC are reported to regulatory organizations like the US EPA and Environment Canada so more confident regulations can be designed.\n\nSome common standard methods include those designed by the governmental organizations like Environment Canada, the US EPA, or the United States Food and Drug Administration (FDA). Others are designed by scientific organizations such as ASTM International, or the OECD.\n\nMany of these methods use the same test organism or are designed for the same exposure time. Common test organisms include, but are not limited to, daphnia, fathead minnow, rainbow trout, and mussels. Acute toxicity tests are normally 24–96 hours, whereas chronic tests will typically run for a week or longer.\n\nUsing the MATC to derive regulatory guidelines has been accompanied with some debate. Hypothesis testing, or statistical tests performed with data sets that only report a significant difference, are not considered the most statistically robust. There are no confidence intervals to show a measure of uncertainty in a NOEC and LOEC. In addition, the NOEC and LOEC can only be concentrations in the test, and nothing in between. Because of these reasons, values that are derived through curve fitting methods, such as an LC50, or EC10 (the concentration that causes the measured effect in 10% of organisms) would be preferred if it was possible more often.\n\nFrom a regulatory standpoint, there are advantages to using results from hypothesis tests. NOEC and LOEC’s were used more often in the past, and there are more test results reporting NOEC and LOEC’s than EC10’s. The time and effort required to perform all of the previous tests to derive a different value is not seen as a good use of resources. In addition, the use of NOEC and LOEC’s allows for reporting of one number to regulatory agencies. Water Quality Criteria are reported as one number that the actual concentration must remain below. If the MATC were reported as a range of values to account for uncertainty, the regulatory guidelines would not be presented as a single value.\n\n"}
{"id": "729264", "url": "https://en.wikipedia.org/wiki?curid=729264", "title": "Mercury-Redstone 1A", "text": "Mercury-Redstone 1A\n\nMercury-Redstone 1A (MR-1A) was launched on December 19, 1960 from LC-5 at Cape Canaveral, Florida. The mission objectives of this unmanned suborbital flight were to qualify the spacecraft for space flight and qualify the system for an upcoming primate suborbital flight. The spacecraft tested its instrumentation, posigrade rockets, retrorockets and recovery system. The mission was completely successful. The Mercury capsule reached an altitude of and a range of . The launch vehicle reached a slightly higher velocity than expected - . The Mercury spacecraft was recovered from the Atlantic Ocean by recovery helicopters about 15 minutes after landing. Serial numbers: Mercury Spacecraft #2 was reflown on MR-1A, together with the escape tower from Capsule #8 and the antenna fairing from Capsule #10. Redstone MRLV-3 was used. The flight time was 15 minutes and 45 seconds.\n\nMercury spacecraft #2, used in both the Mercury-Redstone 1 and Mercury-Redstone 1A missions, is currently displayed at the NASA Ames Exploration Center, Moffett Federal Airfield, near Mountain View, California.\n\n\n"}
{"id": "17174845", "url": "https://en.wikipedia.org/wiki?curid=17174845", "title": "MindModeling@Home", "text": "MindModeling@Home\n\nMindModeling@Home is a non-profit research project that uses a combination of high performance computing and volunteer distributed computing for the advancement of cognitive science. The research focuses on utilizing computational cognitive process modeling to understand the human mind better. The project aims to improve on the scientific foundations that explain the mechanisms and processes that enable and moderate human performance and learning. MindModeling@Home is hosted by Wright State University and the University of Dayton in Dayton, Ohio.\n\n\n"}
{"id": "52801011", "url": "https://en.wikipedia.org/wiki?curid=52801011", "title": "Normalized Difference Red Edge Index", "text": "Normalized Difference Red Edge Index\n\nThe normalized difference red edge index (NDRE) is a metric that can be used to analyse whether images obtained from multi-spectral image sensors contain healthy vegetation or not. It is similar to Normalized Difference Vegetation Index (NDVI) but uses the ratio of Near-Infrared and the edge of Red as follows:\n\nThe red edge is the term used to describe the part of the spectrum centred around 715 nm.\n\n"}
{"id": "16606924", "url": "https://en.wikipedia.org/wiki?curid=16606924", "title": "Operation Arbor", "text": "Operation Arbor\n\nThe United States's Arbor nuclear test series was a group of 18 nuclear tests conducted in 1973-1974. These tests followed the \"Operation Toggle\" series and preceded the \"Operation Bedrock\" series.\n"}
{"id": "17616269", "url": "https://en.wikipedia.org/wiki?curid=17616269", "title": "Our Synthetic Environment", "text": "Our Synthetic Environment\n\nOur Synthetic Environment is a 1962 book by Murray Bookchin, published under the pseudonym \"Lewis Herber\".\n\nBookchin warns of the dangers of pesticide use, and espouses an ecological and environmentalist worldview.\n\n\"Our Synthetic Environment\" was one of the first books of the modern period in which an author espoused an ecological and environmentalist worldview. It predates \"Silent Spring\" (1962) by Rachel Carson, a more widely known book on the same topic widely credited as starting the environmental movement.\n\n"}
{"id": "43595892", "url": "https://en.wikipedia.org/wiki?curid=43595892", "title": "POlarization Emission of Millimeter Activity at the Sun", "text": "POlarization Emission of Millimeter Activity at the Sun\n\nThe POlarization Emission of Millimeter Activity at the Sun (POEMAS) is a solar patrol system composed of two radio telescopes with superheterodyne circular polarization receivers at 45 and 90 GHz. Since their half power beam width is around 1.4°, they observe the full sun. The acquisition system allows to gather 100 values per second at both frequencies and polarizations, with a sensitivity of around 20 solar flux units (SFU) (1 SFU ≡ 10 Jy). The telescope saw first light in November 2011, and showed excellent performance during two years, when it observed many flares. Since November 2013 is stopped for repairing. The main interest of POEMAS is the observation of solar flares in a frequency range where there are very few detectors and fill the gap between microwaves observed with the Radio Solar Telescope Network (1 to 15.4 GHz) and submillimeter observations of the Solar Submillimeter Telescope (212 and 405 GHz). Moreover, POEMAS is the only current telescope capable of carrying on circular polarization solar flare observations at 90 GHz. (Although, in principle, ALMA band 3 may also observe at 90 GHz with circular polarization).\n\nPOEMAS was designed to \"fill the gap\" between microwaves and submillimeter (less than 1 mm) wavelength observations of the solar activity. There are a number of different instruments around the world that monitors the sun at microwaves. The Nobeyama Radio Heliograph (NoRH) (Nobeyama, Japan) makes daily maps at 17 GHz (1.7 cm) with circular left and right polarizations and 34 GHz (8.8 mm), total intensity. The Nobeyama Radio Polarimeters (NoRP), (Nobeyama, Japan) is a set of patrol telescopes with receivers from 1 GHz (λ≈30 cm) to 80 GHz (λ≈3.7 mm) at selected frequencies and circular polarization detection (except at 80 GHz) with full sun disk spatial resolution. The Radio Solar Telescope Network (RSTN) is worldwide network of telescopes with receivers at selected bands from few hundred MHz (λ≈75 cm) to 15.4 GHz (λ≈2 cm). At the other end of the band, the Solar Submillimeter Telescope (SST) installed at Complejo Astronomico El Leoncito in San Juan Province, Argentina observes the sun at 212 GHz (λ≈1.4 mm) and 405 GHz (λ≈0.7 mm). Since there is no observational time overlap between Japan and Argentina, the NoRH and NoRP cannot be used together with SST, and only data from some RSTN observatories have times in common with the SST.\n\nThe relevance of observing at 45 and 90 GHz comes from the necessity to determine the \"upturn\" frequency in the so-called \"THz events\": if the main radiation mechanism is synchrotron radiation from accelerated electrons emitting at chromospheric or coronal heights, it is expected a spectrum with a long and descending logarithmic tail towards millimeter wavelengths. In some cases this classical (\"textbook\") shape is broken and an \"upturn\" or \"spectral reversion\" is observed. Since the reversion or \"upturn\" frequency has been estimated around 50 to 100 GHz for the observed cases, the importance of POEMAS is justified.\n\n\n"}
{"id": "18409252", "url": "https://en.wikipedia.org/wiki?curid=18409252", "title": "Pfaffian constraint", "text": "Pfaffian constraint\n\nIn robot motion planning, a Pfaffian constraint is a set of \"k\" linearly independent constraints linear in velocity, i.e., of the form\n\nformula_1\n\nOne source of Pfaffian constraints is rolling without slipping in wheeled robots.\n"}
{"id": "520196", "url": "https://en.wikipedia.org/wiki?curid=520196", "title": "Positivity effect", "text": "Positivity effect\n\nIn psychology and cognitive science, the positivity effect is the ability to constructively analyze a situation where the desired results are not achieved; but still obtain positive feedback that assists our future progression. When a person is considering people they like (including themselves), the person tends to make situational attributions about their negative behaviors and dispositional attributions about their positive behaviors. The reverse may be true for people that the person dislikes. This may well be because of the dissonance between liking a person and seeing them behave negatively.\nExample: If a friend hits someone, one would tell them the other guy deserved it or that he had to defend himself.\n\nThe positivity effect pertains to the tendency of people, when evaluating the causes of the behaviors of a person they like or prefer, to attribute the person's inherent disposition as the cause of their positive behaviors and the situations surrounding them as the cause of their negative behaviors. The positivity effect is the inverse of the \"negativity effect\", which is found when people evaluate the causes of the behaviors of a person they dislike. Both effects are attributional biases.\n\nOn online social networks like Twitter, users prefer to share positive news, and are emotionally affected by positive news more than twice than by negative news.\n\n\n\n"}
{"id": "649539", "url": "https://en.wikipedia.org/wiki?curid=649539", "title": "Project Ozma", "text": "Project Ozma\n\nProject Ozma was a pioneering SETI experiment started in 1960 by Cornell University astronomer Frank Drake, at the National Radio Astronomy Observatory at Green Bank, West Virginia. The object of the experiment was to search for signs of life in distant planetary systems through interstellar radio waves. The program was named after Princess Ozma, ruler of the fictional land of Oz, inspired by L. Frank Baum's supposed communication with Oz by radio to learn of the events in the books taking place after \"The Emerald City of Oz\". The search was publicized in articles in the popular media of the time, such as \"Time\" magazine.\n\nDrake used a radio telescope with a diameter of to examine the stars Tau Ceti and Epsilon Eridani near the 1.420 gigahertz marker frequency. Both are nearby Sun-like stars that then seemed reasonably likely to have inhabited planets. A 400 kilohertz band was scanned around the marker frequency, using a single-channel receiver with a bandwidth of 100 hertz. The information was stored on tape for off-line analysis. Some 150 hours of intermittent observation during a four-month period detected no recognizable signals. A false signal was detected on April 8, 1960, but it was determined to have originated from a high-flying aircraft.\n\nThe receiver was tuned to wavelengths near 21 cm, which is the wavelength of radiation emitted naturally by interstellar hydrogen; it was thought that this would be familiar, as a kind of universal standard, to anyone attempting interstellar radio communication.\n\nA second experiment, called Ozma II, was conducted with a larger () telescope at the same observatory by Patrick Palmer and Benjamin Zuckerman, who intermittently monitored 670 nearby stars for about four years (1972–76). They examined a 10 MHz bandwidth with 52 kHz resolution and a 625 kHz bandwidth with 4 kHz resolution. The spectrometer was centered on the 21 cm hydrogen line in the rest frame of each observed star.\n\n"}
{"id": "23047", "url": "https://en.wikipedia.org/wiki?curid=23047", "title": "Pseudoscience", "text": "Pseudoscience\n\nPseudoscience consists of statements, beliefs, or practices that are claimed to be both scientific and factual, but are incompatible with the scientific method. Pseudoscience is often characterized by contradictory, exaggerated or unfalsifiable claims; reliance on confirmation bias rather than rigorous attempts at refutation; lack of openness to evaluation by other experts; and absence of systematic practices when developing theories, and continued adherence long after they have been experimentally discredited. The term \"pseudoscience\" is considered pejorative because it suggests something is being presented as science inaccurately or even deceptively. Those described as practicing or advocating pseudoscience often dispute the characterization.\n\nThe demarcation between science and pseudoscience has philosophical and scientific implications. Differentiating science from pseudoscience has practical implications in the case of health care, expert testimony, environmental policies, and science education. Distinguishing scientific facts and theories from pseudoscientific beliefs, such as those found in astrology, alchemy, alternative medicine, occult beliefs, and creation science, is part of science education and scientific literacy.\n\nPseudoscience can cause negative consequences in the real world. Antivaccine activists present pseudoscientific studies that falsely call into question the safety of vaccines. Homeopathic remedies with no active ingredients have been promoted as treatment for deadly diseases.\n\nThe word \"pseudoscience\" is derived from the Greek root \"pseudo\" meaning false and the English word \"science\", from the Latin word \"scientia\", meaning \"knowledge\". Although the term has been in use since at least the late 18th century (e.g. in 1796 by James Pettit Andrews in reference to alchemy) the concept of pseudoscience as distinct from real or proper science seems to have become more widespread during the mid-19th century. Among the earliest uses of \"pseudo-science\" was in an 1844 article in the \"Northern Journal of Medicine\", issue 387:\nAn earlier use of the term was in 1843 by the French physiologist François Magendie. During the 20th century, the word was used pejoratively to describe explanations of phenomena which were claimed to be scientific, but which were not in fact supported by reliable experimental evidence. From time-to-time, though, the usage of the word occurred in a more formal, technical manner in response to a perceived threat to individual and institutional security in a social and cultural setting.\n\nPhilosophers classify types of knowledge. In English, the word \"science\" is used to indicate specifically the natural sciences and related fields, which are called the social sciences.\n\nDifferent philosophers of science may disagree on the exact limits – for example, is mathematics a formal science that is closer to the empirical ones, or is pure mathematics closer to the philosophical study of logic and therefore not a science? – but all agree that all of the ideas that are not scientific are non-scientific. The large category of non-science includes all matters outside the natural and social sciences, such as the study of history, metaphysics, religion, art, and the humanities.\n\nDividing the category again, unscientific claims are a subset of the large category of non-scientific claims. This category specifically includes all matters that are directly opposed to good science. Un-science includes both bad science (such as an error made in a good-faith attempt at learning something about the natural world) and pseudoscience. Thus pseudoscience is a subset of un-science, and un-science, in turn, is subset of non-science.\n\nPseudoscience is differentiated from science because – although it claims to be science – pseudoscience does not adhere to accepted scientific standards, such as the scientific method, falsifiability of claims, and Mertonian norms.\n\nA number of basic principles are accepted by scientists as standards for determining whether a body of knowledge, method, or practice is scientific. Experimental results should be reproducible and verified by other researchers. These principles are intended to ensure experiments can be reproduced measurably given the same conditions, allowing further investigation to determine whether a hypothesis or theory related to given phenomena is valid and reliable. Standards require the scientific method to be applied throughout, and bias to be controlled for or eliminated through randomization, fair sampling procedures, blinding of studies, and other methods. All gathered data, including the experimental or environmental conditions, are expected to be documented for scrutiny and made available for peer review, allowing further experiments or studies to be conducted to confirm or falsify results. Statistical quantification of significance, confidence, and error are also important tools for the scientific method.\n\nDuring the mid-20th century, the philosopher Karl Popper emphasized the criterion of falsifiability to distinguish science from nonscience. Statements, hypotheses, or theories have falsifiability or refutability if there is the inherent possibility that they can be proven false. That is, if it is possible to conceive of an observation or an argument which negates them. Popper used astrology and psychoanalysis as examples of pseudoscience and Einstein's theory of relativity as an example of science. He subdivided nonscience into philosophical, mathematical, mythological, religious and metaphysical formulations on one hand, and pseudoscientific formulations on the other, though he did not provide clear criteria for the differences.\n\nAnother example which shows the distinct need for a claim to be falsifiable was stated in Carl Sagan's publication \"The Demon-Haunted World\" when he discusses an invisible dragon that he has in his garage. The point is made that there is no physical test to refute the claim of the presence of this dragon. No matter what test you think you can devise, there is then a reason why this does not apply to the invisible dragon, so one can never prove that the initial claim is wrong. Sagan concludes; \"Now, what's the difference between an invisible, incorporeal, floating dragon who spits heatless fire and no dragon at all?\". He states that \"your inability to invalidate my hypothesis is not at all the same thing as proving it true\", once again explaining that even if such a claim were true, it would be outside the realm of scientific inquiry.\n\nDuring 1942, Robert K. Merton identified a set of five \"norms\" which he characterized as what makes a real science. If any of the norms were violated, Merton considered the enterprise to be nonscience. These are not broadly accepted by the scientific community. His norms were:\n\nDuring 1978, Paul Thagard proposed that pseudoscience is primarily distinguishable from science when it is less progressive than alternative theories over a long period of time, and its proponents fail to acknowledge or address problems with the theory. During 1983, Mario Bunge has suggested the categories of \"belief fields\" and \"research fields\" to help distinguish between pseudoscience and science, where the former is primarily personal and subjective and the latter involves a certain systematic method.\n\nPhilosophers of science such as Paul Feyerabend argued that a distinction between science and nonscience is neither possible nor desirable. Among the issues which can make the distinction difficult is variable rates of evolution among the theories and methods of science in response to new data.\n\nLarry Laudan has suggested pseudoscience has no scientific meaning and is mostly used to describe our emotions: \"If we would stand up and be counted on the side of reason, we ought to drop terms like 'pseudo-science' and 'unscientific' from our vocabulary; they are just hollow phrases which do only emotive work for us\". Likewise, Richard McNally states, \"The term 'pseudoscience' has become little more than an inflammatory buzzword for quickly dismissing one's opponents in media sound-bites\" and \"When therapeutic entrepreneurs make claims on behalf of their interventions, we should not waste our time trying to determine whether their interventions qualify as pseudoscientific. Rather, we should ask them: How do you know that your intervention works? What is your evidence?\"\n\nFor philosophers Silvio Funtowicz and Jerome R. Ravetz \"pseudo-science may be defined as one where the uncertainty of its inputs must be suppressed, lest they render its outputs totally indeterminate\". The definition, in the book \"Uncertainty and quality in science for policy\" (p. 54), alludes to the loss of craft skills in handling quantitative information, and to the bad practice of achieving precision in prediction (inference) only at the expenses of ignoring uncertainty in the input which was used to formulate the prediction. This use of the term is common among practitioners of post-normal science. Understood in this way, pseudoscience can be fought using good practices to assesses uncertainty in quantitative information, such as NUSAP and – in the case of mathematical modelling – sensitivity auditing.\n\nThe history of pseudoscience is the study of pseudoscientific theories over time. A pseudoscience is a set of ideas that presents itself as science, while it does not meet the criteria to be properly called such.\n\nDistinguishing between proper science and pseudoscience is sometimes difficult. One proposal for demarcation between the two is the falsification criterion, attributed most notably to the philosopher Karl Popper. In the history of science and the history of pseudoscience it can be especially difficult to separate the two, because some sciences developed from pseudosciences. An example of this transformation is the science chemistry, which traces its origins to pseudoscientific or pre-scientific study of alchemy.\n\nThe vast diversity in pseudosciences further complicates the history of science. Some modern pseudosciences, such as astrology and acupuncture, originated before the scientific era. Others developed as part of an ideology, such as Lysenkoism, or as a response to perceived threats to an ideology. Examples of this ideological process are creation science and intelligent design, which were developed in response to the scientific theory of evolution.\n\nA topic, practice, or body of knowledge might reasonably be termed pseudoscientific when it is presented as consistent with the norms of scientific research, but it demonstrably fails to meet these norms.\n\nKarl Popper stated it is insufficient to distinguish science from pseudoscience, or from metaphysics (such as the philosophical question of what existence means), by the criterion of rigorous adherence to the empirical method, which is essentially inductive, based on observation or experimentation. He proposed a method to distinguish between genuine empirical, nonempirical or even pseudoempirical methods. The latter case was exemplified by astrology, which appeals to observation and experimentation. While it had astonishing empirical evidence based on observation, on horoscopes and biographies, it crucially failed to use acceptable scientific standards. Popper proposed falsifiability as an important criterion in distinguishing science from pseudoscience.\n\nTo demonstrate this point, Popper gave two cases of human behavior and typical explanations from Sigmund Freud and Alfred Adler's theories: \"that of a man who pushes a child into the water with the intention of drowning it; and that of a man who sacrifices his life in an attempt to save the child.\" From Freud's perspective, the first man would have suffered from psychological repression, probably originating from an Oedipus complex, whereas the second man had attained sublimation. From Adler's perspective, the first and second man suffered from feelings of inferiority and had to prove himself, which drove him to commit the crime or, in the second case, drove him to rescue the child. Popper was not able to find any counterexamples of human behavior in which the behavior could not be explained in the terms of Adler's or Freud's theory. Popper argued it was that the observation always fitted or confirmed the theory which, rather than being its strength, was actually its weakness.\n\nIn contrast, Popper gave the example of Einstein's gravitational theory, which predicted \"light must be attracted by heavy bodies (such as the Sun), precisely as material bodies were attracted.\" Following from this, stars closer to the Sun would appear to have moved a small distance away from the Sun, and away from each other. This prediction was particularly striking to Popper because it involved considerable risk. The brightness of the Sun prevented this effect from being observed under normal circumstances, so photographs had to be taken during an eclipse and compared to photographs taken at night. Popper states, \"If observation shows that the predicted effect is definitely absent, then the theory is simply refuted.\" Popper summed up his criterion for the scientific status of a theory as depending on its falsifiability, refutability, or testability.\n\nPaul R. Thagard used astrology as a case study to distinguish science from pseudoscience and proposed principles and criteria to delineate them. First, astrology has not progressed in that it has not been updated nor added any explanatory power since Ptolemy. Second, it has ignored outstanding problems such as the precession of equinoxes in astronomy. Third, alternative theories of personality and behavior have grown progressively to encompass explanations of phenomena which astrology statically attributes to heavenly forces. Fourth, astrologers have remained uninterested in furthering the theory to deal with outstanding problems or in critically evaluating the theory in relation to other theories. Thagard intended this criterion to be extended to areas other than astrology. He believed it would delineate as pseudoscientific such practices as witchcraft and pyramidology, while leaving physics, chemistry and biology in the realm of science. Biorhythms, which like astrology relied uncritically on birth dates, did not meet the criterion of pseudoscience at the time because there were no alternative explanations for the same observations. The use of this criterion has the consequence that a theory can be scientific at one time and pseudoscientific at a later time.\n\nScience is also distinguishable from revelation, theology, or spirituality in that it offers insight into the physical world obtained by empirical research and testing. The most notable disputes concern the evolution of living organisms, the idea of common descent, the geologic history of the Earth, the formation of the solar system, and the origin of the universe. Systems of belief that derive from divine or inspired knowledge are not considered pseudoscience if they do not claim either to be scientific or to overturn well-established science. Moreover, some specific religious claims, such as the power of intercessory prayer to heal the sick, although they may be based on untestable beliefs, can be tested by the scientific method.\n\nSome statements and common beliefs of popular science may not meet the criteria of science. \"Pop\" science may blur the divide between science and pseudoscience among the general public, and may also involve science fiction. Indeed, pop science is disseminated to, and can also easily emanate from, persons not accountable to scientific methodology and expert peer review.\n\nIf the claims of a given field can be tested experimentally and standards are upheld, it is not pseudoscience, however odd, astonishing, or counterintuitive the claims are. If claims made are inconsistent with existing experimental results or established theory, but the method is sound, caution should be used, since science consists of testing hypotheses which may turn out to be false. In such a case, the work may be better described as ideas that are \"not yet generally accepted\". \"Protoscience\" is a term sometimes used to describe a hypothesis that has not yet been tested adequately by the scientific method, but which is otherwise consistent with existing science or which, where inconsistent, offers reasonable account of the inconsistency. It may also describe the transition from a body of practical knowledge into a scientific field.\n\n\n\n\n\n\n\nA large percentage of the United States population lacks scientific literacy, not adequately understanding scientific principles and method. In the \"Journal of College Science Teaching\", Art Hobson writes, \"Pseudoscientific beliefs are surprisingly widespread in our culture even among public school science teachers and newspaper editors, and are closely related to scientific illiteracy.\" However, a 10,000-student study in the same journal concluded there was no strong correlation between science knowledge and belief in pseudoscience.\n\nIn his book \"The Demon-Haunted World\" Carl Sagan discusses the government of China and the Chinese Communist Party's concern about Western pseudoscience developments and certain ancient Chinese practices in China. He sees pseudoscience occurring in the United States as part of a worldwide trend and suggests its causes, dangers, diagnosis and treatment may be universal.\n\nDuring 2006, the U.S. National Science Foundation (NSF) issued an executive summary of a paper on science and engineering which briefly discussed the prevalence of pseudoscience in modern times. It said, \"belief in pseudoscience is widespread\" and, referencing a Gallup Poll, stated that belief in the 10 commonly believed examples of paranormal phenomena listed in the poll were \"pseudoscientific beliefs\".\n\nThe items were \"extrasensory perception (ESP), that houses can be haunted, ghosts, telepathy, clairvoyance, astrology, that people can communicate mentally with someone who has died, witches, reincarnation, and channelling\". Such beliefs in pseudoscience represent a lack of knowledge of how science works. The scientific community may attempt to communicate information about science out of concern for the public's susceptibility to unproven claims.\n\nThe National Science Foundation stated that pseudoscientific beliefs in the U.S. became more widespread during the 1990s, peaked about 2001, and then decreased slightly since with pseudoscientific beliefs remaining common. According to the NSF report, there is a lack of knowledge of pseudoscientific issues in society and pseudoscientific practices are commonly followed. Surveys indicate about a third of all adult Americans consider astrology to be scientific.\n\nIn a report Singer and Benassi (1981) wrote that pseudoscientific beliefs have their origin from at least four sources.\n\nAnother American study (Eve and Dunn, 1990) supported the findings of Singer and Benassi and found pseudoscientific belief being promoted by high school life science and biology teachers.\n\nThe psychology of pseudoscience attempts to explore and analyze pseudoscientific thinking by means of thorough clarification on making the distinction of what is considered scientific vs. pseudoscientific. The human proclivity for seeking confirmation rather than refutation (confirmation bias), the tendency to hold comforting beliefs, and the tendency to overgeneralize have been proposed as reasons for pseudoscientific thinking. According to Beyerstein (1991), humans are prone to associations based on resemblances only, and often prone to misattribution in cause-effect thinking.\n\nMichael Shermer's theory of belief-dependent realism is driven by the belief that the brain is essentially a \"belief engine,\" which scans data perceived by the senses and looks for patterns and meaning. There is also the tendency for the brain to create cognitive biases, as a result of inferences and assumptions made without logic and based on instinct — usually resulting in patterns in cognition. These tendencies of patternicity and agenticity are also driven \"by a meta-bias called the bias blind spot, or the tendency to recognize the power of cognitive biases in other people but to be blind to their influence on our own beliefs.\"\nLindeman states that social motives (i.e., \"to comprehend self and the world, to have a sense of control over outcomes, to belong, to find the world benevolent and to maintain one's self-esteem\") are often \"more easily\" fulfilled by pseudoscience than by scientific information. Furthermore, pseudoscientific explanations are generally not analyzed rationally, but instead experientially. Operating within a different set of rules compared to rational thinking, experiential thinking regards an explanation as valid if the explanation is \"personally functional, satisfying and sufficient\", offering a description of the world that may be more personal than can be provided by science and reducing the amount of potential work involved in understanding complex events and outcomes.\n\nThere is a trend to believe in pseudoscience more than scientific evidence. Some people believe the prevalence of pseudoscientific beliefs is due to widespread \"scientific illiteracy\". Individuals lacking scientific literacy are more susceptible to wishful thinking, since they are likely to turn to immediate gratification powered by System 1, our default operating system which requires little to no effort. This system encourages one to accept the conclusions they believe, and reject the ones they do not. Further analysis of complex pseudoscientific phenomena require System 2, which follows rules, compares objects along multiple dimensions and weighs options. These two systems have several other differences which are further discussed in the dual-process theory. The scientific and secular systems of morality and meaning are generally unsatisfying to most people. Humans are, by nature, a forward-minded species pursuing greater avenues of happiness and satisfaction, but we are all too frequently willing to grasp at unrealistic promises of a better life.\n\nPsychology has much to discuss about pseudoscience thinking, as it is the illusory perceptions of causality and effectiveness of numerous individuals that needs to be illuminated. Research suggests that illusionary thinking happens in most people when exposed to certain circumstances such as reading a book, an advertisement or the testimony of others are the basis of pseudoscience beliefs. It is assumed that illusions are not unusual, and given the right conditions, illusions are able to occur systematically even in normal emotional situations. One of the things pseudoscience believers quibble most about is that academic science usually treats them as fools. Minimizing these illusions in the real world is not simple. To this aim, designing evidence-based educational programs can be effective to help people identify and reduce their own illusions.\n\nIn the philosophy and history of science, Imre Lakatos stresses the social and political importance of the demarcation problem, the normative methodological problem of distinguishing between science and pseudoscience. His distinctive historical analysis of scientific methodology based on research programmes suggests: \"scientists regard the successful theoretical prediction of stunning novel facts – such as the return of Halley's comet or the gravitational bending of light rays – as what demarcates good scientific theories from pseudo-scientific and degenerate theories, and in spite of all scientific theories being forever confronted by 'an ocean of counterexamples'\". Lakatos offers a \"novel fallibilist analysis of the development of Newton's celestial dynamics, [his] favourite historical example of his methodology\" and argues in light of this historical turn, that his account answers for certain inadequacies in those of Karl Popper and Thomas Kuhn. \"Nonetheless, Lakatos did recognize the force of Kuhn's historical criticism of Popper – all important theories have been surrounded by an 'ocean of anomalies', which on a falsificationist view would require the rejection of the theory outright... Lakatos sought to reconcile the rationalism of Popperian falsificationism with what seemed to be its own refutation by history\".\n\nThe boundary between science and pseudoscience is disputed and difficult to determine analytically, even after more than a century of study by philosophers of science and scientists, and despite some basic agreements on the fundamentals of the scientific method. The concept of pseudoscience rests on an understanding that the scientific method has been misrepresented or misapplied with respect to a given theory, but many philosophers of science maintain that different kinds of methods are held as appropriate across different fields and different eras of human history. According to Lakatos, the typical descriptive unit of great scientific achievements is not an isolated hypothesis but \"a powerful problem-solving machinery, which, with the help of sophisticated mathematical techniques, digests anomalies and even turns them into positive evidence.\"\n\nThe demarcation problem between science and pseudoscience brings up debate in the realms of science, philosophy and politics. Imre Lakatos, for instance, points out that the Communist Party of the Soviet Union at one point declared that Mendelian genetics was pseudoscientific and had its advocates, including well-established scientists such as Nikolai Vavilov, sent to a Gulag and that the \"liberal Establishment of the West\" denies freedom of speech to topics it regards as pseudoscience, particularly where they run up against social mores.\n\nIt becomes pseudoscientific when science cannot be separated from ideology, scientists misrepresent scientific findings to promote or draw attention for publicity, when politicians, journalists and a nation's intellectual elite distort the facts of science for short-term political gain, or when powerful individuals of the public conflate causation and cofactors by clever wordplay. These ideas reduce the authority, value, integrity and independence of science in society.\n\nDistinguishing science from pseudoscience has practical implications in the case of health care, expert testimony, environmental policies, and science education. Treatments with a patina of scientific authority which have not actually been subjected to actual scientific testing may be ineffective, expensive and dangerous to patients and confuse health providers, insurers, government decision makers and the public as to what treatments are appropriate. Claims advanced by pseudoscience may result in government officials and educators making bad decisions in selecting curricula.\n\nThe extent to which students acquire a range of social and cognitive thinking skills related to the proper usage of science and technology determines whether they are scientifically literate. Education in the sciences encounters new dimensions with the changing landscape of science and technology, a fast-changing culture and a knowledge-driven era. A reinvention of the school science curriculum is one that shapes students to contend with its changing influence on human welfare. Scientific literacy, which allows a person to distinguish science from pseudosciences such as astrology, is among the attributes that enable students to adapt to the changing world. Its characteristics are embedded in a curriculum where students are engaged in resolving problems, conducting investigations, or developing projects.\n\nFriedman mentions why most scientists avoid educating about pseudoscience, including that paying undue attention to pseudoscience could dignify it. On the other hand, Park emphasizes how pseudoscience can be a threat to society and considers that scientists have a responsibility to teach how to distinguish science from pseudoscience.\n\nPseudosciences such as homeopathy, even if generally benign, are used by charlatans. This poses a serious issue because it enables incompetent practitioners to administer health care. True-believing zealots may pose a more serious threat than typical con men because of their affection to homeopathy's ideology. Irrational health care is not harmless and it is careless to create patient confidence in pseudomedicine.\n\nOn December 8, 2016, Michael V. LeVine, writing in \"Business Insider\", pointed out the dangers posed by the \"Natural News\" website: \"Snake-oil salesmen have pushed false cures since the dawn of medicine, and now websites like \"Natural News\" flood social media with dangerous anti-pharmaceutical, anti-vaccination and anti-GMO pseudoscience that puts millions at risk of contracting preventable illnesses.\"\n\n\n\n\n"}
{"id": "56005999", "url": "https://en.wikipedia.org/wiki?curid=56005999", "title": "SHERPA/Juliet", "text": "SHERPA/Juliet\n\nSHERPA/Juliet is an online database of open access mandates adopted by academic funding bodies.\nIt is part of the SHERPA suite of services around open access and is run by Jisc (formerly the University of Nottingham).\n\nThe database contains information about more than 100 funders, mostly from the United Kingdom. For each of them, Juliet indicates their policy regarding self-archiving, open access journals and archival of research data. Users can suggest updates to the records or the addition of a new funder via a form.\n\nThis service is mainly useful to researchers who have received project-based funding and want a clear summary of their funder. Links to the original policies are also provided.\n\n\n"}
{"id": "32449181", "url": "https://en.wikipedia.org/wiki?curid=32449181", "title": "School of Naturalists", "text": "School of Naturalists\n\nThe School of Naturalists or the School of Yin-yang (陰陽家/阴阳家; \"Yīnyángjiā\"; \"Yin-yang-chia\"; \"School of Yin-Yang\") was a Warring States era philosophy that synthesized the concepts of yin-yang and the Five Elements.\n\nZou Yan is considered the founder of this school. His theory attempted to explain the universe in terms of basic forces in nature: the complementary agents of yin (dark, cold, female, negative) and yang (light, hot, male, positive) and the Five Elements or Five Phases (water, fire, wood, metal, and earth). In its early days, this theory was most strongly associated with the states of Yan and Qi. In later periods, these epistemological theories came to hold significance in both philosophy and popular belief. This school was absorbed into the alchemic and magical dimensions of Taoism as well as into the Chinese medical framework. The earliest surviving recordings of this are in the Ma Wang Dui texts and Huang Di Nei Jing.\n\nZou Yan (; 305240 BC) was an ancient Chinese philosopher best known as the representative thinker of the Yin and Yang School (or School of Naturalists) during the Hundred Schools of Thought era in Chinese philosophy. Zou Yan was a noted scholar of the Jixia Academy in the state of Qi. Joseph Needham, a British sinologist, describes Zou as \"The real founder of all Chinese scientific thought.\" His teachings combined and systematized two current theories during the Warring States period: Yin-Yang and the Five Elements/Phases (wood, fire, earth, metal, and water).\n\nDuring the Han dynasty, the concepts of the school were integrated into Confucian ideology, Zhang Cang (253-152 BCE) and Dong Zhongshu (179-104 BCE) being the chief instrumental figures behind this process.\n"}
{"id": "13561780", "url": "https://en.wikipedia.org/wiki?curid=13561780", "title": "Seychelles sheath-tailed bat", "text": "Seychelles sheath-tailed bat\n\nThe Seychelles sheath-tailed bat (\"Coleura seychellensis\") is a sac-winged bat found in the central granitic islands of the Seychelles. It is an insectivorous bat, feeding primarily in forest clearings at night and roosting in communal roosts by day. Although previously abundant across the island group, it now only occurs on three islands. Its numbers have been declining to such an extent that the International Union for Conservation of Nature has listed it as being critically endangered. Increases in the cultivation of coconut palms in plantations, and the introduction of alien plant species, seem to have reduced the availability of insect food.\n\nThe weight of Seychelles sheath-tailed bats averages about . Bats in this genus generally roost in caves and houses, in crevices and cracks. In the 1860s, the Seychelles sheath-tailed bat was reported to fly around clumps of bamboo towards twilight, and in the daytime to be found roosting in the clefts of the mountainside facing the sea and with a more or less northern aspect. These hiding places were generally covered over with the large fronds of endemic palms. The Seychelles sheath-tailed bat is insectivorous. Its colonies are apparently divided into harem groups.\n\nIt has been the focus of recent intensive research, which has determined that it is a species associated with small clearings in forest where it feeds on a wide variety of insect species. Observations of coastal or marsh feeding are thought to be bats that have been forced into feeding in unusual situations due to habitat deterioration. Although the species is not a specialist and has a high reproductive potential, it is very vulnerable to disturbance and requires several roost sites within healthy habitat.\n\nIt was probably abundant throughout the Seychelles in the past, but it has declined drastically and is now extinct on most islands. The International Union for Conservation of Nature lists this bat as being critically endangered. In 2013, Bat Conservation International listed this species as one of the 35 species of its worldwide priority list of conservation.\n\nIt is one of the most endangered animals, fewer than 100 are believed to exist in the world. The Seychelles sheath-tailed bat has suffered from habitat deterioration due to the effects of cultivation of coconut plantations and the introduction of the kudzu vine, both of which have reduced the incidence of scrub and the availability of insect prey. The largest surviving roost is on Silhouette Island, although small roosts do exist in Mahé and also Praslin and La Digue islands. Its lifespan is 20 years; its length is . It finds its mates by fighting with another male bat in front of the females.\n\n\n"}
{"id": "35835958", "url": "https://en.wikipedia.org/wiki?curid=35835958", "title": "WISE 2220−3628", "text": "WISE 2220−3628\n\nWISE J222055.31−362817.4 (designation abbreviated to WISE 2220−3628) is a brown dwarf of spectral class Y0, located in constellation Grus at approximately 26 light-years from Earth.\n\nWISE 2220−3628 was discovered in 2012 by J. Davy Kirkpatrick et al. from data, collected by Wide-field Infrared Survey Explorer (WISE) Earth-orbiting satellite — NASA infrared-wavelength 40 cm (16 in) space telescope, which mission lasted from December 2009 to February 2011. In 2012 Kirkpatrick et al. published a paper in The Astrophysical Journal, where they presented discovery of seven new found by WISE brown dwarfs of spectral type Y, among which also was WISE 2220−3628.\n\nY-class dwarfs are among the coldest of all brown dwarfs.\n\nTrigonometric parallax of WISE 2220−3628 is not yet measured. Therefore, there are only distance estimates of this object, obtained by indirect — spectrofotometric — means (see table).\n\n"}
{"id": "58853920", "url": "https://en.wikipedia.org/wiki?curid=58853920", "title": "Whendee Silver", "text": "Whendee Silver\n\nWhendee Silver is an American ecosystem ecologist and biochemist.\n\nSilver grew up in Southern California. She earned her MS in Forest Science from Yale School of Forestry in 1987 and in 1992, received her PhD from Yale University.\n\nSilver is a professor of ecosystem ecology at University of California, Berkeley. With a focus on ecosystem ecology and biogeochemistry, her research is often aimed at better understanding the soil system to mitigate the effects of climate change. A significant portion of her work has focused on tropical ecosystems, their soils, plants, and how nutrients and carbon cycle through them. \n\nSilver is the lead scientist at the Marin Carbon Project, which she helped found in 2008. The Marin Carbon Project uses science to improve land management, to think about the whole system and thus consider and value ecosystem services such as soil's C sequestration ability, and make farm and ranch management more centered around n carbon-sequestration.Through this project she is working with ranchers, using compost for carbon sequestration on ranch land in California, greatly improving the soil's ability to sequester carbon. \n\n\nSilver's research on the biogeochemistry of tropical plants has been published in multiple academic journals. Silver's research was featured in the book \"Physiological Ecology of Tropical Plants\" by Ulrich Lüttge. Silver has over 145 publications as of 2018.\n\n"}
{"id": "51100996", "url": "https://en.wikipedia.org/wiki?curid=51100996", "title": "Workers Development Union", "text": "Workers Development Union\n\nWorkers Development Union (\"Shramik Abhivrudhi Sangh\") is the social action wing of the Goa Jesuits, with activities concentrated in Belgaum and other districts of north Karnataka and in the Kolhapur district of Maharashtra. SAS works among the masses to transform unjust structures of society and to build harmonious communities of diverse peoples, religions, languages, and cultures, helping them to satisfy their basic human needs.\n\nTwo Jesuits and forty social activists along with many part-time animators facilitate the programs listed below, sharing with the poor their struggles, joys, and hopes. This also join in critical reflection and action on the socio-political and economic situation.\n\nThe Union has a shepherd training program in sheep care, modern medicines, and modern breeding practices, carried on in the largely pastoral northern districts of Karnataka.\n\n \n"}
