{"id": "49292835", "url": "https://en.wikipedia.org/wiki?curid=49292835", "title": "Andrew Phillip Brown", "text": "Andrew Phillip Brown\n\nAndrew Phillip Brown (born 1951) is a conservation biologist and taxonomist at the Western Australian Department of Environment and Conservation. He is also curator of Orchidaceae and Myoporaceae at the Western Australian Herbarium and a foundation member of the Australian Orchid Foundation and the Western Australia Native Orchid Study and Conservation Group. He is the author of more than 100 journal articles and seven books on the flora of Western Australia including a field guide to the eremophilas of that state.\n"}
{"id": "17970498", "url": "https://en.wikipedia.org/wiki?curid=17970498", "title": "Biofact (archaeology)", "text": "Biofact (archaeology)\n\nIn archaeology, a biofact (or ecofact) is organic material found at an archaeological site that carries archaeological significance. Biofacts are natural objects found alongside artifacts or features, such as animal bones, charcoal, plants, and pollen. Biofacts are passively consumed or handled by humans; as opposed to artefacts, which are purposefully manipulated. Biofacts reveal how people respond to their surroundings.\n\nA common type of biofact is a [plant] [seed]. Plant remains, often referred to as macrobotanicals, provide a variety of information ranging from diet to medicine to textile production. Pollen preserved on archaeological sites informs researchers about the ancient environment, and the foods processed and/or grown by prehistoric people. Pollen, when examined over time, also informs on environmental and dietary changes. A seed can be linked to the species of plant that produced it; if massive numbers of seeds of a cultivated species are found at a site, it may be inferred that the species may have been grown for food or other products that are useful to humans, such as clothing, bedding or building materials.\n\nAnother type of biofact is wood. Wood is made up cellulose, carbohydrates, and lignin. Every year that passes, a new ring is added to the trunk of tree, allowing for dendrochronological dating. Charcoal is burned wood that archaeologist are able to extract. It can be dated using carbon-14, and through other methods, information such as local environment and human adaptation can be revealed from the charcoal. To help determine the date during which a site was occupied, dendrochronological analysis can be used on wood samples. Wood that has been altered by humans is properly an artifact, not a biofact.\n\n"}
{"id": "19602006", "url": "https://en.wikipedia.org/wiki?curid=19602006", "title": "Bunyamwera virus", "text": "Bunyamwera virus\n\nThe Bunyamwera virus (BUNV) is a negative-sense, single-stranded enveloped RNA virus. It is the type species of the \"Orthobunyavirus\" genus, in the \"Bunyavirales\" order.\n\nBunyamwera virus can infect both humans and \"Aedes aegypti\" (yellow fever mosquito).\n\nIt is named for Bunyamwera, a town in western Uganda, where the type species was isolated in 1943. Reassortant viruses derived from Bunyamwera virus, such as \"Ngari virus\", which has been associated with large outbreaks of viral haemorrhagic fever in Kenya and Somalia.\n\nThe genetic structure of \"Bunyamwera virus\" is typical for \"Bunyavirales\" viruses, which are a family of enveloped negative-sense, single-stranded RNA viruses with a genome split into three parts—Small (S), Middle (M), and Large (L). The L RNA segment encodes an RNA-dependent RNA polymerase (L protein), the M RNA segment encodes two surface glycoproteins (Gc and Gn) and a nonstructural protein (NSm), while the S RNA segment encodes a nucleocapsid protein (N) and, in an alternative overlapping reading frame, a second nonstructural protein (NSs). The genomic RNA segments are encapsidated by copies of the N protein in the form of ribonucleoprotein (RNP) complexes. The N protein is the most abundant protein in virus particles and infected cells and, therefore, the main target in many serological and molecular diagnostics.\n\nIn humans, \"Bunyamwera virus\" causes Bunyamwera fever.\n"}
{"id": "102858", "url": "https://en.wikipedia.org/wiki?curid=102858", "title": "Cell theory", "text": "Cell theory\n\nIn biology, cell theory is the historic scientific theory, now universally accepted, that living organisms are made up of cells, that they are the basic structural/organizational unit of all organisms, and that all cells come from pre-existing cells. Cells are the basic unit of structure in all organisms and also the basic unit of reproduction. With continual improvements made to microscopes over time, magnification technology advanced enough to discover cells in the 17th century. This discovery is largely attributed to Robert Hooke, and began the scientific study of cells, also known as cell biology. Over a century later, many debates about cells began amongst scientists. Most of these debates involved the nature of cellular regeneration, and the idea of cells as a fundamental unit of life. Cell theory was eventually formulated in 1839. This is usually credited to Matthias Schleiden and Theodor Schwann. However, many other scientists like Rudolf Virchow contributed to the theory. It was an important step in the movement away from spontaneous generation.\n\nThe three tenets to the cell theory are as described below:\n\nThe first of these tenets is disputed, as non-cellular entities such as viruses are sometimes considered life-forms.\n\nThe discovery of the cell was made possible through the invention of the microscope. In the first century BC, Romans were able to make glass, discovering that objects appeared to be larger under the glass. In Italy during the 12th century, Salvino D’Armate made a piece of glass fit over one eye, allowing for a magnification effect to that eye. The expanded use of lenses in eyeglasses in the 13th century probably led to wider spread use of simple microscopes (magnifying glasses) with limited magnification. Compound microscope, which combine an objective lens with an eyepiece to view a real image achieving much higher magnification, first appeared in Europe around 1620 In 1665, Robert Hooke used a microscope about six inches long with two convex lenses inside and examined specimens under reflected light for the observations in his book \"Micrographia\". Hooke also used a simpler microscope with a single lens for examining specimens with directly transmitted light, because this allowed for a clearer image.\n\nExtensive microscopic study was done by Anton van Leeuwenhoek, a draper who took the interest in microscopes after seeing one while on an apprenticeship in Amsterdam in 1648. At some point in his life before 1668, he was able to learn how to grind lenses. This eventually led to Leeuwenhoek making his own unique microscope. His were a single lens simple microscope, rather than a compound microscope. This was because he was able to use a single lens that was a small glass sphere but allowed for a magnification of 270x. This was a large progression since the magnification before was only a maximum of 50x. After Leeuwenhoek, there was not much progress for the microscopes until the 1850s, two hundred years later. Carl Zeiss, a German engineer who manufactured microscopes, began to make changes to the lenses used. But the optical quality did not improve until the 1880s when he hired Otto Schott and eventually Ernst Abbe.\n\nOptical microscopes can focus on objects the size of a wavelength or larger, giving restrictions still to advancement in discoveries with objects smaller than the wavelengths of visible light. Later in the 1920s, the electron microscope was developed, making it possible to view objects that are smaller than optical wavelengths, once again, changing the possibilities in science.\n\nThe cell was first discovered by Robert Hooke in 1665, which can be found to be described in his book Micrographia. In this book, he gave 60 ‘observations’ in detail of various objects under a coarse, compound microscope. One observation was from very thin slices of bottle cork. Hooke discovered a multitude of tiny pores that he named \"cells\". This came from the Latin word Cella, meaning ‘a small room’ like monks lived in and also Cellulae, which meant the six sided cell of a honeycomb. However, Hooke did not know their real structure or function. \nWhat Hooke had thought were cells, were actually empty cell walls of plant tissues. With microscopes during this time having a low magnification, Hooke was unable to see that there were other internal components to the cells he was observing. Therefore, he did not think the \"cellulae\" were alive. His cell observations gave no indication of the nucleus and other organelles found in most living cells. In Micrographia, Hooke also observed mould, bluish in color, found on leather. After studying it under his microscope, he was unable to observe “seeds” that would have indicated how the mould was multiplying in quantity. This led to Hooke suggesting that spontaneous generation, from either natural or artificial heat, was the cause. Since this was an old Aristotelian theory still accepted at the time, others did not reject it and was not disproved until Leeuwenhoek later discovers generation is achieved otherwise.\n\nAnton van Leeuwenhoek is another scientist who saw these cells soon after Hooke did. He made use of a microscope containing improved lenses that could magnify objects almost 300-fold, or 270x. Under these microscopes, Leeuwenhoek found motile objects. In a letter to The Royal Society on October 9, 1676, he states that motility is a quality of life therefore these were living organisms. Over time, he wrote many more papers in which described many specific forms of microorganisms. Leeuwenhoek named these “animalcules,” which included protozoa and other unicellular organisms, like bacteria. Though he did not have much formal education, he was able to identify the first accurate description of red blood cells and discovered bacteria after gaining interest in the sense of taste that resulted in Leeuwenhoek to observe the tongue of an ox, then leading him to study \"pepper water\" in 1676. He also found for the first time the sperm cells of animals and humans. Once discovering these types of cells, Leeuwenhoek saw that the fertilization process requires the sperm cell to enter the egg cell. This put an end to the previous theory of spontaneous generation. After reading letters by Leeuwenhoek, Hooke was the first to confirm his observations that were thought to be unlikely by other contemporaries.\n\nThe cells in animal tissues were observed after plants were because the tissues were so fragile and susceptible to tearing, it was difficult for such thin slices to be prepared for studying. Biologists believed that there was a fundamental unit to life, but were unsure what this was. It would not be until over a hundred years later that this fundamental unit was connected to cellular structure and existence of cells in animals or plants. This conclusion was not made until Henri Dutrochet. Besides stating “the cell is the fundamental element of organization”, Dutrochet also claimed that cells were not just a structural unit, but also a physiological unit.\n\nIn 1804, Karl Rudolphi and J.H.F. Link were awarded the prize for \"solving the problem of the nature of cells\", meaning they were the first to prove that cells had independent cell walls by the Königliche Societät der Wissenschaft (Royal Society of Science), Göttingen. Before, it had been thought that cells shared walls and the fluid passed between them this way.\n\nCredit for developing cell theory is usually given to two scientists: Theodor Schwann and Matthias Jakob Schleiden. While Rudolf Virchow contributed to the theory, he is not as credited for his attributions toward it. In 1839, Schleiden suggested that every structural part of a plant was made up of cells or the result of cells. He also suggested that cells were made by a crystallization process either within other cells or from the outside. However, this was not an original idea of Schleiden. He claimed this theory as his own, though Barthelemy Dumortier had stated it years before him. This crystallization process is no longer accepted with modern cell theory. In 1839, Theodor Schwann states that along with plants, animals are composed of cells or the product of cells in their structures. This was a major advancement in the field of biology since little was known about animal structure up to this point compared to plants. From these conclusions about plants and animals, two of the three tenets of cell theory were postulated.\n\n1. All living organisms are composed of one or more cells\n\n2. The cell is the most basic unit of life\n\nSchleiden's theory of free cell formation through crystallization was refuted in the 1850s by Robert Remak, Rudolf Virchow, and Albert Kolliker. In 1855, Rudolf Virchow added the third tenet to cell theory. In Latin, this tenet states \"Omnis cellula e cellula\". This translated to:\n\n3. All cells arise only from pre-existing cells\n\nHowever, the idea that all cells come from pre-existing cells had in fact already been proposed by Robert Remak; it has been suggested that Virchow plagiarized Remak and did not give him credit. Remak published observations in 1852 on cell division, claiming Schleiden and Schawnn were incorrect about generation schemes. He instead said that binary fission, which was first introduced by Dumortier, was how reproduction of new animal cells were made. Once this tenet was added, the classical cell theory was complete.\n\nThe generally accepted parts of modern cell theory include:\n\n\nThe modern version of the cell theory includes the ideas that:\n\nThe cell was first discovered by Robert Hooke in 1665 using a microscope. The first cell theory is credited to the work of Theodor Schwann and Matthias Jakob Schleiden in the 1830s. In this theory the internal contents of cells were called protoplasm and described as a jelly-like substance, sometimes called living jelly. At about the same time, colloidal chemistry began its development, and the concepts of bound water emerged. A colloid being something between a solution and a suspension, where Brownian motion is sufficient to prevent sedimentation.\nThe idea of a semipermeable membrane, a barrier that is permeable to solvent but impermeable to solute molecules was developed at about the same time. The term osmosis originated in 1827 and its importance to physiological phenomena realized, but it wasn’t until 1877, when the botanist Pfeffer proposed the membrane theory of cell physiology. In this view, the cell was seen to be enclosed by a thin surface, the plasma membrane, and cell water and solutes such as a potassium ion existed in a physical state like that of a dilute solution. In 1889 Hamburger used hemolysis of erythrocytes to determine the permeability of various solutes. By measuring the time required for the cells to swell past their elastic limit, the rate at which solutes entered the cells could be estimated by the accompanying change in cell volume. He also found that there was an apparent nonsolvent volume of about 50% in red blood cells and later showed that this includes water of hydration in addition to the protein and other nonsolvent components of the cells.\n\nTwo opposing concepts developed within the context of studies on osmosis, permeability, and electrical properties of cells. The first held that these properties all belonged to the plasma membrane whereas the other predominant view was that the protoplasm was responsible for these properties.\nThe membrane theory developed as a succession of ad-hoc additions and changes to the theory to overcome experimental hurdles. Overton (a distant cousin of Charles Darwin) first proposed the concept of a lipid (oil) plasma membrane in 1899. The major weakness of the lipid membrane was the lack of an explanation of the high permeability to water, so Nathansohn (1904) proposed the mosaic theory. In this view, the membrane is not a pure lipid layer, but a mosaic of areas with lipid and areas with semipermeable gel. Ruhland refined the mosaic theory to include pores to allow additional passage of small molecules. Since membranes are generally less permeable to anions, Leonor Michaelis concluded that ions are adsorbed to the walls of the pores, changing the permeability of the pores to ions by electrostatic repulsion. Michaelis demonstrated the membrane potential (1926) and proposed that it was related to the distribution of ions across the membrane. Harvey and Danielli (1939) proposed a lipid bilayer membrane covered on each side with a layer of protein to account for measurements of surface tension. In 1941 Boyle & Conway showed that the membrane of frog muscle was permeable to both and , but apparently not to , so the idea of electrical charges in the pores was unnecessary since a single critical pore size would explain the permeability to , , and as well as the impermeability to , , and .\nOver the same time period, it was shown (Procter & Wilson, 1916) that gels, which do not have a semipermeable membrane, would swell in dilute solutions. Loeb (1920) also studied gelatin extensively, with and without a membrane, showing that more of the properties attributed to the plasma membrane could be duplicated in gels without a membrane. In particular, he found that an electrical potential difference between the gelatin and the outside medium could be developed, based on the concentration. Some criticisms of the membrane theory developed in the 1930s, based on observations such as the ability of some cells to swell and increase their surface area by a factor of 1000. A lipid layer cannot stretch to that extent without becoming a patchwork (thereby losing its barrier properties. Such criticisms stimulated continued studies on protoplasm as the principal agent determining cell permeability properties. In 1938, Fischer and Suer proposed that water in the protoplasm is not free but in a chemically combined form—the protoplasm represents a combination of protein, salt and water—and demonstrated the basic similarity between swelling in living tissues and the swelling of gelatin and fibrin gels. Dimitri Nasonov (1944) viewed proteins as the central components responsible for many properties of the cell, including electrical properties.\nBy the 1940s, the bulk phase theories were not as well developed as the membrane theories. In 1941, Brooks & Brooks published a monograph, \"The Permeability of Living Cells\", which rejects the bulk phase theories.\n\nWith the development of radioactive tracers, it was shown that cells are not impermeable to . This was difficult to explain with the membrane barrier theory, so the sodium pump was proposed to continually remove as it permeates cells. This drove the concept that cells are in a state of dynamic equilibrium, constantly using energy to maintain ion gradients. In 1935, Karl Lohmann discovered ATP and its role as a source of energy for cells, so the concept of a metabolically-driven sodium pump was proposed.\nThe tremendous success of Hodgkin, Huxley, and Katz in the development of the membrane theory of cellular membrane potentials, with differential equations that modeled the phenomena correctly, provided even more support for the membrane pump hypothesis.\nThe modern view of the plasma membrane is of a fluid lipid bilayer that has protein components embedded within it. The structure of the membrane is now known in great detail, including 3D models of many of the hundreds of different proteins that are bound to the membrane.\nThese major developments in cell physiology placed the membrane theory in a position of dominance and stimulated the imagination of most physiologists, who now apparently accept the theory as fact—there are, however, a few dissenters.[citation needed]\n\nIn 1956, Afanasy S. Troshin published a book, \"The Problems of Cell Permeability\", in Russian (1958 in German, 1961 in Chinese, 1966 in English) in which he found that permeability was of secondary importance in determination of the patterns of equilibrium between the cell and its environment. Troshin showed that cell water decreased in solutions of galactose or urea although these compounds did slowly permeate cells. Since the membrane theory requires an impermanent solute to sustain cell shrinkage, these experiments cast doubt on the theory. Others questioned whether the cell has enough energy to sustain the sodium/potassium pump. Such questions became even more urgent as dozens of new metabolic pumps were added as new chemical gradients were discovered.\n\nIn 1962, Gilbert Ling became the champion of the bulk phase theories and proposed his association-induction hypothesis of living cells.\n\nCells can be subdivided into the following subcategories:\n\n\nAnimals have evolved a greater diversity of cell types in a multicellular body (100–150 different cell types), compared\nwith 10–20 in plants, fungi, and protoctista.\n\n\n"}
{"id": "47880766", "url": "https://en.wikipedia.org/wiki?curid=47880766", "title": "Charles H. Holbrow", "text": "Charles H. Holbrow\n\nCharles H. Holbrow (born September 23, 1935) is an American physicist.\n\nCharles Howard Holbrow was born in Melrose, Massachusetts to parents Frederick Holbrow and Florence Louisa (Gile) Holbrow. His earliest memory of interest in physics dates from about age 13, when he saved the money he earned delivering newspapers to buy a book by Robert Andrews Millikan, \"Electrons (+And -) Protons Photons Neutrons Mesotrons and Cosmic Rays\". \"I read about two pages, and it made no sense to me whatsoever,\" he said. \"But I still have the book, and now it makes lots of sense.\"\n\nIn 1951, at age 15, he enrolled at the University of Wisconsin-Madison as a Ford Foundation Pre-Induction Fellow. Despite his early interest and intention to major in physics, he found physics coursework so challenging that he changed his major and, in 1955, earned a BA in history instead, then pursued graduate study in history at Columbia University. In 1956, he married Mary Louise Ross, with whom he has five daughters: Gwendolyn Holbrow, Elizabeth, Alice, Katherine and Martha. In 1957, he earned a master's degree in history and Russian studies from Columbia with a master's essay titled \"Lenin's views of the United States\". Inspired by the Sputnik launch, Holbrow then returned to study physics at the University of Wisconsin-Madison, where his doctoral advisor was Henry H. Barschall (\"Heinz\" Barschall). Holbrow earned his MS in physics in 1960 and PhD in physics in 1963, with the dissertation \"Neutrons from protons on nickel, rhodium, tantalum, and gold\".\n\nAfter earning his doctorate, Holbrow taught for three years at Haverford College and two years at the University of Pennsylvania, then served as assistant editor at Physics Today. In 1967, he became an associate professor of physics at Colgate University, where he remained for 36 years until his retirement in 2003. In addition to teaching and research, Holbrow was instrumental in establishing first the Colgate Computer Center and later the Department of Computer Science. He became associate director of the Colgate Computer Center and then, in 1968, its director. He was promoted to full professor in 1975 and named Charles A. Dana Professor of Physics in 1986. Holbrow also served as chairman of the Department of Physics and Astronomy, director of Institutional Research, and director of the Division of Mathematics and Natural Sciences.\n\nDuring his years at Colgate, Holbrow frequently joined other academic institutions temporarily as a visiting professor and researcher, including SUNY Stony Brook, Massachusetts Institute of Technology, Cornell University, Brookhaven National Laboratory, SRI International, Stanford University, the University of Wisconsin-Madison, University of Vienna, and the GSI Helmholtz Centre for Heavy Ion Research. He spent a sabbatical year in 1969-70 at the Stanford Linear Accelerator, and another at the California Institute of Technology in 1975-76, working in the Kellogg Radiation Laboratory. During the summer of 1975, Holbrow was a NASA-ASEE Summer Faculty Fellow at Stanford University and Ames Research Center, where he participated in a NASA project to develop colonies in space, culminating in the report Space Settlements: A Design Study, and featured in an article by Isaac Asimov in the July, 1976, issue of National Geographic.\n\nThroughout his physics career, Holbrow maintained his interest in reading and writing about history. His fascination and feeling of kinship with Danish-born Caltech physicist Charles Christian Lauritsen led to a biographical article, \"Charles C. Lauritsen: A Reasonable Man in an Unreasonable World,\" in Physics in Perspective. During a second stay at Caltech, he wrote a short history of Robert Andrews Millikan and the Kellogg Radiation Laboratory titled \"The giant cancer tube and the Kellogg Radiation Laboratory,\" published in Physics Today.\n\nIn addition to physics and history, Holbrow is interested in physics pedagogy. In 1998, he and colleagues at Colgate published a new introductory physics textbook, \"Modern Introductory Physics\", which reversed the usual introductory physics sequence by presenting relativity and quantum theory before mechanics and electromagnetism. A second edition appeared in 2010. He has also served as president of the American Association of Physics Teachers, and was awarded the 2012 Oersted Medal for his major contributions to physics education and research.\n\nSince Holbrow's retirement from Colgate University in 2003, he has been a visiting professor of physics at Massachusetts Institute of Technology, where he works on developing physics MOOCs, and a visiting scholar at Harvard University, where he won an award for excellence in teaching.\n\nHolbrow's research has included studying the properties of unstable ytterbium atoms produced in a particle accelerator, and using lead ions with only one electron, which share traits with hydrogen, to study relativistic effects.\n\n\n\n"}
{"id": "21251237", "url": "https://en.wikipedia.org/wiki?curid=21251237", "title": "Combinatio nova", "text": "Combinatio nova\n\nCombinatio nova, abbreviated comb. nov. (sometimes n. comb.), is Latin for \"new combination\". It is used in life sciences literature when a new name is introduced based on a pre-existing name. The term should not to be confused with \"\", used for a previously unnamed species.\n\nThere are three situations:\n\nWhen an earlier named species is assigned to a different genus, the new genus name is combined with of said species, e.g. when \"Calymmatobacterium granulomatis\" was renamed \"Klebsiella granulomatis\", it was referred to as \"Klebsiella granulomatis comb. nov.\" to denote it is a new combination.\n\n\n"}
{"id": "29492719", "url": "https://en.wikipedia.org/wiki?curid=29492719", "title": "D'Aveni's 7S framework", "text": "D'Aveni's 7S framework\n\nD'Aveni's 7S framework is Richard D'Aveni's approach to directing a firm in a high velocity or Hypercompetitive markets. it is designed to enable firms sustain the momentum of their competitiveness through a series of initiatives that are poised to give temporary advantages rather than just structuring the firm to achieve internal or external fit aimed at maintaining equilibrium that are designed to sustain unsustainable competitive advantages. Based on factors such as:\nAll of these factors address the Four Arenas of Competition referred to in his book, Hypercompetition.\n\n"}
{"id": "2109801", "url": "https://en.wikipedia.org/wiki?curid=2109801", "title": "Daryabar Fossa", "text": "Daryabar Fossa\n\nDaryabar Fossa is an east-west trending trough on Saturn's moon Enceladus. Daryabar Fossa was first seen in \"Voyager 2\" images, though a small section was see at much higher resolution by \"Cassini\". It is centered at 9.7° North Latitude, 359.1° West Longitude and is approximately 201 kilometers long. Based on limb profiles of \"Voyager 2\" images, Daryabar Fossa was determined to be a 400-meter deep and 4 kilometers wide (Kargel and Pozio 1996). Daryabar Fossa runs perpendicular to the scarp Isbanir Fossa and is right-laterally offset 15–20 km by the scarp, suggesting Isbanir is a strike-slip or transform fault (Rothery 1999).\n\nDaryabar Fossa is named for the land from which Princess Daryabar came in \"One Thousand and One Nights\". The word Daryabar is Persian دریابار and means \"Seaside\".\n\n"}
{"id": "26205878", "url": "https://en.wikipedia.org/wiki?curid=26205878", "title": "Dicke effect", "text": "Dicke effect\n\nDicke effect, also known as Dicke narrowing (or sometimes collisional narrowing) in spectroscopy, named after Robert H. Dicke, refers to narrowing of the Doppler broadening of a spectral line due to collisions the emitting species (usually an atom or a molecule) experiences with other particles. \n\nWhen the mean free path of an atom is much smaller than the wavelength of the radiative transition, the atom changes velocity and direction many times during the emission or absorption of a photon. This causes an averaging over different Doppler states and results in an atomic linewidth that is narrower than the Doppler width.\n\n"}
{"id": "54077462", "url": "https://en.wikipedia.org/wiki?curid=54077462", "title": "EngrXiv", "text": "EngrXiv\n\nengrXiv (\"Engineering Archive\") is a preprint repository for engineering launched in July 2016. It is hosted by the Center for Open Science and administrated by the University of Wisconsin–Stout. \"engrXiv\" is directed by a steering committee of engineers and members of the engineering librarian community.\n\nA paper template for submissions to \"engrXiv\" is available online via Overleaf. As of April 2017, \"engrXiv\" content is now indexed in Google Scholar.\n\n"}
{"id": "5163904", "url": "https://en.wikipedia.org/wiki?curid=5163904", "title": "Error exponent", "text": "Error exponent\n\nIn information theory, the error exponent of a channel code or source code over the block length of the code is the logarithm of the error probability. For example, if the probability of error of a decoder drops as \"e\", where \"n\" is the block length, the error exponent is α. Many of the information-theoretic theorems are of asymptotic nature, for example, the channel coding theorem states that for any rate less than the channel capacity, the probability of the error of the channel code can be made to go to zero as the block length goes to infinity. In practical situations, there are limitations to the delay of the communication and the block length must be finite. Therefore, it is important to study how the probability of error drops as the block length go to infinity.\n\nThe channel coding theorem states that for any ε > 0 and for any rate less than the channel capacity, there is an encoding and decoding scheme that can be used to ensure that the probability of block error is less than ε > 0 for sufficiently long message block \"X\". Also, for any rate greater than the channel capacity, the probability of block error at the receiver goes to one as the block length goes to infinity.\n\nAssuming a channel coding setup as follows: the channel can transmit any of formula_1 messages, by transmitting the corresponding codeword (which is of length \"n\"). Each component in the codebook is drawn i.i.d. according to some probability distribution with probability mass function \"Q\". At the decoding end, maximum likelihood decoding is done.\n\nLet formula_2 be the formula_3th random codeword in the codebook, where formula_3 goes from formula_5 to formula_6. Suppose the first message is selected, so codeword formula_7 is transmitted. Given that formula_8 is received, the probability that the codeword is incorrectly detected as formula_9 is:\n\nThe function formula_11 has upper bound\n\nfor formula_13 Thus,\n\nSince there are a total of \"M\" messages, and the entries in the codebook are i.i.d., the probability that formula_7 is confused with any other message is formula_6 times the above expression. Using the Hokey union bound, the probability of confusing formula_7 with any message is bounded by:\n\nfor any formula_19. Averaging over all combinations of formula_20:\n\nChoosing formula_22 and combining the two sums over formula_23 in the above formula:\n\nUsing the independence nature of the elements of the codeword, and the discrete memoryless nature of the channel:\n\nUsing the fact that each element of codeword is identically distributed and thus stationary:\n\nReplacing \"M\" by 2 and defining\n\nprobability of error becomes\n\n\"Q\" and formula_29 should be chosen so that the bound is tighest. Thus, the error exponent can be defined as\n\nThe source coding theorem states that for any formula_31 and any discrete-time i.i.d. source such as formula_32 and for any rate less than the entropy of the source, there is large enough formula_33 and an encoder that takes formula_33 i.i.d. repetition of the source, formula_35, and maps it to formula_36 binary bits such that the source symbols formula_35 are recoverable from the binary bits with probability at least formula_38.\n\nLet formula_39 be the total number of possible messages. Next map each of the possible source output sequences to one of the messages randomly using a uniform distribution and independently from everything else. When a source is generated the corresponding message formula_40 is then transmitted to the destination. The message gets decoded to one of the possible source strings. In order to minimize the probability of error the decoder will decode to the source sequence formula_41 that maximizes formula_42, where formula_43 denotes the event that message formula_44 was transmitted. This rule is equivalent to finding the source sequence formula_41 among the set of source sequences that map to message formula_44 that maximizes formula_47. This reduction follows from the fact that the messages were assigned randomly and independently of everything else.\n\nThus, as an example of when an error occurs, supposed that the source sequence formula_48 was mapped to message formula_49 as was the source sequence formula_50. If formula_51 was generated at the source, but formula_52 then an error occurs.\n\nLet formula_53 denote the event that the source sequence formula_54 was generated at the source, so that formula_55 Then the probability of error can be broken down as formula_56 Thus, attention can be focused on finding an upper bound to the formula_57.\n\nLet formula_58 denote the event that the source sequence formula_59 was mapped to the same message as the source sequence formula_54 and that formula_61. Thus, letting formula_62 denote the event that the two source sequences formula_63 and formula_64 map to the same message, we have that\n\nand using the fact that formula_66 and is independent of everything else have that\n\nA simple upper bound for the term on the left can be established as\n\nfor some arbitrary real number formula_69 This upper bound can be verified by noting that formula_70 either equals formula_71 or formula_72 because the probabilities of a given input sequence are completely deterministic. Thus, if formula_73 then formula_74 so that the inequality holds in that case. The inequality holds in the other case as well because\n\nfor all possible source strings. Thus, combining everything and introducing some formula_76, have that\n\nWhere the inequalities follow from a variation on the Union Bound. Finally applying this upper bound to the summation for formula_78 have that:\n\nWhere the sum can now be taken over all formula_64 because that will only increase the bound. Ultimately yielding that\n\nNow for simplicity let formula_82 so that formula_83 Substituting this new value of formula_84 into the above bound on the probability of error and using the fact that formula_64 is just a dummy variable in the sum gives the following as an upper bound on the probability of error:\n\nThe term in the exponent should be maximized over formula_90 in order to achieve the tightest upper bound on the probability of error.\n\nLetting formula_91 see that the error exponent for the source coding case is:\n\n\nR. Gallager, Information Theory and Reliable Communication, Wiley 1968\n"}
{"id": "10055024", "url": "https://en.wikipedia.org/wiki?curid=10055024", "title": "Gravitational compression", "text": "Gravitational compression\n\nGravitational compression is a phenomenon in which gravity, acting on the mass of an object, compresses it, reducing its size and increases the object's density. \n\nAt the center of a planet or star, gravitational compression produces heat by the Kelvin–Helmholtz mechanism. This is the mechanism that explains how Jupiter continues to radiate heat produced by its gravitational compression.\n\nThe most common reference to gravitational compression is stellar evolution. The Sun and other main-sequence stars are produced by the initial gravitational collapse of a molecular cloud. Assuming the mass of the material is large enough, gravitational compression reduces the size of the core, increasing its temperature until hydrogen fusion can begin. This hydrogen-to-helium fusion reaction releases energy that balances the inward gravitational pressure and the star becomes stable for millions of years. No further gravitational compression occurs until the hydrogen is nearly used up, reducing the thermal pressure of the fusion reaction. At the end of the Sun's life, gravitational compression will turn it into a white dwarf.\n\nAt the other end of the scale are massive stars. These stars burn their fuel very quickly, ending their lives as supernovae. After which further gravitational compression will produce either a neutron star or a black hole from the remnants.\n\nFor planets and moons, equilibrium is reached when the compression is balanced by a pressure gradient. This is due to gravity. This pressure gradient is in the opposite direction due to the strength of the material, at which point gravitational compression ceases.\n"}
{"id": "38448482", "url": "https://en.wikipedia.org/wiki?curid=38448482", "title": "Index of physics articles (T)", "text": "Index of physics articles (T)\n\nThe index of physics articles is split into multiple pages due to its size.\n\nTo navigate by individual letter use the table of contents below.\n\n"}
{"id": "29110711", "url": "https://en.wikipedia.org/wiki?curid=29110711", "title": "Indian Public Health Association", "text": "Indian Public Health Association\n\nIndian Public Health Association shortly IPHA is a professional health organization working for the cause of Public Health in India since 1956. It is registered under Societies Registration Act XXI of 1860.\n\nThe mission is to protect and promote the health of the people of India by facilitating the exchange of information, experience, and research, and advocating for policies, programs and practices that improve public health.\n\nThe Idea of establishing an association of Public Health was first mooted out at the All India Institute of Hygiene and Public Health, Calcutta, in 1935. The permission of Government was necessary to create such an association. Most of the health staff then was working under government or municipalities. Hence permission was denied until the Indian Independence. After 1948, Dr. Ganguly and Dr. S. C. Seal revived the movement. The Association was inaugurated on 29 September 1956 by Smt. Rajkumari Amrit Kaur, the then Union Health Minister, Government of India. The Public Health personalities responsible for the success of the association were Dr. B. C. Roy, Chief Minister of West Bengal, Lt. Col. C. K. Lakshmanan, Director of All India Institute of Hygiene and Public Health, Dr. K. C. K. E. Raja, Director General of Health Services, Government of India, Dr. B. C. Dasgupta, Director of Health Services, West Bengal and Dr. T. Lakshminarayanan of Madras. The association has grown over the years and now has a strength of nearly 4,000 members. The body has 22 branches in India. In Chandigarh, IPHA has celebrated World's AIDS day 2014 in Tagore Theatre, where they have invited Snehil Sharma's dance group Adi Shakti Nrityashala and other theater groups for the cultural show.\n\nThe association holds a National Conference for its members every year since 1956 in different regions of the country.\n\nIndian Journal of Public Health is their Official Publication. The Association started the Journal from the day of its birth in September 1956. Original articles, results of investigation and research, special articles, short communications, National Health Program reports, Reviews, Notes and News of the association are published in the journal. An annual prize in the memory of Late Dr. R. N. Roy, is given to the authors for the best article of the year. The journal is circulated to more than 4,500 members and 250 subscribers throughout the country and abroad.\n\n"}
{"id": "31981081", "url": "https://en.wikipedia.org/wiki?curid=31981081", "title": "Intel ISEF Finalist Medal", "text": "Intel ISEF Finalist Medal\n\nThe Intel ISEF Finalist Medal is given to about 1800 students from 75 countries each year, which are participating at the Intel International Science and Engineering Fair, which is owned and administered by the Society for Science and the Public, a 501(c)(3) non-profit organization based in Washington, DC.\n\nEach year about 7 million students participate in different regional, district and state ISEF affiliated fairs. Some of the winners of these affiliated fairs, which exist in over 75 countries, get the chance to take part at the Intel ISEF as a finalist, and each of them is awarded the Intel ISEF Finalist Medal. In 2013 there were 1611 finalists at the Intel ISEF in Phoenix, Arizona.\n\nThe medal has a diameter of 48 mm and is golden galvanized. The obverse shows the official logo of the Intel ISEF, the reverse shows the year of participating and the location of that year's Intel ISEF.\n\nThe ribbon bar is blue with a width of 40mm and has a golden romanic 1 in the middle.\n\n"}
{"id": "20633778", "url": "https://en.wikipedia.org/wiki?curid=20633778", "title": "Journal of Social, Political, and Economic Studies", "text": "Journal of Social, Political, and Economic Studies\n\nThe Journal of Social, Political, and Economic Studies () is a quarterly journal published by Scott-Townsend Publishers, the publishing arm of the Council for Social and Economic Studies. It was founded in 1976 by anthropologist Roger Pearson, and was originally published by his Council for American Affairs, an American representative in the World Anti Communist League. It is now published by the Council for Social and Economic Studies, of which Pearson was the president as of 1982. It has been identified as one of two international journals which regularly publishes articles pertaining to race and intelligence with the goal of supporting the idea that white people are inherently superior (the other such journal being \"Mankind Quarterly\"). Notable contributors to the journal include Jack Kemp and Jesse Helms. In 1982, U.S. President Ronald Reagan wrote a letter to Pearson personally thanking him for the most recent issue of the Journal, which was never disavowed by the White House. The White House did, however, request that Pearson stop using the letter.\n"}
{"id": "9105584", "url": "https://en.wikipedia.org/wiki?curid=9105584", "title": "Leverett J-function", "text": "Leverett J-function\n\nIn petroleum engineering, the Leverett \"J\"-function is a dimensionless function of water saturation describing the capillary pressure, \nwhere formula_2 is the water saturation measured as a fraction, formula_3 is the capillary pressure (in pascal), formula_4 is the permeability (measured in m²), formula_5 is the porosity (0-1), formula_6 is the surface tension (in N/m) and formula_7 is the contact angle. The function is important in that it is constant for a given saturation within a reservoir, thus relating reservoir properties for neighboring beds.\n\nThe Leverett \"J\"-function is an attempt at extrapolating capillary pressure data for a given rock to rocks that are similar but with differing permeability, porosity and wetting properties. It assumes that the porous rock can be modelled as a bundle of non-connecting capillary tubes, where the factor formula_8 is a characteristic length of the capillaries' radii.\n\nThis function is also widely used in modeling two-phase flow of proton-exchange membrane fuel cells. A large degree of hydration is needed for good proton conductivity while large liquid water saturation in pores of catalyst layer or diffusion media will impede gas transport in the cathode.\n\n\n"}
{"id": "2536043", "url": "https://en.wikipedia.org/wiki?curid=2536043", "title": "List of immunologists", "text": "List of immunologists\n\nThis is a \"list of notable immunologists.\"\n\n\n\n"}
{"id": "41689969", "url": "https://en.wikipedia.org/wiki?curid=41689969", "title": "List of medical schools in Sindh", "text": "List of medical schools in Sindh\n\nIn Pakistan, a medical school is more often referred to as a medical college. A medical college is affiliated with a university as a department which usually has a separate campus. The medical schools in province of Sindh are both private and public.\n\n! No.\n! Name of medical school\n! Funding\n! Established\n! Enrollment\n! University\n! District\n! Province\n! Website\n! IMED profile\n\n"}
{"id": "16718318", "url": "https://en.wikipedia.org/wiki?curid=16718318", "title": "List of tripoints of English counties", "text": "List of tripoints of English counties\n\nThis page contains a list of tripoints of English counties. A tripoint is the point at which three geographical regions meet.\n\nThe table contains a list of the 68 tripoints for the ceremonial counties of England as per the Lieutenancies Act 1997 as subsequently amended. Also included are the four points at which two counties meet the borders with Wales and Scotland. For each tripoint the counties are ordered with the first alphabetically given first, and the counties listed anti-clockwise around the point from there.\n\nIn addition as a result of the Local Government Act 1972, between 1974 and 1996 the counties of Avon and Cleveland resulted in three further tripoints that are no longer in existence.\n\nThe table contains a list of the 58 principal tripoints for the historic counties of England prior to 1800. As the English county boundaries had remained essentially unchanged since the eleventh century, the list can thus be seen to represent the \"original\" locations of the English county tripoints.\n\nDuring the nineteenth century a number of laws, most notably the Counties Act of 1844, resulted in the relocation of some sections of historic boundaries, principally to remove detached portions of counties. With the notable exception of the creation of the metropolitan counties in 1974, the majority of the boundaries have remained unchanged since then. For comparison the locations of the tripoints in 1890 are also listed in the table, by which time the majority of the localised changes had been made. The handful of additional tripoints that were formed by detached parts of counties are not listed in the table (e.g. Devon(detached)-Dorset-Somerset, Staffordshire-Shropshire(detached)-Worcestershire(detached)).\n\nThe symbol (†) indicates that the original point still exists unchanged as a tripoint of the current ceremonial counties. (Many of the other points still exist as the meeting of three administrative districts.) For each tripoint the counties are ordered with the first alphabetically given first, and the counties listed anti-clockwise around the point from there.\n\n"}
{"id": "11561548", "url": "https://en.wikipedia.org/wiki?curid=11561548", "title": "List of video game emulators", "text": "List of video game emulators\n\nThe following is a list of notable emulation software for arcade games, home video game consoles and handheld game consoles.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-system emulators are capable of emulating the functionality of multiple systems.\n\n\n"}
{"id": "49643295", "url": "https://en.wikipedia.org/wiki?curid=49643295", "title": "Lucchesiite", "text": "Lucchesiite\n\nLucchesiite is a new member of tourmaline-group of minerals. Lucchesiite has the formula CaFeAl(SiO)(BO)(OH)O. It is the calcium and oxygen-analogue of schorl. It has two co-type localizations, one in Czech Republic and the other in Sri Lanka. As the other members of the tourmaline group, it is trigonal. \n\nImpurites in lucchesiite, depending on the provenience, are sodium, magnesium, aluminium, titanium, trivalent iron, and minor vanadium, potassium, manganese and zinc.\n"}
{"id": "4366909", "url": "https://en.wikipedia.org/wiki?curid=4366909", "title": "Ludwik Fleck Prize", "text": "Ludwik Fleck Prize\n\nThe Ludwik Fleck Prize is an annual award given for a book in the field of science and technology studies. It was created by the 4S Council (Society for the Social Studies of Science) in 1992 and is named after microbiologist Ludwik Fleck.\n"}
{"id": "58904196", "url": "https://en.wikipedia.org/wiki?curid=58904196", "title": "Marcey Waters", "text": "Marcey Waters\n\nMarcey Lynn Waters is a chemical biologist and Vice Chair for Education at the University of North Carolina, Chapel Hill. She studies peptide folding and supramolecular systems of biomolecules. Waters has received multiple awards, distinguished professorships, fellowships, and serves as the 2018 President of the American Peptide Society.\n\nWaters graduated from the University of California, San Diego with a degree in chemistry. While an undergraduate, she worked with Prof. Charles L. Perrin on 1H NMR spectroscopy of organic molecules. Waters entered the University of Chicago for her doctoral degree in chemistry, working with Prof. William D. Wulff on the physical chemistry of organometallic alkyne systems, yielding Fischer carbene complexes. The mechanism for the Wulff-Dotz benzannulation reaction is still under discussion.\n\nWaters joined UNC-Chapel Hill as an assistant professor in 1999. As of 2018, she was the Gordon and Bowman Gray Distinguished term professor and Glen H. Elder Distinguished professor. Waters' research began with studies of non-covalent organizing interactions in peptide beta-hairpin turn backbone model systems, specifically, how pi-pi and cation-pi system interactions could influence peptide organization. This research can also extend to molecular recognition, in which specific peptide cavities can be designed to \"host\" selected organic molecules. Waters' group also studies protein and peptide methylation patterns and their biophysical interactions, which can relate to epigenetic disease mechanisms. She collaborated with faculty colleague Nancy Albritton to study degrons of proteolytically cleaved ubiquitins.\n\nWaters was involved in mentorship of chemistry students from nontraditional and disadvantaged backgrounds as part of the American Chemical Society's Project SEED. She has advanced multiple campus groups encouraging women scholars in physical sciences. Waters mentored for TANDEMplusIDEA, the international mentoring program for female scientists from 2007 to 2009. Waters was on the Board of Directors for the Mesilla Chemistry Workshop held in July from 2006 to 2018. From 2011 to 2013, Waters was a UNC WOWS Scholar (Working on Women in Science). From 2013 to 2018, she was a Faculty Advisor for UNC WISE (Women in Science and Engineering, graduate student organization).\n\nIn 2007, Waters served as Co-chair for the International Symposium on Dynamic Combinatorial Chemistry (November). The year after that, she was an Advisory Board Member of International Symposia on Macrocyclic and Supramolecular Chemistry (ISMSC). In 2009, she was a guest editor for the December issue of Current Opinion in Chemical Biology. From 2009 to 2015, she was an Advisory Board Member of International Symposia on Macrocyclic and Supramolecular Chemistry (ISMSC). In 2011, Waters was a Co-Organizer of the Mesilla Chemistry Workshop on “Aromatic Interactions in Chemistry and Biology”(with Ken Houk, UCLA Dept of Chemistry). In 2012, Waters was a Section Editor for “Supramolecular Chemistry: From Molecules to Nanomaterials” (John Wiley and Sons). In 2013, she was the Co-Chair, of American Peptide Society Meeting (with David Lawrence, UNC Dept of Chemistry) and a guest editor for Accounts of Chemical Research for the “Aromatic Interactions in Chemistry and Biology” article in the April Issue. \n\nFrom 2014 to 2020, Waters was an Editorial Advisory Board Member of the Journal of the American Chemical Society.\n"}
{"id": "847731", "url": "https://en.wikipedia.org/wiki?curid=847731", "title": "Michael J. Adams", "text": "Michael J. Adams\n\nMichael James \"Mike\" Adams (May 5, 1930 – November 15, 1967), (Maj, USAF), was an American aviator, aeronautical engineer, and USAF astronaut. He was the first American space mission fatality, according to the United States definition.\n\nAdams was born May 5, 1930, in Sacramento, California. He graduated from Sacramento Junior College. He enlisted in the United States Air Force in 1950, and earned his pilot wings and commission in 1952 at Webb Air Force Base, Texas. He served as a fighter-bomber pilot during the Korean War, where he flew 49 combat missions. This was followed by 30 months with the 613th Fighter-Bomber Squadron at England Air Force Base, Louisiana, and six months rotational duty at Chaumont Air Base in France.\n\nIn 1958, Adams received a Bachelor of Science degree in Aeronautical Engineering from the University of Oklahoma and, after 18 months of astronautics study at Massachusetts Institute of Technology, was selected in 1962 for the U.S. Air Force Test Pilot School at Edwards Air Force Base, California. Here, he won the A.B. Honts Trophy as the best scholar and pilot in his class. Adams subsequently attended the Aerospace Research Pilot School (ARPS), graduating with honors in December 1963. He was one of four Edwards aerospace research pilots to participate in a five-month series of NASA Moon landing practice tests at the Martin Company in Baltimore, Maryland. In November 1965 he was selected to be an astronaut in the United States Air Force Manned Orbiting Laboratory program. In July 1966, Major Adams came to the North American X-15 program, a joint USAF/NASA project. He made his first X-15 flight on 6 October 1966.\n\nAdams' seventh X-15 flight, Flight 3-65-97, took place on 15 November 1967. He reached a peak altitude of ; the nose of the aircraft was off heading by 15 degrees to the right. While descending, at the aircraft encountered rapidly increasing aerodynamic pressure which impinged on the airframe, causing the X-15 to enter a violent Mach 5 spin. As the X-15 neared , it was diving at Mach 3.93 and experiencing more than 15 \"g\" vertically (positive and negative), and 8 g laterally, which inevitably exceeded the design limits of the aircraft. The aircraft broke up 10 minutes and 35 seconds after launch, killing Adams. The United States Air Force posthumously awarded him Astronaut Wings for his last flight.\n\nAn excerpt from NASA's biography page on Mike Adams discusses findings from the crash investigation:\n\nDuring his military career he was awarded: \n\nIn 1991, Adams' name was added to the Space Mirror Memorial at the Kennedy Space Center in Florida.\n\nOn June 8, 2004, a memorial monument to Adams was erected near the crash site, northwest of Randsburg, California.\n\n"}
{"id": "14051879", "url": "https://en.wikipedia.org/wiki?curid=14051879", "title": "Minicircle", "text": "Minicircle\n\nMinicircles are small (~4kb) circular plasmid derivatives that have been freed from all prokaryotic vector parts. They have been applied as transgene carriers for the genetic modification of mammalian cells, with the advantage that, since they contain no bacterial DNA sequences, they are less likely to be perceived as foreign and destroyed. (Typical transgene delivery methods involve plasmids, which contain foreign DNA.) The smaller size of minicircles also extends their cloning capacity and facilitates their delivery into cells.\n\nTheir preparation usually follows a two-step procedure:\n\n\nThe purified minicircle can be transferred into the recipient cell by transfection or lipofection and into a differentiated tissue by, for instance, jet injection.\n\nConventional minicircles lack an origin of replication, so they do not replicate within the target cells and the encoded genes will disappear as the cell divides (which can be either an advantage or disadvantage depending on whether the application demands persistent or transient expression). A novel addition to the field are nonviral self-replicating minicircles, which owe this property to the presence of a S/MAR-Element. Self-replicating minicircles hold great promise for the systematic modification of stem cells and will significantly extend the potential of their plasmidal precursor forms (\"parental plasmids\"), the more as the principal feasibility of such an approach has amply been demonstrated for their plasmidal precursor forms.\n\n"}
{"id": "14439210", "url": "https://en.wikipedia.org/wiki?curid=14439210", "title": "Nuclear detection", "text": "Nuclear detection\n\nThe threat of radiological attacks has led several organizations to develop specially designed nuclear detection systems. These systems differ in design and abilities.\n\n"}
{"id": "14061452", "url": "https://en.wikipedia.org/wiki?curid=14061452", "title": "Nuclear or Not?", "text": "Nuclear or Not?\n\nNuclear or Not? Does Nuclear Power Have a Place in a Sustainable Energy Future? is a 2007 book edited by Professor David Elliott. The book offers various views and perspectives on nuclear power. Authors include:\n\n\nProfessor Elliott calls for continued debate on the nuclear power issue. He has worked with the United Kingdom Atomic Energy Authority before moving to the Open University where he is Professor of Technology Policy and has developed courses on technological innovation, focusing in particular on renewable energy technology.\n\n\n"}
{"id": "43056101", "url": "https://en.wikipedia.org/wiki?curid=43056101", "title": "Nuclear testing at Bikini Atoll", "text": "Nuclear testing at Bikini Atoll\n\nThe nuclear testing at Bikini Atoll program was a series of 23 nuclear devices detonated by the United States between 1946 and 1958 at seven test sites on the reef itself, on the sea, in the air and underwater. The test weapons produced a combined fission yield of 42.2 Mt of explosive power.\n\nThe United States was engaged in a Cold War Nuclear arms race with the Soviet Union to build more advanced bombs from 1947 until 1991. The first series of tests over Bikini Atoll in July 1946 was code named Operation Crossroads. The first test was dropped from an aircraft and detonated above the target fleet. The second, \"Baker\", was suspending under a barge. It produced a large Wilson cloud and contaminated all of the target ships. Chemist Glenn T. Seaborg, the longest-serving chairman of the Atomic Energy Commission, called the second test \"the world's first nuclear disaster.\"\n\nThe second series of tests in 1954 was codenamed Operation Castle. The first detonation, Castle Bravo, was a new design utilizing a dry fuel thermonuclear hydrogen bomb. It was detonated at dawn on March 1, 1954. Scientists miscalculated and the 15 megaton (Mt) nuclear explosion far exceeded the expected yield of 4 to 8 Mt (6 Mt predicted), and was about 1,000 times more powerful than each of the atomic bombs dropped on Hiroshima and Nagasaki during World War II. The scientists and military authorities were shocked by the size of the explosion and many of the instruments they had put in place to evaluate the effectiveness of the device were destroyed.\n\nThe military authorities and scientists had promised the Bikini Atoll's native residents that they would be able to return home after the nuclear tests. A majority of the island's family heads agreed to leave the island, and most of the residents were moved to the Rongerik Atoll and later to Kili Island. Both locations proved unsuitable to sustaining life, resulting in starvation and requiring the residents to receive ongoing aid.\n\nDespite the promises made by authorities, this and further nuclear tests (Redwing in 1956 and Hardtack in 1958) rendered Bikini unfit for habitation, contaminating the soil and water, making subsistence farming and fishing too dangerous. The United States later paid the islanders and their descendants $125 million in compensation for damage caused by the nuclear testing program and their displacement from their home island. As of 2014, it may be technically possible for the former residents and their descendants to live on the atoll's islands, but virtually none of those alive today have ever lived on the atoll and very few want to move there. A 2016 investigation found radiation levels on Bikini Atoll as high as 639 mrem yr, well above the established safety standard threshold for habitation of 100 mrem yr. However, in 2017, Stanford University scientists reported \"an abundance of marine life apparently thriving in the crater of Bikini Atoll.\" Research is being undertaken on how marine organisms are surviving in a radiation-filled environment, which could lead to improved understanding of cancer and increased human longevity.\n\nIn February 1946, the United States government asked the 167 Micronesian inhabitants of the atoll to \"temporarily\" relocate, so the United States government could begin testing atomic bombs for \"the good of mankind and to end all world wars.\" After \"confused and sorrowful deliberation\" among the Bikinians, their leader, King Juda, agreed to the U.S. relocation request, announcing \"We will go believing that everything is in the hands of God.\" Nine of the eleven family heads, or \"alaps\", chose Rongerik as their new home. United States Navy Seabees helped them to disassemble their church and community house and prepare to relocate them to their new home. On March 7, 1946, the residents gathered their personal belongings and saved building supplies. They were transported eastward on U.S. Navy landing craft 1108 and LST 861 to the uninhabited Rongerik Atoll, which was one-sixth the size of Bikini Atoll. No one lived on Rongerik because it had an inadequate water and food supply and due to deep-rooted traditional beliefs that the island was haunted by the \"Demon Girls of Ujae\". The Navy left them with a few weeks of food and water which soon proved to be inadequate.\n\nTo conduct the tests, the United States assembled a support fleet of more than 242 ships that provided quarters, experimental stations, and workshops for more than 42,000 personnel. The islands were primarily used as recreation and instrumentation sites. To support the nuclear bomb testing program, Seabees built bunkers, floating dry docks, steel towers for cameras and recording instruments, and other facilities on the island to support the servicemen. These included the \"Up and Atom Officer's Club\" and the \"Cross Spikes Club\", a bar and hangout created by servicemen on Bikini Island between June and September 1946 during the preparation for Operation Crossroads. The \"club\" was little more than a small open-air building that served alcohol to servicemen and provided outdoor entertainment, including a ping pong table. The \"Cross Spikes Club\" was the only entertainment the enlisted servicemen had access to during their June to September stay at Bikini.\n\nThe Bikini Atoll lagoon was designated a ship graveyard by the United States Navy. The United States brought in 95 ships including carriers, battleships, cruisers, destroyers, submarines, attack transports, landing ships, and auxiliary the island, along with 150 airplanes on the island's airstrip, to test the effect of a nuclear weapon. The proxy fleet if active would have comprised the sixth largest naval fleet in the world. All carried varying amounts of fuel and some carried live ordnance.\n\nCrossroads consisted of two detonations, each with a yield of . Able was detonated over Bikini on July 1, 1946 and exploded at an altitude of but was dropped by aircraft about off target. It sank only five of the ships in the lagoon. The second, Baker, was detonated underwater at a depth of on July 25, sinking eight ships. The second underwater blast created a large condensation cloud and contaminated the ships with more radioactive water than was expected. Many of the surviving ships were too contaminated to be used again for testing and were sunk. The air-borne nuclear detonation raised the surface seawater temperature by , created blast waves with speeds of up to , and shock and surface waves up to high. Blast columns reached the floor of the lagoon which is approximately deep. The Bikini Island King visited Bikini Atoll in July after the second atomic bomb test code-named Baker and found it apparently in good condition.\n\nA third burst, \"Charlie\", planned for 1947, was canceled primarily because of the United States Navy's inability to decontaminate the target ships after the \"Baker\" test. \"Charlie\" was rescheduled as Operation Wigwam, a deep water shot conducted in 1955 off the California coast.\n\nThe United States was in a Cold War Nuclear arms race with the Soviet Union to build bigger and better bombs. The next series of tests over Bikini Atoll was code named Operation Castle. The first test of that series was Castle Bravo, a new design utilizing a dry fuel thermonuclear hydrogen bomb. It was detonated at dawn on March 1, 1954.\n\nThe 15 megaton (Mt) nuclear explosion far exceeded the expected yield of 4 to 8 Mt (6 Mt predicted), and was about 1,000 times more powerful than each of the atomic bombs dropped on Hiroshima and Nagasaki during World War II. The nuclear weapon was the most powerful device ever detonated by the United States (and just under one-third the energy of the Tsar Bomba, the largest ever tested). The scientists and military authorities were shocked by the size of the explosion and many of the instruments they had put in place to evaluate the effectiveness of the device were destroyed.\n\nThe unexpectedly large yield, combined with other factors, led to the most significant accidental radiological contamination caused by the United States. A few minutes after the detonation, blast debris began to fall on Eneu/Enyu Island on Bikini Atoll where the U.S crew that triggered the device were located. When their geiger counters detected the unexpected fallout, they were forced to shelter in place indoors for a number of hours before it was safe enough for an airlift rescue operation to be carried out.\n\nAfter impacting Eneu, the fallout continued to spread across the inhabited islands of the Rongelap, Rongerik, and Utrik Atolls. The inhabitants of Rongelap and Rongerik Atolls were evacuated by US servicemen two days after the detonation, but the residents of the more distant Utrik Atoll weren't evacuated for three days. Many of the inhabitants soon began to show symptoms of acute radiation syndrome. They returned to the islands three years later but were forced to relocate again when their islands were found to be unsafe.\n\nThe fallout gradually dispersed around the globe, depositing traces of radioactive material in Australia, India and Japan, and parts of the United States and Europe. Though organized as a secret test, Castle Bravo quickly became an international incident, prompting calls for a ban on the atmospheric testing of thermonuclear devices.\n\nThe Rongelap Atoll was coated with up to of snow-like irradiated calcium debris and ash over the entire island. Virtually all the inhabitants experienced severe radiation sickness, including itchiness, sore skin, vomiting, diarrhea, and fatigue. Their symptoms also included burning eyes and swelling of the neck, arms, and legs. The inhabitants were forced to abandon the islands, leaving all their belongings, three days after the test. They were relocated to Kwajalein for medical treatment.\n\nSix days after the Castle Bravo test, the U.S government set up a secret project to study the medical effects of the weapon on the residents of the Marshall Islands.\n\nThe United States was subsequently accused of using the inhabitants as medical research subjects, without obtaining their consent, to study the effects of nuclear exposure. Until that time, the United States Atomic Energy Commission had given little thought to the potential impact of widespread fallout contamination and health and ecological impacts beyond the formally designated boundary of the test site.\n\nNinety minutes after the detonation, 23 crew members of the Japanese fishing boat the \"Daigo Fukuryū Maru\" (\"Lucky Dragon No. 5\") also were contaminated by the snow-like irradiated debris and ash. They had no idea what the explosion they'd seen meant nor any inkling of the nature of the deadly debris that rained down on them like snow. But they all soon became ill with the effects of acute radiation poisoning. One fisherman died almost a half year later while under doctor supervision in late September, with his cause of death regarded to have been due to underlying liver cirrhosis compounded by an infection. The majority of medical experts believe that the crew members were infected with hepatitis C through blood transfusions during part of their Acute radiation syndrome treatment.\n\nEdward Teller, one of the driving minds behind the development of the hydrogen bomb, and architect of the Marshall Island tests, upon learning of the death of the fisherman through the mass media which painted his death as an anti-nuclear casus belli, commented, \"It's unreasonable to make such a big deal over the death of a fisherman.\"\n\nThe seventeen-shot Redwing series followed—eleven tests at Enewetak Atoll and six at Bikini. The island residents who had been promised they would be able to return home to Bikini was thwarted indefinitely by the U.S. decision to resume nuclear testing at Bikini in 1954. During 1954, 1956, and 1958, twenty-one more nuclear bombs were detonated at Bikini, yielding a total of , equivalent to more than three thousand \"Baker\" bombs. Only one was an air burst, the 3.8 Mt Redwing \"Cherokee\" test. Air bursts distribute fallout in a large area, but surface bursts produce intense local fallout. These tests were followed by the 33-shot Hardtack tests which began in late April 1958. The last of ten tests were detonated on Bikini Atoll on 22 July 1958.\n\nShipwrecks in the lagoon include:\n\n\nThe following above-ground nuclear device tests were conducted on or near Bikini Atoll from 1946 to 1958, comprising 15.1% of total test yield worldwide. These dates are given in US Eastern time zone The days of the week are a day earlier than they were at Bikini \n\nIn 1947, the United States convinced the United Nations to designate the islands of Micronesia a United Nations Strategic Trust Territory. This was the only trust ever granted by the U.N. The United States Navy controlled the Trust from a headquarters in Guam until 1951, when the United States Department of the Interior took over control, administering the territory from a base in Saipan. The directive to which the United States agreed stipulated that it would \"promote the economic advancement and self-sufficiency of the inhabitants, and to this end shall... protect the inhabitants against the loss of their lands and resources...\"\n\nDespite the promise to \"protect the inhabitants,\" from July 1946 through July 1947 the residents of Bikini Atoll were left alone on Rongerik Atoll. In January 1948, Dr. Leonard E. Mason, an anthropologist from the University of Hawaii, visited the temporary home of the relocated islanders on Rongerik Atoll and was horrified when he found the people were starving. A team of U.S. investigators concluded in late 1947 that the islanders must be moved immediately. Press from around the world harshly criticized the U.S. Navy for ignoring the people. Harold Ickes, a syndicated columnist, wrote \"The natives are actually and literally starving to death.\"\n\nThe Navy then selected Ujelang Atoll for their temporary home and some young men from the Bikini Atoll population went ahead to begin constructing living accommodations. But U.S. Trust Authorities changed their mind. They decided to use Enewetak Atoll as a second nuclear weapons test site and relocated that atoll's residents to Ujelang Atoll instead and to the homes built for the Bikini Islanders.\n\nIn March 1948, 184 malnourished Bikini islanders were temporarily relocated again to Kwajalein Atoll. In June 1948 the Bikini residents chose Kili Island as a long-term home. The small, () island, one of the smallest islands in the Marshall Island chain, was uninhabited and wasn't ruled by a paramount \"iroij\", or king. In November 1948, the residents, now totaling 184 individuals, moved there. Living on Kili Island effectively destroyed their culture that had been based on fishing and island-hopping canoe voyages to various islets around the Bikini Atoll. Kili does not provide enough food for the transplanted residents.\n\nIn June 1968, based on scientific advice that the radiation levels were sufficiently reduced, President Lyndon B. Johnson promised the 540 Bikini Atoll families living on Kili and other islands that they would be able to return to their home. But the Atomic Energy Commission learned that the coconut crabs, an essential food source, retained high levels of radioactivity and could not be eaten. The Bikini Council voted to delay a return the island as a result.\n\nIn 1987, a few Bikini elders returned to the island to reestablish old property lines. Construction crews began building a hotel on Bikini, and installed generators, desalinators, and power lines. A packed coral and sand runway still exists on Enyu Island. Three extended families, eventually totaling about 100 people, moved back to their home island in 1972 despite the risk. But 10 years later a team of French scientists found some wells were too radioactive for use and determined that the pandanus and breadfruit were also dangerous for human consumption. Women were experiencing miscarriages, stillbirths, and genetic abnormalities in their children. The U.S.-administered Strategic Trust Territory decided that the islanders had to be evacuated from the atoll a second time.\n\nFollowing their evacuation from the island, an 11-year-old boy, born on Bikini in 1971, died from cancer that was linked to radiation exposure he received while living on Bikini. The records obtained by the Marshallese Nuclear Claims Tribunal later revealed that Dr. Robert Conard, head of Brookhaven National Laboratory's medical team in the Marshall Islands, understated the risk of returning to the atoll. Dr. Konrad Kotrady was contracted by Brookhaven National Laboratory (BNL) to treat the Marshall Island residents. In 1977, he wrote a 14-page report to BNL that raised serious questions about the residents' return to Bikini and questioned the accuracy of Brookhaven's prior work on the islands. After they were promised their home was safe, and then being removed after this was found to be wrong, the Bikini Atoll islanders grew to distrust the official reports of the U.S. scientists.\n\nThe special International Atomic Energy Agency (IAEA) Bikini Advisory Group determined in 1997 that \"It is safe to walk on all of the islands ... although the residual radioactivity on islands in Bikini Atoll is still higher than on other atolls in the Marshall Islands, it is not hazardous to health at the levels measured ... The main radiation risk would be from the food: eating locally grown produce, such as fruit, could add significant radioactivity to the body...Eating coconuts or breadfruit from Bikini Island occasionally would be no cause for concern. Eating many over a long period of time without having taken remedial measures, however, might result in radiation doses higher than internationally agreed safety levels.\" IAEA estimated that living in the atoll and consuming local food would result in an effective dose of about 15 mSv/year.\n\nAfter the aborted resettlement in 1978, the leaders of the Bikini community have insisted since the early 1980s that the top of soil be excavated from the entire island. Scientists reply that while removing the soil would rid the island of cesium-137, it would also severely damage the environment, turning the atoll into a virtual wasteland of windswept sand. The Bikini Council has repeatedly contended that removing the topsoil is the only way to guarantee future generations safe living conditions.\n\nIn 1997, researchers found that the dose received from background radiation on the island was between 2.4 mSv/year—the same as natural background radiation—and 4.5 mSv/year, assuming that residents consumed a diet of imported foods. Because the local food supply is still irradiated, the group did not recommend resettling the island. A 1998 International Atomic Energy Agency report found that Bikini is still not safe for habitation because of dangerous levels of radiation.\n\nA 2002 survey found that the coral inside the Bravo Crater has partially recovered. Zoe Richards of the ARC Centre of Excellence for Coral Reef Studies and James Cook University observed matrices of branching Porites coral up to 8 meters high.\n\nIn 1975 the islanders, who had returned to Bikini Atoll and later learned that it wasn't safe, sued the United States for the first time. They demanded a radiological study of the northern islands. In 1975, the United States set up \"The Hawaiian Trust Fund for the People of Bikini\", totaling $3 million. When the islanders were removed from the island in 1978, the U.S. added $3 million to the fund. The U.S. created a second trust fund, \"The Resettlement Trust Fund for the People of Bikini\", containing $20 million in 1982. The U.S. added another $90 million to that fund to pay to clean up, reconstruct homes and facilities, and resettle the islanders on Bikini and Eneu islands.\n\nIn 1983, the U.S. and the Marshall islanders signed the Compact of Free Association, which gave the Marshall Islands independence. The Compact became effective in 1986 and was subsequently modified by the Amended Compact that became effective in 2004. It also established the Nuclear Claims Tribunal, which was given the task of adjudicating compensation for victims and families affected by the nuclear testing program. Section 177 of the compact provided for reparations to the Bikini islanders and other northern atolls for damages. It included $75 million to be paid over 15 years. On March 5, 2001, the Nuclear Claims Tribunal ruled against the United States for damages done to the islands and its people.\n\nThe payments began in 1987 with $2.4 million paid annually to entire Bikini population, while the remaining $2.6 million is paid into \"The Bikini Claims Trust Fund\". This trust is intended to exist in perpetuity and to provide the islanders a 5% payment from the trust annually.\n\nThe United States provided $150 million in compensation for damage caused by the nuclear testing program and their displacement from their home island.\n\nBy 2001, of the original 167 residents who were relocated, 70 were still alive, and the entire population has grown to 2800. Most of the islanders and their descendants live on Kili, in Majuro, or in the United States. The opportunity for some Bikini islanders to potentially relocate back to their home island creates a dilemma. Only a few living islanders were born there. Most of the younger generation have never lived there or even visited and do not have a desire to return. Unemployment in the Marshall Islands was as of 2013 at about 40 percent. The population is growing at a four percent growth rate, so increasing numbers are taking advantage of terms in the Marshall Islands' Compact of Free Association that allow them to obtain jobs in the United States.\n\nA 2017 study led by Steve Palumbi, a Stanford University professor of marine sciences, reported ocean life that seems highly resilient to the effects of radiation poisoning. The team described substantial diversity in the marine ecosystem, with animals appearing healthy to the naked eye, and according to Palumbi, the atoll's \"lagoon is full of schools of fish all swirling around the living coral. In a strange way they are protected by the history of this place, the fish populations are better than in some other places because they have been left alone, the sharks are more abundant and the coral are big. It is a remarkable environment, quite odd.\" Both corals and long-lived animals such as coconut crabs should be vulnerable to radiation-induced cancers, and understanding how they have thrived in might lead to discoveries about preserving DNA – making Bikini Atoll, according to Pambuli, \"an ironic setting for research that might help people live longer\" by improving the scientific understanding of cancer. PBS documented field work undertaken by Palumbi and his graduate student Elora López on Bikini Atoll for the second episode (\"Violent\") of their series \"Big Pacific.\" The episode explored \"species, natural phenomena and behaviors of the Pacific Ocean\" and the way the team is using DNA sequencing to study the rate and pattern of any mutations. López suggested possible explanations for the health of the marine life to \"The Stanford Daily\", such as a mechanism for DNA repair which is superior to that possessed by humans, or a method of maintaining a genome in the face of nuclear radiation.\n\nThe area has become, in effect, something of an unplanned sanctuary – as has also occurred in Europe in the Chernobyl exclusion zone – where the effects of radiation on animal life is being tested. Making an observation similar to that following the 1986 Chernobyl disaster, where serious animal deformities and mutations were abundant only in the immediate aftermath, Palumbi suggested that, as \"fish have relatively short life-spans, it is possible the worst-affected fish died off many decades ago ... and the fish living in Bikini Atoll today are only subject to low-levels of radiation exposure as they frequently swim in and out of the atoll.\" Nurse sharks have two dorsal fins but possibly-mutated individuals with only a single fin were observed. Pambuli and his team have focussed on the hubcap-sized crabs as their coconut diet is contaminated with radioactive caesium-137 from groundwater and on the corals because both have longer life-spans that allow the scientists \"to delve into what effect the radiation exposure has had on the animals' DNA after building up in their systems for many years.\"\n\nBikini Atoll remains uninhabitable for humans due to what United Nations special rapporteur Călin Georgescu reported in 2012 as \"near-irreversible environmental contamination.\" Gamma radiation levels in 2016 averaged 184 mrem yr, well above the maximum allowed for human habitation of 100 mrem yr, thereby rendering the water, seafood, and plants as all unsafe for human consumption. In his book \"Strange Glow: The Story of Radiation\", Timothy Jorgensen reported on the increased cancer risk, especially for leukemia and thyroid cancers, amongst inhabitants of nearby islands. Given this broader context, the resilience and apparent cancer-resistance of marine organism is particularly interesting. As Palumbi stated: \"The fact there is life there and the life there is trying to come back from the most violent thing we've ever done to it is pretty hopeful.\"\n\nThe inhabitants of the Marshall Islands, particularly those closest to Bikini Atoll, were exposed to high levels of radiation. The highest levels of radiation exposure were found in the areas of local fallout. The fallout produced from nuclear tests can affect the human populations internally or externally. External irradiation is from penetrating gamma rays that come from particles on the ground. The levels of external radiation exposure can be reduced if one was indoors because buildings act as a shield. Inhalation of radioactive fallout and epidermal absorption are the primary means of irradiation. However most exposure is from consumption of food that has been contaminated through fallout. The people of the islands would consume meat or products from animals that had been irradiated, therefore irradiating the consumer. Food shipped into the islands was also affected by contamination through contaminated cooking utensils. Many dairy products, such as milk and yogurt, were contaminated as a result of radionuclides landing on pastures. Iodine-131, a highly radioactive isotope, was ingested or inhaled by many through various forms. The iodine-131 consumed would become concentrated in one's thyroid.\n\nOn the Marshall Islands, the detonation of Castle Bravo was the cause of most of the radiation exposure to the surrounding populations. The fallout levels attributed to the Castle Bravo test are the highest in history. The exposure to fallout has been linked to increase the likelihood of several types of cancer such as leukemia and thyroid cancer. The relationship between I-131 levels and thyroid cancer is continuing to be researched. There are also correlations between fallout exposure levels and diseases such as thyroid disease like hypothyroidism. Populations of the Marshall Islands that received significant exposure to radionuclides have a much greater risk of developing cancer. The Castle Bravo test detonation produced an explosion of approximately 15 megatons, which is about three times its predicted value. Populations neighboring the test site were exposed to high levels of radiation resulting in mild radiation sickness of many (nausea, vomiting, diarrhea). Several weeks later, many people began suffering from alopecia (hair loss) and skin lesions as well. The female population of the Marshall Islands have a sixty times greater cervical cancer mortality than a comparable mainland United States population. The Islands populations also have a five time greater likelihood of breast or gastrointestinal mortality, and lung cancer mortality is three times higher than the mainland population. The male population on the Marshall Islands lung cancer mortality is four times greater than the overall United States rates, and the oral cancer rates are ten times greater.\n\nThere is a presumed association between radiation levels and female reproductive system functioning.\n\n"}
{"id": "55910824", "url": "https://en.wikipedia.org/wiki?curid=55910824", "title": "OpenCitations", "text": "OpenCitations\n\nOpenCitations (established in 2010) is a project aiming to publish open bibliographic citation information in RDF. It produces the \"OpenCitations Corpus\" database.\n\n"}
{"id": "33497008", "url": "https://en.wikipedia.org/wiki?curid=33497008", "title": "Paperless society", "text": "Paperless society\n\nA Paperless society is a society in which paper communication (written documents, mail, letters, etc.) is replaced by electronic communication and storage. The concept originated by Frederick Wilfrid Lancaster in 1978. Furthermore, libraries would no longer be needed to handle printed documents. \"Librarians will, in time, become information specialists in a deinstitutionalized setting\" (Lancaster & Smith, 1980). Lancaster also stated that both computers and libraries will not always give us the information that other people and living life will.\n\n\n"}
{"id": "15091455", "url": "https://en.wikipedia.org/wiki?curid=15091455", "title": "Parlour boarder", "text": "Parlour boarder\n\nA parlour boarder is an archaic term for a privileged category of pupil at a boarding school. Parlour boarders are described by a modern historian as paying more than the other pupils, in return for which they got a room of their own A parlour was a small reception room, from the French \"parler\", implying a place for quiet conversation; \"board\" means meals, as in the expression room and board. The term is mostly historic in British English.\n\nIn 18th and 19th century England, there were a profusion of small schools, always single-sex, with the number of pupils ranging from fewer than a dozen to a few score, on a much more domestic scale than the so-called public schools such as Eton and Harrow. Many of these small schools were operated on a family basis, often by a married couple (for boys), or by sisters or female friends (for girls). They would accept day pupils, common boarders, and parlour boarders.\n\nElizabeth Lachlan was at school in London when its owner, a Miss Shepherd, impulsively decided to move her school from Percy Street to France during the Peace of Amiens in 1802. She set out on this venture with \"thirty to forty girls of respectable families, and ten or twelve ladies as parlour boarders\".\n\nThomas Reynolds (1771–1836), the Irish informer, son of a wealthy textile manufacturer, was sent at eight years old as a parlour boarder to the school of Rev. Archibald Crawford at Chiswick, then a village on the outskirts of London.\n\nThe Indian diplomat Venkata Siddharthacharry was largely educated in England, and entitled a chapter of his memoir \"Parlour Boarder\". He defines it as a situation that allows access \"to both the family dining room and the family drawing room\", \"a great privilege naturally, paid for sumptuously\". One much-valued benefit was the fire, which was lit from mid-autumn \"right up to the end of spring\", in contrast to the frigid dormitories.\n\nThe Jesuit school named after Francis de Sales in Nagpur, India, even in the mid 20th century:\n\nOne Anglo-Indian family sent the light-skinned son as a parlour boarder, while his darker brothers were merely ordinary boarders.\n\n"}
{"id": "73355", "url": "https://en.wikipedia.org/wiki?curid=73355", "title": "Pioneer Venus Orbiter", "text": "Pioneer Venus Orbiter\n\nThe Pioneer Venus Orbiter, also known as Pioneer Venus 1 or Pioneer 12, was a mission to Venus conducted by the United States as part of the Pioneer Venus project. Launched in May 1978 atop an Atlas-Centaur rocket, the spacecraft was inserted into an elliptical orbit around Venus on December 4, 1978. It returned data on Venus until October 1992.\n\nThe Pioneer Venus Orbiter was launched by an Atlas SLV-3D Centaur-D1AR rocket, which flew from Launch Complex 36A at the Cape Canaveral Air Force Station. The launch occurred at 13:13:00 on May 20, 1978, and deployed the Orbiter into heliocentric orbit for its coast to Venus. Venus orbit insertion occurred on December 4, 1978.\n\nManufactured by Hughes Aircraft Company, the Pioneer Venus Orbiter was based on the HS-507 bus. The spacecraft was a flat cylinder, in diameter and long. All instruments and spacecraft subsystems were mounted on the forward end of the cylinder, except the magnetometer, which was at the end of a boom. A solar array extended around the circumference of the cylinder. A despun dish antenna provided S and X band communication with Earth. A Star-24 solid rocket motor was integrated into the spacecraft to provide the thrust to enter orbit around Venus.\n\nFrom Venus orbit insertion to July 1980, periapsis was held between (at 17 degrees north latitude) to facilitate radar and ionospheric measurements. The spacecraft was in a 24-hour orbit with an apoapsis of . Thereafter, the periapsis was allowed to rise to a maximum of and then fall, to conserve fuel.\n\nIn 1991, the Radar Mapper was reactivated to investigate previously inaccessible southern portions of the planet, in conjunction with the recently arrived Magellan spacecraft. In May 1992, Pioneer Venus began the final phase of its mission, in which the periapsis was held between , until the spacecraft's propellant was exhausted, after which the orbit decayed naturally. The spacecraft continued to return data until 8 October 1992, with the last signals being received at 19:22 UTC. The Pioneer Venus Orbiter disintegrated upon entering the atmosphere of Venus on October 22, 1992.\n\nThe Pioneer Venus Orbiter carried 17 experiments with a total mass of :\n\nThe spacecraft conducted radar altimetry observations allowing the first global topographic map of the Venusian surface to be constructed.\n\nFrom its orbit of Venus, the Pioneer Venus Orbiter was able to observe Halley's Comet when it was unobservable from Earth due to its proximity to the sun during February 1986. UV spectrometer observations monitored the loss of water from the comet's nucleus at perihelion on February 9.\n\n\n\n"}
{"id": "19349690", "url": "https://en.wikipedia.org/wiki?curid=19349690", "title": "Practical engineer", "text": "Practical engineer\n\nA practical engineer (Senior Technician)() is a professional degree awarded by technological colleges in Israel and validated by the National Institute for Technological Training of the Ministry of Industry, Trade & Labor. It is a unique qualification that combines theoretical study with practical training, and is currently available only in Israel. It normally takes up to 3 academic years to obtain a Practical Engineer's degree, depending on the taught discipline.\n\nPractical Engineers are used to mediate between professional engineers and engineering technicians. While most of the decision making process is performed by professional engineers and most of the practical work is done by technicians, practical engineers are taught to be able to follow instructions and re-adapt concepts designed by professional engineers.\n\nIn Israel, practical engineers are an integral part of the hi-tech industry and form an important work-force.\n\nPractical Engineers are licensed to lead and hold responsible for engineering projects up to a certain level. For example, in Israel, architectural practical engineers are permitted to design constructions of up to four stories high with 2 dimensional static scheme and spans up to 6 meters, while for larger scale projects a full engineering degree is required.\n\nPractical engineers receive formal education that meets with criteria published by the National Institute for Technological Training.\nIn sum over 2100 frontal academic hours are taught over a period of two to three years.\n\nFrom an academic point of view, the main things differentiating engineers from practical engineers are the level and magnitude of foundation subjects, such as mathematics and physics.\n\nPractical Engineers are taught foundations but only to the extent required by their profession, hence making their study shorter and more focused.\n\nAs part of the certification process, practical engineers are also required to undertake national certification tests, and to complete and defend a final project in their field of study or to participate in an internship program.\n\nThere are some international qualifications equivalent to the Practical Engineer's degree. In Spain, the title of Ingeniero Técnico (Spanish for Technical Engineer) bears many similarities to its Israeli equivalent. Also, although not exactly the same, Practical Engineers are often thought of as Engineering technicians, Associate Engineers or Jr. Engineers in some English speaking countries. The French Brevet de Technicien Supérieur is similar.\n\nAlthough not an academic degree in its traditional sense, practical engineers are normally awarded some academic credit when pursuing further study at a Bachelor's degree\n\n"}
{"id": "572830", "url": "https://en.wikipedia.org/wiki?curid=572830", "title": "S-IC", "text": "S-IC\n\nThe S-IC (pronounced \"ess one see\") was the first stage of the American Saturn V rocket. The S-IC stage was built by the Boeing Company. Like the first stages of most rockets, most of its mass of more than 2,000 tonnes at launch was propellant, in this case RP-1 rocket fuel and liquid oxygen (LOX) oxidizer. It was 42 meters tall and 10 meters in diameter, and provided 33,000 kN of thrust to get the rocket through the first 61 kilometers of ascent. The stage had five F-1 engines in a quincunx arrangement. The center engine was fixed in position, while the four outer engines could be hydraulically gimballed to control the rocket.\n\nThe Boeing Co. was awarded the contract to manufacture the S-IC on December 15, 1961. By this time the general design of the stage had been decided on by the engineers at the Marshall Space Flight Center (MSFC). The main place of manufacture was the Michoud Assembly Facility, New Orleans. Wind tunnel testing took place in Seattle and the machining of the tools needed to build the stages at Wichita, Kansas.\n\nMSFC built the first three test stages (S-IC-T, the S-IC-S, and the S-IC-F) and the first two flight models (S-IC-1 and -2). They were built using tools produced in Wichita.\n\nIt took roughly seven to nine months to build the tanks and 14 months to complete a stage. The first stage built by Boeing was S-IC-D, a test model.\n\nThe largest and heaviest single component of the S-IC was the thrust structure, with a mass of 21 tonnes. It was designed to support the thrust of the five engines and redistribute it evenly across the base of the rocket. There were four anchors which held down the rocket as it built thrust. These were among the largest aluminum forgings produced in the U.S. at the time, 4.3 meters long and 816 kilograms in weight. The four stabilizing fins withstood a temperature of 1100 °C.\n\nAbove the thrust structure was the fuel tank, containing 770,000 liters of RP-1 fuel. The tank itself had a mass of 11 tonnes dry and could release 7300 liters per second. Nitrogen was bubbled through the tank before launch to keep the fuel mixed. During flight the fuel was pressurized using helium, that was stored in tanks in the liquid oxygen tank above.\n\nBetween the fuel and liquid oxygen tanks was the intertank.\n\nThe liquid oxygen tank held 1,305,000 liters of LOX. It raised special issues for the designer. The lines through which the LOX ran to the engine had to be straight and therefore had to pass through the fuel tank. This meant insulating these lines inside a tunnel to stop fuel freezing to the outside and also meant five extra holes in the top of the fuel tank.\n\nTwo solid motor retrorockets were located inside each of the four conical engine fairings. At separation of the S-IC from the flight vehicle, the eight retrorockets fired, blowing off removable sections of the fairings forward of the fins, and backing the S-IC away from the flight vehicle as the engines on the S-II stage were ignited.\n\nIt also carried the ODOP transponder to track the flight after takeoff.\n\n\n"}
{"id": "5336467", "url": "https://en.wikipedia.org/wiki?curid=5336467", "title": "Safety valve theory", "text": "Safety valve theory\n\nThe safety valve theory was a theory about how to deal with unemployment which gave rise to the Homestead Act of 1862 in the United States. Given the concentration of immigrants (and population) on the Eastern coast, it was hypothesized that making free land available in the West would relieve the pressure for employment in the East. By analogy with steam pressure (= the need for work), the enactment of a free land law, it was believed, would act as a safety valve. This theory meant that if the East started filling up with immigrants, you could always go West until they reached a point where they could not move any farther.\n\nA distinction has to be made between (1) the safety valve theory as an ideal and (2) the safety valve theory as embodied in the Homestead Act of 1862.\n\nThere is a dispute whether and to what extent the Homestead Act did or did not succeed as a safety valve in ameliorating the problem of unemployment in the East.\n\nOpposition to giving away free land came from employers, who anticipated either a shortage of employees or conditions favorable to employees.\n\n"}
{"id": "48304379", "url": "https://en.wikipedia.org/wiki?curid=48304379", "title": "Sensitivity auditing", "text": "Sensitivity auditing\n\nSensitivity auditing is an extension of sensitivity analysis for use in policy-relevant modelling studies. Its use is recommended - e.g. in the European Commission Impact assessment guidelines - when a sensitivity analysis (SA) of a model-based study is meant to demonstrate the robustness of the evidence provided by the model, but in a context where the inference feeds into a policy or decision-making process.\n\nIn these cases, the framing of the analysis itself, its institutional context, and the motivations of its author may become highly relevant, and a pure SA - with its focus on parametric (i.e. quantified) uncertainty - may be insufficient. The emphasis on the framing may, among other things, derive from the relevance of the policy study to different constituencies that are characterized by different norms and values, and hence by a different story about `what the problem is' and foremost about `who is telling the story'. Most often the framing includes implicit assumptions, which could be political (e.g. which group needs to be protected) all the way to technical (e.g. which variable can be treated as a constant).\n\nIn order to take these concerns into due consideration, sensitivity auditing extends the instruments of sensitivity analysis to provide an assessment of the entire knowledge- and model-generating process. It takes inspiration from NUSAP, a method used to qualify the worth (quality) of quantitative information with the generation of `Pedigrees' of numbers. Likewise, sensitivity auditing has been developed to provide pedigrees of models and model-based inferences. Sensitivity auditing is especially suitable in an adversarial context, where not only the nature of the evidence, but also the degree of certainty and uncertainty associated to the evidence, is the subject of partisan interests. These are the settings considered in Post-normal science or in Mode 2 science. Post-normal science (PNS) is a concept developed by Silvio Funtowicz and Jerome Ravetz, which proposes a methodology of inquiry that is appropriate when “facts are uncertain, values in dispute, stakes high and decisions urgent” (Funtowicz and Ravetz, 1992: 251–273). Mode 2 Science, coined in 1994 by Gibbons et al., refers to a mode of production of scientific knowledge that is context-driven, problem-focused and interdisciplinary. Carrozza (2015) offers a discussion of these concepts and approaches.\n\nSensitivity auditing is recommended by the European Commission for use in impact assessments in order to improve the quality of model-based evidence used to support policy decisions.\n\nSensitivity auditing is summarised by seven rules or guiding principles:\n\nThe first rule looks at the instrumental use of mathematical modeling to advance one's agenda. This use is called rhetorical, or strategic, like the use of Latin to confuse or obfuscate an interlocutor.\n\nThe second rule about `assumption hunting' is a reminder to look for what was assumed when the model was originally framed. Modes are full of ceteris paribus assumptions. For example, in economics, the model can predict the result of a shock to a given set of equations, assuming that all the rest - all other input variables and inputs - remain equal, but in real life \"ceteris\" are never \"paribus\", meaning that variables tend to be linked with one another, so they cannot realistically change independently of one another.\n\nRule three is about artificially exaggerating or playing down uncertainties wherever convenient. The tobacco lobbies exaggerated the uncertainties about the health effects of smoking according to Oreskes and Conway, while advocates of the death penalty played down the uncertainties in the negative relations between capital punishment and crime rate. Clearly the latter wanted the policy, in this case the death penalty, and were interested in showing that the supporting evidence was robust. In the former case the lobbies did not want regulation (e.g. bans on tobacco smoking in public places) and were hence interested in amplifying the uncertainty in the smoking-health effect causality relationship.\n\nRule four is about `confessing' uncertainties before going public with the analysis. This rule is also one of the commandments of applied econometrics according to\nKennedy: `Thou shall confess in the presence of sensitivity. Corollary: Thou shall anticipate criticism'. According to this rule, a sensitivity analysis should be performed before the results of a modeling study are published. There are many good reasons for doing this, one being that a carefully performed sensitivity analysis often uncovers plain coding mistakes or model inadequacies. The other is that, more often than not, the analysis reveals uncertainties that are larger than those anticipated by the model developers.\n\nRule five is about presenting the results of the modeling study in a transparent fashion. Both rules originate from the practice of impact assessment, where a modeling study presented without a proper SA, or as originating from a model which is in fact a black box, may end up being rejected by stakeholders. Both rules four and five suggest that reproducibility may be a condition for transparency and that this latter may be a condition for legitimacy.\n\nRule six, about doing the right sum, is not far from the `assumption-hunting' rule; it is just more general. It deals with the fact that often an analyst is set to work on an analysis arbitrarily framed to the advantage of a party. Sometime this comes via the choice of the discipline selected to do the analysis. Thus an environmental impact problem may be framed through the lenses of economics, and presented as a cost benefit or risk analysis, while the issue has little to do with costs or benefits or risks and a lot to do with profits, controls, and norms. An example is in Marris et al. on the issue of GMOs, mostly presented in the public discourse as a food safety issue while the spectrum of concerns of GMO opponents - including lay citizens - appears broader.\n\nRule seven is about avoiding a perfunctory sensitivity analysis. A SA where each uncertain input is moved at a time while leaving all other inputs fixed is perfunctory. A true SA should make an honest effort at exploring all uncertainties simultaneously, leaving the model free to display its full nonlinear and possibly non-additive behaviour. A similar point is made in Sam L. Savage's book `The flaw of averages'.\n\nIn conclusion, these rules are meant to help an analyst to anticipate criticism, in particular relating to model-based inference feeding into an impact assessment. What questions and objections may be received by the modeler? Here is a possible list:\n\nSensitivity auditing is described in the European Commission Guidelines for impact assessment . Relevants excerpts are (pp. 392): \n"}
{"id": "50631177", "url": "https://en.wikipedia.org/wiki?curid=50631177", "title": "Simple clinical colitis activity index", "text": "Simple clinical colitis activity index\n\nThe Simple Clinical Colitis Activity Index (SCCAI) is a diagnostic tool and questionnaire used to assess the severity of symptoms in people who have from ulcerative colitis. It was created in 1998 and is still used to assess the severity of symptoms. It is also used for research purposes to determine the efficacy of various treatments aimed at relieving symptoms. The calculated score ranges from 0 to 19, where active disease is a score of 5 or higher.\n\nThe score is determined by asking the person with colitis questions regarding:\n\n"}
{"id": "18776424", "url": "https://en.wikipedia.org/wiki?curid=18776424", "title": "Technological apartheid", "text": "Technological apartheid\n\nTechnological apartheid is the denial of useful modern technologies to Third World or developing nations. The term is based upon the South African term \"apartheid\", partly refers to the practice of keeping certain populations in a separate, lower-class status. It has been used to describe situations that are unintended, such as the absence of computers and information technology in the favelas of Brazil or other impoverished areas. The term also applies to the deliberate denial of technology for geopolitical or neocolonialist reasons. The government of Iran has characterized the efforts by Western governments to deny nuclear technology to Iran as technological apartheid.\n\nSome of the technologies in question are dual-use technologies, advanced technologies which can have both civilian and military applications. Some commentators allege that the issue of dual-use technologies is a red herring, and that some advanced-sector nations, who wish to keep the Third World nations as poor client states, withhold technologies that are essential for economic development, using the pretext that they will be used for military purposes.\n"}
{"id": "27636262", "url": "https://en.wikipedia.org/wiki?curid=27636262", "title": "The All-Earth Ecobot Challenge", "text": "The All-Earth Ecobot Challenge\n\nThe All-Earth Ecobot Challenge (or Ecobots) is a competition that occurs yearly for students throughout Texas in grades 5-8 that started in 2009. It encourages students to be creative, learn more about robots, learn about the environment and how to help protect it, and prepares them for future jobs. The Ecobot Challenge uses various Lego pieces, Lego NXT sets, and PowerPoint.\n\nThe All Earth Ecobot Challenge happens once a year for students in Texas. It started in 2009 and is for grades 5-8 It helps students learn about the environment, robots, creativity, and preparation for their future careers.\n\nUsually, a teacher from each school helps coaches teams that come before and/or after school to prepare by programming their NXT and sometimes creating their Marketing Presentation(which could also be done at home, but not the NXT programming) in that teacher's classroom. Usually, the team starts to program and make their presentation in early March of earlier.\n\nThe competition held yearly consists of two sections: the Robotics section and the Marketing section. They are two separate parts to the competition. The competition is usually help around mid-April. The robot in the competition is only allowed to be touched in \"Home\". Home is where the robot starts and ends when completing challenges.\n\nThis section requires students to program a Lego NXT to complete various tasks that change each year. The more tasks students complete, the more points they get (for a maximum of 300 points). To complete the various tasks, students build Lego attachments usually attached to the motors included in the Lego NXT kit.\n\nThis section requires students to think of and to make a PowerPoint presentation about an imaginary robot that will help the Earth environmentally. They present their presentation to judges who score them.\n\nThe Lego NXT sets and the competition is costly. Ecobots rely on sponsors to fund the competition. For a list of sponsors, go to their sponsor list page.\n\nTo learn more go to the Ecobots home page.\n"}
{"id": "31358", "url": "https://en.wikipedia.org/wiki?curid=31358", "title": "The Art of Computer Programming", "text": "The Art of Computer Programming\n\nThe Art of Computer Programming (sometimes known by its initials TAOCP) is a comprehensive monograph written by Donald Knuth that covers many kinds of programming algorithms and their analysis.\n\nKnuth began the project, originally conceived as a single book with twelve chapters, in 1962. The first three volumes of what was then expected to be a seven-volume set were published in 1968, 1969, and 1973. The first published installment of Volume 4 appeared in paperback as Fascicle 2 in 2005.\nThe hardback Volume 4A, combining Volume 4, Fascicles 0–4, was published in 2011. Volume 4, Fascicle 6 (\"Satisfiability\") was released in December 2015, to be followed by Volume 4, Fascicle 5 (\"Mathematical Preliminaries Redux; Backtracking; Dancing Links\") in December 2018. Fascicles 5 and 6 are expected to comprise the first two thirds of Volume 4B.\n\nAfter winning a Westinghouse Talent Search scholarship, Knuth enrolled at the Case Institute of Technology (now Case Western Reserve University), where his performance was so outstanding that the faculty voted to award him a master of science upon his completion of the baccalaureate degree. During his summer vacations, Knuth was hired by the Burroughs Corporation to write compilers, earning more in his summer months than full professors did for an entire year. Such exploits made Knuth a topic of discussion among the mathematics department, which included Richard S. Varga.\n\nKnuth started to write a book about compiler design in 1962, and soon realized that the scope of the book needed to be much larger. In June 1965, Knuth finished the first draft of what was originally planned to be a single volume of twelve chapters. His hand-written first-draft manuscript (completed in 1966) was pages long: he had assumed that about five hand-written pages would translate into one printed page, but his publisher said instead that about 1½ hand-written pages translated to one printed page. This meant the book would be approximately pages in length. The publisher was nervous about accepting such a project from a graduate student. At this point, Knuth received support from Richard S. Varga, who was the scientific adviser to the publisher. Varga was visiting Olga Taussky-Todd and John Todd at Caltech. With Varga's enthusiastic endorsement, the publisher accepted Knuth's expanded plans. In its expanded version, the book would be published in seven volumes, each with just one or two chapters. Due to the growth in the material, the plan for Volume 4 has since expanded to include Volumes 4A, 4B, 4C, 4D, and possibly more.\n\nIn 1976, Knuth prepared a second edition of Volume 2, requiring it to be typeset again, but the style of type used in the first edition (called hot type) was no longer available. In 1977, he decided to spend some time creating something more suitable. Eight years later, he returned with TX, which is currently used for all volumes.\n\nThe offer of a so-called Knuth reward check worth \"one hexadecimal dollar\" (100 base 16 cents, in decimal, is $2.56) for any errors found, and the correction of these errors in subsequent printings, has contributed to the highly polished and still-authoritative nature of the work, long after its first publication. Another characteristic of the volumes is the variation in the difficulty of the exercises. The level of difficulty ranges from \"warm-up\" exercises to unsolved research problems. \n\nKnuth's dedication reads:\n\nThis series of books is affectionately dedicated<br>to the Type 650 computer once installed at<br>Case Institute of Technology,<br>with whom I have spent many pleasant evenings.\n\nAll examples in the books use a language called \"MIX assembly language\", which runs on the hypothetical MIX computer. Currently, the MIX computer is being replaced by the MMIX computer, which is a RISC version. Software such as GNU MDK exists to provide emulation of the MIX architecture. Knuth considers the use of assembly language necessary for the speed and memory usage of algorithms to be judged.\n\nKnuth was awarded the 1974 Turing Award \"for his major contributions to the analysis of algorithms […], and in particular for his contributions to the 'art of computer programming' through his well-known books in a continuous series by this title.\" \"American Scientist\" has included this work among \"100 or so Books that shaped a Century of Science\", referring to the twentieth century, and within the computer science community it is regarded as the first and still the best comprehensive treatment of its subject. Covers of the third edition of Volume 1 quote Bill Gates as saying, \"If you think you're a really good programmer… read (Knuth's) \"Art of Computer Programming\"… You should definitely send me a résumé if you can read the whole thing.\" \"The New York Times\" referred to it as \"the profession's defining treatise\".\n\n\n\n\n\n\nThese are the current editions in order by volume number:\n\nThese volumes were superseded by newer editions and are in order by date.\n\nVolume 4 fascicles 0–4 were revised and published as Volume 4A.\n\nVolume 4 pre-fascicles 5A–5C and 6A were revised and published as fascicles 5 and 6.\n\n\nNotes\nCitations\nSources\n"}
{"id": "49158253", "url": "https://en.wikipedia.org/wiki?curid=49158253", "title": "Thonny", "text": "Thonny\n\nThonny is an integrated development environment for Python that is designed for beginners. It supports different ways of stepping through the code, step-by-step expression evaluation, detailed visualization of the call stack and a mode for explaining the concepts of references and heap.\n\nThe program works on Windows, macOS and Linux. It is available as binary bundle including recent Python interpreter or pip-installable package. It can be installed via operating-system package manager on Debian, Raspberry Pi, Ubuntu and Fedora.\n\nThonny has received favorable reviews from Python and computer science education communities. \nIt has been recommended tool in several programming MOOCs.\nSince June 2017 it has been included by default in the Raspberry Pi's official operating system distribution Raspbian.\n\n\n"}
{"id": "532379", "url": "https://en.wikipedia.org/wiki?curid=532379", "title": "Type species", "text": "Type species\n\nIn zoological nomenclature, a type species (\"species typica\") is the species name with which the name of a genus or subgenus is considered to be permanently taxonomically associated, i.e., the species that contains the biological type specimen(s). A similar concept is used for suprageneric groups called a type genus.\n\nIn botanical nomenclature, these terms have no formal standing under the code of nomenclature, but are sometimes borrowed from zoological nomenclature. In botany, the type of a genus name is a specimen (or, rarely, an illustration) which is also the type of a species name. The species name that has that type can also be referred to as the type of the genus name. Names of genus and family ranks, the various subdivisions of those ranks, and some higher-rank names based on genus names, have such types.\n\nIn bacteriology, a type species is assigned for each genus.\n\nEvery named genus or subgenus in zoology, whether or not currently recognized as valid, is theoretically associated with a type species. In practice, however, there is a backlog of untypified names defined in older publications when it was not required to specify a type.\n\nA type species is both a concept and a practical system that is used in the classification and nomenclature (naming) of animals. The \"type species\" represents the reference species and thus \"definition\" for a particular genus name. Whenever a taxon containing multiple species must be divided into more than one genus, the type species automatically assigns the name of the original taxon to one of the resulting new taxa, the one that includes the type species.\n\nThe term \"type species\" is regulated in zoological nomenclature by article 42.3 of the \"International Code of Zoological Nomenclature\", which defines a type species as the name-bearing type of the name of a genus or subgenus (a \"genus-group name\"). In the Glossary, type species is defined as\n\nThe type species permanently attaches a formal name (the generic name) to a genus by providing just one species within that genus to which the genus name is permanently linked (i.e. the genus must include that species if it is to bear the name). The species name in turn is fixed, in theory, to a type specimen.\n\nFor example, the type species for the land snail genus \"Monacha\" is \"Helix cartusiana\", the name under which the species was first described, known as \"Monacha cartusiana\" when placed in the genus \"Monacha\". That genus is currently placed within the family Hygromiidae. The type genus for that family is the genus \"Hygromia\".\n\nThe concept of the type species in zoology was introduced by Pierre André Latreille.\n\nThe \"International Code of Zoological Nomenclature\" states that the original name (binomen) of the type species should always be cited. It gives an example in Article 67.1. \"Astacus marinus\" was later designated as the type species of the genus \"Homarus\", thus giving it the name \"Homarus marinus\" . However, the type species of \"Homarus\" should always be cited using its original name, i.e. \"Astacus marinus\" .\n\nAlthough the \"International Code of Nomenclature for algae, fungi, and plants\" does not contain the same explicit statement, examples make it clear that the original name is used, so that the \"type species\" of a genus name need not have a name within that genus. Thus in Article 10, Ex. 3, the type of the genus name \"Elodes\" is quoted as the type of the species name \"Hypericum aegypticum\", not as the type of the species name \"Elodes aegyptica\". (\"Elodes\" is not now considered distinct from \"Hypericum\".)\n\n"}
{"id": "29389753", "url": "https://en.wikipedia.org/wiki?curid=29389753", "title": "UDF 2457", "text": "UDF 2457\n\nUDF 2457 is the Hubble Ultra Deep Field (UDF) identifier for a red dwarf star calculated to be about from Earth with a very dim apparent magnitude of 25.\n\nThe Milky Way galaxy is about 100,000 light-years in diameter, and the Sun is about 25,000 light-years from the galactic center. The small common star UDF 2457 may be one of the farthest known stars inside the main body of the Milky Way. Globular clusters (such as Messier 54 and NGC 2419) and stellar streams are located further out in the galactic halo.\n\n"}
{"id": "49987586", "url": "https://en.wikipedia.org/wiki?curid=49987586", "title": "Understory (Company)", "text": "Understory (Company)\n\nUnderstory (founded in 2012 as WInstruments) is a company that forecasts weather and collects data using a grid of weather-sensing hardware that tracks weather from the ground level.\nAlthough originally known as WInstruments, founder and CEO Alex Kubicek changed the company name to Subsidence and joined Gener8tor, a group in Madison, Wisconsin that provides funding to startups. After receiving funding from Gener8tor, the company moved to Boston to the Bolt hardware accelerator. Kubicek soon renamed the company “Understory,” and combined funding from Gener8tor and Bolt came to $68,000. After raising $1.9 million in seed funding led by True Ventures, with participation by RRE Ventures, Vegas Tech Fund, SK Ventures, and Andrew C. Payne, the company moved from Madison to Boston, Massachusetts. \nHeadquarters was set up in Somerville at the clean tech incubator, Greentown Labs, and the company set up pilot tests in Kansas City, Missouri, Dallas, Texas, and Boston.\nSeries A funding resulted in another $7.5 million for the company, co-led by 4490 Ventures and Monsanto Growth Ventures and joined by CSA Partners, True Ventures, RRE Ventures, and SK Ventures. The company then announced a plan to move back to Madison, Wisconsin.\nThe company’s first customer was American Family Insurance, which uses weather data to adjust claims.\nUnderstory makes solar-powered weather stations that detect three-dimensional rain, hail, wind and other weather in real-time at the ground level, instead of using atmospheric data like traditional weather detectors. Each weather station is about 1 foot wide and 2 feet tall, and connects in a grid through cellular connections. The stations can collect up to 3,000 data points per minute.\n"}
{"id": "41248918", "url": "https://en.wikipedia.org/wiki?curid=41248918", "title": "University of Valencia Science Park", "text": "University of Valencia Science Park\n\nThe University of Valencia Science Park ( also known by the acronym \"PCUV\") provides spaces and services to companies resulting from university research, –spin-off–, and other companies and R & D departments with content related to the innovative nature of the PCUV.\n\nThe PCUV has a scientific area that includes six research institutes, two centres and services and facilities for research; and a business area that currently houses more than seventy young or already established companies, mainly from the sectors of biotechnology and information and communication technologies (ICT).\n\nIn addition, The University of Valencia Science Park houses the technical office of the Emprendia Network (Iberoamerican University Network of Business Incubation), which enables the expansion of businesses in the science parks of universities that make up the network. It is also a member of the rePCV (Network of Valencian Science Parks), APTE (Association of Science and Technology Parks of Spain) and the IASP (International Association of Science and Technology Parks).\n\nThe University of Valencia Science Park Foundation\n\nThe University of Valencia Science Park Foundation was established 9 March 2009 as a private organization with a general interest established under the aegis of the protectorate\nof the Generalitat Valenciana. The founding benefactors are Fundación Bancaja, Banco Santander, the Valencian Chamber of Commerce, the Valencian Business Confederation, and the\nUniversity of Valencia. Foundation manages the Science Park with the purpose of promoting technological development, knowledge transfer and industrial innovation, among other things.\n\nThe PCUV currently includes more than eighty companies generating close to 500 direct jobs, mainly from the ICT and biotechnology sectors.\n\nThe PCUV has six research institutes, four of them from the University of Valencia, one from the National Scientific Research Council (CSIC), and one a joint venture of the University of Valencia and CSIC. The Astronomical Observatory of the institution and the Image Processing Laboratory (IPL) are also housed at the park. Together they form the academic area of this framework for innovation that is the Science Park. They all stand out for their level of collaboration with companies and institutions or for their participation in projects implemented for the benefit of society.\n\nCreated in 1995, the Institute of Materials Science of the University of Valencia focuses on the development of different national and international projects and multiple contracts with industry. Among the lines of research of the ICMUV, the following stand out: the study of quantum semiconductor nanostructures and devices, nanomaterials for energy, high pressure physics, photonic crystals, synthesis and characterization of porous materials and zeotypes, alternative synthesis strategies, surface treatments for laser marking, functional nanomaterials, structured nanomaterials, catalysis, hybrid polymers and historical heritage.\n\nFounded in 2000, the Institute for Molecular Science of the University of Valencia is for chemistry and molecular nanoscience. Its scientific objectives are focused on areas such as the design and synthesis of functional molecules, supramolecular associations and molecular materials with physical or chemical properties of interest. The fields of application range from molecular magnetism and molecular electronics to nanotechnology and biomedicine.\n\nFounded in 1950, the Institute of Corpuscular Physics, a joint centre of the University of Valencia and CSIC, is dedicated to basic research in particle, astroparticle and nuclear physics. Its most direct fields of application are medical physics and GRID technology. Its main lines of research are experimental high energy physics based on accelerators, experimental neutrino and astroparticle physics, experimental nuclear physics, theoretical astroparticle physics, the phenomenology of high energy physics, and nuclear theory, among others.\n\nThe Astronomical Observatory of the University of Valencia is an institution dedicated to research and education, to the study of the Universe and the popularization of astronomy in society. Founded in 1909, it is dedicated to research topics as hot as the study of the nature of dark energy, the evolution of the Universe and its galaxies, the formation and evolution of stars, and the study of near-Earth asteroids.\n\nIRTIC is a research centre of the University of Valencia, founded in the early 90s. Consisting of four research groups associated with the area of information and communication technologies, they develop projects of information management systems, traffic and transport telematics, computer and virtual reality graphics, integration systems for the disabled, civil machinery simulation, network services, computer security and digital image processing.\n\nThe Institute of Agrochemistry and Food Technology (IATA) is a centre of the CSIC in Valencia. Its lines of research include food biotechnology and microbiology, development of processes and technology for food processing and packaging, technologies for food preservation, quality and functionality, and advanced techniques for food analysis.\n\nThe Image Processing Laboratory (IPL) of the University of Valencia consists of four research groups with a common area: imaging (the creation of actual images or of geo-biophysical parameters) from satellite and remote sensing data. The research groups are UCG (Global Change Unit), GPDS (Digital Processing of Signals Group), GACE (Astronomy and Space Sciences Group) and LEO (Laboratory for Earth Observation).\n\nFounded in 1998, the Cavanilles Institute of the University of Valencia is dedicated to the study of biodiversity and evolutionary biology with an integrative and multidisciplinary approach. It has the following research groups: evolutionary genetics, limnology, entomology, evolutionary ecology, plant conservation biology, marine zoology, paleontology, vertebrate ecology, bacteriology, ethology, evolutionary biology of plants, comparative neurobiology and plant biodiversity/ecophysiology.\n\nAt present, cooperation and investment in R&D are the most effective tools that the entrepreneur has to face growing competition and to increase competitiveness. For this purpose, the PCUV has the collaboration of the University of Valencia, whose aims and objectives are to promote university-industry cooperation and the transference of research results: Support organizations include the Technology Transfer Office of the University of Valencia (OTRI), the Valencia University-Enterprise Foundation (ADEIT), the Centre\nof Professional Integration and Employment Counselling (OPAL), and the Office of European Projects (OPER), among others.\n\nThe Central Service for the Support of Experimental Research (SCSIE) is a general service of technology resources whose mission is to provide centralized and comprehensive support for research carried out in the university community, businesses, and public and private institutions.\n\nThe PCUV also offers the following business support services:\n\n"}
{"id": "8967576", "url": "https://en.wikipedia.org/wiki?curid=8967576", "title": "WARFT", "text": "WARFT\n\nWARFT or WAran Research FoundaTion is a nonprofit organization promoting interdisciplinary research among undergraduate students in the city of Chennai, India. Professor N. Venkateswaran founded the group in 2000 and continues to manage it as of 2011. The aim of WARFT is to understand and model the brain to enable drug discovery so that spastic children can live a normal life.\n\nSince its inception, WARFT has researched brain modeling, supercomputing and associated areas. The goal of WARFT is to unravel the connectivity of the human brain regions through the MMINi-DASS project. Biologically accurate brain simulations require massive computational power and thus another research initiative at WARFT is the MIP Project directed towards evolving a design method for the development of a tera-operations supercomputing cluster.\n\nUndergraduate research trainees at WARFT engage themselves in the areas of neuroscience, supercomputing architectures, processor design towards deep sub-micrometre, power-aware computing, low power issues, mixed signal design, fault tolerance and testing, digital signal processing. WARFT conducts Dhi Yantra, a workshop on brain modeling and supercomputing every year.\n\nWARFT's mission is twofold. Firstly to promote innovation and research awareness in the minds of young undergraduate students. In this respect, WARFT conducts a two-year part-time Research Awareness Programme and Training (RAPT) for undergraduate students. Secondly to solve the mysteries of the brain and to hasten the discovery of drugs that can cure brain diseases.\n\nThere are two main inter-disciplinary research initiatives at WARFT :\nThe MMINi-DASS project is a large-scale brain simulation carried out to predict interconnectivity of a specific brain region and makes use of fMRI BOLD response of brain regions. This results in understanding of brain dynamics from the most fundamental level to cognitive and behavioral aspects. Modeling individual brain entities is a challenging task. Predicting their interconnectivity through simulation requires enormous computing power and thus, the project banks on the exponentially increasing computing power and its decreasing cost.\n\nThe immense computational demand imposed by the MMINi-DASS PROJECT has given rise to the novel supercomputer design known as the MIP SCOC. The MIP approach incorporates the memory within the logic, reminiscent of The Berkeley IRAM Project. In the MIP SCOC architecture, memory is physically and logically integrated with the functional units of the processor. This bit-level integration of processing logic and memory has led to a tremendous increase in functionality of a single MIP SCOC node. \n\nThe MIP SCOC architecture includes powerful ALFU (Algorithm Level Functional units) like chain matrix adders, multipliers, sorters, multiple operand adders and graph theoretic units like Depth-First-Search, Breadth-First-Search. This introduces a higher level of abstraction through the algorithm-level instructions (ALISA). A single ALISA is equivalent to multiple parallel VLIW. The MIP SCOC architecture includes an on-chip compiler (Compiler-On-Silicon) to generate the required instructions to feed the ALFUs of the MIP node. The Primary COS (PCOS) partitions the incoming problem according to the algorithms involved. Each SCOS generates the instructions corresponding to that column. A distributed control design is employed specific to ALFU population type (forming different heterogeneous cores) enabling parallel operation of a very large number of ALFUs.\n\nWARFT is divided into seven research groups:\n\nAccording to WARFT's website, it has published 50 research papers as of 2008.\n\nDhi Yantra is a workshop on brain modeling and supercomputing organized by WARFT every year. Three editions of this workshop, featuring scientists and researchers from various fields and geography, have been held. The fourth workshop was held in Chennai, India on July 10, 11 and 12, 2009. \n\n"}
{"id": "789425", "url": "https://en.wikipedia.org/wiki?curid=789425", "title": "Willem Hendrik van den Bos", "text": "Willem Hendrik van den Bos\n\nWillem Hendrik van den Bos (25 September 1896, Rotterdam – 30 March 1974) was a Dutch–South African astronomer. At least one source refers to him as Van der Bos, but this seems to be an error.\n\nHe initially worked at Leiden Observatory in the Netherlands, but came to Union Observatory in South Africa in 1925, becoming director in 1941.\n\nHe discovered thousands of double stars and recorded tens of thousands of micrometer measurements of such stars, calculating the orbits of a number of binary stars.\n\nHe was president of the Astronomical Society of South Africa in 1943 and 1955.\n\nSome biographical sources say he discovered more than a hundred asteroids. However, the Minor Planet Center does not credit him with any asteroid discoveries.\n\nThe asteroid 1663 van den Bos is named after him, as is the lunar crater van den Bos.\n\n"}
