{"id": "36057380", "url": "https://en.wikipedia.org/wiki?curid=36057380", "title": "ANAEM", "text": "ANAEM\n\nThe Ankara Nuclear Research and Training Center (), known as ANAEM, is a nuclear research and training center of Turkey. The organization was established on August 18, 2010 as a subunit of Turkish Atomic Energy Administration (, TAEK) in its campus at Ankara University's Faculty of Science situated in Beşevler neighborhood in central Ankara. The organization consists of three divisions, which are engaged in education, training and public relations on nuclear matters.\n\n"}
{"id": "11814042", "url": "https://en.wikipedia.org/wiki?curid=11814042", "title": "Adolf Thiel", "text": "Adolf Thiel\n\nAdolf Thiel (February 12, 1915 – June 2, 2001 Los Angeles) was an Austrian-born German expert in guided missiles during World War II, and later worked for the United States Army and TRW.\n\nThiel had been an associate professor of engineering at the Institute of Technology in Darmstadt before joining Wernher von Braun's team at the Army Research Center Peenemünde, where he was involved in developing the V-2 rocket. By the end of the war, he was transferred to the United States by the US Army (see Operation Paperclip and German rocket scientists in the US) where he resumed his work with von Braun's group in Fort Bliss, Texas. During the nine years Thiel worked for the U.S. Army, he held positions at White Sands Missile Range in New Mexico and at Huntsville, Alabama. He primarily supervised preliminary design of the Redstone missile and other short- and intermediate-range ballistic missile systems.\n\nThiel left the Army in 1955 to join Space Technology Laboratories, which later became TRW. During the late 1950s, he was program manager for the Thor ballistic missile, which became a first-stage launch for the Explorer spacecraft. He was director of space projects for TRW when it developed Explorer VI and Pioneer V, two of the earliest US craft to explore interplanetary space. He oversaw all of TRW's space programs during the 1970s.\n\nAfter his retirement in 1980 as a senior vice president, Thiel served as an executive consultant to TRW and on NASA planning groups. He was named a fellow of the American Astronautical Society in 1968.\n\nHe married Frances Thiel; they had a son.\n"}
{"id": "14573864", "url": "https://en.wikipedia.org/wiki?curid=14573864", "title": "Adparticle", "text": "Adparticle\n\nAn adparticle is an atom, molecule, or cluster of atoms or molecules that lies on a crystal surface. The term is used in surface chemistry. The word is a contraction of \"adsorbed particle\". An adparticle that is a single atom may be referred to as an \"adatom\".\n"}
{"id": "6041076", "url": "https://en.wikipedia.org/wiki?curid=6041076", "title": "Airy points", "text": "Airy points\n\nAiry points (after George Biddell Airy) are used for precision measurement (metrology) to support a length standard in such a way as to minimise bending or droop of a horizontally supported beam.\n\nA kinematic support for a one-dimensional beam requires exactly two support points. Three or more support points will not share the load evenly (unless they are hinged in a non-rigid whiffle tree or similar). The position of those points can be chosen to minimize various forms of gravity deflection.\n\nA beam supported at the ends will sag in the middle, resulting in the ends moving closer together and tilting upward. A beam supported only in the middle will sag at the ends, making a similar shape but upside down.\n\nSupporting a uniform beam at the Airy points produces zero angular deflection of the ends. The Airy points are symmetrically arranged around the centre of the length standard and are separated by a distance equal to \nof the length of the rod.\n\n\"End standards\", that is standards whose length is defined as the distance between their flat ends such as long gauge blocks or the , must be supported at the Airy points so that their length is well-defined; if the ends are not parallel, the measurement uncertainty is increased because the length depends on which part of the end is measured. For this reason, the Airy points are commonly identified by inscribed marks or lines. For example, a 1000 mm length gauge would have an Airy point separation of 577.4 mm. A line or pair of lines would be marked onto the gauge 211.3 mm in from each end. Supporting the artifact at these points ensures that the calibrated length is preserved.\n\nAiry's 1845 paper derives the equation for equally spaced support points. In this case, the distance between each support is the fraction\nthe length of the rod. He also derives the formula for a rod which extends beyond the reference marks.\n\n\"Line standards\" are measured between lines marked on their surfaces. They are much less convenient to use than end standards but, when the marks are placed on the neutral plane of the beam, allow greater accuracy.\n\nTo support a line standard, one wishes to minimise the \"linear\", rather than angular, motion of the ends. The Bessel points (after Friedrich Bessel) are the points at which the length of the beam is maximized. Because this is a maximum, the effect of a small positioning error is proportional to the square of the error, an even smaller amount.\n\nThe Bessel points are located 0.5594 of the length of the rod apart, slightly closer than the Airy points.\n\nBecause line standards invariably extend beyond the lines marked on them, the optimal support points depend on both the overall length and the length to be measured. The latter is the quantity to be maximized, requiring a more complex calculation. For example, the 1927–1960 definition of the metre specified that the International Prototype Metre bar was to be measured while \"supported on two cylinders of at least one centimetre diameter, symmetrically placed in the same horizontal plane at a distance of 571 mm from each other.\" Those would be the Bessel points of a beam 1020 mm long.\n\nOther sets of support points, even closer than the Bessel points, which may be wanted in some applications are:\n\n\n"}
{"id": "20065577", "url": "https://en.wikipedia.org/wiki?curid=20065577", "title": "Armadillo (C++ library)", "text": "Armadillo (C++ library)\n\nArmadillo is a linear algebra software library for the C++ programming language. It aims to provide efficient and streamlined base calculations, while at the same time having a straightforward and easy-to-use interface. Its intended target users are scientists and engineers.\n\nIt supports integer, floating point (single and double precision), complex numbers, and a subset of trigonometric and statistics functions. Various matrix decompositions are provided through optional integration with Linear Algebra PACKage (LAPACK) and Automatically Tuned Linear Algebra Software (ATLAS) libraries. High-performance BLAS/LAPACK replacement libraries such as OpenBLAS and Intel MKL can also be used.\n\nThe library employs a delayed-evaluation approach (during compile time) to combine several operations into one and reduce (or eliminate) the need for temporaries. Where applicable, the order of operations is optimised. Delayed evaluation and optimisation are achieved through template metaprogramming.\n\nArmadillo is related to the Boost Basic Linear Algebra Subprograms (uBLAS) library, which also uses template metaprogramming. However, Armadillo builds upon ATLAS and LAPACK libraries, thereby providing machine-dependent optimisations and functions not present in uBLAS.\n\nIt is open-source software distributed under the permissive Apache License, making it applicable for the development of both open source and proprietary software. The project is supported by the NICTA research centre in Australia.\n\nHere is a trivial example demonstrating Armadillo functionality:\n\nHere is an other trivial example in C++ 98:\n\n"}
{"id": "3296057", "url": "https://en.wikipedia.org/wiki?curid=3296057", "title": "Background noise", "text": "Background noise\n\nBackground noise or ambient noise is any sound other than the sound being monitored (primary sound). Background noise is a form of noise pollution or interference. Background noise is an important concept in setting noise levels affect your background in formations. See noise criteria for cinema/home cinema applications.\n\nExamples of background noises are environmental noises such as waves, traffic noise, alarms, people talking, bioacoustic noise from animals or birds and mechanical noise from devices such as refrigerators or air conditioning, power supplies or motors.\n\nThe prevention or reduction of background noise is important in the field of active noise control. It is an important consideration with the use of ultrasound (e.g. for medical diagnosis or imaging), sonar, and sound reproduction.\n\nIn astronomy, background noise or cosmic background radiation is electromagnetic radiation from the sky with no discernible source.\n\nIn information architecture, irrelevant, duplicate or incorrect information may be called background noise.\n\nIn physics and telecommunication, background signal noise can be detrimental or in some cases beneficial. The study of avoiding, reducing or using signal noise is information theory.\n\nIn telephony, artificial comfort noise is used as a substitute for natural background noise, to fill in artificial silence created by discontinuous transmission systems using voice activity detection.\nBackground noise can also affect concentration.\n\n\n"}
{"id": "58077748", "url": "https://en.wikipedia.org/wiki?curid=58077748", "title": "Bagele Chilisa", "text": "Bagele Chilisa\n\nBagele Chilisa is a renowned post-colonial scholar who has written and spoken extensively about indigenous research and evaluation methodologies. She is a full professor at the University of Botswana, where she teaches courses on social science research methods and evaluation research to undergraduate and graduate students. Chilisa has served as an evaluater on multiple global projects, and is considered to be an important \"African thought leader.\" Chilisa identifies as a member of the Bantu people of Africa.\n\nChilisa was born and raised in the small village of Nshakazhogwe in Botswana. Her parents, Motlalepul and Zwambele Kenosi, were subsistence farmers. Her parents, and her grandmother, Ponya Kenosi, passed oral literature to her as a child, stimulating her lifelong interest in indigenous knowledge systems. In her book, \"Indigenous Research Methodologies\" (2012), Chilisa states:I belong to the Bantu people of Africa, who live a communal life based on a connectedness that stretches from birth to death, continues beyond death, and extends to the living and the nonliving. I am known and communicate in relational terms that connect me to all my relations, living and the nonliving. It is common for people to refer to each other using totems as well as relational terms such as \"uncle, aunt, brother,\" and so on. For instance, my totem is a crocodile, and depending on who is talking to me and on what occasion, I can be referred to using my totem.\n\nChilisa credits her father with teaching her the alphabet before she began attending school. Because her family lived near a village granary, she also learned to count before she began formal schooling. She attended primary and secondary school in Botswana. From the University of Pittsburgh, Chilisa earned her Master of Arts degree in Research Methodology, and her PhD in Policy, Planning, and Evaluation.\n\nChilisa is a full professor at the University of Botswana, housed in the Post Graduate Research and Evaluation Programme. She teaches courses in research methods and evaluation, and policy design. She teaches both graduate and undergraduate students at the University of Botswana.\n\nChilisa is a renowned scholar of indigenous research methods, having published books and journal articles on the topic and spoken at national and international conferences. Her research focuses on communities in Botswana and other regions in southern Africa, but application of her ideas about indigenous research methods are worldwide. She is hailed as an \"African thought leader driving the concept of African-rooted evaluation.\" In the preface to her important text, \"Indigenous Research Methodologies\", Chilisa credits her father with instilling within her the \"ways in which indigenous practices and values on connectedness and relational ways of knowing of the colonized Other could be valorized in research.\"\n\nAt the University of Botswana, Chilisa has an extensive record of service. She was a member of the board for the University of Botswana Centre for Scientific Research and Indigenous Knowledge and Innovations, as well as serving on the board of the University of Botswana Research Ethics Committee. \n\nChilisa has served non-governmental organizations (NGOs) as an evaluator for international projects including UNESCO, United Nations Development Programme (UNDP), the World Bank, the International Labour Organisation, and the United Nations Children's Fund. Chilisa has received multiple grants from international organizations to do research, including grants from the World Bank, the National Institute of Health (USA), and UNICEF. In 2018, Chilisa was appointed to serve on the United Nations Development Programme (UNDP) Evaluation Advisory Panel (EAP).\n\nShe co-founded the Botswana, Lesotho, Swaziland and Namibia Research Association, was the president of the Botswana Educational Research Association, and edited the \"Botswana Educational Research Journal.\"\n\n"}
{"id": "1109946", "url": "https://en.wikipedia.org/wiki?curid=1109946", "title": "Balance theory", "text": "Balance theory\n\nIn the psychology of motivation, balance theory is a theory of attitude change, proposed by Fritz Heider. It conceptualizes the cognitive consistency motive as a drive toward psychological balance. The consistency motive is the urge to maintain one's values and beliefs over time. Heider proposed that \"sentiment\" or liking relationships are balanced if the affect valence in a system multiplies out to a positive result.\n\nIn social network analysis, balance theory is the extension proposed by Frank Harary and Dorwin Cartwright. It was the framework for the discussion at a Dartmouth College symposium in September 1975.\n\nFor example: a Person (formula_1) who likes (formula_2) an Other (formula_3) person will be balanced by the same valence attitude on behalf of the other. Symbolically, formula_4 and formula_5 results in psychological balance.\n\nThis can be extended to things or objects (formula_6) as well, thus introducing triadic relationships. If a person formula_1 likes object formula_6 but dislikes other person formula_3, what does formula_1 feel upon learning that person formula_3 created the object formula_6? This is symbolized as such:\n\nCognitive balance is achieved when there are three positive links or two negatives with one positive. Two positive links and one negative like the example above creates imbalance or Cognitive dissonance.\n\nMultiplying the signs shows that the person will perceive imbalance (a negative multiplicative product) in this relationship, and will be motivated to correct the imbalance somehow. The Person can either:\n\nAny of these will result in psychological balance, thus resolving the dilemma and satisfying the drive. (Person formula_1 could also avoid object formula_6 and other person formula_3 entirely, lessening the stress created by psychological imbalance.)\n\nTo predict the outcome of a situation using Heider's balance theory, one must weigh the effects of all the potential results, and the one requiring the least amount of effort will be the likely outcome.\nDetermining if the triad is balanced is simple math:\n\nformula_23; Balanced.\n\nformula_24; Balanced.\n\nformula_25; Unbalanced.\n\nBalance theory is also useful in examining how celebrity endorsement affects consumers' attitudes toward products. If a person likes a celebrity and perceives (due to the endorsement) that said celebrity likes a product, said person will tend to like the product more, in order to achieve psychological balance.\n\nHowever, if the person already had a dislike for the product being endorsed by the celebrity, they may begin disliking the celebrity, again to achieve psychological balance.\n\nHeider's balance theory can explain why holding the same negative attitudes of others promotes closeness (see The enemy of my enemy is my friend).\n\nFrank Harary and Dorwin Cartwright looked at Heider's triads as 3-cycles in a signed graph. The sign of a path in a graph is the product of the signs of its edges. They considered cycles in a signed graph representing a social network.\nHarary proved that a balanced graph is polarized, that is, it decomposes into two positive subgraphs that are joined by negative edges.\n\nIn the interest of realism, a weaker property was suggested by Davis:\nGraphs with this property may decompose into more than two positive subgraphs called clusters. The property has been called the \"clusterability axiom\". Then balanced graphs are recovered by assuming the \n\nThe significance of balance theory for social dynamics was expressed by Anatol Rapoport:\nNote that a triangle of three mutual enemies makes a clusterable graph but \"not\" a balanced one. Therefore, in a clusterable network one \"cannot\" conclude that the enemy of my enemy is my friend, although this aphorism is a fact in a balanced network.\nClaude Flament expressed a limit to balance theory imposed by reconciling weak ties with relationships of stronger force such as family bonds:\n\nAt the 1975 Dartmouth College colloquium on balance theory, Bo Anderson struck at the heart of the notion:\n\n\n"}
{"id": "17158634", "url": "https://en.wikipedia.org/wiki?curid=17158634", "title": "Bane (plant)", "text": "Bane (plant)\n\nThe term bane (from , meaning \"thing causing death, poison\"), in botany, is an archaic element in the common names of plants known to be toxic or poisonous.\n\nIn the Middle Ages, several poisonous plants of the genus Aconitum were thought to have or prophylactic qualities, repelling and protecting against that which they were \"banes\" to (e.g. \"Henbane\", \"Wolfsbane\").\n\nThere is no single species, genus, or family of poisonous plant exclusively referred to as \"banes\". Several unrelated plants bear the name.\n\n\n\n\n\nIn medieval Europe, the toxic entheogen Aconitine was believed to prevent werewolves from undergoing their dire transformations.\n\nIn the Southeastern United States, sheep and cattle straying into woodland and grazing \"Kalmia latifolia\" have been known to suffer from its toxic effects.\n"}
{"id": "44352322", "url": "https://en.wikipedia.org/wiki?curid=44352322", "title": "Barranco Glacier", "text": "Barranco Glacier\n\nBarranco Glacier (once known as the Great Barranco Glacier) is near the summit of Mount Kilimanjaro in Tanzania, on the southwest slope of the peak and is a small remnant of an icecap which once crowned the top of Mount Kilimanjaro. The glacier is situated at an elevation of between . The Great Barranco Glacier was far larger when first documented in the late 19th century and it along with the now extinct Little Barranco Glacier may have been fed by the Furtwängler Glacier which is on the top of the mountain. By 2011, Barranco Glacier was reduced to two small disconnected and dormant ice bodies.\n\n"}
{"id": "12540463", "url": "https://en.wikipedia.org/wiki?curid=12540463", "title": "Blunt-eared bat", "text": "Blunt-eared bat\n\nThe blunt-eared bat or Peruvian crevice-dwelling bat (\"Tomopeas ravus\") is a species of bat in the family Molossidae. It is monotypic within the genus \"Tomopeas\" and subfamily Tomopeatinae. It is endemic to Peru, where it is considered critically endangered. It is threatened by habitat loss.\n\nThe classification of the blunt-eared bat has historically been problematic. When it was first described by Gerrit Smith Miller in 1900, it was placed in Vespertilionidae. \nIn 1970, it was proposed that it should be in its own family, due to its intermediate qualities between Vespertilionidae and Molossidae.\nTaxonomists continued to place it in the vesper bat family until a 1994 study of their mitochondrial DNA showed that they were better placed in Molossidae.\nIt is now widely recognized as the only member of subfamily Tomopeatinae of family Molossidae.\nThere is evidence that it is basal to all other genera of the Molossidae.\n\nIts genus name \"Tomopeas\" is possibly a reference to the Malay word \"mops\", meaning \"bat\"; Miller used \"mops\" in other genera he described, including Eumops and Nyctinomops.\n\"Ravus\" is Latin for tawny.\n\nTheir fur is pale yellowish-gray in color, while their flight membranes are dark.\nTheir face and ears are also dark in color.\nTheir ventral fur is lighter than their dorsal fur, and is a creamy buff color.\nThe fur is soft and dense, with individual hairs approximately long.\nThe uropatagium is sparsely furred on both its dorsal and ventral sides.\nLike other free-tailed bats, the blunt-eared bat has a blunt tragus, a small but defined antitragus, tubular nostrils, and fusion of the seventh cervical and first thoracic vertebrae.\nThey are extremely small in size, weighing only .\nFrom nose to tail, they are long.\nTheir forearms are long.\nMales and females are similar in size.\nUnlike other free-tailed bats, the projection of the tail from the uropatagium is very short, with only the last two caudal vertebrae extending past the membrane, measuring long.\nTheir dental formula is , for a total of 28 teeth.\nThe calcar forms a small but distinct lobe, and its keel is narrow and inconspicuous.\n\nAs they are relatively uncommon, not much is known about their biology. Juveniles, lactating females, and lactating females have all been encountered in July and August, suggesting that this is a time of high reproductive activity.\nThey are insectivorous.\nThey have been found to be infected with the protozoan endoparasite \"Eimeria\".\nA new species of \"Eimeria\" was described from the blunt-eared bat; it was named \"Eimeria tomopea\" in reference to this fact.\n\nIt is endemic to Peru.\nIts range is restricted to the arid and semiarid regions of Peru's coastal region.\nThey are found from above sea level.\nDuring the day, it roosts in the crevices of granite boulders and outcroppings.\nThe crevices they use are small, at only .\nThey possibly prefer crevices that face the west or southwest.\nIn 2010, a blunt-eared bat was discovered south of the previous estimated range. This capture was the first time the blunt-eared bat was observed in over thirty years.\nA 2013 study suggested that their range might extend into southwest Ecuador, as the habitat would be similar to where they are found in Peru. So far, there is no evidence to confirm this.\n\nIt has only been encountered in twelve localities, four of which are less than apart. Its area of occupancy is estimated at less than .\nIn Peru, it is considered critically endangered.\nThe International Union for Conservation of Nature assessed it as vulnerable in 1996 and 2008, but revised its classification to endangered in 2016.\nThey identify habitat destruction as the primary threat to this species; its habitat is being lost to agricultural conversion and urbanization.\nThis species is possibly under threat by efforts to control vampire bats. Sometimes, entire caves are fumigated, which kills all the bats inside.\nIn 2013, bat researchers reported capturing two individuals with \"relatively little sampling effort,\" causing them to question if the species is actually rare or if it is not encountered due to inappropriate sampling methods.\n"}
{"id": "873744", "url": "https://en.wikipedia.org/wiki?curid=873744", "title": "Calyptra", "text": "Calyptra\n\nCalyptra (from καλύπτρα (\"kalúptra\") \"veil\") is a scientific term used in botany. It describes a feature in plant morphology.\n\nIn bryophytes, the calyptra (\"plural\" calyptrae) is an enlarged archegonial venter that protects the capsule containing the embryonic sporophyte. The calyptra is usually lost before the spores are released from the capsule. The shape of the calyptra can be used for identification purposes.\n\nIn flowering plants, the calyptra is a covering tissue for stamens and carpels. The name is also used for the capping tissue of roots, the root cap.\n"}
{"id": "45041972", "url": "https://en.wikipedia.org/wiki?curid=45041972", "title": "Charles E. Resser", "text": "Charles E. Resser\n\nCharles Elmer Resser (1889-1943), was an American paleontologist, born in East Berlin, Pennsylvania. He was educated at Pennsylvania State Teachers College (graduation in 1912), Franklin and Marshall College (B.A., 1913), Princeton University (M.A., 1915) and George Washington University (Ph.D., 1917). Resser developed an interest in Cambrian fossils when he was a student of H. Justin Roddy at Franklin and Marshall College.\n\nIn 1914 Resser came to the United States National Museum as an assistant to Walcott. He was appointed Assistant Curator in the Division of Paleontology in 1915. Further positions were Assistant Curator, Division of Stratigraphic Paleontology (1923), Associate Curator, Division of Stratigraphic Paleontology (1924-1928), Curator, Division of Stratigraphic Paleontology (1929-1940); and Curator, Division of Invertebrate Paleontology and Paleobotany (1941-1943). Resser was a part-time faculty member at George Washington University from 1915 to 1932. At the University of Maryland Resser taught geology for a number of years.\n\n"}
{"id": "27940837", "url": "https://en.wikipedia.org/wiki?curid=27940837", "title": "Co-partnership housing movement", "text": "Co-partnership housing movement\n\nHousing co-partnership was a social movement that developed alongside the garden city movement in Britain between 1900 and 1914 and which financed and built most of the suburbs and villages associated with that movement. It was also a unique form of tenure combining features of a tenant co-operative and a limited dividend company.\n\nThe idea of co-operative housing can be traced back to early 19th Century figures, notably Robert Owen and Charles Fourier. Providing housing was one of the key objectives of the Rochdale Pioneers, an early British co-op whose principles were associated with the rapid growth of the co-operative movement in the second half of the 19th century. However, it was not until 1901 that the first successful wave of co-partnerships was set up at Brentham in Ealing in west London. Its leading figure was Henry Harvey Vivian.\n\nThe connections between the garden city and co-operative movements go back to the 1870s and 1880s when Ebenezer Howard was moving in radical circles which included utopian community builders and land reformers. But the practical link came in 1901 when London lawyer and chairman of the Labour Association, Ralph Neville, was persuaded by Howard to become chair of the Garden City Association (GCA). Neville introduced Howard to a group of wealthy and influential people who had already invested in the Ealing project. By 1903, this group was ready to invest in the Letchworth project. Two years later the link was sealed when Vivian brought in GCA architects Unwin and Parker to work on the Ealing project.\n\n"}
{"id": "7839", "url": "https://en.wikipedia.org/wiki?curid=7839", "title": "Corona", "text": "Corona\n\nA corona (Latin, 'crown') is an aura of plasma that surrounds the Sun and other stars. The Sun's corona extends millions of kilometres into outer space and is most easily seen during a total solar eclipse, but it is also observable with a coronagraph. The word \"corona\" is a Latin word meaning \"crown\", from the Ancient Greek κορώνη (korōnè, “garland, wreath”).\n\nSpectroscopy measurements indicate strong ionization and plasma temperature in excess of 1,000,000 kelvins, much hotter than the surface of the Sun.\n\nLight from the corona comes from three primary sources, from the same volume of space. The K-corona (K for \"kontinuierlich\", \"continuous\" in German) is created by sunlight scattering off free electrons; Doppler broadening of the reflected photospheric absorption lines spreads them so greatly as to completely obscure them, giving the spectral appearance of a continuum with no absorption lines. The F-corona (F for Fraunhofer) is created by sunlight bouncing off dust particles, and is observable because its light contains the Fraunhofer absorption lines that are seen in raw sunlight; the F-corona extends to very high elongation angles from the Sun, where it is called the zodiacal light. The E-corona (E for emission) is due to spectral emission lines produced by ions that are present in the coronal plasma; it may be observed in broad or forbidden or hot spectral emission lines and is the main source of information about the corona's composition.\n\nIn 1724, French-Italian astronomer Giacomo F. Maraldi recognized that the aura visible during a solar eclipse belongs to the Sun not to the Moon. In 1809, Spanish astronomer José Joaquín de Ferrer coined the term 'corona'. Based in his own observations of the 1806 solar eclipse at Kinderhook (New York), de Ferrer also proposed that the corona was part of the Sun and not of the Moon. English astronomer Norman Lockyer identified the first element unknown on Earth in the Sun's chromosphere, which was called helium. French astronomer Jules Jenssen noted that the size and shape of the corona changes with the sunspot cycle. In 1930, Bernard Lyot invented the coronograph, which allows to see the corona without a total eclipse. In 1952, American astronomer Eugene Parker proposed that the solar corona might be heated by myriad tiny 'nanoflares', miniature brightenings resembling solar flares that would occur all over the surface of the Sun.\n\nThe high temperature of the Sun's corona gives it unusual spectral features, which led some in the 19th century to suggest that it contained a previously unknown element, \"coronium\". Instead, these spectral features have since been explained by highly ionized iron (Fe-XIV, or Fe). Bengt Edlén, following the work of Grotrian (1939), first identified the coronal spectral lines in 1940 (observed since 1869) as transitions from low-lying metastable levels of the ground configuration of highly ionised metals (the green Fe-XIV line from Fe at 5303 Å, but also the red Fe-X line from Fe at 6374 Å).\n\nThe sun's corona is much hotter (by a factor from 150 to 450) than the visible surface of the Sun: the photosphere's average temperature is 5800 kelvins compared to the corona's one to three million kelvins. The corona is 10 times as dense as the photosphere, and so produces about one-millionth as much visible light. The corona is separated from the photosphere by the relatively shallow chromosphere. The exact mechanism by which the corona is heated is still the subject of some debate, but likely possibilities include induction by the Sun's magnetic field and magnetohydrodynamic waves from below. The outer edges of the Sun's corona are constantly being transported away due to open magnetic flux and hence generating the solar wind.\n\nThe corona is not always evenly distributed across the surface of the sun. During periods of quiet, the corona is more or less confined to the equatorial regions, with coronal holes covering the polar regions. However, during the Sun's active periods, the corona is evenly distributed over the equatorial and polar regions, though it is most prominent in areas with sunspot activity. The solar cycle spans approximately 11 years, from solar minimum to the following minimum. Since the solar magnetic field is continually wound up due to the faster rotation of mass at the sun's equator (differential rotation), sunspot activity will be more pronounced at solar maximum where the magnetic field is more twisted. Associated with sunspots are coronal loops, loops of magnetic flux, upwelling from the solar interior. The magnetic flux pushes the hotter photosphere aside, exposing the cooler plasma below, thus creating the relatively dark sun spots.\n\nSince the corona has been photographed at high resolution in the X-ray range of the spectrum by the satellite Skylab in 1973, and then later by Yohkoh and the other following space instruments, it has been seen that the structure of the corona is quite varied and complex: different zones have been immediately classified on the coronal disc.\nThe astronomers usually distinguish several regions, as described below.\n\nActive regions are ensembles of loop structures connecting points of opposite magnetic polarity in the photosphere, the so-called coronal loops.\nThey generally distribute in two zones of activity, which are parallel to the solar equator. The average temperature is between two and four million kelvins, while the density goes from 10 to 10 particle per cm.\nActive regions involve all the phenomena directly linked to the magnetic field, which occur at different heights above the Sun's surface: sunspots and faculae, occur in the photosphere, spicules, Hα filaments and plages in the chromosphere, prominences in the chromosphere and transition region, and flares and coronal mass ejections happen in the corona and chromosphere. If flares are very violent, they can also perturb the photosphere and generate a Moreton wave. On the contrary, quiescent prominences are large, cool dense structures which are observed as dark, \"snake-like\" Hα ribbons (appearing like filaments) on the solar disc. Their temperature is about 5000–8000 K, and so they are usually considered as chromospheric features.\n\nIn 2013, images from the High Resolution Coronal Imager revealed never-before-seen \"magnetic braids\" of plasma within the outer layers of these active regions.\n\nCoronal loops are the basic structures of the magnetic solar corona. These loops are the closed-magnetic flux cousins of the open-magnetic flux that can be found in coronal hole (polar) regions and the solar wind. Loops of magnetic flux well-up from the solar body and fill with hot solar plasma. Due to the heightened magnetic activity in these coronal loop regions, coronal loops can often be the precursor to solar flares and coronal mass ejections (CMEs).\n\nThe Solar plasma that feed these structures is heated from under 6000 K to well over 10 K from the photosphere, through the transition region, and into the corona. Often, the solar plasma will fill these loops from one point and drain to another, called foot points (siphon flow due to a pressure difference, or asymmetric flow due to some other driver).\n\nWhen the plasma rises from the foot points towards the loop top, as always occurs during the initial phase of a compact flare, it is defined as chromospheric evaporation. When the plasma rapidly cools and falls toward the photosphere, it is called chromospheric condensation. There may also be symmetric flow from both loop foot points, causing a build-up of mass in the loop structure. The plasma may cool rapidly in this region (for a thermal instability), its dark filaments obvious against the solar disk or prominences off the Sun's limb.\n\nCoronal loops may have lifetimes in the order of seconds (in the case of flare events), minutes, hours or days. Where there is a balance in loop energy sources and sinks, coronal loops can last for long periods of time and are known as \"steady state\" or \"quiescent\" coronal loops. ().\n\nCoronal loops are very important to our understanding of the current \"coronal heating problem\". Coronal loops are highly radiating sources of plasma and are therefore easy to observe by instruments such as \"TRACE\". An explanation of the coronal heating problem remains as these structures are being observed remotely, where many ambiguities are present (i.e. radiation contributions along the LOS). \"In-situ\" measurements are required before a definitive answer can be had, but due to the high plasma temperatures in the corona, \"in-situ\" measurements are, at present, impossible. The next mission of the NASA, the Parker Solar Probe will approach the Sun very closely allowing more direct observations.\n\nLarge-scale structures are very long arcs which can cover over a quarter of the solar disk but contain plasma less dense than in the coronal loops of the active regions.\n\nThey were first detected in the June 8, 1968 flare observation during a rocket flight.\n\nThe large-scale structure of the corona changes over the 11-year solar cycle and becomes particularly simple during the minimum period, when the magnetic field of the Sun is almost similar to a dipolar configuration (plus a quadrupolar component).\n\nThe interconnections of active regions are arcs connecting zones of opposite magnetic field, of different active regions. Significant variations of these structures are often seen after a flare.\n\nSome other features of this kind are helmet streamers—large cap-like coronal structures with long pointed peaks that usually overlie sunspots and active regions. Coronal streamers are considered to be sources of the slow solar wind.\n\nFilament cavities are zones which look dark in the X-rays and are above the regions where Hα filaments are observed in the chromosphere. They were first observed in the two 1970 rocket flights which also detected \"coronal holes\".\n\nFilament cavities are cooler clouds of gases (plasma) suspended above the Sun's surface by magnetic forces. The regions of intense magnetic field look dark in images because they are empty of hot plasma. In fact, the sum of the magnetic pressure and plasma pressure must be constant everywhere on the heliosphere in order to have an equilibrium configuration: where the magnetic field is higher, the plasma must be cooler or less dense. The plasma pressure formula_1 can be calculated by the state equation of a perfect gas formula_2, where formula_3 is the particle number density, formula_4 the Boltzmann constant and formula_5 the plasma temperature. It is evident from the equation that the plasma pressure lowers when the plasma temperature decreases with respect to the surrounding regions or when the zone of intense magnetic field empties. The same physical effect renders sunspots apparently dark in the photosphere.\n\nBright points are small active regions found on the solar disk. X-ray bright points were first detected on April 8, 1969 during a rocket flight.\n\nThe fraction of the solar surface covered by bright points varies with the solar cycle. They are associated with small bipolar regions of the magnetic field. Their average temperature ranges from 1.1x10 K to 3.4x10 K. The variations in temperature are often correlated with changes in the X-ray emission.\n\nCoronal holes are the Polar Regions which look dark in the X-rays since they do not emit much radiation. These are wide zones of the Sun where the magnetic field is unipolar and opens towards the interplanetary space. The high speed solar wind arises mainly from these regions.\n\nIn the UV images of the coronal holes, some small structures, similar to elongated bubbles, are often seen as they were suspended in the solar wind. These are the coronal plumes. More exactly, they are long thin streamers that project outward from the Sun's north and south poles.\n\nThe solar regions which are not part of active regions and coronal holes are commonly identified as the quiet Sun.\n\nThe equatorial region has a faster rotation speed than the polar zones. The result of the Sun's differential rotation is that the active regions always arise in two bands parallel to the equator and their extension increases during the periods of maximum of the solar cycle, while they almost disappear during each minimum. Therefore, the quiet Sun always coincides with the equatorial zone and its surface is less active during the maximum of the solar cycle. Approaching the minimum of the solar cycle (also named butterfly cycle), the extension of the quiet Sun increases until it covers the whole disk surface excluding some bright points on the hemisphere and the poles, where there are coronal holes.\n\nA portrait as diversified as the one already pointed out for the coronal features is emphasized by the analysis of the dynamics of the main structures of the corona, which evolve in times very different among them. Studying the coronal variability in its complexity is not easy because the times of evolution of the different structures can vary considerably: from seconds to several months. The typical sizes of the regions where coronal events take place vary in the same way, as it is shown in the following table.\n\nFlares take place in active regions and are characterized by a sudden increase of the radiative flux emitted from small regions of the corona. They are very complex phenomena, visible at different wavelengths; they involve several zones of the solar atmosphere and many physical effects, thermal and not thermal, and sometimes wide reconnections of the magnetic field lines with material expulsion.\n\nFlares are impulsive phenomena, of average duration of 15 minutes, and the most energetic events can last several hours. Flares produce a high and rapid increase of the density and temperature.\n\nAn emission in white light is only seldom observed: usually, flares are only seen at extreme UV wavelengths and into the X-rays, typical of the chromospheric and coronal emission.\n\nIn the corona, the morphology of flares, is described by observations in the UV, soft and hard X-rays, and in Hα wavelengths, and is very complex. However, two kinds of basic structures can be distinguished:\n\nAs for temporal dynamics, three different phases are generally distinguished, whose duration are not comparable. The durations of those periods depend on the range of wavelengths used to observe the event:\nSometimes also a phase preceding the flare can be observed, usually called as \"pre-flare\" phase.\n\nAccompanying solar flares or large solar prominences, \"coronal transients\" (also called coronal mass ejections) are sometimes released. These are enormous loops of coronal material that travel outward from the Sun at over a million kilometers per hour, containing roughly 10 times the energy of the solar flare or prominence that accompanies them. Some larger ejections can propel hundreds of millions of tons of material into space at roughly 1.5 million kilometers an hour.\n\nCoronal stars are ubiquitous among the stars in the cool half of the Hertzsprung–Russell diagram. These coronae can be detected using X-ray telescopes. Some stellar coronae, particularly in young stars, are much more luminous than the Sun's. For example, FK Comae Berenices is the prototype for the FK Com class of variable star. These are giants of spectral types G and K with an unusually rapid rotation and signs of extreme activity. Their X-ray coronae are among the most luminous (\"L\" ≥ 10 erg·s or 10W) and the hottest known with dominant temperatures up to 40 MK.\n\nThe astronomical observations planned with the Einstein Observatory by Giuseppe Vaiana and his group showed that F-, G-, K- and M-stars have chromospheres and often coronae much like our Sun.\nThe \"O-B stars\", which do not have surface convection zones, have a strong X-ray emission. However these stars do not have coronae, but the outer stellar envelopes emit this radiation during shocks due to thermal instabilities in rapidly moving gas blobs.\nAlso A-stars do not have convection zones but they do not emit at the UV and X-ray wavelengths. Thus they appear to have neither chromospheres nor coronae.\n\nThe matter in the external part of the solar atmosphere is in the state of plasma, at very high temperature (a few million kelvins) and at very low density (of the order of 10 particles/m).\nAccording to the definition of plasma, it is a quasi-neutral ensemble of particles which exhibits a collective behaviour.\n\nThe composition is similar to that in the Sun's interior, mainly hydrogen, but with much greater ionization than that found in the photosphere. Heavier metals, such as iron, are partially ionized and have lost most of the external electrons. The ionization state of a chemical element depends strictly on the temperature and is regulated by the Saha equation in the lowest atmosphere, but by collisional equilibrium in the optically-thin corona. Historically, the presence of the spectral lines emitted from highly ionized states of iron allowed determination of the high temperature of the coronal plasma, revealing that the corona is much hotter than the internal layers of the chromosphere.\n\nThe corona behaves like a gas which is very hot but very light at the same time: the pressure in the corona is usually only 0.1 to 0.6 Pa in active regions, while on the Earth the atmospheric pressure is about 100 kPa, approximately a million times higher than on the solar surface. However it is not properly a gas, because it is made of charged particles, basically protons and electrons, moving at different velocities. Supposing that they have the same kinetic energy on average\n(for the equipartition theorem), electrons have a mass roughly 1800 times smaller than protons, therefore they acquire more velocity. Metal ions are always slower. This fact has relevant physical consequences either on radiative processes (that are very different from the photospheric radiative processes), or on thermal conduction.\nFurthermore, the presence of electric charges induces the generation of electric currents and high magnetic fields.\nMagnetohydrodynamic waves (MHD waves) can also propagate in this plasma, even if it is not still clear how they can be transmitted or generated in the corona.\n\nThe corona emits radiation mainly in the X-rays, observable only from space.\n\nThe plasma is transparent to its own radiation and to that one coming from below, therefore we say that it is optically-thin. The gas, in fact, is very rarefied and the photon mean free-path overcomes by far all the other length-scales, including the typical sizes of the coronal features.\n\nDifferent processes of radiation take place in the emission, due to binary collisions between plasma particles, while the interactions with the photons, coming from below; are very rare.\nBecause the emission is due to collisions between ions and electrons, the energy emitted from a unit volume in the time unit is proportional to the squared number of particles in a unit volume, or more exactly, to the product of the electron density and proton density.\n\nIn the corona thermal conduction occurs from the external hotter atmosphere towards the inner cooler layers. Responsible for the diffusion process of the heat are the electrons, which are much lighter than ions and move faster, as explained above.\n\nWhen there is a magnetic field the thermal conductivity of the plasma becomes higher in the direction which is parallel to the field lines rather than in the perpendicular direction.\nA charged particle moving in the direction perpendicular to the magnetic field line is subject to the Lorentz force which is normal to the plane individuated by the velocity and the magnetic field. This force bends the path of the particle. In general, since particles also have a velocity component along the magnetic field line, the Lorentz force constrains them to bend and move along spirals around the field lines at the cyclotron frequency.\n\nIf collisions between the particles are very frequent, they are scattered in every direction. This happens in the photosphere, where the plasma carries the magnetic field in its motion. In the corona, on the contrary, the mean free-path of the electrons is of the order of kilometres and even more, so each electron can do a helicoidal motion long before being scattered after a collision. Therefore, the heat transfer is enhanced along the magnetic field lines and inhibited in the perpendicular direction.\n\nIn the direction longitudinal to the magnetic field, the thermal conductivity of the corona is\n\nformula_6\n\nwhere formula_7 is the Boltzmann constant,\nformula_5 is the temperature in kelvins,\nformula_9 the electron mass,\nformula_10 the electric charge of the electron,\n\nformula_11\n\nthe Coulomb logarithm, and\n\nformula_12\n\nthe Debye length of the plasma with particle density formula_3.\nThe Coulomb logarithm formula_14 is roughly 20 in the corona, with a mean temperature of 1 MK and a density of 10 particles/m, and about 10 in the chromosphere, where the temperature is approximately 10kK and the particle density is of the order of 10 particles/m, and in practice it can be assumed constant.\n\nThence, if we indicate with formula_15 the heat for a volume unit, expressed in J m, the Fourier equation of heat transfer, to be computed only along the direction formula_16 of the field line, becomes\n\nformula_17.\n\nNumerical calculations have shown that the thermal conductivity of the corona is comparable to that of copper.\n\nCoronal seismology is a new way of studying the plasma of the solar corona with the use of magnetohydrodynamic (MHD) waves. Magnetohydrodynamics studies the dynamics of electrically conducting fluids—in this case the fluid is the coronal plasma. Philosophically, coronal seismology is similar to the Earth's seismology, the Sun's helioseismology, and MHD spectroscopy of laboratory plasma devices. In all these approaches, waves of various kinds are used to probe a medium. The potential of coronal seismology in the estimation of the coronal magnetic field, density scale height, fine structure and heating has been demonstrated by different research groups.\n\nThe coronal heating problem in solar physics relates to the question of why the temperature of the Sun's corona is millions of kelvins higher than that of the surface. The high temperatures require energy to be carried from the solar interior to the corona by non-thermal processes, because the second law of thermodynamics prevents heat from flowing directly from the solar photosphere (surface), which is at about 5800 K, to the much hotter corona at about 1 to 3 MK (parts of the corona can even reach 10 MK).\n\nBetween the photosphere and the corona, is the thin region through which the temperature increases known as the transition region. It ranges from only tens to hundreds of kilometers thick. Energy cannot be transferred from the cooler photosphere to the corona by conventional heat transfer as this would violate the second law of thermodynamics. An analogy of this would be a light bulb raising the temperature of the air surrounding it to something greater than its glass surface. Hence, some other manner of energy transfer must be involved in the heating of the corona.\n\nThe amount of power required to heat the solar corona can easily be calculated as the difference between coronal radiative losses and heating by thermal conduction toward the chromosphere through the transition region. It is about 1 kilowatt for every square meter of surface area on the Sun's chromosphere, or 1/40000 of the amount of light energy that escapes the Sun.\n\nMany coronal heating theories have been proposed, but two theories have remained as the most likely candidates: wave heating and magnetic reconnection (or nanoflares). Through most of the past 50 years, neither theory has been able to account for the extreme coronal temperatures.\n\nIn 2012, high resolution (<0.2″) soft X-ray imaging with the High Resolution Coronal Imager aboard a sounding rocket revealed tightly wound braids in the corona. It is hypothesized that the reconnection and unravelling of braids can act as primary sources of heating of the active solar corona to temperatures of up to 4 million kelvins. The main heat source in the quiescent corona (about 1.5 million kelvins) is assumed to originate from MHD waves.\n\nThe NASA mission Parker Solar Probe is intended to approach the Sun to a distance of approximately 9.5 solar radii to investigate coronal heating and the origin of the solar wind. It has been successfully launched on August 12, 2018 and is currently leaving earth for the first orbit.\n\nThe wave heating theory, proposed in 1949 by Evry Schatzman, proposes that waves carry energy from the solar interior to the solar chromosphere and corona. The Sun is made of plasma rather than ordinary gas, so it supports several types of waves analogous to sound waves in air. The most important types of wave are magneto-acoustic waves and Alfvén waves. Magneto-acoustic waves are sound waves that have been modified by the presence of a magnetic field, and Alfvén waves are similar to ultra low frequency radio waves that have been modified by interaction with matter in the plasma. Both types of waves can be launched by the turbulence of granulation and super granulation at the solar photosphere, and both types of waves can carry energy for some distance through the solar atmosphere before turning into shock waves that dissipate their energy as heat.\n\nOne problem with wave heating is delivery of the heat to the appropriate place. Magneto-acoustic waves cannot carry sufficient energy upward through the chromosphere to the corona, both because of the low pressure present in the chromosphere and because they tend to be reflected back to the photosphere. Alfvén waves can carry enough energy, but do not dissipate that energy rapidly enough once they enter the corona. Waves in plasmas are notoriously difficult to understand and describe analytically, but computer simulations, carried out by Thomas Bogdan and colleagues in 2003, seem to show that Alfvén waves can transmute into other wave modes at the base of the corona, providing a pathway that can carry large amounts of energy from the photosphere through the chromosphere and transition region and finally into the corona where it dissipates it as heat.\n\nAnother problem with wave heating has been the complete absence, until the late 1990s, of any direct evidence of waves propagating through the solar corona. The first direct observation of waves propagating into and through the solar corona was made in 1997 with the Solar and Heliospheric Observatory space-borne solar observatory, the first platform capable of observing the Sun in the extreme ultraviolet (EUV) for long periods of time with stable photometry. Those were magneto-acoustic waves with a frequency of about 1 millihertz (mHz, corresponding to a 1,000 second wave period), that carry only about 10% of the energy required to heat the corona. Many observations exist of localized wave phenomena, such as Alfvén waves launched by solar flares, but those events are transient and cannot explain the uniform coronal heat.\n\nIt is not yet known exactly how much wave energy is available to heat the corona. Results published in 2004 using data from the TRACE spacecraft seem to indicate that there are waves in the solar atmosphere at frequencies as high as 100 mHz (10 second period). Measurements of the temperature of different ions in the solar wind with the UVCS instrument aboard SOHO give strong indirect evidence that there are waves at frequencies as high as 200 Hz, well into the range of human hearing. These waves are very difficult to detect under normal circumstances, but evidence collected during solar eclipses by teams from Williams College suggest the presences of such waves in the 1–10 Hz range.\n\nRecently, Alfvénic motions have been found in the lower solar atmosphere \nThese Alfvénic oscillations have significant power, and seem to be connected to the chromospheric Alfvénic oscillations previously reported with the Hinode spacecraft\n\nSolar wind observations with the WIND (spacecraft) have recently shown evidence to support theories of Alfvén-cyclotron dissipation, leading to local ion heating.\n\nThe magnetic reconnection theory relies on the solar magnetic field to induce electric currents in the solar corona. The currents then collapse suddenly, releasing energy as heat and wave energy in the corona. This process is called \"reconnection\" because of the peculiar way that magnetic fields behave in plasma (or any electrically conductive fluid such as mercury or seawater). In a plasma, magnetic field lines are normally tied to individual pieces of matter, so that the topology of the magnetic field remains the same: if a particular north and south magnetic pole are connected by a single field line, then even if the plasma is stirred or if the magnets are moved around, that field line will continue to connect those particular poles. The connection is maintained by electric currents that are induced in the plasma. Under certain conditions, the electric currents can collapse, allowing the magnetic field to \"reconnect\" to other magnetic poles and release heat and wave energy in the process.\n\nMagnetic reconnection is hypothesized to be the mechanism behind solar flares, the largest explosions in our solar system. Furthermore, the surface of the Sun is covered with millions of small magnetized regions 50–1,000 km across. These small magnetic poles are buffeted and churned by the constant granulation. The magnetic field in the solar corona must undergo nearly constant reconnection to match the motion of this \"magnetic carpet\", so the energy released by the reconnection is a natural candidate for the coronal heat, perhaps as a series of \"microflares\" that individually provide very little energy but together account for the required energy.\n\nThe idea that nanoflares might heat the corona was proposed by Eugene Parker in the 1980s but is still controversial. In particular, ultraviolet telescopes such as TRACE and SOHO/EIT can observe individual micro-flares as small brightenings in extreme ultraviolet light, but there seem to be too few of these small events to account for the energy released into the corona. The additional energy not accounted for could be made up by wave energy, or by gradual magnetic reconnection that releases energy more smoothly than micro-flares and therefore doesn't appear well in the TRACE data. Variations on the micro-flare hypothesis use other mechanisms to stress the magnetic field or to release the energy, and are a subject of active research in 2005.\n\nFor decades, researchers believed spicules could send heat into the corona. However, following observational research in the 1980s, it was found that spicule plasma did not reach coronal temperatures, and so the theory was discounted.\n\nAs per studies performed in 2010 at the \"National Center for Atmospheric Research\" in Colorado, in collaboration with the \"Lockheed Martin's Solar and Astrophysics Laboratory\" (LMSAL) and the \"Institute of Theoretical Astrophysics\" of the University of Oslo, a new class of spicules (TYPE II) discovered in 2007, which travel faster (up to 100 km/s) and have shorter lifespans, can account for the problem. These jets insert heated plasma into the Sun's outer atmosphere.\n\nThus, a much greater understanding of the Corona and improvement in the knowledge of the Sun's subtle influence on the Earth's upper atmosphere can be expected henceforth. The Atmospheric Imaging Assembly on NASA's recently launched Solar Dynamics Observatory and NASA's Focal Plane Package for the Solar Optical Telescope on the Japanese Hinode satellite which was used to test this hypothesis. The high spatial and temporal resolutions of the newer instruments reveal this coronal mass supply.\n\nThese observations reveal a one-to-one connection between plasma that is heated to millions of degrees and the spicules that insert this plasma into the corona.\n\n\n"}
{"id": "32484750", "url": "https://en.wikipedia.org/wiki?curid=32484750", "title": "Fiber-optic Improved Next-generation Doppler Search for Exo-Earths", "text": "Fiber-optic Improved Next-generation Doppler Search for Exo-Earths\n\nThe Fiber-Optic Improved Next-Generation Doppler Search for Exo-Earths (FINDS Exo-Earths) is a radial-velocity spectrograph developed by Debra Fischer. It is installed on the 3 m telescope in Lick Observatory in Mount Hamilton. It has been in operation since 2009 and is being used to verify exoplanet candidates found by Kepler.\n\nAt Yale University, Debra Fischer and Julien Spronck, along with Geoff Marcy of the University of California, Berkeley, set out to improve existing spectrograph technologies. Spurred by a $45,000 grant from The Planetary Society, the team built FINDS, a spectrograph add-on device.\n\n"}
{"id": "5520080", "url": "https://en.wikipedia.org/wiki?curid=5520080", "title": "Flags of Africa", "text": "Flags of Africa\n\nThese are the various flags of Africa.\n\n"}
{"id": "12704285", "url": "https://en.wikipedia.org/wiki?curid=12704285", "title": "Flow conditions", "text": "Flow conditions\n\nIn fluid measurement, the fluid's flow conditions (or flowing conditions) refer to quantities like temperature and static pressure of the metered substance. The flowing conditions are required data in order to calculate the density of the fluid at flowing conditions. The flowing density is in turn required in order to compensate the measured volume to quantity at base conditions.\n\nThe density of a gas is calculated using the ideal gas law and an equation of state calculation such as the one described in AGA Report No. 8.\n\nThere are broad general methodologies used to calculate the density of a liquid at specific conditions. In order to discuss a specific methodology, one must choose a liquid that holds sufficient interest to warrant a calculation specific to it. EOS 87.3 is a density calculation for seawater; API chapter 11 specifies calculations pertaining to oil, fuels and natural gas liquids.\n\n"}
{"id": "31539062", "url": "https://en.wikipedia.org/wiki?curid=31539062", "title": "Frustration–aggression hypothesis", "text": "Frustration–aggression hypothesis\n\nFrustration–aggression hypothesis, otherwise known as the frustration–aggression–displacement theory, is a theory of aggression proposed by John Dollard, Neal Miller, Leonard Doob, Orval Mowrer, and Robert Sears in 1939, and further developed by Neal Miller in 1941 and Leonard Berkowitz in 1969. The theory says that aggression is the result of blocking, or frustrating, a person's efforts to attain a goal.\n\nWhen first formulated, the hypothesis stated that frustration always precedes aggression, and aggression is the sure consequence of frustration. Two years later, however, Miller and Sears re-formulated the hypothesis to suggest that while frustration creates a need to respond, some form of aggression is one possible outcome. Therefore, the re-formulated hypothesis stated that while frustration prompts a behavior that may or may not be aggressive, any aggressive behavior is the result of frustration, making frustration not sufficient, but a necessary condition for aggression.\n\nThe hypothesis attempts to explain why people scapegoat. It attempts to give an explanation as to the cause of violence. According to Dollard and colleagues, frustration is the \"condition which exists when a goal-response suffers interference,\" while aggression is defined as \"an act whose goal-response is injury to an organism (or an organism surrogate).\" The theory says that frustration causes aggression, but when the source of the frustration cannot be challenged, the aggression gets displaced onto an innocent target. For example, if a man is disrespected and humiliated at his work, but cannot respond to this for fear of losing his job, he may go home and take his anger and frustration out on his family. This theory is also used to explain riots and revolutions, which both are believed to be caused by poorer and more deprived sections of society who may express their bottled up frustration and anger through violence.\n\nWhile some researchers criticized the hypothesis and proposed moderating factors between frustration and aggression, several empirical studies were able to confirm it as is. In 1989, Berkowitz expanded on the hypothesis by suggesting that negative affect and personal attributions play a major role in whether frustration instigates aggressive behavior.\n\nThe frustration-aggression hypothesis emerged in 1939 through the form of a monograph published by the Yale University Institute of Human Relations. The Yale psychologists behind the monograph were John Dollard, Leonard Doob, Neal Miller, O. H Mowrer, and Robert Sears. The book is based on many studies conducted by the group that touched a variety of disciplines including psychology, anthropology and sociology. Marxism, psychoanalysis and behaviorism were used by the Yale group throughout their research. Their work, \"Frustration and Aggression (1939)\", was soon having repercussions on the explanation of aggressive behavior theories. Their theory applied to human beings, but also to animals. The book created controversy on the subject which led to more than 7 articles critiquing the new theory. The \"Psychological Review\" and the \"Reading in Social Psychology\" are two of the papers that published articles on the subject. Many social scientists disclaimed the rather strict definition of frustration reactions as well as how the frustration concept is defined in itself. By 1941, the Yale group modified their theory following the multiple critics and studies published by other psychologists. From there, many pioneers in the social science world modified and brought their knowledge to the original theory.\n\nIn 1989 Berkowitz published an article, \"Frustration-Aggression Hypothesis: Examination and Reformulation\", which addressed the inconsistency of empirical studies aiming to test the hypothesis, as well as its criticism. He proposed a modification to the hypothesis that would take into an account negative affect and individual attributions. More recently, Breuer and Elson published a comprehensive overview of the \"Frustration-Aggression Theory\"\".\" The authors stated that despite an ample amount of empirical research that examines the link between frustration and aggressive behaviors, there is a decline in the number of studies that specifically refers to the frustration-aggression hypothesis. Breuer and Elson propose that there is utility in using the frustration-aggression hypothesis as a theoretical foundation for aggression literature and that this theory may have novel applications for other areas such as media psychology.\n\nIn 1941, the Yale group clarified their original statement which was “that the occurrence of aggressive behavior always presuppose the existence of frustration and, contrariwise, that the existence of frustration always lead to some form of aggression”. As it was, the second part of this hypothesis lead readers to think that frustration could only have aggression as a consequence, and it did not allow the possibility that other responses could arise and override the aggression response. The Yale group thus reformulated the hypothesis as following: “frustration produces instigation to a number of different types of response, one of which is aggression”. With this new formulation, the researchers left more place for the idea that aggressive impulses are not the only kinds that can emerge when an individual feels frustration. Other impulses, such as fear of punishment, can outweigh or even attenuate aggression instigations until it disappears, which would explain situations where frustration does not lead to outright aggression.\n\nIn his article published in 1941, Gregory Bateson observed the frustration-aggression hypothesis under a cultural angle. According to him, culture was implicitly involved in the hypothesis itself, as it was dealing with human behaviour, which is always formed and influenced by the environment, be it social or cultural. He stated that it is easier to fit the hypothesis in people whose culture portray life as series of neutral or frustrating events that lead to satisfying ends. This would be the case for European culture and for Iatmul culture. However, it is harder to apply the hypothesis to the Balinese culture. Indeed, Balinese children are taught to take pleasure, satisfaction, in the steps that lead to their goals, without waiting for satisfaction climaxes by completion of such goals. Following the same line of thoughts, Arthur R. Cohen considered social norms to be an important factor in whether or not aggression will be following frustration. In 1955, he published results of a study he conducted, which included 60 female students, that showed that people were less likely to demonstrate aggression when social standards were stressed. Moreover, he built on what Doob and Sears' study previously claimed, which is that demonstration of aggressive behavior will depend on the anticipation of punishment. Indeed, Cohen's result showed that people were less likely to demonstrate aggression towards the frustration agent if the latter was an authoritative figure. He also investigated Nicholas Pastore's statement that aggression was more likely to follow in a context of a arbitrary context when compared to an non-arbitrary one, and reached the same conclusions.\n\nThe frustration–aggression theory has been studied since 1939, and there have been modifications. Dill and Anderson conducted a study investigating whether hostile aggression differs in justified vs. unjustified frustration conditions—compared to the control condition which would not induce frustration. The study task required participants to learn and make an origami bird. The experimental procedure comprised an instruction phase and a folding phase. During the instruction phase, a participant paired with a confederate was shown how to fold a bird only one time. The folding phase was timed and each subject was required to make the bird alone as quickly and as accurately as possible. In all conditions, the experimenter started presenting the instructions in a deliberately fast manner. The conditions differed on how the experimenter responded to the confederate's request to slow down. In the non-frustration control condition, the experimenter apologized and slowed down. In the unjustified frustration condition, the experimenter revealed his desire to leave as quickly as possible due to personal reasons. In the justified frustration condition, the experimenter revealed a need to clear the room as fast as possible due to the supervisor demand. The subjects were then given questionnaires on their levels of aggression as well as questionnaires about the competence of the research staff. They were told that these questionnaires would determine whether the research staff would receive financial aid, or verbal reprimands and a reduction in financial awards. The questions presented on the questionnaire were designed to reflect the research staff's competence and likability. Dill and Anderson found that participants in the unjustified frustration condition rated the research staff as less able and less likable, knowing this would affect their financial situation as graduate students. The justified frustration group rated the staff as less likable and less competent than the control group, but higher on both rating scales than the unjustified condition participants. The authors concluded that unjustified frustration leads to greater level of aggression, compared to justified frustration, which, in turn, results in higher levels of aggression compared to the non-frustration situations.\n\nIn 1964, Leonard Berkowitz stated that it is necessary to have an aggression stimulus to make aggression take place. Then in 1974 and 1993, he remodified the frustration/aggression hypothesis into a theory that removed the importance of aggressive cues to the aggressive behavior. Which is to say, extremely angry subject will show aggression even if the aggression cue is absent. The most provocative theory introduced by Berkowitz is “aggressive cues” hypothesis, stating that for young children, previous exposure to any objects or events such as military weapon toys showing destruction effects will work as aggressive cues to increase the chances of aggression behaviors. The modification of frustration/aggression hypothesis by Berkowitz discussed that the aggressive behavior originates from internal forces such as anger, aggressive habits and external stimuli.These theories help explain the reasons why aggression is evoked but didn’t explain well the procedure of aggressive habits developments into aggressive stimuli.\n\nIn his article published in 1980, Leonard Berkowitz further discussed the relationship between the frustration and the level of aggression by adding the differentiation between the internal and external reaction to the frustration. In his first part of experiment, he found that for both of the types of frustration (legitimate and illegitimate), compared to the control group which finished the task successfully, the internal reaction measured by heart rate and rating of three 21-step bipolar scales shows great level. Nevertheless, there is no significant difference of internal reaction between legitimate and illegitimate groups. For the 2nd part of the experiment, when previous 2 groups experiencing legitimate and illegitimate frustration, encounter an innocent partner in order to perform an unrelated task, the group with previous illegitimate frustration shows greater external reaction which is openly punitive actions towards the innocent partner than the group experiencing previous legitimate frustration does.\n\nSome studies have shown that frustrating and equally threatening events may generate feelings of aggression. This is based on the account that one of our neural systems is responsible for executing the basic responses to threat. It so happens that one of these basic responses from this system is that of aggression. The system is made up of and follows from the amygdala to the hypothalamus and finally to the periaqueductal gray matter (PAG) In greater detail, research suggests that when one is threatened or frustrated by some stimuli, parts of our frontal cortex, that is our orbital, medial and ventrolateral frontal cortex, is activated which works in tandem with our threat response system, the amygdala-hypothalamus-PAG. More simply put, threatening events generate more action potentials in the frontal cortex regions which then relay onto the amygdala-hypothalamus-PAG. It is in this basic threat response system where the decision on which response should take hold based on the information received from the frontal cortex regions. As mentioned, there are varying degrees and responses that could take hold within an animal in the presence of a frustrating event. This has not shown to interfere with the basic circuitry at the neuronal level and simply implies that certain stimuli generate more action potentials than others, and thus stronger responses than others respectively. In the face of this, animals portray a response hierarchy at the onset of a frustrating event. For example, when low levels of danger are perceived, the threat response system induces freezing in the animal; closer subjects of threat generate the act of fleeing from their surroundings and finally, where the source of the threat is so close that escape is no longer an option, the threat circuitry system will induce reactive aggression in the animal. What this means is that the closer a frustrating stimulus is presented to us, the greater the chances our basic response systems will be activated and thus will give rise to certain behaviors accordingly. Furthermore, some research has shown that \"individuals with elevated susceptibility for frustration [showed] greater activity within these regions [amygdala-hypothalarmus-PAG] in response to frustrating events relative to those with less susceptibility\". What this research suggests is that people who get frustrated more easily than others show greater activity in the frontal cortex in connection with the amygdala-hypothalamus-PAG, the system that makes us act, given a strong enough stimulus, aggressively with reference to the studies at hand.\n\nOne study by Williams examined the impact of violent content and frustration with game-play and assessed how these factors are related to aggressive personality (i.e., trait hostility).  His study collected data from 150 male college undergraduates. The study consisted of two phases. The first phase lasted 45 minutes and was in a large group setting. During this phase participants were asked to complete a series of questionnaires that assessed their video game playing habits and aggression. The second phase was a one-on-one session with each participant. During this phase participants played video games and were assigned to one of four conditions: 1) video game with violent content in low/nonfrustrating mode, 2) video game with violent content in frustrating mode, 3) video game with nonviolent content in low/nonfrustrating mode, and 4) video game with nonviolent content in frustration mode. As part of the frustrating conditions, participants were informed that their scores would be compared to other participants and that higher performance would be rewarded with a $100 gift card. Afterwards, participants completed a questionnaire similar to phase one. Ultimately, this study found that exposure to violent content influenced participants’ aggressive responses when playing video games.  He also found that frustration with gameplay was just as impactful, if not greater, on participants’ aggressive responses.  Participants who were exposed to violent content and presented frustration with game-play reported the highest scores in trait hostility.\n\nAnother study by Shackman and Pollak tested the impact of physical maltreatment of children on their reactive aggression. The authors tested the relationships between individual differences in social information processing, history of physical maltreatment, and child negative affect and their aggressive behaviors. The study collected data from 50 boys through the Madison, Wisconsin Public Schools. Within this sample, 17 children had a history of physical maltreatment. Families attended two separate sessions in the laboratory. The first session involved the children completing an emotional oddball task while having their neural responses recorded via event-related potentials (ERPs). After this task, parents and children participated in a semistructured dyadic interaction, which involved the researchers assessment of child-directed parental hostility during a 10 minute interaction. Families then returned to the laboratory between 2 to 20 days for the second session of the experiment. The second session asked children to participate in a provocation task, which was designed to evoke a reactive aggression response. All families were paid $50 for their participation and were debriefed. The authors reported that physically maltreated children displayed greater negative affect and aggressive behavior compared to children that were not physically maltreated. This relationship was mediated by the children's attention to angry faces, as measured by the ERP. Ultimately, these findings suggest that physical maltreatment of children leads to child dysregulation of their negative affect and aggression.\n\nThe publication of \"Frustration and Aggression\" gave rise to criticism from several scientists, including animal behaviorists, psychologists, and psychiatrists. For example, Seward, who studied rat behavior, suggested that aggression can also be caused by dominance struggles, which for him were different from frustration. Durbin and Bowlby, by observing apes and children, placed reasons for breaking of a fight into three different categories. While one of the categories was frustration, the other two were classified as possession disputes and resentment of a stranger intrusion. Addressing this criticism, Berkowitz suggested that the controversy around the frustration-aggression hypothesis has its roots in the lack of a common definition for frustration. He advocated that if frustration is defined as a reaction to a blocking of a drive or an interruption of some internal response sequence, those various reasons for aggression actually fall under the frustration umbrella.\n\nLater research was focused more on refining the hypothesis, rather than on denying its correctness. In one of the earlier studies, following the publication of Dollard et al.'s book, Pastore argued that the hypothesis should distinguish between arbitrary and non-arbitrary situations, as non-arbitrary situations decrease the aggressiveness of response. In this study, participants from a sample of 131 college students were presented with the verbal description of two types of situations, arbitrary and non-arbitrary. One of the arbitrary situation examples was being intentionally passed by the bus driver, while waiting at the correct bus stops. A non-arbitrary situation was described in one of the examples as being passed by the bus, while it was specifically marked as heading for a garage. The study results suggested that arbitrariness of the situation is an important factor in eliciting aggressive behavior in frustrating situations, with arbitrary situations inducing more aggression.\n\nBuilding on Pastore's work, in his 1955 empirical study, Cohen confirmed that the arbitrariness of a situation affects the level of aggressiveness. However, the study also supported his hypothesis that two more factors need to be accounted for in the frustration-aggression hypothesis. Those factors are social norms and the relationship with the frustrating agent. In his study, 60 volunteer participants were rating 14 statements on the levels of predicted aggressiveness. Cohen found that people tend to respond less aggressively if the frustrating agent is an authority figure, rather than a friend and that people respond to frustration with less aggression if the socially accepted norms require to do so. Berkowitz addressed this criticism in his 1989 article and proposed that frustration, and ultimately aggression, is induced when individuals think they have been deliberately and wrongly kept from their goal.\n\nSome sources suggest that there is little empirical support for it, even though researchers have studied it for more than sixty years. Also, this theory suggests frustrated, prejudiced individuals should act more aggressively towards out-groups they are prejudiced against, but studies have shown that they are more aggressive towards everyone.\n\nThe frustration-aggression hypothesis implies that aggression is followed or triggered by a feeling of frustration as proposed by the Yale group. Yet, other studies support contradictory claims. Certain subjects in some studies have shown to not respond aggressively to frustration given their personal, moral and educational backgrounds. For instance, the Indian culture uses the Satyagraha, which means “non-violent resistance” to a trigger. Mahatma Gandhi exemplified this technique that essentially denounces the principles of the frustration-aggression theory in that he restrained himself from feeling these innate desires.\n\nIndeed, the hypothesis does not take into consideration the individuality of human beings. According to Dixon and Johnson, two people can respond differently to the same frustration stimuli. For instance, some could respond aggressively while driving on the highway after being cut off by another car, whereas others with a different temperament could not react to it. However, the theory assumes that if two different people receive the same frustration stimuli, they will react similarly or equally aggressively\n\nThe Yale group’s hypothesis does not explain why aggressive behavior could be manifested in different social environments without previous provocation or feeling of frustration. However, according to Gross and Osterman, people may lose their sense of uniqueness in mass societal contexts because it tends to deindividuate them. For instance, individuals may behave aggressively when they are with their friends or in a big crowd (e.g. while watching a hockey game), but might not behave aggressively when they are by themselves (e.g. watching the game alone at home). When individuals are in a crowd, they are more likely to become desensitised of their own actions and less likely to take responsibility. This phenomenon is known as deindividuation.\n\n"}
{"id": "38632380", "url": "https://en.wikipedia.org/wiki?curid=38632380", "title": "GC skew", "text": "GC skew\n\n]\nGC skew is when the nucleotides Guanine and Cytosine are over- or under-abundant in a particular region of DNA or RNA. In equilibrium conditions (without mutational or selective pressure and with nucleotides randomly distributed within the genome) there is an equal frequency of the four DNA bases (Adenine, Guanine, Thymine, and Cytosine) on both single strands of a DNA molecule. However, in most prokaryotes (e.g. \"E. coli\") and some archaea (e.g. \"Sulfolobus solfataricus\"), nucleotide compositions are asymmetric between the leading strand and the lagging strand: the leading strand contains more Guanine (G) and Thymine (T), whereas the lagging strand contains more Adenine (A) and Cytosine (C). This phenomenon is referred to as GC and AT skew. It is represented mathematically as follows:\n\nGC skew = (G - C)/(G + C) \nAT skew = (A - T)/(A + T)\n\nErwin Chargaff's work in 1950 demonstrated that, in DNA, the bases guanine and cytosine were found in equal abundance, and the bases adenine and thymine were found in equal abundance. However, there was no equality between the amount of one pair versus the other. Chargaff’s finding is referred to as Chargaff's rule or parity rule 1. Three years later Watson and Crick used this fact during their derivation the structure of DNA, their double helix model.\n\nA natural result of parity rule 1, at the state of equilibrium, in which there is no mutation and/or selection biases in any of the two DNA strands, is that when there is an equal substitution rate, the complementary nucleotides on each strand have equal amounts of a given base and its complement. In other words, in each DNA strand the frequency of occurrence of T is equal to A and the frequency of occurrence of G is equal to C because the substitution rate is presumably equal. This phenomenon is referred to as parity rule 2. Hence, the second parity rule only exists, when there is no mutation or substitution.\n\nAny deviation from parity rule 2 will result in asymmetric base composition that discriminates the leading from the lagging strand. This asymmetry is referred to as GC or AT skew.\n\nThere is a richness of guanine over cytosine and thymine over adenine in the leading strand and vice versa for the lagging strand. The nucleotide composition skew spectra ranges from -1, which corresponds to G = 0 or A = 0, to +1, which corresponds to T= 0 or C = 0. Therefore, positive GC skew represents richness of G over C and the negative GC skew represents richness of C over G. As a result, one expects to see a positive GC skew and negative AT skew in the leading strand, and a negative GC skew and a positive AT skew in the lagging strand. GC or AT skew changes sign at the boundaries of the two replichores, which correspond to DNA replication origin or terminus. Originally, this asymmetric nucleotide composition was explained as a different mechanism used in DNA replication between the leading strand and lagging strand. DNA replication is semi-conservative and an asymmetric process itself. This asymmetry is due the formation of the replication fork and its division into nascent leading and lagging strands. The leading strand is synthesized continuously and in juxtapose to the leading strand; the lagging strand is replicated through short fragments of polynucleotide (Okazaki fragments) in a 5' to 3' direction.\n\nThere are three major approaches to calculate and graphically demonstrate GC skew and its properties.\n\nThe first approach is GC and AT Asymmetry. Jean R. Lobry was the first to illustrate the nucleotide composition asymmetry throughout the genome of three bacterium: \"E. coli\", \"Bacillus subtilis\", and \"haemophilus influenzae\" by using GC and AT bias. This is the most common and traditional way to quantitatively evaluate base composition asymmetry. The original formulas at the time were not named skew, but rather deviation from [A] = [T] or [C] = [G]:\n\ndeviation from [A] = [T] as (A - T)/(A + T)\n\ndeviation from [C] = [G] as (C - G)/(C + G)\n\nwhere A, T, G, and C represent the frequency of occurrence of the equivalent base in a particular sequence in a defined length. A window sliding strategy is used to calculate deviation from C through the genome. In these plots, a positive deviation from C corresponds to lagging strand and negative deviation from C corresponds to leading strand. Furthermore, the site where the deviation sign switches corresponds to origin or terminal. The x-axis represents the chromosome locations plotted 5' to 3' and y-axis represents the deviation value. The major weakness of this method is its window-size dependent property. Therefore, choosing an adequate window size greatly affects the outcome of the plot. Other techniques should be combined with deviation in order to identify and locate the origin of the DNA replication with greater accuracy.\n\nThe second approach is referred to as cumulative GC skew (CGC skew). This method still uses the sliding window strategy but it takes advantage of the sum of the adjacent windows from an arbitrary start. In this scheme, the entire genome is usually plotted 5' to 3' using an arbitrary start and arbitrary strand. In the cumulative GC skew plot, the peaks corresponds to the switch points (terminus or origin).\n\nIn contrast to Lobry's earlier paper, recent implementations of GC skew flips the original definition, redefining it to be:\n\nGC skew = (G - C)/(G + C).\n\nWith the flipped definition of GC skew, the maximum value of the cumulative skew corresponds to the terminal, and the minimum value corresponds to the origin of replication.\n\nThe final approach is the Z curve. Unlike the previous methods, this method do not uses the sliding window strategy and is thought to perform better as to finding the origin of replication. In this method each base’s cumulative frequency with respect to the base at the beginning of the sequence is investigated. Z curve uses a three-dimensional representation with the following parameters:\n\nXn = (An + Gn) – (Cn + Tn)\n\nYn = (An + Cn) – (Gn + Tn)\n\nZn = (An + Tn) – (Cn + Gn)\n\nWhere n = 0, 1, 2, …, N, Xn represents the excess of purine over pyrimidine, Yn denotes excess of keto over amino, and Zn shows the relationship between the weak and strong hydrogen bonds. X and Y components can alone detect the replication origin and asymmetric composition of the strands.\nA combination of these methods should be used for prediction of replication origin and terminal, in order to compensate for their weakness.\n\nThere is lack of consensus in scientific community with regard to the mechanism underlining the bias in nucleotide composition within each DNA strand. There are two major schools of thought that explain the mechanism behind the strand specific nucleotide composition in bacteria.\n\nThe first one describes a bias and an asymmetric mutational pressure on each DNA strand during replication and transcription. Due to the asymmetric nature of the replication process, an unequal mutational frequency and DNA repair efficiency during the replication process can introduce more mutations in one strand as compared to the other. Furthermore, the time used for replication between the two strands varies and may lead to asymmetric mutational pressure between leading and lagging strand. In addition to mutations during DNA replication, transcriptional mutations can create strand specific nucleotide composition skew. Deamination of cytosine and ultimately mutation of cytosine to thymine in one DNA strand can increase the relative number of guanine and thymine to cytosine and adenine. In most bacteria majority of the genes are encoded in the leading strand. For instance, the leading strand in \"Bacillus\" \"subtilis\" encodes 75% of the genes. In addition an excess of deamination and conversion of cytosine to thymine in the coding strand compared to the non-coding strand has been reported. One possible explanation is that the non-transcribed strand (coding strand) is single stranded during the transcription process; therefore, it is more vulnerable to deamination compared to the transcribed strand (non-coding strand). Another explanation is that the deamination repair activity during transcription does not occur on the coding strand. Only the transcribed strand benefits from these deamination repair events.\n\nThe second school of thought describes the mechanism of GC and AT skew as resulting from differential selective pressure between the leading and lagging strands. Examination of the prokaryotic genome shows a preference in third codon position for G over C and T over A. This discrimination creates an asymmetric nucleotide composition, if the coding strand is unequally distributed between the leading and lagging strands, as in the case for bacteria. In addition, the highly transcribed genes, such as ribosomal proteins, have been shown to be located mostly on the leading strand in bacteria. Therefore, a bias in the third-position codon choice of G over C can lead to GC skew. Additionally, some signal sequences are rich in guanine and thymine, such as chi sequences, and these sequences might have a higher frequency of occurrence in one strand compared to the other.\n\nBoth mutational and selective pressure can independently introduce asymmetry in DNA strands. However the combination and cumulative effect of both mechanisms is the most plausible explanation for GC and AT skew.\n\nIn the majority of prokaryotes there is a richness of G over C and T over A in the leading strand and vice versa for the lagging strand. However, it has been reported that there is positive AT skew in the leading strand in some prokaryotes, such as the phylum Firmicutes. Firmicutes demonstrate an atypical AT skew. This unique nucleotide composition is thought to be due to selection pressure of adenine over thymine in the coding region. This biased selection avoids the formation of stop codons and use of metabolically expensive amino acids. The coding regions are mostly distributed on the leading strand; therefore, A over T richness is observed in Firmicutes.\n\nThe GC skew is proven to be useful as the indicator of the DNA leading strand, lagging strand, replication origin, and replication terminal. Most prokaryotes and archaea contain only one DNA replication origin. The GC skew is positive and negative in the leading strand and in the lagging strand respectively; therefore, it is expected to see a switch in GC skew sign just at the point of DNA replication origin and terminus. GC skew can also be used to study the strand biases and mechanism related to them by calculating the excess of one base over its complementary base in different milieus. Method such as GC skew, CGC skew, and Z-curve are tools that can provide opportunity to better investigate the mechanism of DNA replication in different organisms.\n\n"}
{"id": "38085936", "url": "https://en.wikipedia.org/wiki?curid=38085936", "title": "Glossary of meteoritics", "text": "Glossary of meteoritics\n\nThis is a glossary of terms used in meteoritics, the science of meteorites.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "49641785", "url": "https://en.wikipedia.org/wiki?curid=49641785", "title": "Heptasartorite", "text": "Heptasartorite\n\nHeptasartorite is a very rare mineral with formula TlPbAsS. It belongs to sartorite homologous series. It is related to other recently approved minerals of the series: enneasartorite and hendekasartorite. All three minerals come from a quarry in Lengenbach, Switzerland, which is famous of thallium minerals. Chemically similar minerals include edenharterite and hutchinsonite.\n"}
{"id": "210385", "url": "https://en.wikipedia.org/wiki?curid=210385", "title": "Ice XII", "text": "Ice XII\n\nIce XII is a metastable, dense, crystalline phase of solid water, a type of ice. Ice XII was first reported in 1996 by C. Lobban, J.L. Finney and W.F. Kuhs and, after initial caution, was properly identified in 1998.\n\nIt was first obtained by cooling liquid water to at a pressure of . Ice XII was discovered existing within the phase stability region of ice V. Later research showed that ice XII could be created outside that range. Pure ice XII can be created from ice I at by rapid compression (0.81-1.00 GPa/min) or by warming high density amorphous ice at pressures between . The proton-ordered form of ice XII is ice XIV.\n\nWhile it is similar in density (1.29 g/cm at ) to ice IV (also found in the ice V space) it exists as a tetragonal crystal. Topologically it is a mix of seven- and eight-membered rings, a 4-connected net (4-coordinate sphere packing)—the densest possible arrangement without hydrogen bond interpenetration.\n\nOrdinary water ice is known as ice I, (in the Bridgman nomenclature). Different types of ice, from ice II to ice XVI, have been created in the laboratory at different temperatures and pressures.\n\n\n"}
{"id": "730958", "url": "https://en.wikipedia.org/wiki?curid=730958", "title": "Julian year (astronomy)", "text": "Julian year (astronomy)\n\nIn astronomy, a Julian year (symbol: a) is a unit of measurement of time defined as exactly 365.25 days of SI seconds each. The length of the Julian year is the average length of the year in the Julian calendar that was used in Western societies until some centuries ago, and from which the unit is named. Nevertheless, because astronomical Julian years are measuring duration rather than designating dates, this Julian year does not correspond to years in the Julian calendar or any other calendar. Nor does it correspond to the many other ways of defining a year.\n\nThe Julian year is not a unit of measurement in the International System of Units (SI), but it is recognized by the International Astronomical Union (IAU) as a non-SI unit for use in astronomy. Before 1984, both the Julian year and the mean tropical year were used by astronomers. In 1898, Simon Newcomb used both in his \"Tables of the Sun\" in the form of the Julian century (36,525 days) and the \"solar century\" ( days), a rounded form of 100 mean tropical years of each according to Newcomb. However, the mean tropical year is not suitable as a unit of measurement because it varies from year to year by a small amount,  days according to Newcomb. In contrast, the Julian year is defined in terms of SI units so is as accurate as those units and is constant. It approximates both the sidereal year and the tropical year to about ±0.008 days.\nThe Julian year is the basis of the definition of the light-year as a unit of measurement of distance.\n\nIn astronomy, an \"epoch\" specifies a precise moment in time. The positions of celestial objects and events, as measured from Earth, change over time, so when measuring or predicting celestial positions, the epoch to which they pertain must be specified. A new standard epoch is chosen about every 50 years.\n\nThe standard epoch in use today is \"Julian epoch J2000.0\". It is exactly 12:00 TT (close to but not exactly Greenwich mean noon) on in the Gregorian (\"not\" Julian) calendar. \"Julian\" within its name indicates that other Julian epochs can be a number of Julian years of 365.25 days each before or after J2000.0. For example, the future epoch J2100.0 will be exactly 36,525 days (one Julian century) from J2000.0 at 12:00 TT on (the dates will still agree because the Gregorian century 2000–2100 will have the same number of days as a Julian century).\n\nBecause Julian years are not exactly the same length as years on the Gregorian calendar, astronomical epochs will diverge noticeably from the Gregorian calendar in a few hundred years. For example, in the next 1000 years, seven days will be dropped from the Gregorian calendar but not from 1000 Julian years, so J3000.0 will be .\n\nThe \"Julian year\", being a uniform measure of duration, should not be confused with the variable length historical years in the Julian calendar. An astronomical Julian year is never individually numbered. Astronomers follow the same calendar conventions that are accepted in the world community: They use the Gregorian calendar for events since its introduction on (or later, depending on country), and the Julian calendar for events before that date.\n\nA \"Julian year\" should not be confused with the \"Julian day\" (also \"Julian day number\" or \"JDN\"), which is also used in astronomy. Despite the similarity of names, there is little connection between the two. It is a way of expressing a date as the integer number of days that have elapsed since a reference date or initial epoch. The Julian day uniquely specifies a date without reference to its day, month, or year in any particular calendar. A specific time within a day is specified via a decimal fraction.\n"}
{"id": "48706684", "url": "https://en.wikipedia.org/wiki?curid=48706684", "title": "Kepler de Souza Oliveira", "text": "Kepler de Souza Oliveira\n\nKepler de Souza Oliveira Filho (born 16 February 1956), also known as S. O. Kepler, is a Brazilian astronomer primarily known for his work on white dwarfs, variable stars, and magnetars. A member of the Brazilian Academy of Sciences, he is currently a professor at Universidade Federal do Rio Grande do Sul (UFRGS).\n\nBorn in Salvador, Bahia, Brazil, Kepler obtained his Ph.D. from the University of Texas at Austin in 1984. In January 2006, Oliveira and researchers at the University of Texas identified a pulsating white dwarf star, G117-B15A, as the most stable known optical clock, more stable than an atomic clock. The team's findings were published in \"The Astrophysical Journal\".\n\nHe was president of the \"Sociedade Brasileira de Astronomia\" from 2002 to 2004, and is its current vice-president (2014-2016). He served on the SOAR and Gemini Board for the Association of Universities for Research in Astronomy, which is responsible for managing the Gemini Observatory. Together with Antonio Nemmer Kanaan Neto and other researchers, he is the co-discoverer of BPM37093, the \"Diamond Star\", a crystallized carbon-oxygen core pulsating white dwarf. With Detlev Koester and Gustavo Ourique, he discovered SDSSJ1240+6710, an oxygen white dwarf, \"Dox\".\n\nTogether with Maria de Fátima Oliveira Saraiva, he is the author of the book and site Astronomia e Astrofísica.\n\n"}
{"id": "18347714", "url": "https://en.wikipedia.org/wiki?curid=18347714", "title": "List of Connecticut state symbols", "text": "List of Connecticut state symbols\n\nThe following is a list of symbols of the U.S. state of Connecticut.\n\n"}
{"id": "43028515", "url": "https://en.wikipedia.org/wiki?curid=43028515", "title": "List of botanists by author abbreviation (E–F)", "text": "List of botanists by author abbreviation (E–F)\n\nTo find entries for A–D, use the table of contents above.\n\n\n\nTo find entries for G–Z, use the table of contents above.\n"}
{"id": "2680898", "url": "https://en.wikipedia.org/wiki?curid=2680898", "title": "List of compounds with carbon number 2", "text": "List of compounds with carbon number 2\n\nThis is a partial list of molecules that contain 2 carbon atoms.\n\n"}
{"id": "2388068", "url": "https://en.wikipedia.org/wiki?curid=2388068", "title": "List of contributors to general relativity", "text": "List of contributors to general relativity\n\nThis is a partial list of persons who have made \"major\" contributions to the development of standard mainstream general relativity. One simple rule of thumb for who belongs here is whether their contribution is recognized in the canon of standard general relativity textbooks. Some related lists are mentioned at the bottom of the page.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "24096816", "url": "https://en.wikipedia.org/wiki?curid=24096816", "title": "List of important publications in concurrent, parallel, and distributed computing", "text": "List of important publications in concurrent, parallel, and distributed computing\n\nThis is a list of important publications in concurrent, parallel, and distributed computing, organized by field.\n\nSome reasons why a particular publication might be regarded as important:\n\nSynchronizing concurrent processes. Achieving consensus in a distributed system in the presence of faulty nodes, or in a wait-free manner. Mutual exclusion in concurrent systems.\n\nDijkstra: “Solution of a problem in concurrent programming control”\n\nPease, Shostak, Lamport: “Reaching agreement in the presence of faults”\nLamport, Shostak, Pease: “The Byzantine generals problem”\n\nHerlihy, Shavit: “The topological structure of asynchronous computation”\nSaks, Zaharoglou: “Wait-free \"k\"-set agreement is impossible …”\n\nFundamental concepts such as time and knowledge in distributed systems.\n\nHalpern, Moses: “Knowledge and common knowledge in a distributed environment”\n"}
{"id": "16175377", "url": "https://en.wikipedia.org/wiki?curid=16175377", "title": "List of pest-repelling plants", "text": "List of pest-repelling plants\n\nThis list of pest-repelling plants includes plants believed for their ability to repel insects, nematodes, and other pests. They have been used in companion planting for pest control in agricultural and garden situations, and in households, although most research indicates that there are no plants that actually deter pests.\n\nThe essential oils of many plants are also well known for their pest-repellent properties. Oils from the families Lamiaceae (mints), Poaceae (true grasses), and Pinaceae (pines) are common insect repellents worldwide.\n\nPlants that can be planted or used fresh to repel pests include:\n"}
{"id": "40280998", "url": "https://en.wikipedia.org/wiki?curid=40280998", "title": "List of things named after Ludwig Boltzmann", "text": "List of things named after Ludwig Boltzmann\n\nThis refers to a list of things named after physicist Ludwig Eduard Boltzmann (February 20, 1844 – September 5, 1906)\n\n\n\n7 streets in Austria and 5 in Germany are named after him:\n\n1 house in Austria,\n\n"}
{"id": "11486239", "url": "https://en.wikipedia.org/wiki?curid=11486239", "title": "List of towns and cities with 100,000 or more inhabitants/cityname: W", "text": "List of towns and cities with 100,000 or more inhabitants/cityname: W\n\n\n"}
{"id": "56626141", "url": "https://en.wikipedia.org/wiki?curid=56626141", "title": "Maria Andrade (scientist)", "text": "Maria Andrade (scientist)\n\nMaria Isabel Andrade (born 1958) is a Cape Verdean food scientist. Andrade has worked in Mozambique as a sweet potato researcher since 1996 and was a co-winner of the 2016 World Food Prize.\n\nIn 1958, Andrade was born in São Filipe, Cape Verde. She began her education in Cape Verde in Fogo and Santiago. After completing high school, she went to the University of Arizona and graduated in 1983 with a Bachelor of Science in Master of Science, specializing in plant genetics. She completed further education at the North Carolina State University, earning a Doctor of Philosophy degree in plant breeding in 1994.\n\nAndrade began her agricultural career when she started a Cape Verdean vegetable planting program in 1984. While leading the National Research Institute in Cape Verde, Andrade became a member of the Food and Agriculture Organization in 1994. From 1996 to 2001, she worked for the International Institute of Tropical Agriculture as a sweet potato agronomist for a research group in Southern Africa.\n\nAndrade subsequently led a project in Mozambique that distributed sweet potatoes from 2002 to 2006. In 2006, shee began working for the International Potato Center as a manager of sweet potato breeding in Southern Africa. Outside of her work with the International Potato center, Andrade was the vice president of fundraising for the International Society for Tropical Root Crops from 2012 to 2016.\n\nIn 2013, Andrade was named a Nutrition Champion by Transform Nutrition. She was also a co-winner of the 2016 World Food Prize and the sole winner of the 2017 M.S. Swaminathan Award for Environment Protection.\n"}
{"id": "53823061", "url": "https://en.wikipedia.org/wiki?curid=53823061", "title": "María José Alonso", "text": "María José Alonso\n\nMaría José Alonso (Carrizo de la Ribera, León, Spain, 1958) is full professor of Biopharmaceutics and Pharmaceutical Technology at the University of Santiago de Compostela (USC), Spain. The laboratory she leads is specialized in Pharmaceutical Nanotechnology and Nanomedicine, and her research is oriented to the development of nanostructures for targeted delivery of drugs and vaccines. Her discoveries have led to significant clinical advances in the development of potential new treatments for cancer, ocular diseases, skin diseases, diabetes, obesity and other autoimmune pathologies, as well as new vaccines.\n\nMaría José Alonso has a Master degree in Pharmacy (University of Santiago de Compostela - USC, 1985) and a Ph.D. in Pharmaceutical Technology (USC, 1985). She has also developed her research career at the University of Paris XI and at the University of Angers (France), as well as in the Massachusetts Institute of Technology (MIT).\n\nShe is a member of the National Academy of Medicine (U.S.), besides three academies in Spain (Real Academia Nacional de Farmacia, Academia de Farmacia de Galicia, and Real Academia Galega de Ciencias).\n\nAlonso has held responsibilities in several scientific societies, among them the Controlled Release Society (CRS). She first contributed to the CRS as the Founder of the Spanish-Portuguese Local Chapter (1994), and she is currently the president-elect of this Society. Moreover, she is part of the editorial board of 11 scientific journals.\n\nShe has coordinated several research consortia financed by the World Health Organization (WHO), the Bill and Melinda Gates Foundation and the European Commission. Currently, she is involved in 7 international consortia.\n\nWith more than 260 scientific contributions and 20 patent families, she is the most influential researcher in Spain in the area of Pharmacology and Pharmacy (h-index), and is also classified among the top ten researchers in her field worldwide.\n\nMaría José Alonso’s Lab is focused on designing novel nanostructured materials intended to transport drugs and antigens across biological barriers (such as cellular, ocular, nasal, skin and intestinal barriers) and deliver them to the target tissue. Alonso’s research is specialized in the association of delicate molecules, including peptides, siRNA and antigens to these nanovehicles, with the final goal of producing innovative nanomedicines.\n\n\n\n\n"}
{"id": "18426245", "url": "https://en.wikipedia.org/wiki?curid=18426245", "title": "Mitutoyo", "text": "Mitutoyo\n\nMitutoyo, established on October 22, 1934 was founded by Yehan Numata ( \"Numata Ehan\") with one product, the micrometer. Mitutoyo's philosophy at that time was to make high-quality micrometers, but also to produce them in quantities that made them affordable and available to all of manufacturing. This philosophy was expanded in the next several decades to include a wider product offering focused on mechanical, dimensional gaging products, such as calipers, dial indicators, and other measuring tools.\n\nAs electronic technology became more widespread in the 1970s, Mitutoyo applied electronics to its line of dimensional gaging equipment to include electronic, or digital, measuring tools. During this time it also began to offer larger, more complex and more sensitive measuring instruments, including optical comparators, form measuring equipment, and coordinate measuring machines (CMMs). As statistical process control (SPC) was introduced, Mitutoyo led the world in the development of output gages, interfaces, data collectors and analysis software to take advantage of this new metrological science.\n\nWhen the computer made its way into the field of metrology, Mitutoyo again shifted its focus to include this technology into its product offering and push measuring accuracies into the sub-micrometre range. Today, Mitutoyo presents its 6,000+ products as integrated, computer-based metrology systems, where they can be interconnected to form closed-loop-measuring networks.\n\nMitutoyo America Corporation was formed in 1963 and is headquartered in Aurora, Illinois (just outside Chicago). Mitutoyo America offers the full product line of precision measuring tools, instruments and equipment with a distribution network, training and education classes, software development, and service support to provide a comprehensive metrology organization.\n\nDr. Yehan Numata is also the founder of Bukkyo Dendo Kyokai (BDK), the Society for the Promotion of Buddhism, which sponsors the Buddhist Canon Translation Project.\n\nThe company name is usually spelled in the Kunrei-shiki/Nihon-shiki manner \"Mitutoyo\". Because most publications for foreigners as of 2000 require the usage of Hepburn romanization, as of 2000 in the Tokyo Anglophone yellow pages the company name is rendered in the Hepburn style \"Mitsutoyo\".\n\nOn 14 September 2006, the Tokyo District Public Prosecutor’s Office indicted four former executives of the Mitutoyo Corporation. The company was penalized for violating the Foreign Exchange and Foreign Trade Law prohibiting the company from exporting any products for 6 months and from exporting measuring devices for an additional 2½ years (2007–mid 2010). In addition, the Japanese court gave the former executives multi-year jail sentences (suspended) and fined Mitutoyo ¥45 million (approx. 350,000 USD). It was found that Mitutoyo created software for their products that would falsify the accuracy of the measurements taken in order to circumvent customs inspections.\n\nThere is evidence that a portion of Mitutoyo’s illegal exports helped in nuclear-weapon programs in Libya, Iran, and North Korea. In particular, several Mitutoyo coordinate-measuring machines were allegedly sold to Scomi Precision Engineering in Malaysia. The Scomi scandal was part of a wider arms-smuggling operation masterminded by Pakistani nuclear scientist Abdul Qadeer Khan.\n\n"}
{"id": "40151251", "url": "https://en.wikipedia.org/wiki?curid=40151251", "title": "Montenegrin Sports Academy", "text": "Montenegrin Sports Academy\n\nThe Montenegrin Sports Academy (MSA) is a sport scientific society founded in 2003 in Podgorica, Montenegro, dedicated to the collection, generation and dissemination of scientific knowledge in the multidisciplinary area of sports sciences. The spirit of this non-profit organization is to promote sports science worldwide and share the knowledge among sport scientists.\n\nThe MSA is the leading association of sport scientists at the Montenegrin level and practices extensive co operations with corresponding non-Montenegrin associations.\nThe purpose of the Academy is the promotion of science and research, with special attention to sport science across Montenegro and beyond. \nIts topics include the motivation, attitudes, values and responses, adaptation, performance and health aspects of people engaged in physical activity and the relation of physical activity and lifestyle to health, prevention and aging. These topics are dealt with on an interdisciplinary basis.\n\nThe MSA is a non-profit organization. It supports Montenegrin institutions, such as the Ministry of Education and Sports and the Council of Youth and Sports, by offering scientific advice and assistance for coordinated Montenegrin, European and worldwide research projects defined by these bodies. \nAdditionally it serves as the most important Montenegrin network of sport scientists from all relevant sub disciplines.\n\nThe MSA offers individual membership to sport and related scientists. The objective is to create a scientific Montenegrin, European and worldwide network for scientific exchange and interaction. This is strengthened by the annual Congresses and other membership benefits such as the Montenegrin Journal of Sports Science and Medicine (MJSS), Email Newsletters, etc.\nThe qualification for the MSA membership students is a university level degree (master’s or doctor’s degree, university examination) in the field of sport science, or an equivalent university degree in other related areas.\nMembers comprise scientists from all areas of sport science such as Physiology, Sports Medicine, Psychology, Molecular Biology, Sociology, Biochemistry, Motor Control, Biomechanics, Training Science and many more.\n\nAnnual Congresses have been organized since the inauguration of the MSA in 2003. The MSA Congresses are attended by sport scientists worldwide with an academic career. On average over 70% of the participants have at least a PhD or an equivalent in the field of sport science or a related discipline.\nFuture MSA Congresses include 2014 Podgorica, comprises a range and selection of invited lecturers, as well as serious of various debates through oral and poster presentation.\n\n\n"}
{"id": "35975372", "url": "https://en.wikipedia.org/wiki?curid=35975372", "title": "Moscow Urban Forum", "text": "Moscow Urban Forum\n\nThe Moscow Urban Forum (MUF) is an international forum about urbanization issues held in Moscow annually under the auspices of the Government of Moscow. The first MUF was held in 7-9 December 2011, the second took place 4-5 December 2012, and the third in 5-6 December 2013. MUF serves as a platform for open dialogue among representatives of the government, the managers of architectural and urban planning organizations, the real estate industry, the citizens of Moscow and leading foreign and Russian experts on urban planning. The Urban Land Institute is the international partner of MUF.\n\n"}
{"id": "20501785", "url": "https://en.wikipedia.org/wiki?curid=20501785", "title": "Otto Andreas Lowson Mörch", "text": "Otto Andreas Lowson Mörch\n\nOtto Andreas Lowson Mörch (his last name also spelled Mørch) (17 May 1828 – 25 January 1878) was a biologist, specifically a malacologist. He lived in Sweden, in Denmark, and in France.\n\nBibliography and taxa described by Otto Andreas Lowson Mörch include:\n\n1863\n\n\n1864\n\n\n"}
{"id": "6867017", "url": "https://en.wikipedia.org/wiki?curid=6867017", "title": "Paratype", "text": "Paratype\n\nIn zoology and botany, a paratype is a specimen of an organism that helps define what the scientific name of a species and other taxon actually represents, but it is not the holotype (and in botany is also neither an isotype nor a syntype). Often there is more than one paratype. Paratypes are usually held in museum research collections.\n\nThe exact meaning of the term \"paratype\" when it is used in zoology is not the same as the meaning when it is used in botany. In both cases however, this term is used in conjunction with \"holotype\".\n\nIn zoological nomenclature, a paratype is officially defined as \"Each specimen of a type series other than the holotype.\"\n\nIn turn, this definition relies on the definition of a \"type series.\" A type series is the material (specimens of organisms) that was cited in the original publication of the new species or subspecies, and was not excluded from being type material by the author (this exclusion can be implicit, e.g., if an author mentions \"paratypes\" and then subsequently mentions \"other material examined\", the latter are not included in the type series), nor referred to as a variant, or only dubiously included in the taxon (e.g., a statement such as \"I have before me a specimen which agrees in most respects with the remainder of the type series, though it may yet prove to be distinct\" would exclude this specimen from the type series).\n\nThus, in a type series of five specimens, if one is the holotype, the other four will be paratypes.\n\nA paratype may originate from a different locality than the holotype. A paratype cannot become a lectotype, though it is eligible (and often desirable) for designation as a neotype.\n\nThe International Code of Zoological Nomenclature (ICZN) has not always required a type specimen, but any species or subspecies newly described after the end of 1999 must have a designated holotype or syntypes.\n\nA related term is allotype, a term that indicates a specimen that exemplifies the opposite sex of the holotype, and is almost without exception designated in the original description, and, accordingly, part of the type series, and thus a paratype; in such cases, it is functionally no different from any other paratype. It has no nomenclatural standing whatsoever, and although the practice of designating an allotype is recognized by the Code, it is not a \"name-bearing type\" and there are no formal rules controlling how one is designated. Apart from species exhibiting strong sexual dimorphism, relatively few authors take the trouble to designate such a specimen. It is not uncommon for an allotype to be a member of an entirely different species from the holotype, because of an incorrect association by the original author.\n\nIn botanical nomenclature, a paratype is a specimen cited in the original description that may not have been said to be a type. It is not the holotype nor an isotype (duplicate of the holotype).\n\n\nLike other types, a paratype may be specified for taxa at the rank of family or below (Article 7).\n\nA paratype may be designated as a lectotype if no holotype, isotype, syntype, or isosyntype (duplicate of a syntype) is extant (Article 9.12).\n\n"}
{"id": "4043742", "url": "https://en.wikipedia.org/wiki?curid=4043742", "title": "Physics beyond the Standard Model", "text": "Physics beyond the Standard Model\n\nPhysics beyond the Standard Model (BSM) refers to the theoretical developments needed to explain the deficiencies of the Standard Model, such as the origin of mass, the strong CP problem, neutrino oscillations, matter–antimatter asymmetry, and the nature of dark matter and dark energy. Another problem lies within the mathematical framework of the Standard Model itself: the Standard Model is inconsistent with that of general relativity, to the point where one or both theories break down under certain conditions (for example within known spacetime singularities like the Big Bang and black hole event horizons).\n\nTheories that lie beyond the Standard Model include various extensions of the standard model through supersymmetry, such as the Minimal Supersymmetric Standard Model (MSSM) and Next-to-Minimal Supersymmetric Standard Model (NMSSM), and entirely novel explanations, such as string theory, M-theory, and extra dimensions. As these theories tend to reproduce the entirety of current phenomena, the question of which theory is the right one, or at least the \"best step\" towards a Theory of Everything, can only be settled via experiments, and is one of the most active areas of research in both theoretical and experimental physics.\n\nDespite being the most successful theory of particle physics to date, the Standard Model is not perfect. A large share of the published output of theoretical physicists consists of proposals for various forms of \"Beyond the Standard Model\" new physics proposals that would modify the Standard Model in ways subtle enough to be consistent with existing data, yet address its imperfections materially enough to predict non-Standard Model outcomes of new experiments that can be proposed.\n\nThe Standard Model is inherently an incomplete theory. There are fundamental physical phenomena in nature that the Standard Model does not adequately explain:\n\nNo experimental result is accepted as definitively contradicting the Standard Model at the five sigma level, widely considered to be the threshold of a discovery in particle physics. But because every experiment contains some degree of statistical and systemic uncertainty, and the theoretical predictions themselves are also almost never calculated exactly and are subject to uncertainties in measurements of the fundamental constants of the Standard Model (some of which are tiny and others of which are substantial), it is mathematically expected that some of the hundreds of experimental tests of the Standard Model will deviate to some extent from it, even if there were no new physics to be discovered.\n\nAt any given time there are a number of experimental results that are significantly different from the Standard Model expectation, although many of these have been found to be statistical flukes or experimental errors as more data has been collected. On the other hand, any physics beyond the Standard Model would necessarily first manifest experimentally as a statistically significant difference between an experiment and the theoretical prediction.\n\nIn each case, physicists seek to determine if a result is a mere statistical fluke or experimental error on the one hand, or a sign of new physics on the other. More statistically significant results cannot be mere statistical flukes but can still result from experimental error or inaccurate estimates of experimental precision. Frequently, experiments are tailored to be more sensitive to experimental results that would distinguish the Standard Model from theoretical alternatives.\n\nSome of the most notable examples include the following:\n\n\nObservation at particle colliders of all of the fundamental particles predicted by the Standard Model has been confirmed. The Higgs boson is predicted by the Standard Model's explanation of the Higgs mechanism, which describes how the weak SU(2) gauge symmetry is broken and how fundamental particles obtain mass; it was the last particle predicted by the Standard Model to be observed. On July 4, 2012, CERN scientists using the Large Hadron Collider announced the discovery of a particle consistent with the Higgs boson, with a mass of about . A Higgs boson was confirmed to exist on March 14, 2013, although efforts to confirm that it has all of the properties predicted by the Standard Model are ongoing.\n\nA few hadrons (i.e. composite particles made of quarks) whose existence is predicted by the Standard Model, which can be produced only at very high energies in very low frequencies have not yet been definitively observed, and \"glueballs\" (i.e. composite particles made of gluons) have also not yet been definitively observed. Some very low frequency particle decays predicted by the Standard Model have also not yet been definitively observed because insufficient data is available to make a statistically significant observation.\n\nSome features of the standard model are added in an ad hoc way. These are not problems per se (i.e. the theory works fine with these ad hoc features), but they imply a lack of understanding. These ad hoc features have motivated theorists to look for more fundamental theories with fewer parameters. Some of the ad hoc features are:\n\nThe standard model has three gauge symmetries; the colour SU(3), the weak isospin SU(2), and the weak hypercharge U(1) symmetry, corresponding to the three fundamental forces. Due to renormalization the coupling constants of each of these symmetries vary with the energy at which they are measured. Around these couplings become approximately equal. This has led to speculation that above this energy the three gauge symmetries of the standard model are unified in one single gauge symmetry with a simple group gauge group, and just one coupling constant. Below this energy the symmetry is spontaneously broken to the standard model symmetries. Popular choices for the unifying group are the special unitary group in five dimensions SU(5) and the special orthogonal group in ten dimensions SO(10).\n\nTheories that unify the standard model symmetries in this way are called Grand Unified Theories (or GUTs), and the energy scale at which the unified symmetry is broken is called the GUT scale. Generically, grand unified theories predict the creation of magnetic monopoles in the early universe, and instability of the proton. Neither of these have been observed, and this absence of observation puts limits on the possible GUTs.\n\nSupersymmetry extends the Standard Model by adding another class of symmetries to the Lagrangian. These symmetries exchange fermionic particles with bosonic ones. Such a symmetry predicts the existence of \"supersymmetric particles\", abbreviated as \"sparticles\", which include the sleptons, squarks, neutralinos and charginos. Each particle in the Standard Model would have a superpartner whose spin differs by 1/2 from the ordinary particle. Due to the breaking of supersymmetry, the sparticles are much heavier than their ordinary counterparts; they are so heavy that existing particle colliders may not be powerful enough to produce them.\n\nIn the standard model, neutrinos have exactly zero mass. This is a consequence of the standard model containing only left-handed neutrinos. With no suitable right-handed partner, it is impossible to add a renormalizable mass term to the standard model. Measurements however indicated that neutrinos spontaneously change flavour, which implies that neutrinos have a mass. These measurements only give the mass differences between the different flavours. The best constraint on the absolute mass of the neutrinos comes from precision measurements of tritium decay, providing an upper limit 2 eV, which makes them at least five orders of magnitude lighter than the other particles in the standard model. This necessitates an extension of the standard model, which not only needs to explain how neutrinos get their mass, but also why the mass is so small.\n\nOne approach to add masses to the neutrinos, the so-called seesaw mechanism, is to add right-handed neutrinos and have these couple to left-handed neutrinos with a Dirac mass term. The right-handed neutrinos have to be sterile, meaning that they do not participate in any of the standard model interactions. Because they have no charges, the right-handed neutrinos can act as their own anti-particles, and have a Majorana mass term. Like the other Dirac masses in the standard model, the neutrino Dirac mass is expected to be generated through the Higgs mechanism, and is therefore unpredictable. The standard model fermion masses differ by many orders of magnitude; the Dirac neutrino mass has at least the same uncertainty. On the other hand, the Majorana mass for the right-handed neutrinos does not arise from the Higgs mechanism, and is therefore expected to be tied to some energy scale of new physics beyond the standard model, for example the Planck scale. Therefore, any process involving right-handed neutrinos will be suppressed at low energies. The correction due to these suppressed processes effectively gives the left-handed neutrinos a mass that is inversely proportional to the right-handed Majorana mass, a mechanism known as the see-saw. The presence of heavy right-handed neutrinos thereby explains both the small mass of the left-handed neutrinos and the absence of the right-handed neutrinos in observations.\nHowever, due to the uncertainty in the Dirac neutrino masses, the right-handed neutrino masses can lie anywhere. For example, they could be as light as keV and be dark matter, they can have a mass in the LHC energy range and lead to observable lepton number violation, or they can be near the GUT scale, linking the right-handed neutrinos to the possibility of a grand unified theory.\n\nThe mass terms mix neutrinos of different generations. This mixing is parameterized by the PMNS matrix, which is the neutrino analogue of the CKM quark mixing matrix. Unlike the quark mixing, which is almost minimal, the mixing of the neutrinos appears to be almost maximal. This has led to various speculations of symmetries between the various generations that could explain the mixing patterns. The mixing matrix could also contain several complex phases that break CP invariance, although there has been no experimental probe of these. These phases could potentially create a surplus of leptons over anti-leptons in the early universe, a process known as leptogenesis. This asymmetry could then at a later stage be converted in an excess of baryons over anti-baryons, and explain the matter-antimatter asymmetry in the universe.\n\nThe light neutrinos are disfavored as an explanation for the observation of dark matter, due to considerations of large-scale structure formation in the early universe. Simulations of structure formation show that they are too hot—i.e. their kinetic energy is large compared to their mass—while formation of structures similar to the galaxies in our universe requires cold dark matter. The simulations show that neutrinos can at best explain a few percent of the missing dark matter. However, the heavy sterile right-handed neutrinos \"are\" a possible candidate for a dark matter WIMP.\n\nSeveral preon models have been proposed to address the unsolved problem concerning the fact that there are three generations of quarks and leptons. Preon models generally postulate some additional new particles which are further postulated to be able to combine to form the quarks and leptons of the standard model. One of the earliest preon models was the Rishon model.\n\nTo date, no preon model is widely accepted or fully verified.\n\nTheoretical physics continues to strive toward a theory of everything, a theory that fully explains and links together all known physical phenomena, and predicts the outcome of any experiment that could be carried out in principle. In practical terms the immediate goal in this regard is to develop a theory which would unify the Standard Model with General Relativity in a theory of quantum gravity. Additional features, such as overcoming conceptual flaws in either theory or accurate prediction of particle masses, would be desired.\nThe challenges in putting together such a theory are not just conceptual - they include the experimental aspects of the very high energies needed to probe exotic realms.\n\nSeveral notable attempts in this direction are supersymmetry, string theory, and loop quantum gravity.\n\nExtensions, revisions, replacements, and reorganizations of the Standard Model exist in attempt to correct for these and other issues. String theory is one such reinvention, and many theoretical physicists think that such theories are the next theoretical step toward a true Theory of Everything. Theories of quantum gravity such as loop quantum gravity and others are thought by some to be promising candidates to the mathematical unification of quantum field theory and general relativity, requiring less drastic changes to existing theories. However recent work places stringent limits on the putative effects of quantum gravity on the speed of light, and disfavours some current models of quantum gravity.\n\nAmong the numerous variants of string theory, M-theory, whose mathematical existence was first proposed at a String Conference in 1995, is believed by many to be a proper \"ToE\" candidate, notably by physicists Brian Greene and Stephen Hawking. Though a full mathematical description is not yet known, solutions to the theory exist for specific cases. Recent works have also proposed alternate string models, some of which lack the various harder-to-test features of M-theory (e.g. the existence of Calabi–Yau manifolds, many extra dimensions, etc.) including works by well-published physicists such as Lisa Randall.\n\n"}
{"id": "27927060", "url": "https://en.wikipedia.org/wiki?curid=27927060", "title": "Planet Earth (1986 TV series)", "text": "Planet Earth (1986 TV series)\n\nPlanet Earth is a seven-episode 1986 PBS television documentary series focusing on the Earth, narrated by Richard Kiley.\n\n\"Planet Earth\" explores geoscience and how discoveries of the early and mid-1980s were revolutionizing mankind's understanding of the Earth's past, present, and future. It also highlights scientific discoveries not yet fully understood and still under study in the mid-1980s. The series explores the Earth's origins, history, and structure; the forces that operate continually to alter its surface; its oceans; its climate; its natural resources; its biosphere and the effects of life on the physical world; its relationship to the Sun and other bodies in the solar system; and its possible future in the face of pressures the growing human population places on the natural world.\n\nThe BBC used the same title for its 2006 series, but the two series are completely unrelated and quite different in focus and content.\n\nProduced by WQED in Pittsburgh, Pennsylvania, in association with the National Academy of Sciences as the centerpiece for a college-credit telecourse, \"Planet Earth\" was filmed over a period of four years on all seven continents and from the ocean bottom to earth orbit. The Annenberg/CPB Project and IBM funded production of the series. It enjoyed success in its original run, airing weekly on Thursday evenings on PBS from January 22 to March 5, 1986.\n\nA companion book to the series written by Jonathan Weiner, also entitled \"Planet Earth\", was published in 1986 by Bantam Books. Both the series and the companion book sometimes are marketed as \"Our Planet Earth\" in an attempt to avoid confusion with the 2006 BBC series \"Planet Earth\".\n\nSome footage shot for \"Planet Earth\" later also was used in the 1992 PBS series \"Earth Revealed\".\n\nIn January 1986, \"Los Angeles Times\" critic Lee Margulies praised \"Planet Earth\" as \"serious, but not dry\" and credited it for its vivid filming of natural scenery, use of computer graphics, and achievement of depicting ongoing scientific research of the early and mid-1980s as \"challenging, interesting, and worthwhile.\"\n\n\"Planet Earth\" was the co-winner of the 1985-1986 Primetime Emmy Award for Outstanding Informational Series or Special, sharing it with \"Laurence Olivier - A Life\", a multi-part biography of Laurence Olivier that aired on the PBS series \"Great Performances\" that season.\n\n\n"}
{"id": "12944950", "url": "https://en.wikipedia.org/wiki?curid=12944950", "title": "Regional Science Association International", "text": "Regional Science Association International\n\nThe Regional Science Association International (RSAI) is a cluster of scholarly societies whose members engage in regional science.\n\nIn the late 1940s and the early 1950s, the economist Walter Isard worked to draw together a group of academics interested in analyzing regional (i.e., sub-national) development. These academics were drawn from a number of disciplines: economics, geography, city planning, political science and rural sociology. Naming their new approach \"regional science\", they envisioned it as an interdisciplinary effort, one that would require unique theoretical concepts, methodological tools, and data. Because their effort was interdisciplinary, no existing academic organization brought the participants together; they therefore created their own organization, the \"Regional Science Association\", which first met in December 1954.\n\nBy 1961, the RSA had 960 members, and its first local \"section\", which organized conferences for regional scientists located in the western United States. The next section to form (in 1963) was in Japan, followed by sections throughout Europe, as well as in India, Argentina, and Brazil. In 1969, the Western section and the Japan section began to cooperate in holding a biennial \"Pacific Rim\" international conference.\n\nGradually issues of coordination among the various sections were worked out, resulting in much the present structure by 1990. The \"Regional Science Association International\" (RSAI) now functions as the umbrella organization for three other organizations: the \"Pacific Regional Science Conference Organization\", the \"North American Regional Science Council\", and the \"European Regional Science Association\". Each of these serve as the umbrella organization for a number of other organizations, as seen in the next section.\nRegional Science Association International is the umbrella organization for all affiliated regional science associations:\n\n"}
{"id": "9801143", "url": "https://en.wikipedia.org/wiki?curid=9801143", "title": "Research Council for Complementary Medicine", "text": "Research Council for Complementary Medicine\n\nThe Research Council for Complementary Medicine (RCCM) is a charitable organisation (UK Registered Charity Number 1146724) founded in 1983 to develop and promote good quality research into alternative and complementary medicine (CAM) and enhance evidence-based medicine in this area.\n\nThe RCCM works with the complementary therapy professions and CAM researchers to promote safe and effective integrative and personalised medicine, and to promote the development of high quality research and evidence in the UK into complementary and alternative medicine.\n\nAs the burden of complex chronic disease increases more people are turning to alternative and complementary medicine (CAM) and need guidance in making their healthcare choices. CAM is used by at least a quarter of the population of the UK. The research evidence base is limited at present. The ever increasing cost of chronic disease will require government and society to make decisions on the cost-effectiveness of the available health care options. Such decisions are best informed by evidence and currently accepted standards of care.\n\nA greater evidence base for CAM should facilitate wider availability of access to safe and effective complementary therapies within the National Health Service and across the UK, in order to help in preventing disease and improvinghealth.\n\nThe House of Lords Science and Technology Select Committee in 2000 published a report on CAM which emphasised the need for improved regulation and increased research into CAM therapies. While regulation of the main CAM professions has made considerable progress, a paucity of funding has limited the development of CAM research.\n\nThe RCCM was co-founded in 1983 by a medical doctor Dr Richard Tonkins MD FRCP and a lay businessman, Mr Harold Wicks. During the 1980s, the focus of RCCM activity was on encouraging and publishing research projects, and on the exploration of appropriate research methodologies. A scientific journal was established, Complementary Medical Research (). A University Fellowship to explore CAM was established in partnership with the Medical Research Council and the University of Glasgow. A series of annual conferences on research methodology were organised.\n\nDuring the 1990s, the RCCM focus was on undergraduate medical education and the inclusion of a CAM module as a part of the curricula for medical education; Dr Catherine Zollman became Director of the RCCM’s Medical Education Service and established a register of GPs with an interest in CAM. An RCCM research project was undertaken on the use of CAM by women with breast cancer, commissioned by South Thames Regional Health Authority\n\nAfter 2000, the RCCM was commissioned to undertake and completed some very large projects in partnership with universities, including the CAMEOL project for the Department of Health to assess CAM interventions in NHS priority areas by detailed review and critical appraisal of published research\n\nThe current activities of RCCM are directed towards supporting the CAM professions in research and evidence-based practice, through increasing engagement between the professional bodies, the regulators, and the research community.\n\nThe CAM professions in the UK are evolving and maturating. Regulation is well developed, with statutory regulation of osteopathy and chiropractic in place, and herbal medicine, acupuncture and traditional Chinese medicine moving towards statutory regulation. Other therapies such as massage and naturopathy have established voluntary regulation under the Complementary and Natural Healthcare Council (CNHC) and others are moving towards this goal.\n\nDoctoral and post-doctoral research within CAM has guidance on methodology from the MRC. Research capacity in terms of on-going PhD/ MPhil research activity in complementary medicine within UK universities is increasing. However, evidence on the safety and effectiveness of CAM remains much needed, and more remains to be done in terms of translation of research into clinical practice\n\nFunding from the government for research to evaluate the safety and effectiveness of CAM, similar to that in the USA where a research funding council NCCAM provides substantial research funding specifically for CAM research, would enable the evidence base for CAM to develop in the UK.\n\n• The web site of RCCM \n• ISCMR is an international organisation to promote CAM research\n"}
{"id": "40329294", "url": "https://en.wikipedia.org/wiki?curid=40329294", "title": "Reticular theory", "text": "Reticular theory\n\nReticular theory is an obsolete scientific theory in neurobiology that stated that everything in the nervous system, such as brain, is a single continuous network. The concept was postulated by a German anatomist Joseph von Gerlach in 1871, and was most popularised by the Nobel laureate Italian physician Camillo Golgi.\n\nHowever, the theory was refuted by later observations of a Spanish pathologist Santiago Ramón y Cajal, using a staining technique discovered by Golgi, which showed that nervous tissue, like other tissues, is made of discrete cells. This neuron doctrine turned out to be the correct description of the nervous system, whereas the reticular theory was discredited.\n\nThe proponents of the two contrasting theories, Golgi and Ramón y Cajal were jointly awarded the Nobel Prize in Physiology or Medicine in 1906, \"in recognition of their work on the structure of the nervous system\".\n\nIn 1863 a German anatomist Otto Friedrich Karl Deiters described the existence of an unbranched tubular process (the axon) extending from some cells in the central nervous system, specifically from the lateral vestibular nucleus. In 1871 Gerlach proposed that the brain is composed of \"protoplasmic network\", hence the basis of reticular theory. According to Gerlach, the nervous system simply consisted of a single continuous network called the reticulum. In 1873 Golgi invented a revolutionary method for microscopic research based on a specific technique for staining nerve cells, which he called \"la reazione nera\" (the \"black reaction\"). He was able to provide an intricate description of nerve cells in various regions of the cerebro-spinal axis, clearly distinguishing the axon from the dendrites. He drew up a new classification of cells on the basis of the structure of their nervous prolongation, and he criticized Gerlach's theory of the \"protoplasmic network\". Golgi claimed to observe in the gray matter an extremely dense and intricate network, composed of a web of intertwined branches of axons coming from different cell layers (\"diffuse nervous network\"). This structure, which emerges from the axons and is therefore essentially different from that hypothesized by Gerlach, appeared in his view to be the main organ of the nervous system, the organ that connected different cerebral areas both anatomically and functionally by means of the transmission of an electric nervous impulse. Although Golgi's earlier works between 1873 and 1885 clearly depicted the axonal connections of cerebellar cortex and olfactory bulb as independent of one another, his later works including the Nobel Lecture showed the entire granular layer of the cerebellar cortex occupied by a network of branching and anastomosing nerve processes. This was due to his strong conviction in the reticular theory.\n\nIn 1877 an English physiologist Edward Schäfer described the absence of connections between the nerve elements in the mantles of the jellyfish. The Norwegian zoologist Fridtjof Nansen also reported in 1887 that he found no connections between the processes of the ganglion cells of aquatic animals in his doctoral research (\"The Structure and Combination of Histological Elements of the Central Nervous System\"). By the late 1880s, serious opposition to the reticular theory began to emerge. Wilhelm His in Leipzig studied the embryological development of the central nervous system and concluded that his observations were consistent with the classic cell theory (that nerve cells were individual cells), and not the reticular theory. In 1891, another German anatomist Wilhelm Waldeyer also supported the theory by stating that the nervous system, as other tissues, was composed of cells, which he named \"neurons.\" Using the very same Golgi's technique, Ramón y Cajal confirmed that discrete neurons did exist, thereby strengthening the concept of the growing neuron doctrine. Golgi, however, never accepted these new findings, and a controversy and rivalry between the two scientists lasted even after they were jointly awarded the Nobel Prize in 1906. The Nobel award is even dubbed as creating the \"storm center of histological controversy\". Ramón y Cajal even commented that: \"What a cruel irony of fate of pair, like Siamese twins united by the shoulders, scientific adversaries of such contrasting character!\".\n\nIn the 1950s electron microscopy finally confirmed the existence of individual neurons in the central nervous system, and the existence of gaps in between neurons called synapse. The reticular theory was finally put to rest.\n\n"}
{"id": "56680786", "url": "https://en.wikipedia.org/wiki?curid=56680786", "title": "Road to Survival", "text": "Road to Survival\n\nRoad to Survival is a 1948 book by William Vogt. It was a major inspiration for the modern environmentalist movement.\n\n\"Road to Survival\" is a summary of the ecological status of the world. Vogt attacks capitalism, and describes United States history as a \"march of destruction\".\n\nThe book is filled with scientific data, and its world-wide scope was unusual at the time. Ultimately, the book advocates population control as the only way to prevent environmental disaster. Human population could not exceed the planet's carrying capacity without disaster.\n\nThe book was commercially successful. A condensed version was published in \"Reader's Digest\", and many universities used the book as a textbook. However, his message was attacked by ideologues of all varieties, by conservatives for opposing capitalism and supporting birth control, and by liberals as proof of \"science's bankruptcy in the face of pressing modern problems\".\n\nLater on, the book would inspire the modern environmental movement, with both Rachel Carson and Paul Ehrlich being inspired by it.\n\n"}
{"id": "12225559", "url": "https://en.wikipedia.org/wiki?curid=12225559", "title": "SMART (Scottish business grant)", "text": "SMART (Scottish business grant)\n\nSMART: SCOTLAND is a high profile innovation support grant scheme in Scotland aimed at small and medium-sized firms. It supports commercially viable projects which represent a significant technological advance for the UK sector or industry concerned.\n\nThe SMART grant supports technology development through:\n\n\nApplicants must be based in Scotland or planning to set up in Scotland and must meet the European Community definition of a small and medium-sized enterprise which in summary means they must:\n\n\nAwards are judged according to technological, intellectual property, financial and commercial aspects of the application as well as the management expertise available to the business. In addition other aspects such as the project's implications on society such as environmental impact, sustainability and health and safety may be taken into account.\n\nThe grant was launched in Scotland as SMART in 1999 and by April 2007 grants had been given to almost 400 businesses across Scotland amounting to some £14.7 million funding. In April 2007 SMART was relaunched as the \"SMART:SCOTLAND\" Programme .\n\n\n"}
{"id": "47130200", "url": "https://en.wikipedia.org/wiki?curid=47130200", "title": "Soyuz 7K-OK No.1", "text": "Soyuz 7K-OK No.1\n\nSoyuz 7K-OK No.1 was an unmanned spacecraft of the Soyuz program, originally intended to perform a rendezvous maneouver with Kosmos 133 (Soyuz 7K-OK No.2). After the Kosmos 133 mission failed, the rocket was moved to the launch pad on December 12, 1966 and scheduled to launch on December 14, 1966, 4 PM local time.\n\nAt ignition, one of the strap-on boosters failed to start and so an automatic command shut down the core stage and remaining strap-ons. Launch personnel began safeing the booster in preparation to take it down from the pad for examination. About 27 minutes after the aborted launch, the launch escape system suddenly activated. The Soyuz descent module was blasted free of the stack and touched down a quarter mile from the pad. Meanwhile, the exhaust from the LES caught the third stage of the booster on fire. Flames began curling down the side of the booster as launch personnel ran for cover. After a few minutes, the core stage and strap-ons exploded, completely destroying the entire launch vehicle and causing major damage to LC-31. One person on the ground was killed and the pad was not used again for seven months following the disaster.\n\nSince tracking cameras around LC-31 had been turned off when the launch aborted, there was no film footage of the fire or explosion for analysis, but telemetry data found that the igniter in the Blok D strap-on had failed to activate. This was a minor problem and could have been easily fixed by simply installing a new igniter. The bigger question was why the LES activated. Initially, it was suspected that the booster had been bumped when the gantry tower was put back in place following the abort and that this somehow managed to trigger the LES, but a more thorough investigation found a different cause. During the attempted launch, the booster switched from external to internal power as it normally would do, which then activated the abort sensing system. The Earth's rotation caused the rate gyros to register an approximately 8° tilt 27 minutes after the aborted liftoff, which the abort sensing system then interpreted as meaning that the booster had deviated from its flight path, and thus it activated the LES. The abort sensing system in the Soyuz was thus redesigned to prevent a recurrence of this unanticipated design flaw. On the other hand, the LES had also worked flawlessly and demonstrated its ability to safely pull cosmonauts from the booster should an emergency arise as it did years later in the Soyuz T-10-1 abort.\n\nOne more mystery remained, which was why the LES had ignited the third stage on fire, something it was not supposed to do. The conclusion was that when the Soyuz descent module separated from the service module during the abort, it had inadvertently severed coolant lines in the service module, which then leaked out their highly flammable contents and started a fire when they were contacted by the LES exhaust.\n"}
{"id": "25029369", "url": "https://en.wikipedia.org/wiki?curid=25029369", "title": "The Canon (Natalie Angier book)", "text": "The Canon (Natalie Angier book)\n\nThe Canon: A Whirligig Tour of the Beautiful Basics of Science is a book written by American science author Natalie Angier.\n\n\"The Canon\" presents a summary of some of the different areas of science, as well as extensive descriptions of, and interviews with, contemporary scientists who work in these fields. Angier’s tenet is that an understanding of the basics of major areas of science can assist with providing a means by which to understand current scientific issues, and that this process should be fun. In her Introduction, Angier writes:\n\nAngier included quotes from the scientists she interviewed throughout her descriptions of different scientific topics in an attempt to show how scientists experience and think about their work, and why they do it.\n\nTo obtain material for \"The Canon\", Angier interviewed a number of scientists, professors, and other science professionals, and incorporated their stories and quotes into her work. She asked them questions such as, \"What does it mean to think scientifically?\" and \"What should nonspecialist nonchildren know about science, and how should they know it, and what is this thing called fun?\" Most of these scientists are presently active in their field of research. In addition, many of these scientists have extensive bodies of work listed in detail elsewhere. The below list links the science professionals who Angier interviewed for \"The Canon\" with additional details relating to their work:\n\n\n"}
{"id": "54672437", "url": "https://en.wikipedia.org/wiki?curid=54672437", "title": "The Magic School Bus Rides Again", "text": "The Magic School Bus Rides Again\n\nThe Magic School Bus Rides Again is a Canadian-American animated children's television series, based on the book series of the same name by Joanna Cole and Bruce Degen. It also serves as a continuation of the 1994–97 PBS Kids series \"The Magic School Bus\", with Lily Tomlin reprising her role as Ms. Frizzle. The series premiered on Netflix on September 29, 2017. The second season premiered on April 13, 2018.\n\nIn a sequel to the original 1994–97 series, Valerie Frizzle retires from teaching and passes the keys of the Magic School Bus over to her younger sister, Fiona Frizzle, to embark on new adventures with her class. As they journey on their exciting new field-trips, they discover new locations, creatures, time periods and more to learn about the wonders of science, educating young viewers along the way.\n\n\n\n\nIn 2014, the series was first announced by Netflix and Scholastic Media, and was entitled \"The Magic School Bus 360°\". The new iteration of the franchise features a modernized \"Ms. Frizzle\" (replacing Valerie with her younger sister Fiona) and a high-tech bus that stresses modern inventions such as robotics, wearables and camera technology. The hope is to captivate children's imaginations and motivate their interest in the sciences. It was produced by 9 Story Media Group. Producer Stuart Stone, who voiced Ralphie in the original series, stated that \"The Magic School Bus 360°\" will feature some of the original voice actors in different roles. The show's voice cast is based in Los Angeles, California, United States and Toronto, Ontario, Canada\nwith Susan Blu as the Los Angeles voice director and Alyson Court as the Toronto voice director. In February 2017, Netflix announced that Kate McKinnon was cast in the role of Ms. Frizzle (without clarifying it was Fiona, the younger sister of Valerie, now Professor Frizzle, still voiced by Tomlin). The series then changed its name to \"The Magic School Bus Rides Again\". Lin-Manuel Miranda of \"Hamilton\" sings the theme song. Phoebe Terese, a character from the original series, is replaced by a new student named Jyoti, as it was revealed that Phoebe transferred back to her old school. The series was released on Netflix on September 29, 2017. The second season was released on April 13, 2018 in association with Nelvana, which also worked on the original series.\n\nThe show is Flash animated unlike the original series that is traditionally-animated. Despite its second season made with Nelvana, the entire Flash-animated show is made with Adobe After Effects, Adobe Animate (formerly Adobe Flash) and Adobe Photoshop.\n\n"}
