{"id": "22395556", "url": "https://en.wikipedia.org/wiki?curid=22395556", "title": "Agricultural engineering", "text": "Agricultural engineering\n\nAgricultural engineering is the engineering discipline that studies agricultural production and processing. Agricultural engineering combines the disciplines of mechanical, civil, electrical and chemical engineering principles with a knowledge of agricultural principles according to technological principles. A key goal of this discipline is to improve the efficacy and sustainability of agricultural practices. One of the leading organizations in this industry is the American Society of Agricultural and Biological Engineering.\n\nThe ASABE provides safety and regulatory standards for the agricultural industry. These standards and regulations are developed on an international scale and include topics on fertilizers, soil conditions, fisheries, biofuels, biogass, feed machinery, tractors, and machinery.\n\nAgricultural engineers may engage in any of the following areas: \n\nThe first curriculum in agricultural engineering was established at Iowa State University by Professor J. B. Davidson in 1903. The American Society of Agricultural Engineers, now known as the American Society of Agricultural and Biological Engineers, was founded in 1907. A full history of events directly and indirectly influenced by agricultural engineering, see here.\n\nAgricultural engineers may perform tasks such as planning, supervising and managing the building of dairy effluent schemes, irrigation, drainage, flood water control systems, performing environmental impact assessments, agricultural product processing and interpret research results and implement relevant practices. A large percentage of agricultural engineers work in academia or for government agencies such as the United States Department of Agriculture or state agricultural extension services. Some are consultants, employed by private engineering firms, while others work in industry, for manufacturers of agricultural machinery, equipment, processing technology, and structures for housing livestock and storing crops. Agricultural engineers work in production, sales, management, research and development, or applied science.\n\nIn the United Kingdom the term Agricultural Engineer is often also used to describe a person that repairs or modifies agricultural equipment.\n\nBelow is a listing of known academic programs that offer bachelor's degrees (B.S. or B.S.E. or B.E / B.Tech) in what ABET terms \"Agricultural Engineering\", \"Biosystems Engineering\", \"Biological Engineering\", or similarly named programs. ABET accredits college and university programs in the disciplines of applied science, computing, engineering, and engineering technology.\n\n\"North America\"\n\"Mexico, Central and South America\"\n\n\n"}
{"id": "7973645", "url": "https://en.wikipedia.org/wiki?curid=7973645", "title": "Al-Haddar", "text": "Al-Haddar\n\nAl-Haddar is an impact crater on Saturn's moon Enceladus, first discovered by the Voyager spacecraft. It is named after Al-Haddar, one of the barber's six brothers in \"The Hunchback's Tale\" from \"The Book of One Thousand and One Nights\".\n\nAl-Haddar is located at 50.5° North Latitude, 200.6° West Longitude and is approximately 14 kilometers across. It is the northernmost and smallest crater of a prominent crater triplet on Enceladus' anti-Saturnian hemisphere (there is no evidence that the impacts are related or were formed from break-up of a single body, like Shoemaker-Levy 9). \"Voyager 2\" observations of Al-Haddar revealed a bowl-shaped, simple crater, compared to its larger, more complex southern neighbors, Shahrazad and Dunyazad craters. The crater was mostly in shadow during the \"Cassini\" Spacecraft's close flyby on March 9, 2005, but numerous tectonic fractures were observed along its northern rim.\n"}
{"id": "51030078", "url": "https://en.wikipedia.org/wiki?curid=51030078", "title": "Aída Mencía Ripley", "text": "Aída Mencía Ripley\n\nAída Teresa Mencía Ripley is a scientist and clinical psychologist from the Dominican Republic. She is UNESCO Chair on Social and Academic Inclusion for People with Disabilities and Special Education Needs and Dean of Academic Research at the Iberoamerican University of the Dominican Republic (UNIBE).\n\nShe was born in Santo Domingo and raised in New York. She is the daughter of Aída Ripley Gómez (the daughter of Dominican hall of fame sportsman Enrique Ripley Marín and Aída Gómez Rueda ) and Rafael Mencía Ochoa. She is the niece of artist Geo Ripley. Her paternal grandfather was Dominican diplomat Rafael Mencía Lister.\n\nMencía-Ripley graduated \"Summa Cum Laude\" from St. John's University with a B.A. in psychology and earned an M.A. from the Graduate Faculty of the New School for Social Research. She later earned an M.A. and a Ph.D. from St. John´s University. She is a member of the American Psychological Association, WPATH, The Interamerican Psychology Society, and the Young Scientist Chapter of the Dominican Academy of Sciences. She is also a member of the \"Comité Dominicano Barón Pierre de Coubertin\".\n\nHer main areas of research are gender and health, clinical psychology, psychometrics, and psychology of language. She is an advocate for inclusive education in the Dominican Republic and deals with issues related to ethical and culturally respectful research in developing countries that pay special attention to intersectional issues of gender, ethnicity, and disability . As Dean of Research at UNIBE, she has steered initiatives that led to the creation of several research laboratories and international agreements to foster scientific research in the Dominican Republic . She is the 2017 winner of the José Salazar Award for best research paper in the Revista Interamericana de Psicología \n\nSome publications of Mencía Ripley are:\n\n"}
{"id": "28928580", "url": "https://en.wikipedia.org/wiki?curid=28928580", "title": "Beaglehole Glacier", "text": "Beaglehole Glacier\n\nBeaglehole Glacier () is a glacier between Spur Point and Friederichsen Glacier on the east coast of Graham Land. It was named by the UK Antarctic Place-Names Committee after John Cawte Beaglehole, New Zealand historian of the Antarctic and biographer of Captain James Cook.\n\n"}
{"id": "1589303", "url": "https://en.wikipedia.org/wiki?curid=1589303", "title": "Behavioural sciences", "text": "Behavioural sciences\n\nBehavioural sciences explore the cognitive processes within organisms and the behavioural interactions between organisms in the natural world. It involves the systematic analysis and investigation of human and animal behavior through the study of the past, controlled and naturalistic observation of the present, and disciplined scientific experimentation and modeling. It attempts to accomplish legitimate, objective conclusions through rigorous formulations and observation. Examples of behavioral sciences include psychology, psychobiology, anthropology, and cognitive science. Generally, behavior science deals primarily with human action and often seeks to generalize about human behavior as it relates to society. \n\nBehavioural sciences includes two broad categories: neural — \"Information sciences\" and social — \"Relational sciences\".\n\nInformation processing sciences deals with information processing of stimuli from the social environment by cognitive entities in order to engage in decision making, social judgment and social perception for individual functioning and survival of organism in a social environment. These include psychology, cognitive science, psychobiology, neural networks, social cognition, social psychology, semantic networks, ethology, and social neuroscience.\n\nOn the other hand, Relational sciences deals with relationships, interaction, communication networks, associations and relational strategies or dynamics between organisms or cognitive entities in a social system. These include fields like sociological social psychology, social networks, dynamic network analysis, agent-based model and microsimulation.\n\nInsights from several pure disciplines across behavioural sciences are explored by various applied disciplines and practiced in the context of everyday life and business. These applied disciplines of behavioural science include: organizational behavior, operations research, consumer behaviour and media psychology.\n\nThe term \"behavioural sciences\" is often confused with the term \"social sciences\". Though these two broad areas are interrelated and study systematic processes of behaviour, they differ on their level of scientific analysis of various dimensions of behaviour.\n\nBehavioural sciences abstract empirical data to investigate the decision processes and communication strategies within and between organisms in a social system. This involves fields like psychology, social neuroscience ethology, and cognitive science. In contrast, social sciences provide a perceptive framework to study the processes of a social system through impacts of social organization on structural adjustment of the individual and of groups. They typically include fields like sociology, economics, public health, anthropology, demography and political science.\n\nMany subfields of these disciplines cross the boundaries between behavioral and social sciences. For example, political psychology and behavioral economics use behavioural approaches, despite the predominant focus on systemic and institutional factors in the broader fields of political science and economics.\n\n\n"}
{"id": "52148685", "url": "https://en.wikipedia.org/wiki?curid=52148685", "title": "Boudewijn Karel Boom", "text": "Boudewijn Karel Boom\n\nBoudewijn Karel Boom (1903–1980) was a Dutch botanist and author.\n"}
{"id": "27545006", "url": "https://en.wikipedia.org/wiki?curid=27545006", "title": "Brøgger Glacier", "text": "Brøgger Glacier\n\nBrøgger Glacier () is a glacier long, flowing west into the southern part of Undine South Harbour on the south coast of South Georgia. The name appears on a chart by Professor Olaf Holtedahl, Norwegian geologist who investigated South Georgia in 1928, and is probably for Professor Waldemar Brøgger, a Norwegian geologist and mineralogist, and member of the Norwegian Parliament, 1900–09.\n\n"}
{"id": "16617061", "url": "https://en.wikipedia.org/wiki?curid=16617061", "title": "CETpD", "text": "CETpD\n\nThe Technical Research Centre for Dependency Care and Autonomous Living (CETpD) is an applied research and technology transfer centre created for the Universitat Politèncica de Catalunya and the \"Fundació Hospital Comarcal Sant Antoni Abat\" on behalf of the \"Consorci de Servei a les Persones\" de Vilanova i la Geltrú, with the aim of covering the demand for research and development in the field of Gerontechnology, Ambient Intelligence, Assistive Robotics and User Experience Technologies.\n\nThe CETpD carried out important applied research work and innovation on socially relevant developments, especially on technologies and systems designed to enhance independent living of elderly and disabled people.\n\nAmbient Intelligence (AmI) implies an environment of distributed computing systems, unobstrusive and often invisible, with user friendly interfaces capable of learning and adapting to the particular needs of the users.\nCETpD AmI projects are included in the assistance field mainly for housing and health application. We are looking for technological solutions that have the potential to include everyone, that are affordable and build trust and confidence directed to improve the quality of life of any person specially elders and disabled.\n\nThe priority lines of work are three:\n\nThe purpose of the user experience field, usability, is to provide relevant information about users expectations, capabilities and preferences, in order to ensure that products and services would meet end users -and other stakeholders- functional and affective needs. All the CETpD projects incorporates user's experience feedback in the process of contextual product and services development.\n\nGait alterations, balance alterations and falls are major causes of disability in the elderly population. Such pathologies seriously impair elders' quality of life; furthermore, their consequences have high economic and social impact. Therefore, clinical and epedimiological studies on these pathologies are conducted at the CETpD in order to gain insight into their causes and to find novel therapeutics.\n\nA major objective in geriatrics is to maintain elderly persons' independence to carry out their daily life activities and to interact with their environment. In this regard, technology may be an essential tool to facilitate elderly persons' autonomy. At the CETpD, they develop technology aimed to their environment, as well as remote diagnosis devices designed for early detection of health alterations in frail persons or persons living alone.\n\n"}
{"id": "1506230", "url": "https://en.wikipedia.org/wiki?curid=1506230", "title": "Carolyn Porco", "text": "Carolyn Porco\n\nCarolyn C. Porco (born March 6, 1953) is an American planetary scientist who explores the outer solar system, beginning with her imaging work on the Voyager missions to Jupiter, Saturn, Uranus and Neptune in the 1980s. She led the imaging science team on the \"Cassini\" mission in orbit around Saturn and September 15, 2017 when Cassini was de-orbited to burn up in Saturn's upper atmosphere. She is an expert on planetary rings and the Saturnian moon, Enceladus.\n\nShe has co-authored more than 110 scientific papers on subjects ranging from the spectroscopy of Uranus and Neptune, the interstellar medium, the photometry of planetary rings, satellite/ring interactions, computer simulations of planetary rings, the thermal balance of Triton's polar caps, heat flow in the interior of Jupiter, and a suite of results on the atmosphere, satellites, and rings of Saturn from the \"Cassini\" imaging experiment. In 2013, Cassini data confirmed a 1993 prediction by Porco and Mark Marley that acoustic oscillations within the body of Saturn are responsible for creating particular features in the rings of Saturn.\n\nPorco was founder of The Day the Earth Smiled. She was also responsible for the epitaph and proposal to honor the late renowned planetary geologist Eugene Shoemaker by sending his cremains to the Moon aboard the Lunar Prospector spacecraft in 1998.\n\nA frequent public speaker, Porco has given two popular lectures at TED as well as the opening speech for Pangea Day, a May 2008 global broadcast coordinated from six cities around the world, in which she described the cosmic context for human existence. Porco has also won a number of awards and honors for her contributions to science and the public sphere; for instance, in 2009, \"New Statesman\" named her as one of 'The 50 People Who Matter Today.' In 2010 she was awarded the Carl Sagan Medal, presented by the American Astronomical Society for Excellence in the Communication of Science to the Public. And in 2012, she was named one of the 25 most influential people in space by \"Time\" magazine.\n\nPorco was born in New York City. She graduated in 1970 from Cardinal Spellman High School in the Bronx, in New York City. She earned a BS degree from the State University of New York at Stony Brook in 1974. She received her Ph.D in 1983 from the California Institute of Technology in the Division of Geological and Planetary Sciences. Supervised by dynamicist Peter Goldreich, she wrote her doctoral dissertation focused on Voyager discoveries in the rings of Saturn.\n\nIn the fall of 1983, Dr. Porco joined the faculty of the Department of Planetary Sciences at the University of Arizona; the same year she was made a member of the Voyager Imaging Team. In the latter capacity, she was an active participant in the \"Voyager 2\" encounters with Uranus in 1986 and Neptune in 1989, leading the Rings Working Group within the Voyager Imaging Team during the Neptune encounter.\n\nAs a young Voyager scientist, she was the first person to describe the behavior of the eccentric ringlets and the \"spokes\" discovered by Voyager within the rings of Saturn; to elucidate the mechanism by which the outer Uranian rings were being shepherded by the Voyager-discovered moons Cordelia and Ophelia; and to provide an explanation for the shepherding of the rings arcs of Neptune by the moon Galatea, also discovered by Voyager. She was a co-originator of the idea to take a 'portrait of the planets' with the \"Voyager 1\" spacecraft, and participated in the planning, design, and execution of those images in 1990, including the famous \"Pale Blue Dot\" image of Earth.\n\nIn November 1990, Porco was selected as the leader of the Imaging Team for the \"Cassini-Huygens\" mission, an international mission that successfully placed a spacecraft in orbit around Saturn and deployed the atmospheric \"Huygens\" probe to Saturn's largest satellite, Titan. She is also the Director of the Cassini Imaging Central Laboratory for Operations (CICLOPS), which was the center of uplink and downlink operations for the \"Cassini\" imaging science experiment and the place where \"Cassini\" images are processed for release to the public. CICLOPS is part of the Space Science Institute in Boulder, Colorado.\n\nIn the course of the ongoing mission, Porco and her team have discovered seven moons of Saturn: Methone and Pallene, Polydeuces, Daphnis, Anthe, Aegaeon, and a small moonlet in the outer B ring. They also found several new rings, such as rings coincident with the orbits of Atlas, Janus and Epimetheus (the Saturnian 'co-orbitals') and Pallene; a diffuse ring between Atlas and the F ring; and new rings within several of the gaps in Saturn's rings.\n\nIn 2013, \"Cassini\" data confirmed a 1993 prediction by Porco and Mark Marley that acoustic oscillations within the body of Saturn are responsible for creating particular features in the rings of Saturn. This confirmation, the first to demonstrate that planetary rings can act like a seismograph in recording oscillatory motions within the host planet, should provide new constraints on the interior structure of Saturn. Such oscillations are known to exist in the sun as well as other stars.\n\nPorco's team was responsible for the first sighting of a hydrocarbon lake, as well as a lake district, in the south polar region of Titan in June 2005. (A group of similar – and larger – features were sighted in the north polar region in February 2007.) The possibility that these sea-sized features are either completely or partially filled with liquid hydrocarbons is significantly strengthened by subsequent observations by other \"Cassini\" instruments.\n\nHer team was also responsible for the first sighting of plumes erupting from Enceladus, Saturn's sixth largest moon. They first suggested, and provided detailed scientific arguments, that these jets might be geysers erupting from reservoirs of near-surface liquid water under the south pole of the small moon.\n\nPorco was a member of the imaging team for the \"New Horizons\" mission to Pluto and the Kuiper Belt through 2014. The probe made its Pluto flyby in 2015.\n\nAs the \"Cassini\" imaging team lead, Porco initiated and planned the capture of a picture of Saturn with the Earth in the distance on July 19, 2013, an image along the lines of the famous \"Pale Blue Dot\" photo. The taking of the image was part of a larger concept entitled The Day The Earth Smiled, in which people the world over were invited to celebrate humanity's place in the cosmos and life on Earth by smiling the moment the picture was taken.\n\nPorco served in the faculty of the University of Arizona from 1983 to 2001, achieving tenured professorship in 1991. She taught both graduates and undergraduates and was one of five finalists for the University of Arizona Honors Center \"Five Star Faculty Award\", a campus-wide student-nominated, student-judged award for outstanding undergraduate teaching.\n\nPorco is a Senior Research Scientist at the Space Science Institute in Boulder, Colorado, and she is an Adjunct Professor at the University of Colorado at Boulder.\n\nPorco has been an active participant in guiding the American planetary exploration program through membership on many important NASA advisory committees, including the Solar System Exploration Subcommittee, the Mars Observer Recovery Study Team, and the Solar System Road Map Development Team. In the mid-1990s, she served as the chairperson for a small NASA advisory working group to study and develop future outer solar system missions and she served as the Vice Chairperson of the Steering Group for the first Solar System Decadal Survey, sponsored by NASA and the National Academy of Sciences.\nPorco speaks frequently on the \"Cassini\" mission and planetary exploration in general, and has appeared at renowned conferences such as PopTech 2005 and TED (2007, 2009). She attended and was a speaker at the symposium on November 2006.\n\nPorco's 2007 TED talk, \"The Human Journey,\" detailed two major areas of discovery made by the \"Cassini\" mission: the exploration of the Saturnian moons Titan and Enceladus. In her introductory remarks, Porco explained:\n\nIn describing the environment of Titan, with its molecular nitrogen atmosphere suffused with organic compounds, Porco invited her audience to imagine the scene on the moon's surface:\n\nAfter describing various features discovered on Titan by \"Cassini\", and presenting the historic first photograph of Titan's surface by the \"Huygens\" lander, Porco went on to describe Enceladus and the jets of \"fine icy particles\" which erupt from the moon's southern pole:\n\nPorco's 2009 TED Talk was \"Could a Saturn moon harbor life?\".\n\nShe was a speaker at the 2016 Reason Rally.\n\nPorco has been a regular CNN guest analyst and consultant on astronomy, has made many radio and television appearances explaining science to the lay audience, including appearances on the MacNeil/Lehrer \"Newshour\", CBS's \"60 Minutes\", Peter Jennings's \"The Century\", and TV documentaries on planetary exploration such as \"The Planets\" on the Discovery Channel and the BBC, \"A Traveler's Guide to the Planets\" on the National Geographic Channel, \"Horizon\" on the BBC, and a Nova Cassini special on PBS. For the 2003 A&E special on the Voyager mission entitled \"Cosmic Journey: The Voyager Interstellar Mission and Message\", Porco appeared onscreen and also served as the show's science advisor and animation director.\n\nPorco served as an adviser for the film \"Contact\" (1997), which was based on a novel by the well-known astronomer Carl Sagan. The actress Jodie Foster portrayed the heroine in the movie, and Sagan reportedly suggested that she use Porco as a real-life model to guide her performance.\n\nPorco was also an adviser on the film \"Star Trek\" (2009). The scene in which the \"Enterprise\" comes out of warp drive into the atmosphere of Titan, and rises submarine-style out of the haze, with Saturn and the rings in the background, was Porco's suggestion.\n\nPorco was a guest on the BBC's \"Stargazing Live\" Series 4 in January 2014\n\nPorco has given numerous interviews in print media on subjects ranging from planetary exploration to the conflict between science and religion (for example, \"Newsweek\" and the journal \"The Humanist\").\n\nShe has been profiled many times in print, beginning in the \"Boston Globe\" (October 1989), \"The New York Times\" (August 1999, September 2009), the \"Tucson Citizen\" (2001), \"Newsday\" (June 2004), for the Royal Astronomical Society of Canada (2006), in \"Astronomy Now\" (2006), in \"Discover Magazine\" (2007), and also online on CNN.com (2005) and Edge.org.\n\nPrior to \"Cassini\"s launch, she was a strong and visible defendant of the usage of radioactive materials on the \"Cassini\" spacecraft. She is a supporter of a plan for human spaceflight toward the Moon and Mars, and in an op-ed piece published in \"The New York Times\", she highlighted the benefits of a deep-space-capable heavy launch vehicle for the robotic exploration of the solar system. Porco has advocated for prioritizing the exploration of Enceladus over Europa.\n\nIn 1994, Porco was a member of a committee (chaired by Carl Sagan) entitled \"Public Communication of NASA's Science\", and in 1999, she reviewed a biography of Sagan for \"The Guardian\". Her popular science articles have been published in \"The Sunday Times\", \"Astronomy\", the \"Arizona Daily Star\", \"Sky & Telescope\", \"American Scientist\", and \"Scientific American\". She is active in the presentation of science to the public as the leader of the Cassini Imaging Team, as the creator/editor of the website where \"Cassini\" images are posted. She writes the site's homepage \"Captain's Log\" greeting to the public. She is an atheist.\n\nShe is also the CEO of Diamond Sky Productions, a small company devoted to the scientific, as well as artful, use of planetary images and computer graphics for the presentation of science to the public.\n\nIn 1999, Porco was selected by \"The Sunday Times\" (London) as one of 18 scientific leaders of the 21st century, and by \"Industry Week\" as one of 50 Stars to Watch. In 2008 she was chosen to be on \"Wired\" magazine's inaugural 'Smart List: 15 People the Next President Should Listen To.'\n\nHer contributions to the exploration of the outer solar system were recognized with the naming of Asteroid (7231) Porco which is \"Named in honor of Carolyn C. Porco, a pioneer in the study of planetary ring systems...and a leader in spacecraft exploration of the outer solar system.\"\n\nIn 2008, Porco was awarded the Isaac Asimov Science Award by the American Humanist Association.\n\nIn September 2009, Porco was awarded The Huntington Library's Science Writer Fellowship for 2010. That same month, \"New Statesman\" named her as one of 'The 50 People Who Matter Today.'\n\nIn October 2009, she and Babak Amin Tafreshi were each awarded the 2009 Lennart Nilsson Award in recognition of their photographic work. The award panel's citation for Dr. Porco reads as follows:\n\nIn October 2010, Porco was awarded the 2010 Carl Sagan Medal for Excellence in the Communication of Science to the Public, presented by the American Astronomical Society's Division for Planetary Sciences.\n\nIn May 2009, Porco received an Honorary Doctorate of Science from the State University of New York at Stony Brook, of which she is an alumna. And in 2011 she won the Distinguished Alumni Award from the California Institute of Technology, the highest honor regularly bestowed by Caltech.\n\nIn 2012, Porco was named one of the 25 most influential people in space by \"Time\" magazine.\n\nPorco is fascinated by the 1960s and The Beatles and has, at times, incorporated references to The Beatles and their music into her presentations, writings, and press releases. The first color image released by \"Cassini\" to the public was an image of Jupiter, taken during \"Cassini\"s approach to the giant planet and released on October 9, 2000 to honor John Lennon’s 60th birthday. In 2006, she produced and directed a brief 8-minute movie of 64 of Cassini’s most spectacular images, put to the music of the Beatles, in honor of Paul McCartney’s 64th birthday. And in 2007, she produced a poster showing 64 scenes from Saturn.\n\nPorco is also interested in dance and fascinated with Michael Jackson. In August 2010, she won a Michael Jackson costume/dance contest held in Boulder, Colorado.\n\nQuotes of Porco's were used in the production of \"The Poetry of Reality (An Anthem for Science)\", \"A Wave of Reason\", \"Children of Africa (The Story of Us)\", and \"Onward to the Edge!\" by Symphony of Science.\n\n"}
{"id": "694424", "url": "https://en.wikipedia.org/wiki?curid=694424", "title": "Chiromyiformes", "text": "Chiromyiformes\n\nChiromyiformes is an order of strepsirrhine primates that includes the extant aye-aye from Madagascar.\n\nThe aye-aye is sometimes classified as a member of Lemuriformes, but others treat Chiromyiformes as a separate infraorder, based on the their very reduced dental formula. Gunnell et al. (2018) reclassified the putative bat \"Propotto\" as a close relative of the aye-aye, as well as assigning the problematic strepsirrhine primate \"Plesiopithecus\" to Chiromyiformes.\n\nThe molecular clock puts the divergence of Chiromyiformes and Lemuriformes at 50-49 million years ago.\n\n"}
{"id": "2578582", "url": "https://en.wikipedia.org/wiki?curid=2578582", "title": "Conserved sequence", "text": "Conserved sequence\n\nIn evolutionary biology, conserved sequences are identical or similar sequences in nucleic acids (DNA and RNA) or proteins across species (orthologous sequences), or within a genome (paralogous sequences), or between donor and receptor taxa (xenologous sequences). Conservation indicates that a sequence has been maintained by natural selection.\n\nA highly conserved sequence is one that has remained relatively unchanged far back up the phylogenetic tree, and hence far back in geological time. Examples of highly conserved sequences include the RNA components of ribosomes present in all domains of life, the homeobox sequences widespread amongst Eukaryotes, and the tmRNA in Bacteria. The study of sequence conservation overlaps with the fields of genomics, proteomics, evolutionary biology, phylogenetics, bioinformatics and mathematics.\n\nThe discovery of the role of DNA in inheritance, and observations by Frederick Sanger of variation between animal insulins in 1949, prompted early molecular biologists to study taxonomy from a molecular perspective. Studies in the 1960s used DNA hybridization and protein cross-reactivity techniques to measure similarity between known orthologous proteins, such as hemoglobin and Cytochrome C. In 1965, Émile Zuckerkandl and Linus Pauling introduced the concept of the molecular clock, proposing that steady rates of mutation could be used to estimate the time since two organisms diverged. While initial phylogenies closely matched the fossil record, observations that some genes appeared to evolve at different rates led to the development of theories of molecular evolution. Margaret Dayhoff's 1966 comparison of ferrodoxin sequences showed that natural selection would act to conserve and optimise protein sequences essential to life.\n\nOver many generations, nucleic acid sequences in the genome of an evolutionary lineage can gradually change over time due to random mutations and deletions. Sequences may also recombine or be deleted due to chromosomal rearrangements. Conserved sequences are sequences which persist in the genome despite such forces, and have slower rates of mutation than the background mutation rate.\n\nConservation can occur in coding and non-coding nucleic acid sequences. Highly conserved DNA sequences are thought to have functional value, although the role for many highly conserved non-coding DNA sequences is poorly understood. The extent to which a sequence is conserved can be affected by varying selection pressures, its robustness to mutation, population size and genetic drift. Many functional sequences are also modular, containing regions which may be subject to independent selection pressures, such as protein domains.\n\nIn coding sequences, the nucleic acid and amino acid sequence may be conserved to different extents, as the degeneracy of the genetic code means that synonymous mutations in a coding sequence do not affect the amino acid sequence of its protein product.\n\nAmino acid sequences can be conserved to maintain the structure or function of a protein or domain. Conserved proteins undergo fewer amino acid replacements, or are more likely to substitute amino acids with similar biochemical properties. Within a sequence, amino acids that are important for folding, structural stability, or that form a binding site may be more highly conserved.\n\nThe nucleic acid sequence of a protein coding gene may also be conserved by other selective pressures. The codon usage bias in some organisms may restrict the types of synonymous mutations in a sequence. Nucleic acid sequences that cause secondary structure in the mRNA of a coding gene may be selected against, as some structures may negatively affect translation, or conserved where the mRNA also acts as a functional non-coding RNA.\n\nNon-coding sequences important for gene regulation, such as the binding or recognition sites of ribosomes and transcription factors, may be conserved within a genome. For example, the promoter of a conserved gene or operon may also be conserved. As with proteins, nucleic acids that are important for the structure and function of non-coding RNA (ncRNA) can also be conserved. However, sequence conservation in ncRNAs is generally poor compared to protein-coding sequences, and base pairs that contribute to structure or function are often conserved instead.\n\nConserved sequences are typically identified by bioinformatics approaches based on sequence alignment. Advances in high-throughput DNA sequencing and protein mass spectrometry has substantially increased the availability of protein sequences and whole genomes for comparison since the early 2000s.\n\nConserved sequences may be identified by homology search, using tools such as BLAST, HMMER and Infernal. Homology search tools may take an individual nucleic acid or protein sequence as input, or use statistical models generated from multiple sequence alignments of known related sequences. Statistical models such as profile-HMMs, and RNA covariance models which also incorporate structural information, can be helpful when searching for more distantly related sequences. Input sequences are then aligned against a database of sequences from related individuals or other species. The resulting alignments are then scored based on the number of matching amino acids or bases, and the number of gaps or deletions generated by the alignment. Acceptable conservative substitutions may be identified using substitution matrices such as PAM and BLOSUM. Highly scoring alignments are assumed to be from homologous sequences. The conservation of a sequence may then be inferred by detection of highly similar homologs over a broad phylogenetic range.\n\nMultiple sequence alignments can be used to visualise conserved sequences. The CLUSTAL format includes a plain-text key to annotate conserved columns of the alignment, denoting conserved sequence (*), conservative mutations (:), semi-conservative mutations (.), and non-conservative mutations ( ) Sequence logos can also show conserved sequence by representing the proportions of characters at each point in the alignment by height.\n\nWhole genome alignments (WGAs) may also be used to identify highly conserved regions across species. Currently the accuracy and scalability of WGA tools remains limited due to the computational complexity of dealing with rearrangements, repeat regions and the large size of many eukaryotic genomes. However, WGAs of 30 or more closely related bacteria (prokaryotes) are now increasingly feasible.\n\nOther approaches use measurements of conservation based on statistical tests that attempt to identify sequences which mutate differently to an expected background (neutral) mutation rate.\n\nThe GERP (Genomic Evolutionary Rate Profiling) framework scores conservation of genetic sequences across species. This approach estimates the rate of neutral mutation in a set of species from a multiple sequence alignment, and then identifies regions of the sequence that exhibit fewer mutations than expected. These regions are then assigned scores based on the difference between the observed mutation rate and expected background mutation rate. A high GERP score then indicates a highly conserved sequence.\n\nOther approaches such as PhyloP and PhyloHMM incorporate statistical phylogenetics methods to compare probability distributions of substitution rates, which allows the detection of both conservation and accelerated mutation. First, a background probability distribution is generated of the number of substitutions expected to occur for a column in a multiple sequence alignment, based on a phylogenetic tree. The estimated evolutionary relationships between the species of interest are used to calculate the significance of any substitutions (i.e. a substitution between two closely related species may be less likely to occur than distantly related ones, and therefore more significant). To detect conservation, a probability distribution is calculated for a subset of the multiple sequence alignment, and compared to the background distribution using a statistical test such as a likelihood-ratio test or score test. P-values generated from comparing the two distributions are then used to identify conserved regions. PhyloHMM uses hidden markov models to generate probability distributions. The PhyloP software package compares probability distributions using a likelihood-ratio test or score test, as well as using a GERP-like scoring system.\n\nUltra-conserved elements or UCEs are sequences that are highly similar or identical across multiple taxonomic groupings. These were first discovered in vertebrates, and have subsequently been identified within widely-differing taxa. While the origin and function of UCEs are poorly understood, they have been used to investigate deep-time divergences in amniotes, insects, and between animals and plants.\n\nThe most highly conserved genes are those that can be found in all organisms. These consist mainly of the ncRNAs and proteins required for transcription and translation, which are assumed to have been conserved from the last universal common ancestor of all life.\n\nGenes or gene families that have been found to be universally conserved include GTP-binding elongation factors, Methionine aminopeptidase 2, Serine hydroxymethyltransferase, and ATP transporters. Components of the transcription machinery, such as RNA polymerase and helicases, and of the translation machinery, such as ribosomal RNAs, tRNAs and ribosomal proteins are also universally conserved.\n\nSets of conserved sequences are often used for generating phylogenetic trees, as it can be assumed that organisms with similar sequences are closely related. The choice of sequences may vary depending on the taxonomic scope of the study. For example, the most highly conserved genes such as the 16S RNA and other ribosomal sequences are useful for reconstructing deep phylogenetic relationships and identifying bacterial phyla in metagenomics studies. Sequences that are conserved within a clade but undergo some mutations, such as housekeeping genes, can be used to study species relationships. The internal transcribed spacer (ITS) region, which is required for spacing conserved rRNA genes but undergoes rapid evolution, is commonly used to classify fungi and strains of rapidly evolving bacteria.\n\nAs highly conserved sequences often have important biological functions, they can be useful a starting point for identifying the cause of genetic diseases. Many congenital metabolic disorders and Lysosomal storage diseases are the result of changes to individual conserved genes, resulting in missing or faulty enzymes that are the underlying cause of the symptoms of the disease. Genetic diseases may be predicted by identifying sequences that are conserved between humans and lab organisms such as mice or fruit flies, and studying the effects of knock-outs of these genes. Genome-wide association studies can also be used to identify variation in conserved sequences associated with disease or health outcomes.\n\nIdentifying conserved sequences can be used to discover and predict functional sequences such as genes. Conserved sequences with a known function, such as protein domains, can also be used to predict the function of a sequence. Databases of conserved protein domains such as Pfam and the Conserved Domain Database can be used to annotate functional domains in predicted protein coding genes.\n\n"}
{"id": "38169567", "url": "https://en.wikipedia.org/wiki?curid=38169567", "title": "Direct Benefit Transfer", "text": "Direct Benefit Transfer\n\nDirect Benefit Transfer or DBT is an attempt to change the mechanism of transferring subsidies launched by Government of India on 1 January 2013. This program aims to transfer subsidies directly to the people through their bank accounts. It is hoped that crediting subsidies into bank accounts will reduce leakages, delays, etc.\n\nThe primary aim of this Direct Benefit Transfer program is to bring transparency and terminate pilferage from distribution of funds sponsored by Central Government of India. In DBT, benefit or subsidy will be directly transferred to citizens living below poverty line. Central Plan Scheme Monitoring System (CPSMS), being implemented by the Office of Controller General of Accounts, will act as the common platform for routing DBT. CPSMS can be used for the preparation of beneficiary list, digitally signing the same and processing of payments in the bank accounts of the beneficiary using the Aadhaar Payment Bridge of NPCI. All relevant orders related with the DBT are available on the CPSMS website.\n\nThe program was launched in selected cities of India on 1 January 2013. It was launched in 20 districts, covering scholarships and social security pensions initially.\n\nFormer Union Minister for Rural Development of India Jairam Ramesh and former Chief Minister of Andhra Pradesh N. Kiran Kumar Reddy inaugurated the scheme at Gollaprolu in East Godavari district on 6 January 2013. The government has decided to review the progress on regular basis.\n\nThe first review was scheduled to be undertaken on 15 January 2013. According to P. Chidambaram, former Union Minister of Finance of India, the scheme will be rolled out across 11 more districts by 1 February and 12 more districts by 1 March 2013.\n\nIn April 2013 the government decided to extend the DBT scheme in 78 more districts of the country from July 1, 2013. The decision was taken by then Prime Minister Dr. Manmohan Singh after a review meeting. The 78 new districts will include 6 districts each from Uttar Pradesh and Himachal Pradesh, 3 each from Bihar and Tamil Nadu, 2 from West Bengal and 4 each from Odisha and Gujarat.\n\nIn a review by the Prime Minister's Office on 5 August 2013, the minutes reported that two schemes dominated transfers through CPSMS - 83% of all transfers were for the Janani Suraksha Yojana and scholarships. Lack of computerized records for schemes to be linked to DBT was hindering rollout. The minutes show that out of 39.76 lakh beneficiaries who ought to have been covered under various schemes, only 56% had bank accounts, 25.3% had both bank accounts and aadhaar numbers, but only 9.62% have bank accounts seeded with aadhaar numbers.\n\nOn June 1, 2013, the minister of Petroleum & Natural Gas, M Veerappa Moily formally launched the scheme direct benefit transfer for LPG (DBTL) Scheme in 20 high Aadhaar coverage districts. The subsidy on LPG cylinders will be credited directly to consumers' Aadhaar-linked bank accounts. All Aadhaar-linked domestic LPG consumers will get an advance in their bank account as soon as they book the first subsidized cylinder before delivery. On receiving the first subsidized cylinder subsidy for next will again get credited in their bank account, which can then be available for the purchase of the next cylinder at market rate until the cap of 12 cylinders per year is reached.\n\nModified Version of DBTL Scheme : (November 2014)\nGovernment of India Introduced Modified Direct Benefit Transfer of LPG (DBTL) scheme in 54 districts in 11 states including all in Kerala starting November 15, 2014 whereby LPG consumers who have not yet availed the benefit will be able to get cash subsidy amount transferred into their accounts to buy Liquefied Petroleum Gas (LPG) cylinders at market price.\n\n74 Schemes of 17 ministries of central government were under DBT by 31 May 2016. As on Dec 2017,DBT has been implemented in 400 schemes from 46 ministries.\n\n\n"}
{"id": "1755501", "url": "https://en.wikipedia.org/wiki?curid=1755501", "title": "Double terminated crystal", "text": "Double terminated crystal\n\nA double terminated crystal is a crystal with two naturally faceted ends. Technically, they have a termination on both ends. It is a rarer form of crystal as it forms free-floating in pockets of clay, rather than on one side of a stone. They are sought after by collectors and are often made into pendants. \n\nAccording to \"Gems & Gemology\", \"the topmost grade of amethyst consists of double-terminated single crystals.\" \n"}
{"id": "6772817", "url": "https://en.wikipedia.org/wiki?curid=6772817", "title": "Dynamic scattering mode", "text": "Dynamic scattering mode\n\nGeorge Heilmeier proposed the dynamic scattering effect which causes a strong scattering of light when the electric field applied to a special liquid crystal mixture exceeds a threshold value.\n\nA DSM cell requires the following ingredients:\n\nWith no voltage applied the LC-cell with the homeotropically aligned LC is clear and transparent. With increasing voltage and current, the electric field is trying to align the long molecular axis of the LC perpendicular to the field while the ion transport through the layer has the tendency to align the LC perpendicular to the substrate plates. As a result, a striped repetitive pattern is generated in the cell, of which the building blocks are named \"Williams domains\". Upon further increase of the voltage this regular pattern is replaced by a turbulent state which is strongly scattering light. This effect belongs to the class of electro-hydrodynamic effects in LCs. Electro-optic displays can be realized with that effect in the transmissive and reflective mode of operation. The driving voltages required for light scattering are in the range of several ten volts and the non-trivial current is depending on the area of the activated segments. The DMS effect was thus not suited for battery powered electronic devices.\n\n"}
{"id": "22374513", "url": "https://en.wikipedia.org/wiki?curid=22374513", "title": "East Midlands Councils", "text": "East Midlands Councils\n\nEast Midlands Councils is a consultative forum for local government in the East Midlands region of England. It is a regional grouping of the Local Government Association and the regional employers organisation.\n\nIt was established in April 2010 as a consequence of the dissolution of the former East Midlands Regional Assembly. East Midlands Councils consists of 98 Members; 92 Local Authority Members, two Fire and Rescue Authority Members, two Police Authority Members and two representatives of Parish and Town Councils. Councillor Martin Hill is the chair.\n\nThe Executive Board is the executive arm of East Midlands Councils and functions as the Local Authority Leaders' Board for the region. It was established to jointly produce the Single Regional Strategy for the East Midlands, with the East Midlands Development Agency. In May 2010 East Midlands Councils announced that it is winding down regional strategy functions by the end of June that year in line with planning changes required by the new UK Government.\n\nThe Board has 17 members with nine representatives from the county councils and unitary authorities in the region and a further five representatives form district councils, one from each county. The county and unitary authority members will be considered permanent members while the district members will serve one year terms, but are eligible for re-nomination. When fully established, the board will meet four times per year with provision to hold special meetings as necessary.\n\n"}
{"id": "16200313", "url": "https://en.wikipedia.org/wiki?curid=16200313", "title": "Effective number of parties", "text": "Effective number of parties\n\nThe effective number of parties is a concept introduced by Laakso and Taagepera (1979) which provides for an adjusted number of political parties in a country's party system. The idea behind this measure is to count parties and, at the same time, to weight the count by their relative strength. The relative strength refers to their vote share (\"effective number of electoral parties\") or seat share in the parliament (\"effective number of parliamentary parties\"). This measure is especially useful when comparing electoral systems across countries, as is done in the field of political science. The number of parties equals the effective number of parties only when all parties have equal strength. In any other case, the effective number of parties is lower than the actual number of parties. The effective number of parties is a frequent operationalization for the fragmentation of a party system.\n\nThere are two major alternatives to the effective number of parties-measure. John K. Wildgen's index of \"hyperfractionalization\" accords special weight to small parties. Juan Molinar's index gives special weight to the largest party. Dunleavy and Boucek provide a useful critique of the Molinar index.\n\nAccording to Laakso and Taagepera (1979) the \"effective number of parties\" is computed by the following formula:\n\nWhere n is the number of parties with at least one vote/seat and formula_1 the square of each party’s proportion of all votes or seats. The proportions need to be \"normalised\" such that, for example, 50 per cent is 0.5 and 1 per cent is 0.01. This is also the formula for the inverse Simpson index, or the true diversity of order 2.\n\nAn alternative formula proposed by Golosov (2010) is\n\nwhich is equivalent - if we only consider parties with at least one vote/seat - to\nHere, n is the number of parties, formula_1 the square of each party’s proportion of all votes or seats, and formula_3 is the square of the largest party’s proportion of all votes or seats.\n\nThe following table illustrates the difference between the values produced by the two formulas for eight hypothetical vote or seat constellations:\n"}
{"id": "9977829", "url": "https://en.wikipedia.org/wiki?curid=9977829", "title": "Engineer in Training", "text": "Engineer in Training\n\nEngineer in Training, or EIT, is a professional designation from the National Council of Examiners for Engineering and Surveying (NCEES) used in the United States to designate a person certified by the state as having completed two requirements:\n\nOnce an individual has passed the exam the state board awards that person an Engineer-in-Training (EIT) or an Engineer Intern (EI) designation. EIT and EI are equivalent variations in nomenclature that vary from state to state. Receiving an EIT designation is one step along the path toward Professional Engineer (PE) licensure.\n\n\"Engineer Intern\" term could be possibly misleading term as it may imply that the engineer is still in college and is working merely in an intern position.\n\nAn Engineer-in-Training does engineering work, such as design, under the supervision and direction of a Professional Engineer, who are exclusively able to perform certain tasks, such as stamp and seal designs and offer services to the public.\n\nIn Canada, an \"E.I.T.\" designation means that the person has completed the educational requirement of their P.Eng but has yet to accrue enough work experience (or has yet to apply for a P.Eng). An EIT can perform engineering work under the supervision of a professional engineer. With the exception of New Brunswick, PEO (Ontario) and OIQ (Quebec), EITs are allowed to use the title \"Engineer\" as long as they also include the EIT designation in either their name or their title. \n\nIn the United Kingdom and the United States of America, having an EIT designation shows an understanding of fundamental engineering principles, as EITs have passed the Fundamentals of Engineering (FE) exam. Canadian engineering has no equivalent to the FE exam for an EIT level.\n\nEach state's statutes delineate the requirements for the experience and education needed to become a PE once EIT or EI certification has been earned. The requirements vary depending on the State and the licensing board, but for most engineers the process typically includes the following steps:\nIn California, passing two additional exams is required to gain PE licensure. A seismic exam and a surveying exam, are in addition to the FE and PE exams for civil engineers.\n\n"}
{"id": "2056828", "url": "https://en.wikipedia.org/wiki?curid=2056828", "title": "Eugenius Johann Christoph Esper", "text": "Eugenius Johann Christoph Esper\n\nEugenius Johann Christoph Esper (2 June 1742 – 27 July 1810) was a German entomologist. Born in Wunsiedel in Bavaria, he was professor of zoology at Erlangen university.\n\nEugen and his brother Friedrich were introduced to natural history at an early age by their father Friedrich Lorenz Esper, an amateur botanist. Encouraged to abandon his theology course by his professor of botany Casimir Christoph Schmidel (1718–1792) Eugen Esper, instead, took instruction in natural history.\n\nHe obtained his doctorate of philosophy at the university of Erlangen in 1781 with a thesis entitled \"De varietatibus specierum in naturale productis\". The following year, he started to teach at the university initially as extraordinary professor, a poorly paid position, then in 1797 as the professor of philosophy. He directed the department of natural history in Erlangen from 1805. Thanks to him the university collections of minerals, birds, plants, shells and insects grew very rapidly.\n\nDuring his leisure hours Esper devoted himself to the study of nature and the preparation of manuscripts relating to natural history. He was the author of a series of booklets entitled \"Die Schmetterlinge in Abbildungen nach der Natur mit Beschreibungen\" which were published between 1776 and 1807. These were richly illustrated; minerals, birds, plants, shells and insects being presented on 438 hand-coloured plates. A second work was published in 1829-1830 with Toussaint de Charpentier (1779–1847). This is an important work on the butterflies of Germany, following the Linnean System. Esper was also the very first person to research palaeopathology.\n\nThe review of entomology, \"Esperiana, Buchreihe zur Entomologie\", created in 1990, commemorates his name and work.\n\nEsper's collection is in the Zoologische Staatssammlung München \n\nTranslated from French Wikipedia on Esper\n\n"}
{"id": "71289", "url": "https://en.wikipedia.org/wiki?curid=71289", "title": "Extended Backus–Naur form", "text": "Extended Backus–Naur form\n\nIn computer science, extended Backus-Naur form (EBNF) is a family of metasyntax notations, any of which can be used to express a context-free grammar. EBNF is used to make a formal description of a formal language which can be a computer programming language. They are extensions of the basic Backus–Naur form (BNF) metasyntax notation.\n\nThe earliest EBNF was originally developed by Niklaus Wirth incorporating some of the concepts (with a different syntax and notation) from Wirth syntax notation. However, many variants of EBNF are in use. The International Organization for Standardization has adopted an EBNF standard (ISO/IEC 14977). This article uses EBNF as specified by the ISO for examples applying to all EBNFs. Other EBNF variants use somewhat different syntactic conventions.\n\nEBNF is a code that expresses the grammar of a formal language. An EBNF consists of terminal symbols and non-terminal production rules which are the restrictions governing how terminal symbols can be combined into a legal sequence. Examples of terminal symbols include alphanumeric characters, punctuation marks, and whitespace characters.\n\nThe EBNF defines production rules where sequences of symbols are respectively assigned to a nonterminal:\ndigit excluding zero = \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\" | \"7\" | \"8\" | \"9\" ;\ndigit = \"0\" | digit excluding zero ;\nThis production rule defines the nonterminal \"digit\" which is on the left side of the assignment. The vertical bar represents an alternative and the terminal symbols are enclosed with quotation marks followed by a semicolon as terminating character. Hence a \"digit\" is a \"0\" or a \"digit excluding zero\" that can be \"1\" or \"2\" or \"3\" and so forth until \"9\".\n\nA production rule can also include a sequence of terminals or nonterminals, each separated by a comma:\ntwelve = \"1\", \"2\" ;\ntwo hundred one = \"2\", \"0\", \"1\" ;\nthree hundred twelve = \"3\", twelve ;\ntwelve thousand two hundred one = twelve, two hundred one ;\nExpressions that may be omitted or repeated can be represented through curly braces { ... }:\nnatural number = digit excluding zero, { digit } ;\nIn this case, the strings \"1\", \"2\", ...,\"10\"...,\"12345\"... are correct expressions. To represent this, everything that is set within the curly braces may be repeated arbitrarily often, including not at all.\n\nAn option can be represented through squared brackets [ ... ]. That is, everything that is set within the square brackets may be present just once, or not at all:\ninteger = \"0\" | [ \"-\" ], natural number ;\nTherefore, an integer is a zero (\"0\") or a natural number that may be preceded by an optional minus sign.\n\nEBNF also provides, among other things, the syntax to describe repetitions (of a specified number of times), to exclude some part of a production, and to insert comments in an EBNF grammar.\n\nThe following represents a proposed ISO/IEC 14977 standard, by R. S. Scowen, page 7, table 1.\n\nEven EBNF can be described using EBNF. Consider the sketched grammar below:\nletter = \"A\" | \"B\" | \"C\" | \"D\" | \"E\" | \"F\" | \"G\"\ndigit = \"0\" | \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\" | \"7\" | \"8\" | \"9\" ;\nsymbol = \"[\" | \"]\" | \"{\" | \"}\" | \"(\" | \")\" | \"<\" | \">\"\ncharacter = letter | digit | symbol | \"_\" ;\nidentifier = letter , { letter | digit | \"_\" } ;\nterminal = \"'\" , character , { character } , \"'\" \nlhs = identifier ;\nrhs = identifier\n\nrule = lhs , \"=\" , rhs , \";\" ;\ngrammar = { rule } ;\nA Pascal-like programming language that allows only assignments can be defined in EBNF as follows:\nA syntactically correct program then would be:\nThe language can easily be extended with control flows, arithmetical expressions, and Input/Output instructions. Then a small, usable programming language would be developed.\n\nAny grammar defined in EBNF can also be represented in BNF, though representations in the latter are generally lengthier. E.g., options and repetitions cannot be directly expressed in BNF and require the use of an intermediate rule or alternative production defined to be either nothing or the optional production for option, or either the repeated production of itself, recursively, for repetition. The same constructs can still be used in EBNF.\n\nThe BNF uses the symbols (<, >, |, ::=) for itself, but does not include quotes around terminal strings. This prevents these characters from being used in the languages, and requires a special symbol for the empty string. In EBNF, terminals are strictly enclosed within quotation marks (\"...\" or '...'). The angle brackets (\"<...>\") for nonterminals can be omitted.\n\nBNF syntax can only represent a rule in one line, whereas in EBNF a terminating character, the semicolon, marks the end of a rule.\n\nFurthermore, EBNF includes mechanisms for enhancements, defining the number of repetitions, excluding alternatives, comments, etc.\n\n1. The following conventions are used:\n2. The normal character representing each operator of Extended BNF and its implied precedence is (highest precedence at the top):\n3. The normal precedence is overridden by the following bracket pairs:\n\nThe first-quote-symbol is the apostrophe as defined by ISO/IEC 646:1991, that is to say Unicode U+0027 ('); the font used in ISO/IEC 14977:1996(E) renders it very much like the acute, Unicode U+00B4 (´), so confusion sometimes arises. However, the ISO Extended BNF standard invokes ISO/IEC 646:1991, \"ISO 7-bit coded character set for information interchange\", as a normative reference and makes no mention of any other character sets, so formally, there is no confusion with Unicode characters outside the 7-bit ASCII range.\n\nAs examples, the following syntax rules illustrate the facilities for expressing repetition:\n\naa = \"A\";\nbb = 3 * aa, \"B\";\ncc = 3 * [aa], \"C\";\ndd = {aa}, \"D\";\nee = aa, {aa}, \"E\";\nff = 3 * aa, 3 * [aa], \"F\";\ngg = {3 * aa}, \"G\";\n\nTerminal strings defined by these rules are as follows:\n\nAccording to the ISO 14977 standard EBNF is meant to be extensible, and two facilities are mentioned. The first is part of EBNF grammar, the special sequence, which is arbitrary text enclosed with question marks. The interpretation of the text inside a special sequence is beyond the scope of the EBNF standard. For example, the space character could be defined by the following rule:\nThe second facility for extension is using the fact that parentheses cannot in EBNF be placed next to identifiers (they must be concatenated with them). The following is valid EBNF:\nThe following is \"not\" valid EBNF:\nTherefore, an extension of EBNF could use that notation. For example, in a Lisp grammar, function application could be defined by the following rule:\n\n"}
{"id": "52237028", "url": "https://en.wikipedia.org/wiki?curid=52237028", "title": "Ferdinand Seidl", "text": "Ferdinand Seidl\n\nFerdinand Seidl (March 10, 1856December 1, 1942) was a Slovenian naturalist, and geologist.\n\nSeidl is considered the founder of Slovenian geology and geological terminology.\n\nHe died in his native Novo Mesto, where a street bears his name (\"Seidlova ulica\"). \n"}
{"id": "681625", "url": "https://en.wikipedia.org/wiki?curid=681625", "title": "Group velocity dispersion", "text": "Group velocity dispersion\n\nIn optics, group velocity dispersion (GVD) is a characteristic of a dispersive medium, used most often to determine how the medium will affect the duration of an optical pulse traveling through it. Formally, GVD is defined as the derivative of the inverse of group velocity of light in a material with respect to angular frequency,\nwhere formula_2 and formula_3 are angular frequencies, and the group velocity formula_4 is defined as formula_5. The units of group velocity dispersion are [time]/[distance], often expressed in fs/mm.\n\nEquivalently, group velocity dispersion can be defined in terms of the medium-dependent wave vector formula_6 according to\nor in terms of the refractive index formula_8 according to\n\nGroup velocity dispersion is most commonly used to estimate the amount of chirp that will be imposed on a pulse of light after passing through a material of interest. The relevant expression is given by\n\nA simple illustration of how GVD can be used to determine pulse chirp can be seen by looking at the effect of a transform-limited pulse of duration formula_11 passing through a planar medium of thickness \"d\". Before passing through the medium, the phase offsets of all frequencies are aligned in time, and the pulse can be described as a function of time according to the expression\nor equivalently, as a function of frequency according to the expression\n(the parameters \"A\" and \"B\" are normalization constants).\nPassing through the medium results in a frequency-dependent phase accumulation formula_14 , such that the post-medium pulse can be described by \nIn general, the refractive index formula_8, and therefore the wave vector formula_17, can be an arbitrary function of formula_2, making it difficult to analytically perform the inverse Fourier transform back into the time domain. However, if the bandwidth of the pulse is narrow relative to the curvature of formula_19, then good approximations of the impact of the refractive index can be obtained by replacing formula_6 with its Taylor expansion centered about formula_3:\n\nTruncating this expression and inserting it into the post-medium frequency-domain expression results in a post-medium time-domain expression of \nOn balance, the pulse will have lengthened to an intensity standard deviation value of\nthus validating the initial expression. Note that for a transform-limited pulse σσ = 1/2, which makes it appropriate to identify 1/(2σ) as the bandwidth.\n\nAn alternate derivation of the relationship between pulse chirp and GVD, which more immediately illustrates the reason why GVD can be defined by the derivative of inverse group velocity, can be outlined as follows. Consider two transform-limited pulses of carrier frequencies formula_25 and formula_26, which are initially overlapping in time. After passing through the medium, these two pulses will exhibit a time delay between their respective pulse-envelope centers, given by \nThe expression can be approximated as a Taylor expansion, giving \nor,\nFrom here it is possible to imagine scaling this expression up two pulses to infinitely many. The frequency difference formula_30 must be replaced by the bandwidth, and the time delay formula_31 evolves into the induced chirp.\n\nA closely related yet independent quantity is the group delay dispersion (GDD), defined such that group velocity dispersion is the group delay dispersion per unit length. GDD is commonly used as a parameter in characterizing layered mirrors, where the group velocity dispersion is not particularly-well defined, yet the chirp induced after bouncing off the mirror can be well-characterized. The units of group delay dispersion are [time], often expressed in fs.\n\nThe group delay dispersion (GDD) of an optical element is the derivative of the group delay with respect to angular frequency, and also the second derivative of the optical phase. formula_32. It is a measure of the chromatic dispersion of the element. GDD is related to the total dispersion parameter formula_33 as\n\nformula_34\n\n\n"}
{"id": "32341908", "url": "https://en.wikipedia.org/wiki?curid=32341908", "title": "Géza Horváth", "text": "Géza Horváth\n\nGéza Horváth (23 November 1847, Csécs – 8 September 1937, Budapest) was a Hungarian doctor and entomologist internationally recognized for his work on bugs (Hemiptera). He also contributed extensively to the study of Hungarian scale insect fauna. He published over 350 papers in his lifetime.\n\nHe was made director of the newly established National Phylloxera Research Station in Budapest in 1880, where he did research on aphids, Phylloxera and psyllids. He continued as director after it was renamed The State Entomological Station and broadened its focus to other kinds of noxious insects.\n\nIn 1896 he returned to the Hungarian National Museum, where he was director of its Zoology Department until he retired. He remained active in entomology after retirement, and was president of the 10th International Zoological Conference when Budapest hosted it in 1927 (his 80th year).\n\nA species of lizard, \"Iberolacerta horvathi\", is named in his honor. \n"}
{"id": "11378462", "url": "https://en.wikipedia.org/wiki?curid=11378462", "title": "Health and Ageing", "text": "Health and Ageing\n\nHealth and Ageing is a research programme set up by the Geneva Association, also known as the International Association for the Study of Insurance Economics.\nThe Geneva Association Research Programme on Health and Ageing seeks to bring together facts, figures and analyses linked to issues in health. The key is to test new and promising ideas, linking them to related studies and initiatives in the health sector and trying to find solutions for the future financing of healthcare.\n\nMajor concerns are generally directed at the rising health costs resulting from technological advances and the changing demographic structure where the population over 60 largely exceeds other sectors. Importance is placed on two major issues:\n\n\nIt is important to view these issues from the proper perspective. We are not ageing as a society but benefiting from an extended period of good health, which is largely a consequence of technological advances. It is not the increased spending on health that should be the concern but what it is spent on. It is crucial that spending is targeted and appropriately controlled with respect to the intended aim. Demographic changes and technological progress are driving changes in the governments' finances; the proportion of people in work compared with those already retired will decrease, leading to shrinking the tax base. The difficulties of financing the care of an increasing number of elderly people for increasingly long periods combined with an ever-shrinking tax base are very great. Faced with the growing health expenditure, changes have affected entire healthcare systems. The main trend in most developed countries is a creeping decentralization combined with a change in funding emphasis from public to a mix of public and private.\n\nAs the life cycle is getting longer, people have the opportunity to be productive for a longer period of time than ever before, which will therefore extend the period of wealth accumulation. This can allow funds or premiums to build up over a long period in order to cover the cost of care in the later stages of life. A good balance of the two complementarity systems public and private appears to be the best way to cope with the increase in health cost. To what extent could such mechanisms be managed in conjunction with building up pension or retirement funds? Would this imply that for some markets, in respect to certain products and circumstances we could adopt a common and coordinated approach to life and health insurance?\n\n\n\n"}
{"id": "33807629", "url": "https://en.wikipedia.org/wiki?curid=33807629", "title": "Hindgut fermentation", "text": "Hindgut fermentation\n\nHindgut fermentation is a digestive process seen in monogastric herbivores, animals with a simple, single-chambered stomach. Cellulose is digested with the aid of symbiotic bacteria. The microbial fermentation occurs in the digestive organs that follow the small intestine: the large intestine and cecum. Examples of hindgut fermenters include proboscideans and large odd-toed ungulates such as horses and rhinos, as well as small animals such as rodents, rabbits and koalas. In contrast, foregut fermentation is the form of cellulose digestion seen in ruminants such as cattle which have a four-chambered stomach, as well as in sloths, macropodids, some monkeys, and one bird, the hoatzin.\n\nHindgut fermenters generally have a cecum and large intestine that are much larger and more complex than those of a foregut or midgut fermenter. Research on small cecum fermenters such as flying squirrels, rabbits and lemurs has revealed these mammals to have a GI tract about 10-13 times the length of their body. This is due to the high intake of fiber and other hard to digest compounds that are characteristic to the diet of monogastric herbivores. Unlike in foregut fermenters, the cecum is located after the stomach and small intestine in monogastric animals, which limits the amount of further digestion or absorption that can occur after the food is fermented.\n\nIn smaller hindgut fermenters of the order Lagomorpha (rabbits, hares, and pikas), cecotropes formed in the cecum are passed through the large intestine and subsequently reingested to allow another opportunity to absorb nutrients. Cecotropes are surrounded by a layer of mucus which protects them from stomach acid but which does not inhibit nutrient absorption in the small intestine. Coprophagy is also practiced by some rodents, such as the capybara, guinea pig and related species, and by the marsupial common ringtail possum. This process is also beneficial in allowing for restoration of the microflora population, or gut flora. These microbes are found in the digestive organs of living creatures and can act as protective agents that strengthen the immune system. Hindgut fermenters do have the ability to expel their microflora, which is useful during the acts of hibernation, estivation and torpor.\n\nWhile foregut fermentation is generally considered more efficient, and monogastric animals cannot digest cellulose as efficiently as ruminants, hindgut fermentation allows animals to consume small amounts of low-quality forage all day long and thus survive in conditions where ruminants might not be able to obtain nutrition adequate for their needs. Hindgut fermenters are able to extract more nutrition out of small quantities of feed. The large hind-gut fermenters are bulk feeders: they ingest large quantities of low-nutrient food, which they process more rapidly than would be possible for a similarly sized foregut fermenter. The main food in that category is grass, and grassland grazers move over long distances to take advantage of the growth phases of grass in different regions.\n\nThe ability to process food more rapidly than foregut fermenters gives hindgut fermenters an advantage at very large body size, as they are able to accommodate significantly larger food intakes. The largest extant and prehistoric megaherbivores, elephants and indricotheres (a type of rhino), respectively, have been hindgut fermenters. Study of the rates of evolution of larger maximum body mass in different terrestrial mammalian groups has shown that the fastest growth in body mass over time occurred in hindgut fermenters (perissodactyls, rodents and proboscids).\n\nHindgut fermenters are subdivided into two groups based on the relative size of various digestive organs in relationship to the rest of the system: colonic fermenters tend to be larger species such as horses, and cecal fermenters are smaller animals such as rabbits and rodents. However, in spite of the terminology, colonic fermenters such as horses make extensive use of the cecum to break down cellulose. Also, colonic fermenters typically have a proportionally longer large intestine than small intestine, whereas cecal fermenters have a considerably enlarged cecum compared to the rest of the digestive tract.\n\nIn addition to mammals, several insects are also hindgut fermenters, the best studied of which are the termites, which are characterised by an enlarged \"paunch\" of the hindgut that also houses the bulk of the gut microbiota. Digestion of wood particles in lower termites is accomplished inside the phagosomes of gut flagellates, but in the flagellate-free higher termites, this appears to be accomplished by fibre-associated bacteria.\n\n"}
{"id": "8736919", "url": "https://en.wikipedia.org/wiki?curid=8736919", "title": "Information Technology and Innovation Foundation", "text": "Information Technology and Innovation Foundation\n\nThe Information Technology and Innovation Foundation (ITIF) is a U.S. nonprofit public policy think tank based out of Washington, D.C. The organization focuses on public policies that spur technology innovation. The University of Pennsylvania rates ITIF the most authoritative science and technology think tank in the United States, and the second most authoritative science and technology think tank in the world, behind Germany's Max Planck Institutes. Ars Technica has described ITIF as \"one of the leading, and most prolific, tech policy think tanks.\"\n\nReferred to as \"scrupulously nonpartisan,\" the think tank was established in 2006 with two former U.S. Representatives, Republican Jennifer Dunn and Democrat Calvin Dooley, as co-chairs. Currently, Republican Philip English and Democrat Vic Fazio, also former U.S. Representatives, co-chair ITIF, while Senators Orrin Hatch and Chris Coons and Representatives Anna Eshoo and Darrell Issa serve as honorary co-chairs. Robert D. Atkinson, former vice-president at the Progressive Policy Institute, is president of ITIF.\n\nIn 2018 the website of the Information Technology and Innovation Foundation listed 31 members.\n\nITIF's stated mission is to promote new ways of thinking about technology-driven productivity, competitiveness and globalization. The newspaper \"Roll Call\" described ITIF as trying to \"navigate the ideological waters to promote government support for innovation in many forms and with a broad range of ideals.\"\n\nITIF has called for the United States government to implement a national manufacturing strategy to combat job losses and the trade deficit which they attribute to declining international competitiveness. They have argued that the U.S. government's gross domestic product (GDP) statistics suffer from statistical bias and thus overstate U.S. manufacturing output and productivity growth. They have also criticized the Chinese government for behaviors they label \"innovation mercantilism\" including standards manipulation and intellectual property theft.\n\nIn Internet policy, ITIF supported both the PROTECT IP Act (PIPA) and the Stop Online Piracy Act (SOPA) in the U.S. Congress. They oppose stringent net neutrality legislation, arguing that it would stifle Internet innovation. ITIF has praised both the U.S. and the European Union \"open Internet\" rulings. For similar reasons, they have supported legislation aimed at curtailing Internet piracy, stirring some controversy when they argued that data caps on Internet usage would be an effective anti-piracy tool.\n\nAlong with the Breakthrough Institute, ITIF has called for increased public funding for clean energy innovation, arguing that the United States is falling behind countries like China, Japan and South Korea.\n\nIn economic policy, ITIF publishes the \"State New Economy Index\", which measures how much U.S. states’ economies are driven by knowledge and innovation. They publish \"The Atlantic Century\", which ranks countries on their competitiveness and innovative capacity. ITIF took over publishing the \"B-index,\" which measures the strength of countries' R&D tax incentive systems, from the Organisation for Economic Co-operation and Development (OECD) in 2012.\n\nIn the life sciences field, ITIF published \"Leadership in Decline: Assessing U.S. International Competitiveness in Biomedical Research\" in 2012, which director of the National Institutes of Health and leader of the Human Genome Project Francis S. Collins deemed the \"one book\" he would require President Barack Obama to read in his second term in office.\n\nITIF has published several reports advocating greater deployment of information technologies, including \"Digital Prosperity\" and \"Digital Quality of Life\". In \"Digital Prosperity\", ITIF found that IT investment delivered three to five times the productivity growth of other types of investments. Commenting on the study, former Dean of Wisconsin School of Business Michael Knetter agreed with the productivity figures, though expressed caution given that some of ITIF's contributors are in the technology industry. ITIF's report \"Steal These Policies: Strategies for Reducing Digital Piracy\" provided the foundation for the controversial PROTECT IP and Stop Online Piracy Acts in the U.S. Congress, which the think tank acknowledged were at odds with the positions of many of its contributors.\n\nIn 2013, the think tank published a widely cited report which found that the U.S. National Security Agency's PRISM electronic data surveillance program could cost the U.S. economy between $21.5 and $35 billion in lost cloud computing business over three years.\n\nThe Foundation yearly awards the Luddite Award for the \"Year’s Worst Innovation Killers\".\n\nITIF contributors have included the Alfred P. Sloan Foundation, the Atlantic Philanthropies, Cisco, Communications Workers of America, eBay, the Ewing Marion Kauffman Foundation, Google, IBM, the Information Technology Industry Council, the Nathan Cummings Foundation, and Bernard L. Schwartz. ITIF's research has also been funded by U.S. government agencies such as the National Institute of Standards and Technology (NIST) and the United States Agency for International Development (USAID). In September 2010, ITIF received funding from the U.S. Election Assistance Commission to study means for improving voting accessibility for U.S. military service members who have sustained disabling injuries in combat.\n\n\n"}
{"id": "21471659", "url": "https://en.wikipedia.org/wiki?curid=21471659", "title": "Leiden Classical", "text": "Leiden Classical\n\nLeiden Classical was a distributed computing project run by the Theoretical Chemistry Department of the Leiden Institute of Chemistry at Leiden University. Leiden Classical is part of the BOINC system, and enables scientists or science students to submit their own test simulations of various molecules and atoms in a classical mechanics environment. ClassicalDynamics is a program (and with it a library) completely written in C++. The library is covered by the LGPL license and the main program is covered by the GPL. The project shut down on June 5, 2018.\n\nParticipation was possible via the BOINC manager. Using this software one was once able to create an account in the project. Then someone can make a model of a dynamic system and simulation participating run. There are several models possible, to interactions between molecules or planets.\n\nTo create a personal calculation, your model must have 6 defined variables:\n\n\n\n"}
{"id": "5814915", "url": "https://en.wikipedia.org/wiki?curid=5814915", "title": "List of Inferno applications", "text": "List of Inferno applications\n\nThis is a list of Inferno programs. Most of these programs are very similar to the Plan 9 applications or UNIX programs with the same name.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "2142", "url": "https://en.wikipedia.org/wiki?curid=2142", "title": "List of artificial intelligence projects", "text": "List of artificial intelligence projects\n\nThe following is a list of current and past, nonclassified notable artificial intelligence projects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "18760137", "url": "https://en.wikipedia.org/wiki?curid=18760137", "title": "List of dams and reservoirs in Alaska", "text": "List of dams and reservoirs in Alaska\n\nAlaska has about 67 named artificial reservoirs, approximately 167 named dams,and about 3,197 officially named natural lakes, out of over 3,000,000 unnamed natural lakes.\nFor named natural lakes, see the list of lakes of Alaska.\n\n \n\n \n\n"}
{"id": "25250359", "url": "https://en.wikipedia.org/wiki?curid=25250359", "title": "List of freeware", "text": "List of freeware\n\nFreeware is software that is available for use at no monetary cost or for an optional fee, but usually (although not necessarily) closed source with one or more restricted usage rights. Freeware is in contrast to commercial software, which is typically sold for profit, but might be distributed for a business or commercial purpose in the aim to expand the marketshare of a \"premium\" product. Popular examples of closed-source freeware include Adobe Reader, Free Studio and Skype.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "14485845", "url": "https://en.wikipedia.org/wiki?curid=14485845", "title": "List of members of the National Academy of Sciences (Mathematics)", "text": "List of members of the National Academy of Sciences (Mathematics)\n"}
{"id": "7119984", "url": "https://en.wikipedia.org/wiki?curid=7119984", "title": "List of volcanoes in Colombia", "text": "List of volcanoes in Colombia\n\nThis is a list of active and extinct volcanoes in Colombia.\n\n"}
{"id": "28809921", "url": "https://en.wikipedia.org/wiki?curid=28809921", "title": "Markarian 231", "text": "Markarian 231\n\nMarkarian 231 (UGC 8058) is a Type-1 Seyfert galaxy that was discovered in 1969 as part of a search of galaxies with strong ultraviolet radiation. It contains the nearest known quasar, and in 2015 it was shown that the powerful active galactic nucleus present in the center of the galaxy may in fact be a supermassive binary black hole. It is located about 581 million light years away from Earth.\n\nThe galaxy is now undergoing an energetic starburst. A nuclear ring of active star formation has been found in the center with a rate of formation greater than 100 solar masses per year. It is one of the most ultraluminous infrared galaxies with power derived from an accreting black hole in the center, and the closest known quasar. A 2015 study has found that the central black hole, estimated to be 150 million times the mass of our Sun, has a black hole companion weighing in at 4 million solar masses, and the duo completes an orbit around each other every 1.2 years. Subsequently, that model has been definitively shown to be unfeasible.\n\n\n"}
{"id": "11817317", "url": "https://en.wikipedia.org/wiki?curid=11817317", "title": "Mechanical explanations of gravitation", "text": "Mechanical explanations of gravitation\n\nMechanical explanations of gravitation (or kinetic theories of gravitation) are attempts to explain the action of gravity by aid of basic mechanical processes, such as pressure forces caused by pushes, without the use of any action at a distance. These theories were developed from the 16th until the 19th century in connection with the aether. However, such models are no longer regarded as viable theories within the mainstream scientific community and general relativity is now the standard model to describe gravitation without the use of actions at a distance. Modern \"quantum gravity\" hypotheses also attempt to describe gravity by more fundamental processes such as particle fields, but they are not based on classical mechanics.\n\nThis theory is probably the best-known mechanical explanation, and was developed for the first time by Nicolas Fatio de Duillier in 1690, and re-invented, among others, by Georges-Louis Le Sage (1748), Lord Kelvin (1872), and Hendrik Lorentz (1900), and criticized by James Clerk Maxwell (1875), and Henri Poincaré (1908).\n\nThe theory posits that the force of gravity is the result of tiny particles or waves moving at high speed in all directions, throughout the universe. The intensity of the flux of particles is assumed to be the same in all directions, so an isolated object A is struck equally from all sides, resulting in only an inward-directed pressure but no net directional force. With a second object B present, however, a fraction of the particles that would otherwise have struck A from the direction of B is intercepted, so B works as a shield, so-to-speak—that is, from the direction of B, A will be struck by fewer particles than from the opposite direction. Likewise, B will be struck by fewer particles from the direction of A than from the opposite direction. One can say that A and B are \"shadowing\" each other, and the two bodies are pushed toward each other by the resulting imbalance of forces.\nThis shadow obeys the inverse square law, because the imbalance of momentum flow over an entire spherical surface enclosing the object is independent of the size of the enclosing sphere, whereas the surface area of the sphere increases in proportion to the square of the radius. To satisfy the need for mass proportionality, the theory posits that a) the basic elements of matter are very small so that gross matter consists mostly of empty space, and b) that the particles are so small, that only a small fraction of them would be intercepted by gross matter. The result is, that the \"shadow\" of each body is proportional to the surface of every single element of matter.\n\n\"Criticism\": This theory was declined primarily for thermodynamic reasons because a shadow only appears in this model if the particles or waves are at least partly absorbed, which should lead to an enormous heating of the bodies. Also drag, \"i.e.\" the resistance of the particle streams in the direction of motion, is a great problem too. This problem can be solved by assuming superluminal speeds, but this solution largely increases the thermal problems and contradicts special relativity.\n\nBecause of his philosophical beliefs, René Descartes proposed in 1644 that no empty space can exist and that space must consequently be filled with matter. The parts of this matter tend to move in straight paths, but because they lie close together, they can not move freely, which according to Descartes implies that every motion is circular, so the aether is filled with vortices. Descartes also distinguishes between different forms and sizes of matter in which rough matter resists the circular movement more strongly than fine matter. Due to centrifugal force, matter tends towards the outer edges of the vortex, which causes a condensation of this matter there. The rough matter cannot follow this movement due to its greater inertia—so due to the pressure of the condensed outer matter those parts will be pushed into the center of the vortex. According to Descartes, this inward pressure is nothing else than gravity. He compared this mechanism with the fact that if a rotating, liquid filled vessel is stopped, the liquid goes on to rotate. Now, if one drops small pieces of light matter (e.g. wood) into the vessel, the pieces move to the middle of the vessel.\n\nFollowing the basic premises of Descartes, Christiaan Huygens between 1669 and 1690 designed a much more exact vortex model. This model was the first theory of gravitation which was worked out mathematically. He assumed that the aether particles are moving in every direction, but were thrown back at the outer borders of the vortex and this causes (as in the case of Descartes) a greater concentration of fine matter at the outer borders. So also in his model the fine matter presses the rough matter into the center of the vortex. Huygens also found out that the centrifugal force is equal to the force, which acts in the direction of the center of the vortex (centripetal force). He also posited that bodies must consist mostly of empty space so that the aether can penetrate the bodies easily, which is necessary for mass proportionality. He further concluded that the aether moves much faster than the falling bodies. At this time, Newton developed his theory of gravitation which is based on attraction, and although Huygens agreed with the mathematical formalism, he said the model was insufficient due to the lack of a mechanical explanation of the force law. Newton's discovery that gravity obeys the inverse square law surprised Huygens and he tried to take this into account by assuming that the speed of the aether is smaller in greater distance.\n\n\"Criticism\": Newton objected to the theory because drag must lead to noticeable deviations of the orbits which were not observed. Another problem was that moons often move in different directions, against the direction of the vortex motion. Also, Huygens' explanation of the inverse square law is circular, because this means that the aether obeys Kepler's third law. But a theory of gravitation has to explain those laws and must not presuppose them.\n\nIn a 1675 letter to Henry Oldenburg, and later to Robert Boyle, Newton wrote the following: [Gravity is the result of] “a condensation causing a flow of ether with a corresponding thinning of the ether density associated with the increased velocity of flow.” He also asserted that such a process was consistent with all his other work and Kepler's Laws of Motion. Newtons' idea of a pressure drop associated with increased velocity of flow was mathematically formalised as Bernoulli's principle published in Daniel Bernoulli's book \"Hydrodynamica\" in 1738.\n\nHowever, although he later proposed a second explanation (see section below), Newton's comments to that question remained ambiguous. In the third letter to Bentley in 1692 he wrote:\n\nIt is inconceivable that inanimate brute matter should, without the mediation of something else which is not material, operate upon and affect other matter, without mutual contact, as it must do if gravitation in the sense of Epicurus be essential and inherent in it. And this is one reason why I desired you would not ascribe 'innate gravity' to me. That gravity should be innate, inherent, and essential to matter, so that one body may act upon another at a distance, through a vacuum, without the mediation of anything else, by and through which their action and force may be conveyed from one to another, is to me so great an absurdity, that I believe no man who has in philosophical matters a competent faculty of thinking can ever fall into it. Gravity must be caused by an agent acting constantly according to certain laws; but whether this agent be material or immaterial, I have left to the consideration of my readers.\n\nOn the other hand, Newton is also well known for the phrase Hypotheses non fingo, written in 1713:\n\nI have not as yet been able to discover the reason for these properties of gravity from phenomena, and I do not feign hypotheses. For whatever is not deduced from the phenomena must be called a hypothesis; and hypotheses, whether metaphysical or physical, or based on occult qualities, or mechanical, have no place in experimental philosophy. In this philosophy particular propositions are inferred from the phenomena, and afterwards rendered general by induction.\n\nAnd according to the testimony of some of his friends, such as Nicolas Fatio de Duillier or David Gregory, Newton thought that gravitation is based directly on divine influence.\n\nSimilar to Newton, but mathematically in greater detail, Bernhard Riemann assumed in 1853 that the gravitational aether is an incompressible fluid and normal matter represents sinks in this aether. So if the aether is destroyed or absorbed proportionally to the masses within the bodies, a stream arises and carries all surrounding bodies into the direction of the central mass. Riemann speculated that the absorbed aether is transferred into another world or dimension.\n\nAnother attempt to solve the energy problem was made by Ivan Osipovich Yarkovsky in 1888. Based on his aether stream model, which was similar to that of Riemann, he argued that the absorbed aether might be converted into new matter, leading to a mass increase of the celestial bodies.\n\n\"Criticism\": As in the case of Le Sage's theory, the disappearance of energy without explanation violates the energy conservation law. Also some drag must arise, and no process which leads to a creation of matter is known.\n\nNewton updated the second edition of \"Optics\" (1717) with another mechanical-ether theory of gravity. Unlike his first explanation (1675 - see Streams), he proposed a stationary aether which gets thinner and thinner nearby the celestial bodies. On the analogy of the lift, a force arises, which pushes all bodies to the central mass. He minimized drag by stating an extremely low density of the gravitational aether.\n\nLike Newton, Leonhard Euler presupposed in 1760 that the gravitational aether loses density in accordance with the inverse square law. Similarly to others, Euler also assumed that to maintain mass proportionality, matter consists mostly of empty space.\n\n\"Criticism\": Both Newton and Euler gave no reason why the density of that static aether should change. Furthermore, James Clerk Maxwell pointed out that in this \"hydrostatic\" model \"the state of stress... which we must suppose to exist in the invisible medium, is 3000 times greater than that which the strongest steel could support\".\n\nRobert Hooke speculated in 1671 that gravitation is the result of all bodies emitting waves in all directions through the aether. Other bodies, which interact with these waves, move in the direction of the source of the waves. Hooke saw an analogy to the fact that small objects on a disturbed surface of water move to the center of the disturbance.\n\nA similar theory was worked out mathematically by James Challis from 1859 to 1876. He calculated that the case of attraction occurs if the wavelength is large in comparison with the distance between the gravitating bodies. If the wavelength is small, the bodies repel each other. By a combination of these effects, he also tried to explain all other forces.\n\n\"Criticism\": Maxwell objected that this theory requires a steady production of waves, which must be accompanied by an infinite consumption of energy.\nChallis himself admitted, that he hadn't reached a definite result due to the complexity of the processes.\n\nLord Kelvin (1871) and Carl Anton Bjerknes (1871) assumed that all bodies pulsate in the aether. This was in analogy to the fact that, if the pulsation of two spheres in a fluid is in phase, they will attract each other; and if the pulsation of two spheres is \"not\" in phase, they will repel each other. This mechanism was also used for explaining the nature of electric charges. Among others, this hypothesis has also been examined by George Gabriel Stokes and Woldemar Voigt.\n\n\"Criticism\" : To explain universal gravitation, one is forced to assume that all pulsations in the universe are in phase—which appears very implausible. In addition, the aether should be incompressible to ensure that attraction also arises at greater distances. And Maxwell argued that this process must be accompanied by a permanent new production and destruction of aether.\n\nIn 1690, Pierre Varignon assumed that all bodies are exposed to pushes by aether particles from all directions, and that there is some sort of limitation at a certain distance from the Earth's surface which cannot be passed by the particles. He assumed that if a body is closer to the Earth than to the limitation boundary, then the body would experience a greater push from above than from below, causing it to fall toward the Earth.\n\nIn 1748, Mikhail Lomonosov assumed that the effect of the aether is proportional to the complete surface of the elementary components of which matter consists (similar to Huygens and Fatio before him). He also assumed an enormous penetrability of the bodies. However, no clear description was given by him as to how exactly the aether interacts with matter so that the law of gravitation arises.\n\nIn 1821, John Herapath tried to apply his co-developed model of the kinetic theory of gases on gravitation. He assumed that the aether is heated by the bodies and loses density so that other bodies are pushed to these regions of lower density.\nHowever, it was shown by Taylor that the decreased density due to thermal expansion is compensated for by the increased speed of the heated particles; therefore, no attraction arises.\n\nThese mechanical explanations for gravity never gained widespread acceptance, although such ideas continued to be studied occasionally by physicists until the beginning of the twentieth century, by which time it was generally considered to be conclusively discredited. However, some researchers outside the scientific mainstream still try to work out some consequences of those theories.\n\nLe Sage's theory was studied by Radzievskii and Kagalnikova (1960), Shneiderov (1961), Buonomano and Engels (1976), Adamut (1982), Jaakkola (1996), Tom Van Flandern (1999), and Edwards (2007). A variety of Le Sage models and related topics are discussed in Edwards, et al.\n\nGravity due to static pressure was recently studied by Arminjon.\n\n"}
{"id": "20911119", "url": "https://en.wikipedia.org/wiki?curid=20911119", "title": "Medical Apartheid", "text": "Medical Apartheid\n\nMedical Apartheid: The Dark History of Medical Experimentation on Black Americans from Colonial Times to the Present is a 2007 book by Harriet A. Washington. It is a history of medical experimentation on African Americans. From the era of slavery to the present day, this book presents the first full account of black America's mistreatment as unwitting subjects of medical experimentation.\n\n\"Medical Apartheid\" traces the convoluted history of medical experimentation on Black Americans in the United States since the middle of the eighteenth century. Harriet Washington argues that \"diverse forms of racial discrimination have shaped both the relationship between white physicians and black patients and the attitude of the latter towards modern medicine in general\".\nThe book is divided into three parts: the first is about the cultural memory of medical experimentation; the second examines recent cases of medical abuse and research; while the last addresses the complex relationship between racism and medicine. Some topics discussed are well-known, such as the ‘Tuskegee Syphilis Study’ (1932–72), in which African Americans suffering from the disease were prevented from receiving the necessary medication by the US Public Health Service so that the evolution of the disease could be observed, but other episodes are less well known to the general public. The book also mentions cases of Medical Experimentation in Africa and their links to African-American cases.\n\n\"Medical Apartheid\" won the 2007 National Book Critics Circle Award for Nonfiction. Harriet Washington has been a fellow in ethics at the Harvard Medical School, a fellow at the Harvard School of Public Health, and a senior research scholar at the National Center for Bioethics at Tuskegee University.\n\n"}
{"id": "6704202", "url": "https://en.wikipedia.org/wiki?curid=6704202", "title": "NATO Science for Peace and Security", "text": "NATO Science for Peace and Security\n\nThe NATO Science for Peace and Security (SPS) Programme is a NATO programme supporting civil science cooperation and innovation. Created in 2006 as the merger of the NATO Science Committee (SCOM) and the Committee on the Challenges of Modern Society (CCMS), the SPS offers grants to scientists in NATO and NATO Partner countries for work on civil science projects. Partner countries include Eastern Europe and the former Soviet Union. Grants are also available to scientists in seven countries known as the Mediterranean Dialogue: Algeria, Egypt, Israel, Jordan, Mauritania, Morocco, and Tunisia.\n\nEach SPS project is conducted in a specific NATO priority area by a collaboration between working scientists in eligible Partner countries and scientists in NATO Allied countries. Applications must be in the area of the SPS Key Priorities.\n\n\n\n"}
{"id": "9787557", "url": "https://en.wikipedia.org/wiki?curid=9787557", "title": "National Research Service Award", "text": "National Research Service Award\n\nThe Ruth L. Kirschstein National Research Service Awards (usually referred to as NRSA) are a family of grants provided by the United States National Institutes of Health for training researchers in the behavioral sciences and health sciences. They are a highly selective and very prestigious source of funding for doctoral and postdoctoral trainees. The grants are awarded based on lengthy proposals submitted by applicants in which original experimental plans are described. The proposals are evaluated and given an impact score reflecting scientific merit by a study section at the Center for Scientific Review at the NIH. Only applications with very good impact scores are funded, based on budget cutoffs determined by each individual institute. US citizenship or permanent residency is required. The NIH awarded $77,000,000 in individual grants and over $600,000,000 in institutional training grants in fiscal year 2005 .\n\nNRSA awards are mostly given to students working on a Ph.D or an MD or other medical degree, or to individuals who have just earned one of these degrees and are beginning their careers. The NRSA program also provides institutions with training grants that can be used to fund one or more students. NRSA grants are notable for their flexibility: postdoctoral researchers can propose to work at any university, and the only requirement is that they commit to at least one year of research in their field following their first year of funding.\n\n"}
{"id": "1791712", "url": "https://en.wikipedia.org/wiki?curid=1791712", "title": "Normality (behavior)", "text": "Normality (behavior)\n\nNormality is a behavior that can be normal for an individual (intrapersonal normality) when it is consistent with the most common behaviour for that person. Normal is also used to describe individual behaviour that conforms to the most common behaviour in society (known as conformity). Definitions of normality vary by person, time, place, and situation – it changes along with changing societal standards and norms. Normal behavior is often only recognized in contrast to abnormality. In its simplest form, normality is seen as good while abnormality is seen as bad. Someone being seen as normal or not normal can have social ramifications, such as being included, excluded or stigmatized by larger society.\n\nNormality has been functionally and differentially defined by a vast number of disciplines, so there is not one single definition.\n\nIn general, 'normal' refers to a lack of significant deviation from the average. The word normal is used in a more narrow sense in mathematics, where a normal distribution describes a population whose characteristics centers around the average or the norm. When looking at a specific behaviour, such as the frequency of lying, a researcher may use a Gaussian bell curve to plot all reactions, and a normal reaction would be within one standard deviation, or the most average 68.3%. However, this mathematical model only holds for one particular trait at a time, since, for example, the probability of a single individual being within one standard deviation for 36 independent variables would be one in a million. In statistics, normal is often arbitrarily considered anything that falls within about 1.96 standard deviations of the mean, or the most average 95% (see 1.96). The probability of an individual being within 1.96 standard deviations for 269 independent variables is approximately one in a million. For only 59 independent variables, the probability is just under 5%. Under this definition of normal, it is abnormal to be normal for 59 independent variables.\n\nThe French sociologist Émile Durkheim indicated in his \"Rules of the Sociological Method\" that it was necessary for the sociological method to offer parameters to distinguish normality from pathology or abnormality. He suggested that behaviors or \"social facts\" which are present in the majority of cases are normal, and exceptions to that behavior indicate pathology. Durkheim's model of normality further explained that the most frequent or general behaviors, and thus the most normal behaviors, will persist through transition periods in society. Crime, for instance, exists under every society through every time period, and so should be considered normal. There is a two-fold version of normality; behaviors considered normal on a societal level may still be considered pathological on an individual level. On the individual level, people who violate social norms, such as criminals, will invite a punishment from others in the society.\n\nIndividuals' behaviours are guided by what they perceive to be society's expectations and their peers' norms. People measure the appropriateness of their actions by how far away they are from those social norms. However, what is perceived as the norm may or may not actually be the most common behaviour. In some cases of pluralistic ignorance, most people falsely believe the social norm is one thing, but in fact very few people hold that view.\n\nWhen people are made more aware of a social norm, particularly a descriptive norm (a norm describing what is done), their behaviour changes to become closer to that norm. The power of these norms can be harnessed by social norms marketing, where the social norm is advertised to people in an attempt to stop extreme behaviour, such as binge drinking. However, people at the other extreme (very little alcohol consumption) are equally likely to change their behaviour to become closer to the norm, in this case by increasing alcohol consumption. Instead of using descriptive norms, more effective social norms marketing may use injunctive norms. Instead of describing what behaviour is most commonly done, an injunctive norm is what is approved or disapproved of by society. When individuals become aware of the injunctive norm, only the extremes will change their behaviour (by decreasing alcohol consumption) without the boomerang effect of under-indulgers increasing their drinking.\n\nThe social norms that guide people are not always normal for everyone. Behaviours that are abnormal for most people may be considered normal for a subgroup or subculture. For example, normal college student behaviour may be to party and drink alcohol, but for a subculture of religious students, normal behaviour may be to go to church and pursue religion related activities. Subcultures may actively reject \"normal\" behaviour, instead replacing society norms with their own.\n\nA disharmony exists between a virtual identity of the self and a real social identity, whether it be in the form of a trait or attribute. If a person does not have this disharmony, then he or she is described as normal. A virtual identity can take many definitions, but in this case a virtual identity is the identity that persons mentally create that conforms to societal standards and norms, it may not represent how they actually are, but it represents what they believe is the typical \"normal\" person. A real social identity is the identity that persons actually have in their society or is perceived, by themselves or others, to have. If these two identities have differences between each other, there is said to be disharmony. Individuals may monitor and adapt their behaviour in terms of others' expected perceptions of the individual, which is described by the social psychology theory of self-presentation. In this sense, normality exists based on societal norms, and whether someone is normal is entirely up to how he or she views him- or herself in contrast to how society views him or her. While trying to define and quantify normality is a good start, all definitions confront the problem of whether we are even describing an idea that even exists since there are so many different ways of viewing the concept.\n\nMany difficulties arise in measuring normal behaviors – biologists come across parallel issues when defining normality. One complication which arises regards whether 'normality' is used correctly in everyday language. People say \"This heart is abnormal\" if only a portion of it is not working correctly, yet it may be inaccurate to include the entirety of the heart under the abnormal description. There can be a difference between the normality of the structure and function of a body part. Similarly, a behavioural pattern may not conform to social norms, but still be effective and non-problematic for that individual. Where there is a dichotomy between appearance and function of a behaviour, it may be difficult to measure its normality. This is applicable when trying to diagnose a pathology and is addressed in the DSM.\n\nWhat is viewed as normal can change dependent on both timeframe and environment. Normality can be viewed as \"an endless process of man's self-creation and his reshaping of the world\". Within this idea, it is possible to surmise that normality is not an all-encompassing term, but simply a relative term based around a current trend in time. With statistics, this is likened to the thought that if the data gathered provides a mean and standard deviation, over time these data that predict \"normalness\" start to predict or dictate it less and less since the social idea of normality is dynamic. This is shown in studies done on behavior in psychology and sociology where behavior in mating rituals or religious rituals can change within a century in humans, showing that the \"normal\" way that these rituals are performed shift and a new procedure becomes the normal one.\n\nAs another example, understandings of what is normal sexual behaviour varies greatly across time and place. In many countries, perceptions on sexuality are largely becoming more liberal, especially views on the normality of masturbation and homosexuality. Social understanding on normal sexual behaviour also varies greatly country by country – countries can be divided into categories of how they approach sexual normality, as conservative, homosexual-permissive, or liberal. The United States, Ireland, and Poland have more conservative social understanding of sexuality among university students, while Scandinavian students consider a wider variety of sexual acts as normal. Although some attempts have been made to define sexual acts as normal, abnormal, or indeterminate, these definitions are time-sensitive. Gayle Rubin's 1980s model of sexual 'normality' was comprehensive at the time but has since become outdated as society has liberalized.\n\nSince normality shifts in time and environment, the mean and standard deviation are only useful for describing normality from the environment from which they are collected.\n\nMost definitions of normality consider interpersonal normality, the comparison between many different individual's behaviours to distinguish normality from abnormality. Intrapersonal normality looks at what is normal behaviour for one particular person (consistency within a person) and would be expected to vary person-to-person. A mathematical model of normality could still be used for intrapersonal normality, by taking a sample of many different occurrences of behaviour from one person over time. Also like interpersonal normality, intrapersonal normality may change over time, due to changes in the individual as they age and due to changes in society (since society's view of normality influences individual peoples' behaviour).\n\nIt is most comfortable for people to engage in behaviour which conforms to their own personal habitual norms. When things go wrong, people are more likely to attribute the negative outcome on any abnormal behaviour leading up to the mishap. After a car crash, people may say \"if only I didn't leave work early\", blaming the crash on their actions which were not normal. This counterfactual thinking particularly associates abnormal behaviour with negative outcomes.\n\nIn medicine, behavioral normality pertains to a patient's mental condition aligning with that of a model, healthy patient. A person without any mental illness is considered a normal patient, whereas a person with a mental disability or illness is viewed as abnormal. These normals and abnormals in the context of mental health subsequently create negative stigmatic perceptions towards individuals with mental illness. The Brain & Behavior Research Foundation stated that \"an estimated 26.2 percent of Americans ages 18 and older – about 1 in 4 adults – suffer from one or more of (several) disorders in a given year\". Though the population of American individuals living with mental illness is not as small of a minority as commonly perceived, it is considered abnormal nonetheless, therefore the subject of discrimination and abuse such as violent therapies, punishments, or labeling for life by the normal, healthy majority. The CDC reported that \"cluster[s] of negative attitudes and beliefs motivate the general public to fear, reject, avoid, and discriminate against people with mental illnesses\". In continuum, the resources available to those who suffer from such illness are limited, and government support is constantly being cut from programs that help individuals living with mental illness live more comfortable, accommodative, happier lives.\n\nHebbian associative learning and memory maintenance depends on synaptic normalization mechanisms to prevent synaptic runaway. Where synaptic runaway describes overcrowding of dendritic associations, which reduce sensory or behavioural acuteness proportional to the level of synaptic runaway. Synaptic/neuronal normalization refers to synaptic competition, where the prosper of one synapse may weakening the efficacy of other nearby surrounding synapses with redundant neurotransmission.\n\nAnimal dendritic density greatly increases throughout waking hours despite intrinsic normalization mechanisms as described as above. The growth rate of synaptic density is not sustained in a cumulative fashion. Without a pruning state, the signal to noise ratio of CNS mechanism would not be able to operate with maximum effectiveness, and learning would be detrimental to animal survival. Neuronal and synaptic normalization mechanisms must operate so positive association feedback loops to not become rampant while constantly processing new environmental information.\n\nSome researchers speculate that the slow oscillation (nREM) cycles of animal sleep constitute an essential 're-normalization' phase. The re-normalization occurs from cortical large amplitude brain rhythm, in the low delta range (0.5–2 Hz), synaptically downscaling the associations from the wakeful learning state. Only the strongest associations survive the pruning from this phase. This allows retention of salient information coding from the previous day, but also allows more cortical space and energy distribution to continue effective learning subsequently after a slow-wave oscillation episode of sleep.\n\nAlso, organisms tend to have a normal biological developmental pathway as a central nervous system ages and/or learns. Deviations for a species' normal development frequently will result in behaviour dysfunction, or death, of that organism.\n\nWhen people do not conform to the normal standard, they are often labelled as sick, disabled, abnormal, or unusual, which can lead to marginalization or stigmatization. Most people want to be normal and strive to be perceived as such, so that they can relate to society at large. Without having things in common with the general population, people may feel isolated among society. The abnormal person feels like they have less in common with the normal population, and others have difficulty relating to things that they have not experienced themselves. Additionally, abnormality may make others uncomfortable, further separating the abnormally labelled individual.\n\nSince being normal is generally considered an ideal, there is often pressure from external sources to conform to normality, as well as pressure from people's intrinsic desire to feel included. For example, families and the medical community will try to help disabled people live a normal life. However, the pressure to appear normal, while actually having some deviation, creates a conflict – sometimes someone will appear normal, while actually experiencing the world differently or struggling. When abnormality makes society feel uncomfortable, it is the exceptional person themselves who will laugh it off to relieve social tension. A disabled person is given normal freedoms, but may not be able to show negative emotions. Lastly, society's rejection of deviance and the pressure to normalize may cause shame in some individuals. Abnormalities may not be included in an individual's sense of identity, especially if they are unwelcome abnormalities.\n\nWhen an individual's abnormality is labelled as a pathology, it is possible for that person to take on both elements of the sick role or the stigmatization that follows some illnesses. Mental illness, in particular, is largely misunderstood by the population and often overwhelms others' impression of the patient.\n\nApplying normality clinically depends on the field and situation a practitioner is in. In the broadest sense, clinical normality is the idea of uniformity of physical and psychological functioning across individuals. Normality, and abnormality, can be characterized statistically.\nRelated to the previous definition, statistically normality is usually defined it in terms of a normal distribution curve, with the so-called 'normal zone' commonly accounting for 95.45% percent of all the data. The remaining 4.55% will lie split outside of two standard deviations from the mean. Thus any variable case that lies outside of two deviations from the mean would be considered abnormal. However, the critical value of such statistical judgments may be subjectively altered to a less conservative estimate. It is in fact normal for a population to have a proportion of abnormals. The presence of abnormals is important because it is necessary to define what 'normal' is, as normality is a relative concept. So at a group, or macro level, of analysis; abnormalities are normal given a demographic survey, but at an individual level abnormal individuals are seen as being deviant in someway that needs to be corrected.\nStatistical normality is important in determining demographic pathologies. When a variable rate, such as virus spread within a human population, exceeds its normal infection rate then preventative or emergency measures can be introduced. \nIt is often impractical to apply statistical normality to diagnose individuals. Symptom normality is the current, and assumed most effective, way to assess patient pathology. Psychiatric normality, in a broad sense, states that psychopathology are disorders that are deviations from normality.\n\nNormality, as a relative concept, is intrinsically involved with contextual elements. As a result, clinical disorder classification has particular challenges in discretely diagnosing 'normal' constitutions from true disorders. The \"Diagnostic and Statistical Manual of Mental Disorders\" (DSM) is the psychiatric profession's official classification manual of mental disorders since its first published version DSM-I in by the APA, 1952. As the DSM evolved into its current version, DSM-5 in late 2013, there have been numerous conflicts in proposed classification between mental illness and normal mentality. Dr. Allen Frances, who chaired the task force for content in the DSM-IV and DSM-IV-TR even wrote a scathing indictment of the pressures incumbent on the definition of \"normal\" relative to psychological constructs and mental illness in his book, \"Saving Normal\".\n\nMost of this difficulty stems from the DSM's ambiguity of natural contextual stressor reactions versus individual dysfunction. There are some key progressions along the DSM history that have attempted to integrate some aspects of normality into proper diagnosis classification. As a diagnostic manual for classification of abnormalities, all DSMs have been biased towards classifying symptoms as disorders by emphasizing symptomatic singularity. The result is an encompassing misdiagnosis of possible normal symptoms, appropriate as contextually derived.\n\nThe second edition of the DSM, DSM-II, could not be effectively applied because of its vague descriptive nature. Psychodynamic etiology was a strong theme in classifying mental illnesses. The applied definitions became idiosyncratic, stressing individual unconscious roots. This made applying the DSM unreliable across psychiatrists. No distinction between abnormal to normal was established.\n\nEvidence of the classification ambiguity were punctated by the Rosenhan experiment of 1972. This experiment demonstrated that the methodology of psychiatric diagnosis could not effectively distinguish normal from disordered mentalities. DSM-II labelled 'excessive' behavioral and emotional response as an index of abnormal mental wellness to diagnose some particular disorders. 'Excessiveness' of a reaction implied alternative normal behaviour which would have to include a situational factor in evaluation. As an example; a year of intense grief from the death of a spouse may be a normal appropriate response. To have intense grief for twenty years would be indicative of a mental disorder. As well, to grieve intensely over the loss of a sock would also not be considered normal responsiveness and indicate a mental disorder. The consideration of proportionality to stimuli was a perceived strength in psychiatric diagnosis for the DSM-II.\n\nAnother characteristic of the DSM-II systemization was that it classified homosexuality as a mental disorder. Thus, homosexuality was psychiatrically defined a pathological deviation from 'normal' sexual development. Homosexuality was later replaced in the 7th printing of DSM-II, instead categorized as a 'Sexual orientation disturbance'. The intent was to have a label that applied only to those homosexual individuals who were bothered by their sexual orientation. In this manner homosexuality would not be viewed as an atypical illness. Only if it was distressing would homosexuality be classified as a mental illness. However, the DMS-II did not explicitly state that any homosexuality was normal either. This stigma lasted into DSM-III until it was reformed entirely from DSM classifications in 1987.\n\nDSM-III was a best attempt to credit psychiatry as a scientific discipline, from the opprobrium resulting from DSM-II. A reduction in the psychodynamic etiologies of DSM-II spilled over into a reduction symptom etiology altogether. Thus, DSM-III was a specific set of definitions for mental illnesses, and entities more suited to diagnostic psychiatry, but which annexed response proportionality as a classification factor. The product was that all symptoms, whether normal proportional response or inappropriate pathological tendencies, could both be treated as potential signs of mental illness.\n\nDSM-IV explicitly distinguishes mental disorders and non-disordered conditions. A non-disordered condition results from, and is perpetuated by, social stressors. Included in DSM-IV's classification is that a mental disorder \"must not be merely an expectable and culturally sanctioned response to a particular event, for example, the death of a loved one. Whatever its original cause, it must currently be considered a manifestation of a behavioral, psychological, or biological dysfunction in the individual\" (American Psychiatric Association 2000:xxxi)\nThis had supposedly injected normality consideration back into the DSM, from its removal from DSM-II. However, it has been speculated that DSM-IV still does not escape the problems DSM-III faced, where psychiatric diagnoses still include symptoms of expectable responses to stressful circumstances to be signs of disorders, along with symptoms that are individual dysfunctions. The example set by DSM-III, for principally symptom-based disorder classification, has been integrated as the norm of mental diagnostic practice.\n\nThe DSM-5 was released in the second half of 2013. It has significant differences from DSM IV-TR, including the removal of the multi-axial classifications and reconfiguring the Asperger's/autistic spectrum classifications.\n\nSince the advent of DSM-III, the subsequent editions of the DSM have all included a heavy symptom based pathology diagnosis system. Although there have been some attempts to incorporate environmental factors into mental and behavioural diagnostics, many practitioners and scientists believe that the most recent DSM's are misused. The symptom bias makes diagnosing quick and easier allowing for practitioners to increase their clientele because symptoms can be easier to classify and deal with than dealing with life or event histories which have evoked what may be a temporary and normal mental state in reaction to a patients environmental circumstances. \nThe easy-to-use manual not only has increased the perceived need for more mental health care, stimulating funding for mental health care facilities, but also has had a global impact on marketing strategies. Many pharmaceutical commercial ads list symptoms such as fatigue, depression, or anxiety. However, such symptoms are not necessarily abnormal, and are appropriate responses to such occurrences as the loss of a loved one. The targets of such ads in such cases do not need medication, and can naturally overcome their grief, but with such an advertising strategy pharmaceutical companies can greatly expand their marketing.\n\n\n"}
{"id": "17615632", "url": "https://en.wikipedia.org/wiki?curid=17615632", "title": "Our Plundered Planet", "text": "Our Plundered Planet\n\nOur Plundered Planet is a book published in 1948 that was written by Fairfield Osborn about environmental destruction by humankind. With a focus on soil, the book is a critique of humankind's poor stewardship of Earth. It typifies the earliest apocalyptic environmental literature, in which human beings are seen as destroyers of the natural world.\n\n\"Our Plundered Planet\", along with William Vogt’s \"Road to Survival\", also published in 1948, launched a Malthusian revival in the post War era, and would inspire Paul R. Ehrlich, author of \"The Population Bomb\" among many others.\n\nIn writing this book, Osborn was influenced by Guy I. Burch and Elmer Pendell’s overpopulation tract \"Population Roads to Peace or War\" (1945) and Paul Sears’ analysis of dust bowls in \"Deserts on the March\" (1935). He had also been influenced by various \"New Deal\" initiatives in the public planning of land use and restoration, such as the creation of the Tennessee Valley Authority, the Civilian Conservation Corps and various policies to address the “dust bowls” of the time. Osborn, as well as his famous father, Henry Fairfield Osborn, was also heavily influenced by the eugenics movement prior to the war.\n\n"}
{"id": "1018482", "url": "https://en.wikipedia.org/wiki?curid=1018482", "title": "Palomar Planet-Crossing Asteroid Survey", "text": "Palomar Planet-Crossing Asteroid Survey\n\nThe Palomar Planet-Crossing Asteroid Survey (PCAS) was an astronomical survey, initiated by American astronomers Eleanor Helin and Eugene Shoemaker at the U.S Palomar Observatory, California, in 1973. The program is responsible for the discovery of 95 near-Earth Objects including 17 comets, while the Minor Planet Center directly credits PCAS with the discovery of 20 numbered minor planets during 1993–1994. PCAS ran for nearly 25 years until June 1995. It had an international extension, INAS, and was the immediate predecessor of the outstandingly successful NEAT program.\n\nThe first NEO discovered by PACS was (5496) 1973 NA, an Apollo asteroid with an exceptional orbital inclination of 68°, the most highly inclined minor planet known until 1999. In 1976, Elenor Helin discovered 2062 Aten, the first of a new class of asteroids called the Aten asteroids with small orbits that are never far from Earth's orbit. As a result, these objects have a particularly high probability of colliding with the Earth. In 1979, Helin discovered an Apollo-type asteroid, that they later identified with the comet 4015 Wilson–Harrington. It was the first confirmation that a comet can evolve into an asteroid after it has degassed.\n\n\n"}
{"id": "51630875", "url": "https://en.wikipedia.org/wiki?curid=51630875", "title": "Paul Willis (science communicator)", "text": "Paul Willis (science communicator)\n\nPaul M. A. Willis is a palaeontologist, science communicator and former Director of the Royal Institution of Australia (RiAus).\n\nWillis studied zoology and geology at University of Sydney and went on to complete a PhD in palaeontology at the University of New South Wales . He has been a resident palaeontologist on ten Antarctic expeditions and has written or co-authored eight books on dinosaurs, rocks and fossils.\n\nWhile Willis found his first fossil when he was six, the earliest part of his collection was a small echinoid collected by his parents on their honeymoon . Willis completed a BSc at Sydney University in zoology and geology before conducting a PhD at the University of New South Wales. His Doctoral thesis was on The Phylogenetic Systematics of Australasian Crocodilians. Willis's doctoral studies resulted in the erection of several new taxa including the subfamily Mekosuchinae, and the genera Baru, Kambara, Australosuchus, Trilophosuchus and Harpacoshampsa. He conducted extensive field work, mostly in North Queensland, as well as completing a period as a paleontological Preparator at The Australian Museum. During this time Willis prepared an opalised skeleton dubbed by him as Eric the Plesiosaur and later named Umoonasaurus.\n\nIn 1997, Willis commenced a traineeship with the ABC. During his career in public broadcasting, Willis went on to report and present stories for \"Quantum\" and \"Catalyst\" on ABC and the series \"Monster Bug Wars\" on SBS. Willis produced over 350 stories for \"Catalyst\" during a ten-year stint with the program. He left his employment at the ABC to take up directorship of the Royal Institution of Australia in 2011 which ended in July 2017. He also worked for ABC radio, creating \"The Correx Files\" for Triple J, presenting numerous regular science talkback segments on various radio stations across the country and contributing to \"The Science Show\", \"Earthbeat\" and \"The Health Report.\". \n\nWillis is now freelancing science communications through his company Media Engagement Services as well as conducting some paleontological studies as an Adjunct Associate Professor at Flinders University.\n\nIn 2000 Willis was a joint recipient of the Eureka Prize for Science Communication and was voted Australian Skeptic of the Year in 2002 for working to counter pseudoscience.\n\n \n"}
{"id": "12943388", "url": "https://en.wikipedia.org/wiki?curid=12943388", "title": "Richard Keith Ellis", "text": "Richard Keith Ellis\n\nRichard Keith Ellis (born November 17, 1949) is a British theoretical physicist, at the University of Durham and a leading authority on perturbative quantum chromodynamics and collider phenomenology.\n\nHe graduated from the University of Oxford (MA 1971, D. Phil 1974). He has held positions at Imperial College, MIT, Caltech, CERN and the University of Rome.\n\nHe went to Fermilab in 1984. He was Head of the Theoretical Physics Department at Fermilab from 1993 to 2004. In 2015 he moved to the University of Durham in the UK, where he is a professor of Physics and Director of the Institute for Particle Physics Phenomenology.\n\nEllis' work is of importance to the study of elementary particles at colliders, such as the Fermilab Tevatron, and the CERN Large Hadron Collider.\nEllis has contributed in a substantial way to the interpretation\nof experiments performed at high energy.\nTogether with Douglas Ross and Tony Terrano he performed the \nfirst calculation of jet structure in e+e- annihilation which\nallowed precise determination of the strong coupling.\n\nIn addition, with Guido Altarelli and Guido Martinelli\nhe performed a calculation of lepton pair production\nwhich allow reconciliation of observed rates with theoretical\ncalculations.\nHe has also co-authored a number of widely read papers on the theory of heavy quark production.\nHe is also co-author for the parton-level Monte Carlo program MCFM.\n\nHe is the coauthor with W. J. Stirling and B. R. Webber of a book on QCD and collider physics published by Cambridge University Press in 1996.\n\nEllis was elected a Fellow of the American Physical Society in 1988 and a Fellow of the Royal Society of London in 2009. Also in 2009, Ellis together with John Collins and Davison Soper won the J. J. Sakurai Prize for Theoretical Particle Physics, \"For work in perturbative Quantum Chromodynamics, including applications to problems pivotal to the interpretation of high energy particle collisions.\" \n"}
{"id": "43864268", "url": "https://en.wikipedia.org/wiki?curid=43864268", "title": "SPEDAS", "text": "SPEDAS\n\nSPEDAS (Space Physics Environment Data Analysis Software) is an open-source data analysis tool intended for Space Physics users. It was developed using Interactive Data Language (IDL).\n\nSPEDAS is free software that can download and manipulate data from scientific space missions. It contains both a GUI (Graphical User Interface) and a command line mode for advanced users. It offers various tools for performing calculations and transformations of the data and for visualizing the results. Software modules can be developed for SPEDAS, extending its capabilities. It also includes a tool for downloading data from NASA servers using CDAWeb.\n\nSPEDAS evolved from software developed for the THEMIS mission, which was called TDAS (THEMIS Data Analysis Software). In turn, TDAS used IDL code developed previously for earlier missions going back to the 1990s.\n\nSPEDAS was developed by scientists and programmers of the UC Berkeley's Space Sciences Laboratory, UCLA's IGPP and other contributors.\n\nThree different types of SPEDAS deployment are available:\n\n\nOne of the main goals of SPEDAS is to accommodate the needs of different NASA missions. Towards this goal, its architecture is modular. Users can develop plugins for loading data, for configuration and for specialized calculations or operations on the data.\n\nVersion 1.00 of SPEDAS includes plugins for loading data from the following missions or data sets: \n\nPlugins for specialized calculations are: \n\n"}
{"id": "11978746", "url": "https://en.wikipedia.org/wiki?curid=11978746", "title": "Saint Petersburg Botanical Garden", "text": "Saint Petersburg Botanical Garden\n\nThe Saint Petersburg Botanical Garden, also known as the Botanic Gardens of the Komarov Botanical Institute or the Komarov Botanical Garden, is the oldest botanical garden in Russia. It consists of outdoor and indoor collections situated on Aptekarsky Island in Saint Petersburg and belongs to the Komarov Botanical Institute of the Russian Academy of Sciences. It is 18.9 ha in area, and is bordered by Aptekarsky Prospekt (main entrance), Prof. Popov Street (second entrance), as well as the embankments of the Karpovka and Bolshaya Neva rivers.\n\nThe garden was founded by Peter I in 1714 as a herb garden in order to grow medicinal plants and re-established as a botanical institution under the name Imperial Botanical Garden in 1823. Ivan Lepyokhin was in charge of the botanical garden from 1774 until 1802. Beginning in 1855, Eduard August von Regel was associated with the garden, first as Scientific Director and then as Director General (1875-1892). Regel had a particular fascination with the genus \"Allium\", overseeing collections of these plants in the Russian Far East and writing about them in two monographs. More than 60 of the alliums he identified bear his name, e.g., \"A. giganteum\" Regel and \"A. rosenbahianum\" Regel. Many alliums can be viewed in the Northern Yard of the garden. In 1897, \"Konstantin George Alexander Winkler\"(14 June 1848 - 3 February 1900), became head botanist at the Garden. He then reorganized the herbaria and greenhouse collections. Around 1900, Boris Fedtschenko became head botanist and he organised investigations of various Russian regions including Siberia, Caucusus, Middle Asia and Asiatic Russia. All published in various volumes and books.\n\nIn 1930, the garden became subordinate to the Academy of Sciences of the Soviet Union and, in 1931, was merged with the Botanical Museum into the Botanical Institute.\n\nThe garden has 25 greenhouses constructed in 1823-1824. They are numbered from 1 to 28 (No. 5 and No. 25 don't exist; No. 10 and No. 11 are shared). Some of them are open to the public (guided visits only), including the large collections of azaleas and other Ericaceae (No. 6), ferns (No. 15), cacti and other succulents (No. 16), various tropical plants (No. 18), the 23.5 m high Big Palm Greenhouse with an important collection of orchids (No. 26) and the greenhouse with a pond containing \"Victoria amazonica\" (no. 28). The night blossom of cactus \"Selenicereus grandiflorus\", cultivated there since 1857, is a celebrated event announced in mass media and open to the public in the 16th greenhouse in June-July. The indoor collections suffered significant losses during the Siege of Leningrad in 1941-1944; out of 6367 species only 861 survived.\n\nThe chain of greenhouses encircles the Southern Yard and the Northern Yard, the latter featuring an extensive outdoor collection of Iridaceae and bulbous plants, including many species of \"Allium\". The building of the botanical museum faces the Northern Yard in place of the non-existent greenhouse No. 5.\n\nThe outer park includes a small rock garden (constructed in the end of the 19th century) located in front of the Big Palm Greenhouse, and a 0.16 km² arboretum, organized partly as an English garden and partly as a formal garden. The park, unlike the greenhouses, is closed for visitors from October 1 to May 8. It is elevated only 1.5-3 m above sea-level and has thus regularly suffered from catastrophic floods characteristic of Saint Petersburg. The herbarium edifice built in 1913 stands in front of the main entrance.\n\n\n\n"}
{"id": "26798613", "url": "https://en.wikipedia.org/wiki?curid=26798613", "title": "Science Theatre", "text": "Science Theatre\n\nScience Theatre is an undergraduate student-run science outreach organization at Michigan State University's East Lansing campus. Science Theatre visits schools and events throughout Michigan performing interactive science demonstrations for K-12 students on-stage or up-close. Science Theatre performers are undergraduate and graduate student volunteers and all performances are made free of charge.\n\nThe group's performances consist of arrangements from its catalog of more than seventy demonstrations in biology, chemistry, and physics. Additionally, Science Theatre performs comprehensive shows in Astronomy, Environmental science, Pressure, the Periodic Table, Quantum Mechanics, and FRIB-related science.\n\nScience Theatre was founded in April 1991 under a grant from the National Science Foundation and received the 1993 AAAS Award for Public Understanding of Science and Technology. Science Theatre is a four-time winner of the Outreach Award from the Michigan State University Department of Physics and Astronomy.\n\n"}
{"id": "2491665", "url": "https://en.wikipedia.org/wiki?curid=2491665", "title": "Star clock", "text": "Star clock\n\nA star clock (or nocturnal) is a method of using the stars to determine the time. This is accomplished by measuring the Big Dipper's position in the sky based on a standard clock, and then employing simple addition and subtraction. This method requires no tools; others use an astrolabe and a planisphere.\n\nA clock's regulator can be adjusted so that it keeps the Mean Sidereal Time rate. When it is then set to an observer's Local Mean Sidereal Time then a star will transit the meridian (passing directly north or south) at the sidereal time of the star's Right Ascension.\n\n\n"}
{"id": "9843744", "url": "https://en.wikipedia.org/wiki?curid=9843744", "title": "Substantivism", "text": "Substantivism\n\nSubstantivism is a position, first proposed by Karl Polanyi in his work \"The Great Transformation\" (1944), which argues that the term 'economics' has two meanings. The formal meaning, used by today's neoclassical economists, refers to economics as the logic of rational action and decision-making, as rational choice between the alternative uses of limited (scarce) means, as 'economising,' 'maximizing,' or 'optimizing.'\n\nThe second, substantive meaning presupposes neither rational decision-making nor conditions of scarcity. It refers to how humans make a living interacting within their social and natural environments. A society's livelihood strategy is seen as an adaptation to its environment and material conditions, a process which may or may not involve utility maximisation. The substantive meaning of 'economics' is seen in the broader sense of 'provisioning.' Economics is the way society meets material needs.\n\n"}
{"id": "50535883", "url": "https://en.wikipedia.org/wiki?curid=50535883", "title": "THE GrEEK CAMPUS", "text": "THE GrEEK CAMPUS\n\nThe GrEEK Campus is the world’s first heritage-driven technology center. Not just a physical platform for entrepreneurs, startups, and SMEs, The GrEEK Campus is a catalyst for innovators, creators and GEEKs and a propeller of new-age startup value hubs. A platform home to aspiring entrepreneurs looking to thrive and grow in collaborative environments.\n\nA community built from ground up for creative individuals to meet, exchange, showcase ideas and work together towards homegrown technologies and innovations. Founded Dec 2013, The GrEEK Campus became the fastest growing community of resilient startups, venture capital representation hub and academic collaboration center of The Middle East.   \n\nA physical ecosystem designer and developer shaping the tech industry, hosting more than 140 companies, carefully selecting and bringing together the right mix of ecosystem players such as seed companies, angel investors, incubators and accelerators - enabling the evolution of our members towards successful entrepreneurial ventures and startup journeys. \n\nToday, THE GrEEK Campus is made up of five office buildings on a total space of 25,000 m2, and hosts regular high-profile technology seminars, conferences, open art exhibitions and music concerts, making it an event destination for the entire ecosystem. The campus is just the beginning of a compelling journey that will feature more campuses across town, inside, and outside of Egypt. \n\nThe GrEEK Campus is Cairo's first technology and innovation park, offering state-of-the-art office spaces in Downtown Cairo, Egypt. In 1964, The GrEEK Campus was formed by former president Dr. Thomas A. Bartlett of the American University in Cairo (AUC). When AUC began its relocation to another larger campus on the outskirts of Cairo - and in the outcome of the events of June 2013 – as the campus was left virtually abandoned since Tahrir 2011, Ahmed El Alfi decided to take over The Greek Campus and bring it back to life - more than it ever has been.\n\nThe concept and vision for The GrEEK Campus wasn’t just about offering traditional commercial work spaces and offices to businesses. It was more about building a community/an ecosystem from the ground up into a campus for techpreneurs, enthusiasts and creative individuals to meet, exchange and showcase their ideas and work together towards new technology and innovation that would ultimately benefit Egypt and the world.\n\nThe GrEEK CAMPUS was a part of American University for 50 years. In 2013, former president of AUC Lisa Anderson signed an agreement with Tahrir Alley Technology Park, a Cairo-based company, who leased the property to run a Technology Park. The concept and vision were crafted by Ahmed El Alfi and the technology park was officially launched in 2014. \n\nThe GrEEK CAMPUS offers technologically advanced office spaces and promotes innovation and entrepreneurship.\n\n"}
{"id": "15919867", "url": "https://en.wikipedia.org/wiki?curid=15919867", "title": "UCI School of Biological Sciences", "text": "UCI School of Biological Sciences\n\nThe School of Biological Sciences is one of the academic units of the University of California, Irvine (UCI). The school is divided into four departments: developmental and cell biology, ecology and evolutionary biology, molecular biology and biochemistry, and neurobiology and behavior. With over 3,700 students it is in the top four largest schools in the university.\nIt is consistently ranked in the top one hundred in \"U.S. News & World Report’s\" yearly list of best graduate schools.\n\nThe School of Biological Sciences first opened in 1965 at the University of California, Irvine and was one of the first schools founded when the University campus opened. The school's founding Dean, Edward A. Steinhaus, had four founding department chairs and started out with 17 professors.\n\nOn March 12, 2014, the School was officially renamed after UCI professor and donor Francisco J. Ayala by then-Chancellor Michael V. Drake. Ayala had previously pledged to donate $10 million to the School of Biological Sciences in 2011. The school reverted to its previous name in June 2018, after a university investigation confirmed that Ayala had sexually harassed at least four women colleagues and graduate students.\n"}
{"id": "2622712", "url": "https://en.wikipedia.org/wiki?curid=2622712", "title": "VTK", "text": "VTK\n\nThe Visualization Toolkit (VTK) is an open-source, freely available software system for 3D computer graphics, image processing and visualization. VTK consists of a C++ class library and several interpreted interface layers including Tcl/Tk, Java, and Python. Kitware, whose team created and continues to extend the toolkit, offers professional support and consulting services for VTK. VTK supports a wide variety of visualization algorithms including: scalar, vector, tensor, texture, and volumetric methods; and advanced modeling techniques such as: implicit modeling, polygon reduction, mesh smoothing, cutting, contouring, and Delaunay triangulation. VTK has an extensive information visualization framework, has a suite of 3D interaction widgets, supports parallel processing, and integrates with various databases and GUI toolkits such as Qt and Tk. VTK is cross-platform and runs on Linux, Windows, Mac and Unix platforms. At its core VTK is implemented as a C++ toolkit, requiring users to build applications by combining various objects into an application. The system also supports automated wrapping of the C++ core into Python, Java and Tcl, so that VTK applications may also be written using these interpreted programming languages.\n\nVTK is used worldwide in commercial applications, research and development, and is the basis of many advanced visualization applications such as: Molekel, ParaView, VisIt, VisTrails, MOOSE, 3DSlicer, MayaVi, and OsiriX.\n\nVTK is an open-source toolkit licensed under the BSD license.\n\nVTK was initially created in 1993 as companion software to the book \"The Visualization Toolkit: An Object-Oriented Approach to 3D Graphics\" published by Prentice-Hall. The book and software were written by three researchers (Will Schroeder, Ken Martin and Bill Lorensen) on their own time and with permission from GE (thus the ownership of the software resided with, and continues to reside with, the authors). After the core of VTK was written, users and developers around the world began to improve and apply the system to real-world problems. In particular, GE Medical Systems and other GE businesses graciously contributed to the system. Some researchers, such as Penny Rheinghans began to teach with the book. Other early supporters included Jim Ahrens at Los Alamos National Labs, and unnamed, but generous oil and gas supporters. In recent years, Sandia National Labs have been strong supporters and co-developers with particular focus on adding information visualization to VTK.\n\nTo support what was becoming a large, active and worldwide VTK community Ken and Will, along with Lisa Avila, Charles Law and Bill Hoffman left GE Research to found Kitware Inc. in 1998. Since that time, hundreds of additional developers have created what is now the premier visualization system in the world today.\n\nWith the founding of Kitware, the VTK community grew rapidly, and toolkit usage expanded into academic, research and commercial applications. For example, VTK forms the core of the 3DSlicer biomedical computing application, and numerous research papers at IEEE Visualization and other conferences based on VTK have appeared. VTK has been used on a large 1024-processor computer at the Los Alamos National Laboratory to process nearly a Petabyte of data. In 2005, ParaView (based on VTK) was used for real-time rendering of a ZSU-23-4 Russian Anti-Aircraft vehicle being hit by a planar wave, with 2.5 billion cell calculation, in the United States Army Research Laboratory. VTK also forms the basis of several collaborations between Kitware and national organizations such as Sandia, Los Alamos, and Livermore National Labs, who are using VTK as the foundation for their large data visualization needs.\n\nVTK is also one of the key computing tools for the recently established National Alliance for Medical Image Computing, NA-MIC (), part of NIH's roadmap initiative for future computing tools.\n\nRecently work on VTK includes a significant expansion of the toolkit to support the ingestion, processing and display of informatics data. This work is supported by Sandia National Laboratories under the 'Titan' project and represents one of the first concentrated efforts to unify scientific visualization with informatics functionality.\n\n\n\n\n\n"}
