{"id": "3852404", "url": "https://en.wikipedia.org/wiki?curid=3852404", "title": "Acoustic holography", "text": "Acoustic holography\n\nAcoustic holography is a method for estimating the sound field near a source by measuring acoustic parameters away from the source by means of an array of pressure and/or particle velocity transducers. The Measuring techniques included in acoustic holography are becoming increasingly popular in various fields, most notably those of transportation, vehicle and aircraft design, and noise, vibration, and harshness (NVH). The general idea of acoustic holography has led to different versions such as near-field acoustic holography (NAH) and statistically optimal near-field acoustic holography (SONAH).\nFor audio rendition, the wave field synthesis is the most related procedure.\n\n\n"}
{"id": "31636552", "url": "https://en.wikipedia.org/wiki?curid=31636552", "title": "Andreas Albrecht (cosmologist)", "text": "Andreas Albrecht (cosmologist)\n\nAndreas J. Albrecht is a theoretical physicist and cosmologist who is a professor and chair of the Physics Department at the University of California, Davis. He is one of the founders of inflationary cosmology and studies the formation of the early universe, cosmic structure, and dark energy.\n\nAlbrecht graduated in 1979 from Cornell University, Ithaca and was awarded a doctorate in 1983 at University of Pennsylvania, Philadelphia on cosmology. His thesis advisor was Paul Steinhardt.\n\nHe later carried out post-doctoral research at University of Texas, Austin and at Los Alamos National Laboratory. Albrecht later worked at Fermilab from 1987 to 1992 and subsequently taught at Imperial College, London from 1992 to 1998.\n\nAlbrecht is a Fellow of the American Physical Society and Fellow of the Institute of Physics (UK).\n\nTogether with his thesis advisor, Albrecht developed New Inflation, solving the bubble collision problem of Alan Guth's original model of inflation. Later, Albrecht studied the observable effects of cosmic topological defects, contributing to ruling out cosmic strings as the dominant mechanism for structure formation.\n\nAlong with João Magueijo, Albrecht independently proposed a model of varying speed of light cosmology which posits that the speed of light in the early universe was a trillion times faster in order to explain the horizon problem of cosmology.\n\nIn the 21st century, Albrecht worked on quantum mechanics, as well as probability and quantum theory.\n\n\n"}
{"id": "1367992", "url": "https://en.wikipedia.org/wiki?curid=1367992", "title": "Anthrobotics", "text": "Anthrobotics\n\nAnthrobotics is the science of developing and studying robots that are either entirely or in some way human-like.\n\nThe term \"anthrobotics\" was originally coined by Mark Rosheim in a paper entitled \"Design of An Omnidirectional Arm\" presented at the IEEE International Conference on Robotics and Automation, May 13–18, 1990, pp. 2162–2167. Rosheim says he derived the term from \"...Anthropomorphic and Robotics to distinguish the new generation of dexterous robots from its simple industrial robot forebears.\" The word gained wider recognition as a result of its use in the title of Rosheim's subsequent book \"Robot Evolution: The Development of Anthrobotics\", which focussed on facsimiles of human physical and psychological skills and attributes.\n\nHowever, a wider definition of the term \"anthrobotics\" has been proposed, in which the meaning is derived from \"anthropology\" rather than \"anthropomorphic\". This usage includes robots that respond to input in a human-like fashion, rather than simply mimicking human actions, thus theoretically being able to respond more flexibly or to adapt to unforeseen circumstances. This expanded definition also encompasses robots that are situated in social environments with the ability to respond to those environments appropriately, such as insect robots, robotic pets, and the like.\n\nAnthrobotics is now taught at some universities, encouraging students not only to design and build robots for environments beyond current industrial applications, but also to speculate on the future of robotics that are embedded in the world at large, as mobile phones and computers are today. In 2016 philosopher Luis de Miranda created the Anthrobotics Cluster at the University of Edinburgh \"a platform of cross-disciplinary research that seeks to investigate some of the biggest questions that will need to be answered\" on the relationship between humans, robots and intelligent systems and \"a think tank on the social spread of robotics, and also how automation is part of the definition of what humans have always been\". to explore the symbiotic relationship between humans and automated protocols.\n\n"}
{"id": "16491262", "url": "https://en.wikipedia.org/wiki?curid=16491262", "title": "Astakhov Glacier", "text": "Astakhov Glacier\n\nAstakhov Glacier () is the glacier next south of Chugunov Glacier in the Explorers Range, Bowers Mountains. It flows northeast from Mount Hager and enters Ob' Bay just west of Platypus Ridge, the glacier is situated in Victoria Land, Antarctica. It was mapped by the United States Geological Survey from surveys and from U.S. Navy air photos, 1960–65, and named by the Advisory Committee on Antarctic Names for Petr Astakhov, Soviet exchange scientist at the U.S. South Pole Station in 1967. The glacier lies on the Pennell Coast, a portion of Antarctica lying between Cape Williams and Cape Adare.\n\n"}
{"id": "29333396", "url": "https://en.wikipedia.org/wiki?curid=29333396", "title": "Brandau Glacier", "text": "Brandau Glacier\n\nBrandau Glacier () is a wide tributary glacier, long, flowing westward from an ice divide between Haynes Table and Husky Heights to enter Keltie Glacier just west of Ford Spur. It was named by the Advisory Committee on Antarctic Names for Lieutenant Commander James F. Brandau, U.S. Navy, a pilot with Squadron VX-6, Operation Deepfreeze 1964 and 1965.\n\n"}
{"id": "11958485", "url": "https://en.wikipedia.org/wiki?curid=11958485", "title": "Britton-Robinson buffer", "text": "Britton-Robinson buffer\n\nBritton–Robinson buffer (aka BRB aka PEM) is a \"universal\" pH buffer used for the range pH 2 to pH 12. Universal buffers consist of mixtures of acids of diminishing strength (increasing pK) so that the change in pH is approximately proportional to the amount of alkali added. It consists of a mixture of 0.04 M HBO, 0.04 M HPO and 0.04 M CHCOOH that has been titrated to the desired pH with 0.2 M NaOH. Britton and Robinson also proposed a second formulation that gave an essentially linear pH response to added alkali from pH 2.5 to pH 9.2 (and buffers to pH 12). This mixture consists of 0.0286 M citric acid, 0.0286 M KHPO, 0.0286 M HBO, 0.0286 M veronal and 0.0286 M HCl titrated with 0.2 M NaOH.\n\nThis was invented in 1931 by the English chemist Hubert Thomas Stanley \"Kevin\" Britton (1892–1960) with the New Zealand chemist Robert Anthony Robinson (1904-1979).\n\n"}
{"id": "43605222", "url": "https://en.wikipedia.org/wiki?curid=43605222", "title": "COART", "text": "COART\n\nCOART (Coupled Ocean-Atmospheric Radiative Transfer code) - COART is established on the Coupled DIScrete Ordinate Radiative Transfer (Coupled DISORT or CDISORT) code, developed from DISORT. It is designed to simulate radiance (including water-leaving radiance) and irradiance (flux) at any levels in the atmosphere and ocean consistently.\n\n\nJin, Z., T.P. Charlock, K. Rutledge, K. Stamnes, and Y. Wang, An analytical solution of radiative transfer in the coupled atmosphere-ocean system with rough surface. Appl. Opt., 45, 7443-7455, 2006.\n\nJin, Z., and K. Stamnes, Radiative transfer in nonuniformly refracting layered media: atmosphere-ocean system, Appl. Opt., 33, 431-442, 1994.\n\n"}
{"id": "2196050", "url": "https://en.wikipedia.org/wiki?curid=2196050", "title": "Castle Romeo", "text": "Castle Romeo\n\nCastle Romeo was the code name given to one of the tests in the Operation Castle series of American nuclear tests. It was the first test of the TX-17 thermonuclear weapon (initially the \"emergency capability\" EC-17), the first deployed thermonuclear bomb.\n\nThe so-called \"runt\" device was a weaponized \"dry\" fusion bomb, using lithium deuteride fuel for the fusion stage of a \"staged\" fusion bomb, unlike the cryogenic liquid deuterium of the first-generation Ivy Mike fusion device.\n\nSimilar to the \"Shrimp\" device tested shortly before, in the Castle Bravo test, it differed from that device in using lithium deuteride derived from natural lithium (a mixture of lithium-6 and lithium-7 isotopes, with 7.5% of the former) as the source of the tritium and deuterium fusion fuels, as opposed to the relative high enrichment level of lithium (approximately 40% lithium-6) deuteride used in Bravo.\n\nIt was detonated on March 27, 1954, after several delays (which played havoc with the planned experimental measurements program) at Bikini Atoll of the Marshall Islands, on a barge moored in the middle of the crater from the Castle Bravo test. It was the first such barge-based test, a necessity that had come about because the powerful thermonuclear devices completely obliterated the small islands following detonation.\n\nLike the Bravo test, it \"ran away\" and produced far more than its predicted yield, and for the same reason—an unexpected participation of the common lithium-7 isotope in fusion reactions. Although it had been predicted to produce a yield of 4 megatons with a range of 1.5 to 7 megatons (before the results of the Bravo test caused an upgrade in the estimates, it had originally been estimated to produce 3–5 megatons), it actually produced a yield of 11 megatons, the third-largest test ever conducted by the U.S.\n\nLike the Ivy Mike and Castle Bravo tests, a large percentage of the yield was produced by fast fission of the natural uranium \"tamper\"; 7 megatons of the yield were from this source.\n\nThis became the first air-droppable thermonuclear device, the EC-17, of which only 5 were made and the first deployable staged radiation implosion Teller-Ulam thermonuclear weapon. This evolved into the Mk 17 of which 200 were made. Both of these were huge devices, weighing 39,000 and 42,000 pounds respectively. As a result, they were only capable of being carried by the B-36. They were also some of the largest yield devices deployed by Strategic Air Command—the EC-17 giving around 10 Mt and the Mk17 between 11 and 15 Mt. They were all out of service by August 1957.\n\nOne particular image of the Castle Romeo fireball (above, at right) has been one of the most highly reprinted images of a nuclear explosion, often serving as a stand-in for nuclear weapons in general for news stories, book covers, magazine articles, and even congressional reports (such as the Cox Report), likely because of its threatening appearance and extreme red, orange, and yellow hues. In many cases the fact that the explosion is of a US megaton-range weapon has not prevented it to be used to represent the arsenals of other states or weapons of far lower yields, which would have a very different appearance.\n\nOne prominent usage is as the backdrop for heavy metal band Megadeth's greatest hits compilation \"\". The image of Castle Romeo was also used on the cover of the New York hardcore music pioneers Cro-Mags debut album \"The Age of Quarrel\". It is also featured on the title screen of Team17's turn-based artillery game \"Worms Armageddon\".\n\nThe Castle Romeo photos are sometimes confused with that of Castle Bravo; the two nuclear blasts looked very similar, and they were both conducted in the same location.\n\n\n"}
{"id": "10692807", "url": "https://en.wikipedia.org/wiki?curid=10692807", "title": "Circumscribed halo", "text": "Circumscribed halo\n\nA circumscribed halo is a type of halo, an optical phenomenon typically in the form of a more or less oval ring that circumscribes the circular 22° halo centred on the sun or moon. \nAs the sun rises above 70° it essentially covers the 22° halo. Like many other halos, it is slightly reddish on the inner edge, facing the sun or moon, and bluish on the outer edge.\n\nThe shape of the circumscribed halo is strongly dependent on the distance of the sun or moon above the horizon. Its top and bottom (i.e., the points directly below and above the sun or moon) always lie directly tangential to the 22° halo, but its left and right sides take on different shapes depending on solar (or lunar) elevation. At an elevation between about 35°–50°, the sides form two distinct, downward-drooping \"lobes\" outside of the 22° halo. As the sun or moon rises higher (between c. 50°–70°), the drooping diminishes towards a more regular oval shape. At an elevation of c. 70° or more, the shape of the circumscribed halo approaches a circle, and as such becomes nearly indistinguishable from the 22° halo, only to be identified by its tendency to show more saturated colors than the latter. When the sun or moon is at an elevation lower than c. 35°, the circumscribed halo breaks up into the upper tangent and lower tangent arcs.\n\n"}
{"id": "14683289", "url": "https://en.wikipedia.org/wiki?curid=14683289", "title": "Co-production (society)", "text": "Co-production (society)\n\nThere are many definitions of co-production. One technical definition is that Co-production is where technical experts and other groups co-generate new knowledge and technologies. It is the dynamic interaction between technology and society. It has a long history, particularly arising out of radical theories of knowledge in the 1970s.\n\nCo-production forms part of Mode 2, a term used in the sociology of science to describe one of the modes, or ways that knowledge is formed. In Mode 2, science and technology studies the move from extreme technological determinism and social constructivism, to a more systemic understanding of how technology and society ‘co-produce’ each other. Co-production is functionally comparable to the concepts of causality loop, positive feedback, and co-evolution – all of which describe how two or more variables of a system affect and essentially create each other, albeit with respect to different variables operating at different scales. And as with these other concepts, if used too broadly/uncritically, co-production risks noetic flatness – if technology and society co-produce each other equally, the justification for maintaining the boundary between them dissolves (in which case actor-network theory may be invoked). Unless overlapping sets of boundary-work are employed, co-production may also fail to account for power differentials within each variable, (in this case, within technology and society).\n\nFrom a more STS perspective, Sheila Jasanoff, has written that \"Co-production is shorthand for the proposition that the ways in which we know and represent the world (both nature and society) are inseparable from the ways in which we chose to live in.\" Co-production draws on constitutive (such as Actor–network theory) and interactional work (such as the Edinburgh School) in STS. As a sensitizing concept, the idiom of co-production looks at four themes: \"the emergence and stabilization of new techno-scientific objects and framings, the resolution of scientific and technical controversies; the processes by which the products of techno-science are made intelligible and portable across boundaries; and the adjustment of science’s cultural practices in response to the contexts in which science is done.\" Studies employing co-production often follow the following pathways: \"making identities, making institutions, making discourses, and making representations\"\n"}
{"id": "12791352", "url": "https://en.wikipedia.org/wiki?curid=12791352", "title": "Columbia–Chicago School of Economics", "text": "Columbia–Chicago School of Economics\n\nThe Columbia–Chicago School of Economics refers to a group of economists at Columbia University and the University of Chicago that helped to develop the empirical foundations of human capital theory, consequently revolutionizing the field of labor economics.\n\nLeading members of the group include Jacob Mincer and Nobel Laureate Gary Becker.\n"}
{"id": "28117700", "url": "https://en.wikipedia.org/wiki?curid=28117700", "title": "Coulomb excitation", "text": "Coulomb excitation\n\nCoulomb excitation is a technique in experimental nuclear physics to probe the electromagnetic aspect of nuclear structure. In coulomb excitation, a nucleus is excited by an inelastic collision with another nucleus through the electromagnetic interaction. In order to ensure that the interaction is electromagnetic in nature — and not nuclear — a \"safe\" scattering angle is chosen.\nThis method is particularly useful for investigating collectivity in nuclei, as collective excitations are often connected by electric quadrupole transitions.\n\n"}
{"id": "362546", "url": "https://en.wikipedia.org/wiki?curid=362546", "title": "Crivitz (crater)", "text": "Crivitz (crater)\n\nCrivitz is an impact crater on Mars. It is named after the small town of Crivitz in western Mecklenburg-Vorpommern, Germany. The crater has a diameter of 6.1 kilometers and is located at 14.5 deg south and 174.7 deg east, within the larger crater Gusev.\n\nThe name was proposed by Stephan Gehrke in 2002, at the time a research associate of the TU Berlin working on cartographic software and a visiting researcher at the United States Geological Survey. \n\n \n"}
{"id": "7354309", "url": "https://en.wikipedia.org/wiki?curid=7354309", "title": "Cyanophage", "text": "Cyanophage\n\nA cyanophage is a virus that infects cyanobacteria. The name is constructed from the term phage, for a virus that targets bacteria, and cyanobacteria for the specific type of bacteria this virus uses for replication. Because of the important role of cyanobacteria as primary producers in the world's oceans, the study of their phage ecology is important toward understanding global carbon cycling.\n\nThe first cyanophage, LPP-1, was discovered in 1963. Cyanophages are classified within the bacteriophage families \"Myoviridae\" (e.g. AS-1, N-1), \"Podoviridae\" (e.g. LPP-1) and \"Siphoviridae\" (e.g. S-1).\n\n\n"}
{"id": "56249865", "url": "https://en.wikipedia.org/wiki?curid=56249865", "title": "Dehalogenimonas formicexedens", "text": "Dehalogenimonas formicexedens\n\nDehalogenimonas formicexedens is a Gram-negative, strictly anaerobic and non-spore-forming bacterium from the genus of \"Dehalogenimonas\" which has been isolated from contaminated groundwater in Louisiana in the United States.\n\n \n"}
{"id": "18328784", "url": "https://en.wikipedia.org/wiki?curid=18328784", "title": "Digital Life (magazine)", "text": "Digital Life (magazine)\n\nDigital Life (previously \"Unwired\") was a monthly lifestyle and consumer technology magazine aimed at South African consumers published by technology media house ITWeb. The magazine was started in 2006 and the first issue appeared in October 2006. The magazine ceased publication with the June/July 2010 issue.\n\n"}
{"id": "26400029", "url": "https://en.wikipedia.org/wiki?curid=26400029", "title": "Discoverability", "text": "Discoverability\n\nDiscoverability is the degree to which of something, especially a piece of content or information, can be found in a search of a file, database, or other information system. Discoverability is a concern in library and information science, many aspects of digital media, software and web development, and in marketing, since something (e.g., a website, product, service, etc.) cannot be used if people cannot find it or do not understand what it can be used for. Metadata, or \"information about information,\" such as a book's title, a product's description, or a website's keywords, affects how discoverable something is on a database or online. In the 2010s, adding metadata to a product that is available online can make it easier for end users to find the product. For example, if a song file is made available online, making the title, name of the band, genre, year of release, and other pertinent information available in connection with this song file will make it easier for users to find this song file. Organizing information by putting it into alphabetical order or including it in a search engine is an example of how to improve discoverability. Discoverability is related to, but different from, accessibility and usability, other qualities that affect the usefulness of a piece of information.\nThe concept of \"discoverability\" in an information science and online context is a loose borrowing from the concept of the similar name in the legal profession. In law, \"discovery\" is a pre-trial procedure in a lawsuit in which each party, through the law of civil procedure, can obtain evidence from the other party or parties by means of discovery devices such as a request for answers to interrogatories, request for production of documents, request for admissions and depositions. Discovery can be obtained from non-parties using subpoenas. When a discovery request is objected to, the requesting party may seek the assistance of the court by filing a motion to compel discovery.\n\nThe usability of any piece of information directly relates to how discoverable it is, either in a \"walled garden\" database or on the open Internet. The quality of information available on this database or on the Internet depends upon the quality of the meta-information about each item, product, or service. In the case of a service, because of the emphasis placed on service reusability, opportunities should exist for reuse of this service. However, reuse is only possible if information is discoverable in the first place. To make items, products, and services discoverable, the following set of activities need to be performed:\n\n\nRegarding number 2, storing the information in a searchable repository: while technically a human-searchable repository, such as a printed paper list would qualify, in the 2010s, \"searchable repository\" is usually taken to mean a computer-searchable repository, such as a database that a human user can search using some type of search engine or \"find\" feature. Number 3 further supports this analysis of number 2, because while reading through a printed paper list by hand might be feasible in a theoretical sense, it is not time and cost-efficient in comparison with computer-based searching.\n\nApart from increasing the reuse potential of the services, discoverability is also required to avoid development of solution logic that is already contained in an existing service. To design services that are not only discoverable but also provide interpretable information about their capabilities, the service discoverability principle provides guidelines that could be applied during the service-oriented analysis phase of the service delivery process.\n\nIn relation to audiovisual content, according to the meaning given by the Canadian Radio-television and Telecommunications Commission (CRTC) for the purpose of its 2016 Discoverability Summit, discoverability can be summed up to the intrinsic ability of given content to \"stand out of the lot\", or to position itself so as to be easily found and discovered. A piece of audiovisual content can be a movie, a TV series, music, a book (eBook), an audio book or podcast. When audiovisual content such as a digital file for a TV show, movie, or song, is made available online, if the content is \"tagged\" with identifying information such as the names of the key artists (e.g., actors, directors and screenwriters for TV shows and movies; singers, musicians and record producers for songs) and the genres (e.g., for movies genres such as action, drama or comedy; for songs, genres such as heavy metal music, hip hop music, etc.), this makes it easier for end users to find the content they are interested in.\n\nIn the 2010s, when users interact with online content, algorithms typically determine what types of content the user is interested in, and then a computer program suggests \"more like this\", which is other content that the user may be interested in. Different websites and systems have different algorithms, but one approach, used by Amazon for its online store, is to indicate to a user, once the user searches for or looks at content/product \"x\" that \"other users who purchased \"x\" also purchased the following items\". This example is oriented around online purchasing behaviour, but an algorithm could also be programmed to provide suggestions based on other factors (e.g., searching, viewing, etc.). \n\nIn the 2010s, discoverability is typically referred to in connection with search engines. A highly \"discoverable\" piece of content (e.g., a certain movie) would be a movie that appears at the top, or near the top of a user's search results. A related concept is the role of \"recommendation engines\", which are computer programs which give a user recommendations based on his/her online activity. In the 2010s, \"discoverabilty\" applies to desktop and laptop computers and do the increasingly widening range of devices that can access the Internet, including various console video game systems and mobile devices such as tablets and smartphones. When organizations make an effort to promote certain content (e.g., a TV show, film, song, or video game), they can use \"traditional marketing\" (billboards, TV ads, radio ads) and digital ads (pop-up ads, pre-roll ads, etc.), or a mix of traditional and digital marketing.\n\nEven before the user’s intervention by searching for a certain content or type of content, discoverability is the prime factor which contributes to whether a piece of audiovisual content will be likely to be found in the various digital modes of content consumption. As of 2017, modes of searching include looking on Netflix for movies, Spotify for music, Audible for audio books, etc., although the concept can also more generally be applied to content found on Twitter, Tumblr, Instagram and other websites. It involves more than a content’s mere presence on a given platform; it can involve associating this content with \"keywords\" (tags), search algorithms, positioning within different categories, metadata, etc. Thus, discoverability enables as much as it promotes. For audiovisual content broadcast or streamed on digital media using the Internet, discoverability includes the underlying concepts of information science and programming architecture, which are at the very foundation of the search for a specific product, information or content.\n\nWithin a specific webpage or software application (\"app\"), the discoverability of a feature, content or link depends on a range of factors, including the size, colour, highlighting features, and position within the page. When colour is used to communicate the importance of a feature or link, designers typically use other elements as well, such as shadows or bolding, for individuals who cannot see certain colours. Just as traditional paper printing created other physical locations that stood out, such as being \"above the fold\" of a newspaper versus \"below the fold\", a web page or app's screenview may have certain locations that give features additional visibility to users, such as being right at the bottom of the web page or screen. \n\nThe positional advantages or disadvantages of various locations depend on different cultures and languages (e.g., left to right vs. right to left). Some locations have become established, such as having toolbars at the top of a screen or webpage. Some designers have argued that commonly used features (e.g., a print button) should be much more visually prominent than very rarely used features. Some features cannot be seen, but there is a convention that if the user places the mouse cursor in a certain area, then a toolbar or function option will become visible. In general, because of the smaller screen of mobile devices, controls are often not placed right in the centre of the screen, because that is where the user views content or text.\n\nSome organizations try to increase the discoverability of a certain feature by adding animation, such as a moving \"click here\" icon. As of 2017, the addition of motion sensors and geotracking to mobile devices has made webpage design for discoverability more complex, because smartphones and tablets are typically capable of having many more inputs from the user than a 1980s era desktop, including \"swiping\" the touchscreen, touching images on the screen, or tilting the device. One of the challenges in webpage and app design is that the degree of sophistication and experience of users with navigating in the webpage or app environment varies a great deal, from individuals who are new to using these applications at one extreme to experienced computer users.\n\nFor items that are searched for online, the goal of discoverability is to be at or near the top of the search results. Organizations may make efforts to make it more likely that \"their\" content or webpages are at the top, or close to the top, of search results; these approaches are often collectively called search engine optimization (SEO). Note that when an organization takes action to increase the SEO of its website, this does not normally involve changes to the search engine itself; rather, it involves adding metadata tags and original content, among other strategies, to increase the \"visibility\" of the website to search engine algorithms.\n\nIn a service delivery context, the application of this principle requires collecting information about the service during the service analysis phase as during this phase; maximum information is available about the service’s functional context and the capabilities of the service. At this stage, the domain knowledge of the business experts could also be enlisted to document meta-data about the service. In the service-oriented design phase, the already gathered meta-data could be made part of the service contract. The OASIS SOA-RM standard specifies \"service description\" as an artifact that represents service meta-data.\n\nTo make the service meta-data accessible to interested parties, it must be centrally accessible. This could either be done by publishing the service-meta to a dedicated 'service registry' or by simply placing this information in a 'shared directory'. In case of a 'service registry', the repository can also be used to include QoS, SLA and the current state of a service.\n\nThis is the basic type of meta-information that expresses the functional context of the service and the details about the product, content, or service’s capabilities. The application of the standardized service contract principle helps to create the basic functional meta-data in a consistent manner. The same standardization should be applied when the same meta-information is being outside the technical contract of the service e.g. when publishing information to a service registry.\n\nFor general items, the data that might be used to categorize them may include:\n\nFor services, to know about the service behavior and its limitations, and about the user's service experience, all of this information needs to be documented within the service registry. This way potential consumers can use this meta-information by comparing it against their performance requirements.\n\nThe effective application of this design principle requires that the meta-information recorded against each service needs to be consistent and meaningful. This is only possible if organization-wide standards exist that enforce service developers to record the required meta-data in a consistent way. The information recorded as the meta-data for the service needs to be presented in a way so that both technical and non-technical IT experts can understand the purpose and the capabilities of the service, as an evaluation of the service may be required by the business people before the service is authorized to be used.\n\nThis principle is best applied during the service-oriented analysis phase as during this time, all the details about the service’s purpose and functionality are available. Although most of the service design principles support each other in a positive manner, however, in case of service abstraction and service discoverability principle, there exists an inversely proportional relationship. This is because as more and more details about the service are hidden away from the service consumers, less discoverable information is available for discovering the service. This could be addressed by carefully recording the service meta-information so that the inner workings of the service are not documented within this meta-information.\nIn the online economy, sophisticated computer programs called algorithms analyse the ways that end users search for, access and use different content or products online. Thus, not only is metadata created regarding the content or product (e.g., the author and genre of an e-book), but also data is generated about specific human users' interaction with this content. If an organization such as a social media website has a user profile for a given person, indicating demographic information (e.g., age, gender, location of residence, employment status, education, etc.), then this social media website can collect and analyse information about tendencies of a given user or a given subcategory of users. When social media websites are collecting data about human users' online activities and preferences, this may raise potential privacy concerns. \n\nAlgorithms have been called “black boxes”, because the factors used by the leading websites in their algorithms are typically proprietary information which is not released to the public. While a number of search engine optimization (SEO) firms offer the services of attempting to increase the ranking of a client’s web content or website, these SEO firms do not typically know the exact algorithms used by Google and Facebook. Ghosh et al found that algorithm \"web crawler\"s can only access 26% of new online content \"...by recrawling a constant fraction of the entire web\". \n\nIn the 2010s, one concern raised with the increasing role of algorithms on search engines and databases, is that once a specific person indicates a preference for a certain type of content or product, the computer algorithm may increasingly focus on making recommendations in this type of content. To give a practical example, if a person searches for comedy movies online, a search engine algorithm may start mainly recommending comedies to this user, and not showing him or her the range of other films (e.g., drama, documentary, etc.). On the positive side, if this person only likes comedy films, then this restricted \"filter\" will reduce the information load of scanning through vast numbers of films. However, various cultural stakeholders have raised concerns about how these filter algorithms may restrict the diversity of material that is discoverable to users. Concerns about the dangers of \"filter bubbles\" have been raised in regards to online news services, which provide types of news, news sources, or topics to a user based on his/her previous online activities. Thus a person who has previously searched for Fox TV content will mainly be shown more Fox TV content and a person who has previously searched for PBS content will be shown more PBS search results, and so on. This could lead to news readers becoming only aware of a certain news source's viewpoints.\n\nThe search behaviour of video content viewers has changed a great deal since the widespread popularity of video sharing websites and video streaming. Whereas a typical TV show consumer of the 1980s would read a print edition of \"TV Guide\" to find out what shows were on, or click from channel to channel (\"channel surfing\") to see if any shows appealed to them, in the 2010s, video content consumers are increasingly watching on screens (either smart TVs, tablet computer screens or smartphones) that have a computerized search function and often automated algorithm-created suggestions for the viewer. With this search function, a user can enter the name of a TV show, producer, actor, screenwriter or genre to help them find content of interest to them. If the user is searching on a search engine on a device (laptop, tablet computer, smartphone) they own, the device may transmit information about the user's preferences and previous online searches to the website. Continuing with this 1980s to 2010s comparison, in the 1980s, the type or brand of television a user was watching on did not affect his/her viewing habits. However, a person searching for TV shows in the 2010s on different brands of computerized smart TVs will probably get different search results for the same search term.\n\nFor organizations that are trying to get maximal user uptake of their product, content or service online, discoverability has become an important goal. However, achieving discovery does not automatically translate into market success. For example, if the hypothetical online game \"xyz\" is easily discoverable, but it will not function on most mobile devices, then this video game will not perform well in the mobile game market, despite it being at the top of search results. As well, even if the product functions, that is it runs or plays properly, as well, users may not like the product. \n\nIn the case that a user does like a certain online product or service, the discoverability has to be repeatable. If the user cannot find the product or service on a subsequent search, she or he may no longer look for this product/service, and instead shift to a substitute that is easily and reliably findable. It is not enough to make the online product or service discoverable for only a short period, unless the goal is only to create “viral” content as part of a short-term marketing campaign.\n\n\n\n"}
{"id": "25214737", "url": "https://en.wikipedia.org/wiki?curid=25214737", "title": "Etiology, Concept and Prophylaxis of Childbed Fever", "text": "Etiology, Concept and Prophylaxis of Childbed Fever\n\nEtiology, Concept and Prophylaxis of Childbed Fever () is a medical book by Ignaz Semmelweis, published in 1861. It includes studies in hospitals conducted in Vienna in 1847, dealing largely with the field of obstetrics. It was translated into English by Kay Codell Carter in 1983. The book explains how his research shows that hand hygiene in hospitals can prevent unnecessary deaths.\n\n"}
{"id": "14345140", "url": "https://en.wikipedia.org/wiki?curid=14345140", "title": "European Society for Engineering Education", "text": "European Society for Engineering Education\n\nThe European Society for Engineering Education is the largest organisation for engineering education in Europe. Commonly known as SEFI, an acronym for its French name, Société Européenne pour la Formation des Ingénieurs, it is also known in German as the Europäische Gesellschaft für Ingenieur-Ausbildung. SEFI was founded in Brussels in 1973 and has more than 300 members in 40 countries. It promotes information exchange about current developments in the field of engineering education, between teachers, researchers and students in the various European countries.Additionally, it develops the cooperation between higher engineering education institutions and promotes cooperation with industry, acting as a link between its members and other scientific and international bodies, in collaboration with other international organisations like its European sister organisation IGIP, the American Society for Engineering Education, and the Board of European Students of Technology.\n\nSEFI is primarily a network of universities however, it offers four types of membership: individual, institutional \"(list)\", associate \"(list)\", and industrial \"(list)\"\n\nInstitutional - Educational institutions and other teaching establishments involved in the education and training of engineers. \"(list)\"\n\nIndustrial - Enterprises, companies and administrations employing engineers or interested in the education and training of engineers. \"(list)\"\n\nAssociate - Professional organizations involved in engineering education or improvement of engineering profession, or institutions not fulfilling the criteria of the institutional membership \"(list)\"\n\nIndividual - Persons involved in the engineering education and the improvement of the engineering profession, and individuals interested in joining our Working groups or EEDC\n\nSEFI is governed by a Board of Directors composed of 21 elected members and members’ representatives, two Vice-Presidents and is presently chaired by President Prof. Mike Murphy \"(Director - Digital Campus and Learning Transformation at Dublin Institute of Technology).\"\n\nSEFI Working Groups connect the educators, students and industrial stakeholders with interests in similar aspects of the engineering education and they are open to SEFI members. These groups organize meetings, workshops, write on position papers and EU projects. Current working groups include:\n\n\n\n\nEEDC is a permanent platform of European engineering deans which brings together their collective strengths for the advancement of engineering education and research. The EEDC provides a forum for exchange of information about experiences, challenges and best practices in engineering education institution leadership. This forum helps the engineering deans find opportunities for collaboration with industry and with other stakeholders in education, research and innovation. It also to encourages engineering deans to play a leading role in developing European and national policies for the benefit of society.\n\nWorkshops and are regularly organized by the working groups and committees on specific themes of engineering education and in the context of SEFI’s priorities. Participation in the working groups is reserved to the members of SEFI.\n\nThe SEFI Annual Conferences represent a unique opportunity for the members and all those involved in Engineering Education to meet colleagues, exchange views and opinions and to establish new contacts. The themes of the Conferences reflect the interests of SEFI members.\n\nThe last conferences were Annual Conference 2017 Organised by the ISEP (Porto), Azores Islands \"“Education Excellence for Sustainable Development” and\" Annual Conference 2016 \"“Engineering Education on Top of the World: Industry-University Cooperation”\"\n\nFuture Conferences are Annual Conference 2018 - organised by Technical University of Denmark on the topic of '“\"Creativity, Innovation and Entrepreneurship for Engineering Education Excellence”\" and Annual Conference 2019 - organised by the Budapest University of Technology and Economics.\n\nThe general objective of the Conventions is to bring together Deans from whole over Europe to meet and to discuss in depth common topics, share experiences, identify solutions for problems and build up a network with peers in different European countries. SEFI launched the conventions in 2005, under the Presidency of Prof. Borri, University of Florence. Since 2011, ECEDs are organised by SEFI, in cooperation with CESAER and other partners. The ECED 2018 will take be organised by the Norwegian University of Science and Technology in Trondheim on 27–29 May 2018.\n\nThe European Journal of Engineering Education published by Taylor and Francis is the official scientific journal of SEFI.\n\nSEFI also publishes a monthly electronic Newsletter and a weekly Press review as a benefit to SEFI membership.\n\nAmong regular publications are also SEFI Annual Reports and the Proceedings of SEFI Annual Conferences, indexed in Scopus.\n\nOther publications consist of Ad Hoc Documents presenting the outcomes of seminars organised by the Working Groups/Committees/Projects as well as reference documents.\n\nEBCC Model – Education, Business and Community Cooperation Model for a Creative European Engineering Education\n\nSTELA – Successful transition for secondary to higher education using learning analytics\n\nPREFER – Professional Roles and Employability of Future EngineeRs\n\n\n\n\n\nSEFI cooperates with other major European and international associations (ASEE, GEDC, IFEES, WFEO, IGIP, BEST, LACCEI, EDEN, JSEE, ...) and international bodies (European Commission, UNESCO, Council of Europe, OECD) . SEFI also participated in the creation of numerous international organisations such as ENAEE, IFEES, EuroPace, IACEE, IIDEA, or EEDC.\n\n"}
{"id": "50732", "url": "https://en.wikipedia.org/wiki?curid=50732", "title": "Extreme value theory", "text": "Extreme value theory\n\nExtreme value theory or extreme value analysis (EVA) is a branch of statistics dealing with the extreme deviations from the median of probability distributions. It seeks to assess, from a given ordered sample of a given random variable, the probability of events that are more extreme than any previously observed. Extreme value analysis is widely used in many disciplines, such as structural engineering, finance, earth sciences, traffic prediction, and geological engineering. For example, EVA might be used in the field of hydrology to estimate the probability of an unusually large flooding event, such as the 100-year flood. Similarly, for the design of a breakwater, a coastal engineer would seek to estimate the 50-year wave and design the structure accordingly.\n\nTwo approaches exist for practical extreme value analysis.\n\nThe first method relies on deriving block maxima (minima) series as a preliminary step. In many situations it is customary and convenient to extract the annual maxima (minima), generating an \"Annual Maxima Series\" (AMS).\n\nThe second method relies on extracting, from a continuous record, the peak values reached for any period during which values exceed a certain threshold (falls below a certain threshold). This method is generally referred to as the \"Peak Over Threshold\" method (POT).\n\nFor AMS data, the analysis may partly rely on the results of the Fisher–Tippett–Gnedenko theorem, leading to the generalized extreme value distribution being selected for fitting. However, in practice, various procedures are applied to select between a wider range of distributions. The theorem here relates to the limiting distributions for the minimum or the maximum of a very large collection of independent random variables from the same distribution. Given that the number of relevant random events within a year may be rather limited, it is unsurprising that analyses of observed AMS data often lead to distributions other than the generalized extreme value distribution (GEVD) being selected.\n\nFor POT data, the analysis may involve fitting two distributions: one for the number of events in a time period considered and a second for the size of the exceedances.\n\nA common assumption for the first is the Poisson distribution, with the generalized Pareto distribution being used for the exceedances. \nA tail-fitting can be based on the Pickands–Balkema–de Haan theorem.\n\nNovak reserves the term “POT method” to the case where the threshold is non-random, and distinguishes it from the case where one deals with exceedances of a random threshold.\n\nApplications of extreme value theory include predicting the probability distribution of:\n\nThe field of extreme value theory was pioneered by Leonard Tippett (1902–1985). Tippett was employed by the British Cotton Industry Research Association, where he worked to make cotton thread stronger. In his studies, he realized that the strength of a thread was controlled by the strength of its weakest fibres. With the help of R. A. Fisher, Tippet obtained three asymptotic limits describing the distributions of extremes assuming independent variables. Emil Julius Gumbel codified this theory in his 1958 book \"Statistics of Extremes\", including the Gumbel distributions that bear his name. These results can be extended to allowing for slight correlations between variables, but the classical theory does not extend to strong correlations of the order of the variance. One universality class of particular interest is that of log-correlated fields, where the correlations decay logarithmically with the distance.\n\nA summary of historically important publications relating to extreme value theory can be found in the article List of publications in statistics.\n\nLet formula_1 be a sequence of independent and identically distributed random variables with cumulative distribution function \"F\" and let formula_2 denote the maximum.\n\nIn theory, the exact distribution of the maximum can be derived:\n\nThe associated indicator function formula_4 is a Bernoulli process with a success probability formula_5 that depends on the magnitude formula_6 of the extreme event. The number of extreme events within formula_7 trials thus follows a binomial distribution and the number of trials until an event occurs follows a geometric distribution with expected value and standard deviation of the same order formula_8.\n\nIn practice, we might not have the distribution function formula_9 but the Fisher–Tippett–Gnedenko theorem provides an asymptotic result. If there exist sequences of constants formula_10 and formula_11 such that\n\nas formula_13 then\n\nwhere formula_15 depends on the tail shape of the distribution.\nWhen normalized, \"G\" belongs to one of the following non-degenerate distribution families:\n\nWeibull law: formula_16 has a light tail with finite upper bound. Also known as Type 3.\n\nGumbel law: formula_17 when the distribution of formula_18 has an exponential tail. Also known as Type 1\n\nFréchet Law: formula_19 when the distribution of formula_18 has a heavy tail (including polynomial decay). Also known as Type 2.\n\nIn all cases, formula_21.\n\n\n\n\n"}
{"id": "20626924", "url": "https://en.wikipedia.org/wiki?curid=20626924", "title": "Friedrich Knauer (zoologist)", "text": "Friedrich Knauer (zoologist)\n\nFriedrich Carl Knauer (31 March 1850, Graz – 31 July 1926, Vienna) was an Austrian zoologist.\n\nFriedrich Knauer studied physics, chemistry and zoology at the University of Vienna from 1868 to 1872. In 1887, he became a director Vivarium in Vienna Prater. In 1893, he became the director of Vienna Zoo. \n\nKnauer wrote zoological books for schools and instruction in science as well as popular scientific works. After his death, Knauer was buried in Zentralfriedhof Cemetery. In 1930, a street in Favoriten was named \"Friedrich Knauer Gasse\".\n\n"}
{"id": "51708042", "url": "https://en.wikipedia.org/wiki?curid=51708042", "title": "Georg Gottlieb Pusch", "text": "Georg Gottlieb Pusch\n\nGeorg Gottlieb Pusch or in Polish Jerzy Bogumił Pusz (15 December 1790, Kohren - 2.October 1846, Warsaw) was a German geologist.\nHe wrote \"Geognostischer Katechismus oder Anweisung zum praktischen Geognosiren für angehende Bergleute und Geognosten\" Craz und Gerlach., Freiburg 1819., 212 S\n\nPusch described\nin Pusch, G. G., 1837 \"Polens Paläontologie : oder, Abbildung und Beschreibung der vorzüglichsten und der noch unbeschriebenen Petrefakten aus den Gebirgsformationen in Polen, Volhynien und den Karpathen nebst einigen allgemeinen Beiträgen zur Petrefaktenkunde und einem Versuch zur Vervollständigung der Geschechte des europäischen Auer-Ochsen\" Stuttgart : E. Schweizerbart's Verlagshandlung, 1837.\n"}
{"id": "14536981", "url": "https://en.wikipedia.org/wiki?curid=14536981", "title": "Haystack Catena", "text": "Haystack Catena\n\nHaystack Catena (Haystack Vallis until March 2013) is a catena at 4.7° N, 46.2° W on Mercury. It superficially resembles a graben but is a chain of overlapping secondary craters. It was named after Haystack Observatory.\n"}
{"id": "34011935", "url": "https://en.wikipedia.org/wiki?curid=34011935", "title": "Historical Technical Museum, Peenemünde", "text": "Historical Technical Museum, Peenemünde\n\nThe Peenemünde Historical Technical Museum ( or HTM), former \"Peenemünde Information Centre for History and Technology\" ( or HTI) is a museum, founded in 1991, in the observation bunker and site of the former power station in Peenemünde on the island of Usedom in eastern Mecklenburg-Vorpommern in Germany. The museum is dedicated to the history of the Peenemünde Army Research Centre and the Luftwaffe test site of \"Peenemünde-West\", especially the rockets and missiles developed there between 1936 and 1945. Since January 2007 the information centre has become an anchor point on the European Route of Industrial Heritage (ERIH), a Europe-wide network of industrial monuments, and a part of the ERIH themed routes for \"Energy\" and \"Transport & Communication\".\n\nIn 2008 around the museum had 222,000 visitors including many school classes. Around € 6.5M were invested in the museum's renovation and expansion; a further investment of € 3.9 M is planned. In 2002 the HTM was given the Coventry Cross of Nails and in 2013 the \"European Union Prize for Cultural Heritage / Europa Nostra Award\" .\n\nThe main purpose of the exhibition in the power station is to be a memorial site where visitors can learn from exhibits, documents and films about the fateful pact made by the rocket engineers around Wernher von Braun with former powers in order to develop the aerospace industry.\n\nIn the mid-1960s, building on his technical experience from Peenemünde, Wernher von Braun was able to design the Saturn V rocket for NASA that was used to fly to the moon. The role of the former rocket engineer in Peenemünde, however, was to develop weapons of war. Spectacular films show visitors how V-1 flying bombs worked.\n\nThese experiences formed the basis for the development of nuclear missiles by the Allies after the war. According to documents in the exhibition even Peenemünde experts took part, including in Great Britain and France, where they helped to develop the Force de Frappe.\n\nAs part of a detailed chronicle about the test site, the living and working conditions of the forced labourers and prisoners of war in Peenemünde is illustrated. In addition, there is extensive information on concentration camp prisoners, who assembled the V1-flying bombs in Bavaria and Austria under inhuman conditions.\n\nThere is a chapel near the site which commemorates all the victims.\n\nAmongst the showpieces on display in the open-air part of the site are a replica V-1 flying bomb (\"Fieseler Fi 103\") and the A4 rocket.\n\nExhibits in the Peenemünde Historical Technical Museum include:\n\nThere is a large number of information boards at historic locations, connected with the Army Research Centre and Luftwaffe test site, spread over the whole \"Peenemünder Haken\" (\"Peenemünde Hook\") and managed by the HTI. The sites include Oxygen Factory II (\"Sauerstoffwerk II\"), two former forced labour camps and a halt on the old Peenemünde industrial railway. The displayed objects and terrain a generally freely accessible and often inconspicuous. Several were laid out with the help of youth volunteers.\n\n\n\n"}
{"id": "43147105", "url": "https://en.wikipedia.org/wiki?curid=43147105", "title": "Ideal town", "text": "Ideal town\n\nIdeal town was a Renaissance concept developed by Italian polymath Leon Battista Alberti (14041472), author of ten books of treatises on modern architecture titled \"De Re Aedificatoria\" written about 1450 with additions made until the time of his death in 1472. Alberti's architectural theories concerned the planning and building of an entire town as opposed to individual edifices for private patrons or ecclesiastical purposes. \n\nAlberti insisted on choosing the location of the town first, followed by careful setting up of the size and direction of streets, then location of bridges and gates, and finally a building pattern ruled by perfect symmetry. One of the more prominent examples of a town modelled on this theory was Zamość founded in the 16th century by the chancellor Jan Zamoyski. At present, it is a World Heritage Site in Poland.\n\n"}
{"id": "49664150", "url": "https://en.wikipedia.org/wiki?curid=49664150", "title": "Issues in Science and Technology Librarianship", "text": "Issues in Science and Technology Librarianship\n\nIssues in Science and Technology Librarianship is a quarterly peer-reviewed open access academic journal covering issues of interest to science and technology librarians. It was established in 1991 and the editor-in-chief is Andrea L. Duda (University of California, Santa Barbara).\n\nThe journal is abstracted and indexed in Inspec, Library and Information Science Abstracts, ProQuest databases, and Scopus.\n"}
{"id": "1160141", "url": "https://en.wikipedia.org/wiki?curid=1160141", "title": "Jagannatha Samrat", "text": "Jagannatha Samrat\n\nPaṇḍita Jagannātha Samrāṭ (1652–1744) was an Indian astronomer and mathematician who served in the court of Jai Singh II of Amber, and was also his guru.\n\nJagannātha, whose father's name was Gaṇeśa, and grandfather's Viṭṭhala was from a Vedic family originally from Maharashtra.\n\nAt the suggestion of Jai Singh, he learned Arabic and Persian, in order to study Islamic astronomy. Having become proficient in these languages, he translated texts in these languages into Sanskrit. These translations include:\n\nHis original works include:\n\nJagannātha held that when theory and observation differed, observation was the true \"pramāṇa\" and overruled theory. While he used and described a number of astronomical instruments, telescopes were not one of them.\n\n\n"}
{"id": "53853401", "url": "https://en.wikipedia.org/wiki?curid=53853401", "title": "Jens G. Eggers", "text": "Jens G. Eggers\n\nJens G. Eggers from the University of Bristol, was awarded the status of Fellow in the American Physical Society, after they were nominated by their Division of Fluid Dynamics in 2009, for \"applications of the ideas of singularities to free-boundary problems such as jet breakup, drop formation, air entrainment, thin-film dynamics including wetting, dewetting and contact line motions, and with further applications to polymeric flows and models for granular dynamics.\"\n"}
{"id": "56629526", "url": "https://en.wikipedia.org/wiki?curid=56629526", "title": "Josef Penninger", "text": "Josef Penninger\n\nJosef Penninger (born 5 September 1964) is an Austrian biomedical researcher specialising in molecular immunology. He is the scientific director of the Institute of Molecular Biotechnology located at the Vienna Biocenter. In February 2018, he announced his decision to leave Vienna and become the head of the Life Sciences Institute of the University of British Columbia in Vancouver, British Columbia, Canada.\n\nThe asteroid 48801 Penninger is named in his honour.\n"}
{"id": "52495758", "url": "https://en.wikipedia.org/wiki?curid=52495758", "title": "Kodi Ravichandran", "text": "Kodi Ravichandran\n\nKodimangalam S. Ravichandran is the chair of the Department of Microbiology at the University of Virginia School of Medicine and director of the UVA Center for Cell Clearance. He studies apoptosis in relation to many models of human disease.\n\nHe received a BVSc in Veterinary Medicine from Madras Veterinary College in India, followed by a PhD in Molecular and Cell Biology from the University of Massachusetts and did a postdoc in immunology at Dana-Farber Cancer Institute. Before joining the faculty at UVA in 1996, he worked as an instructor at Harvard Medical School.\n\nAs of 2016, he has published more than 100 papers, with more than 10 in Nature. He received the Governor's Award for Science Innovation in 2011. In 2016, he won the Odysseus 1 Award from the Research Foundation Flanders (FWO) in Belgium, which provided over $8 million in funding for establishing a lab at Ghent University’s Vlaams Institute for Biotechnology. He will split his time between the two institutions.\n"}
{"id": "15650509", "url": "https://en.wikipedia.org/wiki?curid=15650509", "title": "List of Apple drives", "text": "List of Apple drives\n\nA list of all Apple internal and external drives in chronological order of introduction.\n\n\n\n\n"}
{"id": "31229123", "url": "https://en.wikipedia.org/wiki?curid=31229123", "title": "List of U.S. state firearms", "text": "List of U.S. state firearms\n\nA state firearm has only been designated by eight of the fifty States in the United States: Alaska, Arizona, Utah, Indiana, Kentucky, Pennsylvania, West Virginia, and Tennessee.\n\nIn March 2011, Utah adopted the M1911 pistol as its state firearm. This gun was designed by Ogden, Utah native John Browning. The adoption was supported by Republican Utah State Representative Carl Wimmer, who said, \"It does capture a portion of Utah's history\" and \"even bigger than that, it captures a portion of American history.\" The adoption was opposed by Democratic Utah State Representative Brian King who said, \"When we are talking about a state symbol we would do well to come up with one that is more unifying than divisive and this is a very divisive symbol for obvious reasons. This is just a poor choice for a state symbol\".\n\nIn April 2011, Arizona Governor Jan Brewer signed a bill into law which designated the Colt Single Action Army Revolver as Arizona's state firearm.\n\nIn March 2012, Indiana adopted the Grouseland rifle as its state firearm. This rifle is kept at Grouseland, the home of President William Henry Harrison and was made between 1803 and 1812 by John Small, who later became the first sheriff in the state. \"This rifle and its maker are both integral parts of Indiana history, and as such, the rifle is worthy of its designation as the Indiana State Rifle,\" said Senator John Waterman.\n\nIn June 2013, Kentucky adopted the Kentucky Long Rifle as its state firearm.\n\nIn June 2014, Pennsylvania adopted the long rifle as its state firearm.\n\nIn July 2014, Alaska adopted the pre-1964 Winchester Model 70 rifle as its state firearm. The bill, sponsored by Senate President Charlie Huggins, refers to the gun as the \"rifleman's rifle.\" The bill says the gun helped Alaskans \"establish a firm foothold\" in the wilderness between 1930 and 1963.\n\n\n1. Official Website displaying photograph of the Grouseland Rifle: http://www.grouselandfoundation.org/new.html\n"}
{"id": "242621", "url": "https://en.wikipedia.org/wiki?curid=242621", "title": "List of bones of the human skeleton", "text": "List of bones of the human skeleton\n\nThe human skeleton of an adult consists of 206-208 bones. It is composed of 270 bones at birth, but later decreases to 80 bones in the axial skeleton and 126 bones in the appendicular skeleton. Many small supernumerary bones, such as some sesamoid bones, are not included in this count.\n\nAs a human ages, some of its bones fuse, a process which typically lasts until sometime within the third decade of life. Therefore, the number of bones in an individual may be evaluated differently throughout their life. In addition, the bones of the skull and face are counted as separate bones, despite being fused naturally. Some reliable sesamoid bones such as the pisiform are counted, while others, such as the hallux sesamoids, are not.\n\nIndividuals may have more or fewer bones than the average (even accounting for developmental stage) owing to anatomical variations. The most common variations include sutural (wormian) bones, which are located along the sutural lines on the back of the skull, and sesamoid bones which develop within some tendons, mainly in the hands and feet. Some individuals may also have additional (i.e., supernumerary) cervical ribs or lumbar vertebrae.\n\nA fully grown adult features 26 bones in the spine, whereas a child can have 33.\n\nThere are usually 26 bones in the chest but sometimes there can be additional cervical ribs in humans. Cervical ribs occur naturally in other animals such as reptiles.\n\nThere are 22 bones in the skull. Including the bones of the middle ear, the head contains 28 bones.\n\nThere are a total of 64 bones in the arm.\n\nThe hip bone has three regions which are included or fused to form two coxal bones.They are: ilium, ischium, and pubis\n\n\nSpine 26\nChest 26\nHead 28\nArms 64\nLegs 60\nPelvis 2\n\n\n"}
{"id": "1687463", "url": "https://en.wikipedia.org/wiki?curid=1687463", "title": "List of botanical gardens and arboretums in the United States", "text": "List of botanical gardens and arboretums in the United States\n\nThis list is intended to include all significant botanical gardens and arboretums in the United States.\n\n"}
{"id": "19482086", "url": "https://en.wikipedia.org/wiki?curid=19482086", "title": "List of cultural icons of Scotland", "text": "List of cultural icons of Scotland\n\nThis List of cultural icons of Scotland is a list of objects, topics or people identified as cultural icons of Scotland.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "9735959", "url": "https://en.wikipedia.org/wiki?curid=9735959", "title": "List of lakes in Kentucky", "text": "List of lakes in Kentucky\n\nThe following is a list of lakes and reservoirs in the state of Kentucky in the United States.\n\n\n"}
{"id": "236722", "url": "https://en.wikipedia.org/wiki?curid=236722", "title": "Master of Science", "text": "Master of Science\n\nA Master of Science (; abbreviated MS, M.S., MSc, M.Sc., SM, S.M., ScM or Sc.M.) is a master's degree in the field of science awarded by universities in many countries or a person holding such a degree. In contrast to the Master of Arts degree, the Master of Science degree is typically granted for studies in sciences, engineering and medicine and is usually for programs that are more focused on scientific and mathematical subjects; however, different universities have different conventions and may also offer the degree for fields typically considered within the humanities and social sciences. While it ultimately depends upon the specific program, earning a Master of Science degree typically includes writing a thesis.\n\nAlgeria follows the Bologna Process.\n\nIn Argentina, Brazil, Ecuador, Mexico, Colombia, Panamá, Perú and Uruguay, the Master of Science or Magister is a postgraduate degree of two to four years of duration. The admission to a Master's program (Spanish: \"Maestría\"; Portuguese: \"Mestrado\") requires the full completion of a four to five years long undergraduate degree, bachelor's degree or a Licentiate's degree of the same length. Defense of a research thesis is required. All master's degrees qualify for a doctorate program.\n\nAustralian universities commonly have coursework or research-based Master of Science courses for graduate students. They typically run for 1–2 years full-time, with varying amounts of research involved.\n\nIn Bangladesh, all universities, including Bangladesh Agricultural University Jagannath University, Dhaka University, University of Chittagong, Jahangirnagar University, Islamic University, Bangladesh and Rajshahi University have Master of Science courses as postgraduate degrees. After passing Bachelor of Science, any student becomes eligible to study in this discipline.\n\nIn Canada, Master of Science (MSc) degrees may be entirely course-based, entirely research-based or (more typically) a mixture. Master's programs typically take one to three years to complete and the completion of a scientific thesis is often required. Admission to a master's program is contingent upon holding a four-year university bachelor's degree. Some universities require a master's degree in order to progress to a doctoral program (PhD).\n\nIn the province of Quebec, the Master of Science follows the same principles as in the rest of Canada. There is one exception, however, regarding admission to a master's program. Since Québécois students complete two to three years of college before entering university, they have the opportunity to complete a bachelor's degree in three years instead of four. Some undergraduate degrees such as the Bachelor of Education and the Bachelor of Engineering requires four years of studies. Following the obtention of their bachelor's degree, students can be admitted into a graduate program to eventually obtain a master's degree.\n\nCommonly the Chilean universities have used \"Magíster\" for a master degree, but other than that is similar to the rest of South America.\n\nLike all EU member states, the Czech Republic and Slovakia follow the Bologna Process. The Czech Republic and Slovakia are using two master's degree systems. Both award a title of \"Mgr.\" or \"Ing.\" to be used before the name.\nThe older system requires a 5-year program. The new system takes only 2 years but requires a previously completed 3-year bachelor program (a \"Bc.\" title). It is required to write a thesis (in both master and bachelor program) and also to pass final exams. It is mostly the case that the final exams cover the main study areas of the whole study program, i.e. a student is required to prove his/her knowledge in many subjects he attended during the 2 resp. 3 years.\n\nThe Master of Science (M.Sc.) is an academic degree for a post-graduate candidates or researchers, it usually takes from 4 to 7 years after passing the Bachelor of Science (B.Sc.) degree. Master programs are awarded in many sciences in the Egyptian Universities. A completion of the degree requires finishing a pre-master studies followed by a scientific thesis or research. All M.Sc. degree holders are allowable to take a step forward in the academic track to get the PhD degree.\n\nLike all EU member states, Finland follows the Bologna Process. The Master of Science (M.Sc.) academic degree usually follows the Bachelor of Science (B.Sc.) studies which typically last five years. For the completion of both the bachelor and the master studies the student must accumulate a total of 300 ECTS credits, thus most Masters programs are two-year programs with 120 credits. The completion of a scientific thesis is required.\n\nLike all EU member states, Germany follows the Bologna Process. The Master of Science (M.Sc.) academic degree replaces the once common \"Diplom\" or \"Magister\" programs that typically lasted four to five years. It is awarded in science related studies with a high percentage of mathematics. For the completion the student must accumulate 300 ECTS Credits, thus most Masters programs are two-year programs with 120 credits. The completion of a scientific thesis is required.\n\nIn Slavic countries in European southeast (particularly former Yugoslavian republics), the education system was largely based on the German university system (largely due to the presence and influence of the Austria-Hungary Empire in the region). Prior to the implementation of the Bologna Process, academic university studies comprised a 4-5 year long graduate \"Diplom\" program, which could have been followed by a 2-4 year long \"Magister\" program and then later with 2-4 year long \"Doctoral studies\".\n\nAfter the Bologna Process implementation, again based on the German implementation, \"Diplom\" titles and programs were replaced by the M.Sc. and M.A. programs (depending on the field of study). The studies are structured such that a \"Master\" program lasts long enough for the student to accumulate a total of 300 ECTS credits, so its duration would depend on a number of credits acquired during the Bachelor studies. Pre-Bologna \"Magister\" programs were abandoned - after earning an M.Sc/M.A. degree and satisfying other academic requirements a student could proceed to earn a Doctor of Philosophy degree directly.\n\nIn Guyana, all universities, including University of Guyana, Texila American University, American International School of Medicine have Master of Science courses as postgraduate degrees. Students who have completed undergraduate Bachelor of Science degree are eligible to study in this discipline\n\nIn Iran, similar to Canada, Master of Science (MSc) or in Iranian form \"Karshenasi-arshad\" degrees may be entirely course-based, entirely research-based or sometimes a mixture. Master's programs typically take two to three years to complete and the completion of a scientific thesis is often required.\n\nLike all EU member states, Ireland follows the Bologna Process. In Ireland, Master of Science (MSc) may be course-based with a research component or entirely research based. The program is most commonly a one-year program and a thesis is required for both course-based and research based degrees.\n\nIn Israel, Master of Science (MSc) may be entirely course-based or include research. The program is most commonly a two-year program and a thesis is required only for research based degrees.\n\nIn India, universities offer MSc programs usually in science subjects. Generally speaking, in India, post-graduate scientific courses lead to MSc degree while post-graduate engineering courses lead to ME or MTech degree. For example, a master's in automotive engineering would normally be an ME or MTech, while a master's in physics would be an MSc. A few top universities also offer undergraduate programs leading to a master's degree which is known as integrated masters.\n\nA Master of Science in Engineering (M.Sc.Eng.) degree is also offered in India. It is usually structured as an engineering research degree, lesser than Ph.D. and considered to be parallel to M.Phil. degree in humanities and science. Some institutes such as IITs offer an MS degree for postgraduate engineering courses. This degree is considered a research-oriented degree where as MTech or ME degree is usually not a research degree in India. M.S. degree is also awarded by various IISERs which are one of the top institutes in India.\n\nLike all EU member states, Italy follows the Bologna Process. The degree \"Master of Science\" is awarded in the Italian form, \"Laurea Magistrale\" (formerly \"Laurea specialistica\"; before the introduction of the \"Laurea\" the corresponding degree was \"Laurea quinquennale\" or \"Vecchio Ordinamento\").\n\nIn Nepal, universities offer the master of science degree usually in science and engineering areas. Tribhuvan University offers MSc degree for all the science and engineering courses. Pokhara University offers ME for engineering and MSc for science. Kathmandu University offers MS by Research and ME degrees for science and engineering.\n\nLike all EU member states, the Netherlands follows the Bologna Process. A graduate who is awarded the title Master of Science (abbreviated as MSc) may still use the previously awarded Dutch title \"ingenieur\" (abbreviated as ir.) (for graduates who followed a technical or agricultural program), \"meester\" (abbreviated as mr.) (for graduates who followed an LLM law program) or \"doctorandus\" (abbreviated as drs.)(in all other cases).\n\nNew Zealand universities commonly have coursework or research-based Master of Science courses for graduate students. They typically run for 2 years full-time, with varying amounts of research involved.\n\nNorway follows the Bologna Process. For engineering, the Master of Science academic degree has been recently introduced and has replaced the previous award forms \"Sivilingeniør\" (engineer, a.k.a. engineering master) and \"Hovedfag\" (academic master). Both were awarded after 5 years university-level studies and required the completion of a scientific thesis.\n\n\"Siv.ing\", is a protected title exclusively awarded to engineering students who completed a five-year education at The Norwegian University of Science and Technology (, NTNU) or other universities. Historically there was no bachelor's degree involved and today's program is a five years master's degree education. The \"Siv.ing\" title is in the process of being phased out, replaced by (for now, complemented by) the \"M.Sc.\" title. By and large, \"Siv.ing\" is a title tightly being held on to for the sake of tradition. In academia, the new program offers separate three-year bachelor and two-year master programs. It is awarded in the natural sciences, mathematics and computer science fields. The completion of a scientific thesis is required. All master's degrees are designed to certify a level of education and qualify for a doctorate program.\n\nMaster of Science in Business is the English title for those taking a higher business degree, \"Siviløkonom\" in Norwegian. In addition, there is, for example, the 'Master of Business Administration' (MBA), a practically oriented master's degree in business, but with less mathematics and econometrics, due to its less specific entry requirements and smaller focus on research.\n\nPakistan inherited its conventions pertaining to higher education from United Kingdom after independence in 1947. Master of Science degree is typically abbreviated as M.Sc. (as in the United Kingdom) and which is awarded after 16 years of education (equivalent with a bachelor's degree in the USA and many other countries). Recently, in pursuance to some of the reforms by the Higher Education Commission of Pakistan (the regulatory body of higher education in Pakistan), the traditional 2-year Bachelor of Science (B.Sc.) degree has been replaced by the 4-year Bachelor of Science degree, which is abbreviated as B.S. to enable the Pakistani degrees with the rest of the world. Subsequently, students who pass 4-year B.S. degree that is awarded after 16 years of education are then eligible to apply for M.S. degree, which is considered at par with Master of Philosophy (M.Phil.) degree.\n\nLike all EU member states, Poland follows the Bologna Process. The Polish equivalent of Master of Science is \"magister\" (abbreviated \"mgr\", written pre-nominally much like \"Dr\"). Starting in 2001, the MSc programs typically lasting 5 years began to be replaced as below:\nThe degree is awarded predominantly in the natural sciences, mathematics, computer science, economics, as well as in the arts and other disciplines. Those who graduate from an engineering program prior to being awarded a master's degree are allowed to use the \"mgr inż.\" pre-nominal (\"master engineer\"). This is most common in engineering and agricultural fields of study. Defense of a research thesis is required. All master's degrees in Poland qualify for a doctorate program.\n\nThe title of \"master\" was introduced by Alexander I at 24 January 1803. The Master had an intermediate position between the candidate and doctor according to the decree \"About colleges structure\". The master's degree was abolished from 1917 to 1934. Russia follows the Bologna Process for higher education in Europe since 2011.\n\nLike all EU member states, Spain follows the Bologna Process. The Master of Science (MSc) degree is a program officially recognized by the Spanish Ministry of Education. It usually involves 1 or 2 years of full-time study. It is targeted at pre-experience candidates who have recently finished their undergraduate studies. An MSc degree can be awarded in every field of study. An MSc degree is required in order to progress to a PhD. MSci, MPhil and DEA are equivalent in Spain.\n\nLike all EU member states, Sweden follows the Bologna Process. The Master of Science academic degree has, like in Germany, recently been introduced in Sweden. Students studying Master of Science in Engineering programs are rewarded both the English Master of Science Degree, but also the Swedish equivalent \"Teknologisk masterexamen\". Whilst \"Civilingenjör\" is an at least five year long education .\n\nThe Master of Science is a degree that can be studied only in public universities. The program is usually 2 years, but it can be extended to 3 or 4 years, the student is required to pass a specific bachelor's degree to attend a specific master of science degree program, the master of science is mostly a research master (except for some types of programs held with cooperation of foreign universities), The student should attend some courses in the first year of the master then he/she should prepare a research thesis. Publishing two research papers is recommended and will increase the final evaluation grade.\n\nThe Master of Science (MSc) is typically a taught postgraduate degree, involving lectures, examinations and a project dissertation (normally taking up a third of the program). Master's programs usually involve a minimum of 1 year of full-time study (180 UK credits, of which 150 must be at master's level) and sometimes up to 2 years of full-time study (or the equivalent period part-time). Taught master's degrees are normally classified into Pass, Merit and Distinction (although some universities do not give Merit). Some universities also offer MSc by research programs, where a longer project or set of projects is undertaken full-time; master's degrees by research are normally pass/fail, although some universities may offer a distinction.\n\nThe more recent Master \"in\" Science (MSci or M.Sci.) degree (Master of Natural Science at the University of Cambridge), is an undergraduate (UG) level integrated master's degree offered by UK institutions since the 1990s. It is offered as a first degree with the first three (four in Scotland) years similar to a BSc course and a final year (120 UK credits) at master's level, including a dissertation. The final MSci qualification is thus at the same level as a traditional MSc.\n\nThe \"Master of Science\" (\"Magister Scientiæ\") degree is normally a full-time two-year degree often abbreviated \"MS\" or \"M.S.\" It is the primary type in most subjects and may be entirely course-based, entirely research-based or (more typically) a combination of the two. The combination often involves writing and defending a thesis or completing a research project which represents the culmination of the material learned.\n\nAdmission to a master's program is normally contingent upon holding a bachelor's degree and progressing to a doctoral program may require a master's degree. In some fields or graduate programs, work on a doctorate can begin immediately after the bachelor's degree. Some programs provide for a joint bachelor's and master's degree after about five years. Some universities use the Latin degree names and due to the flexibility of word order in Latin, \"Artium Magister\" (A.M.) or \"Scientiæ Magister\" (S.M. or Sc.M.) may be used in some institutions.\n\n"}
{"id": "4456100", "url": "https://en.wikipedia.org/wiki?curid=4456100", "title": "Mimaland", "text": "Mimaland\n\nMimaland was a recreation theme park in Gombak, Selangor, Malaysia. It opened in 1971 and it ceased operations in 1994 after a landslide damaged the property.\n\nMimaland started operating in 1971. The name Mimaland is actually an acronym of the word combination Malaysia In Miniature Land. Mimaland was built on a hilly area of 300 acres in Ulu Gombak near Kuala Lumpur, served by an exit to the old Gombak-Bentong road. It was owned by Mimaland Berhad, a member of Magnum Group of Companies (now Magnum Corporation).\n\nMimaland's construction was done in phases and the theme park was opened in stages. Mimaland's first phase was completed in 1970, and consisted of 24 motels, 5 chalets and a swimming pool.\n\nAmong the main attractions, was the 'prehistoric animal park' containing dinosaur models of a very impressive size. Additionally, Mimaland also had a lake for boating and fishing activities, a 'giant maze', as well as a cross-forest area.\n\nIn January 1993, a 27-year-old Singaporean was killed while using the giant slide at the Mimaland pool. The incident shocked the public and resulted in a temporary closure of Mimaland. After improvement was done to the giant slide, Mimaland was reopened. However, a minor landslide caused damage to the walls of the same pool in May 1994. This incident caused muddy water to seep into the pool, causing the area to close to the public.\n\nSeveral parties, including government agencies, took the management of Mimaland to court on a number of offenses, among which were security control issues. As a result, the court barred Mimaland from continuing its operations. Mimaland was closed permanently in 1994. Mimaland relics still exist to this day.\n"}
{"id": "23664156", "url": "https://en.wikipedia.org/wiki?curid=23664156", "title": "Moonshot (film)", "text": "Moonshot (film)\n\nMoonshot is a 2009 television film depicting the story leading up to the landing of Apollo 11 on the surface of the Moon on 20 July 1969.\n\n\nThe film utilizes actual footage taken during the time period known as the Space Race.\n"}
{"id": "9861756", "url": "https://en.wikipedia.org/wiki?curid=9861756", "title": "Nikolay Belov (geochemist)", "text": "Nikolay Belov (geochemist)\n\nNikolay Vasilyevich Belov (; December 14, 1891 – March 6, 1982) was a Soviet crystallographer, geochemist, academician (1953), and Hero of Socialist Labor (1969).\n\n"}
{"id": "174855", "url": "https://en.wikipedia.org/wiki?curid=174855", "title": "Null-move heuristic", "text": "Null-move heuristic\n\nIn computer chess programs, the null-move heuristic is a heuristic technique used to enhance the speed of the alpha-beta pruning algorithm.\n\nAlpha-beta pruning speeds the minimax algorithm by identifying \"cutoffs\", points in the game tree where the current position is so good for the side to move that best play by the other side would have avoided it. Since such positions could not have resulted from best play, they and all branches of the game tree stemming from them can be ignored. The faster the program produces cutoffs, the faster the search runs. The null-move heuristic is designed to guess cutoffs with less effort than would otherwise be required, whilst retaining a reasonable level of accuracy.\n\nThe null-move heuristic is based on the fact that most reasonable chess moves improve the position for the side that played them. So, if the player whose turn it is to move can forfeit the right to move (or make a null move - an illegal action in chess) and still have a position strong enough to produce a cutoff, then the current position would almost certainly produce a cutoff if the current player actually moved.\n\nIn employing the null-move heuristic, the computer program first forfeits the turn of the side whose turn it is to move, and then performs an alpha-beta search on the resulting position to a shallower depth than it would have searched the current position had it not used the null move heuristic. If this shallow search produces a cutoff, it assumes the full-depth search in the absence of a forfeited turn would also have produced a cutoff. Because a shallow search is faster than deeper search, the cutoff is found faster, accelerating the computer chess program. If the shallow search fails to produce a cutoff, then the program must make the full-depth search.\n\nThis approach makes two assumptions. First, it assumes that the disadvantage of forfeiting one's turn is greater than the disadvantage of performing a shallower search. Provided the shallower search is not too much shallower (in practical implementation, the null-move search is usually 2 or 3 plies shallower than the full search would have been), this is usually true. Second, it assumes that the null-move search will produce a cutoff frequently enough to justify the time spent performing null-move searches instead of full searches. In practice, this is also usually true.\n\nThere are a class of chess positions where employing the null-move heuristic can result in severe tactical blunders. In these \"zugzwang\" (German for \"forced to move\") positions, the player whose turn it is to move has only bad moves as their legal choices, and so would actually be better off if allowed to forfeit the right to move. In these positions, the null-move heuristic may produce a cutoff where a full search would not have found one, causing the program to assume the position is very good for a side when it may in fact be very bad for them.\n\nTo avoid using the null-move heuristic in zugzwang positions, most chess-playing programs that use the null-move heuristic put restrictions on its use. Such restrictions often include not using the null-move heuristic if\n\nAnother heuristic for dealing with the zugzwang problem is Omid David and Nathan Netanyahu's verified null-move pruning. In verified null-move pruning, whenever the shallow null-move search indicates a fail-high, instead of cutting off the search from the current node, the search is continued with reduced depth.\n"}
{"id": "16607102", "url": "https://en.wikipedia.org/wiki?curid=16607102", "title": "Operation Fulcrum", "text": "Operation Fulcrum\n\nThe United States's Fulcrum nuclear test series was a group of 21 nuclear tests conducted in 1976-1977. These tests followed the \"Operation Anvil (Nuclear test)\" series and preceded the \"Operation Cresset\" series.\n"}
{"id": "9423838", "url": "https://en.wikipedia.org/wiki?curid=9423838", "title": "Protocol analysis", "text": "Protocol analysis\n\nProtocol analysis is a psychological research method that elicits verbal reports from research participants. Protocol analysis is used to study thinking in cognitive psychology (Crutcher, 1994), cognitive science (Simon & Kaplan, 1989), and behavior analysis (Austin & Delaney, 1998). It has found further application in the design of surveys and interviews (Sudman, Bradburn & Schwarz, 1996), usability testing (Henderson, Smith, Podd, & Varela-Alvarez, 1995), educational psychology (Pressley & Afflerbach 1995; Renkl, 1997) and design research (Gero & McNeill 1998) .\n\n\n\n"}
{"id": "53034636", "url": "https://en.wikipedia.org/wiki?curid=53034636", "title": "Shifra Baruchson Arbib", "text": "Shifra Baruchson Arbib\n\nShifra Baruchson-Arbib (born 1951) is a Full Professor in the Department of Information Science at Bar-Ilan University Israel, specializing in the history and sociology of media. She was the Head of the Information Science Department (1990-1998, 2005-2008, 2012–2014) and has served as the Dean of the Faculty of Humanities at Bar- Ilan University (2015-2016). Her major achievements include her studies in the fields of the \"History of Hebrew Books and Manuscripts\" and of \"Information Science\", leading the Department of Information Science; and developing the academic field of Social Information Science.\n\nShifra Baruchson-Arbib was born in December 1951 in Tel-Aviv, as Zipora Baruchson, to a learned rabbinical family. She is married to Chaim Eric Arbib. Her paternal grandfather, Rabbi Yitzhak Baruchson, was a descendent of 17 generations of \"Dayanim\" (judges of Jewish Law) and a leader in the Musar (Ethical) movement. Her maternal grandfather, Rabbi Avraham Sochovolski, was one of the first Scribes (\"Sofer Stam\") in Tel-Aviv in the 1930s. Her father, Shlomo Baruchson, was the owner of the \"Shalom publishing house\" and was the first person in Israel to turn the Bible, the Siddur (prayer book), and the Haggadah (the prayer book of Passover) into gifts by using luxurious silver book covers using the work of the Bezalel Academy of Arts and Design in Israel. Baruchson-Arbib's uncle, Yehuda Makavy, was the owner of the \"International Company of Publishing\" house and he initiated and published the first historical-archeological project in Israel. This project was published in a series of books called \"The view of the Biblical World\", edited by the know archeologists Michael Avi-Yonah and Avraham Malamat, and \"The Military Art in the Lands of the Bible\", by Yigael Yadin. Baruchson-Arbib's family background greatly influenced her scientific and research work.\n\nBaruchson-Arbib began her studies at Bar-Ilan University in 1970, in the Departments of Bible Studies, Jewish History, and Library Science. She completed her master's and doctorate degrees in the field of Jewish History in topics related to manuscripts, rare books, and the reading culture. In 1974, she began to work at Bar-Ilan University as a research assistant]. In 1990, Baruchson-Arbib was appointed Head of the Department of Bibliographical and Librarianship Studies at Bar-Ilan University. As only a few students were interested in the field at the time, she changed the name of the department and its content to The Department of Information Science. This interdisciplinary field is known throughout the world and regards the theory and practice of producing, collecting, processing, storing, retrieving, representing, and distributing information. The field aims to understand the process that information undergoes as it transforms from data to knowledge, and it trains information scientists, information managers, and librarians. Changing the name of the department and its curriculum increased the number of students in the department from 50 to 500 in two years, and it allowed its graduates to work in an innovative and modern field. Today, graduates of the department are employed in various fields, including in high-tech companies, industry, hospitals, libraries, and the private sector. To date, the Department of Information Science at Bar-Ilan University is the largest and most significant in the field in Israel, and it awards bachelor's, master's, and doctorate degrees. Baruchson-Arbib has served as the Head of the department for many years and has supervised more than 70 research students. She has also developed a new discipline in the field of Information Science – the \"Social Information Science\" discipline. which teaches to balance between information overload and the need for reliable and available information, especially in the fields of health and welfare.\n\nIn 1993, Baruchson-Arbib received the Zalman Shazar Award for Outstanding Research in Jewish History, due to her book \"Books and Readers: The Reading Culture of Italian Jews at the Close of the Renaissance\", which was published in that year. This book was based on her doctorate work and received the Award due to its innovative methodology and findings. In addition to her research activities, Baruchson-Arbib studied the Milton Erickson hypnosis technique in the United States, and she was accredited to practice hypnosis in the State of New York. Baruchson-Arbib has participated in numerous international conferences and has lectured at various universities, such as the Sorbonne in Paris and the University of British Columbia. She is currently a member of several organizations, including the International Center for Information Ethics (ICIE) and the Royal Society for the Encouragement of Arts, Manufactures & Commerce (RSA). In addition, she is a member of several journal boards, including the Israeli journal 'Alei Sefer: Studies in Bibliography and in the History of the Printed and the Digital Hebrew Book', the British 'Journal of Communication & Ethics in Society', and the Polish journal 'Przeglad Biblioteczny Library Review'. She is also a member of the board of directors of the 'Joshua and Bracha Barzilai Scholarship Fund for Jewish Bibliographic Research', a member of the board of directors of the 'Naima and Yehoshua Salti Center for Ladino Studies', and a member of the high-school professional committee for 'Digital Discovery and Finding' in the Ministry of Education. In addition to being the dean of Humanities at Bar-Ilan University, Baruchson-Arbib also teaches in the Department of Information Science, where she gives various courses and seminars on information ethics, the \"digital man\" in the information society, social information, and reference work. In 2013, she established the \"Information 21\" project, within which the annual conference \"Information, Society, and Industry\" is held.\nUntil the 1990s, the research of Baruchson-Arbib consisted mainly of the study of Hebrew books, manuscripts, rare books, the reading culture, and the Interdisciplinary methodology of book research – also known as \"the new bibliology\". Since establishing the Department of Information Science, Baruchson-Arbib has published research in the fields of social information, public libraries, information behavior, library management, and information skills.\n\nBaruchson-Arbib began studying the field of \"Hebrew books\" in her thesis on Rabbi Jacob Margalith and his writings. In that study, and in the related research publications that followed, the mystery of the controversial identity of Rabbi Jacob Margalith was solved, identifying him as a Rabbi who lived and served in the German cities of Nürnberg and Regensburg, and who died in 1501. Rabbi Jacob Margalith was the editor of the manuscript \"Sedder Gittin Ve'Halitza\" (The Ritual of Divorce and Levirate Marriage), which aimed to serve as a unified codex of the Jewish ruling in laws of the \"Gett\" (the Jewish divorce document) and was included in the Shulchan Aruch (the Code of Jewish Law). Within the hand-written manuscripts of this essay, Baruchson-Arbib also identified previously unknown letters of Jewish Ashkenazi (European) sages.\n\nIn her doctorate work, Baruchson-Arbib studied the private libraries of the north Italian Jews at the close of the Renaissance period. This study was later published in the book \"Books and Readers\", as well as in a French-translated edition with a preface written by Professor Jean-Pierre Rothschild. The book and the subsequent research publications provide numerous scientific innovations on the literary interest and printing culture of Jews at the time of the counter-reformation when Hebrew books were persecuted. The book is based on a collection of manuscripts, altogether amounting to 628 pages that include 438 book inventories, which were located in the homes of 430 Jews and 8 synagogues of the Mantua Duchy, and which were presented to the censor, upon his request, for examination in 1595. The entire list of books is written in different handwriting, and part of the lists mention the name of the owner of the particular library and sometimes his profession. Altogether, the collection included 21,142 volumes, divided into 1234 titles in Hebrew, Yiddish, Italian, and Latin. The research by Baruchson-Arbib was based on deciphering and analyzing the entire dataset, in addition to researching the tax records of the community in an attempt to understand the influence of the socioeconomic background on the content of the libraries. The censors who were appointed to examine and expurgate the books [including deleting 'forbidden' parts, such as the words \"\"Goy\" (gentile, non-Jew), idolater, etc.] were three apostates, headed by Domenico Gerosolimitano. The censors, who were highly paid by the Jewish community, visited the Jewish homes, deleted the 'forbidden' parts in red ink (which, to this day, allows reading the text underneath the ink), and signed their names on the last page of the books to affirm that the books do not contain anti-Christian paragraphs. Thus, Baruchson-Arbib concluded that all were satisfied with the arrangement; the church, which censored the books, the Jews, whose books were not burned (thus allowing them to study and pray), and Duke of Mantua, who continued to receive taxes from the Jews. Years later, Domenico Gerosolimitano wrote the \"Index Librorum Prohibitorum\"\" (\"Sefer ha-Zikkuk\") – a guide to the forbidden parts in the books of Jews. The life and work of Domenico Gerosolimitano, who was originally a Rabbi in Israel, were studied extensively by a doctorate student of Baruchson-Arbib, Dr. Gila Prebor.\n\nThe research of Baruchson-Arbib opened, for the first time, a gateway to analyzing private libraries from a time when the cost of purchasing a book was very high. This is true mostly for the great Bibles and \"Pesika\" ('religious rulings') books, which were extremely costly at the time. For instance, the price of the \"Mikraot Gedolot\" book (that includes commentaries on the Bible) was equivalent to a month and a half worth of work; the price of the \"Talmud\" was equivalent to three and a half months of work; and the price of the \"Siddur\" (prayer book) was equivalent to a day's work. Under these conditions, it is no wonder that the typical library in most communities comprised between 26 and 50 books, whereas wealthy families held libraries of between 101 and 350 books. Baruchson-Arbib found that the most prominent books in the libraries were prayer books, \"Bibles\", \"Pirkei Avot\" (Ethics), \"Mishneh\" \"Torah\" by Maimonides, and the \"Mishnah\" (the oral ruling). In this society, the \"Zohar\" book (the Jewish book of \"Kabbalah\" and mysticism) was prevalent in approximately 10.5% of the libraries, and the philosophical book of Maimonides, \"Moreh Nevukhim\", was prevalent in approximately 9.5% of the libraries, which indicates an intellectual interest in the study of the occult and in philosophy. In addition, the libraries of the Mantuan Jews contained copies of books in Latin and Italian, written by the great writers of the Greek and Roman cultures, of the Middle Ages, and of the Renaissance (including writings of Virgil, Cicero, Dante, Boccaccio, and others). Nevertheless, Baruchson-Arbib found that the most popular non-Jewish writings were Orlando Furioso by Ludovico Ariosto and the collection of love sonatas by Petrarch.\n\nBaruchson-Arbib found that, in the Mantuan community, the reading culture also reflected class differences. The wealthier Jews, such as Jewish doctors and Rabbis, owned the largest libraries, while the libraries of the tailor, the hatter, and the servant comprised only a few books. The study shed light also on the fact that the interest of the Jewish community in books was greater than that of the surrounding society. The study also delved into research on publishing houses, book prices, means of book distribution, and the ambivalent relationships between the Jews, the church, and the local ruler. Some of the volumes of the Jews of Mantua are today stored in the Israeli National Library and in Bar-Ilan University's library. In addition to many new data revealed in the book, the study stands out due to its methodology, which integrates many disciplines, such as history, statistics, sociology, and bibliography. Thus, the study of Baruchson-Arbib is part of the modern trend of the study of history, and it implements the French school of \"Livre et Societe\" into the study of Judaism. This field of research, also called \"the new bibliology\", is discussed in other publications by Baruchson-Arbib. As a byproduct of studying the Mantuan libraries, 202 unknown published Hebrew books were found. Quantitatively, this is the one of most comprehensive data found in this field of Hebrew bibliography. Baruchson-Arbib has continued studying Hebrew books, together with her doctorate student, Dr. Esther Lapon-Kandelshein, in the fields of 16th century \"science books\" and the publication of \"\"Ladino\" \"literature\"\" in Israel\n\nBaruchson-Arbib began her research in the field of Information Science in the early 1990s, when she published her book \"Social Information Science: Love, Health and the Information Society\" (1996). In this book, Baruchson-Arbib predicted the benefits, problems, and challenges of the digital world, and set out a way of intelligently using digital and printed information for the benefit of society. The novel discipline developed by Baruchson-Arbib was termed \"Social Information science\" – a new field of research that studies the development and implementation of the various aspects related to the transfer and sharing of social and medical information. These aspects include locating and processing information, researching the needs of the society, the ethics of sharing information, and the development of computerized tools for providing social information. In addition, the field includes the development of relevant institutions and professions, such as information banks (data centers) for social and medical information that will be managed by a social information scientist, sections for self-help in libraries, and private information services, information websites, and social portals. While developing this field of research, Baruchson-Arbib has relied on several hypotheses:\nBaruchson-Arbib describes her vision and the ideal for the field of social information:In order to have a large group of informed and skilled people in our society, and for humans to understand what is the power of information and the great potential that it has in assisting our lives, a new generation must be educated, .A generation that will learn, in schools, the new profession of information skills, and will be provided with information tools for social needs…\"Social information science\", at this stage, is the beginning of a vision to enlarge educated smart users and diminish \"digital divide \"; the materialization depends on the integration of currently available\" information technologies\" in a creative and responsible way by knowledgeable specialists, such as the \"social information scientist\" and a group of people proficient in information literacy.According to Baruchson-Arbib, the \"social information\" approach can greatly influence the role of public and school libraries in the community. The librarian/information scientist can serve as a \"social information scientist\", constructing sections for \"self-help\" in libraries, directing readers toward relevant and accurate sources, and turning the library into a center of information and culture. Due to the seminars given by Baruchson-Arbib in the field of \"social information\", some of her students have begun to practice the social aspects of information; some of them have established self-counseling and assistance sections in public libraries, and even initiated a project of \"a Reading School\" in school libraries. In addition, due to her research in the field of social information, Baruchson-Arbib was asked to edit two volumes in the field for the \"\"Journal of Information \" Communication and Ethics in Society\"\"\" Baruchson-Arbib has continued her research in the relationship between society and modern information tools in a series of studies. These studies touch upon various aspects of the field, including social information in a multicultural society, assistance and support through books, the relationship between hospital libraries and public libraries, the information website of the public library, community information and the public library, evaluation of medical website, and a series of papers regarding SHIL (the Israeli citizen advice bureau), which she published together with a group of researchers and which was funded by the Israel Science Foundation. Alongside these social issues, Baruchson-Arbib has published studies regarding various aspects in the field of information science, some of which were conducted together with her students. For instance, she has published papers on the future of the printed book in the information society, information behavior of researchers in the field of Jewish studies and of university students, teaching information skills, community information and the public library, plagiarism in the information society, information behavior of start-up entrepreneurs, public library management in an era of changes, and the future of the information science profession (a Delphi research). In recent years, Baruchson-Arbib is engaged in research on information in multicultural societies, information ethics, and smart information usage.\n\n\n\n1993 – The Zalman Shazar Award for Outstanding Research in Jewish History, for her book \"Books and Readers: The Reading Culture of Italian Jews at the Close of the Renaissance\", 1993.\n\nTogether with a group of researchers from Haifa University, Ben-Gurion University of the Negev, and Bar-Ilan University.\nAwarded by the Israel Science Foundation.\n\n2008 – 2012 – Automatic methods for evaluating the quality of Wikipedia content. Together with a group of researchers from Bar-Ilan University. Awarded by the Israel Internet Association.\n"}
{"id": "24055988", "url": "https://en.wikipedia.org/wiki?curid=24055988", "title": "Spencer Lister", "text": "Spencer Lister\n\nSir Frederick Spencer Lister (8 April 1876 – 6 September 1939) was an English-born South African doctor and bacteriologist.\n\nLister was born in Norwell, Nottinghamshire. In 1897 he joined West Hertfordshire Football Club (later Watford Football Club) as an amateur association football player, making twelve appearances and scoring three goals for the team in all competitions.\n\nHe trained as a doctor at St Bartholomew's Hospital Medical College in London, qualifying in 1905. He then went to the Transvaal, serving as medical officer to the Premier Diamond Mines from 1907 to 1912 and to the Rand Gold Mines near Johannesburg from 1912 to 1917. In 1917 he was appointed Research Bacteriologist at the South African Institute for Medical Research in Johannesburg. He later became Director of the Institute and Professor of Pathology and Bacteriology at the University of the Witwatersrand. From 1928 he served on the South African Medical Council. He wrote important papers on pneumonia and influenza and was also an expert on leprosy.\n\nHe was knighted in the 1920 New Year Honours, for services to bacteriology.\n\nLister died of a heart attack in the library of the Institute for Medical Research.\n\n"}
{"id": "39331424", "url": "https://en.wikipedia.org/wiki?curid=39331424", "title": "Séralini affair", "text": "Séralini affair\n\nThe Séralini affair was the controversy surrounding the publication, retraction, and republication of a journal article by French molecular biologist Gilles-Éric Séralini. First published by \"Food and Chemical Toxicology \" in September 2012, the article presented a two-year feeding study in rats, and reported an increase in tumors among rats fed genetically modified corn and the herbicide RoundUp. Scientists and regulatory agencies subsequently concluded that the study's design was flawed and its findings unsubstantiated. A chief criticism was that each part of the study had too few rats to obtain statistically useful data, particularly because the strain of rat used, Sprague Dawley, develops tumors at a high rate over its lifetime.\n\nThe publicity surrounding publication of the article also attracted criticism, with science writer Declan Butler calling it \"a tightly orchestrated media offensive\". As part of a news embargo, Séralini required journalists to sign an unusual confidentiality agreement in exchange for advance access to the article, prohibiting them from conferring with other scientists before the press conference announcing publication. At the press conference, Séralini emphasized the study's potential cancer implications, and photographs from the article of treated rats with large tumors were widely circulated by the media. The French Society of Toxicologic Pathology pointed out that, because such tumors are commonly found in older rats, the inclusion in the article of those images from treated rats, without also showing control rats, was misleading. Séralini also released a book and documentary film about the study in conjunction with the press conference.\n\nFollowing widespread criticism by scientists, \"Food and Chemical Toxicology\" retracted the paper in November 2013 after the authors refused to withdraw it. The editor-in-chief said that the article was retracted because its data were inconclusive and its conclusions unreliable. In June 2014 an amended version of the article was republished in \"Environmental Sciences Europe\", and the raw data were made public. According to writer Nathanael Johnson, not all of the raw data was, in fact, released. The journal did not conduct any further peer review; reviewers checked only that the scientific content of the paper had not changed.\n\nSéralini, a professor of molecular biology at the University of Caen, is president of the scientific advisory board of the Committee of Research and Independent Information on Genetic Engineering (CRIIGEN), which opposes genetically modified food (GM food). Séralini co-founded CRIIGEN in 1999 because he judged that studies on GM food safety were inadequate.\n\nBefore 2012 Séralini had published other peer-reviewed papers that concluded there were health risks to GM foods. In 2007 he and two others published a Greenpeace-funded study (Séralini 2007). It concluded that MON 863, a corn rootworm-resistant Bt corn developed by Monsanto, caused health problems in rats, including weight changes, triglyceride level increases in females, changes in urine composition in males, and reduced function or organ damage in the liver, kidney, adrenal glands, heart and haematopoietic system. The European Food Safety Authority (EFSA) concluded that all blood chemistry and organ weight values fell within the normal range for control animals, and that the paper had used incorrect statistical methods. The French Commission du Génie Biomoléculaire (AFBV) also criticized the study's conclusions.\n\nIn 2009 the Séralini lab published another study (Séralini 2009), which re-analyzed toxicity data for NK 603 (glyphosate resistant), MON 810 and MON 863 strains. The data included three rat-feeding studies published by Monsanto scientists on MON 810. This study concluded that the three crops caused liver, kidney and heart damage in the rats. The EFSA concluded that the authors' claims were not supported by their data, that many of the statistical criticisms of Séralini 2007 applied to Séralini 2009, and that the study included no new information that would change the EFSA's conclusions. The French (High Council of Biotechnologies Scientific Committee or HCB) reviewed Séralini 2009 and concluded that it \"presents no admissible scientific element likely to ascribe any haematological, hepatic or renal toxicity to the three re-analysed GMOs.\" The HCB questioned the authors' independence, noting that, in 2010, the \"body to which the authors belong\" displayed material from a 2008 Austrian anti-GM study, the results of which had been acknowledged as mistaken by the study's authors. Food Standards Australia New Zealand concluded that the results of Séralini 2009 were due to chance alone.\n\nIn 2010 Séralini sued , president of the , for libel, after Fellous criticized Séralini's research, in part because it was funded by Greenpeace. The judge ruled that the charge about the funding was defamatory. Fellous was fined €1000; Séralini was awarded a symbolic €1 in damages.\n\nA 2011 article by the Séralini lab that reviewed 19 published animal-feeding studies, as well as data from animal-feeding studies submitted for regulatory approval, concluded that GM food had liver and kidney effects that were sex and dose dependent, and advocated for longer and more elaborate toxicology tests for regulatory approval.\n\nOn 19 September 2012, the journal \"Food and Chemical Toxicology\" published a peer-reviewed paper entitled \"Long term toxicity of a Roundup herbicide and a Roundup-tolerant genetically modified maize.\" The two-year toxicity study, which cost €3.2 million, was conducted at the University of Caen by Séralini and seven colleagues. It had been funded by and run with the collaboration of CRIIGEN.\n\nThe study used 100 male and 100 female Sprague Dawley rats, divided into twenty groups with 10 rats each. Ten diets were tested separately on the males and females. The diets comprised 11 percent, 22 percent and 33 percent genetically modified corn (NK603) and the rest standard laboratory rat food; NK603 corn that had been treated with Roundup, also at 11, 22 and 33 percent; and corn that had not been genetically modified, accompanied by differing concentrations of Roundup in the water. A control group was fed 33 percent non-GMO corn; the rest of their diet was standard laboratory rat food.\n\nThe paper's abstract stated: \"In females, all treated groups died 2–3 times more than controls, and more rapidly. This difference was visible in 3 male groups fed GMOs. All results were hormone and sex dependent, and the pathological profiles were comparable.\"\n\nSéralini held a press conference on the day the study was released in which he \"promoted the cancer results as the study’s major finding.\" At the press conference he also announced the release of a book and film about the study. Selected journalists were given early access to the paper on condition they sign a confidentiality agreement, which meant they were unable to confer with other scientists before the embargo expired. In contrast, embargo guidelines by journals such as \"Nature\" allow reporters to check their stories with independent experts.\n\nSeralini's approach was widely criticized. A \"Nature\" editorial called it \"a public-relations offensive.\" The result of the confidentiality agreement, the journal said, was that critical commentary was absent from the first round of stories, the ones most likely to be remembered. The press conference and publication occurred weeks before the vote on California Proposition 37, which called for labeling genetically modified food. The study was cited by supporters of the proposition.\n\nThe ethics committee of the French National Centre for Scientific Research wrote that Seralini's public-relations approach was \"inappropriate for a high-quality and objective scientific debate.\" Science journalist Carl Zimmer criticized the science journalists who participated. \"Cosmos Magazine\"'s Elizabeth Finkel said that the confidentiality clause had allowed Seralini's story to \"prance unfettered\" before second opinions arrived.\n\nThe study was criticized by various regulatory authorities and scientists. With few exceptions, the scientific community dismissed the study and called for a more rigorous peer-review system in scientific journals.\n\nMany said that Séralini's conclusions were impossible to justify given the statistical power of the study. Sprague-Dawley rats have a lifespan of about two years and have a high risk of cancer over their lifespan (one study concluded that over eighty percent of males and over seventy percent of females developed cancer under normal conditions). The Séralini experiment covered the normal lifespan of these rats. The longer an experiment continues, the more rats get cancer naturally, that makes it harder to separate statistical \"noise\" from the hypothetical signal. For the study to achieve such separation (statistical power), each control and test group would have to include sufficiently many subjects. Organisation for Economic Co-operation and Development (OECD) guidelines recommend 20 rats for chemical-toxicity studies, and 50 rats for carcinogenicity studies. In addition, if the survival of the rats is less than 50% at 104 weeks (which is likely for Sprague-Dawley rats) the recommended number of rats is 65. The Séralini study had only ten per group.\n\nTom Sanders from King's College London noted a lack of data on amount of food given, and on growth rates. Further noting that rats are susceptible to mammary tumors when food intake is not restricted. Sanders said, \"The statistical methods are unconventional ... and it would appear the authors have gone on a statistical fishing trip.\"\n\nThe \"Washington Post\" quoted Marion Nestle, the Paulette Goddard professor in the Department of Nutrition, Food Studies and Public Health at New York University and food safety advocate: \"'[I] can’t figure it out yet...It’s weirdly complicated and unclear on key issues: what the controls were fed, relative rates of tumors, why no dose relationship, what the mechanism might be. I can’t think of a biological reason why GMO corn should do this...So even though I strongly support labeling, I’m skeptical of this study.'\" Likewise, Dan Charles, writing for NPR, noted that in the study, rats that ate 33% GM food developed fewer tumors than did those who ate 11% GM food, suggesting the absence of a dose response. University of Calgary Professor Maurice Moloney publicly wondered why the paper contained so many pictures of treated rats with horrific tumors, but no pictures of control group rats.\n\nMany national food safety and regulatory agencies condemned the paper. The German Federal Institute for Risk Assessment VP Reiner Wittkowski said in a statement, \"The study shows both shortcomings in study design and in the presentation of the collected data. This means that the conclusions drawn by the authors are not supported by the available data.\" A joint report by three Canadian regulatory agencies also \"identified significant shortcomings in the study design, implementation and reporting.\" Similar conclusions were reached by the French HCB and the National Agency for Food Safety, the Vlaams Instituut voor Biotechnologie, the Technical University of Denmark, Food Standards Australia New Zealand, the Brazilian National Technical Commission on Biosafety, and EFSA. EFSA concluded:\n\nThe study as reported by Séralini \"et al.\" was found to be inadequately designed, analysed and reported...The study as described by Séralini et al. does not allow giving weight to their results and conclusions as published. Conclusions cannot be drawn on the difference in tumour incidence between treatment groups on the basis of the design, the analysis and the results as reported. Taking into consideration Member States’ assessments and the authors’ answer to critics, EFSA finds that the study as reported by Séralini et al. is of insufficient scientific quality for safety assessments.\n\nThe European Federation of Biotechnology industry association, which counts Monsanto and other biotech firms among its members, called for the paper to be retracted, calling its publication a \"dangerous failure of the peer-review system.\" Six French national academies (of Agriculture, Medicine, Pharmacy, Science, Technology and Veterinarians) issued a joint statement – \"an extremely rare event in French science\" – condemning the study and the journal that published it. The joint statement dismissed the study as 'a scientific non-event'. FCT\",\" an Elsevier imprint, has a peer review process, and at least three scientists reviewed the paper prior to publication. The journal published a statement in their November 2012 issue, that \"the Editors have encouraged those people with concerns to write formally to the Editor-in-Chief, so that their views can be publicly aired.\"\n\nIn March 2013 FCT published a letter from Erio Barale-Thomas, Principal Scientist of Johnson & Johnson Pharmaceutical Research and Development and the President of the Conseil d’Administration of The Société Française de Pathologie Toxicologique (SFPT, French Society of Toxicologic Pathology). SFPT is \"a non governmental/non profit organization formed by veterinarians, physicians, pharmacists and biologists specialized in veterinary and toxicologic pathology. Its aim is to promote knowledge in pathology, toxicology and laboratory animal sciences for safety studies of drugs, chemicals and food products, and the role of the pathologist in the study design and data interpretation.\" The letter criticized the Seralini study on several fronts, and concluded: \"However, given this study presents serious deficiencies in the protocol, the procedures and the interpretation of the results, the SFPT cannot support any of the scientific claims drawn by the authors, and any relevance for human risk assessment. This letter presents the consensus scientific opinion of the Conseil d’Administration of the SFPT.\"\n\nThe Belgian Federal Minister of Public Health asked the Belgian Biosafety Advisory Council (BBAC) to evaluate the paper. The BBAC was asked to \"inform the Minister whether this paper (i) contains new scientific information with regard to risks for human health of GM maize NK603 and (ii) whether this information triggers a revision of the current authorisation for commercialisation for food and feed use of this GM maize in the European Union (EU).\" The BBAC committee, whose members are drawn from the Belgian biotech Professoriat, pointed out that \"the long duration of this study is a positive aspect since most of the toxicity studies on GMOs are performed on shorter periods,\" and concluded that:\n\"Given the shortcomings identified by the experts regarding the experimental design, the statistical analysis, the interpretation of the results, the redaction of the article and the presentation of the results, the Biosafety Advisory Council concludes that this study does not contain new scientifically relevant elements that may lead to reconsider immediately the current authorisation for food and feed use of GM maize NK603. Considering the issues raised by the study (i.e. long term assessment), the Biosafety Advisory Council proposes EFSA urgently to study in depth the relevance of the actual guidelines and procedures. It can find inspiration in the GRACE project to find useful information and new concerted ideas.\" \n\nThe study was also criticized by the European Society of Toxicologic Pathology, which expressed shock at the way the rats in the study were treated and questioned whether the study was legal to perform under European law.\n\nA 2015 reanalysis of multiple animal studies found that Seralini chose to forgo statistical tests in the main conclusions of the study. Using Seralini's published numerical data, the review found no significant effects on animal health after analysis with statistical tests. The finding that \"in females, all the treated groups died 2–3 times more than controls\" was not statistically significant. The highest mortality was observed for the group of female rats fed 22% genetically modified maize. This difference was not statistically significant. Seralini also originally claimed males in groups fed 22% and 33% genetically modified maize had three times lower mortality than controls, but this was also not statistically significant. The findings of liver necrosis and mammary tumors were also not significant.\n\nA 2017 study found that since it was retracted, Seralini et al. (2012) had been cited 60 times after it was retracted, and that more of these citations were negative (39%) than were positive (26%).\n\nSéralini and supporters defended the study design, the interpretation of the results, and manner and content of the publication. Support for the study came from the European Network of Scientists for Social and Environmental Responsibility (ENSSER), of which CRIIGEN is a member. A subsequent study published in 2013 by ENSSER concluded that EFSA (European Food Safety Authority) applied double standards in evaluation of feeding studies, criticized EFSA's applied criteria. An open letter in support of Seralini's article, signed by about 130 scientists, scholars, and activists, was published in \"Independent Science News\", a project of the Bioscience Resource Project.\n\nSéralini responded to criticisms of his methodology (and specifically a lack of difference between rodent groups at higher doses) with a July 2015 paper in \"PLOS ONE\" claiming that all laboratory rodent diets are contaminated with \"dangerous\" levels of GMOs. This has been strongly criticised by numerous experts, for example, Tamara Galloway said that the study \"speculates beyond the evidence presented in this paper\".\n\nOther Séralini supporters criticized the retraction of the study, concluding the response was a product of industry-driven campaign and regard this as a concerning example of industry interference in the scientific process.\n\nAt the time of the initial release, French Prime Minister Jean-Marc Ayrault said that, if the results are confirmed, the government would press for a Europe-wide ban on the maize and The European Commission instructed the EFSA in Parma, Italy, to assess the study. In late September 2012, Russia temporarily suspended importing GM corn as a result of the study and in November 2012, Kenya banned all GM crops.\n\nThe press conference led to widespread negative media coverage for GM food, especially in Europe. \"Le Nouvel Observateur\" covered the press conference in a story called, \"Yes, GMOs are poisons!\".\n\nJon Entine in \"Forbes\" stated, \"Seralini's research is anomalous. Previous peer-reviewed rat feeding studies using the same products (NK603 and Roundup) have not found any negative food safety impacts. The Japanese Department of Environmental Health and Toxicology released a 52-week feeding study of GM soybeans in 2007, finding \"no apparent adverse effect in rats.\" In 2012, a team of scientists at the University of Nottingham School of Biosciences released a review of 12 long-term studies (up to two years) and 12 multi-generational studies (up to 5 generations) of GM foods, concluding there is no evidence of health hazards.\" Andrew Revkin wrote in a blog the study was another instance of \"single-study syndrome\", and that the study was in support of an \"agenda\".\n\nHenry I. Miller, in an opinion piece for Forbes, said \"[Seralini] has crossed the line from merely performing and reporting flawed experiments to committing gross scientific misconduct and attempting fraud.\" Séralini responded by saying, \"...that he won't make any data available to the EFSA and the BfR until the EFSA makes public all the data under-pinning its 2003 approval of NK603 maize for human consumption and animal feed.\"\n\n\"The Guardian\"'s Environmental Blog stated that the study linking GM maize to cancer \"must be taken seriously by regulators\" and that although it \"attracted a torrent of abuse\", \"it cannot be swept under the carpet\". They also noted CRIIGEN's funding of the research and reported Séralini's response: namely, that studies in support of GM food are usually funded by \"corporates or by pro-biotech institutions\". Proponents of California's GM labeling referendum, Proposition 37, hailed the study.\n\nA statement about the controversy, and especially the attacks on Seralini, was published in \"Le Monde,\" signed by 140 French scientists; the letter said: \"...the protocol followed in this study presents defects that are subject to debate within the scientific community... We are deeply shocked by the image of our community that this controversy gives citizens. The risk expertise to human health or the environment is a difficult activity which is facing many uncertainties. Many of the threats to our planet have been revealed by scientists isolated and confirmed by many studies coming from the scientific community. In this case, it would be more efficient to implement research on the health and environmental risks of GMOs and pesticides, improve toxicological protocols used for placing on the market and finance a variety of researchers in this domain...\"\n\nIn 2012 Séralini sued the editor of \"Marianne\" and journalist Jean-Claude Jaillet for defamation after they accused him of fraud. The High Court of Paris ruled in Seralini's favor in 2015. The court said that the fraud allegation had first been made by Henry I. Miller in \"Forbes\". The journalist was fined a total of 3,500 euros, while the editor was fined twice that amount because of past convictions.\n\nIn November 2013, Elsevier announced that FCT was retracting the paper, after the authors refused to withdraw it. The journal's editors concluded that while there was \"no evidence of fraud or intentional misrepresentation of the data\", the results were inconclusive and \"[did] not reach the threshold of publication for Food and Chemical Toxicology\". After an in-depth look at the study's raw data, no definitive conclusions could be reached regarding the role of either NK603 or glyphosate in overall mortality or tumor rates, given the high incidence of tumors in Sprague-Dawley rats and the small sample size. Normal variance could not be excluded as the cause of the results. Following many enquiries about the retraction, FCT's editor-in-chief said that:\nSéralini and his supporters strongly objected to the retraction, and Séralini himself threatened to sue FCT. A bioethicist with the NIH examined the case and wrote in the \"Journal of Agricultural and Environmental Ethics \" that articles should not be retracted for inconclusiveness, but that retraction due to flaws in study design or due to ethical violations may be appropriate, and that republication of retracted papers should occur only after additional peer review.\n\nOn 1 August 2017, as part of a lawsuit against Monsanto, documents were released showing, among other things, that the Editor-in-Chief, Wallace Hayes, had once had a contractual relationship with Monsanto. Hayes said in an interview that he did not have a contract with Monsanto when he retracted Seralini's paper, and that his decision to retract it was not influenced by Monsanto at all.\n\nIn June 2014, the original study was republished with the addition of the entire data set, in the journal \"Environmental Sciences Europe\". The entire data set was published because of requests from the national regulatory bodies CFIA, EFSA, FSANZ, ANSES and BfR.\n\nThe editor said that the paper was republished without further scientific peer review, \"because this had already been conducted by \"Food and Chemical Toxicology\", and had concluded there had been no fraud nor misrepresentation.\" The republication renewed the controversy, but now with additional controversy over the behavior of the editors of both journals.\n\nIn July 2015, the International Agency for Research on Cancer published a monograph on glyphosate, which contained an evaluation of the Séralini paper as republished in June 2014 and the conclusion, that the study \"was inadequate for evaluation because the number of animals per group was small, the histopathological description of tumours was poor, and incidences of tumours for individual animals were not provided.\"\n\n"}
{"id": "1171583", "url": "https://en.wikipedia.org/wiki?curid=1171583", "title": "The Computational Brain", "text": "The Computational Brain\n\nThe Computational Brain is a book by Patricia Churchland and Terrence J. Sejnowski and published in 1992 by The MIT Press, Cambridge, Massachusetts, . It has cover blurbs by Karl Pribram, Francis Crick, and Carver Mead.\n"}
{"id": "3573788", "url": "https://en.wikipedia.org/wiki?curid=3573788", "title": "Thermal science", "text": "Thermal science\n\nThermal science is the combined study of thermodynamics, fluid mechanics, heat transfer, and combustion.\n\nIntroductory subjects studied in thermal science generally are focused on thermodynamics. These include studies of properties of pure substances, pressure-volume-temperature diagrams, the ideal gas law, heat and its relationship to work, heat transfer, the laws of thermodynamics, engine and refrigeration cycles, and combustion.\n\nAnother area of concern in thermal science is fluid mechanics. These include fluid statics, fluid flows, e.g. laminar flow vs. turbulent flow, the Bernoulli equation. The applicability of this area is piping networks, turbo-machineries, airfoils.\n\nHeat transfer has applications for heat exchangers, heat engines, heating, ventilating, and air-conditioning, and cooling of microelectronics.\n\nCombustion involves both thermal effects and chemical reaction.\n\nIntensive study of thermal sciences requires additional knowledge and experience in other areas such as experimental techniques and numerical or computational methods.\n\n\nPotter, M. & Scott, E. (2003). \"Thermal Sciences: An Introduction to Thermodynamics, Fluid Mechanics, and Heat Transfer.\" New York: Thomson-Engineering. \n\nFaghri, A., Zhang, Y., and Howell, J. R. (2010). \"Advanced Heat and Mass Transfer.\" Global Digital Press, Columbia, MO. \n\n\n"}
{"id": "6952289", "url": "https://en.wikipedia.org/wiki?curid=6952289", "title": "Undoing Gender", "text": "Undoing Gender\n\nUndoing Gender is a 2004 book by the philosopher Judith Butler.\n\nButler examines gender, sex, psychoanalysis and the way medicine and the law treat intersex and transgender people. Focusing on the case of David Reimer who was medically reassigned from male to female after a botched circumcision, Butler reexamines the theory of performativity that she originally explored in \"Gender Trouble\" (1990). David then renamed as a girl- Brenda, rediscovers their masculinity and goes on to live his life as a male again. While many of Butler's books are intended for a highly academic audience, \"Undoing Gender\" reaches out to a much broader readership.\n\nButler discusses how gender is performed without one being conscious of it, but says that it does not mean this performativity is \"automatic or mechanical\". She argues that we have desires that do not originate from our personhood, but rather, from social norms. The philosopher also debates our notions of \"human\" and \"less-than-human\" and how these culturally imposed ideas can keep one from having a \"viable life\" as the biggest concerns are usually about whether a person will be accepted if their desires differ from normality. She states that one may feel the need of being recognized in order to live, but that at the same time, the conditions to be recognized make life \"unlivable\". The writer proposes an interrogation of such conditions so that people who resist them may have more possibilities of living.\n\n\n"}
