{"id": "40695634", "url": "https://en.wikipedia.org/wiki?curid=40695634", "title": "Al-Adami", "text": "Al-Adami\n\nʿAbū ʿAlī al‐Ḥusayn ibn Muḥammad al‐Ādamī (flourished in Baghdad, c. 925) was a maker of scientific instruments who wrote an extant work on vertical sundials. According to al-Biruni, al-Adami was the first to construct a \"disc of\neclipses\", an instrument which demonstrates solar and lunar eclipses.\n\nHe should not be confused with his son Ibn al-Adami.\n\n"}
{"id": "14597914", "url": "https://en.wikipedia.org/wiki?curid=14597914", "title": "Alain Haché", "text": "Alain Haché\n\nAlain Haché (born 14 December 1970, in Tracadie, New Brunswick) is an experimental physicist, a professor at the University of Moncton, Canada. From 2003 to 2013 he held the Canada Research Chair in Photonics. He is also the author of \"The Physics of Hockey\" and \"Slap Shot Science\", two popular science books on ice hockey.\n\nIn 2002, he and undergraduate student Louis Poirier transmitted faster-than-light electrical pulses through a 120-metre long \"photonic crystal\" made of coaxial cables of alternating characteristic impedance (12 pairs of 50 Ω and 75 Ω cables). The experiment showed that the pulse envelope was recreated at the end of the cables at a speed of >3 \"c\". Since this speed represents the group velocity but not the signal velocity, no energy or information was actually traveling faster than light.\n\n"}
{"id": "57267186", "url": "https://en.wikipedia.org/wiki?curid=57267186", "title": "Alexander MacPherson (explorer)", "text": "Alexander MacPherson (explorer)\n\nAlexander MacPherson was on the Burke and Wills expedition support expedition from the Murray River to Coopers Creek, in September 1860.\n\nHe and Trouper Lyons became lost in the stoney desert south of Coopers Creek and were saved by a Barkinji guide called Dick. \" When they became lost and desperately short of provisions and water, Dick conveyed them to the care of local Aborigines\" (probably relatives) \"walked for eight days after having run his horse into the ground, and\" called for help.\n"}
{"id": "43340434", "url": "https://en.wikipedia.org/wiki?curid=43340434", "title": "Alfred Schmidt bibliography", "text": "Alfred Schmidt bibliography\n\nThe following is a list of the works by Alfred Schmidt, a 20th-century German philosopher, sociologist and critical theorist associated closely with the Frankfurt School. This list also includes information regarding his work as translator and editor.\n\n\n\n\n\n\n\n\n"}
{"id": "1845", "url": "https://en.wikipedia.org/wiki?curid=1845", "title": "Alternative medicine", "text": "Alternative medicine\n\nAlternative medicine, fringe medicine, pseudomedicine or simply questionable medicine is the use and promotion of practices which are unproven, disproven, impossible to prove, or excessively harmful in relation to their effect — in the attempt to achieve the healing effects of medicine. They differ from experimental medicine in that the latter employs responsible investigation, and accepts results that show it to be ineffective. The scientific consensus is that alternative therapies either do not, or cannot, work. In some cases laws of nature are violated by their basic claims; in some the treatment is so much worse that its use is unethical. Alternative practices, products, and therapies range from only ineffective to having known harmful and toxic effects. \n\nAlternative therapies may be credited for perceived improvement through placebo effects, decreased use or effect of medical treatment (and therefore either decreased side effects; or nocebo effects towards standard treatment), or the natural course of the condition or disease. Alternative treatment is not the same as experimental treatment or traditional medicine, although both can be misused in ways that are alternative. Alternative or complementary medicine is dangerous because it may discourage people from getting the best possible treatment, and may lead to a false understanding of the body and of science.\n\nAlternative medicine is used by a significant number of people, though its popularity is often overstated. Large amounts of funding go to testing alternative medicine, with more than US$2.5 billion spent by the United States government alone. Almost none show any effect beyond that of false treatment, and most studies showing any effect have been statistical flukes. Alternative medicine is a highly profitable industry, with a strong lobby. This fact is often overlooked by media or intentionally kept hidden, with alternative practice being portrayed positively when compared to \"big pharma\". The lobby has successfully pushed for alternative therapies to be subject to far less regulation than conventional medicine. Alternative therapies may even be allowed to promote use when there is demonstrably no effect, only a tradition of use. Regulation and licensing of alternative medicine and health care providers varies between and within countries. Despite laws making it illegal to market or promote alternative therapies for use in cancer treatment, many practitioners promote them. Alternative medicine is criticized for taking advantage of the weakest members of society.\n\nTerminology has shifted over time, reflecting the preferred branding of practitioners. For example, the United States National Institutes of Health department studying alternative medicine, currently named National Center for Complementary and Integrative Health, was established as the \"Office of Alternative Medicine\" and was renamed the \"National Center for Complementary and Alternative Medicine\" before obtaining its current name. Therapies are often framed as \"natural\" or \"holistic\", in apparent opposition to conventional medicine which is \"artificial\" and \"narrow in scope\", statements which are intentionally misleading. When used together with functional medical treatment, alternative therapies do not \"complement\" (improve the effect of, or mitigate the side effects of) treatment. Significant drug interactions caused by alternative therapies may instead negatively impact functional treatment by making prescription drugs less effective, such as interference by herbal preparations with warfarin.\n\nAlternative medicine is defined loosely as a set of products, practices, and theories that are believed or perceived by their users to have the healing effects of medicine, but whose effectiveness has not been clearly established using scientific methods, or whose theory and practice is not part of biomedicine, or whose theories or practices are directly contradicted by scientific evidence or scientific principles used in biomedicine. \"Biomedicine\" or \"medicine\" is that part of medical science that applies principles of biology, physiology, molecular biology, biophysics, and other natural sciences to clinical practice, using scientific methods to establish the effectiveness of that practice. Unlike medicine, an alternative product or practice does not originate from using scientific methods, but may instead be based on hearsay, religion, tradition, superstition, belief in supernatural energies, pseudoscience, errors in reasoning, propaganda, fraud, or other unscientific sources.\n\nIn \"General Guidelines for Methodologies on Research and Evaluation of Traditional Medicine\", published in 2000 by the World Health Organization (WHO), complementary and alternative medicine were defined as a broad set of health care practices that are not part of that country's own tradition and are not integrated into the dominant health care system.\n\nThe expression also refers to a diverse range of related and unrelated products, practices, and theories ranging from biologically plausible practices and products and practices with some evidence, to practices and theories that are directly contradicted by basic science or clear evidence, and products that have been conclusively proven to be ineffective or even toxic and harmful.\n\nThe terms \"alternative medicine\", \"complementary medicine\", \"integrative medicine,\" \"holistic medicine\", \"natural medicine\", \"unorthodox medicine\", \"fringe medicine\", \"unconventional medicine\", and \"new age medicine\" are used interchangeably as having the same meaning and are almost synonymous in most contexts.\n\nThe meaning of the term \"alternative\" in the expression \"alternative medicine\", is not that it is an effective alternative to medical science, although some alternative medicine promoters may use the loose terminology to give the appearance of effectiveness. Loose terminology may also be used to suggest meaning that a dichotomy exists when it does not, e.g., the use of the expressions \"western medicine\" and \"eastern medicine\" to suggest that the difference is a cultural difference between the Asiatic east and the European west, rather than that the difference is between evidence-based medicine and treatments that do not work.\n\nComplementary medicine (CM) or integrative medicine (IM) is when alternative medicine is used together with functional medical treatment, in a belief that it improves the effect of treatments. However, significant drug interactions caused by alternative therapies may instead negatively influence treatment, making treatments less effective, notably cancer therapy. Both terms refer to use of alternative medical treatments alongside conventional medicine, an example of which is use of acupuncture (sticking needles in the body to influence the flow of a supernatural energy), along with using science-based medicine, in the belief that the acupuncture increases the effectiveness or \"complements\" the science-based medicine.\n\nCAM is an abbreviation of the phrase \"complementary and alternative medicine\". It has also been called sCAM or SCAM with the addition of \"so-called\" or \"supplements\".\n\n\"Allopathic medicine\" or \"allopathy\" is an expression commonly used by homeopaths and proponents of other forms of alternative medicine to refer to mainstream medicine. It was used to describe the traditional European practice of heroic medicine, but later continued to be used to describe anything that was not homeopathy.\n\nAllopathy refers to the use of pharmacologically active agents or physical interventions to treat or suppress symptoms or pathophysiologic processes of diseases or conditions. The German version of the word, , was coined in 1810 by the creator of homeopathy, Samuel Hahnemann (1755–1843). The word was coined from (different) and (relating to a disease or to a method of treatment). In alternative medicine circles the expression \"allopathic medicine\" is still used to refer to \"the broad category of medical practice that is sometimes called Western medicine, biomedicine, evidence-based medicine, or modern medicine\" (see the article on scientific medicine).\n\nUse of the term remains common among homeopaths and has spread to other alternative medicine practices. The meaning implied by the label has never been accepted by conventional medicine and is considered pejorative. More recently, some sources have used the term \"allopathic\", particularly American sources wishing to distinguish between Doctors of Medicine (MD) and Doctors of Osteopathic Medicine (DO) in the United States. William Jarvis, an expert on alternative medicine and public health, states that \"although many modern therapies can be construed to conform to an allopathic rationale (e.g., using a laxative to relieve constipation), standard medicine has never paid allegiance to an allopathic principle\" and that the label \"allopath\" was from the start \"considered highly derisive by regular medicine\".\n\nMany conventional medical treatments do not fit the nominal definition of \"allopathy\", as they seek to prevent illness, or remove its cause.\n\nCAM is an abbreviation of complementary and alternative medicine. It has also been called sCAM or SCAM with the addition of \"so-called\" or \"supplements\". The words balance and holism are often used, claiming to take into account a \"whole\" person, in contrast to the supposed reductionism of medicine. Due to its many names the field has been criticized for intense rebranding of what are essentially the same practices: as soon as one name is declared synonymous with quackery, a new name is chosen.\n\nTraditional medicine refers to the pre-scientific practices of a certain culture, contrary to what is typically practiced in other cultures where medical science dominates.\n\n\"Eastern medicine\" typically refers to the traditional medicines of Asia where conventional bio-medicine penetrated much later.\n\nThe words \"balance\" and \"holism\" are often used alongside \"complementary\" or \"integrative\" medicine, claiming to take into account a \"whole\" person, in contrast to the supposed reductionism of medicine. Due to its many names the field has been criticized for intense rebranding of what are essentially the same practices.\n\nProminent members of the science and biomedical science community say that it is not meaningful to define an alternative medicine that is separate from a conventional medicine, that the expressions \"conventional medicine\", \"alternative medicine\", \"complementary medicine\", \"integrative medicine\", and \"holistic medicine\" do not refer to any medicine at all.\n\nOthers in both the biomedical and CAM communities say that CAM \"cannot\" be precisely defined because of the diversity of theories and practices it includes, and because the boundaries between CAM and biomedicine overlap, are porous, and change. The expression \"complementary and alternative medicine\" (CAM) resists easy definition because the health systems and practices it refers to are diffuse, and its boundaries poorly defined. Healthcare practices categorized as alternative may differ in their historical origin, theoretical basis, diagnostic technique, therapeutic practice and in their relationship to the medical mainstream. Some alternative therapies, including traditional Chinese medicine (TCM) and Ayurveda, have antique origins in East or South Asia and are entirely alternative medical systems; others, such as homeopathy and chiropractic, have origins in Europe or the United States and emerged in the eighteenth and nineteenth centuries. Some, such as osteopathy and chiropractic, employ manipulative physical methods of treatment; others, such as meditation and prayer, are based on mind-body interventions. Treatments considered alternative in one location may be considered conventional in another. Thus, chiropractic is not considered alternative in Denmark and likewise osteopathic medicine is no longer thought of as an alternative therapy in the United States.\n\nCritics say the expression is deceptive because it implies there is an effective alternative to science-based medicine, and that \"complementary\" is deceptive because it implies that the treatment increases the effectiveness of (complements) science-based medicine, while alternative medicines that have been tested nearly always have no measurable positive effect compared to a placebo.\n\nOne common feature of all definitions of alternative medicine is its designation as \"other than\" conventional medicine. For example, the widely referenced descriptive definition of complementary and alternative medicine devised by the US National Center for Complementary and Integrative Health (NCCIH) of the National Institutes of Health (NIH), states that it is \"a group of diverse medical and health care systems, practices, and products that are not generally considered part of conventional medicine\". For conventional medical practitioners, it does not necessarily follow that either it or its practitioners would no longer be considered alternative.\n\nSome definitions seek to specify alternative medicine in terms of its social and political marginality to mainstream healthcare. This can refer to the lack of support that alternative therapies receive from the medical establishment and related bodies regarding access to research funding, sympathetic coverage in the medical press, or inclusion in the standard medical curriculum. In 1993, the British Medical Association (BMA), one among many professional organizations who have attempted to define alternative medicine, stated that it referred to \"...those forms of treatment which are not widely used by the conventional healthcare professions, and the skills of which are not taught as part of the undergraduate curriculum of conventional medical and paramedical healthcare courses\". In a US context, an influential definition coined in 1993 by the Harvard-based physician, David M. Eisenberg, characterized alternative medicine \"as interventions neither taught widely in medical schools nor generally available in US hospitals\". These descriptive definitions are inadequate in the present-day when some conventional doctors offer alternative medical treatments and CAM introductory courses or modules can be offered as part of standard undergraduate medical training; alternative medicine is taught in more than 50 per cent of US medical schools and increasingly US health insurers are willing to provide reimbursement for CAM therapies. In 1999, 7.7% of US hospitals reported using some form of CAM therapy; this proportion had risen to 37.7% by 2008.\n\nAn expert panel at a conference hosted in 1995 by the US Office for Alternative Medicine (OAM), devised a theoretical definition of alternative medicine as \"a broad domain of healing resources ... other than those intrinsic to the politically dominant health system of a particular society or culture in a given historical period\". This definition has been widely adopted by CAM researchers, cited by official government bodies such as the UK Department of Health, attributed as the definition used by the Cochrane Collaboration, and, with some modification, was preferred in the 2005 consensus report of the US Institute of Medicine, \"Complementary and Alternative Medicine in the United States\".\n\nThe 1995 OAM conference definition, an expansion of Eisenberg's 1993 formulation, is silent regarding questions of the medical effectiveness of alternative therapies. Its proponents hold that it thus avoids relativism about differing forms of medical knowledge and, while it is an essentially political definition, this should not imply that the dominance of mainstream biomedicine is solely due to political forces. According to this definition, alternative and mainstream medicine can only be differentiated with reference to what is \"intrinsic to the politically dominant health system of a particular society of culture\". However, there is neither a reliable method to distinguish between cultures and subcultures, nor to attribute them as dominant or subordinate, nor any accepted criteria to determine the dominance of a cultural entity. If the culture of a politically dominant healthcare system is held to be equivalent to the perspectives of those charged with the medical management of leading healthcare institutions and programs, the definition fails to recognize the potential for division either within such an elite or between a healthcare elite and the wider population.\n\nNormative definitions distinguish alternative medicine from the biomedical mainstream in its provision of therapies that are unproven, unvalidated, or ineffective and support of theories with no recognized scientific basis. These definitions characterize practices as constituting alternative medicine when, used independently or in place of evidence-based medicine, they are put forward as having the healing effects of medicine, but are not based on evidence gathered with the scientific method. Exemplifying this perspective, a 1998 editorial co-authored by Marcia Angell, a former editor of \"The New England Journal of Medicine\", argued that:\n\nIt is time for the scientific community to stop giving alternative medicine a free ride. There cannot be two kinds of medicine – conventional and alternative. There is only medicine that has been adequately tested and medicine that has not, medicine that works and medicine that may or may not work. Once a treatment has been tested rigorously, it no longer matters whether it was considered alternative at the outset. If it is found to be reasonably safe and effective, it will be accepted. But assertions, speculation, and testimonials do not substitute for evidence. Alternative treatments should be subjected to scientific testing no less rigorous than that required for conventional treatments.\n\nThis line of division has been subject to criticism, however, as not all forms of standard medical practice have adequately demonstrated evidence of benefit, and it is also unlikely in most instances that conventional therapies, if proven to be ineffective, would ever be classified as CAM.\n\nSimilarly, the public information website maintained by the National Health and Medical Research Council (NHMRC) of the Commonwealth of Australia uses the acronym \"CAM\" for a wide range of health care practices, therapies, procedures and devices not within the domain of conventional medicine. In the Australian context this is stated to include acupuncture; aromatherapy; chiropractic; homeopathy; massage; meditation and relaxation therapies; naturopathy; osteopathy; reflexology, traditional Chinese medicine; and the use of vitamin supplements.\n\nThe Danish National Board of Health's \"Council for Alternative Medicine\" (Sundhedsstyrelsens Råd for Alternativ Behandling (SRAB)), an independent institution under the National Board of Health (Danish: \"Sundhedsstyrelsen\"), uses the term \"alternative medicine\" for:\n\nProponents of an evidence-base for medicine such as the Cochrane Collaboration (founded in 1993 and from 2011 providing input for WHO resolutions) take a position that \"all\" systematic reviews of treatments, whether \"mainstream\" or \"alternative\", ought to be held to the current standards of scientific method. In a study titled \"Development and classification of an operational definition of complementary and alternative medicine for the Cochrane Collaboration\" (2011) it was proposed that indicators that a therapy is accepted include government licensing of practitioners, coverage by health insurance, statements of approval by government agencies, and recommendation as part of a practice guideline; and that if something is currently a standard, accepted therapy, then it is not likely to be widely considered as CAM.\n\nAlternative medicine consists of a wide range of health care practices, products, and therapies. The shared feature is a claim to heal that is not based on the scientific method. Alternative medicine practices are diverse in their foundations and methodologies. Alternative medicine practices may be classified by their cultural origins or by the types of beliefs upon which they are based. Methods may incorporate or be based on traditional medicinal practices of a particular culture, folk knowledge, superstition, spiritual beliefs, belief in supernatural energies (antiscience), pseudoscience, errors in reasoning, propaganda, fraud, new or different concepts of health and disease, and any bases other than being proven by scientific methods. Different cultures may have their own unique traditional or belief based practices developed recently or over thousands of years, and specific practices or entire systems of practices.\nAlternative medicine, such as using naturopathy or homeopathy in place of conventional medicine, is based on belief systems not grounded in science.\n\nAlternative medical systems may be based on traditional medicine practices, such as traditional Chinese medicine (TCM), Ayurveda in India, or practices of other cultures around the world. Some useful applications of traditional medicines have been researched and accepted within ordinary medicine, however the underlying belief systems are seldom scientific and are not accepted.\n\nTraditional medicine is considered alternative when it is used outside its home region; or when it is used together with or instead of known functional treatment; or when it can be reasonably expected that the patient or practitioner knows or should know that it will not work – such as knowing that the practice is based on superstition.\n\nSince ancient times, in many parts of the world a number of herbs reputed to possess abortifacient properties have been used in folk medicine. Among these are: tansy, pennyroyal, black cohosh, and the now-extinct silphium. Historian of science Ann Hibner Koblitz has written of the probable protoscientific origins of this folk knowledge in observation of farm animals. Women who knew that grazing on certain plants would cause an animal to abort (with negative economic consequences for the farm) would be likely to try out those plants on themselves in order to avoid an unwanted pregnancy.\n\nHowever, modern users of these plants often lack knowledge of the proper preparation and dosage. The historian of medicine John Riddle has spoken of the \"broken chain of knowledge\" caused by urbanization and modernization, and Koblitz has written that \"folk knowledge about effective contraception techniques often disappears over time or becomes inextricably mixed with useless or harmful practices.\" The ill-informed or indiscriminant use of herbs as abortifacients can cause serious and even lethal side-effects.\n\nBases of belief may include belief in existence of supernatural energies undetected by the science of physics, as in biofields, or in belief in properties of the energies of physics that are inconsistent with the laws of physics, as in energy medicine.\n\nSubstance based practices use substances found in nature such as herbs, foods, non-vitamin supplements and megavitamins, animal and fungal products, and minerals, including use of these products in traditional medical practices that may also incorporate other methods. Examples include healing claims for nonvitamin supplements, fish oil, Omega-3 fatty acid, glucosamine, echinacea, flaxseed oil, and ginseng. Herbal medicine, or phytotherapy, includes not just the use of plant products, but may also include the use of animal and mineral products. It is among the most commercially successful branches of alternative medicine, and includes the tablets, powders and elixirs that are sold as \"nutritional supplements\". Only a very small percentage of these have been shown to have any efficacy, and there is little regulation as to standards and safety of their contents. This may include use of known toxic substances, such as use of the poison lead in traditional Chinese medicine.\n\nA US agency, National Center on Complementary and Integrative Health (NCCIH), has created a classification system for branches of complementary and alternative medicine that divides them into five major groups. These groups have some overlap, and distinguish two types of energy medicine: \"veritable\" which involves scientifically observable energy (including magnet therapy, colorpuncture and light therapy) and \"putative\", which invokes physically undetectable or unverifiable energy. None of these energies have any evidence to support that they effect the body in any positive or health promoting way.\n\n\nThe history of alternative medicine may refer to the history of a group of diverse medical practices that were collectively promoted as \"alternative medicine\" beginning in the 1970s, to the collection of individual histories of members of that group, or to the history of western medical practices that were labeled \"irregular practices\" by the western medical establishment. It includes the histories of complementary medicine and of integrative medicine. Before the 1970s, western practitioners that were not part of the increasingly science-based medical establishment were referred to \"irregular practitioners\", and were dismissed by the medical establishment as unscientific and as practicing quackery. Until the 1970s, irregular practice became increasingly marginalized as quackery and fraud, as western medicine increasingly incorporated scientific methods and discoveries, and had a corresponding increase in success of its treatments. In the 1970s, irregular practices were grouped with traditional practices of nonwestern cultures and with other unproven or disproven practices that were not part of biomedicine, with the entire group collectively marketed and promoted under the single expression \"alternative medicine\".\n\nUse of alternative medicine in the west began to rise following the counterculture movement of the 1960s, as part of the rising new age movement of the 1970s. This was due to misleading mass marketing of \"alternative medicine\" being an effective \"alternative\" to biomedicine, changing social attitudes about not using chemicals and challenging the establishment and authority of any kind, sensitivity to giving equal measure to beliefs and practices of other cultures (cultural relativism), and growing frustration and desperation by patients about limitations and side effects of science-based medicine. At the same time, in 1975, the American Medical Association, which played the central role in fighting quackery in the United States, abolished its quackery committee and closed down its Department of Investigation. By the early to mid 1970s the expression \"alternative medicine\" came into widespread use, and the expression became mass marketed as a collection of \"natural\" and effective treatment \"alternatives\" to science-based biomedicine. By 1983, mass marketing of \"alternative medicine\" was so pervasive that the British Medical Journal (BMJ) pointed to \"an apparently endless stream of books, articles, and radio and television programmes urge on the public the virtues of (alternative medicine) treatments ranging from meditation to drilling a hole in the skull to let in more oxygen\".\n\nMainly as a result of reforms following the Flexner Report of 1910 medical education in established medical schools in the US has generally not included alternative medicine as a teaching topic. Typically, their teaching is based on current practice and scientific knowledge about: anatomy, physiology, histology, embryology, neuroanatomy, pathology, pharmacology, microbiology and immunology. Medical schools' teaching includes such topics as doctor-patient communication, ethics, the art of medicine, and engaging in complex clinical reasoning (medical decision-making). Writing in 2002, Snyderman and Weil remarked that by the early twentieth century the Flexner model had helped to create the 20th-century academic health center, in which education, research, and practice were inseparable. While this had much improved medical practice by defining with increasing certainty the pathophysiological basis of disease, a single-minded focus on the pathophysiological had diverted much of mainstream American medicine from clinical conditions that were not well understood in mechanistic terms, and were not effectively treated by conventional therapies.\n\nBy 2001 some form of CAM training was being offered by at least 75 out of 125 medical schools in the US. Exceptionally, the School of Medicine of the University of Maryland, Baltimore includes a research institute for integrative medicine (a member entity of the Cochrane Collaboration). Medical schools are responsible for conferring medical degrees, but a physician typically may not legally practice medicine until licensed by the local government authority. Licensed physicians in the US who have attended one of the established medical schools there have usually graduated Doctor of Medicine (MD). All states require that applicants for MD licensure be graduates of an approved medical school and complete the United States Medical Licensing Exam (USMLE).\n\nThere is a general scientific consensus that alternative therapies lack the requisite scientific validation, and their effectiveness is either unproved or disproved. Many of the claims regarding the efficacy of alternative medicines are controversial, since research on them is frequently of low quality and methodologically flawed. Selective publication bias, marked differences in product quality and standardisation, and some companies making unsubstantiated claims call into question the claims of efficacy of isolated examples where there is evidence for alternative therapies.\n\n\"The Scientific Review of Alternative Medicine\" points to confusions in the general population – a person may attribute symptomatic relief to an otherwise-ineffective therapy just because they are taking something (the placebo effect); the natural recovery from or the cyclical nature of an illness (the regression fallacy) gets misattributed to an alternative medicine being taken; a person not diagnosed with science-based medicine may never originally have had a true illness diagnosed as an alternative disease category.\n\nEdzard Ernst characterized the evidence for many alternative techniques as weak, nonexistent, or negative and in 2011 published his estimate that about 7.4% were based on \"sound evidence\", although he believes that may be an overestimate. Ernst has concluded that 95% of the alternative treatments he and his team studied, including acupuncture, herbal medicine, homeopathy, and reflexology, are \"statistically indistinguishable from placebo treatments\", but he also believes there is something that conventional doctors can usefully learn from the chiropractors and homeopath: this is the therapeutic value of the placebo effect, one of the strangest phenomena in medicine.\n\nIn 2003, a project funded by the CDC identified 208 condition-treatment pairs, of which 58% had been studied by at least one randomized controlled trial (RCT), and 23% had been assessed with a meta-analysis. According to a 2005 book by a US Institute of Medicine panel, the number of RCTs focused on CAM has risen dramatically.\n\n, the Cochrane Library had 145 CAM-related Cochrane systematic reviews and 340 non-Cochrane systematic reviews. An analysis of the conclusions of only the 145 Cochrane reviews was done by two readers. In 83% of the cases, the readers agreed. In the 17% in which they disagreed, a third reader agreed with one of the initial readers to set a rating. These studies found that, for CAM, 38.4% concluded positive effect or possibly positive (12.4%), 4.8% concluded no effect, 0.7% concluded harmful effect, and 56.6% concluded insufficient evidence. An assessment of conventional treatments found that 41.3% concluded positive or possibly positive effect, 20% concluded no effect, 8.1% concluded net harmful effects, and 21.3% concluded insufficient evidence. However, the CAM review used the more developed 2004 Cochrane database, while the conventional review used the initial 1998 Cochrane database.\n\nIn the same way as for conventional therapies, drugs, and interventions, it can be difficult to test the efficacy of alternative medicine in clinical trials. In instances where an established, effective, treatment for a condition is already available, the Helsinki Declaration states that withholding such treatment is unethical in most circumstances. Use of standard-of-care treatment in addition to an alternative technique being tested may produce confounded or difficult-to-interpret results.\n\nCancer researcher Andrew J. Vickers has stated:\n\nContrary to much popular and scientific writing, many alternative cancer treatments have been investigated in good-quality clinical trials, and they have been shown to be ineffective. The label \"unproven\" is inappropriate for such therapies; it is time to assert that many alternative cancer therapies have been \"disproven\".\n\nA research methods expert and author of \"Snake Oil Science\", R. Barker Bausell, has stated that \"it's become politically correct to investigate nonsense.\" There are concerns that just having NIH support is being used to give unfounded \"legitimacy to treatments that are not legitimate.\"\n\nUse of placebos to achieve a placebo effect in integrative medicine has been criticized as, \"...diverting research time, money, and other resources from more fruitful lines of investigation in order to pursue a theory that has no basis in biology.\"\n\nAnother critic has argued that academic proponents of integrative medicine sometimes recommend misleading patients by using known placebo treatments to achieve a placebo effect. However, a 2010 survey of family physicians found that 56% of respondents said they had used a placebo in clinical practice as well. Eighty-five percent of respondents believed placebos can have both psychological and physical benefits.\n\nIntegrative medicine has been criticized in that its practitioners, trained in science-based medicine, deliberately mislead patients by pretending placebos are not. \"quackademic medicine\" is a pejorative term used for \"integrative medicine\", which medical professionals consider an infiltration of quackery into academic science-based medicine.\n\nAn analysis of trends in the criticism of complementary and alternative medicine (CAM) in five prestigious American medical journals during the period of reorganization within medicine (1965–1999) was reported as showing that the medical profession had responded to the growth of CAM in three phases, and that in each phase, changes in the medical marketplace had influenced the type of response in the journals. Changes included relaxed medical licensing, the development of managed care, rising consumerism, and the establishment of the USA Office of Alternative Medicine (later National Center for Complementary and Alternative Medicine, currently National Center for Complementary and Integrative Health). In the \"condemnation\" phase, from the late 1960s to the early 1970s, authors had ridiculed, exaggerated the risks, and petitioned the state to contain CAM; in the \"reassessment\" phase (mid-1970s through early 1990s), when increased consumer utilization of CAM was prompting concern, authors had pondered whether patient dissatisfaction and shortcomings in conventional care contributed to the trend; in the \"integration\" phase of the 1990s physicians began learning to work around or administer CAM, and the subjugation of CAM to scientific scrutiny had become the primary means of control.\n\nPractitioners of complementary medicine usually discuss and advise patients as to available alternative therapies. Patients often express interest in mind-body complementary therapies because they offer a non-drug approach to treating some health conditions.\n\nIn addition to the social-cultural underpinnings of the popularity of alternative medicine, there are several psychological issues that are critical to its growth. One of the most critical is the placebo effect – a well-established observation in medicine. Related to it are similar psychological effects, such as the will to believe, cognitive biases that help maintain self-esteem and promote harmonious social functioning, and the \"post hoc, ergo propter hoc\" fallacy.\n\nThe popularity of complementary & alternative medicine (CAM) may be related to other factors that Edzard Ernst mentioned in an interview in \"The Independent\":\n\nWhy is it so popular, then? Ernst blames the providers, customers and the doctors whose neglect, he says, has created the opening into which alternative therapists have stepped. \"People are told lies. There are 40 million websites and 39.9 million tell lies, sometimes outrageous lies. They mislead cancer patients, who are encouraged not only to pay their last penny but to be treated with something that shortens their lives. \"At the same time, people are gullible. It needs gullibility for the industry to succeed. It doesn't make me popular with the public, but it's the truth.\n\nPaul Offit proposed that \"alternative medicine becomes quackery\" in four ways: by recommending against conventional therapies that are helpful, promoting potentially harmful therapies without adequate warning, draining patients' bank accounts, or by promoting \"magical thinking.\"\n\nAuthors have speculated on the socio-cultural and psychological reasons for the appeal of alternative medicines among the minority using them \"in lieu\" of conventional medicine. There are several socio-cultural reasons for the interest in these treatments centered on the low level of scientific literacy among the public at large and a concomitant increase in antiscientific attitudes and new age mysticism. Related to this are vigorous marketing of extravagant claims by the alternative medical community combined with inadequate media scrutiny and attacks on critics.\n\nThere is also an increase in conspiracy theories toward conventional medicine and pharmaceutical companies, mistrust of traditional authority figures, such as the physician, and a dislike of the current delivery methods of scientific biomedicine, all of which have led patients to seek out alternative medicine to treat a variety of ailments. Many patients lack access to contemporary medicine, due to a lack of private or public health insurance, which leads them to seek out lower-cost alternative medicine. Medical doctors are also aggressively marketing alternative medicine to profit from this market.\n\nPatients can be averse to the painful, unpleasant, and sometimes-dangerous side effects of biomedical treatments. Treatments for severe diseases such as cancer and HIV infection have well-known, significant side-effects. Even low-risk medications such as antibiotics can have potential to cause life-threatening anaphylactic reactions in a very few individuals. Many medications may cause minor but bothersome symptoms such as cough or upset stomach. In all of these cases, patients may be seeking out alternative treatments to avoid the adverse effects of conventional treatments.\n\nComplementary and alternative medicine (CAM) has been described as a broad domain of healing resources that encompasses all health systems, modalities, and practices and their accompanying theories and beliefs, other than those intrinsic to the politically dominant health system of a particular society or culture in a given historical period. CAM includes all such practices and ideas self-defined by their users as preventing or treating illness or promoting health and well-being. Boundaries within CAM and between the CAM domain and that of the dominant system are not always sharp or fixed.\n\nAccording to recent research, the increasing popularity of the CAM needs to be explained by moral convictions or lifestyle choices rather than by economic reasoning.\n\nIn developing nations, access to essential medicines is severely restricted by lack of resources and poverty. Traditional remedies, often closely resembling or forming the basis for alternative remedies, may comprise primary healthcare or be integrated into the healthcare system. In Africa, traditional medicine is used for 80% of primary healthcare, and in developing nations as a whole over one-third of the population lack access to essential medicines.\n\nSome have proposed adopting a prize system to reward medical research. However, public funding for research exists. Increasing the funding for research on alternative medicine techniques is the purpose of the US National Center for Complementary and Alternative Medicine. NCCIH and its predecessor, the Office of Alternative Medicine, have spent more than US$2.5 billion on such research since 1992; this research has largely not demonstrated the efficacy of alternative treatments.\n\nThat alternative medicine has been on the rise \"in countries where Western science and scientific method generally are accepted as the major foundations for healthcare, and 'evidence-based' practice is the dominant paradigm\" was described as an \"enigma\" in the Medical Journal of Australia.\n\nIn the United States, the 1974 Child Abuse Prevention and Treatment Act (CAPTA) required that for states to receive federal money, they had to grant religious exemptions to child neglect and abuse laws regarding religion-based healing practices. Thirty-one states have child-abuse religious exemptions.\n\nThe use of alternative medicine in the US has increased, with a 50 percent increase in expenditures and a 25 percent increase in the use of alternative therapies between 1990 and 1997 in America. Americans spend many billions on the therapies annually. Most Americans used CAM to treat and/or prevent musculoskeletal conditions or other conditions associated with chronic or recurring pain. In America, women were more likely than men to use CAM, with the biggest difference in use of mind-body therapies including prayer specifically for health reasons\". In 2008, more than 37% of American hospitals offered alternative therapies, up from 27 percent in 2005, and 25% in 2004. More than 70% of the hospitals offering CAM were in urban areas.\n\nA survey of Americans found that 88 percent thought that \"there are some good ways of treating sickness that medical science does not recognize\". Use of magnets was the most common tool in energy medicine in America, and among users of it, 58 percent described it as at least \"sort of scientific\", when it is not at all scientific. In 2002, at least 60 percent of US medical schools have at least some class time spent teaching alternative therapies. \"Therapeutic touch\" was taught at more than 100 colleges and universities in 75 countries before the practice was debunked by a nine-year-old child for a school science project.\n\nThe most common CAM therapies used in the US in 2002 were prayer (45%), herbalism (19%), breathing meditation (12%), meditation (8%), chiropractic medicine (8%), yoga (5–6%), body work (5%), diet-based therapy (4%), progressive relaxation (3%), mega-vitamin therapy (3%) and Visualization (2%)\n\nIn Britain, the most often used alternative therapies were Alexander technique, Aromatherapy, Bach and other flower remedies, Body work therapies including massage, Counseling stress therapies, hypnotherapy, Meditation, Reflexology, Shiatsu, Ayurvedic medicine, Nutritional medicine, and Yoga. Ayurvedic medicine remedies are mainly plant based with some use of animal materials. Safety concerns include the use of herbs containing toxic compounds and the lack of quality control in Ayurvedic facilities.\n\nAccording to the National Health Service (England), the most commonly used complementary and alternative medicines (CAM) supported by the NHS in the UK are: acupuncture, aromatherapy, chiropractic, homeopathy, massage, osteopathy and clinical hypnotherapy.\n\nComplementary therapies are often used in palliative care or by practitioners attempting to manage chronic pain in patients. Integrative medicine is considered more acceptable in the interdisciplinary approach used in palliative care than in other areas of medicine. \"From its early experiences of care for the dying, palliative care took for granted the necessity of placing patient values and lifestyle habits at the core of any design and delivery of quality care at the end of life. If the patient desired complementary therapies, and as long as such treatments provided additional support and did not endanger the patient, they were considered acceptable.\" The non-pharmacologic interventions of complementary medicine can employ mind-body interventions designed to \"reduce pain and concomitant mood disturbance and increase quality of life.\"\n\nIn Austria and Germany complementary and alternative medicine is mainly in the hands of doctors with MDs, and half or more of the American alternative practitioners are licensed MDs. In Germany herbs are tightly regulated: half are prescribed by doctors and covered by health insurance.\n\nSome professions of complementary/traditional/alternative medicine, such as chiropractic, have achieved full regulation in North America and other parts of the world and are regulated in a manner similar to that governing science-based medicine. In contrast, other approaches may be partially recognized and others have no regulation at all. Regulation and licensing of alternative medicine ranges widely from country to country, and state to state.\n\nGovernment bodies in the US and elsewhere have published information or guidance about alternative medicine. The U.S. Food and Drug Administration (FDA), has issued online warnings for consumers about medication health fraud. This includes a section on Alternative Medicine Fraud, such as a warning that Ayurvedic products generally have not been approved by the FDA before marketing.\n\nMany of the claims regarding the safety and efficacy of alternative medicine are controversial. Some alternative treatments have been associated with unexpected side effects, which can be fatal.\n\nA commonly voiced concerns about complementary alternative medicine (CAM) is the way it's regulated. There have been significant developments in how CAMs should be assessed prior to re-sale in the United Kingdom and the European Union (EU) in the last 2 years. Despite this, it has been suggested that current regulatory bodies have been ineffective in preventing deception of patients as many companies have re-labelled their drugs to avoid the new laws. There is no general consensus about how to balance consumer protection (from false claims, toxicity, and advertising) with freedom to choose remedies.\n\nAdvocates of CAM suggest that regulation of the industry will adversely affect patients looking for alternative ways to manage their symptoms, even if many of the benefits may represent the placebo affect. Some contend that alternative medicines should not require any more regulation than over-the-counter medicines that can also be toxic in overdose (such as paracetamol).\n\nForms of alternative medicine that are biologically active can be dangerous even when used in conjunction with conventional medicine. Examples include immuno-augmentation therapy, shark cartilage, bioresonance therapy, oxygen and ozone therapies, and insulin potentiation therapy. Some herbal remedies can cause dangerous interactions with chemotherapy drugs, radiation therapy, or anesthetics during surgery, among other problems. An example of these dangers was reported by Associate Professor Alastair MacLennan of Adelaide University, Australia regarding a patient who almost bled to death on the operating table after neglecting to mention that she had been taking \"natural\" potions to \"build up her strength\" before the operation, including a powerful anticoagulant that nearly caused her death.\n\nTo \"ABC Online\", MacLennan also gives another possible mechanism:\n\nAnd lastly there's the cynicism and disappointment and depression that some patients get from going on from one alternative medicine to the next, and they find after three months the placebo effect wears off, and they're disappointed and they move on to the next one, and they're disappointed and disillusioned, and that can create depression and make the eventual treatment of the patient with anything effective difficult, because you may not get compliance, because they've seen the failure so often in the past.\n\nConventional treatments are subjected to testing for undesired side-effects, whereas alternative treatments, in general, are not subjected to such testing at all. Any treatment – whether conventional or alternative – that has a biological or psychological effect on a patient may also have potential to possess dangerous biological or psychological side-effects. Attempts to refute this fact with regard to alternative treatments sometimes use the \"appeal to nature\" fallacy, i.e., \"That which is natural cannot be harmful.\" Specific groups of patients such as patients with impaired hepatic or renal function are more susceptible to side effects of alternative remedies.\n\nAn exception to the normal thinking regarding side-effects is Homeopathy. Since 1938, the U.S. Food and Drug Administration (FDA) has regulated homeopathic products in \"several significantly different ways from other drugs.\" Homeopathic preparations, termed \"remedies\", are extremely dilute, often far beyond the point where a single molecule of the original active (and possibly toxic) ingredient is likely to remain. They are, thus, considered safe on that count, but \"their products are exempt from good manufacturing practice requirements related to expiration dating and from finished product testing for identity and strength\", and their alcohol concentration may be much higher than allowed in conventional drugs.\n\nThose having experienced or perceived success with one alternative therapy for a minor ailment may be convinced of its efficacy and persuaded to extrapolate that success to some other alternative therapy for a more serious, possibly life-threatening illness. For this reason, critics argue that therapies that rely on the placebo effect to define success are very dangerous. According to mental health journalist Scott Lilienfeld in 2002, \"unvalidated or scientifically unsupported mental health practices can lead individuals to forgo effective treatments\" and refers to this as \"opportunity cost\". Individuals who spend large amounts of time and money on ineffective treatments may be left with precious little of either, and may forfeit the opportunity to obtain treatments that could be more helpful. In short, even innocuous treatments can indirectly produce negative outcomes. Between 2001 and 2003, four children died in Australia because their parents chose ineffective naturopathic, homeopathic, or other alternative medicines and diets rather than conventional therapies.\n\nThere have always been \"many therapies offered outside of conventional cancer treatment centers and based on theories not found in biomedicine. These alternative cancer cures have often been described as 'unproven,' suggesting that appropriate clinical trials have not been conducted and that the therapeutic value of the treatment is unknown.\" However, \"many alternative cancer treatments have been investigated in good-quality clinical trials, and they have been shown to be ineffective...The label 'unproven' is inappropriate for such therapies; it is time to assert that many alternative cancer therapies have been 'disproven'.\"\n\nEdzard Ernst has stated:\n\n...any alternative cancer cure is bogus by definition. There will never be an alternative cancer cure. Why? Because if something looked halfway promising, then mainstream oncology would scrutinize it, and if there is anything to it, it would become mainstream almost automatically and very quickly. All curative \"alternative cancer cures\" are based on false claims, are bogus, and, I would say, even criminal.\n\n\"CAM\", meaning \"complementary and alternative medicine\", is not as well researched as conventional medicine, which undergoes intense research before release to the public. Funding for research is also sparse making it difficult to do further research for effectiveness of CAM. Most funding for CAM is funded by government agencies. Proposed research for CAM are rejected by most private funding agencies because the results of research are not reliable. The research for CAM has to meet certain standards from research ethics committees, which most CAM researchers find almost impossible to meet. Even with the little research done on it, CAM has not been proven to be effective.\n\nSteven Novella, a neurologist at Yale School of Medicine, wrote that government funded studies of integrating alternative medicine techniques into the mainstream are \"used to lend an appearance of legitimacy to treatments that are not legitimate.\" Marcia Angell considered that critics felt that healthcare practices should be classified based solely on scientific evidence, and if a treatment had been rigorously tested and found safe and effective, science-based medicine will adopt it regardless of whether it was considered \"alternative\" to begin with. It is possible for a method to change categories (proven vs. unproven), based on increased knowledge of its effectiveness or lack thereof. A prominent supporter of this position is George D. Lundberg, former editor of the Journal of the American Medical Association (JAMA).\n\nWriting in 1999 in \"CA: A Cancer Journal for Clinicians\" Barrie R. Cassileth mentioned a 1997 letter to the US Senate Subcommittee on Public Health and Safety, which had deplored the lack of critical thinking and scientific rigor in OAM-supported research, had been signed by four Nobel Laureates and other prominent scientists. (This was supported by the National Institutes of Health (NIH).)\n\nIn March 2009 a staff writer for \"the Washington Post\" reported that the impending national discussion about broadening access to health care, improving medical practice and saving money was giving a group of scientists an opening to propose shutting down the National Center for Complementary and Alternative Medicine. They quoted one of these scientists, Steven Salzberg, a genome researcher and computational biologist at the University of Maryland, as saying \"One of our concerns is that NIH is funding pseudoscience.\" They noted that the vast majority of studies were based on fundamental misunderstandings of physiology and disease, and had shown little or no effect.\n\nWriters such as Carl Sagan, a noted astrophysicist, advocate of scientific skepticism and the author of \"The Demon-Haunted World: Science as a Candle in the Dark\" (1996), have lambasted the lack of empirical evidence to support the existence of the putative energy fields on which these therapies are predicated.\n\nSampson has also pointed out that CAM tolerated contradiction without thorough reason and experiment. Barrett has pointed out that there is a policy at the NIH of never saying something doesn't work only that a different version or dose might give different results. Barrett also expressed concern that, just because some \"alternatives\" have merit, there is the impression that the rest deserve equal consideration and respect even though most are worthless, since they are all classified under the one heading of alternative medicine.\n\nSome critics of alternative medicine are focused upon health fraud, misinformation, and quackery as public health problems, notably Wallace Sampson and Paul Kurtz founders of Scientific Review of Alternative Medicine and Stephen Barrett, co-founder of The National Council Against Health Fraud and webmaster of Quackwatch. Grounds for opposing alternative medicine include that:\n\nMany alternative medical treatments are not patentable, which may lead to less research funding from the private sector. In addition, in most countries, alternative treatments (in contrast to pharmaceuticals) can be marketed without any proof of efficacy – also a disincentive for manufacturers to fund scientific research.\n\nEnglish evolutionary biologist Richard Dawkins, in his 2003 book \"A Devil's Chaplain\", defined alternative medicine as a \"set of practices that cannot be tested, refuse to be tested, or consistently fail tests.\" Dawkins argued that if a technique is demonstrated effective in properly performed trials then it ceases to be alternative and simply becomes medicine.\n\nCAM is also often less regulated than conventional medicine. There are ethical concerns about whether people who perform CAM have the proper knowledge to treat patients. CAM is often done by non-physicians who do not operate with the same medical licensing laws which govern conventional medicine, and it is often described as an issue of non-maleficence.\n\nAccording to two writers, Wallace Sampson and K. Butler, marketing is part of the training required in alternative medicine, and propaganda methods in alternative medicine have been traced back to those used by Hitler and Goebels in their promotion of pseudoscience in medicine.\n\nIn November 2011 Edzard Ernst stated that the \"level of misinformation about alternative medicine has now reached the point where it has become dangerous and unethical. So far, alternative medicine has remained an ethics-free zone. It is time to change this.\"\n\nSome commentators have said that special consideration must be given to the issue of conflicts of interest in alternative medicine. Edzard Ernst has said that most researchers into alternative medicine are at risk of \"unidirectional bias\" because of a generally uncritical belief in their chosen subject. Ernst cites as evidence the phenomenon whereby 100% of a sample of acupuncture trials originating in China had positive conclusions. David Gorski contrasts evidence-based medicine, in which researchers try to disprove hyphotheses, with what he says is the frequent practice in pseudoscience-based research, of striving to confirm pre-existing notions. Harriet Hall writes that there is a contrast between the circumstances of alternative medicine practitioners and disinterested scientists: in the case of acupuncture, for example, an acupuncturist would have \"a great deal to lose\" if acupuncture were rejected by research; but the disinterested skeptic would not lose anything if its effects were confirmed; rather their change of mind would enhance their skeptical credentials.\n\n\n\n\n\n\n\n"}
{"id": "9095203", "url": "https://en.wikipedia.org/wiki?curid=9095203", "title": "Antagonism (chemistry)", "text": "Antagonism (chemistry)\n\nChemical antagonists impede the normal function of a system. They function to invert the effects of other molecules. The effects of antagonists can be seen after they have encountered an agonist, and as a result, the effects of the agonist is neutralized. Antagonists such as dopamine antagonist slow down movement in lab rats. Although they hinder the joining of enzymes to substrates, Antagonists can be beneficial. For example, not only do angiotensin receptor blockers, and angiotensin-converting enzyme (ACE) inhibitors work to lower blood pressure, but they also counter the effects of renal disease in diabetic and non-diabetic patients. Chelating agents, such as calcium di sodium defeated, fall into the category of antagonists and operate to minimize the lethal effects of heavy metals such as mercury or lead.\n\nIn chemistry, antagonism is a phenomenon wherein two or more agents in combination have an overall effect that is less than the sum of their individual effects.\n\nThe word is most commonly used in this context in biochemistry and toxicology: interference in the physiological action of a chemical substance by another having a similar structure. For instance, a receptor antagonist is an agent that reduces the response that a ligand produces when the receptor antagonist binds to a receptor on a cell. An example of this is the interleukin-1 receptor antagonist. The opposite of antagonism is synergy. It is a negative type of synergism.\n\nExperiments with different combinations show that binary mixtures of phenolics can lead to either a synergetic antioxidant effect or to an antagonistic effect.\n"}
{"id": "2598134", "url": "https://en.wikipedia.org/wiki?curid=2598134", "title": "Arts and Humanities Research Council", "text": "Arts and Humanities Research Council\n\nThe Arts and Humanities Research Council (AHRC) was established in April 2005 as successor to the Arts and Humanities Research Board and is a British Research Council; non-departmental public body that provides approximately £102 million from the government to support research and postgraduate study in the arts and humanities, from languages and law, archaeology and English literature to design and creative and performing arts. In any one year, the AHRC makes approximately 700 research awards and around 1,350 postgraduate awards. Awards are made after a rigorous peer review process, to ensure that only applications of the highest quality are funded.\n\nThe Stonehenge Riverside Project is a major five year AHRC-funded archaeological research study interested in the development of the Stonehenge landscape in Neolithic and Bronze Age Britain. In particular, the project is interested in the relationship between the stones and surrounding monuments and features including; The River Avon, Durrington Walls, the Cursus, the Avenue, Woodhenge, burial mounds, and nearby standing stones. In August 2009 the project discovered a new stone circle, which was named Bluestonehenge by the research team, about one mile away from Stonehenge in Wiltshire, England. The project is run by a consortium of university teams. It is directed by Prof. Mike Parker Pearson of Sheffield University, with co-directors Dr Josh Pollard (University of Southampton), Prof. Julian Thomas (Manchester University), Dr Kate Welham (Bournemouth University) and Dr Colin Richards (Manchester University).\n\nResearchers at the University of Reading and University of Southampton analysed historic sources such as muster rolls records in the National Archives at Kew and the Bibliothèque nationale de France in Paris (for records of English garrisons in France). The resulting Medieval Soldier online database enables people to search for soldiers by surname, rank or year of service. The online database contains 250,000 service records of soldiers who saw active duty in the latter phases of the Hundred Years' War (1369–1453).\n\nAn AHRC research grant enabled academics from the University of Hertfordshire, University of Sheffield and the Open University to double in size the Old Bailey trial proceedings available to view on the Old Bailey Proceedings Online website and provide access to the largest single source of searchable information about ordinary British lives and behaviour ever published.\n\nThe Old Bailey Proceedings Online makes available a fully searchable, digitised collection of all surviving editions of the Old Bailey Proceedings from 1674 to 1913, and of the Ordinary of Newgate's Accounts, 1679 to 1772. It allows access to over 197,000 trials and biographical details of approximately 2,500 men and women executed at Tyburn.\n\nIn 2005 the AHRC replaced the Arts and Humanities Research Board (AHRB), founded in 1998.\n\nThe AHRC publish reviews and reports on arts and humanities subjects, as well as corporate publications. Research news and findings are communicated in website features, press releases, and multimedia content such as podcasts.\n\nBetween 2005 and 2010, the AHRC published a magazine called \"Podium\" twice a year, which contained news and case studies based on research that they have funded.\n\nThe AHRC is one of seven Research Councils in the UK.\n\nProfessor Andrew S Thompson is serving as Interim Chief Executive of the AHRC, from December 2015. The previous CE of the AHRC was Professor Rick Rylance who took up the post on 1 September 2009, and was re-appointed in September 2013 to serve until August 2017.\n\nThe current Council Chair is Sir Drummond Bone. Sir Alan Wilson stepped down in December 2013.\n"}
{"id": "49864220", "url": "https://en.wikipedia.org/wiki?curid=49864220", "title": "Association for Women in Computing", "text": "Association for Women in Computing\n\nThe Association for Women in Computing (AWC) is a professional organization for women in computing. It was founded in 1978 in Washington, D.C. and is a member of the Institute for Certification of Computing Professionals (ICCP).\n\nThe purpose of AWC is to provide opportunities for professional growth for women in computing through networking, continuing education and mentoring. To accomplish this they promote awareness of issues affecting women in the computing industry, further the professional development and advancement of women in computing, and encourage women to pursue careers in computer science. The AWC is a national, nonprofit, professional organization for women and men with an interest in information technology.\n\nAWC was founded in 1978 as a non-profit organization, originally under the name National Association for Women in Computing. The Puget Sound Chapter was founded in the winter of 1979 by Donnafaye Carroll Finger and Diane Haelsig. These two women read an article about a new association for women in computing and were soon discussing the formation of a Puget Sound Chapter. The Twin Cities Chapter of the AWC first met in December, 1979 and became a chartered chapter on May 6, 1981.\n\nAWC has chapters in:\n\n\n"}
{"id": "56768571", "url": "https://en.wikipedia.org/wiki?curid=56768571", "title": "Benefits of space exploration", "text": "Benefits of space exploration\n\nSpace is a high-tech arena where rival ideologies clashed, showcasing their technological prowess under the watchful eyes of the entire globe. The competition between the former USSR and the United States brought about a rapid succession of firsts – the first human in orbit, the first steps on the Moon, the first space station and the first reusable launch system. However, once the dust settled, and the eye-popping bill of the early space activities hit government coffers, a new rationale emerged: pragmatic use of space as the higher ground for improving life on Earth. From satellite telecommunications to remote sensing, micro gravity research and satellite navigation government, space programs began investing in practical outcomes for the public good. Therefore, the importance of measuring the socio-economic benefits of space activities steadily grew throughout the 1970s and 1980s, an era defined as “Mission to Planet Earth.” These benefits continue into today's society as the privatization of space travel and space exploration continues to show scientific and socio-economic benefits to those around them. \n\nSpace programs have generated a wide range of hardware, software and processes that have made their way into a myriad of applications. The benefits of these applications are directly attributable to the original investments made by the space agencies and the private sector. Space exploration alone has provided a significant amount of knowledge that is important for the education of people about a basic understanding of our planet and the universe. Some of the more direct benefits of space exploration include an increase in the knowledge that is out there about space and the discovery of distant planets and galaxies, it also gives us insight into the beginnings of our universe.\n\nSince Sputnik 1 entered orbit in 1957 to perform Ionospheric experiments, the human understanding of earth and space have increased. The list of missions to the moon begin as early as 1958 and continued into the current age. A few successful lunar missions, by the USSR, include missions such as, the Luna 1 space craft that completed the first flyby of the moon in 1959, the Luna 3 lunar probe that took the first pictures of the far side of the moon in 1959, the Luna 10 orbiter that was the first orbiter of the moon in 1966, and the Lunokhod 1 lunar rover in 1970, which was the first rover that explored the surface of a world beyond earth. The USA also added significant lunar first, such as Apollo 8 in 1968 being the first successful human mission to orbit the moon and the historic Apollo 11 in 1969 when man first landed on the moon. Missions to the moon have collected samples of Lunar materials and there are now multiple satellites such as ARTEMIS P1 that currently orbit the moon and collect data. \n\nTelescopic satellites have captured millions of images of the universe since the introduction of satellites, such as the Mariner Space Satellites. TheMariner 4 space satellite in 1965 was the first successful satellite to capture up close images of the planet Mars. Mariner missions 6 and 7 additionally mapped the poles of the Red planet and relayed further images of the surface. By the end of the Mariner missions in 1972,Mariner 9 spent almost a year in the Martian orbit (687 days), collected images of nearly 100% of the surface of the moon, and observed notable discoveries including the humongousOlympus Mons volcano, more than 4000 km of Valles Marineris, and dust storms lasting over a month. Further missions since 1972, like the Viking project, Mars observer, Mars pathfinder, Mars climate orbiter, Deep Space 2, Mars Global surveyor, andPhoenix, have obtained understandings of the climate, flood plains, and rock samples on mars.\n\nThe Hubble space telescope has contributed more than a million observations of the universe that have been influential in understanding the magnitude of the universe since its introduction into orbit. The intense focus ofHubble [is] for observation. The Hubble telescope has been used to discover galaxies and planets across the universe. It was pointed at a speck of black space in space for ten consecutive days and the resultant image that is known as the Hubble Deep Field revealed at least 1,500 galaxies in this small location in the sky. Views like this allow the realization of the size and scope of the larger universal understanding of the types and variances of galaxies out in space. The Hubble space telescope has cost about $12 billion in the last 30 years including the cost of 5 shuttle servicing missions, the information received by the astronomers monitoring the Hubble telescope provides a better understanding that would not be possible on from earth telescopes.\n\nHuman culture exists as a social environment made up by traditions, norms, rules written or unwritten, and social practices. Cultures can be specific to groups of any size such as a family or group of friends but also as large as a state or nation. The range and diversity of human culture is markedly large. International collaboration in the space age brought together different cultures and as a result, the exchange and advancement of human culture. In over fifty years of space travel, the diversity of those working in space and in the field as a whole has dramatically increased from the beginnings of space exploration. This progression in diversity brought more cultures into close quarters and resulted in the enrichment of human culture globally. \n\nA testament to space exploration facilitating cross-cultural growth, the International Space Station exhibits an important collaboration between nations to construct and continually man the station. The ISS operates as a research facility that contributes cutting edge scientific work from a unique environment that is nearly impossible to recreate on earth. \n\nThe innovation and exploration of the space age has served as an inspiration to humankind. Breaking through into space travel, man leaving earth and defeating gravity, taking steps on the moon, and various other achievements were pivotal moments in human cultural development. In particular, the scientific and technological advancements stand as an inspiration to the scientific community of students, teachers, and researchers worldwide. Moreover, space exploration has also inspired innovative training programs aimed at preschoolers, such as the Future Astronauts Program. It is evident that by drawing in the wonder of space together with the knowledge and skills developed through space exploration into classrooms, children can be strongly motivated and empowered from a young age.\n\nSpace exploration has nearly no limit to how far humankind can adventure as our technology advances. Space exploration will continue to foster international inspiration and collaboration, and pose revolutionary philosophical, political, and scientific questions and debates.\n\nThe International Space Station (ISS), is a space station assembled in low Earth orbit largely by the United States and Russia, with assistance and components from a multinational consortium. In 1993, the United States and Russia agreed to merge their separate space station plans into a single facility integrating their respective modules and incorporating contributions from the European Space Agency and Japan. Collaborations among the larger international community have provided the ability to test things in space and see how objects and people react to space for extended periods of time. Work on the sustainability of long term space flights and the possibilities of Interplanetary spaceflights have been major objectives of the ISS.\n\nThese partnerships benefit all nations, even those outside the industrialized world. Some African countries, including Nigeria and South Africa, have successfully employed space technology—specifically satellite-based disaster management, climate monitoring, and green systems—to tackle workforce development issues and launch new space activities.\n\nNASA reports that 444,000 lives have been saved, 14,000 jobs have been created, 5 billion dollars in revenue has been generated, and there has been 6.2 billion dollars in cost reduction due to Spin-off programs from NASA research in collaboration with various companies. Of the many beneficial NASA spinoff technologies there has been advancements in the fields of health and medicine, transportation, public safety, consumer goods, energy and environment, information technology, and industrial productivity. Multiple products and innovations used in the daily life are results of space generated research. Solar panels, water-purification systems, dietary formulas and supplements, space suit materials in clothing, and global search and rescue systems are but a few examples of the beneficiary spinoffs that have been produced.\n\nThe unifying efforts to create the International Space Station took form in December of 1998 as the space shuttle Endeavor launched from the Kennedy Space Center. Collectively, 18 nations took some part in the development and construction of space station and the five notable agencies that encompassed these 18 nations were NASA, Roscosmos, the European Space Agency, the Canadian Space agency and the National Space Development Agency of Japan. The international space station serves as a depot of collaboration across the world for innovative research, human health research, global education and earth observation. The international space station itself contains complex sensors including Light Detection and Ranging systems are influential in the understanding of sea surface winds and of atmospheric transportation patterns. Additional astronaut observation from the station with powerful cameras provides detailed images of the earth and panoramic views of the atmosphere of the earth.\n\nData collected from the space station complex orbital sensor system have been a valuable tool in evaluating the extent of damage resulting from natural disaster. Real time mapping coupled with a human crew allows for space to ground collaboration.\n\nThe first communication \"satellite\" was actually a balloon called Echo 1 it was launched on August 12, 1960 and stayed roughly 1000 miles (1,609 km) above the Earth and didn't come back down until May 24, 1968. The first communication satellite that would be an actual satellite by modern standards was launched on July 10, 1962 and it was named Telstar. It allowed communication between the United States and Europe to occur almost instantaneously. Telstar operated for approximately a year before it shutdown, but even with its rather short lifespan, it proved that the usage of satellites as a form of long distance communication was possible and better than forms of communication before it. Now, communications satellites have many different functions and are far more sophisticated than the original ones. They can be used to communicate long distances in remote areas with the use of satellite phones. Airlines use them to provide internet service on their flights. They are also starting to be used to deliver internet to areas that are unable to get internet in any other way due to their location. The number of communication satellites has since expanded to about 2,000 in orbit that are able to provide continuous communication around the globe. \n\nThe first weather satellite named TIROS I (Television and InfraRed Observation Satellite) was launched on April 1, 1960. The satellite used television cameras to take pictures of clouds as it orbited the Earth. TIROS I only operated for 78 days, however during that time it was able to prove that using a satellite to observe weather conditions on Earth was not only possible, but that it could greatly increase the amount of data available to meteorologists with the result was that more accurate weather forecasts were possible. The use of satellites has since come a long way. The number of weather satellites has increased significantly, and the satellites themselves are now more sophisticated. In addition to taking pictures of clouds they can now observe weather patterns in the same area 24/7 because they are in geostationary orbit, meaning that they move at the same speed as the Earth's rotation and thus they always see the same area. Even when it is dark in the part of the world that they are observing they can see through the use of infrared technology. This allows meteorologists to monitor the development of weather in areas that are not covered by traditional means like Doppler radar, such as over the ocean, which, in turn, allows them to give advanced warnings for severe weather such as hurricanes thus saving lives. Weather satellites can also be used to observe the amount of ice at the poles, and where that ice may go if it breaks off. Also, the satellites are used to take pictures of the snow coverage in an area which can help determine the amount of water that will run off when the snow eventually melts. They are even being used to predict the amount of power that solar panels can make in certain areas by monitoring the cloud cover and weather patterns at the proposed solar panel site.\n\nBeginning in 1967, NASA successfully began its Biosatellite program that initially took frog eggs, amoeba, bacteria, planets and mice and studied the effects of zero gravity on these biological life forms. Studies of human life in space have augmented the understanding of the effects of adjusting to a space environment, such as alterations in body fluids, negative influences on the immune system and effects of space on sleep patterns. Current space research pursuits are divided into the subjects of Space Biology, which studies the effects of space on smaller organisms such as cells, Space Physiology, which is the study of the effects of space on the human body and Space Medicine, which examines the possible dangers of space on the human body. Discoveries concerning the human body and space, particularly the effects on the development of bones, may provide further understanding of biomineralization and the process of gene transcription.\n"}
{"id": "4407262", "url": "https://en.wikipedia.org/wiki?curid=4407262", "title": "Billions and Billions", "text": "Billions and Billions\n\nBillions and Billions: Thoughts on Life and Death at the Brink of the Millennium is a 1997 book by the American astronomer and science popularizer Carl Sagan. The last book written by Sagan before his death in 1996, it was published by Random House.\n\nThe book is a collection of essays Sagan wrote covering diverse topics such as global warming, the population explosion, extraterrestrial life, morality, and the abortion debate. The last chapter is an account of his struggle with myelodysplasia, the disease which finally took his life in December 1996. Sagan's wife, Ann Druyan, wrote the epilogue of the book after his death.\n\nTo help viewers of \"\" distinguish between \"millions\" and \"billions\", Sagan stressed the \"b\". Sagan never did, however, say \"\". The public's association of the phrase and Sagan came from a \"Tonight Show\" skit. Parodying Sagan's affect, Johnny Carson quipped \"billions and billions\". The phrase has, however, now become a humorous fictitious number—the Sagan.\n\n"}
{"id": "57759", "url": "https://en.wikipedia.org/wiki?curid=57759", "title": "Biophoton", "text": "Biophoton\n\nBiophotons (from the Greek βίος meaning \"life\" and φῶς meaning \"light\") are photons of light in the ultraviolet and low visible light range that are produced by a biological system. They are non-thermal in origin, and the emission of biophotons is technically a type of bioluminescence, though bioluminescence is generally reserved for higher luminance luciferin/luciferase systems. The term \"biophoton\" used in this narrow sense should not be confused with the broader field of biophotonics, which studies the general interaction of light with biological systems.\n\nBiological tissues typically produce an observed radiant emittance in the visible and ultraviolet frequencies ranging from 10 to 10 W/cm (approx 1-1000 photons/cm/second).\nThis low level of light has a much weaker intensity than the visible light produced by bioluminescence, but biophotons are detectable above the background of thermal radiation that is emitted by tissues at their normal temperature.\n\nWhile detection of biophotons has been reported by several groups, hypotheses that such biophotons indicate the state of biological tissues and facilitate a form of cellular communication are still under investigation, and claims that biophotons are responsible for physical healing are unsupported. Alexander Gurwitsch, who discovered the existence of biophotons, was awarded the Stalin Prize in 1941 for his mitogenic radiation work.\n\nBiophotons may be detected with photomultipliers or by means of an ultra low noise CCD camera to produce an image, using an exposure time of typically 15 minutes for plant materials.\nPhotomultiplier tubes have also been used to measure biophoton emissions from fish eggs, and some applications have measured biophotons from animals and humans.\nThe typical observed radiant emittance of biological tissues in the visible and ultraviolet frequencies ranges from 10 to 10 W/cm with a photon count from a few to nearly 1000 photons per cm in the range of 200 nm to 800 nm.\n\nChemi-excitation via oxidative stress by reactive oxygen species and/or catalysis by enzymes (i.e., peroxidase, lipoxygenase) is a common event in the biomolecular milieu. Such reactions can lead to the formation of triplet excited species, which release photons upon returning to a lower energy level in a process analogous to phosphorescence. That this process is a contributing factor to spontaneous biophoton emission has been indicated by studies demonstrating that biophoton emission can be increased by depleting assayed tissue of antioxidants or by addition of carbonyl derivatizing agents. Further support is provided by studies indicating that emission can be increased by addition of reactive oxygen species.\n\nImaging of biophotons from leaves has been used as a method for Assaying R Gene Responses. These genes and their associated proteins are responsible for pathogen recognition and activation of defense signaling networks leading to the hypersensitive response, which is one of the mechanisms of the resistance of plants to pathogen infection. It involves the generation of reactive oxygen species (ROS), which have crucial roles in signal transduction or as toxic agents leading to cell death.\n\nBiophoton have been observed in stressed plant's roots, too. In healthy cells, the concentration of ROS is minimized by a system of biological antioxidants. However, heat shock and other stresses changes the equilibrium between oxidative stress and antioxidant activity, for example, the rapid rise in temperature induces biophoton emission by ROS.\n\nIn the 1920s, the Russian embryologist Alexander Gurwitsch reported \"ultraweak\" photon emissions from living tissues in the UV-range of the spectrum. He named them \"mitogenetic rays\" because his experiments convinced him that they had a stimulating effect on cell division.\n\nBiophotons were claimed to have been employed by the Stalin regime to diagnose cancer. The method has not been tested in the West. However, failure to replicate his findings and the fact that, though cell growth can be stimulated and directed by radiation this is possible only at much higher amplitudes, evoked a general skepticism about Gurwitsch's work. In 1953 Irving Langmuir dubbed Gurwitsch's Mitogenetic Rays pathological science. Commercial products, therapeutic claims and services supposedly based on his work appear at present to be best regarded as such.\n\nBut in the later 20th century Gurwitsch's daughter Anna, Colli, Quickenden and Inaba separately returned to the subject, referring to the phenomenon more neutrally as \"dark luminescence\", \"low level luminescence\", \"ultraweak bioluminescence\", or \"ultraweak chemiluminescence\". Their common basic hypothesis was that the phenomenon was induced from rare oxidation processes and radical reactions.\n\nIn the 1970s Fritz-Albert Popp and his research group at the University of Marburg (Germany) showed that the spectral distribution of the emission fell over a wide range of wavelengths, from 200 to 750 nm. Popp proposed that the radiation might be both semi-periodic and coherent.\n\nOne biophoton mechanism focuses on injured cells that are under higher levels of oxidative stress, which is one source of light, and can be deemed to constitute a \"distress signal\" or background chemical process is yet to be demonstrated. The difficulty of teasing out the effects of any supposed biophotons amid the other numerous chemical interactions between cells makes it difficult to devise a testable hypothesis. A 2010 review article discusses various published theories on this kind of signaling.\n\nMany claims with no scientific proof have been made for cures and diagnosis using biophotons. An appraisal of \"biophoton therapy\" by the IOCOB notes that biophoton therapy claims to treat a wide variety of diseases, such as malaria, Lyme disease, multiple sclerosis, schizophrenia, and depression, but that all these claims remain unproven. F. Popp, a researcher who investigates biophoton emission, concludes that the complexity of cellular chemical reactions in living systems is such that it excludes the possibility to create a machine to selectively heal systems using biophotons, but there are always people who believe in these \"miracles.\"\n\nThis claims:\n\"The quantum level possesses the highest level of coherence within the human organism. Sick individuals with weak immune systems or cancer have poor and chaotic coherence with disturbed biophoton cellular communication. Therefore, disease can be seen as the result of disturbances on the cellular level that act to distort the cell's quantum perspective. This causes electrons to become misplaced in protein molecules and metabolic processes become derailed as a result. Once cellular metabolism is compromised the cell becomes isolated from the regulated process of natural growth control.\"\n\nA review of the \"American Academy of Quantum Medicine\" concludes that many quantum medicine practitioners are not licensed as health care professionals, that quantum medicine uses scientific terminology but is nonsense, and that the practitioners have created \"a nonexistent 'energy system' to help peddle products and procedures to their clients.\"\n\n\n"}
{"id": "49916168", "url": "https://en.wikipedia.org/wiki?curid=49916168", "title": "Cavity optomechanics", "text": "Cavity optomechanics\n\nCavity optomechanics is a branch of physics which focuses on the interaction between light and mechanical objects on low-energy scales. It is a cross field of optics, quantum optics, solid-state physics and materials science. The motivation for research on cavity optomechanics comes from fundamental effects of quantum theory and gravity, as well as technological applications.\n\nThe name of the field relates to the main effect of interest, which is the enhancement of radiation pressure interaction between light (photons) and matter using optical resonators (cavities). It first became relevant in the context of gravitational wave detection, since optomechanical effects have to be taken into account in interferometric gravitational wave detectors. Furthermore, one may envision optomechanical structures to allow the realization of Schrödinger's cat. Macroscopic objects consisting of billions of atoms share collective degrees of freedom which may behave quantum mechanically, e.g. a sphere of micrometer diameter being in a spatial superposition between two different places. Such a quantum state of motion would allow to experimentally investigate decoherence, which describes the process of objects transitioning between states which are described by quantum mechanics to states which are described by Newtonian mechanics. Optomechanical structures pave a new way for testing the predictions of quantum mechanics and decoherence models and thereby might allow to answer some of the most fundamental questions in modern physics.\n\nThere is a broad range of experimental optomechanical systems which are almost equivalent in their description, but completely different in size, mass and frequency, ranging from attograms and gigahertz to kilograms and hertz. Cavity optomechanics was featured as the most recent \"milestone of photon history\" in nature photonics along well established concepts and technology like Quantum information, Bell inequalities and the laser.\n\nThe most elementary light-matter interaction is the light beam scattering off an arbitrary object (atom, molecule, nanobeam etc.). There is always elastic light scattering, with the outgoing light frequency identical to the incoming frequency formula_1. Inelastic scattering, in contrast, will be accompanied by excitation or de-excitation of the material object (e.g. internal atomic transitions may be excited). However, independent of the internal electronic details of the atoms or molecules, it is always possible to have Raman scattering due to the object's mechanical vibrations: \nwhere formula_3 is the vibrational frequency. The vibrations gain or lose energy, respectively, for these Stokes/anti-Stokes processes, while optical sidebands are created around the incoming light frequency, i.e. \nIf both of these processes (Stokes and anti-Stokes scattering) occur at an equal rate, the vibrations will merely heat up the object. However, one may use an optical cavity to suppress e.g. the Stokes process. This reveals the principle of the basic optomechanical setup: A laser-driven optical cavity is coupled to the mechanical vibrations of some object. This is a very generic setting. The purpose of the cavity is to select optical frequencies (e.g. to suppress the Stokes process), to resonantly enhance the light intensity and to enhance the sensitivity to the mechanical vibrations. This setup displays features of a true two-way interaction between light and mechanics. This is in contrast to optical tweezers, optical lattices, or vibrational spectroscopy, where the light field controls the mechanics (or vice versa) but the loop is not closed.\n\nAnother but equivalent way to interpret the principle of optomechanical cavities is by using the concept of radiation pressure. According to the quantum theory of light, every photon with wave number formula_5 carries a momentum formula_6 with Planck's constant formula_7. This means that a photon, which is reflected off a mirror surface, transfers a momentum formula_8 onto the mirror due to the conservation of momentum. This effect is extremely small and can not be observed on most every-day objects, however it becomes more significant when the mass of the mirror is very small and/or the number of photons is very large (i.e. high intensity of the light). Since the momentum of photons is extremely small and not enough to change the position of a suspended mirror significantly, one needs to enhance the interaction. One possible way to do this is by using optical cavities. If a photon is enclosed between two mirrors, one being the oscillator and the other a heavy fixed one, it will bounce off the mirrors many times and transfer its momentum every time it hits the mirrors. The number of times a photon can transfer its momentum is directly related to the finesse of the cavity, which can be improved with highly reflective mirror surfaces. To understand why this radiation pressure of the photons does not simply shift the suspended mirror further and further away, one has to take into account the effect on the cavity light field: If the mirror is displaced, the cavity becomes longer (or shorter) which changes the cavity resonance frequency. Thus the detuning between the changed cavity and the unchanged laser driving frequency is modified. This detuning determines the light amplitude inside the cavity - at smaller detuning more light actually enters the cavity, because it is closer to the cavity resonance frequency. Since the light amplitude, i.e. the number of photons inside the cavity, causes the radiation pressure force and thus the displacement of the mirror we have closed the loop: The radiation pressure force effectively depends on the mirror position. Another advantage of optical cavities is that the modulation of the cavity length through an oscillating mirror can directly be seen in the spectrum of the cavity.\n\nSome first effects of the light on the mechanical resonator can be captured by converting the radiation pressure force into a potential, \nand adding it to the intrinsic harmonic oscillator potential of the mechanical oscillator. This combined potential reveals the possibility of static multi-stability in the system, i.e. the potential can feature several stable minima. In addition, the slope of the radiation pressure force formula_10 can be understood as a modification of the mechanical spring constant, \nThis effect is known as \"optical spring effect\" (light-induced spring constant).\n\nHowever, this picture is not complete, as it neglects retardation effects due to the finite cavity photon decay rate formula_12. The force follows the motion of the mirror only with some time delay. This leads to effects such as friction. For example, let us assume the equilibrium position sit somewhere on the rising slope of the resonance. In thermal equilibrium, there will be oscillations around this position, which do not follow the shape of the resonance because of retardation. The consequence of this delayed radiation force during one cycle of oscillation turns out to be that work is carried out, in this particular case it is negative,formula_13, i.e. the radiation force extracts mechanical energy (there is extra, light-induced damping). This can be used to cool down the mechanical motion and is referred to as optical or optomechanical cooling. It is important for reaching the quantum regime of the mechanical oscillator where thermal noise effects on the device become negligible. Similarly, if the equilibrium position sits on the falling slope of the cavity resonance, the work is positive and the mechanical motion is amplified. In this case the extra, light-induced damping is negative and leads to amplification of the mechanical motion (heating). Radiation-induced damping of this kind has first been observed in pioneering experiments by Braginsky and coworkers in 1970.\n\nAnother explanation for the basic optomechanical effects of cooling and amplification can be given in a quantized picture. By detuning the incoming light from the cavity resonance to the red sideband, the photons can only enter the cavity if they take phonons with energy formula_14 from the mechanics. This effectively cools the device until a balance with heating mechanisms from the environment and laser noise is reached. In the same fashion it is also possible to heat structures (amplify the mechanical motion) by detuning the driving laser to the blue side. In this case the laser photons scatter into a cavity photon and create an additional phonon in the mechanical oscillator.\n\nIn general, one can divide the basic behaviour of the optomechanical system into different regimes, depending on the detuning between the laser frequency and the cavity resonance frequency, \n\n\nAlso the optical spring effect depends on the detuning. It can be observed for large detuning formula_21 and its strength varies with detuning and the laser drive.\n\nThe standard optomechanical setup is a Fabry–Pérot cavity, where one mirror is movable and thus provides an additional mechanical degree of freedom. Mathematically this system can be described by a single optical cavity mode coupled to a single mechanical mode. The coupling originates from the radiation pressure of the light field, that eventually moves the mirror a little bit, thus changing the cavity length and resonance frequency. The optical mode is driven by an external laser. This system can be described by the following effective Hamiltonian\n\nwhere formula_23 and formula_24 are the bosonic annihilation operators of the given cavity mode and the mechanical resonator respectively, satisfying the commutation relations \nformula_26 is the frequency of the optical mode, now dependant on the position formula_27 of the mechanical resonator, whereas formula_3 is the mechanical mode frequency. The last term describes the driving, with formula_29 the driving laser frequency and formula_30 the amplitude, given by \nwith formula_32 the input power coupled to the optical mode under consideration and formula_12 its linewidth. Of course, the system is coupled to the environment so the full treatment of the system should also include optical and mechanical dissipation (denoted by formula_12 and formula_35 respectively) and the corresponding noise entering the system.\n\nThe standard optomechanical Hamiltonian is obtained by getting rid of the explicit time dependence of the laser driving term and separating the optomechanical interaction from the free optical oscillator. This is achieved by switching into a reference frame rotating at the laser frequency formula_29 (in which case the optical mode annihilation operator undergoes the transformation\nformula_37) and applying a Taylor expansion on formula_26. Quadratic and higher order coupling terms are usually neglected, such that the standard Hamiltonian becomes\n\nwith formula_40 the laser detuning and the position operator formula_41.\nThe first two terms are the free optical and mechanical Hamiltonian respectively. The third term contains the optomechanical interaction, with formula_42 the single-photon optomechanical coupling strength (sometimes also called the bare optomechanical coupling). It determines by how much the cavity resonance frequency is shifted if the mechanical oscillator is displaced by the zero point uncertainty formula_43. Here, formula_44 is the effective mass of the mechanical oscillator. Sometimes, it is more convenient to use formula_45 instead, which determines the frequency change per displacement of the mirror, also called frequency pull parameter.\n\nE.g., for a Fabry–Pérot cavity of length formula_46 with a moving end-mirror the optomechanical coupling strength can directly be determined from the geometry to be formula_47.\n\nThis mathematical description using formula_48 is based on the assumption that only one optical and mechanical mode interact. Each optical cavity supports in principle an infinite number of modes and mechanical oscillators have more than a single oscillation/vibration mode. The validity of this approach relies on the possibility to tune the laser in such a way, that it populates a single optical mode only (that implies that the spacing between the cavity modes needs to be sufficiently large). Furthermore, scattering of photons to other modes is supposed to be negligible, which holds if the mechanical (motional) sidebands of the driven mode do not overlap with other cavity modes, i.e. if the mechanical mode frequency is smaller than the typical separation of the optical modes.\n\nUsually, the single-photon optomechanical coupling strength formula_49 is a small frequency, much smaller than the cavity decay rate formula_12, but the effective optomechanical coupling can be enhanced by increasing the drive power. Namely, with a strong enough drive, the dynamics of the system can be considered as quantum fluctuations around a classical steady state, i.e. formula_51, where formula_52 is the mean light field amplitude and formula_53 denotes the fluctuations. Expanding the photon number formula_54, we can omit the term formula_55 as it leads to a constant radiation pressure force which simply shifts the resonator's equilibrium position. Also neglecting the second order term formula_56, we obtain the \"linearized\" optomechanical Hamiltonian,\n\nformula_57\n\nwhere formula_58. This Hamiltonian is quadratic in operators, but it is called \"linearized\", because it leads to linear equations of motion. It is a valid description of many experiments, where formula_49 is typically very small and needs to be enhanced by the driving laser. Again, for a realistic description dissipation should be added to both the optical and the mechanical oscillator. The driving term from the standard Hamiltonian is not part of the linearized Hamiltonian, since it is the source of the classical light amplitude formula_52 around which the linearization was executed.\n\nWith a particular choice of detuning, different phenomena can be observed (see also the section about physical processes). The clearest distinction can be made between the following three cases:\n\nformula_61: In this case a rotating wave approximation of the linearized Hamiltonian, where one omits all non-resonant terms, reduces the coupling Hamiltonian to a beamsplitter operator, formula_62. This approximation works best on resonance, i.e. if the detuning becomes exactly equal to the negative mechanical frequency. Negative detuning (red detuning of the laser from the cavity resonance) by an amount equal to the mechanical mode frequency favors the anti-Stokes sideband, leading to a net cooling of the resonator. Laser photons absorb energy from the mechanical oscillator by annihilating phonons in order to become resonant with the cavity.\n\nformula_63: In this case a rotating wave approximation of the linearized Hamiltonian leads to other resonant terms. The coupling Hamiltonian takes the form formula_64, which is proportional to the two-mode squeezing operator. Therefore, two-mode squeezing and entanglement between the mechanical and optical modes can be observed with this parameter choice. Positive detuning (blue detuning of the laser from the cavity resonance) can also lead to instability. In the mechanical sideband picture, this corresponds to enhancing the Stokes sideband, i.e. the laser photons shed energy, increasing the number of phonons and becoming resonant with the cavity in the process.\n\nformula_65: In this case of driving on-resonance, one has to consider all terms in formula_66. The optical mode experiences a shift proportional to the mechanical displacement, which translates into a phase shift of the light transmitted through (or reflected off) the cavity. Thus, the cavity serves as an interferometer augmented by the factor of the optical finesse and can be used to measure very small displacements. (In fact, this setup has recently enabled LIGO to detect gravitational waves.)\n\nFrom the linearized Hamiltonian one can, adding dissipation and noise terms to the Heisenberg equations of motion, derive the so-called linearized quantum Langevin equations, which govern the dynamics of the optomechanical system.\n\nformula_67\n\nformula_68\n\nHere formula_69 and formula_70 are the input noise operators (either quantum or thermal noise) and formula_71 and formula_72 are the corresponding dissipative terms. Note that for optical photons, thermal noise can be neglected due to the high frequencies, such that the optical input noise can be described by quantum noise only (this does not hold for microwave implementations of the optomechanical system). For the mechanical oscillator thermal noise has to be taken into account and is the reason why many experiments are placed in additional cooling environments to lower the ambient temperature.\n\nRewritten in frequency space (i.e. a Fourier transform is applied), these first order differential equations become easily solvable.\n\nTwo main effects of the light on the mechanical oscillator can then be expressed in the following way:\n\nformula_73\n\nformula_74\n\nThe former is termed the optical-spring effect and may lead to significant frequency shifts in the case of low-frequency oscillators, such as pendulum mirrors. In the case of higher resonance frequencies, formula_75 MHz, it does not significantly alter the frequency. For a harmonic oscillator, the relation between a frequency shift and a change in the spring constant originates from Hooke's law.\n\nThe latter equation shows optical damping, i.e. the intrinsic mechanical damping formula_35 becomes stronger (or weaker) due to the optomechanical interaction. From the formula one can see that for negative detuning and large coupling, the mechanical damping can be greatly increased, which corresponds to cooling of the mechanical oscillator. In the case of positive detuning the optomechanical interaction leads to a negative contribution to the effective damping. This can lead to instability, when the effective damping drops below zero, formula_77, which means that it turns into an overall amplification rather than a damping of the mechanical oscillator.\n\nThe most basic regimes in which the optomechanical system can be operated are defined by the laser detuning formula_78 and described above. The resulting phenomenas are basically either cooling or heating of the mechanical oscillator. However, additional parameters determine what effects can actually be observed.\n\nThe \"good/bad cavity regime\" (also called the \"resolved/unresolved sideband regime\") relates the mechanical frequency to the optical linewidth: The good cavity regime (resolved sideband limit) is of experimental relevance since it is a necessary requirement to achieve ground-state cooling of the mechanical oscillator, i.e. cooling to an average mechanical occupation number below formula_79. The name \"resolved sideband regime\" refers to the possibility to distinguish the motional sidebands from the cavity resonance, which is the case if the linewidth of the cavity, formula_12 is smaller than the distance from the cavity resonance to the sideband, which is formula_3. This requirement leads to a condition for the so-called sideband parameter: formula_82. If formula_83 the system resides in the bad cavity regime (unresolved sideband limit). Here, the motional sideband lies within the peak of the cavity resonance. Actually, deep in the unresolved sideband regime, many motional sidebands can be included in the broad cavity linewidth, thus e.g. allowing a single photon to create more than one phonon, which leads to larger amplification of the mechanical oscillator.\n\nAnother distinction can be made depending on the optomechanical coupling strength. If the (enhanced) optomechanical coupling becomes larger than the cavity linewidth, formula_84 one enters the so-called \"strong-coupling regime\". There the optical and mechanical modes hybridize and normal-mode splitting occurs. This regime has to be distinguished from the (experimentally much more challenging) \"single-photon strong-coupling regime\", where the bare optomechanical coupling becomes of the order of the cavity linewidth, formula_85. Only in this regime, effects of the full non-linear interaction described by formula_86 become observable. For example, it is a precondition to create non-Gaussian states with the optomechanical system. Typical experiments currently operate in the linearized regime (small formula_87) thus investigating only effects of the linearized Hamiltonian.\n\nThe strength of the optomechanical Hamiltonian is the large range of experimental implementations to which it can be applied. This results in wide parameter ranges for the optomechanical parameters. For example, the size of optomechanical systems can be micrometers but also of the order of kilometers as it is the case for LIGO (although LIGO is dedicated to the detection of gravitational waves and not the investigation of optomechanics specifically).\n\nExamples of real optomechanical implementations are:\n\nOne purpose of studying so many different designs of the same system is the different parameter regimes that are accessible by different setups and their different potential to be converted into tools of commercial use.\n\nThe optomechanical system can be measured using e.g. a homodyne detection scheme. Either the light of the driving laser is measured, or a two-mode scheme is followed where a strong laser is used to drive the optomechanical system into the state of interest and a second laser is used for the read-out of the state of the system. This second “probe” laser is typically weak, i.e. its optomechanical interaction can be neglected as compared to the effects caused by the strong “pump” laser.\n\nThe optical output field can also be measured with single photon detectors e.g. to achieve photon counting statistics.\n\nOne of the questions which are still subject to current debate is the exact mechanism of decoherence. As Schrödinger pointed out, we would never see something like a cat in a quantum state. There needs to be something like a collapse of the quantum wave functions, which brings it from a quantum state to a pure classical state. Now one can ask where the boundary lies between objects with quantum properties and classical objects. Taking spatial superpositions as an example, there might be a size limit to objects which can be brought into superpositions, there might be a limit to the spatial separation of the centers of mass of a superposition or even a limit to the superposition of gravitational fields and its impact on small test masses. Those predictions could be checked with large mechanical structures which can be manipulated at the quantum level.\n\nSome easier to check predictions of quantum mechanics are the prediction of negative Wigner functions for certain quantum states, measurement precision beyond the standard quantum limit using squeezed states of light or the asymmetry of the sidebands in the spectrum of a cavity near the quantum ground state.\n\nYears before cavity optomechanics gained the status of an independent field of research, many of its techniques were already used in Gravitational wave detectors where it is necessary to measure displacements of mirrors on the order of the Planck scale. Even if these detectors do not address the measurement of quantum effects, they encounter related issues (photon shot noise) and use similar tricks (squeezed coherent states) to enhance the precision. Further applications include the development of quantum memory for quantum computers, high precision sensors (e.g. acceleration sensors) and quantum transducers e.g. between the optical and the microwave domain (taking advantage of the fact that the mechanical oscillator can easily couple to both frequency regimes).\n\nIn addition to the above explained standard cavity optomechanics there exist variations of this simplest model:\n\nExtensions to the standard optomechanical system include coupling to more and physically different systems:\n\nCavity optomechanics is closely related to trapped ion physics and Bose–Einstein condensates. These systems share very similar Hamiltonians, but they have less particles (about 10 for ion traps and formula_94-formula_95 for BECs) interacting with the field of light. It is also related to the field of cavity quantum electrodynamics.\n\n\n"}
{"id": "1636593", "url": "https://en.wikipedia.org/wiki?curid=1636593", "title": "Collaboratory", "text": "Collaboratory\n\nA collaboratory, as defined by William Wulf in 1989, is a “center without walls, in which the nation’s researchers can perform their research without regard to physical location, interacting with colleagues, accessing instrumentation, sharing data and computational resources, [and] accessing information in digital libraries” (Wulf, 1989).\n\nBly (1998) refines the definition to “a system which combines the interests of the scientific community at large with those of the computer science and engineering community to create integrated, tool-oriented computing and communication systems to support scientific collaboration” (Bly, 1998, p. 31).\n\nRosenberg (1991) considers a collaboratory as being an experimental and empirical research environment in which scientists work and communicate with each other to design systems, participate in collaborative science, and conduct experiments to evaluate and improve systems.\n\nA simplified form of these definitions would describe the collaboratory as being an environment where participants make use of computing and communication technologies to access shared instruments and data, as well as to communicate with others.\n\nHowever, a wide-ranging definition is provided by Cogburn (2003) who states that “a collaboratory is more than an elaborate collection of information and communications technologies; it is a new networked organizational form that also includes social processes; collaboration techniques; formal and informal communication; and agreement on norms, principles, values, and rules” (Cogburn, 2003, p. 86).\n\nThis concept has a lot in common with the notions of Interlock research, Information Routing Group and Interlock diagrams introduced in 1984.\n\nOther meaning\n\nThe word “collaboratory” is also used to describe an open space, creative process where a group of people work together to generate solutions to complex problems.\n\nThis meaning of the word originates from the visioning work of a large group of people – including scholars, artists, consultant, students, activists, and other professionals – who worked together on the 50+20 initiative aiming at transforming management education.\n\nIn this context, by fusing two elements, “collaboration” and “laboratory”, the word “collaboratory” suggests the construction of a space where people explore collaborative innovations. \nIt is, as defined by Dr. Katrin Muff, “an open space for all stakeholders where action learning and action research join forces, and students, educators, and researchers work with members of all facets of society to address current dilemmas.”\n\nThe concept of the collaboratory as a creative group process and its application are further developed in the book “The Collaboratory: A co-creative stakeholder engagement process for solving complex problems”.\n\nExamples of collaboratory events are provided on the website of the Collaboratory community as well as by Business School Lausanne- a Swiss business school that has adopted the collaboratory method to harness collective intelligence.\n\nProblems of geographic separation are especially present in large research projects. The time and cost for traveling, the difficulties in keeping contact with other scientists, the control of experimental apparatus, the distribution of information, and the large number of participants in a research project are just a few of the issues researchers are faced with.\n\nTherefore, collaboratories have been put into operation in response to these concerns and restrictions. However, the development and implementation proves to be not so inexpensive. From 1992 to 2000 financial budgets for scientific research and development of collaboratories ranged from US$447,000 to US$10,890,000 and the total use ranged from 17 to 215 users per collaboratory (Sonnenwald, 2003). Particularly higher costs occurred when software packages were not available for purchase and direct integration into the collaboratory or when requirements and expectations were not met.\n\nChin and Lansing (2004) state that the research and development of scientific collaboratories had, thus far, a tool-centric approach. The main goal was to provide tools for shared access and manipulation of specific software systems or scientific instruments. Such an emphasis on tools was necessary in the early development years of scientific collaboratories due to the lack of basic collaboration tools (e.g. text chat, synchronous audio or videoconferencing) to support rudimentary levels of communication and interaction. Today, however, such tools are available in off-the-shelf software packages such as Microsoft NetMeeting, IBM Lotus Sametime, Mbone Videoconferencing (Chin and Lansing, 2004). Therefore, the design of collaboratories may now move beyond developing general communication mechanisms to evaluating and supporting the very nature of collaboration in the scientific context (Chin & Lansing, 2004).\n\nThe evolution of the collaboratory\n\nAs stated in Chapter 4 of the 50+20 \"Management Education for the World\" book, \"the term collaboratory was first introduced in the late 1980s to address problems of geographic separation in large research projects related to travel time and cost, difficulties in keeping contact with other scientists, control of experimental apparatus, distribution of information, and the large number of participants. In their first decade of use, collaboratories were seen as complex and expensive information and communication technology (ICT) solutions supporting 15 to 200 users per project, with budgets ranging from 0.5 to 10 million USD. At that time, collaboratories were designed from an ICT perspective to serve the interests of the scientific community with tool-oriented computing requirements, creating an environment that enabled systems design and participation in collaborative science and experiments.\n\nThe introduction of a user-centered approach provided a first evolutionary step in the design philosophy of the collaboratory, allowing rapid prototyping and development circles. Over the past decade the concept of the collaboratory expanded beyond that of an elaborate ICT solution, evolving into a “new networked organizational form that also includes social processes, collaboration techniques, formal and informal communication, and agreement on norms, principles, values, and rules”. The collaboratory shifted from being a tool-centric to a data-centric approach, enabling data sharing beyond a common repository for storing and retrieving shared data sets. These developments have led to the evolution of the collaboratory towards a globally distributed knowledge work that produces intangible goods and services capable of being both developed and distributed around the world using traditional ICT networks.\n\nInitially, the collaboratory was used in scientific research projects with variable degrees of success. In recent years, collaboratory models have been applied to areas beyond scientific research and the national context. The wide acceptance of collaborative technologies in many parts of the world opens promising opportunities for international cooperation in critical areas where societal stakeholders are unable to work out solutions in isolation, providing a platform for large multidisciplinary teams to work on complex global challenges.\n\nThe emergence of open-source technology transformed the collaboratory into its next evolution. The term open-source was adopted by a group of people in the free software movement in Palo Alto in 1998 in reaction to the source code release of the Netscape Navigator browser. Beyond providing a pragmatic methodology for free distribution and access to an end product’s design and implementation details, open-source represents a paradigm shift in the philosophy of collaboration. The collaboratory has proven to be a viable solution for the creation of a virtual organization. Increasingly, however, there is a need to expand this virtual space into the real world. We propose another paradigm shift, moving the collaboratory beyond its existing ICT framework to a methodology of collaboration beyond the tool- and data-centric approaches, and towards an issue-centered approach that is transdisciplinary in nature.\"\n\nThe Collaboratory as a creative group process\n\nCopyright 50+20 \"Management Education for the World\" book\n\n\"Translating the concept of the collaboratory from the virtual space into a real environment demands a number of significant adjustments, leading us to yet another evolution. While the virtual collaboratory could count on ICT solutions to create and maintain an environment of collaboration, real-life interactions require facilitation experts to create and hold a space for members of the community, jointly developing transdisciplinary solutions around issues of concern. The ability to hold a space is central to the vision of management education.\n\nThe technology involved with holding a space implies the ability to create and maintain a powerful and safe learning platform. Such a space invites the whole person (mind, heart, soul and hands) into a place where the potential of a situation is fully realized. Holding a space is deeply grounded in our human heritage, and is still considered an important duty of the elders amongst many indigenous peoples. In Western society, good coaches fulfill a similar role, including the ability to be present in the moment, listening with all senses, being attuned to the invisible potential about to be expressed. As a result, what needs to happen, will happen. Facilitation and coaching experts understand the specific challenges involved in setting up an environment in which a great number of people can meet to discuss solutions that none of them could develop individually. Coaching and facilitation solutions already exist to create and hold such spaces, but are nevertheless distinctly different in a felt sense from the ICT-driven virtual collaboratories we have seen over the past two decades.\n\nThe evolution from the virtual collaboratory bears its own challenges and opportunities. In the co-creative process of the 50+20 vision, we learned to appreciate the power of the collaboratory both in real-life retreats as well as interactions between our gatherings. We propose that the next evolutionary step of the collaboratory will include both the broader community of researchers engaged in collaboratories around the world, as well as stakeholders in management education who seek to transform themselves by providing responsible leadership.\n\nIn our new definition, a collaboratory is an inclusive learning environment where action learning and action research meet. It involves the active collaboration of a diversified group of participants that bring in different perspectives on a given issue or topic. In such a space, learning and research is organized around issues rather than disciplines or theory. Such issues include: hunger, energy, water, climate change, migration, democracy, capitalism, terrorism, disease, the financial crisis, new economic models, management education that serves the world and similarly pressing matters. These issues are usually complex, messy and hard to resolve, demanding creative, systemic and divergent approaches. The collaboratory’s primary aim is to foster collective creativity.\nThe collaboratory is a place where people can think, work, learn together, and invent their respective futures. Its facilitators are highly experienced coaches who act as lead learners and guardians of the collaboratory space. They see themselves as transient gatekeepers of a world in need of new solutions. Subject experts are responsible for providing relevant knowledge and contributing it to the discussion in a relevant and pertinent matter. Students will continue to acquire subject knowledge outside the collaboratories – both through traditional and developing channels (such as online or blended learning).\n\nAs such, the faculty (of a business school, note added by the editor) is challenged to develop their capacities as facilitators and coaches in order to effectively guide these collaborative learning and research processes. To do this, they must step back from their role as experts and rather serve as facilitators in an open, participative and creative process. Faculty training and development needs to include not only a broad understanding of global issues, but also the development of facilitation and coaching skills.\n\nThe circular space of the collaboratory can become the preferred meeting place for citizens to jointly question, discuss and construct new ideas and approaches to resolve environmental, societal and economic challenges on both a regional and global level. Collaboratories should always reflect a rich combination of stakeholders: coaches, business and management faculty, citizens, politicians, entrepreneurs, people from different regions and cultures, youth and elders. Together they assemble differences in perspective, expertise and personal backgrounds, thereby adding a vital creative edge to every encounter, negotiation or problem-solving session.\"\n\nA distinctive characteristic of collaboratories is that they focus on data collection and analysis. Hence the interest to apply collaborative technologies to support data sharing as opposed to tool sharing. Chin and Lansing (2004) explore the shift of collaboratory development from traditional tool-centric approaches to more data-centric ones, to effectively support data sharing. This means more than just providing a common repository for storing and retrieving shared data sets. Collaboration, Chin and Lansing (2004) state, is driven both by the need to share data and to share knowledge about data. Shared data is only useful if sufficient context is provided about the data such that collaborators may comprehend and effectively apply it. It is therefore imperative, according to Chin and Lansing (2004), to know and understand how data sets relate to aspects of overall data space, applications, experiments, projects, and the scientific community, identifying the critical features or properties among which we can mention:\n\n\nHenline (1998) argues that communication about experimental data is another important characteristic of a collaboratory. By focusing attention on the dynamics of information exchange, the study of Zebrafish Information Network Project (Henline, 1998) concluded that the key challenges in creating a collaboratory may be social rather than technical. “A successful system must respect existing social conventions while encouraging the development of analogous mechanisms within the new electronic forum” (Henline, 1998, p. 69). Similar observations were made in the Computer-supported collaborative learning (CSCL) case study (Cogburn, 2003). The author (Cogburn, 2003) is investigating a collaboratory established for researchers in education and other related domains from United States of America and southern Africa. The main finding was that there have been important intellectual contributions on both sides, although the context was that of a developed country working together with a developing one and there have been social as well as cultural barriers. He further develops the idea that a successful CSCL would need to draw the best lessons learned on both sides in computer-mediated communication (CMC) and computer-supported cooperative work (CSCW).\n\nSonnenwald (2003) conducted seventeen interviews with scientists and revealed important considerations. Scientists expect a collaboratory to “support their strategic plans; facilitate management of the scientific process; have a positive or neutral impact on scientific outcomes; provide advantages and disadvantages for scientific task execution; and provide personal conveniences when collaborating across distances” (Sonnenwald, 2003, p. 68). Many scientists looked at the collaboratory as means to achieve strategic goals that were organizational and personal in nature. Other scientists anticipated that the scientific process would speed up when they had access to the collaboratory.\n\nFinholt (1995), based on the case studies of the Upper Atmospheric Research Collaboratory (UARC) and the Medical Collaboratory, establishes a design philosophy: a collaboratory project must be dedicated to a user-centered design (UCD) approach. This means a commitment to develop software in programming environments that allow rapid prototyping, rapid development cycles (Finholt, 1995). A consequence of the user-centered design in the collaboratory is that the system developers must be able to distinguish when a particular system or modification has positive impact on users’ work practices. An important part of obtaining this understanding is producing an accurate picture of how work is done prior to the introduction of technology. Finholt (1995) explains that behavioral scientists had the task of understanding the actual work settings for which new information technologies were developed. The goal of a user-centered design effort was to inject those observations back into the design process to provide a baseline for evaluating future changes and to illuminate productive directions for prototype development (Finholt, 1995).\n\nA similar viewpoint is expressed by Cogburn (2003) who relates the collaboratory to a globally distributed knowledge work, stating that human-computer interaction (HCI) and user-centered design (UCD) principles are critical for organizations to take advantage of the opportunities of globalization and the emergence of an Information society. He (Cogburn, 2003) refers to distributed knowledge work as being a set of “economic activities that produce intangible goods and services […], capable of being both developed and distributed around the world using the global information and communication networks” (Cogburn, 2003, p. 81). Through the use of these global information and communications networks, organizations are able to take part in globally disarticulated production, which means they can locate their research and development facilities almost anywhere in the world, and engineers can collaborate across time zones, institutions and national boundaries.\n\nMeeting expectations is a factor that influences adoption of innovations, including scientific collaboratories. Some of the collaboratories implemented thus far have not been entirely successful. The Mathematics and Computer Science Division of Argonne National Laboratory, Waterfall Glen collaboratory (Henline, 1998) is an illustrative example. This collaboratory had its shares of problems. There have been the occasional technical and social disasters, but most importantly it did not meet all of the collaboration and interaction requirements.\n\nThe vast majority of the evaluations performed thus far are concentrating mainly on the usage statistics (e.g. total number of members, hours of use, amount of data communicated) or on the immediate role in the production of traditional scientific outcomes (e.g. publications and patents). Sonnenwald (2003), however, argues that we should rather look for longer-term and intangible measures such as new and continued relationship among scientists, and subsequent, longer-term creation of new knowledge.\n\nRegardless of the criteria used for evaluation, we must focus on understanding the expectations and requirements defined for a collaboratory. Without such understanding a collaboratory runs the risk of not being adopted.\n\nOlson, Teasley, Bietz, and Cogburn (2002) ascertain some of the success factors of a collaboratory. They are: collaboration readiness, collaboration infrastructure readiness, and collaboration technology readiness.\n\nCollaboration readiness is the most basic pre-requisite for an effective collaboratory, according to Olson, Teasley, Bietz, and Cogburn (2002). Often the critical component to collaboration readiness is based on the concept of “working together in order to achieve a science goal” (Olson, Teasley, Bietz, & Cogburn, 2002, p. 46). Incentives to collaborate, shared principles of collaboration, and experience with the elements of collaboration are also crucial. Successful interaction between users requires a certain amount of common ground. Interactions require a high degree of trust or negotiation, especially when they involve areas where there is a cultural difference. “Ethical norms tend to be culturally specific, and negotiations about ethical issues require high levels of trust” (Olson, Teasley, Bietz, & Cogburn, 2002, p. 49).\n\nWhen analyzing the collaboration infrastructure readiness Olson, Teasley, Bietz, and Cogburn (2002) state that modern collaboration tools require adequate infrastructure to operate properly. Many off-the-shelf applications will run effectively only on state-of-the-art workstations. An important piece of the infrastructure is the technical support necessary to ensure version control, to get participants registered, and to recover in case of disaster. Communications cost is another element which can be critical for collaboration infrastructure readiness (Olson, Teasley, Bietz, & Cogburn, 2002). Pricing structures for network connectivity can affect the choices that users will make and therefore have an effect on the collaboratory’s final design and implementation.\n\nCollaboration technology readiness, according to Olson, Teasley, Bietz, and Cogburn (2002), refers to the fact that collaboration does not involve only technology and infrastructure, but also requires a considerable investment in training. Thus, it is essential to assess the state of technology readiness in the community to ensure success. If the level is too primitive more training is required to bring the users’ knowledge up-to-date.\n\nA comprehensively described example of a collaboratory, the Biological Sciences Collaboratory (BSC) at the Pacific Northwest National Laboratory (Chin & Lansing, 2004), enables the sharing and analysis of biological data through metadata capture, electronic laboratory notebooks, data organization views, data provenance tracking, analysis notes, task management, and scientific workflow management. BSC supports various data formats, has data translation capabilities, and can interact and exchange data with other sources (external databases, for example). It offers subscription capabilities (to allow certain individuals to access data) and verification of identities, establishes and manages permissions and privileges, and has data encryption capabilities (to ensure secure data transmission) as part of its security package.\n\nBSC also provides a data provenance tool and a data organization tool. These tools allow a hierarchical tree to display the historical lineage of a data set. From this tree-view the scientist may select a particular node (or an entire branch) to access a specific version of the data set (Chin & Lansing, 2004).\n\nThe task management provided by BSC allows users to define and track tasks related to a specific experiment or project. Tasks can have deadlines assigned, levels of priority, and dependencies. Tasks can also be queried and various reports produced. Related to task management, BSC provides workflow management to capture, manage, and supply standard paths of analyses. The scientific workflow may be viewed as process templates that captures and semi-automate the steps of an analysis process and its encompassing data sets and tools (Chin & Lansing, 2004).\n\nBSC provides project collaboration by allowing scientists to define and manage members of their group. Security and authentication mechanisms are therefore applied to limit access to project data and applications. Monitoring capability allows for members to identify other members that are online working on the project (Chin & Lansing, 2004).\n\nBSC offers community collaboration capabilities: scientists may publish their data sets to a larger community through the data portal. Notifications are in place for scientists interested in a particular set of data - when that data changes, the scientists get notification via email (Chin & Lansing, 2004).\n\nThe Collaboratory for Adaptation to Climate Change is an interdisciplinary project funded by a grant from the National Science Foundation’s Office of CyberInfrastructure and supported by the University of Notre Dame.\n\nThe research mission of the Collaboratory is to improve the dissemination and integration of knowledge that will inform the development of prescient adaptation strategies and policies. Goals of the Collaboratory include:\n\n\nThe Collaboratory is not a place for political debate, but is a place for pitching ideas and information with legitimate scientific backing that inform adaptation decisions.\n\nThe website is a resource for research, education, and collaboration in the area of adaptation and climate change. It incorporates a multitude of tools, which take several forms including biological simulation, searchable clearinghouses of legal information, and dissemination of emerging opinion from experts on the benefits and risks of adaptation. These tools can be used individually and in an integrative way to inform decision-making, research, and awareness. The site was created in April 2011 and is under continual development. Check back often for new additions and improvements.\n\nAdaptation, together with reduction of greenhouse gas emissions, is an essential part of solving the climate change crisis. Adaptation includes all of the steps that humans might take to help reduce the effects of climate change that are projected to occur in the 21st century and beyond. The Intergovernmental Panel on Climate Change (IPCC), and international body of scientists, projects that the global climate is likely to shift 2–6 °C (4–11 °F) warmer within 100 years. That amount of warming will disrupt ecosystems, raise sea levels, and perturb agriculture and human infrastructure. It also will affect human health. We can take steps now to reduce these effects in some instances and places. Adaptation includes, for example, helping species relocate to newly-suitable locations, protecting shores from rising waters and contoling pests that increase or spread under altered conditions. In 2011, Adapt is focused on adaptation for wildlife and biological resources.\n\nWe welcome scientists, natural resource managers and planners, students and the interested public to participate.Take a tour of our web site and see how you can use our infrastructure to further your own research and educational activities. Create your own account. It's free and will give you access to our online simulation tools and other features. Become a contributor by uploading your own presentations and simulation tools for others to share. Ask a question in our community forum, and let the community help you out.\n\nPancerella, Rahn, and Yang (1999) analyzed the Diesel Combustion Collaboratory (DCC) which was a problem-solving environment for combustion research. The main goal of DCC was to make the information exchange for the combustion researchers more efficient. Researchers would collaborate over the Internet using various DCC tools. These tools included “a distributed execution management system for running combustion models on widely distributed computers (distributed computing), including supercomputers; web accessible data archiving capabilities for sharing graphical experimental or modeling data; electronic notebooks and shared workspaces for facilitating collaboration; visualization of combustion data; and videoconferencing and data conferencing among researchers at remote sites” (Pancerella, Rahn, & Yang, 1999, p. 1).\n\nThe collaboratory design team defined the requirements to be (Pancerella, Rahn, & Yang, 1999):\n\n\nEach of these requirements had to be done securely and efficiently across the Internet. Resources availability was a major concern because many of the chemistry simulations could run for hours or even days on high-end workstations and produce Kilobytes to Megabytes of data sets. These data sets had to be visualized using simultaneous 2-D plots of multiple variables (Pancerella, Rahn, & Yang, 1999).\n\nThe deployment of the DCC was done in a phased approach. The first phase was based on iterative development, testing, and deployment of individual collaboratory tools. Once collaboratory team members had adequately tested each new tool, it was deployed to combustion researchers. The deployment of the infrastructure (videoconferencing tools, multicast routing capabilities, and data archives) was done in parallel (Pancerella, Rahn, & Yang, 1999). The next phase was to implement full security in the collaboratory. The primary focus was on two-way synchronous and multi-way asynchronous collaborations (Pancerella, Rahn, & Yang, 1999). The challenge was to balance the increased access to data that was needed with the security requirements. The final phase was the broadening of the target research to multiple projects including a broader range of collaborators.\n\nThe collaboratory team found that the highest impact was perceived by the geographically separated scientists that truly depended on each other to achieve their goals. One of the team’s major challenges was to overcome the technological and social barriers in order to meet all of the objectives (Pancerella, Rahn, & Yang, 1999). User openness and low maintenance security collaboratories are hard to achieve, therefore user feedback and evaluation are constantly required.\n\nOther collaboratories that have been implemented and can be further investigated are:\n\nSpecial consideration should be attributed to TANGO (Henline, 1998) because it is a step forward in implementing collaboratories, as it has distance learning and health care as main domains of operation. Henline (1998) mentions that the collaboratory has been successfully used to implement applications for distance learning, command and control center, telemedical bridge, and a remote consulting tool suite.\n\nTo date, most collaboratories have been applied largely in scientific research projects, with various degrees of success and failure. Recently, however, collaboratory models have been applied to additional areas of scientific research in both national and international contexts. As a result, a substantial knowledge base has emerged helping us in understanding their development and application in science and industry (Cogburn, 2003). Extending the collaboratory concept to include both social and behavioral research as well as more scientists from the developing world could potentially strengthen the concept and provide opportunities of learning more about the social and technical factors that support a distributed knowledge network (Cogburn, 2003).\n\nThe use of collaborative technologies to support geographically distributed scientific research is gaining wide acceptance in many parts of the world. Such collaboratories hold great promise for international cooperation in critical areas of scientific research and not only. As the frontiers of knowledge are pushed back the problems get more and more difficult, often requiring large multidisciplinary teams to make progress. The collaboratory is emerging as a viable solution, using communication and computing technologies to relax the constraints of distance and time, creating an instance of a virtual organization. The collaboratory is both an opportunity with very useful properties, but also a challenge to human organizational practices (Olson, 2002).\n\n\n"}
{"id": "5723", "url": "https://en.wikipedia.org/wiki?curid=5723", "title": "Constellations (journal)", "text": "Constellations (journal)\n\nConstellations: An International Journal of Critical and Democratic Theory is a quarterly peer-reviewed academic journal of critical and democratic theory and successor of \"Praxis International\". It is edited by Jean L. Cohen, Amy Allen, and Andreas Kalyvas. Seyla Benhabib is a co-founding former editor and Nancy Fraser a former co-editor. With a broad and international editorial contribution, it is based at the New School in New York.\n"}
{"id": "15555654", "url": "https://en.wikipedia.org/wiki?curid=15555654", "title": "Disinhibited attachment disorder", "text": "Disinhibited attachment disorder\n\nDisinhibited attachment disorder of childhood (DAD) according to the International Classification of Diseases (ICD-10), is defined as:\n\nDisinhibited attachment disorder is a subtype of the ICD-10 category F94, \"Disorders of social functioning with onset specific to childhood and adolescence\". The other subtype of F94 is reactive attachment disorder of childhood (RAD – F94 .1).\n\nSynonymous or similar disorders include Affectionless psychopathy and Institutional syndrome.\n\nWithin the ICD-10 category scheme, disinhibited attachment disorder specifically excludes Asperger syndrome (F84.5), hospitalism in children (F43.2), and hyperkinetic disorders (F90.-).\n\nThe DSM-IV distinguishes two categories of RAD: an inhibited subtype and a disinhibited subtype (in the DSM it is listed as 313.89 under infant diagnoses). The ICD-10 describes the former, emotionally withdrawn subtype as RAD and the latter subtype as Disinhibited Attachment Disorder (DAD) (Zeanah \"et al.\", 2004).\n\nGenerally, the DSM-IV criteria for the inhibited subtype of RAD were generated by studies done on children who were maltreated or abused. Criteria for the DSM-IV disinhibited subtype of RAD were based on research on children raised in institutions (Zeanah, 1996). This is largely based on the fact that inhibited subtype of RAD is more prevalent in maltreated children, and the disinhibited subtype of RAD is more prevalent in children raised in institutions (Zeanah, 2000).\n\nIn a study by Zeanah, (Zeanah \"et al.\", 2004) on reactive attachment disorder in maltreated toddlers, the criteria for DSM-IV \"disinhibited\" RAD (i.e. disinhibited attachment disorder) were:\n\n\nFor comparison, the criteria for DSM-IV \"inhibited\" RAD were:\n\n\nThe authors found that these two disorders were not completely independent; a few children may exhibit symptoms of both types of the disorder.\n\n\n"}
{"id": "9546", "url": "https://en.wikipedia.org/wiki?curid=9546", "title": "Engineering statistics", "text": "Engineering statistics\n\nEngineering statistics combines engineering and statistics using scientific methods for analyzing data. Engineering statistics involves data concerning manufacturing processes such as: component dimensions, tolerances, type of material, and fabrication process control. There are many methods used in engineering analysis and they are often displayed as histograms to give a visual of the data as opposed to being just numerical. Examples of methods are:\n\nEngineering statistics dates back to 1000 B.C. when the Abacus was developed as means to calculate numerical data. In the 1600s, the development of information processing to systematically analyze and process data began. In 1654, the Slide Rule technique was developed by Robert Bissaker for advanced data calculations. In 1833, a British mathematician named Charles Babbage designed the idea of an automatic computer which inspired developers at Harvard University and IBM to design the first mechanical automatic-sequence-controlled calculator called MARK I. The integration of computers and calculators into the industry brought about a more efficient means of analyzing data and the beginning of engineering statistics.\n\n"}
{"id": "1827600", "url": "https://en.wikipedia.org/wiki?curid=1827600", "title": "Environmental history", "text": "Environmental history\n\nEnvironmental history is the study of human interaction with the natural world over time, emphasising the active role nature plays in influencing human affairs and vice versa.\n\nEnvironmental history emerged in the United States out of the environmental movement of the 1960s and 1970s, and much of its impetus still stems from present-day global environmental concerns. The field was founded on conservation issues but has broadened in scope to include more general social and scientific history and may deal with cities, population or sustainable development. As all history occurs in the natural world, environmental history tends to focus on particular time-scales, geographic regions, or key themes. It is also a strongly multidisciplinary subject that draws widely on both the humanities and natural science.\n\nThe subject matter of environmental history can be divided into three main components. The first, nature itself and its change over time, includes the physical impact of humans on the Earth's land, water, atmosphere and biosphere. The second category, how humans use nature, includes the environmental consequences of increasing population, more effective technology and changing patterns of production and consumption. Other key themes are the transition from nomadic hunter-gatherer communities to settled agriculture in the neolithic revolution, the effects of colonial expansion and settlements, and the environmental and human consequences of the industrial and technological revolutions. Finally, environmental historians study how people think about nature - the way attitudes, beliefs and values influence interaction with nature, especially in the form of myths, religion and science.\n\nIn 1967 Roderick Nash published \"Wilderness and the American Mind\", a work that has become a classic text of early environmental history. In an address to the \"Organization of American Historians\" in 1969 (published in 1970) Nash used the expression \"environmental history\", although 1972 is generally taken as the date when the term was first coined. The 1959 book by Samuel P. Hays, \"Conservation and the Gospel of Efficiency: The Progressive Conservation Movement, 1890-1920\", while being a major contribution to American political history, is now also regarded as a founding document in the field of environmental history. Hays is Professor Emeritus of History at the University of Pittsburgh.\n\nBrief overviews of the historiography of environmental history have been given by J. R. McNeill, Richard White, and J. Donald Hughes. In 2014 Oxford University Press published a volume of 25 essays called \"The Oxford Handbook of Environmental History\".\n\nThere is no universally accepted definition of environmental history. In general terms it is a history that tries to explain why our environment is like it is and how humanity has influenced its current condition, as well as commenting on the problems and opportunities of tomorrow. Donald Worster's widely quoted 1988 definition states that environmental history is the \"interaction between human cultures and the environment in the past\".\n\nIn 2001, J. Donald Hughes defined the subject as the “study of human relationships through time with the natural communities of which they are a part in order to explain the processes of change that affect that relationship”. and, in 2006, as \"history that seeks understanding of human beings as they have lived, worked and thought in relationship to the rest of nature through the changes brought by time\". \"As a method, environmental history is the use of ecological analysis as a means of understanding human history...an account of changes in human societies as they relate to changes in the natural environment”. Environmental historians are also interested in \"what people think about nature, and how they have expressed those ideas in folk religions, popular culture, literature and art”. In 2003, J. R. McNeill defined it as \"the history of the mutual relations between humankind and the rest of nature\".\n\nTraditional historical analysis has over time extended its range of study from the activities and influence of a few significant people to a much broader social, political, economic, and cultural analysis. Environmental history further broadens the subject matter of conventional history. In 1988, Donald Worster stated that environmental history “attempts to make history more inclusive in its narratives” by examining the “\"role and place of nature in human life\"”, and in 1993, that “Environmental history explores the ways in which the biophysical world has influenced the course of human history and the ways in which people have thought about and tried to transform their surroundings\"”. The interdependency of human and environmental factors in the creation of landscapes is expressed through the notion of the cultural landscape. Worster also questioned the scope of the discipline, asking: \"\"We study humans and nature; therefore can anything human or natural be outside our enquiry?\"\"\n\nEnvironmental history is generally treated as a subfield of history. But some environmental historians challenge this assumption, arguing that while traditional history is human history – the story of people and their institutions, \"humans cannot place themselves outside the principles of nature\". In this sense, they argue that environmental history is a version of human history within a larger context, one less dependent on anthropocentrism (even though anthropogenic change is at the center of its narrative).\n\nJ. Donald Hughes responded to the view that environmental history is \"light on theory\" or lacking theoretical structure by viewing the subject through the lens of three \"dimensions\": nature and culture, history and science, and scale. This advances beyond Worster's recognition of three broad clusters of issues to be addressed by environmental historians although both historians recognize that the emphasis of their categories might vary according to the particular study as, clearly, some studies will concentrate more on society and human affairs and others more on the environment.\n\nSeveral themes are used to express these historical dimensions. A more traditional historical approach is to analyse the transformation of the globe’s ecology through themes like the separation of man from nature during the neolithic revolution, imperialism and colonial expansion, exploration, agricultural change, the effects of the industrial and technological revolution, and urban expansion. More environmental topics include human impact through influences on forestry, fire, climate change, sustainability and so on. According to Paul Warde, “\"the increasingly sophisticated history of colonization and migration can take on an environmental aspect, tracing the pathways of ideas and species around the globe and indeed is bringing about an increased use of such analogies and ‘colonial’ understandings of processes within European history.\"” The importance of the colonial enterprise in Africa, the Caribbean and Indian Ocean has been detailed by Richard Grove. \nMuch of the literature consists of case-studies targeted at the global, national and local levels.\n\nAlthough environmental history can cover billions of years of history over the whole Earth, it can equally concern itself with local scales and brief time periods. Many environmental historians are occupied with local, regional and national histories. Some historians link their subject exclusively to the span of human history – \"every time period in human history\" while others include the period before human presence on Earth as a legitimate part of the discipline. Ian Simmons's \"Environmental History of Great Britain\" covers a period of about 10,000 years. There is a tendency to difference in time scales between natural and social phenomena: the causes of environmental change that stretch back in time may be dealt with socially over a comparatively brief period.\n\nAlthough at all times environmental influences have extended beyond particular geographic regions and cultures, during the 20th and early 21st centuries anthropogenic environmental change has assumed global proportions, most prominently with climate change but also as a result of settlement, the spread of disease and the globalization of world trade.\n\nThe questions of environmental history date back to antiquity, including Hippocrates, the father of medicine, who asserted that different cultures and human temperaments could be related to the surroundings in which peoples lived in \"Airs, Waters, Places\". Scholars as varied as Ibn Khaldun and Montesquieu found climate to be a key determinant of human behavior. During the Enlightenment, there was a rising awareness of the environment and scientists addressed themes of sustainability via natural history and medicine. However, the origins of the subject in its present form are generally traced to the 20th century.\n\nIn 1929 a group of French historians founded the journal \"Annales\", in many ways a forerunner of modern environmental history since it took as its subject matter the reciprocal global influences of the environment and human society. The idea of the impact of the physical environment on civilizations was espoused by this Annales School to describe the long term developments that shape human history by focusing away from political and intellectual history, toward agriculture, demography, and geography. Emmanuel Le Roy Ladurie, a pupil of the Annales School, was the first to really embrace, in the 1950s, environmental history in a more contemporary form. One of the most influential members of the Annales School was Lucien Febvre (1878–1956), whose book \"A Geographical Introduction to History\" is now a classic in the field.\n\nThe most influential empirical and theoretical work in the subject has been done in the United States where teaching programs first emerged and a generation of trained environmental historians is now active. In the United States environmental history as an independent field of study emerged in the general cultural reassessment and reform of the 1960s and 1970s along with environmentalism, \"conservation history\", and a gathering awareness of the global scale of some environmental issues. This was in large part a reaction to the way nature was represented in history at the time, which “portrayed the advance of culture and technology as releasing humans from dependence on the natural world and providing them with the means to manage it [and] celebrated human mastery over other forms of life and the natural environment, and expected technological improvement and economic growth to accelerate”. Environmental historians intended to develop a post-colonial historiography that was \"more inclusive in its narratives\".\n\nMoral and political inspiration to environmental historians has come from American writers and activists such as Henry Thoreau, John Muir, Aldo Leopold, and Rachel Carson. Environmental history \"frequently promoted a moral and political agenda although it steadily became a more scholarly enterprise\". Early attempts to define the field were made in the United States by Roderick Nash in “The State of Environmental History” and in other works by frontier historians Frederick Jackson Turner, James Malin, and Walter Prescott Webb, who analyzed the process of settlement. Their work was expanded by a second generation of more specialized environmental historians such as Alfred Crosby, Samuel P. Hays, Donald Worster, William Cronon, Richard White, Carolyn Merchant, J. R. McNeill, Donald Hughes, and Chad Montrie in the United States and Paul Warde, Sverker Sorlin, Robert A. Lambert, T.C. Smout, and Peter Coates in Europe.\n\nAlthough environmental history was growing rapidly after 1970, it only reached historians of the British Empire in the 1990s. Gregory Barton argues that the concept of environmentalism emerged from forestry studies, and emphasizes the British imperial role in that research. He argues that imperial forestry movement in India around 1900 included government reservations, new methods of fire protection, and attention to revenue-producing forest management. The result eased the fight between romantic preservationists and laissez-faire businessmen, thus giving the compromise from which modern environmentalism emerged.\n\nIn recent years numerous scholars cited by James Beattie have examined the environmental impact of the Empire. Beinart and Hughes argue that the discovery and commercial or scientific use of new plants was an important concern in the 18th and 19th centuries. The efficient use of rivers through dams and irrigation projects was an expensive but important method of raising agricultural productivity. Searching for more efficient ways of using natural resources, the British moved flora, fauna and commodities around the world, sometimes resulting in ecological disruption and radical environmental change. Imperialism also stimulated more modern attitudes toward nature and subsidized botany and agricultural research. Scholars have used the British Empire to examine the utility of the new concept of eco-cultural networks as a lens for examining interconnected, wide-ranging social and environmental processes.\n\nIn the United States the American Society for Environmental History was founded in 1975 while the first institute devoted specifically to environmental history in Europe was established in 1991, based at the University of St. Andrews in Scotland. In 1986, the Dutch foundation for the history of environment and hygiene \"Net Werk\" was founded and publishes four newsletters per year. In the UK the White Horse Press in Cambridge has, since 1995, published the journal \"Environment and History\" which aims to bring scholars in the humanities and biological sciences closer together in constructing long and well-founded perspectives on present day environmental problems and a similar publication \"Tijdschrift voor Ecologische Geschiedenis\" (\"Journal for Environmental History\") is a combined Flemish-Dutch initiative mainly dealing with topics in the Netherlands and Belgium although it also has an interest in European environmental history. Each issue contains abstracts in English, French and German. In 1999 the Journal was converted into a yearbook for environmental history. In Canada the Network in Canadian History and Environment facilitates the growth of environmental history through numerous workshops and a significant digital infrastructure including their website and podcast.\n\nCommunication between European nations is restricted by language difficulties. In April 1999 a meeting was held in Germany to overcome these problems and to co-ordinate environmental history in Europe. This meeting resulted in the creation of the European Society for Environmental History in 1999. Only two years after its establishment, ESEH held its first international conference in St. Andrews, Scotland. Around 120 scholars attended the meeting and 105 papers were presented on topics covering the whole spectrum of environmental history. The conference showed that environmental history is a viable and lively field in Europe and since then ESEH has expanded to over 400 members and continues to grow and attracted international conferences in 2003 and 2005. In 1999 the \"Centre for Environmental History\" was established at the University of Stirling. Some history departments at European universities are now offering introductory courses in environmental history and postgraduate courses in Environmental history have been established at the Universities of Nottingham, Stirling and Dundee and more recently a Graduierten Kolleg was created at the University of Göttingen in Germany. In 2009, the Rachel Carson Center for Environment and Society (RCC), an international, interdisciplinary center for research and education in the environmental humanities and social sciences, was founded as a joint initiative of Munich's Ludwig-Maximilians-Universität and the Deutsches Museum, with the generous support of the German Federal Ministry of Education and Research. The Environment & Society Portal (environmentandsociety.org) is the Rachel Carson Center's open access digital archive and publication platform. \nEnvironmental history prides itself in bridging the gap between the arts and natural sciences although to date the scales weigh on the side of science. A definitive list of related subjects would be lengthy indeed and singling out those for special mention a difficult task. However, those frequently quoted include, historical geography, the history and philosophy of science, history of technology and climate science. On the biological side there is, above all, ecology and historical ecology, but also forestry and especially forest history, archaeology and anthropology. When the subject engages in environmental advocacy it has much in common with environmentalism.\n\nWith increasing globalization and the impact of global trade on resource distribution, concern over never-ending economic growth and the many human inequities environmental history is now gaining allies in the fields of ecological and environmental economics.\n\nEngagement with sociological thinkers and the humanities is limited but cannot be ignored through the beliefs and ideas that guide human action. This has been seen as the reason for a perceived lack of support from traditional historians.\n\nThe subject has a number of areas of lively debate. These include discussion concerning: what subject matter is most appropriate; whether environmental advocacy can detract from scholarly objectivity; standards of professionalism in a subject where much outstanding work has been done by non-historians; the relative contribution of nature and humans in determining the passage of history; the degree of connection with, and acceptance by, other disciplines - but especially mainstream history. For Paul Warde the sheer scale, scope and diffuseness of the environmental history endeavour calls for an analytical toolkit \"a range of common issues and questions to push forward collectively\" and a \"core problem\". He sees a lack of \"human agency\" in its texts and suggest it be written more to act: as a source of information for environmental scientists; incorporation of the notion of risk; a closer analysis of what it is we mean by \"environment\"; confronting the way environmental history is at odds with the humanities because it emphasises the division between \"materialist, and cultural or constructivist explanations for human behaviour\".\n\nMany of the themes of environmental history inevitably examine the circumstances that produced the environmental problems of the present day, a litany of themes that challenge global sustainability including: population, consumerism and materialism, climate change, waste disposal, deforestation and loss of wilderness, industrial agriculture, species extinction, depletion of natural resources, invasive organisms and urban development. The simple message of sustainable use of renewable resources is frequently repeated and early as 1864 George Perkins Marsh was pointing out that the changes we make in the environment may later reduce the environments usefulness to humans so any changes should be made with great care - what we would nowadays call enlightened self-interest. Richard Grove has pointed out that \"States will act to prevent environmental degradation only when their economic interests are threatened\".\n\nIt is not clear whether environmental history should promote a moral or political agenda. The strong emotions raised by environmentalism, conservation and sustainability can interfere with historical objectivity: polemical tracts and strong advocacy can compromise objectivity and professionalism. Engagement with the political process certainly has its academic perils although accuracy and commitment to the historical method is not necessarily threatened by environmental involvement: environmental historians have a reasonable expectation that their work will inform policy-makers.\n\nA recent historiographical shift has placed an increased emphasis on inequality as an element of environmental history. Imbalances of power in resources, industry, and politics have resulted in the burden of industrial pollution being shifted to less powerful populations in both the geographic and social spheres. An critical examination of the traditional environmentalist movement from this historical perspective notes the ways in which early advocates of environmentalism sought the aesthetic preservation of middle-class spaces and sheltered their own communities from the worst effects of air and water pollution, while neglecting the plight of the less privileged. \n\nCommunities with less economic and sociopolitical power often lack the resources to get involved in environmental advocacy. Environmental history increasingly highlights the ways in which the middle-class environmental movement has fallen short and left behind entire communities. Interdisciplinary research now understands historic inequality as a lens through which to predict future social developments in the environmental sphere, particularly with regard to climate change. The United Nations Department of Economic and Social Affairs cautions that a warming planet will exacerbate environmental and other inequalities, particularly with regard to: \"(a) increase in the exposure of the disadvantaged groups to the adverse effects of climate change; (b) increase in their susceptibility to damage caused by climate change; and (c) decrease in their ability to cope and recover from the damage suffered.\" As an interdisciplinary field that encompasses a new understanding of social justice dynamics in a rapidly changing global climate, environmental history is inherently advocative.\n\nNarratives of environmental history tend to be declensionist, that is, accounts of progressive decline under human activity.\n\nUnder the accusation of \"presentism\" it is sometimes claimed that, with its genesis in the late 20th century environmentalism and conservation issues, environmental history is simply a reaction to contemporary problems, an \"attempt to read late twentieth century developments and concerns back into past historical periods in which they were not operative, and certainly not conscious to human participants during those times\". This is strongly related to the idea of culpability. In environmental debate blame can always be apportioned, but it is more constructive for the future to understand the values and imperatives of the period under discussion so that causes are determined and the context explained.\n\nFor some environmental historians \"the general conditions of the environment, the scale and arrangement of land and sea, the availability of resources, and the presence or absence of animals available for domestication, and associated organisms and disease vectors, that makes the development of human cultures possible and even predispose the direction of their development\" and that \"history is inevitably guided by forces that are not of human origin or subject to human choice\". This approach has been attributed to American environmental historians Webb and Turner and, more recently to Jared Diamond in his book \"Guns, Germs, and Steel\", where the presence or absence of disease vectors and resources such as plants and animals that are amenable to domestication that may not only stimulate the development of human culture but even determine, to some extent, the direction of that development. The claim that the path of history has been forged by environmental rather than cultural forces is referred to as environmental determinism while, at the other extreme, is what may be called cultural determinism. An example of cultural determinism would be the view that human influence is so pervasive that the idea of pristine nature has little validity - that there is no way of relating to nature without culture.\n\nUseful guidance on the process of doing environmental history has been given by Donald Worster, Carolyn Merchant, William Cronon and Ian Simmons. Worster's three core subject areas (the environment itself, human impacts on the environment, and human thought about the environment) are generally taken as a starting point for the student as they encompass many of the different skills required. The tools are those of both history and science with a requirement for fluency in the language of natural science and especially ecology. In fact methodologies and insights from a range of physical and social sciences is required, there seeming to be universal agreement that environmental history is indeed a multidisciplinary subject.\n\n\nIn 2004 a theme issue of \"Environment and History 10(4)\" provided an overview of environmental history as practiced in Africa, the Americas, Australia, New Zealand, China and Europe as well as those with global scope. J. Donald Hughes (2006) has also provided a global conspectus of major contributions to the environmental history literature.\n\n\n\n\n\n\n\n\n\n\n\nEnvironmental history, like all historical studies, shares the hope that through an examination of past events it may be possible to forge a more considered future. In particular a greater depth of historical knowledge can inform environmental controversies and guide policy decisions.\n\nThe subject continues to provide new perspectives, offering cooperation between scholars with different disciplinary backgrounds and providing an improved historical context to resource and environmental problems. There seems little doubt that, with increasing concern for our environmental future, environmental history will continue along the path of environmental advocacy from which it originated as “\"human impact on the living systems of the planet bring us no closer to utopia, but instead to a crisis of survival\"” with key themes being population growth, climate change, conflict over environmental policy at different levels of human organization, extinction, biological invasions, the environmental consequences of technology especially biotechnology, the reduced supply of resources - most notably energy, materials and water. Hughes comments that environmental historians “\"will find themselves increasingly challenged by the need to explain the background of the world market economy and its effects on the global environment. Supranational instrumentalities threaten to overpower conservation in a drive for what is called sustainable development, but which in fact envisions no limits to economic growth\"”. Hughes also notes that \"\"environmental history is notably absent from nations that most adamantly reject US, or Western influences\".\n\nMichael Bess sees the world increasingly permeated by potent technologies in a process he calls “artificialization” which has been accelerating since the 1700s, but at a greatly accelerated rate after 1945. Over the next fifty years, this transformative process stands a good chance of turning our physical world, and our society, upside-down. Environmental historians can “\"play a vital role in helping humankind to understand the gale-force of artifice that we have unleashed on our planet and on ourselves\"”.\n\nAgainst this background “\"environmental history can give an essential perspective, offering knowledge of the historical process that led to the present situation, give examples of past problems and solutions, and an analysis of the historical forces that must be dealt with\"” or, as expressed by William Cronon, \"The viability and success of new human modes of existing within the constraints of the environment and its resources requires both an understanding of the past and an articulation of a new ethic for the future\".\"\n\nKey journals in this field include: \n\n\n\n\n\n\n\n\n\n"}
{"id": "5657152", "url": "https://en.wikipedia.org/wiki?curid=5657152", "title": "ExPASy", "text": "ExPASy\n\nExPASy is a bioinformatics resource portal operated by the SIB Swiss Institute of Bioinformatics and in particular the SIB Web Team. It is an extensible and integrative portal accessing many scientific resources, databases and software tools in different areas of life sciences. Scientists can access a wide range of resources in many different domains, such as proteomics, genomics, phylogenetics/evolution, systems biology, population genetics, and transcriptomics. The individual resources (databases, web-based and downloadable software tools) are hosted in a decentralised way by different groups of the SIB Swiss Institute of Bioinformatics and partner institutions. Specifically, a single web portal provides a common entry point to a wide range of resources developed and operated by many different SIB groups and external institutions. The portal features a search function across selected resources. Internally, the availability and usage of resources are monitored. The portal is aimed for both expert users and for people who are not familiar with a specific domain in life sciences: in particular, the new web interface provides visual guidance for newcomers to ExPASy.\n\nOriginally, ExPASy was called ExPASy (Expert Protein Analysis System) and acted as a proteomics server to analyze protein sequences and structures and two-dimensional gel electrophoresis (2-D Page electrophoresis). Among others, ExPASy hosted the protein sequence knowledgebase, UniProtKB/Swiss-Prot, and its computer annotated supplement, UniProtKB/TrEMBL, before these moved to the UniProt website.\n\nExPASy was the first website of the life sciences.\n\n, ExPASy had been consulted 1 billion times since its installation on 1 August 1993.\n\n"}
{"id": "57651092", "url": "https://en.wikipedia.org/wiki?curid=57651092", "title": "Explorer 29", "text": "Explorer 29\n\nExplorer 29 (also called GEOS 1 or GEOS A, acronym to Geodetic Earth Orbiting Satellite) was an American satellite launched as part of the Explorers program, being the first of the two satellites GEOS. Explorer 29 was launched on 6 November 1965 from Cape Canaveral, Florida, United States, with Delta rocket.\n\nExplorer 29 was a gravity-gradient-stabilized, solar cell powered unit designed exclusively for geodetic studies. It was the first successful active spacecraft of the \"National Geodetic Satellite Program\". Instrumentation included:\n\n\nThese were designed to operate simultaneously to fulfill the objectives of locating observation points (geodetic control stations) in a 3 dimensional earth center-of-mass coordinate system within of accuracy, of defining the structure of the earth's irregular gravitational field and refining the locations and magnitudes of the large gravity anomalies, and of comparing results of the various systems onboard the spacecraft to determine the most accurate and reliable system. Acquisition and recording of data were the responsibility of the GSFC \"Space Tracking and Data Acquisitions Network\" (STADAN). Ten major observing networks were used.\n\n"}
{"id": "50143397", "url": "https://en.wikipedia.org/wiki?curid=50143397", "title": "Fitness-density covariance", "text": "Fitness-density covariance\n\nThe fitness-density covariance is a coexistence mechanism that can allow similar species to coexist because they are in different locations. This effect will be the strongest if species are completely segregated, but can also work if their populations overlap somewhat. If a fitness-density covariance is operating, then when a species becomes very rare, its population will shift to predominantly locations with favorable conditions (e.g., less competition or good habitat). Similarly, when a species becomes very common, then conditions will worsen where they are most common, and they will spread into areas where conditions are less favorable. This negative feedback can help species avoid being driven extinct by competition, and it can prevent stronger species from becoming too common and crowding out other species.\n\nAlong with storage effects and relative nonlinearities, fitness-density covariances make up the three variation-dependent mechanisms of modern coexistence theory.\n\nHere, we will consider competition between \"n\" species. We will define \"N\"(\"t\") as the number of individuals of species \"j\" at patch \"x\" and time \"t\", and \"λ\"(\"t\") the fitness (i.e., the per-capita contribution of individual to the next time period through survival and reproduction) of individuals of species \"j\"s at patch \"x\" and time \"t\". \"λ\"(t) will be determined by many things, including habitat, intraspecific competition, and interspecific competition at \"x\". Thus, if there are currently \"N\"(\"t\") individuals at \"x\", then they will contribute \"N\"\"λ\"(\"t\") individuals to the next time period (i.e., \"t\"+1). Those individuals may stay at \"x\", or they may move; the net contribution of \"x\" to next year's population will be the same.\n\nWith our definitions in place, we want to calculate the finite rate of increase of species \"j\" (i.e., its population-wide growth rate), formula_1. It is defined such that formula_2, where each average is across all space. In essence, it is the average fitness of members of species \"j\" in year \"t\". We can calculate \"N\"(\"t\"+1) by summing \"N\"\"λ\"(\"t\") across all patches, giving\nwhere \"X\" is the number of patches. Defining formula_4 as species \"j\"'s relative density at \"x\", this equation becomes\nUsing the theorem that formula_6, this simplifies to\nSince formula_8, its average will be 1. Thus,\nThus, we have partitioned formula_10 into two key parts: formula_11 calculates the fitness of an individual, on average in any given site. Thus, if species are distributed uniformly across the landscape, formula_12. If, however, they are distributed non-randomly across the environment, then cov(\"ν\", \"λ\"(\"t\")) will be non-zero. If individuals are found predominantly in good sites, then cov(\"ν\", \"λ\"(\"t\")) will be positive; if they are found predominantly in poor sites, then cov(\"ν\", \"λ\"(\"t\")) will be negative.\n\nTo analyze how species coexist, we perform an invasion analysis. In short, we remove one species (called the \"invader\") from the environment, and allow the other species (called the \"residents\") to come the equilibrium (so that formula_13 for each resident). We then determine if the invader has a positive growth rate. If each species has a positive growth rate as an invader, then they can coexist.\n\nBecause formula_13 for each resident, we can calculate the invader's growth rate, formula_15, as\nwhere \"n\"-1 is the number of residents (since \"n\" is the number of species), and the sum is over all residents (and thus represents an average). Using our formula for formula_10, we find that\nThis rearranges to\nwhere \nis the fitness-density covariance, and formula_21 contains all other mechanisms (such as the spatial storage effect).\n\nThus, if Δ\"κ\" is positive, then the invader's population is more able to build up its population in good areas (i.e., \"ν\" is higher where \"λ\"(\"t\") is large), compared to the residents. This can occur if the invader builds up in good areas (i.e., cov(\"ν\", \"λ\"(\"t\")) is very positive) or if the residents are forced into poor areas (i.e., cov(\"ν\", \"λ\"(\"t\")) is less positive, or negative). In either case, species gain an advantage when they are invaders, a key point of any stabilizing mechanism.\n"}
{"id": "17863991", "url": "https://en.wikipedia.org/wiki?curid=17863991", "title": "Group medical practice in the United States", "text": "Group medical practice in the United States\n\nGroup medical practices practice medicine by physicians who share resources.\n\nThere are approximately 230,187 physician practices in the United States. Among the physician practices, 16.5% had only one office-based physician in 2016. Physician group practices with 2-4 physicians make up 22.3% of physician offices in the United States, 19.8% have 5-10 physicians, 12.1% have 11-24 physicians, 6.3% have 25-49, and the remaining 13.5% have 50 or more physicians.\n\nIn recent years, many small or solo practitioners have come together to form larger specialty groups because of managed care.\n\nThe Centers for Medicare and Medicaid Services (CMS) changed the definition of \"group practice\" in its 2012 physician fee schedule to mean 25 or more eligible professionals in the same practice.\n"}
{"id": "9258706", "url": "https://en.wikipedia.org/wiki?curid=9258706", "title": "Halmos College of Natural Sciences and Oceanography", "text": "Halmos College of Natural Sciences and Oceanography\n\nThe Halmos College of Natural Sciences and Oceanography is a natural science college at Nova Southeastern University in Florida. The college offers programs in subjects like biology and mathematics and conducts oceanographical research.\n\nThe college offers multiple bachelor's, master's and doctoral programs.\n\nThe college has a presence at two campuses: in the Parker Building on the Fort Lauderdale/Davie Campus, and the Oceanographic Center, located on a site on the ocean side of Port Everglades, adjacent to the port's entrance. The center has a boat basin and affords immediate access to the Gulf Stream, the Florida Straits, and the Bahama Banks. The center is composed of three buildings, and several modulars. The main two-story building houses seven laboratories, conference rooms, workroom, and 13 offices. A second building contains a large two-story warehouse and staging area, classroom, biology laboratory, electron microscopy laboratory, darkroom, machine shop, carpentry shop, electronics laboratory, the library, student computer lab, computing center, and 15 offices. A one-story building contains a wetlab/classroom, coral workshop, and an X-ray facility. A modular laboratory is used for aquaculture studies. The Oceanographic Center grows and sells red mangroves.\n\nThe Oceanographic Center is host to the National Coral Reef Institute was established by Congressional mandate in 1998. NCRI's primary objective is the assessment, monitoring, and restoration of coral reefs through basic and applied research and through training and education.\n\nThe Guy Harvey Research Institute conducts basic and applied scientific research needed for effective conservation, biodiversity maintenance, restoration, and understanding of the world's wild fishes. The GHRI also provides scientific training to US and international students interested in ocean health. The GHRI is named for Jamaican artist Guy Harvey known for his marine themed works.\n\nThe college supplies the Broward County Sea Turtle Conservation Program with contract employees and research facilities.\n\n"}
{"id": "18859390", "url": "https://en.wikipedia.org/wiki?curid=18859390", "title": "History of the internal combustion engine", "text": "History of the internal combustion engine\n\nVarious scientists and engineers contributed to the development of internal combustion engines. In 1791, John Barber developed a turbine. In 1794 Thomas Mead patented a gas engine. Also in 1794 Robert Street patented an internal combustion engine, which was also the first to use the liquid fuel (gasoline), and built an engine around that time. In 1798, John Stevens designed the first American internal combustion engine. In 1807, French engineers Nicéphore (who went on to invent photography) and Claude Niépce ran a prototype internal combustion engine, using controlled dust explosions, the Pyréolophore. This engine powered a boat on the Saône river, France. The same year, the Swiss engineer François Isaac de Rivaz built an internal combustion engine ignited by electric spark. In 1823, Samuel Brown patented the first internal combustion engine to be applied industrially, one of his engines pumped water on the Croydon Canal from 1830 to 1836. He also demonstrated a boat using his engine on the Thames in 1827, and an engine driven carriage in 1828.\n\nFather Eugenio Barsanti, an Italian engineer, together with Felice Matteucci of Florence invented the first real internal combustion engine in 1853. Their patent request was granted in London on June 12, 1854, and published in London's Morning Journal under the title \"Specification of Eugene Barsanti and Felix Matteucci, Obtaining Motive Power by the Explosion of Gasses\". In 1860, Belgian Jean Joseph Etienne Lenoir produced a gas-fired internal combustion engine. In 1864, Nikolaus Otto patented the first atmospheric gas engine. In 1872, American George Brayton invented the first commercial liquid-fueled internal combustion engine. In 1876, Nikolaus Otto, working with Gottlieb Daimler and Wilhelm Maybach, patented the compressed charge, four-cycle engine. In 1879, Karl Benz patented a reliable two-stroke gas engine. In 1892, Rudolf Diesel developed the first compressed charge, compression ignition engine. In 1926, Robert Goddard launched the first liquid-fueled rocket. In 1939, the Heinkel He 178 became the world's first jet aircraft. In 1954 German engineer Felix Wankel patented a \"pistonless\" engine using an eccentric rotary design.\n\n\n \n\n\n\nEarly internal combustion engines were started by hand cranking. Various types of starter motor were later developed. These included:\n\n\nElectric starters are now almost universal for small and medium-sized engines, while pneumatic starters are used for large engines.\n\nThe first piston engines did not have compression, but ran on an air-fuel mixture sucked or blown in during the first part of the intake stroke. The most significant distinction between modern internal combustion engines and the early designs is the use of compression of the fuel charge prior to combustion.\n\nThe problem of ignition of fuel was handled in early engines with an open flame and a sliding gate. To obtain a faster engine speed Daimler adopted a Hot Tube ignition which allowed 600 rpm immediately in his 1883 horizontal cylinder engine and very soon after over 900 rpm. Most of the engines of that time could not exceed 200 rpm due to their ignition and induction systems.\n\nThe first practical engine, Lenoir's, ran on illuminating gas (coal gas). It wasn't until 1883, that Daimler created an engine that ran on liquid petroleum, a fuel called Ligroin which has a chemical makeup of Hexane-N. The fuel is also known as petroleum naptha.\n\nOtto's first engines were push engines which produced a push through the entire stroke (like a Diesel). Daimler's engines produced a rapid pulse, more suitable for mobile engine use.\n\n"}
{"id": "13831369", "url": "https://en.wikipedia.org/wiki?curid=13831369", "title": "Hydrolysis constant", "text": "Hydrolysis constant\n\nA hydrolysis constant is an equilibrium constant for a hydrolysis reaction.\n\nFor example, if a metal salt such as AlCl dissolves in an aqueous solution, the metal cation behaves as a Lewis acid and hydrolyzes the water molecules in the solvent.\n\nThe hydrolysis constant for this reaction is as shown:\n\nIn a more generalized form, the hydrolysis constant can be described as:\nwhere A represents any base, and HA represents any acid.\n"}
{"id": "33289870", "url": "https://en.wikipedia.org/wiki?curid=33289870", "title": "Index of agriculture articles", "text": "Index of agriculture articles\n\nThis is an index of agriculture topics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "38261541", "url": "https://en.wikipedia.org/wiki?curid=38261541", "title": "Induced-self antigen", "text": "Induced-self antigen\n\nInduced-self antigen is a marker of abnormal self, which can be recognized upon infected (in particular, virus-infected) and transformed cells. Therefore, the recognition of \"induced self\" is an important strategy for surveillance of infection or tumor transformation - it results in elimination of the affected cells by activated NK cells or other immunological mechanisms. Similarly γδ T cells can recognize induced-self antigens expressed on cells under stress conditions.\n\nProbably the most studied receptor involved in recognition of induced-self antigens is NKG2D. It is an activating receptor which is expressed on NK cells and subsets of T and NKT cells. NKG2D can bind proteins at the surface of most cells that are not normally expressed, but that are expressed during a stress response of the cells (e.g. induction of the DNA damage pathway). Moreover, other recognition targets exist, for example ligands induced on human macrophages by TLR stimulation. Ligands that bind to NKG2D receptor can be divided into two families of MHC class I-related proteins: MICs (MICA, MICB) and ULBPs (ULBP1, ULBP2, ULBP3, ULBP4, RAET1G, RAET1L).\n\nOther receptors able to bind induced-self antigens are NKG2C, NKG2E, NKG2F (CD94) or some NCRs (e.g. NKp 46 ).\n\nPractical use of the knowledge of induced-self antigens is in targeting tumors for immune response. As tumors are very often capable of escaping the immune system by many ways, upregulation of specific ligands on the tumor cells could mount effective immune mechanisms able to eliminate these cells. For example, upregulation of NKG2D ligands can stimulate the NK cells triggering cell-mediated cytotoxicity.\n"}
{"id": "15459", "url": "https://en.wikipedia.org/wiki?curid=15459", "title": "International Statistical Classification of Diseases and Related Health Problems", "text": "International Statistical Classification of Diseases and Related Health Problems\n\nThe International Classification of Diseases (ICD) is the international \"standard diagnostic tool for epidemiology, health management and clinical purposes.\" Its full official name is International Statistical Classification of Diseases and Related Health Problems.\n\nThe ICD is maintained by the World Health Organization (WHO), which is the directing and coordinating authority for health within the United Nations System. The ICD is originally designed as a health care classification system, providing a system of diagnostic codes for classifying diseases, including nuanced classifications of a wide variety of signs, symptoms, abnormal findings, complaints, social circumstances, and external causes of injury or disease. This system is designed to map health conditions to corresponding generic categories together with specific variations, assigning for these a designated code, up to six characters long. Thus, major categories are designed to include a set of similar diseases. ICD-11 is a major step forward, because it has the necessary terminological and ontological elements for seamless use in digital health.\n\nThe ICD is published by the WHO and used worldwide for morbidity and mortality statistics, reimbursement systems, and automated decision support in health care. This system is designed to promote international comparability in the collection, processing, classification, and presentation of these statistics. Like the analogous \"Diagnostic and Statistical Manual of Mental Disorders\" (which is limited to psychiatric disorders and almost exclusive to the United States), the ICD is a major project to statistically classify all health disorders, and provide diagnostic assistance. The ICD is a core statistically based classificatory diagnostic system for health care related issues of the WHO Family of International Classifications (WHO-FIC).\n\nThe ICD is revised periodically and is currently in its 10th revision. ICD-10, as it is therefore known, is from 1992 and the WHO publishes annual minor updates and triennial major updates. The final draft of the ICD-11 system is expected to be submitted to WHO's World Health Assembly (WHA) for official endorsement in 2019. The version for preparation of approval at the WHA was released on 18 June 2018.\n\nThe ICD is part of a \"family\" of international classifications (WHOFIC) that complement each other, including also the International Classification of Functioning, Disability and Health (ICF) which focuses on the domains of functioning (disability) associated with health conditions, from both medical and social perspectives, and the International Classification of Health Interventions (ICHI) that classifies the whole range of medical, nursing, functioning and public health interventions.\n\nIn 1860, during the international statistical congress held in London, Florence Nightingale made a proposal that was to result in the development of the first model of systemic collection of hospital data. In 1893, a French physician, Jacques Bertillon, introduced the \"Bertillon Classification of Causes of Death\" at a congress of the International Statistical Institute in Chicago.\n\nA number of countries adopted Bertillon's system, which was based on the principle of distinguishing between general diseases and those localized to a particular organ or anatomical site, as used by the City of Paris for classifying deaths. Subsequent revisions represented a synthesis of English, German, and Swiss classifications, expanding from the original 44 titles to 161 titles. In 1898, the American Public Health Association (APHA) recommended that the registrars of Canada, Mexico, and the United States also adopt it. The APHA also recommended revising the system every 10 years to ensure the system remained current with medical practice advances. As a result, the first international conference to revise the International Classification of Causes of Death took place in 1900, with revisions occurring every ten years thereafter. At that time, the classification system was contained in one book, which included an Alphabetic Index as well as a Tabular List. The book was small compared with current coding texts.\n\nThe revisions that followed contained minor changes, until the sixth revision of the classification system. With the sixth revision, the classification system expanded to two volumes. The sixth revision included morbidity and mortality conditions, and its title was modified to reflect the changes: International Statistical Classification of Diseases, Injuries and Causes of Death (ICD). Prior to the sixth revision, responsibility for ICD revisions fell to the Mixed Commission, a group composed of representatives from the International Statistical Institute and the Health Organization of the League of Nations. In 1948, the WHO assumed responsibility for preparing and publishing the revisions to the ICD every ten years. WHO sponsored the seventh and eighth revisions in 1957 and 1968, respectively. It later became clear that the established ten year interval between revisions was too short.\n\nThe ICD is currently the most widely used statistical classification system for diseases in the world. In addition, some countries—including Australia, Canada, and the United States—have developed their own adaptations of ICD, with more procedure codes for classification of operative or diagnostic procedures.\n\nThe ICD-6, published in 1949, was the first to be shaped to become suitable for morbidity reporting. Accordingly, the name changed from International List of Causes of Death to International Statistical Classification of Diseases. The combined code section for injuries and their associated accidents was split into two, a chapter for injuries, and a chapter for their external causes. With use for morbidity there was a need for coding mental conditions, and for the first time a section on mental disorders was added .\n\nThe international Conference for the Seventh Revision of the International Classification of Diseases was held in Paris under the auspices of WHO in February 1955. In accordance with a recommendation of the WHO Expert Committee on Health Statistics, this revision was limited to essential changes and amendments of errors and inconsistencies.\n\nThe 8th Revision Conference convened by WHO met in Geneva, from 6 to 12 July 1965. This revision was more radical than the Seventh but left unchanged the basic structure of the Classification and the general philosophy of classifying diseases, whenever possible, according to their etiology rather than a particular manifestation.\nDuring the years that the Seventh and Eighth Revisions of the ICD were in force, the use of the ICD for indexing hospital medical records increased rapidly and some countries prepared national adaptations which provided the additional detail needed for this application of the ICD. \nIn the USA, a group of consultants was asked to study the 8th revision of ICD (ICD-8a) for its applicability to various users in the United States. This group recommended that further detail be provided for coding hospital and morbidity data. The American Hospital Association's \"Advisory Committee to the Central Office on ICDA\" developed the needed adaptation proposals, resulting in the publication of the International Classification of Diseases, Adapted (ICDA). In 1968, the United States Public Health Service published the International Classification of Diseases, Adapted, 8th Revision for use in the United States (ICDA-8a). Beginning in 1968, ICDA-8a served as the basis for coding diagnostic data for both official morbidity [and mortality] statistics in the United States.\n\nThe International Conference for the Ninth Revision of the International Statistical Classification of Diseases, Injuries, and Causes of Death, convened by WHO, met in Geneva from 30 September to 6 October 1975. In the discussions leading up to the conference, it had originally been intended that there should be little change other than updating of the classification. This was mainly because of the expense of adapting data processing systems each time the classification was revised.\n\nThere had been an enormous growth of interest in the ICD and ways had to be found of responding to this, partly by modifying the classification itself and partly by introducing special coding provisions. A number of representations were made by specialist bodies which had become interested in using the ICD for their own statistics. Some subject areas in the classification were regarded as inappropriately arranged and there was considerable pressure for more detail and for adaptation of the classification to make it more relevant for the evaluation of medical care, by classifying conditions to the chapters concerned with the part of the body affected rather than to those dealing with the underlying generalized disease.\n\nAt the other end of the scale, there were representations from countries and areas where a detailed and sophisticated classification was irrelevant, but which nevertheless needed a classification based on the ICD in order to assess their progress in health care and in the control of disease. A field test with a bi-axial classification approach—one axis (criterion) for anatomy, with another for etiology—showed the impracticability of such approach for routine use.\n\nThe final proposals presented to and accepted by the Conference in 1978 retained the basic structure of the ICD, although with much additional detail at the level of the four digit subcategories, and some optional five digit subdivisions. For the benefit of users not requiring such detail, care was taken to ensure that the categories at the three digit level were appropriate.\n\nFor the benefit of users wishing to produce statistics and indexes oriented towards medical care, the 9th Revision included an optional alternative method of classifying diagnostic statements, including information about both an underlying general disease and a manifestation in a particular organ or site. This system became known as the dagger and asterisk system and is retained in the Tenth Revision. A number of other technical innovations were included in the Ninth Revision, aimed at increasing its flexibility for use in a variety of situations.\n\nIt was eventually replaced by ICD-10, the version currently in use by the WHO and most countries. Given the widespread expansion in the tenth revision, it is not possible to convert ICD-9 data sets directly into ICD-10 data sets, although some tools are available to help guide users.\nPublication of ICD-9 without IP restrictions in a world with evolving electronic data systems led to a range of products based on ICD-9, such as MeDRA or the Read directory.\n\nICPM\n\nWhen ICD-9 was published by the World Health Organization (WHO), the International Classification of Procedures in Medicine (ICPM) was also developed (1975) and published (1978). The ICPM surgical procedures fascicle was originally created by the United States, based on its adaptations of ICD (called ICDA), which had contained a procedure classification since 1962. ICPM is published separately from the ICD disease classification as a series of supplementary documents called fascicles (bundles or groups of items). Each fascicle contains a classification of modes of laboratory, radiology, surgery, therapy, and other diagnostic procedures. Many countries have adapted and translated the ICPM in parts or as a whole and are using it with amendments since then.\n\nICD-9-CM\n\n\"International Classification of Diseases, Clinical Modification\" (ICD-9-CM) is an adaption created by the U.S. National Center for Health Statistics (NCHS) and used in assigning diagnostic and procedure codes associated with inpatient, outpatient, and physician office utilization in the United States. The ICD-9-CM is based on the ICD-9 but provides for additional morbidity detail. It is updated annually on October 1.\n\nIt consists of two or three volumes: \n\nThe NCHS and the Centers for Medicare and Medicaid Services are the U.S. governmental agencies responsible for overseeing all changes and modifications to the ICD-9-CM.\n\nWork on ICD-10 began in 1983, and the new revision was endorsed by the Forty-third World Health Assembly in May 1990. The latest version came into use in WHO Member States starting in 1994. The classification system allows more than 155,000 different codes and permits tracking of many new diagnoses and procedures, a significant expansion on the 17,000 codes available in ICD-9.\nAdoption was relatively swift in most of the world. Several materials are made available online by WHO to facilitate its use, including a manual, training guidelines, a browser, and files for download. Some countries have adapted the international standard, such as the \"ICD-10-AM\" published in Australia in 1998 (also used in New Zealand), and the \"ICD-10-CA\" introduced in Canada in 2000.\n\nICD-10-CM\n\nAdoption of ICD-10-CM was slow in the United States. Since 1979, the US had required ICD-9-CM codes for Medicare and Medicaid claims, and most of the rest of the American medical industry followed suit. On 1 January 1999 the ICD-10 (without clinical extensions) was adopted for reporting mortality, but ICD-9-CM was still used for morbidity. Meanwhile, NCHS received permission from the WHO to create a clinical modification of the ICD-10, and has production of all these systems:\n\n\nOn 21 August 2008, the US Department of Health and Human Services (HHS) proposed new code sets to be used for reporting diagnoses and procedures on health care transactions. Under the proposal, the ICD-9-CM code sets would be replaced with the ICD-10-CM code sets, effective 1 October 2013. On 17 April 2012 the Department of Health and Human Services (HHS) published a proposed rule that would delay, from 1 October 2013 to 1 October 2014, the compliance date for the ICD-10-CM and PCS. Once again, Congress delayed implementation date to 1 October 2015, after it was inserted into \"Doc Fix\" Bill without debate over objections of many.\n\nRevisions to ICD-10-CM Include:\nICD-10-CA\n\nICD-10-CA is a clinical modification of ICD-10 developed by the Canadian Institute for Health Information for morbidity classification in Canada. ICD-10-CA applies beyond acute hospital care, and includes conditions and situations that are not diseases but represent risk factors to health, such as occupational and environmental factors, lifestyle and psycho-social circumstances.\n\nThe World Health Organization has revised the International Classification of Diseases (ICD) towards the ICD-11. Its development has taken place on an internet-based workspace that continues to be used as the maintenance platform for discussions, and proposals for updates of ICD. Anybody can submit an evidence based proposal. The proposals are processed in an open transparent way with reviews for scientific evidence, and usability and utility in the various uses of ICD.\nIt is envisaged, that there will be no need for national modifications of ICD-11, due to its richness and flexibility in the reportable detail \n\nThe final draft of the ICD-11 system is expected to be submitted to WHO's World Health Assembly (WHA) for official endorsement in 2019. The version for implementation (preparation of approval at the WHA) was released on 18 June 2018.\n\nICD-11 comes with an implementation package that includes transition tables from and to ICD-10, a translation tool, a coding tool, web-services, a manual, training material, and more. All tools are accessible after self-registration from the maintenance platform.\n\nThe official release is accessed via icd.who.int\n\n\nAn external review of the ICD-11 Revision has been completed. The report notes the progress in the ICD Revision, and makes clear recommendations about forward progress in the revision.\n\n\nICD-11 invokes a more sophisticated architecture than historical versions, consistent with its generation as a digital resource. The core content of the system, called the Foundation Component, is a semantic network of words and terms, where any given term can have more than one parent. To address the requirement that statistical classifications exhibit mutual exclusiveness (so events are not counted more than once) and exhaustiveness (so there is a place to tally all events), ICD11 supports the serialization of the Foundation Component into an arbitrary number of linearizations, optimized for use cases. The main linearization, presently called the Joint Linearization for Morbidity and Mortality Statistics, is the tabular format with which most traditional users will become familiar. However, other linearizations, for primary care, multiple sub-specialty derivatives, or applications such as clinical decision support are possible. Finally, preliminary work in partnership with the IHTSDO is underway to ensure that the ICD-11 Foundation Component is semantically coherent through development of the Common Ontology, a subset of SNOMED CT which will anchor the Foundation Component to terms defined through description logic.\n\nIn the United States, the U.S. Public Health Service published \"The International Classification of Diseases, Adapted for Indexing of Hospital Records and Operation Classification (ICDA),\" completed in 1962 and expanding the ICD-7 in a number of areas to more completely meet the indexing needs of hospitals. The U.S. Public Health Service later published the \"Eighth Revision, International Classification of Diseases, Adapted for Use in the United States,\" commonly referred to as ICDA-8, for official national morbidity and mortality statistics. This was followed by the \"ICD, 9th Revision, Clinical Modification\", known as ICD-9-CM, published by the U.S. Department of Health and Human Services and used by hospitals and other healthcare facilities to better describe the clinical picture of the patient. The diagnosis component of ICD-9-CM is completely consistent with ICD-9 codes, and remains the data standard for reporting morbidity. National adaptations of the ICD-10 progressed to incorporate both clinical code (ICD-10-CM) and procedure code (ICD-10-PCS) with the revisions completed in 2003. In 2009, the U.S. Centers for Medicare and Medicaid Services announced that it would begin using ICD-10 on April 1, 2010, with full compliance by all involved parties by 2013.\n\nThe years for which causes of death in the United States have been classified by each revision as follows:\n\nCause of death on United States death certificates, statistically compiled by the Centers for Disease Control and Prevention (CDC), are coded in the ICD, which does not include codes for human and system factors commonly called medical errors.\n\nThe ICD includes a section classifying mental and behavioral disorders (). This has developed alongside the Diagnostic and Statistical Manual of Mental Disorders (DSM) of the American Psychiatric Association and the two manuals seek to use the same codes. The WHO is revising their classifications in these sections as part the development of the ICD-11 (scheduled for 2018), and an \"International Advisory Group\" has been established to guide this. Section F66 of the ICD-10 deals with classifications of psychological and behavioural disorders that are associated with sexual development and orientation. It explicitly states that \"sexual orientation by itself is not to be considered a disorder,\" in line with the DSM and other classifications that recognise homosexuality as a normal variation in human sexuality. The Working Group has reported that there is \"no evidence that [these classifications] are clinically useful\" and recommended that section F66 be deleted for the ICD-11.\n\nAn international survey of psychiatrists in 66 countries comparing use of the ICD-10 and DSM-IV found that the former was more often used for clinical diagnosis while the latter was more valued for research. The ICD is actually the official system for the US, although many mental health professionals do not realize this due to the dominance of the DSM. A psychologist has stated: \"Serious problems with the clinical utility of both the ICD and the DSM are widely acknowledged.\"\n\n\nNote: Since adoption of ICD-10 CM in the USA, several online tools have been mushrooming. They all refer to that particular modification and thus are not linked here.\n"}
{"id": "56922028", "url": "https://en.wikipedia.org/wiki?curid=56922028", "title": "Journal of Public Administration Research and Theory", "text": "Journal of Public Administration Research and Theory\n\nThe Journal of Public Administration Research and Theory is a quarterly peer-reviewed scientific journal covering public administration and public policy studies. It was established in 1991 and is published by Oxford University Press on behalf of the Public Management Research Association, of which it is the official journal. The editor-in-chief is Bradley Wright (University of Georgia School of Public and International Affairs). According to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 3.907, ranking it 2nd out of 47 journals in the category \"Public Administration\".\n\n"}
{"id": "26943129", "url": "https://en.wikipedia.org/wiki?curid=26943129", "title": "Kaluza–Klein black hole", "text": "Kaluza–Klein black hole\n\nA Kaluza–Klein black hole is a black brane (generalisation of a black hole) in asymptotically flat Kaluza–Klein space, i.e. higher-dimensional spacetime with compact dimensions. They may also be called KK black holes.\n"}
{"id": "22690948", "url": "https://en.wikipedia.org/wiki?curid=22690948", "title": "Kirchhoff integral theorem", "text": "Kirchhoff integral theorem\n\nKirchhoff's integral theorem (sometimes referred to as the Fresnel–Kirchhoff integral theorem) uses Green's identities to derive the solution to the homogeneous wave equation at an arbitrary point P in terms of the values of the solution of the wave equation and its first-order derivative at all points on an arbitrary surface that encloses P.\n\nThe integral has the following form for a monochromatic wave:\n\nwhere the integration is performed over an arbitrary closed surface \"S\" (enclosing r), \"s\" is the distance from the surface element to the point r, and ∂/∂n denotes differentiation along the surface normal (a normal derivative). Note that in this equation the normal points inside the enclosed volume; if the more usual outer-pointing normal is used, the integral has the opposite sign.\n\nA more general form can be derived for non-monochromatic waves. The complex amplitude of the wave can be represented by a Fourier integral of the form\n\nwhere, by Fourier inversion, we have\n\nThe integral theorem (above) is applied to each Fourier component formula_4, and the following expression is obtained:\n\nwhere the square brackets on \"V\" terms denote retarded values, i.e. the values at time \"t\" − \"s\"/\"c\".\n\nKirchhoff showed that the above equation can be approximated in many cases to a simpler form, known as the Kirchhoff, or Fresnel–Kirchhoff diffraction formula, which is equivalent to the Huygens–Fresnel equation, but provides a formula for the inclination factor, which is not defined in the latter. The diffraction integral can be applied to a wide range of problems in optics.\n\n\n"}
{"id": "46293573", "url": "https://en.wikipedia.org/wiki?curid=46293573", "title": "Laser Inertial Fusion Energy", "text": "Laser Inertial Fusion Energy\n\nLIFE, short for Laser Inertial Fusion Energy, was a fusion energy effort run at Lawrence Livermore National Laboratory between 2008 and 2013. LIFE aimed to develop the technologies necessary to convert the laser-driven inertial confinement fusion concept being developed in the National Ignition Facility (NIF) into a practical commercial power plant, a concept known generally as inertial fusion energy (IFE). LIFE used the same basic concepts as NIF, but aimed to lower costs using mass-produced fuel elements, simplified maintenance, and diode lasers with higher electrical efficiency.\n\nTwo designs were considered, operated as either a pure fusion or hybrid fusion-fission system. In the former, the energy generated by the fusion reactions is used directly. In the later, the neutrons given off by the fusion reactions are used to cause fission reactions in a surrounding blanket of uranium or other nuclear fuel, and those fission events are responsible for most of the energy release. In both cases, conventional steam turbine systems are used to extract the heat and produce electricity.\n\nConstruction on NIF completed in 2009 and it began a lengthy series of run-up tests to bring it to full power. Through 2011 and into 2012, NIF ran the \"national ignition campaign\" to reach the point at which the fusion reaction becomes self-sustaining, a key goal that is a basic requirement of any practical IFE system. NIF failed in this goal, with fusion performance that was well below ignition levels and differing considerably from predictions. With the problem of ignition unsolved, the LIFE project was cancelled in 2013.\n\nLawrence Livermore National Laboratory (LLNL) has been a leader in laser-driven inertial confinement fusion (ICF) since the initial concept was developed by LLNL employee John Nuckols in the late 1950s. The basic idea was to use a \"driver\" to compress a small pellet known as the \"target\" that contains the fusion fuel, a mix of deuterium (D) and tritium (T). If the compression reaches high enough values, fusion reactions begin to take place, releasing alpha particles and neutrons. The alphas may impact atoms in the surrounding fuel, heating them to the point where they undergo fusion as well. If the rate of alpha heating is higher than heat losses to the environment, the result is a self-sustaining chain reaction known as ignition.\n\nComparing the driver energy input to the fusion energy output produces a number known as fusion energy gain factor, labelled \"Q\". A \"Q\" value of at least 1 is required for the system to produce net energy. Since some energy is needed to run the reactor, in order for there to be net electrical output, \"Q\" has to be at least 3. For commercial operation, \"Q\" values much higher than this are needed. For ICF, \"Q\"s on the order of 25 to 50 are needed to recoup both the electrical generation losses and the large amount of power used to power the driver. In the fall of 1960, theoretical work carried out at LLNL suggested that gains of the required order would be possible with drivers on the order of 1 MJ.\n\nAt the time, a number of different drivers were considered, but the introduction of the laser later that year provided the first obvious solution with the right combination of features. The desired energies were well beyond the state of the art in laser design, so LLNL began a development program in the mid-1960s to reach these levels. Each increase in energy led to new and unexpected optical phenomena that had to be overcome, but these were largely solved by the mid-1970s. Working in parallel with the laser teams, physicists studying the expected reaction using computer simulations adapted from thermonuclear bomb work developed a program known as LASNEX that suggested \"Q\" of 1 could be produced at much lower energy levels, in the kilojoule range, levels that the laser team were now able to deliver.\n\nFrom the late-1970s, LLNL developed a series of machines to reach the conditions being predicted by LASNEX and other simulations. With each iteration, the experimental results demonstrated that the simulations were incorrect. The first machine, the Shiva laser of the late 1970s, produced compression on the order of 50 to 100 times, but did not produce fusion reactions anywhere near the expected levels. The problem was traced to the issue of the infrared laser light heating electrons and mixing them in the fuel, and it was suggested that using ultraviolet light would solve the problem. This was addressed on the Nova laser of the 1980s, which was designed with the specific intent of producing ignition. Nova did produce large quantities of fusion, with \"shots\" producing as much as neutrons, but failed to reach ignition. This was traced to the growth of Rayleigh–Taylor instabilities, which greatly increased the required driver power.\n\nUltimately all of these problems were considered to be well understood, and a much larger design emerged, NIF. NIF was designed to provide about twice the required driver energy, allowing some margin of error. NIF's design was finalized in 1994, with construction to be completed by 2002. Construction began in 1997 but took over a decade to complete, with major construction being declared complete in 2009.\n\nThroughout the development of the ICF concept at LLNL and elsewhere, several small efforts had been made to consider the design of a commercial power plant based on the ICF concept. Examples include SOLASE-H and HYLIFE-II. As NIF was reaching completion in 2008, with the various concerns considered solved, LLNL began a more serious IFE development effort, LIFE.\n\nWhen the LIFE project was first proposed, it focused on the nuclear fusion–fission hybrid concept, which uses the fast neutrons from the fusion reactions to induce fission in fertile nuclear materials. The hybrid concept was designed to generate power from both fertile and fissile nuclear fuel and to burn nuclear waste. The fuel blanket was designed to use TRISO-based fuel cooled by a molten salt made from a mixture of lithium fluoride (LiF) and beryllium fluoride (BeF).\n\nConventional fission power plants rely on the chain reaction caused when fission events release thermal neutrons that cause further fission events. Each fission event in U-235 releases two or three neutrons with about 2 MeV of kinetic energy. By careful arrangement and the use of various absorber materials, designers can balance the system so one of those neutrons causes another fission event while the other one or two are lost. This balance is known as criticality. Natural uranium is a mix of three isotopes; mainly U-238, with some U-235, and trace amounts of U-234. The neutrons released in the fission of either of the main isotopes will cause fission in U-235, but not in U-238, which requires higher energies around 5 MeV. There is not enough U-235 in natural uranium to reach criticality. Commercial light water nuclear reactors, the most prevalent power reactors in the world, use nuclear fuel containing uranium enriched to 3 to 5% U-235 while the leftover is U-238.\n\nEach fusion event in the D-T fusion reactor gives off an alpha particle and a fast neutron with around 14 MeV of kinetic energy. This is enough energy to cause fission in U-238, and many other transuranic elements as well. This reaction is used in H-bombs to increase the yield of the fusion section by wrapping it in a layer of depleted uranium, which undergoes rapid fission when hit by the neutrons from the fusion bomb inside. The same basic concept can also be used with a fusion reactor like LIFE, using its neutrons to cause fission in a \"blanket\" of fission fuel. Unlike a fission reactor, which burns out its fuel once the U-235 drops below a certain threshold value, these fission–fusion hybrid reactors can continue producing power from the fission fuel as long as the fusion reactor continues to provide neutrons. As the neutrons have high energy, they can potentially cause multiple fission events, leading to the reactor as a whole producing more energy, a concept known as \"energy multiplication\". Even leftover nuclear fuel taken from conventional nuclear reactors will burn in this fashion. This is potentially attractive because this burns off many of the long lived radioisotopes in the process, producing waste that is only mildly radioactive and lacking most long-lived components.\n\nIn most fusion energy designs, fusion neutrons react with a blanket of lithium to breed new tritium for fuel. A major issue with the fission–fusion design is that the neutrons causing fission are no longer available for tritium breeding. While the fission reactions release additional neutrons, these do not have enough energy to complete the breeding reaction with Li-7, which makes up more than 92% of natural lithium. These lower energy neutrons will cause breeding in Li-6, which could be concentrated from the natural lithium ore. However, the Li-6 reaction only produces one tritium per neutron captured, and more than one T per neutron is needed to make up for natural decay and other losses. Using Li-6, neutrons from the fission would make up for the losses, but only at the cost of removing them from causing other fission reactions, lowering the reactor power output. The designer has to choose which is more important; burning up the fuel through fusion neutrons, or providing power through self-induced fission events.\n\nThe economics of fission–fusion designs have always been questionable. The same basic effect can be created by replacing the central fusion reactor with a specially designed fission reactor, and using the surplus neutrons from the fission to breed fuel in the blanket. These fast breeder reactors have proven uneconomical in practice, and the greater expense of the fusion systems in the fission–fusion hybrid has always suggested they would be uneconomical unless built in very large units.\n\nThe LIFE concept stopped working along fusion-fission lines around 2009. Following consultations with their partners in the utility industry, the project was redirected toward a pure fusion design with a net electrical output around 1 gigawatt.\n\nInertial confinement fusion is one of two major lines of fusion power development, the other being magnetic confinement fusion (MCF), notably the tokamak concept which is being built in a major experimental system known as ITER. Magnetic confinement is widely considered to be the superior approach, and has seen significantly greater development activity over the decades. However, there are serious concerns that the MCF approach of ITER cannot ever become economically practical.\n\nOne of the cost concerns for MCF designs like ITER is that the reactor materials are subject to the intense neutron flux created by the fusion reactions. When high-energy neutrons impact materials they displace the atoms in the structure leading to a problem known as neutron embrittlement that degrades the structural integrity of the material. This is a problem for fission reactors as well, but the neutron flux and energy in a tokamak is greater than most fission designs. In most MFE designs, the reactor is constructed in layers, with a toroidal inner vacuum chamber, or \"first wall\", then the lithium blanket, and finally the superconducting magnets that produce the field that confines the plasma. Neutrons stopping in the blanket are desirable, but those that stop in the first wall or magnets degrade them. Disassembling a toroidal stack of elements would be a time-consuming process that would lead to poor capacity factor, which has a significant impact on the economics of the system. Reducing this effect requires the use of exotic materials which have not yet been developed.\n\nAs a natural side-effect of the size of the fuel elements and their resulting explosions, ICF designs use a very large reaction chamber many meters across. This lowers the neutron flux on any particular part of the chamber wall through the inverse square law. Additionally, there are no magnets or other complex systems near or inside the reactor, and the laser is isolated on the far side of long optical paths. The far side of the chamber is empty, allowing the blanket to be placed there and easily maintained. Although the reaction chamber walls and final optics would eventually embrittle and require replacement, the chamber is essentially a large steel ball of relatively simple multi-piece construction that could be replaced without too much effort. The reaction chamber is, on the whole, dramatically simpler than those in magnetic fusion concepts, and the LIFE designs proposed building several and quickly moving them in and out of production.\n\nNIF's laser uses a system of large flashtubes (like those in a photography flashlamp) to optically \"pump\" a large number of glass plates. Once the plates are flashed and have settled into a population inversion, a small signal from a separate laser is fed into the optical lines, stimulating the emission in the plates. The plates then dump their stored energy into the growing beam, amplifying it billions of times.\n\nThe process is extremely inefficient in energy terms; NIF feeds the flashtubes over 400 MJ of energy which produces 1.8 MJ of ultraviolet (UV) light. Due to limitations of the target chamber, NIF is only able to handle fusion outputs up to about 50 MJ, although shots would generally be about half of that. Accounting for losses in generation, perhaps 20 MJ of electrical energy might be extracted at the maximum, accounting for less than of the input energy.\n\nAnother problem with the NIF lasers is that the flashtubes create a significant amount of heat, which warms the laser glass enough to cause it to deform. This requires a lengthy cooling-off period between shots, on the order of 12 hours. In practice, NIF manages a shot rate of less than one shot per day. To be useful as a power plant, about a dozen shots would have to take place every second, well beyond the capabilities of the NIF lasers.\n\nWhen originally conceived by Nuckols, laser-driven inertial fusion confinement was expected to require lasers of a few hundred kilojoules and use fuel droplets created by a perfume mister arrangement. LLNLs research since that time has demonstrated that such an arrangement cannot work, and requires machined assemblies for each shot. To be economically useful, an IFE machine would need to use fuel assemblies that cost pennies. Although LLNL does not release prices for their own targets, the similar system at the Laboratory for Laser Energetics at the University of Rochester makes targets for about $1 million each. It is suggested that NIF's targets cost more than $10,000.\n\nLLNL had begun exploring different solutions to the laser problem while the system was first being described. In 1996 they built a small testbed system known as the Mercury laser that replaced the flashtubes with laser diodes.\n\nOne advantage of this design was that the diodes created light around the same frequency as the laser glass' output, as compared to the white light flashtubes where most of the energy in the flash was wasted as it was not near the active frequency of the laser glass. This change increased the energy efficiency to about 10%, a dramatic improvement.\n\nFor any given amount of light energy created, the diode lasers give off about as much heat as a flashtube. Less heat, combined with active cooling in the form of helium blown between the diodes and the laser glass layers, eliminated the warming of the glass and allows Mercury to run continually. In 2008, Mercury was able to fire 10 times a second at 50 joules per shot for hours at a time.\n\nSeveral other projects running in parallel with Mercury explored various cooling methods and concepts allowing many laser diodes to be packed into a very small space. These eventually produced a system with 100 kW of laser energy from a box about long, known as a diode array. In a LIFE design, these arrays would replace the less dense diode packaging of the Mercury design.\n\nLIFE was essentially a combination of the Mercury concepts and new physical arrangements to greatly reduce the volume of the NIF while making it much easier to build and maintain. Whereas an NIF beamline for one of its 192 lasers is over long, LIFE was based on a design about long that contained everything from the power supplies to frequency conversion optics. Each module was completely independent, unlike NIF which is fed from a central signal from the Master Oscillator, allowing the units to be individually removed and replaced while the system as a whole continued operation.\n\nEach driver cell in the LIFE baseline design contained two of the high-density diode arrays arranged on either side of a large slab of laser glass. The arrays were provided cooling via hook-up pipes at either end of the module. The initial laser pulse was provided by a preamplifier module similar to the one from the NIF, the output of which was switched into the main beamline via a mirror and Pockel's cell optical switch. To maximize the energy deposited into the beam from the laser glass, optical switches were used to send the beam to mirrors to reflect the light through the glass four times, in a fashion similar to NIF. Finally, focussing and optical cleanup was provided by optics on either side of the glass, before the beam exited the system through a frequency converter at one end.\n\nThe small size and independence of the laser modules allowed the huge NIF building to be dispensed with. Instead, the modules were arranged in groups surrounding the target chamber in a compact arrangement. In baseline designs, the modules were stacked in 2-wide by 8-high groups in two rings above and below the target chamber, shining their light through small holes drilled into the chamber to protect them from the neutron flux coming back out.\n\nThe ultimate goal was to produce a system that could be shipped in a conventional semi-trailer truck to the power plant, providing laser energy with 18% end-to-end efficiency, 15 times that of the NIF system. This reduces the required fusion gains into the 25 to 50 area, within the predicted values for NIF. The consensus was that this \"beam-in-a-box\" system could be built for 3 cents per Watt of laser output, and that would reduce to 0.7 cents/W in sustained production. This would mean that a complete LIFE plant would require about $600 million worth of diodes alone, significant, but within the realm of economic possibility.\n\nTargets for NIF are extremely expensive. Each one consists of a small open ended metal cylinder with transparent double-pane windows sealing each end. In order to efficiently convert the driver laser's light to the x-rays that drive the compression, the cylinder has to be coated in gold or other heavy metals. Inside, suspended on fine plastic wires, is a hollow plastic sphere containing the fuel. In order to provide symmetrical implosion, the metal cylinder and plastic sphere have extremely high machining tolerances. The fuel, normally a gas at room temperature, is deposited inside the sphere and then cryogenically frozen until it sticks to the inside of the sphere. It is then smoothed by slowly warming it with an infrared laser to form a 100 µm smooth layer on the inside of the pellet. Each target costs tens of thousands of dollars.\n\nTo address this concern, a considerable amount of LIFE's effort was put into the development of simplified target designs and automated construction that would lower their cost. Working with General Atomics, the LIFE team developed a concept using on-site fuel factories that would mass-produce pellets at a rate of about a million a day. It was expected that this would reduce their price to about 25 cents per target, although other references suggest the target price was closer to 50 cents, and LLNL's own estimates range from 20 to 30 cents.\n\nOne less obvious advantage to the LIFE concept is that the amount of tritium required to start the system up is greatly reduced over MFE concepts. In MFE, a relatively large amount of fuel is prepared and put into the reactor, requiring much of the world's entire civilian tritium supply just for startup. LIFE, by virtue of the tiny amount of fuel in any one pellet, can begin operations with much less tritium, on the order of .\n\nThe early fusion-fission designs were not well developed and only schematic outlines of the concept were shown. These systems looked like a scaled down version of NIF, with beamlines about long on either side of a target chamber and power generation area. The laser produced 1.4 MJ of UV light 13 times a second. The fusion took place in a target chamber that was surrounded by of unenriched fission fuel, or alternately about of Pu or highly enriched uranium from weapons. The fusion system was expected to produce \"Q\" on the order of 25 to 30, resulting in 350 to 500 MW of fusion energy. The fission processes triggered by the fusion would add an additional energy gain of 4 to 10 times, resulting in a total thermal output between 2000 and 5000 MW. Using high efficiency thermal-to-electric conversion systems like Rankine cycle designs in combination with demonstrated supercritical steam generators would allow about half of the thermal output to be turned into electricity.\n\nBy 2012, the baseline design of the pure fusion concept, known as the Market Entry Plant (MEP), had stabilized. This was a self-contained design with the entire fusion section packaged into a cylindrical concrete building not unlike a fission reactor confinement building, although larger at diameter. The central building was flanked by smaller rectangular buildings on either side, one containing the turbines and power handling systems, the other the tritium plant. A third building, either attached to the plant or behind it depending on the diagram, was used for maintenance.\n\nInside the central fusion building, the beam-in-a-box lasers were arranged in two rings, one above and one below the target chamber. A total of 384 lasers would provide 2.2 MJ of UV light at a 0.351 micrometer wavelength, producing a \"Q\" of 21. A light gas gun was used to fire 15 targets a second into the target chamber. With each shot, the temperature of the target chamber's inner wall is raised from to .\n\nThe target chamber is a two-wall structure filled with liquid lithium or a lithium alloy between the walls. The lithium captures neutrons from the reactions to breed tritium, and also acts as the primary coolant loop. The chamber is filled with xenon gas that would slow the ions from the reaction as well as protect the inner wall, or \"first wall\", from the massive x-ray flux. Because the chamber is not highly pressurized, like a fission core, it does not have to be built as a single sphere. Instead, the LIFE chamber is built from eight identical sections that include built-in connections to the cooling loop. They are shipped to the plant and bolted together on two supports, and then surrounded by a tube-based space frame.\n\nTo deal with embrittlement, the entire target chamber was designed to be easily rolled out of the center of the building on rails to the maintenance building where it could be rebuilt. The chamber was expected to last four years, and be replaced in one month. The optical system is decoupled from the chamber, which isolates it from vibrations during operation and means that the beamlines themselves do not have to be realigned after chamber replacement.\n\nThe plant had a peak generation capability, or nameplate capacity, of about 400 MWe, with design features to allow expansion to as much as 1000 MWe.\n\nThe levelized cost of electricity (LCoE) can be calculated by dividing the total cost to build and operate a power-generating system over its lifetime by the total amount of electricity shipped to the grid during that period. The amount of money is essentially a combination of the capital expense (CAPEX) of the plant and the interest payments on that CAPEX, and the discounted cost of the fuel, the maintenance needed to keep it running and its dismantling, the discounted operational expenses, or OPEX. The amount of power is normally calculated by considering the peak power the plant could produce, and then adjusting that by the capacity factor (CF) to account for downtime due to maintenance or deliberate throttling. As a quick calculation, one can ignore inflation, opportunity costs and minor operational expenses to develop a figure of merit for the cost of electricity.\n\nMEP was not intended to be a production design, and would be able to export only small amounts of electricity. It would, however, serve as the basis for the first production model, LIFE.2. LIFE.2 would produce 2.2 GW of fusion energy and convert that to 1 GW of electrical at 48% efficiency. Over a year, LIFE would produce 365 days x 24 hours x 0.9 capacity factor x 1,000,000 kW nameplate rating = 8 billion kWh. In order to generate that power, the system will have to burn 365 x 24 x 60 minutes x 60 seconds x 15 pellets per second x 0.9 capacity = 425 million fuel pellets. If the pellets cost the suggested price of 50 cents each, that is over $200 million a year to fuel the plant. The average rate for wholesale electricity in the US is around 5 cents/kWh, so this power has a commercial value of about $212 million, suggesting that LIFE.2 would just barely cover, on average, its own fuel costs.\n\nCAPEX for the plant is estimated to $6.4 billion, so financing the plant over a 20-year period adds another $5 billion assuming the 6.5% unsecured rate. Considering CAPEX and fuel alone, the total cost of the plant is 6.4 + 5 + 4 = $15.4 billion. Dividing the total cost by the energy produced over the same period gives a rough estimate of the cost of electricity for a 20-year lifetime operation: $15.4 billion / 160 billion kWh = 9.6 cents/kWh. A 40-year operation lifetime would lead to a cost of electricity of 4.8 cents/kWh. LLNL calculated the LCoE of LIFE.2 at 9.1 cents using the discounted cash flow methodology described in the 2009 MIT report \"the Future of Nuclear Energy\". Using either value, LIFE.2 would be unable to compete with modern renewable energy souces, which are well below 5 cents/kWh .\n\nLLNL projected that further development after widespread commercial deployment might lead to further technology improvements and cost reductions, and proposed a LIFE.3 design of about $6.3 billion CAPEX and 1.6 GW nameplate for a price per watt of $4.2/W. This leads to a projected LCoE of 5.5 cents/kWh, which is competitive with offshore wind , but unlikely to be so in 2040 when LIFE.3 designs would start construction. LIFE plants would be wholesale sellers, competing against a baseload rate of about 5.3 cents/kWh .\n\nThe steam turbine section of a power plant, the \"turbine hall\", generally costs about $1/W, and the electrical equipment to feed that power to the grid is about another $1/W. To reach the projected total CAPEX quoted in LIFE documents, this implies that the entire \"nuclear island\" has to cost around $4/W for LIFE.2, and just over $2/W for LIFE.3. Modern nuclear plants, benefiting from decades of commercial experience and continuous design work, cost just under $8/W, with approximately half of that in the nuclear island. LLNL's estimates require LIFE.3 to be built in 2040 for about half the cost of a fission plant today.\n\nNIF construction was completed in 2009 and the lab began a long calibration and setup period to bring the laser to its full capacity. The plant reached its design capacity of 1.8 MJ of UV light in 2012. During this period, NIF began running a staged program known as the National Ignition Campaign, with the goal of reaching ignition by 30 September 2012. Ultimately, the campaign failed as unexpected performance problems arose that had not been predicted in the simulations. By the end of 2012 the system was producing best-case shots that were still of the pressures needed to achieve ignition.\n\nIn the years since, NIF has run a small number of experiments with the explicit aim of improving that number, but the best result is still away from the required densities, and the method used to achieve those numbers may not be suitable for closing that gap and reaching ignition. It is expected that a number of years of additional work are required before ignition can be achieved, if ever. During a progress review after the end of the Campaign, a National Academy of Sciences review board stated that \"The appropriate time for the establishment of a national, coordinated, broad-based inertial fusion energy program within DOE is when ignition is achieved.\" They noted that \"the panel assesses that ignition using laser indirect drive is not likely in the next several years.\"\n\nThe LIFE effort was quietly cancelled in early 2013. LLNL's acting director, Bret Knapp, commented on the issue stating that \"The focus of our inertial confinement fusion efforts is on understanding ignition on NIF rather than on the LIFE concept. Until more progress is made on ignition, we will direct our efforts on resolving the remaining fundamental scientific challenges to achieving fusion ignition.\"\n\n\n"}
{"id": "58880189", "url": "https://en.wikipedia.org/wiki?curid=58880189", "title": "Lisa Marcaurelle", "text": "Lisa Marcaurelle\n\nLisa A. Marcaurelle is a chemist and a senior executive in multiple biotechnology companies.\n\nMarcaurelle received her B.A. in Chemistry from the College of the Holy Cross in 1997, where she worked with Prof. Timothy Curran on dipeptide scaffolds. She enrolled in UC-Berkeley to pursue a Ph.D. in Chemistry under Carolyn Bertozzi, working on a variety of new glycoside-linking technologies to produce glycoprotein mimetics for natural substances like mucin. Marcaurelle completed a one-year postdoctoral fellowship at the Massachusetts Institute of Technology, working with Peter Seeberger on solid-phase synthesis of oligosaccharides.\n\nMarcaurelle worked at Infinity Pharmaceuticals and the Broad Institute for a combined 9 years, working in high-throughput chemistry, diversity-oriented chemical synthesis, chemical biology, and medicinal chemistry projects. She was recruited as a Vice President at H3 Biomedicine in 2011. At H3, Marcaurelle instituted and continued to develop a diversity-oriented synthesis platform, based on newly-evolving chemotypes such as spirocycles and macrocycles. In 2016, she became Senior Director of Chemistry at Warp Drive Bio, and in 2018 Vice President of Enko Chem, a venture-backed crop protection start-up in the Boston area.\n\nMarcaurelle remains active in professional society volunteering and mentorship of younger scientists in the pharmaceutical industry, and actively advocates for the inclusion of women scientists in leadership roles.\n\n"}
{"id": "37924503", "url": "https://en.wikipedia.org/wiki?curid=37924503", "title": "List of Coraciiformes by population", "text": "List of Coraciiformes by population\n\nThis is a list of Coraciiformes species by global population. While numbers are estimates, they have been made by the experts in their fields. For more information on how these estimates were ascertained, see Wikipedia's articles on population biology and population ecology.\n\nThis list is incomprehensive, as not all Coraciiformes have had their numbers quantified.\n"}
{"id": "918748", "url": "https://en.wikipedia.org/wiki?curid=918748", "title": "List of Fourier analysis topics", "text": "List of Fourier analysis topics\n\nThis is a list of Fourier analysis topics. See also the list of Fourier-related transforms, and the list of harmonic analysis topics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "2678377", "url": "https://en.wikipedia.org/wiki?curid=2678377", "title": "List of compounds with carbon number 19", "text": "List of compounds with carbon number 19\n\nThis is a partial list of molecules that contain 19 carbon atoms.\n\n"}
{"id": "8687012", "url": "https://en.wikipedia.org/wiki?curid=8687012", "title": "List of cryptology conferences", "text": "List of cryptology conferences\n\nThis is a list of conferences and workshops in the area of cryptography. The most important events in this\narea are organized by or affiliated with the IACR. \n\nIACR sponsors the following three conferences:\n\nThe following events are sponsored by IACR as workshops. They have a narrower scope compared to the\nthree main IACR conferences.\n\nThe following events are frequently organized in cooperation with IACR\n\n\n\n"}
{"id": "33400984", "url": "https://en.wikipedia.org/wiki?curid=33400984", "title": "Media Technology Monitor (MTM)", "text": "Media Technology Monitor (MTM)\n\nThe Media Technology Monitor (MTM) is an annual telephone survey and research product designed to monitor Canadians' use and adoption of new and existing technologies. The first MTM questionnaire was distributed in 2005 and now consists of two annual surveys conducted in the spring and fall of each year.\n\nThe survey features questions that focus on current and emerging media activities and technologies such as, television across multiple platforms, radio use social media, and online audio. The results of the Media Technology Monitor are published through a series of syndicated reports and multiple short, targeted theme reports. The Media Technology Monitor reports are distributed to government agencies, broadcasters, and other media stakeholders across Canada who subscribe to the service.\n\nThe MTM is designed, implemented and analyzed by the Canadian Broadcasting Corporation (CBC/Radio-Canada). In 2011, the MTM coordinated with Carleton University in Ottawa, Ontario to provide learning materials to the Communications Department.\n\nThe Media Technology Monitor has evolved from the Canadian Broadcasting Corporation's Quality Ratings Survey (QRS) that began in 1997 to measure Canadians' use and perception of Canada's conventional and specialty television channels as well as some media technology behaviours. In 2005, the MTM was launched with a specific focus on media technology adoption and use among Canadians. In 2010, an online MTM portal (www.mtm-otm.ca) was created to provide a more interactive experience for users. The portal allows users a quick and easy way to download MTM reports and trending data, as well as conduct custom analysis of the data with a proprietary data analysis tool.\n\nMTM-OTM Video on YouTube : https://www.youtube.com/watch?v=oGlObksktTU&feature=youtu.be\n\nSince then the technology has become less and less used due to the rise of the internet and the easier, quicker and cheaper way to send our surveys to users which using certain methods is also further available for free. \n\nThe Media Technology Monitor is released in the fall and spring of each year and collects a total of 12,000 responses through telephone interviews with 6,000 Anglophones and 6,000 Francophones aged 18+. The response rate for both surveys is 30%.\n\n\nMTM survey households are selected by Random Digit Dialing (RDD) and the respondent within each household is randomly selected based on the most recent birthday method. The MTM uses telephone surveys to ensure that the respondents are representative of all Canadians by eliminating bias due to geographic location and connection to the Internet.\n\nThe Media Technology Monitor is revised every year to include emerging media trends and technologies. Recent MTM surveys have asked respondents about their behavior and consumption of:\n\n\n"}
{"id": "6164128", "url": "https://en.wikipedia.org/wiki?curid=6164128", "title": "Michel Brunet (paleontologist)", "text": "Michel Brunet (paleontologist)\n\nMichel Brunet (born on April 6, 1940) is a French paleontologist and a professor at the Collège de France. In 2001 Brunet announced the discovery in Central Africa of the skull and jaw remains of a late Miocene hominid nicknamed Toumaï. These remains may predate the earliest previously known hominid remains, Lucy, by over three million years; however, this conclusion is the subject of a significant controversy.\n\nBrunet was born in 1940 in Vienne, in the region of Poitou. After having passed his first years in the countryside, at 8 he moved with his family to Versailles. He took a Ph.D. in paleontology at the Sorbonne and then became Professor of Vertebrate paleontology at the University of Poitiers, specializing in hoofed mammals.\n\nA turning point in Brunet's career was when he heard that paleoanthropologist David Pilbeam was searching for fossil apes in Pakistan and the ancestors of the hominids. This spurred Brunet to form with his colleague Emile Heintz a team with the idea of also searching for extinct apes across the border from Pakistan in Afghanistan. The expedition was unsuccessful, and no fossil apes were found.\n\nIn the 1980s Brunet and Pilbeam matched together and moved to Africa. Their idea was to verify the theory of Yves Coppens that hominids had first rose in the savannas of Eastern Africa. The two paleontologists idea was that the shores of Lake Chad were particularly indicated to work as a magnet for mammals, and maybe also hominids. In 1984 searching begun in Cameroon, but the nine field seasons spent there were discouraging, with no hominids found.\n\nA new opportunity presented itself to Brunet when the government of Chad gave him the permission to conduct researches in the Djurab Desert, that due the Chadian Civil War had long been closed to foreigners. Brunet promptly formed the French-Chadian Paleoanthropological Mission (\"Mission Paléoanthropologique Franco-Tchadienne\" or MPFT) a Franco-Chadian scientific alliance that united the University of Poitiers, the University of N'Djamena and the Centre Nationale d'Appui à la Recherche (CNAR).\n\nThe area proved itself to be a site rich in fossils, and expeditions headed by Brunet have collected over 8000 of them, including hominid remains. On January 23, 1995 he spotted a jawbone 3.5 million years old, that he classified as a new species of Australopithecine, the \"Australopithecus bahrelghazali\". Informally he called it Abel, as a tribute to his dead friend Abel Brillanceau. Abel was the first fossil hominid found in Western Africa, radically transforming the discussions on early hominid distribution, that until this discovery was thought to center only in Southern and especially Eastern Africa.\n\nWhile much discussed, a yet more important find was to be made by Brunet's team on July 19, 2001; a Chadian student of the mission, Ahounta Djimdoumalbaye, unearthed a nearly complete cranium, from 6 to 7 million years old, nicknamed Toumaï by the Chadian President Idriss Déby, and classified by Brunet as the first exemplary of the \"Sahelanthropus tchadensis\". Brunet and others, like Tim White, are strongly convinced Toumai to be a hominid, though this is contested by colleagues like Milford Wolpoff, who instead believe it to be an ape. Other experts, like Chris Stringer, argued it was inconclusive where the \"Sahelanthropus\" belongs in the human evolutionary line. Brunet has argued that further excavations have uncovered additional remains which further confirm that Sahelanthropus was a hominid, though his conclusions with these newer findings are also debated by some scientists.\n\nThe discovery brought Brunet worldwide recognition in the field of paleoanthropology; and in 2003 he was awarded the Dan David Prize, a prize given to those whose achievements help better understand the world, or affect it.\n\n\n"}
{"id": "20057246", "url": "https://en.wikipedia.org/wiki?curid=20057246", "title": "Minerva Initiative", "text": "Minerva Initiative\n\nThe Minerva Initiative is a research program sponsored by the U.S. Department of Defense (DoD) that provides grants to sustain university-based, social science studies on areas of strategic importance to U.S. national security policy. The program looks to tap into the community of area specialists and other university researchers, particularly those who work on Islam, Iraq, China, and related areas. Since its establishment in 2008, the Department of Defense has awarded over 70 grants to private researchers. Grants are awarded on an annual basis for research projects that typically last three years.\n\nIn 2011 the DoD hosted the first annual Minerva Conference. The event has since become an annual gathering in September for Minerva researchers, DoD keynote speakers, and scholars to highlight the most relevant research conducted through the program’s support.\n\nIn 2008, the project was provided $50 million by the United States Department of Defense to fund research on five separate themes. When the program began in 2008 project funding was split between DoD and the National Science Foundation. Since that time, all projects have been funded solely by the DoD. The goal was to create improved relations between the Department of Defense and the universities and to develop knowledge that the military can benefit from in the long term.\n\nSecretary of Defense Robert M. Gates commissioned the Minerva Initiative under the vision of “…a consortia of universities that will promote research in specific areas.” Originally, Secretary Gates proposed four principal focus areas: 1) Chinese Military and Technology Studies 2) Iraqi and Terrorist Perspective Projects 3) Religious and Ideological Studies, and 4) New Disciplines Project and 5) Open Category. Since its establishment, the Minerva Initiative has expanded the breadth of sponsored research to include a wide range of academic disciplines and topics [see Ongoing Research]. The program continues to fund research on the most important social science topics for universities like Arizona State University, which recently received a grant to support research on terrorists’ use of social media.\n\nThe stated goal of the Minerva Initiative “is to improve DoD's basic understanding of the social, cultural, behavioral, and political forces that shape regions of the world of strategic importance to the U.S.” The program seeks to achieve this by sponsoring research designed to bring together universities, research institutions, and individual scholars. Three key priorities reflect the objectives of the Secretary of Defense for the Minerva Initiative:\n“1) Leverage and focus the resources of the Nation's top universities.\n2) Seek to define and develop foundational knowledge about sources of present and future conflict with an eye toward better understanding of the political trajectories of key regions of the world.\n\nAs of 2015 the Minerva Initiative’s priority research areas fall within four categories: \nI. Identity, Influence, and Mobilization\nII. Contributors to Societal Resilience and Change\n\nIII. Power and Deterrence\n\nIV. Innovations in National Security, Conflict, and Cooperation\n\nA list of all research awards made since the start of the Minerva Initiative are listed at the program’s site: https://web.archive.org/web/20091212104216/http://minerva.dtic.mil/funded.html. In 2015, the Minerva Steering Committee received over 300 applications (297 white papers and 46 full proposals).\n\nThe program's funding of social science research for national security purposes has proven controversial. Although many scholars support Minerva, at the program’s start a number of academic researchers sounded public alarm about the prospect of Defense Department funding for research. In 2008 the American Anthropological Association sent a public letter suggesting that the funding be transferred to a different body, such as the National Science Foundation (NSF). Hugh Gusterson, a prominent anthropologist at George Mason University, wrote a series of articles in a variety of venues that have attracted significant attention,\n\"any attempt to centralize thinking about culture and terrorism under the Pentagon’s roof will inevitably produce an intellectually shrunken outcome...The Pentagon will have the false comfort of believing that it has harnessed the best and the brightest minds, when in fact it will have only received a very limited slice of what the ivory tower has to offer—academics who have no problem taking Pentagon funds. Social scientists call this “selection bias,” and it can lead to dangerous analytical errors.\"\n\nThe journalist Nafeez Ahmed has expressed concern that Minerva research, in its effort to understand mass mobilization, may be targeting peaceful activists, NGOs and protest movements. Others believe social science should continue to emphasize security issues but worry that DoD funding will bias findings. One article notes:\n\n\"In an incentive structure that rewards an emphasis on countering global threats and securing the homeland, the devil lies in the definitions. In this framework, the Boston Marathon bombing becomes a national security problem, whereas the Sandy Hook massacre remains a matter for the police and psychologists—a distinction that is both absurd as social science and troubling as public policy.\"\n\n\n"}
{"id": "29603121", "url": "https://en.wikipedia.org/wiki?curid=29603121", "title": "Multi Autonomous Ground-robotic International Challenge", "text": "Multi Autonomous Ground-robotic International Challenge\n\nThe Multi Autonomous Ground-robotic International Challenge (MAGIC) is a 1.6 million dollar prize competition for autonomous mobile robots funded by TARDEC and the DSTO, the primary research organizations for Tank and Defense research in the United States and Australia respectively. The goal of the competition is to create multi-vehicle robotic teams that can execute an intelligence, surveillance and reconnaissance mission in a dynamic urban environment. The challenge required competitors to map a 500 m x 500 m challenge area in under 3.5 hours and to correctly locate, classify and recognise all simulated threats. The challenge event was conducted in Adelaide, Australia, during .\n\nInitially 12 teams were selected for the competition in , of which 10 teams received funding. These included:\n\nThe first downselection trial required teams to map an indoor area and outdoor area, and to demonstrate distributing and handing over tasks between robots. During the first downselection trial, the top six teams were selected:\n\nBefore the finals were held, Chiba Team withdrew from the competition, leaving five competitors.\n\nUltimately the overall goal of fully autonomous operations without human intervention was not achieved, however, the Secretary for Defence stated \"The competing vehicles demonstrated new advances in robotics technology, which are very promising for their potential deployment in combat zones where they can replace our troops in carrying out life-threatening tasks\" and considered the competition a success.\n\nThe official results of the competition were:\n\nThe \"Old Ram Shed Challenge\" was a single-day competition held after the completion of MAGIC. It was smaller in scale, allowing all of the teams to demonstrate their systems during a single day. The University of Pennsylvania won this challenge, having found a greater number of the target objects than the other teams.\n\nKey technology used by all teams was computer vision, sensor fusion, human-robot interaction,\nand simultaneous localization and mapping (SLAM).\n\n\nSee the September/October 2012 special issue of the Journal of Field Robotics for contest highlights, technical approaches taken by several of the teams, and an explanation of the evaluation metrics used by organizers.\n\n\n\n"}
{"id": "1622543", "url": "https://en.wikipedia.org/wiki?curid=1622543", "title": "Not in Our Genes", "text": "Not in Our Genes\n\nNot in Our Genes: Biology, Ideology and Human Nature is a 1984 book by the evolutionary geneticist Richard Lewontin, the neurobiologist Steven Rose, and the psychologist Leon Kamin, in which the authors criticize sociobiology and genetic determinism and advocate a socialist society.\n\nThe book formed part of a larger campaign against sociobiology. Its authors were praised for their criticism of IQ testing, and were complimented by some for their critique of sociobiology. However, they have been criticized for misrepresenting the views of scientists such as the biologist E. O. Wilson and the ethologist Richard Dawkins, for using \"determinism\" and \"reductionism\" simply as terms of abuse, and for the influence of Marxism on their views. Critics have seen its authors' conclusions as political rather than scientific.\n\nLewontin, Rose and Kamin identify themselves as \"respectively an evolutionary geneticist, a neurobiologist, and a psychologist.\" They criticize biological determinism and reductionism, and state that they share a commitment to the creation of a socialist society and a recognition that \"a critical science is an integral part of the struggle to create that society\". Their understanding of science draws on ideas suggested by Karl Marx and Friedrich Engels and developed by Marxist scholars in the 1930s. They also draw on the ideas of the Marxist philosopher György Lukács, as put forward in \"History and Class Consciousness\" (1923), as well as the ideas of the Marxist philosopher Ágnes Heller and the communist revolutionary Mao Zedong. They discuss and criticize the views of authors such as E. O. Wilson, Richard Dawkins, and Donald Symons. They criticize Wilson's \"\" (1975). They maintain that, like some other sociobiologists, Symons maintains that \"the manifest trait is not itself coded by genes, but that a potential is coded and the trait only arises when the appropriate environmental cue is given.\" In their view, \"Despite its superficial appearance of dependence on environment, this model is completely genetically determined, independent of the environment.\" They write that Symons' arguments in \"The Evolution of Human Sexuality\" (1979) provide examples \"of how sociobiological theory can explain anything, no matter how contradictory, by a little mental gymnastics\".\n\n\"Not in Our Genes\" was first published by Pantheon Books in 1984. Later that year it was published by Pelican Books. In 1990, it was published by Penguin Books.\n\n\"Not in Our Genes\" received positive reviews from the columnist Gene Lyons in \"Newsweek\" and the paleontologist Stephen Jay Gould in \"The New York Review of Books\", a mixed review from the philosopher Philip Kitcher in \"The New York Times Book Review\", and negative reviews from the anthropologist Melvin Konner in \"Natural History\" magazine and the biologist Patrick Bateson and the ethologist Richard Dawkins in \"New Scientist\". Its editors noted that the book would \"inevitably attract either extreme criticism or glowing praise\" depending on the reviewer's stance on sociobiology, and that they published two reviews to help encourage debate, having approached Dawkins \"for the opposition\" and Bateson, \"who feels that the attack on genetic determinism is justified.\" The book was also reviewed by the psychologist Sandra Scarr in \"American Scientist\", Nathaniel S. Lehrman in \"The Humanist\", and in \"The Wilson Quarterly\" and \"Science News\".\n\nLyons described the book as a \"spirited, if often repetitive, demolition of sociobiology's pretensions\", adding that its authors' arguments were \"made doubly impressive\" by their \"analysis of how the economic determinism of what they call '“vulgar” Marxism' and the spinelessness of 'sociological relativism' have contributed to a climate in which the speculations of sociobiology have found a hearing.\" Gould described the book as \"important and timely\". He credited Lewontin \"et al.\" with exposing the fallacies of biological determinism (though he noted that theirs was only one critique among many), and presenting a view of human behavior that went beyond the controversy over nature and nurture. However, he believed that while they exposed problems with research on schizophrenia, they did not reveal \"fatal and debilitating flaws\". He agreed with Lewontin \"et al.\" that \"interactionism is also based on deep fallacies and cultural biases that play into the hands of biological determinism\", showing that it is guilty of the fallacy of \"reductionism\".\n\nKitcher described the book as \"informative, entertaining, lucid, forceful, frequently witty, occasionally unfair, sometimes intemperate, never dull\". He praised Lewontin \"et al.\"′s discussion of intelligence, and complimented their discussions of sex differences and the use of drugs and surgery to modify behavior. He was less convinced by their discussion of schizophrenia, writing that in it their \"policy of treating their opponents as patsies begins to seem unjustified\". Konner believed that the book's authors provided an \"acceptable review of the dismal historical record of abuse of ideas in behavioral genetics\" but that this history had received better discussions. He criticized Lewontin \"et al.\" for giving little attention to \"similar abuses that have occurred under political systems that espouse a cultural-determinist ideology.\" He accused them of falsely attributing a belief in \"heredity privilege\" to advocates of IQ testing, employing tactics such as guilt through association, providing misleading discussions of issues in psychiatry and neurology, such as attention deficit disorder, psychosurgery, and antipsychotic drugs, and criticizing sociobiology on the basis of the weakest studies in the field and popular writings by journalists. He considered Wilon's discussion of the development of behavior in \"Sociobiology\" more sophisticated than that of Lewontin \"et al.\" He called the book \"unfortunate\", writing that its authors \"offer little, except for pious hand-wringing and 'dialectical' rhetoric, that might help us to grapple with the great unanswered questions of our behavior and experience, normal and abnormal.\"\n\nBateson accused the book's authors of making it easy for themselves to criticize the genetic analysis of behavior by focusing on its weakest advocates, though he granted that their \"counter-rhetoric\" was \"brilliant\" and sometimes \"illuminating.\" He also praised their discussion of measuring intelligence, writing that it was clear and \"merciless\" in its \"exposure of poor method.\" He credited them with making a strong case against genetic explanations of both differences in IQ and schizophrenia, but did not consider their conclusions about either issue definitive, noting that both remained subject to dispute. He also found their criticism of ethology and sociobiology distorted by their personal biases, writing that despite errors by some proponents of sociobiology, Lewontin \"et al.\" were incorrect to dismiss it altogether. He noted that they ignored developments in the field that corrected some of the initial mistakes made by Wilson in \"Sociobiology\". He also wrote that their claim that the belief that animals have a tendency not to mate with individuals familiar from early life is based on little evidence is incorrect. According to Bateson, even though he was predisposed to be sympathetic to Lewontin \"et al.\"′s approach, the value of their work was undermined by their poor scholarship and bad arguments, and the errors they made in discussing his field forced him to wonder about the value of their work even when it seemed strong, such as the portions concerning IQ and schizophrenia. Though agreeing with their views about the interaction between the social and physical environment, he accused them of wrongly suggesting that they were novel, when they were held by many others and it was doubtful whether anyone actually believed in the form of interactionism they criticized. He predicted that most scientists would simply disregard their book, and questioned whether discrediting genetic determinism would help create a more just society.\n\nDawkins accused the book's authors of promoting a \"bizarre conspiracy theory of science\" that suggested that sociobiology was a response to 1960s student activism, and of wrongly using quotations from non-sociobiologists such as the Conservative politician Patrick Jenkin and representatives of the British National Front and the French Nouvelle Droite as though they represented sociobiology. He described their claim that sociobiologists believe in genetic determinism as a \"simple lie\", and wrote that they employed the term \"biological determinism\" without having a clear idea of what they meant by it, and used the words \"determinist\" and \"reductionist\" simply as terms of abuse. He argued that biologists practice an appropriate form of \"reductionism\" that involves explaining complex wholes in terms of their parts, and never practice the form of \"reductionism\" criticized by Lewontin \"et al.\", which involves the idea that \"the properties of a complex whole are simply the \"sum\" of those same properties in the parts\". He maintained that the anthropologists Marshall Sahlins and Sherwood Washburn, praised by Lewontin \"et al.\" for their criticism of sociobiology, were both guilty of elementary misunderstandings of kin selection theory and that Lewontin knew enough about genetics that he should have realized this, and that the \"dialectical biology\" advocated by Lewontin \"et al.\" actually involved ideas similar to those suggested by Bateson and Dawkins himself. He attributed the positive reviews of the book from liberals to its authors' opposition to racism. Though he believed that its chapters on \"IQ testing and similar topics\" had some value, he nevertheless concluded that Lewontin \"et al.\"′s book was both poorly written and \"silly, pretentious, obscurantist and mendacious\".\n\n\"Not in Our Genes\" received positive reviews from the biologist Peter Medawar in \"Nature\", the geneticist Alan Emery in \"Trends in Neurosciences\", and T. Benton in \"The Sociological Review\", the biologist Franz M. Wuketits in the \"Journal of Social and Biological Structures\", and a mixed review from the anthropologist Vernon Reynolds in \"Ethnic and Racial Studies\". The book was also reviewed by Howard L. Kaye in \"Society\".\n\nMedawar described the book as a well-written and \"in the main convincing rebuttal of a variety of determinist ideologies that have come to acquire the status of a public nuisance in biology and sociology.\" He endorsed its authors' criticism of IQ testing and their argument that determinism is an expression of conservative ideology. However, he was less satisfied by their criticism of reductionism, writing that despite its shortcomings reductive analysis was \"the most successful research stratagem ever devised in science.\" He argued that it was also the way of understanding the world that made it easiest to see how it could be changed, something left-wing writers such as the authors of \"Not in Our Genes\" should appreciate. Emery welcomed the book as a refreshing attempt to create a more balanced view of the relevance of genetics to human behavior.\n\nBenton described the book as an \"immense achievement\" and a well-written work accessible to a large audience. He complimented its authors for their historical survey of biological determinism and reductionism and their philosophical discussion of their dialectical alternative, and praised their discussions of IQ testing, biological determinist defences of patriarchy, psychiatry, schizophrenia, and sociobiology. He believed that they exposed the logical and conceptual problems of defining and measuring intelligence and identifying schizophrenia as a unitary disorder, as well as problems in the methodologies of heritability studies in both cases, including their assumption that \"the determinants of any characteristic can be analysed as of two, separable kinds, heredity and environment, and that it makes sense to ask what proportion of each went into the making of the particular characteristic.\" He wrote that they dealt \"selectively (and probably appropriately) with the work of Wilson and Dawkins\". However, he believed that they did not have a fully developed alternative to biological and cultural determinism, questioned whether they were able to present a view different from cultural determinism, and noted that while they treated sociobiology as a form of genetic determinism, the main sociobiological writers had become \"more sophisticated and qualified in their assumptions.\" He criticized them for using quotations in a selective fashion to argue that sociobiology is still an unqualified form of genetic determinism, and for equating \"biological determinism and political reaction\", noting that religious fundamentalists wanted to outlaw the teaching of evolutionary theory, and some progressive thinkers accept that biological processes shape personality.\n\nWuketits described the book as \"concise and well written\", and \"more provocative than anything else written in opposition to genetic determinism and its ideological interpretation\" because of its identification of sociobiology with the New Right. Although he agreed with many of Lewontin \"et al.\"′s views, he nevertheless considered them mistaken to view sociobiology as only an \"ideological program\", writing that it was primarily a scientific discipline and should not be dismissed simply for ideological reasons. He expressed regret that the book would give readers not familiar with the scientific background to sociobiology the impression that it is \"nothing but a dangerous pseudoscientific ideology.\"\n\nReynolds argued that because Lewontin \"et al.\" dismissed biological approaches to understanding human nature, they invalidated their own claims about human nature, reducing them from scientific to political statements. He maintained, in opposition to Lewontin \"et al.\", that a single \"committed political position\" cannot be used to evaluate or criticize science, and that determining to what extent scientific claims are actually political in nature requires consideration of all political positions. He wrote that Lewontin \"et al.\" provided a dubious description of science that made it sound like a \"right wing political movement\", noting that their own credentials as scientists suggested that their politicized view of science was incorrect. However, he considered them correct to claim that the arguments of sociobiology were only \"speculative suggestions\" and that it was unfortunate if \"the fascist right\" adopted them as \"scientific validation of its ideology\", and that some scientific work, such as \"IQ testing\", is politicized science, and credited them with showing that \"a good many branches of the science of human nature all revolve around the problem of inequality\" and \"mostly validate it.\" He also found their book enjoyable reading.\n\nThe psychologist David P. Barash, writing in \"The Hare and the Tortoise\" (1986), mentioned \"Not in Our Genes\" as an example of the controversy surrounding sociobiology. He criticized Lewontin \"et al.\" for unfairly connecting sociobiology with \"racist eugenics and misguided Social Darwinism.\" Dawkins, writing in the second edition of \"The Selfish Gene\" (1976; second edition 1989), accused Lewontin \"et al.\" of misquoting his comment of genes, \"they created us, body and mind\", by altering the word \"created\" to \"control\", and maintained that genes do not control people in the way that \"genetic determinism\" suggests. He accused Lewontin \"et al.\" of failing to understand that \"it is perfectly possible to hold that genes exert a statistical influence on human behavior while at the same time believing that this influence can be modified, overriden or reversed by other influences.\" The biologist Dean Hamer, writing in \"The Science of Desire\" (1994), described \"Not in Our Genes\" as \"a political rather than a scientific book\" and expressed his disagreement with its politics. Nevertheless, Hamer commented that it taught him that the genetics of behavior is an emotionally and politically charged topic, especially where it concerns sexuality, and helped motivate him to change fields from metallothionein research to the genetics of homosexuality. The philosopher Daniel Dennett, writing in \"Darwin's Dangerous Idea\" (1995), criticized Lewontin \"et al.\"′s account of reductionism, calling it \"idiosyncratic\". He also criticized their claim that memes involve a Cartesian view of the mind, arguing that memes are \"a key (central but optional) ingredient in the best alternatives to Cartesian models\". Richard Webster, writing in \"Why Freud Was Wrong\" (1995), called \"Not in Our Genes\", \"a critique of sociobiology and genetic determinism which is, for the most part, much more subtle and valuable than the Marxism which frequently informs it.\"\n\nRose, writing in \"Lifelines\" (1997), commented that he and his co-authors in \"Not in Our Genes\" presented a critique of reductionism that was \"systematic and based upon a coherent philosophical and political analysis which sees modern science as the inheritor of nineteenth-century mechanical materialism, itself tightly linked ideologically to a particular phase of the development of industrial capitalism.\" The historian of science Roger Smith, writing in \"The Norton History of the Human Sciences\" (1997), described \"Not in Our Genes\" as an accessible critique of sociobiology. The psychologist Steven Pinker, writing in \"How the Mind Works\" (1997), criticized Lewontin \"et al.\" for engaging in \"innuendos about Donald Symons's sex life\" and misquoting Dawkins. The sociologist Hilary Rose, writing with Steven Rose in \"Alas, Poor Darwin\" (2000), noted that \"Not in Our Genes\" was one of a number of books that criticized sociobiology. In the same work, Hilary Rose suggested that \"Not in Our Genes\" had been misread by critics, and credited its authors with offering \"an alternative theory to biological determinism more robust than the rather weak concept of interaction between nature and nurture\".\n\nThe sociologist Ullica Segerstråle, writing in \"Defenders of the Truth: The Battle for Science in the Sociobiology Debate and Beyond\" (2000), suggested that \"Not in Our Genes\", along with Gould's anti-sociobiological essays in \"Natural History\" magazine, represented the height of the \"critical attack\" on sociobiology from its opponents. She noted that the book constituted a late admission from critics of sociobiology that some of them wanted a socialist society. According to Segerstråle, Rose threatened to sue Dawkins for libel for his review of the book, and although he did not make good on the threat, the evolutionary biologist W. D. Hamilton and other scientists made efforts to protect Dawkins, including seeking help from Segerstråle herself. She suggested that Rose's reaction to Dawkins's review may been influenced by the fact that while \"New Scientist\", which commissioned reviews from Dawkins and Bateson, had expected the former to write a negative and the latter a positive review, both reviews were in fact negative, which may have disappointed Rose, a friend of Bateson. She also noted that the book's attack on sociobiology led Dawkins to identify himself as a sociobiologist for the first time.\n\nThe behavioral ecologist John Alcock, writing in \"The Triumph of Sociobiology\" (2001), argued that while Lewontin \"et al.\" were correct to maintain that no genes for social behavior had been identified as of 1984, it was nevertheless clear that thousands of genes are expressed in human brain cells and must be relevant to the structure of the brain and to human behavior. Pinker, writing in \"The Blank Slate\" (2002), accused Lewontin \"et al.\" of using words such as \"determinism\" and \"reductionism\" as \"vague terms of abuse\", and of misrepresenting the views of scientists such as Wilson and Dawkins, falsely ascribing ridiculous beliefs to them. He saw them and other critics of \"determinism\" as misusing the term by using it to refer to the idea that people simply have a tendency to behave in a certain fashion. Pinker endorsed Dawkins's review of \"Not in Our Genes\". He noted that Lewontin and Rose were themselves both \"reductionist biologists\", and attributed their rejection of the idea of human nature to their acceptance of Marxism.\n\nDennett, writing in \"Freedom Evolves\" (2003), accused Lewontin \"et al.\" of being willing to use unscrupulous tactics to criticize people they considered determinists. \n\n\n\n"}
{"id": "49668510", "url": "https://en.wikipedia.org/wiki?curid=49668510", "title": "Olive oil acidity", "text": "Olive oil acidity\n\nFree acidity is an important parameter that defines the quality of olive oil and is defined as a percentage as grams of free fatty acids (expressed as oleic acid, the main fatty acid present in olive oil) in 100 grams of oil. As defined by the European Commission regulation No. 2568/91 and subsequent amendments, the highest quality olive oil (Extra-Virgin olive oil) must feature a free acidity lower than 0.8%. Virgin olive oil is characterized by acidity between 0.8% and 2%, while lampante olive oil (a low quality oil that is not edible) features a free acidity higher than 2%. The increase of free acidity in olive oil is due to free fatty acids that are released from triglycerides.\n\nThe presence of free fatty acids in olive oil is caused by a reaction (lipolysis) started when lipolytic enzymes (that are normally present in the pulp and seed cells of the olive) come in contact with the oil (that is contained in particular vacuoles) due to loss of integrity of the olive. High values of free acidity in olive oil can be due different factors such as: production from unhealthy olives (due to microorganisms and moulds contamination or attacked by flies and parasites), bruised olives, delayed harvesting and storage before processing. The lipolysis reaction is greatly enhanced by the presence of an aqueous phase, so when oil is separated from water during processing, lipolysis slows down and stops.\n\nFree acidity is a defect of olive oil that is tasteless and odorless, thus can not be detected by sensory analysis. Since vegetable oils are not aqueous fluids, a pH-meter can not be used for this measure. Various approaches exist that can measure oil acidity with good accuracy.\n\nThe official technique to measure free acidity in olive oil (as defined by the European Commission regulation No. 2568/91) is a manual titration procedure: a known volume of the oil to be tested is added to a mix of ether, methanol and phenolphthalein, known volumes of potassium hydroxide KOH 0.1M (the titrant) are added until there is a change in the color of the solution. The total volume of added titrant is then used to estimate the free acidity. The official technique for acidity measure in olive oil is accurate and reliable, but is essentially a laboratory method that must be carried out by trained personnel (mainly because of the toxic compounds used). Hence it is not suitable for \"in situ\" measurements in small oil mills.\n\nOne of the most promising method is based on optical near-infrared spectroscopy (NIR) where the optical absorbance, i.e. the fraction of intensity of the incident light that is absorbed by the oil sample, is used to estimate the oil acidity. An oil sample is placed in a cuvette and analyzed by a spectrophotometer on a wide range of wavelengths. The results (i.e. the absorbance data for every tested wavelength) are thus processed by a statistical algorithm, such as Principal Component Analysis (PCA) or Partial Least Squares regression (PLS), to estimate the oil acidity. The feasibility to measure olive oil free acidity and peroxide value by NIR spectroscopy in the wavenumber range 4,541 to 11,726 cm was reported. Many commercial spectrophotometers exist that can be used for analysis of different quality parameters in olive oil. The main advantage of NIR spectroscopy is the possibility to carry out the analysis on raw olive oil samples, without any chemical pretreatment. The main drawbacks are the high cost of commercial spectrophotometer and the need of calibration for different types of oil (produced by olives of different varieties, different geographical origin, etc.).\n\nAnother approach is based on electrochemical impedance spectroscopy (EIS). EIS is a powerful technique that has been widely used to characterize different food products such as the analysis of milk composition, the characterization and the determination of the freezing end-point of ice-cream mixes, the measure of meat ageing, and the investigation of ripeness and quality in fruits.\n"}
{"id": "15601579", "url": "https://en.wikipedia.org/wiki?curid=15601579", "title": "Orchidology", "text": "Orchidology\n\nThe orchidology is the scientific study of orchids. It is an organismal-level branch of botany.\n\n\n\n"}
{"id": "8990853", "url": "https://en.wikipedia.org/wiki?curid=8990853", "title": "Snow in Florida", "text": "Snow in Florida\n\nIt is very rare for snow to fall in the U.S. state of Florida, especially in the central and southern portions of the state. With the exception of the far northern areas of the state and the Jacksonville area, most of the major cities in Florida have never recorded measurable snowfall, only trace (T) amounts have been recorded, or flurries observed in the air a few times each century. According to the National Weather Service, in the Florida Keys and Key West there is no known occurrence of snow flurries since the settlement of the region. In the Miami, Ft. Lauderdale, and Palm Beach areas, as well as the Tampa area, there is only one known report of snow flurries observed in the air (and in some areas a trace on the ground) in January 1977. \n\nDue to Florida's low latitude and subtropical climate, temperatures cold enough to support significant snowfall are infrequent and their duration is fleeting. In general, frost is more common than snow, requiring temperatures of 32 °F (0 °C) or less at above sea level, a cloudless sky, and a relative humidity of 65% or more. Generally, for snow to occur, the polar jet stream must move southward through Texas and into the Gulf of Mexico, with a stalled cold front across the southern portion of the state curving northeastward to combine freezing air into the frontal clouds. While light snowfall occurs a few times each decade across the northern panhandle, most of the state is too far south of the cold continental air masses responsible for generating snowfall in the rest of the country. The mean maximum monthly snowfall in most parts of Florida is zero. The only other areas in the continental United States with this distinction are extreme southern Texas (around McAllen) and parts of coastal southern California at low elevations.\n\nMuch of the known information on snow in Florida prior to 1900 is from climatological records provided by the National Weather Service meteorological station in Jacksonville, and information for other locations is sparse. The earliest recorded instance of snow in Florida occurred in 1774; being unaccustomed to snow, some Jacksonville residents called it \"extraordinary white rain.\" The first White Christmas in northeastern Florida's history resulted from a snow event that occurred on December 23, 1989.\n\nThe vast majority of snow events in Florida occurred in far northern Florida and the Jacksonville area. According to the National Weather Service, the record snowfall for the city of Jacksonville is 1.9 inches (4.8 cm), which fell on February 12, 1899. Tampa has a record snowfall of 0.2 inches (5.08 mm) which occurred on January 18, 1977.\n\nDue to larger populations and more advanced communication networks, snow events are witnessed and reported much more frequently in recent years than in historical eras. Interpretations of this timeline must therefore be made with caution, as observed patterns may not reflect actual climate-related trends in annual snowfall but rather improved reporting. Additionally, the presence of rime or sleet being mistaken for snowflakes should also be considered. Finally, many of the reports below or not \"official\" National Weather Service reports, many being compiled by the newspapers and media, personal observations, and stories passed down through the years. \n\n\n\n\n"}
{"id": "27168695", "url": "https://en.wikipedia.org/wiki?curid=27168695", "title": "Straight Up (book)", "text": "Straight Up (book)\n\nStraight Up: America's Fiercest Climate Blogger Takes on the Status Quo Media, Politicians, and Clean Energy Solutions is a book by author, blogger, physicist and climate expert Joseph J. Romm. A Fellow of the American Association for the Advancement of Science and former Acting Assistant Secretary of the U.S. Department of Energy, Romm writes about methods of reducing global warming and increasing energy security through energy efficiency, green energy technologies and green transportation technologies.\n\nRomm writes and edits the climate blog ClimateProgress.org for the Center for American Progress, where he is a Senior Fellow. \"Time\" magazine named this blog one of the \"Top 15 Green Websites\" and called Romm \"The Web's most influential climate-change blogger\", naming him as one of its \"Heroes of the Environment (2009)\".\n\n\"Straight Up\" was released on April 19, 2010 by Island Press. It is \"largely a selection of [Romm]'s best blog postings over the past few years related to climate change issues\". \"TreeHugger\" describes the book as \"a whirlwind tour through the state of climate change, the media that so badly neglects it, the politicians who attempt to address it (and those who obstruct their efforts and ignore [the] science), and the clean energy solutions that could help get us out of the mess.\"\n\nThe title of the book's introduction, \"Why I blog\", is a play on the title of George Orwell's essay, \"Why I Write\". Romm states, \"I joined the new media because the old media have failed us. They have utterly failed to force us to face unpleasant facts. From this starting point, Romm posits that global warming is a bipartisan issue. He writes, \"Averting catastrophic global warming requires completely overturning the \"status quo\", changing every aspect of how we use energy – and doing so in under four decades. Failure to do so means humanity's self-destruction.\" The book collects, reprints and updates postings from his blog, ClimateProgress.org, as the main part of his content, adding introductions and some new analysis.\n\nIn his first chapter, Romm argues that the media perpetuates the \"status quo\" through laziness and a misunderstanding of how to present a \"balanced\" story. For example, he believes that the media did a bad job of assessing the outcome of the Copenhagen summit in December 2009. Romm comments that global warming is a science story, but that the traditional news media, which has scaled back on specialized reporting, has given the story to political reporters who don't understand, and have not time to research, the scientific consensus. He next presents research concerning the science of climate change, as explained by what Romm calls \"uncharacteristically blunt scientists\".\n\nIn the third chapter, \"Straight Up\" presents proposed solutions to reducing greenhouse gas emissions through the use of clean energy technologies and other currently available technologies. For example it describes what Romm believes are the advantages of plug-in hybrid electric vehicles, generation of energy through wind and solar power, including concentrated solar power using mirrors to concentrate the sun's energy. He writes that \"A 20 percent reduction in global emissions might be possible in a quarter century with net economic benefits\". \"Our plan\", says Romm, must be \"Deployment, deployment, deployment, R&D, deployment, deployment, deployment.\" The next chapter discusses peak oil.\n\nThe next chapters move into the politics of global warming and what Romm sees as a \"right-wing disinformation machine\" that confuses and misleads the public, by, for example, fostering what Romm calls \"Anti-Scientific Syndrome\". The book says, \"the economic cost of action is low, whereas the cost of inaction is incalculably greater – what exactly is the 'price' of 5 feet of sea level rise in 2100 … and losing all of the inland glaciers that provide a significant fraction of water to a billion people? Or the price of losing half the world's species? ... the bottom line is that the economic cost of action is low, whereas the cost of inaction is incalculably greater\". Romm calculates that deployment of existing technologies on the massive scale that can save the climate can be accomplished at the cost of 0.12 percent of global GDP per year.\n\nRomm advocates citizen action to pressure Washington and industry to act quickly and decisively to reduce greenhouse emissions. Otherwise, he argues, we will fall behind in the race to commercialize profitable technologies. \"China has a excellent track record of achieving gains in energy efficiency and has begun to ramp up its efficiency efforts and aggressively expand its carbon-free electricity targets (recently committing, for instance, to triple its wind goal to 100,000 MW by 2020). ... will the United States be a global leader in creating jobs and exports in clean energy technologies or will we be importing them from Europe, Japan, and the likely clean energy leader in our absence, China\"?\n\nIn the last chapter, Romm posits that progressives are \"lousy\" at educating the public, and he offers ways in which he thinks they can be more effective at messaging. In his conclusion at the end of the book, Romm argues that the global economy is a sort of ponzi scheme, in which our failure to prevent the worst effects of climate change now could eventually cause the world economy to fall apart just like a ponzi scheme.\n\nA review in \"USA Today\" called the book \"a gut-wrenching wakeup call\". Thomas L. Friedman, in his op-ed column in \"The New York Times\", called the book \"insightful\", agreeing with Romm's arguments in the book that the proposed \"cap and trade\" climate bill \"is a step in the right direction toward reducing greenhouse gases and expanding our base of clean power technologies\". Former U.S. Vice-President Al Gore endorsed the book as \"important\" on his blog, writing, \"If you are interested in the fight to solve the climate crisis, I recommend you read this book.\"\n\nThe book has been reviewed by many of the \"green\" websites. For example, the blog of the American Solar Energy Society, \"Solar Today\", commented, \"It's a collection of spirited and readable critiques of the delaying forces – the corporations and institutions who want to see no changes in national policies and tax codes that now work to make them rich. In particular, Romm eviscerates the American news establishment for ignoring climate catastrophe issues\". ... It's full of solid fact-based arguments, properly referenced within the text (no footnotes!), along with a lot of low-carbon fire and brimstone. \"Daily Kos\" commented, \"Romm's forceful, impassioned blogging – and his book publishing – are a shining light in the confrontation of those 'misguided seals of approval'\" being given out by the mainstream press to climate disinformers. The review continues, \"Romm is a tenacious fighter ... ready to take on all comers to the point that he can even rub 'friends' and allies the wrong way at times. ... Romm's knowledge, writing skills, and passion enable most to see past those conflicts since, on so many issues, Romm is simply – well – correct and laying out viable paths forward. ... Simply put, if the 'nation' would read \"Straight Up\" and follow Romm's prescriptions, we would find ourselves moving away from decline into a new era of prosperity.\" The \"Green Energy Reporter\" stated that \"\"Straight Up's\" indictment of \"status-quo media\" like \"The New York Times\" lays bare the inadequacies of traditional he-said-she-said media coverage when faced with a civilizational challenge like climate change. ... Strong opinions, muscular writing.\"\n\nReporter Tyler Hamilton calls the book \"a stinging critique of how poorly the mainstream media has covered global warming\" and says that the book, like Romm's blog, \"cuts through the crap in a way no mainstream media outlet has or will.\" A review in TreeHugger termed the book \"an essential guide to climate, energy, and politics for the blog era.\" It continued, \"nobody knows the game like Romm – both in terms of ability to interpret and explain the latest science, and in boasting expertise on the politics and policy process that, whether you like it our not, is going to be instrumental in mitigating climate change on a large scale.\" Even for readers of Romm's blog, the book \"provides an important narrative flow, and condenses everything you need to know about the current state of climate science and politics into a nice, quick read. While extremely thorough, it may make some beginners' head spin, and it can get combative and wonky in places. But such is the nature of this beast – climate change \"should\" make a beginner's head spin, and as Romm makes clear, addressing it is going to be messy, politically charged, and a daunting battle.\" The \"Environmental Defense Fund\" review opined, \"\"Straight Up\" is well-researched, provides insightful political analysis, and showcases compelling data on the economic benefits of climate change solutions.\"\n\nRoss Gelbspan reviewed the book for \"Grist\" magazine, writing that Romm's \"unfailing sense of priorities shines through his startlingly thoughtful and brutally blunt writing.\" Gelbspan continues, \"while one wishes Romm would have stitched the blog posts together into a more coherent narrative – and omitted a few that addressed transitory, fleeting events – his book is absolutely on point in its insistence that climate change long ago ceased to be a scientific issue and, instead, is most clearly a political one.\" Gelbspan agrees with Romm that \"a central reason that most political conservatives and libertarians deny the reality of human-induced climate change 'is that they simply cannot stand the solution. So they attack both the solution and the science.' I don't recall reading that simple truth in [traditional media,] virtually all of which treat the climate debate as though it actually had some legitimacy.\" He also agrees with Romm that the major media \"have failed, in the name of 'journalistic balance', to distinguish between legitimate, peer-reviewed scientific research and the deliberate obfuscation by a cadre of climate skeptics, many of whom have been funded by coal and oil companies. As a result, the public has no idea that we are already at a point of no return in terms of staving off climate chaos.\" Gelbspan notes, \"Romm happens to favor both efficiency and concentrated solar thermal power. But, his technological preferences aside, he's right on point when he describes the call for more R&D as a stalling tactic to avoid coming to grips with the threat. As Romm writes, 'deployment completely trumps research'.\" However, Gelbspan criticizes Romm for \"wandering\", at the end of the book, \"into the question of why climate advocates are so bad at 'messaging.' It may be a valid question. ... But I'm afraid the issue [is] a diversion from the real question facing all of us at this moment of history. ... We are already beginning to see crop failures, water shortages, increasing extinctions, migrations of environmental refugees, and all manner of potential breakdowns in our social lives. Where \"Straight Up\" falls short is in its failure to deal with this reality head on.\" Still, he says, \"This is not at all to minimize the value of Romm's book. To the contrary, if you think the most pressing task today is to limit the coming damage through a transition to non-carbon technologies, I can't think of a better place to start than by reading \"Straight Up\".\"\n\nIn June 2010, FDL Book Salon said of the book, \"the whole is greater than the sum of the parts. By pulling together the very best content from the blog and thoughtfully organizing it in a logical way, the book achieves ... cohesiveness. ... [What makes Romm's] writing on climate change and energy policy so valuable is his comprehensive knowledge of the subject matter.\" A review the same month in the New Zealand climate blog \"Hot Topic\" contains a detailed summary of the book. A July 2010 review in RenewableEnergyWorld.com agreed with Romm that \"with the little time we have left to avert climate chaos, we must devote most of our resources to deploying existing technologies like solar, wind and geothermal that we know can bring atmospheric carbon back down down to safe levels.\"\n\nIn July 2010, Bill McKibben wrote in \"Washington Monthly\":\n\n\n"}
{"id": "941923", "url": "https://en.wikipedia.org/wiki?curid=941923", "title": "Sudines", "text": "Sudines\n\nSudines (or Soudines) (Greek: Σουδινες) (fl. c. 240 BC): Babylonian sage. He is mentioned as one of the famous Chaldean mathematicians and astronomer-astrologers by later Roman writers like Strabo (\"Geografia\" 16:1–6).\n\nLike his predecessor Berossos, he moved from Babylonia and established himself among the Greeks; he was an advisor to King Attalus I (Attalos Soter) of Pergamon. He is said (e.g. by Roman astronomer/astrologer Vettius Valens) to have published tables to compute the motion of the Moon; said to have been used by the Greeks, until superseded by the work of Hipparchus and later by Ptolemy (Claudius Ptolemaios). Soudines may have been important in transmitting the astronomical knowledge of the Babylonians to the Greeks, but little is known about his work and nothing about his life. He is also said to have been one of the first to assign astrological meaning to gemstones.\n\n"}
{"id": "11059542", "url": "https://en.wikipedia.org/wiki?curid=11059542", "title": "The Malay Archipelago", "text": "The Malay Archipelago\n\nThe Malay Archipelago is a book by the British naturalist Alfred Russel Wallace which chronicles his scientific exploration, during the eight-year period 1854 to 1862, of the southern portion of the Malay Archipelago including Malaysia, Singapore, the islands of Indonesia, then known as the Dutch East Indies, and the island of New Guinea. It was published in two volumes in 1869, delayed by Wallace's ill health and the work needed to describe the many specimens he brought home. The book went through ten editions in the nineteenth century; it has been reprinted many times since, and has been translated into at least eight languages.\n\nThe book describes each island that he visited in turn, giving a detailed account of its physical and human geography, its volcanoes, and the variety of animals and plants that he found and collected. At the same time, he describes his experiences, the difficulties of travel, and the help he received from the different peoples that he met. The preface notes that he travelled over 14,000 miles and collected 125,660 natural history specimens, mostly of insects though also thousands of molluscs, birds, mammals and reptiles.\n\nThe work was illustrated with engravings, based on Wallace's observations and collection, by the leading illustrators Thomas Baines, Walter Hood Fitch, John Gerrard Keulemans, E. W. Robinson, Joseph Wolf and T. W. Wood.\n\n\"The Malay Archipelago\" attracted many reviews, with interest from scientific, geographic, church and general periodicals. Reviewers noted and sometimes disagreed with various of his theories, especially the division of fauna and flora along what soon became known as the Wallace line, natural selection and uniformitarianism. Nearly all agreed that he had provided an interesting and comprehensive account of the geography, natural history, and peoples of the archipelago, which was little known to their readers at the time, and that he had collected an astonishing number of specimens. The book is much cited, and is Wallace's most successful, both commercially and as a piece of literature.\n\nIn 1847, Wallace and his friend Henry Walter Bates, both in their early twenties, agreed that they would jointly make a collecting trip to the Amazon \"towards solving the problem of origin of species\". (Charles Darwin's book on the \"Origin of Species\" was not published until 11 years later, in 1859. It was based on Darwin's own long collecting trip on HMS Beagle, its publication precipitated by a famous letter from Wallace, sent during the period covered by \"The Malay Archipelago\" while he was staying in Ternate, which described the theory of evolution by natural selection in outline.) Wallace and Bates had been inspired by reading the American entomologist William Henry Edwards's pioneering 1847 book \"A Voyage Up the River Amazon, with a residency at Pará\". Bates stayed in the Amazons for 11 years, going on to write \"The Naturalist on the River Amazons\" (1863); however, Wallace, ill with fever, went home in 1852 with thousands of specimens, some for science and some for sale. The ship and his collection were destroyed by fire at sea near the Guianas. Rather than giving up, Wallace wrote about the Amazon in both prose and poetry, and then set sail again, this time for the Malay Archipelago.\n\nThe preface summarises Wallace's travels, the thousands of specimens he collected, and some of the results from their analysis after his return to England. In the preface he notes that he travelled over 14,000 miles and collected 125,660 specimens, mostly of insects: 83,200 beetles, 13,100 butterflies and moths, 13,400 other insects. He also returned to England 7,500 \"shells\" (such as molluscs), 8,050 birds, 310 mammals and 100 reptiles.\n\nThe book is dedicated to Charles Darwin, but as Wallace explains in the preface, he has chosen to avoid discussing the evolutionary implications of his discoveries. Instead he confines himself to the \"interesting facts of the problem, whose solution is to be found in the principles developed by Mr. Darwin\", so from a scientific point of view, the book is largely a descriptive natural history. This modesty belies the fact that while in Sarawak in 1855 Wallace wrote the paper \"On the Law which has Regulated the Introduction of New Species\", concluding with the evolutionary \"Sarawak Law\", \"Every species has come into existence coincident both in space and time with a closely allied species\", three years before he fatefully wrote to Darwin proposing the concept of natural selection.\n\nThe first chapter describes the physical geography and geology of the islands with particular attention to the role of volcanoes and earthquakes. It also discusses the overall pattern of the flora and fauna including the fact that the islands can be divided, by what would eventually become known as the Wallace line, into two parts, those whose animals are more closely related to those of Asia and those whose fauna is closer to that of Australia.\n\nThe following chapters describe in detail the places Wallace visited. Wallace includes numerous observations on the people, their languages, ways of living, and social organisation, as well as on the plants and animals found in each location. He talks about the biogeographic patterns he observes and their implications for natural history, in terms both of the movement of species and of the geologic history of the region. He also narrates some of his personal experiences during his travels. The final chapter is an overview of the ethnic, linguistic, and cultural divisions among the people who live in the region and speculation about what such divisions might indicate about their history.\n\n\"The Malay Archipelago\" was largely written at Treeps, Wallace's wife's family home in Hurstpierpoint, West Sussex. It was first published in Spring 1869 as a single volume first edition, however was reprinted in two volumes by Macmillan (London), marked second edition the same year by Harper & Brothers (New York). Wallace returned to England in 1862, but explains in the Preface that given the large quantity of specimens and his poor health after his stay in the tropics, it took a long time. He noted that he could at once have printed his notes and journals, but felt that doing that would have been disappointing and unhelpful. Instead, therefore, he waited until he had published papers on his discoveries, and other scientists had described and named as new species some 2,000 of his beetles (Coleoptera), and over 900 Hymenoptera including 200 new species of ant. The book went through 10 editions, with the last published in 1890.\n\nThe illustrations are, according to the \"Preface\", made from Wallace's own sketches, photographs, or specimens. Wallace thanks Walter and Henry Woodbury for some photographs of scenery and native people. He acknowledges William Wilson Saunders and Mr Pascoe for horned flies and very rare longhorn beetles: all the rest were from his own enormous collection.\n\nThe original drawings were made directly on to the wood engraving blocks by leading artists Thomas Baines, Walter Hood Fitch, John Gerrard Keulemans, E. W. Robinson, Joseph Wolf, and T. W. Wood, according to the \"List of Illustrations\". Wood also illustrated Darwin's \"The Descent of Man\", while Robinson and Wolf both also provided illustrations for \"The Naturalist on the River Amazons\" (1863), written by Wallace's friend Henry Walter Bates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"The Malay Archipelago\" was warmly received on publication, often in lengthy reviews that attempted to summarise the book, from the perspective that suited the reviewing periodical. It was reviewed in more than 40 periodicals: a selection of those reviews is summarised below.\n\nThe \"Anthropological Review\" notes that while the descriptions of animal life are \"full of interest\", \"our readers, as anthropologists, will, however, take a keener interest\" in the \"great man-like ape of Borneo,—the orang-utan, or \"mias\", as it is called by the aborigines\". Two pages are taken up with a discussion of the orang utan. The review then turns to Wallace's observations on \"the races of man\" in the book, observing that the anthropological details given are useful but perhaps chosen to support \"a particular theory\", namely Wallace's belief that there were eastern and western races—\"Malays\" and \"Papuans\", though the boundary between them was east of the Wallace line. The review accepts Wallace's data on natural history, but suspects he was selective in recording details of individuals. It notes that Wallace agreed with French authors that the Polynesians (included in his Papuans) \"had a local origin\". The review remarks that \"Mr Wallace relies more on the diversity of moral features to prove differences of race than on physical peculiarities, although he declares that these are strongly marked\" and doubts the difference, and wonders whether the \"Javan chief\" and the Dyak do not differ more. The review, after ten pages of reflections on race, concludes by recommending the book to its readers as much better than ordinary travel books \"and even in the absence of any very stirring incidents\" that it will \"amply repay the perusal\" of both scientific and general readers.\n\nThe \"Journal of the Ethnological Society of London\" focussed exclusively on the ethnology in the book, praising the value both of the information and of Wallace's \"thoughtful and suggestive speculation\". The review notes that Wallace identified two \"types of mankind\" in the archipelago, \"the Malayan and the Papuan\", and that he thought these two had \"no traceable affinity to each other\". It remarks that Wallace greatly extends knowledge of the people of Timor, Celebes, and the Maluccas, while also adding to what is known of the Malays and Papuans, reprinting his entire description and his engraving of a Papuan. The reviewer remarks that the portrait \"would as well suit a Papuan of the south-east coast of New Guinea as any of those whom Mr. Wallace saw\", noting however that the southern tribes are more varied in skin colour. The reviewer disagrees with Wallace about the extension of this \"Papuan race\" as far as Fiji, noting that there are or were people like that in Tasmania, but that their features and height varied widely, perhaps forming a series. The reviewer disagrees also that the Sandwich Islanders and \"New Zealanders\" (Maori) are related to the Papuans; and with Wallace's claim that the presence of Malay words in Polynesian languages is caused by the \"roaming habits\" — trade and navigation – of the Malays, arguing instead that the Polynesians long ago migrated from \"some common seat in, or near, the Malay Archipelago\". The review ends by stating that despite all these disagreements, it holds Wallace's ethnology in \"high estimation\".\n\nSir Roderick Murchison, giving a speech at the \"Royal Geographical Society\", felt able to \"feel a pride\" in Wallace's success, and in the \"striking contributions\" made to science. He takes interest in \"Wallace's line\" which he calls \"this ingenious speculation\", with \"the two faunas wonderfully contrasted\" either side of the deep channel between Borneo and Celebes, or Bali and Lombok. He points out the same principle between the British Isles and continental Europe, though there the conclusion is rather that the same fauna and flora is found on both sides. However, Murchison states his disagreement with Wallace's support for James Hutton's principle of uniformitarianism, that \"\"all\" former changes of the outline of the earth were produced slowly\", opining that the Bali–Lombok channel probably formed suddenly. He mentions in one sentence that the book contains \"interesting and important facts\" on physical geography, native inhabitants, climate and products of the archipelago, and describes Wallace as a great naturalist and a \"most attractive writer\".\n\nOne of the shortest reviews was in \"The Ladies' Repository\", which found it\n\nThe reviewer notes the region is \"of terrific grandeur, parts of it being perpetually illuminated by discharging volcanoes, and all of it frequently shaken with earthquakes.\" The review summarises the book's geographical reach and style in a paragraph.\n\n\"The Popular Science Review\" began by writing that \"We never remember to have taken up a book which gave us more pleasure\". It was quite unlike the dull journey logs of most travel books; it was \"a romance, which is, nevertheless, plain matter of fact\". The review especially admires the way that Wallace \"has generalised on the facts\" rather than just shooting \"a multitude of birds\" and interminably describing them. The account notes that Wallace was the joint originator of the theory of natural selection, and summarises the discovery of the Wallace line in some detail. The review ends by placing the \"Malay Archipelago\" between Charles Lyell's \"Principles of Geology\" and Darwin's \"Origin of Species\".\n\nThe \"American Quarterly Church Review\" admires Wallace's bravery in going alone among the \"barbarous races\" in a \"villainous climate\" with all the hardships of travel, and his hard work in skinning, stuffing, drying and bottling so many specimens. Since \"As a scientific man he follows Darwin\" the review finds \"his theories sometimes need as many grains of salt as his specimens.\" But the review then agrees that the book will \"make the world wiser about its more solitary and singular children, hid away over the seas\", and opines that no-one will mind paying the price of the book to read about the birds of paradise, \"those bird-angels, with flaming wings of crimson and gold and scarlet, who twitter and gambol and make merry among the great island trees, while the Malay hunts for them with his blunt-headed arrows...\" The review concludes that the book is a fresh and valuable record of \"a remote and romantic land\".\n\nThe \"Australian Town and Country Journal\" begins by stating that\n\nand quickly makes clear that it objects to Wallace's doubts about \"indications of design\" in plants. Despite this \"grave\" fault, the reviewer considers the book to be of immense value, and that it would become a standard work on the region. The review quotes a paragraph that paints \"a picture of country life in the Celebes\", where Wallace describes his host, a Mr. M., who relied on his gun to supply his table with wild pigs, deer, and jungle fowl, while enjoying his own milk, butter, rice, coffee, ducks, palm wine and tobacco. However, the Australian reviewer doubted Wallace's judgement about flavours, given that he praised the Durian fruit, namely that it tastes of custard, cream cheese, onion sauce, brown sherry \"and other incongruities\", whereas \"most Europeans\" found it \"an abomination\".\n\nOtherwise, the review notes that Wallace seemed to have enjoyed his time in the Celebes, with the hornbills flapping past, and the baboons staring down from their trees, and enjoys his enthusiasm for the birds of paradise. The review is respectful of his account of the Wallace line, having no difficulty agreeing that the Australian-type vegetation continues into the archipelago as far as Lombok and Celebes. It concludes that he covers almost every natural phenomenon he came across \"with the accuracy and discriminating sagacity of an accomplished naturalist\", and explains that the \"great charm\" of the book is \"a truthful simplicity\" which inspires confidence.\n\nThe \"Calcutta Review\" starts by noting that this is a book that cannot be done justice in a brief notice, that Wallace is a most eminent naturalist, and chiefly known as a Darwinian. the book was the most interesting to cross the reviewer's desk since Palgrave's \"Arabia\" (1865) and Sir Samuel Baker's \"Explorations of the Nile\" (1866). By combining geography, geology and ethnology into one narrative, the reader is saved \"the monotony of traversing the same regions several times\". The review describes in detail Wallace's findings of different birds and mammals either side of the Wallace line. It notes Wallace's cheerfulness and good temper in the face of \"the difficulties and inconveniences attendant upon foreign travel\", such as having to cross \"a hundred miles of open sea in a little boat of four tons burthen\", which Wallace calmly describes as comparatively comfortable. The reviewer remarks that Wallace was \"set down as a conjuror by these simple people\" with unimaginable purposes from a faraway country, but is less admiring about Wallace's moralising tone, especially when he supposes that \"wild communities\" can be happier than \"in a more highly civilised society\". The review ends with some reflections of surprise on how little-known the Malay Archipelago is in India, given that they were closely connected with Hindu temples in Java and Bali, and hopes that soon there will be some \"productions\" of the archipelago in the Indian Museum of Calcutta.\n\nThe book's fame spread beyond the English-speaking world. R. Radau wrote a lengthy review of \"Un naturaliste dans l'Archipel Malais\" in the French \"Revue des Deux Mondes\". Radau notes the many deaths from volcanic eruptions in the archipelago, before explaining the similarity of the fauna of Java and Sumatra with that of central Asia, while that of the Celebes carries the mark of Australia, seeming to be the last representatives of another age. Radau describes Wallace's experiences in Singapore, where goods were far cheaper than in Europe – wire, knives, corkscrews, gunpowder, writing-paper, and he remarks on the spread of the Jesuits into the interior, though the missionaries had to live on just 750 francs a year. Singapore was covered in wooded hills, and the sawn wood and rotten trunks supported innumerable beetles for the naturalist to study. The only disagreeable element was that the tigers that roared in the forest devoured on average one Chinese per day, especially in the ginger plantations.\n\nRadau summarises one passage from the book after another: the orang utans of Borneo wrestling open the jaws of a crocodile, or killing a python; the Timorese walking up tall trees, leaning back on ropes as they pull themselves upwards; the indescribable taste of a durian fruit, at once recalling custard, almond paste, roasted onions, sherry and a host of other things, that melts on the tongue, that one does not want to stop eating; more, the fruit has a repulsive odour, and the tree is dangerous, as the hard and heavy fruits can fall on your head. Radau follows Wallace up to the high plateaux of Java, where there are cypress forests covered in moss and lichen; finally at the summit the vegetation seems European, an island vegetation recalling the resemblance between the plants of the high Alps and of Lapland. And in Celebes, men run amok, generally killing a dozen people before meeting their own death.\n\nRadau returns to food, describing sago and the breadfruit tree. The breadfruit tastes like Yorkshire pudding or mashed potato; with meat it is the best of vegetables; with sugar, milk, butter or molasses, it is a delicious pudding with a special flavour; Radau hopes that perhaps it will one day be found in European markets. As for the sago palm, one tree yields 1,800 cakes, enough to feed a man for a year.\n\nThere is torrential rain; there are savages; there are dangerous trips in small boats. Only in the final paragraph does Radau reflect on it all: \"We have tried, in this study on Wallace's two volumes, to give an idea of what he saw in his eight year stay in the Far East.\" He admits he has left out most of the natural history, and regrets not having space for more \"charming pages\" which would have taken him too far. He joins Wallace in reflecting on the relative state of \"civilized\" and \"savage\", wondering which is morally superior, and notes the \"nostalgia for the primitive state\", concluding that civilisation brings the benefit of reason to restrain hasty action.\n\nTim Radford, writing in \"The Guardian\", considers that \"The Malay Archipelago\" shows Wallace to be \"an extraordinary figure\", since he is\n\nRadford finds \"delights on every page\", such as the Wallace line between the islands of Bali and Lombok; the sparkling observations, like \"the river bed 'a mass of pebbles, mostly pure white quartz, but with abundance of jasper and agate'\"; the detailed but lively accounts of natural history and physical geography; the respectful and friendly attitude to the native peoples such as the hill Dyaks of Borneo; and his unclouded observations of human society, such as the way a Bugis man in Lombok runs amok, where Wallace\n\nRobin McKie, in \"The Observer\", writes that the common view of Wallace \"as a clever, decent cove who knew his place\" as second fiddle to Charles Darwin is rather lopsided. Wallace, he writes, is \"capable of great insights\" in the \"Malay Archipelago\". Travelling over 14,000 miles and collecting 125,000 specimens, he also made \"scrupulous notes\" for the book which\n\nIn McKie's view, Wallace was a gifted writer with \"an eye for catchy observation\", and this is one of the finest of travel books. McKie liked the account of Wallace's night sleeping \"'with half-a-dozen smoke-dried human skulls suspended over my head'\".\n\nThe researcher Charles Smith rates the \"Malay Archipelago\" as \"Wallace's most successful work, literarily and commercially\", placing it second only to his \"Darwinism\" (1889) among his books for academic citations.\n\n\"The Malay Archipelago\" influenced many works starting with those of Wallace's contemporaries. The novelist Joseph Conrad used it as source material for some of his novels, including \"Almayer's Folly\", \"An Outcast of the Islands\", and \"The Rescue\". Commentators have suggested it had a particularly profound influence on \"Lord Jim\", crediting it with among other things the inspiration for the character Stein the entomologist. Conrad's assistant Richard Curle wrote that \"The Malay Archipelago\" was Conrad's favourite bedside book; Conrad refers directly to what he calls Alfred Wallace's famous book on the Malay Archipelago in The Secret Agent. In his short story, \"Neil MacAdam\", W. Somerset Maugham has the title character read \"The Malay Archipelago\" while travelling to Borneo, and its influence can be felt in the story's description of that island.\n\nMore recently, the book has influenced a number of non-fiction books including \"The Song of the Dodo\" by David Quammen (1997), which discussed Wallace's contributions to the field of island biogeography; \"The Spice Islands Voyage\" by Tim Severin (1997) that retraced Wallace's travels; and \"Archipelago: The Islands of Indonesia\", by Gavan Daws (1999), which compared the environment described by Wallace with the modern state of the archipelago. \"The Malay Archipelago\" is considered to be one of the most influential books ever written about the Indonesian islands. It remains a resource for modern authors of works about the region such as the 2014 book \"Indonesia Etc\", which contains multiple quotations from Wallace's book as well as recommending it as further reading on the geography of Indonesia.\n\nThe English comedian Bill Bailey travelled around Indonesia in the footsteps of Wallace for a two-part television programme on BBC Two, first broadcast in 2013, the centenary of Wallace's death.\n\nEach edition was reprinted in subsequent years, so for example the tenth edition appeared in 1890, 1893, 1894, 1898, 1902, 1906 and later reprints, so many different dates can be found in library catalogues.\n\n\n"}
{"id": "38948080", "url": "https://en.wikipedia.org/wiki?curid=38948080", "title": "Trained incapacity", "text": "Trained incapacity\n\nIn sociology, trained incapacity is \"that state of affairs in which one's abilities function as inadequacies or blind spots.\" It means that people's past experiences can lead to wrong decisions when circumstances change. Thorstein Veblen invented the concept in 1933.\n"}
{"id": "23380499", "url": "https://en.wikipedia.org/wiki?curid=23380499", "title": "ULAS J003402.77−005206.7", "text": "ULAS J003402.77−005206.7\n\nULAS J003402.77-005206.7 (also ULAS J0034-00) is a T-type brown dwarf in the constellation of Cetus.\n\nULAS J0034-00 is one of the coolest brown dwarfs known. It was first identified in data from the UK Infrared Telescope (UKIRT) Infrared\nDeep Sky Survey (UKIDSS). Infrared spectra subsequently taken with the IRS instrument on the Spitzer Space Telescope give an estimated effective temperature of between 550 and 600 K and does not emit any visible light. Its mass is estimated at between 5 and 20 Jupiter masses and its age at between 0.1 and 2.0 billion years.\n"}
{"id": "1194470", "url": "https://en.wikipedia.org/wiki?curid=1194470", "title": "Value of information", "text": "Value of information\n\nValue of information (VOI or VoI) is the amount a decision maker would be willing to pay for information prior to making a decision.\n\nVoI is sometimes distinguished into value of perfect information, also called value of clairvoyance (VoC), and value of imperfect information. They are closely related to the widely known expected value of perfect information and expected value of sample information. Note that VoI is not necessarily equal to \"value of decision situation with perfect information\" - \"value of current decision situation\" as commonly understood.\n\nA simple example best illustrates the concept. Consider the decision situation with one decision, for example deciding on a 'Vacation Activity'; and one uncertainty, for example what will the 'Weather Condition' be? But we will only know the 'Weather Condition' after we have decided and begun the 'Vacation Activity'.\n\nThe above definition illustrates that the value of imperfect information of any uncertainty can always be framed as the value of perfect information, i.e., VoC, of another uncertainty, hence only the term VoC will be used onwards.\n\nConsider a general decision situation having \"n\" decisions (\"d\", \"d\", \"d\", ..., \"d\") and \"m\" uncertainties (\"u\", \"u\", \"u\", ..., \"u\"). Rationality assumption in standard individual decision-making philosophy states that what is made or known are not forgotten, i.e., decision-has perfect recall. This assumption translates into the existence of a linear ordering of these decisions and uncertainties such that;\n\nConsider cases where the decision-maker is enabled to know the outcome of some additional uncertainties earlier in his/her decision situation, i.e., some \"u\" are moved to appear earlier in the ordering. In such case, VoC is quantified as the highest price which the decision-maker is willing to pay for all those moves.\n\nThe standard then is further generalized in team decision analysis framework where there is typically incomplete sharing of information among team members under the same decision situation. In such case, what is made or known might not be known in later decisions belonging to different team members, i.e., there might not exist linear ordering of decisions and uncertainties satisfying perfect recall assumption. VoC thus captures the value of being able to know \"not only additional uncertainties but also additional decisions already made by other team members\" before making some other decisions in the team decision situation.\n\nThere are two extremely important characteristics of VoI that always hold for any decision situation:\n\nVoC is derived strictly following its definition as the monetary amount that is big enough to just offset the additional benefit of getting more information. In other words; VoC is calculated iteratively until\n\nA special case is when the decision-maker is risk neutral where VoC can be simply computed as\n\nThis special case is how expected value of perfect information and expected value of sample information are calculated where risk neutrality is implicitly assumed. For cases where the decision-maker is risk averse or risk seeking, this simple calculation does not necessarily yield the correct result, and iterative calculation is the only way to ensure correctness.\n\nDecision trees and influence diagrams are most commonly used in representing and solving decision situations as well as associated VoC calculation. The influence diagram, in particular, is structured to accommodate team decision situations where incomplete sharing of information among team members can be represented and solved very efficiently. While decision trees are not designed to accommodate team decision situations, they can do so by augmenting them with information sets widely used in game trees.\n\nVoC is often illustrated using the example of paying for a consultant in a business transaction, who may either be perfect (expected value of perfect information) or imperfect (expected value of imperfect information).\n\nIn a typical consultant situation, the consultant would be paid up to cost \"c\" for their information, based on the expected cost \"E\" without the consultant and the revised cost \"F\" with the consultant's information. In a perfect information scenario, \"E\" can be defined as the sum product of the probability of a good outcome \"g\" times its cost \"k\", plus the probability of a bad outcome (1-\"g\") times its cost \"k\"'>k:\n\n\"E\" = \"gk\" + \"(1-g)k',\"\n\nwhich is revised to reflect expected cost \"F\" of perfect information including consulting cost \"c\". The perfect information case assumes the bad outcome does not occur due to the perfect information consultant.\n\n\"F\" = \"g(k+c)\"\n\nWe then solve for values of \"c\" for which \"F<E\" to determine when to pay the consultant.\n\nIn the case of a recursive decision tree, we often have an additional cost \"m\" that results from correcting the error, and the process restarts such that the expected cost will appear on both the left and right sides of our equations. This is typical of hiring-rehiring decisions or value chain decisions for which assembly line components must be replaced if erroneously ordered or installed:\n\n\"E\" = \"gk\" + \"(1-g)(k'+m+E)\"\n\n\"F\" = \"g(k+c)\"\n\nIf the consultant is imperfect with frequency \"f\", then the consultant cost is solved with the probability of error included:\n\n\"F\" = \"g(k+c)(1-f) + g(k+c+F)f + (1-g)(1-f)(k+c+F) + (1-g)f(k'+c+m+F)\"\n\n"}
