{"id": "30944095", "url": "https://en.wikipedia.org/wiki?curid=30944095", "title": "Alan Hastings", "text": "Alan Hastings\n\nAlan Hastings is a mathematical ecologist and distinguished professor in the Department of Environmental Science and Policy at the University of California, Davis. In 2005 he became a fellow of the American Academy of Arts and Sciences and in 2006 he won the Robert H. MacArthur Award.\n\nIn 2008 he founded the journal \"Theoretical Ecology\", in which he currently holds the position of editor in chief. Formerly, he was co-editor in chief of the \"Journal of Mathematical Biology\". His research expands through many areas in theoretical ecology including spatial ecology, biological invasions, structured populations, and model fitting.\n\n"}
{"id": "18463895", "url": "https://en.wikipedia.org/wiki?curid=18463895", "title": "Armen Abaghian", "text": "Armen Abaghian\n\nArmen Artavazdi Abaghian (; January 1, 1933 in Stepanakert, Nagorno-Karabakh – November 18, 2005 in Moscow, Russia) was a Russian-Armenian specialist on nuclear power, Doctor of Technical Sciences, Professor (1985), Corresponding Member of the Russian Academy of Sciences. In 1984 he became the general director of \"Energy\" scientific and industrial state holding and the director of All-Soviet (then Russian) Institute of Atomic Energy Stations (AES). Then he became the deputy director of Rosenergoatom, a member of IAEA Consultative Committee.\n\nHe finished the Moscow engineero-physical institute in 1956. His works are dedicated to the mathematical models of AES blocks, their security and anti-crisis information data-centers.\n\n\n"}
{"id": "2201259", "url": "https://en.wikipedia.org/wiki?curid=2201259", "title": "Ballistic coefficient", "text": "Ballistic coefficient\n\nIn ballistics, the ballistic coefficient (BC) of a body is a measure of its ability to overcome air resistance in flight. It is inversely proportional to the negative acceleration: a high number indicates a low negative acceleration—the drag on the vehicle or projectile is small in proportion to its mass.\n\nWhere:\n\nThe formula for calculating the ballistic coefficient for small and large arms projectiles \"only\" is as follows:\n\nWhere:\n\nThe Coefficient of form (\"i\") can be derived by 6 methods and applied differently depending on the trajectory models used: G Model, Beugless/Coxe; 3 Sky Screen; 4 Sky Screen; Target Zeroing; Doppler radar.\n\nHere are several methods to compute i or C:\n\nWhere:\n\nor\n\nA drag coefficient can also be calculated mathematically: \n\nWhere:\n\nor\n\nFrom standard physics as applied to “G” models:\n\nWhere:\n\nThis formula is for calculating the ballistic coefficient within the smalls arms shooting community, but is redundant with BC:\n\nWhere:\n\nIn 1537, Niccolò Tartaglia did some test firing to determine the maximum angle and range for a shot. His conclusion was near 45 degrees. He noted that the shot trajectory was continuously curved.\n\nIn 1636, Galileo Galilei published results in \"Dialogues Concerning Two New Sciences\". He found that a falling body had a constant acceleration. This allowed Galileo to show that a bullet's trajectory was a curve.\n\nCirca 1665, Sir Isaac Newton derived the law of air resistance. Newton's experiments on drag were through air and fluids. He showed that drag on shot increases proportionately with the density of the air (or the fluid), cross sectional area, and the square of the speed. Newton's experiments were only at low velocities to about .\n\nIn 1718, John Keill challenged the Continental Mathematica, \"“To find the curve that a projectile may describe in the air, on behalf of the simplest assumption of gravity, and the density of the medium uniform, on the other hand, in the duplicate ratio of the velocity of the resistance”\". This challenge supposes that air resistance increases exponentially to the velocity of a projectile. Keill gave no solution for his challenge. Johann Bernoulli took up this challenge and soon thereafter solved the problem and air resistance varied as “any power” of velocity; known as the Bernoulli equation. This is the precursor to the concept of the “standard projectile”.\n\nIn 1742, Benjamin Robins invented the ballistic pendulum. This was a simple mechanical device that could measure a projectile's velocity. Robins reported muzzle velocities ranging from to . In his book published that same year “New Principles of Gunnery”, he uses numerical integration from Euler's method and found that air resistance varies as the square of the velocity, but insisted that it changes at the speed of sound.\n\nIn 1753, Leonhard Euler showed how a theoretical trajectories might be calculated using his method as applied to the Bernoulli equation, but only for resistance varying as the square of the velocity.\n\nIn 1844, the Electro-ballistic chronograph was invented and by 1867 the electro-ballistic chronograph was accurate to within one ten millionth of a second.\n\nMany countries and their militaries carried out test firings from the mid eighteenth century on using large ordnance to determine the drag characteristics of each individual projectile. These individual test firings were logged and reported in extensive ballistics tables.\n\nOf the test firing, most notably were: Francis Bashforth at Woolwich Marshes & Shoeburyness, England (1864-1889) with velocities to and M. Krupp (1865–1880) of Friedrich Krupp AG at Meppen, Germany, Friedrich Krupp AG continued these test firings to 1930; to a lesser extent General Nikolai V. Mayevski, then a Colonel (1868–1869) at St. Petersburg, Russia; the Commission d'Experience de Gâvre (1873 to 1889) at Le Gâvre, France with velocities to and The British Royal Artillery (1904–1906).\n\nThe test projectiles (shot) used, vary from spherical, spheroidal, ogival; being hollow, solid and cored in design with the elongated ogival-headed projectiles having 1, 1½, 2 and 3 caliber radii. These projectiles varied in size from, at to at \n\nMany militaries up until the 1860s used calculus to compute projectile trajectory. The numerical computations necessary to calculate just a single trajectory was lengthy, tedious and done by hand. So, investigations to develop a theoretical drag model began. The investigations led to a major simplification in the experimental treatment of drag. This was the concept of a “standard projectile”. The ballistic tables are made up for a factitious projectile being defined as: \"a factitious weight and with a specific shape and specific dimensions in a ratio of calibers.\" This simplifies calculation for the ballistic coefficient of a standard model projectile, which could mathematically move through the standard atmosphere with the same ability as any actual projectile could move through the actual atmosphere.\n\nIn 1870, Bashforth publishes a report containing his ballistic tables. Bashforth found that the drag of his test projectiles varied with the square of velocity (v) from to and with the cube of velocity (v) from to . As of his 1880 report, he found that drag varied by v from to . Bashforth used rifled guns of , , and ; smooth-bore guns of similar caliber for firing spherical shot and howitzers propelled elongated projectiles having an ogival-head of 1½ caliber radius.\n\nBashforth uses \"b\" as the variable for ballistic coefficient. When \"b\" is equal to or less than v, then \"b\" is equal to \"P\" for the drag of a projectile. It would be found that air does not deflect off the front of a projectile in the same direction, when there are of differing shapes. This prompted the introduction of a second factor to \"b\", the coefficient of form (\"i\"). This is particularly true at high velocities, greater than . Hence, Bashforth introduced the “undetermined multiplier” of any power called the formula_11 factor that compensate for this unknown effects of drag above ; formula_12. Bashforth then integrated formula_11 and formula_14 as formula_15.\n\nAlthough Bashforth did not conceive the “restricted zone”, he showed mathematically there were 5 restricted zones. Bashforth did not propose a standard projectile, but was well aware of the concept.\n\nIn 1872, General Mayevski published his report \"Trité Balistique Extérieure\", which included the Mayevski model. Using his ballistic tables along with Bashforth's tables from the 1870 report, Mayevski created an analytical math formula that calculated the air resistances of a projectile in terms of log A and the value \"n\". Although Mayevski's math used a differing approach than Bashforth, the resulting calculations of air resistance was the same. Mayevski proposed the restricted zone concept and found there to be 6 restricted zones for projectiles.\n\nCirca 1886, General Mayevski published the results from a discussion of experiments made by M. Krupp (1880). Though the ogival-headed projectiles used varied greatly in caliber, they had essentially the same proportions as the standard projectile, being mostly 3 caliber in length, with an ogive of 2 calibers radius. Giving the standard projectile dimensionally as and .\n\nIn 1880, Colonel Francesco Siacci published his work “Balistica”. Siacci found as did those who came before him that the resistance and density of the air becomes greater and greater as a projectile displaced the air at higher and higher velocities.\n\nSiacci's method was for flat-fire trajectories with angles of departure of less than 20 degrees. He found that the angle of departure is sufficiently small to allow for air density to remain the same and was able to reduce the ballistics tables to easily tabulated quadrants giving distance, time, inclination and altitude of the projectile. Using Bashforth's \"k\" and Mayevski's tables, Siacci created a 4 zone model. Siacci used Mayevski's standard projectile. From this method and standard projectile, Siacci formulated a shortcut.\n\nSiacci found that within a low velocity restricted zone, projectiles of similar shape, and velocity in the same air density behave similar; formula_16 or formula_17. Siacci used the variable formula_18 for ballistic coefficient. Meaning, air density is the generally the same for flat-fire trajectories, thus sectional density is equal to the ballistic coefficient and air density can be dropped. Then as the velocity rises to Bashforth's formula_11 for high velocity when formula_18 requires the introduction of formula_14. Following within today's currently used ballistic trajectory tables for an average ballistic coeficient: formula_22 would equal formula_23 equals formula_24 as formula_25.\n\nSiacci wrote that within any restricted zone, \"C\" being the same for two or more projectiles, the trajectories differences will be minor. Therefore, \"C\" agrees with an average curve, and this average curve applies for all projectiles. Therefore, a single trajectory can be computed for the standard projectile without having to resort to tedious calculus methods, and then a trajectory for any actual bullet with known \"C\" can be computed from the standard trajectory with just simple algebra.\n\nThe aforementioned ballistics tables are generally: functions, air density, projectile time at range, range, degree of projectile departure, weight and diameter to facilitate the calculation of ballistic formulae. These formulae produce the projectile velocity at range, drag and trajectories. The modern day commercially published ballistic tables or software computed ballistics tables for small arms, sporting ammunition are exterior ballistic, trajectory tables.\n\nThe 1870 Bashforth tables were to . Mayevski, using his tables, supplemented by the Bashforth tables (to 6 restricted zones) and the Krupp tables. Mayevski conceived a 7th restricted zone and extended the Bashforth tables to . Mayevski converted Bashforth's data from Imperial units of measure to metric units of measure (now in SI units of measure). In 1884, James Ingalls published his tables in the U.S. Army Artillery Circular M using the Mayevski tables. Ingalls extended Mayevski's ballistics tables to within an 8th restricted zone, but still with the same \"n\" value (1.55) as Mayevski's 7th restricted zone. Ingalls, converted Mayevski's results back to Imperial units. The British Royal Artillery results were very similar to those of Mayevski's and extended their tables to within the 8th restricted zone changing the \"n\" value from 1.55 to 1.67. These ballistic tables were published in 1909 and almost identical to those of Ingalls. In 1971 the Sierra Bullet company calculated their ballistic tables to 9 restricted zones but only within .\n\nIn 1881, the Commission d'Experience de Gâvre did a comprehensive survey of data available from their tests as well as other countries. After adopting a standard atmospheric condition for the drag data the Gavre drag function was adopted. This drag function was known as the Gavre function and the standard projectile adopted was the Type 1 projectile. Thereafter, the Type 1 standard projectile was renamed by Ballistics Section of Aberdeen Proving Grounds in Maryland, USA as G after the Commission d'Experience de Gâvre. For practical purposes the subscript 1 in G is generally written in normal font size as G1.\n\nThe general form for the calculations of trajectory adopted for the G model is the Siacci method. The standard model projectile is a \"fictitious projectile\" used as the mathematical basis for the calculation of actual projectile's trajectory when an initial velocity is known. The G1 model projectile adopted is in dimensionless measures of 2 caliber radius ogival-head and 3.28 caliber in length. By calculation this leaves the body length 1.96 caliber and head, 1.32 caliber long.\n\nOver the years there has been some confusion as to adopted size, weight and radius ogival-head of the G1 standard projectile. This misconception may be explained by Colonel Ingalls in the 1886 publication, Exterior Ballistics in the Plan Fire; page 15, \"In the following tables the first and second columns give the velocities and corresponding resistance, in pounds, to an elongated one inch in diameter and having an ogival head of one and a half calibers. They were deduced from Bashforth's experiments by Professor A. G. Greenhill, and are taken from his papers published in the Proceedings of the Royal Artillery Institution, No 2, Vol. XIII.\" Further it is discussed that said projectile's weight was one pound.\n\nFor the purposes of mathematical convenience for any standard projectile (G) the \"BC\" is 1.00. Where as the projectile's sectional density (SD) is dimensionless with a mass of 1 divided by the square of the diameter of 1 caliber equaling an SD of 1. Then the standard projectile is assigned a coefficient of form of 1. Following that formula_26. \"BC\", as a general rule, within flat-fire trajectory, is carried out to 2 decimal points. \"BC\" is commonly found within commercial publications to be carried out to 3 decimal points as few sporting, small arms projectiles rise to the level of 1.00 for a ballistic coefficient.\n\nWhen using the Siacci method for different G models, the formula used to compute the trajectories is the same. What differs is retardation factors found through testing of actual projectiles that are similar in shape to the standard project reference. This creates slightly different set of retardation factors between differing G models. When the correct G model retardation factors are applied within the Siacci mathematical formula for the same G model \"BC\", a corrected trajectory can be calculated for any G model.\n\nAnother method of determining trajectory and ballistic coefficient was developed and published by Wallace H. Coxe and Edgar Beugless of DuPont in 1936. This method is by shape comparison an logarithmic scale as drawn on 10 charts. The method estimates the ballistic coefficient related to the drag model of the Ingalls tables. When matching an actual projectile against the drawn caliber radii of Chart No. 1, it will provide \"i\" and by using Chart No. 2, \"C \"can be quickly calculated. Coxe and Beugless used the variable \"C\" for ballistic coefficient.\n\nThe Siacci method was abandoned by the end of the World War I for artillery fire. But the U.S. Army Ordnance Corps continued using the Siacci method into the middle of the 20th century for direct (flat-fire) tank gunnery. The development of the electromechanical analog computer contributed to the calculation of aerial bombing trajectories during World War II. After World War II the advent of the silicon semiconductor based digital computer made it possible to create trajectories for the guided missiles/bombs, intercontinental ballistic missiles and space vehicles.\n\nBetween World War I and II the U.S. Army Ballistics research laboratories at Aberdeen Proving Grounds, Maryland, USA developed the standard models for G2, G5, G6. In 1965, Winchester Western published a set of ballistics tables for G1, G5, G6 and GL. In 1971 Sierra Bullet Company retested all their bullets and concluded that the G5 model was not the best model for their boat tail bullets and started using the G1 model. This was fortunate, as the entire commercial sporting and firearms industries had based their calculations on the G1 model. The G1 model and Mayevski/Siacci Method continue to be the industry standard today. This benefit allows for comparison of all ballistic tables for trajectory within the commercial sporting and firearms industry.\n\nIn recent years there have been vast advancements in the calculation of flat-fire trajectories with the advent of Doppler radar and the personal computer and handheld computing devices. Also, the newer methodology proposed by Dr. Arthur Pejsa and the use of the G7 model used by Mr. Brian Litz, ballistic engineer for Berger Bullets, LLC for calculating boat tailed spitzer rifle bullet trajectories and 6 Dof model based software have improved the prediction of flat-fire trajectories.\n\nMost ballistic mathematical models and hence tables or software take for granted that one specific drag function correctly describes the drag and hence the flight characteristics of a bullet related to its ballistic coefficient. Those models do not differentiate between wadcutter, flat-based, spitzer, boat-tail, very-low-drag, etc. bullet types or shapes. They assume one invariable drag function as indicated by the published BC. Several different drag curve models optimized for several standard projectile shapes are available, however.\n\nThe resulting drag curve models for several standard projectile shapes or types are referred to as:\n\nSince these standard projectile shapes differ significantly the G\"x\" BC will also differ significantly from the G\"y\" BC for an identical bullet. To illustrate this the bullet manufacturer Berger has published the G1 and G7 BCs for most of their target, tactical, varmint and hunting bullets. Other bullet manufacturers like Lapua and Nosler also published the G1 and G7 BCs for most of their target bullets. How much a projectile deviates from the applied reference projectile is mathematically expressed by the form factor (\"i\"). The applied reference projectile shape always has a form factor (\"i\") of exactly 1. When a particular projectile has a sub 1 form factor (\"i\") this indicates that the particular projectile exhibits lower drag than the applied reference projectile shape. A form factor (\"i\") greater than 1 indicates the particular projectile exhibits more drag than the applied reference projectile shape. In general the G1 model yields comparatively high BC values and is often used by the sporting ammunition industry.\n\nVariations in BC claims for exactly the same projectiles can be explained by differences in the ambient air density used to compute specific values or differing range-speed measurements on which the stated G1 BC averages are based. Also, the BC changes during a projectile's flight, and stated BCs are always averages for particular range-speed regimes. Further explanation about the variable nature of a projectile's G1 BC during flight can be found at the external ballistics article. The external ballistics article implies that knowing how a BC was determined is almost as important as knowing the stated BC value itself.\n\nFor the precise establishment of BCs (or perhaps the scientifically better expressed drag coefficients), Doppler radar-measurements are required. The normal shooting or aerodynamics enthusiast, however, has no access to such expensive professional measurement devices. Weibel 1000e or Infinition BR-1001 Doppler radars are used by governments, professional ballisticians, defense forces, and a few ammunition manufacturers to obtain exact real-world data on the flight behavior of projectiles of interest.\n\nDoppler radar measurement results for a lathe turned monolithic solid .50 BMG very-low-drag bullet (Lost River J40 , monolithic solid bullet / twist rate 1:) look like this:\n\nThe initial rise in the BC value is attributed to a projectile's always present yaw and precession out of the bore. The test results were obtained from many shots, not just a single shot. The bullet was assigned 1.062 for its BC number by the bullet's manufacturer, Lost River Ballistic Technologies.\n\nMeasurements on other bullets can give totally different results. How different speed regimes affect several 8.6 mm (.338 in calibre) rifle bullets made by the Finnish ammunition manufacturer Lapua can be seen in the .338 Lapua Magnum product brochure which states Doppler radar established BC data.\n\nSporting bullets, with a calibre \"d\" ranging from , have BCs in the range 0.12 to slightly over 1.00. Those bullets with the higher BCs are the most aerodynamic, and those with low BCs are the least. Very-low-drag bullets with BCs ≥ 1.10 can be designed and produced on CNC precision lathes out of mono-metal rods, but they often have to be fired from custom made full bore rifles with special barrels.\n\nAmmunition makers often offer several bullet weights and types for a given cartridge. Heavy-for-caliber pointed (spitzer) bullets with a boattail design have BCs at the higher end of the normal range, whereas lighter bullets with square tails and blunt noses have lower BCs. The 6 mm and 6.5 mm cartridges are probably the most well known for having high BCs and are often used in long range target matches of – . The 6 and 6.5 have relatively light recoil compared to high BC bullets of greater caliber and tend to be shot by the winner in matches where accuracy is key. Examples include the 6mm PPC, 6mm Norma BR, 6x47mm SM, 6.5×55mm Swedish Mauser, 6.5×47mm Lapua, 6.5 Creedmoor, 6.5 Grendel, .260 Remington, and the 6.5-284. The 6.5 mm is also a popular hunting caliber in Europe.\n\nIn the United States, hunting cartridges such as the .25-06 Remington (a 6.35 mm caliber), the .270 Winchester (a 6.8 mm caliber), and the .284 Winchester (a 7 mm caliber) are used when high BCs and moderate recoil are desired. The .30-06 Springfield and .308 Winchester cartridges also offer several high-BC loads, although the bullet weights are on the heavy side.\n\nIn the larger caliber category, the .338 Lapua Magnum and the .50 BMG are popular with very high BC bullets for shooting beyond 1,000 meters. Newer chamberings in the larger caliber category are the .375 and .408 Cheyenne Tactical and the .416 Barrett.\n\nFor many years, bullet manufacturers were the main source of ballistic coefficients for use in trajectory calculations. However, in the past decade or so, it has been shown that ballistic coefficient measurements by independent parties can often be more accurate than manufacturer specifications. Since ballistic coefficients depend on the specific firearm and other conditions that vary, it is notable that methods have been developed for individual users to measure their own ballistic coefficients.\n\nSatellites in Low Earth Orbit (LEO) with high ballistic coefficients experience smaller perturbations to their orbits due to atmospheric drag.\n\nThe ballistic coefficient of an atmospheric reentry vehicle has a significant effect on its behavior. A very high ballistic coefficient vehicle would lose velocity very slowly and would impact the Earth's surface at higher speeds. In contrast, a low ballistic coefficient would reach subsonic speeds before reaching the ground.\n\nIn general, reentry vehicles that carry human beings back to Earth from space have high drag and a correspondingly low ballistic coefficient. Vehicles that carry nuclear weapons launched by an intercontinental ballistic missile (ICBM), by contrast, have a high ballistic coefficient, which enables them to travel rapidly from space to a target on land. That makes the weapon less affected by crosswinds or other weather phenomena, and harder to track, intercept, or otherwise defend against.\n\n\n"}
{"id": "442313", "url": "https://en.wikipedia.org/wiki?curid=442313", "title": "Blue Flame", "text": "Blue Flame\n\nThe Blue Flame is a rocket-powered vehicle that was driven by Gary Gabelich and achieved the world land speed record on Bonneville Salt Flats in Utah on October 23, 1970. The vehicle set the FIA world record for the flying mile at and the flying kilometer at .\n\nThe Blue Flame was constructed in Milwaukee, Wisconsin by Reaction Dynamics, a company formed by Pete Farnsworth, Ray Dausman and Dick Keller who had developed the first hydrogen peroxide rocket dragster, called the X-1 and driven by Chuck Suba. The Blue Flame used a combination of high-test peroxide and liquified natural gas (LNG), pressurized by helium gas. The effort was sponsored by the American Gas Association, with technical assistance from the Institute of Gas Technology of Des Plaines, IL. \n\nReaction Dynamics was formed in 1965 and started out as \"DFK Enterprises\", for Dausman, Farnsworth and Keller. At that time Keller worked part-time as a research assistant into gas technology at the Illinois Institute of Technology, which was the research arm of the American Gas Association. Farnsworth was a top alcohol dragster racer. \n\nThe engine of the Blue Flame was designed by Reaction Dynamics, and some of the components were manufactured by Galaxy Manufacturing of Tonawanda, New York. Galaxy Mfg. was formed in 1966 by Donald J Magro and Gerald Muhs and was principally engaged in flow control systems, cavitating venturi, and precision machining fields.\n\nThe Blue Flame's engine is a regeneratively cooled, liquid-propellent engine of the variable thrust type. It can operate on either a single- or dual-propellant basis. In operation, the engine permits natural gas use as a liquid or gas or both with a two-stage combustion start. The oxidizer flow is established first, then LNG enters a heat exchanger where it vaporizes and is brought to combustion temperature. The gas is then injected into the combustion chamber with the oxygen provided by the hydrogen peroxide. A stable flame front is established and the remaining LNG is injected to bring the engine to full power. \n\nNominal design engine running time was 20 seconds at full thrust of generating the equivalent of . Keller stated that the Goodyear Tire Company restricted their top speed to . Reaction Dynamics subsequently modified the LNG flow in the two-stage LNG injector system to almost halve the maximum thrust. The actual thrust during the record runs was between [equivalent of ] and . According to Keller the kilometer timing traps were inside the mile. The Blue Flame's record runs involved accelerating continuously to the mile midpoint, then coasting through the mile. The peak speed, of approximately was reached at that point and then the vehicle decelerated the rest of the way. The kilometer speed trap was biased towards one end of the mile, resulting in the higher speed.\n\nThe frame of the Blue Flame is a semi-monocoque type aluminum, with welded tubular structure in the nose section and with an aluminum \"skin.\" The nose cone and possibly body panels were built by Ray Besasie, a noted Milwaukee fabricator and mechanic in his garage. The vehicle is long, high to the top of the tail fin, wide and the wheelbase is . It has an empty weight of and is approximately fully fueled and loaded. The Goodyear Tire and Rubber Co. designed 8:00-25 tires for the vehicle, with an outside diameter of and smooth tire tread surface to help prevent heat buildup using nitrogen gas at .\n\nOn 23 October 1970 at Bonneville, Gary Gabelich drove the Blue Flame to a new record of for the flying mile, for the flying kilometre. According to the worldwide rules that govern land speed record attempts, a land speed mark is recognized only after two runs through the flying kilometre and measured mile clocks and both runs must be made within one hour.\n\nThe land speed record set by \"Blue Flame\" was broken on 4 October 1983 by Richard Noble driving his turbojet-powered \"Thrust2\". This broke the mile record of , raising it to . The kilometer record of stood until ThrustSSC went supersonic in 1997, raising it to . \n\nThe Blue Flame is now on permanent exhibition at the Auto- and Technik Museum Sinsheim in Germany.\n\n\n"}
{"id": "1802693", "url": "https://en.wikipedia.org/wiki?curid=1802693", "title": "Caliche", "text": "Caliche\n\nCaliche () is a sedimentary rock, a hardened natural cement of calcium carbonate that binds other materials—such as gravel, sand, clay, and silt. It occurs worldwide, in aridisol and mollisol soil orders—generally in arid or semiarid regions, including in central and western Australia, in the Kalahari Desert, in the High Plains of the western USA, in the Sonoran Desert and Mojave Desert, and in Eastern Saudi Arabia Al-Hasa. Caliche is also known as calcrete or kankar (in India). It belongs to the duricrusts. The term \"caliche\" is Spanish and is originally from the Latin \"calx\", meaning lime.\n\nCaliche is generally light-colored, but can range from white to light pink to reddish-brown, depending on the impurities present. It generally occurs on or near the surface, but can be found in deeper subsoil deposits, as well. Layers vary from a few inches to feet thick, and multiple layers can exist in a single location.\n\nIn northern Chile and Peru, \"caliche\" also refers to mineral deposits that include nitrate salts. Caliche can also refer to various claylike deposits in Mexico and Colombia. In addition, it has been used to describe some forms of quartzite, bauxite, kaolinite, laterite, chalcedony, opal, and soda niter.\n\nA similar material, composed of calcium sulfate rather than calcium carbonate, is called gypcrust.\n\nCaliche generally forms when minerals leach from the upper layer of the soil (the A horizon) and accumulate in the next layer (the B horizon), at depths around 3 to 10 feet under the surface. It generally consists of carbonates in semiarid regions—in arid regions, less-soluble minerals form caliche layers after all the carbonates have been leached from the soil. The deposited calcium carbonate accumulates—first forming grains, then small clumps, then a discernible layer, and finally, a thicker, solid bed. As the caliche layer forms, the layer gradually becomes deeper, and eventually moves into the parent material, which lies under the upper soil horizons.\n\nHowever, caliche also forms in other ways. It can form when water rises through capillary action. In an arid region, rainwater sinks into the ground very quickly. Later, as the surface dries out, the water below the surface rises, carrying up dissolved minerals from lower layers. This water movement forms a caliche that tends to grow thinner and branch out as it nears the surface. Plants can contribute to the formation of caliche, as well. Plant roots take up water through transpiration, and leave behind the dissolved calcium carbonate, which precipitates to form caliche. It can also form on outcrops of porous rocks or in rock fissures where water is trapped and evaporates. In general, caliche deposition is a slow process, but if enough moisture is present in an otherwise arid site, it can accumulate fast enough to block a drain pipe.\n\nWhile the formation of other caliches is relatively well understood, the origin of Chilean caliche is not clearly known. One possibility is that the deposits were formed when a prehistoric inland sea evaporated. Another theory is that it was deposited due to weathering of the Andes.\n\nOne of the world's largest deposits of calcrete is in the Makgadikgadi Pans in Botswana, where surface calcretes occur at the location of a now-desiccated prehistoric lake.\n\nCaliche is used in construction worldwide. Its reserves in the Llano Estacado in Texas can be used in the manufacture of Portland cement; the caliche meets the chemical composition requirements and has been used as a principal raw material in Portland cement production in at least one Texas plant. Where the calcium carbonate content is over 80%, caliche can also be fired and used as a source of lime, which can then be used for soil stabilization.\nWhen mixed with small amounts of either pozzolan or Portland cement, caliche can also be used as a building material that exceeds building code requirements for unfired masonry materials. For example, caliche was used to build some of the Mayan buildings in the Yucatán Peninsula in Mexico. A dormitory in Ingram, Texas, and a demonstration building in Carrizo Springs, Texas, for the United States Department of Energy were also built using caliche as part of studies by the Center for Maximum Potential Building Systems.\n\nIn many areas, caliche is also used for road construction, either as a surfacing material, or more commonly, as base material. It is one of the most common road materials used in Southern Africa. Caliche is widely used as a base material when it is locally available and cheap. However, it does not hold up to moisture (rain), and is never used if a hard-rock base material, such as limestone, is available.\n\nA nearly pure source of calcium carbonate is necessary to refine sugar. It must contain at least 95% calcium carbonate (CaCO) and have a low magnesium content. In addition, the material must meet certain physical requirements so it does not break down when burned. Although caliche does not generally meet all of the requirements for sugar refining, it is used in areas where another source of calcium carbonate, such as limestone, is not present. While caliche requires beneficiation to meet the requirements, its use can still be significantly cheaper than shipping in limestone.\n\nIn the Atacama Desert in northern Chile, vast deposits of a mixture, also referred to as \"caliche\", are composed of gypsum, sodium chloride and other salts, and sand, associated to \"salitre\" (\"Chile saltpeter\"). \"Salitre\", in turn, is a composite of sodium nitrate (NaNO) and potassium nitrate (KNO). \"Salitre\" was an important source of export revenue for Chile until World War I, when Europe began to produce both nitrates industrially in large quantities.\n\nThese deposits are the largest known natural source of nitrates in the world, containing up to 25% sodium nitrate and 3% potassium nitrate, as well as iodate minerals, sodium chloride, sodium sulfate, and sodium borate (borax). The caliche beds are from 0.2 to 5.0 m thick, and they are mined and refined to produce a variety of products, including sodium nitrate (for agriculture or industry uses), potassium nitrate, sodium sulfate, iodine, and iodine derivatives.\n\nCaliche beds can cause problems for agriculture. First, an impermeable caliche layer prevents water from draining properly, which can keep roots from getting enough oxygen. Salts can also build up in the soil due to the lack of drainage. Both of these situations are detrimental to plant growth. Second, the impermeable nature of caliche beds prevents plant roots from penetrating the bed, which limits the supply of nutrients, water, and space so they cannot develop normally. Third, caliche beds can also cause the surrounding soil to be basic. The basic soil, along with calcium carbonate from the caliche, can prevent plants from getting enough nutrients, especially iron. An iron deficiency makes the youngest leaves turn yellow. Soil saturation above the caliche bed can make the condition worse.\n\n\n"}
{"id": "25279655", "url": "https://en.wikipedia.org/wiki?curid=25279655", "title": "Carbide-derived carbon", "text": "Carbide-derived carbon\n\nCarbide-derived carbon (CDC), also known as tunable nanoporous carbon, is the common term for carbon materials derived from carbide precursors, such as binary (e.g. SiC, TiC), or ternary carbides, also known as MAX phases (e.g., TiAlC, TiSiC). CDCs have also been derived from polymer-derived ceramics such as Si-O-C or Ti-C, and carbonitrides, such as Si-N-C. CDCs can occur in various structures, ranging from amorphous to crystalline carbon, from sp- to sp-bonded, and from highly porous to fully dense. Among others, the following carbon structures have been derived from carbide precursors: micro- and mesoporous carbon, amorphous carbon, carbon nanotubes, onion-like carbon, nanocrystalline diamond, graphene, and graphite. Among carbon materials, microporous CDCs exhibit some of the highest reported specific surface areas (up to more than 3000 m/g). By varying the type of the precursor and the CDC synthesis conditions, microporous and mesoporous structures with controllable average pore size and pore size distributions can be produced. Depending on the precursor and the synthesis conditions, the average pore size control can be applied at sub-Angstrom accuracy. This ability to precisely tune the size and shapes of pores makes CDCs attractive for selective sorption and storage of liquids and gases (e.g., hydrogen, methane, CO) and the high electric conductivity and electrochemical stability allows these structures to be effectively implemented in electrical energy storage and capacitive water desalinization.\n\nThe production of SiCl by high temperature reaction of Chlorine gas with Silicon Carbide was first patented in 1918 by Otis Hutchins, with the process further optimized for higher yields in 1956. The solid porous carbon product was initially regarded as a waste byproduct until its properties and potential applications were investigated in more detail in 1959 by Walter Mohun. Research was carried out in the 1960-1980s mostly by Russian scientists on the synthesis of CDC via halogen treatment, while hydrothermal treatment was explored as an alternative route to derive CDCs in the 1990s. Most recently, research activities have centered on optimized CDC synthesis and nanoengineered CDC precursors.\n\nHistorically, various terms have been used for CDC, such as \"mineral carbon\" or \"nanoporous carbon\". Later, a more adequate nomenclature introduced by Yury Gogotsi was adopted that clearly denotes the precursor. For example, CDC derived from silicon carbide has been referred to as SiC-CDC, Si-CDC, or SiCDC. Recently, it was recommended to adhere to a unified precursor-CDC-nomenclature to reflect the chemical composition of the precursor (e.g., BC-CDC, TiSiC-CDC, WC-CDC).\n\nCDCs have been synthesized using several chemical and physical synthesis methods. Most commonly, dry chlorine treatment is used to selectively etch metal or metalloid atoms from the carbide precursor lattice. The term \"chlorine treatment\" is to be preferred over chlorination as the chlorinated product, metal chloride, is the discarded byproduct and the carbon itself remains largely unreacted. This method is implemented for commercial production of CDC by Skeleton in Estonia and Carbon-Ukraine. Hydrothermal etching has also been used for synthesis of SiC-CDC which yielded a route for porous carbon films and nanodiamond synthesis.\nThe most common method for producing porous carbide-derived carbons involves high-temperature etching with halogens, most commonly chlorine gas. The following generic equation describes the reaction of a metal carbide with chlorine gas (M: Si, Ti, V; similar equations can be written for other CDC precursors):\n\nHalogen treatment at temperatures between 200 and 1000 °C has been shown to yield mostly disordered porous carbons with a porosity between 50 and ~80 vol% depending on the precursor. Temperatures above 1000 °C result in predominantly graphitic carbon and an observed shrinkage of the material due to graphitization.\n\nThe linear growth rate of the solid carbon product phase suggests a reaction-driven kinetic mechanism, but the kinetics become diffusion-limited for thicker films or larger particles. A high mass transport condition (high gas flow rates) facilitates the removal of the chloride and shifts the reaction equilibrium towards the CDC product. Chlorine treatment has successfully been employed for CDC synthesis from a variety of carbide precursors, including SiC, TiC, BC, BaC, CaC, CrC, FeC, MoC, AlC, NbC, SrC, TaC, VC, WC, WC, ZrC, ternary carbides such as TiAlC, TiAlC, and TiSiC, and carbonitrides such as TiAlCN.\n\nMost produced CDCs exhibit a prevalence of micropores (< 2 nm) and mesopores (between 2 and 50 nm), with specific distributions affected by carbide precursor and synthesis conditions. Hierarchic porosity can be achieved by using polymer-derived ceramics with or without utilizing a templating method. Templating yields an ordered array of mesopores in addition to the disordered network of micropores.\nIt has been shown that the initial crystal structure of the carbide is the primary factor affecting the CDC porosity, especially for low-temperature chlorine treatment. In general, a larger spacing between carbon atoms in the lattice correlates with an increase in the average pore diameter. As the synthesis temperature increases, the average pore diameter increases, while the pore size distribution becomes broader. The overall shape and size of the carbide precursor, however, is largely maintained and CDC formation is usually referred to as a conformal process.\nMetal or metalloid atoms from carbides can selectively be extracted at high temperatures (usually above 1200 °C) under vacuum. The underlying mechanism is incongruent decomposition of carbides, using the high melting point of carbon compared to corresponding carbide metals that melt and eventually evaporate away, leaving the carbon behind.\n\nLike halogen treatment, vacuum decomposition is a conformal process. The resulting carbon structures are, as a result of the higher temperatures, more ordered, and carbon nanotubes and graphene can be obtained. In particular, vertically aligned carbon nanotubes films of high tube density have been reported for vacuum decomposition of SiC. The high tube density translates into a high elastic modulus and high buckling resistance which is of particular interest for mechanical and tribological applications.\n\nWhile carbon nanotube formation occurs when trace oxygen amounts are present, very high vacuum conditions (approaching 10–10 torr) result in the formation of graphene sheets. If the conditions are maintained, graphene transitions into bulk graphite. In particular, by vacuum annealing silicon carbide single crystals (wafers) at 1200–1500 °C, metal/metalloid atoms are selectively removed and a layer of 1–3 layer graphene (depending on the treatment time) is formed, undergoing a conformal transformation of 3 layers of silicon carbide into one monolayer of graphene. Also, graphene formation occurs preferentially on the Si-face of the 6H-SiC crystals, while nanotube growth is favored on the c-face of SiC.\n\nThe removal of metal atoms from carbides has been reported at high temperatures (300–1000 °C) and pressures (2–200 MPa). The following reactions are possible between metal carbides and water:\n\nOnly the last reaction yields solid carbon. The yield of carbon-containing gases increases with pressure (decreasing solid carbon yield) and decreases with temperatures (increasing the carbon yield). The ability to produce a usable porous carbon material is dependent on the solubility of the formed metal oxide (such as SiO) in supercritical water. Hydrothermal carbon formation has been reported for SiC, TiC, WC, TaC, and NbC. Insolubility of metal oxides, for example TiO, is a significant complication for certain metal carbides (e.g., TiSiC).\n\nOne application of carbide-derived carbons is as active material in electrodes for electric double layer capacitors which have become commonly known as supercapacitors or ultracapacitors. This is motivated by their good electrical conductivity combined with high surface area, large micropore volume, and pore size control that enable to match the porosity metrics of the porous carbon electrode to a certain electrolyte. In particular, when the pore size approaches the size of the (desolvated) ion in the electrolyte, there is a significant increase in the capacitance. The electrically conductive carbon material minimizes resistance losses in supercapacitor devices and enhances charge screening and confinement, maximizing the packing density and subsequent charge storage capacity of microporous CDC electrodes.\n\nCDC electrodes have been shown to yield a gravimetric capacitance of up to 190 F/g in aqueous electrolytes and 180 F/g in organic electrolytes. The highest capacitance values are observed for matching ion/pore systems, which allow high-density packing of ions in pores in superionic states. However, small pores, especially when combined with an overall large particle diameter, impose an additional diffusion limitation on the ion mobility during charge/discharge cycling. The prevalence of mesopores in the CDC structure allows for more ions to move past each other during charging and discharging, allowing for faster scan rates and improved rate handling abilities. Conversely, by implementing nanoparticle carbide precursors, shorter pore channels allow for higher electrolyte mobility, resulting in faster charge/discharge rates and higher power densities.\n\nTiC-CDC activated with KOH or CO store up to 21 wt.% of methane at 25 °C at high pressure. CDCs with subnanometer pores in the 0.50–0.88 nm diameter range have shown to store up to 7.1 mol CO/kg at 1 bar and 0 °C. CDCs also store up to 3 wt.% hydrogen at 60 bar and −196 °C, with additional increases possible as a result of chemical or physical activation of the CDC materials. SiOC-CDC with large subnanometer pore volumes are able to store over 5.5 wt.% hydrogen at 60 bar and −196 °C, almost reaching the goal of the US Department of Energy of 6 wt.% storage density for automotive applications. Methane storage densities of over 21.5 wt.% can be achieved for this material at those conditions. In particular, a predominance of pores with subnanometer diameters and large pore volumes are instrumental towards increasing storage densities.\n\nCDC films obtained by vacuum annealing (ESK) or chlorine treatment of SiC ceramics yield a low friction coefficient. The friction coefficient of SiC, which is widely used in tribological applications for its high mechanical strength and hardness, can therefore decrease from ~0.7 to ~0.2 or less under dry conditions. It’s important to mention that graphite cannot operate in dry environments. The porous 3-dimensional network of CDC allows for high ductility and an increased mechanical strength, minimizing fracture of the film under an applied force. Those coatings find applications in dynamic seals. The friction properties can be further tailored with high-temperature hydrogen annealing and subsequent hydrogen termination of dangling bonds.\n\nCarbide-derived carbons with a mesoporous structure remove large molecules from biofluids. As other carbons, CDCs possess good biocompatibility. CDCs have been demonstrated to remove cytokines such as TNF-alpha, IL-6, and IL-1beta from blood plasma. These are the most common receptor-binding agents released into the body during a bacterial infection that cause the primary inflammatory response during the attack and increase the potential lethality of sepsis, making their removal a very important concern. The rates and levels of removal of above cytokines (85–100% removed within 30 minutes) are higher than those observed for comparable activated carbons.\n\nPt nanoparticles can be introduced to the SiC/C interface during chlorine treatment (in the form of PtCl). The particles diffuse through the material to form Pt particle surfaces, which may serve as catalyst support layers. In particular, in addition to Pt, other noble elements such as gold can be deposited into the pores, with the resulting nanoparticle size controlled by the pore size and overall pore size distribution of the CDC substrate. Such gold or platinum nanoparticles can be smaller than 1 nm even without employing surface coatings. Au nanoparticles in different CDCs (TiC-CDC, MoC-CDC, BC-CDC) catalyze the oxidation of carbon monoxide.\n\nAs desalinization and purification of water is critical for obtaining deionized water for laboratory research, large-scale chemical synthesis in industry and consumer applications, the use of porous materials for this application has received particular interest. Capacitive deionization operates in a fashion with similarities to a supercapacitor. As an ion-containing water (electrolyte) is flown between two porous electrodes with an applied potential across the system, the corresponding ions assemble into a double layer in the pores of the two terminals, decreasing the ion content in the liquid exiting the purification device. Due to the ability of carbide-derived carbons to closely match the size of ions in the electrolyte, side-by-side comparisons of desalinization devices based on CDCs and activated carbon showed a significant efficiency increase in the 1.2–1.4 V range compared to activated carbon.\n\nHaving originated as the by-product of industrial metal chloride synthesis, CDC has certainly a potential for large-scale production at a moderate cost. Currently, only small companies engage in production of carbide-derived carbons and their implementation in commercial products. For example, Skeleton, which is located in Tartu, Estonia, and Carbon-Ukraine, located in Kiev, Ukraine, have a diverse product line of porous carbons for supercapacitors, gas storage, and filtration applications. In addition, numerous education and research institutions worldwide are engaged in basic research of CDC structure, synthesis, or (indirectly) their application for various high-end applications.\n\n\n"}
{"id": "1230691", "url": "https://en.wikipedia.org/wiki?curid=1230691", "title": "Chester Thordarson", "text": "Chester Thordarson\n\nChester Hjortur Thordarson (May 12, 1867 – January 6, 1945) — born Hjörtur Þórðarson — was an Icelandic-American inventor and manufacturer of electrical apparatus who eventually held nearly a hundred technology patents related to transformers, inductors, high voltage coils, and more.\n\nThordarson immigrated to the United States from Iceland in 1873 with his parents Gudrun Grimsdotter and Thordur Arnason. In 1887, Thordarson took a job in Chicago, Illinois working for Chicago Edison Co. In 1895, he founded the Thordarson Electric Manufacturing Company, a manufacturing company in Chicago that produced industrial and commercial transformers. Thordarson's company is now called Thordarson Meissner, Inc. and has locations in Mount Carmel, Illinois, and Henderson, Nevada.\n\nHe was instrumental in the development of the modern energy transmission grid with his work in transformers. He achieved his first distinction at the 1904 World's Fair in St. Louis, where for the Purdue University exhibit he designed and built the first million-volt transformer. For his efforts he won the fair's gold medal.\n\nIn 1910, Thordarson began purchasing property on Rock Island, an island off the tip Wisconsin's Door Peninsula. Thordarson established a private vacation retreat on Rock Island. He was intensely interested in preserving the island’s natural beauty. In 1965 the state of Wisconsin purchased Rock Island from his heirs. Thordarson's former estate has been designated Rock Island State Park. His buildings, including the water tower, were added to the National Register of Historic Places as the Thordarson Estate Historic District during 1985.\n\nThordarson bequeathed his book collection to the University of Wisconsin. The Thordarson collection was estimated to be worth one million dollars in 1945 (equivalent to $ in ) and led to the establishment of the rare books room of the University of Wisconsin Memorial Library. Jen Christian Bay, a member of the Bibliographical Society of America, commented on the collection in 1929:\n\nAmong his awards and honors, the University of Wisconsin and the University of Iceland conferred honorary doctorate degrees. He was awarded medals from the Louisiana Purchase Exposition in 1904 and the Panama–Pacific International Exposition in 1916. King Christian X of Denmark presented Thordarson with the Order of the Falcon in 1939.\n\nThordarson died of heart failure in Chicago, Illinois on January 6, 1945.\n\n\n\n"}
{"id": "21529462", "url": "https://en.wikipedia.org/wiki?curid=21529462", "title": "Colloid-facilitated transport", "text": "Colloid-facilitated transport\n\nColloid-facilitated transport designates a transport process by which colloidal particles serve as transport vector\nof diverse contaminants in the surface water (sea water, lakes, rivers, fresh water bodies) and in underground water circulating in fissured rocks\n(limestone, sandstone, granite, ...). The transport of colloidal particles in surface soils and in the ground can also occur, depending on the soil structure, soil compaction, and the particles size, but the importance of colloidal transport was only given sufficient attention during the 1980 years.\nRadionuclides, heavy metals, and organic pollutants, easily sorb onto colloids suspended in water and that can easily act as contaminant carrier. \n\nVarious types of colloids are recognised: inorganic colloids (clay particles, silicates, iron oxy-hydroxides, ...), organic colloids (humic and fulvic substances). When heavy metals or radionuclides form their own pure colloids, the term \"Eigencolloid\" is used to designate pure phases, e.g., Tc(OH), Th(OH), U(OH), Am(OH). Colloids have been suspected for the long range transport of plutonium on the Nevada Nuclear Test Site. They have been the subject of detailed studies for many years. However, the mobility of inorganic colloids is very low in compacted bentonites and in deep clay formations\nbecause of the process of ultrafiltration occurring in dense clay membrane.\nThe question is less clear for small organic colloids often mixed in porewater with truly dissolved organic molecules.\n\n\n\n\n"}
{"id": "44309873", "url": "https://en.wikipedia.org/wiki?curid=44309873", "title": "Computational Fluid Dynamics for Phase Change Materials", "text": "Computational Fluid Dynamics for Phase Change Materials\n\nComputational Fluid Dynamics (CFD) modeling and simulation for phase change materials (PCMs) is a technique to analyze the performance and behavior of PCMs. The CFD models have been successful in studying and analyzing the air quality, natural ventilation and stratified ventilation, air flow initiated by buoyancy forces and temperature space for the systems integrated with PCMs. Simple shapes like flat plates, cylinders or annular tubes,fins, macro- and micro-encapsulations with containers of different shape are often modeled in CFD software's to study.\n\nTypically the CFD models generally include Reynold’s Averaged Navier-Stokes equation (RANS) modeling and Large Eddy Simulation (LES). Conservation equations of mass, momentum and energy (Navier – Stokes) are linearised, discretised, and applied to finite volumes to obtain a detailed solution for field distributions of air pressure, velocity and temperature for both indoor spaces integrated with PCMs.\n\nwhere\n\nwhere\n\nformula_3\n\nHere f represents \"other\" body forces (per unit volume), such as gravity or centrifugal force. The shear stress term formula_4 becomes formula_5, where formula_6 is the vector Laplacian.\n\nwhere\n\ncommonly used assumptions are\n\nTwo main thermal characteristics of phase change are the enthalpy-temperature relationship and temperature hysteresis. PCMs tend to have varying enthalpy temperature relationships due to the fact that they are blends of different materials, but pure PCMs have a more localized relationship, which can be approximated by single values for the enthalpy and phase change temperature.\n\nHysteresis is the phenomenon which causes the PCM to melt and freezes in different temperature ranges and with different enthalpies,which results in a different temperature-enthalpy curve for melting and freezing. Hysteresis is related to the chemical and kinetic properties of the material.\n\nThe commonly used enthalpy-porosity model in commercial CFD codes assumes, a linear enthalpy-temperature relationship and ignores hysteresis.[8]\n\nThe alternate is to use enthalpy-porosity method. When used to simulate PCM sails and a PCM plate-fin unit it produce reasonable temperature prediction in global space temperature terms. However there are inaccuracies in transient simulations where time dependent PCM and local wall and air temperatures are of interest. This is over come by use of source terms that considers hysteresis and varying enthalpy-temperature relationship. [9][10]\n\nCFD-DEM model are also used sometimes. Phase motion of discrete solids or particles is obtained by the Discrete Element Method (DEM) which applies Newton's laws of motion to every particle and the flow of continuum fluid is described by the local averaged Navier–Stokes equations that can be solved by the traditional Computational Fluid Dynamics (CFD).CFDEMcoupling (DCS Computing GmbH) is one such open source toolbox for CFD-DEM coupling.\nThe Governing equations are discretized using an explicit Finite Volume Method. The velocity-pressure coupling is resolved by adopting a Fractional Step Method. The adoption of the enthalpy method allows working with a fixed grid instead of an interface tracking method.\n\nThe momentum source term intended to model the presence of solid is only needed in the control volumes that contain solid and liquid, not in the pure solid containing volumes.\n\nThe final form of the source term coefficient(S)depends on the approximation adopted for the behavior of the flow in the “mushy zone” (where mixed solid and liquid states are present). However, in the case of constant phase change temperature, the solid-liquid interface should be of infinitesimal width (although it cannot be thinner than one control volume width in our simulations); therefore,the formulation used for the source term is not very important in a physical sense, as long as it manages to bring the velocity to zero in mostly solid control volumes and to vanish if the volume contains pure liquid.[11]\n\nThe various CFD codes[1-3] has been employed for the modeling and simulation of the PCM system to understand the heat transfer mechanism, solidification and melting process, distribution of temperature profile and prediction of the air flow. Various commercial packages have been coupled with the CFD analysis to appreciate the feasibility of evaluating the behavior of PCM integrated system.\n\nThe simulated heat transfer behavior of the PCM in Mobilized Thermal Energy Storage, during the charging process can be successfully conducted by CFD modeling [4] The Volume-Of-Fluid(VOF) method is employed to solve for the temperature distribution in the multiphase, 2-dimensional pressure-based model. It accounts for the heat transfer mechanism, melting time, and the influence of the structure in charging process using Fluent 12.1. The governing equations employed are mass conversion and continuity equations.\n\nIntegral, quasi-1D calculations have been reported [5] mainly for conduction-dominated problem using CFD simulation. It was reported that out of three geometries (cubic, cylindrical and spherical), the spherical capsule will have the maximum heat for the heat transfer fluid. Also it is concluded that salt hydrates based PCMs are the better choice over organic PCMs.\n\n\nThe systems are developed in such manner that phase change materials are in the shell portion of the module and passage for the flow of air through the tubes. Conjugate steady state CFD heat transfer analysis has been carried out [6] to analyze the flow and temperature variation of heat transfer fluid in the system. It paves the way for selection and assessment of the geometrical and flow parameters, PCM solidification characteristics for the given boundary conditions\n\nThe comparative analysis, to further enhance the effectiveness of shell and tube PCMs has also been accomplished via CFD analysis[7]. Various CFD models with different configuration such as pins embedded on a tube with heat transfer fluid (HTF) flowing in it, with PCM surrounding the tube, fins embedded instead of pins and different configurations of fins on the tube are analyzed, by employing ANSYS code.\n\n"}
{"id": "39562189", "url": "https://en.wikipedia.org/wiki?curid=39562189", "title": "Crosswind kite power", "text": "Crosswind kite power\n\nCrosswind kite power is power derived from a class of airborne wind-energy conversion systems (AWECS, aka AWES) or crosswind kite power systems (CWKPS) characterized by a kite system that has energy-harvesting parts that fly transverse to the direction of the ambient wind, i.e., to crosswind mode; sometimes the entire wing set and tether set is flown in crosswind mode. These systems at many scales from toy to power-grid-feeding sizes may be used as high-altitude wind power (HAWP) devices or low-altitude wind power (LAWP) devices without having to use towers. Flexible wings or rigid wings may be used in the kite system. A tethered wing, flying in crosswind at many times wind speed, harvests wind power from an area that is many times exceeding the wing’s own area. Crosswind kite power systems have some advantages over conventional wind turbines: access to more powerful and stable wind resource, high capacity factor, capability for deployment on and offshore at comparable costs, and no need for a tower. Additionally, the wings of the CWKPS may vary in aerodynamic efficiency; the movement of crosswinding tethered wings is sometimes compared with the outer parts of conventional wind turbine blades. However, a conventional traverse-to-wind rotating blade set carried aloft in a kite-power system has the blade set cutting to crosswind and is a form of crosswind kite power. Miles L. Loyd furthered studies on crosswind kite power systems in his work \"Crosswind Kite Power\" in 1980. Some believe that crosswind kite power was introduced by P. Payne and C. McCutchen in their patent No. 3,987,987, filed in 1975, however, crosswind kite power was used far before such patent, e.g., in target kites for war-target practice where the crosswinding power permitted high speeds to give practice to gunners.\n\nHow a system extracts energy from the wind and transfers energy to useful purposes helps to define types of crosswind kite power systems. One typing parameter regards the position of the generator or pump or tasking line or device. Another typing parameter regards how the tethers of the tether set of the kite system are utilized; the tethers holding the kiting wing elements aloft may be used in various ways to form types; tethers may simply hold working wings aloft, or they may be pulling loads on the ground, or multitasking by sending aloft-gained electricity to ground receivers or by pulling loads or by being the tasking device itself as when used for pulling people or things or cutting or grinding things. Some types are distinguished by fast motion transfer or slow motion transfer. Typing of crosswind kite power system also occurs by the nature of the wing set where count of wings and types of wings matter to designers and users; a wing set might be in a train arrangement, stack configuration, arch complex, dome mesh, coordinating family of wings, or just be a simple single-wing with single tether. Types of crosswind kite power devices are also distinguished by scale, purpose, intended life, and cost level. Typing by economic success occurs; is the system effective in the energy or task market or not? Some CWKPS are a type called lifters; they are purposed just for lifting loads, perhaps humans; the type is frequented by the use of autorotating blades that appear then to look like helicopters. A single crosswind kite power system (CWKPS) may be a hybrid complex performing aloft energy generation while also performing ground-based work through tether pulling of loads. The crosswind kite power systems that involve fluttering elements are being explored in several research centers; flutter is mined for energy conversion in a few ways. Researchers are showing types of CWKPS that are difficult to classify or type.\n\nIn the systems of this type of CWKPS, the pulling tether set drives the resisting people and objects to various points on the surface of water bodies or land or points in the atmosphere. In this type of crosswind kite power operation, the design of the resistive objects (people, boards, hulls, boats, ships, water turbines, air turbines, other wings) makes for further types. Crosswinding of the upper flying wings provide power to achieve certain final objectives. The objectives are found in such as kiteboarding, kite windsurfing, snowkiting, yacht kiting, freighter-ship sailing, kite boating,and free-flight soaring and jumping. A collection of researchers have explored the historic free-flight parakite realm to where crosswind flying of the systems' wings would enable free-flight in the atmosphere; fundamentally this is a kite-string set with a wing above and a wing as the resistive anchor set; control of the separate wing set, especially in crosswinding efforts mine the power of winds in different layers of the atmosphere.\n\nIn the systems of this type, an electrical generator, pump, or tasking line is installed on the ground. There are two subtypes, with or without a secondary vehicle.\nIn the subtype without a secondary vehicle,\"Yo-Yo\" method, the tether slowly unwinds off a drum on the ground, due to the windward pull of the kite system's wing, while the wing travels crosswind, that is, left-right of the wind's ambient direction, along various paths, e.g., a figure-8 flight path, or optimized lemniscate paths, or circular paths (small or large radius). The turning drum rotates the rotor of the generator or pump through, perhaps, a high-ratio gearbox. Periodically, the wing is depowered, and the tether is reeled in, or, using the crosswind for a constant pull, the tether is re-connected to a different section of the drum while the wing is traveling in a \"downwind\" cycle. In some systems two tethers are used instead of one.\n\nIn another subtype, a secondary vehicle is used. Such a vehicle can be a carousel, a car, railed cart, wheeled land vehicle, or even a ship on the water. The electrical generator is installed on the vehicle. The rotor of the electrical generator is brought in motion by the carousel, the axle of the car, or the screw of the ship, correspondingly.\n\nIn the systems of this type, one or more flying blades and electrical generators are installed on the wing. The relative airflow rotates the blades by way of autorotation, an interaction with the wind, which transfer the power to the generators. Produced electrical energy is transmitted to the ground through an electrical cable laid along the tether or integrated with the tether. The same blades are sometimes used for double purpose where they are propellers positively driven by costed electricity for launching or special landing or calm-air flight-maintaining purpose.\n\nIn this type, an electrical generator is installed on the ground and a separate cable or belt, trailing behind the wing, transfers the power to a sprocket on the ground, which rotates the rotor of the generator. The separate belt extends at approximately the speed of the wing. Because of the high speed of that belt, the gearbox is not required.\n\nIn this type an electrical generator, pump, tasking line set, or lever is installed on the ground upwind of the wing and driven by the operation of two or three or more tethers arranged from a fast-moving crosswinding flying wing set. Examples are found in the research centers of several universities and kite-energy research centers.\n\nSeveral research centers are exploring twin wing sets employing tether pulling of upwind ground-based loads where the crosswinding wing sets use lighter-than-air devices to assure flight in case of lulls in the ambient wind.\n\nMany in-public-domain patent disclosed teachings and some current research centers are with a focus on using LTA kites to hold bladed turbines using autorotation to drive flown generators.\n\nWhen a wing element in a kite system is designed to have flutter occur, then that fluttering may be harvested for energy to power various loads. In flutter, the wing element travels to crosswind and then reverses to travel to crosswind in a generally opposite direction; the frequency of cycles of reversed direction is high. Flutter in traditional aviation is usually considered a bad and destructive dynamic to be designed out of an aircraft; but in CWKPS, flutter is sometimes designed into the kite system for the specific purpose of converting the wind's kinetic energy to useful purposes; the fast motion of flutter is prized by some kite-energy systems development centers. Harvesting the energy of flutter in kite systems has been done in several ways. One way is to convert the flutter energy into sound, even pleasant sound or music; purposes vary from entertaining one person or a crowd of persons; bird-scaring has been an application. Jerking tether lines by the kite-flown fluttering elements to drive loads to make electricity has been done and is being explored. Pumping fluids by use of flutter-derived energy has been proposed in the kite-energy community. And having the fluttering wing made with appropriate materials and arrangement to be a direct electric-generator part, then electricity can be generated immediately; part of the fluttering wing that is formed to be a magnet flutters by conductive coils forms the parts of the electric generator.\n\nCWKPS are used to move objects immediately over ice, snow, land, ponds, lakes, or oceans. The movement of objects may be done for various reasons: recreation, sport, commerce, industry, science, travel, mine-clearing, defense, offense, plowing, landscaping, etc. The multitude of kite systems flown to crosswind to move kite boarders, land sailors, kite surfers, kite boaters, yachts, ships, catamarans, kayaks, power kiters, kite buggies, kite skiers, kite water skiers, etc., is keeping kite-wing manufacturers busy. SkySails is a leader in saving fuel in the shipping industry by using CWKPS.\n\nIn this type of CWKPS the fast-motion of the flying blades or wings harvest the wind's energy to power the lifting capacity of the system. Mass loads are sometimes close-coupled with the wing set; at other times the mass lifted is distributed along the tether set. A military use of this type involved the rotary-wing kites that appear to be helicopters (but are not) tethered by the kite line; a human observer gets lifted to high points for observation purposes; some of these were used in conjunction with submarine operations with the submarine's towing motion providing the apparent wind for the CWKPS. One example is the Focke-Achgelis Fa 330 Lift-and-place or lift-and-drop uses occur in this type; mass loads are lifted and then placed or dropped; this is done sometimes to overcome barriers or to save ground-transportation fuel costs. When the mass that is lifted is a generator coupled with the crosswinding blades, then the AWES type is changed; this change is the foundation for the focus of some current wind power companies; David Lang is carefully modelling such AWES in coordination with Grant Calverley.\n\nIn this type, rotary cross wind kites drive rings around a guiding tether line. Since the rings are tied together and in tension, torsion can be transferred from the rotating kites to a ground generator. Rotary kite motion around a main lifted tether can rotates either the tether itself, a rotary tether set, or lines fixed across the axis of the main lift tether. On 15 December 2015 this method was the first to successfully complete the someawe.org 100*3 challenge \nFor a prototype demonstration see \n\nIn all types of the crosswind kite power system, the useful power can be approximately described by the Loyd’s formula:\n\nwhere P is power; C and C are coefficients of lift and drag, respectively; ρ is the air density at the altitude of the wing; A is the wing area and V is the wind speed. This formula disregards tether drag, wing and tether weights, change of the air density with altitude and angle of the wing motion vector to the plane, perpendicular to the wind. A more precise formula is:\n\nwhere G is the effective gliding ratio, taking into account the tether drag.\n\nExample: a system with a rigid wing, having dimensions 50 m x 2 m and G=15 in the 12 m/s wind will provide 40 MW of electric power. \n\nWhere is a citation of the example? The Loyds papers formula does not show it in thowse simple terms. What is CL What is Pa Where does 2/27 come from? How can this be scaled to loyds c5-a example? \n\n50Mx2M =100m^2 2/27=0.07407407 pa=1.225 kg/m^3 P=9.074074*CL*G^2*V^3 g^2=225 v^3= 1728 P=9.074074*CL*388800 P=3527999.97*CL CL=0.0882\n\nThe free and open source OpenVSP can be used to model designs of CWKP systems. http://hangar.openvsp.org/vspfiles/350\n\nDepending on the final application of a crosswind-kite-power source, appropriate kite control methods are involved. Human control exercised during the full flight session is exampled in crosswind stunt kiting and kiteboarding; the same has been in place for some electricity-producing crosswind-kite-power source, e.g., by Pierre Benhaiem of France. When the crosswind-kite-power source becomes too large to handle, then companies are building both human-assisted devices and also fully autonomous robotic control systems. Also, there has been demonstrated fully passive crosswind-kite-power source where natural frequencies of a system do permit the absence of human or robot controls; actually, anyone seeing a kited wing toss left and then right in constant motion is seeing a primitive passively controlled crosswind-kite-power source. Advances in computers, sensors, kite steering units, and servo-mechanisms are being applied to attain full autonomy of the launching, flying, and landing of crosswind-kite-power source that are aiming for the utility-scale energy-production market.\n\nSome sectors of crosswind kite power are already commercially robust; the sport low altitude traction industry is one of those sectors; toy sport crosswind kite power systems kept at low altitude must remain safe. But the sectors of high altitude larger CWKPS aiming for utility-scale electrical production to compete against other forms of energy production must overcome various challenges to achieve mainstream acceptance. Some of the challenges are regulatory permissions, including use of airspace and land; safety considerations; reliable operation in varying conditions (day, night, summer, winter, fog, high wind, low wind, etc.); third-party assessment and certification; lifecycle cost modeling.\n\nThe early the 1800s witnessed George Pocock using control of kite system wings to crosswind to good effect. In early the 1900s Paul Garber would produce high speed wings by two-line controls to give targets for aircraft gunners. Crosswind kite power was brought again into focus when Miles L. Loyd carefully described the mathematics and potential of crosswind kite power in 1980. In 1980 it was not possible to create an economical automatic control system to control the wings of a kite system, though passive control of crosswinding kite systems had been ancient. With the advance of computational and sensory resources fine control of the wings of a kite system became not only affordable, but cheap. In the same time significant progress was made in the materials and wing construction techniques; new types of flexible kites with good L/D ratio have been invented. Synthetic materials suitable for the wing and tether became affordable; among those materials are UHMWPE, carbon fiber, PETE, and rip-stop nylon. A large number of people became engaged in the sports of kitesurfing, kiteboarding, kite buggying, snowkiting, and power kiting. Multiple companies and academic teams work on crosswind kite power. Most of the progress in the field has been achieved in the last 10 years.\n\nCurrent trends in CWKPS sectors will have their follow-on stories. Enthusiasm seems to be at a high level among over a thousand workers in the crosswind kite power realm that includes scales from toy scale to utility-grid. Speculation for traveling and moving goods without fuel around the world by use of CWKPS is envisioned both by systems staying connected to the ground and some systems fully disconnected from the ground. Objectives for the future discussed in the literature regard CWKPS facing toy, sport, industry, science, commerce, energy for electrical grid, sailing, and a host of other tasking applications. For CWKPS to compete with solar energy, nuclear energy, fossil fuels, conventional wind power, DWKPS, or other renewable energy sources, the levelized cost of energy from CWKPS will need to become competitive, proven, made known, and adopted; during CWKPS march into the future, other competing sectors will be advancing also. The variety of configurations of kite systems that will fly wings to crosswind for the enhanced power is expected to grow; however, for specific purposes and applications, some winning formats are expected to eventually shine. Placing wing elements that fly to crosswind on huge lofted rope-based arches or even net domes is being researched.\n\nThe are two sectors of crosswind kite power patents, those that have placed some technology into the public domain and those that are within protection periods and perhaps have valid claims. Crosswind kite power teachings in each patent are part of what is reviewed by the crosswind kite power research and development community and interested readers.\n\n\nCrosswind kite power systems are found in toy power kites, sport power kites, and experimental-handy sizes; proposed by research centers are huge utility-grid-power-feeding sizes. The power gained in toy sizes is used to excite product users; two-line and four-line crosswind toy kite-power systems fill kite festival skies. Serious sport crosswind kite power systems drive the movement of athletes around race courses in local and national competitions. Experimental-handy sizes of crosswind kite power systems are explored while furthering research toward utility-scale systems.\n\nCrosswind kite power has been put to various uses throughout history. And the variety of devices that produce crosswind kite power have a historical progression. A simple kite system sitting passively without crosswind kite power production is contrasted with kite systems that fly crosswind producing greater harvesting of energy from the wind's kinetic energy. For perspective, a timeline of crosswind kite power uses and device progress may aid in understanding crosswind kite power.\n\nKite-power systems dedicated to operating without its energy-harvesting elements flying to crosswind are not CWKPS. Examples help to clarify the two branches of kite-power systems. A simple symmetrical two-stick diamond kite let out to downwind flight while the system's tether pulls to turn an at-ground generator shaft is producing energy for use by flying downwind without flying to crosswind; such is a non-CWKPS. Some hefty downwind kite-power systems (DWKPS) are proposed by serious researchers; some DWKPS instruction is found in the patent literature; one trend involves the opening and closing of pilot-kite-lifted opening-and-closing parachutes to drive generators. Notice that some CWKPS, such as Jalbert parafoil working in figure-8 patterns to turn a ground-stationed generator, could be commissioned to operate fully without flying to crosswind, and the resultant kite-power system would then be a DWKPS. Differently, the CWKPS proposed by users of the autorotating blades stay necessarily as CWKPS. Magenn Power's flip-wing kite-balloon is a DWKPS. Similar flip-wing rotating wings are DWKPS, e.g. that taught in Edwards and Evan patent. Benjamin Franklin's legendary pond-crossing by kite power was a simple DWKPS; he was merely dragged downwind by a downwind-flying kite. A non-CWPKS is historically illustrated by a kite-power harvesting system such as was used by Samuel Franklin Cody for man-lifting with the involved wings set in stable downwind flight without flying to crosswind.\n\n\n"}
{"id": "747152", "url": "https://en.wikipedia.org/wiki?curid=747152", "title": "Desert rose (crystal)", "text": "Desert rose (crystal)\n\nDesert rose is the colloquial name given to rose-like formations of crystal clusters of gypsum or baryte which include abundant sand grains. The 'petals' are crystals flattened on the \"c\" crystallographic axis, fanning open in radiating flattened crystal clusters.\n\nThe rosette crystal habit tends to occur when the crystals form in arid sandy conditions, such as the evaporation of a shallow salt basin. The crystals form a circular array of flat plates, giving the rock a shape similar to a rose blossom. Gypsum roses usually have better defined, sharper edges than baryte roses. Celestine and other bladed evaporite minerals may also form rosette clusters. They can appear either as a single rose-like bloom or as clusters of blooms, with most sizes ranging from pea sized to in diameter.\n\nThe ambient sand that is incorporated into the crystal structure, or otherwise encrusts the crystals, varies with the local environment. If iron oxides are present, the rosettes take on a rusty tone.\n\nThe desert rose may also be known by the names: sand rose, rose rock, selenite rose, gypsum rose and baryte (barite) rose.\n\nRose rocks are found in Tunisia, Libya, Morocco, Algeria, Jordan, Saudi Arabia, Qatar, Egypt, the United Arab Emirates, Spain (Fuerteventura, Canary Islands; Canet de Mar, Catalonia; La Almarcha, Cuenca), the United States (central Oklahoma; Cochise County, Arizona; Texas), Mexico (Ciudad Juárez, Chihuahua), Australia and Namibia. Red River Floodway, Winnipeg Manitoba Canada \n\nRose rock in Oklahoma was formed during the Permian Period, 250 million years ago, when western and central Oklahoma was covered by a shallow sea. As the sea retreated, baryte precipitated out of the water and crystallized around grains of quartz sand. This left behind large formation of reddish sandstone, locally called Garber Sandstone, containing deposits of rose rock.\n\nThe rose rock was selected as the official rock of the US state of Oklahoma in 1968.\n\nThe average size of rose rocks are anywhere from to in diameter. The largest recorded by the Oklahoma Geological Survey was across and high, weighing . Clusters of rose rocks up to tall and weighing more than have been found.\n\n"}
{"id": "42074292", "url": "https://en.wikipedia.org/wiki?curid=42074292", "title": "Drammen Heat Pump", "text": "Drammen Heat Pump\n\nDrammen Fjernvarme District Heating is a district heating system in Drammen, Norway, a regional capital some 65km west of Oslo.\n\nThe heat pump was manufactured by Star Refrigeration in 2011 with 3 systems giving a combined capacity of 14 megawatts to central Drammen providing 85% of hot water needed for the city. The district heating system is owned and operated by Drammen Fjernvarme who have the rights to the concession area given by the Drammen Municipality. This requires all new buildings larger than 1000 m to be built with a water-based heating system and connected to the district heating system.\n\nThe heat pump uses the natural refrigerant ammonia that has a zero global warming potential and is not a greenhouse gas. The heat source is seawater that is taken in around 8 or 9 °C from a depth of 18 m and is cooled by low pressure liquid refrigerant. Using a vapor-compression refrigeration cycle, the system heats district water from 65 °C to 90 °C for use in building heating and hot water systems. The system has an average coefficient of performance (COP) of 3.0 which means 1 unit of electricity is combined with 2 units of heat from the seawater to provide 3 units of heat to the district heating circuit. With the low cost of hydro-based electricity, it is cheaper to run a heat pump than a gas or electric boiler. In addition, the compressor technology used in the Drammen heat pump is the single screw compressor from Vilter (Emerson). Its internal design allows for balanced forces allowing it to perform with a very long bearing life at more than 120,000 hours for normal refrigeration compressors. \n\nThe heat is extracted from a local fjord whose water temperature is around 8 °C. The water is heated from recovered energy at a temperature of 120 °C. A city ordinance requires most new buildings to exploit this form of heating.\n"}
{"id": "25465727", "url": "https://en.wikipedia.org/wiki?curid=25465727", "title": "East African mangroves", "text": "East African mangroves\n\nEast African mangroves are an ecoregion consisting of mangrove swamps along the Indian Ocean coast of East Africa in southern Mozambique, Tanzania, Kenya and southern Somalia.\n\nThe ecoregion consists of two large areas of mangove in the deltas of the Zambezi in Mozambique and the Rufiji River in Tanzania, which can extend as far as 50 km inland, as well as smaller areas along the coast. This coast experiences two monsoon seasons each year, strong ocean currents and rising seas up to 5.6m in Mozambique. Rainfall is high especially in southern Kenya and northern Tanzania.\n\nThe mangroves include tall trees, up to 30m. Compared to Central African mangroves of West Africa, mangroves of East Africa have a greater variety of vegetation with two distinctive types: the mangroves on the coast itself such as the birdwatching site Mida Creek near the Arabuko Sokoke National Park and the town of Watamu, and the Lamu Archipelago both in Kenya, which are fed by constant streams of fresh water; and the mangroves in river inlets where more salt accumulates in the water. East African mangrove species are similar to those found on other coasts around the Indian Ocean. The Bazaruto Archipelago is an example of offshore mangroves sheltered by coral and intermingled with a mixture of shoreline habitats such as grassy sand dunes and rockpools.\n\nThe mangroves are an important habitat for a variety of wildlife from fish crustaceans and molluscs in the waters to snakes and monkeys such as Sykes' monkey in the trees and animals including antelopes, elephants and African buffalo who come to graze on the fringes of the swamps. Larger animals that feed in the swamp waters include hippopotamus, green turtle (\"Chelonia mydas\"), hawksbill turtle (\"Eretmochelys imbricata\"), and olive ridley (\"Lepidochelys olivacea\") turtles, porpoises and important populations of the endangered dugong. Located alongside coral reefs, these mangroves are sheltered by the coral from ocean tides and storms, and the swamps provide food for the many fish, shrimps and other marine fauna that shelter in the coral. The swamps are also important feeding grounds for large numbers of migratory birds such as curlew sandpiper (\"Calidris ferruginea\"), little stint (\"Calidris minuta\") and Caspian tern (\"Hydroprogne caspia\"), waterbirds such as crab-plover (\"Dromas ardeola\"), yellow-billed stork and malachite kingfisher, and seabirds such as roseate tern (\"Sterna dougallii\").\n\nThe mangroves have been harvested for timber for centuries by traders from the nearby Arabian Peninsula. All along the coast mangrove swamps have been cleared, not only for timber but for urban areas, salt panning and agriculture including rice growing and shrimp cultivation. The habitat is further diminished by pollution of rivers from urban and industrial waste and agrochemicals. Urban areas near the mangroves include: the Swahili town of Lamu, the beach town of Malindi and the large port city of Mombasa in Kenya; the port of Tanga in Tanzania; and Quelimane, the large city of Beira (famous for its prawns), Inhassoro and Vilankulo (the ports for Bazaruto), and Maxixe in Mozambique. \n\nProtected areas include Watamu Marine National Park and Ras Tenewi Marine National Park in Kenya; Mafia Island Marine Park, Jozani Chwaka Bay National Park and Saadani National Park in Tanzania; and the Bazaruto Archipelago, Inhaca and Portuguese Island, Marromeu Game Reserve, and Pomene Reserve in Mozambique. Also some areas of mangrove in Kenya and Tanzania are managed as forest reserves.\n"}
{"id": "48629982", "url": "https://en.wikipedia.org/wiki?curid=48629982", "title": "Edward Hubert Cunningham Craig", "text": "Edward Hubert Cunningham Craig\n\nEdward Hubert Cunningham Craig (sometimes Cunningham-Craig) FRSE FGS (1874-1946) was a Scottish geologist and cartographer. He is remembered for extensive mapping as part of HM Geological Survey 1896 to 1907. He was an expert at finding new oil-fields in the early 20th century and worked across the planet in this role.\n\nHe was born in Edinburgh the son of Edward Cunningham Craig and his wife, Mary Elizabeth Pattison, on 22 April 1874. His father died shortly before he was born.\nHe attended Glenalmond College in Perthshire and from 1892 studied Geology at Cambridge University. He was elected a Fellow of the Geological Society in 1893.\n\nHe received a post mapping for HM Geological Survey in 1896. He was promoted to head Geologist on 1 April 1901. From 1903 until 1905 he made official maps of the island of Trinidad and Tobago as government geologist. From 1907 until death he worked for the Burmah Oil Company. His task here was largely on the location of oil fields. He found six especially important oil fields in Persia at Masjed-e-Suleiman in 1908. From 1910 to 1912 he sought oil fields in Baluchistan. In 1912 he found fields in Venezuela and in 1913 fields in South Africa. He also spent time in Canada (Manitoba, Alberta and New Brunswick) searching for both oil and coal-fields.\n\nHe was elected a Fellow of the Royal Society of Edinburgh in 1916. His proposers were John Horne, Ben Peach, Sir John Smith Flett and James Currie.\n\nDuring the First World War he worked with the director of supply for the factory production of poison gas, then, from 1917, was appointed Senior Geologist in petroleum research to the Ministry of Munitions.\n\nAfter the war he found oil variously and sequentially in Egypt, Ecuador, the East Indies, Hungary, Romania, Estonia, Yugoslavia, Java, the United States and Turkey.\n\nIn late life he lived at The Dutch House in Beaconsfield, Buckinghamshire.\nHe died on 24 April 1946 at Kinellan Nursing Home in Beaconsfield.\n\nHe was married to Anna Irene Cleaver in 1910. They had no children.\n\n\nHis portrait by Spy appeared in \"Vanity Fair\" in 1920.\n"}
{"id": "35557526", "url": "https://en.wikipedia.org/wiki?curid=35557526", "title": "Ernsthof Solar Park", "text": "Ernsthof Solar Park\n\nErnsthof Solar Park is a photovoltaic power station near Dörlesberg, Wertheim, Germany. It has a capacity of 34.4 MWp making it the largest Solar Park in the state of Baden-Württemberg. Phase I of 6.88 MWp and covering an area of , consisted of 31,280 modules by LDK Solar Energy Systems was completed on March 30, 2010. The second phase of 7.25 MWp was completed in December 2010, and it was expanded to 29.5 MW and then to 34.4 MW, with over 120,000 modules being fitted in December. Ernsthof East is 6.8 MW and Ernsthof West is 27.5 MW. The total area is . Ernsthof is part of the Tauberlandpark, a planned 72 MW solar park consisting of Ernsthof and Solarpark Gickelfeld (28 MW), which has been delayed due to the political uncertainty of solar parks of over 10 MW receiving the FIT. They may need, instead, to sign a Power Purchase Agreement with the grid distribution power company, which is better than subdividing large projects into multiple smaller segments just to qualify for the FIT.\n\n"}
{"id": "997483", "url": "https://en.wikipedia.org/wiki?curid=997483", "title": "Feller buncher", "text": "Feller buncher\n\nA feller buncher is a type of harvester used in logging. It is a motorized vehicle with an attachment that can rapidly gather and cut a tree before felling it.\n\n\"Feller\" is a traditional name for someone who cuts down trees, and \"bunching\" is the skidding and assembly of two or more trees. A feller buncher performs both of these harvesting functions and consists of a standard heavy equipment base with a tree-grabbing device furnished with a chain-saw, circular saw or a shear—a pinching device designed to cut small trees off at the base. The machine then places the cut tree on a stack suitable for a skidder, forwarder, or yarder for transport to further processing such as delimbing, bucking, loading, or chipping.\n\nSome wheeled feller bunchers lack an articulated arm, and must drive close to a tree to grasp it.\n\nIn cut-to-length logging a harvester performs the tasks of a feller buncher and additionally does delimbing and bucking.\n\n"}
{"id": "34919471", "url": "https://en.wikipedia.org/wiki?curid=34919471", "title": "Green Energy Hub", "text": "Green Energy Hub\n\nThe Green Energy Hub is a region in the Canadian province of Ontario that extends as far west as Port Rowan, as far north as Paris, as far east as Dunnville, and as far south as Lake Erie. Counties covered by the \"Green Energy Hub\" include Brant County, Haldimand County and Norfolk County.\n\nAs a result of the programs that were initiated in the Green Energy Hub, the air has become cleaner between the years 2000 and 2010. However, increasing levels of greenhouse gases from automobiles have ruined the balance caused by the overall cooling of the Earth's atmosphere in 2011. Simcoe, an important community located in the Green Energy Hub, is 25 times less likely to face a major smog disaster when compared to major metropolitan cities like London, Windsor, and Sarnia due to embracing environmental programs in their area.\n\nJobs that come to the \"Green Energy Hub\" would be linked to solar power panels and the collection of the water that comes from rain. This hub helps to respect the environment and brings benefits to those who participate in the movement. Hundreds of millions of dollars are being spent in Norfolk County alone to assure a \"greener\" world for its residents. Norfolk County mayor Dennis Travale is one of the participants in the \"Green Energy Hub.\" Research and development jobs may emerge in the hub; replacing low-technology jobs that were eliminated in the 2000s due to the rising cost of Canadian labor. The purpose behind the Green Energy Hub program is to stop the exploitation of natural resources so that future generations can enjoy them without the threat of 20-year droughts that global warming could bring to Southern Ontario.\n\nAt least 60% of the unemployed in the \"Green Energy Hub\" are under the age of 30. They have to go to larger cities such as Hamilton, Windsor and Toronto in order to find well-paying jobs that allow them to support a household. While manufacturing jobs have disappeared from the local area within the past 20 years, there are promising careers in construction and a variety of technical vocations that may attract current Green Energy Hub residents to the Hamilton area.\n\nEducation and jobs are vital to sustain the needs of the youth who reside within the hub. Approximately 50,000 green collar jobs may be created within the next five years; possibly replacing blue collar jobs. Land in Ontario that is traditionally for agricultural use may be affected by the \"Green Energy Hub.\" This hub may become the central component in the sustainable energy grid of the future. As people learn to dispose less and sustain more of the environment around them, youthful businessmen will learn to take advantages of the economic potentials of the Green Energy Hub; allowing them to escape the welfare programs that currently entraps their parents. These new jobs will be especially found in Brantford; where access to an established railroad link and future GO Transit terminus may guarantee the economic future of the region. Access to GO Transit service will be available by 2020; facilitating the flow of people who work in jobs related to the Green Energy Hub in addition to interurban trade to cities like Windsor, Ontario and Montreal, Quebec.\n\nService sector jobs in the area generally are accountable for nature's services and integrate more easily with state services under a globalized trade system.\n\n2,600 people have officially joined the Norfolk County workforce between June 2012 and June 2013. Businesses have been expanding throughout Norfolk County again; often hiring people two at a time. Innovative business running measures are also improving the quality of life for Norfolk County residents who are members of the workforce. The economy of the Greater Toronto Area along with the rest of the world may become further interconnected with the changing economy of the Green Energy Hub. Expansion jobs done at Toyotetsu Canada will add up to 100 Canadian jobs to a dying industry like manufacturing; bringing 500 local people into the labor force between 2006 and 2013. People throughout Canada have been buying Toyota vehicles more often considering that their MPG rating is greater than Ford and Chevrolet vehicles. This serves as a positive sign for the Toyotetsu plant in addition to the Toyota plants in Cambridge and Woodstook; as the demand for Toyota parts increase dramatically.\n\nMost employers that are actually hiring people for jobs tend to shun people under the age of 25 in addition to certain male personality types, as they see them lacking in \"proper\" work experience. People who haven't worked in years likely haven't updated their résumés in years; relegating them to \"occasional worker\" status and accepting jobs that pay very little compared to their old employment. New employment opportunities are not labelled for students; indicating the preference for local employers to hire \"mature\" people and/or females into their service sector jobs.\n\nHowever, economic recovery has been complicated somewhat by Norfolk County banning the building of new wind turbines. One of the council members were worried about Norfolk County becoming \"industrialized\" and \"unnatural.\" Many leaders in Norfolk County envision the surrounding area as being an agricultural hub for Southern Ontario even by the middle of the 22nd century.\n"}
{"id": "32483660", "url": "https://en.wikipedia.org/wiki?curid=32483660", "title": "Grid-connected photovoltaic power system", "text": "Grid-connected photovoltaic power system\n\nA grid-connected photovoltaic power system, or grid-connected PV power system is an electricity generating solar PV power system that is connected to the utility grid. A grid-connected PV system consists of solar panels, one or several inverters, a power conditioning unit and grid connection equipment. They range from small residential and commercial rooftop systems to large utility-scale solar power stations. Unlike stand-alone power systems, a grid-connected system rarely includes an integrated battery solution, as they are still very expensive. When conditions are right, the grid-connected PV system supplies the excess power, beyond consumption by the connected load, to the utility grid.\n\nResidential, grid-connected rooftop systems which have a capacity more than 10 kilowatts can meet the load of most consumers. They can feed excess power to the grid where it is consumed by other users. The feedback is done through a meter to monitor power transferred. Photovoltaic wattage may be less than average consumption, in which case the consumer will continue to purchase grid energy, but a lesser amount than previously. If photovoltaic wattage substantially exceeds average consumption, the energy produced by the panels will be much in excess of the demand. In this case, the excess power can yield revenue by selling it to the grid. Depending on their agreement with their local grid energy company, the consumer only needs to pay the cost of electricity consumed less the value of electricity generated. This will be a negative number if more electricity is generated than consumed. Additionally, in some cases, cash incentives are paid from the grid operator to the consumer.\n\nConnection of the photovoltaic power system can be done only through an interconnection agreement between the consumer and the utility company. The agreement details the various safety standards to be followed during the connection.\n\nSolar energy gathered by photovoltaic solar panels, intended for delivery to a power grid, must be conditioned, or processed for use, by a grid-connected inverter. Fundamentally, an inverter changes the DC input voltage from the PV to AC voltage for the grid. This inverter sits between the solar array and the grid, draws energy from each, and may be a large stand-alone unit or may be a collection of small inverters, each physically attached to individual solar panels. See AC Module. The inverter must monitor grid voltage, waveform, and frequency. One reason for monitoring is if the grid is dead or strays too far out of its nominal specifications, the inverter must not pass along any solar energy. An inverter connected to a malfunctioning power line will automatically disconnect in accordance with safety rules, for example UL1741, which vary by jurisdiction. Another reason for the inverter monitoring the grid is because for normal operation the inverter must synchronize with the grid waveform, and produce a voltage slightly higher than the grid itself, in order for energy to smoothly flow outward from the solar array.\n\nIslanding is the condition in which a distributed generator continues to power a location even though power from the electric utility grid is no longer present. Islanding can be dangerous to utility workers, who may not realize that a circuit is still powered, even though there is no power from the electrical grid. For that reason, distributed generators must detect islanding and immediately stop producing power; this is referred to as anti-islanding.\n\nIn the case of a utility blackout in a grid-connected PV system, the solar panels will continue to deliver power as long as the sun is shining. In this case, the supply line becomes an \"island\" with power surrounded by a \"sea\" of unpowered lines. For this reason, solar inverters that are designed to supply power to the grid are generally required to have automatic anti-islanding circuitry in them. In intentional islanding, the generator disconnects from the grid, and forces the distributed generator to power the local circuit. This is often used as a power backup system for buildings that normally sell their power to the grid.\n\nThere are two types of anti-islanding control techniques:\n\n\n\n\n"}
{"id": "49694381", "url": "https://en.wikipedia.org/wiki?curid=49694381", "title": "Ground-Mobile Command Center", "text": "Ground-Mobile Command Center\n\nThe Ground-Mobile Command Center was, or is, a U.S. Army program to develop and deploy hardened and secure, mobile command posts for use by the President of the United States to command retaliation and counterattack by the U.S. armed forces in response to a catastrophic assault against North America.\n\nThe Ground-Mobile Command Center program was initiated in 1981. A predecessor program, the National Mobile Land Command Post (NMLCP), had been considered as far back as the 1960s, but was shelved.\n\nDeveloped by TRW Inc. under a government contract awarded during the administration of Ronald Reagan, ground-mobile command centers were, or are, an army counterpart to the better-known \"Nightwatch\", the U.S. Air Force's National Emergency Airborne Command Post, a fleet of hardened aircraft designed to allow the president to remain airborne and mobile during a severe crisis to minimize the possibility of a decapitation strike. Ground-mobile command centers were, or are, 18-wheel tractor-trailers outfitted with defensive systems and sophisticated communications equipment that permits the president or his successor to directly command American nuclear retaliation against another nation while \"on the road\" in an irradiated and devastated post-attack environment. They were, or are, hardened to protect against electromagnetic pulse attack.\n\nColloquially known as \"doomsday trucks\", ground-mobile command centers were reportedly put into service and positioned in locations around the United States that were considered unlikely to be targeted in an initial nuclear volley launched by a warring state. According to one report these locations were in Colorado and Nebraska. They would be supported by co-located fuel depots and spare parts. As intended, the vehicles would not be the primary transportation mode for the president, but would rather be used only after the air evacuation of the National Command Authority from an area of danger at which point they would \"gradually take over full command operations in the post-attack period\".\n\nThe United States Northern Command (USNORTHCOM) operates a \"mobile consolidated control center\" (MCCC) for use by the combatant commander (CCDR) as an \"alternative HQ\" for coordination of emergency and counteroffensive operations following a mainland invasion of the United States. The MCCC consists of a convoy of trucks described as a \"survivable, road-mobile backup\" from which the CCDR can command U.S. military forces in repelling an attack, should primary and secondary facilities be destroyed or overrun.\n\n"}
{"id": "3292644", "url": "https://en.wikipedia.org/wiki?curid=3292644", "title": "Heptanoic acid", "text": "Heptanoic acid\n\nHeptanoic acid, also called enanthic acid, is an organic compound composed of a seven-carbon chain terminating in a carboxylic acid. It is an oily liquid with an unpleasant, rancid odor. It contributes to the odor of some rancid oils. It is slightly soluble in water, but very soluble in ethanol and ether.\n\nThe methyl ester of ricinoleic acid, obtained from castor bean oil, is the main commercial precursor to heptanoic acid. It is pyrolyzed to the methyl ester of 10-undecenoic acid and heptanal, which is then air oxidized to the carboxylic acid. Approximately 20,000 tons were consumed in Europe and US in 1980. \n\nHeptanoic acid is used in the preparation of esters, such as ethyl heptanoate, which are used in fragrances and as artificial flavors. Heptanoic acid is used to esterify steroids in the preparation of drugs such as testosterone enanthate, trenbolone enanthate, drostanolone enanthate, and methenolone enanthate (Primobolan).\n\nThe triglyceride ester of heptanoic acid is the triheptanoin, which is used in certain medical conditions as a nutritional supplement.\n\n"}
{"id": "1310493", "url": "https://en.wikipedia.org/wiki?curid=1310493", "title": "Hook echo", "text": "Hook echo\n\nA hook echo is a pendant or hook-shaped weather radar signature as part of some supercell thunderstorms. It is found in the lower portions of a storm as air and precipitation flow into a mesocyclone resulting in a curved feature of reflectivity. The echo is produced by rain, hail, or even debris being wrapped around the supercell. It is one of the classic hallmarks of tornado-producing supercells. The National Weather Service may consider the presence of a hook echo coinciding with a Tornado vortex signature as sufficient to justify issuing a tornado warning.\n\nThe hook echo was recognized as a sign of tornado development early in the history of weather radar. In 1949, E. M Brooks had referred to circulations having radii of approximately 8–16 km with supercells thunderstorms and named them tornado cyclones. The first documented tracking of a hook echo associated with a tornado was on April 9, 1953 by the Illinois State Water Survey near Urbana-Champaign. An experimental radar unit designed to measure rainfall was being tested by Donald Staggs, an electrical engineer, after Staggs had performed some work on the equipment. Staggs noticed an unusual echo associated with a local severe thunderstorm and began recording the incoming data. Later, it was found that a tornado had been present at the location corresponding to the echo.\n\nTed Fujita documented hook echoes associated with other supercells on the same day as Huff team made their observations. He inferred the concept of thunderstorm rotation from viewing the evolution of the hook echoes, which he studied in detail. In 1962, J. R. Fulks was the first to hypothesize on the formation of hook echoes. Wind velocity data obtained following the installation of weather radars having Doppler effect capabilities in central Oklahoma in the late 1960s confirmed an association between hook echoes and strong horizontal shear zones associated with storm rotation and tornadoes.\n\nHook echoes are a reflection of the movement of air inside and around a supercell thunderstorm. Ahead of the base of the storm, the inflow from the environment is sucked in by the instability of the airmass. As it moves upward, it cools slower than the cloud environment, because it mixes very little with it, creating an echo free tube which ends at higher levels to form a bounded weak echo region or BWER.\n\nAt the same time, a mid-level flow of cool and drier air enters the thunderstorm cloud. Because it is drier than the environment, it is less dense and sinks down behind the cloud and forms the rear flank downdraft, drying the mid level portion of the back of the cloud. Those two currents have a rotation, due to the vertical windshear, and interact to form a mesocyclone. Tightening of the rotation due to the interaction of those two air currents near the surface will create the tornado.\nNear the interaction zone at the surface, there will be a dry slot caused by the updraft on one side and the cloudy area below the rear flank downdraft on the other side. This is the source of the hook echo seen on radar near the surface. Hook echoes are thus a relatively reliable indicator of tornadic activity, however, they merely indicate the presence of a larger mesocyclone structure in the tornadic storm rather than directly detecting a tornado. During some destructive tornadoes, debris lofted from the surface may be detected as a \"debris ball\" on the end of the hook structure.\n\nNot all thunderstorms exhibiting hook echoes produce tornadoes, and not all tornado-producing supercells contain hook echoes. Also, especially preceding the advent of the more advanced Doppler weather radars, by the time a hook echo could be detected a tornado had often already formed—especially when the storm was far away from the location of the radar.\n\nThe use of Doppler weather radar systems, such as NEXRAD, allows for the detection of strong, low-level mesocyclones that produce tornadoes even when the hook echo is not present and also grant greater certainty when a hook echo is present. By detecting hydrometeors moving toward and away from the radar location, the relative velocities of air flowing within different parts of a storm are revealed. These areas of tight rotation known as \"velocity couplets\" are now the primary trigger for the issuance of a tornado warning. The tornado vortex signature is an algorithm-based detection of this.\n\nHook echoes are not always obvious. Particularly in the Southern United States, thunderstorms tend to take on a structure of more precipitation surrounding a mesocyclone, which leads to the high precipitation (HP) variation supercell that obscures the hook shape. HP supercells instead often have a high reflectivity pendent or front flank notch (FFN), appearing like a \"kidney bean\" shape. Another limiting factor is radar resolution. Prior to 2008, NEXRAD had a range resolution of 1000m, while the processes which lead to a hook echo happen on a smaller scale.\n\n\n"}
{"id": "461477", "url": "https://en.wikipedia.org/wiki?curid=461477", "title": "Incompressible flow", "text": "Incompressible flow\n\nIn fluid mechanics or more generally continuum mechanics, incompressible flow (isochoric flow) refers to a flow in which the material density is constant within a fluid parcel—an infinitesimal volume that moves with the flow velocity. An equivalent statement that implies incompressibility is that the divergence of the flow velocity is zero (see the derivation below, which illustrates why these conditions are equivalent).\n\nIncompressible flow does not imply that the fluid itself is incompressible. It is shown in the derivation below that (under the right conditions) even compressible fluids can – to a good approximation – be modelled as an incompressible flow. Incompressible flow implies that the density remains constant within a parcel of fluid that moves with the flow velocity.\n\nThe fundamental requirement for incompressible flow is that the density, formula_1, is constant within a small element volume, \"dV\", which moves at the flow velocity u. Mathematically, this constraint implies that the material derivative (discussed below) of the density must vanish to ensure incompressible flow. Before introducing this constraint, we must apply the conservation of mass to generate the necessary relations. The mass is calculated by a volume integral of the density, formula_1:\n\nThe conservation of mass requires that the time derivative of the mass inside a control volume be equal to the mass flux, J, across its boundaries. Mathematically, we can represent this constraint in terms of a surface integral:\n\nThe negative sign in the above expression ensures that outward flow results in a decrease in the mass with respect to time, using the convention that the surface area vector points outward. Now, using the divergence theorem we can derive the relationship between the flux and the partial time derivative of the density:\n\ntherefore:\n\nThe partial derivative of the density with respect to time need not vanish to ensure incompressible \"flow\". When we speak of the partial derivative of the density with respect to time, we refer to this rate of change within a control volume of \"fixed position\". By letting the partial time derivative of the density be non-zero, we are not restricting ourselves to incompressible \"fluids\", because the density can change as observed from a fixed position as fluid flows through the control volume. This approach maintains generality, and not requiring that the partial time derivative of the density vanish illustrates that compressible fluids can still undergo incompressible flow. What interests us is the change in density of a control volume that moves along with the flow velocity, u. The flux is related to the flow velocity through the following function:\n\nSo that the conservation of mass implies that:\n\nThe previous relation (where we have used the appropriate product rule) is known as the continuity equation. Now, we need the following relation about the total derivative of the density (where we apply the chain rule):\n\nSo if we choose a control volume that is moving at the same rate as the fluid (i.e. (\"dx\"/\"dt\", \"dy\"/\"dt\", \"dz\"/\"dt\") = u), then this expression simplifies to the material derivative:\n\nAnd so using the continuity equation derived above, we see that:\n\nA change in the density over time would imply that the fluid had either compressed or expanded (or that the mass contained in our constant volume, \"dV\", had changed), which we have prohibited. We must then require that the material derivative of the density vanishes, and equivalently (for non-zero density) so must the divergence of the flow velocity:\n\nAnd so beginning with the conservation of mass and the constraint that the density within a moving volume of fluid remains constant, it has been shown that an equivalent condition required for incompressible flow is that the divergence of the flow velocity vanishes.\n\nIn some fields, a measure of the incompressibility of a flow is the change in density as a result of the pressure variations. This is best expressed in terms of the compressibility\n\nIf the compressibility is acceptably small, the flow is considered incompressible.\n\nAn incompressible flow is described by a solenoidal flow velocity field. But a solenoidal field, besides having a zero divergence, also has the additional connotation of having non-zero curl (i.e., rotational component).\n\nOtherwise, if an incompressible flow also has a curl of zero, so that it is also irrotational, then the flow velocity field is actually Laplacian.\n\nAs defined earlier, an incompressible (isochoric) flow is the one in which \nThis is equivalent to saying that\ni.e. the material derivative of the density is zero. Thus if we follow a material element, its mass density remains constant. Note that the material derivative consists of two terms. The first term formula_15 describes how the density of the material element changes with time. This term is also known as the \"unsteady term\". The second term, formula_16 describes the changes in the density as the material element moves from one point to another. This is the \"advection term\" (convection term for scalar field). For a flow to be incompressible the sum of these terms should be zero.\n\nOn the other hand, a homogeneous, incompressible material is one that has constant density throughout. For such a material, formula_17. This implies that, \nFrom the continuity equation it follows that \nThus homogeneous materials always undergo flow that is incompressible, but the converse is not true.\n\nIt is common to find references where the author mentions incompressible flow and assumes that density is constant. Even though this is technically incorrect, it is an accepted practice. One of the advantages of using the incompressible material assumption over the incompressible flow assumption is in the momentum equation where the kinematic viscosity (formula_21) can be assumed constant. The subtlety above is frequently a source of confusion. Therefore, many people prefer to refer explicitly to \"incompressible materials\" or \"isochoric flow\" when being descriptive about the mechanics.\n\nIn fluid dynamics, a flow is considered incompressible if the divergence of the flow velocity is zero. However, related formulations can sometimes be used, depending on the flow system being modelled. Some versions are described below:\n\n\nThese methods make differing assumptions about the flow, but all take into account the general form of the constraint formula_24 for general flow dependent functions formula_26 and formula_27.\n\nThe stringent nature of the incompressible flow equations means that specific mathematical techniques have been devised to solve them. Some of these methods include:\n\n"}
{"id": "500700", "url": "https://en.wikipedia.org/wiki?curid=500700", "title": "Jeremy Rifkin", "text": "Jeremy Rifkin\n\nJeremy Rifkin (born January 26, 1945) is an American economic and social theorist, writer, public speaker, political advisor, and activist. Rifkin is the author of 20 books about the impact of scientific and technological changes on the economy, the workforce, society, and the environment. His most recent books include \"The Zero Marginal Cost Society\" (2014), \"The Third Industrial Revolution\" (2011), \"The Empathic Civilization\" (2010), and \"The European Dream\" (2004).\n\nRifkin has been an unpaid advisor to the European Union since 2000. He has advised the current president and the past two presidents of the European Commission and their leadership teams. Rifkin has also served as an unpaid advisor to the leadership of the European Parliament and prominent European heads of state - including Chancellor Angela Merkel of Germany - on issues related to the economy, climate change, and energy security.\n\nRifkin is the principal architect of the Third Industrial Revolution long-term economic sustainability plan to address the triple challenge of the global economic crisis, energy security, and climate change. The Third Industrial Revolution was formally endorsed by the European Parliament in 2007 and is now being implemented by various agencies within the European Commission.\n\nRifkin has been advising the leadership of the People's Republic of China in recent years. \"The Huffington Post\" reported from Beijing in October 2015 that \"Chinese Premier Li Keqiang has not only read Jeremy Rifkin's book, \"The Third Industrial Revolution\", but taken it to heart\", he and his colleagues having incorporated ideas from this book into the core of the country's thirteenth Five-Year Plan. According to EurActiv, \"Jeremy Rifkin is an American economist and author whose best-selling \"Third Industrial Revolution\" arguably provided the blueprint for Germany's transition to a low-carbon economy, and China's strategic acceptance of climate policy.\"\n\nRifkin has taught at the Wharton School's Executive Education Program at the University of Pennsylvania since 1995, where he instructs CEOs and senior management on transitioning their business operations into sustainable economies. Rifkin is ranked #123 in the WorldPost / HuffingtonPost 2015 global survey of \"The World's Most Influential Voices.\" He is also listed among the top 10 most influential economic thinkers in the survey. Rifkin has lectured before many Fortune 500 companies, and hundreds of governments, civil society organizations, and universities over the past thirty five years.\n\nRifkin is also the President of the TIR Consulting Group, LLC, in connection with a wide range of industries including renewable energy, power transmission, architecture, construction, IT, electronics, transport, and logistics. TIR's global economic development team is working with cities, regions, and national governments to develop the Internet of Things (IoT) infrastructure for a Collaborative Commons and a Third Industrial Revolution. TIR is currently working with the regions of Hauts-de-France in France, the Metropolitan Region of Rotterdam and The Hague, and the Grand Duchy of Luxembourg in the conceptualization, build-out, and scale-up of a smart Third Industrial Revolution infrastructure to transform their economies.\n\nRifkin was born in Denver, Colorado, to Vivette Ravel Rifkin, daughter of Russian Jewish immigrants to Texas, and Milton Rifkin, a plastic-bag manufacturer. He grew up on the southwest side of Chicago. He was president of the graduating class of 1967 at the University of Pennsylvania, where he received a Bachelor of Science in Economics at the Wharton School of Finance and Commerce. Rifkin was also the recipient of the University of Pennsylvania's General Alumni Association's Award of Merit 1967. He had an epiphany when one day in 1967 he walked past a group of students protesting the Vietnam War and picketing the administration building and was amazed to see, as he recalls, that \"my frat friends were beating the living daylights out of them. I got very upset.\" He organized a freedom-of-speech rally the next day. From then on, Rifkin quickly became an active member of the peace movement. He attended the Fletcher School of Law and Diplomacy at Tufts University (MA, International Affairs, 1968) where he continued anti-war activities. Later he joined Volunteers in Service to America (VISTA).\n\nIn 1973, Rifkin organized a mass protest against oil companies at the commemoration of the 200th Anniversary of the Boston Tea Party at Boston's Harbor. Thousands joined the protest, as activists dumped empty oil barrels into Boston's Harbor. The protest came in the wake of the increase in gasoline prices in the fall of 1973, following the OPEC oil embargo. This was later called \"Boston Oil Party\" by the press.\n\nIn 1978, with Ted Howard, he founded the Foundation on Economic Trends (FOET), which is active in both national and international public policy issues related to the environment, the economy, and climate change. FOET examines new trends and their impacts on the environment, the economy, culture and society, and engages in litigation, public education, coalition building and grassroots organizing activities to advance their goals. Rifkin became one of the first major critics of the nascent biotechnology industry with the 1978 publication of his book, \"Who Should Play God?\"\n\nRifkin's 1981 book \"\" discusses how the physical concept of entropy applies to nuclear and solar energy, urban decay, military activity, education, agriculture, health, economics, and politics. It was called \"A comprehensive worldview\" and \"an appropriate successor to ... \"Silent Spring\", \"The Closing Circle\", \"The Limits to Growth\", and \"Small Is Beautiful\",\" by the \"Minneapolis Tribune\". Rifkin's work was heavily influenced by the ideas expressed by Nicholas Georgescu-Roegen in his 1971 book \"The Entropy Law and the Economic Process\". In Rifkin's 1989 revised edition of \"Entropy:...\", entitled \"Entropy: Into the Greenhouse World\", the \"Afterword\" was written by Georgescu-Roegen.\n\nIn 1989, Rifkin brought together climate scientists and environmental activists from 35 nations in Washington, D.C. for the first meeting of the Global Greenhouse Network. In the same year, Rifkin did a series of Hollywood lectures on global warming and related environmental issues for a diverse assortment of film, television and music industry leaders, with the goal of organizing the Hollywood community for a campaign. Shortly thereafter, two Hollywood environmental organizations, Earth Communications Office (ECO) and Environmental Media Association, were formed.\n\nIn 1993, Rifkin launched the Beyond Beef Campaign, a coalition of six environmental groups including Greenpeace, Rainforest Action Network, and Public Citizen, with the goal of encouraging a 50% reduction in the consumption of beef, arguing that methane emissions from cattle has a warming effect 23 times greater than carbon dioxide.\n\nHis 1995 book, \"The End of Work\", is credited by some with helping shape the current global debate on automation, technology displacement, corporate downsizing and the future of jobs. Reporting on the growing controversy over automation and technology displacement in 2011, \"The Economist\" pointed out that Rifkin drew attention to the trend back in 1996 with the publication of his book \"The End of Work\". \"The Economist\" asked \"what happens... when machines are smart enough to become workers? In other words, when capital becomes labor.\" \"The Economist\" noted that \"this is what Jeremy Rifkin, a social critic, was driving at in his book, \"The End of Work,\" published in 1996... Mr. Rifkin argued prophetically that society was entering a new phase, one in which fewer and fewer workers would be needed to produce all the goods and services consumed. 'In the years ahead,' he wrote, 'more sophisticated software technologies are going to bring civilisation ever closer to a near-workerless world. The process has already begun.\"\n\nHis 1998 book, \"The Biotech Century\", addresses issues accompanying the new era of genetic commerce. In its review of the book, the journal \"Nature\" observed that \"Rifkin does his best work in drawing attention to the growing inventory of real and potential dangers and the ethical conundrums raised by genetic technologies...At a time when scientific institutions are struggling with the public understanding of science, there is much they can learn from Rifkin's success as a public communicator of scientific and technological trends.\"\n\nIn \"The Biotech Century\" Rifkin argues that 'Genetic engineering represents the ultimate tool.' 'With genetic technology we assume control over the hereditary blueprints of life itself. Can any reasonable person believe for a moment that such unprecedented power is without substantial risk?' Some of the changes he highlights are: replication partially replacing reproduction; and 'Genetically customized and mass-produced animal clones could be used as chemical factories to secrete—in their blood and milk—large volumes of inexpensive chemicals and drugs for human use.'\n\nMr. Rifkin's work in the biological sciences includes advocacy of animal rights and animal protection around the world.\n\nRifkin's book, \"The Age of Access\", published in the year 2000, was the first to introduce the idea that society is beginning to move from ownership of property in markets, to access to services in networks, giving rise to the Sharing Economy. According to the Journal of Consumer Research, \"the phenomenon of access was first documented in the popular business press by Rifkin (2000), who primarily examines the business-to-business sector and argues that we are living in an age of access in which property regimes have changed to access regimes characterized by short-term limited use of assets controlled by networks of suppliers.\"\n\nAfter the publication of \"The Hydrogen Economy\" (2002), Rifkin worked both in the U.S. and Europe to advance the political cause of renewably generated hydrogen. In the U.S., Rifkin was instrumental in founding the Green Hydrogen Coalition, consisting of thirteen environmental and political organizations (including Greenpeace and MoveOn.org) that are committed to building a renewable hydrogen based economy. His 2004 book, \"The European Dream\", was an international bestseller and winner of the 2005 Corine International Book Prize in Germany for the best economics book of the year.\n\nIn 2011, Rifkin published \"The Third Industrial Revolution; How Lateral Power is Transforming Energy, the Economy, and the World\". The book was a \"New York Times\" best-seller, and has been translated into 19 languages. By 2014, approximately 500,000 copies were in print in China alone.\n\nRifkin delivered a keynote address at the Global Green Summit 2012 on May 10, 2012. The conference was hosted by the Government of the Republic of Korea and the Global Green Growth Institute (GGGI), in association with the Organisation for Economic Co-operation and Development (OECD) and United Nations Environment Programme (UNEP). President Lee Myung-bak of South Korea also gave a speech at the conference and embraced the Third Industrial Revolution to advance a green economy.\n\nIn December 2012, \"Bloomberg Businessweek\" reported that the newly elected premier of China, Li Keqiang is a fan of Rifkin and had \"told his state scholars to pay close attention\" to Rifkin's book, \"The Third Industrial Revolution: How Lateral Power is Transforming Energy, the Economy, and the World\".\n\nRifkin received the \"America Award\" of the Italy-USA Foundation in 2012. He currently works out of an office in Bethesda, Maryland, a suburb of Washington, D.C.\n\nIn April 2014, Rifkin published \"The Zero Marginal Cost Society: The Internet of Things, the Collaborative Commons, and the Eclipse of Capitalism\".\n\nRifkin was awarded an honorary doctorate from Hasselt University in Belgium in the spring of 2015. Rifkin also received an honorary doctorate from the University of Liege in Belgium in the Fall of 2015.\n\nIn November 2015, the Huffington Post reported from Beijing that \"Chinese Premier Li Keqiang has not only read Jeremy Rifkin's book, The Third Industrial Revolution, and taken it to heart. He and his colleagues have also made it the core of the country's thirteenth Five-Year Plan announced in Beijing on October 29th.\" The Huffington Post went on to say that \"this blueprint for China's future signals the most momentous shift in direction since the death of Mao and the advent of Deng Xiaoping's reform and opening up in 1978.\"\n\nAccording to The \"European Energy Review\" \"Perhaps no other author or thinker has had more influence on the EU's ambitious climate and energy policy than the famous American 'visionary' Jeremy Rifkin. In the United States, he has testified before numerous congressional committees and has had success in litigation to ensure responsible government policies on a variety of environmental, scientific and technology related issues. The Union of Concerned Scientists has cited some of Rifkin's publications as useful references for consumers and \"The New York Times\" once stated that \"others in the scholarly, religious, and political fields praise Jeremy Rifkin for a willingness to think big, raise controversial questions, and serve as a social and ethical prophet\"\n\nRifkin's work has also been controversial. Opponents have attacked the lack of scientific rigor in his claims as well as some of the tactics he has used to promote his views. The Harvard scientist Stephen Jay Gould characterized Rifkin's 1983 book \"Algeny\" as \"a cleverly constructed tract of anti-intellectual propaganda masquerading as scholarship\".\n\nA 1989 \"Time\" article about Rifkin's activist methods (entitled \"The Most Hated Man in Science\") details reactions by scientists, especially geneticists, of that decade.\n\n\n\n"}
{"id": "525023", "url": "https://en.wikipedia.org/wiki?curid=525023", "title": "Lecithin", "text": "Lecithin\n\nLecithin (, , from the Greek \"lekithos\", \"egg yolk\") is a generic term to designate any group of yellow-brownish fatty substances occurring in animal and plant tissues, which are amphiphilic – they attract both water and fatty substances (and so are both hydrophilic and lipophilic), and are used for smoothing food textures, dissolving powders (emulsifying), homogenizing liquid mixtures, and repelling sticking materials.\nLecithins are mixtures of glycerophospholipids including phosphatidylcholine, phosphatidylethanolamine, phosphatidylinositol, phosphatidylserine, and phosphatidic acid.\n\nLecithin was first isolated in 1845 by the French chemist and pharmacist Theodore Gobley. In 1850, he named the phosphatidylcholine \"lécithine\". Gobley originally isolated lecithin from egg yolk—λέκιθος \"lekithos\" is \"egg yolk\" in Ancient Greek—and established the complete chemical formula of phosphatidylcholine in 1874; in between, he had demonstrated the presence of lecithin in a variety of biological matters, including venous blood, in human lungs, bile, human brain tissue, fish eggs, fish roe, and chicken and sheep brain.\n\nLecithin can easily be extracted chemically using solvents such as hexane, ethanol, acetone, petroleum ether, benzene, etc., or extraction can be done mechanically. It is usually available from sources such as soybeans, eggs, milk, marine sources, rapeseed, cottonseed, and sunflower. It has low solubility in water, but is an excellent emulsifier. In aqueous solution, its phospholipids can form either liposomes, bilayer sheets, micelles, or lamellar structures, depending on hydration and temperature. This results in a type of surfactant that usually is classified as amphipathic. Lecithin is sold as a food additive and dietary supplement. In cooking, it is sometimes used as an emulsifier and to prevent sticking, for example in nonstick cooking spray.\n\nLecithin, as a food additive, is also a dietary source of several active compounds: Choline and its metabolites are needed for several physiological purposes, including cell membrane signaling and cholinergic neurotransmission, although its exact function has not been determined, and the involvement of choline in long-term health and development of clinical disorders, such as cardiovascular diseases, cognitive decline in aging and regulation of blood lipid levels, has not been well defined, and remains under research as of 2015.\n\nWhile lecithin is also a rich source of a variety of types of dietary fats, the small amounts of lecithin typically used for food additive purposes mean it is not a significant source of fats. Lecithin is a source for methyl groups via its metabolite, trimethylglycine (betaine) although this is mostly consumed from plants (and is abundant in sugar beets for example).\n\nPhosphatidylcholine occurs in all cellular organisms, being one of the major components of the phospholipid portion of the cell membrane.\n\nCommercial lecithin, as used by food manufacturers, is a mixture of phospholipids in oil. The lecithin can be obtained by water degumming the extracted oil of seeds. It is a mixture of various phospholipids, and the composition depends on the origin of the lecithin. A major source of lecithin is soybean oil. Because of the EU requirement to declare additions of allergens in foods, in addition to regulations regarding genetically modified crops, a gradual shift to other sources of lecithin (e.g., sunflower oil) is taking place. The main phospholipids in lecithin from soya and sunflower are phosphatidyl choline, phosphatidyl inositol, phosphatidyl ethanolamine, phosphatidylserine, and phosphatidic acid. They often are abbreviated to PC, PI, PE, PS and PA, respectively. Purified phospholipids are produced by companies commercially.\n\nTo modify the performance of lecithin to make it suitable for the product to which it is added, it may be hydrolysed enzymatically. In hydrolysed lecithins, a portion of the phospholipids have one fatty acid removed by phospholipase. Such phospholipids are called lysophospholipids. The most commonly used phospholipase is phospholipase A2, which removes the fatty acid at the C2 position of glycerol. Lecithins may also be modified by a process called fractionation. During this process, lecithin is mixed with an alcohol, usually ethanol. Some phospholipids, such as phosphatidylcholine, have good solubility in ethanol, whereas most other phospholipids do not dissolve well in ethanol. The ethanol is separated from the lecithin sludge, after which the ethanol is removed by evaporation to obtain a phosphatidylcholine-enriched lecithin fraction.\n\nAs described above, lecithin is highly processed. Therefore, genetically modified (GM) protein or DNA from the original GM crop from which it is derived often is undetectable – in other words, it is not substantially different from lecithin derived from non-GM crops. Nonetheless, consumer concerns about genetically modified food have extended to highly purified derivatives from GM food, such as lecithin. This concern led to policy and regulatory changes in the European Union in 2000, when Commission Regulation (EC) 50/2000 was passed which required labelling of food containing additives derived from GMOs, including lecithin. Because it is nearly impossible to detect the origin of derivatives such as lecithin, the European regulations require those who wish to sell lecithin in Europe to use a meticulous, but essential system of identity preservation (IP).\n\nLecithin has emulsification and lubricant properties, and is a surfactant. It can be totally metabolized (see Inositol) by humans, so is well tolerated by humans and nontoxic when ingested; some other emulsifiers can only be excreted via the kidneys .\n\nThe major components of commercial soybean-derived lecithin are:\n\nLecithin is used for applications in human food, animal feed, pharmaceuticals, paints, and other industrial applications.\n\nApplications include:\n\nThe nontoxicity of lecithin leads to its use with food, as an additive or in food preparation. It is used commercially in foods requiring a natural emulsifier or lubricant.\n\nIn confectionery, it reduces viscosity, replaces more expensive ingredients, controls sugar crystallization and the flow properties of chocolate, helps in the homogeneous mixing of ingredients, improves shelf life for some products, and can be used as a coating. In emulsions and fat spreads, such as margarines with a high fat content of more than 75%, it stabilizes emulsions, reduces spattering during frying, improves texture of spreads and flavor release. In doughs and bakery, it reduces fat and egg requirements, helps even distribution of ingredients in dough, stabilizes fermentation, increases volume, protects yeast cells in dough when frozen, and acts as a releasing agent to prevent sticking and simplify cleaning. It improves wetting properties of hydrophilic powders (e.g., low-fat proteins) and lipophilic powders (e.g., cocoa powder), controls dust, and helps complete dispersion in water. Lecithin keeps cocoa and cocoa butter in a candy bar from separating. It can be used as a component of cooking sprays to prevent sticking and as a releasing agent.\n\nLecithin is approved by the United States Food and Drug Administration for human consumption with the status \"generally recognized as safe\". Lecithin is admitted by the EU as a food additive, designated as E322.\n\nBecause it contains phosphatidylcholines, lecithin is a source of choline, an essential nutrient. Clinical studies have shown benefit in acne, in improving liver function, and in lowering cholesterol, but older clinical studies in dementia and dyskinesias had found no benefit. However, a more recent double-blind, randomized, placebo-controlled trial in 2003 found that it was necessary to raise the lecithin dose to 1 gram and maintain daily supplementation for at least 90 days in order to see measurable benefits, which were confirmed in \"all testing parameters.\"\n\nAn earlier study using a small sample (20 men divided in 3 groups) did not detect statistically significant short term (2–4 weeks) effects on cholesterol in hyperlipidemic men.\n\nLa Leche League recommends its use to prevent blocked or plugged milk ducts which can lead to mastitis in breastfeeding women.\n\nEgg-derived lecithin is not usually a concern for those allergic to eggs since commercially available egg lecithin is highly purified and devoid of allergy-causing egg proteins.\n\nSoy-derived lecithin is considered by some to be \"kitniyot\" and prohibited on Passover for Ashkenazi Jews when many grain-based foods are forbidden, but not at other times. This does not necessarily affect Sephardi Jews, who do not have the same restrictions on rice and \"kitniyot\" during \"Pesach\"/Passover.\n\nMuslims are not forbidden to eat lecithin \"per se\"; however, since it may be derived from animal as well as plant sources, care must be taken to ensure this source is halal. Lecithin derived from plants and egg yolks is permissible, as is that derived from animals slaughtered according to the rules of \"dhabihah\".\n\nThe key companies operating in the global lecithin market include Archer Daniels Midland Company, Lipoid GmbH, Ruchi Soya Industries Ltd., Cargill Incorporated, Lasenor Emul S.L., Kewpie Corporation, E. I. du Pont de Nemours and Company, Aceitera General Deheza S.A., VAV Life Sciences Pvt. Ltd. and Sternchemie GmbH & Co. KG.\n\nResearch suggests soy-derived lecithin has significant effects on lowering serum cholesterol and triglycerides, while increasing HDL (\"good cholesterol\") levels in the blood of rats. However, a growing body of evidence indicates lecithin is converted by gut bacteria into trimethylamine-N-oxide (TMAO), which is absorbed by the gut and may with time contribute to atherosclerosis and heart attacks.\n\n\n"}
{"id": "14090587", "url": "https://en.wikipedia.org/wiki?curid=14090587", "title": "Low-carbon power", "text": "Low-carbon power\n\nLow-carbon power comes from processes or technologies that produce power with substantially lower amounts of carbon dioxide emissions than is emitted from conventional fossil fuel power generation. It includes low carbon power generation sources such as wind power, solar power, hydropower and nuclear power. The term largely excludes conventional fossil fuel plant sources, and is only used to describe a particular subset of operating fossil fuel power systems, specifically, those that are successfully coupled with a flue gas carbon capture and storage (CCS) system.\n\nOver the past 30 years, significant findings regarding global warming highlighted the need to curb carbon emissions. From this, the idea for low-carbon power was born. The Intergovernmental Panel on Climate Change (IPCC), established by the World Meteorological Organization (WMO) and the United Nations Environment Program (UNEP) in 1988, set the scientific precedence for the introduction of low-carbon power. The IPCC has continued to provide scientific, technical and socio-economic advice to the world community, through its periodic assessment reports and special reports.\n\nInternationally, the most prominent early step in the direction of low carbon power was the signing of the Kyoto Protocol, which came into force on February 16, 2005, under which most industrialized countries committed to reduce their carbon emissions. The historical event set the political precedence for introduction of low-carbon power technology.\n\nOn a social level, perhaps the biggest factor contributing to the general public’s awareness of climate change and the need for new technologies, including low carbon power, came from the documentary \"An Inconvenient Truth\", which clarified and highlighted the problem of global warming.\n\nThe Swedish utility Vattenfall did a study of full life cycle emissions of nuclear, hydro, coal, gas, peat and wind which the utility uses to produce electricity. The result of the study concluded that the grams of CO per kWh of electricity by source are nuclear (5), hydroelectric (9), wind (15), natural gas (503), peat (636), coal (781).\n\nA 2008 meta analysis, \"Valuing the use Gas Emissions from Nuclear Power: A Critical Survey,\" by Benjamin K. Sovacool, analysed 103 life cycle studies of greenhouse gas-equivalent emissions for nuclear power plants. The studies surveyed included the 1997 Vattenfall comparative emissions study, among others. Sovacool's analysis calculated that the mean value of emissions over the lifetime of a nuclear power plant is 66 g/kWh. Comparative results for wind power, hydroelectricity, solar thermal power, and solar photovoltaic, were 9-10 g/kWh, 10-13 g/kWh, 13 g/kWh and 32 g/kWh respectively. Sovacool's analysis has been criticized for poor methodology and data selection.\n\nA 2012 life cycle assessment (LCA) review by Yale University said that \"depending on conditions, median life cycle GHG emissions [for nuclear electricity generation technologies] could be 9 to 110 g -eq/kWh by 2050.\" It stated: \nIt added that for the most common category of reactors, the Light water reactor (LWR):\nThere are many options for lowering current levels of carbon emissions. Some options, such as wind power and solar power, produce low quantities of total life cycle carbon emissions, using entirely renewable sources. Other options, such as nuclear power, produce a comparable amount of carbon dioxide emissions as renewable technologies in total life cycle emissions, but consume non-renewable, but sustainable materials (uranium). The term \"low-carbon power\" can also include power that continues to utilize the world’s natural resources, such as natural gas and coal, but only when they employ techniques that reduce carbon dioxide emissions from these sources when burning them for fuel, such as the, as of 2012, pilot plants performing Carbon capture and storage.\n\nAs the single largest emitter of carbon dioxide in the United States, the electric-power industry accounted for 39% of CO emissions in 2004, a 27% increase since 1990. Because the cost of reducing emissions in the electricity sector appears to be lower than in other sectors such as transportation, the electricity sector may deliver the largest proportional carbon reductions under an economically efficient climate policy.\n\nTechnologies to produce electric power with low-carbon emissions are already in use at various scales. Together, they account for roughly 28% of all U.S. electric-power production, with nuclear power representing the majority (20%), followed by hydroelectric power (7%). However, demand for power is increasing, driven by increased population and per capita demand, and low-carbon power can supplement the supply needed.\n\nAccording to a transatlantic collaborative research paper on Energy return on energy Invested (EROEI), conducted by six analysts led by D. Weißbach, and described as \"...the most extensive overview so far based on a careful evaluation of available Life Cycle Assessments\". It was published in the peer reviewed journal \"Energy\" in 2013. The uncorrected for their intermittency(\"unbuffered\") EROEI for each energy source analyzed is as depicted in the attached table at right. While the buffered (corrected for their intermittency) EROEI stated in the paper for all low-carbon power sources, with the exception of nuclear and biomass, were yet lower still. As when corrected for their weather intermittency/\"buffered\", the EROEI figures for intermittent energy sources as stated in the paper is diminished – a reduction of EROEI dependent on how reliant they are on back up energy sources.\n\nAlthough the methodological integrity of this paper was challenged by, Marco Raugei, in late 2013. The authors of the initial paper responded to each of Raugei's concerns in 2014, and after analysis, each of Raugei's concerns were summarized as \"not scientifically justified\" and based on faulty EROEI understandings due to \"politically motivated energy evaluations\". In a second reply in 2014, Marco Raugei et al. re-explained their criticism and concluded once again that \"there appears to be ample reason to question the reliability of the authors' numerical results, and, most importantly, their internal as well as external combarability to those produced by previously published studies\".\n\nThe 2014 Intergovernmental Panel on Climate Change report identifies nuclear, wind, solar and hydroelectricity in suitable locations as technologies that can provide electricity with less than 5% of the lifecycle greenhouse gas emissions of coal power.\n\nHydroelectric plants have the advantage of being long-lived and many existing plants have operated for more than 100 years. Hydropower is also an extremely flexible technology from the perspective of power grid operation. Large hydropower provides one of the lowest cost options in today’s energy market, even compared to fossil fuels and there are no harmful emissions associated with plant operation. However there are typically low greenhouse gas emissions with reservoirs, and possibly high emissions in the tropics.\n\nHydroelectric power is the world’s largest low carbon source of electricity, supplying 16.6% of total electricity in 2014. China is by far the world's largest producer of hydroelectricity in the world, followed by Brazil and Canada.\n\nHowever, there are several significant social and environmental disadvantages of large-scale hydroelectric power systems: dislocation, if people are living where the reservoirs are planned, release of significant amounts of carbon dioxide and methane during construction and flooding of the reservoir, and disruption of aquatic ecosystems and birdlife. There is a strong consensus now that countries should adopt an integrated approach towards managing water resources, which would involve planning hydropower development in co-operation with other water-using sectors.\n\nNuclear power, with a 10.6% share of world electricity production as of 2013, is the second largest low-carbon power source.\n\nNuclear power, in 2010, also provided two thirds(2/3) of the twenty seven nation European Union's low-carbon energy, with some EU nations sourcing a large fraction of their electricity from nuclear power; for example France derives 79% of its electricity from nuclear.\n\nAccording to the IAEA and the European Nuclear Society, worldwide there were 68 civil nuclear power reactors under construction in 15 countries in 2013. China has 29 of these nuclear power reactors under construction, as of 2013, with plans to build many more, while in the US the licenses of almost half its reactors have been extended to 60 years, and plans to build another dozen are under serious consideration. There is also a considerable number of new reactors being built in South Korea, India, and Russia.\n\nNuclear power's capability to add significantly to future low carbon energy growth depends on several factors, including the economics of new reactor designs, such as Generation III reactors, public opinion and national and regional politics.\n\nThe 104 U.S. nuclear plants are undergoing a Light Water Reactor Sustainability Program, to sustainably extend the life span of the U.S. nuclear fleet by a further 20 years. With further US power plants under construction in 2013, such as the two AP1000s at Vogtle Electric Generating Plant. However the Economics of new nuclear power plants are still evolving and plans to add to those plants are mostly in flux.\n\nWorldwide there are now over two hundred thousand wind turbines operating, with a total nameplate capacity of 238,351 MW as of end 2011, while not correcting for Wind power's comparatively low ~30% capacity factor. The European Union alone passed some 100,000 MW nameplate capacity in September 2012, while the United States surpassed 50,000 MW in August 2012 and China passed 50,000 MW the same month. World wind generation capacity more than quadrupled between 2000 and 2006, doubling about every three years. The United States pioneered wind farms and led the world in installed capacity in the 1980s and into the 1990s. In 1997 German installed capacity surpassed the U.S. and led until once again overtaken by the U.S. in 2008. China has been rapidly expanding its wind installations in the late 2000s and passed the U.S. in 2010 to become the world leader.\n\nAt the end of 2011, worldwide nameplate capacity of wind-powered generators was 238 gigawatts (GW), growing by 40.5 GW of nameplate capacity over the preceding year. Between 2005 and 2010 the average annual growth in new installations was 27.6 percent. According to the World Wind Energy Association, an industry organization, in 2010 wind power generated 430 TWh or about 2.5% of worldwide electricity usage, up from 1.5% in 2008 and 0.1% in 1997. Wind power's share of worldwide electricity usage at the end of 2014 was 3.1%. Several countries have already achieved relatively high levels of penetration, such as 28% of stationary (grid) electricity production in Denmark (2011), 19% in Portugal (2011), 16% in Spain (2011), 14% in Ireland (2010 to 2014) and 8% in Germany (2011). As of 2011, 83 countries around the world were using wind power on a commercial basis.\n\nSolar power is the conversion of sunlight into electricity, either directly using photovoltaics (PV), or indirectly using concentrated solar power (CSP). Concentrated solar power systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. Photovoltaics convert light into electric current using the photoelectric effect.\n\nCommercial concentrated solar power plants were first developed in the 1980s. The 354 MW SEGS CSP installation is the largest solar power plant in the world, located in the Mojave Desert of California. Other large CSP plants include the Solnova Solar Power Station (150 MW) and the Andasol solar power station (150 MW), both in Spain. The over 200 MW Agua Caliente Solar Project in the United States, and the 214 MW Charanka Solar Park in India, are the world’s largest photovoltaic plants. Solar power's share of worldwide electricity usage at the end of 2014 was 1%.\n\nGeothermal electricity is electricity generated from geothermal energy. Technologies in use include dry steam power plants, flash steam power plants and binary cycle power plants. Geothermal electricity generation is used in 24 countries while geothermal heating is in use in 70 countries.\n\nCurrent worldwide installed capacity is 10,715 megawatts (MW), with the largest capacity in the United States (3,086 MW), Philippines, and Indonesia. Estimates of the electricity generating potential of geothermal energy vary from 35 to 2000 GW.\n\nGeothermal power is considered to be sustainable because the heat extraction is small compared to the Earth's heat content. The emission intensity of existing geothermal electric plants is on average 122 kg of per megawatt-hour (MW·h) of electricity, a small fraction of that of conventional fossil fuel plants.\n\nTidal power is a form of hydropower that converts the energy of tides into electricity or other useful forms of power. The first large-scale tidal power plant (the Rance Tidal Power Station) started operation in 1966. Although not yet widely used, tidal power has potential for future electricity generation. Tides are more predictable than wind energy and solar power.\n\nCarbon capture and storage captures carbon dioxide from the flue gas of power plants or other industry, transporting it to an appropriate location where it can be buried securely in an underground reservoir. While the technologies involved are all in use, and carbon capture and storage is occurring in other industries (e.g., at the Sleipner gas field), no large scale integrated project has yet become operational within the power industry.\n\nImprovements to current carbon capture and storage technologies could reduce CO capture costs by at least 20-30% over approximately the next decade, while new technologies under development promise more substantial cost reduction.\n\nThe Intergovernmental Panel on Climate Change stated in its first working group report that “most of the observed increase in globally averaged temperatures since the mid-20th century is very likely due to the observed increase in anthropogenic greenhouse gas concentrations, contribute to climate change.\n\nAs a percentage of all anthropogenic greenhouse gas emissions, carbon dioxide (CO) accounts for 72 percent (see Greenhouse gas), and has increased in concentration in the atmosphere from 315 parts per million (ppm) in 1958 to more than 375 ppm in 2005.\n\nEmissions from energy make up more than 61.4 percent of all greenhouse gas emissions. Power generation from traditional coal fuel sources accounts for 18.8 percent of all world greenhouse gas emissions, nearly double that emitted by road transportation.\n\nEstimates state that by 2020 the world will be producing around twice as much carbon emissions as it was in 2000.\n\nWorld energy consumption is predicted to increase from 421 quadrillion British Thermal Units (BTU) in 2003 to 722 quadrillion BTU in 2030. Coal consumption is predicted to nearly double in that same time. The fastest growth is seen in non-OECD Asian countries, especially China and India, where economic growth drives increased energy use. By implementing low-carbon power options, world electricity demand could continue to grow while maintaining stable carbon emission levels.\n\nIn the transportation sector there are moves away from fossil fuels and towards electric vehicles, such as mass transit and the electric car. These trends are small, but may eventually add a large demand to the electrical grid.\n\nDomestic and industrial heat and hot water have largely been supplied by burning fossil fuels such as fuel oil or natural gas at the consumers' premises. Some countries have begun heat pump rebates to encourage switching to electricity, potentially adding a large demand to the grid.\n\nBy 2015, one-third of the 2007 U.S. coal plants were more than 50 years old. Nearly two-thirds of the generation capacity required to meet power demand in 2030 is yet to be built. There were 151 new coal-fired power plants planned for the U.S., providing 90 GW of power. By 2012, that had dropped to 15, mostly due to new rules limiting mercury emissions, and limiting carbon emissions to 1,000 pounds of CO per megawatt-hour of electricity produced.\n\nInvestment in low-carbon power sources and technologies is increasing at a rapid rate. Zero-carbon power sources produce about 2% of the world's energy, but account for about 18% of world investment in power generation, attracting $100 billion of investment capital in 2006.\n\n"}
{"id": "238246", "url": "https://en.wikipedia.org/wiki?curid=238246", "title": "Megatsunami", "text": "Megatsunami\n\nA megatsunami is a very large wave created by a large, sudden displacement of material into a body of water.\n\nMegatsunamis have quite different features from other, more usual types of tsunamis. Most tsunamis are caused by underwater tectonic activity (movement of the earth's plates) and therefore occur along plate boundaries and as a result of earthquake and rise or fall in the sea floor, causing water to be displaced. Ordinary tsunamis have shallow waves out at sea, and the water piles up to a wave height of up to about 10 metres (33 feet) as the sea floor becomes shallow near land. By contrast, megatsunamis occur when a very large amount of material suddenly falls into water or anywhere near water (such as via a meteor impact), or are caused by volcanic activity. They can have extremely high initial wave heights of hundreds and possibly thousands of metres, far beyond any ordinary tsunami, as the water is \"splashed\" upwards and outwards by the impact or displacement. As a result, two heights are sometimes quoted for megatsunamis – the height of the wave itself (in water), and the height to which it surges when it reaches land, which depending upon the locale, can be several times larger.\n\nModern megatsunamis include the one associated with the 1883 eruption of Krakatoa (volcanic eruption), the 1958 Lituya Bay megatsunami (landslide into a bay), and the wave resulting from the Vajont Dam landslide (caused by human activity destabilizing sides of valley). Prehistoric examples include the Storegga Slide (landslide), and the Chicxulub, Chesapeake Bay and Eltanin meteor impacts.\n\nA megatsunami is a tsunami—a large wave due to displacement of a body of water—with an initial wave amplitude (height) measured in several tens, hundreds, or possibly thousands of metres.\n\nNormal tsunamis generated at sea result from movement of the sea floor. They have a small wave height offshore, are very long (often hundreds of kilometres), and generally pass unnoticed at sea, forming only a slight swell usually of the order of above the normal sea surface. When they reach land, the wave height increases dramatically as the base of the wave pushes the water column above it upwards.\n\nBy contrast, megatsunamis are caused by giant landslides and other impact events. This could also refer to a meteorite hitting an ocean. Underwater earthquakes or volcanic eruptions do not normally generate such large tsunamis, but landslides next to bodies of water resulting from earthquakes can, since they cause a large amount of displacement. If the landslide or impact occurs in a limited body of water, as happened at the Vajont Dam (1963) and Lituya Bay (1958) then the water may be unable to disperse and one or more exceedingly large waves may result.\n\nA way to visualize the difference, is that an ordinary tsunami is caused by sea floor changes, somewhat like pushing up on the floor of a large tub of water to the point it overflows, and causing a surge of water to \"run off\" at the sides. In this analogy, a megatsunami would be more similar to dropping a large rock from a considerable height into the tub, at one end, causing water to splash up and out, and overflow at the other end.\n\nTwo heights are sometimes quoted for megatsunamis – the height of the wave itself (in water), and the height to which it surges when it reaches land, which depending upon the locale, can be several times larger.\n\nGeologists searching for oil in Alaska in 1953 observed that in Lituya Bay, mature tree growth did not extend to the shoreline as it did in many other bays in the region. Rather, there was a band of younger trees closer to the shore. Forestry workers, glaciologists, and geographers call the boundary between these bands a trim line. Trees just above the trim line showed severe scarring on their seaward side, whilst those from below the trim line did not. The scientists hypothesized that there had been an unusually large wave or waves in the deep inlet. Because this is a recently deglaciated fjord with steep slopes and crossed by a major fault, one possibility was a landslide-generated tsunami.\n\nOn 9 July 1958, a 7.8 strike-slip earthquake in southeast Alaska caused 90 million tonnes of rock and ice to drop into the deep water at the head of Lituya Bay. The block fell almost vertically and hit the water with sufficient force to create a wave that surged up the opposite side of the head of the bay to a height of 1720 feet (524 m), and was still many tens of metres high further down the bay, when it carried eyewitnesses Howard Ulrich and his son Howard Jr. over the trees in their fishing boat. They were washed back into the bay and both survived.\n\nThe mechanism giving rise to megatsunamis was analysed for the Lituya Bay event in a study presented at the Tsunami Society in 1999; this model was considerably developed and modified by a second study in 2010.\n\nAlthough the earthquake which caused the megatsunami was considered very energetic, and involving strong ground movements, several possible mechanisms were not likely or able to have caused the resulting megatsunami. Neither water drainage from a lake, nor landslide, nor the force of the earthquake itself led to the megatsunami, although all of these may have contributed.\n\nInstead, the megatsunami was caused by a massive and sudden impulsive impact when about 40 million cubic yards of rock several hundred metres above the bay was fractured from the side of the bay, by the earthquake, and fell \"practically as a monolithic unit\" down the almost vertical slope and into the bay. The rockfall also caused air to be \"dragged along\" due to viscosity effects, which added to the volume of displacement, and further impacted the sediment on the floor of the bay, creating a large crater. The study concluded that:\n\nA 2010 model examined the amount of infill on the floor of the bay, which was many times larger than that of the rockfall alone, and also the energy and height of the waves, and the accounts given by eyewitnesses, concluded that there had been a \"dual slide\" involving a rockfall, which also triggered a release of 5 to 10 times its volume of sediment trapped by the adjacent Lituya Glacier, as an almost immediate and many times larger second slide, a ratio comparable with other events where this \"dual slide\" effect is known to have happened.\n\n\n\n\nIn 1792, Mount Unzen in Japan erupted, causing part of the volcano to collapse into the sea. The landslide caused a megatsunami that reached high and killed 15,000 people in the local fishing villages.\n\nThe eruption of Krakatoa created pyroclastic flows which generated megatsunamis when they hit the waters of the Sunda Strait on 27 August 1883. The waves reached heights of up to 24 metres (79 feet) along the south coast of Sumatra and up to 42 metres (138 feet) along the west coast of Java.\n\nOn July 9, 1958, a giant landslide at the head of Lituya Bay in Alaska, caused by an earthquake, generated a wave that washed out trees to a maximum altitude of at the entrance of Gilbert Inlet. The wave surged over the headland, stripping trees and soil down to bedrock, and surged along the fjord which forms Lituya Bay, destroying a fishing boat anchored there and killing two people.\n\nOn October 9, 1963, a landslide above Vajont Dam in Italy produced a surge that overtopped the dam and destroyed the villages of Longarone, Pirago, Rivalta, Villanova and Faè, killing nearly 2,000 people.\n\nOn May 18, 1980, the upper 460 metres (1,509 feet) of Mount St. Helens collapsed, creating a massive landslide. This released the pressure on the magma trapped beneath the summit bulge which exploded as a lateral blast, which then released the pressure on the magma chamber and resulted in a plinian eruption.\n\nOne lobe of the avalanche surged onto Spirit Lake, causing a megatsunami which pushed the lake waters in a series of surges, which reached a maximum height of 260 metres (853 feet) above the pre-eruption water level (~975 m asl/3,199 ft). Above the upper limit of the tsunami, trees lie where they were knocked down by the pyroclastic surge; below the limit, the fallen trees and the surge deposits were removed by the megatsunami and deposited in Spirit Lake.\n\nIn a BBC television documentary broadcast in 2000, experts said that they thought that a massive landslide on a volcanic ocean island is the most likely future cause of a megatsunami. The size and power of a wave generated by such means could produce devastating effects, travelling across oceans and inundating up to inland from the coast. This research, however, was later found to be flawed. The documentary was produced before the experts' scientific paper was published and before responses were given by other geologists. There have been megatsunamis in the past, and future megatsunamis are possible but current geological consensus is that these are only local. A megatsunami in the Canary Islands would diminish to a normal tsunami by the time it reached the continents. Also, the current consensus for La Palma is that the region conjectured to collapse is too small and too geologically stable to do so in the next 10,000 years, although there is evidence for past megatsunamis local to the Canary Isles thousands of years ago. Similar remarks apply to the suggestion of a megatsunami in Hawaii.\n\nSome geologists consider an unstable rock face at Mount Breakenridge, above the north end of the giant fresh-water fjord of Harrison Lake in the Fraser Valley of southwestern British Columbia, Canada, to be unstable enough to collapse into the lake, generating a megatsunami that might destroy the town of Harrison Hot Springs (located at its south end).\n\nGeologists Dr. Simon Day and Dr. Steven Neal Ward consider that a megatsunami could be generated during an eruption of Cumbre Vieja on the volcanic ocean island of La Palma, in the Canary Islands, Spain.\n\nIn 1949, this volcano erupted at its Duraznero, Hoyo Negro and Llano del Banco vents, and there was an earthquake with an epicentre near the village of Jedey. The next day Juan Bonelli Rubio, a local geologist, visited the summit area and found that a fissure about long had opened on the east side of the summit. As a result, the west half of the volcano (which is the volcanically active arm of a triple-armed rift) had slipped about downwards and westwards towards the Atlantic Ocean.\n\nCumbre Vieja is currently dormant, but will almost certainly erupt again. Day and Ward hypothesize that if such an eruption causes the western flank to fail, a mega-tsunami could be generated.\n\nLa Palma is currently the most volcanically active island in the Canary Islands Archipelago. It is likely that several eruptions would be required before failure would occur on Cumbre Vieja. However, the western half of the volcano has an approximate volume of and an estimated mass of . If it were to catastrophically slide into the ocean, it could generate a wave with an initial height of about at the island, and a likely height of around at the Caribbean and the Eastern North American seaboard when it runs ashore eight or more hours later. Tens of millions of lives could be lost in the cities and/or towns of St. John's, Halifax, Boston, New York, Baltimore, Washington, D.C., Miami, Havana and the rest of the Eastern Coasts of the United States and Canada, as well as many other cities on the Atlantic coast in Europe, South America and Africa. The likelihood of this happening is a matter of vigorous debate.\n\nThe last eruption on the Cumbre Vieja occurred in 1971 at the Teneguia vent at the southern end of the sub-aerial section without any movement. The section affected by the 1949 eruption is currently stationary and does not appear to have moved since the initial rupture.\n\nGeologists and volcanologists are in general agreement that the initial study was flawed. The current geology does not suggest that a collapse is imminent. Indeed, it seems to be geologically impossible right now, the region conjectured as prone to collapse is too small and too stable to collapse within the next 10,000 years. They also concluded that a landslide is likely to happen as a series of smaller collapses rather than a single landslide from closer study of deposits left in the ocean by previous landslides. A megatsunami does seem possible locally in the distant future as there is geological evidence from past deposits suggesting that a megatsunami occurred with marine material deposited 41 to 188 meters above sea level between 32,000 and 1.75 million years ago. This seems to have been local to Gran Canaria.\n\nDay and Ward have admitted that their original analysis of the danger was based on several worst case assumptions. A 2008 paper looked into this very worst-case scenario, the most massive slide that could happen (though unlikely and probably impossible right now with the present day geology). Although it would be a megatsunami locally in the Canary Isles, it would diminish in height to a regular tsunami when it reaches the continents as the waves interfere and spread across the oceans.\n\nFor more details see Cumbre Vieja#Criticism\n\nSharp cliffs and associated ocean debris at the Kohala Volcano, Lanai and Molokai indicate that landslides from the flank of the Kilauea and Mauna Loa volcanoes in Hawaii may have triggered past megatsunamis, most recently at 120,000 BP. A tsunami event is also possible, with the tsunami potentially reaching up to about in height According to the documentary \"National Geographic's Ultimate Disaster: Tsunami\", if a big landslide occurred at Mauna Loa or the Hilina Slump, a tsunami would take only thirty minutes to reach Honolulu. There, hundreds of thousands of people could be killed as the tsunami could level Honolulu and travel inland. Also, the West Coast of America and the entire Pacific Rim could potentially be affected.\n\nHowever, other research suggests that such a single large landslide is not likely. Instead, it would collapse as a series of smaller landslides.\n\nIn 2018, shortly after the beginning of the 2018 lower Puna eruption, a National Geographic article responded to such claims with \"Will a monstrous landslide off the side of Kilauea trigger a monster tsunami bound for California? Short answer: No.\"\n\nIn the same article, geologist Mika McKinnon stated:\n\nAnother volcanologist, Janine Krippner, added:\n\nSteep cliffs on the Cape Verde Islands have been caused by catastrophic debris avalanches. These have been common on the submerged flanks of ocean island volcanoes and more can be expected in the future.\n\n\n\n"}
{"id": "7269291", "url": "https://en.wikipedia.org/wiki?curid=7269291", "title": "Merseyside Waste Disposal Authority", "text": "Merseyside Waste Disposal Authority\n\nThe formerly named Merseyside Waste Disposal Authority (MWDA) is a statutory Waste Disposal Authority that manages the municipal solid waste produced in Merseyside, England. MWDA was established in 1986 following the abolition of Merseyside County Council, to undertake the waste disposal for local authorities across Merseyside – Liverpool, Knowsley, Sefton, Wirral and St Helens. MWDA takes a lead in advocating waste minimisation, recycling and the safe and effective disposal of waste for Merseyside’s residents and operates 14 Household Waste Recycling Centres. Veolia Environmental Services is the current contractor for the Authority's major waste services contract.\n\nThe Authority's Revenue Budget for 2010/2011 was set at £70.9m and is funded by way of a levy on each of the Merseyside District Councils which is apportioned on a waste tonnage basis. \n\nThe Authority is also responsible for the aftercare of a number of closed landfill sites, which historically had been operated by the Authority or its predecessors. These include the former landfill sites at Bidston Moss and Sefton Meadows.\n\nMWDA is made up of nine elected Members representing the five constituent district councils in the Merseyside area and consists of five Labour councillors, three Liberal Democrat councillors and one Conservative councillor.\n\nAs of Wednesday, December 7th, 2011, the MWDA was renamed the Merseyside Recycling and Waste Authority (MRWA) and a new corporate logo with slogan \"Merseyside ... A place where nothing is wasted\" was introduced to replace the former corporate MWDA logo.\n\n"}
{"id": "32270935", "url": "https://en.wikipedia.org/wiki?curid=32270935", "title": "Metal Stocks in Society report", "text": "Metal Stocks in Society report\n\nThe report Metal Stocks in Society: Scientific Synthesis was the first of six scientific assessments on global metals to be published by the International Resource Panel (IRP) of the United Nations Environment Programme. The IRP provides independent scientific assessments and expert advice on a variety of areas, including:\n\nMetals were an early priority for the International Resource Panel, since little was known about them, their impacts, their economic importance or their scarcity. The report aimed to calculate the amount of metals present in society and assess the potential for utilising in-use stock to offset demand from virgin metal. Knowing how much metal stock there is in use, and how long the lifespan of the metal is, can help planners know when these metal stocks will enter recycling or waste streams. It suggested that these 'mines above ground' had growing potential for future metals supply; the authors found that there is about 50 kg of above-ground copper for every person on earth, and more than two tons of iron per capita. However, they noted that enormous disparities in global metals stocks existed between developed and developing nations including Brazil, China and India.\n\nCalculating ‘anthropogenic stocks’ of metals already in use in society is a precursor to stimulating efforts to increase recycling. The authors reported that very little information is presently known about different metals, making it difficult for policy makers to develop and plan recycling systems. However, what is known is that recycling can not only reduce negative impacts on the environment but also save energy. For example, 95% of the energy used to make aluminium from bauxite ore is saved by using recycled material.\n\nThe authors quantified per capita stocks of the following metals, for some countries (stocks were mostly quantified for developed countries but data was also available for some developing nations).\n\nExtant in-use metal-stock estimations for the major engineering metals:\n\nSome primary stocks of rare but useful metals are already running low. For example, rhenium only occurs at seven parts per billion in the Earth’s crust, making it one of the rarest elements on the planet. However, its high melting point of 3,186 °C makes it valuable in the manufacture of jet engines. Demand for the metal is rising, with increasing air travel, but its rarity means increasing extraction is not simple. This is where recycling comes in; rhenium is one of the few metals that has witnessed a rise in recycling rates. It is likely that recycling will become a more viable option than extraction for other metals in future, saving energy, cutting greenhouse gases emitted to the atmosphere and reducing negative impacts on the environment.\n\n"}
{"id": "37630", "url": "https://en.wikipedia.org/wiki?curid=37630", "title": "Neutron bomb", "text": "Neutron bomb\n\nA neutron bomb, officially defined as a type of enhanced radiation weapon (ERW), is a low yield thermonuclear weapon designed to maximize lethal neutron radiation in the immediate vicinity of the blast while minimizing the physical power of the blast itself. The neutron release generated by a nuclear fusion reaction is intentionally allowed to escape the weapon, rather than being absorbed by its other components. The neutron burst, which is used as the primary destructive action of the warhead, is able to penetrate enemy armor more effectively than a conventional warhead, thus making it more lethal as a tactical weapon.\n\nThe concept was originally developed by the US in the late 1950s and early 1960s. It was seen as a \"cleaner\" bomb for use against massed Soviet armored divisions. As these would be used over allied nations, notably West Germany, the reduced blast damage was seen as an important advantage. \n\nERWs were first operationally deployed for anti-ballistic missiles (ABM). In this role the burst of neutrons would cause nearby warheads to undergo partial fission, preventing them from exploding properly. For this to work, the ABM would have to explode within ca. of its target. The first example of such a system was the W66, used on the Sprint missile used in the US's Nike-X system. It is believed the Soviet equivalent, the A-135's 53T6 missile, uses a similar design.\n\nThe weapon was once again proposed for tactical use by the US in the 1970s and 1980s, and production of the W70 began for the MGM-52 Lance in 1981. This time it experienced a firestorm of protest as the growing anti-nuclear movement gained strength through this period. Opposition was so intense that European leaders refused to accept it on their territory. President Ronald Reagan bowed to pressure and the built examples of the W70-3 remained stockpiled in the US until they were retired in 1992. The last W70 was dismantled in 2011.\n\nIn a standard thermonuclear design, a small fission bomb is placed close to a larger mass of thermonuclear fuel. The two components are then placed within a thick radiation case, usually made from uranium, lead or steel. The case traps the energy from the fission bomb for a brief period, allowing it to heat and compress the main thermonuclear fuel. The case is normally made of depleted uranium or natural uranium metal, because the thermonuclear reactions give off massive numbers of high-energy neutrons that can cause fission reactions in the casing material. These can add considerable energy to the reaction; in a typical design as much as 50% of the total energy comes from fission events in the casing. For this reason, these weapons are technically known as fission-fusion-fission designs.\n\nIn a neutron bomb, the casing material is selected either to be transparent to neutrons or to actively enhance their production. The burst of neutrons created in the thermonuclear reaction is then free to escape the bomb, outpacing the physical explosion. By designing the thermonuclear stage of the weapon carefully, the neutron burst can be maximized while minimizing the blast itself. This makes the lethal radius of the neutron burst greater than that of the explosion itself. Since the neutrons disappear from the environment rapidly, such a burst over an enemy column would kill the crews and leave the area able to be quickly reoccupied.\n\nCompared to a pure fission bomb with an identical explosive yield, a neutron bomb would emit about ten times the amount of neutron radiation. In a fission bomb, at sea level, the total radiation pulse energy which is composed of both gamma rays and neutrons is approximately 5% of the entire energy released; in neutron bombs it would be closer to 40%, with the percentage increase coming from the higher production of neutrons. Furthermore, the neutrons emitted by a neutron bomb have a much higher average energy level (close to 14 MeV) than those released during a fission reaction (1–2 MeV).\n\nTechnically speaking, every low yield nuclear weapon is a radiation weapon, including non-enhanced variants. Up to about 10 kilotons in yield, all nuclear weapons have prompt neutron radiation as their furthest-reaching lethal component, after which point the lethal blast and thermal effects radius begins to out-range the lethal ionizing radiation radius. Enhanced radiation weapons also fall into this same yield range and simply enhance the intensity and range of the neutron dose for a given yield.\n\nThe conception of neutron bombs is generally credited to Samuel T. Cohen of the Lawrence Livermore National Laboratory, who developed the concept in 1958. Initial development was carried out as part of projects Dove and Starling, and an early device was tested underground in early 1962. Designs of a \"weaponized\" version were carried out in 1963.\n\nDevelopment of two production designs for the army's MGM-52 Lance short-range missile began in July 1964, the W63 at Livermore and the W64 at Los Alamos. Both entered phase three testing in July 1964, and the W64 was cancelled in favor of the W63 in September 1964. The W63 was in turn cancelled in November 1965 in favor of the W70 (Mod 0), a conventional design. By this time, the same concepts were being used to develop warheads for the Sprint missile, an anti-ballistic missile (ABM), with Livermore designing the W65 and Los Alamos the W66. Both entered phase three testing in October 1965, but the W65 was cancelled in favor of the W66 in November 1968. Testing of the W66 was carried out in the late 1960s, and entered production in June 1974, the first neutron bomb to do so. Approximately 120 were built, with about 70 of these being on active duty during 1975 and 1976 as part of the Safeguard Program. When that program was shut down they were placed in storage, and eventually decommissioned in the early 1980s.\n\nDevelopment of ER warheads for Lance continued, but in the early 1970s attention had turned to using modified versions of the W70, the W70 Mod 3. Development was subsequently postponed by President Jimmy Carter in 1978 following protests against his administration's plans to deploy neutron warheads to ground forces in Europe. On November 17, 1978, in a test the USSR detonated its first similar-type bomb. President Ronald Reagan restarted production in 1981. The Soviet Union renewed a propaganda campaign against the US's neutron bomb in 1981 following Reagan's announcement. In 1983 Reagan then announced the Strategic Defense Initiative, which surpassed neutron bomb production in ambition and vision and with that, neutron bombs quickly faded from the center of the public's attention.\n\nThree types of enhanced radiation weapons (ERW) were deployed by the United States. The W66 warhead, for the anti-ICBM Sprint missile system, was deployed in 1975 and retired the next year, along with the missile system. The W70 Mod 3 warhead was developed for the short-range, tactical MGM-52 Lance missile, and the W79 Mod 0 was developed for nuclear artillery shells. The latter two types were retired by President George H. W. Bush in 1992, following the end of the Cold War. The last W70 Mod 3 warhead was dismantled in 1996, and the last W79 Mod 0 was dismantled by 2003, when the dismantling of all W79 variants was completed.\n\nAccording to the Cox Report, as of 1999 the United States had never deployed a neutron weapon. The nature of this statement is not clear; it reads \"The stolen information also includes classified design information for an enhanced radiation weapon (commonly known as the \"neutron bomb\"), which neither the United States, nor any other nation, has ever deployed.\" However, the fact that neutron bombs had been produced by the US was well known at this time and part of the public record. Cohen suggests the report is playing with the definitions; while the US bombs were never deployed \"to Europe\", they remained stockpiled in the US.\n\nIn addition to the two superpowers, France and China are known to have tested neutron or enhanced radiation bombs. France conducted an early test of the technology in 1967 and tested an \"actual\" neutron bomb in 1980. China conducted a successful test of neutron bomb principles in 1984 and a successful test of a neutron bomb in 1988. However, neither of those countries chose to deploy neutron bombs. Chinese nuclear scientists stated before the 1988 test that China had no need for neutron bombs, but it was developed to serve as a \"technology reserve\", in case the need arose in the future.\n\nIn August, 1999, the Indian government disclosed that India was capable of producing a neutron bomb.\n\nAlthough no country is currently known to deploy them in an offensive manner, all thermonuclear dial-a-yield warheads that have about 10 kiloton and lower as one dial option, with a considerable fraction of that yield derived from fusion reactions, can be considered able to be neutron bombs in use, if not in name. The only country definitely known to deploy dedicated (that is, not dial-a-yield) neutron warheads for any length of time is the Soviet Union/Russia, which inherited the USSR's neutron warhead equipped ABM-3 Gazelle missile program. This ABM system contains at least 68 neutron warheads with a 10 kiloton yield each and it has been in service since 1995, with inert missile testing approximately every other year since then (2014). The system is designed to destroy incoming \"endoatmospheric\" level nuclear warheads aimed at Moscow and other targets and is the lower-tier/last umbrella of the A-135 anti-ballistic missile system (NATO reporting name: ABM-3).\n\nBy 1984, according to Mordechai Vanunu, Israel was mass-producing neutron bombs. \n\nConsiderable controversy arose in the US and Western Europe following a June 1977 \"Washington Post\" exposé describing US government plans to purchase the bomb. The article focused on the fact that it was the first weapon specifically intended to kill humans with radiation. Lawrence Livermore National Laboratory director Harold Brown and Soviet General Secretary Leonid Brezhnev both described neutron bombs as a \"capitalist bomb\", because it was designed to destroy people while preserving property. Science fiction author and commentator Isaac Asimov also stated that \"Such a neutron bomb or N bomb seems desirable to those who worry about property and hold life cheap.\"\n\nNeutron bombs are purposely designed with explosive yields lower than other nuclear weapons. Since neutrons are scattered and absorbed by air, neutron radiation effects drop off rapidly with distance in air. As such, there is a sharper distinction, relative to thermal effects, between areas of high lethality and areas with minimal radiation doses. All high yield (more than c. 10 kiloton) nuclear bombs, such as the extreme example of a device that derived 97% of its energy from fusion, the 50 megaton Tsar Bomba, are not able to radiate sufficient neutrons beyond their lethal blast range when detonated as a surface burst or low altitude air burst and so are no longer classified as neutron bombs, thus limiting the yield of neutron bombs to a maximum of about 10 kilotons. The intense pulse of high-energy neutrons generated by a neutron bomb is the principal killing mechanism, not the fallout, heat or blast.\n\nThe inventor of the neutron bomb, Sam Cohen, criticized the description of the W70 as a neutron bomb since it could be configured to yield 100 kilotons:\n\nAlthough neutron bombs are commonly believed to \"leave the infrastructure intact\", with current designs that have explosive yields in the low kiloton range, detonation in (or above) a built-up area would still cause a sizable degree of building destruction, through blast and heat effects out to a moderate radius, albeit considerably less destruction, than when compared to a standard nuclear bomb of the \"exact\" same total energy release or \"yield\".\n\nThe Warsaw Pact tank strength was over twice that of NATO, and Soviet deep battle doctrine was likely to be to use this numerical advantage to rapidly sweep across continental Europe if the Cold War ever turned hot. Any weapon that could break up their intended mass tank formation deployments and force them to deploy their tanks in a thinner, more easily dividable manner, would aid ground forces in the task of hunting down solitary tanks and using anti-tank missiles against them, such as the contemporary M47 Dragon and BGM-71 TOW missiles, of which NATO had hundreds of thousands.\n\nRather than making extensive preparations for battlefield nuclear combat in Central Europe, \"The Soviet military leadership believed that conventional superiority provided the Warsaw Pact with the means to approximate the effects of nuclear weapons and achieve victory in Europe without resort to those weapons.\"\n\nNeutron bombs, or more precisely, enhanced [neutron] radiation weapons were also to find use as strategic anti-ballistic missile weapons, and in this role they are believed to remain in active service within Russia's Gazelle missile.\n\nUpon detonation, a near-ground airburst of a 1 kiloton neutron bomb would produce a large blast wave and a powerful pulse of both thermal radiation and ionizing radiation, and non-ionizing radiation in the form of fast (14.1 MeV) neutrons. The thermal pulse would cause third degree burns to unprotected skin out to approximately 500 meters. The blast would create pressures of at least 4.6 psi out to a radius of 600 meters, which would severely damage all non-reinforced concrete structures. At the conventional effective combat range against modern main battle tanks and armored personnel carriers (< 690–900 m), the blast from a 1 kt neutron bomb would destroy or damage to the point of non-usability of almost all un-reinforced civilian buildings.\n\nUsing neutron bombs to stop an enemy armored attack by rapidly incapacitating crews with a dose of 8000+ rads of radiation would require exploding large numbers of them to blanket the enemy forces, destroying all normal civilian buildings within c. 600 meters of the immediate area. Neutron activation from the explosions could make many building materials in the city radioactive, such as zinc coated steel/galvanized steel (see area denial use below).\n\nBecause liquid-filled objects like the human body are resistant to gross overpressure, the 4–5 psi blast overpressure would cause very few direct casualties at a range of c. 600 m. The powerful winds produced by this overpressure, however, could throw bodies into objects or throw debris at high velocity, including window glass, both with potentially lethal results. Casualties would be highly variable depending on surroundings, including potential building collapses.\n\nThe pulse of neutron radiation would cause immediate and permanent incapacitation to unprotected outdoor humans in the open out to 900 meters, with death occurring in one or two days. The median lethal dose (LD) of 600 rads would extend to between 1350 and 1400 meters for those unprotected and outdoors, where approximately half of those exposed would die of radiation sickness after several weeks.\n\nA human residing within, or simply shielded by, at least one concrete building with walls and ceilings thick, or alternatively of damp soil 24 inches thick, would receive a neutron radiation exposure reduced by a factor of 10. Even near ground zero, basement sheltering or buildings with similar radiation shielding characteristics would drastically reduce the radiation dose.\n\nFurthermore, the neutron absorption spectrum of air is disputed by some authorities, and depends in part on absorption by hydrogen from water vapor. Thus, absorption might vary exponentially with humidity, making neutron bombs far more deadly in desert climates than in humid ones.\n\nThe questionable effectiveness of ER weapons against modern tanks is cited as one of the main reasons that these weapons are no longer fielded or stockpiled. With the increase in average tank armor thickness since the first ER weapons were fielded, it was argued in the March 13, 1986 \"New Scientist\" magazine, that tank armor protection, approaches the level where tank crews are now almost fully protected from radiation effects. Thus, for an ER weapon to incapacitate a modern tank crew through irradiation, the weapon must now be detonated at such a close proximity to the tank that the nuclear explosion's blast would now be equally effective at incapacitating it and its crew. However this assertion was regarded as dubious in the 12 June, 1986 \"New Scientist\" reply by C.S. Grace, a member of the Royal Military College of Science, as neutron radiation from a 1 kiloton neutron bomb would incapacitate the crew of a tank with a protection factor of 35 out to a range of 280 meters, but the incapacitating blast range, depending on the exact weight of the tank, is much less, from 70 to 130 meters. However although the author did note that effective neutron absorbers and neutron poisons such as boron carbide can be incorporated into conventional armor and strap-on neutron moderating hydrogenous material (substances containing hydrogen atoms), such as explosive reactive armor, can both increase the protection factor, the author holds that in practice combined with neutron scattering, the actual average total tank area protection factor is rarely higher than 15.5 to 35. According to the Federation of American Scientists, the neutron protection factor of a \"tank\" can be as low as 2, without qualifying whether the statement implies a light tank, medium tank, or main battle tank.\n\nA composite high density concrete, or alternatively, a laminated graded-Z shield, 24 units thick of which 16 units are iron and 8 units are polyethylene containing boron (BPE), and additional mass behind it to attenuate neutron capture gamma rays, is more effective than just 24 units of pure iron or BPE alone, due to the advantages of both iron and BPE in combination. During Neutron transport Iron is effective in slowing down/scattering high-energy neutrons in the 14-MeV energy range and attenuating gamma rays, while the hydrogen in polyethylene is effective in slowing down these now slower fast neutrons in the few MeV range, and boron 10 has a high absorption cross section for thermal neutrons and a low production yield of gamma rays when it absorbs a neutron. The Soviet T72 tank, in response to the neutron bomb threat, is cited as having fitted a boronated polyethylene liner, which has had its neutron shielding properties simulated.\nHowever, some tank armor material contains depleted uranium (DU), common in the US's M1A1 Abrams tank, which incorporates steel-encased depleted uranium armor, a substance that will fast fission when it captures a fast, fusion-generated neutron, and thus on fissioning will produce fission neutrons and fission products embedded within the armor, products which emit among other things, penetrating gamma rays. Although the neutrons emitted by the neutron bomb may not penetrate to the tank crew in lethal quantities, the fast fission of DU within the armor could still ensure a lethal environment for the crew and maintenance personnel by fission neutron and gamma ray exposure, largely depending on the exact thickness and elemental composition of the armor—information usually hard to attain. Despite this, Ducrete—which has an elemental composition similar (but not identical) to the ceramic second generation heavy metal Chobham armor of the Abrams tank—is an effective radiation shield, to both \"fission\" neutrons and gamma rays due to it being a graded Z material. Uranium, being about twice as dense as lead, is thus nearly twice as effective at shielding gamma ray radiation per unit thickness.\n\nAs an anti-ballistic missile weapon, the first fielded ER warhead, the W66, was developed for the Sprint missile system as part of the Safeguard Program to protect United States cities and missile silos from incoming Soviet warheads.\n\nA problem faced by Sprint and similar ABMs was that the blast effects of their warheads change greatly as they climb and the atmosphere thins out. At higher altitudes, starting around and above, the blast effects begin to drop off rapidly as the air density becomes very low. This can be countered by using a larger warhead, but then it becomes too powerful when used at lower altitudes. An ideal system would use a mechanism that was less sensitive to changes in air density.\n\nNeutron-based attacks offer one solution to this problem. The burst of neutrons released by an ER weapon can induce fission in the fissile materials of primary in the target warhead. The energy released by these reactions may be enough to melt the warhead, but even at lower fission rates the \"burning up\" of some of the fuel in the primary can cause it to fail to explode properly, or \"fizzle\". Thus a small ER warhead can be effective across a wide altitude band, using blast effects at lower altitudes and the increasingly long-ranged neutrons as the engagement rises.\n\nThe use of neutron-based attacks was discussed as early as the 1950s, with the US Atomic Energy Commission mentioning weapons with a \"clean, enhanced neutron output\" for use as \"antimissile defensive warheads.\" Studying, improving and defending against such attacks was a major area of research during the 1950s and 60s. A particular example of this is the US Polaris A-3 missile, which delivered three warheads travelling on roughly the same trajectory, and thus with a short distance between them. A single ABM could conceivably destroy all three through neutron flux. Developing warheads that were less sensitive to these attacks was a major area of research in the US and UK during the 1960s.\n\nSome sources claim that the neutron flux attack was also the main design goal of the various nuclear-tipped anti-aircraft weapons like the AIM-26 Falcon and CIM-10 Bomarc. One F-102 pilot noted:\n\nIt has also been suggested that neutron flux's effects on the warhead electronics are another attack vector for ER warheads in the ABM role. Ionization greater than 5,000 rads in silicon chips delivered over seconds to minutes will degrade the function of semiconductors for long periods. However, while such attacks might be useful against guidance systems which used relatively advanced electronics, in the ABM role these components have long ago separated from the warheads by the time they come within range of the interceptors. The electronics in the warheads themselves tend to be very simple, and hardening them was one of the many issues studied in the 1960s.\n\nLithium-6 hydride (Li6H) is cited as being used as a countermeasure to reduce the vulnerability and \"harden\" nuclear warheads from the effects of externally generated neutrons.\nRadiation hardening of the warhead's electronic components as a countermeasure to high altitude neutron warheads somewhat reduces the range that a neutron warhead could successfully cause an unrecoverable glitch by the \"transient radiation effects on electronics\" (TREE) effects.\n\nAt very high altitudes, at the edge of the atmosphere and above it, another effect comes into play. At lower altitudes, the x-rays generated by the bomb are absorbed by the air and have mean free paths on the order of meters. But as the air thins out, the x-rays can travel further, eventually outpacing the area of effect of the neutrons. In exoatmospheric explosions, this can be on the order of in radius. In this sort of attack, it is the x-rays promptly delivering energy on the warhead surface that is the active mechanism; the rapid ablation (or \"blow off\") of the surface creates shock waves that can break up the warhead.\n\nIn November 2012, during the planning stages of Operation Hammer of God, British Labour peer Lord Gilbert suggested that multiple enhanced radiation reduced blast (ERRB) warheads could be detonated in the mountain region of the Afghanistan-Pakistan border to prevent infiltration. He proposed to warn the inhabitants to evacuate, then irradiate the area, making it unusable and impassable. Used in this manner, the neutron bomb(s), regardless of burst height, would release neutron activated casing materials used in the bomb, and depending on burst height, create radioactive soil activation products.\n\nIn much the same fashion as the area denial effect resulting from fission product (the substances that make up most fallout) contamination in an area following a conventional surface burst nuclear explosion, as considered in the Korean War by Douglas MacArthur, it would thus be a form of radiological warfare—with the difference that neutron bombs produce half, or less, of the quantity of fission products relative to the same-yield pure fission bomb. Radiological warfare with neutron bombs that rely on fission primaries would thus still produce fission fallout, albeit a comparatively \"cleaner\" and shorter lasting version of it in the area than if air bursts were used, as little to no fission products would be deposited on the direct immediate area, instead becoming diluted global fallout.\n\nHowever the most effective use of a neutron bomb with respect to area denial would be to encase it in a thick shell of material that could be neutron activated, and use a surface burst. In this manner the neutron bomb would be turned into a \"salted bomb\"; a case of zinc-64, produced as a byproduct of depleted zinc oxide enrichment, would for example probably be the most attractive for military use, as when activated, the zinc-65 so formed is a gamma emitter, with a half life of 244 days.\n\nNeutron bombs-warheads require considerable maintenance for their abilities, requiring some tritium for fusion boosting and tritium in the secondary stage (yielding more neutrons), in amounts on the order of a few tens of grams (10–30 grams estimated). Because tritium has a relatively short half-life of 12.32 years (after that time, half the tritium has decayed), it is necessary to replenish it periodically to keep the bomb effective. (For instance: to maintain a constant level of 24 grams of tritium in a warhead, about 1.3 grams per bomb per year must be supplied.) Moreover, tritium decays into helium-3, which absorbs neutrons and will thus further reduce the bomb's neutron yield.\n\nWith considerable overlap between the two devices, the prompt radiation effects of a pure fusion weapon would similarly be much higher than that of a pure-fission device: approximately twice the initial radiation output of current standard fission-fusion-based weapons. In common with all neutron bombs that must presently derive a small percentage of trigger energy from fission, in any given yield a 100% pure fusion bomb would likewise generate a more diminutive atmospheric blast wave than a \"pure\"-fission bomb. The latter fission device has a higher kinetic energy-ratio per unit of reaction energy released, which is most notable in the comparison with the D-T fusion reaction. A larger percentage of the energy from a D-T fusion reaction, is inherently put into uncharged neutron generation as opposed to charged particles, such as the alpha particle of the D-T reaction, the primary species, that is most responsible for the coulomb explosion/fireball.\n\n\n\n"}
{"id": "39368651", "url": "https://en.wikipedia.org/wiki?curid=39368651", "title": "Paper sack", "text": "Paper sack\n\nUsually consists of several layers to provide strength, with high elasticity and high tear resistance, designed for packaging products with high demands for strength and durability and resistant outer surface where instructions are printed, trademark, etc.\n\nPaper sack are produced on paper sack machine consisting of tuber and a bottomer. \n\nWood pulp for sack paper is made from softwood by the kraft process. The long fibers provides the paper its strength and is added wet strength chemicals to even further improve the strength. Both white and brown grades are made. Sack paper is then produced on a paper machine from the wood pulp. The paper is microcrepped to give porosity and elasticity. Microcrepping is done by drying with loose draws allowing it to shrink. This is causing the paper to elongate 4% in machine direction and 10% in cross direction before busting. Machine direction elongation can be further improved by pressing between very elastic cylinders causing more microcrepping. The paper may be coated with polyethylene (PE) to ensure an effective barrier against moisture, grease and bacteria. A paper sack can be made of several layers of sack paper depending on the toughness needed.\n\n\"Paper sacks\" are usually made of Kraft paper having the advantage of being soft and strong at the same time. The stretch or elongation increases the energy required to break the material. They easily carry and protect products up to 50 kg, and adapt easily to the nature of their contents and to handling constraints. At the same time sacks are provide an excellent media for promotional messages and sophisticated printing designs.\n\n\"Paper sacks\" capacity is variable, each country sets a maximum ground rules such as job security, it comes in a variety of calipers, basis weights, and treatments.\n\nPlastic hazard free \"paper sacks\" also called \"multiwall paper sacks\" are used for cement, food, chemicals, consumer goods, flour bags etc.\n\n\n"}
{"id": "28543734", "url": "https://en.wikipedia.org/wiki?curid=28543734", "title": "Parablennius goreensis", "text": "Parablennius goreensis\n\nParablennius goreensis is a species of combtooth blenny found in the eastern Atlantic ocean. This species reaches a length of SL.\n"}
{"id": "14655635", "url": "https://en.wikipedia.org/wiki?curid=14655635", "title": "Planta Solar de Salamanca", "text": "Planta Solar de Salamanca\n\nPlanta Solar de Salamanca is a 13.8 peak MW photovoltaic power plant located in Salamanca, Spain. The plant consists of approximately 70,000 Kyocera solar panels, occupying approximately 36-hectare (89-acre) site.\n\nThe plant was opened for operation on September 18, 2007.\n\n"}
{"id": "7032028", "url": "https://en.wikipedia.org/wiki?curid=7032028", "title": "Pohjolan Voima", "text": "Pohjolan Voima\n\nPohjolan Voima Oy (PVO; ) is the second biggest Finnish energy company, which owns hydropower and thermal power plants (including biofuel-fired power plants).\n\nPohjolan Voima is a founder and main shareholder of the Olkiluoto Nuclear Power Plant operator Teollisuuden Voima Oy. Pohjolan Voima has 25% of shares in the Finnish electricity transmission system operator Fingrid Oyj.\n\nMajor shareholders of Pohjolan Voima are Finnish pulp and paper manufacturers UPM Oyj (42,0%) and Stora Enso Oyj (15,6%). Other shareholders include power and utility companies owned by several municipalities. The energy produced by the company is distributed among its shareholders based on their ownership and the shareholders pay for the actual production costs. This arrangement allows for smaller power companies to participate in larger power station projects together and to benefit from the economics of scale.\n\nIts service company PVO-Palvelut Oy was renamed to Empower Oy in 1998.\n\nPVO sold its share of the Fingrid national electricity transmission grid operator with €247.4 million in April 2011. Income was delivered total to the owners in 2012. Biggest shares were UPM/Myllykoski €109 million and Stora Enso ca €37 million.\n\n"}
{"id": "2171486", "url": "https://en.wikipedia.org/wiki?curid=2171486", "title": "Ponderomotive force", "text": "Ponderomotive force\n\nIn physics, a ponderomotive force is a nonlinear force that a charged particle experiences in an inhomogeneous oscillating electromagnetic field.\n\nThe ponderomotive force F is expressed by\nwhich has units of newtons (in SI units) and where \"e\" is the electrical charge of the particle, \"m\" is its mass, \"ω\" is the angular frequency of oscillation of the field, and \"E\" is the amplitude of the electric field. At low enough amplitudes the magnetic field exerts very little force.\n\nThis equation means that a charged particle in an inhomogeneous oscillating field not only oscillates at the frequency of \"ω\" of the field, but is also accelerated by F toward the weak field direction. This is a rare case where the sign of the charge on the particle does not change the direction of the force ((-e)=(+e)).\n\nThe mechanism of the ponderomotive force can be understood by considering the motion of a charge in an oscillating electric field. In the case of a homogeneous field, the charge returns to its initial position after one cycle of oscillation. In the case of an inhomogeneous field, the force exerted on the charge during the half-cycle it spends in the area with higher field amplitude points in the direction where the field is weaker. It is larger than the force exerted during the half-cycle spent in the area with a lower field amplitude, which points towards the strong field area. Thus, averaged over a full cycle there is a net force that drives the charge toward the weak field area.\n\nThe derivation of the ponderomotive force expression proceeds as follows.\n\nConsider a particle under the action of a non-uniform electric field oscillating at frequency formula_5 in the x-direction. The equation of motion is given by:\nneglecting the effect of the associated oscillating magnetic field.\n\nIf the length scale of variation of formula_7 is large enough, then the particle trajectory can be divided into a slow time motion and a fast time motion:\nwhere formula_9 is the slow drift motion and formula_10 represents fast oscillations. Now, let us also assume that formula_11. Under this assumption, we can use Taylor expansion on the force equation about formula_9, to get:\n\nOn the time scale on which formula_10 oscillates, formula_9 is essentially a constant. Thus, the above can be integrated to get:\n\nSubstituting this in the force equation and averaging over the formula_21 timescale, we get,\n\nThus, we have obtained an expression for the drift motion of a charged particle under the effect of a non-uniform oscillating field.\n\nInstead of a single charged particle, there could be a gas of charged particles confined by the action of such a force. Such a gas of charged particles is called plasma. The distribution function and density of the plasma will fluctuate at the applied oscillating frequency and to obtain an exact solution, we need to solve the Vlasov Equation. But, it is usually assumed that the time averaged density of the plasma can be directly obtained from the expression for the force expression for the drift motion of individual charged particles:\n\nwhere formula_25 is the ponderomotive potential and is given by\n\nInstead of just an oscillating field, a permanent field could also be present. In such a situation, the force equation of a charged particle becomes:\n\nTo solve the above equation, we can make a similar assumption as we did for the case when formula_28. This gives a generalized expression for the drift motion of the particle:\n\nThe idea of a ponderomotive description of particles under the action of a time-varying field has applications in areas like:\n\nThe ponderomotive force also plays an important role in laser induced plasmas as a major density lowering factor.\n\n"}
{"id": "23182078", "url": "https://en.wikipedia.org/wiki?curid=23182078", "title": "Power-line flicker", "text": "Power-line flicker\n\nPower-line flicker is a visible change in brightness of a lamp due to rapid fluctuations in the voltage of the power supply. The voltage drop is generated over the source impedance of the grid by the changing load current of an equipment or facility. These fluctuations in time generate flicker. The effects can range from disturbance to epileptic attacks of photosensitive persons. Flicker may also affect sensitive electronic equipment such as television receivers or industrial processes relying on constant electrical power.\n\nFlicker may be produced, for example, if a steel mill uses large electric motors or arc furnaces on a distribution network, or frequent starting of an elevator motor in an office building, or if a rural residence has a large water pump starting regularly on a long feeder system. The likelihood of flicker increase as the size of the changing load becomes larger with respect to the prospective short-circuit current available at the point of common connection.\n\nThe requirements of a flicker measurement equipment are defined in the international electro-technical standard IEC 61000-4-15.\n\nA flicker meter is composed of several function blocks which simulate a 230 V/60 W incandescent lamp (reference lamp) and the human perception system (eye-brain model).\n\nFrom the resulting momentary value of flicker the short term flicker \"perceptibility\" value Pst is calculated according to a statistical process over a standardized 10-minute observation interval. Long term flicker Plt is calculated as the cubic mean of several Pst values over a standardized two-hour period.\n\nThe perceptibility value calculation and scaling algorithm were chosen such that a P value of 1.0 corresponds to a level at which 50% of test subjects found the flicker to be both noticeable and irritating.\n\nIn the standard IEC 61000-3-3 the observation intervals and the limiting values for Pst and Plt are specified:\n\nThe IEC-flicker standard states that the EUT (Equipment Under Test) has to be operated during the test in a way which is the worst case state with respect to flicker.\nIf the EUT is operated in a (relatively) constant fashion during the whole test, Plt = Pst will result. If this state is feasible and realistic this means Pst has to fulfill the limits for Plt (which are lower).\n\nA purely analytical calculation of Pst is almost impossible. In the standard there are formulas which allow the estimation of the Pst values to be expected.\n\nFlicker is generated by load changes. Only the amplitude of the load change is relevant, not the absolute value. A reduction in flicker can be attained through making less frequent load changes, or smaller load changes. If the load is changed gradually (for example, by the help of power electronics) instead of step fashion, this also makes flicker less perceptible.\n\nThe relationship between amplitude of load changes and Pst is linear, i.e. halving the switched load results in half the Pst. The relationship between number of load changes per time (n/ Tp) and Pst is non-linear. A halving of load changes reduces Pst by only about 20%. In order to have half the Pst, the number of load changes must be reduced by a factor of 9.\n\n\n"}
{"id": "18785802", "url": "https://en.wikipedia.org/wiki?curid=18785802", "title": "Prandtl–Glauert transformation", "text": "Prandtl–Glauert transformation\n\nThe Prandtl–Glauert transformation is a mathematical technique which allows solving certain compressible flow problems by incompressible-flow calculation methods. It also allows applying incompressible-flow data to compressible-flow cases.\n\nInviscid compressible flow over slender bodies is governed by linearized compressible small-disturbance potential equation:\n\nformula_1\n\ntogether with the small-disturbance flow-tangency boundary condition.\n\nformula_2\n\nformula_3 is the freestream Mach number, and formula_4 are the surface-normal vector components. The unknown variable is the perturbation potential formula_5, and the total velocity is given by its gradient plus the freestream velocity formula_6 which is assumed here to be along formula_7.\n\nformula_8\n\nThe above formulation is valid only if the small-disturbance approximation applies\n\nformula_9\n\nand in addition that there is no transonic flow, approximately stated by the requirement that the local Mach number not exceed unity.\n\nformula_10\n\nThe Prandtl–Glauert (PG) transformation uses the Prandtl–Glauert factor formula_11. It consists of scaling down all \"y\" and \"z\" dimensions and angle of attack by the factor of formula_12, the potential by formula_13, and the \"x\" component of the normal vectors by formula_12:\n\nformula_15\n\nThis formula_16 geometry will then have normal vectors whose x components are reduced by formula_12 from the original ones:\n\nformula_18\n\nThe small-disturbance potential equation then transforms to the Laplace equation,\n\nformula_19\n\nand the flow-tangency boundary condition retains the same form.\n\nformula_20\n\nThis is the incompressible potential-flow problem about the transformed formula_16 geometry. It can be solved by incompressible methods, such as thin airfoil theory, vortex lattice methods, panel methods, etc. The result is the transformed perturbation potential formula_22 or its gradient components \nformula_23 in the transformed space. \nThe physical linearized pressure coefficient is then obtained by the inverse transformation\n\nformula_24\n\nwhich is known as Göthert's rule \n\nFor two-dimensional flow, the net result is that formula_25 and also the lift and moment coefficients formula_26 are increased by the factor formula_27:\n\nwhere formula_29 are the incompressible-flow values for the original (unscaled) formula_30 geometry. This 2D-only result is known as the Prandtl Rule.\n\nFor three-dimensional flows, these simple formula_27 scalings do NOT apply. Instead, it is necessary to work with the scaled formula_16 geometry as given above, and use the Göthert's Rule to compute the formula_25 and subsequently the forces and moments. No simple results are possible, except in special cases. For example, using Lifting-Line Theory for a flat elliptical wing, the lift coefficient is\n\nwhere \"AR\" is the wing's aspect ratio. Note that in the 2D case where \"AR\" → ∞ this reduces to the 2D case, since in incompressible 2D flow for a flat airfoil we have formula_35, as given by Thin airfoil theory.\n\nThe PG transformation works well for all freestream Mach numbers up to 0.7 or so, or once transonic flow starts to appear.\n\nLudwig Prandtl had been teaching this transformation in his lectures for a while, however the first publication was in 1928 by Hermann Glauert. The introduction of this relation allowed the design of aircraft which were able to operate in higher subsonic speed areas. Originally all these results were developed for 2D flow. B.H. Göthert then pointed out that the geometry distortion of the PG transformation renders the simple 2D Prandtl Rule invalid for 3D, and properly stated the full 3D problem as described above.\n\nThe PG transformation was extended by Jakob Ackeret to supersonic-freestream flows. Like for the subsonic case, the supersonic case is valid only if there are no transonic effect, which requires that the body be slender and the freestream Mach is sufficiently far above unity.\n\nNear the sonic speed formula_36 the PG transformation features a singularity. The singularity is also called the Prandtl–Glauert singularity, and the flow resistance is calculated to approach infinity. In reality, aerodynamic and thermodynamic perturbations get amplified strongly near the sonic speed, but a singularity does not occur. An explanation for this is that the linearized small-disturbance potential equation above is not valid, since it assumes that there are only small variations in Mach number within the flow and absence of compression shocks and thus is missing certain nonlinear terms. However, these become relevant as soon as any part of the flow field accelerates above the speed of sound, and become essential near formula_36. \nThe more correct nonlinear equation does not exhibit the singularity.\n\n"}
{"id": "18434469", "url": "https://en.wikipedia.org/wiki?curid=18434469", "title": "Radius of outermost closed isobar", "text": "Radius of outermost closed isobar\n\nThe radius of outermost closed isobar (ROCI) is one of the quantities used to determine the size of a tropical cyclone. It is determined by measuring the radii from the center of the storm to its outermost closed isobar in four quadrants, which is then averaged to come up with a scalar value. It generally delimits the outermost extent of a tropical cyclone's wind circulation.\n\nUse of this measure has objectively determined that tropical cyclones in the northwest Pacific Ocean are the largest on earth on average, with North Atlantic tropical cyclones roughly half their size. Active databases of ROCI are maintained by the National Hurricane Center for systems tracked in the eastern north Pacific and north Atlantic basins.\n\nActive databases of ROCI are maintained by the National Hurricane Center for systems tracked in the eastern north Pacific and north Atlantic basins, within a database known as the Extended Best Track Database. The values are determined in real-time every six hours. The eastern north Pacific database runs from 2001 to present, while the north Atlantic database runs from 1988 to present. Other than these official databases, a global once-daily dataset was compiled for a 1984 research paper, which covered global tropical cyclones between 1957 and 1977. Previously, a database was created to determine ROCI values for the western north Pacific Ocean in 1972, using data from 1945 to 1968. Another database with additional ROCI information is currently being modified at the Hydrometeorological Prediction Center for use in matching ongoing tropical cyclones to past systems for the purposes of finding rainfall analogs to an ongoing event, which has fairly continuous data running back to 1959 for the north Atlantic ocean.\n\nTropical cyclones tend to be smaller during the mid-summer, and largest in October in the Northern Hemisphere. As tropical cyclones initially develop, the size of their ROCI initially contracts. Once tropical cyclones reach maturity, their isobaric patterns increase in size. During the mature stage, the broadening pressure pattern leads to some reduction in their maximum sustained wind, but the extent of their tropical storm and hurricane-force winds is the most extensive within the storm's life cycle. An increase in size is also noted as a tropical cyclone gains latitude. As tropical cyclones weaken, their ROCI values diminish. In general, the size of a tropical cyclone shows little relation to its intensity. Use of this measure has objectively determined that tropical cyclones in the northwest Pacific Ocean are the largest on earth on average, with North Atlantic tropical cyclones roughly half their size.\n\n"}
{"id": "18037400", "url": "https://en.wikipedia.org/wiki?curid=18037400", "title": "Renewable energy law", "text": "Renewable energy law\n\nRenewable energy law is a particular kind of energy law, and relates primarily to the transactional legal and policy issues that surround the development, implementation, and commercialization of renewable sources of energy, such as solar, wind, geothermal and tidal. Renewable energy, (RE) law also relates to the land use, siting, and finance issues encountered by developers of renewable energy projects.\n\nRenewable energy law also encompasses policies that relate to renewable energy and legislative instruments that further encourage its growth. \nOne such form of legislation is feed-in tariffs, which provide economic incentives to the developers of renewable energy projects by setting a fixed price for the sale of energy produced from renewable sources. Feed-in tariff laws also provide financial certainty, are more cost effective and less bureaucratic than other support schemes such as investment or production tax credits, quota based renewable portfolio standards (RPS), and auction mechanisms. In addition, the feed-in tariff generates more competition, more jobs, and more rapid deployment for manufacturing; it also does not pick technological winners, for instance between more mature wind power technology versus solar photovoltaics technology.\nThis type of law is in force in 37 of the States within the United States, as well as Australia and a minority of European nations. It works by fixing the quantity of renewable electricity that must be produced, and leaving it to the market at what price this extra renewable electricity will be produced. This form of legislation typically employs a tradeable certificates mechanism, where 1 Megawatt Hour of electricity is equivalent to 1 renewable energy certificate.\nThis form of renewable energy incentive is established by legislation and regulations and is increasingly popular throughout the world as a policy choice for governments. The details of the auction need to be carefully designed to prevent sub-optimal outcomes.\n\nThe Role of the Sector Regulator is specified in the enabling legislation. For example, regulatory oversight of feed-in tariff programs is essential, whether the price is based on a predetermined number (and with some maximum capacity), an auction/bidding process, or avoided cost. In each case, the regulator monitors activities to ensure abuses do not arise. How external (environmental and health) costs are factored into program evaluation is partly dependent on the enabling legislation (or executive order). If the law establishes Renewable Portfolio Standards, the energy regulator will need to oversee the system and evaluate its effectiveness in meeting RE objectives. Generally, some other agency is responsible for certifying the generators and handling the certification system.\n\nThe sector regulator has a number of roles and responsibilities for operationalizing and implementing RE. The policy instruments include those oriented towards prices and quantities. The former (such as Feed-in Tariffs) provide the supplier with certainty regarding price, but the volume depends on whether that price is high or relatively low. The latter includes renewable portfolio standards that require distribution companies to purchase specific quantities of electricity generated by renewable technologies.\n\nIn addition, the sector regulator is in a position to give advice to the government regarding the full implications of focusing on climate change or energy security. Policymakers, however, may choose to delegate these decisions, or a subset of them, to regulators; on the other hand, they may choose to remain silent on such issues. In the former case, of course, regulators have the power to exercise their discretion. In the latter case, the scope of regulatory discretion depends on what the legal system provides. In either case, the internal practices followed by the regulator need to provide legitimacy for regulatory rulings related to RE. Such practices include transparency and evidence-based decision-making.\n\nRenewable energy lawyers focus their practice on serving the legal needs of renewable energy project developers and companies that develop clean technologies (see clean tech law).\n\nA chart summarizing German energy legislation is available.\n\nRenewable energy laws can either be 'technology neutral' or provide specific assistance to particular selected groupings of renewable energy technology. \nOther aspects of land use planning law can have particular application to the implications of particular energy technologies, such as wind power.\n\nConventional hydroelectric dams in most countries are highly regulated, with environmental reviews before construction and operational limits afterwards. Operation normally places river conditions before power interest, ie: power generation may not be needed at night while rivers are kept flowing.\n\nLaws and Regulations Applicable to Geothermal Energy Development. ... A number of federal laws, regulations, and Executive Orders apply to geothermal energy development activities. For the most part, state laws and regulations do not apply to geothermal energy development on tribal lands\n\n"}
{"id": "51947323", "url": "https://en.wikipedia.org/wiki?curid=51947323", "title": "Rural Electrification Board", "text": "Rural Electrification Board\n\nBangladesh Rural Electrification Board or BREB, is government owned and operated corporation in Dhaka, Bangladesh and is responsible for rural electrification. It is major power distribution company in Bangladesh. Major General Moin Uddin is the present chairman of the board.\n\nRural Electrification Board was established in 1977. It implements electrification of rural areas in Bangladesh and builds electrics lines and sub stations. It counterpart Power Development Board manages electric distribution in urban areas. Palli Bidyut Samities in a subsidiary of the board and acts as a consumer cooperative. The board has expanded rural electric connections rapidly. It has taken some market shares of solar energy.\n"}
{"id": "7115752", "url": "https://en.wikipedia.org/wiki?curid=7115752", "title": "SGN (company)", "text": "SGN (company)\n\nSGN (previously known as Scotia Gas Networks) is a UK gas distribution company which manages the network that distributes natural and green gas to 5.9 million homes and businesses across Scotland and the south of England. As of 2014/15 SGN operates more than 71,000 km of pipes. In the same period SGN spent £500 million on upgrading the network.\n\nThe company was formed in 2005 as Scotia Gas Networks. In September 2014 they changed their name to SGN. The rebrand was driven by a desire to present with a greater unity between the two distinct geographic areas of its business. Those are Scotland and the south of England, where SGN owns and maintains the gas network. The Abu Dhabi Investment Authority bought a 16.7% equity stake in SGN in 2016.\n\nIn 2014, Scotia Gas Networks, Scotland Gas Networks and Southern Gas Networks were rebranded to SGN. The company believed the previous names did not represent the business as \"bright, forward thinking experts\", and that the name \"SGN\" would be better for customers to understand what SGN does as a company.\n\nThe logo was also changed. From 2005, the logo was originally red and blue dots forming a shape of the United Kingdom. The red dots covered Scotland and the southern area on the \"Scotia Gas Networks\" logo, just Scotland for the \"Scotland Gas Networks\" logo and just the southern area for \"Southern Gas Networks\". This was to highlight where works are carried out by the company. The new logo shows a cylinder filled with orange, blue and purple dots.\n\nIn 2014 SGN was awarded funding from the industry regulator, Ofgem, to develop two projects. Opening up the Gas Market is an investigation into whether the British Gas Safety Regulations could be changed to accept different types of gas. Robotics is a project to develop technology for repairing steel mains without interrupting the gas.\n\nA partnership between Girlguiding and SGN is designed to encourage young women into male dominated industries and also spread a message about gas safety and carbon monoxide. SGN also works with Royal Voluntary Service to produce a leaflet for winter safety, targeting the elderly.\n\n"}
{"id": "43359746", "url": "https://en.wikipedia.org/wiki?curid=43359746", "title": "SHARC International Systems Inc.", "text": "SHARC International Systems Inc.\n\nSHARC International Systems Inc. (CSE: SHRC), DBA SHARC Energy Systems, is a Canadian public corporation, based in Vancouver, British Columbia, that specializes in wastewater heat recovery. Founded in 2010 by Lynn Mueller, the company operates throughout North America. A subsidiary, SHARC Energy Systems, based in Leicester, UK, operates throughout Europe.\n\nThe company is one of only a few worldwide focusing on waste heat recovery as an alternative energy source, reducing the need to burn fossil fuels. Their systems can be used for domestic hot water production as well as building space heating & cooling.\n\nSHARC Energy's feature products are the SHARC™ and PIRANHA™, which remove solids from wastewater, allowing the wastewater to effectively move through a heat exchanger and heat pump. The systems provide hot water heating, space heating, air conditioning, and wastewater cooling.\n\n"}
{"id": "31645155", "url": "https://en.wikipedia.org/wiki?curid=31645155", "title": "Slush flow", "text": "Slush flow\n\nA slushflow is rapid mass movement of water and snow, and is categorized as a type of debris flow. Slushflows are caused when water reaches a critical concentration in the snowpack due to more water inflow than outflow. The high water concentration weakens the cohesion of the snow crystals and increases the weight of the snowpack. A slushflow is released when the component of the force of gravity parallel to the slope generates a hydraulic pressure gradient exceeding the tensile strength and basal friction of the snowpack. \n\nWhile frequently compared to an avalanche, they have some key differences. Slushflows are lower in frequency than avalanches, have higher water content, are more laminar in flow, and have lower velocity. They most commonly occur at higher latitudes in the winter during high precipitation and in the spring during high snowmelt. Because of their high water content, they can flow down gentle slopes of only a few degrees at low altitudes. They are a significant hazard in Norway and Iceland and are responsible for the deaths of dozens of people as well as the destruction of buildings and the closure of roads.\n"}
{"id": "54269557", "url": "https://en.wikipedia.org/wiki?curid=54269557", "title": "Solar energy conversion", "text": "Solar energy conversion\n\nSolar energy conversion describes technologies devoted to the transformation of solar energy to other (useful) forms of energy, including electricity, fuel, and heat. It covers light-harvesting technologies including traditional semiconductor photovoltaic devices (PVs), emerging photovoltaics, solar fuel generation via electrolysis, artificial photosynthesis, and related forms of photocatalysis directed at the generation of energy rich molecules.\n\nFundamental electro-optical aspects in several emerging solar energy conversion technologies for generation of both electricity (photovoltaics) and solar fuels constitute an active area of current research.\n\nSolar cells started in 1876 with William Grylls Adams along with an undergraduate student of his. A French scientist, by the name of Edmond Becquerel, first discovered the photovoltaic effect in the summer of 1839. He theorized that certain elements on the periodic table, such as silicon, reacted to the exposure of sunlight in very unusual ways. Solar power is created when solar radiation is converted to heat or electricity. English electrical engineer Willoughby Smith, between 1873 and 1876, discovered that when selenium is exposed to light, it produced a high amount of electricity. The use of selenium was highly inefficient, but it proved Becquerel’s theory that light could be converted into electricity through the use of various semi-metals on the periodic table, that were later labeled as photo-conductive material. By 1953, Calvin Fuller, Gerald Pearson, and Daryl Chapin, discovered the use of silicon to produce solar cells was extremely efficient and produced a net charge that far exceeded that of selenium. Today solar power has many uses, from heating, electrical production, thermal processes, water treatment and storage of power that is highly prevalent in the world of renewable energy.\n\n By the 1960’s solar power was the standard for powering space-bound satellites. Early 1970’s solar cell technology became cheaper and more available ($20/watt). Between 1970 and 1990, solar power became more commercially operated. Railroad crossings, oil rigs, space stations, microwave towers, aircraft, etc. Now, houses and businesses all over the world use solar cells to power electrical devices with a wide variety of uses. Solar power is the dominant technology in the renewable energy field, primarily due to its high efficiency and cost-effectiveness. By the early 90’s, photovoltaic conversion had reached an unprecedented new height. Scientists used solar cells constructed of highly conductive photovoltaic materials such as gallium, indium, phosphide and gallium arsenide that increased total efficiency by over 30%. By the end of the century, scientists created a special type of solar cells that converted upwards of 36% of the sunlight it collected into usable energy. These developments built tremendous momentum for not only solar power, but for renewable energy technologies around the world.\n\nPhotovoltaics (PV) use silicon solar cells to convert the energy of sunlight into electricity. They operate under the photoelectric effect which results in the emission of electrons. Concentrated solar power (CSP) uses lenses or mirrors and tracking devices to focus a large area of sunlight into a small beam. Solar power is anticipated to be the world’s largest source of electricity by 2050. Solar power plants, such as Ivanpah Solar Power Facility in the Mojave Desert, produce over 392MW of power. Solar projects exceeding 1 GW (1 billion watts) are in development and are anticipated to be the future of solar power in the US.\n\nThe heat collected by the sun is highly intensive and radioactive. The sun bombards the earth with billions of charged nanoparticles with an immense amount of energy stored in them. This heat can be used for water heating, space heating, space cooling and process heat generation. Many steam generation systems have adapted to using sunlight as a primary source for heating feed water, a development that has greatly increased the overall efficiency of boilers and many other types of waste heat recovery systems. Solar cookers use sunlight for cooking, drying and pasteurization. Solar distillation is used for water treatment processes to create potable drinking water, which has been an extremely powerful player in providing countries in need with relief efforts through the use of advancing technology.\n\nPhotovoltaics (PV) use silicon solar cells to convert the energy of sunlight into electricity. Operates under the photoelectric effect which results in the emission of electrons. Concentrated solar power (CSP) Uses lenses or mirrors and tracking devices to focus a large area of sunlight into a small beam. Solar power is anticipated to be the world’s largest source of electricity by 2050. Solar power plants, such as Ivanpah Solar Power Facility in the Mojave Desert produces over 392MW of power. Solar projects exceeding 1 GW (1 billion watts) are in development and are anticipated to be the future of solar power in the US\n\nVery cost-effective technology. Solar installations are becoming cheaper and more readily available to countries where energy demand is high, but supply is low due to economic circumstances. Provide areas affected by natural disaster with cheap, clean energy. 1 Gigawatt solar power plants can produce almost 10 times as much power as a fossil fuel combustion power plant for more than half the cost. Projected to be the leader of energy production by the year 2050.\n\n Creates jobs for local communities. Construction, design and operation of solar power facilities. Provides clean energy for hundreds of thousands of homes and businesses. Drastically reduces the cost for energy consumption.\n\n Installations can destroy and/or relocate ecological habitats. Covers and huge amount of area that may be protected land Solar facilities constructed on Indian reservations have interrupted Native American traditions, practices and the lives of various animal life on the reservations. Causes some degree of direct and/or indirect defragmenting of various habitats.\n\n Overview of the basic science, construction, design and operation of solar technologies. Societal, environmental, economic and local community impacts that solar technology can have. Projections/future of renewable energy, specifically solar power, around the world. Increased standards of living, population control, readily available energy supplies, increased technological and scientific development among third world countries. Significantly reduced emissions and pollution in Earth’s atmosphere.\n\n"}
{"id": "32124338", "url": "https://en.wikipedia.org/wiki?curid=32124338", "title": "Steam pumper", "text": "Steam pumper\n\nSteam pumper fire engines were used roughly from 1840 to 1920 to pump water on city fires. Large urban fire departments would invest in fire brigades, or engine companies, to assist in fire fighting. Concomitant with the steam engine would be a house, horses and dalmatian dogs (used to guide and calm the horses). The growing cities in the US west would invest in steam pumpers usually after a devastating fire that would destroy large parts of the wooden frontier town.\n\nThe first steam pumper was built by Braithwaite and Ericcson in England in 1829. In the following decades, European cities invested in steam powered fire equipment.\n\nThe vertical fast firing boilers, while heavy, was an effective fire fighting equipment. For a few years, c. 1910-20, the horses were retired and the steam pumper was hauled by a gas tractor.\n\nSmall two wheel units for hand movement were built in England to help fight incendiary fires during the Luftwaffe Blitz.\n\nA few builders were American: Waterous, Amoskeag; Merryweather and Shand Mason were London-based fire engine manufacturers.\n\n"}
{"id": "58008485", "url": "https://en.wikipedia.org/wiki?curid=58008485", "title": "Topological insulator growth", "text": "Topological insulator growth\n\nA topological insulator (TI) is a material that has a nontrivial symmetry, protected by a topological ordering at the surface whiles the interior behaves as an insulator. This ordering is observed when electrons that are subject to strong external magnetic field, exhibit electronic excitation gap in the sample bulk and metallic conduction at the boundaries or surfaces. The metallic conduction are dictated by the conventional charge quantum Hall effect. The surface has a two dimensional symmetry and the ordering on the surface is due to the strong magnetic field on the electrons confined to the surface. Figure 1 is a schematic of the conduction, valence band and the surface state of a topological insulator.The electronic band structure is typical to that of an insulator with the Fermi level in between the valence and conduction band.\nTopological ordering has also been observed in three dimensional (3D) materials. In contrast to their 2D counterparts, the role of the magnetic field in3D topological insulators is assumed by the spin-orbit coupling, which is an inherent property of all solids. The distinction between 2D and 3D topological insulators is characterized by the Z-2 topological invariant, which defines the ground state. In 2D, there is a single Z-2 invariant distinguishing the insulator from the quantum spin-Hall phase whiles in 3D, there are four Z-2 invariant that differentiate the insulator from “weak” and “strong” topological insulators.\n\nThe predictions that led to the discovery of topological insulators date back in the 1980s. It was discovered that electrons that are confined to two dimensions and subject to strong magnetic field show a different topological ordering, which underlies the quantum Hall effect. The effect of this topological ordering results in the emergence of particles with fractional charges and non-dissipation transport. The distinguishing features of topological materials stems in the fact that they are insulating (have energy gaps) in the bulk but have a \"protected\" metallic properties (gapless) at the edge or surface state [5]. These \"protected\" gapless states are governed by the time-reversal symmetry and the band structure of the material.\n\nThe first topological insulator to be realized experimentally was Bi Sb . Bismuth in its pure state, is a semimetal with a small electronic band gap. Using angle- resolved photoemission spectroscopy, and other measurements, it was observed that BiSb alloy exhibits an odd surface state (SS) crossing between any pair of Kramers points. Additionally, bulk BiSb has been predicted to have 3D Dirac particles. This prediction is of particular interest due to the observation of charge quantum Hall fractionalization in 2D graphene and pure bismuth.\n\nTopological ordering with the help of ARPES, has also been observed in other materials such as bismuth telluride and bismuth selenide.\n\nTopological insulators can be grown using different methods such as metal-organic chemical vapor deposition (MOCVD), solvothermal synthesis, sonochemical technique and molecular beam epitaxy\n\n(MBE). MBE has so far been the most common experimental technique used in the growth of topological insulators. The growth of thin film topological insulators is governed by weak Van der Waals interactions. The weak interaction allows to exfoliate the thin film from bulk crystal with a clean and perfect surface. The Van der Waals interactions in epitaxy also known as Van der Waals epitaxy (VDWE), is a phenomenon governed by weak Van der Waal’s interactions between layered materials of different or same elements in which the materials\n\nare stacked on top of each other. This approach allows the growth of layered topological insulators on other substrates for heterostructure and integrated circuits.\n\nMolecular Beam Epitaxial (MBE) growth of topological insulators\n\nMBE is an epitaxy method for the growth of a crystalline material on a crystalline substrate to form an ordered layer. MBE is performed in high vacuum or ultra-high vacuum. In solid states, the elements are heated in different electron beam evaporators until they sublime. The gaseous elements then condense on the wafer where they react with each other to form single crystals. In the growth of thin film layers on a substrate, the reaction is through vapor deposition. By heating the elemental source material, the vapor pressure increases, reaching a critical value that leads\n\nto the deposition of the source material onto the substrate. Both crystals then react with each other for the source material to form layered thin films on the substrate. MBE is an appropriate technique\n\nfor the growth of high quality single-crystal films. In order to avoid a huge lattice mismatch and defects at the interface, the substrate and thin film are expected to have similar lattice constants. MBE has an advantage over other methods due to the fact that the synthesis is performed in high vacuum hence resulting in less contamination. Additionally, lattice defect is reduced due to the ability to influence the growth rate and the ratio of species of source materials present at the substrate interface. Furthermore, in MBE, samples can be grown layer by layer which results in flat surfaces with smooth interface for engineered heterostructures. Moreover, MBE synthesis technique benefits from the ease of moving a topological insulator sample from the growth chamber to a characterization chamber such as angle-resolved photoemission spectroscopy (ARPES) or scanning tunneling microscopy (STM) studies.\n\nDue to the weak Van der Waals bonding, which relaxes the lattice-matching condition, TI can be grown on a wide variety of substrates such as Si(111), AlO , GaAs(111),\n\nInP(111), CdS(0001) and YFeO .\n\nThus far, the field of topological insulators has been focused on bismuth and antimony chalcogenide based materials such as BiSe , BiTe , SbTe or BiSb . The choice of chalcogenides is related to the Van der Waals relaxation of the lattice matching strength which restricts the number of materials and substrates. Bismuth chalcogenides have been studied extensively for TIs and their applications in thermoelectric materials. The Van der Waals interaction in TIs exhibit important features due to low surface energy. For instance, the surface of BiTe is usually terminated by Te due to its low surface energy.\n\nBismuth chalcogenides have been successfully grown on different substrates. In particular, Si has been a good substrate for the successful growth of BiTe . However, the use of sapphire as substrate has not been so encouraging due to a large mismatch of about 15%. The selection of appropriate substrate can improve the overall properties of TI. The use of buffer layer can reduce the lattice match hence improving the electrical properties of TI. BiSe can be grown on top of various BiInSe buffers. Table 1 shows BiSe , BiTe , SbTe on different substrates and the resulting lattice mismatch. Generally, regardless of the substrate\n\nused, the resulting films have a textured surface that is characterized by pyramidal single-crystal domains with quintuple-layer steps. The size and relative proportion of these pyramidal domains vary with factors that include film thickness, lattice mismatch with the substrate and interfacial chemistry-dependent film nucleation. The synthesis of thin films have the stoichiometry problem due to the high vapor pressures of the elements. Thus, binary tetradymites are extrinsically doped as n-type (BiSe , BiTe ) or p-type (SbTe ).\n\nThe first step of topological insulators characterization takes place right after synthesis, meaning without breaking the vacuum and moving the sample to an atmosphere. It is important because a possible chemical reaction on the surface with the atmosphere can change the surface states. That could be done by using angle-resolved photoemission spectroscopy (ARPES) or scanning tunneling microscopy (STM) techniques. Further measurements includes structural and chemical probes such as X-ray diffraction and energy-dispersive spectroscopy but depending on the sample quality, the lack of sensitivity could remain. The most effective metric is the electrical transport measurements of resistivity, Hall effect, and magnetoresistance.\n\nIt should be noted that the field of topological insulators still needs to be developed. The best bismuth chalcogenide topological insulators have about 10 meV bandgap variation due to the charge change. Further development should focus on the examination of both: the presence of high-symmetry electronic bands and simply synthesized materials. One of the promising candidates is half-Heusler compounds. These crystal structures can consist of a large number of elements. Band structures and energy gaps are very sensitive to the valence configuration; because of the increased likelihood of intersite exchange and disorder, they are also very sensitive to specific crystalline configurations. A nontrivial band structure that exhibits band ordering analogous to that of the known 2D and 3D TI materials was predicted in a variety of 18-electron half-Heusler compounds using first-principles calculations.\n"}
{"id": "4038182", "url": "https://en.wikipedia.org/wiki?curid=4038182", "title": "Tornado outbreak sequence", "text": "Tornado outbreak sequence\n\nA tornado outbreak sequence, or tornado outbreak day sequence, sometimes referred to as an extended tornado outbreak, is a period of continuous or nearly continuous high tornado activity consisting of a series of tornado outbreaks over multiple days with no or very few days lacking tornado outbreaks.\n\nMajor tornado outbreak sequences occurred in the United States in May 1917, 1930, 1949, 1965, 2003, and 2011. Another exceptional outbreak sequence apparently occurred during mid to late May 1896. Although some days lacked tornado outbreaks, the period from mid to late April 2011 also was a period of especially high tornado activity.\n\nTornado outbreak sequences tend to dominate the tornado statistics for a year and often cause a spike in tornado numbers for the entire year. Not all periods of active tornado occurrences are outbreak sequences, there must be no break in the activity to satisfy the definition. Active periods happen from every year to every several years whereas continuously active periods are less common and can be rare depending on the parameters applied to define a sequence.\n\n"}
{"id": "18796313", "url": "https://en.wikipedia.org/wiki?curid=18796313", "title": "Toronto propane explosion", "text": "Toronto propane explosion\n\nThe Toronto propane explosion (also known as the Sunrise Propane incident) was a series of explosions and ensuing fire that took place on the morning of August 10, 2008, in Downsview, North York, Toronto, Ontario, Canada. The explosions occurred at the Sunrise Propane Industrial Gases propane facility, located near Keele Street and Wilson Avenue around 03:50 ET. The blasts caused thousands of people to be evacuated from their homes and cost C$1.8 million to clean up, half of which was paid by the province of Ontario.<ref name=\"cbc 08/09\"></ref> An employee of Sunrise died in the initial explosions and a firefighter died of cardiac arrest the next day while at the scene.\n\nSunrise Propane Industrial Gases was a company that sold propane for commercial and home purposes, in addition to other gases such as helium and acetylene. The company has operated under a number of names since at least 1999. In 2002, a company named Sunrise Petroleum was successfully sued by First Choice Petroleum Inc., an oil and lubricants supplier, that claimed the company owed them C$54,063.73 in products and that Sunrise had forged a document to avoid settling their account. In that case, it was found that Sunrise had forged the signature of a First Choice employee named Thomas Tims in a 1999 document, which stated Sunrise Petroleum would be taken over by a new company called Sunrise Petroleum Lubricants, and that Sunrise Petroleum would thereby not be responsible for any outstanding, unpaid, or unsettled accounts. However, Tims would not have signed the document because he was listed on it as \"Tim Toms\", rather than Tom Tims. As a result of the case, Sunrise was forced to pay the account owed plus interest, totalling C$93,389.54, and an additional C$34,284.71 in legal fees. Court documents also revealed a third name, Sunrise Propane & Petroleum, that the company had previously used.\n\nAn Ontario corporate profile states the facility was incorporated in 2004, though a Sunrise corporate solicitor and spokesperson is uncertain how long the facility was in operation. The facility was built in a residential neighbourhood in the North York area of Toronto. Toronto mayor David Miller stated that the facility was allowed to be built in the neighbourhood under zoning that was in place for over a decade.\n\nThe facility had previously been warned by Ontario's Technical Standards and Safety Authority for its lack of safety by not stopping truck-to-truck transfers at the company's facilities. During the investigation following the explosions, investigators found that truck-to-truck transfers were common at the facility. Truck-to-truck transfers are prohibited in Ontario because they increase the risk of a gas leak or a fire.\n\nAt approximately 03:50 ET on the morning of August 10, 2008, a large explosion occurred at Sunrise Propane Industrial Gases, located near Murray Road and Spalding Road. This was followed by a series of explosions which sent large fireballs and clouds of smoke billowing into the sky. Large pieces of metal from the exploding propane tanks were ejected onto nearby streets and properties. Many homes and offices were damaged, windows were shattered, and doors were ripped from their hinges. About 200 firefighters battled the seven-alarm fire that resulted from the explosions.\n\nThe threat of further blasts and concerns about the air quality forced the police to conduct a voluntary evacuation of a large area in the surrounding community. Residents living within a radius were told to leave their homes in the early hours of the morning. Toronto Transit Commission buses were used to evacuate them to Downsview Park and then to York University.\n\nThe explosions rocked the area and also caused the closure of part of Highway 401, between highways 404 and 400, for over 12 hours. Emergency crews feared another major explosion as two rail tankers continued to burn more than five hours after the initial explosion. Regular commercial air traffic was allowed to continue in and out of Pearson International Airport while smaller, privately owned aircraft were restricted from flying over the area.\n\nSix people were sent to the hospital, 18 people admitted themselves to emergency clinics, and Emergency Medical Services treated 40 people on the site. During the course of the emergency response to the scene, a Toronto firefighter was found lifeless by emergency crews. Paramedics and firefighters tried to revive him but were unsuccessful. He was then rushed to a hospital where he was pronounced dead. The firefighter was identified as Bob Leek, a 55-year-old district chief of emergency planning and a 25-year veteran. Leek, who was off duty that night, had been asked to bring some equipment to aid the activities of his colleagues, which he gladly did. He just happened to have had a heart attack at that time. Sunrise employee Parminder Saini was unaccounted for. On August 11, a body was found at the scene. On September 3, the body was confirmed to be that of Saini.\n\nThe Ontario Fire Marshal's Office handled the investigation of the explosions. While the cause of the explosions has not yet been determined, on August 21, 2008, Ontario's independent safety regulator for fuels, the Technical Standards and Safety Authority, released a statement saying that just before the explosion, a truck driver was illegally transferring propane from one truck to another. The agency also reported that in November 2006, Sunrise Propane was warned about its lack of safety by not stopping the truck-to-truck transfers at the company's facilities, and that truck-to-truck transfers were a frequent and routine operating practice at the facility. An investigator with the Ontario Fire Marshal's Office stated that it could take months before the cause of the explosions could be determined. Ontario Premier Dalton McGuinty also said that the province was willing to provide financial aid to residents whose homes were damaged by the explosions.\n\nOn August 4, 2010, the Toronto Star reported that the massive Sunrise propane explosion in 2008 was caused by an illegal \"tank-to-tank transfer\" along with a gas hose leak. The report said that liquid propane was released from a hose after a \"tank-to-tank transfer\" was completed. The Star reports that Sunrise didn't have the right licence to perform those types of transfers, and it was previously barred from doing so by the Technical Standards and Safety Authority in November 2006.\n\nThe Environment Ministry has argued that Sunrise failed \"to show that there was a proper preventative maintenance system in place.\" Leo Adler, Sunrise's lawyer, argued that the event was an unforeseeable accident because Sunrise kept their equipment in good order and cannot be held responsible for a hose failure.\n\nDue to its proximity to the site, Highway 401 was shut down from Highway 404 to Highway 400, and the local Yorkdale Mall was closed for part of the day. Toronto Transit Commission routes and the York Region Transit Viva Orange route were affected as a result of evacuation zone. Bus routes were diverted and a section of the Yonge-University-Spadina subway line between Downsview Station and Lawrence West Station was shut down for part of the day. GO Transit services to Yorkdale Bus Terminal were also suspended.\n\nAbout 15 hours after the first explosions, some residents were given the go-ahead to return to their homes. However, many people returning to their homes were stopped at police checkpoints and turned back, or not permitted to take their vehicles into the immediate area. Approximately 100 of the 12,000 evacuated homes were left uninhabitable. On August 11, almost all residents who had to be evacuated were allowed back, though about 35 families had to wait while tests by health officials were conducted over concerns about airborne asbestos.\n\nAs a result of the explosion, Toronto officials plan on reviewing all industrial areas that could pose a potential threat to residential neighbourhoods to prevent similar situations. As part of its investigation, TSSA officials and the Ontario Fire Marshal are reviewing past inspections of the facility to determine the cause of the explosions.\n\nThe explosion caused damage to one of Toronto's oldest Jewish cemeteries, the Mount Sinai Memorial Park. The cemetery is over 100 years old and has more than 11,000 graves, of which at least 20 were damaged.\n\nVarious residents were angry because the municipal government allowed Sunrise to build a propane facility in a residential area. Some residents claim that the community was not consulted or notified about the facility when it was being built. However, Shelley Carroll, Toronto's acting deputy mayor, suggested that the facility had been zoned before many of the homes were built.\n\nThe Technical Standards and Safety Authority, the agency that regulates fuel safety in Ontario, originally said that it had only inspected Sunrise once since it opened in 2005. They later contradicted this by stating that they had issued stop work orders in 2006 and 2007 over safety violations.\n\nOn August 19, nine days after the explosions, Sunrise issued a short news release, stating that the company regrets the loss of life and that they were co-operating with authorities' investigations. The news release also said that they would not be making any more public comments in the near future to prevent speculation and misinformation. On August 21, 2008 the TSSA issued a notice that Sunrise Propane should immediately have its authorization revoked.\n\nSix other propane facilities in the province have been shut down as a result of an audit prompted by the explosions. Facilities in Kitchener, Waterloo, Cornwall, Ottawa, and two in Toronto have been ordered to shut down after failing to show that their employees were properly trained at the facilities.\n\nParminder Saini's father was granted a visa to travel from Punjab to Canada to aid in the investigation, although Parminder's brother and mother were denied by the Canadian Consulate. This decision was later changed after the Department of Citizenship and Immigration was informed of the situation.\n\nFelipe De Leon, an employee at Sunrise, stated that he had completed an illegal propane transfer when he noticed smoke at the north end of the facility. De Leon said he then went inside the facility's office to warn Saini to flee the building, but he refused. De Leon then fled from the facility while Parminder walked towards the smoke.\n\nCleaning up the result of the explosion cost the city of Toronto $CAD 1.8 million, half of which was paid by the province of Ontario.\n\nOn August 5, 2009, the Ontario Ministry of Labour laid two charges in the incident. One of the charges relates to the failure of protecting Saini. The other charge alleges that the company failed to operate within mandatory industry standards. If convicted, the company could be fined up to C$1 million. Additionally, the residents filed a $300 million lawsuit. On June 27, 2013, Sunrise Propane was found guilty of nine non-criminal charges. On January 25, 2016, Sunrise Propane and its directors Shay Ben-Moshe and Valery Belahov were fined $5.3 million for the offenses. Sunrise was no longer in operation at the time and the defence lawyer argued his clients did not have the money to pay millions in fines.\n\n"}
{"id": "10470182", "url": "https://en.wikipedia.org/wiki?curid=10470182", "title": "Waste oil", "text": "Waste oil\n\nWaste oil is defined as any petroleum-based or synthetic oil that, through contamination, has become unsuitable for its original purpose due to the presence of impurities or loss of original properties.\n\nThe U.S. EPA defines the term \"used oil\" as any petroleum or synthetic oil that has been used, and as a result of such use is contaminated by physical or chemical properties. \"Used oil\" is a precise regulatory term. \"Waste oil\" is a more generic term for oil that has been contaminated with substances that may or may not be hazardous. Any oil contaminated with hazardous waste may itself be a hazardous waste, and if so, must be managed subject to hazardous waste management standards. Both used oil and waste oil require proper recycling or disposal to avoid creating an environmental problem. \n\nSome examples of types of products that after use, can be labeled as used oil are: hydraulic oil, transmission oil, brake fluids, motor oil, crankcase oil, gear box oil, synthetic oil, and grades 1, 2, 3 and 4 fuel oil.\n\nWaste oil can be disposed of in different ways, including sending the used oil off-site (some facilities are permitted to handle the used oil such as your local garages and local waste disposal facilities), burning used oil as a fuel (some used oil is not regulated by burner standards, but others that are off-specification used oil can only be burned in either industrial furnaces, certain boilers, and permitted hazardous waste incinerators), and marketing the used oil (claims are made that the used oil is to be burned for energy recovery, it is then shipped to a used oil burner who burns the used oil in an approved industrial furnace or boiler).\nOils that are off-specification typically contain: Arsenic 5 ppm, Cadmium 2 ppm, Chromium 10 ppm, Lead 100 ppm, Flash point 100°F, minimum (i.e., fp must be greater than 100°F), Total Halogens >4,000 ppm \n\nFor on-site burning of used oil, the oil must be stored in tanks or containers, above or underground. The containers must be in good condition with no leaks, the tanks/containers must be labeled with the words “used oil”, and there must be a spill prevention plan (or a control and countermeasures plan).\n\nIn 1984 the Florida Department of Environmental Protection (DEP) implemented a used oil management program under Sections 403.75 through 403.769, Florida Statutes. Florida’s Used Oil Recycling Program has grown to become one of the most successful in the United States and has received national recognition.\n\nWaste oil furnace is a type of furnace used for heating purposes and is fueled by used oil that is free of hazardous contaminants, as described by the EPA. Waste-oil-fueled boilers can be used for various industrial purposes as well as heating.\n\n\n"}
{"id": "1019188", "url": "https://en.wikipedia.org/wiki?curid=1019188", "title": "Williamson amplifier", "text": "Williamson amplifier\n\nA Williamson amplifier refers to a type of vacuum tube (valve) amplifier whose circuit design uses the same principles as a design published by D.T.N. Williamson. Williamson proposed the standard which became generally accepted as the target figure for high-quality audio power amplifiers, for less than 0.1% total harmonic distortion at full rated power output.\n\nIn April and May 1947, the British magazine \"Wireless World\" published a pair of articles by D.T.N. Williamson, under the title \"Design for a High-quality Amplifier.\" The design, which became known as the \"Williamson Amplifier\", was an early example of high fidelity in an audio amplifier (the commercial LEAK \"Point One\" Type 15 amplifier of 1945 pre-dates it). The design became widely known for the notably high quality of its audio reproduction, and many Williamson amplifiers were built, both for own use and for sale; follow-up articles were published, with a slightly revised design\n\nThe simplicity of the design shows in the circuit diagram. The overall negative feedback is clearly seen to be applied by the resistor R25 from the secondary of the output transformer to the cathode of the input V1. While the output transformer is represented simply by the appropriate circuit symbol, it is a component which must be designed with great care.\n\nPrinted circuit boards (PCBs) were rarely used at the time (they were less necessary than they became for complex semiconductor circuits, due to the relative simplicity of circuits and the use of large socketed components); point-to-point wiring was used.\n\nThe Williamson amplifier was of symmetric push-pull design and used negative feedback and a specially designed output transformer to produce lower levels of distortion than previous designs.\n\nThe design used triodes as phase inverters and drivers. The original output stages used triode-connected KT66 tetrodes, although a 6L6, with slightly lower output, could be used.\n\nA notable characteristic of the design was the use of a negative feedback loop enclosing the whole amplifier, including the output transformer, rather than separate feedback around individual stages. This approach to feedback requires care to be taken to avoid excessive phase-shift around the feedback loop (the transformer being a particular problem), to avoid the feedback becoming positive at any frequency, which would cause undesirable oscillation.\n\nEarlier designs used transformers to couple the output signal from one stage to the next. A transformer with a centre-tapped secondary was used as a simple method to drive the push-pull output valves in anti-phase from the previous single-ended stage.\n\nSignal transformers are a source of distortion; if the transformers are not included in the feedback loop, this distortion is not corrected. They also produce large, frequency-dependent phase shifts at higher frequencies, making feedback loops including the transformers problematical.\n\nWilliamson eliminated all transformers except the output transformer (unavoidable for matching the high valve impedance to the low loudspeaker impedance) by using capacitor coupling between stages, and a split-load valve voltage amplifier stage (\"phase splitter\"), again capacitor-coupled, to provide antiphase signal inputs to the symmetrical push-pull output stages. There was thus a single signal transformer in the circuit; with careful design of the transformer and the feedback network significant negative feedback could be used before the onset of instability. A 1993 book by Linsley Hood discusses the characteristics of the Williamson amplifier design and its performance compared to earlier and later designs.\n\nThe Williamson design demonstrates the maturity that tube amplification had reached by the late 1940s. Other than the British Mullard 5-10 circuit and David Hafler's Dynaco ST-70, there was little improvement in the fundamentals. One improvement that was widely used was an output transformer with additional taps allowing use of Blumlein's distributed load or ultra-linear technique, which allowed the output stage, before applying feedback, to operate at a power intermediate between that available from a triode and the significantly higher power from a pentode or beam tetrode, with distortion better than either.\n\nThe valves used in the original 1947 design were four single triodes (the L63), a directly heated U52 rectifier, and a pair of KT66 beam tetrodes, all from the Marconi-Osram Valve (MOV) Company. In D.T.N. Williamson's 1949 update in Wireless World several small changes to the circuit were suggested along with a list of alternative triodes and the more significant change in rectifier to the indirectly heated Cossor 53KU or 5V4 to prevent \"a damaging voltage surge when the amplifier is switched on\". \nWilliamson said there \"is no exact equivalent for the Osram type KT66, and its use is recommended where possible. When the equipment is to be used overseas, the KT66 may be difficult to obtain, and the 6L6 glass and metal types may be regarded as direct replacements, with the proviso that the total anode and screen dissipation should be reduced from 25 W to 21.5 W by reducing the total current from 125 mA to 110 mA by adjustment of R.\"\n\nFollow-up articles in Wireless World from October 1949 to May 1952 included circuits for Tone controls, Radio Feeder Unit, and modifications for High-impedance Pickups and Long-playing record equalisation. The valves used are:\nEven two decades later, the Williamson amplifier's performance was a standard of comparison for innovative successor developments, including well-known semiconductor audio amplifiers such as Linsley Hood's transistor \"Simple Class-A Amplifier\", published in 1969, with explicit discussion of Williamson's criterion.\n\nWilliamson - 1947:L'anno di Mr.Williamson - P.Paolo FERRARI - SANDIT Editor - ITALY - March 2013\n"}
