{"id": "58011682", "url": "https://en.wikipedia.org/wiki?curid=58011682", "title": "Advanced Electric Propulsion System", "text": "Advanced Electric Propulsion System\n\nAdvanced Electric Propulsion System (AEPS) is a solar electric propulsion system for spacecraft that is being designed, developed and tested by NASA and Aerojet Rocketdyne for large-scale science missions and cargo transportation. \"The most probable first application of the AEPS\" is to propel the PPE module of the planned Lunar Orbital Platform-Gateway to be launched in 2022. Four identical AEPS engines would consume most of the 50 kW generated by solar power.\n\nThe Power and Propulsion Element (PPE) for the Lunar Gateway will have a mass of 8-9 metric tons and will be capable of generating 50 kW of solar electric power for its ion thrusters for maneuverability, which can be supported by chemical propulsion. \n\nSolar-electric propulsion has shown to be reliable, efficient and allows a significant mass reduction of spacecraft. High-power solar electric propulsion is a key technology that has been prioritized because of its significant exploration benefits in cis-lunar space and crewed missions to Mars.\n\nThe AEPS Hall thruster system was originally developed since 2015 by NASA Glenn Research Center and the Jet Propulsion Laboratory to be used on the now cancelled Asteroid Redirect Mission. Work on the thruster did not stop following the mission cancellation in April 2017 because there is demand of such thrusters for a range of NASA, defense and commercial missions in deep space. Since May 2016, further work on AEPS has been transitioned to Aerojet Rocketdyne via a competitive contract, that is currently designing and testing the engineering-model hardware.\n\nAEPS is based on the 12.5 kW development model thruster called 'Hall Effect Rocket with Magnetic Shielding' (HERMeS). The AEPS solar electric engine makes use of the Hall-effect thruster in which the propellant is ionized and accelerated by an electric field to produce thrust. Four identical 14 kW AEPS engines would consume most of the 50 kW generated by solar panels. \n\nThe engineering model is also undergoing various vibration tests, thruster dynamic and thermal environment tests. AEPS is expected to accumulate about 5,000 hr by the end of the contract and the design aims to achieve a flight model that offers a half-life of at least 23,000 hours and a full life of about 50,000 hours. \n\nThe three main components of the AEPS propulsion engine are: a Hall thruster, Power Processor Unit (PPU), and the Xenon Flow Controller (XFC). The thrusters are throttleable over an input power range of 6.67 - 40 kW with input voltages ranging from 95 to\n140 V. The estimated xenon propellant mass for the Lunar Gateway would be 5,000 kg. The Preliminary Design Review took place in August 2017. It was concluded that \"The Power Processing Unit successfully demonstrated stable operation of the propulsion system and responded appropriately to all of our planned contingency scenarios.\"\n\nIn July 2017, AEPS was tested at Glenn Research Center. The tests used a Power Processing Unit (PPU), which could also be used for other advanced spacecraft propulsion technology.\n\n"}
{"id": "13094468", "url": "https://en.wikipedia.org/wiki?curid=13094468", "title": "Ancient Chinese wooden architecture", "text": "Ancient Chinese wooden architecture\n\nAncient Chinese wooden architecture is among the least studied of any of the world's great architectural traditions from the western point of view. Although Chinese architectural history reaches far back in time, descriptions of Chinese architecture are often confined to the well known Forbidden City with little else explored by the West. Although common features of Chinese architecture have been unified into a vocabulary illustrating uniquely Chinese forms and methods, until recently data has not been available. Because of the lack of knowledge of the roots of Chinese architecture, description of its elements is often translated into Western terms and architectural theory, losing its unique Chinese meanings.\nA cause of this deficiency is that the two most important Chinese government architecture\nmanuals, the Song Dynasty \"Yingzao Fashi\" and Qing Architecture Standards have never being translated into any western language.\n\nAncient Chinese architecture has numerous similar elements in part, because of the early Chinese method of standardizing and prescribing uniform features of structures. The standards are recorded in bureaucratic manuals and drawings that were passed down through generations and dynasties. These account for the similar architectural features persisting over thousands of years, starting with the earliest evidence of Chinese imperial urbanism, now available through excavations starting in the early 1980s. The plans include, for example, two-dimensional architectural drawings as early as the first millennium AD, and explain the strong tendency for the shared architectural features in Chinese architecture, that evolved through a complicated but unified evolutionary process over the millennia. Generations of builders and craftsmen recorded their work and the collectors who collated the information into building standards (for example \"Yingzao Fashi\") and Qing Architecture Standards were widely available, in fact strictly mandated, and passed down. The recording of architectural practice and details facilitated a transmission throughout the subsequent generations of the unique system of construction that became a body of unique architectural characteristics.\n\nMore recently, the dependence on text for archaeological descriptions has yielded to the realization that archaeological excavations by the People's Republic of China provides better evidence of Chinese daily life and ceremonies from the Neolithic times to the more recent centuries. For example, the excavation of tombs has provided evidence to produce facsimiles of wooden building parts and yielded site plans several thousand years old. The recent excavation of the Prehistoric Beifudi site is an example.\n\nThree components make up the foundation of ancient Chinese architecture: the foundation platform, the timber frame, and the decorative roof. In addition, the most fundamental feature is a four-sided rectangular enclosure, that is, structures with walls that are formed at right angles and oriented cardinally. The traditional Chinese belief in a square-shaped universe with the four world quarters is manifested physically in its architecture.\n\nBy the middle Neolithic period, the use of rammed earth and unbaked mud bricks was prevalent. \"Hangtu\"(loess), the pounding of layers of earth to make walls, altars, and foundations remained an element of Chinese construction for the next several millennia. The Great Wall of China, built of Hangtu, was erected beginning in the first millennium BC. Sundried mud bricks and rammed mud walls were typically constructed within wood frames. Hard pounded earth floors were strengthened by heating.\nA fundamental achievement of Chinese wooden architecture is the load-bearing timber frame, a network of interlocking wooden supports forming the skeleton of the building. This is considered China's major contribution to worldwide architectural technology. However, it is not known how the builders got the huge wooden support columns into position.\n\nUnlike western architecture, in ancient Chinese wooden architecture, the wall only defined an enclosure, and did not form a load-bearing element. Buildings in China have been supported by wooden frames for as long as seven millennia. The emergence of the characteristic articulated wooden Chinese frame emerged during the Neolithic period. Seven thousand years ago mortise and tenon joinery was used to build wood-framed houses. (The oldest are at Hemudu site at Zhejiang). Over a thousand of these sites have been identified, usually with circular, square or oblong shaped buildings. During the Yangshao culture in the Middle Neolithic, circular and rectangular semisubterranean structures are found with wooden beams and columns. Wooden beams or earth supported the roofs which were most likely thatched.\n\nAs the villages and towns grew they adhered to symmetrical shapes. Symmetry was also important in the layout of homes, altars, and villages.\nIn traditional Chinese architecture, every facet of a building was decorated using various materials and techniques. Simple ceiling \nornamentations in ordinary buildings were made of wooden strips and covered with paper. More decorative was the lattice ceiling, constructed of woven wooden strips or sorghum stems fastened to the beams.\n\n\"Dougong\" is a unique structural element of interlocking wooden brackets, one of the most important elements in traditional Chinese architecture. It first appeared in buildings of the last centuries BC and evolved into a structural network that joined pillars and columns to the frame of the roof. \"Dougong\" was widely used in the Spring and Autumn period (770–476 BC) and developed into a complex set of interlocking parts by its peak in the Tang and Song periods. Since ancient times when the Chinese first began to use wood for building, joinery has been a major focus and craftsmen cut the wooden pieces to fit so perfectly that no glue or fasteners were necessary.\nIn traditional Chinese architecture, every facet of a building was decorated using various materials and techniques. Simple ceiling ornamentations in ordinary buildings were made of wooden strips and covered with paper. More decorative was the lattice ceiling, constructed of woven wooden strips or sorghum stems fastened to the beams. Because of the intricacy of its ornamentation, elaborate cupolas were reserved for the ceilings of the most important structures such as tombs and altars, although it is not clear what the spiritual beliefs of the early Chinese were, as altars appear to have served as burial sites.\nIn traditional Chinese architecture, the layered pieces of the ceiling are held together by interlocking bracket sets (斗拱 \"dǒugǒng\").\n\nElaborate wooden coffers (藻井 \"zǎojǐng\") bordered by a round, square, or polygon frame with its brackets projecting inward and upward from its base were used around the 7th century. Deeply recessed panels shaped like a well (square at the base with a rounded top) were fitted into the ceiling's wooden framework. The center panel of the ceiling was decorated with water lilies or other water plants. The relationship of the name to water has been linked to an ancient fear that wooden buildings would be destroyed by fire and that water from the \"zǎojǐng\" would prevent or quell the fire's flames.\n\nThe tomb of Empress Dowager Wenming of the Northern Wei Dynasty has a coffer in the flat-topped, vaulted ceiling in the back chamber of her tomb. The Baoguo Temple in Yuyao in Zhejiang has three cupolas in the ceiling, making it unique among surviving examples of Song architecture.\n\nSanqing Hall (Hall of the Three Purities) is the only Yuan period structure with three cupolas in its ceiling.\n\n\n"}
{"id": "26400488", "url": "https://en.wikipedia.org/wiki?curid=26400488", "title": "Ariana Afghan Airlines Flight 701", "text": "Ariana Afghan Airlines Flight 701\n\nAriana Afghan Airlines Flight 701 was the flight involved in a fatal air accident on 5 January 1969, when a Boeing 727 with 62 people on board crashed into a house on its approach to London Gatwick Airport in heavy fog. Due to pilot error the flaps were not extended to maintain flight at final approach speed.\n\nAt 0135 on a Sunday morning on which the Gatwick area was affected by patches of dense freezing fog, Boeing 727 registration number YA-FAR (the only such aircraft in the company's fleet) descended below its correct glide slope as it approached the airport from the east. As it passed over the hamlet of Fernhill on the Surrey/Sussex border, it hit trees and roofs, began to roll and crashed into a field south of Fernhill Lane, short of the runway. It collided with a large detached house, demolished it and caught fire.\n\nForty-eight passengers and crew died, and two adult occupants of the house were killed when it was destroyed by the impact. A baby in the house survived with minor injuries. The captain, first officer, flight engineer and eleven passengers also survived.\n\nFernhill is a hamlet about from the east end of Gatwick Airport's runway and a similar distance south of the nearest town, Horley. Until boundary changes brought it fully into West Sussex (and the borough of Crawley) in 1990, it straddled the Sussex/Surrey border and was in the parish of Burstow. The two main roads, Peeks Brook Lane and Fernhill Road (named Fernhill Lane at the time of the accident), run south–north and west–east respectively.\n\nThe crash site was a field west of Peeks Brook Lane, south of Fernhill Lane and east of Balcombe Road, a B-road which forms the eastern boundary of the airport. Antlands Lane is further to the south. A house called \"Longfield\" south of Fernhill Lane was destroyed by the impact.\n\nFlight FG 701 from Kabul International Airport to Gatwick Airport was a weekly scheduled service which stopped intermediately at Kandahar, Istanbul, and Frankfurt. A crew change also took place at Beirut, at which point Captain Nowroz, First Officer Attayee and Flight Engineer Formuly took charge.\n\nWeather in the Gatwick area overnight on 4–5 January 1969 was poor. There was heavy, freezing fog, and no aircraft had landed at the airport since around 1600 the previous day, although the airport remained open. (International regulations require airports to remain open irrespective of ground conditions, in case of emergencies.) The fog had persisted since the previous day, and although it had cleared from most of southeast England some patches remained at Gatwick at a height of no more than . The Captain was given weather reports which indicated that visibility varied between and , the air temperature was and freezing fog was predominant. Reports for London Stansted Airport (the designated alternate destination for this flight) and London Heathrow Airport indicated much clearer conditions, and the flight could also have returned to Frankfurt as enough fuel was carried. (The accident report determined that about was left when the aircraft crashed.)\n\nAs the aircraft approached Fernhill and was within of Gatwick's runway, it clipped the top of some oak trees in the garden of a house called Twinyards on Peeks Brook Lane. This was about from the point of impact on the ground. It then left tyre marks on the roof of the neighbouring house and knocked chimney-pots off the house opposite, a further on. At this point the aircraft was only off the ground. It then caught a television aerial and another group of trees, damaging components on the right-side wing. As it started to roll, the aircraft's wheels touched down briefly and it started to rise again. It failed to clear \"Longfield\", a detached house owned by William and Ann Jones which stood further west, and completely destroyed it. One engine landed in the wreckage of the house along with the rear section of the fuselage, while the forward section of the aircraft disintegrated over a trail. The fuel spilt and immediately caught fire, engulfing the fuselage and the wreckage of the house. The Joneses were killed, but their baby survived with minor injuries: the sides of her cot collapsed inwards, \"forming a protective tent under one of the engines\".\n\nResidents of Peeks Brook Lane were the first to arrive at the crash site and to contact the emergency services. The first call was received at 0138 at Surrey Police's control room, and officers were dispatched from Horley police station. The first officers arrived seven minutes later, soon followed by PC Keith Simmonds of Oxted station who was on traffic duty that night and who saved the injured baby from the wrecked house. The fire services were also summoned at 0138, and vehicles arrived from 0156 onwards. Surrey and Sussex Fire Brigades sent 20 vehicles to the scene, and more were supplied from the airport by the British Airports Authority. Board of Trade accident investigators led by George Kelly also went to the scene. Despite a considerable police presence, their efforts were affected by onlookers obstructing them in the narrow lanes. Police blocks were set up at both ends of Fernhill Lane, and other officers were stationed at Antlands Lane diverting traffic away from Balcombe Road.\n\nThe Boeing 727 was less than a year old at the time of the accident and was Ariana's only such aircraft. YA-FAR was built in February 1968 and received its American airworthiness certificate on 25 March 1968. On 29 April 1968 it was granted its registration in Afghanistan, and that country issued its own airworthiness certificate on 14 May 1968. At the time of the crash, the aircraft had recorded 1,715 hours of flying time.\n\nAccident investigators from the Board of Trade took the wreckage to a hangar at Farnborough Airport for analysis. Also involved in the investigation were officials from the United States and Afghanistan. A preliminary statement was issued on 17 January 1969, and the full accident report followed in June 1970.\n\nCaptain Rahim Nowroz, First Officer and co-pilot Abdul Zahir Attayee and Flight engineer Mohammed Hussain Furmuly were injured but survived. The five flight attendants were killed. Captain Nowroz qualified as a pilot in 1956, was employed by Ariana the following year as a co-pilot and had flown 10,400 hours since then—including 512 in Boeing 727 aircraft, which he qualified to fly after training in 1968.\n\nThere were 54 passengers on board, 43 of whom were killed. The other 11 suffered serious injuries; they had mostly sat in the forward section of the aircraft. Apart from one girl from the United States, all were from Afghanistan, Pakistan and India (especially the Punjab region). There was a mixture of British residents returning after visiting their families and new immigrants.\n\nThe emergency services established a temporary triage facility and rescue centre outside Yew Tree Cottage and later an incident room at Horley police station. Survivors were taken into Fernhill House before being transferred to Redhill General Hospital or, in the case of five badly burned people, the McIndoe Burns Unit at East Grinstead Hospital. Two passengers died en route to Redhill General. The baby who survived in the wreckage of the house was also taken there suffering from \"severe bruising and slight cuts\".\n\nThe victims' bodies were transferred to the St. John Ambulance Hall at Massetts Road in Horley, where a temporary mortuary was set up. Relatives were then taken there to identify them. Some bodies were so badly burnt that personal effects had to be used to confirm the victim's identity. Other bodies were moved later to the Kenyon's undertakers firm in London. Inquiries into the 50 deaths started within days: the first inquest was that of William and Ann Jones, held at Reigate from 10 January 1969.\n\nQueen Elizabeth II conveyed a message of condolence to Mohammed Zahir Shah, King of Afghanistan. Five police officers, including PC Simmonds, were awarded the Queen's Commendation for Brave Conduct in respect of their \"service exceeding the bounds of duty\" at the crash site. Also given this award were five local residents and a passenger on the aircraft who returned to the inferno to rescue family members and also put out the flames on another passenger's clothes.\n\nIn terms of fatalities, the accident was (and remains as of 2018) the worst in the vicinity of Gatwick Airport. It was the first serious incident at the airport since a crash in February 1959, when a Vickers Viscount operated by Turkish Airlines came down in a wooded area between Rusper and Newdigate, also on the Surrey/Sussex border, killing 14 passengers and injuring the Turkish Prime Minister Adnan Menderes.\n\nInvestigators found the cause of the crash was pilot error by the captain. His decision to land at Gatwick was an \"error of judgment\" brought about by the \"deceptive nature\" of the weather conditions, which were very difficult—although this itself did not cause the accident. Instead, failure to extend the flaps in the correct sequence and at an appropriate speed caused the aircraft to fall below its glide slope, roll to the right in a nose-high attitude, and crash.\n\nThe accident report noted that YA-FAR had a full and \"serviceable\" instrument panel, a working VHF omnidirectional radio range (VOR) system and Instrument landing system (ILS) equipment. \"Satisfactory and routine\" communication between air traffic control and the aircraft was noted, and the cockpit voice recorder was recovered. There was also a flight recorder unit in the rear of the fuselage, and this was recovered on 6 January and its contents analysed.\n\nThe Captain's decision to fly to London rather than remain at Frankfurt was not criticised: he could have landed at Heathrow and Stansted, where the weather was clear, instead of Gatwick if he felt conditions were too bad, and the aircraft could even return to Germany if necessary. By the time the aircraft approached Gatwick, the runway visual range was according to the latest weather report at 2350 on 4 January, and was not expected to improve that night; furthermore, this reading was confirmed at 0123 and 0127. At the time, British-registered aircraft were not allowed to land at an airport at a time when its runway visual range was lower than its \"declared minimum\" (Gatwick's was ), but foreign aircraft had their own rules and were not subject to British legislation. Ariana Afghan Airlines pilots were instructed not to land when the runway visual range was lower than an airport's declared minimum (although this was not prohibited by law), but they could use their judgment on whether to descend to critical height ( for this aircraft) and then attempt a landing.\n\nCaptain Nowroz \"decided that since patchy fog shifts quickly he would make an approach with a view to landing at Gatwick\". The accident report stated that because he was relying principally on visual indications as he came in to land, he may have been distracted from his flight-deck duties; and patchy fog in otherwise clear conditions has been known to severely affect the sighting of visual references, sometimes leading to \"disastrous errors of judgment\".\n\nNevertheless, Captain Nowroz's decision to approach Gatwick with a view to landing there presented \"no undue risk\" and did not cause the accident. Instead, the cause was found to be a series of changes to speed, power and flap angle settings which were not in accordance with the airline's operating procedures and which took place in the last of the approach. At 0128, the aircraft picked up the ILS localiser beam, and the flaps were lowered in three stages as the aircraft's speed reduced. Soon afterwards, as it approached the ILS glideslope beam, its height and speed were reduced further and the undercarriage was extended. Then the Captain saw a light which he mistook for one at the far end of the runway—it was actually on a hill beyond the airport—and the \"stabiliser out of trim\" warning light came on. This had been faulty earlier in the flight, and the Captain disengaged the autopilot and the automatic glideslope tracker. At 0133, the flap angle was increased; the aircraft then began to fall below the approach slope and was travelling faster than the crew thought. Only when it reached a height of was an attempt made to gain height, but this happened too late.\n\nThe first three flap adjustments took place at higher speeds than recommended in the airline's procedures, although they did not exceed the Boeing 727's limits. The undercarriage was extended at too high a speed, and the next flap adjustment should have been done in two stages. The sudden change of angle caused the nose to pitch downwards and the aircraft to descend more rapidly than was appropriate for the conditions. The Captain and other crew members did not react to this for about 45 seconds, though, until at about from the ground they applied full power and full up elevator to try to bring the aircraft up. The accident report states that during this 45-second period, they may have been preoccupied by looking for visual confirmation of their position, such as the runway lights.\nThe legislation prohibiting British aircraft from landing when the runway visual range was too short was extended in September 1969 to cover aircraft from all other countries when flying to airports anywhere in the United Kingdom.\n\n\n"}
{"id": "29164194", "url": "https://en.wikipedia.org/wiki?curid=29164194", "title": "Australian Petroleum Production and Exploration Association", "text": "Australian Petroleum Production and Exploration Association\n\nThe Australian Petroleum Production and Exploration Association, known as APPEA, is an Australian industry association representing companies which explore and produce oil and gas in Australia. APPEA is a non-profit organisation.\n\nThe organisation was originally established in 1959 and previously used the acronym APEA (Australian Petroleum Exploration Association). It is the peak national body representing the collective interests of the upstream oil and gas exploration and production industry. The association has more than 80 full member companies that explore for and produce Australia’s oil and gas resources. APPEA also represents more than 250 associate member companies that provide a wide range of goods and services to the upstream oil and gas industry. APPEA members account for an estimated 98 per cent of the nation’s petroleum production\n\nAPPEA works with Australia’s national, state and territory governments to ensure the country’s regulatory and commercial framework promotes investment and maximises the return to the Australian industry and community from developing the nation’s oil and gas resources.\n\nThe APPEA Conference and Exhibition is the largest oil and gas event in the southern hemisphere. The conference bring together business experts, researchers, leading minds and professionals from the oil and gas industry to share their insights, knowledge and experience of the issues surrounding the industry. Each year, this conference highlights and defines the issues and challenges of upstream petroleum exploration and development on a national and international level. In 2010 the APPEA conference won \"Association or Government Meeting of the Year\" award from Meetings & Events Australia.\n\nThe APPEA Conference attracts more than 3000 delegates, exhibitors and important players of the global oil and gas industry. The 2018 APPEA Conference will be held in Adelaide, South Australia on 14-17 May 2018.\n\nThe APPEA annual conference also involves the presentation of a multidisciplinary technical journal of peer-reviewed papers. The APPEA Journal is recognised in the Australian upstream oil and gas industry as the leading peer-reviewed publication for information on geoscience, engineering and management.\n\nThe \"APPEA Journal\" (ISSN 1326-4966) is distributed annually to conference attendees. The journal has its roots in the development of the association and the first edition of the APPEA (then APEA) Journal was published in 1961. The APPEA journal is listed on the ERA (Excellence in Research for Australia) 2012 Journal List as ERAID 1707.\n\n\n\nThe following websites are produced and managed by APPEA:\n\n"}
{"id": "54476668", "url": "https://en.wikipedia.org/wiki?curid=54476668", "title": "Australian Wormy Chestnut", "text": "Australian Wormy Chestnut\n\nAustralian Wormy Chestnut or Firestreak is a common name for lumber of Eucalyptus obliqua, Eucalyptus sieberi and Eucalyptus fastigata grown in Victoria, southern New South Wales, and Tasmania in Australia. It is a hardwood species commonly used in flooring applications.\n"}
{"id": "6917139", "url": "https://en.wikipedia.org/wiki?curid=6917139", "title": "Band diagram", "text": "Band diagram\n\nIn solid-state physics of semiconductors, a band diagram is a diagram plotting various key electron energy levels (Fermi level and nearby energy band edges) as a function of some spatial dimension, which is often denoted \"x\".\nThese diagrams help to explain the operation of many kinds of semiconductor devices and to visualize band bending. The bands may be coloured to distinguish level filling.\n\nA band diagram should not be confused with a band structure plot. In both a band diagram and a band structure plot, the vertical axis corresponds to the energy of an electron. The difference is that in a band structure plot the horizontal axis represents the wavevector of an electron in an infinitely large, homogeneous material (a crystal or vacuum), whereas in a band diagram the horizontal axis represents position in space, usually passing through multiple materials.\n\nA band diagram does however try to show the \"changes\" in the band structure from place to place.\nIn doing so, there is an intrinsic conflict due to the Heisenberg uncertainty principle: the band structure relies on momentum which is only precisely defined for large length scales.\nFor this reason, the band diagram can only accurately depict evolution of band structures over long length scales, and has difficulty in showing the microscopic picture of sharp, atomic scale interfaces between different materials (or between a material and vacuum).\nTypically, an interface must be depicted as a \"black box\", though its long-distance effects can be shown in the band diagram as asymptotic band bending.\n\nThe vertical axis of the band diagram represents the energy of an electron, which includes both kinetic and potential energy.\nThe horizontal axis represents position, often not being drawn to scale. Note that the Heisenberg uncertainty principle prevents the band diagram from being drawn with a high positional resolution, since the band diagram shows energy bands (as resulting from a momentum-dependent band structure).\n\nWhile a basic band diagram only shows electron energy levels, often a band diagram will be decorated with further features.\nIt is common to see cartoon depictions of the motion in energy and position of an electron (or electron hole) as it drifts, is excited by a light source, or relaxes from an excited state.\nThe band diagram may be shown connected to a circuit diagram showing how bias voltages are applied, how charges flow, etc.\nThe bands may be colored to indicate filling of energy levels, or sometimes the band gaps will be colored instead.\n\nDepending on the material and the degree of detail desired, a variety of energy levels will be plotted against position:\n"}
{"id": "30056405", "url": "https://en.wikipedia.org/wiki?curid=30056405", "title": "Brazilian Forest Code", "text": "Brazilian Forest Code\n\nThe Brazilian Forest Code is a piece of legislation passed in 1965. There has been controversy over the code, mostly centered on legal requirement for landowners in the Brazilian Amazon to maintain 80% of forests as legal reserves. This particular requirement has never been effectively implemented and implementation has again been delayed by President Luiz Inacio Lula da Silva until June 2011.\n\nThe original law, passed in 1965, required only 50%. Neither this nor the 80% requirement have ever been prosecuted. This had been expected to change with harsher and criminal penalties to be introduced in 2009. Then President Luiz Inacio Lula da Silva however, delayed this until the post election period in 2011, though Presidential Decree number 7029.\n\nWhile the measure has never been formally adopted into law it has been estimated that other Government policies have reduced logging from 21.5 thousand square kilometers in 2002 to 7.0 thousand in 2009.\n\nThe code remains an enduring source of controversy for environmentalists and agriculturalists.\n\nDilma vetoed 12 parts and made 32 smaller cuts to the rewrite in May 2012.\n\nOn 28 February 2018, Brazil's Supreme Court upheld forestry law changes which comes as a blow to environmentalists trying to protect the world's largest rain forest. According to environmentalists, the revised laws which are also known as the forest code, would give a rise to illegal deforestation. Whereas the farmers and the agricultural lobby welcomes the new law and suggest that it is going to be pivotal for the growth of agricultural sector of the Brazilian economy.\n\n\n"}
{"id": "79836", "url": "https://en.wikipedia.org/wiki?curid=79836", "title": "Charles Wheatstone", "text": "Charles Wheatstone\n\nSir Charles Wheatstone FRS (6 February 1802 – 19 October 1875), was an English scientist and inventor of many scientific breakthroughs of the Victorian era, including the English concertina, the stereoscope (a device for displaying three-dimensional images), and the Playfair cipher (an encryption technique). However, Wheatstone is best known for his contributions in the development of the Wheatstone bridge, originally invented by Samuel Hunter Christie, which is used to measure an unknown electrical resistance, and as a major figure in the development of telegraphy.\n\nCharles Wheatstone was born in Barnwood, Gloucestershire. His father was a music-seller in the town, who moved to 128 Pall Mall, London, four years later, becoming a teacher of the flute. Charles, the second son, went to a village school, near Gloucester, and afterwards to several institutions in London. One of them was in Kennington, and kept by a Mrs. Castlemaine, who was astonished at his rapid progress. From another he ran away, but was captured at Windsor, not far from the theatre of his practical telegraph. As a boy he was very shy and sensitive, liking well to retire into an attic, without any other company than his own thoughts.\n\nWhen he was about fourteen years old he was apprenticed to his uncle and namesake, a maker and seller of musical instruments at 436 Strand, London; but he showed little taste for handicraft or business, and loved better to study books. His father encouraged him in this, and finally took him out of the uncle's charge.\n\nAt the age of fifteen, Wheatstone translated French poetry, and wrote two songs, one of which was given to his uncle, who published it without knowing it as his nephew's composition. Some lines of his on the lyre became the motto of an engraving by Bartolozzi. He often visited an old book-stall in the vicinity of Pall Mall, which was then a dilapidated and unpaved thoroughfare. Most of his pocket-money was spent in purchasing the books which had taken his fancy, whether fairy tales, history, or science. One day, to the surprise of the bookseller, he coveted a volume on the discoveries of Volta in electricity, but not having the price, he saved his pennies and secured the volume. It was written in French, and so he was obliged to save again, until he could buy a dictionary. Then he began to read the volume, and, with the help of his elder brother, William, to repeat the experiments described in it, with a home-made battery, in the scullery behind his father's house. In constructing the battery, the boy philosophers ran short of money to procure the requisite copper-plates. They had only a few copper coins left. A happy thought occurred to Charles, who was the leading spirit in these researches, 'We must use the pennies themselves,' said he, and the battery was soon complete.\n\nAt Christchurch, Marylebone, on 12 February 1847, Wheatstone was married to Emma West. She was the daughter of a Taunton tradesman, and of handsome appearance. She died in 1866, leaving a family of five young children to his care. His domestic life was quiet and uneventful.\n\nThough silent and reserved in public, Wheatstone was a clear and voluble talker in private, if taken on his favourite studies, and his small but active person, his plain but intelligent countenance, was full of animation. Sir Henry Taylor tells us that he once observed Wheatstone at an evening party in Oxford earnestly holding forth to Lord Palmerston on the capabilities of his telegraph. 'You don't say so!' exclaimed the statesman. 'I must get you to tell that to the Lord Chancellor.' And so saying, he fastened the electrician on Lord Westbury, and effected his escape. A reminiscence of this interview may have prompted Palmerston to remark that a time was coming when a minister might be asked in Parliament if war had broken out in India, and would reply, 'Wait a minute; I'll just telegraph to the Governor-General, and let you know.'\nWheatstone was knighted in 1868, after his completion of the automatic telegraph. He had previously been made a Chevalier of the Legion of Honour. Some thirty-four distinctions and diplomas of home or foreign societies bore witness to his scientific reputation. Since 1836 he had been a Fellow of the Royal Society, and in 1859 he was elected a foreign member of the Royal Swedish Academy of Sciences, and in 1873 a Foreign Associate of the French Academy of Sciences. The same year he was awarded the Ampere Medal by the French Society for the Encouragement of National Industry. In 1875, he was created an honorary member of the Institution of Civil Engineers. He was a D.C.L. of Oxford and an LL.D. of Cambridge.\n\nWhile on a visit to Paris during the autumn of 1875, and engaged in perfecting his receiving instrument for submarine cables, he caught a cold, which produced inflammation of the lungs, an illness from which he died in Paris, on 19 October 1875. A memorial service was held in the Anglican Chapel, Paris, and attended by a deputation of the Academy. His remains were taken to his home in Park Crescent, London, (marked by a blue plaque today) and buried in Kensal Green Cemetery.\n\nIn September 1821, Wheatstone brought himself into public notice by exhibiting the 'Enchanted Lyre,' or 'Aconcryptophone,' at a music-shop at Pall Mall and in the Adelaide Gallery. It consisted of a mimic lyre hung from the ceiling by a cord, and emitting the strains of several instruments – the piano, harp, and dulcimer. In reality it was a mere sounding box, and the cord was a steel rod that conveyed the vibrations of the music from the several instruments which were played out of sight and ear-shot. At this period Wheatstone made numerous experiments on sound and its transmission. Some of his results are preserved in Thomson's \"Annals of Philosophy\" for 1823. He recognised that sound is propagated by waves or oscillations of the atmosphere, as light was then believed to be by undulations of the luminiferous ether. Water, and solid bodies, such as glass, or metal, or sonorous wood, convey the modulations with high velocity, and he conceived the plan of transmitting sound-signals, music, or speech to long distances by this means. He estimated that sound would travel through solid rods, and proposed to telegraph from London to Edinburgh in this way. He even called his arrangement a 'telephone.' (Robert Hooke, in his \"Micrographia\", published in 1667, writes: 'I can assure the reader that I have, by the help of a distended wire, propagated the sound to a very considerable distance in an instant, or with as seemingly quick a motion as that of light.' Nor was it essential the wire should be straight; it might be bent into angles. This property is the basis of the mechanical or lover's telephone, said to have been known to the Chinese many centuries ago. Hooke also considered the possibility of finding a way to quicken our powers of hearing.) A writer in the \"Repository of Arts\" for 1 September 1821, in referring to the 'Enchanted Lyre,' beholds the prospect of an opera being performed at the King's Theatre, and enjoyed at the Hanover Square Rooms, or even at the Horns Tavern, Kennington. The vibrations are to travel through underground conductors, like to gas in pipes.\n\nBesides transmitting sounds to a distance, Wheatstone devised a simple instrument for augmenting feeble sounds, to which he gave the name of 'Microphone.' It consisted of two slender rods, which conveyed the mechanical vibrations to both ears, and is quite different from the electrical microphone of Professor Hughes.\n\nIn 1823, his uncle, the musical instrument maker, died, and Wheatstone, with his elder brother, William, took over the business. Charles had no great liking for the commercial part, but his ingenuity found a vent in making improvements on the existing instruments, and in devising philosophical toys. He also invented instruments of his own. One of the most famous was the Wheatstone concertina. It was a six sided instrument with 64 keys. These keys provided for simple chromatic fingerings. The English Concertina became increasingly famous throughout his lifetime, however it didn't reach its peak of popularity until the early 20th century.\n\nIn 1827, Wheatstone introduced his 'kaleidophone', a device for rendering the vibrations of a sounding body apparent to the eye. It consists of a metal rod, carrying at its end a silvered bead, which reflects a 'spot' of light. As the rod vibrates the spot is seen to describe complicated figures in the air, like a spark whirled about in the darkness. His photometer was probably suggested by this appliance. It enables two lights to be compared by the relative brightness of their reflections in a silvered bead, which describes a narrow ellipse, so as to draw the spots into parallel lines.\n\nIn 1828, Wheatstone improved the German wind instrument, called the \"Mundharmonika\", until it became the popular concertina, patented on 19 December 1829. The portable harmonium is another of his inventions, which gained a prize medal at the Great Exhibition of 1851. He also improved the speaking machine of De Kempelen, and endorsed the opinion of Sir David Brewster, that before the end of this century a singing and talking apparatus would be among the conquests of science.\n\nIn 1834, Wheatstone, who had won a name for himself, was appointed to the Chair of Experimental Physics in King's College London. His first course of lectures on sound were a complete failure, due to his abhorrence of public speaking. In the rostrum he was tongue-tied and incapable, sometimes turning his back on the audience and mumbling to the diagrams on the wall. In the laboratory he felt himself at home, and ever after confined his duties mostly to demonstration.\n\nHe achieved renown by a great experiment made in 1834 – the measurement of the velocity of electricity in a wire. He cut the wire at the middle, to form a gap which a spark might leap across, and connected its ends to the poles of a Leyden jar filled with electricity. Three sparks were thus produced, one at each end of the wire, and another at the middle. He mounted a tiny mirror on the works of a watch, so that it revolved at a high velocity, and observed the reflections of his three sparks in it. The points of the wire were so arranged that if the sparks were instantaneous, their reflections would appear in one straight line; but the middle one was seen to lag behind the others, because it was an instant later. The electricity had taken a certain time to travel from the ends of the wire to the middle. This time was found by measuring the amount of lag, and comparing it with the known velocity of the mirror. Having got the time, he had only to compare that with the length of half the wire, and he could find the velocity of electricity. His results gave a calculated velocity of 288,000 miles per second, i.e. faster than what we now know to be the speed of light (), but were nonetheless an interesting approximation.\n\nIt was already appreciated by some scientists that the “velocity” of electricity was dependent on the properties of the conductor and its surroundings. Francis Ronalds had observed signal retardation in his buried electric telegraph cable (but not his airborne line) in 1816 and outlined its cause to be induction. Wheatstone witnessed these experiments as a youth, which were apparently a stimulus for his own research in telegraphy. Decades later, after the telegraph had been commercialised, Michael Faraday described how the velocity of an electric field in a submarine wire, coated with insulator and surrounded with water, is only , or still less.\n\nWheatstone's device of the revolving mirror was afterwards employed by Léon Foucault and Hippolyte Fizeau to measure the velocity of light.\n\nWheatstone and others also contributed to early spectroscopy through the discovery and exploitation of spectral emission lines.\n\nAs John Munro wrote in 1891, \"In 1835, at the Dublin meeting of the British Association, Wheatstone showed that when metals were volatilised in the electric spark, their light, examined through a prism, revealed certain rays which were characteristic of them. Thus the kind of metals which formed the sparking points could be determined by analysing the light of the spark. This suggestion has been of great service in spectrum analysis, and as applied by Robert Bunsen, Gustav Robert Kirchhoff, and others, has led to the discovery of several new elements, such as rubidium and thallium, as well as increasing our knowledge of the heavenly bodies.\"\n\nWheatstone abandoned his idea of transmitting intelligence by the mechanical vibration of rods, and took up the electric telegraph. In 1835 he lectured on the system of Baron Schilling, and declared that the means were already known by which an electric telegraph could be made of great service to the world. He made experiments with a plan of his own, and not only proposed to lay an experimental line across the Thames, but to establish it on the London and Birmingham Railway. Before these plans were carried out, however, he received a visit from Mr William Fothergill Cooke at his house in Conduit Street on 27 February 1837, which had an important influence on his future.\n\nMr. Cooke was an officer in the Madras army, who, being home on leave, was attending some lectures on anatomy at the University of Heidelberg, where, on 6 March 1836, he witnessed a demonstration with the telegraph of professor Georg Wilhelm Munke, and was so impressed with its importance, that he forsook his medical studies and devoted all his efforts to the work of introducing the telegraph. He returned to London soon after, and was able to exhibit a telegraph with three needles in January 1837. Feeling his want of scientific knowledge, he consulted Michael Faraday and Peter Mark Roget (then secretary of the Royal Society), the latter of whom sent him to Wheatstone.\n\nAt a second interview, Mr. Cooke told Wheatstone of his intention to bring out a working telegraph, and explained his method. Wheatstone, according to his own statement, remarked to Cooke that the method would not act, and produced his own experimental telegraph. Finally, Cooke proposed that they should enter into a partnership, but Wheatstone was at first reluctant to comply. He was a well-known man of science, and had meant to publish his results without seeking to make capital of them. Cooke, on the other hand, declared that his sole object was to make a fortune from the scheme. In May they agreed to join their forces, Wheatstone contributing the scientific, and Cooke the administrative talent. The deed of partnership was dated 19 November 1837. A joint patent was taken out for their inventions, including the five-needle telegraph of Wheatstone, and an alarm worked by a relay, in which the current, by dipping a needle into mercury, completed a local circuit, and released the detent of a clockwork.\n\nThe five-needle telegraph, which was mainly, if not entirely, due to Wheatstone, was similar to that of Schilling, and based on the principle enunciated by André-Marie Ampère – that is to say, the current was sent into the line by completing the circuit of the battery with a make and break key, and at the other end it passed through a coil of wire surrounding a magnetic needle free to turn round its centre. According as one pole of the battery or the other was applied to the line by means of the key, the current deflected the needle to one side or the other. There were five separate circuits actuating five different needles. The latter were pivoted in rows across the middle of a dial shaped like a diamond, and having the letters of the alphabet arranged upon it in such a way that a letter was literally pointed out by the current deflecting two of the needles towards it.\n\nAn experimental line, with a sixth return wire, was run between the Euston terminus and Camden Town station of the London and North Western Railway on 25 July 1837. The actual distance was only one and a half-mile (2.4 km), but spare wire had been inserted in the circuit to increase its length. It was late in the evening before the trial took place. Mr Cooke was in charge at Camden Town, while Mr Robert Stephenson and other gentlemen looked on; and Wheatstone sat at his instrument in a dingy little room, lit by a tallow candle, near the booking-office at Euston. Wheatstone sent the first message, to which Cooke replied, and 'never' said Wheatstone, 'did I feel such a tumultuous sensation before, as when, all alone in the still room, I heard the needles click, and as I spelled the words, I felt all the magnitude of the invention pronounced to be practicable beyond cavil or dispute.'\n\nIn spite of this trial, however, the directors of the railway treated the 'new-fangled' invention with indifference, and requested its removal. In July 1839, however, it was favoured by the Great Western Railway, and a line erected from the Paddington station terminus to West Drayton railway station, a distance of . Part of the wire was laid underground at first, but subsequently all of it was raised on posts along the line. Their circuit was eventually extended to in 1841, and was publicly exhibited at Paddington as a marvel of science, which could transmit fifty signals a distance of 280,000 miles per minute (7,500 km/s). The price of admission was a shilling (£0.05), and in 1844 one fascinated observer recorded the following:\n\"\"It is perfect from the terminus of the Great Western as far as\"\n\"Slough – that is, eighteen miles; the wires being in some places\"\n\"underground in tubes, and in others high up in the air, which last,\"\n\"he says, is by far the best plan. We asked if the weather did not\"\n\"affect the wires, but he said not; a violent thunderstorm might\"\n\"ring a bell, but no more. We were taken into a small room (we\"\n\"being Mrs Drummond, Miss Philips, Harry Codrington and\"\n\"myself – and afterwards the Milmans and Mr Rich) where were\"\n\"several wooden cases containing different sorts of telegraphs. \"\n\"In one sort every word was spelt, and as each letter was placed in turn\"\n\"in a particular position, the machinery caused the electric fluid to run\"\n\"down the line, where it made the letter show itself at Slough, by what\"\n\"machinery he could not undertake to explain. After each word came a \"\n\"sign from Slough, signifying \"I understand\", coming certainly in less \"\n\"than one second from the end of the word...Another prints the messages\"\n\"it brings, so that if no-one attended to the bell...the message would not\"\n\"be lost. This is effected by the electrical fluid causing a little hammer to strike the\"\n\"letter which presents itself, the letter which is raised hits some manifold \"\n\"writing paper (a new invention, black paper which, if pressed, leaves an \"\n\"indelible black mark), by which means the impression is left on white paper\"\n\"beneath. This was the most ingenious of all, and apparently Mr. Wheatstone's\"\n\"favourite; he was very good-natured in explaining but\"\n\"understands it so well himself that he cannot feel how little we\"\n\"know about it, and goes too fast for such ignorant folk to follow\"\n\"him in everything. Mrs Drummond told me he is wonderful for\"\n\"the rapidity with which he thinks and his power of invention; he\"\n\"invents so many things that he cannot put half his ideas into\"\n\"execution, but leaves them to be picked up and used by others,\"\n\"who get the credit of them.\"\"\nThe public took to the new invention after the capture of the murderer John Tawell, who in 1845, had become the first person to be arrested as the result of telecommunications technology. In the same year, Wheatstone introduced two improved forms of the apparatus, namely, the 'single' and the 'double' needle instruments, in which the signals were made by the successive deflections of the needles. Of these, the single-needle instrument, requiring only one wire, is still in use.\n\nThe development of the telegraph may be gathered from two facts. In 1855, the death of the Emperor Nicholas at St. Petersburg, about one o'clock in the afternoon, was announced in the House of Lords a few hours later. The result of The Oaks of 1890 was received in New York fifteen seconds after the horses passed the winning-post.\n\nIn 1841 a difference arose between Cooke and Wheatstone as to the share of each in the honour of inventing the telegraph. The question was submitted to the arbitration of the famous engineer, Marc Isambard Brunel, on behalf of Cooke, and Professor Daniell, of King's College, the inventor of the Daniell battery, on the part of Wheatstone. They awarded to Cooke the credit of having introduced the telegraph as a useful undertaking which promised to be of national importance, and to Wheatstone that of having by his researches prepared the public to receive it. They concluded with the words: 'It is to the united labours of two gentlemen so well qualified for mutual assistance that we must attribute the rapid progress which this important invention has made during five years since they have been associated.' The decision, however vague, pronounces the needle telegraph a joint production. If it had mainly been invented by Wheatstone, it was chiefly introduced by Cooke. Their respective shares in the undertaking might be compared to that of an author and his publisher, but for the fact that Cooke himself had a share in the actual work of invention.\n\nFrom 1836–7 Wheatstone had thought a good deal about submarine telegraphs, and in 1840 he gave evidence before the Railway Committee of the House of Commons on the feasibility of the proposed line from Dover to Calais. He had even designed the machinery for making and laying the cable. In the autumn of 1844, with the assistance of Mr. J. D. Llewellyn, he submerged a length of insulated wire in Swansea Bay, and signalled through it from a boat to the Mumbles Lighthouse. Next year he suggested the use of gutta-percha for the coating of the intended wire across the English Channel.\n\nIn 1840 Wheatstone had patented an alphabetical telegraph, or, 'Wheatstone A B C instrument,' which moved with a step-by-step motion, and showed the letters of the message upon a dial. The same principle was used in his type-printing telegraph, patented in 1841. This was the first apparatus which printed a telegram in type. It was worked by two circuits, and as the type revolved a hammer, actuated by the current, pressed the required letter on the paper.\n\nThe introduction of the telegraph had so far advanced that, on 2 September 1845, the Electric Telegraph Company was registered, and Wheatstone, by his deed of partnership with Cooke, received a sum of £33,000 for the use of their joint inventions.\n\nIn 1859 Wheatstone was appointed by the Board of Trade to report on the subject of the Atlantic cables, and in 1864 he was one of the experts who advised the Atlantic Telegraph Company on the construction of the successful lines of 1865 and 1866.\n\nIn 1870 the electric telegraph lines of the United Kingdom, worked by different companies, were transferred to the Post Office, and placed under Government control.\n\nWheatstone further invented the automatic transmitter, in which the signals of the message are first punched out on a strip of paper, which is then passed through the sending-key, and controls the signal currents. By substituting a mechanism for the hand in sending the message, he was able to telegraph about 100 words a minute, or five times the ordinary rate. In the Postal Telegraph service this apparatus is employed for sending Press telegrams, and it has recently been so much improved, that messages are now sent from London to Bristol at a speed of 600 words a minute, and even of 400 words a minute between London and Aberdeen. On the night of 8 April 1886, when Mr. Gladstone introduced his Bill for Home Rule in Ireland, no fewer than 1,500,000 words were dispatched from the central station at St. Martin's-le-Grand by 100 Wheatstone transmitters. The plan of sending messages by a running strip of paper which actuates the key was originally patented by Bain in 1846; but Wheatstone, aided by Mr. Augustus Stroh, an accomplished mechanician, and an able experimenter, was the first to bring the idea into successful operation. This system is often referred to as the Wheatstone Perforator and is the forerunner of the stock market Ticker tape\n\nStereopsis was first described by Wheatstone in 1838. In 1840 he was awarded the Royal Medal of the Royal Society for his explanation of binocular vision, a research which led him to make stereoscopic drawings and construct the stereoscope. He showed that our impression of solidity is gained by the combination in the mind of two separate pictures of an object taken by both of our eyes from different points of view. Thus, in the stereoscope, an arrangement of lenses or mirrors, two photographs of the same object taken from different points are so combined as to make the object stand out with a solid aspect. Sir David Brewster improved the stereoscope by dispensing with the mirrors, and bringing it into its existing form with lenses.\n\nThe 'pseudoscope' (Wheatstone coined the term from the Greek ψευδίς σκοπειν) was introduced in 1852, and is in some sort the reverse of the stereoscope, since it causes a solid object to seem hollow, and a nearer one to be farther off; thus, a bust appears to be a mask, and a tree growing outside of a window looks as if it were growing inside the room. Its purpose was to test his theory of stereo vision and for investigations into what would now be called experimental psychology.\n\nIn 1840, Wheatstone introduced his chronoscope, for measuring minute intervals of time, which was used in determining the speed of a bullet or the passage of a star. In this apparatus an electric current actuated an electro-magnet, which noted the instant of an occurrence by means of a pencil on a moving paper. It is said to have been capable of distinguishing 1/7300 part of a second (137 microsecond), and the time a body took to fall from a height of one inch (25 mm).\n\nOn 26 November 1840, he exhibited his electro-magnetic clock in the library of the Royal Society, and propounded a plan for distributing the correct time from a standard clock to a number of local timepieces. The circuits of these were to be electrified by a key or contact-maker actuated by the arbour of the standard, and their hands corrected by electro-magnetism. The following January Alexander Bain took out a patent for an electro-magnetic clock, and he subsequently charged Wheatstone with appropriating his ideas. It appears that Bain worked as a mechanist to Wheatstone from August to December 1840, and he asserted that he had communicated the idea of an electric clock to Wheatstone during that period; but Wheatstone maintained that he had experimented in that direction during May. Bain further accused Wheatstone of stealing his idea of the electro-magnetic printing telegraph; but Wheatstone showed that the instrument was only a modification of his own electro-magnetic telegraph.\n\nIn 1840, Alexander Bain mentioned to the Mechanics Magazine editor his financial problems. He introduced him to Sir Charles Wheatstone. Bain demonstrated his models to Wheatstone, who, when asked for his opinion, said \"Oh, I shouldn't bother to develop these things any further! There's no future in them.\"[2] Three months later Wheatstone demonstrated an electric clock to the Royal Society, claiming it was his own invention. However, Bain had already applied for a patent for it. Wheatstone tried to block Bain's patents, but failed. When Wheatstone organised an Act of Parliament to set up the Electric Telegraph Company, the House of Lords summoned Bain to give evidence, and eventually compelled the company to pay Bain £10,000 and give him a job as manager, causing Wheatstone to resign.\n\nOne of Wheatstone's most ingenious devices was the 'Polar clock,' exhibited at the meeting of the British Association in 1848. It is based on the fact discovered by Sir David Brewster, that the light of the sky is polarised in a plane at an angle of ninety degrees from the position of the sun. It follows that by discovering that plane of polarisation, and measuring its azimuth with respect to the north, the position of the sun, although beneath the horizon, could be determined, and the apparent solar time obtained. The clock consisted of a spyglass, having a Nicol (double-image) prism for an eyepiece, and a thin plate of selenite for an object-glass. When the tube was directed to the North Pole—that is, parallel to the Earth's axis—and the prism of the eyepiece turned until no colour was seen, the angle of turning, as shown by an index moving with the prism over a graduated limb, gave the hour of day. The device is of little service in a country where watches are reliable; but it formed part of the equipment of the 1875–1876 North Polar expedition commanded by Captain Nares.\n\nIn 1843 Wheatstone communicated an important paper to the Royal Society, entitled 'An Account of Several New Processes for Determining the Constants of a Voltaic Circuit.' It contained an exposition of the well known balance for measuring the electrical resistance of a conductor, which still goes by the name of Wheatstone's Bridge or balance, although it was first devised by Samuel Hunter Christie, of the Royal Military Academy, Woolwich, who published it in the \"Philosophical Transactions\" for 1833. The method was neglected until Wheatstone brought it into notice. His paper abounds with simple and practical formulae for the calculation of currents and resistances by the law of Ohm. He introduced a unit of resistance, namely, a foot of copper wire weighing one hundred grains (6.5 g), and showed how it might be applied to measure the length of wire by its resistance. He was awarded a medal for his paper by the Society. The same year he invented an apparatus which enabled the reading of a thermometer or a barometer to be registered at a distance by means of an electric contact made by the mercury. A sound telegraph, in which the signals were given by the strokes of a bell, was also patented by Cooke and Wheatstone in May of that year.\n\nWheatstone's remarkable ingenuity was also displayed in the invention of cyphers. He was responsible for the then unusual Playfair cipher, named after his friend Lord Playfair. It was used by the militaries of several nations through at least World War I, and is known to have been used during World War II by British intelligence services.\n\nIt was initially resistant to cryptoanalysis, but methods were eventually developed to break it. He also became involved in the interpretation of cypher manuscripts in the British Museum. He devised a cryptograph or machine for turning a message into cypher which could only be interpreted by putting the cypher into a corresponding machine adjusted to decrypt it.\n\nIn 1840, Wheatstone brought out his magneto-electric machine for generating continuous currents.\n\nOn 4 February 1867, he published the principle of reaction in the dynamo-electric machine by a paper to the Royal Society; but Mr. C. W. Siemens had communicated the identical discovery ten days earlier, and both papers were read on the same day.\n\nIt afterwards appeared that Werner von Siemens, Samuel Alfred Varley, and Wheatstone had independently arrived at the principle within a few months of each other. Varley patented it on 24 December 1866; Siemens called attention to it on 17 January 1867; and Wheatstone exhibited it in action at the Royal Society on the above date.\n\nWheatstone was involved in various disputes with other scientists throughout his life regarding his role in different technologies and appeared at times to take more credit than he was due. As well as William Fothergill Cooke, Alexander Bain and David Brewster, mentioned above, these also included Francis Ronalds at the Kew Observatory. Wheatstone was erroneously believed by many to have created the atmospheric electricity observing apparatus that Ronalds invented and developed at the observatory in the 1840s and also to have installed the first automatic recording meteorological instruments there (see for example, Howarth, p158).\n\n\n\n"}
{"id": "18244767", "url": "https://en.wikipedia.org/wiki?curid=18244767", "title": "Circlotron", "text": "Circlotron\n\nCirclotron valve amplifier is a type of power amplifier utilizing symmetrical cathode-coupled bridge layout of the output stage. Original circlotrons of 1950s used output transformers to couple relatively high output impedance of vacuum tubes to low-impedance loudspeakers. Circlotron architecture, easily scalable, was eventually adapted to operate without output transformers, and present-day commercially produced circlotron models are of output transformerless (OTL) type.\n\nThe \"Circlotron\" name emerged as a trademark of Electro-Voice. A U.S. patent for a circlotron \"High Fidelity Audio Amplifier\" was filed by Alpha Wiggins of Electro-Voice on March 1, 1954, and granted March 28, 1958. However, other inventors filed the same concept earlier:\n\nAll these patents called for a transformer-coupled, fully balanced design; commercial transformerless amplifiers were not feasible at this time due to high costs of power supply capacitors required for an OTL design (at least thousands of microfarads at 200 volts or better). In the 1950s and 1960s, circlotrons were produced by Electro-Voice (eight models, 15 to 100 watts per channel), Finnish Voima and Philips. These inspired other local manufacturers, such as Carad. All these models employed output transformers and beam tetrode or pentode tubes (for increased efficiency). Similar amateur designs were published in the USSR. One can observe that the output stage of the successful McIntosh amplifier is a circlotron too because the special winding of the output transformer eliminates one of the two floating power supplies.\n\nThe concept was resurrected in its transformerless form in early 1980s by Ralph Karsten, founder of Atma-Sphere, which remains the principal contemporary manufacturer of circlotrons. Other OTL circlotrons were made by Tenor Audio, Joule Electra and Einstein.\n\nThe electrical bridge of a circlotron is formed by a matched pair of triodes (V1, V2) and two floating power supplies ('B batteries'), B1+ and B2+. Grids of each triode are driven in opposite phases with a balanced, symmetrical input signal; differential current flows through the loudspeaker load and a simple, relatively high impedance, resistor network that ties floating supplies to the ground. Tubes are usually fixed biased with an external negative power supply ('C battery'); each side normally has independent bias adjustment to compensate for minor tube mismatch.\n\nOutput impedance \"Z\" of a transformerless circlotron where each stage is a single triode with plate impedance of \"R\" and voltage gain of \"μ\" is defined by the formula\n\nThe tubes best fitting the circlotron concept are triodes designed for use in power supplies as series regulators: 6080/6AS7, 6C33C, 6C19P. An amplifier with one 6AS7 dual triode per channel (\"R\" = 270 ohms, \"μ\" = 2) will have a \"Z\" of about 67 ohms—sufficient to drive most headphones. Driving loudspeakers requires paralleling output tubes, and in practice, V1 and V2 are not single triodes, but massive banks of paralleled triodes. Simple OTL amplifiers, for example Atma-Sphere M-60, employ 8 double triodes of 6AS7 type per channel; each continuously dissipates an average of 30 watts. Without feedback, this arrangement results in an output impedance of 6–8 ohms. It can be further lowered by applying global negative feedback or paralleling more output tubes. On the extreme end, Atma-Sphere MA-2 amplifier uses 20 6AS7 tubes per channel, with a continuous power consumption of 600 watts per channel (800 watts at maximum rated output of 220 watts, or a 27% maximum efficiency).\n\nPerceived fidelity (quality) of audio reproduction is a hardly quantifiable, subjective measure that should be left to each individual listener's taste. Apart from audio fidelity, circlotron OTL have certain benefits versus asymmetrical OTLs (i.e. Futterman amplifier) and conventional, transformer-coupled amplifiers:\n\nThese benefits are at the cost of high power consumption and concomitant heat generation: minimal stereo power amplifier (Atma-Sphere S-30) continuously dissipates 400 watts, while practical low-impedance stereo setups approach 1 kilowatt, the power of a small electric heater.\n\nCirclotrons, unlike minimalistic single-ended amplifiers, need to employ quite complex balanced input and driver stages delivering at least 100V peak-to-peak voltage \"and\" able to drive relatively high capacitance loads (due to the number of paralleled power triodes and associated long wiring). The first requirement is typically solved with a long-tailed cascode stage (2 or 3 double triodes), the second - with two cathode followers inserted between the driver stage and the power stage. Both need a bipolar power supply, independent of the power stage power supplies, with power established before the power stage supply starts.\n\n"}
{"id": "28886290", "url": "https://en.wikipedia.org/wiki?curid=28886290", "title": "Cleveland open-cup method", "text": "Cleveland open-cup method\n\nThe Cleveland open-cup method is one of three main methods in chemistry for determining the flash point of a petroleum product using a Cleveland open-cup apparatus, also known as a Cleveland open-cup tester. First, the test cup of the apparatus (usually brass) is filled to a certain level with a portion of the product. Then, the temperature of this chemical is increased rapidly and then at a slow, constant rate as it approaches the theoretical flash point. The increase in temperature will cause the chemical to begin to produce flammable vapor in increasing quantities and density. The lowest temperature at which a small test flame passing over the surface of the liquid causes the vapor to ignite is considered the chemical's flash point. This apparatus may also be used to determine the chemical's fire point which is considered to have been reached when the application of the test flame produces at least five continuous seconds of ignition.Temperature range of this apparatus is 120 to 250 degree c \n\nThe other principal methods of establishing chemical flash points are the Pensky–Martens closed-cup test and the Tagliabue cup method (often called simply the \"Tag method\").\n"}
{"id": "44903845", "url": "https://en.wikipedia.org/wiki?curid=44903845", "title": "Colíder Dam", "text": "Colíder Dam\n\nThe Colíder Dam is a rock-fill dam with an asphalt-concrete core, currently under construction on the Teles Pires river about southeast of Colíder in the state of Mato Grosso, Brazil. The dam's hydroelectric power stations will have 3 turbines each 115 MW resulting in a total installed capacity of 306.9 MW. Construction on the run-of-the-river type station was initiated in May 2011, and is expected to be operational in 2015.\n\nThe dam is part of a planned six power plant \"Hidrovia Tapajos/Teles Pires\" project to create a navigable waterway connecting the interior of Brazil to the Atlantic Ocean. \nThe waterway will consist of five dams on the Teles Pires river ( Magessi Dam, Sinop Dam, Colíder Dam, Teles Pires Dam, Sao Manoel Dam) and the Foz do Apiacas Dam on the Apiacas river.\n\nThe Colíder Dam and the Teles Pires Dam are currently under construction, while the smaller upstream dams are still in the planning stages.\n\nThe Colíder Dam will be a combination embankment dam with concrete sections for the power stations and spillway. The length of the entire dam will be . The dam will utilize 4.7 million cubic meters of earth and 260 thousand cubic meters of concrete, and will impound a reservoir with a surface area of 182.8 square kilometers. The power plant is being constructed by the Brazilian utility Companhia Paranaense de Energia (Copel).\n\nTo reduce emissions of methane, a powerful greenhouse gas, 82 square kilometers of vegetation were cleared before the reservoir was filled.\n\n"}
{"id": "1177019", "url": "https://en.wikipedia.org/wiki?curid=1177019", "title": "Copper(II) oxide", "text": "Copper(II) oxide\n\nCopper(II) oxide or cupric oxide is the inorganic compound with the formula CuO. A black solid, it is one of the two stable oxides of copper, the other being CuO or cuprous oxide. As a mineral, it is known as tenorite. It is a product of copper mining and the precursor to many other copper-containing products and chemical compounds.\n\nIt is produced on a large scale by pyrometallurgy used to extract copper from ores. The ores are treated with an aqueous mixture of ammonium carbonate, ammonia, and oxygen to give copper(I) and copper(II) ammine complexes, which are extracted from the solids. These complexes are decomposed with steam to give CuO.\n\nIt can be formed by heating copper in air at around 300 – 800°C:\nFor laboratory uses, pure copper(II) oxide is better prepared by heating copper(II) nitrate, copper(II) hydroxide or copper(II) carbonate:\n\nCopper(II) oxide dissolves in mineral acids such as hydrochloric acid, sulfuric acid or nitric acid to give the corresponding copper(II) salts:\n\nIt reacts with concentrated alkali to form the corresponding cuprate salts:\n\nIt can also be reduced to copper metal using hydrogen, carbon monoxide, or carbon:\nWhen cupric oxide is substituted for iron oxide in thermite the resulting mixture is a low explosive, not an incendiary.\n\nCopper(II) oxide belongs to the monoclinic crystal system. The copper atom is coordinated by 4 oxygen atoms in an approximately square planar configuration.\n\nThe work function of bulk CuO is 5.3eV\n\nCopper(II) oxide is a p-type semiconductor, with a narrow band gap of 1.2 eV. Cupric oxide can be used to produce dry cell batteries.\n\nAs a significant product of copper mining, copper(II) oxide is the starting point for the production of other copper salts. For example, many wood preservatives are produced from copper oxide.\n\nCupric oxide is used as a pigment in ceramics to produce blue, red, and green, and sometimes gray, pink, or black glazes. \n\nIt is also incorrectly used as a dietary supplement in animal feed. Due to low bioactivity, negligible copper is absorbed.\n\nIt is also used when welding with copper alloys.\n\nCupric oxide can be used to safely dispose of hazardous materials such as cyanide, hydrocarbons, halogenated hydrocarbons and dioxins, through oxidation.\n\nThe decomposition reactions of phenol and pentachlorophenol follow these pathways:\n\n\n"}
{"id": "7301270", "url": "https://en.wikipedia.org/wiki?curid=7301270", "title": "Deoxygenation", "text": "Deoxygenation\n\nDeoxygenation is a chemical reaction involving the removal of oxygen atoms from a molecule. The term also refers to the removal molecular oxygen (O) from gases and solvents, a step in air-free technique and gas purifiers. As applied to organic compounds, deoxygenation is a component of fuels production as well a type of reaction employed in organic synthesis, e.g. of pharmaceuticals.\n\nThe main examples involving replacement of an oxo group by two hydrogen atoms (A=O → A) is hydrogenolysis. Typical examples use metal catalysts and H2 as the reagent. Conditions are typically more forcing than hydrogenation.\n\nStoichiometric reactions that effect deoxygenation include the Wolff-Kishner reduction for aryl ketones. The replacement of a hydroxyl group by hydrogen (A-OH → A-H) is the point of the Barton-McCombie deoxygenation and the Markó-Lam deoxygenation.\n\nDeoxygenation is an important goal of the conversion of biomass to useful fuels and chemicals. Partial deoxygenation is effected by dehydration and decarboxylation.\n\nOxygen groups can also be removed by reductive coupling of ketones, as illustrated by the McMurry reaction.\n\nEpoxides can be deoxygenated using the oxophilic reagent produced by combining tungsten hexachloride and \"n\"-butyllithium generates the alkene. This reaction in effect is a de-epoxidation:\n\nPhosphorus occurs in nature as oxides, so to produce elemental form of the element, deoxygenation is required. The main method involves carbothermic reduction (i.e., carbon is the deoxygenation agent).\nOxophilic main group compounds are useful reagents for certain deoxygenations conducted on laboratory scale. The highly oxophilic reagent hexachlorodisilane (SiCl) stereospecifically deoxygenates phosphine oxides.\n\nA chemical reagent for the deoxygenation of many sulfur and nitrogen oxo compounds is the combination trifluoroacetic anhydride/sodium iodide. for example in the deoxygenation of the sulfoxide \"diphenylsulfoxide\" to the sulfide \"diphenylsulfide\": \n\nThe reaction mechanism is based on activation of the sulfoxide by a trifluoroacetyl group and oxidation of iodine. Iodine is formed quantitatively in this reaction and therefore the reagent is used for the analytical detection of many oxo compounds.\n\n"}
{"id": "54479213", "url": "https://en.wikipedia.org/wiki?curid=54479213", "title": "Electric turbo compound", "text": "Electric turbo compound\n\nAn electric turbo compound (ETC) system is defined where a turbine coupled to a generator (turbogenerator) is located in the exhaust gas flow of a reciprocating engine to harvest waste heat energy and convert it into electrical power.\n\nAn example of an ETC system is where a turbogenerator is located downstream of a turbocharger turbine of an Internal Combustion Engine (ICE). The power generated from the ETC system can be used to feed into an electrical grid or provide power to local electrical loads such as engine auxiliaries.\n\n\n\nETC systems are commercially available for stationary power generator sets and at an advanced stage of development for automotive applications (See Figure 1).\n\nThe operating cost of an ICE can be very significant, for example, a continuously operated 1 MWe diesel generator set may have an annual fuel bill which could represent more than five times the capital cost of the generator set.\n\nThe exhaust gas of an ICE contains 30% to 40% of the chemical fuel energy as heat. As a consequence, even a limited recovery of this energy would represent a significant contribution in terms of overall system efficiency improvement.\n\nExhaust energy recovery systems may play an increasingly important role to meet the exhaust emission regulations and to improve fuel efficiency for today’s on-highway, off-highway and power generation markets globally. Exhaust recovery systems work by either converting the exhaust energy into mechanical or electrical energy or to transfer the energy for heating purposes.\n\nTurbo Compound systems have been used for piston aircraft engines since the late 1950s until superseded by turboprop and turbojet engines (see http://en.wikipedia.org/wiki/Turbocompound_engine). In the 1980s the mechanical turbo compound was applied in motorsport racing cars and in the 1990s in heavy truck diesel engines. In 2001, Caterpillar launched a program to develop an Electric Assisted Turbocharger (EAT) for truck applications. In 2004, the first ETC prototype was created for the heavy truck industry by Bowman Power Ltd in partnership with John Deere.\n\nIn 2009, Bowman Power Ltd developed an ETC system for the power generation industry. In 2010, Controlled Power Technologies (CPT) designed an ETC system called TIGERS for passenger car applications (Green cars congress reference2010). In 2014, F1 includes heat recovery technology to complement kinetic energy recovery under the name MGU-H to boost the engines power output.\n\nThe ETC system is typically located downstream of the turbocharger of an ICE. The exhaust gases expand first through the turbocharger turbine and then through the ETC turbine driving a high-speed generator. The high-speed generator produces high-frequency power that is converted to either DC or AC power. In a generator set application, this extra power is added to the power produced by the primary generator of the engine, increasing the system efficiency whereas for an automotive application the ETC would become the primary generator used to power the vehicle’s auxiliary systems. It is claimed that an Electric Turbo Compounded ICE can achieve SFC (Specific fuel consumption) improvements in the range of 4-6%.\nIn order to recover the exhaust gas energy after the TC turbine, it is necessary to create an additional expansion step which increases the engine backpressure; this results in the following effect:\nNevertheless, the power produced by the ETC system more than compensates for this power loss in the engine and therefore provides an overall efficiency improvement for the system.\n\nTo aid understanding the differences between a standard turbocharged ICE and one fitted with an ETC system, the following highlights the main component differences:\n\nThe turbogenerator comprises a turbine and an electrical machine. The turbine transforms a proportion of the energy contained in the fluid into mechanical energy, whilst the electrical machine converts the mechanical energy into electrical energy.\n\nThe turbine typically consists of a turbine wheel, nozzle guide vanes (NGV) and the volute. The turbine wheel can be axial or radial and can be directly coupled to a high-speed alternator which results in a compact and efficient machine.\n\nAlthough standard turbogenerator technology is not new [TG patent, 1923], the development of high-speed turbogenerator technology is relatively modern with significant development occurring during the 1990s. Challenges associated with high-speed turbogenerators are:\nThe Power electronics control the turbogenerator operation and convert high frequency unregulated alternating current into either DC for vehicle applications or AC 50/60 Hz for power generation application.\n\nThe power electronics architecture can have different configurations but may consist of a rectifier, a converter, and an inverter stage. The rectifier transforms the unregulated AC into a DC power. As the turbogenerator output voltage depends on speed and load, the rectified voltage is subsequently regulated to a fixed voltage by a converter stage. For AC applications the inverter transforms DC to AC and synchronizes it with the electricity grid. A contactor connects or disconnects the inverter output to the grid depending on whether the grid is within specification. The EMC (electromagnetic compliance) filter reduces the high-frequency emissions from the power electronics and improves its immunity to high-frequency interference and transients from the grid.\n\nThe turbocharger controls the air flow into the ICE. The addition of the turbogenerator changes the pressures either side of the turbocharger turbine hence it is necessary to change the turbine to provide the correct match to the higher pressure flow conditions. This is usually achieved by either changing the turbine wheel and/or modifying the NGV angle.\n\nCCGT Wikipedia article mentioning Bowman Power: <nowiki>http://en.wikipedia.org/wiki/Combined_cycle</nowiki>\n"}
{"id": "1078869", "url": "https://en.wikipedia.org/wiki?curid=1078869", "title": "Enercon", "text": "Enercon\n\nEnercon GmbH, based in Aurich, Lower Saxony, Germany, is the fourth-largest wind turbine manufacturer in the world and has been the market leader in Germany since the mid-nineties. Enercon has production facilities in Germany (Aurich, Emden and Magdeburg), Sweden, Brazil, India, Canada, Turkey and Portugal. In June 2010, Enercon announced that they would be setting up Irish headquarters in Tralee.\n\n, Enercon had installed more than 26,300 wind turbines, with a power generating capacity exceeding 43 GW. The most-often installed model is the E-40, which pioneered the gearbox-less design in 1993. As of July 2011, Enercon has a market share of 7.2% world-wide (fifth-highest) and 59.2% in Germany.\n\nCurrently Enercon supplies wind turbines to the British electricity company Ecotricity, including one billed as \"the UK's most visible turbine\", an E-70 at Green Park Business Park.\n\nEnercon wind turbines have some special technical features compared to turbines of most other wind turbine manufacturers . Characteristic is the gearless propulsion concept, on which Enercon has pioneered this technology since 1993. The E-40/500 kW series was the first gearless generation of turbines. Earlier Enercon facilities had a transmission train. The hub with the rotor blades is gearless directly connected to the rotor of the ring generator (direct drive). The rotor unit rotates about a front and rear main bearing about a fixed axis (bearings). Thus, the speed of the rotor is transmitted directly to the high-pole synchronous generator, wherein the rotor rotates in the stator (inner rotor), differently. The Enercon generator is installed without permanent magnets, thus allowing the company to not rely on rare-earth metals. However, grid losses accompany the provision of electrical excitation power. Compared to transmission systems, both the speed of the rotating components and the mechanical load changes over the service life are lower. Depending on the wind speed, the speed is variable between 18-45 RPM for the E-33 and 5-11.7 RPM for the E-126, while for a gearbox a generator speed of about 1500 / min is at Rated power is achieved. The large Enercon generators in turn lead to high tower head masses, constructive and logistical challenges.\n\nThe Enercon systems are visually easy to distinguish from the systems of other manufacturers. The nacelles of the plants have been drop-shaped since 1995/96. The design of this unique gondola/teardrop was developed by the British architect Norman Foster , whom is also notable for designing the dome of the Berlin Reichstag . In Germany and many other countries, the tower carries coloured green rings above the foundation, which are getting brighter from bottom to top. On islands, the manufacturer alternatively offers a gradation in blue, as it was implemented on the island of Borkum . The NCS grading is intended to better integrate the plant towers into the horizon. The rotor blades were the only ones on the market with blade tips similar to the winglets on aircraft, the technical term for this being \"Tips\". \n\nIn 2008, the first E-126 turbines (successor of the E-112) were installed at various sites throughout Germany and Belgium, including the Estinnes wind farm (consisting of eleven E-126 turbines) in Belgium. Although the E-126 turbine was initially developed with a power rating of 6 MW, it has since been upgraded to 7.5 MW. The E-82 turbine was also upgraded and is available in 2, 2.3, and 3 MW versions.\n\nCurrently Enercon does not offer or supply wind turbines to offshore projects and has at times expressed skepticism about offshore wind parks. Enercon was rumored to have been ready to supply turbines to Germany's Alpha Ventus offshore wind farm and to a near-shore park near Wilhelmshaven but did not do so.\n\nNote: wind turbine designations in brackets mean the turbine either temporarily unavailable. or has been taken off sale permanently.\nEnercon was prohibited from exporting their wind turbines to the US until 2010 due to alleged infringement of \n. In a dispute before the United States International Trade Commission, Enercon did not challenge the validity of the US patent but argued that their technology was not affected. The ITC decided that the patent covered the technology in question and banned Enercon turbines from the US market until 2010. Later on, a cross patent agreement was made with the competitor General Electric, the successor of Kenetech, after similar claims of Enercon against GE. According to a NSA employee detailed information concerning Enercon was passed on to Kenetech via ECHELON. The aim of the alleged industrial espionage against Enercon was the forwarding of details of Wobben's generator technology to a US firm.\n\n\n"}
{"id": "18243405", "url": "https://en.wikipedia.org/wiki?curid=18243405", "title": "Energy Technology Perspectives", "text": "Energy Technology Perspectives\n\nEnergy Technology Perspectives (ETP) is a publication on energy technology published by the International Energy Agency’s (IEA). It demonstrates how technologies can help achieve the objective of limiting the global temperature rise to 2°C and enhancing energy security.\n\nG8 leaders invited the International Energy Agency to contribute to the Gleneagles G8 Plan of Action on Climate Change, Clean Energy and Sustainable Development in 2005. They asked the IEA to advise on alternative scenarios and strategies for a \"clean, clever, competitive energy future\". The results of this initiative included the Energy Technology Perspectives, first published in 2006.\n\n\"Energy Technology Perspectives\" (ETP) gives a long-term outlook on the global energy system. The main results are presented up to the year 2050. For the 2012 edition the analysis was extended to include scenarios up to 2075. ETP models the global energy system under different scenarios for around 500 technology options.\n\nAimed at policy makers, energy experts, business leaders and investors the book's purpose is to serve as a guide for decision makers on energy trends when setting policy and business objectives concerning energy technology.\n\nThe book is accompanied by an online component which includes data visualisations, downloadable data and figures complements.\n\nInformation on the modelling methodology, assumptions and results are also presented online.\n\n\"Energy Technology Perspectives\" 2014 (ETP 2014) was published in May 2014.\n\nThe publication focuses on electricity’s potential to increase energy efficiency and reduce emissions in the long-term. Limiting the long-term global temperature increase to 2 degrees remains a central objective of the work. \nThe book, subtitled \"Harnessing Electricity’s Potential\" explains that electricity is key to sustainable energy systems for the future and focuses on what this means for generation, distribution and end-use consumption.\n\nETP 2014 explores the possibility of \"pushing the limits\" in six areas:\n\n• Decarbonising energy supply: is solar the answer to a cleaner energy future? \n• What role will natural gas play: flexibility vs. base load? \n• Electrified transport: how quickly and far can we go? \n• Can energy storage become a game changer? \n• What is the best way to finance low-carbon sources for electricity generation? \n• How can India prepare for population growth driven energy demand increases?\n\nEnergy Technology Perspectives scenarios are also the basis for other technology and sector focused IEA publications.\n\nTransition to Sustainable Buildings presents detailed scenarios and strategies to 2050, and demonstrates how to reach deep energy and emissions reduction in the buildings sector through a combination of best available technologies and intelligent public policy. \nThis publication is one of three end-use studies, together with industry and transport, which looks at the role of technologies and policies in transforming the way energy is used. It was published in June 2013.\n\nThe Tracking Clean Energy Progress 2013 is the IEA’s input to the Clean Energy Ministerial. It examines progress in the development and deployment of key clean energy technologies. Each technology and sector is tracked against interim 2020 targets in the IEA 2012 ETP 2 °C scenario, which lays out pathways to a sustainable energy system in 2050.\nThis report provides targeted recommendations to policy makers on how to scale up deployment of key clean energy technologies.\nThe Clean Energy Ministerial is a global forum to share best practices and promote policies and programs that encourage and facilitate the transition to a global clean energy economy. It was published in April 2013.\n\nNordic Energy Technology Perspectives is the first regional edition of the Energy Technology Perspectives series. It assesses how the Nordic region can achieve a carbon-neutral energy system by 2050. It was published in January 2013.\n\nEnergy Technology Perspectives 2012 (ETP 2012) presents global scenarios and strategies to 2050, a special chapter with scenario analysis to 2075 and detailed scenarios for nine world regions.\n\nETP 2010 builds on the success of earlier editions by providing updated scenarios with greater regional detail giving insights on which new technologies will be most important in the different regions of the world.\nIt highlights the key technological challenges and opportunities in each of the main energy-using sectors and describes the new policies that will be needed to realise change.\n\nThe second edition of Energy Technology Perspectives, ETP 2008 addresses the need of ever increasing energy supplies to sustain economic growth and development.\nThe study contains technology road maps for all key energy sectors, including electricity generation, buildings, industry and transport.\n\n\n"}
{"id": "30871267", "url": "https://en.wikipedia.org/wiki?curid=30871267", "title": "Equinox Summit: Energy 2030", "text": "Equinox Summit: Energy 2030\n\nThe Equinox Summit: Energy 2030 is an international conference that took place from June 5 to 9, 2011 in Waterloo, Ontario. The summit was presented by the Waterloo Global Science Initiative (WGSI) to develop scientific and technological solutions for the world’s current energy situation. It examines the present state of energy production, distribution, and storage, and then set benchmarks for where we need to be in the year 2030 to ensure global energy needs are met in a sustainable fashion.\n\nThe Equinox Summit: Energy 2030 aims to produce a blueprint detailing what must be done to accelerate the development of alternative, non-carbon producing sources of energy. The blueprint will provide recommended technologies and implementation strategies for future scientific investment. The blueprint will focus on energy production, distribution, and storage. This blueprint is aimed to be released in the year-long global impact phase that follows the Equinox Summit: Energy 2030.\n\nThe core of the Equinox Summit: Energy 2030 consisted of three days of working sessions where, under the mentorship of a team of seasoned advisors, a quorum of scientific experts in collaboration with a forum of next-generation leaders will discuss the recommendations and implementation strategies.\n\nThe conference ended with the presentation of the Quorum’s key findings in the Equinox Communiqué to a group of policymakers, science and technology influencers, and media.\n\nThe Quorum consisted of a panel of scientific experts representing different approaches to electricity energy production, distribution, and storage. They will present and debate their visions for solutions that will lead to viable sources of low-carbon energy. The Quorum will work towards producing a shortlist of key technology recommendations that policy makers can use to guide investment in science and technology over the next 20 years.\n\nA Forum of international, emerging leaders in public policy, industry and civil society interacted with the Quorum during the daily working sessions and observe how scientific process can inform policy debate. The Forum further examined the Quorum’s findings and propose their own recommendations for implementing the technologies proposed.\n\nA panel of advisors advised the Quorum on how to accelerate the research and implementation of future technologies. These advisors were public policy experts, investors and entrepreneurs who are experienced in political and economic aspects of energy, industry leaders, and government figures.\n\nThe Summit provided avenues for public engagement through daily public lectures and panel discussions that focus on energy literacy and other key issues that inform the energy dialogue globally. All public events were streamed live online, broadcast on television, and are archived for future on-demand viewing at wgsi.org.\n\n\n\n\n"}
{"id": "9568581", "url": "https://en.wikipedia.org/wiki?curid=9568581", "title": "Fatty acid degradation", "text": "Fatty acid degradation\n\nFatty acid degradation is the process in which fatty acids are broken down into their metabolites, in the end generating acetyl-CoA, the entry molecule for the citric acid cycle, the main energy supply of animals. It includes three major steps:\n\nInitially in the process of degradation, fatty acids are stored in fat cells (adipocytes). The breakdown of this fat is known as lipolysis. The products of lipolysis, free fatty acids, are released into the bloodstream and circulate throughout the body. During the breakdown of triacylglycerols into fatty acids, more than 75% of the fatty acids are converted back into triacylglycerol, a natural mechanism to conserve energy, even in cases of starvation and exercise.\n\nFatty acids must be activated before they can be carried into the mitochondria, where fatty acid oxidation occurs. This process occurs in two steps catalyzed by the enzyme fatty acyl-CoA synthetase.\n\nThe enzyme first catalyzes nucleophilic attack on the α-phosphate of ATP to form pyrophosphate and an acyl chain linked to AMP. The next step is formation of an activated thioester bond between the fatty acyl chain and Coenzyme A.\n\nThe balanced equation for the above is:\n\nRCOO + CoA + ATP → RCO-CoA + AMP + PP + 2H\nThis two-step reaction is freely reversible and its equilibrium lies near 1. To drive the reaction forward, the reaction is coupled to a strongly exergonic hydrolysis reaction: the enzyme inorganic pyrophosphatase cleaves the pyrophosphate liberated from ATP to two phosphate ions, consuming one water molecule in the process. Thus the net reaction becomes:\n\nRCOO + CoA + ATP + HO → RCO-CoA + AMP + 2P + 2H\n\nThe inner mitochondrial membrane is impermeable to fatty acids and a specialized carnitine carrier system operates to transport activated fatty acids from cytosol to mitochondri\n\nOnce activated, the acyl CoA is transported into the mitochondrial matrix. This occurs via a series of similar steps:\n\n\nIt is important to note that \"carnitine acyltransferase I\" undergoes allosteric inhibition as a result of malonyl-CoA, an intermediate in fatty acid biosynthesis, in order to prevent futile cycling between beta-oxidation and fatty acid synthesis.\n\nThe mitochondrial oxidation of fatty acids takes place in three major steps:\n1.β-oxidation: conversion of fatty acids into 2-carbon acetyl Co-A units.\n2.entry of acetyl Co-A into TCA cycle to yield energy.\n3.finally, the electron transport chain in the mitochondria.though in this step, no direct participation of fatty acids, the reaction continues.\n\nOnce inside the mitochondria, the β-oxidation of fatty acids occurs via five recurring steps:\n\n"}
{"id": "10598441", "url": "https://en.wikipedia.org/wiki?curid=10598441", "title": "Frederick Rossini", "text": "Frederick Rossini\n\nFrederick Dominic Rossini (1899- 1990) was an American thermodynamicist noted for his work in chemical thermodynamics.\n\nIn 1920, at the age of twenty-one, Rossini entered Carnegie-Mellon University in Pittsburgh, and soon was awarded a full-time teaching scholarship. He graduated with a B.S. in chemical engineering in 1925, followed by an M.S. degree in science in physical chemistry in 1926.\n\nAs a result of reading Lewis and Randall's classical 1923 textbook \"Thermodynamics and the Free Energy of Chemical Substances\" he wrote to Gilbert N. Lewis and as a result he was offered a teaching fellowship at the University of California at Berkeley. Among his teachers were Gilbert Lewis and William Giauque. Rossini's doctoral dissertation on the heat capacities of strong electrolytes in aqueous solution was supervised by Merle Randall. His Ph.D. degree was awarded in 1928, after only 21 months of graduate work, even though he continued to serve as a teaching fellow throughout this entire period.\n\nIn 1932, Frederick Rossini, Edward W. Washburn, and Mikkel Frandsen authored “The Calorimetric Determination of the Intrinsic Energy of Gases as a Function of the Pressure.” This experiment resulted in the development of the Washburn Correction for bomb calorimetry, a decrease or correction of the results of a calorimetric procedure to normal states.\n\nIn 1950, he published his popular textbook \"Chemical Thermodynamics\".\n\nHe served as dean of the Notre Dame College of Science from 1960 to 1967.\n\n"}
{"id": "25543982", "url": "https://en.wikipedia.org/wiki?curid=25543982", "title": "Fréedericksz transition", "text": "Fréedericksz transition\n\nThe Fréedericksz transition is a phase transition in liquid crystals produced when a sufficiently strong electric or magnetic field is applied to a liquid crystal in an undistorted state. Below a certain field threshold the director remains undistorted. As the field value is gradually increased from this threshold, the director begins to twist until it's aligned with the field. In this fashion the Fréedericksz transition can occur in three different configurations known as the twist, bend, and splay geometries. The phase transition was first observed by Fréedericksz and Repiewa in 1927. In this first experiment of theirs, one of the walls of the cell was concave so as to produce a variation in thickness along the cell. The phase transition is named in honor of the Russian physicist Vsevolod Frederiks.\n\nIf a nematic liquid crystal that is confined between two parallel plates that induce a planar anchoring is placed in a sufficiently high constant electric field then the director will be distorted. If under zero field the director aligns along the x-axis then upon application of an electric field along the y-axis the director will be given by:\nUnder this arrangement the distortion free energy density becomes:\nThe total energy per unit volume stored in the distortion and the electric field is given by:\nThe free energy per unit area is then:\nMinimizing this using calculus of variations gives:\nRewriting this in terms of formula_9 and formula_10 where formula_11 is the separation distance between the two plates results in the equation simplifying to:\nBy multiplying both sides of the differential equation by formula_13 this equation can be simplified further as follows:\nThe value formula_17 is the value of formula_18 when formula_19. Substituting formula_20 and formula_21 into the equation above and integrating with respect to formula_22 from 0 to 1 gives:\nThe value K(k) is the complete elliptic integral of the first kind. By noting that formula_24 one finally obtains the threshold electric field formula_25.\nAs a result, by measuring the threshold electric field one can effectively measure the twist Frank constant so long as the anisotropy in the electric susceptibility and plate separation is known.\n\n"}
{"id": "31325710", "url": "https://en.wikipedia.org/wiki?curid=31325710", "title": "Genpatsu-shinsai", "text": "Genpatsu-shinsai\n\n, meaning \"nuclear power plant earthquake disaster\" (from the two words \"Genpatsu\" – nuclear power plant – and \"Shinsai\" – earthquake disaster) is a term which was coined by Japanese seismologist Professor Katsuhiko Ishibashi in 1997. It describes a domino effect scenario in which a major earthquake causes a severe accident at a nuclear power plant near a major population centre, resulting in an uncontrollable release of radiation in which the radiation levels make damage control and rescue impossible, and earthquake damage severely impedes the evacuation of the population. Ishibashi envisages that such an event would have a global impact and a 'fatal' effect on Japan, seriously affecting future generations.\n\nIn Japan, Ishibashi believes that a number of nuclear power stations could be involved in such a scenario, but that the Hamaoka Nuclear Power Plant, located near the centre of the expected Tōkai earthquakes, is the most likely candidate. He is also concerned that a similar scenario could take place elsewhere in the world. As a result, he believes that the matter should be a global concern.\n\n\n"}
{"id": "30176398", "url": "https://en.wikipedia.org/wiki?curid=30176398", "title": "Golding Bird", "text": "Golding Bird\n\nGolding Bird (9 December 1814 – 27 October 1854) was a British medical doctor and a Fellow of the Royal College of Physicians. He became a great authority on kidney diseases and published a comprehensive paper on urinary deposits in 1844. He was also notable for his work in related sciences, especially the medical uses of electricity and electrochemistry. From 1836, he lectured at Guy's Hospital, a well-known teaching hospital in London and now part of King's College London, and published a popular textbook on science for medical students called \"Elements of Natural Philosophy\".\n\nHaving developed an interest in chemistry while still a child, largely through self-study, Bird was far enough advanced to deliver lectures to his fellow pupils at school. He later applied this knowledge to medicine and did much research on the chemistry of urine and of kidney stones. In 1842, he was the first to describe oxaluria, a condition which leads to the formation of a particular kind of stone.\n\nBird, who was a member of the London Electrical Society, was innovative in the field of the medical use of electricity, designing much of his own equipment. In his time, electrical treatment had acquired a bad name in the medical profession through its widespread use by quack practitioners. Bird made efforts to oppose this quackery, and was instrumental in bringing medical electrotherapy into the mainstream. He was quick to adopt new instruments of all kinds; he invented a new variant of the Daniell cell in 1837 and made important discoveries in electrometallurgy with it. He was not only innovative in the electrical field, but he also designed a flexible stethoscope, and in 1840 published the first description of such an instrument.\n\nA devout Christian, Bird believed Bible study and prayer were just as important to medical students as their academic studies. He endeavoured to promote Christianity among medical students and encouraged other professionals to do likewise. To this end, Bird was responsible for the founding of the Christian Medical Association, although it did not become active until after his death. Bird had lifelong poor health and died at the age of 39.\n\nBird was born in Downham, Norfolk, England, on 9 December 1814. His father (also named Golding Bird) had been an officer in the Inland Revenue in Ireland, and his mother, Marrianne, was Irish. He was precocious and ambitious, but childhood rheumatic fever and endocarditis left him with poor posture and lifelong frail health. He received a classical education when he was sent with his brother Frederic to stay with a clergyman in Wallingford, where he developed a lifelong habit of self-study. From the age of 12, he was educated in London, at a private school that did not promote science and provided only a classical education. Bird, who seems to have been far ahead of his teachers in science, gave lectures in chemistry and botany to his fellow pupils. He had four younger siblings, of whom his brother Frederic also became a physician and published on botany.\n\nIn 1829, when he was 14, Bird left school to serve an apprenticeship with the apothecary William Pretty in Burton Crescent, London. He completed it in 1833 and was licensed to practise by the Worshipful Society of Apothecaries at Apothecaries' Hall in 1836. He received this licence without examination because of the reputation he had gained as a student at Guy's, the London teaching hospital where he had become a medical student in 1832 while still working at his apprenticeship. At Guy's he was influenced by Thomas Addison, who recognised his talents early on. Bird was an ambitious and very capable student. Early in his career he became a Fellow of the Senior Physical Society, for which a thesis was required. He received prizes for medicine, obstetrics, and ophthalmic surgery at Guy's and the silver medal for botany at Apothecaries' Hall. Around 1839 to 1840, he worked on breast disease at Guy's as an assistant to Sir Astley Cooper.\n\nBird graduated from the University of St Andrews with an MD in 1838 and an MA in 1840 while continuing to work in London. St Andrews required no residence or examination for the MD. Bird obtained his degree by submitting testimonials from qualified colleagues, which was common practice at the time. Once qualified in 1838, at the age of 23, he entered general practice with a surgery at 44 Seymour Street, Euston Square, London, but was unsuccessful at first because of his youth. In the same year, however, he became physician to the Finsbury Dispensary, a post he held for five years. By 1842, he had an income of £1000 a year from his private practice. Adjusted for inflation, this amounts to a spending power of about £ now. At the end of his career, his income was just under £6000. He became a licentiate of the Royal College of Physicians in 1840, and a fellow in 1845.\nBird lectured on natural philosophy, medical botany and urinary pathology from 1836 to 1853 at Guy's. He lectured on \"materia medica\" at Guy's from 1843 to 1853 and at the Royal College of Physicians from 1847 to 1849. He also lectured at the Aldersgate School of Medicine. Throughout his career, he published extensively, not only on medical matters, but also on electrical science and chemistry.\n\nBird became the first head of the electricity and galvanism department at Guy's in 1836, under the supervision of Addison, since Bird did not graduate until 1838. In 1843, he was appointed assistant physician at Guy's, a position for which he had lobbied hard, and in October that year he was put in charge of the children's outpatients ward. Like his electrotherapy patients, the children were largely poor relief cases who could not afford to pay for medical treatment and were much used for the training of medical students. It was generally accepted at this time that poor relief cases could be used for experimental treatment, and their permission was not required. Bird published in the hospital journal a series of reports on childhood diseases, based on case studies from this work.\n\nMarrying Mary Ann Brett in 1842, Bird moved from his family home at Wilmington Square, Clerkenwell, to 19 Myddleton Square. They had two daughters and three sons, the second of whom, Cuthbert Hilton Golding-Bird (1848–1939), became a notable surgeon. Another son, Percival Golding-Bird, became a priest in Rotherhithe,\n\nBird was a Fellow of the Linnaean Society (elected 1836), the Geological Society (elected 1836) and the Royal Society (elected 1846). He joined the Pathological Society of London (which eventually merged into the Royal Society of Medicine) when it was formed in 1846. He also belonged to the London Electrical Society founded by William Sturgeon and others. This body was very unlike the elite scholarly institutions; it was more like a craft guild with a penchant for spectacular demonstrations. Nevertheless, it had some notable members, and new machines and apparatus were regularly discussed and demonstrated. Bird was also a Freemason from 1841 and was the Worshipful Master of the St Paul's lodge in 1850. He left the Freemasons in 1853.\n\nBird was vain, with a tendency to self-promotion, and his driving ambition occasionally led him into conflict with others. He was involved in a number of very public disputes in contemporary medical journals, including the dispute with the Pulvermacher Company and a dispute over the development of the stethoscope. However, he was said to give his patients his undivided attention and a complete commitment to their welfare. He was a fine speaker, a good lecturer and an eloquent debater.\n\nDiagnosed with heart disease by his brother in 1848 or 1849, Bird was forced to stop work. By 1850, however, he was again working as hard as ever and had extended his practice so much that he needed to move to a larger house in Russell Square. But in 1851, acute rheumatism led Bird to take an extended holiday with his wife in Tenby, where he pursued investigations in botany, marine fauna and cave life as pastimes. These long summer breaks were repeated in 1852 and 1853 at Torquay and Tenby. Even on holiday, his fame caused him to receive many requests for consultations. In 1853, he purchased an estate, St Cuthbert, for his retirement in Tunbridge Wells, but it needed some work, and he could not leave London until June 1854. Meanwhile, he continued to see patients, but only in his house, despite seriously deteriorating health. He died on 27 October 1854 at St Cuthbert from a urinary tract infection and suffering from kidney stones. His early death at 39 may have been due to a combination of lifelong frail health and overwork, which Bird himself knew to be destroying him. He is buried in Woodbury Park Cemetery, Tunbridge Wells.\n\nAfter his death, Mary instituted the Golding Bird Gold Medal and Scholarship for sanitary science, later named the Golding Bird Gold Medal and Scholarship for bacteriology, which was awarded annually at Guy's teaching hospital. The prize was instituted in 1887 and was still being awarded in 1983, although it is no longer a current prize. From 1934 onwards, a Golding Bird Gold Medal and Scholarship was also awarded for obstetrics and gynaecology.\nAmong the notable recipients of the medal were Nathaniel Ham (1896), Alfred Salter (1897), Russell Brock (1926), John Beale (1945), and D. Bernard Amos (\"circa\" 1947–1951).\n\nThe collateral sciences are those sciences that have an important role in medicine but do not form part of medicine themselves, especially physics, chemistry, and botany (because botany is a rich source of drugs and poisons). Until the end of the first half of the 19th century, chemical analysis was rarely used in medical diagnosis – there was even hostility to the idea in some quarters. Most of the work in this area at that time was carried out by researchers associated with Guy's.\n\nBy the time Golding Bird was a medical student at Guy's, the hospital already had a tradition of studying physics and chemistry as they related to medicine. Bird followed this tradition and was particularly influenced by the work of William Prout, an expert in chemical physiology. Bird became well known for his knowledge of chemistry. An early example dates from 1832, when he commented on a paper on the copper sulphate test for arsenic poisoning, delivered by his future brother-in-law R. H. Brett to the Pupils' Physical Society. Bird criticised the test's positive result when a green precipitate is formed, claiming the test was inconclusive because precipitates other than copper arsenite can produce the same green colour.\n\nBird did not limit himself to challenging his future brother-in-law. In 1834, Bird and Brett published a paper on the analysis of blood serum and urine, in which they argued against some work by Prout. Prout had said (in 1819) that the pink sediment in urine was due to the presence of ammonium purpurate, but Bird's tests failed to verify this. Though Bird was still only a student and Prout held great authority, Prout felt it necessary to reply to the challenge. In 1843, Bird tried to identify the pink compound; he failed, but was convinced it was a new chemical and gave it the name \"purpurine\". This name did not stick, however, and the compound became known as uroerythrin from the work of Franz Simon. Its structure was finally identified only in 1975.\n\nAround 1839, recognising Bird's abilities in chemistry, Astley Cooper asked him to contribute to his book on breast disease. Bird wrote a piece on the chemistry of milk, and the book was published in 1840. Although the book is primarily about human anatomy, it includes a chapter on comparative anatomy covering several species, for which Bird carried out an analysis of dog and porpoise milk. Also in 1839, Bird published his own \"Elements of Natural Philosophy\", a textbook on physics for medical students. Taking the view that existing texts were too mathematical for medical students, Bird avoided such material in favour of clear explanations. The book proved popular and remained in print for 30 years, although some of its mathematical shortcomings were made good in the fourth edition by Charles Brooke.\n\nIn 1836, Bird was put in charge of the newly formed department of electricity and galvanism under the supervision of Addison. While this was not the first hospital to employ electrotherapy, it was still considered very experimental. Previous hospital uses had either been short-lived or based on the whim of a single surgeon, such as John Birch at St Thomas' Hospital. At Guy's, the treatment was part of the hospital system and became well-known to the public, so much so that Guy's was parodied for its use of electricity in the \"New Frankenstein\" satirical magazine.\n\nIn his electrotherapy, Bird used both electrochemical and electrostatic machines (and later also electromagnetic induction machines) to treat a very wide range of conditions, such as some forms of chorea. Treatments included peripheral nerve stimulation, electrical muscle stimulation and electric shock therapy. Bird also used his invention, the electric moxa, to heal skin ulcers.\n\nIt was already clear from the work of Michael Faraday that electricity and galvanism were essentially the same. Bird realised this, but continued to divide his apparatus into electrical machines, which (according to him) delivered a high voltage at low current, and galvanic apparatus, which delivered a high current at low voltage. The galvanic equipment available to Bird included electrochemical cells such as the voltaic pile and the Daniell cell, a variant of which Bird devised himself. Also part of the standard equipment were induction coils which, together with an interrupter circuit, were used with one of the electrochemical cells to deliver an electric shock. The electrical (as opposed to galvanic) machines then available were friction-operated electrostatic generators consisting of a rotating glass disc or cylinder on which silk flaps were allowed to drag as the glass rotated. These machines had to be hand-turned during treatment, but it was possible to store small amounts of static electricity in Leyden jars for later use.\n\nBy 1849, generators based on Faraday's law of induction had become advanced enough to replace both types of machines, and Bird was recommending them in his lectures. Galvanic cells suffered from the inconvenience of having to deal with the electrolyte acids in the surgery and the possibility of spillages; electrostatic generators required a great deal of skill and attention to keep them working successfully. Electromagnetic machines, on the other hand, have neither of these drawbacks; the only criticism levelled by Bird was that the cheaper machines could only deliver an alternating current. For medical use, particularly when treating a problem with nerves, a unidirectional current of a particular polarity was often needed, requiring the machine to have split rings or similar mechanisms. However, Bird considered alternating current machines suitable for cases of amenorrhœa.\n\nThe required direction of current depended on the direction in which electric current was thought to flow in nerves in the human or animal body. For motor functions, for instance, the flow was taken to be from the centre towards the muscles at the extremities, so artificial electrical stimulation needed to be in the same direction. For sensory nerves, the opposite applied: flow was from the extremity to the centre, and the positive electrode would be applied to the extremity. This principle was demonstrated by Bird in an experiment with a living frog. A supply of frogs was usually on hand, as they were used in the frog galvanoscope. The electromagnetic galvanometer was available at the time, but frogs' legs were still used by Bird because of their much greater sensitivity to small currents. In the experiment, the frog's leg was almost completely severed from its body, leaving only the sciatic nerve connected, and electric current was then applied from the body to the leg. Convulsions of the leg were seen when the muscle was stimulated. Reversing the current, however, produced no movement of the muscle, merely croaks of pain from the frog. In his lectures, Bird describes many experiments with a similar aim on human sensory organs. In one experiment by Grapengiesser, for instance, electric current is passed through the subject's head from ear to ear, causing a sound to be hallucinated. The ear connected to the positive terminal hears a louder sound than that connected to the negative.\n\nBird designed his own interrupter circuit for delivering shocks to patients from a voltaic cell through an induction coil. Previously, the interrupter had been a mechanical device requiring the physician to turn a cog wheel or employ an assistant to do so. Bird wished to free his hands to apply the electricity more exactly to the required part of the patient. His interrupter worked automatically by magnetic induction at a reasonably fast rate. The faster the interrupter switches, the more frequently an electric shock is delivered to the patient; the aim is to make the frequency as high as possible.\n\nBird's interrupter had the medically disadvantageous feature that current was supplied in opposite directions during the make and break operations. Treatment often required the current to be supplied in one specified direction only. Bird produced a unidirectional interrupter using a mechanism now called split rings. This design suffered from the disadvantage that automatic operation was lost and the interrupter had once again to be hand-cranked. Nevertheless, this arrangement remained a cheaper option than electromagnetic generators for some time.\n\nThree classes of electrotherapy were in use. One was the electric bath, which consisted of sitting the patient on an insulated stool with glass legs and connecting the patient to one electrode, usually the positive one, of an electrostatic machine. The patient's skin became charged as if he or she were in a \"bath of electricity\". The second class of treatment could be performed while the patient was in the electric bath. This consisted of bringing a negative electrode close to the patient, usually near the spine, causing sparks to be produced between the electrode and the patient. Electrodes of various shapes were available for different medical purposes and places of application on the body. Treatment was applied in several sessions of around five minutes, often blistering the skin. The third class of treatment was electric shock therapy, in which an electric shock was delivered from a galvanic battery (later electromagnetic generators) via an induction coil to greatly increase the voltage. It was also possible to deliver electric shocks from the charge stored in a Leyden jar, but this was a much weaker shock.\n\nElectric stimulation treatment was used to treat nervous disorders where the nervous system was unable to stimulate a required glandular secretion or muscle activity. It had previously been successfully used to treat some forms of asthma. Bird used his apparatus to treat Sydenham's chorea (St Vitus's dance) and other forms of spasm, some forms of paralysis (although the treatment was of no use where nerves had been physically damaged), opiate overdose (since it kept the patient awake), bringing on menstruation where this had failed (amenorrhoea), and hysteria, a supposed disease of women. Paralysed bladder function in young girls was attributed to the now archaic condition of hysteria. It was treated with the application of a strong electric current between the sacrum and the pubis. Although the treatment worked, in that it caused the bladder to empty, Bird suspected in many cases it did so more through fear and pain than any therapeutic property of electricity.\n\nElectric shock treatment had become fashionable among the public, but often was not favoured by physicians except as a last resort. Its popularity led to many inappropriate treatments, and fraudulent practitioners were widespread. Quack practitioners claimed the treatment as a cure for almost anything, regardless of its effectiveness, and made large sums of money from it. Bird, however, continued to stand by the treatment when properly administered. He convinced an initially sceptical Addison of its merits, and the first publication (in 1837) describing the work of the electrifying unit was authored by Addison, not Bird, although Bird is clearly, and rightly, credited by Addison. Having the paper authored by Addison did a great deal to gain acceptability in a still suspicious medical fraternity. Addison held great authority, whereas Bird at this stage was unknown. Bird's 1841 paper in \"Guy's Hospital Reports\" contained an impressively long list of successful case studies. In 1847 he brought the subject fully into the realm of \"materia medica\" when he delivered the annual lecture to the Royal College of Physicians on this subject. He spoke out tirelessly against the numerous quack practitioners, in one case exposing railway telegraph operators who were claiming to be medical electricians, although they had no medical training at all. In this way, Bird was largely responsible for the rehabilitation of electrical treatment among medical practitioners. His work, with Addison's support, together with the increasing ease of using the machines as the technology progressed, brought the treatment into wider use in the medical profession.\n\nBird invented the \"electric moxa\" in 1843. The name is a reference to the acupuncture technique of moxibustion and was probably influenced by the introduction of electroacupuncture, in which the needles are augmented by an electric current, two decades earlier in France. The electric moxa, however, was not intended for acupuncture. It was used to produce a suppurating sore on the skin of the patient to treat some conditions of inflammation and congestion by the technique of counter-irritation. The sore had previously been created by much more painful means, such as cautery or even burning charcoal. Bird's design was based on a modification of an existing instrument for the local electrical treatment of hemiplegia, and consisted of a silver electrode and a zinc electrode connected by copper wire. Two small blisters were produced on the skin, to which the two electrodes were then connected and held in place for a few days. Electricity was generated by electrolytic action with body fluids. The blister under the silver electrode healed, but the one under the zinc electrode produced the required suppurating sore.\n\nThe healing of the blister under the silver electrode was of no importance for a counter-irritation procedure, but it suggested to Bird that the electric moxa might be used for treating obstinate leg ulcers. This was a common complaint among the working classes in Bird's time, and hospitals could not admit the majority of cases for treatment. The moxa improved the situation by enabling sufferers to be treated as outpatients. The silver electrode of the moxa was applied to the ulcer to be healed, while the zinc electrode was applied a few inches away to a place where the upper layer of skin had been cut away. The whole apparatus was then bandaged in place as before. The technique was successfully applied by others on Bird's recommendation. Thomas Wells later discovered that it was unnecessary to damage the skin under the zinc plate. He merely moistened the skin with vinegar before applying the zinc electrode.\n\nThere was some controversy over Bird's endorsement of a machine invented by one I. L. Pulvermacher that became known as Pulvermacher's chain. The main market for this device was the very quack practitioners that Bird so detested, but it did actually work as a generator. Bird was given a sample of this machine in 1851 and was impressed enough to give Pulvermacher a testimonial stating that the machine was a useful source of electricity. Bird thought that it could be used by physicians as a portable device. Electrically, the machine worked like a voltaic pile, but was constructed differently. It consisted of a number of wooden dowels, each with a bifilar winding of copper and zinc coils. Each winding was connected to the next dowel by means of metal hooks and eyes, which also provided the electrical connection. The electrolyte was provided by soaking the dowels in vinegar.\n\nNaively, Bird appears to have expected Pulvermacher not to use this testimonial in his advertising. When Pulvermacher's company did so, Bird suffered some criticism for unprofessional behaviour, although it was never suggested that Bird benefited financially, and Bird stated in his defence that the testimonial was only ever intended as a letter of introduction to physicians in Edinburgh. Bird was particularly upset that Pulvermacher's company had used quotations from Bird's publications about the benefits of electrical treatment and misrepresented them as describing benefits of Pulvermacher's product. Bird also criticised Pulvermacher's claim that the chain could be wrapped around an affected limb for medical treatment. Although the flexible nature of its design lent itself to wrapping, Bird said that it would be next to useless in this configuration. According to Bird, the patient's body would provide a conductive path across each cell, thus preventing the device from building up a medically useful voltage at its terminals.\n\nBird used his position as head of the department of electricity and galvanism to further his research efforts and to aid him in teaching his students. He was interested in electrolysis and repeated the experiments of Antoine César Becquerel, Edmund Davy and others to extract metals in this way. He was particularly interested in the possibility of detecting low levels of heavy metal poisons with this technique, pioneered by Davy. Bird also studied the properties of albumen under electrolysis, finding that the albumen coagulated at the anode because hydrochloric acid was produced there. He corrected an earlier erroneous conclusion by W. T. Brande that high electric current caused coagulation at the cathode also, showing that this was entirely due to fluid flows caused by the strong electric field.\n\nThe formation of copper plates on the cathode was noticed in the Daniell cell shortly after its invention in 1836. Bird began a thorough investigation of this phenomenon in the following year. Using solutions of sodium chloride, potassium chloride and ammonium chloride, He succeeded in coating a mercury cathode with sodium, potassium and ammonium respectively, producing amalgams of each of these. Not only chlorides were used; beryllium, aluminium and silicon were obtained from the salts and oxides of these elements.\n\nIn 1837, Bird constructed his own version of the Daniell cell. The novel feature of Bird's cell was that the two solutions of copper sulphate and zinc sulphate were in the same vessel, but kept separate by a barrier of Plaster of Paris, a common material used in hospitals for setting bone fractures. Being porous, Plaster of Paris allows ions to cross the barrier, while preventing the solutions from mixing. This arrangement is an example of a single-cell Daniell cell, and Bird's invention was the first of this kind. Bird's cell was the basis for the later development of the porous pot cell, invented in 1839 by John Dancer.\n\nBird's experiments with his cell were important for the new discipline of electrometallurgy. An unforeseen result was the deposition of copper on and within the plaster, without any contact with the metal electrodes. On breaking apart the plaster it was found that veins of copper were formed running right through it. So surprising was this result that it was at first disbelieved by electrochemical researchers, including Faraday. Deposition of copper and other metals had previously been noted, but only on metal electrodes. Bird's experiments sometimes get him credit for being the founder of the industrial field of electrometallurgy. In particular, Bird's discovery is the principle behind electrotyping. However, Bird himself never made practical use of this discovery, nor did he carry out any work in metallurgy as such. Some of Bird's contemporaries with interests in electrometallurgy wished to bestow the credit on Bird in order to discredit the commercial claims of their rivals.\n\nBird thought there was a connection between the functioning of the nervous system and the processes seen in electrolysis at very low, steady currents. He knew that the currents in both were of the same order. To Bird, if such a connection existed, it made electrochemistry an important subject to study for biological reasons.\n\nIn 1837 Bird took part in an investigation of the dangers posed by the arsenic content of cheap candles. These were stearin candles with white arsenic added, which made them burn more brightly than ordinary candles. The combination of cheapness and brightness made them popular. The investigation was conducted by the Westminster Medical Society, a student society of Westminster Hospital, and was led by John Snow, later to become famous for his public health investigations. Snow had previously investigated arsenic poisoning when he and several fellow students were taken badly ill after he introduced a new process for preserving cadavers at the suggestion of lecturer Hunter Lane. The new process involved injecting arsenic into the blood vessels of the corpse. Snow found that the arsenic became airborne as a result of chemical reactions with the decomposing corpse, and this was how it was ingested. Bird's part in the candle investigation was to analyse the arsenic content of the candles, which he found to have recently been greatly increased by the manufacturers. Bird also confirmed by experiment that the arsenic became airborne when the candles were burnt. The investigators exposed various species of animal and bird to the candles in controlled conditions. The animals all survived, but the birds died. Bird investigated the bird deaths and analysed the bodies, finding small amounts of arsenic. No arsenic was found on the feathers, however, indicating that poisoning was not caused by breathing airborne arsenic, since arsenic in the air would be expected to adhere to the feathers. However, Bird found that large amounts of arsenic were in the birds' drinking water, indicating that this was the route taken by the poison.\n\nAlthough it had been known how to prepare carbon monoxide since 1776, it was not at first recognised that carbon monoxide poisoning was the mechanism of death and injury from stoves burning carbonaceous fuels. A coroner's inquest into the death in 1838 of James Trickey, a nightwatchman who had spent all night by a new type of charcoal burning stove in St Michael, Cornhill, concluded that the poison involved was carbonic acid (that is, carbon dioxide) rather than carbon monoxide. Both Bird and Snow gave evidence to the inquest supporting poisoning by carbonic acid. Bird himself started to suffer ill effects while collecting air samples from the floor near the stove. However, the makers of the stove, Harper and Joyce, produced a string of their own expert witnesses, who convinced the jury to decide that death was caused by apoplexy, and that \"impure air\" was only a contributing factor. Among the unscientific claims made at the inquest by Harper and Joyce were that carbonic gas would rise to the ceiling (in fact it is heavier than air and, according to Bird, would lie in a layer close to the floor, just where the sleeping Trickey's head would rest) and that \"deleterious vapour\" from the coffins in the vaults had risen into the church. After the inquest Joyce threatened to sue a journal which continued to criticise the stove for its lack of ventilation. In a subsequent clarification, Bird made it clear that any stove burning carbonaceous fuel was dangerous if it did not have a chimney or other means of ventilation. In fact, Trickey had only been placed in the church in the first place at the suggestion of Harper, who was expecting him to give favourable reports of the new stove's performance.\n\nBird read a paper to the Senior Physical Society in 1839, reporting on tests he conducted of the effects on sparrows of poisoning by carbonaceous fumes. This paper was of some importance and resulted in Bird giving his views to the British Association in the same year. (He acted as a secretary to the chemical section of the British Association in Birmingham.) Bird also presented the paper at the Westminster Medical School, where Snow took a special interest in it. Until then, Snow and many others had believed that carbonic acid acted merely by excluding oxygen. The experiments of Bird and others convinced him that it was harmful in its own right, but he still did not subscribe to Bird's view that it was an active poison. Also in 1839, Bird published a comprehensive paper in \"Guy's Hospital Reports\", complete with many case histories, in which he documents the state of knowledge. He realised that at least some cases of poisoning from stoves were due not to carbonic acid, but to some other agent, although he still had not identified it as carbon monoxide.\n\nBird did a great deal of research in urology, including the chemistry of both urine and kidney stones, and soon became a recognised expert. This work occupied a large proportion of his effort, and his writings on urinary sediments and kidney stones were the most advanced at the time. His work followed on from, and was much influenced by, that of Alexander Marcet and William Prout. Marcet was also a physician at Guy's; Prout held no position at Guy's, but was connected with the hospital and well known there. For instance, when Marcet discovered a new constituent of kidney stones, xanthic oxide, he sent it to Prout for analysis. Prout discovered a new substance himself in 1822, a constituent of urine which he named melanic acid, because it turned black on contact with air.\n\nBird studied and categorised the collection of stones at Guy's, concentrating particularly on the crystal structures of the nuclei, since stone formation followed once there was a nucleus on which to form. He considered the chemistry of the nuclei to be the most important aspect of stone formation. Bird identified many species of stone, classified by the chemistry of the nucleus, but determined that they all fell within two overall groups: organic stones caused by a misfunctioning bodily process, and excessive inorganic salts causing sediment on which the stone could nucleate. In 1842, Bird became the first to describe oxaluria, sometimes called Bird's disease, which is caused by an excess of oxalate of lime in the urine. This is the second most common cause of kidney stones, the first being uric acid and its ammonium salt. There are several others, such as ammonium oxalate. In his great work \"Urinary Deposits\", Bird devotes much space to the identification of chemicals in urine by microscopic examination of the appearance of crystals in it. He shows how the appearance of crystals of the same chemical can vary greatly under differing conditions, and especially how the appearance changes with disease. \"Urinary Deposits\" became a standard text on the subject; there were five editions between 1844 and 1857. In the fourth edition Bird added a recommendation to wash out the bladder in cases of alkaline urine, after an experiment by Snow showed that stale urine became alkaline when fresh urine was slowly dripped into it. Bird knew that alkaline urine encouraged phosphate precipitation and the consequent encrustation and stone formation. The last edition of \"Urinary Deposits\" was updated after Bird's death by Edmund Lloyd Birkett.\n\nBird was the first to recognise that urinary casts are an indication of Bright's disease. Casts were first discovered by Henry Bence Jones. They are microscopic cylinders of Tamm-Horsfall protein that have been precipitated in the kidneys and then released into the urine.\n\nA prevalent idea in the 18th and early 19th centuries was that illness was a result of the condition of the whole body. The environment and the activity of the patient thus played a large part in any treatment. The epitome of this kind of thinking was the concept of the vital force, which was supposed to govern the chemical processes within the body. This theory held that organic compounds could only be formed within living organisms, where the vital force could come into play. This belief had been known to be false ever since Friedrich Wöhler succeeded in synthesising urea from inorganic precursors in 1828. Nevertheless, the vital force continued to be invoked to explain organic chemistry in Bird's time. Sometime in the middle of the 19th century, a new way of thinking started to take shape, especially among younger physicians, fuelled by rapid advances in the understanding of chemistry. For the first time, it became possible to identify specific chemical reactions with specific organs of the body, and to trace their effects through the various functional relations of the organs and the exchanges between them.\n\nAmong these younger radicals were Bird and Snow; among the old school was William Addison (a different person from Bird's superior at Guy's). Addison disliked the modern reliance on laboratory and theoretical results favoured by the new generation, and challenged Richard Bright (who gave his name to Bright's disease) when Bright suggested that the source of the problem in oedema was the kidneys. Addison preferred to believe that the condition was caused by intemperance or some other external factor, and that since the whole body had been disrupted, it could not be localised to a specific organ. Addison further challenged Bright's student, Snow, when in 1839 Snow suggested from case studies and laboratory analysis that oedema was associated with an increase in albumin in the blood. Addison dismissed this as a mere epiphenomenon. Bird disagreed with Snow's proposed treatment, but his arguments clearly show him to be on the radical side of the debate, and he completely avoided whole-body arguments. Snow had found that the proportion of urea in the urine of his patients was low and concluded from this that urea was accumulating in the blood, and therefore proposed bloodletting to counter this. Bird disputed that increased urea in the blood was the cause of kidney disease and doubted the effectiveness of this treatment, citing the results of François Magendie, who had injected urea into the blood, apparently with no ill effects. It is not clear whether Bird accepted Snow's reasoning that urea must be accumulating, or whether he merely adopted it for the sake of argument; while a student in 1833, he had disputed this very point with another of Bright's students, George Rees.\n\nJustus von Liebig is another important figure in the development of the new thinking, although his position is ambiguous. He explained chemical processes in the body in terms of addition and subtraction of simple molecules from a larger organic molecule, a concept that Bird followed in his own work. But even the materialistic Liebig continued to invoke the vital force for processes inside \"living\" animal bodies. This seems to have been based on a belief that the entire living animal is required for these chemical processes to take place. Bird helped to dispel this kind of thinking by showing that specific chemistry is related to specific organs in the body rather than to the whole animal. He challenged some of Liebig's conclusions concerning animal chemistry. For example, Liebig had predicted that the ratio of uric acid to urea would depend on the level of activity of a species or individual; Bird showed this to be false. Bird also felt that it was not enough simply to count atoms as Liebig did, but that an explanation was also required as to why the atoms recombined in one particular way rather than any other. He made some attempts to provide this explanation by invoking the electric force, rather than the vital force, based on his own experiments in electrolysis.\n\nBird designed and used a flexible tube stethoscope in June 1840, and in the same year he published the first description of such an instrument. In his paper he mentions an instrument already in use by other physicians (Drs. Clendinning and Stroud), which he describes as the \"snake ear trumpet\". He thought this instrument had some severe technical faults; in particular, its great length led to poor performance. The form of Bird's invention is similar to the modern stethoscope, except that it has only one earpiece. An ill-tempered exchange of letters occurred in the \"London Medical Gazette\" between another physician, John Burne, and Bird. Burne claimed that he also used the same instrument as Clendinning and Stroud and was offended that Bird had not mentioned him in his paper. Burne, who worked at the Westminster Hospital, pointed with suspicion to the fact that Bird's brother Frederic also worked there. In a reply full of anger and sarcasm, Bird pointed out that in his original paper he had already made clear that he claimed no credit for the earlier instrument. Bird found the flexible stethoscope convenient as it avoided uncomfortably leaning over patients (as would be required by a rigid stethoscope) and the earpiece could be passed to other doctors and students to listen. It was particularly useful for Bird, with his severe rheumatism, as he could apply the stethoscope to the patient from a seated position.\n\nWhen Bird took up lecturing on science at Guy's, he could not find a textbook suitable for his medical students. He needed a book that went into some detail of physics and chemistry, but which medical students would not find overwhelmingly mathematical. Bird reluctantly undertook to write such a book himself, based on his 1837–1838 lectures, and the result was \"Elements of Natural Philosophy\", first published in 1839. It proved to be spectacularly popular, even beyond its intended audience of medical students, and went through six editions. Reprints were still being produced more than 30 years later in 1868. The fourth edition was edited by Charles Brooke, a friend of Bird's, after the latter's death. Brooke made good many of Bird's mathematical omissions. Brooke edited further editions and, in the sixth edition of 1867, thoroughly updated it.\n\nThe book was well received and was praised by reviewers for its clarity. The \"Literary Gazette\", for instance, thought that it \"teaches us the elements of the entire circle of natural philosophy in the clearest and most perspicuous manner\". The reviewer recommended it as suitable not just for students and not just for the young, saying that it \"ought to be in the hands of every individual who desires to taste the pleasures of divine philosophy, and obtain a competent knowledge of that creation in which they live\".\n\nMedical journals, on the other hand, were more restrained in their praise. The \"Provincial Medical and Surgical\", for instance, in its review of the second edition, thought that it was \"a good and concise elementary treatise ... presenting in a readable and intelligible form, a great mass of information not to be found in any other single treatise\". But the \"Provincial\" had a few technical quibbles, among which was the complaint that there was no description of the construction of a stethoscope. The \"Provincial\" reviewer thought that the book was particularly suitable for students who had no previous instruction in physics. The sections on magnetism, electricity and light were particularly recommended.\n\nIn their review of the 6th edition, \"Popular Science Review\" noted that the author was now named as Brooke and observed that he had now made the book his own. The reviewers looked back with nostalgia to the book they knew as \"the Golding Bird\" when they were students. They note with approval the many newly included descriptions of the latest technology, such as the dynamos of Henry Wilde and Werner von Siemens, and the spectroscope of Browning.\n\nThe scope of the book was wide-ranging, covering much of the physics then known. The 1839 first edition included statics, dynamics, gravitation, mechanics, hydrostatics, pneumatics, hydrodynamics, acoustics, magnetism, electricity, atmospheric electricity, electrodynamics, thermoelectricity, bioelectricity, light, optics, and polarized light. In the 1843 second edition Bird expanded the material on electrolysis into its own chapter, reworked the polarized light material, added two chapters on \"thermotics\" (thermodynamics – a major omission from the first edition), and a chapter on the new technology of photography. Later editions also included a chapter on electric telegraphy. Brooke was still expanding the book for the sixth and final edition. New material included the magnetic properties of iron in ships and spectrum analysis.\n\nBird was a committed Christian throughout his life. Despite his extremely busy professional life, he meticulously observed the Sabbath and saw to the Christian education of his children. He showed generosity to the poor, offering them treatment at his house every morning before going about his professional schedule. After it became clear that the remainder of his life was going to be very limited, he devoted much time to his religion. He wanted to promote Christian teachings and Bible reading among medical students. From 1853 Bird organised a series of religious meetings of medical professionals in London, aiming to encourage physicians and surgeons to exert a religious influence over their students.\n\nFor several years prior to 1853, student prayer meetings had been held in some of the London hospitals, particularly St Thomas'. Bird aimed to mould this movement into a formal association, an ambition which was to crystallise as the Christian Medical Association. He was heavily influenced in this by the Medical Missionary Society of John Hutton Balfour at Edinburgh University. Bird aimed to form a national body with a chapter in each teaching hospital; a prototype student group was already in existence at Guy's. He was strongly opposed by some sections of the medical profession, who felt that students should concentrate on their studies. Among the insults levelled at Bird were \"saponaceous piety\" and being a Mawworm. This opposition continued after the formation of the Association. The constitution of the new Christian Medical Association was agreed at Bird's home on 17 December 1853 in a meeting of medical and surgical teachers and others. It was based on a draft prepared by the Guy's student group. Bird died before the inaugural public meeting of the Association in November 1854 at Exeter Hall.\n\nBird was quick to defend the virtuousness of students. In November 1853, in a reply to a letter from a student in the \"Provincial Medical and Surgical Journal\" complaining of a lack of moral care from his superiors, Bird attacked the prevalent public view that students were \"guilty of every kind of open vice and moral depravity\". Bird laid much of the blame for this public opinion on the caricatures of students in the writings of Charles Dickens. He went on to say that the behaviour and character of students had greatly improved over the preceding ten years. He attributed this improvement in part to the greatly increased study requirements imposed on students, but also in part to Christian influences acting on them. He also commented that pious students had once been ridiculed, but were now respected.\n\n\n\nBird was frequently mentioned in the transactions of the Medical Society of London. Some examples are:\n\n\n"}
{"id": "33784859", "url": "https://en.wikipedia.org/wiki?curid=33784859", "title": "Green guides", "text": "Green guides\n\nA green guide (or sustainability guide) is a set of rules and guidelines provided for the use of a general or selective population to achieve the goal of becoming more green or sustainable. In this, the guide serves to direct individuals, agencies, companies, businesses, etc. to resources that can help them become more sustainable (or ‘green’) as sustainability becomes a more popular and growing lifestyle choice. Guides are available in many ways, but the most popular being through websites as to avoid using paper. There has also been a surge of guides in university websites as to encourage students towards a more sustainable way of life.\n\nThe original “green guides” were created by the Federal Trade Commission. The Federal Trade Commission’s slogan is “protecting America’s consumers,” so naturally they created these “green guides” to “to help marketers avoid making environmental claims that are unfair or deceptive under Section 5 of the Federal Trade Commission Act”. In order to guide consumers away from dishonest environmental claims the Federal Trade Commission issued two brochures called “Eco-Speak: A User’s Guide to the Language of Recycling” and “Sorting out ‘Green’ Advertising Claims”. Not only does the Federal Trade Commission guide the consumer, but it also issues a brochure for businesses called “Complying With the Environmental Marketing Guides”. This guide provides businesses with the complete guides and a review of green marketing claims. The Federal Trade Commission issued their first guide in 1992 and since then has updated them in 1996 and 1998. Most recently the Federal Trade Commission has proposed a revision of their guides in October 2010. Due to the proliferation of products claiming to be green in the marketplace in recent years the Federal Trade Commission began this third revision process early as the Commission held several workshop meetings open to the public to discuss green marketing issues. These workshops covered carbon offsets, “green” product packaging, building products and textiles claiming to be green. Through this review process the Federal Trade Commission is able to gain feedback from the public as well as perform a cost-benefit analysis, determine the efficacy of their guides, and decide whether to maintain, modify, or discard the current set of guides and rules.\n\nEven before the Federal Trade Commission created its first green guide in 1992 there were plenty of people interested in sustainability and the environment who wanted some sort of guidance when it came to living and purchasing more eco friendly. Especially in recent years with the growing environmental movement, a myriad of organizations and individuals have released their own guides to living sustainably or in other words their own “green guides.” For example, National Geographic a magazine company whose slogan is “inspiring people to care about the planet since 1888” first launched its National Geographic Green Guide in 2003 . This magazine guide gave readers tips and examples on how to live a more sustainable and “greener” life. National Geographic discontinued the print version of their guide in January 2009, but it continues to run the guide on their webpage. There readers can read over guides on living more sustainably with their home and garden, travel and transport, food, and purchases. Also National Geographic provides readers with recent environmental news, “green living hot topics,” and interactive quizzes to determine how sustainably their living and what they can do to change\nEvery day more and more people and organizations keep generating new ideas and novel innovations on how to live more sustainable lives. These fresh ideas act themselves as green guides as long as their information is spread, either by print or on the internet. For example, back in November 2007 Rebecca Kelley and Joy Hatch were just two friends who happened to be pregnant at the same time, but by sharing their ideals and interests of raising their children sustainably they created a blog called “The Green Baby Guide”. This blog created a guide for mothers interested in how they could make their child rearing process more environmentally friendly and sustainable. Eventually their community of interested mothers grew so large that they came out with a book in March 2010 called \"The Eco-nomical Baby Guide: Down-to-Earth Ways for Parents to Save Money and the Planet\". With more people wanting to become enlightened about sustainable living every day there is plenty of room for additional green guides to be produced causing the history of green guides to be changing and evolving all the time.\n\nThe Federal Trade Commission green guide is a general guide made by the United States government. It uses examples of everyday ‘green items’. The articles are quite broad, and can be applied to almost every ‘green’ consumer product. The FTC issued its Green Guides, to help marketers avoid making environmental claims that are unfair or deceptive under Section 5 of the FTC Act. The Green Guides outline general principles that apply to all environmental marketing claims and then provide guidance on specific green claims, such as biodegradable, compostable, recyclable, recycled content, and ozone safe. The FTC issued the Guides in 1992, and updated them in 1996 and 1998. Currently, the FTC is making revisions to the guides to keep up with the times.\n\nThe National Geographic website gives a variety of resources aimed primarily at individuals in\ntheir easy to comprehend green guide. Here, the website focuses on ways everyday people can help towards a sustainable world as well as changes they can make to live that world as well. With categories involving home and garden, travel, transportation, and food, National Geographic encompasses the areas people have the most control over in their life.\n\nUnder the home and garden, the site provides simple yet shocking facts about certain products and practices that one might not even think about in one’s everyday home life. Also, the site provides alternatives to these practices as well as tips to make both your home and your garden sustainable.\n\nThe travel and transportation section also provides other tips as to how one would green up their traveling as well as provides destinations for travel. In this category, National Geographic, as per its name, also has some information on geotourism. Defined as \"tourism that sustains or enhances the\ngeographical character of place,\" this section gives information about places that one might decide to go on for travel.\n\nWith food, the site gives valuable information about food and the impact to the environment that it could have; moreover, the site educates readers as to up to date advances in technology that could prove more sustainable than past technologies when it comes to cultivation of food. Finally, there is a section on certain recipes that provides information to certain aspects of food preparation, as well as food itself, that proves harmful towards sustainability and the environment in general.\n\nAdding to that, the website also provides a ‘buying guides’ option that educates people in the purchases they make and how that could affect the environment as well as their wallets. Under this option, the website provides tips and services as to products that everyday people might want to purchase. These tips encompass things like cost, what kinds of regulations one would have to understand for the product, pertinent information regarding the kind of product, as well as how the product itself and the kind you decide to buy impact the environment.\n\n“The Green Guide” is a version of a sustainable living guide that was created in the UK in 1997. This guide consists of an online database as well as a published print version. Not only is “the Green Guide” a guide to sustainable living, but it is also a directory that contains information about “products, services and organisations that help promote and encourage a sustainable lifestyle”. So not only does this guide provide useful advice and tips, but it also contains lists of organizations and companies that can further inform and assist readers in their quests to live more sustainably. Since this guide is produced in the United Kingdom the vast majority of the listings are of companies located in the United Kingdom, but it also contains over 350 international listings as well as information and tips that are applicable almost anywhere in the world.\n\n“The Green Guide” is broken down into twelve chapters each dealing with different themes spanning a wide variety of lifestyles. The twelve different chapters are as listed: 1. Food and Farming 2. Fashion and Beauty 3. Building, Home and Garden 4. Renewable Energy and Recycling 5. Health and Wellbeing 6. Children, Family, Community and Gifts 7. Transport and Travel 8. Leisure, Activities and Holidays 9. Money, Sustainable Business and CSR 10. Government, Campaigning and Change 11. Media, Arts, Events and Awards 12. Centres, Research, Education and Careers. Each one of these chapters is then further broken down into sections and subsections where the writers found appropriate. In total the guide consists of 994 different sections and subsections all pertaining to different strategies, tips and information that can help readers live a more sustainable life.\n\nBy June 14, 2010, the directory portion of “the Green Guide” had over 15,000 entries, however, only just over 10,000 are available to view online. This is because not all of the potential entries are accepted by “the Green Guide” due to the fact that they do not match the standards the guide has set. One of the major problems organizations like “the Green Guide” face is the issue of green washing. Green washing is the issue of companies attempting display themselves as environmentally friendly or green just to gain customers when in fact they are not at all green. This can be an enormous concern as companies like “the Green Guide” try their best to create guides full of truly sustainable companies. For this reason “the Green Guide” is always asking for feedback and any information that can help them enhance their guide.\n\nIn addition to published green guides, there are many grassroots green guides for the average, everyday consumer. These grassroots guides cover anything a consumer cannot find in any published green guides, such as green guides to weddings or even babies. The importance of amateur writers to create green guides is cooperative to the green guide movement itself as its sole purpose it to provide information for consumers.\nGrassroots green guides can be written by anyone with any sort of knowledge in the subject they are writing about. Green Guides are sparse, so any help to further the movement is encouraged. It is the hope of the individuals who create these green guides to have available, over the internet, a publicly view able 'encyclopedia' or green guides based on the public's needs and desires.\n\nAccording to the Princeton Review, these\nsixteen Universities in the United States have gained the Green Honor Roll based on the criteria listed in\nthe Princeton Review website:\n\npreferable food\nstudents\nsustainability on campus\naction plan consistent with 80% greenhouse gas reduction by 2050\n\nThe following list of schools have received a Green rating of 99, the highest grade possible given\nby the Princeton Review.\n\n\n"}
{"id": "1700782", "url": "https://en.wikipedia.org/wiki?curid=1700782", "title": "HVDC Three Gorges – Guangdong", "text": "HVDC Three Gorges – Guangdong\n\nThe HVDC Three Gorges – Guangdong is a 940 kilometre-long bipolar HVDC transmission line in China for the transmission of electric power from the Three Gorges power plant to the area of Guangdong. The powerline went into service in 2004. It runs from the static inverter station Jingzhou near the Three Gorges power plant to the static inverter plant Huizhou near Guangdong. The HVDC Three Gorges-Guangdong is a bipolar 500 kV powerline with a maximum transmission power rating of 3,000 megawatts.\n\n"}
{"id": "1914606", "url": "https://en.wikipedia.org/wiki?curid=1914606", "title": "Headwind and tailwind", "text": "Headwind and tailwind\n\nA tailwind is a wind that blows in the direction of travel of an object, while a headwind blows against the direction of travel. A tailwind increases the object's speed and reduces the time required to reach its destination, while a headwind has the opposite effect. \n\nIn aeronautics, a headwind is favorable in takeoffs and landings because an airfoil moving into a headwind is capable of generating greater lift than the same airfoil moving through tranquil air, or with a tailwind, at equal ground speed. As a result, aviators and air traffic controllers commonly choose to take off or land in the direction of a runway that will provide a headwind.\n\nIn sailing, a headwind may make forward movement difficult, and necessitate tacking into the wind.\n\nTailwinds and headwinds are commonly measured in relation to the speed of vehicles — commonly air and watercraft — as well as in running events — particularly sprints.\n\nPilots calculate the Headwind or Tailwind Component and the Crosswind Component of local wind before takeoff. The direction of wind at a runway is measured using a windsock and the speed by an anemometer, often mounted on the same post. Headwind and Tailwind are opposite interpretations of the wind component which is parallel to the direction of travel, while Crosswind represents the perpendicular component. Determining the ground speed of an aircraft requires the calculation of the head or tailwind.\n\nAssume:\n\nformula_1\n\nformula_2\n\nformula_3\n\nformula_4\n\nformula_5\n\nThen\n\nformula_6\n\nformula_7\n\nFor example, if the wind is at 09015 that means the wind is currently from heading 090 degrees with a speed of 15 Knots and the aircraft is taking-off from runway 24; having heading of 240. The pilot prefers the runway side with less than 90 difference from wind direction, in this case Runway 06; heading 060. Here, formula_8.\n\nformula_9\n\nformula_10\n\nThe aircraft is said to have 7.5 knots of crosswind and 13 knots of headwind on runway 06, or 13 knots of tailwind on runway 24.\n\nAircraft usually have maximum tailwind and crosswind components which they cannot exceed. If the wind is at eighty degrees or above it is said to be full-cross. If the wind exceeds 100 degrees it is common practice to takeoff and land from the opposite end of the runway, it has a heading of 060 in the above-mentioned example.\n\n"}
{"id": "614302", "url": "https://en.wikipedia.org/wiki?curid=614302", "title": "Hydrocarbon exploration", "text": "Hydrocarbon exploration\n\nHydrocarbon exploration (or oil and gas exploration) is the search by petroleum geologists and geophysicists for hydrocarbon deposits beneath the Earth's surface, such as oil and natural gas. Oil and gas exploration are grouped under the science of petroleum geology.\n\nVisible surface features such as oil seeps, natural gas seeps, pockmarks (underwater craters caused by escaping gas) provide basic evidence of hydrocarbon generation (be it shallow or deep in the Earth). However, most exploration depends on highly sophisticated technology to detect and determine the extent of these deposits using exploration geophysics. Areas thought to contain hydrocarbons are initially subjected to a gravity survey, magnetic survey, passive seismic or regional seismic reflection surveys to detect large-scale features of the sub-surface geology. Features of interest (known as \"leads\") are subjected to more detailed seismic surveys which work on the principle of the time it takes for reflected sound waves to travel through matter (rock) of varying densities and using the process of depth conversion to create a profile of the substructure. Finally, when a prospect has been identified and evaluated and passes the oil company's selection criteria, an exploration well is drilled in an attempt to conclusively determine the presence or absence of oil or gas.Offshore the risk can be reduced by using electromagnetic methods \n\nOil exploration is an expensive, high-risk operation. Offshore and remote area exploration is generally only undertaken by very large corporations or national governments. Typical shallow shelf oil wells (e.g. North Sea) cost US$10 – 30 million, while deep water wells can cost up to US$100 million plus. Hundreds of smaller companies search for onshore hydrocarbon deposits worldwide, with some wells costing as little as US$100,000.\n\nA prospect is a potential trap which geologists believe may contain hydrocarbons. A significant amount of geological, structural and seismic investigation must first be completed to redefine the potential hydrocarbon drill location from a lead to a prospect. Four geological factors have to be present for a prospect to work and if any of them fail neither oil nor gas will be present.\n\nHydrocarbon exploration is a high risk investment and risk assessment is paramount for successful project portfolio management. Exploration risk is a difficult concept and is usually defined by assigning confidence to the presence of the imperative geological factors, as discussed above. This confidence is based on data and/or models and is usually mapped on Common Risk Segment Maps (CRS Maps). High confidence in the presence of imperative geological factors is usually coloured green and low confidence coloured red. Therefore, these maps are also called Traffic Light Maps, while the full procedure is often referred to as Play Fairway Analysis. The aim of such procedures is to force the geologist to objectively assess all different geological factors. Furthermore, it results in simple maps that can be understood by non-geologists and managers to base exploration decisions on.\n\n\nPetroleum resources are typically owned by the government of the host country. In the USA most onshore (land) oil and gas rights (OGM) are owned by private individuals, in which case oil companies must negotiate terms for a lease of these rights with the individual who owns the OGM. Sometimes this is not the same person who owns the land surface. In most nations the government issues licences to explore, develop and produce its oil and gas resources, which are typically administered by the oil ministry. There are several different types of licence. Oil companies often operate in joint ventures to spread the risk; one of the companies in the partnership is designated the operator who actually supervises the work.\n\n\nResources are hydrocarbons which may or may not be produced in the future. A resource number may be assigned to an undrilled prospect or an unappraised discovery. Appraisal by drilling additional delineation wells or acquiring extra seismic data will confirm the size of the field and lead to project sanction. At this point the relevant government body gives the oil company a production licence which enables the field to be developed. This is also the point at which oil reserves and gas reserves can be formally booked.\n\nOil and gas reserves are defined as volumes that will be commercially recovered in the future. Reserves are separated into three categories: proved, probable, and possible. To be included in any reserves category, all commercial aspects must have been addressed, which includes government consent. Technical issues alone separate proved from unproved categories. All reserve estimates involve some degree of uncertainty.\n\nProved oil and gas reserves are those quantities of oil and gas, which, by analysis of geoscience and engineering data, can be estimated with reasonable certainty to be economically producible—from a given date forward, from known reservoirs, and under existing economic conditions, operating methods, and government regulations—prior to the time at which contracts providing the right to operate expire, unless evidence indicates that renewal is reasonably certain, regardless of whether deterministic or probabilistic methods are used for the estimation. The project to extract the hydrocarbons must have commenced or the operator must be reasonably certain that it will commence the project within a reasonable time.\n\nThe term 1P is frequently used to denote proved reserves; 2P is the sum of proved and probable reserves; and 3P the sum of proved, probable, and possible reserves. The best estimate of recovery from committed projects is generally considered to be the 2P sum of proved and probable reserves. Note that these volumes only refer to currently justified projects or those projects already in development.\n\nOil and gas reserves are the main asset of an oil company. Booking is the process by which they are added to the balance sheet.\n\nIn the United States, booking is done according to a set of rules developed by the Society of Petroleum Engineers (SPE). The reserves of any company listed on the New York Stock Exchange have to be stated to the U.S. Securities and Exchange Commission. Reported reserves may be audited by outside geologists, although this is not a legal requirement.\n\nIn Russia, companies report their reserves to the State Commission on Mineral Reserves (GKZ).\n\n"}
{"id": "24184", "url": "https://en.wikipedia.org/wiki?curid=24184", "title": "Infant baptism", "text": "Infant baptism\n\nInfant baptism is the practice of baptising infants or young children. In theological discussions, the practice is sometimes referred to as paedobaptism, or pedobaptism, from the Greek \"pais\" meaning \"child\". This can be contrasted with what is called \"believer's baptism\", or credobaptism, from the Latin word \"credo\" meaning \"I believe\", which is the religious practice of baptising only individuals who personally confess faith in Jesus, therefore excluding underage children. Opposition to infant baptism is termed catabaptism. Infant baptism is also called \"christening\" by some faith traditions.\n\nMost Christians belong to denominations that practice infant baptism. Denominational families that practice infant baptism include Catholics, Eastern and Oriental Orthodox, Anglicans, Lutherans, Presbyterians, Congregationalists and other Reformed denominations, Methodists and some Nazarenes, and the Moravian Church.\n\nThe exact details of the baptismal ceremony vary among Christian denominations. Many follow a prepared ceremony, called a rite or liturgy. In a typical ceremony, parents or godparents bring their child to their congregation's priest or minister. The rite used would be the same as that denomination's rite for adults, i.e., by pouring holy water (affusion) or by sprinkling water (aspersion). Eastern Orthodox and Eastern Catholic traditions practise total immersion and baptise babies in a font, and this practice is also the first method listed in the baptismal ritual of the Roman Catholic, although pouring is the standard practice within the Latin branch of Catholicism. Catholic and Orthodox churches that do this do not sprinkle. At the moment of baptism, the minister utters the words \"I baptise you (or, 'The servant of God (name) is baptised') in the name of the Father, and of the Son, and of the Holy Spirit\" (see Matthew 28:19).\n\nAlthough it is not required, many parents and godparents choose to dress the baby in a white gown called a christening gown for the baptism ceremony. Christening gowns often become treasured keepsakes that are used by many other children in the family and handed down from generation to generation. Traditionally, this gown is white or slightly off white and made with much lace, trim and intricate detail. In the past, a gown was used for both boys and girls; in the present day it has become more common to dress children in a baptismal outfit. Also normally made of white fabric, the outfit consists of a romper with a vest or other accessories.\nThese clothes are often kept as a memento after the ceremony.\n\nIt is a naval tradition to baptise children using the ship's bell as a baptismal font and to engrave the child's name on the bell afterwards. Tracking down and searching for an individual's name on a specific bell from a ship may be a difficult and time-consuming task. Christening information from the bells held by the Canadian Forces Base Esquimalt Museum has been entered into a searchable data archive that is accessible to any interested web site visitors.\n\nScholars disagree on the date when infant baptism was first practiced. Some believe that 1st-century Christians did not practice it, noting the lack of any explicit evidence of paedobaptism. Others, noting the lack of any explicit evidence of exclusion of paedobaptism, believe that they did, understanding biblical references to individuals \"and [her] household\" being baptised (, , ) as well as \"the promise to you and your children\" () as including young children.\n\nThe earliest extra-biblical directions for baptism, which occur in the Didache (c. 100), are taken to be about baptism of adults, since they require fasting by the person to be baptised. However, inscriptions dating back to the 2nd century which refer to young children as \"children of God\" may indicate that Christians customarily baptised infants too. The earliest reference to infant baptism was by Irenaeus (c. 130–202) in his work \"Against Heresies\". Due to its reference to Eleutherus as the current bishop of Rome, the work is usually dated . Irenaeus speaks of children being \"born again to God.\" This reference has been described as \"obscure.\" Three passages by Origen (185–c. 254) mention infant baptism as traditional and customary. While Tertullian writing c. 198–203 advises the postponement of baptism of little children and the unmarried, he mentions that it was customary to baptise infants, with sponsors speaking on their behalf. The Apostolic Tradition, sometimes attributed to Hippolytus of Rome (died 235), describes how to perform the ceremony of baptism; it states that children were baptised first, and if any of them could not answer for themselves, their parents or someone else from their family was to answer for them.\n\nFrom at least the 3rd century onward Christians baptised infants as standard practice, although some preferred to postpone baptism until late in life, so as to ensure forgiveness for all their preceding sins.\n\nBased on their understanding of New Testament passages such as Colossians 2:11–12, paedobaptists believe that infant baptism is the New Testament counterpart to circumcision. In the Old Testament, all male converts to Judaism, male infants born to Jewish parents, and male servants were circumcised as ceremony of initiation into the Jewish community. Paedobaptists believe that baptism has replaced Old Testament circumcision and is the religious ceremony of initiation into the Christian community.\n\nDuring the medieval and Reformation eras, infant baptism was seen as a way to incorporate newborn babies into the secular community as well as inducting them into the Christian faith.\n\nDifferent Christian denominations who practice infant baptism attach different meanings to the sacrament and explain its efficacy in different ways.\n\nThe Roman Catholic Church considers baptism, even for an infant, so important that \"parents are obliged to see that their infants are baptised within the first few weeks\" and, \"if the infant is in danger of death, it is to be baptised without any delay.\" It declares: \"The practice of infant Baptism is an immemorial tradition of the Church. There is explicit testimony to this practice from the second century on, and it is quite possible that, from the beginning of the apostolic preaching, when whole 'households' received baptism, infants may also have been baptised\". It notes that \"when the first direct evidence of infant Baptism appears in the second century, it is never presented as an innovation\", that 2nd-century Irenaeus treated baptism of infants as a matter of course, and that, \"at a Synod of African Bishops, St. Cyprian stated that 'God's mercy and grace should not be refused to anyone born', and the Synod, recalling that 'all human beings' are 'equal', whatever be 'their size or age', declared it lawful to baptize children 'by the second or third day after their birth'\". In the 17th and 18th centuries, many infants were baptised on the day of their birth as in the cases of Francoise-Athenais, Marquise de Montespan, Jeanne Du Barry and Marie Anne de Cupis de Camargo. Infant baptism is seen as showing very clearly that salvation is an unmerited favour from God, not the fruit of human effort. \"Born with a fallen human nature and tainted by original sin, children also have need of the new birth in Baptism to be freed from the power of darkness and brought into the realm of the freedom of the children of God, to which all men are called . . . The Church and the parents would deny a child the priceless grace of becoming a child of God were they not to confer Baptism shortly after birth\".\n\nThe Church has no official teaching regarding the fate of infants who die without baptism, and theologians of the Church hold various views (for instance, some have asserted that they go to Limbo, which has never been official Catholic doctrine). \"The Church entrusts these infants to the mercy of God.\"\n\nThe Congregation for the Doctrine of the Faith issued on 20 October 1980 an instruction on infant baptism, whose purpose was \"to recall the principal points of doctrine in this field which justify the Church's constant practice down the centuries and demonstrate its permanent value in spite of the difficulties raised today\". The document then indicated some general guidelines for pastoral action.\n\nThe document recalled that infant baptism has long been considered of apostolic origin and that the first direct evidence of its practice, dating from the 2nd century, does not present it as an innovation. It then responded to objections that baptism should follow faith, that the person baptised should consciously receive the grace of the sacrament, that the person should freely accept baptism, that infant baptism is unsuitable in a society marked by instability of values and conflicts of ideas, and that the practice is inimical to a missionary outlook on the part of the Church.\n\nThe instruction then gave guidelines for pastoral practice, based on two principles. The major principle is that baptism, as the sign and means of God's love that precedes any action on our part and that frees from original sin and communicates divine life, must not be delayed. The subordinate principle is that assurances must be given that the gift thus granted can grow by authentic education in the faith and Christian life. If these assurances are not really serious, there can be grounds for delaying baptism. If they are certainly absent, the sacrament should even be refused.\n\nAccordingly, the rules for involvement on the part of practising Christian parents must be supplemented with other considerations in the case of \"families with little faith or non-Christian families\". If these request that a child of theirs be baptised, there must be assurances that the child will be given the benefit of the Christian upbringing required by the sacrament. Examples of such assurances are \"the choice of godparents who will take sincere care of the child, or the support of the community\". If there is satisfactory assurance, i.e., \"any pledge giving a well-founded hope for the Christian upbringing of the children\", then \"the priest cannot refuse to celebrate the sacrament without delay, as in the case of children of Christian families\". If there is insufficient assurance, \"it will be prudent to delay baptism\", while keeping contact with the parents in the hope of securing the required conditions for celebrating the sacrament. As a last resort, enrollment of the child in a course of catechetical instruction on reaching school age can be offered in lieu of immediate celebration of baptism.\n\nThe Catechism of the Catholic Church states: \"Since Baptism signifies liberation from sin and from its instigator the devil, one or more exorcisms are pronounced over the candidate\". In the Roman Rite, the wording of the prayer of exorcism is: \"Almighty and ever-living God, you sent your only Son into the world to cast out the power of Satan, spirit of evil, to rescue man from the kingdom of darkness and bring him into the splendour of your kingdom of light. We pray for this child: set him (her) free from original sin, make him (her) a temple of your glory, and send your Holy Spirit to dwell with him (her). Through Christ our Lord.\"\n\nThe Eastern Orthodox Church, Oriental Orthodoxy and the Assyrian Church of the East also insist on the need to have infants baptised as soon as is practicable after birth. Similar to the Roman Catholic Church, they teach that baptism is not merely a symbol but actually conveys grace. Baptism is a sacrament because it is an \"instrument\" instituted by Jesus Christ to impart grace to its recipients. Infants are traditionally baptised on the eighth day, recalling the biblical injunction to circumcise on the eighth day. However, this is not mandatory. In many of these churches, the Sacred Mystery of Chrismation (Confirmation) is administered by the priest immediately after baptism. Holy Communion, in the form of consecrated wine and bread, is also given to infants after they are baptised.\n\nLutherans practice infant baptism because they believe that God mandates it through the instruction of Jesus Christ, \"Go and \"make disciples of all nations\", baptising them in the name of the Father and of the Son and of the Holy Spirit ()\", in which Jesus does not set any age limit:\n\nThey also cite other biblical passages such as , , and in support of their position. For example, in the \"Acts of the Apostles\" Saint Peter's teachings on Pentecost included children in the promise of Baptism, \"Repent and be baptized, every one of you, in the name of Jesus Christ for the forgiveness of your sins. And you will receive the gift of the Holy Spirit. The promise is for \"you and your children\"\" \n\nFor them baptism is a \"means of grace\" through which God creates and strengthens \"saving faith\" as the \"washing of regeneration\" () in which people are reborn (John 3:3–7): \"baptismal regeneration\". Since the creation of faith is exclusively God's work, it does not depend on the actions of the one baptised, whether infant or adult. Even though baptised infants cannot articulate that faith, Lutherans believe that it is present all the same. Because it is faith alone that receives these divine gifts, Lutherans confess that baptism \"works forgiveness of sins, delivers from death and the devil, and gives eternal salvation to all who believe this, as the words and promises of God declare\". In the special section on infant baptism in his Large Catechism Luther argues that infant baptism is God-pleasing because persons so baptised were reborn and sanctified by the Holy Spirit.\n\nMethodists contend that infant baptism has spiritual value for the infant. John Wesley, the founder of Methodism, held that baptism is a means of grace, but it was symbolic. Methodists view baptism in water as symbolic and believe that it does not regenerate the baptised nor cleanse them from sin.\n\nWesley's own views of infant baptism shifted over time as he put more emphasis on salvation by faith and new birth by faith alone. This has fueled much debate within Methodism over the purpose of infant baptism, though most agree it should be continued. Wesley and the Methodists would agree with the Reformed or Presbyterian denominations that infant baptism is symbolic.\n\nInfant baptism is particularly illustrative of the Methodist doctrine of prevenient grace. The principle is that The Fall of Man ruined the human soul to such an extent that nobody \"wants\" a relationship with God. In order for humans to even want to be able to choose, God must empower their will (so that they may choose Christ) which he does by means of prevenient grace. Thus God takes the very first step in salvation, preceding any human effort or decision. Methodists justify infant baptism by this principle of prevenient grace, often arguing that infant baptism is God's promise or declaration to the infant that calls that infant to (eventually) believe in God's promises (God's Word) for salvation. When the individual believes in Jesus they will profess their faith before the church, often using a ritual called confirmation in which the Holy Spirit is invoked with the laying on of hands. Methodists also use infant baptism symbolically, as an illustration of God approaching the helpless. They see the ceremony additionally as a celebration of God's prevenient grace.\n\nIt should be noted that Wesley was an Anglican minister. Not all Anglicans in Wesley's time were Arminian. Augustus Toplady, John Newton, and George Whitefield were all Anglican ministers and Calvinists. They interpreted the Anglican formularies of the 39 Articles of Religion, the 1662 Book of Common Prayer, and the Second Book of the Anglican Homilies from a Calvinist perspective and would have been more in agreement with the Reformed churches and the Puritans on the issue of infant baptism. The Catechism in the 1662 Book of Common Prayer shows that baptism was an outward sign of an inward grace. Prevenient grace, according to the Calvinist Anglicans, referred to unconditional election and irresistible grace, which is necessary for conversion of the elect. Infants are to be baptised because they are children of believers who stand in surety for them until they \"come of age\" and are bound to the same requirements of repentance and faith as adults.\n\nPresbyterian, Congregational and Reformed Christians believe that baptism, whether of infants or adults, is a \"sign and seal of the covenant of grace\", and that baptism admits the party baptised into the visible church. Being a member of the visible church does not guarantee salvation; though it does provide the child with many benefits, including that of one's particular congregation consenting to assist in the raising of that child in \"the way he should go, (so that) when he is old he will not turn from it\". Elect infants (those predestined for salvation) who die in infancy are by faith considered regenerate on the basis of God's covenant promises in the covenant of grace.\n\nPresbyterian, Congregational and many Reformed Christians see infant baptism as the New Testament form of circumcision in the Jewish covenant (Joshua 24:15). Circumcision did not create faith in the 8-day-old Jewish boy. It merely marked him as a member of God's covenant people Israel. Likewise, baptism doesn't create faith; it is a sign of membership in the visible covenant community.\n\nPresbyterian, Congregational and Reformed Christians consider children of professing Christians to be members of the visible Church (the covenant community). They also consider them to be full members of the local congregation where their parents are members and members of the universal Church (the set of all true believers who make up the invisible church) unless and until they prove otherwise. Baptism is the mark of membership in the covenant of grace and in the universal church, although regeneration is not inseparably connected with baptism.\n\nThe disagreement about infant baptism is grounded in differing theological views at a more basic level. Christians disagree about infant baptism because they disagree about the nature of faith, the role of baptism, the means of salvation, the nature of grace, and the function of the sacraments. Pedobaptism and credobaptism are positions which bubble up from theological views at a more fundamental level of one's theological system.\n\nPaedobaptists do not completely agree on the reasons for baptising infants, and offer different reasons in support of the practice. Among the arguments made in support of the practice are:\n\nSome supporters of infant baptism argue that circumcision is the sign of the covenant God made with Abraham and should be received by all the members of his covenant. The children of members of Abraham's covenant are themselves members of Abraham's covenant. Christians are members of Abraham's covenant Therefore, the children of Christians are members of Abraham's covenant. Since baptism is the New Testament form of circumcision, the children of Christians should receive the sign of the covenant by being baptised.\n\nPresbyterian, Congregationalists and Reformed Christians base their case for infant baptism on Covenant theology. Covenant theology is a broad interpretative framework used to understand the Bible. Reformed Baptists are in many ways Reformed yet, as their name suggests, adhere to Believers Baptism.\n\nAccording to Covenant theology God makes two basic covenants, or agreements, with humans. The first one, the Covenant of Works is an agreement that bases man's relationship with God on human obedience and morality. The covenant was made with Adam in the Garden of Eden. Adam broke this covenant so God replaced it with a second more durable covenant—the Covenant of Grace. The Covenant of Grace is an agreement that bases man's relationship with God on God's grace and generosity. The Covenant of Works failed because it was based on human performance. The Covenant of Grace is durable because it is based on God's performance.\n\nAll the covenants that God makes with humans after the Fall, (e.g. with Noah, Abraham, Moses, and David) all extend the Covenant of Grace to its logical conclusion in Jesus Christ. In Covenant theology, however, there is a long-standing understanding that the Mosaic Covenant is also a republication of the Covenant of Works, which required obedience to receive its benefits. The underlying Covenant of Grace extends through the whole Old Testament, which suggests that two covenants are in play throughout the history of Israel. Consequently, Covenant theologians see that the Church, or the people of God, existed in the Old Testament. These are the people who placed their faith in Christ in advance, and they are saved in the same way Christians are. Not every Israelite is in the Church (or elect), many exist under the Covenant of Works and its strict unattainable requirements, but not under the Covenant of Grace.\n\nAccording to Presbyterian and Reformed Christians, this theological framework is important to the Biblical case for infant baptism because it provides a reason for thinking there is strong continuity between the Old and New Testaments. It provides a bridge linking the two Testaments together.\n\nCovenant Theologians claim that the New Testament book of Hebrews demonstrates that much of Israel's worship has been replaced by the person and work of Christ. The result is that some important forms of worship in the Old Testament have New Testament equivalents. The Passover festival, for example, was replaced by the Lord's Supper (or Eucharist).\n\nIt is across the bridge of Covenant Theology that the sign of Abraham's covenant, circumcision, walks into the New Testament. The sign of the Covenant changes its external form to reflect new spiritual realities. It was a bloody sign in the Old Testament but because Christ has shed His blood, it has been transformed into a bloodless sign, i.e. washing with water. Passover was a bloody form of Old Testament worship and also transitions into the New Testament in the bloodless form of bread and wine.\n\nCovenant theologians point out that the external sign of the covenant in the Old Testament was circumcision. Circumcision was performed upon the male children of Israelites to signify their external membership in God's people, not as a guarantee of true faith; the Old Testament records many Israelites who turned from God and were punished, showing that their hearts were not truly set on serving God. So while all male Israelites had the sign of the covenant performed on them in a once off ceremony soon after birth, such a signifier was external only and not a true indicator of whether or not they would later exhibit true faith in Yahweh.\n\nIn the New Testament, circumcision is no longer seen as mandatory for God's people. However, there is compelling evidence to suggest that the Old Testament circumcision rite has been replaced by baptism. For instance: \"In Him you were also circumcised with the circumcision made without hands, by putting off the body of the sins of the flesh, by the circumcision of Christ, buried with Him in baptism.\" (Colossians 2:11–12a)\n\nSome paedobaptists, then, think the analogy of baptism to circumcision correctly points to children, since the historic Israelite application of circumcision was to infants, not to adult converts, of which there were few. Covenant theology, then, identifies baptism less as a statement of faith than as an assumption of identity; that is to say that infant baptism is a sign of covenantal inclusion.\n\nPaedobaptists point to a number of passages in the New Testament which seem to corroborate the above argument.\n\nIn the Old Testament, if the head of a household converted to Judaism, all the males in the house, even the infants, were circumcised. Some paedobaptists argue this pattern continues into the New Testament. Reference is made, for example, to baptising a person \"and their whole household\"—the households of Lydia, Crispus, and Stephanas are mentioned by name Acts 16:14–15, 18:8; 1 Cor 1:16.\n\nPaedobaptists challenge credobaptists on this point: Why would a whole household be baptised just because the head of the house had faith? Shouldn't they baptise each member of the family as they come to individual faith? Household baptism implies that the rules for membership in Abraham's covenant have continued into the New Testament, the main difference is the sign of the covenant.\n\nCredobaptists counter with verses such as John 4:53, Acts 16:34 and Acts 18:8 in which entire households are said to have \"believed\". As such, the paedobaptist assumption is that household baptisms mentioned in the Bible involved infants, presumably incapable of personal belief.\n\nPaedobaptists also point to Psalm 51, which reads, in part, \"surely I was sinful from birth\", as indication that infants are sinful (\"vid.\" original sin) and are thus in need of forgiveness that they too might have salvation.\n\nCredobaptists agree that infants are in need of salvation, but paedobaptists push the point a step further arguing that it makes no theological sense for infants to need salvation but for God to make no provision for them to be saved (See 1 Cor 7:14 where Paul says that the children of a believer are holy—separated—and therefore, perhaps, would not need baptising even if baptism saved). Credobaptists recant that there is a provision through which God enables infants to be saved, belief on Jesus Christ (See Mark 9:42, John 3:14-21, John 11:25-26, Acts 2:21, Romans 10:1-21). Furthermore, credobaptists argue that paedobaptism is affirming believer's hereditary grace, a doctrine not found in scripture. Some credobaptists who agree to the Psalm 51 interpretation, argue that even though infants are sinful they are not accountable, because of the \"age of accountability\". Although many theologians would argue that an \"age of accountability\" is nowhere mentioned in the Bible.\n\nAn alternative viewpoint of some credobaptists is that since all Christians are predestined to salvation (John 15:16, 1 Cor.1:27, Eph.1:4, 1 Pt.2:4), God will not allow his elect to die before receiving their need, even if they are in old age (Luke 2:25–35), an argument whose relation to baptism whether of infants or adults is unclear, unless it means that infants who die without coming to explicit belief and baptism are not among God's elect.\n\nAnother Credobaptist position is called predispositionalism. This suggests that baptism is only a mature response to eternal life, and that infants generate their inner response to God's presence, i.e. those who warm to him would, if dying in infancy, be with him eternally; contra-wise those who chilled to him. This aligns to the idea of individual faith/welcome (Jhn.1:14). Its point of determinism predates the age of evangelism, which it holds is still valid in crystallising predisposition into maturing faith in this life. It considers shades of meaning in the keyword salvation.\n\nIn John 3:5, Jesus says, \"Verily, verily, I say unto thee, Except a man be born of water and of the Spirit, he cannot enter into the kingdom of God,\" which, according to some religious groups, means that an infant who dies without being baptized cannot enter heaven and may go to limbo instead.\n\nAccording to the Book of Acts, \"Peter replied, 'Repent and be baptised, \"every one of you\", in the name of Jesus Christ for the forgiveness of your sins. And you will receive the gift of the Holy Spirit. The promise is for you and your children and for all who are far off—for all whom the Lord our God will call.'\" (Acts 2:38–39, NIV–UK, emphasis added) The United Methodist Church argues that the phrase \"every one of you\" recalls the use of the same phrase in , where there is explicit mention of the \"little ones\" present; and it takes the phrase \"and your children\" to mean that Peter included children in the covenant community.\n\nCredobaptists counter that only those who believe, and children who believe are recipients of the promise. Otherwise, all children of Adam would be saved. Caleb Colley says that, also, Peter's first instruction was to repent, and since repentance requires an awareness and understanding of sin, baptizing an infant is pointless, because they are not capable of such awareness and understanding.\n\nSeveral early Church Fathers seem to have taught that infant baptism is proper; Origen states that the practice of baptising infants is of apostolic origin.. The Didache, the earliest appearance of non-biblical baptismal instruction, speaks only of adult baptism.\n\n\nSome opponents of paedobaptism point out that Jesus himself was baptised at the age of 30.\nThey also point to the two (out of five) Great Commission passages that speak of baptism. They see as giving exclusive instructions about who is to be baptised: \"Go therefore and make disciples of all nations, baptizing them in the name of the Father and of the Son and of the Holy Spirit, teaching them to observe all things that I have commanded you\" (verses 19–20, NKJV). They interpret this as referring to three successive stages, with baptism following on becoming a disciple (which is beyond the power of an infant), and instruction following on baptism, not preceding it.\n\nThe Great Commission passage speaks of believing: \"He who believes and is baptised will be saved; but he who does not believe will be condemned\" (verse 16, NKJV). This, they say, excludes infants, whom they see as incapable of believing.\nIf pedobaptists accept this text as canonical, they can still point out that the second clause mentions believing, but not baptism. Therefore, one could be baptised and still not be a believer. They argue that this may not exclude infant baptism, but rather corroborate it, since it indicates that one baptised as an infant who rejects the faith is not saved against their will. Pedobaptists who accept this passage may also point out that although belief and baptism are both mentioned, there is no order implied. In return, opposers declare that baptism is for those who already believe and are able to state their belief, which infants cannot do. In Peter's address to adults, \"Repent and be baptised\" , they see repentance as a prerequisite, and this requires a mature understanding of sin and a decision to turn away from sin. However, St. Peter was speaking to those who were already adults, not to infants. Pedobaptists claim that it would follow that his instructions are meant for adults and not for infants. Indeed, adult candidates for baptism are required by most branches that practice pedobaptism to make a confession of faith before baptism. Some point to or as evidence that each individual must make a mature decision regarding baptism. See Believer's Baptism.\n\nSome oppose baptism of children as ostensibly incorporating them into the church without their own consent. This, however does not absolve the responsibility of biblical parents to raise their children in the training and admonition of the Lord within the cultural context of the church.\n\nDenominations that do not accept infant baptism as valid generally require those who join them, after being baptised as infants elsewhere, to be \"rebaptised,\" or rather to be baptised for the first time. They deny that they in fact rebaptise, saying that Christians are to be baptised only once, but as believers, and they reject the term \"Anabaptist\" (i.e. Rebaptiser) as a description of them.\n\nTrinitarian Christian denominations that oppose infant baptism include the Assemblies of God, Association of Vineyard Churches, Christian and Missionary Alliance, Church of God (Cleveland, Tennessee), Calvary Chapel, Community Churches, Community of Christ, Elim Pentecostal Church, all Baptist denominations and including Independent Baptists, Gnostic Churches, the groups which originated out of the Restoration Movement (Churches of Christ, Christian Church (Disciples of Christ), and Christian Churches/Churches of Christ), as well as other nondenominational churches, International Churches of Christ, International Christian Church, Newfrontiers, Foursquare Gospel Church, Church of God in Christ, Church of God of Prophecy, Anabaptists (such as the Church of the Brethren, Mennonite, and Amish), Schwarzenau Brethren/German Baptists, the Seventh-day Adventist Church, some Methodists, and most Pentecostal denominations.\n\nSeveral nontrinitarian religious groups also oppose infant baptism, including Oneness Pentecostals, Christadelphians, Jehovah's Witnesses, United Church of God, and The Church of Jesus Christ of Latter-day Saints.\n\nReligious groups that oppose infant baptism have sometimes been persecuted by paedobaptist churches. During the Reformation, Anabaptists were persecuted by Lutheran, Calvinist, Anglican and Catholic regimes. The English government imposed restrictions on Baptists in Britain and Ireland during the 17th century. The Russian Orthodox Church repressed Baptists prior to the 1917 revolution, and sought restrictions on Baptists and Pentecostals after being re-established after the fall of Communism.\n\nB.R. White describes the motivations behind persecution of the Anabaptists during the Reformation as follows:\n\nOther Christians saw the baptism of each new-born baby into the secular parish community and close links between church and state as the divinely-ordained means of holding society together. Hence many other Christians saw the Anabaptists as subversive of all order. Consequently, from the earliest days, they were sharply persecuted and leaders were soon executed.\nNote: Christian Scientists, Quakers, the Salvation Army, and Unitarians cannot be classified as specifically opposing \"infant\" baptism, since they generally do not observe baptism in any form.\n\nThe Church of Jesus Christ of Latter-day Saints (LDS Church) completely rejects infant baptism. Little children are considered both born without sin and incapable of committing sin. They have no need of baptism until age eight, when they can begin to learn to discern right from wrong, and are thus accountable to God for their own actions. However, the LDS Church performs a non-saving ordinance to name and bless children, customarily performed on infants.\n\nFor Roman Catholics, Confirmation is a sacrament that \"confirms\" or \"strengthens\" (the original meaning of the word \"confirm\") the grace of Baptism, by conferring an increase and deepening of that grace.\n\nFor some other Christians the ceremony of Confirmation is a matter not of \"being confirmed\" but of \"confirming\" the baptismal vows taken on one's behalf when an infant. This is the essential significance of the Lutheran non-sacramental ceremony called in German \"Konfirmation\", but in English \"affirmation of baptism\" (see Confirmation (Lutheran Church)).\n\nIn Eastern Christianity, including the Eastern Catholic Churches, the sacrament of Confirmation is conferred immediately after baptism, and there is no renewal of baptismal promises. In the Latin-Rite (i.e. Western) Catholic Church, the sacrament is to be conferred at about the age of discretion (generally taken to be about 7), unless the Episcopal Conference has decided on a different age, or there is danger of death or, in the judgement of the minister, a grave reason suggests otherwise (canon 891 of the Code of Canon Law). The renewal of baptismal promises by those receiving the sacrament in the Western Catholic Church is incidental to the rite and not essentially different from the solemn renewal of their baptismal promises that is asked of all members of this Church each year at the Easter Vigil service. Only in French-speaking countries has there been a development of ceremonies, quite distinct from the sacrament of Confirmation, for young Catholics to profess their faith publicly, in line with their age.\n\nThe Anglican Book of Common Prayer requires that all who are to be confirmed should first know and understand the Creed, the Lord's Prayer, and the Ten Commandments, and be able to answer the other questions in the Church Catechism. Confirmation enables those who have been baptised as infants, when they are of age to do so, openly before the church, to take upon themselves and confirm the promises made on their behalf by their godparents.\n\nWithin The Church of Jesus Christ of Latter-day Saints, confirmation or \"the laying on of hands\" is an essential part of the baptismal ordinance, and to receive baptism without confirmation is to leave the ordinance incomplete. Confirmation is the conferring of the gift of the Holy Ghost as a constant companion. To confirm means to \"make more sure\" and the ordinance of confirmation stands as a witness of the individual becoming a member of the LDS Church and not just an acceptance of Jesus.\n\n\n\n"}
{"id": "19670568", "url": "https://en.wikipedia.org/wiki?curid=19670568", "title": "Jacobs Wind", "text": "Jacobs Wind\n\nJacobs Wind Electric Co. Inc. is the oldest renewable energy company in the United States. It has been designing consumer and commercial renewable energy systems sized to the changing distributed electric loads of their periods since the mid-1920s.\n\nThe firm was started and established by Marcellus and Joseph Jacobs, after local interest in their wind electric system for their family’s Montana Ranch, built in 1922, brought them requests from neighbors to provide them with wind generated electric power as well.\n\nM.L. & Joe moved the firm to Minneapolis in 1963 to begin production of improved wind/engine distributed energy systems which were sold in the U.S., Canada, and Mexico, as well as on every major continent, through a Dealer network that grew to over 300. Early Jacobs' machines included one taken to Antarctica by Richard Evelyn Byrd and installed (at Byrd's 'Little America') in 1933, running until 1955. Before production ceased in the late 1950s, about 20,000 Jacobs Wind Energy Systems (1 - 3 kW) were shipped from Minneapolis.\n\nIn the 80's via a partnership with Control Data a new line of production of Jacobs Wind Energy Systems began in Minneapolis, with marketing of larger 10 - 20 kW systems. Most of these 1,500+ wind systems produced from 1980-85 were grid connected. The majority of them went to pioneering windfarms in Hawaii and California.\n\nIn Minnesota, Jacobs units began being connected to Rural Electric Cooperative (REC) grids starting in 1981. Many of these systems are still on line to REC grids’ selling renewable wind power (AG-WATTS).\n\nToday, the firm’s designers (several now in their 4th decade in the wind industry) are working with local RECs on a new generation of consumer renewable energy systems sized to ever larger consumer electric loads at distributed rural sites.\n\n\n"}
{"id": "330095", "url": "https://en.wikipedia.org/wiki?curid=330095", "title": "Kernel (set theory)", "text": "Kernel (set theory)\n\nIn set theory, the kernel of a function \"f\" may be taken to be either\n\n\nFor the formal definition, let \"X\" and \"Y\" be sets and let \"f\" be a function from \"X\" to \"Y\".\nElements \"x\" and \"x\" of \"X\" are \"equivalent\" if \"f\"(\"x\") and \"f\"(\"x\") are equal, i.e. are the same element of \"Y\".\nThe kernel of \"f\" is the equivalence relation thus defined.\n\nLike any equivalence relation, the kernel can be modded out to form a quotient set, and the quotient set is the partition:\n\nThis quotient set /= is called the \"coimage\" of the function , and denoted (or a variation).\nThe coimage is naturally isomorphic (in the set-theoretic sense of a bijection) to the image, ; specifically, the equivalence class of in (which is an element of ) corresponds to in (which is an element of ).\n\nLike any binary relation, the kernel of a function may be thought of as a subset of the Cartesian product \"X\" × \"X\".\nIn this guise, the kernel may be denoted (or a variation) and may be defined symbolically as\n\nThe study of the properties of this subset can shed light on .\n\nIf \"X\" and \"Y\" are algebraic structures of some fixed type (such as groups, rings, or vector spaces), and if the function \"f\" from \"X\" to \"Y\" is a homomorphism, then ker \"f\" will be a subalgebra of the direct product \"X\" × \"X\". Subalgebras of \"X\" × \"X\" that are also equivalence relations (called \"congruence relations\") are important in abstract algebra, because they define the most general notion of quotient algebra. Thus the coimage of \"f\" is a quotient algebra of \"X\" much as the image of \"f\" is a subalgebra of \"Y\"; and the bijection between them becomes an isomorphism in the algebraic sense as well (this is the most general form of the first isomorphism theorem in algebra). The use of kernels in this context is discussed further in the article Kernel (algebra).\n\nIf \"X\" and \"Y\" are topological spaces and \"f\" is a continuous function between them, then the topological properties of ker \"f\" can shed light on the spaces \"X\" and \"Y\".\nFor example, if \"Y\" is a Hausdorff space, then ker \"f\" must be a closed set.\nConversely, if \"X\" is a Hausdorff space and ker \"f\" is a closed set, then the coimage of \"f\", if given the quotient space topology, must also be a Hausdorff space.\n"}
{"id": "467008", "url": "https://en.wikipedia.org/wiki?curid=467008", "title": "Kosmos 954", "text": "Kosmos 954\n\nKosmos 954 () was a reconnaissance satellite launched by the Soviet Union in 1977. A malfunction prevented safe separation of its onboard nuclear reactor; when the satellite reentered the Earth's atmosphere the following year, it scattered radioactive debris over northern Canada, prompting an extensive cleanup operation known as Operation Morning Light.\n\nThe satellite was part of the Soviet Union's RORSAT programme, a series of reconnaissance satellites which observed ocean traffic, including surface vessels and nuclear submarines, using active radar. It was assigned the Kosmos number 954 and was launched on September 18, 1977 at 13:55 UTC from the Baikonur Cosmodrome, on a Tsyklon-2 carrier rocket. With an orbital inclination of 65°, a periapsis of and apoapsis of , it orbited the Earth every 89.5 minutes. Powered by a liquid sodium–potassium thermionic converter driven by a nuclear reactor containing around of uranium-235, the satellite was intended for long-term on-orbit observation, but by December 1977 the satellite had deviated from its designed orbit and its flightpath was becoming increasingly erratic.\n\nIn mid-December North American Aerospace Defense Command, which had assigned the satellite the Satellite Catalog Number 10361, noticed Kosmos 954 making erratic manoeuvres, changing the altitude of its orbit by up to 50 miles, as its Soviet operators struggled to control their failing spacecraft. In secret meetings, Soviet officials warned their US counterparts that they had lost control over the vehicle, and that the system which was intended to propel the spent reactor core into a safe disposal orbit had failed.\n\nAt 11:53 AM GMT on January 24, 1978, Kosmos 954 reentered the Earth's atmosphere while travelling on a northeastward track over western Canada. At first the USSR claimed that the satellite had been completely destroyed during re-entry, but later searches showed debris from the satellite had been deposited on Canadian territory along a path from Great Slave Lake to Baker Lake. The area spans portions of the Northwest Territories, present-day Nunavut, Alberta and Saskatchewan.\n\nThe effort to recover radioactive material from the satellite was dubbed Operation Morning Light. Covering a total area of , the joint Canadian–American team swept the area by foot and air in Phase I from January 24, 1978 to April 20, 1978 and Phase II from April 21, 1978 to October 15, 1978. They were ultimately able to recover 12 large pieces of the satellite. 10 of the fragments recovered were radioactive. These pieces displayed radioactivity of up to 1.1 sieverts per hour, yet they only comprised an estimated 1% of the fuel. One fragment had a radiation level of 500 R/h, which \"is sufficient to kill a person ... remaining in contact with the piece for a few hours.\"\n\nUnder the terms of the 1972 Space Liability Convention, a state which launches an object into space is liable for damages caused by that object. For the recovery efforts, the Canadian government billed the Soviet Union C$6,041,174.70 for actual expenses and additional compensation for future unpredicted expenses; the USSR eventually paid the sum of C$3 million.\n\nKosmos 954 was not the first nuclear-powered RORSAT to fail; a launch of a similar satellite in 1973 failed, dropping its reactor into the Pacific Ocean north of Japan. Kosmos 1402 also failed, dropping its reactor into the South Atlantic in 1983. Subsequent RORSATs were equipped with a backup core ejection mechanism – when the primary failed on Kosmos 1900 in 1988 this system succeeded in raising the core to a safe disposal orbit.\n\nSearch teams did not find re-entry debris at the predicted location until they recalculated where that location would be based upon data indicating a stratospheric warming event had been in progress during re-entry. The stratospheric warming was first documented by the US Army Meteorological Rocket Network station at Poker Flat Research Range near Fairbanks, Alaska.\n\nKosmos 954 has become a well known piece of history and lore in Yellowknife, the capital of the Northwest Territories. Yellowknife painter Nick MacIntosh has created works of art featuring the satellite and well known local landmarks.\n\nThe January 28, 1978, episode of \"Saturday Night Live\" featured a running gag about the radioactive debris from the crashed satellite having created giant, mutant lobsters heading for the US east coast, concluding with them invading the studio at the show's end.\n\n\n"}
{"id": "1041063", "url": "https://en.wikipedia.org/wiki?curid=1041063", "title": "Kármán vortex street", "text": "Kármán vortex street\n\nIn fluid dynamics, a Kármán vortex street (or a von Kármán vortex street) is a repeating pattern of swirling vortices, caused by a process known as vortex shedding, which is responsible for the unsteady separation of flow of a fluid around blunt bodies. It is named after the engineer and fluid dynamicist Theodore von Kármán, and is responsible for such phenomena as the \"singing\" of suspended telephone or power lines and the vibration of a car antenna at certain speeds.\n\nA vortex street will only form at a certain range of flow velocities, specified by a range of Reynolds numbers (\"Re\"), typically above a limiting \"Re\" value of about 90. The (\"global\") Reynolds number for a flow is a measure of the ratio of inertial to viscous forces in the flow of a fluid around a body or in a channel, and may be defined as a nondimensional parameter of the global speed of the whole fluid flow:\n\nwhere:\n\n\nbetween:\n\n\nFor common flows (the ones which can usually be considered as incompressible or isothermal), the kinematic viscosity is everywhere uniform over all the flow field and constant in time, so there is no choice on the viscosity parameter, which becomes naturally the kinematic viscosity of the fluid being considered at the temperature being considered. On the other hand, the reference length is always an arbitrary parameter, so particular attention should be put when comparing flows around different obstacles or in channels of different shapes: the global Reynolds numbers should be referred to the same reference length. This is actually the reason for which most precise sources for airfoil and channel flow data specify the reference length at a pedix to the Reynolds number. The reference length can vary depending on the analysis to be performed: for body with circle sections such as circular cylinders or spheres, one usually chooses the diameter; for an airfoil, a generic non-circular cylinder or a bluff body or a revolution body like a fuselage or a submarine, it is usually the profile chord or the profile thickness, or some other given widths that are in fact stable design inputs; for flow channels usually the hydraulic diameter) about which the fluid is flowing.\n\nFor an aerodynamic profile the reference length depends on the analysis. In fact, the profile chord is usually chosen as the reference length also for aerodynamic coefficient for wing sections and thin profiles in which the primary target is to maximize the lift coefficient or the lift/drag ratio (i.e. as usual in thin airfoil theory, one would employ the \"chord Reynolds\" as the flow speed parameter for comparing different profiles). On the other hand, for fairings and struts the given parameter is usually the dimension of internal structure to be streamlined (let us think for simplicity it is a beam with circular section), and the main target is to minimize the drag coefficient or the drag/lift ratio. The main design parameter which becomes naturally also a reference length is therefore the profile thickness (the profile dimension or area perpendicular to the flow direction), rather than the profile chord.\n\nThe range of \"Re\" values will vary with the size and shape of the body from which the eddies are being shed, as well as with the kinematic viscosity of the fluid. Over a large \"Re\" range (47<Re<10 for circular cylinders; reference length is d: diameter of the circular cylinder) eddies are shed continuously from each side of the circle boundary, forming rows of vortices in its wake. The alternation leads to the core of a vortex in one row being opposite the point midway between two vortex cores in the other row, giving rise to the distinctive pattern shown in the picture. Ultimately, the energy of the vortices is consumed by viscosity as they move further down stream, and the regular pattern disappears.\n\nWhen a single vortex is shed, an asymmetrical flow pattern forms around the body and changes the pressure distribution. This means that the alternate shedding of vortices can create periodic lateral (sideways) forces on the body in question, causing it to vibrate. If the vortex shedding frequency is similar to the natural frequency of a body or structure, it causes resonance. It is this forced vibration that, at the correct frequency, causes suspended telephone or power lines to \"sing\" and the antenna on a car to vibrate more strongly at certain speeds.\nThe flow of atmospheric air over obstacles such as islands or isolated mountains sometimes gives birth to von Kármán vortex streets. When a cloud layer is present at the relevant altitude, the streets become visible. Such cloud layer vortex streets have been photographed from satellites.\n\nIn low turbulence, tall buildings can produce a Kármán street, so long as the structure is uniform along its height. In urban areas where there are many other tall structures nearby, the turbulence produced by these prevents the formation of coherent vortices. Periodic crosswind forces set up by vortices along object's sides can be highly undesirable, and hence it is important for engineers to account for the possible effects of vortex shedding when designing a wide range of structures, from submarine periscopes to industrial chimneys and skyscrapers.\n\nIn order to prevent the unwanted vibration of such cylindrical bodies, a longitudinal fin can be fitted on the downstream side, which, provided it is longer than the diameter of the cylinder, will prevent the eddies from interacting, and consequently they remain attached. Obviously, for a tall building or mast, the relative wind could come from any direction. For this reason, helical projections that look like large screw threads are sometimes placed at the top, which effectively create asymmetric three-dimensional flow, thereby discouraging the alternate shedding of vortices; this is also found in some car antennas. Another countermeasure with tall buildings is using variation in the diameter with height, such as tapering - that prevents the entire building being driven at the same frequency.\n\nEven more serious instability can be created in concrete cooling towers, for example, especially when built together in clusters. Vortex shedding caused the collapse of three towers at Ferrybridge Power Station C in 1965 during high winds.\n\nThe failure of the original Tacoma Narrows Bridge was originally attributed to excessive vibration due to vortex shedding, but was actually caused by aeroelastic flutter.\n\nKármán turbulence is also a problem for airplanes, especially at landing.\n\nThis formula will generally hold true for the range 40 < Re < 150:\n\nwhere:\n\n\nThis dimensionless parameter St is known as the Strouhal number and is named after the Czech physicist, Vincenc Strouhal (1850–1922) who first investigated the steady humming or singing of telegraph wires in 1878.\n\nAlthough named after Theodore von Kármán, he acknowledged that the vortex street had been studied earlier by Mallock and Bénard. In fact, Kármán tells us the story in his book Aerodynamics: \n\"...Prandtl had a doctoral candidate, Karl Hiemenz, to whom he gave the task of constructing a water channel in which he could observe the separation of the flow behind a cylinder. The object was to check experimentally the separation point calculated by means of the boundary-layer theory. For this purpose, it was first necessary to know the pressure distribution around the cylinder in a steady flow. Much to his suprise, Hiemenz found that the flow in his channel oscillates violently. When he reported this to Prandtl, the latter told him: 'Obviously your cylinder is not circular.' However, even after very careful machining of the cylinder, the flow continued to oscillate. Then Hiemenz was told that possibly the channel was not symmetric, and he started to adjust it. I was not concerned with this problem, but every morning when I came in the laboratory I asked him, 'Herr Hiemenz, is the flow steady now?' He answered very sadly, 'It always oscillates.' \"\n\n\n"}
{"id": "5289693", "url": "https://en.wikipedia.org/wiki?curid=5289693", "title": "Lagrangian and Eulerian specification of the flow field", "text": "Lagrangian and Eulerian specification of the flow field\n\nIn classical field theory the Lagrangian specification of the field is a way of looking at fluid motion where the observer follows an individual fluid parcel as it moves through space and time. Plotting the position of an individual parcel through time gives the pathline of the parcel. This can be visualized as sitting in a boat and drifting down a river.\n\nThe Eulerian specification of the flow field is a way of looking at fluid motion that focuses on specific locations in the space through which the fluid flows as time passes. This can be visualized by sitting on the bank of a river and watching the water pass the fixed location. \n\nThe Lagrangian and Eulerian specifications of the flow field are sometimes loosely denoted as the Lagrangian and Eulerian frame of reference. However, in general both the Lagrangian and Eulerian specification of the flow field can be applied in any observer's frame of reference, and in any coordinate system used within the chosen frame of reference.\n\nThese specifications are reflected in computational fluid dynamics, where \"Eulerian\" simulations employ a fixed mesh while \"Lagrangian\" ones (such as \nmeshfree simulations) feature simulation nodes that may move following the velocity field.\n\nIn the \"Eulerian specification\" of a field, it is represented as a function of position x and time \"t\". For example, the flow velocity is represented by a function \n\nOn the other hand, in the \"Lagrangian specification\", individual fluid parcels are followed through time. The fluid parcels are labelled by some (time-independent) vector field x. (Often, x is chosen to be the center of mass of the parcels at some initial time \"t\". It is chosen in this particular manner to account for the possible changes of the shape over time. Therefore the center of mass is a good parameterization of the flow velocity u of the parcel.) In the Lagrangian description, the flow is described by a function\n\ngiving the position of the particle labeled x at time \"t\".\n\nThe two specifications are related as follows:\n\nbecause both sides describe the velocity of the particle labeled x at time \"t\".\n\nWithin a chosen coordinate system, x and x are referred to as the Lagrangian coordinates and Eulerian coordinates of the flow.\n\nThe Lagrangian and Eulerian specifications of the kinematics and dynamics of the flow field are related by the material derivative (also called the Lagrangian derivative, convective derivative, substantial derivative, or particle derivative).\n\nSuppose we have a flow field u, and we are also given a generic field with Eulerian specification F(x,\"t\"). Now one might ask about the total rate of change of F experienced by a specific flow parcel. This can be computed as\n\nwhere ∇ denotes the gradient with respect to x, and the operator u⋅∇ is to be applied to each component of F. This tells us that the total rate of change of the function F as the fluid parcels moves through a flow field described by its Eulerian specification u is equal to the sum of the local rate of change and the convective rate of change of F. This is a consequence of the chain rule since we are differentiating the function F(X(x,\"t\"),\"t\") with respect to \"t\".\n\nConservation laws for a unit mass have a Lagrangian form, which together with mass conservation produce Eulerian conservation; on the contrary, when fluid particles can exchange a quantity (like energy or momentum), only Eulerian conservation laws exist.\n\n\n"}
{"id": "17803969", "url": "https://en.wikipedia.org/wiki?curid=17803969", "title": "List of Star Trek materials", "text": "List of Star Trek materials\n\nThis is a list of notable fictional materials from the science fiction universe of Star Trek. Like other aspects of stories in the franchise, some were recurring plot elements from one episode or series to another.\n\nThe fictional metals duranium and tritanium were referred to in many episodes as extremely hard alloys used in starship hulls and hand-held tools. The planet-killer in \"\" had a hull made of solid neutronium, which is capable of withstanding a starship's phasers. Neutronium is considered to be virtually indestructible; the only known way of stopping the planet-killer is to destroy it from the inside via the explosion of a starship's warp core.\n\n\"Star Trek\" technical manuals indicate that transparent aluminum is used in various fittings in starships, including exterior ship portals and windows. It was notably mentioned in the 1986 film \"\". Ultra-strong transparent panels were needed to construct water tanks within their ship's cargo bay for containing two humpback whales and hundreds of tons of water. However, the \"Enterprise\" crew, without money appropriate to the period, found it necessary to barter for the required materials. Chief Engineer Montgomery Scott exchanges the chemical formula for transparent aluminum for the needed material. When Dr. Leonard McCoy informs Scott that giving Dr. Nichols (Alex Henteloff) the formula is altering the future, the engineer responds, \"Why? How do we know he didn't \"invent\" the thing?\" (In the novelization of the film, Scott is aware that Dr. Marcus \"Mark\" Nichols, the Plexicorp scientist with whom he and McCoy deal, was its \"inventor,\" and concludes that his giving of the formula is a predestination paradox/bootstrap paradox.) The substance is described as being as transparent as glass while possessing the strength and density of high-grade aluminum. It was also mentioned in \"\" episode \"\".\n\nThe series' science consultant André Bormanis has concluded that the material would not be a good conductor of electricity.\n\n\nAn aluminum window pane, \"of glasslike transparency\" was reported from Germany in 1933.\n\nSapphire (AlO) is transparent and is widely used in commercial and industrial settings. It has a hardness of 9 Mohs, making it the third hardest mineral after diamond and moissanite.\n\nAluminum oxynitride ((AlN)·(AlO)) is a transparent ceramic which has a hardness of 7.7 Mohs, and has military applications as bullet-resistant armour, but is too expensive for widespread use. It was patented in 1986.\n\nPure transparent aluminum was created as a new state of matter by a team of scientists in 2009. A laser pulse removed an electron from every atom without disrupting the crystalline structure. However, the transparent state lasted for only 40 femtoseconds, until electrons returned to the material.\n\nA group of scientists led by Ralf Röhlsberger at Deutsches Elektronen-Synchrotron (DESY), Hamburg, Germany, succeeded in turning iron transparent during research in 2012 to create quantum computers.\n\nTrellium-D, shown in \"\", was an alloy used in the Delphic Expanse as a protection against spatial anomalies there. It had unusual effects on Vulcan physiology, and became a recurring plot element in the third season of \"\", exploring the theme of drug addiction.\n\nOther materials were occasionally mentioned in the scripts, such as , a radiation-resistant material.\n\n\nDilithium crystals, in all \"Star Trek\" series, were shown to be an essential component for a starship's faster than light drive, or warp drive, since they were necessary to regulate the matter-antimatter reactions needed to generate the required energy. Dilithium was frequently featured in as a scarce resource. By the time in which the later series were set, dilithium could be synthesized.\n\n\nReal-world dilithium (Li) is a gas composed of two lithium atoms covalently bonded together, and is a strongly electrophilic, diatomic molecule.\n\nDilithium oxide (LiO) is white cubic crystal with ionic bonds between the lithium and the oxygen. This material must be kept dry however as it will react strongly with water to form lithium hydroxide.\n\n\nTrilithium is a material used in a star-destroying weapon in \"Star Trek Generations\", and an explosive in Season 3: The Chute. This is due to the fact that Trilithium is termed as a \"nuclear inhibitor\", which is believed to be any substance that interferes with nuclear reactions. In the film, Trilithium is known to be capable, when used to its full potential, of stopping all fusion within a star, thereby collapsing the star and destroying everything within its solar system via a shock wave. Trilithium resin is a toxic byproduct of warp engines, and can be used as a powerful, and quite unstable, explosive (see \"Starship Mine\", the 18th episode of the ). It is not known whether this is related to the nuclear inhibitor.\n\nLatinum featured in many episodes of \"\" as a medium of exchange used by Ferengi and others. For convenience's sake (Jadzia Dax joked \"probably someone got tired of making change with an eyedropper\") the actual currency consisted of the latinum, which is a liquid in its natural state, enclosed in gold casings of standardized size (called slips, strips, bars, and bricks) and was referred to as \"gold-pressed latinum\". Latinum was useful as a medium of exchange, unlike the (worthless) gold in which it was enclosed, because it is impossible to replicate.\n\nTholian silk was a valuable fabric mentioned in multiple series.\n\nBio-mimetic gel is a volatile substance with medical applications. It is also highly sought after for use in illegal activities, such as genetic experimentation and biological weapons development. As such, its use is strictly regulated by the United Federation of Planets, and sale of the substance is prohibited. The substance was first mentioned in an episode of \"\", and was used as a plot element in several episodes of \"\".\n\nVerterium cortenide is a usually synthetically generated compound, the only known substance to be capable of generating warp fields, when supplied with energy, in form of plasma, from the warp core. Warp coils are made of this material.\n\nKironide is a mineral by which, upon consuming plants containing the mineral, the Platonians (the inhabitants of the planet Platonius) acquire telekinetic powers, including the ability to levitate, in the original series episode \"Plato's Stepchildren\".\n\nPergium is a substance mined in \"The Devil in the Dark\", and fictionally given the atomic number 112 as a chemical element in a non-canon \"Star Trek\" medical manual publication.\n\nCordrazine, introduced in \"The City on the Edge of Forever\" is a powerful stimulant used to revive patients in an emergency. Overdoses cause hallucinations, madness and death.\n\nVenus drug, introduced in \"Mudd's Women\", causes women to appear much lovelier and more exciting.\n\nInaprovaline, Introduced in \"Transfigurations\". Helps resuscitate the neurological and cardiovascular systems by reinforcing the cell membranes. It is also frequently used as an analgesic.\n\nKetracel-White, introduced in \"\", is a narcotic stimulant drug intravenously taken among the Jem'Hadar soldiers of The Dominion. The Jem'Hadar were created by the Founders- a shape-shifting species in the Gamma Quadrant- with a genetic predisposition for addiction to the drug. This was done to ensure their loyalty to the Founders. The drug is synthetically manufactured and refined at guarded facilities throughout Dominion space. Ketracel-White is stored as a liquid in glass vials locked in portable cases held by Vorta field supervisors. A Vorta must dispense the drug among the unit he/she commands at regular hourly intervals, otherwise the Jem'Hadar will suffer withdrawal leading to death. A vial of White is inserted into a dispensing mechanism embedded in the soldier's chest armor, and automatically pumped through a tube inserted into the common carotid artery.\n\nRetinax-5, introduced in \"\", a drug that corrects vision problems.\n\n is a key component of the Genesis Device prototype—an experimental terraformation device introduced in \".\" Protomatter is presented as an unstable substance that, due to its instability, is considered unethical for usage in scientific research. The substance is used as a plot device to compare David Marcus with his father, James T. Kirk, both of whom, in Lieutenant Saavik's words, \"changed the rules\"—David Marcus by using the forbidden protomatter, and James T. Kirk by \"cheating\" to win the \"Kobayashi Maru\" test. The inclusion of protomatter ultimately results in both the accelerated maturation of the regenerated Spock during his stay on the Genesis planet, and the planet's subsequent explosion into an asteroid belt.\n\nIn the \"Deep Space Nine\" episode \"By Inferno's Light\", Protomatter was used by a Dominion changeling in a bomb plot that, if successful, would have destroyed the Bajoran sun and the forces of the Alpha Quadrant.\n\nProtomatter is also mentioned in the \"Star Trek Voyager\" episode \"\", where it is said, \"Protomatter's one of the most sought-after commodities. The best energy source in the quadrant.\"\n\nThe Omega Molecule is a highly unstable molecule believed to be the most powerful substance known to exist. If not properly disposed of, it may destroy subspace and render warp travel impossible. In \"\", during the episode The Omega Directive, \"Voyager\" encounters Omega particles and Captain Janeway must comply with the Omega Directive and destroy the particles. Later in the episode, they spontaneously stabilize for a brief moment before they are destroyed.\n\nRed matter is a red liquid material introduced in \"Star Trek\" (the 2009 film) that is able to create a black hole when not properly contained. Spock attempts to use it to stop a massive, galaxy-threatening supernova, but the resulting black hole causes his own ship and a Romulan mining vessel to travel back in time. Later in the film, the antagonist Nero uses it to destroy the planet Vulcan. Shortly after, the future Spock's ship containing the red matter is used to destroy Nero's Romulan mining vessel.\n\nCorbomite was named by Captain Kirk in a bluff in \"The Corbomite Maneuver\" as a material and a device that prevents attack, because if any destructive energy touches the vessel, a reverse reaction of equal strength is created, destroying the attacker.\n\nArcherite was named by Commander Shran also in a bluff in \"\" as a material that his ship was looking to mine, during an encounter at the test site of the Xindi planet killer weapon.\n\n\n"}
{"id": "50543122", "url": "https://en.wikipedia.org/wiki?curid=50543122", "title": "List of hydroelectric power stations in Brazil", "text": "List of hydroelectric power stations in Brazil\n\nThe following is a list of hydroelectric power stations in Brazil with a nameplate capacity of at least 100 MW.\nAccording to the \"Associação Brasileira de Distribuidores de Energia Elétrica\" (ABRADEE) there are 201 hydroelectric power stations in Brazil with a nameplate capacity of more than 30 MW; the total capacity of these power stations in 2015 was 84,703 MW. There are an additional 476 hydroelectric power stations with a nameplate capacity between 1 and 30 MW and 496 micro hydroelectric power stations with a nameplate capacity of less than 1 MW.\n\n"}
{"id": "28063512", "url": "https://en.wikipedia.org/wiki?curid=28063512", "title": "List of run-of-the-river hydroelectric power stations", "text": "List of run-of-the-river hydroelectric power stations\n\nThe following page lists hydroelectric power stations that generate power using the run-of-the-river method. This list includes most power stations that are larger than in maximum net capacity, which are currently operational or under construction. Those power stations that are smaller than , and those that are only at a planning/proposal stage may be found in regional lists, are listed at the end of the page. \n\nThis table lists currently operational power stations. Some of these may have additional units under construction, but only current net capacity is listed.\n\nThis table lists stations under construction or operational stations with under-construction and current net capacity over 100 MW.\n"}
{"id": "12154926", "url": "https://en.wikipedia.org/wiki?curid=12154926", "title": "Marshall Bluesbreaker", "text": "Marshall Bluesbreaker\n\nThe Marshall Bluesbreaker is the popular name given to the Models 1961 and 1962 guitar amplifiers made by Marshall from 1964/1965 to 1972.\n\nThe Bluesbreaker, which derives its nickname from being used by Eric Clapton with John Mayall & The Bluesbreakers, is credited with delivering \"the sound that launched British blues-rock in the mid-1960s.\" It was Marshall's first combo amplifier, and was described as \"arguably the most important [amplifier] in the company's history\" and \"the definitive rock amplifier.\"\n\nAccording to the most widely accepted story, Eric Clapton wanted an amp that would fit in the boot of his car, so he asked Jim Marshall (whose store in London he frequented) to make him a combo amp powerful enough to use on stage. According to Robb Lawrence's \"The Early Years of the Les Paul Legacy\", Jim Marshall initially gave Clapton a Model 1961 with 4×10\" speakers, which was soon replaced with a 2×12\" Model 1962. Clapton used the combo amplifier with his 1960 Gibson Les Paul Standard, allegedly in combination with a Dallas Rangemaster Treble Booster, which resulted in the creation of a texture of sound that would become regarded as iconic in the realm of blues oriented rock.\n\nMarshall's Model 1961/1962 combo amplifier entered the market at an affordable price—one third cheaper than a Vox AC30 and half the price of a Fender Bassman combo. Its reputation was cemented when Clapton, who had rejoined John Mayall & the Bluesbreakers, used one to record \"Blues Breakers with Eric Clapton\"—a set of sessions now widely regarded as \"historic\". After that, the combo became known as the \"Bluesbreaker.\" The model was discontinued in 1972.\n\nDue to its iconic status amongst collectors, the Bluesbreaker has become one of the most collectible and valuable vintage guitar amplifiers. According to a 2011 \"Vintage Guitar\" article ranking the twenty-five \"most valuable amplifiers\", the 1966/1967 Bluesbreaker is seventh on the list, with solid original examples fetching prices between US$8,300 and US$10,000.\n\nMarshall reissued the 2×12\" Bluesbreaker in 1989; the 4×10\" was never reissued. This version used 6L6 tubes. In 1991, Marshall began making a guitar effect pedal that was intended to emulate the sound of the original combo. In 1999, a second version of the amplifier, the Bluesbreaker II, was released, with 5881 tubes. An amplifier head, the 2245THW, was reissued in Marshall's \"Handwired\" series, with circuitry identical to the Bluesbreaker. \"Vintage Guitar\" called it a \"fine high-end piece\"; it was listed at $4800.\n\nMarshall's original Model 1961 and 1962 were basically JTM 45 combo amplifiers. Model 1961 was essentially the lead version of the Model 1987 JTM 45, fitted with tremolo and installed into an open backed speaker cabinet, while Model 1962 was the bass version of the JTM 45 (Model 1986), also fitted with tremolo and open backed cabinet. These amplifiers both feature the basic JTM 45 modified Fender Bassman circuit, which provided the origin of what became known as the \"Marshall sound\". The first versions of these combo amplifiers were made in 1964–1965, with Models 1961 and 1962 being fitted with 4×10\" and 2×12\" Celestion speakers respectively. An extremely rare 2×12\" extension cabinet was also offered. A later model had a slightly thinner cabinet with different acoustics. Production JTM45 amplifiers used KT66 output tubes, which are credited with providing \"a round, bell like tone with soft distortion character.\" Also contributing to the overall sound picture of the JTM45 series amplifiers was a GZ34 rectifier tube.\n\nMarshall also made an 18-watt combo amplifier with 2×10\" speakers (Model 1958) that looked like a smaller version of the Bluesbreaker, and is sometimes referred to as its \"little brother\".\n\nThe output of a typical Bluesbreaker was only about 35 watts, and thus the sound would break up at more moderate volumes as compared to larger amplifiers. It was precisely this distortion that Eric Clapton was after. Reportedly, Clapton told the engineer during the Bluesbreakers sessions that he should mike the amplifier from across the room, because he intended to play it as loud as possible. Producer Mike Vernon is credited with allowing Clapton to play in the studio as if he were playing live, and to improvise his solos played at full volume through the Marshall 1962 combo.\n\nIn comparison with the Marshall JTM45 half-stacks of the time, the open-back combo amps had less low and a bit more crisp high-end response, which suits the Les Paul well, especially when recording blues.\n\nSince the Bluesbreaker ultimately derives from the Fender Bassman, it is possible to create an approximate reproduction of a Bluesbreaker by modifying a Bassman; in February 1993 \"Guitar Player\" magazine published this modification.\n\n"}
{"id": "44464909", "url": "https://en.wikipedia.org/wiki?curid=44464909", "title": "Masrur Temples", "text": "Masrur Temples\n\nThe Masrur Temples, also referred to as Masroor Temples or Rock-cut Temples at Masrur, is an early 8th-century complex of rock-cut Hindu temples in the Kangra Valley of Beas River in Himachal Pradesh, India. The temples face northeast, towards the Dhauladhar range of the Himalayas. They are a version of North Indian Nagara architecture style, dedicated to Shiva, Vishnu, Devi and Saura traditions of Hinduism, with its surviving iconography likely inspired by a henotheistic framework. Though a major temples complex in the surviving form, the archaeological studies suggest that the artists and architects had a far more ambititious plan and the complex remains incomplete. Much of the Masrur's temple's sculpture and reliefs have been lost. They were also quite damaged, most likely from earthquakes.\n\nThe temples were carved out of monolithic rock with a shikhara, and provided with a sacred pool of water as recommended by Hindu texts on temple architecture. The temple has three entrances on its northeast, southeast and northwest side, two of which are incomplete. Evidence suggests that a fourth entrance was planned and started but left mostly incomplete, something acknowledged by the early 20th-century colonial era archaeology teams but ignored leading to misidentification and erroneous reports. The entire complex is symmetrically laid out on a square grid, where the main temple is surrounded by smaller temples in a mandala pattern. The main sanctum of the temples complex has a square plan, as do other shrines and the mandapa. The temples complex features reliefs of major Vedic and Puranic gods and goddesses, and its friezes narrate legends from the Hindu texts.\n\nThe temple complex was first reported by Henry Shuttleworth in 1913 bringing it to the attention of archaeologists. They were independently surveyed by Harold Hargreaves of the Archaeological Survey of India in 1915. According to Michael Meister, an art historian and a professor specializing in Indian temple architecture, the Masrur temples are a surviving example of a temple mountain-style Hindu architecture which embodies the earth and mountains around it.\n\nThe Masrur Temples are about southwest of the Dharmashala-McLeod Ganj and west from the Kangra town in the mountainous state of Himachal Pradesh in north India. The temple is built in the Beas River valley, in the foothills of the Hamalayas, facing the snowy peaks of the Dhauladhar range. The temples are about northwest from Shimla, about north of Jalandhar and about east of Pathankot. The nearest railway station is Nagrota Surian, and the nearest airport is Dharamshala(IATA: DHM). The closest major airports with daily services are Amritsar and Jammu.\nThe rock-cut temple is located in the valley, on the top of a naturally rocky hill, which Hargreaves in 1915 described as, \"standing some 2,500 feet above sea level, and commanding, as they [Hindu temples] do, a magnificent view over a beautiful, well-watered and fertile tract, their situation, though remote, is singularly pleasing\".\n\nAccording to Khan, the Hindu temples in Masrur show similarities to the Elephanta Caves near Mumbai (1,900 km away), Angkor Wat in Cambodia (4,000 km away), and the rock-cut temples of Mahabalipuram in Tamil Nadu (2,700 km away). The features also suggest the influence of \"Gupta classicism\", and therefore he places their construction in the 8th century. The area around the temple complex has caves and ruins which, states Khan, suggests that the Masrur region once had a large human settlement.\n\nAccording to Meister, the temples are from the first half of the 8th century based on the regional political and art history. The temples follow one version of the Nagara architecture, a style that developed in Central India, particularly during the rule of the Hindu king Yasovarman, an art patron. In Kashmir, a region immediately north and northwest of the site, Hindus built temples with square pyramidal towers by the mid 1st millennium CE, such as the numerous stone temples built by Hindu king Lalitaditya, another art patron. These kingdoms traditionally collaborated as well as competed in their construction projects rivalry, while the guilds of artists moved between the two regions, through the valleys of ancient Himachal Pradesh. The region is in the Himalayan terrain and forested, making conquests difficult and expensive. Historical records from the 6th to 12th centuries do not mention any military rivalries in the Beas river Kangra valley region. There is a mention of a Himalayan kingdom of Bharmour just north of Masrur area in early medieval era texts. The 11th-century text \"Rajatarangini\" as well as the 12th-century Kashmir chronicle by Kalhana, both mention political rivalries in the 8th century but these 11th and 12th century authors were too far removed in time from 8th century events, and they weave in so much ancient mythology that their semi-fictional texts are largely ahistorical and unreliable.\n\nThe inscriptions and architecture suggest that Yasovarman's influence had reached the Himalayan foothills in north India, and the central Indian influence is illustrated in the architectural style adopted for Masrur temples rather than the styles found in ruined and excavated temples of the northwestern Indian subcontinent. According to Meister, the influence of middle India must have reached the north Indian region earlier than the 8th century and this style was admired by the royal class and the elites, because this style of temple building is now traceable in many more historic sites such as those in Bajaura and many places in Himachal Pradesh, Uttarakhand, and Nepal where many of the holy rivers of Hindus emerge. Some of the smaller stone temples that have survived from this region in steep mountain terrain are from the 7th century. Further, these large temple complexes in the second half of the 1st millennium were expensive projects and required substantial patronage, which suggests that the earlier examples must have preceded them for wide social and theological acceptance.\n\nThe period between 12th and 19th century was largely of religious wars and geo-political instability across the Indian subcontinent, and the literature of this era do not mention Masrur temples or present any scholarly studies on any Hindu, Jain or Buddhist temples for that matter, rather they mention iconoclasm and temple destruction. After the 12th century, first northwestern Indian subcontinent, then India, in general, witnessed a series of plunder raids and attacks of Turko-Afghan sultans led Muslim armies seeking wealth, geopolitical power and the spread of Islam. Successive Muslim dynasties controlled the Delhi Sultanate as waves of wars, rebellions, secessions, and brutal counter-conquests gripped Indian regions including those in and around Kashmir. The Mughal Empire replaced the Delhi Sultanate in early 16th-century. The Mughal dynasty ruled much of the Indian subcontinent through early 18th-century, and parts of it nominally through the 19th century. The Kangra valley region with Masrur in the Himalayas was ruled by smaller jagirdars and feudatory Hill Rajas who paid tribute to the Mughal administration for many centuries. The arrival of the colonial era marked another seismic shift in the region's politics. By the late 19th century, British India officials had begun archeological surveys and heritage preservation efforts. The first known visits to study the Masrur temples occurred in 1887.\n\nA British empire officer Henry Shuttleworth visited and photographed the temples in 1913, calling it a \"Vaishnava temple\" and claiming in his report that he was the first European to visit them. He wrote a paper on the temples, which was published by the journal \"The Indian Antiquary\". He shared his findings with Harold Hargreaves, then an officer of the Northern Circle of the Archaeology Survey of India. Hargreaves knew more about Hindu theology, noticed the Shiva linga in the sanctum and he corrected Shuttleworth's report. Hargreaves wrote up his tour and published his photographs and observations in 1915 as a part of the ASI Annual Report Volume 20. Hargreaves acknowledged the discovery that a draftsman in his office had already toured, measured and created temples plans and sections in 1887, and that some other ASI workers and Europeans had visited the temple in 1875 and after 1887. The Hargreaves report described the site as many temples, listed iconography at these temples from different Hindu traditions, mentioned his speculations on links with Mahabalipuram monuments and Gandhara art, and other theories. The Hargreaves text became the introduction to Masrur temples for guides by reporters with little to no background knowledge of Indian temple traditions or Hindu theology. According to Meister, these early 20th century writings became a source of the temple's misidentification and misrepresentations that followed.\n\nThe site was already damaged but still in decent condition in the late 19th-century. Hargreaves wrote that, \"the remote situation and general inaccessibility of the temples have been at one the cause of their neglect and of their fortunate escape from the destroying hands of the various Muhammadan invaders of the valley\". In the 1905 Kangra earthquake, the Himachal valley region was devastated. The site was affected, the temple parts cracked and tumbled, but because the temple was created out of the stone in-situ, its monolithic nature left many parts of the temple standing, while it destroyed numerous other ancient monuments in the Himachal region.\n\nThe damage from wars and 1905 earthquake of the region has made comparative studies difficult. However, the careful measurements and drawings made by the unknown draftsperson in 1887, particularly of the roof level and mandapa which were destroyed in 1905, have been a significant source for late 20th-century scholarship. It supports the Shuttleworth's early comments that the temple complex has a \"perfect symmetry of design\".\n\nThe main monument at the Masrur temples site appears, at first sight, to be a complex of shrines, but it is an integrated monument. Its center has a principal shrine which unlike most Hindu temples does not face east, but faces Northeast towards the snowy Himalayan peaks of Dhauladhar range. The main spire is flanked by subsidiary spires of smaller size, all eight symmetrically placed to form an octagon (or two rotated squares). These spires of the temple seem to grow out of the natural rock that makes the mountain. Above the main sanctum, the rock was cut to form the flat roof and the second level of the temple naturally fused with the rising main spire (\"shikhara\") as well as the eight subsidiary shrines.\nThe main sanctum has four entrances, of which one on the east side is complete, two on the north and south side are partially complete and the fourth can be seen but is largely incomplete. The eastern entrance had a large mandapa and a portico, but this was destroyed in the 1905 earthquake, its existence known from site visit notes prior to the earthquake. Attached to this mandapa were two stairs to take the pilgrims to the upper-level views. The stairs were set inside smaller two rotating stair spires, but much of the structure of this too is gone. Thus, at one time the main temple had 13 spires according to Hargreaves count, and 15 according to Shuttleworth's count, all designed to appear growing naturally out of the rock.\n\nAccording to Meister, the early descriptions though well-intentioned were based on information then available and clouded by the presumptions of those authors. These presumptions and generally damaged condition of the complex, for example, led Shuttleworth and Hargreaves to describe the temple in terms such as \"subsidiary\" and \"shrines\" instead of witnessing the integrated plan and architecture in early Hindu texts on temple design.\n\nThe temple complex was carved out of the natural sandstone rock. In some places, the rock is naturally very hard, which would have been difficult to carve, but is also the reason why the intricate carvings on it have preserved for over 1,000 years. In other places the stone was soft or of medium quality. In some cases, the artists carved with a bit softer stone and this has eroded over time from natural causes. In other cases, the stone's hardness was so low that the artists cut out the stone and substituted it with better stone blocks. Then they added their friezes or sculptures. The substituted blocks have better resisted the effect of nature and time.\n\nThe temple complex has a sacred pool in front on the east side. The construction of the sacred pool is dated to the early 8th century. Its rectangular dimensions are about , or two stacked squares. The temple had an outside square mandapa with about side and height. It had a solid thick roof supported by four carved massive pillars. The platform had a covered drainage system to allow water anywhere on the mandapa to naturally drain off. This was visible before the 1905 quake, now only remnants of the floor and a pillar remain.\nThe entrances lead the pilgrim and visitor towards the main sanctum, through a series of mandapas with wall carvings and then an \"antarala\" (vestibule). They also connect the created space to a pair of covered stairs, on the north and south side, to the upper floor from where he or she can complete a \"pradakshina\" (circumambulation) to view more sculptures and the mountain scenery, all of this space and structure created from the pre-existing monolithic rock.\n\nThe garbhagriha, in a square plan with each side of . The main sanctum has a four faced Shiva.\n\nThe ceiling of various mandapa and the sanctum inside the temple are fully carved, predominantly with open lotus. However, the inside walls remained incomplete. This may be because the artists carving into the rock worked on cutting and finishing the ceiling first, then moved on to cutting, finishing and decorating the inside walls and creating pillars below those ceilings. The wall height is , and only the eastern entrance and passage into the sanctum is fully complete, while the side entrances are not and the fourth western entrance being the least complete. The site suggests that the work was completed in parallel by teams of workers. This is a common style of construction found in numerous Hindu temples that have survived, at least in the ruins form, from the 1st millennium. The 8th-century three-entry, four-faced Shiva found at the Masrur temple is not unique as the same plan is found in the Jogesvari Cave temple near Mumbai. The Jogesvari is dated to have been completed between 400-450 CE, or several centuries before the Masrur temple's construction, suggesting a common thematic foundation that inspired these temples pre-existed in the Hindu texts.\n\nThe art historian Stella Kramrisch identified one of these Hindu architecture texts to be the \"Visnudharmottara\", dated to have existed by the 8th-century (floruit), and whose manuscripts have been found with Hindus of the Kashmir valley. This is one of such texts that describe \"hundred-and-one [Hindu] temple\" designs. According to Meister, the sanctum and spire plan for the Masrur temple fits one of these, where it is called the \"Kailasa\" design.\n\nThe \"Kailasa\" style of Hindu temple is one with a central Shikhara (spire) symmetrically surrounded by four smaller spires set between the four entrances into the temple from the four cardinal directions, a format that matches the Masrur temple plan. Further, the \"Visnudharmottara\" text also describes the principles and procedures for image making and painting, the former is also found preserved in the Masrur temple mandapa and sanctum. Further, the Jogesvari and Masrur are not the only surviving temples that correspond to this style, others have been discovered that do, such as the Bajaura Hindu temple in Kulu valley of Himachal Pradesh which is another stone temple.\n\nThe multi-spire style, states Meister, is possibly inspired by the Indian Meru mythology shared by Buddhists, Hindus and Jains. Lush mountainous Meru is heaven and the abode of gods, but mountains are not singular but exist in ranges. The highest Mount Kailasha is the abode of Shiva, and the secondary spires symbolize the mountain range. Eight heavenly continents surround the Mount Kailasha in this mythology, where all the \"Deva\" (gods) and \"Devi\" (goddesses) live together. The Masrur temple symbolically projects this mythical landscape, narrating the Indian cosmology from stone, into stone.\n\nAll spires in the Masrur temple are of \"Nagara\" style, an architecture that was developed and refined in central India in the centuries before the 8th-century. More specifically, these are what Indian texts called the \"latina\" sub-style, from \"lata\". These are curvilinear spires composed of a rhythmic series of superimposed shrinking horizontal square slabs with offsets, each offset called \"lata\" or grape vine-like, in principle reflecting natural growth on a mountain in stone.\nThe superstructure towers embed styles that are found in Indian temples from the 7th and 8th-century such as in the Mahua Hindu temple and the Alampur Navabrahma Temples, but these are no longer found in temples that can be firmly dated after the 8th-century. This supports dating the Masrur temple to about mid 8th-century. The spires show differences, but all spires that are symmetrically position in the temple mandala show the same design. The stairway spire is based on four turned squares, and features eight rotating \"lata\" spines that alternate with eight right-angled projections.\n\nThe temple complex also has two free-standing sub-shrines near the sacred pool. These have spires with sixteen \"lata\" spines, a style that is uncommon in India and found associated with Shiva temples associated with Hindu monks of the Matamayura \"matha\" between the 7th and 12th century CE such as the Bajaura temple in nearby Kulu valley and the Chandrehi temple in central India.\n\nThe main sanctum has nine seated deities. The center one is Shiva, and with him are others including Vishnu, Indra, Ganesa, Kartikiya and Durga. The shrines around the central shrine feature five Devis in one case, while other shrines reverentially enshrine Vishnu, Lakshmi, Ganesha, Kartikeya, Surya, Indra and Saraswati. The avatars of Vishnu such as the Varaha and the Narasimha are presented in the niches. In the ruins have been found large sculptures of Varuna, Agni and others Vedic deities. The temple also includes fusion or syncretic ideas revered in Hinduism, such as Ardhanarishvara (half Parvati, half Shiva), Harihara (half Vishnu, half Shiva) and a three faced trinity that shows Brahma, Vishnu and Shiva in one sculpture. The temple also has secular images from the common life of people, of couples in courtship and various levels of intimacy (mithuna), people making music and dancing, apsaras and ornamental scrollwork.\n\nThe surviving structures in the Masrur temple lacks any image of Lakulisa, the founder of Pashupata Shaivism, which makes it unlikely that this temple was associated with that tradition. According to Meister, the wide range of Shaiva, Vaishnava, Shakti and Saura (Surya, sun god) themes displayed within the Masrur temple suggest that it was built by those who cherish ecumenism or henotheism, of the style commonly found in Pancharatra literature of Hinduism.\n\nAccording to Hargreaves, when he visited the temple for the first time in 1913, the temple complex had a dharmashala (pilgrim's resthouse), a kitchen and there was a priest for whom there was a small integrated living quarters. The temple work was priest's part-time work, while his main source of livelihood was from maintaining cattle and working in farms.\n\nThe Masrur temple and the 8th-century \"Prasat Ak Yum\" temple found in Siam Reap, Cambodia have parallels, in that both are temple mountains with a symmetric design.\n\nAccording to a local legend, the Pandavas of Mahabharata fame resided here during their \"incognito\" exile from their kingdom and built this temple. According to Khan, the identity and location of Pandavas was exposed, so they shifted from here. This is said to be why the temple complex was left unfinished. Sometime in the 20th century, someone introduced three small blackstone statues inside the shrine which faces east. These are of Rama, Lakshmana and Sita of the \"Ramayana\" fame.\n\nAt least since the time Harold Hargreaves visited the temple in 1913, the central temple has been locally called the \"Thakurdvara\".\n\n\n"}
{"id": "39476791", "url": "https://en.wikipedia.org/wiki?curid=39476791", "title": "Mohawkite", "text": "Mohawkite\n\nMohawkite is a rare rock consisting of mixtures of arsenic , silver, nickel, skutterudite and copper, with the formula CuAs up to CuAs, and the most desirable material was usually found in white quartz matrix. It has a hardness of 3-3.5 and a metallic luster. It is named after the Mohawk mine where it was originally found. Colors range from brassy-yellow to metallic gray, and sometimes will have a blue or greenish surface tarnish. These colors come from its two main ingredients, the arsenic-rich copper minerals algodonite and domeykite. Its color may resemble pyrrhotite, but unlike pyrrhotite, mohawkite is not magnetic.\n\nMohawkite is believed to be found only in a copper mine located on the Keweenaw Peninsula of Michigan, known as the Mohawk Mine. The Mohawk Mine is where mohawkite was first discovered, in January, 1900, near No. 1 shaft when a fissure vein of copper ore was cut. Specimens of this ore were sent to George A Koenig, of the Michigan College of Mines (now known as Michigan Technological University) for analysis. The ore was believed to be an entirely new mineral and was named mohawkite by Koenig. A reanalysis of the material in 1971 found it to be an intimate mixture of copper and nickel arsenides and the mohawkite name was discredited as a mineral species.\n\nMohawkite, being a copper ore, is used for obtaining copper. However, when the copper is removed, it is toxic due to the amounts of arsenic in it. Mohawkite, particularly when it contains quartz, is often used in jewelry because of its appearance.\n\n\n"}
{"id": "23608932", "url": "https://en.wikipedia.org/wiki?curid=23608932", "title": "National Nuclear Energy Commission", "text": "National Nuclear Energy Commission\n\nThe National Nuclear Energy Commission (; CNEN) is the Brazilian government agency responsible for the orientation, planning, supervision, and control of Brazil's nuclear program. The agency was created on 10 October 1956. The CNEN is under the direct control of the Ministry of Science and Technology.\n\nThe agency is headquartered in Rio de Janeiro and manages several institutes and facilities throughout Brazil. It has five regional districts, with headquarters in Angra dos Reis, Caetité, Fortaleza, Porto Alegre, and Resende, and a special office in Brasília.\n\nThe Nuclear Energy Research Institute (; IPEN) is an agency managed by CNEN and associated to the São Paulo State government and the University of São Paulo. The IPEN has a broad infrastructure of laboratories, a research reactor (IEA-R1), an industrial particle accelerator, and a compact cyclotron of variable energy. The IPEN is involved primarily in conducting research in the areas of nuclear materials and processes, nuclear reactors, applications of nuclear techniques, and nuclear safety. The IPEN is noted for its production of radioisotopes for nuclear medicine.\n\nThe Radiological Protection and Dosimetry Institute (; IRD) is an institute of CNEN responsible for radiological protection and dosimetry of ionizing radiation, and conducts inspection of radioactive use in the industry, power plants and other areas. Since 1976, the IRD possesses a Secondary Standard Dosimetry Laboratory, recognized by the International Atomic Energy Agency and the World Health Organization.\n\nThe Nuclear Technology Development Center (; CDTN) is a nuclear research institute of CNEN. It originated in the Engineering School of the Federal University of Minas Gerais, and was created in 1952 with the name of Institute of Radioactive Researches (IPR). It was the first institution in Brazil devoted entirely to the nuclear area. Its initial activities included the research of radioactive mineral occurrences, studies in nuclear physics field, metallurgy and in materials of nuclear interest. In 1960 the research Reactor TRIGA Mark 1 was inaugurated at the Institute, with the purpose of training, research and radioisotope production.\n\nThe Nuclear Engineering Institute (; IEN) is a research unit of CNEN. Since 1962, IEN has been contributing to the national mastering of technologies in the nuclear field and its correlates. Its scope is generating and transferring knowledge and technology to the productive sector - public and private - with society as its final beneficiary.\n\nPatents publications, technology licensing, radiopharmaceuticals, materials essays and analysis, radioactive waste collecting, consulting and human resources formation are IEN's main products and services.\n\nThe Central-West Regional Center of Nuclear Science (; CRCN-CO) is a regional institute of CNEN responsible for radioactive waste management and deposit, and nuclear technology in the Central-West Region of Brazil.\n\nThe Northeast Regional Center of Nuclear Science (; CRCN-NE) is a regional institute of CNEN responsible for inspection and control of facilities and radioactive materials in the Northeast Region of Brazil.\n\nNuclear energy accounts for about 4% of the Brazil's electricity. It is produced by two pressurized water reactors at Angra Nuclear Power Plant (Angra I and II). A third reactor, Angra III, with a projected output of 1,350 MW, is planned to be finished by 2010. By 2025 Brazil plans to build seven more reactors.\nCurrently, all uranium exploration, production and export in Brazil is under the control of the state through INB, which is a subsidiary of the National Nuclear Energy Commission, although the Brazilian government has recently announced that it is prepared to move ahead with private sector involvement in the nuclear fuel cycle.\n\n\n"}
{"id": "4403052", "url": "https://en.wikipedia.org/wiki?curid=4403052", "title": "Operating empty weight", "text": "Operating empty weight\n\nOperating empty weight (OEW) or Basic Operating Weight or Empty Operating Weight is the standard basic weight for any particular series or any particular configuration. The aircraft is periodically weighed and its weight is listed with each structural modification order, or any configuration order which may alter the Empty Operating Weight (EOW). The EOW includes all fluids necessary for operation such as engine oil, engine coolant, water, hydraulic fluid and the unusable fuel. Extra fixed operator items and optional equipment required for flight are added to EOW to determine OEW. From there, any weight added to the aircraft is the Total Payload, above the OEW, which consists of (a) cargo (b) luggage (c) passengers and crew (d) stores (e) service load such as meals and beverages (f) fuel load.\n\nThe operating empty weight (OEW) is the sum of the manufacturer's empty weight (MEW), standard items (SI), and operator items (OI). All additional weight added is computed for weight, arm and moment calculations to determine the center of gravity. \n\nOEW = MEW + SI + OI\n\nAircraft purchase price is a near linear function of operating empty weight.\n\n"}
{"id": "20941553", "url": "https://en.wikipedia.org/wiki?curid=20941553", "title": "Petroleum Association of Japan", "text": "Petroleum Association of Japan\n\nThe is composed of 18 refiners and primary distributors in Japan. It was incorporated in November 1955. It deals with the refining and marketing of petroleum products. It is located in Tokyo.\n\nThe PAJ is responsible for activities such as publishing information on issues which are important to the petroleum industry; undertaking various governmental subsidy programs such as research and development; studying and coordinating activities related to issues which are important to petroleum and to provide information on such issues; representing the views or proposals of industry to the government, industrial associations, the media and the general public; and promoting better communication and understanding among member companies.\n\n"}
{"id": "11450403", "url": "https://en.wikipedia.org/wiki?curid=11450403", "title": "Shimose powder", "text": "Shimose powder\n\nShimose was born in Hiroshima Prefecture and was a graduate of Tokyo Imperial University as one of Japan’s earliest holders of a doctorate in engineering. In 1887 he was hired by the Imperial Japanese Navy as a chemical engineer, and in 1899 was appointed head of a research unit to develop a more powerful type of gunpowder for use by naval artillery.\n\nShimose developed a new explosive based on a form of picric acid used by France as melinite and by Britain as lyddite. The new explosive was more stable than picric acid, and generated more heat and blast power than any other explosive available at the time. It also does not produce much smoke, which was an important advantage over normal gunpowder in combat. Shimose powder was adopted by the Imperial Japanese Navy from 1893, not only for naval artillery, but also for naval mines, depth charges and torpedo warheads. It played an important role in the Japanese victory in the Russo-Japanese War of 1904 to 1905.\n"}
{"id": "25613762", "url": "https://en.wikipedia.org/wiki?curid=25613762", "title": "Solar air heat", "text": "Solar air heat\n\nSolar air heating is a solar thermal technology in which the energy from the sun, insolation, is captured by an absorbing medium and used to heat air. Solar air heating is a renewable energy heating technology used to heat or condition air for buildings or process heat applications. It is typically the most cost-effective out of all the solar technologies, especially in commercial and industrial applications, and it addresses the largest usage of building energy in heating climates, which is space heating and industrial process heating.\n\nSolar air collectors can be divided into two categories: \n\nSolar collectors for air heat may be classified by their air distribution paths or by their materials, such as glazed or unglazed. For example:\n\n\nThe term \"unglazed air collector\" refers to a solar air heating system that consists of an absorber without any glass or glazing over top. The most common type of unglazed collector on the market is the transpired solar collector. This technology was invented and patented by Canadian engineer John Hollick of Conserval Engineering Inc. in the 1990s, who worked with the U.S. Department of Energy (NREL) and Natural Resources Canada on the commercialization of the technology around the world. The technology has been extensively monitored by these government agencies, and Natural Resources Canada developed the feasibility tool RETScreen to model the energy savings from transpired solar collectors. John Hollick and the transpired solar collector were honored by the American Society of Mechanical Engineers (ASME) in 2014 as being one of the best inventions of the industrialized age, alongside Thomas Edison, Henry Ford, the steam engine and the Panama Canal – in a New York exhibition recognizing the best inventions, inventors and engineering feats of the past two centuries.\n\nSeveral thousand transpired solar collector systems have been installed in a variety of commercial, industrial, institutional, agricultural, and process applications in over 35 countries around the world. The technology was originally used primarily in industrial applications such as manufacturing and assembly plants where there were high ventilation requirements, stratified ceiling heat, and often negative pressure in the building. The first unglazed transpired collector in the world was installed by Ford Motor Company on their assembly plant in Oakville, Canada.\n\nWith the increasing drive to install renewable energy systems on buildings, transpired solar collectors are now used across the entire building stock because of high energy production (up to 500-600 peak thermal Watts/square metre), high solar conversion (up to 90%) and lower capital costs when compared against solar photovoltaic and solar water heating.\n\nUnglazed air collectors heat ambient (outside) air instead of recirculated building air. Transpired solar collectors are usually wall-mounted to capture the lower sun angle in the winter heating months as well as sun reflection off the snow and achieve their optimum performance and return on investment when operating at flow rates of between 4 and 8 CFM per square foot (72 to 144 m3/h.m2) of collector area.\n\nThe exterior surface of a transpired solar collector consists of thousands of tiny micro-perforations that allow the boundary layer of heat to be captured and uniformly drawn into an air cavity behind the exterior panels. This solar heated ventilation air is drawn into the building’s ventilation system from air outlets positioned along the top of the collector and the air is then distributed in the building via conventional means or using a solar ducting system.\n\nThe extensive monitoring by Natural Resources Canada and NREL has shown that transpired solar collector systems reduce between 10-50% of the conventional heating load and that RETScreen is an accurate predictor of system performance.\n\nTranspired solar collectors act as a rainscreen and they also capture heat loss escaping from the building envelope which is collected in the collector air cavity and drawn back into the ventilation system. There is no maintenance required with solar air heating systems and the expected lifespan is over 30 years.\n\nUnglazed transpired collectors can also be roof-mounted for applications in which there is not a suitable south facing wall or for other architectural considerations. A number of companies offer transpired air collectors suitable for roof mounting either mounted directly onto a sloped metal roof or as modules affixed to ducts and connected to nearby fans and HVAC units.\n\nHigher temperatures are also possible with transpired collectors which can be configured to heat the air twice to increase the temperature rise making it suitable for space heating of larger buildings. In a 2-stage system, the first stage is the typical unglazed transpired collector and the second stage has glazing covering the transpired collector. The glazing allows all of that heated air from the first stage to be directed through a second set of transpired collectors for a second stage of solar heating.\n\nAnother innovation is to recover heat from the PV modules (which is often four times more than the electrical energy produced by the PV module) by mounting the PV modules onto the solar air system. In cases where there is a heating requirement, incorporating a solar air component into the PV system provides two technical advantages; it removes the PV heat and allows the PV system to operate closer to its rated efficiency (which is 25 C); and it decreases the total energy payback period associated with the combined system because the heat energy is captured and used to offset conventional heating.\n\nFunctioning in a similar manner as a conventional forced air furnace, systems provide heat by recirculating conditioned building air through solar collectors. Through the use of an energy collecting surface to absorb the sun’s thermal energy, and ducting air to come in contact with it, a simple and effective collector can be made for a variety of air conditioning and process applications.\nA simple solar air collector consists of an absorber material, sometimes having a selective surface, to capture radiation from the sun and transfers this thermal energy to air via conduction heat transfer. This heated air is then ducted to the building space or to the process area where the heated air is used for or process heating needs.\n\nThe pioneering figure for this type of system was George Löf, who built solar heated air system for a house in Boulder, Colorado, in 1945. He later included a gravel bed for heat storage.\n\nIn the through-pass configuration, air ducted onto one side of the absorber passes through a perforated or fibrous type material and is heated from the conductive properties of the material and the convective properties of the moving air. Through-pass absorbers have the most surface area which enables relatively high conductive heat transfer rates, but significant pressure drop can require greater fan power, and deterioration of certain absorber material after many years of solar radiation exposure can additionally create problems with air quality and performance.\n\nIn back-pass, front-pass, and combination type configurations the air is directed on either the back, the front, or on both sides of the absorber to be heated from the return to the supply ducting headers. Although passing the air on both sides of the absorber will provide a greater surface area for conductive heat transfer, issues with dust (fouling) can arise from passing air on the front side of the absorber which reduces absorber efficiency by limiting the amount of sunlight received. In cold climates, air passing next to the glazing will additionally cause greater heat loss, resulting in lower overall performance of the collector.\n\nA variety of applications can utilize solar air heat technologies to reduce the carbon footprint from use of conventional heat sources, such as fossil fuels, to create a sustainable means to produce thermal energy. Applications such as space heating, greenhouse season extension, pre-heating ventilation makeup air, or process heat can be addressed by solar air heat devices. In the field of ‘solar co-generation’ solar thermal technologies are paired with photovoltaics (PV) to increase the efficiency of the system by cooling the PV panels to improve their electrical performance while simultaneously warming air for space heating.\n\nSpace heating for residential and commercial applications can be done through the use of solar air heating panels. This configuration operates by drawing air from the building envelope or from the outdoor environment and passing it through the collector where the air warms via conduction from the absorber and is then supplied to the living or working space by either passive means or with the assistance of a fan. In the older days, before air conditioning, it would get hot inside buildings, during the day times, because of heat from the sun. Even in cars, the temperature inside can exceed 50 degrees Celsius, if the windows are up and there is no need to switch on the heater\n\nSolar air heat can also be used in process applications such as drying laundry, crops (i.e. tea, corn, coffee) and other drying applications. Air heated through a solar collector and then passed over a medium to be dried can provide an efficient means by which to reduce the moisture content of the material.\n\nRadiation cooling to the night sky is based on the principle of heat loss by long-wave radiation from a warm surface (roof) to another body at a lower temperature (sky). On a clear night, a typical sky-facing surface can cool at a rate of about 75 W/m2 (25 BTU/hr/ft2) This means that a metal roof facing the sky will be colder than the surrounding air temperature. Collectors can take advantage of this cooling phenomena. As warm night air touches the cooler surface of a transpired collector, heat is transferred to the metal, radiated to the sky and the cooled air is then drawn in through the perforated surface. Cool air may then be drawn into HVAC units. See also \n\nBy drawing air through a properly designed air collector or air heater, solar heated fresh air can reduce the heating load during sunny operation. Applications include transpired collectors preheating fresh air entering a heat recovery ventilator, or suction created by venting heated air out of some other solar chimney.\n\n"}
{"id": "19137315", "url": "https://en.wikipedia.org/wiki?curid=19137315", "title": "Solid hydrogen", "text": "Solid hydrogen\n\nSolid hydrogen is the solid state of the element hydrogen, achieved by decreasing the temperature below hydrogen's melting point of . It was collected for the first time by James Dewar in 1899 and published with the title \"Sur la solidification de l'hydrogène\" (English: On the solidification of hydrogen) in the \"Annales de Chimie et de Physique\", 7th series, vol. 18, Oct. 1899. Solid hydrogen has a density of 0.086 g/cm making it one of the lowest-density solids.\n\nAt low temperatures and at pressures up to around 400 GPa, hydrogen forms a series of solid phases formed from discrete H molecules. \"Phase I\" occurs at low temperatures and pressures, and consists of a hexagonal close-packed array of freely rotating H molecules. Upon increasing the pressure at low temperature, a transition to \"Phase II\" occurs at up to 110 GPa. Phase II is a broken-symmetry structure in which the H molecules are no longer able to rotate freely. If the pressure is further increased at low temperature, a \"Phase III\" is encountered at about 160 GPa. Upon increasing the temperature, a transition to a \"Phase IV\" occurs at a temperature of a few hundred kelvin at a range of pressures above 220 GPa.\n\nIdentifying the atomic structures of the different phases of molecular solid hydrogen is extremely challenging, because hydrogen atoms interact with X-rays very weakly and only small samples of solid hydrogen can be achieved in diamond anvil cells, so that X-ray diffraction provides very limited information about the structures. Nevertheless, phase transitions can be detected by looking for abrupt changes in the Raman spectra of samples. Furthermore, atomic structures can be inferred from a combination of experimental Raman spectra and first-principles modelling. Density functional theory calculations have been used to search for candidate atomic structures for each phase. These candidate structures have low free energies and Raman spectra in agreement with the experimental spectra. Quantum Monte Carlo methods together with a first-principles treatment of anharmonic vibrational effects have then been used to obtain the relative Gibbs free energies of these structures and hence to obtain a theoretical pressure-temperature phase diagram that is in reasonable quantitative agreement with experiment. On this basis, Phase II is believed to be a molecular structure of \"P\"2/\"c\" symmetry; Phase III is (or is similar to) a structure of \"C\"2/\"c\" symmetry consisting of flat layers of molecules in a distorted hexagonal arrangement; and Phase IV is (or is similar to) a structure of \"Pc\" symmetry, consisting of alternate layers of strongly bonded molecules and weakly bonded graphene-like sheets.\n\n\n\n"}
{"id": "1473870", "url": "https://en.wikipedia.org/wiki?curid=1473870", "title": "Trace gas", "text": "Trace gas\n\nThe most abundant gases in the atmosphere are nitrogen (78.1%), oxygen (20.9%), and argon (0.934%). Combined, oxygen, argon, and nitrogen make up 99.934% of the gases in the atmosphere (not including water vapor). The other gases that are not part of the 99.934% of the atmosphere are called trace gases.\n\nThe abundance of a trace gas can range from a few parts per trillion (ppt) by volume to several hundred parts per million by volume (ppmv). When a trace gas is added into the atmosphere, that process is called a \"source\". There are two possible types of sources - natural or anthropogenic. Natural sources are caused by processes that occur in nature. In contrast, anthropogenic sources are caused by human activity. Some of the sources of a trace gas are biogenic, solid Earth (outgassing), the ocean, industrial activities, or in situ formation. A few examples of biogenic sources include photosynthesis, animal excrements, termites, rice paddies, and wetlands. Volcanoes are the main source for trace gases from solid earth. The global ocean is also a source of several trace gases, in particular sulfur-containing gases. In situ trace gas formation occurs through chemical reactions in the gas-phase. Anthropogenic sources are caused by human related activities such as fossil fuel combustion (e.g. in transportation), fossil fuel mining, biomass burning, and industrial activity.\n\nIn contrast, a \"sink\" is when a trace gas is removed from the atmosphere. Some of the sinks of trace gases are chemical reactions in the atmosphere, mainly with the OH radical, gas-to-particle conversion forming aerosols, wet deposition and dry deposition. Other sinks include microbiological activity in soils.\n\nBelow is a chart of several trace gases including their abundances, atmospheric lifetimes, sources, and sinks.  \n\nTable of Several Trace Gases – taken at pressure 1 atm, \nA few examples of the major greenhouse gases are water, carbon dioxide, methane, nitrous oxide, ozone, and CFCs. These gases can absorb infrared radiation from the Earth's surface as it passes through the atmosphere. The most important greenhouse gas is water vapor because it can trap about 80 percent of outgoing IR radiation. The second most important greenhouse gas, and the most important one affected by man-made sources into the atmosphere, is carbon dioxide. The reason for why greenhouse gases can absorb infrared radiation is their molecular structure. For example, carbon dioxide has two basic modes of vibration that create a strong dipole-moment, which causes its strong absorption of infrared radiation. Below is a table of some of the major greenhouse gases with man-made sources and their contribution to the enhanced greenhouse effect.\n\nKey Greenhouse Gases and Sources \n\nIn contrast, the most abundant gases in the atmosphere are not greenhouse gases. The main reasons is because they cannot absorb infrared radiation as they do not have vibrations with a dipole moment. For instance, the triple bonds of atmospheric dinitrogen make for a highly symmetric molecule that is very inert in the atmosphere.\n\nThe residence time of a trace gas depends on the abundance and rate of removal. The Junge (empirical) relationship describes the relationship between concentration fluctuations and residence time of a gas in the atmosphere. It can expressed as fc = \"b\"/τ, where fc is the coefficient of variation, τ is the residence time in years, and \"b\" is an empirical constant, which Junge originally gave as 0.14 years. As residence time increases, the concentration variability decreases. This implies that the most reactive gases have the most concentration variability because of their shorter lifetimes. In contrast, more inert gases are non-variable and have longer lifetimes. When measured far from their sources and sinks, the relationship can be used to estimate tropospheric residence-times of gases. \n"}
{"id": "242666", "url": "https://en.wikipedia.org/wiki?curid=242666", "title": "Transducer", "text": "Transducer\n\nA transducer is a device that converts energy from one form to another. Usually a transducer converts a signal in one form of energy to a signal in another.\n\nTransducers are often employed at the boundaries of automation, measurement, and control systems, where electrical signals are converted to and from other physical quantities (energy, force, torque, light, motion, position, etc.). The process of converting one form of energy to another is known as transduction.\n\nTransducers that convert physical quantities into mechanical ones are called mechanical transducers;\nTransducers that convert physical quantities into electrical are called electrical transducers. Examples are a thermocouple that changes temperature differences into a small voltage, or a Linear variable differential transformer (LVDT) used to measure displacement.\n\nTransducers can be categorized by which direction information passes through them:\n\n\nSome specifications that are used to rate transducers\n\n\n\n"}
{"id": "2652237", "url": "https://en.wikipedia.org/wiki?curid=2652237", "title": "Transient recovery voltage", "text": "Transient recovery voltage\n\nA transient recovery voltage (or TRV) for high-voltage circuit breakers is the voltage that appears across the terminals after current interruption. It is a critical parameter for fault interruption by a high-voltage circuit breaker, its characteristics (amplitude, rate of rise) can lead either to a successful current interruption or to a failure (called reignition or restrike).\n\nThe TRV is dependent on the characteristics of the system connected on both terminals of the circuit-breaker, and on the type of fault that this circuit breaker has to interrupt (single, double or three-phase faults, grounded or ungrounded fault ..).\n\nCharacteristics of the system include:\n\n\nThe most severe TRV is applied on the first pole of a circuit-breaker that interrupts current (called the first-pole-to-clear in a three-phase system). The parameters of TRVs are defined in international standards such as IEC and IEEE (or ANSI).\n\nTypical cases of capacitive loads are unloaded lines and capacitor banks.\n\nA terminal fault is a fault that occurs at the circuit breaker terminals. The circuit breaker interrupts a short-circuit at current zero, at this instant the supply voltage is maximum and the recovery voltage tends to reach the supply voltage with a high frequency transient. The normalized value of the overshoot or amplitude factor is 1.4.\n\nA short-line-fault is a fault that occurs on a line a few hundred meters to several kilometers down the line from the circuit breaker terminal. As shown on Figure 5, the TRV is characterized, in its initial part, by a steep rate-of-rise due to a high-frequency oscillation produced by travelling waves that travel on the line with positive and negative reflections at the circuit breaker terminal and at the fault point, respectively. The superposition of these travelling waves gives the voltage profiles on the line shown on Figures 6 to 14 with, on the horizontal axis, the circuit breaker terminal position on the left and the short-circuit point on the right.\n\nThe voltage profile is given at different instants after current interruption, where T is time needed for a wave to travel from the circuit breaker down the line and back to the circuit breaker terminal.\n\nFigure 15 shows,as function of time, the variation of voltage on the line-side terminal of the circuit breaker. The voltage variation is two times the initial voltage if losses are neglected, in reality it is approximately 1.6 times the initial voltage. The triangular waveshape of voltage on the line-side terminal, combined with a supply-side voltage variation at a lower frequency, produces the sawtooth variation of TRV shown on Figure 5.\n\nA short-line-fault TRV is characterized by a rate-of-rise that is proportional to the slope of current at the time of interruption and therefore to the amplitude of the short-circuit current : formula_1, where \"Z\" is the surge impedance of the line.\n\nThe standardized value of \"Z\" is 450 Ω, it is equal to formula_2, where \"l\" and \"c\" are the line self-inductance and capacitance per unit length.\n\n\n"}
{"id": "34578727", "url": "https://en.wikipedia.org/wiki?curid=34578727", "title": "Variable renewable energy", "text": "Variable renewable energy\n\nVariable renewable energy (VRE) is a renewable energy source that is non-dispatchable due to its fluctuating nature, like wind power and solar power, as opposed to a controllable renewable energy source such as hydroelectricity, or biomass, or a relatively constant source such as geothermal power or run-of-the-river hydroelectricity.\n\nConventional hydroelectricity, biomass and geothermal are completely dispatchable as each has a store of potential energy; wind and solar production are typically without storage and can be decreased, but not dispatched, other than when nature provides. Between wind and solar, solar has a more variable daily cycle than wind, but is more predictable in daylight hours than wind. Like solar, tidal energy varies between on and off cycles through each day, unlike solar there is no intermittentcy, tides are available every day without fail. Biofuel and biomass involve multiple steps in the production of energy – growing plants, harvesting, processing, transportation, storage and burning to create heat for electricity, transportation or space heating. In the combined power plant used by the University of Kassel to simulate using 100% renewable energy, wind farms and solar farms were supplemented as needed by hydrostorage and biomass to follow the electricity demand.\n\nWind power is the least predictable of all of the variable renewable energy sources. Grid operators use day ahead forecasting to determine which of the available power sources to use the next day, and weather forecasting is used to predict the likely wind power and solar power output available. The correlation between wind output and prediction can be relatively high, with an average uncorrected error of 8.8% in Germany over a two-year period. The variability of wind power can be seen as one of its defining characteristics.\n\nWaves are primarily created by wind, so the power available from waves tends to follow that available from wind, but due to the mass of the water is less variable than wind power. Wind power is proportional to the cube of the wind speed, while wave power is proportional to the square of the wave height.\n\nSolar power is more predictable than wind power and less variable – while there is never any solar power available during the night, and there is a reduction in winter, the only unknown factors in predicting solar output each day is cloud cover, frost and snow. Many days in a row in some locations are relatively cloud free, just as many days in a row in either the same or other locations are overcast – leading to relatively high predictability. Wind comes from the uneven heating of the earth's surface, and can provide about 1% of the potential energy that is available from solar power. 86,000 TW of solar energy reaches the surface of the world vs. 870 TW in all of the world's winds. Total world demand is roughly 12 TW, many times less than the amount that could be generated from potential wind and solar resources. From 40 to 85 TW could be provided from wind and about 580 TW from solar.\n\nTidal power is the most predictable of all the variable renewable energy sources. Twice a day the tides vary 100%, but they are never intermittent, on the contrary they are completely reliable. It is estimated that Britain could obtain 20% of energy from tidal power, only 20 sites in the world have yet been identified as possible tidal power stations.\n\nIn many European counties and North America the environmental movement has eliminated the construction of dams with large reservoirs. Run of the river projects have continued to be built, such as the 695MW Keeyask Project in Canada which began construction in 2014. The absence of a reservoir results in both seasonal and annual variations in electricity generated.\n\nHistorically grid operators use day ahead forecasting to choose which power stations to make up demand each hour of the next day, and adjust this forecast at intervals as short as hourly or even every fifteen minutes to accommodate any changes. Typically only a small fraction of the total demand is provided as spinning reserve.\n\nSome projections suggest that by 2030 almost all energy could come from non-dispatchable sources – how much wind or solar power is available depends on the weather conditions, and instead of turning on and off available sources becomes one of either storing or transmission of those sources to when they can be used or to where they can be used. Some excess available energy can be diverted to hydrogen production for use in ships and airplanes, a relatively long term energy storage, in a world where almost all of our energy comes from wind, water, and solar (WWS). Hydrogen is not an energy source, but is a storage medium. A cost analysis will need to be made between long distance transmission and excess capacity. The sun is always shining somewhere, and the wind is always blowing somewhere on the Earth, but is it cost effective to bring solar power from Australia to New York?\n\nIf excess capacity is created, the cost is increased because not all of the available output is used. For example, ERCOT predicts that 8.7% of nameplate wind capacity will be reliably available in summer – so if Texas, which has a peak summer demand of 68,379 MW built wind farms of \n786,000 MW (68,379/0.087), they would generate, at a 35% capacity factor, 2.4 million MWh per year – four times use, but might be sufficient to meet summer peaks. In practice it is likely that there are times with almost no wind in the entire region, making this not a practical solution. There were 54 days in 2002 when there was little wind power available in Denmark. The estimated wind power installed capacity potential for Texas, using 100 meter wind turbines at 35% capacity factor, is 1,757,355.6 MW. In locations like British Columbia, with abundant water power resources, water power can always make up any shortfall in wind power.\n\nWind and solar are somewhat complementary. A comparison of the output of the solar panels and the wind turbine at the Massachusetts Maritime Academy shows the effect. In winter there tends to be more wind and less solar, and in summer more solar and less wind, and during the day more solar and less wind. There is always no solar at night, and there is often more wind at night than during the day, so solar can be used somewhat to fill in the peak demand in the day, and wind can supply much of the demand during the night. There is however a substantial need for storage and transmission to fill in the gaps between demand and supply.\n\nAs physicist Amory Lovins has said:\n\nThe variability of sun, wind and so on, turns out to be a non-problem if you do several sensible things. One is to diversify your renewables by technology, so that weather conditions bad for one kind are good for another. Second, you diversify by site so they're not all subject to the same weather pattern at the same time because they're in the same place. Third, you use standard weather forecasting techniques to forecast wind, sun and rain, and of course hydro operators do this right now. Fourth, you integrate all your resources — supply side and demand side...\"\nThe combination of diversifying variable renewables by \"type\" and \"location\", \"forecasting\" their variation, and \"integrating\" them with despatchable renewables, flexible fueled generators, and demand response can create a power system that has the potential to meet our needs reliably. Integrating ever-higher levels of renewables is being successfully demonstrated in the real world:\n\nMark A. Delucchi and Mark Z. Jacobson identify seven ways to design and operate variable renewable energy systems so that they will reliably satisfy electricity demand:\n\n\nJacobson and Delucchi say that wind, water and solar power can be scaled up in cost-effective ways to meet our energy demands, freeing us from dependence on both fossil fuels and nuclear power. In 2009 they published “A Plan to Power 100 Percent of the Planet With Renewables” in \"Scientific American\". A more detailed and updated technical analysis has been published as a two-part article in the refereed journal \"Energy Policy\".\n\nAn article by Kroposki, et al. discuses the technical challenges and solutions to operating electric power systems with extremely high levels of variable renewable energy in IEEE Power and Energy Magazine. This article explains that there are important physical differences between power grids that are dominated by power electronic based sources such as wind and solar and conventional power grids based on synchronous generators. These systems must be properly design to address grid stability and reliability.\n\nRenewable energy is naturally replenished and renewable power technologies increase energy security because they reduce dependence on foreign sources of fuel. Unlike power stations relying on uranium and recycled plutonium for fuel, they are not subject to the volatility of global fuel markets. Renewable power decentralises electricity supply and so minimises the need to produce, transport and store hazardous fuels; reliability of power generation is improved by producing power close to the energy consumer. An accidental or intentional outage affects a smaller amount of capacity than an outage at a larger power station.\n\nThe International Energy Agency says that there has been too much attention on issue of the variability of renewable electricity production. The issue of intermittent supply applies to popular renewable technologies, mainly wind power and solar photovoltaics, and its significance depends on a range of factors which include the market penetration of the renewables concerned, the balance of plant and the wider connectivity of the system, as well as the demand side flexibility. Variability will rarely be a barrier to increased renewable energy deployment when dispatchable generation is also available. But at high levels of market penetration it requires careful analysis and management, and additional costs may be required for back-up or system modification. Renewable electricity supply in the 20-50+% penetration range has already been implemented in several European systems, albeit in the context of an integrated European grid system:\n\nIn 2011, the Intergovernmental Panel on Climate Change, the world's leading climate researchers selected by the United Nations, said \"as infrastructure and energy systems develop, in spite of the complexities, there are few, if any, fundamental technological limits to integrating a portfolio of renewable energy technologies to meet a majority share of total energy demand in locations where suitable renewable resources exist or can be supplied\". IPCC scenarios \"generally indicate that growth in renewable energy will be widespread around the world\". The IPCC said that if governments were supportive, and the full complement of renewable energy technologies were deployed, renewable energy supply could account for almost 80% of the world's energy use within forty years. Rajendra Pachauri, chairman of the IPCC, said the necessary investment in renewables would cost only about 1% of global GDP annually. This approach could contain greenhouse gas levels to less than 450 parts per million, the safe level beyond which climate change becomes catastrophic and irreversible.\n\n"}
{"id": "49596488", "url": "https://en.wikipedia.org/wiki?curid=49596488", "title": "Wase Wind", "text": "Wase Wind\n\nWase Wind is a Flemish energy cooperative, active in the production of renewable energy and the supply of electricity from windturbines.\n\nIts headquarters is in the community of Sint-Gillis-Waas, Belgium).\n\nWase Wind is a member of the Flemish union of renewable energy cooperatives REScoop Vlaanderen.\n\nAs a cooperative company, it unites the contributions of its shareholders and invests in projects for the realization of renewable energy production, via windturbines in the region of Waasland. \n\nThe company was created in 2001, on the initiative of Kris Aper, Chris Derde, Geert De Roover, and Raf Vermeulen.\n\nThe cooperative has invested in 4 wind parks in the Waasland, which together produce clean energy for around 2000 members (shareholders / cooperants).\n"}
