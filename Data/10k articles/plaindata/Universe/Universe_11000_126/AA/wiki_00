{"id": "47836404", "url": "https://en.wikipedia.org/wiki?curid=47836404", "title": "20-Hydroxyeicosatetraenoic acid", "text": "20-Hydroxyeicosatetraenoic acid\n\n20-Hydroxyeicosatetraenoic acid, also known as 20-HETE or 20-hydroxy-5\"Z\",8\"Z\",11\"Z\",14\"Z\"-eicosatetraenoic acid, is an eicosanoid metabolite of arachidonic acid that has a wide range of effects on the vascular system including the regulation of vascular tone, blood flow to specific organs, sodium and fluid transport in the kidney, and vascular pathway remodeling. These vascular and kidney effects of 20-HETE have been shown to be responsible for regulating blood pressure and blood flow to specific organs in rodents; genetic and preclinical studies suggest that 20-HETE may similarly regulate blood pressure and contribute to the development of stroke and heart attacks. Additionally the loss of its production appears to be one cause of the human neurological disease, Hereditary spastic paraplegia. Preclinical studies also suggest that the overproduction of 20-HETE may contribute to the progression of certain human cancers, particularly those of the breast.\n\nA subset of Cytochrome P450 (CYP450) microsome-bound ω-hydroxylases, the Cytochrome P450 omega hydroxylases, metabolize arachidonic acid to 20-HETE by an omega oxidation reaction. CYP450 enzymes belong to a superfamily which in humans is composed of at least 57 members and in mice at least 120 members. Among this superfamily, certain members of the CYP4A and CYP4F subfamilies in the CYP4 family are considered predominant cytochrome P450 enzymes that are responsible in most tissues for forming 20-HETE and, concurrently, smaller amounts of 19-hydroxy-5\"Z\",8\"Z\",11\"Z\",14\"Z\"-eicosatetraenoic acid (19-HETE). However, CYP2U1 may also contribute to the production of these two HETEs and CYP4F8 can metabolize arachidonic acid to 19-HETE while forming little or no 20-HETE.\n\nThe production of 19-HETE with 20-HETE may be significant since 19(\"R\")-HETE, although not its stereoisomer, 19(\"S\")-HETE, inhibits the action of 20-HETE on vascular endothelial cells. Based on studies analyzing the production of other HETEs by CYP enzymes, the production of 19-HETE by these enzymes may include both its \"R\" and \"S\" stereoisomers.\n\nIn humans, the CYP4 ω-hydroxylases include CYP4A11, CYP4F2, and CYP4F3 with the predominant 20-HETE-synthesizing enzymes being CYP4F2, which is the major 20-HETE producing enzyme in the human kidney, followed by CYP4A11. CYP4F3 is expressed as two distinct enzymes, CYP4F3A and CYP4F3B, due to alternative splicing of a single pre-mRNA precursor molecule; CYP4F3A is mostly expressed in leukocytes, CYP4F3B mostly in the liver. Human CYP4Z1, which is expressed in a limited range of tissues such as human breast and ovary, may also metabolize arachidonic acid to 20-HETE while human CYP4A22, once considered as contributing to 20-HETE production, is now regarded as being metabolically inactive. Finally, CYP2U1, the only member of the human CYP2U subfamily, is highly expressed in brain and thymus and to lesser extents in numerous other tissues such as kidney, lung and heart. CYP2U1 protein is also highly expressed, compared to several other cytochrome P450 enzymes, in malignant breast tissue; the MCF-7 human breast cancer cell line express messenger RNA for this cytochrome.\n\nIn mice, the only 20-HETE- and 19-HETE-producing enzymes of the Cyp4a subfamily are two extensively homologous ones, Cyp4a12a and Cyp4a12b; Cyp4a12a is expressed in the male kidney in an androgen hormone-dependent manner. In rats, Cyp4a1, Cyp4a2, Cyp4a3, and Cyp4a8 make 20-HETE. The tissue distribution of these enzymes differs from those of humans making extrapolations from rodent studies to humans somewhat complicated.\n\nMouse CYP2J9, rat CYP2J3, and sheep CYP2J metabolize arachidonic acid primarily to 19-HETE but also to smaller amounts of 20-HETE, and, in the case of the sheep enzyme, 18-HETE; human CYP2J2, however, is an epoxygenase, metabolizing arachidonic acid to epoxide products.\n\nMany agents stimulate cells and tissues to produce 20-HETE in vitro and in vivo. Androgens are particularly potent stimulators of this production. Other stimulators include the powerful vasoconstriction-inducing agents, angiotensin II, endothelins, and alpha adrenergic compounds (e.g. norepinephrine).\n\nNitric oxide, carbon monoxide, and superoxide inhibit 20-HETE production; these non-pharmacological agents do so by binding to the Heme binding site of the 20-HETE producing cytochrome p450 enzymes. Drugs that are substrates for the UGT enzymes which metabolize 20-HETE such as non-steroidal anti-inflammatory agents, opioids, gemfibrozil, Lasix, propanol, and various COX-2 inhibitors may act as perhaps unwanted side effects to increase the levels of 20-HETE. There are a variety of pharmacological agents which inhibit the synthesis of 20-HETE including various fatty acid analogs that compete reversibly with arachidonic acid for the substrate binding site in the CYP enzymes and benzene-based drugs.\n\nThe cytochrome ω-oxidases including those belonging to the CYP4A and CYP4F sub-families and CYPU21 hydroxylate not only arachidonic acid but also various shorter chain (e.g. lauric acid) and/or longer chain (e.g. docosahexaenoic acid) fatty acids. They can also ω-hydroxylate and thereby reduce the activity of various fatty acid metabolites (e.g. LTB4, 5-HETE, 5-oxo-eicosatetraenoic acid, 12-HETE, and several prostaglandins) that regulate inflammation, vascular responses, and other reactions. This metabolism-induced inactivation may underlie the proposed roles of the cytochromes in dampening inflammatory responses and the reported associations of certain CYP4F2 and CYP4F3 single nucleotide variants with human Krohn's disease and Coeliac disease, respectively. While many of the effects and diseases associated with the over- or under-expression, pharmacological inhibition, and single nucleotide or mutant variants of the cytochrome ω-hydroxylases have been attributed to their impact on 20-HETE production, the influence of these alternate metabolic actions have frequently not been defined.\n\nGlucuronidation of 20-HETE by UDP-glucuronosyltransferases (UGTs) is thought to be a primary pathway of 20-HETE elimination and thereby inactivation in humans.\n\nThere are several other pathways that metabolize 20-HETE. Human platelets and other tissues metabolize it via cyclooxygenase(s) to form the 20-hydroxy analogs of prostaglandin G2, thromboxane A2, thromboxane B2 and to 11(\"R\")-hydroperoxy,20-hydroxy-5\"Z\",8\"Z\",11\"Z\",14\"Z\"-eicosatetraenoic acid which is rapidly reduced to 11,20-dihydroxy-5\"Z\",8\"Z\",11\"Z\",14\"Z\"-eicosatetraenoic acid; they also metabolize it through 12-lipoxygenase to form 12(\"S\")-hydroperoxy,20-hydroxy-5\"Z\",8\"Z\",101\"E\",14\"Z\"-eicosatetraenoic acid which is also rapidly reduced to 12,20-dihydroxy-5\"Z\",8\"Z\",101\"E\",14\"Z\"-eicosatetraenoic acid. (The chirality of the hydroperoxy and hydroxyl residues at positions 11 and 12 in the eicosatetraenoic acids are predicted based on studies defining the chirality of the arachdionic metabolites made by these enzymes.) Since the prostaglandin and thromboxane metabolites of 20-HETE lack the platelet-stimulating activities to their prostaglandin and thromboxane precursors and since the 12-hydroxy and 11-hydroxy metabolites of 20-HETE may also be inactive, these metabolic pathways appear to function in inactivating 20-HETE with respect to the platelet system. However, the 20-hydroxy prostaglandin metabolites are able to contract rat aorta rings and thereby could contribute to the hypertensive actions of 20-HETE.\n\nCultured smooth muscle and endothelial cells from mouse brain microvasculature oxidize 20-HETE to its 20-carboxy analog, 20-carboxy- 5\"Z\",8\"Z\",11\"Z\",14\"Z\"-eicosatetraenoic acid, then to 18-carboxy-5\"Z\",8\"Z\",10\"Z\",14\"Z\"-octadecatetraenoic acid, and then to the further chain-shortened dicarboxylic acid, 16-carboxy-5\"Z\",8\"Z\",10\"E\"-hexadecatrrenoic acid, in a series of Beta oxidation reactions. These shortening pathways also are likely to serve in inactivating 20-HETE, although the initial product of this shortening pathway, 20-carboxy-HETE, dilates coronary microvessels in the pig heart and thereby could serve to antagonize the vasoconstrictor actions of 20-HETE, at least in this organ and species. Coronary artery endothelial cells isolated from pigs incorporate 20-HETE primarily into the sn-2 position of phospholipids through a coenzyme A-dependent process. It is likely, although not yet shown, that these mouse and pig 20-HETE metabolizing pathways also occur in humans.\n\n20-HETE-synthesizng enzymes are widely distributed to liver, kidney, brain, lung, intestine and blood vessels. In most vascular systems, 20-HETE synthesizing activity is limited to vascular smooth muscle of small blood vessels with little or no such activity in the vessel's endothelial cells or in large blood vessels. However, both the smooth muscle and endothelial cells obtained from mouse brain microvasculature, produce 20-HETE in culture.\n\n20-HETE is produced by human neutrophils and platelets and by the ascending tubule cells in the medulla as well the pre-glomerular arterioles and certain other localized areas of the rabbit kidney.\n\nIn various rodent models, 20-HETE, at low concentrations (<50 nanomolar), acts to constrict arteries by sensitizing (i.e. increasing) the contraction responses of these artery's smooth muscle cells to other contracting agents such as alpha adrenergic agonists, vasopressin, endothelin, and a product of renin angiotensin system, angiotensin II. 20-HETE has a particularly complex interaction with the renin angiotensin system: angiotensin II stimulates the preglomerular microvessels of the rat kidney to produce 20-HETE; this production is required for angiotensin II to exert its full constrictor effects; and 20-HETE induces transcription of the enzyme which converts angiotensin I to angiotensin II, i.e. angiotensin-converting enzyme. Other agents such as Androgens and alpha adrenergic compounds such as norepinephrine. likewise stimulate 20-HETE production and have vasoconstrictive actions which are enhanced by 20-HETE. These circular or positive feedback interactions may serve to perpetuate vasoconstrictor responses.\n\nAgain in rodent models, 20-HETE acts to block Calcium-activated potassium channels to promote the entry of ionic calcium into vascular smooth muscle cells through the L-type calcium channel; the attendant rise in intracellular calcium triggers these muscles to contract.\n\nStudies in rats also indicate that in vascular endothelial cells 20-HETE inhibits the association of the nitric oxide-producing enzyme, endothelial nitric oxide synthase (eNOS) with heat shock protein 90; this inhibits the ability of eNOS to become activated. The endothelial cells become dysfunctional in exhibiting decreased ability to produce the vasodilating agent, nitric oxide, and in containing elevated levels of a potentially injurious oxygen radical, superoxide anion; the blood vessels to which these dysfunctional endothelial cells belong are less able to dilate in response to the vasodilator, acetylcholine.\n\n20-HETE can also constrict rodent (and human) artery preparations by directly activating the receptor for thromboxane A2. While significantly less potent than thromboxane A2 in activating this receptor, studies on rat and human cerebral artery preparations indicate that increased blood flow through these arteries triggers production of 20-HETE which in turn binds to thromboxane receptors to constrict these vessels and thereby reduce their blood blow. Acting in the latter capacity, 20-HETE, it is proposed, functions as a mediator regulating blood flow to the brain.\n\nThese vasoconstrictor effects of 20-HETE can reduce blood flow to specific parts of the body, not only to brain (see previous paragraph) but also to kidney, liver, heart and other organs, as well as to portions of these organs; they can also contribute to systemic hypertension as well as to the physiological and pathological effects of thromboxane receptor-activation .\n\nSprague Dawley rats that underwent balloon injury of the common carotid artery exhibited elevated levels of CYP4A enzyme immunostaining in the smooth muscle of the injured arteries as well as elevated levels of 20-HETE in the injured arteries. Inhibition of 20-HETE production by two different agents greatly reduced the vascular intima hyperplasia and vascular remodeling that occurred after balloon injury. The studies suggest that the increase in expression of CYP4A and production of 20-HETE contribute to vascular intima growth, remolding, and thereby healing of injured rat carotid arteries.\n\nIn the C57BL/6 mouse laboratory model, 20-HETE pretreatment accelerates the development of thrombosis and reduces blood flow caused by the Thrombosis-inducing agent, ferric chloride, in the common carotid and femoral arteries; companion studies on human umbilical vein endothelial cells indicate that 20-HETE stimulates the activation of Extracellular signal-regulated kinases to cause ERK-dependent and L-type calcium channel-dependent release of von Willebrand factor which in turn stimulates the adhesion of platelets to the endothelial cells. The endothelial, platelet, and pro-clotting actions of 20-HETE may contribute to its ability to disrupt blood flood to tissues.\n\nIn animal models, 20-HETE stimulates the activation of protein kinase C in the epithelial cells of the proximal tubules of the kidney; the kinase then phosphorylates and thereby inhibits the Na+/K+-ATPase and concurrently also blocks the Na-K-Cl cotransporter and 70 pS K+ channel in the thick Ascending limb of loop of Henle (TALH); these effects reduce the absorption of sodium and fluids in the nephron and thereby tend to reduce blood pressure.\n\nAs indicated above, 20-HETE may raise blood pressure by constricting arterial blood vessels but also may lower blood pressure by promoting the loss of sodium and fluids in the kidneys. The effects of 20-HETE therefore are complex, as indicated in studies of the following animal models. Many of these models appear relevant to hypertension in humans in that they parallel the human disease, i.e. men have higher rates of hypertension than women, and women with increased levels of androgens (e.g. postmenopausal women and women with polycystic ovarian disease) and higher rates of hypertension.\n\nSpontaneously hypertensive rats exhibit elevated levels of CYP4A2 and 20-HETE; blockade of 20-HETE production lowers blood pressure in this model. The effect is particularly well seen in female rats: aging post-menopausal but not pre-menopausal female spontaneously hypertensive rats exhibit highly significant falls in blood pressure when treated with non-selective or selective inhibitors of CYP-induced 20-HETE production.\n\nDahl salt-sensitive rats develop hypertension that develops more quickly and exacerbated by high intake of salt (sodium chloride) and ameliorated by low salt intake. In this model, rats exhibit an up-regulated CYP4A/20-HETE pathway within their cerebral vasculature and vascular endothelial cell overproduction of reactive oxygen species that in turn stimulates the CYp4A/20-HETE pathway. Non-selective and non-selective inhibitors of CYP4A and 20-HETE production reduce hypertension in this model. The hypertension in this model is more severe in male rats and appears to be mediated at least in part by vasopressin, the renin-angiotensin system, and androgens.\n\nLewis rats (see Laboratory rat models) that had one kidney removed and then fed a high salt diet are hypertensive. Kidney medullary interstitial infusion of an inhibitor of 20-HETE production reduced the formation of 20-HETE in the outer medulla of the infused kidney, had no effect on the production of 20-HETE in the cortex of the infused kidney, and produced a mean arterial pressure rise from 115 at baseline to 142 mm of mercury; this study indicates that the hypertensive versus hypotensive effects of 20-HETE depend not only on the organ of its production but also, with respect to the kidney, the site within the organ where it is produced.\n\nAndrogen treatment causes hypertension in normal male and female rats; this hypertensive response is greatly reduced by diverse inhibitors of Cyp4a and 20-HETE production.\n\nCyp4a12-transgenic mice overexpressing Cyp4a12 develop androgen-independent hypertension that is associated with increased levels of 20-HETE; this hypertension is fully reversible by treatment with a Cyp4a selective inhibitor of 20-HETE production.\n\nMice depleted of Cyp4a14 by gene knockout (Cyp4a14(-/-) mice develop male-specific, androgen-dependent hypertension. This seemingly paradoxical result is due to the overexpression of Cyp4a12a; the knockout of Cyp4a14 (Cyp4a14 does not produce 20-HETE) leads to the overexpression of the 20-HETE-producing cytochrome, Cyp4a149(-/-), and consequent overproduction of 20-HETE. The model involves increased plasma androgens, increased vascular and urinary levels of 20-HETE, relief of hypertension by castration, and hypertension which is driven by excessive fluid reabsorption in the kidney's proximal tubule secondary to the overexpression of Sodium–hydrogen antiporter 3; these effects are presumed but not yet shown to be due to the overproduction of 20-HETE. The Cyp4a12-transgenic model (above) is referred to in support of this presumption.\n\nMice depleted of Cyp4a10 maintain normal blood pressure on a low salt diet but become hypertensive on normal or high salt diets; this paradoxical result appears due to a decrease in kidney levels of Cyp2C44 caused by the loss of Cyp4a10. Cyp2C44 metabolizes arachidonic acid a family of vasodilation-inducing and anti-hypertensive products, the Epoxyeicosatrienoic acids (EETs). The model involves normal levels of 20-HETE, reduced expression of Cyp2c44, reduced levels of EETs, and deficiencies in kidney tubule absorption of sodium regulated by EETs, and the normalization of hypertensive blood pressure by increasing expression of Cyp2c44 by treating the mice with an inducer of its expression, an activator of PPARα.\n\n20-HETE activates the mouse and human transient receptor potential cation channel subfamily V member 1 (TRPV1, also known as the capsaicin receptor and the vanilloid receptor 1), and through this receptor, cultured dorsal root ganglion cells taken from mice.\n\nHuman CYP4A11 has 72.69% amino acid identity with murine cyp4a14 and 73.02% identity with murine cyp4a10 suggesting that it plays a role in humans similar to that of cyp4a14 and/or cyp4a10 in mice. The association of hypertension with defective CYP4A11 in humans as indicated below seems to parallel the hypertension associated with Cyp4a14 gene knockout in mice (see above section on genetic models).\n\nThe gene polymorphism rs1126742 variant of CYP4A11 switches thymidine to cytosine at nucleotide 8590 [T8590C] and leads to a phenylalanine-to-serine substitution at amino acid 434); this F434S variant has significantly reduced ability to ω-oxidize arachidonic acid to 20-HETE and has been associated with essential hypertension in: 512 white males from Tennessee (Odds ratio=2.31); 1538 males and females from the Framingham Heart Study (Odds ratio=1.23); males but not females in 732 black Americans with hypertensive renal disease participating in the African American Study of Kidney Disease; males in a sample of 507 individuals in Japan and in the third MONICA (MONitoring trends and determinants In Cardiovascular disease survey of 1397 individuals the homozygous C8590C genotype to the homozygous T8590T genotype with odds ratios of 3.31 for all subjects, 4.30 for males 2.93 for women);\n\nA study of 1501 participants recruited from the Tanno-Sobetsu Study found that the variant -845G in the promoter region of CYP411 (−845A is the predominant genotype) is associated with reduced transcription of CYP411 as well as with hypertension (odds ratio of homozygous and heterozygous -845G genotype versus homozygous -845A was 1.42);\n\nA haplotype tagging single-nucleotide polymorphism (SNP) (see Tag SNP) variant of CYP4A11, C296T (cytosine to thymine at position 296), was associated with a significantly increased risk of ischemic stroke (adjusted odds ratio of 1.50 in comparing homozygous and heterozygous C296T subjects to homozygous C286C subjects) in >2000 individuals taken from the Han Chinese population. The effect of the −296C>T single base pair substitution on baseline CYP411 transcriptional activity was not significant, suggesting that this polymorphism may not be the causal variant but instead may be in linkage disequilibrium with the causal variant. Regardless, this SNP may serve as a genetic marker for large vessel disease stroke risk in this population.\n\nThe G1347A variant of CYP4F2 produces an enzyme with methionine in place of valium at position 433 (Val433Met; single nucleotide variant rs2108622); the variant enzyme has reduce capacity to metabolize arachidonic acid to 20-HETE but increased urinary excretion of 20-HETE. Studies found that: a) among 161 hypertensive and 74 normotensive subjects in Australia, the incidence of the Val433Met variant was significantly increased in the hypertensive subjects; b) among a large number of Swedish patients enrolled and monitored over 10 years in the cardiovascular cohort of the Malmö Diet and Cancer Study only males with this variant exhibited hypertension; c) among several hundred subjects in India, the variant was associated with hypertension; and d) in comparing 249 patients with hypertension to 238 age-matched controls in Japan, the variant was not associated with hypertension. The maintenance of the lower blood pressure that followed diet-induced weight loss was more difficult for carriers of the Val433Met variant and may be related to increased arterial stiffness and increased 20-HETE synthesis.\n\nThe Val433Met variant is also associated with an increased incidence of cerebral infarction (i.e. ischemic stroke) in a study of 175 subjects with infarction compared to 246 control subjects in Japan, in 507 stroke patients compared to 487 age- and sex-matched 487 controls in India, and in males but not females in a study of 558 patients compared to 557 controls in China. The variant is associated with myocardial infarction in a study of 507 patients compared to 487 age- and sex-matched controls in India, in males but not females in a study of 234 patients compared to 248 control subjects in Japan, and in male but not female patients in Sweden enrolled in the cardiovascular cohort of the Malmo Diet and Cancer Study. The incidences of cerebral and myocardial infarction in these studies appears to be independent of hypertension. (The platelets of individuals heterozygous or homozygous for the Val433Met variant exhibit increased platelet aggregation responses to epinephrine. This platelet hyper-responsiveness to epinephrine, particularly if also exhibited to other platelet-aggregating agents, could contribute to cerebral and coronary infarctions.)\n\nThe Single-nucleotide polymorphism rs1558139 guanine to cytosine variant in an intron of CYP4F2 is associated with essential hypertension in men only in a study of 249 hypertensive versus 238 age-matched controls in Japan. The impact of this variant on CYP4F2 expression is not known.\n\nThe single-nucleotide polymorphism rs2108622 located in exon 11 of CYP4F2 encodes for an enzyme with reduced ability to metabolize arachidonic acid to 20-HETE; male but not female bearers of this CYP4F2 \"G allele\" have an increased incidence of cerebral infarction base on an analysis of 175 subjects and 246 controls.\n\nA mutation (c.947A>T) in CYP2U1 has been associated with a small number of patients with Hereditary spastic paraplegia in that it segregates with the disease at the homozygous state in two afflicted families. The mutation affects an amino acid (p.Asp316Val) highly conserved among CYP2U1 orthologs as well as other cytochrome P450 proteins; the p.Asp314Val mutation is located in the enzyme's functional domain, is predicted to be damaging to the enzyme's activity, and is associated with mitochondria dysfunction. A second homozygous enzyme-disabling mutation has been identified in CYP2U1, c.1A>C/p.Met1?, that is associated with <1% of hereditary spastic paraplegia sufferers. While the role of 20-HETE in these mutations has not been established, the reduction in 20-HETE production and thereby 20-HETE's activation of the TRPV1 receptor in nerve tissues, it is hypothesized, may contribute to the disease.\n\nTwo human breast cancer cell lines, T47D and BT-474, made to overexpress CYP4Z1 by transfection overexpress messenger RNA for and overproduce vascular endothelial growth factor A while under expressing message and protein for tissue inhibitor of metalloproteinase-2. T47D cells that overexpress CYP4Z1 also overproduce 20-HETE and when ransplanted into athymic Balb/c mice show a greater increase in tumor weight and vascularity compared to control T47D cells; these increases are prevented by an inhibitor of 20-HETE production. Isoliquiritigenin, a proposed drug for treating cancer, cause cultured MDA-MB-231 and MCF-7 human breast cancer cells to die by triggering apoptosis. Among its many other effects, the drug caused these cells to decrease their levels of 20-HETE in vitro; the addition of 20-HETE to these cultures rescued the cells from apoptosis. Isoliquiritigenin also inhibits the in vivo lung metastasis of MDA-MB-231 cell transplants while concurrently decreasing the tumor's levels of 20-HETE. The growth of MDA-MB-231 cells implanted into athymic nude female mice as well as the cells' production of a large variety of agents stimulating vascularization including vascular endothelial growth factor were inhibited by treating the mice with an inhibitor of 20-HETE production.\n\nMessenger RNAs not only for CYP4Z2 but also for CYP4A11, CYP4A22, CYP4F2, and CYP4F3 are more highly expressed in samples of human breast cancer tumors compared to normal breast tissue. The Three prime untranslated regions (3'UTRs) of the CYP4Z1 gene and its Pseudogene, CYP4Z2P, share several miRNA-binding sites, including those for miR-211, miR-125a-3p, miR-197, miR-1226, and miR-204'. Since these miRNA's reduce the translation of CYP4Z1, the expression of CYP4Z2P can bind these miRNAs to reduce their interference with CYP4Z1 and thereby increase the production of CYP4Z1 protein and perhaps 20-HETE; indeed, force expression of these 3'UTRs promoted in vitro tumor angiogenesis in breast cancer cells partly via miRNA-dependent activation of the phosphoinositide 3-kinase-MAPK/ERK pathway and thereby stimulating the production of vascular endothelium growth factor and possibly other endothelium growth factors. Taken together, these pre-clinical studies suggest that 20-HETE made by one or more of the cited cytochrome P450 enzymes may contribute to the progression of breast cancer by promoting its survival, growth, and vascular endothelial growth factor-induced neovascularization.\n\n20-HETE stimulated the proliferation of cultured human brain Glioma cell line U251 and, when forced to overexpress CYP4Z1 by gene transfection, overproduced 20-HETE and exhibited a dramatically increased rate of growth that was blocked by inhibiting the cells from producing 20-HETE. A similar set of findings was found with human non-small cell lung cancer cells. A selective inhibitor of 20-HETE synthesis and a 20-HETE antagonist reduced the growth of two human kidney cancer 786-O and 769-P cell lines in culture; the 20-HETE antagonist also inhibited the growth of 786-O cells transplanted into athymic nude mice.\n\nMessenger RNAs for CYP4A11, CYP4A22, CYP4F2, and/or CYP4F3 are more highly expressed in ovary, colon, thyroid, lung, ovary, cancers compared to their normal tissue counterparts; in ovarian cancer, this higher expression is associated with an increased level of CYP4F2 protein expression and an increased ability to metabolize arachidonic acid to 20-HETE. Ovarian cancers also overexpress CYP4Z1 mRNA protein; this overexpression is associated with a poorer disease outcome.\n\nWhile these studies suggest that CYP4A11, CYP4A22, CYP4F2, and/or CYP4F3 produce 20-HETE which in turn promotes the growth of the cited cancers in model systems and therefore may do so in the human cancers, this suggestion clearly needs much further study. For example, an inhibitor of 20-HETE production blocks the growth of human brain U251 glioma cells in culture; since these cells could not be shown to produce 20-HETE, it was proposed that some other metabolite may by the inhibitor's targeted cytochrome enzymes was responsible for maintaining these cells growth. It is also possible that any such inhibitor has off-target effects that are responsible for its actions.\n\n20-HETE inhibits the aggregation of human platelets by competing with arachidonic acid for the enzymes that produce prostaglandin H2 and thromboxane A2. These products are formed in response to platelet stimulation and then act through the thromboxane receptor to mediate and/or promote the ensuing platelet aggregation response to most stimuli. The platelets metabolize 20-HETE to the 20-hydroxy analogs of prostaglandin H2 and thromboxane A2, products that are essentially inactive in platelets, while consequently form less of the arachidonic acid-derived prostaglandin and thromboxane products. In addition, 20-HETE itself blocks prostaglandin and thromboxane metabolites from interacting with the thromboxane receptor. Both effects, i.e. replacement of prostaglandin and thromboxane production with platelet-inactive products and thromboxane A2 receptor blockade, are responsible for 20-HETE's platelet aggregation-inhibiting action. However, the platelet anti-aggregating activity of 20-HETE requires micromolar levels and therefore may be more of a pharmacological than physiological activity.\n\n20-HETE constricts human artery preparations by directly activating the receptor for thromboxane A2. While significantly less potent than thromboxane A2 in activating this receptor, studies on human cerebral artery preparations indicate that increased blood flow through these arteries triggers production of 20-HETE which in turn binds to thromboxane receptors to constrict these vessels and thereby reduce their blood blow. Acting in the latter capacity, 20-HETE, it is proposed, functions as a mediator regulating blood flow to the human brain.\n\nOne study found that 30 patients with the metabolic syndrome exhibited significantly elevated levels of plasma and urinary 20-HETE compared to matched controls; women with the syndrome had particularly higher urinary 20-HETE levels.\n"}
{"id": "25588439", "url": "https://en.wikipedia.org/wiki?curid=25588439", "title": "Agency for the Cooperation of Energy Regulators", "text": "Agency for the Cooperation of Energy Regulators\n\nThe European Agency for the Cooperation of Energy Regulators (ACER) is an Agency of the European Union by the Third Energy Package in 2009. It was established in 2010 and has its seat in Ljubljana, Slovenia.\n\nOn 6 May, the administrative board designated Alberto Pototschnig as its first director. By deciding on their director and chairs, ACER took a step forward towards becoming fully operational.\n\nThe basis for the establishment of ACER is Regulation 713/2009 of the European Parliament and the Council of 13 July 2009. The regulation describes the establishment and legal status, tasks, organisation and financial provisions.\n\nACER is the second intergovernmental organization headquartered in Slovenia. The first was the International Center for Promotion of Enterprises (ICPE).\n\nThe agency:\n\n\n"}
{"id": "7852179", "url": "https://en.wikipedia.org/wiki?curid=7852179", "title": "Agriculture in Senegal", "text": "Agriculture in Senegal\n\nSenegal's economy is mostly driven by agriculture, fisheries, mining, construction and tourism. Most of Senegal lies within the drought-prone Sahel region, with irregular rainfall and generally poor soils. With only about 5 percent of the land irrigated, Senegal continues to rely on rain-fed agriculture, which occupies about 75 percent of the workforce. Despite a relatively wide variety of agricultural production, the majority of farmers produce for subsistence needs. Production is subject to drought and threats of pests such as locusts, birds, fruit flies, and white flies. Millet, rice, corn, and sorghum are the primary food crops grown in Senegal.\n\nSenegal is a net food importer, particularly for rice, which represents almost 75 percent of cereal imports. Peanuts, sugarcane, and cotton are important cash crops, and a wide variety of fruits and vegetables are grown for local and export markets. In 2006 gum arabic exports soared to $280 million, making it by far the leading agricultural export. Green beans, industrial tomato, cherry tomato, melon, and mango are Senegal's main vegetable cash crops. The Casamance region, isolated from the rest of Senegal by Gambia, is an important agriculture producing area, but without the infrastructure or transportation links to improve its capacity.\n\nDespite the lack of modernization of artisanal fishing, the fishing sector remains Senegal's main economic resource and major foreign exchange earner. The livestock and poultry sectors are relatively underdeveloped and have potential for modernization, development and growth. Senegal imports most of its milk and dairy products. The sector is inhibited due to low output and limited investments. The potential production of fauna and forest products is high and diversified and could, if well organized, benefit poor farmers in rural areas. Although the agricultural sector was impacted by a locust invasion in 2004, it has recovered and gross agricultural production is expected to increase by 6 percent in 2006 and 5 percent in 2007.\n\nMost of Senegal lies within the drought-prone Sahel region, with irregular rainfall and generally poor soils. With only about 5% of the land irrigated, Senegal continues to rely on rain-fed agriculture, which occupies about 75% of its workforce. The sector is inhibited due to low output and limited investments. Although this sector was impacted by a locust invasion in 2004, it has recovered and gross agricultural production was expected to increase by 6.1% in 2006 and 5.1% in 2007. Reforms of the agricultural sector have suppressed direct government support and engaged the privatization of state holdings. The main agricultural crops are peanuts and cotton—both being important sources of foreign exchange income—as well as millet, rice, corn, sugarcane, and livestock.\n\nPeanuts are the engine of the rural economy and their production accounts for around 40 percent of cultivated land, taking up two million hectares. The peanut sector provides employment for as many as one million people. The industry has been suffering from the effects of the privatization of the agricultural sector and the elimination of the import ban on peanut and other edible oils. The peanut sector is still dominated by SONACOS, which has been renamed SUNEOR starting January 1, 2007, thus marking the end of the privatization process which started in 2004 when the government decided to sell its shares to Advens, a private consortium including a Lebanese-French businessman, the Belgian peanut machinery manufacturer Desmet, SODEFITEX (the cotton ginning company), and SONACOS employees.\n\nIn recent years, the reported average annual peanut production lies around 828,000 tons (95% for oil)! Cotton accounts for about 3% of total exports and the third source of export earnings for Senegal (some 28 million US dollars over the period 1995-2000). Cotton is grown in nearly every region and covers almost one third of cultivated acreage. The cotton industry is managed through the former parastatal SODEFITEX, which was privatized in November 2003 with producers holding 30% of the company's shares.\n\nProduction of food crops does not meet Senegal's needs. The production of major staple food crops covers barely 30% of consumption needs. Only in years of favorable rainfall does the country approach self-sufficiency in millet and sorghum, the basic staples with rice. The livestock population includes approximately 3.1 million cattle and 8.7 million sheep and goats. Cattle are reared extensively and on a small-scale basis. Poultry production has increased and has great potential for growth. Despite a significant livestock population, Senegal remains a net importer of meat, especially sheep (live) during major holidays and religious events.\n\nPeanut production accounts for around 40 percent of cultivated land, taking up two million hectares, and provides employment for as many as one million people. Although the peanut sector's contribution to foreign exchange earnings has dropped below those of fishing and mining, peanuts continue to play an important role in the overall economy as the main cash crop for many rural Senegalese farmers. Peanuts are processed locally, and prices of processed peanut oil and other peanut products are set a government controlled commission.\n\nProduction of unshelled peanuts varies widely because of periodic drought, and production is frequently underreported because of unauthorized sales to processors in neighboring countries. Total production was estimated at 850,000 tons in 2005. Exports of peanut products reached about CFA 15 billion ($30 million) in 2005. They account for some 60 percent of total agricultural exports, 75 percent of which is made up of peanut oil. SUNEOR's (former SONACOS) exports of peanut oil account for 45 to 50 percent of the world market trade in peanut oil.\n\nSUNEOR produces approximately 150,000 tons of crude peanut oil per year. The European market, which is its main market, can currently absorb only 90,000 tons. The newly privatized company plans to explore and develop new markets to fully utilize its capacity. In this perspective, exports of peanuts oil to the U.S. have resumed in 2006 and were estimated at about $7 million. Other major peanut oil producers include NOVASEN and the Complex of Touba.\n\nAll these three companies produce mainly non-refined peanut oil and non-grilled peanuts for export. Peanut meal/cake is predominantly sold in the local market as animal feed. The local industry also refines imported edible oils for domestic consumption. In 2005 Senegal imported approximately 90,000 tons of crude soybean oil, primarily from Brazil.\n\nCotton is the second largest agricultural export, accounting for around 16 percent of total agricultural exports. It is grown in nearly every region and covers almost one third of cultivated acreage. However, production is concentrated in the South-Eastern part of the country (South of the Kahone–Tambacounda belt, as well as in the Casamance and Kédougou regions).\n\nSODEFITEX, the main cotton company, forecasts production at 40,000 tons in 2006. Cotton accounts for approximately 3% of total exports and the third source of export earnings for Senegal (some $23 million in 2005). Most cotton lint produced in Senegal is exported, but since the liberalization of the sector in 1984, producers have preferred selling in parallel markets, where they benefited from better prices. SODEFITEX, which manages most of Senegal's cotton production, was privatized in November 2003. Producers acquired 30% of the company's shares (they had no equity interest prior to privatization). Despite stronger incentives (credit to producers and guaranteed producer prices), the company is still striving to fully use its ginning capacity.\n\nThe government of Senegal is committed to participating in the U.S. government funded West Africa Cotton Improvement Program (WACIP) in support of activities that focus on crop diversification and value-added processing.\n\nRice, millet and sorghum are the main subsistence food crops for Senegal's rural population. Corn and fonio are also important cereal crops. Production of cereal food crops, such as rice, millet, corn and sorghum - which is often grown in rotation with peanuts - does not meet Senegal's needs. Only in years of good rainfall does the country approach self-sufficiency in millet, corn, sorghum and fonio, the main staples in rural areas. Local production increased significantly in the early 2000s following the government's decision to subsidise fertilizer and encourage corn production, and thus reduce reliance upon peanuts.\n\nIn 2005/06, total production of cereals (including milled rice) is estimated at 1,177,782 MT, which will cover some 60% of the consumption needs. However, given the segmentation of the rice market (see GAIN SG6002), this production will less likely affect imports. However, in years of poor rainfall and other natural disasters, the shortfall in coarse grains, especially millet, could be more difficult to cover because of low availability and trade of this grain in the region. Such constraints have been overcome with an increase in rice imports, with a shift from millet to rice consumption in households who can afford it.\n\nSenegal is the second largest rice importer in Africa, ahead of Côte d'Ivoire and behind Nigeria. Senegal's imports reached 1,113,000 MT in 2005, with net imports estimated at 854,000 MT. Consumers’ preference is for 100 percent broken rice originating from Asia, mainly Thailand and India, and recently from Brazil, Uruguay and Argentina. Per capita rice consumption continues to grow and is estimated at 70 to 75 kilograms and total annual consumption is estimated at 700,000 MT. Local rice production meets about 20 percent of the country's needs and 30 percent of this production is used for subsistence. In 2005/06, local production of rice paddy was estimated at 265,000 MT.\n\nThe wheat sector has been controlled for years by two flourmills, Grands Moulins de Dakar and Sentenac, which buy about 90 percent of their wheat from France. (See SG7002) In 2001, NMA became the country's third flour and feed mill. The demand for wheat flour is increasing, as the demand for bread increases along with population growth and changes in consumption habits. Senegal has imported 326,287 MT of wheat in 2005 and more than half of this quantity in the first half of 2006 (180,514 MT).\n\nSenegal imported United States wheat most recently in 2004 and again in 2006, making the U.S. the third largest supplier after France and Argentina. U.S. wheat is used for blending because of its high protein content compared to French soft wheat. Despite significant increases in the price of wheat in the international markets, the government froze flour and bread prices in November 2006, following strong pressure from consumers’ unions. The millers’ price of flour is currently CFA 264,000 per MT and the price of a baguette remains at CFA 150 instead of CFA 175 proposed by bakers’ associations. ($1 = CFA 507 on January 10, 2007.)\n\nSenegal's total horticultural production was estimated at 584,000 MT in 2004. Exports of fruits and vegetables are growing steady although they remain low, and it is estimated that they will reach approximately 50,000 tons in 2007. Europe is still the main export market for Senegal's fresh fruits and vegetables. About 70% of the European market is dominated by four products including green beans, cherry tomato, mango and melon. With the increase in size and value of the European market, pre-packed produce such as green beans have promising prospects in the European market and with the possibility to introduce first-stage processing, these produce will likely reach other markets. Under AGOA and its related projects, Senegal's horticultural sector is making efforts to enter the U.S and North American markets.\n\nHowever, in order for Senegal to benefit from these opportunities, Senegal needs to address phytosanitary concerns, improve existing value chains (improved ocean transportation of green beans, extension of the market of cherry tomato, increase the competitiveness of melon and expand the seasonality of mango). The fruit and vegetable industry involves about twenty active companies grouped in two federations (ONAPES and SEPAS). Three companies are involved through the whole chain (production, packaging, trade) export more than 50% of the produce alone. About ten medium-size companies export 200 to 500 tons\nand the other are small enterprises usually serve as suppliers to major exporters. A warehouse for fresh produce is built at Dakar's airport, and other infrastructure is being built to improve storage and transportation to Europe and thereby maintain quality and increase value.\n\nThe potential for the production of industrial tomato is high, especially along Senegal River valley. However, the current level of production of double concentrate tomato paste does not meet Senegal's needs estimated at 18,000 tons. In 2003, total production of fresh tomato was estimated at 53,000 tons, which yielded about 8,000 tons of paste representing only about 45% of the domestic needs. SOCAS, the main processing company with the capacity of 15,000 tons, has been importing triple concentrate to cover the deficit (5,000 tons in 2004 and 2,000 in 2005).\n\nAt the same time the imports of double concentrate are regularly increasing from 2,900 tons in 2003 to 5,500 tons in 2004. Agroline, the other major company has been operating since 2003 with a capacity of 3,300 tons of double concentrate, representing 7% of the tomato paste market. Agroline has been using imported or local triple concentrate which it processes and packages into double concentrate. This company is considering extending its market share through the establishment of a new agro-industrial plant in Taredji, northern Senegal which will produce triple concentrate from fresh tomatoes. This project will start in 2007. Farm gate industrial tomato prices and incentives will have to improve for Senegal to produce more of its paste from local tomatoes. The processors face stiff competition from imports of final products such as tomato sauces, juice and ketchup.\n\nThe overall potential of the horticultural sector is limited by the presence of various pests (including fruit and white flies), and therefore needs technical assistance to develop in-country SPS capacity for meeting international standards, and infrastructure to increase the efficiency of surveillance and compliance. Senegal needs also to work with its regional partners to harmonize phytosanitary standards and procedures, strengthen pest surveillance and detection capabilities, including border inspection operations, develop risk assessment capability, and overcome other bottlenecks related to regulatory issues and the trade.\n\nThe production of sugar in Senegal started back in September 1972, when the Compagnie Sucrière Sénégalaise, CSS, produced its first sugar cubes. CSS benefits from a de facto monopoly and subsidies from the government, which maintain its capacity to plant and process sugar cane, then refines and commercialize the sugar produced in the forms of cubes, powder and crystallized sugar. This year, CSS’ production is estimated at 800,000 tons of sugar cane, from which nearly 90,500 tons of sugar is produced. With an average yield of 120 tons/hectare, CSS cultivates 7,500 hectares of commercial cane on the Senegal River valley in northern Senegal. The company employs 3,000 permanent workers and 2,000 seasonal workers.\n\nDuring the period 2002–2005 CSS faced serious competition from illegal imports of cheaper sugar, mainly from Mauritania which grows cane and processes sugar in the same river valley on the other side of the border. These imports were estimated at 30,000 to 40,000 tons. These imports have decreased significantly in 2006 according to CSS authorities with the support of Customs services. CSS’ ambition is to increase its production meet the national consumption level of 150,000 tons of sugar. This will require a production of 923,000 to 1 million tons of cane. CSS is reported to have the processing capacity to reach this level of production but it will have to increase its cultivated area by 500 hectares. Currently CSS imports 33,000 tons of sugar to compensate the deficit. In 2006, these imports costed about $875 thousand to the company.\n\nThe livestock population includes 3.1 million cattle and 8.7 million sheep and goats. Most cattle systems using feed lots. Despite a significant livestock population, Senegal remains a net importer of meat, especially live sheep during periods of peak consumption (major religious holidays and events). The total production of meat was about 100,000 tons in 2003, which is equivalent to a per capita consumption of 11.5 kg below the government's objective of 14 kg.\n\nIn Senegal, the milk industry is primarily based on the use of imported milk powder. Senegal's milk production is far below the domestic needs. Despite relatively high tariffs on milk powder (26.78%), about 20,000 tons of milk powder is imported each year, primarily from Europe. In fluid milk equivalent, imports represent twice the level of local milk production. Imports of other dairy products are estimated at about $100 million in 2006. Importers of powder milk form a strong political lobby and dominate the dairy industry. Local producers are not well organized except the few modern producers in the major cities.\n\nPart of the import milk powder is processed and marketed through informal channels on which little information is reported. The main products available in the market are sweet concentrate milk, unsweetened concentrated milk, milk powder (in bulk or packaged in bottle or small bags). A few companies produce yogurt.\n\nThe local milk production system relies on climatic conditions with higher production during the rainy season and a slow down and even stoppage during the 7-month-long dry season. Non governmental organizations and donors assist small rural milk producers to improve the distribution systems and increase their capacity to access urban markets. In this perspective, PAPEL, the government's main livestock and dairy development project has rehabilitated the rural milk collection network set up by Nestlé-Senegal in 1991 in the sylvo-pastoral zone.\n\nThis project is helping develop small-scale milk processing units with simple equipment and techniques. Most of these units are found in the Northern and Southern parts of the country, particularly in and around Saint-Louis, Dahra, Tambacounda, Velingara and Kolda. In the Niayes zone around Dakar, other well structured milk processing units benefit from this support and were able to commercialize up to 300,000 liters of milk in Dakar in 2005. The most important of these milk farms are the Wayembam farm and the farms of the Regional Association of Women Cattle Breeders, Dirtel. Other major players in the milk market include Nestlé-Senegal, SATREC, CCMB, Saprolait, and Les Mamelles Jaboot.\n\nThe poultry industry has been increasing its overall production since the announcement in 2005 of the ban of imports of chicken meat and despite the shock created in early 2006 by avian influenza. The sector represents 17% of the animal industry's contribution to GDP and employs about 10,000 people.\n\nIn 2003, there were 3.2 millions chicken producing 5,982 tons of meat. Because of massive imports of low quality and cheap chicken parts from Europe and Brazil the sector decreased its production by 24% from 2001 to 2003. This has prompted the creation of poultry farmers' unions who claimed the loss of 3,000 to 5,000 jobs, and the government decision to ban imports of frozen chicken in October 2005. This ban is still underway and applies to all countries. As the result of this ban, local production increased by 21%. However, because the ban was only effective in January 2006, import orders prior to the ban were authorized and in 2004, 13,700 tons of chicken meat were imported.\n\nLocal production is estimated at 7 million chickens in 2005, which represents a 33% increase compared to 2004. Chicken meat production represents about 75% of this production, and total industrial production of chicken meat has increased to 9,200 tons in 2005, representing a 26% increase compared to 2004. Traditional production (home production) is difficult to evaluate but could be estimated at 8,000 tons of meat. Preliminary government reports suggest that these trends will continue in 2006 with significant increase in local production of chicken meat. However, these trends also suggest that the production of eggs will decrease significantly as the result of the competing chicken meat, and professionals fear that the sector might not be able to meet Senegal's needs in chicken eggs, which may prompt a partial lift of the ban.\n\nMost of the inputs of chicken feed are imported. In 2005, about 85,000 MT of chicken feed were produced. Corn accounts for 60% of ingredients. Producers prefer soy and corn products to peanut cakes because of their better quality and lower costs. Fish meal is another available and important source of protein for the industry. In 2005 the cost of feeding accounted for 59% of poultry farms’ total expenditures, which make the sector less competitive vis-à-vis imported poultry products.\n\nSenegal exports chicken meat to Guinea Bissau (194 Mt in 2005) and day-old chickens to The Gambia, Mauritania, Mali, Burkina Faso and Guinea Bissau (238,250 in 2005).\n\nThe fishing sector benefits from a long coastline (approximately 448 miles) and a productive continental shelf area of approximately 9,653 square miles. Industrial fishing consists of sardine, tuna and trawler harvesting (shrimp, mullet, sole, cuttlefish, etc.). \"Artisanal\" catches are mainly for the local market with a large proportion purchased by local factories\nfor processing. Senegal's fishing sector has historically been one of the country's largest sources of foreign currency.\n\nIn 2005, seafood products represented 22 percent of Senegal's total exports and generated more than $366 million in national income from an annual catch of approximately 40,000 tons, against approximately $374 million for a catch of approximately 430,000 tons in 2004. The fishing industry is also a key sector for employment. At the local level, thousands of families depend on fish as a nutritional staple. The Government estimates that the sector employs more than 200,000 people and generates significant temporary employment in the informal sector, in particular through the artisanal fishing, using lines, traps, and nets with small-scale traditional fishing canoes.\n\nThe European Union is the largest market for Senegal's seafood exports. Senegal signed seventeen agreements with EU allowing EU fishing craft access to Senegalese water while setting export quotas and limits, and requiring that part of the catch, especially tuna, is supplied to local processing industries. The 2002–2006 Senegal/EU agreement, which provided for an annual compensation of $15 million to the Government of Senegal, expired in June 2006. Negotiations to renew it are currently suspended following strong denunciation of previous agreements by Senegalese fishermen's associations for alleged overexploitation\nof high-value fish, declining incomes, and limiting the availability of high value fish in the local markets. The Government of Senegal and local environmental organizations have also expressed concerns about the possible permanent ecological damage caused by the more sophisticated and efficient EU fleets.\n\nSeveral large Senegalese fish processing companies have ceased operations because of Senegal's small and unproductive fishing fleet, high costs of production, over-exploitation and scarcity of high value fish, and lack of investment resources. This crisis is reported to be one of the main causes of clandestine emigration from Senegal's major fishing communities to Europe over the last two years with the death of hundreds of young people, mostly fishermen.\n\nIn Senegal, the contribution of forest and other natural resources to the economy is not visible although it is real and important. The potential production of fauna and forest products is high and diversified but this sector is not fully accounted for in the macroeconomic indicators. Officially, the sector represents less than 1% of GDP. However the production of forest resources, mainly charcoal and wildlife, is estimated at $50 million yearly.\n\nData collected in 2006 by UICN from producers, brokers and consumers of wild plant and animal products indicate that most of non-wood plants, wild animals, and continental fish are commercialized and only a small proportion is used for consumption. The economic importance of forest products varies by region but they account for up to 50% of the revenues of poor rural households. The value of these products, which usually are not included in the national statistics, is estimated at least $19 to $35 million. Gum arabic exports, which are not included in the about figures, soared to over $280 million in 2006.\n\nThe construction of the Diama and Manantali Dams in 1986 created an additional 240,000 hectares of land which can be irrigated on the Senegalese side of the Senegal River in northern Senegal. This gave the country the potential to diversify its crop base and increase food production. However, operation of the upstream dam has also reduced annual floods along the floodplain, where an ancient and productive form of recessional irrigation has been practiced for hundreds of years. Recessional irrigation is still practiced along these flood plains for an estimated 50,000 hectares.\n\nThe best agricultural land along the Senegal River is in the alluvial valley between Bakel and Dagana, and this area is the most densely populated part of the valley. As the floods retreat each year, a variety of crops (including millet, sorghum, rice, and vegetables) are sown, and they grow and mature quickly. These areas also provide pasture for livestock. But because rainfall has been lower in Guinea in 2006, the water table of the Senegal River and its effluents was at a critical level and comparable to a dry year as of early 2007. This situation was expected to limit recession and dry season productions.\n\nIn addition, pests such as insects and locusts have been reported on peanuts, cowpeas, and sorghum. In northern Senegal, rice production will likely be seriously affected this year by the extensive invasion of grain eating birds - the red-billed quelea (\"Quelea quelea\").\n\nDuring this growing season the Government has subsidized the agricultural sector at a level of about $40 million. This investment included the purchase of 40,000 tons of peanut seeds, payments to peanut producers of up to $10 million, the purchase of seeds of special crops such as sesame, cassava, corn, and hisbiscus sabdarifa (bissap), and for the subsidy of fertilizers. The government has also contributed $10 million for the purchase of farming equipment.\n\nSenegal's agricultural policies are historically characterized by the following key features: The government agricultural support system is mainly based on cash crops that have reliable markets. Agricultural research has significantly contributed in maintaining productivity despite irregular rainfall and poor soils. Liberalization of the market of agricultural produce in early 1990 has improved the efficiency of the cereal market.\n\nThe impact of the liberalization has been limited because peanuts still dominate the market; integrated extension, input distribution, credit, and marketing support systems contribute in boosting productivity, especially cash crops and government promoted new crops; yet, support to farmers is costly and inefficient, particularly because the government responds more to political pressure than to economically motivated schemes; literacy programs are not devoted due attention in rural areas, and this limits the efficiency of extension and farm-level adoption of technologies, and therefore farmers capacity to respond to market dynamics.\n\nIn response to increasing rural migration and clandestine emigration, the government has recently launched a new plan, called REVA (\"Return to Agriculture\"). The objective of this program is to develop agricultural infrastructure (construction of rural roads, rehabilitation of wells, and connection to electricity) and provide training and production tools and equipment to young and women farmers, especially former clandestine emigrants. The pilot phase of the program started in August 2006 and will end in December 2008, and during this period the government plans to implement 550 production sites. This plan is gaining increasing support from donors.\n\nThe institutional framework of the agricultural sector is organized through two main ministries. The Ministry of Agriculture, biofuels and Food Security which includes the Directorate of Agriculture responsible for the implementation of food grains and agro-\nindustrial development policies and for overseeing the field based extension services; the Directorate of Horticulture which coordinates government support to the horticultural sector; the Directorate of Agricultural Census; and the Directorate of Plant Protection responsible for government pest control programs, including regulations, management of standards, and\nvarious field interventions. The second ministry involved in the agricultural sector is the Ministry of Animal Husbandry with several services coordinating government support to the livestock, dairy and poultry sub-sectors.\n\nThese services are completed by research and training institutions. ISRA (Senegalese Agricultural Research Institute) is the leading agricultural research institution and works on various issues related to crop and animal production, SPS and veterinary issues, fishing and forest products and rural socio-economy. Other major research institutions are ITA (Food\nTechnology Institute), CDH (horticultural research) and WARDA (The Africa Rice Center). Senegal has also agricultural and veterinary colleges which provide most of human resources used in the sector. The main schools are ENSA (the Agricultural College), EISMV (The Inter-states Veterinary College), and CDH (The Horticultural Development Training Center).\n\nMajor donors involved in the agricultural sector in Senegal include FAO, USAID, USDA, the World Bank, the African Development Bank, the West African Development Bank, the French government, Peace Corps and several other local and international NGOs.\n\n\n"}
{"id": "8558371", "url": "https://en.wikipedia.org/wiki?curid=8558371", "title": "Alclad", "text": "Alclad\n\nAlclad is a corrosion-resistant aluminium sheet formed from high-purity aluminium surface layers metallurgically bonded (rolled onto) to high-strength aluminium alloy core material. These sheets are commonly used by the aircraft industry. The first aircraft to be constructed from Alclad was the all-metal US Navy airship ZMC-2, constructed in 1927 at Naval Air Station Grosse Ile. Alclad is a trademark of Alcoa but the term is also used generically.\n\nDescribed in NACA-TN-259, of August 1927, as \"a new corrosion resistant aluminum product which is markedly superior to the present strong alloys. Its use should result in greatly increased life of a structural part. Alclad is a heat-treated aluminum, copper, manganese, magnesium alloy that has the corrosion resistance of pure metal at the surface and the strength of the strong alloy underneath. Of particular importance is the thorough character of the union between the alloy and the pure aluminum. Preliminary results of salt spray tests (24 weeks of exposure) show changes in tensile strength and elongation of Alclad 17ST, when any occurred, to be so small as to be well within the limits of experimental error.\" In applications involving aircraft construction, Alclad has proven to have increased resistance to corrosion at the expense of increased weight when compared to sheet aluminum.\n\n\n"}
{"id": "153158", "url": "https://en.wikipedia.org/wiki?curid=153158", "title": "Autocatalytic set", "text": "Autocatalytic set\n\nAn autocatalytic set is a collection of entities, each of which can be created catalytically by other entities within the set, such that as a whole, the set is able to catalyze its own production. In this way the set \"as a whole\" is said to be autocatalytic. Autocatalytic sets were originally and most concretely defined in terms of molecular entities, but have more recently been metaphorically extended to the study of systems in sociology and economics.\n\nAutocatalytic sets also have the ability to replicate themselves if they are split apart into two physically separated spaces. Computer models illustrate that split autocatalytic sets will reproduce all of the reactions of the original set in each half, much like cellular mitosis. In effect, using the principles of autocatalysis, a small metabolism can replicate itself with very little high level organization. This property is why autocatalysis is a contender as the foundational mechanism for complex evolution.\n\nPrior to Watson and Crick, biologists considered autocatalytic sets the way metabolism functions in principle, i.e. one protein helps to synthesize another protein and so on. After the discovery of the double helix, the central dogma of molecular biology was formulated, which is that DNA is transcribed to RNA which is translated to protein. The molecular structure of DNA and RNA, as well as the metabolism that maintains their reproduction, are believed to be too complex to have arisen spontaneously in one step from a soup of chemistry.\n\nSeveral models of the origin of life are based on the notion that life may have arisen through the development of an initial molecular autocatalytic set which evolved over time. Most of these models which have emerged from the studies of complex systems predict that life arose not from a molecule with any particular trait (such as self-replicating RNA) but from an autocatalytic set. The first empirical support came from Lincoln and Joyce, who obtained autocatalytic sets in which \"two [RNA] enzymes catalyze each other’s synthesis from a total of four component substrates.\" Furthermore, an evolutionary process that began with a population of these self-replicators yielded a population dominated by recombinant replicators.\n\nModern life has the traits of an autocatalytic set, since no particular molecule, nor any class of molecules, is able to replicate itself. There are several models based on autocatalytic sets, including those of Stuart Kauffman and others.\n\nGiven a set M of molecules, chemical reactions can be roughly defined as pairs r = (A, B) of subsets from M:\n\nLet R be the set of allowable reactions. A pair (M, R) is a \"reaction system\" (RS).\n\nLet C be the set of molecule-reaction pairs specifying which molecules can catalyze which reactions:\n\nLet F ⊆ M be a set of \"food\" (small numbers of molecules freely available from the environment) and R' ⊆ R be some subset of reactions. We define a closure of the food set relative to this subset of reactions Cl(F) as the set of molecules that contains the food set plus all molecules that can be produced starting from the food set and using only reactions from this subset of reactions. Formally Cl(F) is a minimal subset of M such that F ⊆ Cl(F) and for each reaction r'(A, B) ⊆ R':\n\nA reaction system (Cl(F), R') is \"autocatalytic\", if and only if for each reaction r'(A, B) ⊆ R':\n\nLet M = {a, b, c, d, f, g} and F = {a, b}. Let the set R contains the following reactions:\n\nFrom the F = {a, b} we can produce {c, d} and then from {c, b} we can produce {g, a} so the closure is equal to:\n\nAccording to the definition the maximal autocatalytic subset R' will consists of two reactions:\n\nThe reaction for (a + f) does not belong to R' because f does not belong to closure. Similarly the reaction for (c + b) in the autocatalytic set can only be catalyzed by d and not by f.\n\nStudies of the above model show that random RS can be autocatalytic with high probability under some assumptions. This comes from the fact that with a growing number of molecules, the number of possible reactions and catalysations grows even larger if the molecules grow in complexity, producing stochastically enough reactions and catalysations to make a part of the RS self-supported. An autocatalytic set then extends very quickly with growing number of molecules\nfor the same reason. These theoretical results make autocatalytic sets attractive for scientific explanation of the very early origin of life.\n\nFormally, it is difficult to treat molecules as anything but unstructured\nentities, since the set of possible reactions (and molecules) would become infinite. Therefore, a derivation of arbitrarily long polymers as needed to model DNA, RNA or proteins is not possible, yet. Studies of the RNA World suffer from the same problem.\n\nContrary to the above definition, which applies to the field of Artificial chemistry,\nno agreed-upon notion of autocatalytic sets exists today.\n\nWhile above, the notion of catalyst is secondary insofar that only the set as\na whole has to catalyse its own production, it is primary in other definitions,\ngiving the term \"Autocatalytic Set\" a different emphasis. There, \"every\" reaction\n(or function, transformation) has to be mediated by a catalyst. As a consequence,\nwhile mediating its respective reaction, every catalyst \"denotes\"\nits reaction, too, resulting in a self denoting system, which is interesting\nfor two reasons. First, real metabolism is structured in this manner.\nSecond, self denoting systems can be considered as an intermediate step\ntowards self describing systems.\n\nFrom both a structural and a natural historical point of view, one can\nidentify the ACS as seized in the formal definition the more original\nconcept, while in the second, the reflection of the system in itself is\nalready brought to an explicit presentation, since catalysts represent\nthe reaction induced by them. In ACS literature, both concept are present,\nbut differently emphasised.\n\nTo complete the classification from the other side, generalised self\nreproducing systems move beyond self-denotation. There, no\nunstructured entities carry the transformations anymore, but structured,\ndescribed ones. Formally, a generalised self reproducing system consists\nof two function, u and c, together with their descriptions Desc(u) and\nDesc(c) along following definition:\n\nwhere the function 'u' is the \"universal\" constructor, that constructs\neverything in its domain from appropriate descriptions, while 'c' is a copy\nfunction for any description. Practically, 'u' and 'c' can fall apart into many subfunctions or catalysts.\n\nNote that the (trivial) copy function 'c' is necessary because though the universal constructor 'u'\nwould be able to construct any description, too, the description it would base on, would in\ngeneral be longer than the result, rendering full self replication impossible.\n\nThis last concept can be attributed to von Neumann's\nwork on self reproducing automata, where he holds a self description necessary for any\nnontrivial (generalised) self reproducing system to avoid interferences. Von Neumann planned to design\nsuch a system for a model chemistry, too.\n\nVirtually all articles on autocatalytic sets leave open whether the sets are\nto be considered autonomous or not. Often, autonomy of the sets is silently\nassumed.\n\nLikely, the above context has a strong emphasis on autonomous self replication\nand early origin of life. But the concept of autocatalytic sets is really more general and\nin practical use in various technical areas, e.g. where self-sustaining tool chains are\nhandled. Clearly, such sets are not autonomous and are objects of human agency.\n\nExamples of practical importance of non-autonomous autocatalytic sets can be found e.g. in the field of compiler construction and in operating systems, where the self-referential nature of the respective constructions is explicitly discussed, very often as bootstrapping.\n\n"}
{"id": "5712809", "url": "https://en.wikipedia.org/wiki?curid=5712809", "title": "Axe ties", "text": "Axe ties\n\nAxe ties are railway ties (or sleeper) that are hewn by hand, usually with a broadaxe. There are 2,900 ties per mile of track on a first class railroad. The early railways would not accept ties cut with a saw, as it was claimed that the kerf of the saw splintered the fibres of the wood, leaving them more likely to soak up moisture causing premature rot. \n\nGeoff Marples wrote an account of being a \"tiehack\" in the East Kootenays in 1938 and described the process of making axe ties to include: First a suitable tree was chosen and then \"falling\" and \"limbing\" the tree. Next came \"scoring\" which is chopping, by eye without a chalk line, of notches to remove extra wood about every ; \"hewing\" the trunks only on two sides unless the log was over in diameter; \"bucking\" (cutting to in this case ); \"peeling\" any remaining bark off; and stacking the ties so a chain can be wrapped around them. Next came \"skidding\" each group of ties to a landing with a team of horses, and then loading and \"hauling\" the ties to a railway siding by truck and unloading by hand. \"Scaling\" was the key event where a railroad inspector accepted or culled (rejected) and graded each tie as a number one ( by used for the main railroad lines) or number two ( by used for sidings). \"Loading\" the ties by hand onto a car was the last task. Marples wrote that he netted 48 cents for each grade one, and 36 cents for each grade two and made $150 for a winters work.\n\nCedar was the most sought after wood for ties, since it is known for being extremely resistant to rot. However, as electric power came into more common use in the early 1900s, it was substituted with other species such as Tamarack. In northern regions where jack pine was plentiful, that species became a more common source for railway ties. Jack pine ties did not last as long as cedar or tamarack (lying on the ground), but were cheaper to produce. As creosote treatment came into use the axe ties were phased out, but jack pine remained best suited for softwood ties.\n\nAxe tie production was an early industry of importance for many communities in Ontario along the railway in the early 1900s. Examples include Foleyet and Nemegos. \n"}
{"id": "16199304", "url": "https://en.wikipedia.org/wiki?curid=16199304", "title": "Banpu", "text": "Banpu\n\nBanpu Public Company Limited is a mining and power company in Thailand. It is the largest coal producer in Thailand and also has coal mining operations in Indonesia and China, and coal-fired power generation operations in Thailand and China. Banpu plans also to invest in the Hong Sa lignite mine and power plant project in Laos.\n\nBanpu's main coal resources are in Indonesia (26 million tonnes), Australia (14 million tonnes), and China (4.7 million tonnes). It expects its sales to rise to 45 million tonnes in 2018, up from 42 million tonnes in 2017.\n\nIn 2010 Banpu bought the Australian mining company Centennial Coal Company Ltd which operates five mines in New South Wales (NSW) supplying coal for export and approximately 40 percent of NSW's coal-fired electricity. The company sells approximately 40 percent of its coal to export markets, primarily for use in power stations and steel mills in Japan, Korea, Taiwan and Europe.\n\nCentennial Coal has been responsible for more than 900 pollution notices between 2000 and 2013 from the NSW Environment Protection Authority (EPA). In 2015 it was responsible for a major release of coal fines into the Wollangambe River and World Heritage listed areas of the Blue Mountains National Park. Centennial Coal has been dumping mine effluent into the Wollangambe River for approximately 30 years, effectively killing large sections of it. Between 2000-2015, Centennial's Clarence Colliery has been cited for more than 65 non-compliance breaches of its licence. As of 2015, Centennial Coal has applied to extend the Springvale Mine, undermining swamps of \"National Environmental Significance\" and dumping up to 50 million litres a day of mine effluent into the Coxs River which also flows through the Blue Mountains World Heritage area and into Sydney's drinking water catchments.\n\nThe Hong Sa Lignite project was founded by and belonged to Thai-Lao Lignite Co., Ltd. (TLL) and Hong Sa Lignite (Lao PDR) Co., Ltd. (HLL) pursuant to a concession from 1992-1994. In 2005, Banpu entered into a joint venture with TLL and HLL to develop the project, but this agreement was terminated in 2006. A UNCITRAL arbitration found that the Lao government illegally terminated TLL's and HLL's concession (awarding it to Banpu) and ordered Laos to pay US$57 million in damages plus interest. Laos is refusing to pay the award despite its clear agreement with TLL and HLL, its government policy for international arbitration, and despite participating fully.\n"}
{"id": "17263709", "url": "https://en.wikipedia.org/wiki?curid=17263709", "title": "Chicken fat", "text": "Chicken fat\n\nChicken fat is fat obtained (usually as a by-product) from chicken rendering and processing. Of the many animal-sourced substances, chicken fat is noted for being high in linoleic acid, an omega-6 fatty acid. Linoleic acid levels are between 17.9% and 22.8%. It is a common flavoring, additive or main component of chicken soup. It is often used in pet foods, and has been used in the production of biodiesel. One method of converting chicken fat into biodiesel is through a process called supercritical methanol treatment.\n"}
{"id": "13640744", "url": "https://en.wikipedia.org/wiki?curid=13640744", "title": "Coach (carriage)", "text": "Coach (carriage)\n\nA coach is originally a large, usually closed, four-wheeled carriage with two or more horses harnessed as a \"team\", controlled by a coachman and/or one or more postilions. It had doors in the sides, with generally a front and a back seat inside and, for the driver, a small, usually elevated seat in front called a \"box, box seat\" or coach box. The term \"coach\" first came into use in the 15th century, and spread across Europe. There are a number of types of coaches, with differentiations based on use, location and size. Special breeds of horses, such as the now-extinct Yorkshire Coach Horse, were developed to pull the vehicles.\n\nKocs (pronounced \"kotch\") was the Hungarian post town in the 15th century onwards, which gave its name to a fast light vehicle, which later spread across Europe. Therefore, the English word \"coach\", the Spanish and Portuguese \"coche\", the German \"Kutsche\", and the Slovak \" koč\" and Czech \"kočár\" all probably derive from the Hungarian word \"kocsi\", literally meaning \"of Kocs\".\n\nIt was not until about the middle of the reign of Queen Elizabeth I, that coaches were introduced to England. Coaches were reputedly introduced into England from France by Henry FitzAlan, 19th Earl of Arundel.\n\nA coach with four horses is a coach-and-four. A coach together with the horses, harness and attendants is a turnout.\n\nThe bodies of early coaches were hung on leather straps. In the eighteenth century steel springs were substituted, an improvement in suspension. An advertisement in the \"Edinburgh Courant\" for 1754 reads:\n\"The Edinburgh stage-coach, for the better accommodation of passengers, will be altered to a new genteel two-end glass coach-machine, hung on steel springs, exceedingly light and easy...\"\nIn the mid 19th century American Concord stagecoaches used leather straps in a similar way.\n\nA coach might have a built-in compartment called a \"boot\", used originally as a seat for the coachman and later for storage. A luggage case for the top of a coach was called an \"imperial\"; the top, roof or second-story compartment of a coach was also known as an imperial. The front and rear axles were connected by a main shaft called the \"perch\" or \"reach\". A crossbar known as a \"splinter bar\" supported the springs. Coaches were often decorated by painters using a sable brush called a \"liner\".\n\nIn the 19th century the name coach was used for U.S. railway carriages, and in the 20th century to motor coaches.\n\nSee John Taylor (poet) for a very adverse opinion of the arrival of horsedrawn coaches in England.\n\nThere are a number of coach types, including but not limited to:\n\nThe principal ceremonial coaches in the United Kingdom are the Gold State Coach, Irish State Coach and Scottish State Coach.\n\n\nThe business of a coachman like the pilot of an aircraft was to expertly direct and take all responsibility for a coach or carriage and its horses, their stabling feeding and maintenance and the associated staff. He was also called a \"jarvey\" or \"jarvie\", especially in Ireland; Jarvey was a nickname for Jarvis. \n\nIf he drove dangerously fast or recklessly he was a \"jehu\" (from Jehu, king of Israel, who was noted for his furious attacks in a chariot (2 Kings 9:20), or a \"Phaeton\" (from Greek Phaethon, son of Helios, who attempted to drive the chariot of the sun but managed to set the earth on fire). \n\nA \"postilion\" or \"postillion\" sometimes rode as a guide on the near horse of a pair or of one of the pairs attached to a coach, especially when there was no coachman. A guard on a horse-drawn coach was called a \"shooter\".\n\nTraveling by coach, or pleasure driving in a coach, as in a tally-ho, was called coaching. In driving a coach, the coachman used a coachwhip, usually provided with a long lash. Experienced coachmen never used the lash on their horses. They used the whip to flick the ear of the leader to give them the office to move on, or cracked it next to their heads to request increased speed. \n\n\nA coach horse or coacher bred for drawing a coach is typically heavier and of more compact build than a road horse and exhibits good style and action. Breeds include:\n\n"}
{"id": "20529761", "url": "https://en.wikipedia.org/wiki?curid=20529761", "title": "Concentrix Solar", "text": "Concentrix Solar\n\nConcentrix Solar GmbH is a German solar power company based in Freiburg, specialized in concentrated photovoltaics (CPV) technology. In December 2009, Concentrix Solar was acquired by the French Soitec Group.\n\nIt was founded in 2005, as a spin-off company of the Fraunhofer Institute for Solar Energy Systems ISE. Concentrix Solar builds large concentrator solar power plants suitable for sunny areas. In 2007, Concentrix Solar was awarded the Innovation Award of the German Economy for its CPV technology. From February 2006 to December 2009, the investment company Good Energies was invested in Concentrix Solar. Abengoa Solar was an investor of Concentrix Solar from November 2007 to December 2009.\n\nConcentrix concentrated photovoltaics(CPV) modules bundle sunlight up to 500 times with the use of fresnel lenses and focus it on III-V based Triple-junction solar cells (GaInP/GaInAs/Ge), which then convert the light into electrical energy. To ensure that the sunlight is concentrated precisely on the solar cell the CPV modules are installed on a two-axis sun tracking system. With its CPV technology Concentrix Solar achieves a module efficiency of 27%.\n\n\n"}
{"id": "1818025", "url": "https://en.wikipedia.org/wiki?curid=1818025", "title": "Crack spread", "text": "Crack spread\n\nCrack spread is a term used on the oil industry and futures trading for the differential between the price of crude oil and petroleum products extracted from it. The spread approximates the profit margin that an oil refinery can expect to make by \"cracking\" the long-chain hydrocarbons of crude oil into useful shorter-chain petroleum products.\n\nIn the futures markets, the \"crack spread\" is a specific spread trade involving simultaneously buying and selling contracts in crude oil and one or more derivative products, typically gasoline and heating oil. Oil refineries may trade a crack spread to hedge the price risk of their operations, while speculators attempt to profit from changes in the oil/gasoline price differential.\n\nOne of the most important factors affecting the crack spread is the relative proportion of various petroleum products produced by a refinery. Refineries produce many products from crude oil, including gasoline, kerosene, diesel, heating oil, aviation fuel, asphalt and others. To some degree, the proportion of each product produced can be varied in order to suit the demands of the local market. Regional differences in the demand for each refined product depend upon the relative demand for fuel for heating, cooking or transportation purposes. Within a region, there can also be seasonal differences in demand for heating fuel versus transportation fuel.\n\nThe mix of refined products is also affected by the particular blend of crude oil feedstock processed by a refinery, and by the capabilities of the refinery. Heavier crude oils contain a higher proportion of heavy hydrocarbons composed of longer carbon chains. As a result, heavy crude oil is more difficult to refine into lighter products such as gasoline. A refinery using less sophisticated processes will be constrained in its ability to optimize its mix of refined products when processing heavy oil.\n\nFor integrated oil companies that control their entire supply chain from oil production to retail distribution of refined products, their business provides a natural economic hedge against adverse price movements. For independent oil refiners which purchase crude oil and sell refined products in the wholesale market, adverse price movements can present a significant economic risk. Given a target optimal product mix, an independent oil refiner can attempt to hedge itself against adverse price movements by buying oil futures and selling futures for its primary refined products according to the proportions of its optimal mix.\n\nFor simplicity, most refiners wishing to hedge their price exposures have used a crack ratio usually expressed as X:Y:Z where X represents a number of barrels of crude oil, Y represents a number of barrels of gasoline and Z represents a number of barrels of distillate fuel oil, subject to the constraint that X=Y+Z. This crack ratio is used for hedging purposes by buying X barrels of crude oil and selling Y barrels of gasoline and Z barrels of distillate in the futures market. The crack spread X:Y:Z reflects the spread obtained by trading oil, gasoline and distillate according to this ratio. Widely used crack spreads have included 3:2:1, 5:3:2 and 2:1:1. As the 3:2:1 crack spread is the most popular of these, widely quoted crack spread benchmarks are the \"Gulf Coast 3:2:1\" and the \"Chicago 3:2:1\".\n\nVarious financial intermediaries in the commodity markets have tailored their products to facilitate trading crack spreads. For example, NYMEX offers virtual crack spread futures contracts by treating a basket of underlying NYMEX futures contracts corresponding to a crack spread as a single transaction. Treating crack spread futures baskets as a single transaction has the advantage of reducing the margin requirements for a crack spread futures position. Other market participants dealing over the counter provide even more customized products.\n\nThe following discussion of crack spread contracts comes from the Energy Information Administration publication \"Derivatives and Risk Management in the Petroleum, Natural Gas, and Electricity Industries\":\nRefiners’ profits are tied directly to the spread, or difference, between the price of crude oil and the prices of refined products. Because refiners can reliably predict their costs other than crude oil, the spread is their major uncertainty. One way in which a refiner could ensure a given spread would be to buy crude oil futures and sell product futures. Another would be to buy crude oil call options and sell product call options. Both of those strategies are complex, however, and they require the hedger to tie up funds in margin accounts.\n\nTo ease this burden, NYMEX in 1994 launched the crack spread contract. NYMEX treats crack spread purchases or sales of multiple futures as a single trade for the purposes of establishing margin requirements. The crack spread contract helps refiners to lock-in a crude oil price and heating oil and unleaded gasoline prices simultaneously in order to establish a fixed refining margin. One type of crack spread contract bundles the purchase of three crude oil futures (30,000 barrels) with the sale a month later of two unleaded gasoline futures (20,000 barrels) and one heating oil future (10,000 barrels). The 3-2-1 ratio approximates the real-world ratio of refinery output—2 barrels of unleaded gasoline and 1 barrel of heating oil from 3 barrels of crude oil. Buyers and sellers concern themselves only with the margin requirements for the crack spread contract. They do not deal with individual margins for the underlying trades.\n\nAn average 3-2-1 ratio based on sweet crude is not appropriate for all refiners, however, and the OTC market provides contracts that better reflect the situation of individual refineries. Some refineries specialize in heavy crude oils, while others specialize in gasoline. One thing OTC traders can attempt is to aggregate individual refineries so that the trader’s portfolio is close to the exchange ratios. Traders can also devise swaps that are based on the differences between their clients’ situations and the exchange standards.\n"}
{"id": "1190581", "url": "https://en.wikipedia.org/wiki?curid=1190581", "title": "Critical opalescence", "text": "Critical opalescence\n\nCritical opalescence is a phenomenon which arises in the region of a continuous, or second-order, phase transition. Originally reported by Charles Cagniard de la Tour in 1823 in mixtures of alcohol and water, its importance was recognised by Thomas Andrews in 1869 following his experiments on the liquid-gas transition in carbon dioxide, many other examples have been discovered since. The phenomenon is most commonly demonstrated in binary fluid mixtures, such as methanol and cyclohexane. As the critical point is approached, the sizes of the gas and liquid region begin to fluctuate over increasingly large length scales (the correlation length of the liquid diverges). As the density fluctuations become of a size comparable to the wavelength of light, the light is scattered and causes the normally transparent liquid to appear cloudy. Tellingly, the opalescence does not diminish as one gets closer to the critical point, where the largest fluctuations can reach even centimetre proportions, confirming the physical relevance of smaller fluctuations.\n\nIn 1908 the Polish physicist Marian Smoluchowski became the first to ascribe the phenomenon of critical opalescence to large density fluctuations. In 1910 Albert Einstein showed that the link between critical opalescence and Rayleigh scattering is quantitative .\n\nMore-detailed experimental demonstrations of critical opalescence may be found at\n"}
{"id": "16367375", "url": "https://en.wikipedia.org/wiki?curid=16367375", "title": "Cross-laminated timber", "text": "Cross-laminated timber\n\nCross-laminated timber (CLT) is a wood panel product made from gluing layers of solid-sawn lumber together. Each layer of boards is oriented perpendicular to adjacent layers and glued on the wide faces of each board, usually in a symmetric way so that the outer layers have the same orientation. An odd number of layers is most common, but there are configurations with even numbers as well (which are then arranged to give a symmetric configuration). Regular timber is an anisotropic material, meaning that the physical properties change depending on the direction at which the force is applied. By gluing layers of wood at perpendicular angles, the panel is able to achieve better structural rigidity in both directions. It is similar to plywood but with distinctively thicker laminations.\n\nCLT is distinct to glued laminated timber, a product with all laminations orientated in the same way.\n\nCLT was first developed and used in Germany and Austria in the early 1990's, but it was only after the mid 1990's more extensive research was completed. By the 2000's CLT saw much wider usage in Europe, being used in various building systems such as single-family and multi-story housing. As old growth timber become more difficult to source, CLT and other engineered wood products appeared on the market.\n\nIn 2015, CLT was incorporated into the National Design Specification for wood construction. This specification was used as a reference for the 2015 International Building Code, in turn allowing CLT to be recognized as a code compliant construction material. These code changes permitted CLT to be used in the assembly of exterior walls, floors, partition walls and roofs. Also included in the 2015 IBC were char rates for fire protection, connection provisions and fastener requirements specific to CLT. To meet structural performance requirements, the code mandated that structural CLT products met the requirements specified by ANSI/APA PRG 320.\n\nThe manufacturing of CLT can be split up into nine steps, primary lumber selection, lumber grouping, lumber planing, lumber cutting, adhesive application, panel lay-up, assembly pressing, quality control and finally marketing and shipping.\n\nThe primary lumber selection consists of two to three parts, moisture content check, visual grading and sometimes depending on the application structural testing. Depending on the results of this selection, the timber fit for CLT will be used to create either construction grade CLT or appearance grade CLT. Timber that cannot fit into either category may be used for different products such as plywood or glued laminated timber.\n\nThe grouping step ensures the timber of various categories are grouped together. For construction grade CLT, the timber that has better structural properties will be used in the interior layers of the CLT panel while the two outermost layers will be of higher aesthetic qualities. For aesthetic grade CLT, all layers will be of higher visual qualities.\n\nThe planing step improves the surfaces of the timber. The purpose of this is to improve the performance of the adhesive between layers. Approximately 2.5mm is trimmed off the top and bottom faces and 3.8mm is trimmed off the sides to ensure a flat surface. There are some cases in which only the top and bottom faces are treated, this is typically the case if the sides do not have to be adhered to another substance. It is possible that this step may change the overall moisture content of the timber, however this is rarely happens.\n\nThe timber is then cut to a certain length depending on the application and specific client needs.\n\nThe adhesive is then applied to the timber, typically through a machine. It is important to note that application of the adhesive must be airtight to ensure there are no holes or air gaps in the glue and that the adhesive is applied at a constant rate.\n\nA panel lay-up is performed to stick the individual timber layers together. According to section 8.3.1 of the performance standard ANSI/AP PRG 320, at least 80% of surface area between layers must be bound together.\n\nAssembly pressing fully completes the adhering process. There are two main types of pressing methods, vacuum pressing and hydraulic pressing. In vacuum pressing more than one CLT panel can be pressed at one time making the process more time and energy efficient. Another advantage to vacuum pressing is that it can apply pressure to curved shaped CLT panels because of the way the pressure is distributed around the whole structure. As for hydraulic pressing, advantages include higher pressures can be achieved and also the pressures placed on each edge can be specified.\n\nQuality control is then performed on the CLT panels. Typically a sanding machine is used to create a better surface. The CLT panels are also cut to suit their specific design. Often, if the panels need to be conjoined to form longer structures finger joints are used.\n\nCLT has some advantages as a building material, including:\n\nCLT also has some disadvantages, including:\n\nIn September 2016 the world's first timber mega-tube structure was built at the Chelsea College of Arts in London, using hardwood CLT panels. The long \"Smile\" was designed by architect Alison Brooks and engineered by Arup, in collaboration with the American Hardwood Export Council, for the London Design Festival. The structure is a curved tube in a shape of a smile touching the floor at its centre. It has the maximum capacity of 60 people.\n\nBecause of CLT's structural properties, the ability to be prefabricated and how light it is compared to other construction materials, CLT is starting to be used in many high-rise buildings (see: Plyscraper). With its 4,649 cubic metres of CLT provided by UK based B&K Structures, Dalston Lane at Dalston Square is one of the largest CLT projects globally. The project finished in early 2017. Considering the building was built on a brownfield, it was much taller than was thought to be feasible because of how light CLT is.\n\nIn the United States, Framework in Portland, Oregon planned to utilize CLT for its 12-story structure, to become the tallest timber building in North America. This structure was designed by LEVER Architecture, and may have had the first CLT post-tensioned rocking wall core as the lateral seismic system. This project was cancelled due to funding in 2018.\n\nIn Australia, a nine-story all-timber office building is due to be completed in Brisbane in late 2018. Because of CLT’s ability to be prefabricated, construction was finished six weeks earlier than predicted. Due to CLT being lighter than traditional construction materials like concrete and steel, 20% more space was able to be reallocated from structural elements to functional space.\n\nThe current tallest CLT structure and the first hybrid structure more than 14 stories tall is UBC’s Brock Commons Residence hall. Completed in September 2017, the building is approximately 53 meters tall with 18 stories and houses approximately 400 students. The architect firm for this building was Acton Ostry Architects, while the structural engineering company was Fast + Epp. 17 out of the 18 stories use CLT as the floor panels and glue laminated timber as the columns, 70% of cladding used in the facade is made from wood. It is estimated that the carbon dioxide emissions were reduced by 2432 tonnes when compared to using concrete and steel. The approximate cost of the building was $51.5 million. This project targeted to achieve LEED gold upon its completion.\n\nCLT is also used in a number of bridge projects. The 160 meter long Mistissini Bridge is located in Québec, Canada and crosses Uupaachikus Pass. The designer for this bridge was Stantec and it was completed in 2014. Locally sourced CLT panels and glue laminated timber girders were used as the main structural members of the bridge. This project won numerous awards including the National Award of Excellence in the Transportation category at the 48th annual Association of Consulting Engineering Companies (ACEC) and also the Engineering a Better Canada Award.\n\nThe Maicasagi Bridge is located in north of Québec and spans 68 meters. The bridge was completed in 2011 and uses CLT and glue laminated timber in combination to create two box girders. This combination of timber was chosen because of the ability to be prefabricated allowing for a short lead time compared to a traditional steel bridge.\n\n\n"}
{"id": "3100703", "url": "https://en.wikipedia.org/wiki?curid=3100703", "title": "Dongtan, Shanghai", "text": "Dongtan, Shanghai\n\nDongtan is a plan for a new eco-city on the island of Chongming in Shanghai, China.\n\nDongtan was planned to open, with accommodation for 10,000, in time for the Shanghai World Expo in 2010. By 2050 the city was expected to be one-third the size of Manhattan, with a total planned population of 500,000. However, the project has fallen behind schedule, and no construction has taken place yet.\n\nArup, the British engineering consultancy firm, was contracted in 2005 by the developer, The Shanghai Industrial Investment Company (SIIC), to design and masterplan Dongtan, an eco-city on Chongming Island close to Shanghai, the first of a planned series.\n\nDongtan was presented at the United Nations World Urban Forum by China as an example of an eco-city, and is the first of up to four such cities to be designed and built in China by Arup. The cities are planned to be ecologically friendly, with zero-greenhouse-emission transit and complete self-sufficiency in water and energy, together with the use of zero energy building principles. Energy demand will be substantially lower than comparable conventional cities due to the high performance of buildings and a zero emission transport zone within the city. Waste is considered to be a resource and most of the city's waste will be recycled.\n\nDongtan proposes to have only green transport movements along its coastline. People will arrive at the coast and leave their cars behind, travelling along the shore as pedestrians, cyclists or on sustainable public transport vehicles. The only vehicles allowed in the city will be powered by electricity or hydrogen. Houses are now selling here to Shanghai middle classes for use when spending weekends away from the city. The Controlling authorities are now backtracking on these commitments and allowing private vehicles onto the site.\n\nEPSRC, the UK funding body for academic research, is supporting four Dongtan research networks of UK and Chinese universities to study the research agenda for eco-city design. Arup is assisting in the coordination of these networks and in planning associated Institutes for Sustainability.\n\nThe reaction to Dongtan has been mixed, although recent media coverage has largely been negative due to delays and shortcomings in the project's execution.\n\nFormer Mayor of London Ken Livingstone praised Dongtan as pioneering work leading to a more sustainable future. His sentiments were echoed by other prominent British politicians, including Gordon Brown and Tony Blair, although none of them have ever visited the site.\n\nCritics have argued that Dongtan will not have a big impact on existing Chinese cities, which will still house the majority of the population.\n\nThe main designer, Thomas V. Harwood III, is also taking part in many environmentally less friendly projects in China, including airports and office blocks. Arup recently received the \"Greenwasher of the Year Award\" from Ethical Corporation Magazine for the most dubious green claim of the year, describing Dongtan as a Potemkin village.\n\n\n\n"}
{"id": "842947", "url": "https://en.wikipedia.org/wiki?curid=842947", "title": "Dust (His Dark Materials)", "text": "Dust (His Dark Materials)\n\nIn Philip Pullman's writings, Dust or Rusakov Particles are elementary particles associated with consciousness that are integral to the plot. Dust features in the multiverse written about in \"His Dark Materials\" and \"The Book of Dust\". Because Dust is attracted to consciousness, especially after maturation, the Church within the series associates it with original sin and seeks its end. Pullman described Dust in a 2017 interview as “an analogy of consciousness, and consciousness is this extraordinary property we have as human beings”.\n\nIn \"Northern Lights\", Lord Asriel reveals the origins of the term \"Dust\" to be from a passage from the slightly alternate version of the Bible in Lyra's world: \"In the sweat of thy face shalt thou eat bread, till thou return unto the ground; for out of it wast thou taken: for dust thou art, and unto dust shalt thou return\" (). Dust was previously known (in Lyra Belacqua's universe) as 'Rusakov particles' named after their discoverer, Boris Mikhailovitch Rusakov. Rusakov discovers a field permeating the universe enabling consciousness; before the discovery of Dust, its existence is predicted, as \"the existence of a Rusakov field implies the existence of a related particle\".\n\nMary Malone researches dark matter in an alternate universe, referring to it as \"Shadows\". This name was given to the particle by her colleague, Oliver Payne, in references to the Allegory of the Cave, involving \"Shadows on the wall\". When she is able to communicate with Shadows by interfacing with her detector, it confirms that they are the same as Dust and dark matter. The Mulefa, who are able to see Dust directly, use the word \"sraf\" accompanied by a leftward flick of the trunk (or arm for humans) to describe it.\n\nDust is attracted to objects that have been formed by consciousness, and by special photographic emulsions can be viewed. It is particularly attracted to consciousnesses that have matured; in the case of humans in Lyra's world, this happens when their Dæmon is fixed in shape. For the mulefa, this happens when adolescents start using wheels. Dust is also what connects humans to their dæmons. If the bond between a child and their dæmon is severed (as through Intercision), both the child and the dæmon will eventually die. If the separation occurs after Dust has settled on the person (that is, after adolescence), the person becomes a lifeless shell.\n\nIn Pullman's trilogy, Angels are formed when Dust condenses. It's noted that they only appear as winged humans because that's what is expected; in reality, they are much more complex shapes, more similar to \"architecture\".\n\nDust is able to communicate with humans via several methods shown throughout \"His Dark Materials\" and \"The Book of Dust\", including by use of alethiometers. Mary Malone is able to construct a computer program that, when paired with a detector, can communicate with Dust by typed questions. Later, Mary uses I Ching divination, which Lyra confirms to be a method of communicating with Dust.\n\nIn Lyra's world, six alethiometers were constructed. Also known as symbol readers, alethiometers have a ring of 36 standard symbols, but differ in level of decoration. Over the course of the books, several alethiometers are shown being read, including by Lyra Belacqua, Hannah Relf, and Fra Pavel.\n\nEach symbol on the rim of an alethiometer has indefinite levels of meaning; for example, the anchor can mean steadfastness, hope, or the sea. To phrase a question, three adjustable arms are pointed towards symbols, and the reader has to hold the level of meaning for each symbol in their head. A fourth arm, made of an alloy sensitive to Dust, swings around, stopping on different symbols to give an answer, with the number of times it stops at a given symbol indicating the level of meaning intended.\n\nIn \"La Belle Sauvage\", it's said that no more alethiometers can be built as their construction relies on a rare metal being subjected to special treatments, knowledge of which has been lost. Oxford has an alethiometer in its library, which is read by Hannah Relf, various other universities have one, and one is missing. After the Magisterium attempts to steal one, a secret society opposed to their actions is able to obtain it and give it to Relf to use for them. The missing alethiometer is found by Malcolm Polstead in the pack of Gerard Bonneville and given to the Master of Jordan College at the same time as Lyra is given, for safekeeping. \n\nIn \"Northern Lights\", this alethiometer is given by the Master to Lyra, who learns to read it by intuition, and it is said that there are only two others in existence.\n\nDust came into existence when consciousness did, as it is created by consciousness as well as being itself conscious. Dust condenses into Angels; the first of these was called The Authority, and told all future angels that he was the creator of the multiverse.\n\nAbout 33,000 years ago, Dust induced changes in sapient species, including humans and mulefa, to allow them to interact better, conferring more consciousness. For the mulefa, this was the start of their oral history and memory as a species. When asked directly, Dust states that this was done by rebel angels for \"vengeance\" over the war in heaven. \n\n300 years before the start of \"His Dark Materials\", scientists in Cittagazze created the Subtle Knife, which allowed them to travel between worlds. However, Dust was lost in the cracks of these windows into the void between universes, and the mulefa noticed that their trees, which depend on Dust for fertilisation, started producing fewer seeds.\n\nIn Lyra's world, the Bible describes the eating of the apple of knowledge in the Garden of Eden as causing Adam and Eve's dæmons to settle into one shape. The discovery that Dust settles on humans after adolescence, when their dæmons settle, causes the Church to associate it with Original Sin and to demonise it. To that end, they begin a series of experiments on children involving the separation of dæmons.\n\n\n"}
{"id": "24770082", "url": "https://en.wikipedia.org/wiki?curid=24770082", "title": "EDP Peștera Wind Farm", "text": "EDP Peștera Wind Farm\n\nThe EDP Peştera Wind Farm is located in Peștera a commune in the Constanţa County of Dobruja. Costing €200 million, the wind farm consists of 30 three-bladed Danish wind turbines, each capable of generating 3 megawatts (MW) of power, giving a total output of 90 MW. The EDP Peştera Wind Farm is the sister project of the EDP Cernavodă Wind Farm, a 138 MW wind farm located west of the Peştera farm close to the Cernavodă Nuclear Power Plant and the Danube – Black Sea Canal.\n\nThe wind farm is owned by EDP Renováveis, the renewable energy branch of the Portuguese conglomerate Energias de Portugal.\n"}
{"id": "4696596", "url": "https://en.wikipedia.org/wiki?curid=4696596", "title": "Elutriation", "text": "Elutriation\n\nElutriation is a process for separating particles based on their size, shape and density, using a stream of gas or liquid flowing in a direction usually opposite to the direction of sedimentation. This method is mainly used for particles smaller than 1 μm. The smaller or lighter particles rise to the top (overflow) because their terminal sedimentation velocities are lower than the velocity of the rising fluid. The terminal velocity of any particle in any medium can be calculated using Stokes' law if the particle's Reynolds number is below 0.2. Counterflow centrifugation elutriation is a related technique to separate cells.\n\nAn air elutriator is a simple device which can separate particles into two or more groups.\n\nMaterial may be separated by means of an elutriator, which consists of a vertical tube up which fluid is passed at a controlled velocity. When the particles are introduced, often through a side tube, the smaller particles are carried over in the fluid stream while the larger particles settle against the upward current. If one starts with low flow rates, small less dense particle attain their terminal velocities, and flow with the stream. The particle from the stream is collected in overflow and hence will be separated from the feed. Flow rates can be increased to separate higher size ranges.\nFurther size fractions may be collected if the overflow from the first tube is passed vertically upwards through a second tube of greater cross-section, and any number of such tubes can be arranged in series.\n\nIt is used in mineral processing for size classification.\nThe elutriation dust value is a usual measure for quantification of dust, generated by testing wherein mechanical forces such as vibration are applied to granules of e.g. a detergent agent.\n\nElutriation is a common method used by biologists to sample meiofauna. \nThe sediment sample is constantly agitated by a flow of filtered water from below, the action of which dislodges interstitial organisms embedded between sediment grains. A very fine filter at the top captures these organisms from the overflow.\n\n\n"}
{"id": "1195462", "url": "https://en.wikipedia.org/wiki?curid=1195462", "title": "Environmental studies", "text": "Environmental studies\n\nEnvironmental studies is a multidisciplinary academic field which systematically studies human interaction with the environment in the interests of solving complex problems. Environmental studies brings together the principles of the physical sciences, commerce/economics and social sciences so as to solve contemporary environmental problems. It is a broad field of study that includes the natural environment, the built environment, and the sets of relationships between them. The field encompasses study in basic principles of ecology and environmental science, as well as associated subjects such as ethics, geography, anthropology, policy, politics, urban planning, law, economics, philosophy, sociology and social justice, planning, pollution control and natural resource management. There are also many degree programs in Environmental Studies including the Master of Environmental Studies and the Bachelor of Environmental Studies.\n\nThe New York State College of Forestry at Syracuse University established a BS in environmental studies degree in the 1950s, awarding its first degree in 1956. Middlebury College established the major there in 1965.\n\nThe Environmental Studies Association of Canada (ESAC) was established in 1993 \"to further research and teaching activities in areas related to environmental studies in Canada\". ESAC's magazine, \"\" was first published by Robert A. Paehlke on 4 July 1971.\n\nThe Association for Environmental Studies and Sciences (AESS) was founded in 2008 as the first professional association in the interdisciplinary field of environmental studies in the United States. In 2010, the National Council for Science and the Environment (NCSE) agreed to advise and support the Association. The Association's scholarly journal, the \"Journal of Environmental Studies and Sciences\" (JESS), commenced publication in March 2011.\n\nIn the United States, many high school students are able to take environmental science as a college-level course. Over 500 colleges and universities in the United States offer environmental studies as a degree.\n\n"}
{"id": "34957765", "url": "https://en.wikipedia.org/wiki?curid=34957765", "title": "Ford F-Series (twelfth generation)", "text": "Ford F-Series (twelfth generation)\n\nThe twelfth-generation Ford F-Series is a light-duty pickup truck produced by Ford from the 2009 to 2014 model years. On the outside, the design was restricted to evolutionary styling upgrades, with a larger grille and headlights bringing it in line with the styling of the Super Duty trucks; as with many other Ford vehicles of the time, the interior saw the introduction of higher-quality materials in all but the most basic trim levels.\n\nOutside of Mexico, the Lincoln Mark LT was discontinued, replaced by the \"Platinum\" trim of the F-150. However, this trim line is still sold as the Lincoln Mark LT in Mexico. For the 2010 model year, the SVT Raptor high-performance truck was introduced; unlike its Lightning predecessor, it was a vehicle dedicated to off-road driving.\n\nIn North America, the twelfth-generation F-150 was produced at the Dearborn Truck plant in Dearborn, Michigan, as well as the Kansas City Assembly plant in Claycomo, Missouri. In December 2014, it was replaced by the thirteenth-generation 2015 F-150, unveiled in January 2014.\n\nFord revealed the 2009 F-150 design at the 2008 North American International Auto Show in Detroit. Development began under chief engineer Matt O'Leary in November 2003 under the code name \"P-415\", after P-221 production began in June 2003. General design work was done under Patrick Schiavone into late 2005, with further exterior changes taking place during late 2006 to the tailgate design and wheel lip arches during a development hiatus. The final design freeze later took place in early 2007. Production of the series began in August 2008 at Ford's Kansas City Assembly Plant.\n\nFord originally planned to expand the F-Series platform by reintroducing the F-100 as a midsize truck. Known internally as P525, the F-100 would have served as the global replacement for Ford Ranger in 2010 or 2011, The project was ultimately shelved, with the company developing the Ranger T6 as a global midsize truck and in North America, Ford focused on developing fuel-efficient powertrains such as the EcoBoost V6 and the 6-speed automatic transmission for the F-150.\n\nThe 2009 F-150 featured a larger and more flexible interior, an updated three-bar grille, and additional choices of trim levels. The chassis included lighter-weight, high-strength steel for better fuel economy and safety and improved payload and towing capacity. For the first time in the history of the F-Series, a V8 engine was standard in all models; no 6-cylinder was available. Regular cab models were once again produced with standard-length doors rather than two short, rear-opening doors. All Flareside models in 2009 were made with new badging on the previous generation's boxes and were discontinued at the end of the model year when stock had run out, echoing a similar situation in the first year of the eighth generation.\n\nIn 2009, the SVT Raptor off-road truck was introduced. Initially available as a Supercab with a unique 5.5-foot box, it was available with either a 320 hp 5.4L V8 or an optional 411 hp 6.2L V8 from the Super Duty line. The standard wheel of FX4 models was enlarged to 18\".\n\n2011 marked a major upgrade to the powertrain lineup. In the interest of increasing fuel economy, both versions of the 4.6L V8 and the 5.4L V8 were discontinued. In their place were a 3.7L V6 and a 5.0L V8. Between the two engines was an all-new 3.5L twin-turbocharged V6. Dubbed \"EcoBoost\", the 3.5L V6 produced 365 hp.\n\nFor 2012, the F-150 sported a ten-grade lineup (XL, STX, XLT, FX2, FX4, Lariat, King Ranch, Platinum, Harley-Davidson and SVT Raptor).\n\nFor 2013, the F-150 received minor changes such as 3 new grilles (replacing all 4 previous grilles), new optional 18-, 20-, or 22-inch wheels, Sync with MyFord, MyFord Touch navigation system, new power-folding and telescoping trailer tow mirrors (taken from the Super Duty models), high-intensity discharge headlamps, 3 new color options (Blue Jeans Metallic, Kodiak Brown Metallic and Ruby Red Clearcoat Metallic), new Alcantara seats in the FX Appearance Package, black or pecan leather in Platinum, the return of the Limited model and the 6.2-liter V8 being made available in XLT, FX2, and FX4 (SuperCab and SuperCrew only).\n\nFor 2014, a special truck called the Tremor was introduced, essentially an EcoBoost-equipped FX2 or FX4 truck in a regular cab model with a 6.5' bed, a special FX Appearance Package, a flow-through center console with bucket seats and a 4.10 rear gear final drive ratio.\n\nAlso new in 2014, the STX trim level also became available on SuperCrew models with the 5.5' box. In addition an STX Sport package was added for 2014 including 20\" wheels, black cloth seats, and black exterior accents.\n\nThree engines were offered with the 2009 redesign: a revised 5.4 L 3-valve Triton V8 that is E85 capable with an output rating of and of torque, a 4.6 L 3-valve V8, and a 4.6 L 2-valve V8. The 3-valve 5.4 and 4.6 liter V8s were mated to Ford's new 6R80E 6-speed automatic transmission while the 4R75E 4-speed automatic transmission was carried over for the 2-valve 4.6 L V8. The 4.2 L OHV V6 engine, which had been available, was dropped due to the closure of the Essex engine plant where it was produced.\n\nFor the 2011 model year, an all-new engine lineup was offered. Two of the engines, a 3.7 L V6 and a 5.0 L V8, both based on the 2011 Ford Mustang engines, both offer E85 flex-fuel capability. The 6.2 L V8 used in the 2011 Ford Super Duty was made available with the F-150 Platinum, Lariat, SVT Raptor, and Harley Davidson editions. Finally, the 3.5 L direct-injected twin-turbo EcoBoost V6 was offered in the F-150 starting in early 2011. All engines were paired with a new six-speed automatic transmission (6R80). Electric power-assisted steering was made available on all engines besides the 6.2. Since 2008, the Ford F-150 has towing and hauling capacity of 11,300 lbs and 3,060 lbs, respectively.\n\nThe 2009 Ford F-150 featured front-seat side impact airbags and Ford's Safety Canopy System for the first and second rows as head protection in the event of a side impact. It also featured Ford's exclusive ADVANCETRAC RSC (Roll Stability Control)--an electronic Stability control and anti-rollover safety feature also available in other Ford vehicles, from the Fusion to the Expedition.\n\nThe F-150 comes standard with AdvanceTrac Electronic Stability Control, front and rear row side curtain airbags, and front row torso side airbags.\n\nSafety Ratings can be found here (https://www.nhtsa.gov/ratings).\n\nAt the 2008 SEMA show, four 2009 Ford F-150s were unveiled: the F-150 Heavy Duty DEWALT Contractor, the FX-4 by X-Treme Toyz, the F-150 by Street Scene Equipment, and the Hi-Pa Drive F-150. The Heavy Duty DEWALT Contractor was built in a DeWalt theme. The FX-4, also called Fahrenheit F-150, was built for outdoor lifestyle enthusiasts. The Street Scene Equipment version is a lowered truck built with performance and style. The Hi-Pa Drive F-150 was powered by 4 electric in-wheel motors rated over and over torque combined.\n\nThe SVT Raptor entered showrooms in late 2009,as a dedicated off-roader, with sales being much better than expected. It was powered by a 5.4 L (330 CID) V8 engine, with the option of a 6.2 L V8. The 5.4  L (330 CID) engine has and of torque. The 6.2 L (379 CID) engine has and of torque. The suggested retail price of the 6.2 L model was $3000 over the 5.4 L model. A six-speed automatic came standard. The Raptor features Fox Racing internal bypass shocks with external reservoirs which allows for 11\" of suspension travel in front, and 12\" in the rear. It comes standard with 35\" BFGoodrich All Terrain tires, and a rear Locking differential with a 4.10:1 gear ratio. The Raptor is available in tuxedo black, oxford white, blue flame, and molten orange, with a \"digital mud\" decal scheme as an option. In April 2011, Ford sold 1,186 Raptors, outselling Honda's Ridgeline. The first production Raptor, molten orange with the digital mud graphic, sold at auction for $130,000 with all proceeds above the MSRP going to charity. The race version, the F-150 SVT Raptor R, was built for the Baja 1000 races. It uses a 6.2 L V8 engine rated at .\n\nFor 2011, the SVT Raptor was only offered with a 6.2 L V8. Other notable changes to the 2011 model include availability in a 4-door Super Crew cab and a 5th color option of Ingot Silver Metallic.\n\n2012 Raptors eliminated the open differential in the front end in favor of a Torsen helical gear limited slip differential.\n\nShelby American makes a higher performance version of the SVT Raptor, called the Shelby Raptor. And, the Hennessey Special Vehicles division of Hennessey Performance Engineering makes an SUV version of the SVT Raptor, called the Hennessey VelociRaptor SUV.\n\nFrom the A-pillar forward, the Raptor has a composite hood and fenders that are unique from other F-150s and bereft of the blue oval badge in the grille. SVT widened the track by , so its box is unique to the Raptor, as well. The Raptor's height is over a standard 4-wheel drive Supercrew. It also features new, internal bypass shocks, designed by Fox Racing Shox.\n\nThe Raptor has new leaf springs and shocks, new front upper- and lower-A-arms, and a wider, thicker-walled rear axle. It is the first Ford with hill-descent control and comes with an electronic differential locker that lets the driver keep it locked at high speeds when the Raptor's Off Road Mode is engaged. Off Road Mode is a feature unique to the Raptor which allows more controlled, aggressive driving while in situations where increased traction and braking are necessary. The Raptor's Off Road Mode allows ABS, AdvanceTrac RSC (roll stability control), and traction control to be completely switched off giving the driver total control over the driving experience. Off Road Mode also changes the Raptor's throttle sensitivity and transmission shift points causing a more linear power and torque curve for low traction situations.\n\nTowing capacity is up to with a payload (SuperCrew only). Interior changes include high-bolster seats, a special steering wheel, a redesigned center console, and auxiliary switches connected to pre-wired pass through leads allowing ease of aftermarket product installation. Ford originally promised a value price and a multi-year run—final pricing came in at US$42,000.00 for 2010.\n\nIn 2014, Chinese company Jiangsu Kawei Automotive released the Kawei K1, which appeared to be nearly identical to the Ford F-150.\n\nWhen interviewed by Fox News in 2014, a Ford spokesman said they were aware of the Kawei and their legal counsel was investigating how best to address the matter.\n"}
{"id": "44212917", "url": "https://en.wikipedia.org/wiki?curid=44212917", "title": "Fowlers Bay Conservation Park", "text": "Fowlers Bay Conservation Park\n\nFowlers Bay Conservation Park (previously known as the Fowlers Bay Conservation Reserve) is a protected area located in the west of South Australia on the coastline of the Great Australian Bight in the gazetted locality of Fowlers Bay. The conservation park is classified as an IUCN Category VI protected area.\n\nThe historic Whale Bone Area and Point Fowler Look-Out Structure within the conservation park are listed on the South Australian Heritage Register as designated places of archaeological significance.\n\n"}
{"id": "55924477", "url": "https://en.wikipedia.org/wiki?curid=55924477", "title": "Fruitwood", "text": "Fruitwood\n"}
{"id": "27930306", "url": "https://en.wikipedia.org/wiki?curid=27930306", "title": "Global storm activity of 2007", "text": "Global storm activity of 2007\n\nVery rarely, they may form in summer, though it would have to be an abnormally cold summer, such as the summer of 1816 in the Northeast United States of America. In many locations in the Northern Hemisphere, the most powerful winter storms usually occur in March and, in regions where temperatures are cold enough, April.\n\nA low pressure brought up heavy snow and blizzard conditions across the Canadian Prairies. Snowfall locally reached between 8 inches (20 cm) to 1 foot (30 cm) in parts of Alberta, Saskatchewan and Manitoba. Particularly hard-hit was central Saskatchewan, including the city of Saskatoon. The storm was accompanied by strong gusty winds in excess of 40 mph (64 km/h). Two people were killed during the blizzard when their car was stuck near a First Nations reserve in Saskatchewan. Saskatoon's Diefenbaker Airport as well as schools were closed.\n\nPrior of hitting the Prairies, the system brought another windstorm to western British Columbia, with gusts exceeding 60 mph (100 km/h). Additional trees at Stanley Park in Vancouver were uprooted. It also hindered efforts from workers who were trying to repair the inflatable roof of BC Place Stadium (home to the Canadian Football League's BC Lions), which was damaged by winds from a previous storm a few days earlier. A secondary wave following the main storm dumped over 4 inches of snow (10 cm) in the Victoria, Vancouver, and Seattle areas with heavier snow in the mountains. Over 115,000 homes were without power during the storm in B.C.\n\nThe storm would later drop some locally heavy amount of snows in parts of northern Ontario and central Quebec with 8 inches (20 cm) reported in Saguenay.\n\nA cold front sharply drop temperatures from west to east with some areas getting their coldest days of the season across the Canadian and U.S. plains. The cold air later reached the eastern half of the continent at the end of the week.\n\nFollowing a prolonged period of mild weather, a series of winter storms produced several waves of damaging freezing rain across the Midwest of the United States and central Canada from the 12th to the 16th causing the deaths of 85 people as of January 20. Several thousands of customers from Texas to New England lost power, some for several days. Some areas received as much as 4 inches of ice (100 mm).\n\nOklahoma and Missouri were declared disaster areas as they were the most hard hit states from the storms. Areas from Utah to New Brunswick received heavy amounts of snow from the 13th to the 16th. The storm was followed by an intense period of cold across most of the continent from California to Newfoundland and Labrador.\n\nAdditional waves of precipitation have affected the south half of the United States from the 16th to 18th from Texas to North Carolina, while another winter storm, called a weather bomb affected portions of New Brunswick, Quebec and Maine on the 19th and 20th with near blizzard conditions. Portions of eastern Quebec received as much as 32 inches of snow (80 cm) in just over 12 hours\n\nA total of 85 deaths across 12 U.S. states and three Canadian provinces, and caused hundreds of thousands of residents across the U.S. and Canada to lose electric power.\n\nWinter storms contributed to deaths in traffic collisions: 14 in Missouri, 8 in Iowa, 12 in Texas, 2 in Minnesota, 4 in New York, 1 in Maine, 1 in Indiana, 4 in Michigan, 3 in Arkansas, 1 in Quebec, 1 in Ontario, 1 in Nova Scotia, 2 in North Carolina, 2 in Kansas, 4 in Nebraska and 25 in Oklahoma. A crash near Elk City, Oklahoma, killed 7 occupants who were inside a minivan when it collided with a tractor-trailer during the storm.\n\nAnother winter storm affected the central and southern Plains from the 19th to the 21st bringing snow and ice for most of the area with accumulations that topped off at about 4 to 10 inches of snow (10–25 cm). It also brought a light wintry mix across the Ohio Valley and the mid-Atlantic states on the 21st with little accumulation. Newfoundland and Labrador was the last region affected by the series of storms on the 23rd and 24th.\n\nPer was the name of a powerful storm with hurricane winds which hit the west coast of Sweden and Norway on the morning of January 14, 2007. In Sweden six people died from the storm and approx. 300,000 households were left without electricity.\nA major European windstorm gave heavy amounts of snow across portions of Scotland. Most areas of western Europe from Great Britain to the Czech Republic have experienced damaging winds. Wind gusts have reached 90 mph (150 km/h) in the plain and up to 140 mph (225 km/h) in the mountain area. Boat, rail and air traffic have been heavily affected, while several hundreds of flights from London, Berlin, Amsterdam, Vienna, Prague and Paris have been delayed or canceled. U.S. Secretary of State Condoleezza Rice shortened her European trip due to the strong winds. Millions of residents were without power including 1 million in the Czech Republic. As of 9:00 pm GMT on January 21, Kyrill had caused 47 fatalities. They were- 13 in Germany, 11 in United Kingdom, 7 in Ireland, 6 in The Netherlands, 4 in Poland, 3 in the Czech Republic 3, 1 in France, 1in Belgium and 1 in Austria.\n\nGermany had 3 tornadoes on February 22 and more tornadoes were confirmed from Poland.\n\nA snowstorm affected a large area of western and central Europe, including France, Great Britain, Austria and Germany bringing locally heavy snow accumulations and ice which disrupted air and train travel in Berlin, Stuttgart and London. Some areas in the Alps region received as much as 1 meter of snow (40 inches). Three people were killed in Germany due to accidents caused by the storm. Over 5,000 motorists were stranded in a highway in eastern France due to the heavy snow amounts. Scattered power outages were reported with central France being affected the most with nearly 85,000 homes without power.\n\nOn the 27th abolut 40,000 people had been affected by flooding in Bolivia and Peru\n\nA winter storm crossed through the southern United States, with a mix of winter weather. Several inches of snow fell across parts of Arkansas, Georgia, the Carolinas and Tennessee with scattered sleet and freezing rain farther south. Anywhere from 1–4 inches of snow fell across Tennessee and Arkansas, with lighter amounts in the Carolinas.\n\nA major lake effect snow event, titled Lake Storm \"Locust\", occurred across the Great Lakes regions for several days. Areas most affected by the localized heavy burst of snows were just east of Georgian Bay area near Parry Sound, east of Lake Huron near Wiarton, in western Michigan, and in north central New York.\n\nAreas near Oswego and northeast of Syracuse received as much as 141 inches (358 cm) of snow during that period. There were unofficial reports that two towns on the Tug Hill Plateau southeast of Lake Ontario received over 10 feet (305 cm) of snow – Redfield, with 141 inches (358 cm) and Parish, with 121 inches (307 cm). Local accumulations elsewhere on the plateau were well over 1 meter (3.3 ft). A state of emergency was declared in Oswego County due to the intense snow. Portions of central Ontario received 1–3 feet (30–90 cm) of snow over the period while heavy accumulations were also reported in western Michigan just off the shores of Lake Michigan.\n\nOn February 1, a snow squall just east of Oshawa, Ontario on the north shore of Lake Ontario caused a 15-vehicle pileup including a tractor trailer which burst into flames. Two people were killed in the event. There were no reported deaths related to the event in New York State. However, 20 were killed in other states due to cold weather. The event was very localized; areas outside the narrow bands received little or no snow.\n\nA winter storm blanketed parts of the United Kingdom including the City of London disrupting travel all across the city including numerous flights cancelled from all airports and several motorists were stranding on area roads. Service on the Underground subway system was also affected with several stations been closed. Many schools were also closed for one or two days. The heavy snowfall started life as a low-pressure system sitting out in the Atlantic Ocean, at the time the UK was under the influence of a cold northerly wind. The low pressure system tracked towards the UK on the evening of February 7 and turned readily to snow as it hit the cold air. The snow turned back to rain across southern and western regions, but much of Wales, the Midlands and the south-east had significant snowfalls on the 8th. The West Midlands in particular was badly hit, with up to 6 inches (15 cm) reported over high ground – the most snow to fall in this region for 15 years. In Wales, Sennybridge in Powys, reported 15 inches (38 cm) on level snow with drifts of up to 3 feet (90 cm) in places. On the 9th, the low pressure over France tracked further north than forecast, bringing more heavy snow for the Midlands and Wales. This caused additional travel disruption as the roads were not gritted and heavy gridlock formed on many of the roads. The snow began to thaw over the weekend and in turn caused some localised flooding.\n\nA major winter storm affected a large area of eastern North America from Nebraska to the Canadian Maritimes. Numerous areas received snow accumulations of over 6 inches (15 cm) with isolated reports as much as 1 meter (3.3 ft) in the Adirondacks and the Vermont mountains. Burlington, Vermont set a 24-hour snowfall record, with 25.3 inches. Twelve to sixteen inches (30 to 41 cm) of snowfall and blizzard conditions in central Illinois cancelled classes at the University of Illinois at Urbana-Champaign for two days, the first time classes had been cancelled since 1979.\n\nLarge cities including Cleveland, Hamilton, Syracuse, Rochester, Burlington, Quebec City and Sherbrooke received amounts well in excess of 1 foot of snow (30 cm). The city of Hamilton received local snowsqualls bombarding in from Lake Ontario with a north-east wind which dumped over 2 feet of snow (75 cm) in some parts of the city.\n\nMixed precipitation fell across the southern Ohio Valley and the Interstate 95 corridor from Virginia to Boston, including New York City, Washington, D.C. and Philadelphia.\n\nThe storm has been blamed for 35 deaths across 13 states and three Canadian provinces.\n\nA blizzard event took place across eastern Canada on the island of Newfoundland, dumping over 16 inches (40 cm) of snow in St. John's, the capital of Newfoundland and Labrador, shutting down most of the city, and closing all area schools. Heavy amounts were reported in the western Avalon Peninsula of the province. The storm previously affected portions of Nova Scotia and dumped locally heavy amounts of snow due to sea effects coming from the Atlantic Ocean and the Bay of Fundy. A new storm on the 23rd and 24th dumped an additional 6 inches (15 cm)in the capital with freezing rain, while heavier amount fell just to the west.\n\nA storm moved onto the northern California coast early on the 21st, leading to 1–3 feet (30–90 cm) of snow across the southern Cascades, Siskiyous, Sierra Nevada, and the mountains of southern California. It also gave moderate snowfall accumulations across the Canadian Prairies between 4 and 8 inches (10–20 cm) across Manitoba. The storm then moved east, bringing up to 2 feet (60 cm) to the mountains of Utah and Colorado. Late on the 23rd, it moved onto the central High Plains and organized into a major storm that spread snow from eastern Colorado northeast into the Upper Midwest and Great Lakes region, and ice from Iowa to northern Indiana. The system then split into two with the northern branch dissipating due to a blocking ridge of high pressure which prevented the blizzard from moving north into Canada. The storm continued into the Mid-Atlantic on the 25th, dropping snow as far south as the Washington, D.C. area.\n\nSnowfall amounts from 12 to 24 inches (30–60 cm) were common in Minnesota, Iowa, Wisconsin, and Illinois, while lighter amounts were reported in Michigan and Ontario. Winona, Minnesota recorded the highest official snowfall total in this region, with 29.5 inches (75 cm) as well as La Crosse, Wisconsin with 21 inches (53 cm). Up to 1.5 inches (38 mm) of ice accumulation was reported from Iowa eastward into northern Indiana. Sustained winds of 30–40 mph (48–64 km/h) resulted in severe blowing and drifting in some of these locations. 10 people were killed in traffic accidents during the storm including 8 in Wisconsin, one in Ontario and one in Kansas. A forty car pileup resulted in the closing of Interstate 70 between Denver and Goodland, Kansas. This storm caused massive delays and cancellations at Chicago O'Hare and Midway Airport. At one point, 250,000 customers in Iowa were without power. Some people got their power back quickly, for others it took quite a while. The storm then moved into the mid-Atlantic states, where up to 8 inches accumulated. Blizzard or winter storm warnings were in effect at one point in Kansas, Nebraska, South Dakota, Iowa, Minnesota, Wisconsin, Illinois, Indiana, Michigan, Ohio, Pennsylvania, Maryland, Virginia, West Virginia and Washington, D.C.\n\nThe storm also brought severe thunderstorms and tornadoes from Kansas to Alabama, hitting Arkansas especially hard, where Dumas was heavily damaged by a tornado.\n\nA snowstorm moved across Scandinavia in northern Europe dumping heavy amounts of snow. The storm was blamed for one fatality in Denmark, while hundreds of flights from Copenhagen and Sweden were cancelled. Numerous motorists were stranded due to drifts that reached locally 3-meters high. A sports hall in Thisted, Denmark also collapsed but the building was vacant.\nThe 2007 Mozambican flood began in late December 2006 when the Cahora Bassa Dam overflowed from heavy rains on Southern Africa. It worsened in February 2007 when the Zambezi River broke its banks, flooding the surrounding areas in Mozambique. The Chire and Rivubue rivers have also flooded. Avbout 80,600 were evacuated. There were 29 known and 10 unconfirmed deaths in Mozambique.\n\nAnother major storm moved into the Pacific Northwest coast on the 27th, adding to the several feet of snow already recorded in the Cascades and Sierra Nevada in the previous few days. It impacted the Upper Midwest, the northern Plains, the Great Lakes and Quebec regions with heavy snow, sleet, freezing rain and high winds by March 1 and 2, in addition to bringing more severe thunderstorms to the South. Already, numerous tornadoes were reported in Alabama, Georgia, Missouri and Kansas including six killers. A deadly tornado struck Enterprise High School, killing 8 students on\nMarch 1. On March 2, over 2 inches of rain fell in New York City and snow, sleet and freezing rain fell in the interior Northeast.\n\nOmaha, Nebraska was under a blizzard warning for the first time in 9 years, with much of the city receiving a foot (30.5 cm) or more of snow, and thunder snow as reported at the beginning of the storm. Wind speeds in Omaha were clocked as high as 58 miles per hour, creating snow drifts in outlying areas up to 8 feet depth. The entire state of Iowa was placed under a State of Emergency by Governor Chet Culver while large stretches of Interstate 80 were shut down. The National Guard came into the area to bring generators to restore power until utility lines were repaired.\n\nSeveral areas in Manitoba as well as the Northern Plains received over 8 inches (20 cm) of snow with portions of Wisconsin receiving 16 inches (40 cm), 17 inches (42 cm) for parts of Iowa, 12–25 inches (30–63 cm) in parts of Minnesota with the highest accumulations falling in the northwest suburbs of the twin cities metropolitan region, (Anoka, Champlin, Maple Grove, Plymouth, Rogers)., and up to 21 inches (53 cm) across the Dakotas. Portions of Ontario and Quebec from Sault Ste. Marie to Montreal (including Sudbury, North Bay and Ottawa) received between 6 and 10 inches of snow (15–25 cm) on March 2. 80,000 customers lost power in the province with localized heavy amount across the Appalachians. Although Toronto did not receive large amounts of snow around 10 cm (4 in.), hours of freezing rain that followed created a hazardoussituation the next day when the temperature rose in the city core and under the CN Tower causing massive chunks of ice sheets to cascade off the buildings hundreds of metres below, breaking some vehicle windows in a hotel parking lot. It forced City police to close the Gardiner Expressway on March 5.\n\nThe storm with the tornadoes and snow was blamed for 39 deaths including 10 in Alabama, 1 in Missouri, 9 in Georgia, two in Manitoba, two in Ontario, one in Minnesota, three in Michigan, one in Nebraska, four in North Dakota, one in Massachusetts and four in Wisconsin.\n\nPortions of northern China and Mongolia were hit by the worst winter storm in over 50 years. The provinces of Liaoning and Shenyang had adopted emergency measures in able to cope with the storm which shut down numerous highways and canceled numerous flights while disrupting train service. Strong winds created snow drifts of up to 2 meters deep.\n\nRescue ships had to assist a large group of fisherman on the Yellow Sea following a storm tide. Two people were killed in Tianjin when a storm surge collapsed several warehouses. As much as 50 cm (20 inches) fell in the province in Heilongjiang.\n\nJordan's second winter storm of the season shut down most roads, schools and businesses across much of the country due to accumulations exceeding 4 inches (10 cm) including the capital of Amman.\n\nMuch of the Middle East usually have little or no snow during the winters due to much warmer conditions caused by the moderate sea effects from the Mediterranean Sea. However 3 feet (90 cm) of snow fell in a storm 2004, which was the worst since 1950.\n\nA heavy nor'easter caused severe Precipitation that started as rain across the region during the evening of the 15th, but as colder air moved in aloft, precipitation changed quickly to snow in The Poconos around Midnight EDT on the 16th and in Berks County and the Lehigh Valley around 7 am EDT. Farther to the south, the surge of cold air was confined to a more shallower layer and precipitation changed to sleet around the Greater Philadelphia Metropolitan Area between 6 am and 9 am EDT. Precipitation continued as mainly sleet across the greater Philadelphia through the evening. The nor'easter caused heavy sleet to fall across the greater Philadelphia Metropolitan Area, heavy snow and sleet also fell across Berks County, the Lehigh Valley and heavy snow hit the Poconos on both the 16th into the early morning of the 17th.\n\nThe winter storm caused scores of accidents. Various vehicles rolled over, slid off roads, slid into each other, slammed into guardrails and fishtailed. The afternoon and evening commute slowed to a crawl. In the Greater Philadelphia Metropolitan Area, about 265 schools dismissed early and more than 60 community events were either cancelled or postponed on the 17th. A few schools also closed on the 16th, but most of the others had early dismissals and many after school activities were postponed. Some state offices and county courts also closed early and several municipalities declared snow emergencies over the March 16 to 18.\n\nThe winter storm wreaked havoc at Philadelphia International Airport had most flights on the 16th cancelled and it took a couple of days for flights to return to normal. The largest impact of the winter storm within Philadelphia was the cancellation of most of the 1,200 scheduled flights at the Philadelphia International Airport. About 1,000 people were stranded at the airport the night of the 16th. Passengers on about 15 U.S. Airways planes sat on the tarmac for over four hours before gates became available to deplane them. Operations resumed on the 17th, but U.S. Airways still had to cancel about one quarter of its flights because the weather prevented crews and planes from arriving in Philadelphia. About 100 travelers slept at the airport on the night of the 18th. Normal operations resumed on the 19th.\n\nThe Pennsylvania Interscholastic Athletic Association postponed several state playoff high school basketball games. A couple of Saint Patrick's Day parades scheduled for Saturday the 17th were also postponed on the 16th. The Philadelphia Flyers hockey team was forced to fly out of Atlantic City International Airport on the 17th. The horse racing card at Philadelphia Park was also cancelled for three days.\n\nOn the 17th snow accumulations averaged in the local Philadelphia area, in Berks County and the Lehigh Valley and in the Poconos.\n\nIn Berks County and the Lehigh Valley, the snow mixed with and changed over to sleet during the later afternoon and the first half of the evening before it went back to all snow. Precipitation ended early in the day on the 17th (before 3 am EDT) as mainly snow in all areas. Elsewhere in Bucks County, portions of the Northeast Extension of the Pennsylvania Turnpike near Quakertown were stalled when two tractor-trailers collided in the southbound lanes at 2 pm EDT. A six-mile back-up occurred. About 1,900 homes and businesses lost power in Newtown after a vehicle struck a pole.\n\nThe sleet forced the closure of the eastbound lanes of the Vine Expressway within the city for about half an hour between the Schuylkill Expressway and Interstate 95 for its removal. In Montgomery County, in Montgomery Township, an accident on Pennsylvania State Route 309 and Taylor Road badly injured one person. In Towamencin Township, two accidents resulted in two injuries occurring.\n\nIn Chester County, the state police reported 46 accidents in the central part of the county, but only one reported injury. A woman was hospitalized after a crash on Pennsylvania State Route 113 in Phoenixville. Several businesses in the county closed early. In Berks County, in Union Township one vehicle slid off a road and landed upside down in a creek. The driver was treated for non-threatening life injuries. A serious accident occurred on Old U.S. Route 22 in Lenhartsville.\n\nThe Lehigh Valley had a number of serious accidents on Interstate 78. In Lehigh County, a tractor-trailer jack-knifed near Pennsylvania State Route 100 at Fogelsville and closed the interstate from 330 p.m. EDT through 515 p.m. EDT. Both southbound lanes of the Pennsylvania Turnpike Northeast Extension were closed from 155 p.m. EDT through 420 p.m. EDT after two tractor-trailers and a car collided in Lower Milford Township. Many flights at the Lehigh Valley International Airport were delayed, a couple were cancelled.\nCommuter buses bringing workers home from New York City had long delays. In Northampton County, a tractor-trailer jackknifed on the eastbound lanes of Interstate 78 about 4 pm EDT near the Pennsylvania State Route 33 junction. All lanes were not reopened until 545 p.m. EDT. In Moore Township, a school bus collided head-on with a car. Both drivers, but no children were injured.\n\nIn Lower Mount Bethel Township, a Jeep Cherokee slid into a home on Pennsylvania State Route 611 and injured the driver. A Ford Explorer driver was injured after the vehicle struck a tree in Wind Gap. Problems on Lehigh Valley roadways continued long after the snow and sleet ended. A driver was injured on U.S. Route 22 in Whitehall Township (Lehigh County) when ice chunks from a tractor-trailer hit their vehicle. In Upper Macungie Township (Lehigh County), ice chunks that flew off a Wal-Mart tractor-trailer cracked the windshield and dented the hood of a vehicle on Interstate 78 near the Pennsylvania State Route 100's exit.\n\nIn the Poconos, many shopping malls and sports complexes closed early. In Monroe County, a flipped over truck snarled traffic on U.S. Route 209 and Pennsylvania State Route 33. A jack-knifed tractor-trailer on eastbound Interstate 80 near Stroudsburg snarled the evening commute to a crawl.\n\nSnow and sleet depth totals across the north eastern US included in Albrightsville (Carbon County) and Effort (Monroe County), in Lehighton (Carbon County) it was at . In Tobyhanna (Monroe County), in East Stroudsburg (Monroe County) it was at . In Slatington (Lehigh County) it was at in Reading (Berks County) and at the Lehigh Valley International Airport it was at . In Birdsboro (Berks County) and Springtown (Bucks County) it was at .\n\nIn Glenmoore (Chester County) it was . In East Nantmeal (Chester County), in Doylestown (Bucks County) it was at . In Elkins Park and King Of Prussia (both Montgomery County) it was at In Marshalls Creek (Northampton County) it was at .\n\nIn Broomall (Delaware County) and Bethlehem (Northampton County) it was . In Roxborough (Philadelphia County) it was In Drexel Hill (Delaware County) and Neshaminy Falls (Bucks County) it was In West Chester (Chester County) it was . In Wynnewood (Montgomery County) and at the Philadelphia International Airport it was .\n\nA strong high pressure system moved across nearby parts of Canada and supplied a fresh supply of cold air into the region.\n\nHundreds of traffic accidents occurred across the northeast and Canada including one involving a vehicle from George W. Bush's motorcade in Washington, D.C..\n\nThe winter storm was caused by a nor'easter low pressure system that developed on a cold front that moved through the area on the 15th. Prior to that, unseasonably mild air helped push high temperatures as high as the 70s.\n\nMeanwhile, the low pressure system formed over South Carolina and Georgia on the morning of the 16th and moved northeast. At 2 pm EDT on the 16th, it was near Myrtle Beach, South Carolina; at 8 pm EDT that evening, it was just east of Virginia Beach; at 2 am EDT on the 17th, it was about east of Atlantic City, New Jersey and was about south of Cape Cod, Massachusetts at 8 am EDT on the 17th. As central pressures go, this was not particularly a powerful system; it was only 996 millibars the morning of the 17th. What contributed to the event, was the strong high pressure system (about 1040 millibars the morning of the 16th). It supplied the fresh cold air needed to change the precipitation over to sleet and freezing rain and increased the pressure gradient (and consequently the wind) between itself and the developing nor'easter low pressure system.\n\nThe only seriouse reported traffic fatality from the storm in Eastern Pennsylvania occurred in Bucks County. An 18-year-old girl from Plumstead Township was killed when her vehicle crossed the center line of an icy Durham Road and collided with a dump truck on the 16th.\n\nSo far, 10 people have been killed by the storm, all in traffic accidents. This includes six in New Jersey, three in Pennsylvania and one in Maryland.\n\nA storm moved onto the coast on March 26, dropping up to 2 feet of snow in the Sierra Nevada. The storm moved across the Intermountain West on the 27th and developed into a major winter storm across the northern and central Rockies and northern High Plains. Many of the western valleys, from the Wasatch Front through the valleys of Wyoming, onto the Plains of Wyoming and Montana, saw about 6–12 inches of snow, with 1–2 feet in the mountains from the 27th through the 29th. Up to 3 feet fell in the Wasatch Range and Bighorn Mountains. The storm was concentrated around south-central Montana and north-central Wyoming, where such cities as Sheridan and Billings and surrounding areas could see 1–2 feet of snow. Throughout the mountains and on the Plains (including Saskatchewan and Manitoba), this snow was accompanied by strong winds, leading to localized near-blizzard to blizzard conditions.\n\nA late season winter storm dumped a large swath of snow from North and South Dakota, eastward through Minnesota, Wisconsin and into Upper Michigan. Up to 9 inches of snow fell near Bismarck, North Dakota, 11 inches in Brainerd, Minnesota, and areas near Hurley, Wisconsin received 18 inches. Parts of Upper and northern Michigan then saw a major Lake Effect event over approximately five days. Painesdale, Michigan received 65 inches of snow and the National Weather Service in Marquette received 47 inches, shattering most previous April snowfall records for that city. Lake effect also affected the Lake Erie region, cancelling the series between the Cleveland Indians and Seattle Mariners baseball teams in Cleveland, and prompting a move of the next series with the Los Angeles Angels of Anaheim from Jacobs Field to Miller Park in Milwaukee.\n\nIn northern New England, the storm hit on Wednesday afternoon and left behind up to a foot and a half of snow, sleet, and freezing rain. Over 180,000 homes lost power, mostly due to broken tree limbs snapping wires. The storm has caused at least one death.\n\nHeavy snow also fell across much of southern and central Quebec, with amounts in excess of 12 inches (30 cm) across some areas, with higher amounts over higher terrain in the Charlevoix region. Numerous accidents were reported across the provinces including one involving a firetruck. Two people were killed in accidents across the province.\n\nOn the back side of the storm, persistent heavy flurries gave additional accumulations of a few inches across most of Ontario and Quebec.\n\nAnother winter storm affected portions of New Brunswick, Nova Scotia, Quebec and Prince Edward Island on Easter Sunday dumping as much as of snow locally along with strong winds which caused flight cancellations at Halifax International Airport and scattered power outages, mainly in Nova Scotia.\n\nFor the second time in a week, the Northern Plains of the United States was affected by a late-season April winter storm. Snowfall totals of 8 inches was reported in Fairmont, Minnesota while 9 inches was recorded in Victory, Wisconsin. 6 people died in snowfall related traffic accidents near Green Bay, Wisconsin. 5.1 inches fell in Muskegon, Michigan, on April 11, setting a snowfall record for that date.\nHeavy mixed precipitations fell across portions of the Canadian Maritimes and southern Quebec with accumulations that exceed 8 inches (20 cm) across the Eastern Townships and the Beauce region. The storm did shut down some school across Nova Scotia on the 13th.\n\nA major nor'easter struck the eastern half of North America bringing heavy rains, floods, storm surges and damaging wind across coastal areas. New York City itself received nearly 8 inches (200 mm) of rain in one day, making it one of the rainiest days ever for the city. Flooding did occur across many suburbs of the region as well as in other areas of the East Coast from Maine to Virginia. In Cape Elizabeth, Maine, an 80 mph wind gust was recorded, along with 30 foot waves that battered the coast. In New York, the National Guard assisted the emergency procedures while Maine, West Virginia and New Jersey declared state of emergencies. Several tornadoes struck the Carolinas killing at least 1 in South Carolina. Additional tornadoes struck northern Texas on the 13th.\n\nIn addition, heavy snow fell across portions of Colorado, Kansas, New Mexico and Oklahoma on the 13th, bringing about 12 to 18 inches (30–50 cm) across the higher elevations. Then it dumped heavy snow across the Appalachian Mountains and the Laurentians of Quebec on the 15th and 16th. 17 inches (43 cm) fell over portions of Vermont, as much as 26 inches (65 cm) in Tupper Lake, New York, as much as 40 inches (100 cm) in the Charlevoix region of Quebec while 4 to 6.5 (10–16 cm) inches also blanketed the cities of Montreal and Ottawa and the surrounding regions in just a few hours.\n\nIn Quebec as much as 160 000 Hydro-Québec customers lost power from the Outaouais to the Quebec City region while several schools were closed north of Montreal. An additional 17 000 households serviced by Hydro One and Hydro Ottawa suffered power outages in Eastern Ontario\nIn the U.S. over 300,000 customers lost power from Maryland to Maine including 55 000 in New York, 50 000 in Pennsylvania, 43 000 in Connecticut, 46 000 in New Hampshire, 17 000 in Maine, 30 000 in Maryland, 25 000 in Vermont and 12 000 in Massachusetts.\n\nNumerous flights were delayed or canceled from New York, Boston and Philadelphia as well as the Canadian airports of Montreal, Ottawa and Quebec City. In Boston, the annual Boston Marathon when ahead of schedule despite howling winds and pouring rain as well as cold temperatures.\n\nThree people were killed in South Carolina, five in total in Texas and Kansas and five in Quebec.\n\nA strong low pressure system affected southern portions of the Rockies including the higher elevations of Colorado. Areas west of Denver received a much as 26 inches of snow (near Evergreen) with several other reports of 12 inches or more. while severe weather affected eastern portions of the state.\n\nWhile much of the Central Plains received heavy rain and damaging tornadoes, regions in higher elevations across the Rockies, including Colorado, Wyoming, Utah, Nebraska and Idaho, received snow, locally a major winter storm. Portions of central and northern Colorado received as much as 12 inches (30 cm) of snow during the overnight event.\n\nAlso on May 9, a strong tornado struck Bebejia, Chad destroying the town and killing 14 people.\n\nHeavy rain storms hit Poland om May 15 and 16, causing heavy flooding in the south and east of the country. 8 inches of rain also fell in Gdańsk causing heavy localised flooding until the 17th.\n\nOn Sunday, May 16, the rivers in Malopolska had reached alarming leavels in 6 locations, a state of flood alert was issued in 23 places. Flood alerts were announced in the communities of Liszki, Skawina, Cracow, Rzezawa, Łapanów, Bochnia, Borzęcin Gnojnik, Brest-Litovsk, Bobowa and Gorlice.\n\nBetween May 20 and 23, emergency services evacuated the commune of Wilków along with some other parts of the Lublin area as the river Vistula broke its banks. Many people did not want to leave their homes and were forcibly removed for their own safety. The river Wisła fell by 12 cm of rain fell in Sandomierz, but the level of water grew alarmingly in Lublin, Liszki and Łódź. A person was killed in Lubin as he fell into an overflowing stream near their home.\n\nMay 22 saw Warszawa’s opera hall, some schools, kindergartens and babies’ nurseries closed in areas at risk of flooding. Local and state officials also asked for the expertise of German specialists who are experienced in carrying out the mass evacuation. Several hectares of land in the commune of Wilków was flooded by the Vistula River. Wrocław was partly flooded as the river Oder broke a dyke and the district Kozanów flooding an area of about 80 hectares.\n\nOn June 3, a 3rd wave of flooding hit both Wilków, Liszki and Lubin as more powerful storms have passed over many places in the country and brought heavy rainfall. Most dykes and levees had been upgraded mostly held out in the Lublin area. The governor of Mazovia, Jacek Kozlowski, introduced a flood alert for all the municipalities and counties south of Mazovia.\n\nBetween June 3 and 4 dangerous levels of flooding returned to Lower Silesia. Local officials declared flood emergency in 16 counties and the city of Legnica The Polish Hydrological Service also confirmed that the river Oder would probably be involved in the second wave of climactic flooding.\n\nOn June 7 the districts of Tarnów had closed the locks in the drainage ditches as flooding occurred several settlements in the municipality of Wierzchosławice. In the municipality Gromnik a series of landslides occurred, with some threateningly the high voltage poorer lines in Ryglice.\n\nOn June 12, Polish Premier Donald Tusk visited a flooded village as the water began to subside.\n\nA major winter storm occurred in portions of Argentina and Chile creating hazardous traveling through several areas. Hardest hit areas were in the higher elevation along the Chile and Argentina borders. One of the main roads connecting the two counties was fully shut down while numerous trucks were left stranded in the area. The combination of heavy snow and hurricane-force winds force the shutdown of schools and businesses in Bariloche a popular resort destination in the country. Accumulations of several meters of snow fell in the Cristo Redentor Tunnel mountain pass.\n\nA winter storm affected portions of the southeastern coast of Australia and South Island, New Zealand. Heavy snows fell in the mountain regions of the Blue Mountains west of Sydney as well as Oberon and Bathurst while it disrupted air travel in Otago, New Zealand while causing numerous accidents across the area due to slippery conditions.\n\nA winter storm brushed the Antarctic Peninsula with hurricane-force winds in early July 2007. The San Martin Base weather station reported winds gusting up to on the evening of July 1, and winds up to by July 3. The strong winds caused temperatures to drop to and did not rise until July 4. Other weather stations in the Antarctic Peninsula reported similar effects.\n\nAn interaction with an area of low pressure systems across Argentina during the July 6, 7 and 8, 2007, and the entry of a massive polar cold snap made as a result the worst winter of Argentina in almost forty years, where severe snowfalls and blizzards affected the country The cold snap advanced from the south towards the central zone of the country during Friday, July 6, continuing its displacement towards the north during Saturday, July 7 and Sunday, July 8. On Monday July 9, the simultaneous presence of very cold air, above the average levels of the atmosphere as in the surface, gave place to the occurrence of snowfalls even in localities where snow is rare. This phenomenon left at least 23 people dead.\n\nIt was the third time that a phenomenon like this happened in the country. The first time was in 1912 and the second one was in 1918, occasion in which even there was major volume of snow.\n\nCentral Banbury is flooded by heavy rain. The flooded area included the bus and railway station.\n\nThe 2007 floods of Africa were reported by the UN to be one of the worst periods of flooding in recorded history. The flooding started with rains on September 14, 2007, and lasted for 3 days. 14 countries had been affected in the continent of Africa, 250 people were reported to have been killed by the flooding and 2,500,000 were affected. The UN had issued warnings of water borne diseases and locust infestations.\n\nIn Ghana 400,000 were homeless with at least 20 people dead and crops and livestock had been washed away over the 3 day event.\n\n64 people were reported killed in the Sudan. 17 people were reported dead Ethiopia. In the Afar Region, the Awash River flooded caused a dam to collapse. Around 4,500 people were stranded, surrounded by water. 150,000 people were displaced in Uganda and 21 reported dead. 170 schools were under water. 18 people were reported dead and 500 residences were washed away by floods in Rwanda. Mali saw 5 bridges had collapsed and 250 residences were washed away. 33 people were reported dead in Burkina Faso, 12 people were reported dead in Kenya and Togo reported that 20 people were reported dead\n\nHurricane Noel, which killed 163 people in the Caribbean Islands, affected most of Atlantic Canada, eastern Quebec and eastern New England as a post-tropical system with heavy rains and damaging winds in excess of 100 km/h (60 mph). The highest gust was recorded in the Wreckhouse area in Newfoundland and Labrador where gusts reached 180 km/h (110 mph). Nearly 200,000 customers in Atlantic Canada alone lost power during the height of the storm. In the northwestern most edge of the system, Noel produced a narrow swath of snow (thus the first major winter storm across those areas) which affected areas of Maine, as well as Happy Valley – Goose Bay, Newfoundland and Labrador and eastern Quebec from near Rivière-du-Loup to Sept-Îles including Rimouski, Amqui, Cap Chat, Port-Cartier and portions of Baie-Comeau and Forestville. Some areas in Quebec received over 8 inches (20 cm) of snow with the Murdochville area receiving as much as 16 inches (40 cm). 14 people were injured when an Orleans Express bus overturned on Route 132 in the Saint-Simon area. Nearly 20,000 Hydro-Québec customers were without power mostly due to a damaged transmission line in the Minganie region. The storm prompted election director to extend the voting period for school board elections, which the storm disrupted.\n\nThe first lake-effect snow event around the Great Lakes occurred as cold air swept through the region. The Upper Peninsula of Michigan saw up to a foot of snow, while up to of snow fell in northern Pennsylvania. Significant snow also fell in western New York in the typical snowbelt regions. Areas on the southern shores of Lake Superior and Georgian Bay in Ontario also received significant amount of snows in excess of 6 inches (15 cm). The low pressure disturbance continued eastward to produce significant snowfalls across the mountains of central Quebec in excess of 12 inches (30 cm), disrupting traffic in several areas.\n\nA European windstorm crosses over Scotland and plunges into the mouth of the North Sea, to the west of Norway, where its strong winds push large bodies of water Southeast, towards coastal regions in England and the Netherlands. The tidal surge puts both nations on red alert as the English evacuate some coastal villages and close the Thames Barrier. The Dutch close the Eastern Scheldt storm surge barrier and the enormous Maeslant barrier in order to prevent massive flooding as the storm mimics the situation that caused the devastating North Sea flood of 1953. For the first time since 1976, the entire Dutch coastline is put on alert and is closely monitored by officials. The tidal surge turned out to be too weak to cause any significant damage to the strong Dutch coastal defenses. In England, only minor flooding occurred.\n\nA powerful storm in the Black Sea sank or damaged 5–10 ships, one of them, the oil tanker \"MT Volganeft-139\", broke apart spilling most of its 1.3 million gallons of crude oil into the sea. The storm killed 3 crew members and the resulting oil spill killed over 30,000 birds and an unknown number of fish. Several merchant ships carrying over 6,000 tons of sulphur also sank: the \"M/S Nekhichevan\" and \"Kovel\" followed by \"M/S Volnogorsk\" when it collided with the sunken \"Kovel\"; a Georgian cargo carrying steel products also sank.\n\nFurther to the west in southeastern Europe, the storm dumped exceptional amounts of snow over parts of Austria with local reports of over a meter of snow. Some meteorologists mentioned that the weather that took place in the Alps was a once in every 30 to 50 year occurrence. The storm contributed to the closure of several mountain roads and an increased risk of avalanches over the region. The country's avalanche warning system raised its alarm level to the second-highest.\n\nA cold front pushed through eastern North America early on the 15th, bringing lake-effect snow to the typical snowbelt regions, dropping up to a foot of snow in the snow belts. The snow continued into the 17th, with snow developing across the northern Appalachians, central and eastern Quebec and northern Maine. Poor weather conditions were responsible for at least 2 deaths due to traffic accidents in Quebec on Route 175 south of Saguenay and on Highway 20 in Rimouski. Further east, significant rainfalls affected portions of the Gaspésie region with the towns of Matane, Cap-Chat and Sainte-Anne-des-Monts declaring disaster areas due to extensive flooding.\n\nA series of low pressure systems traveled across the central and eastern sections of North America, the Great Lakes and eastern Canada. While some of the systems dumped several inches of snow across portions of eastern Ontario and central Quebec on the 20th and 21st, the strongest storm produced the first major winter storm for southern Ontario and southern Quebec while also affecting portions of central and eastern Quebec and northern New Brunswick. It produced a wide swath of heavy snow in excess of 4–6 inches (10–15 cm) (with areas receiving as much as 8–12 inches (20–30 cm) ) across many regions including Ontario's Cottage country, the Ottawa region and the St Lawrence River Valley in Quebec with some snow affected portions of the Midwest United States from Nebraska to Michigan. Freezing rain and ice pellets affected areas along Highway 401 from east of London to Brockville as well as areas just east of Montreal.\n\nSeveral flights coming out of Toronto, Montreal, and Ottawa were affected. At one point during Ontario Provincial Police reported on average one motor-vehicle accident every minute. Activities surrounding the Canadian Football League's Grey Cup Match in Toronto had to be brought indoors or canceled due to the poor conditions. The storm is responsible for at least two death in Ontario including west of Renfrew on Highway 17 and on Highway 400 in Toronto. Sûreté du Québec reported well over a hundred vehicles running off the road only around Montreal and Montérégie, and a dozen more serious accidents in Mauricie. 20,000 Hydro-Québec were affected in total by power outages, with the most of them east of Montreal\n\nDuring November 25 and 26, heavy rain and snow induced flooding devastates Serbia, especially the towns of Crni Marko and Novi Pazar \n\nPreceding the large winter storm, a significant winter storm affected portions of the Canadian Maritimes and Newfoundland and Labrador on December 2. Initially a weak disturbance, it produced significant lake-effect snows across the traditional snow belts on the southern shores of Lakes Superior, Michigan, Huron and Ontario as well as Georgian Bay. The disturbance intensified over the Maritimes and dumped heavy amounts of snow across Prince Edward Island and Newfoundland and Labrador where accumulations of 8 to 20 inches (20–50 cm) were reported over central parts of the province. The storm registered a minimum of 957 mb off the Atlantic Coast two days later. Due to heavy snow, strong winds, sleet and freezing rain, over 100,000 customers in Newfoundland lost power, with a large portion of the capital St. John's being blacked out for several hours. In the Bonavista Peninsula, several transmission lines and support structures collapsed and telephone service was also disabled for a certain period including cellphone coverage. Some residents remained without power for over a week.\n\nA low-pressure system developed across the southwestern United States moved across the central parts of North America on December 1, becoming a Colorado Low with an initial between moving from Nebraska to northern Ontario and into the Middle Atlantic Coast near New York City. A second band originating from a band of thunderstorms across Missouri then traveled across the Great Lakes and the Northeast. A newly formed low pressure off the coast of New Jersey then moved across Maine and the Canadian Maritimes.\n\nAreas of the Middle Plains and the lower Great Lakes including Des Moines, Chicago, Milwaukee, Detroit and Toronto received a significant wintry mix of precipitation before changing to rain and thunderstorms on December 1 and 2. Des Moines International Airport was shut down for several hours due to the icing conditions on runways and an American Airlines flight with 44 passengers slipped out of a taxiway while another skidded out of a runway at Madison, Wisconsin's Dane County Regional Airport. Numerous passengers were stranded for several hours at Chicago's O'Hare International Airport where 400 flights were canceled on December 1 alone. About 140,000 customers in Illinois alone lost power.\n\nPortions of Wisconsin, Minnesota and northern Ontario received several inches of snow while the mountain regions of Colorado received as much as four feet of snow (120 cm), resulting in the postponement of the men's Super-G alpine skiing event in Beaver Creek, Colorado, where was reported.\n\nPortions of the Northeast including most of northern and eastern Ontario and central and southern Quebec received 8 to of snow from the second band of precipitation while freezing rain was reported south of the Great Lakes across New York and Pennsylvania. Portions of Maine and the Maritimes affected by the coastal low received as much as 18 inches (45 cm) of snow.\n\nThe storm was responsible for at least 16 deaths including three in Quebec, one in New York, one in Maine, one in Indiana, three in Wisconsin, two in Illinois, three in Michigan, one in Utah, and one in Colorado.\n\nAdditionally, on December 1, a large storm off the Pacific Coast brought heavy snow to portions of British Columbia, including the South Coast and Vancouver Island, with amounts in higher elevations exceeding 16 inches (40 cm) and significant accumulations also for Metro Vancouver. Another large storm called a Pineapple Express brought torrential rains to the same areas on December 3 with very strong winds across portions of Oregon and Washington states, freezing rain into valley areas of central British Columbia, and heavy snow of up to across mountainous areas. The heavy rains caused a mudslide inside Stanley Park which closed its seawall which had just recently re-opened in November after it was heavily damaged during a major wind storm in December 2006. Extensive flooding was reported across many areas of Washington and Oregon after heavy rains with amounts of up to were reported. Coast Guard helicopters had to evacuate and saved over 100 residents who were trapped by the high water levels. The town of Vernonia, Oregon was completely cut off by the water and mudslides. Wind gusts locally exceeded 100 mph (160 km/h) with the highest gust registered at recorded in Bay City, Oregon. Over 100,000 customers from northern California to Washington lost electricity while 40,000 lost power in British Columbia. In addition, Amtrak service between Portland, Oregon and Vancouver, British Columbia was disrupted for at least two days.\n\nThe storm was responsible for at least 10 deaths, including five in a single vehicle crash near Prince George, British Columbia where there was snow-covered roads. Three people were killed in Washington and two in Oregon. From the perspective of Chicago, the storm was viewed as an Alberta clipper with the potential for heavy snowfall. During the evening of December 2, the storm was reported to have a central pressure of 949 mb, pressures associated with a Category 3 hurricane.\n\nThe same storm entered the Upper Midwest as an Alberta clipper, which brought light to moderate snowfall over much of the Midwest on December 4 and early December 5, and overspread the Ohio Valley and Mid-Atlantic states on December 5. The Minneapolis-St. Paul, Madison, Milwaukee, and Chicago metropolitan areas saw upwards of 4 to of snow from the storm system, with areas further south and east receiving less.\n\nA series of winter storms impacted widespread areas of North America over a nine-day period. From December 8 to December 11, another major ice storm impacted the midsection of the United States from Texas, northeast through the Midwest, through the Mid-Atlantic States, and into southern New England. At least 38 people were killed by the ice storms, including 23 in Oklahoma, four in Kansas, three in Missouri, and one in Nebraska. Most of the fatalities were the result of traffic accidents caused by the icy weather, including four people in a single accident on Interstate 40 west of Okemah, Oklahoma. The storm caused the largest power outage in Oklahoma history, where 600,000 homes and businesses lost power, while 350,000 customers were also without power in other states, including 100,000 in both Missouri and Kansas, and scattered power outages in Nebraska, Iowa, and Illinois. Overall, over 1.5 million customers lost power throughout the Central United States with some being without electricity for over one week. The storms caused widespread school and flight cancellations with Chicago O'Hare International Airport cancelling at least 560 flights, while Tulsa International Airport was forced to halt flights on the 10th after losing power for 10 hours.\n\nThe energy of the second ice storm produced significant snows over the northeastern part of the US and the Golden Horseshoe region of Ontario on December 13 and dumped as much as 12 inches (30 cm) of snow in parts of New England and New York state. A large system crossed the Central and Eastern part of the continent from December 15 to December 17 dumping as much as over parts of Ontario and New England with mixed precipitation south of the heavy snow bands. The snow storm was responsible for at least 17 deaths across five states and three Canadian provinces as well as numerous flights and school cancellations from Michigan to the Canadian Maritimes.\n\nAfter a mild start to the cold season, a large area of Spain was hit by its first winter storm of the season which brought heavy snow and rain as well as strong winds and much colder temperatures. In the eastern part of the country, several roads were closed due to high amounts of snow. Portions of a key road link between Madrid and Barcelona was also shut down due to the weather.\n\nA new winter storm affected most of Central North America from the Texas Panhandle to northern Ontario while heavy rains, areas of freezing rain, very strong winds and warm temperatures affected most of Eastern North America. Blizzard warnings were issued at one point over southwestern Kansas and locally a foot of snow fell in some regions with several regions registering wind gusts of over . Up to a foot of snow fell across much of Minnesota, Wisconsin, and Michigan, and freezing rain was also reported in many areas. Parts of Michigan's Upper Peninsula saw upwards of of snow. The storm also produced strong winds, including wind gusts of across Lake Michigan, and gusts ranging from 50–68 mph across the Chicago area. The winds caused 300 flights to be canceled at Chicago-O'Hare International Airport. Also in Chicago, crews reported that 170 signals had been knocked out and more than 500 reports of fallen limbs had been attributed to the storm. 11,000 customers in Wisconsin, 92,000 in Michigan and 225,000 in Illinois lost power. The storm was responsible for at least 25 deaths across seven US states and one Canadian province, including eight in Minnesota, three in Indiana, three in Wyoming, five in Wisconsin, one in Texas, one in Kansas, one in Michigan, and three in New Brunswick. In Texas, the fatal crash included 50 vehicles on Interstate 40 while in Kansas and Missouri crashes on Interstate 70 and Interstate 29 respectively also involved several vehicles. Lake-effect snows across the traditional snowbelt region in the Great Lakes also fell on Christmas Eve.\n\n\n"}
{"id": "3222200", "url": "https://en.wikipedia.org/wiki?curid=3222200", "title": "Homonuclear molecule", "text": "Homonuclear molecule\n\nHomonuclear molecules, or homonuclear species, are molecules composed of only one type of element. Homonuclear molecules may consist of various numbers of atoms, depending on the element's properties. Some elements form molecules of more than one size. Noble gases rarely form bonds, so they only have one atom. The most familiar homonuclear molecules are diatomic, meaning they consist of two atoms, though not all diatomic molecules are homonuclear. Homonuclear diatomic molecules include hydrogen (H), oxygen (O), nitrogen (N) and all of the halogens. Ozone (O) is a common triatomic homonuclear molecule. Homonuclear tetratomic molecules include arsenic (As) and phosphorus (P).\n\nAllotropes are different chemical forms of the same element (not containing any other element). In that sense, allotropes are all homonuclear. Many elements have multiple allotropic forms. In addition to the most common form of gaseous oxygen, O, and ozone, there are other allotropes of oxygen. Sulfur forms several allotropes containing different numbers of sulfur atoms, including diatomic, triatomic, hexatomic and octatomic (S, S, S, S) forms, though the first three are rare. The element carbon is known to have a number of homonuclear molecules, the best known being buckminsterfullerene or \"buckyball\".\n\n"}
{"id": "318869", "url": "https://en.wikipedia.org/wiki?curid=318869", "title": "Iceland spar", "text": "Iceland spar\n\nIceland spar, formerly known as Iceland crystal (; lit. \"silver-rock\"), is a transparent variety of calcite, or crystallized calcium carbonate, originally brought from Iceland, and used in demonstrating the polarization of light (see polarimetry). It occurs in large readily cleavable crystals, is easily divisible into rhombuses, and is remarkable for its birefringence. This means that the index of refraction of the crystal is different for light of different polarization. A ray of unpolarized light passing through the crystal divides into two rays of perpendicular polarization directed at different angles, called double refraction. So objects seen through the crystal appear doubled.\n\nHistorically, the double-refraction property of this crystal was important to understanding the nature of light as a wave. This was studied at length by Christiaan Huygens and Isaac Newton. Sir George Stokes also studied the phenomenon. Its complete explanation in terms of light polarization was published by Augustin-Jean Fresnel in the 1820s.\n\nMines producing Iceland spar include many mines producing related calcite and aragonite as well as those famously in Iceland, productively in the greater Sonoran desert region as in Santa Eulalia, Chihuahua, Mexico and New Mexico, United States, as well as in the People's Republic of China.\n\nIt has been speculated that the sunstone (, a different mineral from the gem-quality sunstone) mentioned in medieval Icelandic texts was Iceland spar, and that Vikings used its light-polarizing property to tell the direction of the sun on cloudy days for navigational purposes. The polarization of sunlight in the Arctic can be detected, and the direction of the sun identified to within a few degrees in both cloudy and twilight conditions using the sunstone and the naked eye. The process involves moving the stone across the visual field to reveal a yellow entoptic pattern on the fovea of the eye, probably Haidinger's brush. The recovery of an Iceland spar sunstone from the Elizabethan ship \"Alderney\", which sank in 1592, suggests that this navigational technology may have persisted after the invention of the magnetic compass.\n\nWilliam Nicol (1770–1851) invented the first polarizing prism, using Iceland spar to create his Nicol prism.\n"}
{"id": "216161", "url": "https://en.wikipedia.org/wiki?curid=216161", "title": "Industrial waste", "text": "Industrial waste\n\nIndustrial waste is the waste produced by industrial activity which includes any material that is rendered useless during a manufacturing process such as that of factories, industries, mills, and mining operations. It has existed since the start of the Industrial Revolution. Some examples of industrial wastes are chemical solvents, pigments, sludge, metals, ash, paints, sandpaper, paper products, industrial by-products, and radioactive wastes.\n\nToxic waste, chemical waste, industrial solid waste and municipal solid waste are designations of industrial wastes. Sewage treatment plants can treat some industrial wastes, i.e. those consisting of conventional pollutants such as biochemical oxygen demand (BOD). Industrial wastes containing toxic pollutants require specialized treatment systems. (\"See\" Industrial wastewater treatment).\n\nProduction sites are commonly located near bodies of water due to industrial dependence on large amounts of water as an input. Many areas that are becoming industrialized do not yet have the resources or technology to dispose of waste with lesser effects on the environment. Both untreated and partially treated wastewater are commonly fed back into a near lying body of water. Metals, chemicals and sewage released into bodies of water directly affect marine ecosystems and the health of those who depend on the waters as food or drinking water sources. Toxins from the wastewater can kill off marine life or cause varying degrees of illness to those who consume these marine animals, depending on the contaminant. Metals and chemicals released into bodies of water affect the marine ecosystems. Wastewater containing nitrates and phosphates often causes Eutrophication which can kill off existing life in the water. A Thailand study focusing on water pollution origins found that the highest concentrations of water contamination in the U-tapao river had a direct correlation to industrial wastewater. \n\nIn Thailand the roles in Municipal solid waste (MSW) management and industrial waste management are organized by the Royal Thai Government, which is then divided into central government, regional government, and local government. Each government is responsible for different tasks. The central government is responsible for stimulating regulation, policies, and standards. The regional governments are responsible for coordinating the central and local governments. The local governments are responsible for waste management in their governed area. However, the local governments do not dispose of the waste by themselves but instead hire private companies that have been granted the right from the Pollution Control Department (PCD) in Thailand. The main companies are Bangpoo Industrial Waste Management Center, General Environmental Conservation Public Company Limited (GENCO), SGS Thailand, Waste Management Siam LTD (WMS), and Better World Green Public Company Limited (BWG). These companies are responsible for the waste they have received from their customers before releasing it to the environment, burying it.\n\nThe 1976 Resource Conservation and Recovery Act (RCRA) provides for federal regulation of waste while allowing for individual state control through EPA approved waste disposal programs.\n\nState compliance is monitored by EPA inspections. In the case that waste management guideline standards are not met, action against the site will be taken. Compliance errors may be corrected by enforced cleanup directly by the site responsible for the waste or by a third party hired by that site. Prior to this act, open dumping or releasing wastewater into nearby bodies of water were common waste disposal methods. The negative externalities on human health and environmental health led to the need for such a regulations. The RCRA framework provides specified subsections defining nonhazardous and hazardous waste materials and how each should be properly managed and disposed of. Guidelines for the disposal of nonhazardous solid waste includes the banning of open dumping. Hazardous waste is monitored in a cradle to grave fashion.The EPA now manages 2.96 million tons of solid, hazardous and industrial waste. Since establishment, the RCRA has undergone reforms as inefficiencies arise and as waste management evolves. \n"}
{"id": "30656206", "url": "https://en.wikipedia.org/wiki?curid=30656206", "title": "January 25–27, 2011 North American blizzard", "text": "January 25–27, 2011 North American blizzard\n\nThe January 25–27, 2011 North American blizzard was a major Mid-Atlantic nor'easter and winter storm, and a New England blizzard that affected portions of the northeastern United States and Canada. This storm came just two weeks after a previous major blizzard had already affected most of these same areas earlier on the same month of January 2011. The storm also came just one month after a previous major blizzard that affected the entire area after Christmas in December 2010. This storm was the third significant snowstorm to affect the region during the 2010–11 North American winter storm season. It was followed a few days later by another massive storm that blanketed much of the United States and Canada.\n\nReports of 10–15 inches were common in southern New England.\n\nNear 20 inches were reported on the ground at Newark, New Jersey by January 27, where snowfall rates were at 3 to 4 inches an hour. Portions of Southern New Jersey saw variable snow amounts. Atlantic City saw about 3 inches of snow and Cape May reported about 2.5\". All across the central portion of the state from Westfield Township to Freehold reported snowfall amounts in the ranges of 21–26 inches.\n\nThere were multiple reports of 18–19 inches across the state. During the Blizzard there were snowfall rates of up to 4 inches per hour. Within the first 2 hours of the storm, Meriden recorded 7.5 inches on the ground. The NWS forecasted 5–9 inches Thursday evening. Once the snow ended in the morning it was clear that forecast was underestimated. The higher amounts were in the southern and eastern portions of the state, with the least in the northwestern corner.\n\nAt least 19 inches of snow accumulated in New York City in wake of the storm. As a result, for the ninth time in the city's history, all public schools were closed. All area airports were also closed. Airports re-opened later in the afternoon of January 27. All non-emergency city government offices were closed as well. After the additional snowfall, this made January 2011 the snowiest January on record for New York City. The storm also caused two deaths. A 64-year-old woman was struck and killed by a snow plow on January 25. New York Mayor Michael Bloomberg declared a citywide weather emergency early on the 25th, which resulted in the closing of schools and government buildings as well as temporary closures of all of the area's Airports.\n\nPhiladelphia received almost 15 inches of snow after the storm ended. The one-two punch hit the local Philadelphia the hardest. Many municipalities declared snow emergencies.\n\nThere were more than three dozen accidents during the morning of the 26th. This included an ambulance on its way to a hospital with a patient in West Nantmeal Township (Chester County). No injuries were reported. SEPTA regional transportation reported service disruptions along with systemwide delays. Philadelphia International Airport had 41 morning flights cancelled and had average delays of three hours. With the first surge of snow on the morning of the 26th, many school districts made last minute decisions to either close or have delayed openings. Many closed early that afternoon. Even Saint Joseph's University and West Chester University closed. Conditions though became worse when the second punch of heavier snow came through on the evening of the 26th. Snowfall rates reached 2 to 4 inches per hour. In Philadelphia, the emergency 311 hot line had four times the normal rate of calls. Many vehicles and buses were stuck in the snow. Some SEPTA bus drivers were stranded up to twelve hours. SEPTA still had service suspended on about one third of its routes on the 28th.\n\nAbout 1,500 travelers were stranded overnight on the 26th at Philadelphia International Airport. Schools and courts were closed on the 27th. Schools were also closed on the 28th. In the northwest suburbs, numerous crashes were reported in Berks, Chester and Montgomery Counties on Pennsylvania State Routes 100 and 29 as well as U.S. Routes 202 and 422. In West Pottsgrove (Montgomery County), a 45-year-old man was injured after his vehicle slid down an embankment on Westbound U.S. Route 422 and rolled over. In Northampton County, westbound Interstate 78 was closed for three hours overnight on the 26th between Pennsylvania State Routes 412 and 309 because of disabled commercial vehicles. The weight of the snow also downed some trees in the southeast part of the state, but power outages remained isolated. The continued onslaught of winter weather was causing numerous municipalities to exhaust their snow removal budgets. The city of Philadelphia estimated the clean-up costs from the latest winter storm was at least 6 million dollars.\n\nResidents throughout the Philadelphia area reported thundersnow, a rare meteorological phenomenon in which thunder and lightning occur concurrently with the falling snow. Chester County received over 20 inches of snow. In Berks County most residents received around 13–14 inches of snow.\n\nWashington D.C. received 5 to 10 inches of heavy, wet snow. As many as 650,000 people lost power as a result of the blizzard. Commutes across the region were difficult the afternoon of the storm, with many people spending four to eight hours in traffic on the way home; some, on the George Washington Memorial Parkway, were stuck for up to fourteen hours, and many abandoned their vehicles on the roadway.\n\nA 77-year-old pedestrian in Pasadena, Maryland was killed when he was struck by a snow plow.\n\n"}
{"id": "14260764", "url": "https://en.wikipedia.org/wiki?curid=14260764", "title": "John Sinclair (environmentalist)", "text": "John Sinclair (environmentalist)\n\nJohn Sinclair AO (born 13 July 1939) received the Global 500 Roll of Honour award in 1990, and was awarded the Goldman Environmental Prize in 1993. \n\nBorn in Maryborough, Queensland, Australia, Sinclair fought for thirty years to protect Fraser Island, and succeeded in stopping logging of the island's rainforest, and sand mining by multinational corporations.\n"}
{"id": "12588438", "url": "https://en.wikipedia.org/wiki?curid=12588438", "title": "Lanthanum strontium cobalt ferrite", "text": "Lanthanum strontium cobalt ferrite\n\nLanthanum strontium cobalt ferrite (LSCF), also called lanthanum strontium cobaltite ferrite is a specific ceramic oxide derived from lanthanum cobaltite of the ferrite group. It is a phase containing lanthanum(III) oxide, strontium oxide, cobalt oxide and iron oxide (cobalt iron oxide).\n\nIt is black in color and crystallizes in a distorted hexagonal perovskite structure. LSCF undergoes phase transformations at various temperatures depending on the composition. This material is a mixed ionic electronic conductor with comparatively high electronic conductivity (200+ S/cm) and good ionic conductivity (0.2 S/cm). It is typically non-stoichiometric and can be reduced further at high temperature in low oxygen partial pressures or in the presence of a reducing agent such as carbon.\n\nLSCF is being investigated as a material for intermediate temperature solid oxide fuel cell cathodes and, potentially as a Direct carbon fuel cell anode.\n\nLSCF is also investigated as a membrane material for separation of oxygen from air, for use in e.g. cleaner burning power plants. \n\nIt is commercially available.\n\n\n"}
{"id": "47818742", "url": "https://en.wikipedia.org/wiki?curid=47818742", "title": "McIlvaine buffer", "text": "McIlvaine buffer\n\nMcIlvaine buffer is a buffer solution composed of citric acid and disodium hydrogen phosphate, also known as citrate-phosphate buffer. It was introduced in 1921 by a United States agronomist Theodore Clinton McIlvaine from West Virginia University, and can be prepared in pH 2.2 to 8 by mixing two stock solutions.\n\nMcIlvaine buffer can be used to prepare a water-soluble mounting media when mixed 1:1 with glycerol.\n\nPreparation of McIlvaine buffer requires disodium phosphate and citric acid. One liter of 0.2M stock solution of disodium phosphate can be prepared by dissolving 28.38g of disodium phosphate in water, and adding a quantity of water sufficient to make one liter. One liter of 0.1M stock solution of citric acid can be prepared by dissolving 19.21g of citric acid in water, and adding a quantity of water sufficient to make one liter. From these stock solutions, McIlvaine buffer can be prepared in accordance with the following table:\n"}
{"id": "28544212", "url": "https://en.wikipedia.org/wiki?curid=28544212", "title": "Mud blenny", "text": "Mud blenny\n\nThe mud blenny (\"Parablennius lodosus\") is a species of combtooth blenny found in the western Indian Ocean. This species reaches a length of SL.\n"}
{"id": "35788567", "url": "https://en.wikipedia.org/wiki?curid=35788567", "title": "NEXT (ion thruster)", "text": "NEXT (ion thruster)\n\nThe NASA Evolutionary Xenon Thruster (NEXT) project at Glenn Research Center is an ion thruster about three times as powerful as the NSTAR used on Dawn and Deep Space 1 spacecraft.\n\nNEXT affords larger delivered payloads, smaller launch vehicle size, and other mission enhancements compared to chemical and other electric propulsion technologies for Discovery, New Frontiers, Mars Exploration, and Flagship outer-planet exploration missions. Glenn Research Center manufactured the test engine's core ionization chamber, and Aerojet Rocketdyne designed and built the ion acceleration assembly. The first two flight units will be available in early 2019.\n\nThe NEXT engine is a type of solar electric propulsion in which thruster systems use the electricity generated by the spacecraft's solar panel to accelerate the xenon propellant to speeds of up to 90,000 mph (145,000 km/h or 40 km/s). NEXT can produce 6.9 kW thrust power and 236 mN thrust. It can be throttled down to 0.5 kW power, and has a specific impulse of 4190 seconds (compared to 3120 for NSTAR). The NEXT thruster has demonstrated a total impulse of 17 MN·s; which is the highest total impulse ever demonstrated by an ion thruster. A beam extraction area 1.6 times that of NSTAR allows higher thruster input power while maintaining low voltages and ion current densities, thus maintaining thruster longevity.\n\nIn November 2010, it was revealed that the prototype had completed a 48,000 hours (5.5 years) test in December 2009. Thruster performance characteristics, measured over the entire throttle range of the thruster, were within predictions and the engine showed little signs of degradation and is ready for mission opportunities.\n\nNEXT completed its System Requirement Review in July 2015 and Preliminary Design Review in February 2016. The first two flight units will be available in early 2019. The \"CAESAR\" mission concept to comet 67P/Churyumov–Gerasimenko is a finalist for the New Frontiers program mission #4, and if selected in 2019, it will be propelled by the NEXT ion engine. After that, it will be a commercial product for purchase by NASA and non-NASA customers. Aerojet Rocketdyne, and their major sub-contractor ZIN Technologies retain the rights to produce the system, known as NEXT-C for future commercialization.\n\n"}
{"id": "4131940", "url": "https://en.wikipedia.org/wiki?curid=4131940", "title": "Nuclear power in Japan", "text": "Nuclear power in Japan\n\nPrior to the 2011 Tōhoku earthquake and tsunami, Japan had generated 30% of its electrical power from nuclear reactors and planned to increase that share to 40%. Nuclear power energy was a national strategic priority in Japan.\nAs of May 2018, there are 42 operable reactors in Japan. \nOf these, 8 reactors in 5 power plants are operating.\n\nThough all of Japan's nuclear reactors successfully withstood shaking from the Tohoku earthquake, flooding from the ensuing tsunami caused the failure of cooling systems at the Fukushima I Nuclear Power Plant on 11 March 2011. \nJapan's first-ever nuclear emergency was declared, and 140,000 residents within of the plant were evacuated. \nA comprehensive assessment by international experts on the health risks associated with the Fukushima I nuclear power plant disaster concluded in 2013 that, for the general population inside and outside Japan, the predicted risks were low and no observable increases in cancer rates above baseline rates were anticipated. \nAll Japan's nuclear plants were closed, or their operations suspended for safety inspections. The last of Japan's fifty reactors (Tomari-3) went offline for maintenance on 5 May 2012, leaving Japan completely without nuclear-produced electrical power for the first time since 1970.\n\nProblems in stabilizing the triple reactor meltdowns at Fukushima I nuclear plant hardened attitudes to nuclear power. \nIn June 2011, more than 80 percent of Japanese said they were anti-nuclear and distrusted government information on radiation. \nBy October 2011, there had been electricity shortages, but Japan survived the summer without the extensive blackouts that some had predicted. \nAn energy white paper, approved by the Japanese Cabinet in October 2011, stated that \"Public confidence in safety of nuclear power was greatly damaged\" by the Fukushima nuclear disaster, and called for a reduction in the nation’s reliance on nuclear power.\n\nDespite protests, on 1 July 2012 unit 3 of the Ōi Nuclear Power Plant was restarted. \nIn September 2013, Ōi units 3 and 4 went offline, making Japan again completely without nuclear-produced electrical power. \nOn August 11, 2015, the Sendai Nuclear Power Plant was brought back online, followed by two units (3 and 4) of the Takahama Nuclear Power Plant on January 29, 2016. \nHowever, Unit 4 was shut down three days after restart due to an internal failure and Unit 3 in March 2016 after district court in Shiga prefecture issued an injunction to halt operation of Takahama Nuclear Power Plant. Though 43 of Japan's pre-2011 total of 54 plants remain idled, the Ministry of Economy, Trade and Industry said in 2017 that if the country is to meet its obligations under the Paris climate accord, then nuclear energy needs to make up between 20-22% of the nation's portfolio mix. 26 restart applications are now pending with an estimated 12 units to come back in service by 2025 and 18 by 2030.\n\nIn 1954, the Operations Coordinating Board of the United States National Security Council proposed that the U.S. government undertake a \"vigorous offensive\" urging nuclear energy for Japan in order to overcome the widespread reluctance of the Japanese population to build nuclear reactors in the country. Thirty two million Japanese people, a third of the Japanese population, signed a petition calling for banning hydrogen bombs. The Washington Post called for adopting the proposal to build nuclear reactors in Japan, stating: \"Many Americans are now aware...that the dropping of the atomic bombs on Japan was not necessary...How better to make a contribution to amends than by offering Japan...atomic energy.\" For several years starting in 1954, the United States Central Intelligence Agency and other U.S. government agencies ran a propaganda war targeting the Japanese population to vanquish the Japanese people's opposition to nuclear power.\n\nIn 1954, Japan budgeted 230 million yen for nuclear energy, marking the beginning of the Japan's nuclear program. The Atomic Energy Basic Law limited activities to only peaceful purposes. The first nuclear reactor in Japan was built by the UK's GEC and was commissioned in 1966.\n\nIn the 1970s, the first light water reactors were built in cooperation with American companies. These plants were bought from U.S. vendors such as General Electric and Westinghouse with contractual work done by Japanese companies, who would later get a license themselves to build similar plant designs. Developments in nuclear power since that time have seen contributions from Japanese companies and research institutes on the same level as the other big users of nuclear power. From the early 1970s to the present, the Japanese government promoted the siting of nuclear power plants through a variety of policy instruments involving soft social control and financial incentives. By offering large subsidies and public works projects to rural communities and by using educational trips, junkets for local government officials, and OpEds written as news by pro-nuclear supporters, the central government won over the support of depopulating, hard-on-their-luck coastal towns and villages.\n\nJapan's nuclear industry was not hit as hard by the effects of the Three Mile Island accident (TMI) or the Chernobyl disaster as some other countries. \nConstruction of new plants continued to be strong through the 1980s, 1990s, and up to the present day. \nWhile many new plants had been proposed, all were subsequently canceled or never brought past initial planning. \nCancelled plant orders include:\n\nHowever, starting in the mid-1990s there were several nuclear related accidents and cover-ups in Japan that eroded public perception of the industry, resulting in protests and resistance to new plants. \nThese accidents included the Tokaimura nuclear accident, the Mihama steam explosion, cover-ups after an accident at the Monju reactor, among others, more recently the Chūetsu offshore earthquake aftermath. \nWhile exact details may be in dispute, it is clear that the safety culture in Japan's nuclear industry has come under greater scrutiny.\n\nOn April 18, 2007, Japan and the United States signed the United States-Japan Joint Nuclear Energy Action Plan, aimed at putting in place a framework for the joint research and development of nuclear energy technology. \nEach country will conduct research into fast reactor technology, fuel cycle technology, advanced computer simulation and modeling, small and medium reactors, safeguards and physical protection; and nuclear waste management. \nIn March 2008, Tokyo Electric Power Company announced that the start of operation of four new nuclear power reactors would be postponed by one year due to the incorporation of new earthquake resistance assessments. \nUnits 7 and 8 of the Fukushima Daiichi plant would now enter commercial operation in October 2014 and October 2015, respectively. \nUnit 1 of the Higashidori plant is now scheduled to begin operating in December 2015, while unit 2 will start up in 2018 at the earliest. \nAs of September 2008, Japanese ministries and agencies were seeking an increase in the 2009 budget by 6%. \nThe total requested comes to 491.4 billion Japanese yen (4.6 billion USD), and the focuses of research are development of the fast breeder reactor cycle, next-generation light water reactors, the Iter project, and seismic safety.\n\nA 2011 independent investigation in Japan has \"revealed a long history of nuclear power companies conspiring with governments to manipulate public opinion in favour of nuclear energy\". One nuclear company \"even stacked public meetings with its own employees who posed as ordinary citizens to speak in support of nuclear power plants\". \nAn energy white paper, approved by the Japanese Cabinet in October 2011, says \"public confidence in safety of nuclear power was greatly damaged\" by the Fukushima disaster, and calls for a reduction in the nation’s reliance on nuclear power. \nIt also omits a section on nuclear power expansion that was in last year’s policy review. \nNuclear Safety Commission Chairman Haruki Madarame told a parliamentary inquiry in February 2012 that \"Japan's atomic safety rules are inferior to global standards and left the country unprepared for the Fukushima nuclear disaster last March\". \nThere were flaws in, and lax enforcement of, the safety rules governing Japanese nuclear power companies, and this included insufficient protection against tsunamis.\n\nOn 6 May 2011, Prime Minister Naoto Kan ordered the Hamaoka Nuclear Power Plant be shut down as an earthquake of magnitude 8.0 or higher is likely to hit the area within the next thirty years.\n\nAs of 27 March 2012, Japan had only one out of 54 nuclear reactors operating; the Tomari-3, after the Kashiwazaki-Kariwa 6 was shut down. \nThe Tomari-3 was shut down for maintenance on 5 May, leaving Japan with no nuclear-derived electricity for the first time since 1970, when the country's then only two reactors was taken offline five days for maintenance. On 15 June 2012, approval was given to restart Ōi Units 3 and 4 which could take six weeks to bring them to full operation. On 1 July 2012 unit 3 of the Ōi Nuclear Power Plant was restarted. \nThis reactor can provide 1,180 MW of electricity. On 21 July 2012 unit 4 was restarted, also 1,180 MW. \nThe reactor was shut down again on 14 September 2013, again leaving Japan with no operating power reactors.\n\nGovernment figures in the 2014 Annual Report on Energy show that Japan depended on imported fossil fuels for 88% of its electricity in fiscal year 2013, compared with 62% in fiscal 2010. Without significant nuclear power, the country was self-sufficient for just 6% of its energy demand in 2012, compared with 20% in 2010. \nThe additional fuel costs to compensate for its nuclear reactors being idled was ¥3.6 trillion. \nIn parallel, domestic energy users have seen a 19.4% increase in their energy bills between 2010 and 2013, while industrial users have seen their costs rise 28.4% over the same period.\n\nIn 2018 the Japanese government revised its energy plan to update the 2030 target for nuclear energy to 20%-22% of power generation by restarting reactors, compared to LNG 27%, coal 25%, renewables 23% and oil 3%. This would reduce Japan's carbon dioxide emissions by 26% compared to 2013, and increase self-sufficiency to about 24% by 2030, compared to 8% in 2016.\n\nThe National Diet of Japan Fukushima Nuclear Accident Independent Investigation Commission (NAIIC) is the first independent investigation commission by the National Diet in the 66-year history of Japan’s constitutional government. \nNAICC was established on December 8, 2011 with the mission to investigate the direct and indirect causes of the Fukushima nuclear accident. \nNAICC submitted its inquiry report to both houses on July 5, 2012.\n\nThe 10-member commission compiled its report based on more than 1,167 interviews and 900 hours of hearings. \nIt was a six-month independent investigation, the first of its kind with wide-ranging subpoena powers in Japan's constitutional history, which held public hearings with former Prime Minister Naoto Kan and Tokyo Electric Power Co's former president Masataka Shimizu, who gave conflicting accounts of the disaster response. \nThe commission chairman, Kiyoshi Kurokawa, declared with respect to the Fukushima nuclear incident: “It was a profoundly man-made disaster — that could and should have been foreseen and prevented.” \nHe added that the \"fundamental causes\" of the disaster were rooted in \"the ingrained conventions of Japanese culture.\" \nThe report outlines errors and willful negligence at the plant before the 2011 Tōhoku earthquake and tsunami on March 11, 2011 and a flawed response in the hours, days and weeks that followed. It also offers recommendations and encourages Japan's parliament to \"thoroughly debate and deliberate\" the suggestions.\n\nJapan's new energy plan, approved by the Liberal Democratic Party cabinet in April 2014, calls nuclear power \"the country's most important power source\". Reversing a decision by the previous Democratic Party, the government will re-open nuclear plants, aiming for \"a realistic and balanced energy structure\". In May 2014 the Fukui District Court blocked the restart of the Oi reactors. \nIn April 2015 courts blocked the restarting of two reactors at Takahama Nuclear Power Plant but permitted the restart of two reactors at Sendai Nuclear Power Plant. The government hopes that nuclear power will produce 20% of Japan’s electricity by 2030.\n\nAs of June 2015, approval was being sought from the new Nuclear Regulatory Agency for 24 units to restart, of the 54 pre-Fukushima units. \nThe units also have to be approved by the local prefecture authorities before restarting.\n\nIn July 2015 fuel loading was completed at the Sendai-1 nuclear plant, it restarted August 11, 2015 and was followed by unit 2 on November 1, 2015. Japan’s Nuclear Regulatory Authority approved the restart of Ikata-3 which took place on April 19, 2016, this reactor is the fifth to receive approval to restart. The Takahama Nuclear Power Plant unit 4 restarted in May 2017 and unit 3 in June 2017.\n\nIn November 2016 Japan signed a nuclear cooperation agreement with India. \nJapanese nuclear plant builders saw this as potential lifeline given that domestic orders had ended following the Fukushima disaster, and India is proposing to build about 20 new reactors over the next decade. However, there is Japanese domestic opposition to the agreement, as India has not agreed to the Treaty on the Non-Proliferation of Nuclear Weapons.\n\nIn 2014, following the failure of the prototype Monju sodium-cooled fast reactor, Japan agreed to cooperate in developing the French ASTRID demonstration sodium-cooled fast breeder reactor. As of 2016, France was seeking the full involvement of Japan in the ASTRID development.\n\nJapan has had a long history of earthquakes and seismic activity, and destructive earthquakes, often resulting in tsunamis, occur several times a century. Due to this, concern has been expressed about the particular risks of constructing and operating nuclear power plants in Japan. Amory Lovins has said: \"An earthquake-and-tsunami zone crowded with 127 million people is an un-wise place for 54 reactors\". To date, the most serious seismic-related accident has been the Fukushima Daiichi nuclear disaster, following the 2011 Tōhoku earthquake and tsunami.\n\nProfessor Katsuhiko Ishibashi, one of the seismologists who have taken an active interest in the topic, coined the term \"genpatsu-shinsai\" (原発震災), from the Japanese words for \"nuclear power\" and \"quake disaster\" to express the potential worst-case catastrophe that could ensue. Dr Kiyoo Mogi, former chair of the Japanese Coordinating Committee for Earthquake Prediction, has expressed similar concerns, stating in 2004 that the issue 'is a critical problem which can bring a catastrophe to Japan through a man-made disaster'.\n\nWarnings from Kunihiko Shimazaki, a professor of seismology at the University of Tokyo, were also ignored. In 2004, as a member of an influential cabinet office committee on offshore earthquakes, Mr. Shimazaki \"warned that Fukushima's coast was vulnerable to tsunamis more than twice as tall as the forecasts of as much as five meters put forth by regulators and Tokyo Electric\". Minutes of the meeting on Feb. 19, 2004, show that the government bureaucrats running the committee moved quickly to exclude his views from the committee's final report. He said the committee did not want to force Tokyo Electric to make expensive upgrades at the plant.\n\nHidekatsu Yoshii, a member of the House of Representatives for Japanese Communist Party and an anti-nuclear campaigner, warned in March and October 2006 about the possibility of the severe damage that might be caused by a tsunami or earthquake. During a parliamentary committee in May 2010 he made similar claims, warning that the cooling systems of a Japanese nuclear plant could be destroyed by a landslide or earthquake. In response Yoshinobu Terasaka, head of the Nuclear and Industrial Safety Agency, replied that the plants were so well designed that \"such a situation is practically impossible\". Following damage at the Kashiwazaki-Kariwa Nuclear Power Plant due to the 2007 Chūetsu offshore earthquake, Kiyoo Mogi called for the immediate closure of the Hamaoka Nuclear Power Plant, which was knowingly built close to the centre of the expected Tōkai earthquake. Katsuhiko Ishibashi previously claimed, in 2004, that Hamaoka was \"considered to be the most dangerous nuclear power plant in Japan\".\n\nThe International Atomic Energy Agency (IAEA) has also expressed concern. At a meeting of the G8's Nuclear Safety and Security Group, held in Tokyo in 2008, an IAEA expert warned that a strong earthquake with a magnitude above could pose a 'serious problem' for Japan's nuclear power stations. Before Fukushima, \"14 lawsuits charging that risks had been ignored or hidden were filed in Japan, revealing a disturbing pattern in which operators underestimated or hid seismic dangers to avoid costly upgrades and keep operating. But all the lawsuits were unsuccessful\". Underscoring the risks facing Japan, a 2012 research institute investigation has \"determined there is a 70% chance of a magnitude-7 earthquake striking the Tokyo metropolitan area within the next four years, and 98% over 30 years\". The March 2011 earthquake was a magnitude-9.\n\nBetween 2005 and 2007, three Japanese nuclear power plants were shaken by earthquakes that far exceeded the maximum peak ground acceleration used in their design. The tsunami that followed the 2011 Tōhoku earthquake, inundating the Fukushima I Nuclear Power Plant, was more than twice the design height, while the ground acceleration also slightly exceeded the design parameters.\n\nIn 2006 a Japanese government subcommittee was charged with revising the national guidelines on the earthquake-resistance of nuclear power plants, which had last been partially revised in 2001, resulting in the publication of a new seismic guide — the 2006 \"Regulatory Guide for Reviewing Seismic Design of Nuclear Power Reactor Facilities\". The subcommittee membership included Professor Ishibashi, however his proposal that the standards for surveying active faults should be reviewed was rejected and he resigned at the final meeting, claiming that the review process was 'unscientific' and the outcome rigged to suit the interests of the Japan Electric Association, which had 11 of its committee members on the 19-member government subcommittee. Ishibashi has subsequently claimed that, although the new guide brought in the most far-reaching changes since 1978, it was 'seriously flawed' because it underestimated the design basis earthquake ground motion. He has also claimed that the enforcement system is 'a shambles' and questioned the independence of the Nuclear Safety Commission after a senior Nuclear and Industrial Safety Agency official appeared to rule out a new review of the NSC's seismic design guide in 2007.\n\nFollowing publication of the new 2006 Seismic Guide, the Nuclear and Industrial Safety Agency, at the request of the Nuclear Safety Commission, required the design of all existing nuclear power plants to be re-evaluated.\n\nThe standard of geological survey work in Japan is another area causing concern. In 2008 Taku Komatsubara, a geologist at the National Institute of Advanced Industrial Science and Technology alleged that the presence of active faults was deliberately ignored when surveys of potential new power plant sites were undertaken, a view supported by a former topographer. Takashi Nakata, a seismologist from the Hiroshima Institute of Technology has made similar allegations, and suggest that conflicts of interest between the Japanese nuclear industry and the regulators contribute to the problem.\n\nA 2011 Natural Resources Defense Council report that evaluated the seismic hazard to reactors worldwide, as determined by the Global Seismic Hazard Assessment Program data, placed 35 of Japan's reactors in the group of 48 reactors worldwide in very high and high seismic hazard areas.\n\nAs of May 2018, there are 39 operable reactors in Japan. \nOf these, 9 reactors in 5 power plants are currently operating.\nAdditionally, 5 reactors have been approved for restart and further 12 have restart applications under review.\n\nOn 6 May 2011, then Prime Minister Naoto Kan requested the Hamaoka Nuclear Power Plant be shut down as an earthquake of magnitude 8.0 or higher is estimated 87% likely to hit the area within the next 30 years. Kan wanted to avoid a possible repeat of the Fukushima nuclear disaster. On 9 May 2011, Chubu Electric decided to comply with the government request. In July 2011, a mayor in Shizuoka Prefecture and a group of residents filed a lawsuit seeking the decommissioning of the reactors at the Hamaoka nuclear power plant permanently.\n\nIn April 2014, Reuters reported that Prime Minister Shinzo Abe favours restarting nuclear plants, but that its analysis suggests that only about one-third to two-thirds of reactors will be in a technical and economic position to restart.\nIn April 2017 the Nuclear Regulation Authority approved plans to decommission the Genkai 1, Mihama 1 and 2, Shimane 1 and Tsuruga 1 reactors.\n\nIn terms of consequences of radioactivity releases and core damage the Fukushima I nuclear accidents in 2011 were the worst experienced by the Japanese nuclear industry, in addition to ranking among the worst civilian nuclear accidents, though no fatalities were caused and no serious exposure of radiation to workers occurred. \nThe Tokaimura reprocessing plant fire in 1999 had 2 worker deaths, one more exposed to radiation levels above legal limits and over 660 others received detectable radiation doses but within permissible levels, well below the threshold to affect human health. The Mihama Nuclear Power Plant experienced a steam explosion in one of the turbine buildings in 2004 where five workers were killed and six injured.\n\nThere have been many nuclear shutdowns, failures, and three partial meltdowns which were triggered by the 2011 Tōhoku earthquake and tsunami.\n\nAccording to the Federation of Electric Power Companies of Japan, \"by April 27 approximately 55 percent of the fuel in reactor unit 1 had melted, along with 35 percent of the fuel in unit 2, and 30 percent of the fuel in unit 3; and overheated spent fuels in the storage pools of units 3 and 4 probably were also damaged\". The accident exceeds the 1979 Three Mile Island accident in seriousness, and is comparable to the 1986 Chernobyl disaster. \"The Economist\" reports that the Fukushima disaster is \"a bit like three Three Mile Islands in a row, with added damage in the spent-fuel stores\", and that there will be ongoing impacts:\n\nYears of clean-up will drag into decades. A permanent exclusion zone could end up stretching beyond the plant’s perimeter. Seriously exposed workers may be at increased risk of cancers for the rest of their lives...\n\nOn March 24, 2011, Japanese officials announced that \"radioactive iodine-131 exceeding safety limits for infants had been detected at 18 water-purification plants in Tokyo and five other prefectures\". Officials said also that the fallout from the Dai-ichi plant is \"hindering search efforts for victims from the March 11 earthquake and tsunami\".\n\nProblems in stabilizing the Fukushima Daiichi nuclear power plant have hardened attitudes to nuclear power. As of June 2011, \"more than 80 percent of Japanese now say they are anti-nuclear and distrust government information on radiation\". The ongoing Fukushima crisis may spell the end of nuclear power in Japan, as \"citizen opposition grows and local authorities refuse permission to restart reactors that have undergone safety checks\". Local authorities are skeptical that sufficient safety measures have been taken and are reticent to give their permission – now required by law – to bring suspended nuclear reactors back online.\n\nTwo government advisers have said that \"Japan's safety review of nuclear reactors after the Fukushima disaster is based on faulty criteria and many people involved have conflicts of interest\". Hiromitsu Ino, Professor Emeritus at the University of Tokyo, says\n\"The whole process being undertaken is exactly the same as that used previous to the Fukushima Dai-Ichi accident, even though the accident showed all these guidelines and categories to be insufficient\".\n\nIn 2012, former prime minister Naoto Kan was interviewed about the Fukushima nuclear disaster, and has said that at one point Japan faced a situation where there was a chance that people might not be able to live in the capital zone including Tokyo and would have to evacuate. He says he is haunted by the specter of an even bigger nuclear crisis forcing tens of millions of people to flee Tokyo and threatening the nation's existence. \"If things had reached that level, not only would the public have had to face hardships but Japan's very existence would have been in peril\". That convinced Kan to \"declare the need for Japan to end its reliance on atomic power and promote renewable sources of energy such solar that have long taken a back seat in the resource-poor country's energy mix\".\n\nOther accidents of note include:\n\nJapanese policy is to reprocess its spent nuclear fuel. Originally spent fuel was reprocessed under contract in England and France, but then the Rokkasho Reprocessing Plant was built, with operations originally expected to commence in 2007. The policy to use recovered plutonium as mixed oxide (MOX) reactor fuel was questioned on economic grounds, and in 2004 it was revealed the Ministry of Economy, Trade and Industry had covered up a 1994 report indicating reprocessing spent fuel would cost four times as much as burying it.\n\nIn 2000, a Specified Radioactive Waste Final Disposal Act called for creation of a new organization to manage high level radioactive waste, and later that year the Nuclear Waste Management Organization of Japan (NUMO) was established under the jurisdiction of the Ministry of Economy, Trade and Industry. NUMO is responsible for selecting a permanent deep geological repository site, construction, operation and closure of the facility for waste emplacement by 2040. Site selection began in 2002 and application information was sent to 3,239 municipalities, but by 2006, no local government had volunteered to host the facility. Kōchi Prefecture showed interest in 2007, but its mayor resigned due to local opposition. In December 2013 the government decided to identify suitable candidate areas before approaching municipalities.\n\nIn 2014 the head of the Science Council of Japan’s expert panel has said Japan's seismic conditions makes it difficult to predict ground conditions over the necessary 100,000 years, so it will be impossible to convince the public of the safety of deep geological disposal.\n\nThe cost of MOX fuel had roughly quadrupled from 1999 to 2017, creating doubts about the economics of nuclear fuel reprocessing.\nIn 2018 the Japanese Atomic Energy Commission updated plutonium guidelines to try to reduce plutonium stockpiles, stipulating that the Rokkasho Reprocessing Plant should only produce the amount of plutonium required for MOX fuel for Japan’s nuclear power plants.\n\n\nJapan is divided into a number of regions that each get electric service from their respective regional provider, all utilities hold a monopoly and are strictly regulated by the Japanese government. For more background information, see Energy in Japan. All regional utilities in Japan currently operate nuclear plants with the exception of the Okinawa Electric Power Company. They are also all members of the Federation of Electric Power Companies (FEPCO) industry organization. The companies are listed below.\n\nNuclear vendors provide fuel in its fabricated form, ready to be loaded in the reactor, nuclear services, and/or manage construction of new nuclear plants. The following is an incomplete list of companies based in Japan that provide such services. The companies listed here provide fuel or services for commercial light water plants, and in addition to this, JAEA has a small MOX fuel fabrication plant. Japan operates a robust nuclear fuel cycle.\n\nThere have been discussions between Hitachi, Mitsubishi Heavy Industries and Toshiba about possibly consolidating some of their nuclear activities.\n\nThese organizations are government-funded research organizations, though many of them have special status to give them power of administration separate from the Japanese government. Their origins date back to the Atomic Energy Basic Law, but they have been reorganized several times since their inception.\n\n\n\nLong one of the world’s most committed promoters of civilian nuclear power, Japan's nuclear industry was not hit as hard by the effects of the 1979 Three Mile Island accident (USA) or the 1986 Chernobyl disaster (USSR) as some other countries. Construction of new plants continued to be strong through the 1980s and into the 1990s. However, starting in the mid-1990s there were several nuclear related accidents and cover-ups in Japan that eroded public perception of the industry, resulting in protests and resistance to new plants. These accidents included the Tokaimura nuclear accident, the Mihama steam explosion, cover-ups after accidents at the Monju reactor, and more recently the Kashiwazaki-Kariwa Nuclear Power Plant was completely shut down for 21 months following an earthquake in 2007. While exact details may be in dispute, it is clear that the safety culture in Japan's nuclear industry has come under greater scrutiny.\n\nThe negative impact of the 2011 Fukushima nuclear disaster has changed attitudes in Japan. Political and energy experts describe \"nothing short of a nationwide loss of faith, not only in Japan’s once-vaunted nuclear technology but also in the government, which many blame for allowing the accident to happen\". Sixty thousand people marched in central Tokyo on 19 September 2011, chanting \"Sayonara nuclear power\" and waving banners, to call on Japan's government to abandon nuclear power, following the Fukushima disaster. Bishop of Osaka, Michael Goro Matsuura, has called on the solidarity of Christians worldwide to support this anti-nuclear campaign. In July 2012, 75,000 people gathered near in Tokyo for the capital’s largest anti-nuclear event. Organizers and participants said such demonstrations signal a fundamental change in attitudes in a nation where relatively few have been willing to engage in political protests since the 1960s.\n\nAnti-nuclear groups include the Citizens' Nuclear Information Center, Stop Rokkasho, Hidankyo, Sayonara Nuclear Power Plants, Women from Fukushima Against Nukes, and the Article 9 group. People associated with the anti-nuclear movement include: Jinzaburo Takagi, Haruki Murakami, Kenzaburō Ōe, Nobuto Hosaka, Mizuho Fukushima, Ryuichi Sakamoto and Tetsunari Iida.\n\n\n\n"}
{"id": "46271825", "url": "https://en.wikipedia.org/wiki?curid=46271825", "title": "Nuru Energy", "text": "Nuru Energy\n\nNuru Energy is an international social enterprise which is working to address the global problem of energy poverty and climate change in Africa and India. This is through the distribution of affordable LED lamps that are recharged using a simple to use human powered generator that is more efficient than current solar.\n\nNuru Energy's business model uses a unique distribution strategy to sell its products indirectly to rural customers through micro-franchise entrepreneurs who are equipped and trained by the company and financed through Kiva.\n\nNuru Energy aims to eradicate the use kerosene lamp from Africa through provision of LED lamps to the bottom of the pyramid markets that cannot afford other alternative solar lighting source due to their high cost. This switch from kerosene use would lead to reduction in global carbon emissions leading to the firm earning carbon credits.\n\nThis Clean Development Mechanism project led to the firm partnering with Bank of America Merrill Lynch in 2011, in a deal that would see the investment bank purchase carbon credits from Nuru Energy for a period of 10 years.\n\n"}
{"id": "9969688", "url": "https://en.wikipedia.org/wiki?curid=9969688", "title": "Offshore drilling", "text": "Offshore drilling\n\nOffshore drilling is a mechanical process where a wellbore is drilled below the seabed. It is typically carried out in order to explore for and subsequently extract petroleum which lies in rock formations beneath the seabed. Most commonly, the term is used to describe drilling activities on the continental shelf, though the term can also be applied to drilling in lakes, inshore waters and inland seas.\n\nOffshore drilling presents environmental challenges, both offshore and onshorefrom the produced hydrocarbons and the materials used during the drilling operation. Controversies include the ongoing U.S. offshore drilling debate.\n\nThere are many different types of facilities from which offshore drilling operations take place. These include bottom founded drilling rigs (jackup barges and swamp barges), combined drilling and production facilities either bottom founded or floating platforms, and deepwater mobile offshore drilling units (MODU) including semi-submersibles and drillships. These are capable of operating in water depths up to . In shallower waters the mobile units are anchored to the seabed, however in deeper water (more than the semisubmersibles or drillships are maintained at the required drilling location using dynamic positioning.\n\nAround 1891, the first submerged oil wells were drilled from platforms built on piles in the fresh waters of the Grand Lake St. Marys (a.k.a. Mercer County Reservoir) in Ohio. The wells were developed by small local companies such as Bryson, Riley Oil, German-American and Banker's Oil.\n\nAround 1896, the first submerged oil wells in salt water were drilled in the portion of the Summerland field extending under the Santa Barbara Channel in California. The wells were drilled from piers extending from land out into the channel.\n\nOther notable early submerged drilling activities occurred on the Canadian side of Lake Erie in the 1900s and Caddo Lake in Louisiana in the 1910s. Shortly thereafter wells were drilled in tidal zones along the Texas and Louisiana gulf coast. The Goose Creek Oil Field near Baytown, Texas is one such example. In the 1920s drilling activities occurred from concrete platforms in Venezuela's Lake Maracaibo.\n\nOne of the oldest subsea wells is the Bibi Eibat well, which came on stream in 1923 in Azerbaijan. The well was located on an artificial island in a shallow portion of the Caspian Sea. In the early 1930s, the Texas Co., later Texaco (now Chevron) developed the first mobile steel barges for drilling in the brackish coastal areas of the Gulf of Mexico.\n\nIn 1937, Pure Oil (now Chevron) and its partner Superior Oil (now ExxonMobil) used a fixed platform to develop a field offshore of Calcasieu Parish, Louisiana in of water.\n\nIn 1938, Humble Oil built a mile-long wooden trestle with railway tracks into the sea at McFadden Beach on the Gulf of Mexico, placing a derrick at its end - this was later destroyed by a hurricane. \n\nIn 1945, concern for American control of its offshore oil reserves caused President Harry Truman to issue an Executive Order unilaterally extending American territory to the edge of its continental shelf, an act that effectively ended the 3-mile limit \"freedom of the seas\" regime.\n\nIn 1946, Magnolia Petroleum (now ExxonMobil) drilled at a site off the coast, erecting a platform in of water off St. Mary Parish, Louisiana.\n\nIn early 1947, Superior Oil erected a drilling and production platform in of water some off Vermilion Parish, La. But it was Kerr-McGee Oil Industries (now Anadarko Petroleum), as operator for partners Phillips Petroleum (ConocoPhillips) and Stanolind Oil & Gas (BP) that completed its historic Ship Shoal Block 32 well in October 1947, months before Superior actually drilled a discovery from their Vermilion platform farther offshore. In any case, that made Kerr-McGee's well the first oil discovery drilled out of sight of land.\n\nWhen offshore drilling moved into deeper waters of up to , fixed platform rigs were built, until demands for drilling equipment was needed in the to depth of the Gulf of Mexico, the first jack-up rigs began appearing from specialized offshore drilling contractors such as forerunners of ENSCO International.\n\nThe first semi-submersible resulted from an unexpected observation in 1961. Blue Water Drilling Company owned and operated the four-column submersible Blue Water Rig No.1 in the Gulf of Mexico for Shell Oil Company. As the pontoons were not sufficiently buoyant to support the weight of the rig and its consumables, it was towed between locations at a draught midway between the top of the pontoons and the underside of the deck. It was noticed that the motions at this draught were very small, and Blue Water Drilling and Shell jointly decided to try operating the rig in the floating mode. The concept of an anchored, stable floating deep-sea platform had been designed and tested back in the 1920s by Edward Robert Armstrong for the purpose of operating aircraft with an invention known as the 'seadrome'. The first purpose-built drilling semi-submersible \"Ocean Driller\" was launched in 1963. Since then, many semi-submersibles have been purpose-designed for the drilling industry mobile offshore fleet.\n\nThe first offshore drillship was the \"CUSS 1\" developed for the Mohole project to drill into the Earth's crust.\n\nAs of June, 2010, there were over 620 mobile offshore drilling rigs (Jackups, semisubs, drillships, barges) available for service in the competitive rig fleet.\n\nOne of the world's deepest hubs is currently the Perdido in the Gulf of Mexico, floating in 2,438 meters of water. It is operated by Royal Dutch Shell and was built at a cost of $3 billion. The deepest operational platform is the Petrobras America Cascade FPSO in the Walker Ridge 249 field in 2,600 meters of water.\n\nNotable offshore fields include:\n\nOffshore oil and gas production is more challenging than land-based installations due to the remote and harsher environment. Much of the innovation in the offshore petroleum sector concerns overcoming these challenges, including the need to provide very large production facilities. Production and drilling facilities may be very large and a large investment, such as the Troll A platform standing on a depth of 300 meters.\n\nAnother type of offshore platform may float with a mooring system to maintain it on location. While a floating system may be lower cost in deeper waters than a fixed platform, the dynamic nature of the platforms introduces many challenges for the drilling and production facilities.\n\nThe ocean can add several thousand meters or more to the fluid column. The addition increases the equivalent circulating density and downhole pressures in drilling wells, as well as the energy needed to lift produced fluids for separation on the platform.\n\nThe trend today is to conduct more of the production operations subsea, by separating water from oil and re-injecting it rather than pumping it up to a platform, or by flowing to onshore, with no installations visible above the sea. Subsea installations help to exploit resources at progressively deeper waters—locations which had been inaccessible—and overcome challenges posed by sea ice such as in the Barents Sea. One such challenge in shallower environments is seabed gouging by drifting ice features (means of protecting offshore installations against ice action includes burial in the seabed).\n\nOffshore manned facilities also present logistics and human resources challenges. An offshore oil platform is a small community in itself with cafeteria, sleeping quarters, management and other support functions. In the North Sea, staff members are transported by helicopter for a two-week shift. They usually receive higher salary than onshore workers do. Supplies and waste are transported by ship, and the supply deliveries need to be carefully planned because storage space on the platform is limited. Today, much effort goes into relocating as many of the personnel as possible onshore, where management and technical experts are in touch with the platform by video conferencing. An onshore job is also more attractive for the aging workforce in the petroleum industry, at least in the western world. These efforts among others are contained in the established term integrated operations. The increased use of subsea facilities helps achieve the objective of keeping more workers onshore. Subsea facilities are also easier to expand, with new separators or different modules for different oil types, and are not limited by the fixed floor space of an above-water installation.\n\nOffshore oil production involves environmental risks, most notably oil spills from oil tankers or pipelines transporting oil from the platform to onshore facilities, and from leaks and accidents on the platform. Produced water is also generated, which is water brought to the surface along with the oil and gas; it is usually highly saline and may include dissolved or unseparated hydrocarbons.\n\n"}
{"id": "27876079", "url": "https://en.wikipedia.org/wiki?curid=27876079", "title": "Osaki Channel Crossing", "text": "Osaki Channel Crossing\n\nOsaki Channel Crossing is a power line crossing the Seto Inland Sea south of Yoshina, Takehara in Japan, which was built in 1997 and runs to the Ozaki Power Plant. The Ozaki Power Plant is owned by Chugoku Electric Power Company and is situated on an island on the Inland Sea. It has a span width of 2145 metres.\n\nAs at Chūshi Powerline Crossing, towers with three crossbars are used. Their height is 223 metres, just 3 metres less than that of the Chūshi Powerline crossing, making them the second-tallest electricity pylons in Japan.\n\n"}
{"id": "2301895", "url": "https://en.wikipedia.org/wiki?curid=2301895", "title": "Ozonide", "text": "Ozonide\n\nOzonide is the unstable, reactive polyatomic anion analog of ozone or any of several classes of organic peroxide compounds similar formed by the reaction of ozone with an unsaturated compound.\n\nInorganic ozonides are dark red ionic compounds containing the reactive anion. The anion has the bent shape of the ozone molecule.\n\nInorganic ozonides are formed by burning potassium, rubidium, or caesium in ozone, or by treating the alkali metal hydroxide with ozone; if potassium is left undisturbed in air for years it accumulates a covering of superoxide and ozonide. They are very sensitive explosives that have to be handled at low temperatures in an atmosphere consisting of an inert gas. Lithium and sodium ozonide are extremely unstable and must be prepared by low-temperature ion exchange starting from CsO. Sodium ozonide, , which is prone to decomposition into NaOH and , was previously thought to be impossible to obtain in pure form. However, with the help of cryptands and methylamine, pure may be obtained as red crystals isostructural to .\n\nIonic ozonides are being investigated as sources of oxygen in chemical oxygen generators. Tetramethylammonium ozonide, which can be made by a metathesis reaction with caesium ozonide in liquid ammonia, is stable up to 348K:\n\nPhosphite ozonides, (RO)PO, are used in the production of singlet oxygen. They are made by ozonizing a phosphite ester in dichloromethane at low temperatures, and decompose to yield singlet oxygen and a phosphate ester:\n\nOrganic ozonides are called molozonides and are typically formed by the addition reaction between ozone and alkenes. They are more explosive cousins of the organic peroxides and as such are rarely isolated during the course of the ozonolysis reaction sequence. Molozonides are unstable and rapidly convert to the trioxolane ring structure with a five-membered C–O–O–C–O ring. They usually appear in the form of foul-smelling oily liquids, and rapidly decompose in the presence of water to carbonyl compounds: aldehydes, ketones, peroxides.\n\n\n"}
{"id": "46345245", "url": "https://en.wikipedia.org/wiki?curid=46345245", "title": "Pietra serena", "text": "Pietra serena\n\nPietra serena is a gray sandstone used extensively in Renaissance Florence for architectural details. It is also known as Macigno stone. The material obtained at Fiesole is considered the best and it is also quarried at Arezzo, Cortona and Volterra. Examples of its use in Florence include the interior pilasters, entablatures, and other decorative elements of Brunelleschi's Pazzi Chapel and Michelangelo's Medici Chapel.\n"}
{"id": "34518260", "url": "https://en.wikipedia.org/wiki?curid=34518260", "title": "Piezophototronics", "text": "Piezophototronics\n\nPiezo-phototronic effect is a three way coupling effect of piezoelectric, semiconductor and \nphotonic properties in non-central symmetric semiconductor materials, using the piezoelectric potential (piezopotential) that is generated by applying a strain to a semiconductor with piezoelectricity to control the carrier generation, transport, separation and/or recombination at metal-semiconductor junction or p-n junction for improving the performance of optoelectronic devices, such as photodetector, solar cell and light-emitting diode. Prof. Zhong Lin Wang at Georgia Institute of Technology proposed the fundamental principle of this effect in 2010.\n\nWhen a p-type semiconductor and a n-type semiconductor form a junction, the holes in the p-type side and the electrons in the n-type side tend to redistribute around the interface area to balance the local electric field, which results in a charge depletion layer. The diffusion and recombination of the electrons and holes in the junction region is closely related to the optoelectronic properties of the device, which is greatly affected by the local electric field distribution. The existence of the piezo-charges at the interface introduces three effects: a shift in local electronic band structure due to the introduced local potential, a tilt of the electronic band structure over the junction region for the polarization existing in the piezoelectric semiconductor, and a change in the charge depletion layer due to the redistribution of the local charge carriers to balance the local piezo-charges. The positive piezoelectric charges at the junction lower the energy band and the negative piezoelectric charges raise the energy band in n-type semiconductor region near the junction region. A modification in the local band by piezopotential may be effective for trapping charges so that the electron-hole recombination rate can be largely enhanced, which is very beneficial for improving the efficiency of a light-emitting diode. Furthermore, the inclined band tends to change the mobility of the carriers moving toward the junction.\nThe materials for piezo-phototronics should have three basic properties: piezoelectricity, semiconductor property, and photon excitation property [5]. Typical materials are the wurtzite structures, such as ZnO, GaN and InN. the three-way coupling among piezoelectricity, photoexcitation and semiconductor properties, which is the basis of piezotronics (piezoelectricity-semiconductor coupling), piezophotonics (piezoelectric-photon excitation coupling), optoelectronics, and piezo-phototronics piezoelectricity-semiconductor-photoexcitation). The core of these coupling relies on the piezopotential created by the piezoelectric materials.\n"}
{"id": "14453424", "url": "https://en.wikipedia.org/wiki?curid=14453424", "title": "Pinning points", "text": "Pinning points\n\nIn a crystalline material, a dislocation is capable of traveling throughout the lattice when relatively small stresses are applied. This movement of dislocations results in the material plastically deforming. Pinning points in the material act to halt a dislocation's movement, requiring a greater amount of force to be applied to overcome the barrier. This results in an overall strengthening of materials.\n\nPoint defects (as well as stationary dislocations, jogs, and kinks) present in a material create stress fields within a material that disallow traveling dislocations to come into direct contact. Much like two particles of the same electric charge feel a repulsion to one another when brought together, the dislocation is pushed away from the already present stress field.\n\nThe introduction of atom into a crystal of atom creates a pinning point for multiple reasons. An alloying atom is by nature a point defect, thus it must create a stress field when placed into a foreign crystallographic position, which could block the passage of a dislocation. However, it is possible that the alloying material is approximately the same size as the atom that is replaced, and thus its presence would not stress the lattice (as occurs in cobalt alloyed nickel). The different atom would, though, have a different elastic modulus, which would create a different terrain for the moving dislocation. A higher modulus would look like an energy barrier, and a lower like an energy trough – both of which would stop its movement.\n\nThe precipitation of a second phase within the lattice of a material creates physical blockades through which a dislocation cannot pass. The result is that the dislocation must bend (which requires greater energy, or a greater stress to be applied) around the precipitates, which inevitably leaves residual dislocation loops encircling the second phase material and shortens the original dislocation.\n\nDislocations require proper lattice ordering to move through a material. At grain boundaries, there is a lattice mismatch, and every atom that lies on the boundary is uncoordinated. This stops dislocations that encounter the boundary from moving.\n"}
{"id": "561590", "url": "https://en.wikipedia.org/wiki?curid=561590", "title": "Positive pressure", "text": "Positive pressure\n\nPositive pressure is a pressure within a system that is greater than the environment that surrounds that system. Consequently, if there is any leak from the positively pressured system it will egress into the surrounding environment.\n\nUse is also made of positive pressure to ensure there is no ingress of the environment into a supposed closed system. A typical example of the use of positive pressure is the location of a habitat in an area where there may exist flammable gases such as found on an oil platform or laboratory cleanroom. This kind of positive pressure is also used on operating theaters and \"in vitro\" fertilisation (IVF) labs.\n\nHospitals may have positive pressure rooms for patients with compromised immune systems. Air will flow out of the room instead of in, so that any airborne microorganisms (e.g., bacteria) that may infect the patient are kept away.\n\nThis process is important in human and chick development. Positive pressure, created by the closure of anterior and posterior neuropores of the neural tube during neurulation, is a requirement of brain development.\n\nAmphibians use this process to respire, whereby they use positive pressure to inflate their lungs.\n\n"}
{"id": "23619", "url": "https://en.wikipedia.org/wiki?curid=23619", "title": "Pressure", "text": "Pressure\n\nPressure (symbol: \"p\" or \"P\") is the force applied perpendicular to the surface of an object per unit area over which that force is distributed. Gauge pressure (also spelled \"gage\" pressure) is the pressure relative to the ambient pressure.\n\nVarious units are used to express pressure. Some of these derive from a unit of force divided by a unit of area; the SI unit of pressure, the pascal (Pa), for example, is one newton per square metre; similarly, the pound-force per square inch (psi) is the traditional unit of pressure in the imperial and US customary systems. Pressure may also be expressed in terms of standard atmospheric pressure; the atmosphere (atm) is equal to this pressure, and the torr is defined as of this. Manometric units such as the centimetre of water, millimetre of mercury, and inch of mercury are used to express pressures in terms of the height of column of a particular fluid in a manometer.\n\nPressure is the amount of force applied at right angles to the surface of an object per unit area. The symbol for it is \"p\" or \"P\".\nThe IUPAC recommendation for pressure is a lower-case \"p\".\nHowever, upper-case \"P\" is widely used. The usage of \"P\" vs \"p\" depends upon the field in which one is working, on the nearby presence of other symbols for quantities such as power and momentum, and on writing style.\n\nMathematically:\nwhere:\n\nPressure is a scalar quantity. It relates the vector surface element (a vector normal to the surface) with the normal force acting on it. The pressure is the scalar proportionality constant that relates the two normal vectors:\n\nThe minus sign comes from the fact that the force is considered towards the surface element, while the normal vector points outward. The equation has meaning in that, for any surface \"S\" in contact with the fluid, the total force exerted by the fluid on that surface is the surface integral over \"S\" of the right-hand side of the above equation.\n\nIt is incorrect (although rather usual) to say \"the pressure is directed in such or such direction\". The pressure, as a scalar, has no direction. The force given by the previous relationship to the quantity has a direction, but the pressure does not. If we change the orientation of the surface element, the direction of the normal force changes accordingly, but the pressure remains the same.\n\nPressure is distributed to solid boundaries or across arbitrary sections of fluid \"normal to\" these boundaries or sections at every point. It is a fundamental parameter in thermodynamics, and it is conjugate to volume.\n\nThe SI unit for pressure is the pascal (Pa), equal to one newton per square metre (N/m, or kg·m·s). This name for the unit was added in 1971; before that, pressure in SI was expressed simply in newtons per square metre.\n\nOther units of pressure, such as pounds per square inch and bar, are also in common use. The CGS unit of pressure is the barye (Ba), equal to 1 dyn·cm, or 0.1 Pa. Pressure is sometimes expressed in grams-force or kilograms-force per square centimetre (g/cm or kg/cm) and the like without properly identifying the force units. But using the names kilogram, gram, kilogram-force, or gram-force (or their symbols) as units of force is expressly forbidden in SI. The technical atmosphere (symbol: at) is 1 kgf/cm (98.0665 kPa, or 14.223 psi).\n\nSince a system under pressure has the potential to perform work on its surroundings, pressure is a measure of potential energy stored per unit volume. It is therefore related to energy density and may be expressed in units such as joules per cubic metre (J/m, which is equal to Pa).\nMathematically:\n\nSome meteorologists prefer the hectopascal (hPa) for atmospheric air pressure, which is equivalent to the older unit millibar (mbar). Similar pressures are given in kilopascals (kPa) in most other fields, where the hecto- prefix is rarely used. The inch of mercury is still used in the United States. Oceanographers usually measure underwater pressure in decibars (dbar) because pressure in the ocean increases by approximately one decibar per metre depth.\nThe standard atmosphere (atm) is an established constant. It is approximately equal to typical air pressure at Earth mean sea level and is defined as .\n\nBecause pressure is commonly measured by its ability to displace a column of liquid in a manometer, pressures are often expressed as a depth of a particular fluid (e.g., centimetres of water, millimetres of mercury or inches of mercury). The most common choices are mercury (Hg) and water; water is nontoxic and readily available, while mercury's high density allows a shorter column (and so a smaller manometer) to be used to measure a given pressure. The pressure exerted by a column of liquid of height \"h\" and density \"ρ\" is given by the hydrostatic pressure equation , where \"g\" is the gravitational acceleration. Fluid density and local gravity can vary from one reading to another depending on local factors, so the height of a fluid column does not define pressure precisely. When millimetres of mercury or inches of mercury are quoted today, these units are not based on a physical column of mercury; rather, they have been given precise definitions that can be expressed in terms of SI units. One millimetre of mercury is approximately equal to one torr. The water-based units still depend on the density of water, a measured, rather than defined, quantity. These \"manometric units\" are still encountered in many fields. Blood pressure is measured in millimetres of mercury in most of the world, and lung pressures in centimetres of water are still common.\n\nUnderwater divers use the metre sea water (msw or MSW) and foot sea water (fsw or FSW) units of pressure, and these are the standard units for pressure gauges used to measure pressure exposure in diving chambers and personal decompression computers. A msw is defined as 0.1 bar (= 100000 Pa = 10000 Pa), is not the same as a linear metre of depth. 33.066 fsw = 1 atm (1 atm = 101325 Pa / 33.066 = 3064.326 Pa). Note that the pressure conversion from msw to fsw is different from the length conversion: 10 msw = 32.6336 fsw, while 10 m = 32.8083 ft.\n\nGauge pressure is often given in units with \"g\" appended, e.g. \"kPag\", \"barg\" or \"psig\", and units for measurements of absolute pressure are sometimes given a suffix of \"a\", to avoid confusion, for example \"kPaa\", \"psia\". However, the US National Institute of Standards and Technology recommends that, to avoid confusion, any modifiers be instead applied to the quantity being measured rather than the unit of measure. For example, rather than .\n\nDifferential pressure is expressed in units with \"d\" appended; this type of measurement is useful when considering sealing performance or whether a valve will open or close.\n\nPresently or formerly popular pressure units include the following:\n\nAs an example of varying pressures, a finger can be pressed against a wall without making any lasting impression; however, the same finger pushing a thumbtack can easily damage the wall. Although the force applied to the surface is the same, the thumbtack applies more pressure because the point concentrates that force into a smaller area. Pressure is transmitted to solid boundaries or across arbitrary sections of fluid \"normal to\" these boundaries or sections at every point. Unlike stress, pressure is defined as a scalar quantity. The negative gradient of pressure is called the force density.\n\nAnother example is a knife. If we try to cut a fruit with the flat side, the force is distributed over a large area, and it will not cut. But if we use the edge, it will cut smoothly. The reason is that the flat side has a greater surface area (less pressure), and so it does not cut the fruit. When we take the thin side, the surface area is reduced, and so it cuts the fruit easily and quickly. This is one example of a practical application of pressure.\n\nFor gases, pressure is sometimes measured not as an \"absolute pressure\", but relative to atmospheric pressure; such measurements are called \"gauge pressure\". An example of this is the air pressure in an automobile tire, which might be said to be \"220 kPa (32 psi)\", but is actually 220 kPa (32 psi) above atmospheric pressure. Since atmospheric pressure at sea level is about 100 kPa (14.7 psi), the absolute pressure in the tire is therefore about 320 kPa (46.7 psi). In technical work, this is written \"a gauge pressure of 220 kPa (32 psi)\". Where space is limited, such as on pressure gauges, name plates, graph labels, and table headings, the use of a modifier in parentheses, such as \"kPa (gauge)\" or \"kPa (absolute)\", is permitted. In non-SI technical work, a gauge pressure of 32 psi is sometimes written as \"32 psig\", and an absolute pressure as \"32 psia\", though the other methods explained above that avoid attaching characters to the unit of pressure are preferred.\n\nGauge pressure is the relevant measure of pressure wherever one is interested in the stress on storage vessels and the plumbing components of fluidics systems. However, whenever equation-of-state properties, such as densities or changes in densities, must be calculated, pressures must be expressed in terms of their absolute values. For instance, if the atmospheric pressure is 100 kPa, a gas (such as helium) at 200 kPa (gauge) (300 kPa [absolute]) is 50% denser than the same gas at 100 kPa (gauge) (200 kPa [absolute]). Focusing on gauge values, one might erroneously conclude the first sample had twice the density of the second one.\n\nIn a static gas, the gas as a whole does not appear to move. The individual molecules of the gas, however, are in constant random motion. Because we are dealing with an extremely large number of molecules and because the motion of the individual molecules is random in every direction, we do not detect any motion. If we enclose the gas within a container, we detect a pressure in the gas from the molecules colliding with the walls of our container. We can put the walls of our container anywhere inside the gas, and the force per unit area (the pressure) is the same. We can shrink the size of our \"container\" down to a very small point (becoming less true as we approach the atomic scale), and the pressure will still have a single value at that point. Therefore, pressure is a scalar quantity, not a vector quantity. It has magnitude but no direction sense associated with it. Pressure force acts in all directions at a point inside a gas. At the surface of a gas, the pressure force acts perpendicular (at right angle) to the surface.\n\nA closely related quantity is the stress tensor \"σ\", which relates the vector force formula_7 to the \nvector area formula_8 via the linear relation formula_9.\n\nThis tensor may be expressed as the sum of the viscous stress tensor minus the hydrostatic pressure. The negative of the stress tensor is sometimes called the pressure tensor, but in the following, the term \"pressure\" will refer only to the scalar pressure.\n\nAccording to the theory of general relativity, pressure increases the strength of a gravitational field (see stress–energy tensor) and so adds to the mass-energy cause of gravity. This effect is unnoticeable at everyday pressures but is significant in neutron stars, although it has not been experimentally tested.\n\nFluid pressure is most often the compressive stress at some point within a fluid. (The term \"fluid\" refers to both liquids and gases – for more information specifically about liquid pressure, see section below.)\n\nFluid pressure occurs in one of two situations:\n\nPressure in open conditions usually can be approximated as the pressure in \"static\" or non-moving conditions (even in the ocean where there are waves and currents), because the motions create only negligible changes in the pressure. Such conditions conform with principles of fluid statics. The pressure at any given point of a non-moving (static) fluid is called the hydrostatic pressure. \n\nClosed bodies of fluid are either \"static\", when the fluid is not moving, or \"dynamic\", when the fluid can move as in either a pipe or by compressing an air gap in a closed container. The pressure in closed conditions conforms with the principles of fluid dynamics.\n\nThe concepts of fluid pressure are predominantly attributed to the discoveries of Blaise Pascal and Daniel Bernoulli. Bernoulli's equation can be used in almost any situation to determine the pressure at any point in a fluid. The equation makes some assumptions about the fluid, such as the fluid being ideal and incompressible. An ideal fluid is a fluid in which there is no friction, it is inviscid (zero viscosity). The equation for all points of a system filled with a constant-density fluid is\n\nwhere:\n\n\nExplosion or deflagration pressures are the result of the ignition of explosive gases, mists, dust/air suspensions, in unconfined and confined spaces.\n\nWhile pressures are, in general, positive, there are several situations in which negative pressures may be encountered:\n\nStagnation pressure is the pressure a fluid exerts when it is forced to stop moving. Consequently, although a fluid moving at higher speed will have a lower static pressure, it may have a higher stagnation pressure when forced to a standstill. Static pressure and stagnation pressure are related by:\n\nwhere \n\nThe pressure of a moving fluid can be measured using a Pitot tube, or one of its variations such as a Kiel probe or Cobra probe, connected to a manometer. Depending on where the inlet holes are located on the probe, it can measure static pressures or stagnation pressures.\n\nThere is a two-dimensional analog of pressure – the lateral force per unit length applied on a line perpendicular to the force.\n\nSurface pressure is denoted by π:\nand shares many similar properties with three-dimensional pressure. Properties of surface chemicals can be investigated by measuring pressure/area isotherms, as the two-dimensional analog of Boyle's law, , at constant temperature.\n\nSurface tension is another example of surface pressure, but with a reversed sign, because \"tension\" is the opposite to \"pressure\".\n\nIn an ideal gas, molecules have no volume and do not interact. According to the ideal gas law, pressure varies linearly with temperature and quantity, and inversely with volume:\n\nwhere:\n\nReal gases exhibit a more complex dependence on the variables of state.\n\nVapour pressure is the pressure of a vapour in thermodynamic equilibrium with its condensed phases in a closed system. All liquids and solids have a tendency to evaporate into a gaseous form, and all gases have a tendency to condense back to their liquid or solid form.\n\nThe atmospheric pressure boiling point of a liquid (also known as the normal boiling point) is the temperature at which the vapor pressure equals the ambient atmospheric pressure. With any incremental increase in that temperature, the vapor pressure becomes sufficient to overcome atmospheric pressure and lift the liquid to form vapour bubbles inside the bulk of the substance. Bubble formation deeper in the liquid requires a higher pressure, and therefore higher temperature, because the fluid pressure increases above the atmospheric pressure as the depth increases.\n\nThe vapor pressure that a single component in a mixture contributes to the total pressure in the system is called partial vapor pressure.\n\nWhen a person swims under the water, water pressure is felt acting on the person's eardrums. The deeper that person swims, the greater the pressure. The pressure felt is due to the weight of the water above the person. As someone swims deeper, there is more water above the person and therefore greater pressure. The pressure a liquid exerts depends on its depth.\n\nLiquid pressure also depends on the density of the liquid. If someone was submerged in a liquid more dense than water, the pressure would be correspondingly greater. thus we can say that the depth, density and liquid pressure are directly proportionate. The pressure due to a liquid in liquid columns of constant density or at a depth within a substance is represented by the following formula:\n\nwhere:\n\nAnother way of saying the same formula is the following:\n\nThe pressure a liquid exerts against the sides and bottom of a container depends on the density and the depth of the liquid. If atmospheric pressure is neglected, liquid pressure against the bottom is twice as great at twice the depth; at three times the depth, the liquid pressure is threefold; etc. Or, if the liquid is two or three times as dense, the liquid pressure is correspondingly two or three times as great for any given depth. Liquids are practically incompressible – that is, their volume can hardly be changed by pressure (water volume decreases by only 50 millionths of its original volume for each atmospheric increase in pressure). Thus, except for small changes produced by temperature, the density of a particular liquid is practically the same at all depths.\n\nAtmospheric pressure pressing on the surface of a liquid must be taken into account when trying to discover the \"total\" pressure acting on a liquid. The total pressure of a liquid, then, is \"ρgh\" plus the pressure of the atmosphere. When this distinction is important, the term \"total pressure\" is used. Otherwise, discussions of liquid pressure refer to pressure without regard to the normally ever-present atmospheric pressure.\n\nIt is important to recognize that the pressure does not depend on the \"amount\" of liquid present. Volume is not the important factor – depth is. The average water pressure acting against a dam depends on the average depth of the water and not on the volume of water held back. For example, a wide but shallow lake with a depth of exerts only half the average pressure that a small deep pond does (note that the \"total force\" applied to the longer dam will be greater, due to the greater total surface area for the pressure to act upon, but for a given 5-foot section of each dam, the 10 ft deep water will apply half the force of 20 ft deep water). A person will feel the same pressure whether his/her head is dunked a metre beneath the surface of the water in a small pool or to the same depth in the middle of a large lake. If four vases contain different amounts of water but are all filled to equal depths, then a fish with its head dunked a few centimetres under the surface will be acted on by water pressure that is the same in any of the vases. If the fish swims a few centimetres deeper, the pressure on the fish will increase with depth and be the same no matter which vase the fish is in. If the fish swims to the bottom, the pressure will be greater, but it makes no difference what vase it is in. All vases are filled to equal depths, so the water pressure is the same at the bottom of each vase, regardless of its shape or volume. If water pressure at the bottom of a vase were greater than water pressure at the bottom of a neighboring vase, the greater pressure would force water sideways and then up the narrower vase to a higher level until the pressures at the bottom were equalized. Pressure is depth dependent, not volume dependent, so there is a reason that water seeks its own level.\n\nRestating this as energy equation, the energy per unit volume in an ideal, incompressible liquid is constant throughout its vessel. At the surface, gravitational potential energy is large but liquid pressure energy is low. At the bottom of the vessel, all the gravitational potential energy is converted to pressure energy. The sum of pressure energy and gravitational potential energy per unit volume is constant throughout the volume of the fluid and the two energy components change linearly with the depth. Mathematically, it is described by Bernoulli's equation, where velocity head is zero and comparisons per unit volume in the vessel are\n\nTerms have the same meaning as in section Fluid pressure.\n\nAn experimentally determined fact about liquid pressure is that it is exerted equally in all directions. If someone is submerged in water, no matter which way that person tilts his/her head, the person will feel the same amount of water pressure on his/her ears. Because a liquid can flow, this pressure isn't only downward. Pressure is seen acting sideways when water spurts sideways from a leak in the side of an upright can. Pressure also acts upward, as demonstrated when someone tries to push a beach ball beneath the surface of the water. The bottom of a boat is pushed upward by water pressure (buoyancy).\n\nWhen a liquid presses against a surface, there is a net force that is perpendicular to the surface. Although pressure doesn't have a specific direction, force does. A submerged triangular block has water forced against each point from many directions, but components of the force that are not perpendicular to the surface cancel each other out, leaving only a net perpendicular point. This is why water spurting from a hole in a bucket initially exits the bucket in a direction at right angles to the surface of the bucket in which the hole is located. Then it curves downward due to gravity. If there are three holes in a bucket (top, bottom, and middle), then the force vectors perpendicular to the inner container surface will increase with increasing depth – that is, a greater pressure at the bottom makes it so that the bottom hole will shoot water out the farthest. The force exerted by a fluid on a smooth surface is always at right angles to the surface. The speed of liquid out of the hole is formula_22, where \"h\" is the depth below the free surface. This is the same speed the water (or anything else) would have if freely falling the same vertical distance \"h\".\n\nis the kinematic pressure, where formula_2 is the pressure and formula_25 constant mass density. The SI unit of \"P\" is m/s. Kinematic pressure is used in the same manner as kinematic viscosity formula_26 in order to compute Navier–Stokes equation without explicitly showing the density formula_25.\n\n\n"}
{"id": "31505721", "url": "https://en.wikipedia.org/wiki?curid=31505721", "title": "Revitalizant", "text": "Revitalizant\n\nRevitalizant ( — life, can be literally translated to “bringing back to life”) — a semi permanent treatment for metals found in automobile engines, transmissions, fuel pumps, and other friction surfaces in industrial and other machines. The treatment is added to the engine oil, operating fluids, or fuel. The treatment forms a protective cermet or ceramic-metal coating on the friction metal parts of the mechanisms directly during the process of their operation. The \" Revitalizant \" solves the problem of non-wear operation of cars and mechanisms.\n\nThe \"Revitalizant\" was developed in 1998 in Kharkiv (Ukraine). The composition and the method of its preparation are patented. The notion was included in the textbook on tribology.\n\nVisually it is a gel or a plastic substance. It consists of a lubricant and a mixture of oxides and metal oxide hydrates: AlO and/or SiO and/or MgO and/or CaO and/or FeO etc., with dispersion from 100 to 10,000 nanometers.\n\n\n formula_1\n\n\nThe process of the protective coating formation, called revitalization, is based on physical-chemical interaction between surfaces of the parts on the spots of virtual contact covered with revitalizant in a boundary or mixed lubrication mode. As a result a gradient cermet coating is formed, containing positive compressive stresses all over its depth and concentration of carbon, increasing at the surface (up to the formation of diamond-like structures). Distinctive feature of the process is a hardening of the coating with its simultaneous growth\n\nUsed in the manufacture of lubricants, greases and additives.\n"}
{"id": "1047958", "url": "https://en.wikipedia.org/wiki?curid=1047958", "title": "Split-phase electric power", "text": "Split-phase electric power\n\nA split-phase or single-phase three-wire system is a type of single-phase electric power distribution. It is the AC equivalent of the original Edison three-wire direct-current system. Its primary advantage is that it saves conductor material over a single-ended single-phase system, while only requiring a single phase on the supply side of the distribution transformer.\n\nThe two 120 V AC lines are supplied to the premises from a transformer with a 240 V AC secondary winding which has a center tap connected to ground. The system neutral conductor is connected to ground at the transformer center tap. This results in two 120 V AC line voltages which are out of phase by 180 degrees with each other. When required, 240 V AC can be obtained by connecting the load between the two 120 V AC lines.\nA transformer supplying a three-wire distribution system has a single-phase input (primary) winding. The output (secondary) winding is center-tapped and the center tap connected to a grounded neutral. As shown in Fig. 1. either end to center has half the voltage of end-to-end. Fig. 2 illustrates the phasor diagram of the output voltages for a split-phase transformer. Since the two phasors do not define a unique direction of rotation for a revolving magnetic field, a split single-phase is not a two-phase system. \n\nIn the United States and Canada, the practice originated with the DC distribution system developed by Thomas Edison. By connecting pairs of lamps or groups of lamps on the same circuit in series, and doubling the supply voltage, the size of conductors was reduced substantially.\n\nThe line to neutral voltage is half the line-to-line voltage. Lighting and small appliances requiring less than 1800 watts may be connected between a line wire and the neutral. Higher wattage appliances, such as cooking equipment, space heating, water pumps, clothes dryers, and air conditioners are connected across the two line conductors. This means that (for the supply of the same amount of power) the current is halved. Hence, smaller conductors may be used than would be needed if the appliances were designed to be supplied by the lower voltage.\n\nIf the load were guaranteed to be balanced, then the neutral conductor would not carry any current and the system would be equivalent to a single-ended system of twice the voltage with the line wires taking half the current. This would not need a neutral conductor at all, but would be wildly impractical for varying loads; just connecting the groups in series would result in excessive voltage and brightness variation as lamps are switched on and off.\n\nBy connecting the two lamp groups to a neutral, intermediate in potential between the two live legs, any imbalance of the load will be supplied by a current in the neutral, giving substantially constant voltage across both groups. The total current carried in all three wires (including the neutral) will always be twice the supply current of the most heavily loaded half.\n\nFor short wiring runs limited by conductor ampacity, this allows three half-sized conductors to be substituted for two full-sized ones, using 75% of the copper of an equivalent single-phase system.\n\nLonger wiring runs are more limited by voltage drop in the conductors. Because the supply voltage is doubled, a balanced load can tolerate double the voltage drop, allowing quarter-sized conductors to be used; this uses 3/8 the copper of an equivalent single-phase system.\n\nIn practice, some intermediate value is chosen. For example, if the imbalance is limited to 25% of the total load (half of one half) rather than the absolute worst-case 50%, then conductors 3/8 of the single-phase size will guarantee the same maximum voltage drop, totalling 9/8 of one single-phase conductor, 56% of the copper of the two single-phase conductors.\n\nIn a so-called \"balanced power\" system, an isolation transformer with a center tap is used to create a separate supply with conductors at a balanced Vnom/2 with respect to ground. The purpose of a balanced power system is to minimize the noise coupled into sensitive equipment from the power supply. \n\nUnlike a three-wire distribution system, the grounded neutral is not distributed to the loads; only line-to-line connections at 120 V are used. A balanced power system is only used for specialized distribution in audio and video production studios, sound and television broadcasting, and installations of sensitive scientific instruments. \n\nThe U.S. National Electrical Code provides rules for technical power installations. The systems are not to be used for general-purpose lighting or other equipment, and may use special sockets to ensure only approved equipment is connected to the system. Additionally, technical power systems pay special attention to the way the distribution system is grounded.\n\nA risk of using a balanced power system, in an installation that also uses \"conventional\" power in the same rooms, is that a user may inadvertently interconnect the power systems together via an intermediate system of audio or video equipment, elements of which might be connected to different power systems.\n\nIn Europe, three-phase 230/400 V is most commonly used. However, 230/460 V, three-wire, single-phase systems are used to run farms and small groups of houses when only two of the three-phase high-voltage conductors are used. A split-phase final step-down transformer is then used, with the centre-tap earthed and the two halves usually supplying different buildings with a single phase supply, although in the UK a large farm may be given a 230-0-230 supply.\n\nIn the UK, electric tools and portable lighting at larger construction and demolition sites are governed by BS7375, and where possible are recommended to be fed from a centre-tapped system with only 55 V between live conductors and the earth (so called CTE or Centre Tap Earth, or 55-0-55). This reduced low voltage system is used with 110 V equipment. No neutral conductor is distributed. In high hazard locations, additional double pole RCD protection may be used. The intention is to reduce the electrocution hazard that may exist when using electrical equipment at a wet or outdoor construction site, and eliminate the requirement for rapid automatic disconnection for prevention of shocks during faults. Portable transformers that transform single-phase 240 V to this 110 V split-phase system are a common piece of construction equipment. Generator sets used for construction sites are equipped to supply it directly.\n\nAn incidental benefit is that the filaments of 110 V incandescent lamps used on such systems are thicker and therefore mechanically more rugged than those of 240 V lamps.\n\nThis three-wire single phase system is common in North America for residential and light commercial applications. Circuit breaker panels typically have two hot wires, and a neutral, connected at one point to the grounded center tap of a local transformer). Single pole circuit breakers feed 120 volt circuits from one of the 120 volt busses within the panel, or two-pole circuit breakers feed 240 volt circuits from both busses. 120 V circuits are the most common, and used to power NEMA 1 and NEMA 5 outlets, and most residential and light commercial direct-wired lighting circuits. 240 V circuits are used for high-demand applications, such as air conditioners, space heaters, electric stoves, electric clothes dryers, air conditioners, water heaters, and electric vehicle charge points. These use NEMA 10 or NEMA 14 outlets that are deliberately incompatible with the 120 V outlets.\n\nWiring regulations govern the application of split-phase circuits. Since the neutral (return) conductor is not protected by a fuse or circuit breaker, a neutral wire can be shared only by two circuits fed from opposite lines of the supply system. Two circuits from opposing lines may share a neutral if both breakers are connected by a bar so that both trip simultaneously (NEC 210.4), this prevents 120 V from feeding across 240 V circuits.\n\nIn Sweden split-phase electric power is also used on some railways. The center tap is grounded, one pole is fed with an overhead wire section, while the other wire is used for another section.\n\nAmtrak's 60 Hz traction power system in the Northeast Corridor between New York and Boston also uses split-phase power distribution. Two separate wires are run along the track, the contact wire for the locomotive and an electrically separate feeder wire. Each wire is fed with 25 kV with respect to ground, with 50 kV between them. Autotransformers along the track balance the loads between the contact and feeder wires, reducing resistive losses.\n\nIn the UK Network Rail are using autotransformers on all new 50 Hz electrification, and (as of 2014) are converting many old booster transformer installations to autotransformers, to reduce energy losses and exported electromagnetic interference, both of which increase when longer, heavier, or faster trains are introduced, drawing higher peak current from the supply. Note that booster transformers only \"boost\" the return of traction current through its intended path, the \"return conductor\", rather than randomly through the earth, and do not boost, but rather reduce, the available voltage at the train, and introduce additional losses. The autotransformer system enforces the traction return current taking its intended path, while reducing the transmission losses, and therefore achieves both required objectives, of controlling return current leakage to earth and ensuring low energy loss, simultaneously. There is an initial cost penalty, because the previous return conductor, insulated to a fairly modest voltage, must be replaced by an anti-phase feeder, insulated to 25 kV, and the autotransformers themselves are larger and more expensive than the previous booster transformers.\n\n"}
{"id": "3412125", "url": "https://en.wikipedia.org/wiki?curid=3412125", "title": "Strontium aluminate", "text": "Strontium aluminate\n\nStrontium aluminate (SRA, SrAl, ) is a solid odorless, nonflammable, pale yellow, monoclinic crystalline powder, heavier than water. When activated with a suitable dopant (e.g. europium, then it is labeled Eu:SrAlO), it acts as a photoluminescent phosphor with long persistence of phosphorescence.\n\nThere are also other strontium aluminates, e.g. SrAlO (monoclinic), SrAlO (cubic), SrAlO (hexagonal), SrAlO (orthorhombic).\n\nFor many phosphorescent-based purposes, strontium aluminate is a vastly superior phosphor to its predecessor, copper-activated zinc sulfide; it is about 10 times brighter and 10 times longer glowing, however it is about 10 times more expensive than Cu:ZnS and it cannot produce the unique red phosphorescence of the latter. It is frequently used in glow in the dark toys, where it displaces the cheaper but less efficient Cu:ZnS. However, the material has high hardness, causing abrasion to the machinery handling it; manufacturers frequently coat the particles with a suitable lubricant when adding them to a plastic.\n\nDifferent aluminates can be used as the host matrix. This influences the wavelength of emission of the europium ion, by its covalent interaction with surrounding oxygens, and crystal field splitting of the 5d orbital energy levels.\n\nStrontium aluminate phosphors produce green and aqua hues, where green gives the highest brightness and aqua the longest glow time. The excitation wavelengths for strontium aluminate range from 200 to 450 nm. The wavelength for its green formulation is 520 nm, its aqua, or blue-green, version emits at 505 nm, and the blue one emits at 490 nm. Strontium aluminate can be formulated to phosphoresce at longer (yellow to red) wavelengths as well, though such emission is often dimmer than that of more common phosphorescence at shorter wavelengths.\n\nFor europium-dysprosium doped aluminates, the peak emission wavelengths are 520 nm for SrAlO, 480 nm for SrAlO, and 400 nm for SrAlO.\n\nEu,Dy:SrAlO is important as a persistently luminiscent phosphor for industrial applications. It can be produced by molten salt assisted process at 900 °C.\n\nThe most described kind is the stoichiometric green-emitting (approx. 530 nm) Eu:SrAlO. Eu,Dy,B:SrAlO shows significantly longer afterglow than the europium-only doped material. The Eu dopant shows high afterglow, while Eu has almost none. Polycrystalline Mn:SrAlO is used as a green phosphor for plasma displays, and when doped with praseodymium or neodymium it can act as a good active laser medium. SrCeMgAlO is a phosphor emitting at 305 nm, with quantum efficiency of 70%. Several strontium aluminates can be prepared by the sol-gel process.\n\nThe wavelengths produced depend on the internal crystal structure of the material. Slight modifications in the manufacturing process (the type of reducing atmosphere, small variations of stoichiometry of the reagents, addition of carbon or rare-earth halides) can significantly influence the emission wavelengths.\n\nStrontium aluminate phosphor is usually fired at about 1250 °C, though higher temperatures are possible. Subsequent exposure to temperatures above 1090 °C is likely to cause loss of its phosphorescent properties. At higher firing temperatures, the SrAlO undergoes transformation to SrAlO.\n\nThe glow intensity depends on the particle size; generally, the bigger the particles, the better the glow.\n\nStrontium aluminate based afterglow pigments are marketed under brand names like MX24, ProFx and StarmakerFX from Meg's Glowmix, GloManiaGLO EffexArt 'N Glow, Core Glow, Ambient Glow Technology or AGT. Super-LumiNova or NoctiLumina.\n\nStrontium aluminate doped with europium and dysprosium is called Lumibrite developed by Seiko and claimed to be brighter and longer glow time compare to ordinary strontium aluminate doped with only europium.\n\nEuropium-doped strontium aluminate nanoparticles are proposed as indicators of stress and cracks in materials, as they emit light when subjected to mechanical stress (mechanoluminescence). They are also useful for fabricating mechano-optical nanodevices. Non-agglomerated particles are needed for this purpose; they are difficult to prepare conventionally but can be made by ultrasonic spray pyrolysis of a mixture of strontium acetylacetonate, aluminium acetylacetonate and europium acetylacetonate in reducing atmosphere (argon with 5% of hydrogen).\n\nCerium and manganese doped strontium aluminate (Ce,Mn:SrAlO) shows intense narrowband (22 nm wide) phosphorescence at 515 nm when excited by ultraviolet radiation (253.7 nm mercury emission line, to lesser degree 365 nm). It can be used as a phosphor in fluorescent lamps for e.g. photocopiers. A small amount of silicon substituting the aluminium can increase emission intensity by about 5%; the preferred composition of the phosphor is CeMn:SrAlSiO.\n\nStrontium aluminate cement (SrAlO, or SrO.AlO) can be used as refractory structural material. It can be prepared by sintering of a blend of strontium oxide or strontium carbonate with alumina, in a roughly equimolar ratio, at about 1500 °C. It can be used as a cement for refractory concrete for temperatures up to 2000 °C. Barium aluminate cement has similar characteristics, and can be furthermore used as radiation shielding. The use of barium and especially strontium aluminate cements is limited by the availability of the raw materials.\n\nStrontium aluminates are examined as proposed materials for immobilization of fission products of radioactive waste, namely the strontium-90.\n\n\n"}
{"id": "39585748", "url": "https://en.wikipedia.org/wiki?curid=39585748", "title": "Synthesis of bioglass", "text": "Synthesis of bioglass\n\nUp to now, various methods have been developed for the synthesis of bioglass and its composites including conventional melt quench, sol–gel, flame synthesis and microwave irradiation. Bioglass synthesis has been reviewed by various groups. In this section we will majorly focus on sol-gel synthesis of bioglass composites, which is the highly efficient technique for bioglass composites for tissue engineering applications.\n\nThe first bioactive glass was developed by Hench in 1969 through melting mixture of the related oxide precursors at relatively high temperatures. The original bioactive glass was melt-derived (46.1 mol%, SiO, 24.4 mol%, NaO, 26.9 mol% CaO, and 2.6 mol% PO) and was named Bioglass®.The choice of the glass composition for a specific application should be based on a firm knowledge on the influence of all major components on the most relevant properties of the glass with regard to both the final use and the manufacture of the product. Despite extensive research during the past 40 years, only a few glass compositions have been accepted for clinical use. The two US Food and Drug Administration FDA approved melt-derived compositions 45S5 and S53P4 consist of four oxides: SiO, NaO, CaO and PO. In general, a great number of elements can be dissolved in glasses. The effect of AlO, BO, FeO, MgO, SrO, BaO, ZnO, LiO, KO, CaF and TiO on the in vitro or in vivo properties of certain compositions of bioactive glasses has been reported. However, the influence of the composition on the properties and compatibility of bioactive and biodegradable glasses is not fully understood.\n\nThe scaffolds fabricated by melt quench technique have very less porosity which causes healing and defects in tissue integration during in-vivo testing.\n\nThe sol–gel process has a long history of use for synthesis of silicate systems and other oxides and it has become a widely spread research field with high technological relevance, for example for the fabrication of thin films, coatings, nanoparticles and fibers. Sol-gel processing technology at low temperatures, an alternative to traditional melt processing of glasses, involves the synthesis of a solution (sol), typically composed of metal-organic and metal salt precursors followed by the formation of a gel by chemical reaction or aggregation, and lastly thermal treatment for drying, organic removal, and sometimes crystallization and cooling treatment. The synthesis of specific silicate bioactive glasses by the sol–gel technique at low temperatures using metal alkoxides as precursors was shown in 1991 by Li et al. For the synthesis of bioactive glasses, typical precursors used are tetraethyl orthosilicate, calcium nitrate and triethylphosphate. After hydrolysis and poly-condensation reactions a gel is formed which subsequently is calcinated at 600–700°C to form the glass. Based on the preparation method, sol–gel derived products, e.g. thin films or particles are highly porous exhibiting a high specific surface area. Recent work on fabricating bioactive silicate glass nanoparticles by sol–gel process has been carried out by Hong et al. In their research, nanoscale bioactive glass particles were obtained by the combination of two steps; sol–gel route and co-precipitation method, wherein the mixture of precursors was hydrolyzed in acidic environment and condensed in alkaline condition separately, and then followed by a freeze-drying process. The morphology and size of bioactive glass nanoparticles could be tailored by varying the production conditions and the feeding ratio of reagents.\n\nDifferent ions can be added to bioactive glasses, such as zinc, magnesium, zirconium, titanium, boron, and silver in order to improve the glass functionality and bioactivity. However, it is usually difficult to synthesize bioactive glasses in nano-size scale with addition of those ions. More recently, Delben et al. have developed sol–gel-derived bioactive glass doped with silver and reported that the Si–O–Si bond number increased with increasing silver concentration and this resulted in structural densification. It was also observed that quartz and metallic silver crystallization increased with the increase in silver content in bioactive glass while hydroxyapatite crystallization decreased.\n\nThere is wide agreement about the versatility of the sol–gel technique to synthesize inorganic materials and it has been shown to be suitable for production of a variety of bioactive glasses. However, the method is also limited in terms of compositions that can be produced. Moreover remaining water or residual solvent content may result in complications of the method for the intended biomedical applications of the nanoparticles or nanofibres produced. Usually a high temperature calcination step is required to eliminate organics remnants. In addition, sol–gel processing is relatively time consuming and since it is not a continuous process, batch-to-batch variations may occur.\n\nRecently ultrasonic assisted synthesis and microwave assisted synthesis is gaining attention as they can help to reaction in a short time and can modify the reaction environment to produce nano phase powders. It is a rapid and low cost powder synthesis method for powders.For synthesis, the precursors were dissolved in de-ionized water and transferred to the ultrasonic bath. The irradiation time was varied to obtain the optimum synthesis condition. Microwave operation was performed in a second batch of powders after the ultrasonic irradiation. The obtained amorphous powder was washed in de-ionized water and filtered. After drying for 24 hours in oven at 80°C the powders were calcined at 700°C temperatures for the development of bioglass.\n"}
{"id": "15112768", "url": "https://en.wikipedia.org/wiki?curid=15112768", "title": "Tour de Sol", "text": "Tour de Sol\n\nThe Tour de Sol in Switzerland was the first rally for solar powered vehicles. It was carried out annually from 1985 to 1993. The first event started on June 25 in Romanshorn on the Lake of Constance, and finished on June 30 in Geneva. 72 vehicles started in two classes; over 50 finished. The vehicles were powered exclusively by direct onboard solar power in addition to an initial charge of the onboard accumulators. The second class also allowed direct human power with pedals. The rally was conceived as a kind of race with the winners being those using the least time to travel the set course each day. The course was on unclosed public roads and the drivers were required to adhere all traffic rules and speed limits. The first events were very popular with thousands of spectators lining the roads and visiting the camps where the vehicles stopped each day. In later years the fastest vehicles also raced on round-circuit closed-off courses each day after arriving at the stops. From 1990 the organisers also held separate events called Tour de Sol Alpine. These included closed courses on frozen lakes and snowy roads and on unclosed mountain passes.\n\nAfter a few years other organisers carried out similar rallies, e.g. the American Tour de Sol. In 1988 the Tour de Sol also held the first race for solar powered boats on July 1 at Estavayer-le-Lac.\n\nThe Tour de Sol initially stipulated direct solar power from onboard solar cells. After the first couple of events, a class was introduced which allowed charging accumulators from stationary solar panels and swapping accumulators. Later another class also allowed charging from the 230 VAC mains, provided that this amount of electricity was generated elsewhere by solar power and fed into the mains. This led to the development of the first domestic grid-feed systems.\n\nThe legal form of the Tour de Sol was a foundation. It ceased in 2002. Solar engineer Josef Jenni is credited as the inventor of the event and Urs Muntwyler developed and led it for most of its years.\n\nThe course of the first Tour de Sol was relatively flat and direct, in later years the courses were circuitous and included mountain passes.\n\nThe detailed regulations for the vehicles were described in a 24-page handbook. The main specifications for the vehicles were for the size of the solar cells and the accumulator battery. The racing category allowed a panel up to 6 m or 480 Watts peak and a battery capacity of up to 4.8 kWh. These vehicles were checked by the police and got temporary licences for the duration of the event. For vehicles with a regular road licence other specifications applied.\n\nThe vehicles mostly had three or four wheels but from the beginning a few bicycles with solar trailers participated. This later led to the development of Swiss production electric bicycles and motorcycles.\n\n"}
{"id": "7631731", "url": "https://en.wikipedia.org/wiki?curid=7631731", "title": "Waste Incineration Directive", "text": "Waste Incineration Directive\n\nThe Waste Incineration Directive, more formally Directive 2000/76/EC of the European Parliament and of the Council\nof 4 December 2000 on the incineration of waste (OJ L332, P91 – 111), was a Directive issued by the European Union and relates to standards and methodologies required by Europe for the practice and technology of incineration. The aim of this Directive is to minimise the impact of negative environmental effects on the environment and human health resulting from emissions to air, soil, surface and ground water from the incineration and co-incineration of waste. The requirements of the Directive were developed to reflect the ability of modern incineration plants to achieve high standards of emission control more effectively. The Directive was replaced by the Industrial Emissions Directive with effect from 7 January 2014\n\n\n"}
{"id": "41188056", "url": "https://en.wikipedia.org/wiki?curid=41188056", "title": "Waste broker", "text": "Waste broker\n\nA Waste broker is someone within the waste industry who acts as middle man between other waste industry parties. They are required to be registered with the Environment Agency.\n\nThe client would typically have some rubbish (trash) to take away, and rather than ring round a few contractors in the area they ring the Waste Broker, and give them all the details of what they want. This Broker person or organisation will have a list of those contractors in the area, and it is they who will do the ringing round or emailing, to see who is available to do the client's pickup.\n\nThe client might be a producer of recycled materials, and want to sell them. They will contact a Trade Broker who performs a similar function.\nUsually the Waste Broker is local or at most national, but the Trade Broker could be arranging shipments between countries.\n\nWRAP (definition of Waste Broker): http://aggregain.wrap.org.uk/terminology/waste_broker.html\n\nWho needs to register?\n\nBroker Responsibilities: http://www.netregs.org.uk/library_of_topics/waste/waste_carrier,_broker,_dealer/waste_brokers_responsibilities.aspx\n\nMagazine article: http://www.waste-management-world.com/articles/2012/10/waste-management-brokers-are-good-for-business.html\n\nScotland: http://www.sepa.org.uk/waste/waste_regulation/application_forms/waste_carriers_and_brokers.aspx\n\nWales: http://naturalresourceswales.gov.uk/apply-buy-report/apply-buy-grid/waste/register-as-a-carrier/?lang=en#.UpNRyift9vE\n"}
