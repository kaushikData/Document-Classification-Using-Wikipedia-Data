{"id": "23541193", "url": "https://en.wikipedia.org/wiki?curid=23541193", "title": "Abdus Salam Centre for Physics", "text": "Abdus Salam Centre for Physics\n\nThe Professor Abdus Salam Centre for Physics (), previously known as the Riazzudin National Centre for Physics is an academic national research institute for physics and mathematical sciences located in Islamabad, Pakistan. The Government of Pakistan had the jurisdiction of the institute from 1999 until April 2004, when the institute was made a scientific organization. However, the institute is still funded in large by the Pakistani federal government. The proposal for the establishment of the centre was first put forward in 1951. It is under the \"de facto\" control of the Strategic Plans Division of the Pakistani National Command Authority.\n\nSince its inception in 1999, the institute operates quadripartite supervision of ICTP, PAEC, INSC, and CERN, and main function is to provide the particle accelerators and other infrastructure needed for theoretical and high-energy physics research. As of today, the NCP emerged as world's leading particle physics institute producing hundreds of papers by world's scientists who joined this institute, and numerous scientific experiments have been constructed at NCP by national and international collaborations to make use of them.\n\nEstablishing world-class physics research institutes was proposed by a number of scientists. The roots of NCP institutes go back to when Nobel laureate professor Abdus Salam, after receiving his doctorate in physics, came back to Pakistan in 1951. Joining his alma mater, Government College University as Professor of Mathematics in 1951, Salam made an effort to establish the physics institute but was unable to do so. The same year, he became chairman of the Mathematics Department of the Punjab University where he tried to revolutionise the department by introducing the course of Quantum Mechanics necessary for undergraduate students, but it was soon reverted by the vice-chancellor. He soon faced the choice between intellectual death or migration to the stimulating environment of a western institutions. This choice, however, left a deep impression on him and was behind his determination to create an institution to which physicists from developing countries would come as a right to interact with their peers from industrially advanced countries without permanently leaving their own countries. This resulted in founding to the International Centre for Theoretical Physics (ICTP) by Professor Abdus Salam in Italy.\n\nIn 1974, Prof. Abdus Salam visualised the need of an institution where experts from the industrialised nations and learners from the developing countries could get together for a couple of weeks once a year to exchange views on various subjects of current interest in Physics and allied sciences. His suggestion was accepted by Chairman of Pakistan Atomic Energy Commission (PAEC) Munir Ahmad Khan and it was the year 1976 when the first International Nathiagali Summer College on Physics and Contemporary Needs (INSC) was inaugurated at Nathiagali, with co-sponsorship of ICTP and PAEC, under the directorship of Prof. Riazuddin, a pupil student of Abdus Salam. The same year, Ishfaq Ahmad established the Institute of Nuclear Physics at the University of Engineering and Technology of Lahore where Abdus Salam was invited to give first lectures on particle physics and quantum mechanics.\n\nSince then, it has been regularly held without break and it is a great credit to Prof. Riazuddin for his dedication and commitment as such type of international scientific gathering in a developing country like Pakistan presents a major step for the promotion of science. A major aim and goal, Prof. Abdus Salam had in his mind when he made his original proposal to PAEC in 1974 that Nathiagali Summer College would evolve into a full-fledged Centre for Physics on the pattern of ICTP. To transform his vision to reality, his student Prof. Riazuddin played a major role.\n\nThe National Centre for Physics came into reality when Prof. Riazuddin arranged a one-day symposium on \"Frontiers of Fundamental Physics\" on 27 January 1999 at the Institute of Physics of Quaid-e-Azam University, only seven months before the recent tests, (\"Chagai-I\"). All the leading scientists of the country and some visitors from CERN attended this symposium and they provided their support. Prof. Riazuddin being the founding father of NCP, was its first Director-General and it was inaugurated by Dr. Ishfaq Ahmad, Chairman of Pakistan Atomic Energy Commission during this period, on 16 May 2000. The Director General of European Organization for Nuclear Research (CERN), Dr. Luciano Maiani and distinguished members of his delegation, the Vice-Chancellor of Quaid-i-Azam University, Dr. Tariq Saddiqui and other dignitaries, witnessed the inauguration. The first academic faculty of this institute were included Munir Ahmad Khan, Pervez Hoodbhoy, Fiazuddin, Masud Ahmad, and Ishfaq Ahmad, who first presented their physics papers to the institutes and CERN.\n\nIn 2008, Dr. Hamid Saleem became its Director-General after, his predecessor and founding father of NCP, Prof. Riazuddin, who was made lifetime Director General Emeritus. The vision of Prof. Riazuddin to make NCP one of the leading Physics institute of Pakistan is now being carried by Dr. Hamid Saleem.\n\nNCP offers research in different branches of Physics such as particle Physics, computational physics, Astrophysics, Cosmology, Atmospheric physics, Atomic, molecular, and optical physics, Chemical physics, Condensed matter physics, (Fluid dynamics, Laser Physics, Mathematical physics, Plasma Physics·, Quantum field theory, Nano Physics, Quantum information theory\n\nNCP is collaborating with CERN in the field of experimental high-energy physics. NCP and CERN are involved in the development, testing and fabrication of 432 Resistive Plate Chambers (RPC) required for the CMS muon detector at CERN. The RPC has an excellent time resolution i.e. of the order of 1–2 nanoseconds and it will be used for the bunch tagging at LHC. At the national level, this project is a joint collaboration of NCP and PAEC, whereas at international level, NCP also collaborating with Italy, China, South Korea and US.\n\nThe RPC is a gaseous detector made using two parallel-plates of bakelite with high resistivity. Each RPC for CMS will be equipped with 96 electronic channels, which will be readout are based on 0.14 micrometre BiCMOS technology. For the complete system, number of readout channels are around 50,000. RNCP has an experimental high energy physics laboratory which is equipped with the high speed and advanced data acquisition system based on VME standards. This laboratory is used for prototyping and testing of RPCs at present.\n\nHigh performance and data intense computing is the back bone of modern-day science. There are stringent requirements for computing at LHC. To exploit the full physics potential of LHC data in comprehensive manner. NCP will require high performance computing for accelerator physics, computational condensed matter physics and theoretical particle physics. For accessing and managing the LHC data novel techniques like the concept of data and computing grids are used. CERN has evolved a new project called the LHC Computing Grid (LCG). NCP is a partner of CERN in this project and it is the only LCG node in Pakistan.\n\nNCP signed a memorandum of understanding during dr. K. R. Sreenivasan, Director ICTP's visit to Pakistan from 26 to 30 June 2005. In addition, the Centre carries out research in areas that are not covered by any institute of Physics. One such area being pursued by the Centre involves a number of activities in Experimental High-Energy Physics through a co-operative agreement with CERN in Geneva, Switzerland. Besides this, NCP has collaborations with several international institutes and universities in the field of theoretical physics including AS-ICTP, Trieste, Italy; Centre for Plasma Astrophysics (CPA), K-Leuven University, Belgium; Tokyo University, Tokyo, Japan; Ruhr University, Bochum (RUB), Germany and many others. Several research papers are published in reputed international journals each year from NCP through national and international collaborations.\n\n\nRNCP and the Other independent countries have signed formal Memorandum of Understanding agreements are below:\n\n\n"}
{"id": "25949138", "url": "https://en.wikipedia.org/wiki?curid=25949138", "title": "Annesley Kingsford", "text": "Annesley Kingsford\n\nAnnesley Douglas Kingsford (30 July 1912 – 1 April 2006) was a Canadian rower who competed for Great Britain at the 1936 Summer Olympics.\n\nKingsford was born in Dublin, the son of Douglas Hollingshead Kingsford of Calgary, Alberta, Canada, and his wife Margaret. He was educated at Uppingham School and Pembroke College, Cambridge. In 1934, he was a member of the winning Cambridge boat in the Boat Race. He was in the winning crew again in 1935 when his brother Desmond Kingsford was also in the crew. In 1936, he was a member of the crew of the eight which came fourth representing Great Britain at the 1936 Summer Olympics in Berlin.\n\nHe then served in the Royal Navy and later emigrated with his wife to Canada where he started a successful career in the oil industry. He also acquired the OH ranch during this time. He was also a member of the Glencoe Club and is featured on the \"Glencoe at the Olympics\" wall.\n\nKingsford married Marie Harvie in Basra, Iraq. They had three children, Patrick, Douglas, and Kelly. His grandchildren live mostly in Canada, except for Kelly's children, Brogan Lamoureux and Boyd Lamoureux, who live in Australia.\n\nKingsford died in Qualicum Beach, British Columbia, Canada at the age of 93.\n\n"}
{"id": "607470", "url": "https://en.wikipedia.org/wiki?curid=607470", "title": "Bagasse", "text": "Bagasse\n\nBagasse ( ) is the fibrous matter that remains after sugarcane or sorghum stalks are crushed to extract their juice. It is dry pulpy residue left after the extraction of juice from sugar cane. Bagasse is used as a biofuel and in the manufacture of pulp and building materials.\n\nAgave bagasse is a similar material that consists of the tissue of the blue agave after extraction of the sap.\n\nBagasse can be used to generate electricity. Dry bagasse is burnt to produce steam. The steam is used to rotate turbines to produce power.\n\nFor every 10 tonnes of sugarcane crushed, a sugar factory produces nearly three tonnes of wet bagasse. Since bagasse is a by-product of the cane sugar industry, the quantity of production in each country is in line with the quantity of sugarcane produced.\n\nThe high moisture content of bagasse, typically 40–50 percent, is detrimental to its use as a fuel. In general, bagasse is stored prior to further processing. For electricity production, it is stored under moist conditions, and the mild exothermic process that results from the degradation of residual sugars dries the bagasse pile slightly. For paper and pulp production, it is normally stored wet in order to assist in removal of the short pith fibres, which impede the paper making process, as well as to remove any remaining sugar.\n\nA typical chemical analysis of washed and dried bagasse might show:\n\n\nBagasse is a heterogeneous material containing around 30-40 percent of \"pith\" fibre, which is derived from the core of the plant and is mainly parenchyma material, and \"bast\", \"rind\", or \"stem\" fibre, which makes up the balance and is largely derived from sclerenchyma material. These properties make bagasse particularly problematic for paper manufacture and have been the subject of a large body of literature.\n\nMany research efforts have explored using bagasse as a renewable power generation source and for the production of bio-based materials.\n\nBagasse is often used as a primary fuel source for sugar mills. When burned in quantity, it produces sufficient heat energy to supply all the needs of a typical sugar mill, with energy to spare. To this end, a secondary use for this waste product is in cogeneration, the use of a fuel source to provide both heat energy, used in the mill, and electricity, which is typically sold on to the consumer electrical grid.\n\nThe lower calorific value (LCV) of bagasse in kJ/kg may be estimated using the formula: \"LCV\" = 18260 - 207.01 × \"Moisture\" - 31.14 × \"Brix\" - 182.60 × \"Ash\", where the moisture, brix and ash content of the bagasse are expressed as a percentage by mass. Similarly, the higher calorific value (HCV) of bagasse may be estimated using: \"HCV\" = 19605 - 196.05 × \"Moisture\" - 31.14 × \"Brix\" - 196.05 × \"Ash\".\n\nThe resulting CO emissions are less than the amount of CO that the sugarcane plant absorbed from the atmosphere during its growing phase, which makes the process of cogeneration greenhouse-gas-neutral. In countries such as Australia, sugar factories contribute \"green\" power to the electricity grid. Hawaiian Electric Industries also burns bagasse for cogeneration.\n\nEthanol produced from the sugar in sugarcane is a popular fuel in Brazil. The cellulose-rich bagasse is being widely investigated for its potential for producing commercial quantities of cellulosic ethanol. For example, until May 2015 BP was operating a cellulosic ethanol demonstration plant based on cellulosic materials in Jennings, Louisiana.\n\nBagasse's potential for advanced biofuels has been shown by several researchers. However, the compatibility with conventional fuels and suitability of these crude fuels in conventional engines have yet to be proven.\n\nBagasse is commonly used as a substitute for wood in many tropical and subtropical countries for the production of pulp, paper and board, such as India, China, Colombia, Iran, Thailand, and Argentina. It produces pulp with physical properties that are well suited for generic printing and writing papers as well as tissue products but it is also widely used for boxes and newspaper production. It can also be used for making boards resembling plywood or particle board, called bagasse board and Xanita board, and is considered a good substitute for plywood. It has wide usage for making partitions and furniture.\n\nThe industrial process to convert bagasse into paper was developed in 1937 in a small laboratory in Hacienda Paramonga, a sugar mill on the coast of Peru owned by W.R. Grace Company. With a promising method, the company bought an old paper mill in Whippany, New Jersey and shipped bagasse from Peru to test the viability of the process on an industrial scale. The first paper manufacturing machines were designed in Germany and installed in the Cartavio sugar cane plant in 1938. Sociedad Paramonga was bought in 1997 by Quimpac and in 2015 produced 90,000 metric tons of office paper, toilet paper and cardboard for the Peruvian market.\n\nK-Much Industry has patented a method of converting bagasse into cattle feed by mixing it with molasses and enzymes (such as bromelain) and fermenting it. It is marketed in Thailand, Japan, Malaysia, Korea, Taiwan and Middle East and Australia.\n\nXanita, a South African company, mixes 30 percent bagasse cellulose fibres in with recycled kraft paper fibre to make ultra-lightweight composite boards. These are sold as an environmentally friendly, formaldehyde-free alternative to MDF and particle board.\n\nNanocellulose can be produced from bagasse through various conventional and novel processes. This provides a pathway to generate higher-value products from what can be considered a process waste stream. \n\nWorkplace exposure to dust from the processing of bagasse can cause the chronic lung condition pulmonary fibrosis, more specifically referred to as bagassosis.\n\nProcessed bagasse is added to human food as sugarcane fiber. It is a soluble fiber but can help promote intestinal regularity. One animal study suggests that sugarcane fiber combined with a high-fat diet may help control type 2 diabetes. Bagasse are good sources of lignoceric and cerotic acids.\n\nIn Guangxi Zhuang Autonomous Region, China, bagasse is sometimes used to smoke bacon and sausages.\n\n\n"}
{"id": "3047082", "url": "https://en.wikipedia.org/wiki?curid=3047082", "title": "Blogdex", "text": "Blogdex\n\nBlogdex was an online resource for understanding hot topics of discussion in the blogosphere.\n\nThe site offered a time-weighted list of links to online content cited by more than one monitored blog in the recent past. Each link received a score based both on the number of different blogs citing it and the recency of those citings; the list thus typically features both popular oddities of the day as well as informative and/or controversial source material for current topics of public debate. Despite its explicit focus on blogs, it can be thought of as the original memetracker, and the inspiration for later commercial sites such as tailrank.com, Digg.com, and other social media sites.\n\nAs the previous owner of the domains blogdex.com, blogdex.net and blogdex.org, Jimmy Wales offered the domains to MIT free of charge for use in this project. Blogdex then migrated from the original blogdex.media.mit.edu location to blogdex.net.\n\nBlogdex was created by Cameron Marlow, then a Ph.D. student at MIT along with Elizabeth Wood, then a high school student attending the RSI summer program at MIT. Marlow now works for Facebook.\n\nBlogdex has been offline since May 2006.\n\n"}
{"id": "22821404", "url": "https://en.wikipedia.org/wiki?curid=22821404", "title": "Bulgarian Energy Holding", "text": "Bulgarian Energy Holding\n\nBulgarian Energy Holding EAD (BEH EAD) is a state owned energy holding company in Bulgaria. It was incorporated on 18 September 2008 after renaming Bulgargaz Holding EAD. In November 2009, the Bulgarian Government decided to list the company at the Bulgarian Stock Exchange – Sofia.\n\nIt is the owner of the Maritsa Iztok-2 power station. This power station was ranked as the industrial facility that is causing the highest damage costs to health and the environment in Bulgaria and the entire European Union in November 2014 by the European Environment Agency.\n\nBEH EAD has following subsidiaries:\n\n"}
{"id": "22856433", "url": "https://en.wikipedia.org/wiki?curid=22856433", "title": "CIPAMEX", "text": "CIPAMEX\n\nCIPAMEX, more fully La Sociedad para el Estudio y Conservación de las Aves en México, is a Mexican ornithological non-profit organization, with its principal objective the study and conservation of Mexican birds and their habitats. It publishes the journal \"Huitzil\" in electronic format. CIPAMEX was established in 1947, formally constituted in 1988, and is a member of the Ornithological Council.\n\n\n"}
{"id": "42711198", "url": "https://en.wikipedia.org/wiki?curid=42711198", "title": "Campo Verde Solar Project", "text": "Campo Verde Solar Project\n\nCampo Verde Solar Project is a 139-megawatt (MW) solar photovoltaic power station in Imperial County, California. The project was approved in December 2012. Construction began in early 2013 and was completed the same year. Designed and constructed by U.S. thin-film manufacturer First Solar, the plant uses nearly 2.3 million CdTe-PV modules. Campo Verde Solar was acquired in April 2013 by Southern Power and Turner Renewable Energy. First Solar acquired the project in 2012 from US Solar Holdings LLC, which had developed the project and negotiated the 139 MW PPA with SDG&E. \n\n"}
{"id": "3461780", "url": "https://en.wikipedia.org/wiki?curid=3461780", "title": "Carbenium ion", "text": "Carbenium ion\n\nA carbenium ion is a positive ion with the structure RR′R″C, that is, a chemical species with a trivalent carbon that bears a +1 formal charge.\n\nIn older literature the name carbonium ion was used for this class, but now it refers exclusively to another family of carbocations, the carbonium ions, where the charged carbon is pentavalent. The current definitions were proposed by the chemist George Andrew Olah in 1972, and are now widely accepted.\n\nCarbenium ions are generally highly reactive due to having an incomplete octet of electrons; however, certain carbenium ions, such as the tropylium ion, are relatively stable due to the positive charge being delocalised between the carbon atoms.\n\nCarbenium ions are classified as primary, secondary, or tertiary depending on whether the number of carbon atoms bonded to the ionized carbon is 1, 2, or 3. (Ions with zero carbons attached to the ionized carbon, such as methenium, , are usually included in the primary class).\n\nStability typically increases with the number of alkyl groups bonded to the charge-bearing carbon. Tertiary carbocations are more stable (and form more readily) than secondary carbocations, because they are stabilized by hyperconjugation. Primary carbocations are highly unstable. Therefore, reactions such as the S1 reaction and the E1 elimination reaction normally do not occur if a primary carbenium would be formed.\n\nHowever, a carbon doubly bonded with the ionized carbon can stabilize the ion by resonance. Such cations as the \"allyl\" cation, , and the \"benzyl\" cation, , are more stable than most other carbocations. Molecules which can form allyl or benzyl carbeniums are especially reactive. Carbenium ions can also be stabilized by heteroatoms.\n\nCarbenium ions may undergo rearrangement reactions from less stable structures to equally stable or more stable ones with rate constants in excess of 10 s. This fact complicates synthetic pathways to many compounds. For example, when 3-pentanol is heated with aqueous HCl, the initially formed 3-pentyl carbocation rearranges to a statistical mixture of the 3-pentyl and 2-pentyl. These cations react with chloride ion to produce about 3-chloropentane and 2-chloropentane.\n\nCarbenium ions can be prepared directly from alkanes by removing a hydride anion, , with a strong acid. For example, magic acid, a mixture of antimony pentafluoride () and fluorosulfuric acid (), turns isobutane into the cation .\n\nThe tropylium ion is an aromatic species with the formula . Its name derives from the molecule tropine (itself named for the molecule atropine). Salts of the tropylium cation can be stable, e.g. tropylium tetrafluoroborate. It can be made from cycloheptatriene (tropylidene) and bromine or phosphorus pentachloride\n\nIt is a heptagonal, planar, cyclic ion; it also has 6 π-electrons (4\"n\" + 2, where \"n\" = 1), which fulfills Hückel's rule of aromaticity. It can coordinate as a ligand to metal atoms.\n\nThe structure shown is a composite of seven resonance contributors in which each carbon carries part of the positive charge.\n\nIn 1891 G. Merling obtained a water-soluble salt from a reaction of cycloheptatriene and bromine. The structure was elucidated by Eggers Doering and Knox in 1954.\n\nAnother aromatic carbenium ion is the cyclopropenyl or cyclopropenium ion, , obtained by Ronald Breslow and John T. Groves in 1970. Though less stable than the tropylium cation, this carbenium ion can also form salts at room temperature. Solutions of such salts were found by Breslow and Groves to have spectroscopic and chemical properties matching expectations for an aromatic carbenium ion.\n\nThe triphenylmethyl cation, , is especially stable because the positive charge can be distributed among 10 of the carbon atoms (the 3 carbon atoms in the \"ortho\" and \"para\" positions of each of the three phenyl groups, plus the central carbon atom). It exists in the compounds triphenylmethyl hexafluorophosphate (C(CH)PF) and triphenylmethyl perchlorate (C(CH)ClO).\n\nTriphenylmethyl hexafluorophosphate is used as a catalyst and reagent in organic syntheses; it is generated by combining silver hexafluorophosphate with triphenylmethyl chloride:\n\nThese and other similar cations can be obtained as intensely colored solutions by dissolving aryl-substituted methanols in concentrated sulfuric acid.\n\nAn arenium ion is a cyclohexadienyl cation that appears as a reactive intermediate in electrophilic aromatic substitution. For historic reasons this complex is also called a \"Wheland intermediate\", or a \"σ-complex\".\nTwo hydrogen atoms bonded to one carbon lie in a plane perpendicular to the benzene ring. The arenium ion is no longer an aromatic species; however it is relatively stable due to delocalization: the positive charge is delocalized over 5 carbon atoms via the π system, as depicted on the following resonance structures:\nAnother contribution to the stability of arenium ions is the energy gain resulting from the strong bond between the benzene and the complexed electrophile.\n\nThe smallest arenium ion is protonated benzene, . The benzenium ion can be isolated as a stable compound when benzene is protonated by the carborane superacid, H(CBH(CH)Br). The benzenium salt is crystalline with thermal stability up to 150 °C. Bond lengths deduced from X-ray crystallography are consistent with a cyclohexadienyl cation structure.\n\nAn acylium ion is a cation with the formula RCO. The structure is described as R−C≡O or R−=O. It is the synthetic and reactive equivalent of an acyl carbocation, but the actual structure has the oxygen and carbon linked by a triple bond. Such species are common reactive intermediates, for example, in the Friedel−Crafts acylations also in many other organic reactions such as the Hayashi rearrangement. Salts containing acylium ions can be generated by removal of the halide from acyl halides: \nThe C–O distance in these cations is near 1.1 ångströms, even shorter than that in carbon monoxide. Acylium cations are characteristic fragments observed in EI-mass spectra of ketones.\n\n"}
{"id": "2394627", "url": "https://en.wikipedia.org/wiki?curid=2394627", "title": "Cetyl alcohol", "text": "Cetyl alcohol\n\nCetyl alcohol , also known as hexadecan-1-ol and palmityl alcohol, is a fatty alcohol with the formula CH(CH)OH. At room temperature, cetyl alcohol takes the form of a waxy white solid or flakes. The name cetyl derives from the whale oil () from which it was first isolated.\n\nCetyl alcohol was discovered in 1817 by the French chemist Michel Chevreul when he heated spermaceti, a waxy substance obtained from sperm whale oil, with caustic potash (potassium hydroxide). Flakes of cetyl alcohol were left behind on cooling. Modern production is based around the reduction of palmitic acid, which is obtained from palm oil.\n\nCetyl alcohol is used in the cosmetic industry as an opacifier in shampoos, or as an emollient, emulsifier or thickening agent in the manufacture of skin creams and lotions. It is also employed as a lubricant for nuts and bolts, and is the active ingredient in some \"liquid pool covers\" (forming a surface layer to reduce evaporation and retain heat).\n\nPeople who suffer from eczema can be sensitive to cetyl alcohol, though this may be due to impurities rather than cetyl alcohol itself. However, cetyl alcohol \"is\" sometimes included in medications used for the treatment of eczema.\n\n"}
{"id": "31929168", "url": "https://en.wikipedia.org/wiki?curid=31929168", "title": "Coal upgrading technology", "text": "Coal upgrading technology\n\nCoal upgrading technology refers to a class of technologies developed to remove moisture and certain pollutants from low rank coals such as sub-Bituminous coal and lignite (brown coal) and raise their calorific values. Companies located in Australia, Germany and the United States are the principal drivers of the research, development and commercialisation of these technologies.\n\nAround 30 nations collectively operate more than 1,400 brown coal-fired power stations around the world. Brown coal power stations that cannot economically dewater brown coal are inefficient and cause of high levels of carbon emissions. High emitting power stations, notably the Hazelwood power station in Australia, attract environmental criticism. Many modern economies including Greece and Victoria (Australia) are highly dependent on brown coal for electricity. Improved environmental performance and the need for stable economic environment provide incentive for investment to substantially reduce the negative environmental impact of burning raw ('as mined') brown coal.\n\nCoal upgrading technologies remove moisture from 'as mined' brown coal and transform the calorific performance of brown coal to a 'cleaner' burning status relatively equivalent to high calorific value black coal. Some coal upgrading processes result in a densified coal product that is considered to be a Black coal equivalent product suitable for burning in black coal boilers.\n\nVictorian brown coal with a characteristic moisture content of 60% by weight is regarded as the 'wettest' brown coal in the world. The high moisture content is the key reason why the state's three major power stations are collectively regarded as the dirtiest carbon emitters in the world. Studies undertaken by the University of Melbourne and Monash University confirm that when moisture is removed from Victorian brown coal, naturally low levels of ash, sulfur and other elements rank it as being one of the cleanest coals in the world. When dewatered upgraded brown coal can compete in the export market at comparable prices to black coal.\n\nWith significant levels of brown coal mining occurring around the world, and mining levels increasing, the need for coal upgrading technologies has become more apparent. the technologies will help to address global environmental concern of rising emissions from the burning of brown coal and provide alternative fuel options to rapidly emerging economies such as Vietnam that face difficulty competing for black coal with China, India, Japan and other nations.\n\nBecause of inherent high moisture content, all lignites need to be dried prior to combustion. Depending on the technology type drying is achieved either via a discrete operation or part of a process. The comparison chart identifies different technology drying methods that are in development in different countries and provides a qualitative comparison.\n"}
{"id": "3327855", "url": "https://en.wikipedia.org/wiki?curid=3327855", "title": "Composite plate", "text": "Composite plate\n\nA composite plate is basically a plate made out of composite materials, i.e. a resin and a fibre. Its mechanical evaluation is more detailed than a normal isotropic plate as it has different material properties in different directions. Composite materials are very light and strong and hence much used in aircraft and spacecraft industries. They are also readily available.\n\nComposites are further divided into main two classification they are ... resin material and matrix material\n"}
{"id": "1441111", "url": "https://en.wikipedia.org/wiki?curid=1441111", "title": "Debye sheath", "text": "Debye sheath\n\nThe Debye sheath (also electrostatic sheath) is a layer in a plasma which has a greater density of positive ions, and hence an overall excess positive charge, that balances an opposite negative charge on the surface of a material with which it is in contact. The thickness of such a layer is several Debye lengths thick, a value whose size depends on various characteristics of plasma (e.g. temperature, density, etc.).\n\nA Debye sheath arises in a plasma because the electrons usually have a temperature on the order of magnitude or greater than that of the ions and are much lighter. Consequently, they are faster than the ions by at least a factor of formula_1. At the interface to a material surface, therefore, the electrons will fly out of the plasma, charging the surface negative relative to the bulk plasma. Due to Debye shielding, the scale length of the transition region will be the Debye length formula_2. As the potential increases, more and more electrons are reflected by the sheath potential. An equilibrium is finally reached when the potential difference is a few times the electron temperature.\n\nThe Debye sheath is the transition from a plasma to a solid surface. Similar physics is involved between two plasma regions that have different characteristics; the transition between these regions is known as a double layer, and features one positive, and one negative layer.\n\nSheaths were first described by American physicist Irving Langmuir. In 1923 he wrote:\n\nLangmuir and co-author Albert W. Hull further described a sheath formed in a thermionic valve:\n\nThe quantitative physics of the Debye sheath is determined by four phenomena:\nEnergy conservation of the ions: If we assume for simplicity cold ions of mass formula_3 entering the sheath with a velocity formula_4, having charge opposite to the electron, conservation of energy in the sheath potential requires\n\nformula_5,\n\nwhere formula_6 is the charge of the electron taken positively, i.e. formula_7 x formula_8 formula_9.\n\nIon continuity: In the steady state, the ions do not build up anywhere, so the flux is everywhere the same:\n\nformula_10.\n\nBoltzmann relation for the electrons: Since most of the electrons are reflected, their density is given by\n\nformula_11.\n\nPoisson's equation: The curvature of the electrostatic potential is related to the net charge density as follows:\n\nformula_12.\n\nCombining these equations and writing them in terms of the dimensionless potential, position, and ion speed,\n\nformula_13\n\nformula_14\n\nformula_15\n\nwe arrive at the sheath equation:\n\nformula_16.\n\nThe sheath equation can be integrated once by multiplying by formula_17:\n\nformula_18\n\nAt the sheath edge (formula_19), we can define the potential to be zero (formula_20) and assume that the electric field is also zero (formula_21). With these boundary conditions, the integrations yield\n\nformula_22\n\nThis is easily rewritten as an integral in closed form, although one that can only be solved numerically. Nevertheless, an important piece of information can be derived analytically. Since the left-hand-side is a square, the right-hand-side must also be non-negative for every value of formula_23, in particular for small values. Looking at the Taylor expansion around formula_20, we see that the first term that does not vanish is the quadratic one, so that we can require\n\nformula_25,\n\nor\n\nformula_26,\n\nor\n\nformula_27.\n\nThis inequality is known as the Bohm sheath criterion after its discoverer, David Bohm. If the ions are entering the sheath too slowly, the sheath potential will \"eat\" its way into the plasma to accelerate them. Ultimately a so-called pre-sheath will develop with a potential drop on the order of formula_28 and a scale determined by the physics of the ion source (often the same as the dimensions of the plasma). Normally the Bohm criterion will hold with equality, but there are some situations where the ions enter the sheath with supersonic speed.\n\nAlthough the sheath equation must generally be integrated numerically, we can find an approximate solution analytically by neglecting the formula_29 term. This amounts to neglecting the electron density in the sheath, or only analyzing that part of the sheath where there are no electrons. For a \"floating\" surface, i.e. one that draws no net current from the plasma, this is a useful if rough approximation. For a surface biased strongly negative so that it draws the ion saturation current, the approximation is very good. It is customary, although not strictly necessary, to further simplify the equation by assuming that formula_30 is much larger than unity. Then the sheath equation takes on the simple form\n\nformula_31.\n\nAs before, we multiply by formula_17 and integrate to obtain\n\nformula_33,\n\nor\n\nformula_34.\n\nThis is easily integrated over ξ to yield\n\nformula_35,\n\nwhere formula_36 is the (normalized) potential at the wall (relative to the sheath edge), and \"d\" is the thickness of the sheath. Changing back to the variables formula_4 and formula_38 and noting that the ion current into the wall is formula_39, we have\n\nformula_40.\n\nThis equation is known as Child's law, after Clement D. Child (1868–1933), who first published it in 1911, or as the Child–Langmuir law, honoring as well Irving Langmuir, who discovered it independently and published in 1913. It was first used to give the space-charge-limited current in a vacuum diode with electrode spacing \"d\". It can also be inverted to give the thickness of the Debye sheath as a function of the voltage drop by setting formula_41:\n\nformula_42.\n\n"}
{"id": "12363573", "url": "https://en.wikipedia.org/wiki?curid=12363573", "title": "Diffusionless transformation", "text": "Diffusionless transformation\n\nA diffusionless transformation is a phase change that occurs without the long-range diffusion of atoms but rather by some form of cooperative, homogeneous movement of many atoms that results in a change in crystal structure. These movements are small, usually less than the interatomic distances, and the atoms maintain their relative relationships. The ordered movement of large numbers of atoms lead some to refer to these as \"military\" transformations in contrast to \"civilian\" diffusion-based phase changes.\n\nThe most commonly encountered transformation of this type is the martensitic transformation which, while being probably the most studied, is only one subset of non-diffusional transformations. The martensitic transformation in steel represents the most economically significant example of this category of phase transformations but an increasing number of alternatives, such as shape memory alloys, are becoming more important as well.\n\nWhen a structural change occurs by the coordinated movement of atoms (or groups of atoms) relative to their neighbors then the change is termed \"displacive\" transformation. This covers a broad range of transformations and so further classifications have been developed [Cohen 1979].\n\nThe first distinction can be drawn between transformations dominated by \"lattice-distortive strains\" and those where \"shuffles\" are of greater importance.\n\nHomogeneous lattice-distortive strains, also known as Bain strains, are strains that transform one Bravais lattice into a different one. This can be represented by a strain matrix S which transforms one vector, y, into a new vector, x:\n\nThis is homogeneous as straight lines are transformed to new straight lines. Examples of such transformations include a cubic lattice increasing in size on all three axes (dilation) or shearing into a monoclinic structure.\n\nShuffles, as the name suggests, involve the small movement of atoms within the unit cell. As a result, pure shuffles do not normally result in a shape change of the unit cell - only its symmetry and structure.\n\nPhase transformations normally result in the creation of an interface between the transformed and parent material. The energy required to generate this new interface will depend on its nature - essentially how well the two structures fit together. An additional energy term occurs if the transformation includes a shape change since, if the new phase is constrained by the surrounding material, this may give rise to elastic or plastic deformation and hence a strain energy term. The ratio of these interfacial and strain energy terms has a notable effect on the kinetics of the transformation and the morphology of the new phase. Thus, shuffle transformations, where distortions are small, are dominated by interfacial energies and can be usefully separated from lattice-distortive transformations where the strain energy tends to have a greater effect.\n\nA subclassification of lattice-distortive displacements can be made by considering the dilational and shear components of the distortion. In transformations dominated by the shear component, it is possible to find a line in the new phase that is undistorted from the parent phase while all lines are distorted when the dilation is predominant. Shear dominated transformations can be further classified according to the magnitude of the strain energies involved compared to the innate vibrations of the atoms in the lattice and hence whether the strain energies have a notable influence on the kinetics of the transformation and the morphology of the resulting phase. If the strain energy is a significant factor then the transformations are dubbed \"martensitic\" and if it is not the transformation is referred to as \"quasi-martensitic\".\n\nThe difference between austenite and martensite is, in some ways, quite small: while the unit cell of austenite is, on average, a perfect cube, the transformation to martensite distorts this cube by interstitial carbon atoms that do not have time to diffuse out during displacive transformation. The unit cell becomes slightly longer in one dimension and shorter in the other two. The mathematical description of the two structures is quite different, for reasons of symmetry (see external links), but the chemical bonding remains very similar. Unlike cementite, which has bonding reminiscent of ceramic materials, the hardness of martensite is difficult to explain in chemical terms.\n\nThe explanation hinges on the crystal's subtle change in dimension. Even a microscopic crystallite is millions of unit cells long. Since all of these units face the same direction, distortions of even a fraction of a percent become magnified into a major mismatch between neighboring materials. The mismatch is sorted out by the creation of myriad crystal defects, in a process reminiscent of work hardening. As in work-hardened steel, these defects prevent atoms from sliding past one another in an organized fashion, causing the material to become harder.\n\nShape memory alloys also have surprising mechanical properties, that were eventually explained by an analogy to martensite. Unlike the iron-carbon system, alloys in the nickel-titanium system can be chosen that make the \"martensitic\" phase thermodynamically stable.\n\nIn addition to displacive transformation and diffusive transformation, a new phase transformation that involves displasive sublattice transition and atomic diffusion was discovered using a high-pressure x-ray diffraction system. The new transformation mechanism has been christened a pseudomartensitic transformation.\n\n\n"}
{"id": "172335", "url": "https://en.wikipedia.org/wiki?curid=172335", "title": "Dispersion (materials science)", "text": "Dispersion (materials science)\n\nIn materials science, dispersion is the fraction of atoms of a material exposed to the surface. In general:\n\nwhere \"D\" is the dispersion, \"N\" is the number of surface atoms and \"N\" is the total number of atoms of the material. Dispersion is an important concept in heterogeneous catalysis, since only atoms that are exposed to the surface are able to play a role in catalytic surface reactions. Dispersion increases with decreasing crystallite size and approaches unity at a crystallite diameter of about 0.1 nm.\n\nEmulsion dispersion\n"}
{"id": "28581840", "url": "https://en.wikipedia.org/wiki?curid=28581840", "title": "Doicești Power Station", "text": "Doicești Power Station\n\nThe Doiceşti Power Station is a large thermal power plant located in Doicești, having 7 generation groups, 6 of 20 MW each and one of 200 MW resulting a total electricity generation capacity of 320 MW. The power plant also has another 200 MW unit but it is decommissioned.\n\nThe chimney used by the 200 MW units is 208 metres tall.\n\n"}
{"id": "25086315", "url": "https://en.wikipedia.org/wiki?curid=25086315", "title": "Dynamic hydrogen electrode", "text": "Dynamic hydrogen electrode\n\nA dynamic hydrogen electrode (DHE) is a reference electrode, more specific a subtype of the standard hydrogen electrodes for electrochemical processes by simulating a reversible hydrogen electrode with an approximately 20 to 40 mV more negative potential.\n\nA separator in a glass tube connects two electrolytes and a small current is enforced between the cathode and anode.\n\n\n"}
{"id": "13867295", "url": "https://en.wikipedia.org/wiki?curid=13867295", "title": "Feed-in tariffs in Germany", "text": "Feed-in tariffs in Germany\n\nFeed-in electricity tariffs (FiT) were introduced in Germany to encourage the use of new energy technologies such as wind power, biomass, hydropower, geothermal power and solar photovoltaics. Feed-in tariffs are a policy mechanism designed to accelerate investment in renewable energy technologies by providing them remuneration (a \"tariff\") above the retail or wholesale rates of electricity. The mechanism provides long-term security to renewable energy producers, typically based on the cost of generation of each technology. Technologies such as wind power, for instance, are awarded a lower per-kWh price, while technologies such as solar PV and tidal power are offered a higher price, reflecting higher costs.\n\nAs of July 2014, feed-in tariffs range from 3.33 ¢/kWh (4.4 ¢/kWh) for hydropower facilities over 50 MW to 12.88 ¢/kWh (17.3 ¢/kWh) for solar installations on buildings up to 30kW and 19 ¢/kWh (25.5 ¢/kWh) for offshore wind.\n\nOn 1 August 2014, a revised Renewable Energy Sources Act or EEG (2014) (colloquially called EEG2.0) entered into force. The government will now stipulate specific deployment corridors to control the uptake of renewables and the feed-in tariffs themselves will be determined by auction.\n\nThe aim is to meet Germany's renewable energy goals of 40 to 45% of electricity consumption in 2025 and 55% to 60% in 2035. The policy also aims to encourage the development of renewable technologies, reduce external costs, and increase security of energy supply.\n\nIn the first half of 2014, 28.5% of gross electricity production in Germany came from renewable sources. The Federal Environment Ministry estimated that renewables were to save 87 million tonnes of carbon dioxide by 2012. The average level of feed-in tariff was 9.53 ¢/kWh in 2005 (compared to an average cost of displaced energy of 4.7 ¢/kWh). In 2004, the total level of reallocated EEG surcharges was €2.4 billion, at a cost per consumer of 0.56 ¢/kWh (3% of household electricity costs). By 2013, the figure had risen to €20.4 billion. The tariffs are lowered every year to encourage more efficient production of renewable energy. By 2014, the EEG surcharge – which pays for the additional costs through feed-in tariffs – had increased to 6.24 ¢/kWh. As of July 2014, the regular reductions (degressions) were 1.5% per year for electricity from onshore wind and 1% per month for electricity from photovoltaics.\n\nThe solar sector employed about 56,000 people in 2013, a strong decline from previous years, due to many insolvencies and business closures. \nAlthough most of the installed solar panels are nowadays imported from China, the Fraunhofer institute estimates, that only about 30% of the EEG apportionment outflows to China, while the rest is still spent domestically. The institute also predicts that Germany's solar manufacturing sector will improve its competitive situation in the future.\n\nThe feed-in tariff system has been modified frequently. The feed-in tariff, in force since 1 August 2004, was modified in 2008. In view of the unexpectedly high growth rates, the depreciation was accelerated and a new category (>1000 kW) was created with a lower tariff. The facade premium was abolished. In July 2010, the Renewable Energy Sources Act was again amended to reduce the tariffs by a further 16% in addition to the normal annual depreciation, as the prices for PV panels had dropped sharply in 2009. Another modification of the EEG occurred in 2011, when part of the degression foreseen for 2012 was brought forward to mid-2011 as a response to unexpectedly high installations in the course of 2010.\n\nThe support duration is 20 years plus the year of project commissioning, constant remuneration. Feed-in tariffs was lowered repeatedly (decreasing by 9% default and a maximum of 24% per year). Degression will be accelerated or slowed down by three percentage points for every 1000 MW/a divergence from the target of 3500 MW/a.\n\nAs of July 2014, feed-in tariffs for photovoltaic systems range from 12.88 ¢/kWh for small roof-top system, down to 8.92 ¢/kWh for large utility scaled solar parks. Also, FiTs are restricted to PV system with a maximum capacity of 10MW. The feed-in tariff for solar PV is declining at a faster rate than for any other renewable technology.\n\nOn 1 August 2014, a revised Renewable Energy Sources Act entered into force. Specific deployment corridors now stipulate the extent to which renewable energy is to be expanded in the future and the funding rates (feed-in tariffs) gradually will no longer be fixed by the government, but will be determined by auction. Wind and solar power are to be targeted over hydro, gas (landfill gas, sewage gas, and mine gas), geothermal, and biomass. In late 2015, this new scheme is being tested, as a pilot project, for ground-mounted PV installations. With the Renewable Energy Sources Act (2017), auctions will become commonplace for new installations also for most other types of renewables.\n\n"}
{"id": "35772455", "url": "https://en.wikipedia.org/wiki?curid=35772455", "title": "Glenbrook Power Station", "text": "Glenbrook Power Station\n\nGlenbrook Power Station is a 112MW co-generation plant located at Glenbrook, south of Auckland, New Zealand. Fully integrated into the New Zealand Steel plant, and enables New Zealand Steel to optimise its energy costs. Surplus natural gas from the iron-making direct reduction kilns are used to generate electricity. Since this partnership reduces the energy consumed in the New Zealand Energy Market (NZEM) – and potentially generated from fossil-fuel burning power stations – it also reduces carbon emissions.\n\nTwo cogeneration plants commissioned in 1987 and 1997.\n\n"}
{"id": "30044834", "url": "https://en.wikipedia.org/wiki?curid=30044834", "title": "Golden Eagle Refinery", "text": "Golden Eagle Refinery\n\nThe Andeavor Martinez Refinery refines oil into gasoline and other petroleum based products. It is located in the San Francisco Bay Area in an unincorporated area known as Avon, east of Martinez, California. The refinery is owned by Andeavor.\n\nThe refinery is located on 2,200 acres, has approximately 650 full-time employees, and has a crude oil capacity of 166,000 barrels per day. It is the fourth largest refinery in the state.\n\nThe refinery was first built in 1913 under the name Avon Refinery, and has been continually expanded since. It was purchased by Tosco in 1976. By the 1990s, a history of poor maintenance and under-staffing gave the refinery a reputation for being a hazardous workplace. Throughout the 1990s, it led the US refining industry in the number of environmental and safety code violations. These poor conditions culminated in two catastrophic accidents. In the first, a 1997 hydrocracker explosion killed one worker. In the second of these, four workers died and a fifth was hospitalized in a 1999 naphtha explosion. A maintenance supervisor refused to shut down the unit after corroding valves failed to stop the flow of the extremely hazardous substance.\n\nIn November 2010, the refinery had a flaring event, due to a simultaneous PG&E utility power and Foster Wheeler co-generation plant failure, that produced large quantities of thick black smoke.\n\nIn February 2015, a nationwide strike by USW represented employees resulted in the closure of the Martinez refinery, the only refinery closure resulting from the strike.\n\nOn December 15, 2015, thick smoke and flames could be seen rising up from the refinery. Due to the loss of a primary steam generation unit, flaring was done to release pressure. A Level 2 alert was issued to the community, recommending that they stay indoors due to smoke blowing offsite. The alert was reduced to Level 0 on the same day.\n"}
{"id": "43650493", "url": "https://en.wikipedia.org/wiki?curid=43650493", "title": "Gunaajav Batjargal", "text": "Gunaajav Batjargal\n\nGunaajav Batjargal (born 1966) is a Mongolian diplomat who has been Mongolian Ambassador to Austria since 2013. He concurrently serves as Permanent Representative to the OSCE, UNIDO and CTBTO, and Resident Representative to the IAEA. He graduated from Moscow State Institute of International Relations in 1990, and from the University of East Anglia in 1996 where he was a Chevening Scholar.\n"}
{"id": "25098165", "url": "https://en.wikipedia.org/wiki?curid=25098165", "title": "Hydraulic compressor", "text": "Hydraulic compressor\n\nHydraulic compressors are usually divided in four main groups: piston compressors, rotary vane compressors, rotary screw compressors and gear compressors. The piston-models are the most suitable for basic need for compressed air such as energizing small handtools or airflushing of small quarry drills. Rotary vane-models include also a cooled lubrication system, oil separator, relief valve on the air intake and automatic rotation speed valve. Rotary vane-models are the most suitable for installation on different excavators, mining and other machines.\n\n\n"}
{"id": "25317580", "url": "https://en.wikipedia.org/wiki?curid=25317580", "title": "Hypothiocyanite", "text": "Hypothiocyanite\n\nHypothiocyanite is the anion [OSCN] and the conjugate base of hypothiocyanous acid (HOSCN). It is an organic compound part of the thiocyanates as it contains the functional group SCN. It is formed when an oxygen is singly bonded to the thiocyanate group. Hypothiocyanous acid is a fairly weak acid; its acid dissociation constant (p\"K\") is 5.3.\n\nHypothiocyanite is formed by peroxidase catalysis of hydrogen peroxide and thiocyanate:\n\nHypothiocyanite occurs naturally in the antimicrobial immune system of the human respiratory tract in a redox reaction catalyzed by the enzyme lactoperoxidase. It has been researched extensively for its capabilities as an alternative antibiotic as it is harmless to human body cells while being cytotoxic to bacteria. The exact processes for making hypothiocyanite have been patented as such an effective antimicrobial has many commercial applications.\n\nLactoperoxidase-catalysed reactions yield short-lived intermediary oxidation products of SCN, providing antibacterial activity.\n\nThe major intermediary oxidation product is hypothiocyanite OSCN, which is produced in an amount of about 1 mole per mole of hydrogen peroxide. At the pH optimum of 5.3, the OSCN is in equilibrium with HOSCN. The uncharged HOSCN is considered to be the greater bactericidal of the two forms. At pH 7, it was evaluated that HOSCN represents 2% compare to OSCN 98%.\n\nThe action of OSCN against bacteria is reported to be caused by sulfhydryl (SH) oxidation.\n\nThe oxidation of -SH groups in the bacterial cytoplasmic membrane results in loss of the ability to transport glucose and also in leaking of potassium ions, amino acids and peptide.\n\nOSCN has also been identified as an antimicrobial agent in milk, saliva, tears, and mucus.\n\nOSCN is considered as a safe product as it is not mutagenic.\n\nInitially, this particular lactoperoxidase-catalyzed compound was originally discovered while viewing the specific environment of cystic fibrosis patients' weakened respiratory immune system against bacterial infection.\n\nSymptoms of cystic fibrosis include an inability to secrete sufficient quantities of SCN which results in a shortage of necessary hypothiocyanite, resulting in increasing mucous viscosity, inflammation and bacterial infection in the respiratory tract.\n\nLactoferrin with hypothiocyanite has been granted orphan drug status by the EMEA and the FDA.\n\nNaturally, the discovery correlated with studies exploring different methods seeking to further gain alternative antibiotics, understanding that most older antibiotics are decreasing in effectiveness against bacteria with antibiotic resistance.\n\nOSCN, which is not an antibiotic, has proved efficacy on superbugs including MRSA reference strains, BCC, Mucoid PA\n\nSchema of LPO/SCN/HO in human lung\n\nNon exhaustive list of microorganisms.\n\nBacteria (Gram-positive and -negative)\n\n\nViruses\n\n\nYeasts and moulds\n\n\n\n"}
{"id": "11700256", "url": "https://en.wikipedia.org/wiki?curid=11700256", "title": "Kirishi Power Station", "text": "Kirishi Power Station\n\nKirishi Power Station (Kirishskya GRES) is a thermal power station (GRES) at the town of Kirishi, Kirishsky District, Leningrad Oblast, Russia. The power plant is located adjacent to a larger Kirishi oil refinery. Installed electrical capacity of the power station reached 2,595 MW after completion of modernization program for unit 6 in 2011, which included installation of two gas turbines for this unit to utilize combined cycle with total increase of capacity 500 MW and efficiency 20%. The heating capacity is 1,234 Gcal/h.\n\nKirishskya GRES has two tall flue gas stacks, they belong to the tallest chimneys at Russia.\n"}
{"id": "3261205", "url": "https://en.wikipedia.org/wiki?curid=3261205", "title": "Landspout", "text": "Landspout\n\nA landspout is a term coined by meteorologist Howard B. Bluestein in 1985 for a kind of tornado not associated with a mesocyclone. The \"Glossary of Meteorology\" defines a landspout as \n\nLandspouts are a type of tornado which forms during the growth stage of a cumulus congestus cloud by stretching boundary layer vorticity upward and into the cumulus congestus's updraft. They generally are smaller and weaker than supercell tornadoes and do not form from a mesocyclone or pre-existing rotation in the cloud. Because of this, landspouts are rarely detected by Doppler weather radar.\n\nLandspouts share a strong resemblance and development process to that of waterspouts, usually taking the form of a translucent and highly laminar helical tube. Landspouts are considered tornadoes since a rotating column of air is in contact with both the surface and a cumuliform cloud. Not all landspouts are visible, and many are first sighted as debris swirling at the surface before eventually filling in with condensation and dust. \n\nLandspouts are often mistaken for dust devils and gustnadoes.\n\nA few landspouts can persist in excess of 15 minutes and have produced F3 damage; however, they are most commonly weak, with winds under 85 miles per hour (138 km/h) and causing little damage.\n\n\n"}
{"id": "42398094", "url": "https://en.wikipedia.org/wiki?curid=42398094", "title": "Lanyu Power Plant", "text": "Lanyu Power Plant\n\nThe Lanyu Power Plant () is a fuel-fired power plant in Yuren Village, Orchid Island, Taitung County, Taiwan. It is the only power plant on Orchid Island.\n\n"}
{"id": "16881256", "url": "https://en.wikipedia.org/wiki?curid=16881256", "title": "Lindemann index", "text": "Lindemann index\n\nThe Lindemann index is a simple measure of thermally driven disorder in atoms or molecules. The local Lindemann index is defined as:\n\nformula_1\n\nWhere angle brackets indicate a time average. The global Lindemann index is a system average of this quantity.\n\n\n\nCare must be taken if the molecule possesses globally defined dynamics, such as about a hinge or pivot, because these motions will obscure the local motions which the Lindemann index is designed to quantify. An appropriate tactic in this circumstance is to sum the r only over a small number of neighbouring atoms to arrive at each q. A further variety of such modifications to the Lindemann index are available and have different merits, e.g. for the study of glassy vs crystalline materials.\n"}
{"id": "31495887", "url": "https://en.wikipedia.org/wiki?curid=31495887", "title": "National Renewable Energy Action Plan", "text": "National Renewable Energy Action Plan\n\nA National Renewable Energy Action Plan (NREAP) is a detailed report submitted by countries outlining commitments and initiatives to develop renewable energy that all member states of the European Union were obliged to notify to the European Commission by 30 June 2010. The plan provides a detailed road map of how the member state expects to reach its legally binding 2020 target for the share of renewable energy in their total energy consumption, as required by article 4 of the Renewable Energy Directive 2009/28/EC. In the plan, the member state sets out sectoral targets, the technology mix they expect to use, the trajectory they will follow, and the measures and reforms they will undertake to overcome the barriers to developing renewable energy.\n\nEach NREAP report provides details of the expected share of energy provided by renewable sources up to and including 2020. The overall target for EU countries is to use 20% of their energy use from renewable energy sources although targets for each country vary considerably. In addition targets are broken down further by energy use sector including transport, electricity and the heating and cooling sectors.\n\nThe overall EU target for renewable energy use is 20% by the year 2020. Targets for renewable energy in each country vary from a minimum of 10% in Malta to 72% of total energy use in Iceland.\n\nEach NREAP report provides a roadmap to development of renewable energy as well as details of the expected share of energy provided by renewable sources up to and including 2020. The overall target for EU countries is to use 20% of their energy use from renewable energy sources although targets within each country may vary considerably. In addition targets are broken down further by energy use sector including transport, electricity and the heating and cooling sectors.\n\n\nIn addition to NREAP and the associated progress reports, each country provides energy efficiency reports which provides further details of how each country will meet its energy efficiency objectives. These reports are submitted every three years with details of achievements on targets reported on an annual basis.\n\n\nThe European Commission evaluated the National Action Plans, assessing their completeness and credibility. The Member States must adopt and publish, initially every five years, a report setting the indicative Member State targets for future RES-E consumption for the following ten years and showing what measures have or are to be taken to meet those targets. , ten of the member states were on track to surpass the national goal, and 12 others will meet their target. So far, only five participating member states have not met their goals. Iceland and Norway have also submitted NREAP reports outlining their 2020 targets.\n\nCountries are also obliged to submit country progress reports every two years detailing the progress they have made towards meeting their long term action plans. These provide a detailed breakdown of the development of renewable energy by sector and represents actual results as opposed to targets contained in the action plans. The reports provide useful information on country achievements in renewable energy use by sector. In addition the European Commission can compile an overview of the overall EU wide target and report on progress as in its 2015 report.\n\n\n\n"}
{"id": "39707104", "url": "https://en.wikipedia.org/wiki?curid=39707104", "title": "Northwest Woodworkers Gallery", "text": "Northwest Woodworkers Gallery\n\nNorthwest Woodworkers Gallery (formerly Northwest Gallery of Fine Woodworking) in downtown Seattle, is the oldest and largest woodworking cooperative in the United States. Started in 1980 in the Pioneer Square neighborhood by a small group of studio furniture craftsmen, the gallery has grown and fostered the resurgence of the Northwest Crafts movement. The co-op includes notable master woodworkers such as Evert Sodergren and Stewart Wurtz. Northwest Woodworkers gallery moved to the 2111 1st Ave in the Belltown neighborhood in 2012. The gallery currently represents more than 200 woodworkers, including 27 members.\n\n"}
{"id": "4721894", "url": "https://en.wikipedia.org/wiki?curid=4721894", "title": "OK-550 reactor", "text": "OK-550 reactor\n\nThe OK-550 reactor is the nuclear fission reactor used to power three of the seven boats of the Soviet Navy's Project 705 Лира (Lira or Alfa in NATO designation) fourth generation submarines. It is a liquid metal cooled reactor (LMR), using highly enriched uranium-235 fuel to produce 155 MWt of power.\n\n\"OK-550\" has three separate steam circulation loops, and was used in the boats built at Severodvinsk.\n\nThe reactor was developed by OKBM.\n\n\n"}
{"id": "38854540", "url": "https://en.wikipedia.org/wiki?curid=38854540", "title": "PMUT", "text": "PMUT\n\nPiezoelectric Micromachined Ultrasonic Transducers (PMUT) are MEMS-based piezoelectric ultrasonic transducers. Unlike bulk piezoelectric transducers which use the thickness-mode motion of a plate of piezoelectric ceramic such as PZT or single-crystal PMN-PT, PMUT are based on the flexural motion of a thin membrane coupled with a thin piezoelectric film, such as PVDF. In comparison with bulk piezoelectric ultrasound transducers, PMUT can offer advantages such as increased bandwidth, flexible geometries, natural acoustic impedance match with water, reduced voltage requirements, mixing of different resonant frequencies and potential for integration with supporting electronic circuits especially for miniaturized high frequency applications.\n"}
{"id": "718348", "url": "https://en.wikipedia.org/wiki?curid=718348", "title": "Paper marbling", "text": "Paper marbling\n\nPaper marbling is a method of aqueous surface design, which can produce patterns similar to smooth marble or other kinds of stone. The patterns are the result of color floated on either plain water or a viscous solution known as size, and then carefully transferred to an absorbent surface, such as paper or fabric. Through several centuries, people have applied marbled materials to a variety of surfaces. It is often employed as a writing surface for calligraphy, and especially book covers and endpapers in bookbinding and stationery. Part of its appeal is that each print is a unique monotype.\n\nThere are several methods for making marbled papers. A shallow tray is filled with water, and various kinds of ink or paint colors are carefully applied to the surface with an ink brush. Various additives or surfactant chemicals are used to help float the colors. A drop of \"negative\" color made of plain water with the addition of surfactant is used to drive the drop of color into a ring. The process is repeated until the surface of the water is covered with concentric rings.\n\nThe floating colors are then carefully manipulated either by blowing on them directly or through a straw, fanning the colors, or carefully using a human hair to stir the colors. In the 19th century, Tokutaro Yagi, the Kyoto master of Japanese marbling (\"suminagashi\"), developed a method that uses a split piece of bamboo to gently stir the colors, resulting in concentric spiral designs. A sheet of \"washi\" paper is then carefully laid onto the water surface to capture the floating design. The paper, which is often made of \"kozo\" (paper mulberry), must be unsized and strong enough to withstand being immersed in water without tearing.\n\nAnother method of marbling more familiar to Europeans and Americans is made on the surface of a viscous mucilage, known as \"size\" or \"sizing\" in English. This method is commonly referred to as \"Turkish\" marbling and is called \"ebru\" in modern Turkish, although ethnic Turkic peoples were not the only practitioners of the art, as Persian Tajiks and people of Indian origin also made these papers. The term \"Turkish\" was most likely used as a reference to the fact that many Europeans first encountered the art in Istanbul.\n\nHistoric forms of marbling used both organic and inorganic pigments mixed with water for colors, and sizes were traditionally made from gum tragacanth (\"Astragalus\" spp.), gum karaya, guar gum, fenugreek (\"Trigonella foenum-graecum\"), fleabane, linseed, and psyllium. Since the late 19th century, a boiled extract of the carrageenan-rich alga known as Irish moss (\"Chondrus crispus\"), has been employed for sizing. Today, many marblers use powdered carrageenan extracted from various seaweeds. Another plant-derived mucilage is made from sodium alginate. In recent years, a synthetic size made from hydroxypropyl methylcellulose, a common ingredient in instant wallpaper paste, is often used as a size for floating acrylic and oil paints.\n\nIn the size-based method, colors made from pigments are mixed with a surfactant such as ox gall. Sometimes, oil or turpentine may be added to a color, to achieve special effects. The colors are then spattered or dropped onto the size, one color after another, until there is a dense pattern of several colors. Straw from the broom corn was used to make a kind of whisk for sprinkling the paint, or horsehair to create a kind of drop-brush. Each successive layer of pigment spreads slightly less than the last, and the colors may require additional surfactant to float and uniformly expand. Once the colors are laid down, various tools and implements such as rakes, combs and styluses are often used in a series of movements to create more intricate designs.\n\nPaper or cloth is often mordanted beforehand with aluminium sulfate (alum) and gently laid onto the floating colors (although methods such as Turkish \"ebru\" and Japanese \"suminagashi\" do not require mordanting). The colors are thereby transferred and adhered to the surface of the paper or material. The paper or material is then carefully lifted off the size, and hung up to dry. Some marblers gently drag the paper over a rod to draw off the excess size. If necessary, excess bleeding colors and sizing can be rinsed off, and then the paper or fabric is allowed to dry. After the print is made, any color residues remaining on the size are carefully skimmed off of the surface, in order to clear it before starting a new pattern.\n\nContemporary marblers employ a variety of modern materials, some in place of or in combination with the more traditional ones. A wide variety of colors are used today in place of the historic pigment colors. Plastic broom straw can be used instead of broom corn, as well as bamboo sticks, plastic pipettes, and eye droppers to drop the colors on the surface of the size. Ox gall is still commonly used as a surfactant for watercolors and gouache, but synthetic surfactants are used in conjunction with acrylic, PVA, and oil-based paints.\n\nAn intriguing reference which some think may be a form of marbling is found in a compilation completed in 986 CE entitled 文房四谱 (\"Wen Fang Si Pu\") or \"Four Treasures of the Scholar's Study\" edited by the 10th century scholar-official 蘇易簡 Su Yijian (957–995 CE). This compilation contains information on inkstick, inkstone, ink brush, and paper in China, which are collectively called the four treasures of the study. The text mentions a kind of decorative paper called 流沙箋 \"liu sha jian\" meaning “drifting-sand” or “flowing-sand notepaper\" that was made in what is now the region of Sichuan (Su 4: 7a–8a).\n\nThis paper was made by dragging a piece of paper through a fermented flour paste mixed with various colors, creating a free and irregular design. A second type was made with a paste prepared from honey locust pods, mixed with croton oil, and thinned with water. Presumably both black and colored inks were employed. Ginger, possibly in the form of an oil or extract, was used to disperse the colors, or “scatter” them, according to the interpretation given by T.H. Tsien. The colors were said to gather together when a hair-brush was beaten over the design, as dandruff particles was applied to the design by beating a hairbrush over top. The finished designs, which were thought to resemble human figures, clouds, or flying birds, were then transferred to the surface of a sheet of paper. An example of paper decorated with floating ink has never been found in China. Whether or not the above methods employed floating colors remains to be determined (Tsien 94–5).\n\nSu Yijian was an Imperial scholar-official and served as the chief of the Hanlin Academy from about 985–993 CE. He compiled the work from a wide variety of earlier sources, and was familiar with the subject, given his profession. Yet it is important to note that it is uncertain how personally acquainted he was with the various methods for making decorative papers that he compiled. He most likely reported information given to him, without having a full understanding of the methods used. His original source may have predated him by several centuries. Until the original sources that he quotes are more precisely determined, can it be possible to ascribe a firm date for the production of the papers mentioned by Su Yijian.\n\n\"Suminagashi\" (墨流し), which means \"floating ink\" in Japanese, is a Japanese variant; the oldest example appears in the 12th-century Sanjuurokuninshuu (三十六人集), located in Nishihonganji (西本願寺), Kyoto. Author Einen Miura states that the oldest reference to \"suminagashi\" papers are in the waka poems of Shigeharu, (825–880 CE), a son of the famed Heian era poet Narihira (Muira 14). Various claims have been made regarding the origins of \"suminagashi\". Some think that may have derived from an early form of ink divination. Another theory is that the process may have derived from a form of popular entertainment at the time, in which a freshly painted sumi painting was immersed into water, and the ink slowly dispersed from the paper and rose to the surface, forming curious designs.\n\nOne individual has often been claimed as the inventor of suminagashi. According to legend, Jizemon Hiroba felt he was divinely inspired to make suminagashi paper after he offered spiritual devotions at the Kasuga Shrine in Nara Prefecture. It is said that he then wandered the country looking for the best water with which to make his papers. He arrived in Echizen, Fukui Prefecture where he found the water especially conducive to making \"suminagashi\". So he settled there, and his family carried on with the tradition to this day. The Hiroba Family claims to have made this form of marbled paper since 1151 CE for 55 generations (Narita, 14).\n\nIn the 15th century the method of floating colors on the surface of mucilaginous sizing is thought to have emerged in Central Asia. It is believed to have appeared during the end of the Islamic Timurid Dynasty, whose final capital was in the city of Herat, located in Afghanistan today. Other sources suggest it emerged during the subsequent Shaybanid dynasty, in the cities of Samarqand or Bukhara, in what is now modern Uzbekistan. Whether or not this method was somehow related to earlier Chinese or Japanese methods mentioned above has never been concretely proven.\n\nThis Iranian method came to be known as \" kâghaz-e abrî\" (كاغذ ابرى), although often the simplified form of \"abrî\" (ابرى), is also found in several historic texts. This was translated by the late scholar Dr. Annemarie Schimmel to mean \"clouded paper\" in Persian. Certain Turkish writers have suggested that the word may be of Turkish origin related to the word \"abreh\" ابره meaning \"colorful\" or \"variegated\", though this specific term has never been concretely proven to have been used in relation to the art. It may have been the case that both Persian and Turkish meanings were simultaneously understood by artisans, many of which were conversant in both languages at that time, and even enjoyed as an expression of poetic nuance. Most historical Persian and Turkish texts known that refer to this kind of paper use\nthe word \"abrî\" alone. Today in Iran it is often called \"abr-o-bâd\" (ابرو باد), meaning \"cloud and wind\".\n\nThe art developed in Safavid Persia and Ottoman Turkey, as well as Mughal and the Deccan Sultanates in India. Within these regions, various methods emerged in which colors were made to float on the surface of a bath of viscous liquid mucilage or \"size\", made from various plants. These include \"katheera\" or \"kitre\"- gum tragacanth (\"Astragalus\" often used as a binder by apothecaries in making tablets), \"shambalîleh\" or \"methi\"- fenugreek seed (an ingredient in curry mixtures), and \"sahlab\" or \"salep\" (the roots of \"Orchis mascula\", which is commonly used to make a popular beverage). A method of manipulating colors evolved that employed various tools including rakes, combs, and other apparatus, utilized in a series of movements, resulted in incredibly elaborate, intricate, and mesmerizing designs. In India, the \"abri\" technique was eventually combined with \"'aks\", which are various methods of resist or stencils, to create unique and very rare form of miniature painting. These are commonly associated with the Deccan region today, and especially the city of Bijapur in particular, under Adil Shahi dynasty patronage in the 17th century. The topic of marbling in India is understudied and conclusive determinations have yet to be made, especially in light of discoveries made in the last 20 years.\n\nIn Turkey, the art is widely known as \"ebru\" today, and continues to be very popular. The usage of this term appears in the late 19th century. The earliest examples of Ottoman Ebru are thought to be a copy of the \"Hâlnâmah\" حالنامه by the poet Arifi, popularly known as the \"Guy-i Çevgan\" or \"Ball and Polo-stick\". The text of this manuscript was rendered in a delicate cut paper découpage calligraphy by Mehmed bin Gazanfer and completed in 1540, and features many marbled and decorative paper borders. One early master by the name of Shebek is mention posthumously in the earliest Ottoman text on the art known as the \"Tertib-i Risâle-i Ebrî\" (ترطیبِ رسالۀ ابری), which is dated based on internal evidence to after 1615. Several recipes in the text are accredited to this master. Another famous 18th-century master by the name of Hatip\nMehmed Effendi (died 1773) is accredited with developing motif and perhaps early floral designs, although evidence from India appears to contradict some of these claims. Despite this, marbled motifs are commonly referred to as \"Hatip\" designs today in Turkey.\n\nThe current Turkish tradition of ebru dates to the mid 19th century, with a series of masters associated with a branch of the Naqshbandi Sufi order based at what is known as the Özbekler Tekkesi, located in Sultantepe, near Üsküdar. The founder of this line is accredited to Sadık Effendi (died 1846). It is said that he learned the art in Bukhara and taught it to his sons Edhem and Salıh. Based upon this, many Turkish marblers have stated that the art was perpetuated by Sufis for centuries, although evidence for this claim has never been concretely established. \"Hezarfen\" Edhem Effendi (died 1904) is attributed with developing the art as a kind of cottage industry for the tekke, to supply Istanbul's burgeoning printing industry with the decorative paper. It is said that the papers were tied into bundles and sold by weight. Many of these papers were of the \"neftli\" design, made with turpentine, an equivalent to what is called \"stormont\" in English.\n\nThe premier student of Edhem Effendi was Necmeddin Okyay (1885–1976). He was the first to teach the art at the Fine Arts Academy in Istanbul. He is famous for the development of floral styles of marbling, in addition to \"yazılı ebru\" a method of writing traditional calligraphy using a gum-resist method in conjunction with ebru. Okyay's premier student was Mustafa Düzgünman (1920–1990), the teacher of many contemporary marblers in Turkey today. He is known for codifying the traditional repertoire of patterns, to which he only added a floral daisy design, in the manner of his teacher.\n\nIn the 17th century European travelers to the Middle East collected examples of these papers and bound them into \"alba amicorum\", which literally means \"books of friendship\" in Latin, and is a forerunner of the modern autograph album. Eventually the technique for making the papers reached Europe, where they became a popular covering material not only for book covers and end-papers, but also for lining chests, drawers, and bookshelves. The marbling of the edges of books was also a European adaptation of the art.\n\nThe methods of marbling attracted the curiosity of early scientists during the Renaissance. While the earliest published account was written in German by Daniel Schwenter, it wasn't published in his \"Delicæ Physico-Mathematicæ\" until 1671 (Wolfe, 16). A brief description of the art by Athanasius Kircher, published in \"Ars Magna Lucis et Umbræ\" in Rome in 1646, rapidly spread throughout Europe. (ibid) A thorough overview of the art with illustrations of marblers at work, and images of the tools of the trade was published in the \"Encyclopédie\" of Denis Diderot and Jean le Rond d'Alembert.\n\nThe art became a popular handicraft in the 19th century after the English maker Charles Woolnough published his \"The Art of Marbling\" (1853). In it, he describes how he adapted a method of marbling onto book-cloth, which he exhibited at the Crystal Palace Exhibition in 1851. (Wolfe, 79) Further developments in the art were made by Josef Halfer, a bookbinder of German origin, who lived in Budakeszi, in Hungary. Halfer discovered a method for preserving carrageenan, and his methods superseded earlier ones in Europe and the US.\n\nMarbled paper is still made today, and the method is now applied to fabric and three-dimensional surfaces, as well as paper. Aside from continued traditional applications, artists now explore using the method as a kind of painting technique, and as an element in collage. In the last two decades, marbling has been the subject of international symposia and museum exhibitions. The first International Marblers' Gathering was held in Santa Fe NM in 1989 sponsored by the marbling journal Ink & Gall. Active international groups can be found on social media networks such as Facebook and Yahoo! Groups, as well as sites like the International Marbling Network.\n\nMarbling has been adapted for temporary decoration of hands and faces for events such as festivals. Neon or ultraviolet reactive colours are typically used, and the paint is water-based and non-toxic.\n\n\n\n\n"}
{"id": "19481861", "url": "https://en.wikipedia.org/wiki?curid=19481861", "title": "Paul Palmer (physicist)", "text": "Paul Palmer (physicist)\n\nE. Paul Palmer (1926-2011) is a retired Brigham Young University physicist who specialized in geophysics. He coined the term \"cold fusion\". However he was an early critic of Fleischmann and Pons's claims to have developed a useful method of cold fusion.\n\nPalmer served in the US Navy during World War II. He later served as an LDS Missionary in the East Central States Mission (primarily Tennessee and Kentucky). He received his bachelor's degree in physics from the University of Utah.\n\n"}
{"id": "1930909", "url": "https://en.wikipedia.org/wiki?curid=1930909", "title": "Phosphide", "text": "Phosphide\n\nIn chemistry, a phosphide is a compound containing the P ion or its equivalent. Many different phosphides are known, with widely differing structures. Most commonly encountered on the binary phosphides, i.e. those materials consisting only of phosphorus and a less electronegative element. Numerous are polyphosphides, which are solids consisting of anionic chains or clusters of phosphorus. Phosphides are known with the majority of less electronegative elements with the exception of Hg, Pb, Sb, Bi, Te, and Po. Finally, some phosphides are molecular.\n\nExamples of group 1 include . Notable examples include aluminium phosphide , zinc phosphide , calcium phosphide exploiting their tendency to release toxic phosphine upon hydrolysis. Magnesium phosphide (MgP) also is moisture sensitive. Indium phosphide (InP) and GaP are used as a semi-conductors, often in combination of related arsenides. Copper phosphide (CuP) illustrates a rare stoichiometry for a phosphide. These species are insoluble in all solvents - they are 3-dimensional solid state polymers. For those with electropositive metals, the materials hydrolyze:\nPolyphosphides contain P-P bonds. The simplest polyphosphides contain ions;. Others contain the cluster ions and polymeric chain anions (e.g. the helical ion) and complex sheet or 3-D anions. The range of structures is extensive. Potassium has nine phosphides: KP, KP, KP, KP, KP, KP, KP, KP, KP. Eight mono- and polyphosphides of nickel also exist: (NiP, NiP, NiP, NiP, NiP, NiP, NiP, NiP).\n\nTwo polyphosphide ions, found in and found in KP, are radical anions with an odd number of valence electrons making both compounds paramagnetic.\n\nThere are many ways to prepare phosphide compounds. One common way involves heating a metal and red phosphorus (P) under inert atmospheric conditions or vacuum. In principle, all metal phosphides and polyphosphides can be synthesized from elemental phosphorus and the respective metal element in stoichiometric forms. However, the synthesis is complicated due to several problems. The exothermic reactions are often explosive due to local overheating. Oxidized metals, or even just an oxidized layer on the exterior of the metal, causes extreme and unacceptably high temperatures for beginning phosphorination. Hydrothermal reactions to generate nickel phosphides have produced pure and well crystallized nickel phosphide compounds, NiP and NiP. These compounds were synthesized through a solid-liquid reaction between NiCl∙12HO and red phosphorus at 200 °C for 24 and 48 hours, respectively.\n\nMetal phosphides are also produced by reaction of tris(trimethylsilyl)phosphine with metal halides. In this method, the halide is liberated as the volatile trimethylsilyl chloride.\n\nCompounds with triple bonds between a metal and phosphorus are rare. The main examples have the formula Mo(P)(NR), where R is a bulky organic substituent.\n\nMany organophosphides are known. Common examples have the formular RPM where R is an organic substituent and M is a metal. One example is lithium diphenylphosphide. The Zintl cluster is obtained with diverse alkali metal derivatives.\n\nThe mineral Schreibersite (Fe,Ni)P is common in some meteorites.\n"}
{"id": "55297477", "url": "https://en.wikipedia.org/wiki?curid=55297477", "title": "Phosphirenium ion", "text": "Phosphirenium ion\n\nPhosphirenium ions () are a series of organophosphorus compounds containing unsaturated three-membered ring phosphorus(V) heterocycles and σ*-aromaticity is believed to be present in such molecules. Many of the salts containing phosphirenium ions have been isolated and characterized by NMR spectroscopy and X-ray crystallography.\n\nThe first series of phosphirenium ions were synthesized by reacting alkynes with methyl- or phenylphosphonous dichloride and aluminum trichloride. These reactions may be regarded as formal addition of \"RClP\" to alkynes.\n[2+1]-cycloaddition reactions between phosphaalkynes and chlorocarbene give phosphirenes, which serve as starting materials for the generation of phosphirenium species.\nTreatment of diphenylphosphine oxide with diphenylacetylene affords phosphirenium species.\n\nPhosphirenium ions can also be obtained from reaction between phosphiranes and alkynes, where \"RClP\" is formally transferred from alkenes to alkynes.\n\nIn the literature, P NMR spectra of phosphirenium ions show upfield shifts (−57.3 ppm when R = R = Y = CH, Y = Cl). Large coupling constants \"J\" are also found in H NMR, and are comparable to those found in cyclopropenium ions.\n\nThe first phosphirenium ion characterized by X-ray Crystallography has the following structural formula:\n\nIn the refined crystal structure, average phosphorus–cyclic carbon distance has been found to be 1.731(12) Å, roughly corresponding to a bond order of 1.5. For comparison, typical single- and double-bond P–C distances are 1.86 Å and 1.68 Å, respectively.\n\nReminiscent of π-ligand exchange in coordination compounds, a phosphirenium ion may undergo alkyne exchange with other alkynes to give a mixture of phospinirenium species in equilibrium. Kinetically, elimination of alkyne from the cation is suggested to be rate-determining step.\nIn addition, the three-membered ring of phosphirenium ion may be broken. Successive reactions with suitable nucleophile are able to proceed on the electrophilic phosphorus atom. With the presence of an alkyne:\nWith the presence of water or alcohol:\n\nElectrophilic B(CF) readily reacts with phosphinylalkynes at room temperature to give phosphirenium-borate zwitterions as intermediates, which then generate carbon-phosphorus σ bond activation products at higher temperature. The products are of interest to material science. Dotted line in product indicates week interaction between boron and phosphor atoms (see frustrated Lewis pair).\nQualitative molecular orbital (MO) diagram of a phophirenium ion can be obtained by linear combination of orbitals from a fragment and a bent RC=CR fragment. A low-lying σ* orbital from the former with ungerade symmetry interacts with both π and π* orbitals of the latter, creating a 2π-Hückel system, analogous to the one in cyclopropenium ion. This effect has been named as σ*-aromaticity. It is noteworthy that unlike the case of cyclopropenium ion, interaction between the filled σ orbital of the fragment and π orbitals also leads to some degree of antiaromatic character. Therefore, net 3-center conjugative effect is a combination of both σ* stabilizing contribution and σ destabilizing contribution.\n\nElectronegativity of each substituent on phosphorus plays a role as more electron-donating ones give greater degrees of antiaromatic sigma destabilization. This has been confirmed by Natural Population Analysis (NPA), where the energy changes of the reactions below were calculated with interactions between the C–C double bond and phosphorus both turned on and off by manipulating Fock matrix elements: \nDestabilization energies were the differences between corresponding reactions:\nThis series is in accordance with the trend of electronegativity of the ligand atoms.\n\nNatural bond orbital (NBO) analysis provides possible Lewis structures of a molecule and has been carried out to assess the structure of . Similar to the aromatic cyclopropenium ion, the phosphorus analog shows a resonance between the structure with carbon-carbon double bond (1, 72.02%) and the ones with carbon-phosphorus double bond (3a and 3b, 7.88% combined). In addition, the ring-opening forms 2a and 2b combined also occupy 9.08% weight.\n"}
{"id": "238646", "url": "https://en.wikipedia.org/wiki?curid=238646", "title": "Protostar", "text": "Protostar\n\nA protostar is a very young star that is still gathering mass from its parent molecular cloud. The protostellar phase is the earliest one in the process of stellar evolution. For a one solar-mass star it lasts about 1,000,000 years. The phase begins when a molecular cloud first collapses under the force of self-gravity. It ends when the protostar blows back the infalling gas and is revealed as an optically visible pre-main-sequence star, which later contracts to become a main sequence star.\nThe modern picture of protostars, summarized above, was first suggested by Chushiro Hayashi in 1966. In the first models, the size of protostars was greatly overestimated. Subsequent numerical calculations clarified the issue, and showed that protostars are only modestly larger than main-sequence stars of the same mass. This basic theoretical result has been confirmed by observations, which find that the largest pre-main-sequence stars are also of modest size.\n\nStar formation begins in relatively small molecular clouds called dense cores. Each dense core is initially in balance between self-gravity, which tends to compress the object, and both gas pressure and magnetic pressure, which tend to inflate it. As the dense core accrues mass from its larger, surrounding cloud, self-gravity begins to overwhelm pressure, and collapse begins. Theoretical modeling of an idealized spherical cloud initially supported only by gas pressure indicates that the collapse process spreads from the inside toward the outside. Spectroscopic observations of dense cores that do not yet contain stars indicate that contraction indeed occurs. So far, however, the predicted outward spread of the collapse region has not been observed.\n\nThe gas that collapses toward the center of the dense core first builds up a low-mass protostar, and then a protoplanetary disk orbiting the object. As the collapse continues, an increasing amount of gas impacts the disk rather than the star, a consequence of angular momentum conservation. Exactly how material in the disk spirals inward onto the protostar is not yet understood, despite a great deal of theoretical effort. This problem is illustrative of the larger issue of accretion disk theory, which plays a role in much of astrophysics.\n\nRegardless of the details, the outer surface of a protostar consists at least partially of shocked gas that has fallen from the inner edge of the disk. The surface is thus very different from the relatively quiescent photosphere of a pre-main sequence or main-sequence star. Within its deep interior, the protostar has lower temperature than an ordinary star. At its center, hydrogen-1 is not yet fusing with itself. Theory predicts, however, that the hydrogen isotope deuterium fuses with hydrogen-1, creating helium-3. The heat from this fusion reaction tends to inflate the protostar, and thereby helps determine the size of the youngest observed pre-main-sequence stars.\n\nThe energy generated from ordinary stars comes from the nuclear fusion occurring at their centers. Protostars also generate energy, but it comes from the radiation liberated at the shocks on its surface and on the surface of its surrounding disk. The radiation thus created must traverse the interstellar dust in the surrounding dense core. The dust absorbs all impinging photons and reradiates them at longer wavelengths. Consequently, a protostar is not detectable at optical wavelengths, and cannot be placed in the Hertzsprung-Russell diagram, unlike the more evolved pre-main-sequence stars.\n\nThe actual radiation emanating from a protostar is predicted to be in the infrared and millimeter regimes. Point-like sources of such long-wavelength radiation are commonly seen in regions that are obscured by molecular clouds. It is commonly believed that those conventionally labeled as Class 0 or Class I sources are protostars. However, there is still no definitive evidence for this identification.\n\n\n"}
{"id": "54114085", "url": "https://en.wikipedia.org/wiki?curid=54114085", "title": "QuattroVelo", "text": "QuattroVelo\n\nThe QuattroVelo is a commercial-production velomobile which is notable for using both four wheels and also a highly aerodynamic fairing. The QuattroVelo is manufactured by Velomobiel.nl in the Netherlands, with the first production models delivered in 2016.\n\nVelomobiles are a class of faired cycles. One \"speed\" class of velomobiles emphasizes fairing aerodynamics for high speed. Most velomobiles in this class use three wheels, and so it is instructive to compare the four-wheel QuattroVelo against three-wheel designs.\n\n\"Speed\" velomobiles (compared to \"cargo\" velomobiles) are limited mainly by aerodynamics and rolling drag. Thus, \"low aerodynamic drag\" has long been a focus for such designs.\n\nThree-wheel designs tend to have better aerodynamics than four-wheel designs:\n\nBeyond aerodynamics, three wheels instead of four has other advantages:\n\nThree wheels have at least three significant disadvantages compared to four:\n\nFor the above reasons, most \"aerodynamic\"-class velomobiles have used a tadpole tricycle configuration, while most four-wheel velomobiles have been used for \"transportation\"- and \"cargo\"-class velomobiles.\n\nIn 2009, Miles Kingsbury was racing a tadpole tricycle velomobile, and often lost to a competitor riding a streamliner (which Kingsbury had designed and built). Kingsbury concluded one reason he lost to the streamliner was limited cornering stability of his tricycle. He calculated that it was plausible to build a 4-wheel velomobile which would both have the needed stability and also have good enough aerodynamics to be competitive. He then designed and built a 4-wheel velomobile, which he called the Quattro \n\nKingsbury finished just in time to ship it to the USA to participate in ROAM, a group of several dozen velomobie riders riding from coast to coast. Several ROAM riders were impressed that Kingsbury in his Quattro had good cornering stability; was able to keep pace with riders with \"aerodynaic\"-class velomobiles; and had much larger stowage volume -- an issue of practical value for velomobiles used for everyday transportation.\n\nThis led several velomobile designers to re-consider 4 wheels for aerodynamic velomobiles -- despite the inherent drag losses of 4 wheels compared to three. Velomobiles.nl claims for a given width, four wheels offer 40% higher cornering stability than a tadpole tricycle of similar width and center of gravity. Or, a tricycle would need to be 40% wider to get the same stability; but since air drag is directly related to frontal area, making the tricycle wide would directly hurt aerodynamics.\n\nAs of 2017/05 there are several projects by various makers to introduce \"aerodynamic\"-class 4-wheel velomobiels; the QuattroVelo is of special interest because (as of 2017/05) tens of units are in the hands of private owners and so can be used for independent testing.\n\nThe QuattroVelo may be compared against other \"aerodynamic\"-class velombiles. As of 2017, some of the fastest tadpole velomobiles are the DF, Milan, and Quest. The Quest is made by Velomobiels.nl, but the others are not. Production of the QuattroVelo is (2017/05) tens of units, limiting the volume of independent tests, but several riders have both a QuattroVelo and another \"aerodynamic\"-class velomobie; and report in public forums QuattroVelo speed is nearly as fast as several \"best in class\" three-wheel velomobiles such as the DF and Milan.\n\nAnother comparison is cargo capacity. One practical use is commuting and/or shopping. A tricycle with a single rear wheel typically has two small compartments, one on either side of the rear wheel. A quadracycle can have one large stowage area between the rear wheels. A single large stowage area is often more useful, as it can hold cargo which cannot be easily divided among several small compartments.\n\nA humorous competition demonstrated that a Quattrovelo can hold 26 six-packs of beer in glass bottles Although the competition was humorous, it demonstrates the practical stowage capacity of the Quattrovelo is both much larger than most \"speed\" velos, and also large enough for many daily uses. Another use is carrying a passenger. The Quattrovelo is offered standard with a second seat for a small (non-pedaling) rider.\n\nMost 3-wheel velomobiles drive (power) the single rear wheel, and a four-wheel velomobile with two driven wheels may have better drive traction. One reason is that many tricycles put about 1/3 of total weight on the drive wheel, while many quads put about 1/2 weight on the two drive wheels. Further, the Quattrovelo uses two ratchets, one to drive each rear wheel; thus, if one drive wheel loses traction, the other one continues to drive. Note this is different than driving via a simple differential, in which the loss of traction on one wheel causes the other wheel to stop driving. Several Quattrovelo riders are also riders of 3-wheel velomobiles and report online the Quattrovelo has much better traction and climbing on loose or slippery surfaces. There are two- and even three-wheel drive tricycles; so improved traction is not unique to the Quattrovelo, but it may be unique among \"speed\"-class production velomobiles.\n\nAn alternative to \"more/four wheels\" is a longer and/or wider three-wheel velomobile. A wider profile hurts aerodynamics, but smoother air-flow can compensate to a degree. At the same time, CFRP is stiff and strong enough that the weight penalty for \"larger three-wheeler\" is comparable to the added weight for a \"smaller four-wheeler\". As of 2017/05, a main example of a good-aerodynamics three-wheel velomobile with large cargo capacity is the Milan 4.2 (\"for two\"). The Milan 4.2 is not directly comparable to the Quattrovelo, as the 4.2 has enough stowage for an adult passenger (though no passenger drivetrain). The weight and aerodynamics of the Milan 4.2 are thus broadly similar to the Quattrovelo; and stability depends significantly on cargo loading and weight distribution.\n"}
{"id": "14489981", "url": "https://en.wikipedia.org/wiki?curid=14489981", "title": "Sarah James", "text": "Sarah James\n\nSarah Agnes James (born 1946) is a native Gwich'in from Arctic Village, Alaska, USA, and a board member of the \"International Indian Treaty Council\". She was awarded the Goldman Environmental Prize in 2002, together with Jonathon Solomon and Norma Kassi. They received the prize for their struggles for protection of the Arctic National Wildlife Refuge (ANWR) from plans of oil exploration and drilling. Oil and gas exploration would disturb the life cycle of the Porcupine caribou, which has been a foundation for the Gwich'in culture for 20,000 years.\n\nIn the 1990s James visited communities in South American countries (Brazil, Ecuador, Nicaragua, and Guatemala), speaking for the underprivileged. She also appeared on television programs (CNN, MacNeil-Lehrer, CBS). And she travelled to Washington, trying to clear up concepts that they believe petroleum companies misrepresent, and speaking for preservation of the Arctic National Wildlife Refuge.\n"}
{"id": "57849268", "url": "https://en.wikipedia.org/wiki?curid=57849268", "title": "Silent Hunter (laser weapon)", "text": "Silent Hunter (laser weapon)\n\nThe Silent Hunter is an anti-drone laser weapon developed in China by Poly Technologies. It is an improved version of the 30 kilowatt Low-Altitude Laser Defending System (LASS) and is available in both fixed and mobile versions. \n\nThe Silent Hunter uses an electrically powered fiber optic laser and according to one Poly official, has a maximum power that is between 30 and 100 kilowatts and a maximum range of four kilometers. Although it is primarily designed to search, track, and destroy low-flying drones, it is powerful enough to \"ablate\" or penetrate five 2 millimeter steel plates at a range of 800 meters or a single 5 millimeter steel plate at 1000 meters. The sheer bulk of the Silent Hunter prevents its use on an aerial platform. \n\nA Poly official claimed that the Silent Hunter was used to safe guard the September 2016 G-20 Summit in Hangzhou, China.\n\nThe Silent Hunter was first unveiled at the 2017 IDEX show in Abu Dhabi. It was again showcased at the International Exhibition of Weapons Systems and Military Equipment (KADEX) in Kazakhstan in 2018.\n\n"}
{"id": "5341778", "url": "https://en.wikipedia.org/wiki?curid=5341778", "title": "Steam rocket", "text": "Steam rocket\n\nA steam rocket (also known as a hot water rocket) is a thermal rocket that uses water held in a pressure vessel at a high temperature, such that its saturated vapor pressure is significantly greater than ambient pressure. The water is allowed to escape as steam through a rocket nozzle to produce thrust.\n\nSteam rockets are usually pressure fed, but more complex designs using solar energy or nuclear energy have been proposed. They are probably best known for their use in rocket-powered cars and motorcycles, and they are the type used in aeolipile.\n\nWater, while under pressure, is heated up to a high temperature (approx. 250-500 °C). As the hot water goes through the nozzle (usually a de Laval nozzle) and the pressure reduces, the water flashes to steam pressing on the nozzle, and leaving at high speed. By the recoil the rocket accelerates in the opposite direction to the steam. The nozzle of hot water rockets must be able to withstand high pressure, high temperatures and the particularly corrosive nature of hot water.\n\nThe simplest design has a pressurised water tank where the water is heated before launch, however, this gives a very low exhaust velocity since the high latent heat of vapourisation means that very little actual steam is produced and the exhaust consists mostly of water, or if high temperatures and pressures are used, then the tank is very heavy.\n\nMore complex designs can involve passing the water through pumps and heat exchangers and employing nuclear reactors or solar heating, it is estimated that these can give a specific impulse of over 195 s I, which is still well below the standards of more complex designs, for example the 465 s of the hydrogen-oxygen Vinci engine.\n\n\nSolar or nuclear heated steam thermal rockets have been proposed for use in interplanetary travel. Although the performance is low, high mass fractions are easy to achieve, and water is expected to be very easy to extract and purify from ice deposits that are found around the solar system.\n\n\nSteam rocket have occasionally appeared in science fiction stories, especially steampunk.\n\n"}
{"id": "10103346", "url": "https://en.wikipedia.org/wiki?curid=10103346", "title": "Stone of Tizoc", "text": "Stone of Tizoc\n\nThe Stone of Tizoc, Tizoc Stone or Sacrificial Stone is a large, round, carved Aztec stone. It is thought to have been a \"cuauhxicalli\" or possibly a \"temalacatl\". Richard Townsend maintains, however, that the stone was hollowed in the 16th century for unknown purposes.\n\nThe stone was rediscovered on 17 December 1791 when construction was being done in downtown Mexico City. The workmen had been breaking up monuments that were found and using them as cobblestone. A churchman named Gamboa happened to be passing by and saved the stone from the same result. The stone was then moved to the cemetery of the nearby Cathedral where it stayed until 1824, when it was moved to the University. The stone is currently in the National Museum of Anthropology in Mexico City.\n\nThe monolith is made of basalt and measures 93 cm tall with a diameter of 2.65 meters and a circumference of 8.31 meters.\n\nThe lateral side of the stone depicts 15 separate scenes of a repeated scene of a costumed warrior having their hair grabbed by another warrior. Presumably the first figure, the warrior with the largest headress is identified by the glyph of Tizoc and wears the headdress of the deity Huitzilopotchli, the revered god of war.\n\nIn each scene the warrior being grabbed has an identifying location glyph. Each of the warriors grabbing the other are identified with the 'smoking foot' motif as well as the symbol of the smoking mirror in their headdress both icons associating them with the deity Tezcatlipoca.\n\nAlong the bottom of the rim are glyphs representing Tlaltecuhtli at each of the cardinal points, in between these glyphs are rows of \"tecpatl\", sacrificial knives. Along the top of the rim are eyes and circular pieces of jade, symbols of Venus and the stars.\n\nThe top of the stone has a sun diadem, with large triangles corresponding with the cardinal directions while smaller rays point in the inter-cardinal directions.\n\nThe stone also features a large divot from the center to the edge of the sculpture. This divot is believed to have been done after the creation of the stone due to the rough and asymmetrical nature of the cut.\n\nThe act of grabbing another's hair has long been recognized as a symbol of defeat or conquering in Mesoamerica, as such the stone is interpreted to represent the conquest of other locations by the Mexica. The main interpretation is that the stone is a propaganda piece for Tizoc, the Aztec Emperor from 1481 to 1486; a rather weak emperor who did little to expand the empire.\n\nAztec glyphs above each conquered soldier give the name of the original site which may have already been conquered. The toponyms are written in a mixture of logographic and syllabic signs. The stone also depicts the stars at the top rim, emphasizing the heavens; while the icons at the bottom edge represent the earth. Combined with the solar iconography on the top, this associates Aztec conquering and rule with the divine.\n\nWhile Tizoc is the only identifiable conqueror, each subsequent Mexica warrior shares the same 'smoking foot' motif to link them together. Some historians take this to mean that Tizoc is attempting to link his only large military conquest, depicted as the first scene, to the conquests of previous rulers. The fact that there are fifteen scenes could be related to the 15 lords of the 15 Mexica city-states, emphasizing the political and military divisions of the Mexica emperor.\n\nRichard Townsend argues that the relief may function as a symbolic manifestation of the Aztec empires tribute system. In relation to Mexica tradition, conquered tribes or cities were expected to send sacrificial offerings to the victor. Tizoc, head of the Aztec empire at the time, would therefore be the one collecting these tributes. The stone acts as acknowledgement of such a transaction.\n\nAs a \"temalacatl,\" the stone may have been a used for mock battles between a group of warriors and a victim who was tied to the stone and given a feathery club while the warriors had sharp \"macuahuitl\". It is argued however, that \"temalactal\" are often described of as being flat on the top and the central hole lacks any kind of bar with which to tie a victim. More likely however, the stone was used as a \"cuauhxicalli\", within the center of which the hearts of sacrificial victims were placed. It is argued that central hole likely had a face (like the Calendar Stone) which was later mutilated.\n\n"}
{"id": "309536", "url": "https://en.wikipedia.org/wiki?curid=309536", "title": "Strähle construction", "text": "Strähle construction\n\nSträhle's construction is a geometric method for determining the lengths for a series of vibrating strings with uniform diameters and tensions to sound pitches in a specific rational tempered musical tuning. It was first published in the 1743 \"Proceedings of the Royal Swedish Academy of Sciences\" by Swedish master organ maker Daniel Stråhle (1700–1746). The Academy's secretary Jacob Faggot appended a miscalculated set of pitches to the article, and these figures were reproduced by Friedrich Wilhelm Marpurg in \"Versuch über die musikalische Temperatur\" in 1776. Several German textbooks published about 1800 reported that the mistake was first identified by Christlieb Benedikt Funk in 1779, but the construction itself appears to have received little notice until the middle of the twentieth century when tuning theorist J. Murray Barbour presented it as a good method for approximating equal temperament and similar exponentials of small roots, and generalized its underlying mathematical principles.\n\nIt has become known as a device for building fretted musical instruments through articles by mathematicians Ian Stewart and Isaac Jacob Schoenberg, and is praised by them as a unique and remarkably elegant solution developed by an unschooled craftsman.\n\nThe name \"Strähle\" used in recent English language works appears to be due to a transcription error in Marpurg's text, where the old-fashioned diacritic raised \"e\" was substituted for the raised ring.\n\nDaniel P. Stråhle was active as an organ builder in central Sweden in the second quarter of the eighteenth century. He had worked as a journeyman for the important Stockholm organ builder Johan Niclas Cahman, and in 1741, four years after Cahman's death, Stråhle was granted his privilege for organ making. According to the system in force in Sweden at the time a privilege, a granted monopoly which was held by only a few of the most established makers of each type of musical instruments, gave him the legal right to build and repair organs, as well as to train and examine workers, and it also served as a guarantee of the quality of the work and education of the maker. An organ by him from 1743 is preserved in its original condition at the chapel at Strömsholm Palace; he is also known to have made clavichords, and a notable example with an unusual string scale and construction signed by him and dated 1738 is owned by the Stockholm Music Museum. His apprentices included his nephew Petter Stråhle and Jonas Gren, partners in the famous Stockholm organ builders Gren & Stråhle, and according to Abraham Abrahamsson Hülphers in his book \"Historisk Afhandling om Musik och Instrumenter\" published in 1773, Stråhle himself had studied mechanics (which has been assumed to have included mathematics) with Swedish Academy of Science founding member Christopher Polhem. He died in 1746 at Lövstabruk in northern Uppland.\n\nStråhle published his construction as a \"new invention, to determine the \"Temperament\" in tuning, for the pitches of the clavichord and similar instruments\" in an article that appeared in the fourth volume of the proceedings of the newly formed Royal Swedish Academy of Sciences, which included articles by prominent scholars and Academy members Polhem, Carl Linnaeus, Carl Fredrik Mennander, Augustin Ehrensvärd, and Samuel Klingenstierna. According to organologist Eva Helenius musical tuning was a subject of intense debate in the Academy during the 1740s, and though Stråhle himself was not a member his was the third article on practical musical topics published by the Academy—the first two were by amateur musical instrument maker, minister, and Academy member Nils Brelin which related inventions applicable to harpsichords and clavichords.\n\nStråhle wrote in his article that he had developed the method with \"some thought and a great number of attempts\" for the purpose of creating a gauge for the lengths of the strings in the temperament which he described as that which made the tempering (\"sväfningar\") mildest for the ear, as well comprising as the most useful and even arrangement of the pitches. His instructions produce an irregular tuning with a range of tempered intervals similar to better known tunings published during the same period, but he provided no further comments or description about the tuning itself; today it is generally considered to be an approximation of equal temperament. He also did not elaborate upon any advantages of his construction, which can produce accurate and repeatable results without calculations or measurement with only a straightedge and dividers; he described the construction in only five steps, and it is less iterative than arithmetic methods described by Dom Bédos de Celles method for determining organ pipe lengths in just intonation or Vincenzo Galilei for determining string fret positions in approximate equal temperament, and geometrical methods such as those described by Gioseffo Zarlino and Marin Mersenne—all of which are much better known than Stråhle's. Stråhle concluded by stating that he had applied the system to a clavichord, although the tuning as well as the method of determining a set of sounding lengths can be used for many other musical instruments, but there is little evidence showing whether it was put into more widespread practice other than the two examples described in the article, and whose whereabouts today are unknown.\n\nStråhle instructed first to draw a line segment \"QR\" of a convenient length divided in twelve equal parts, with points labeled I through XIII. \"QR\" is then used as the base of an isosceles triangle with sides \"OQ\" and \"OR\" twice as long as \"QR\", and rays drawn from vertex \"O\" through each of the numbered points on the base. Finally a line is drawn from vertex \"R\" at an angle through a point \"P\" on the opposite leg of the triangle seven units from \"Q\" to a point \"M\", located at twice the distance from \"R\" as \"P\". The length of \"MR\" gives the length of the lowest sounding pitch, and the length of \"MP\" the highest of the string lengths generated by the construction, and the sounding lengths between them are determined by the distances from \"M\" to the intersections of \"MR\" with lines \"O I\" through \"O XII\", at points labeled 1 through 12.\n\nStråhle wrote that he had named the line \"PR\" \"Linea Musica\", which Helenius noted was a term Polhem had used in an undated but earlier manuscript now located at the Linköping Stifts- och Landsbibliotek and which is accompanied by notes from composer and geometer Harald Vallerius (1646–1716) and Stråhle's former employer J. N. Cahman.\n\nStråhle also showed line segments parallel to \"MR\" through points \"NHS\", \"LYT\", and \"KZV\" in order to illustrate how once created the construction could be scaled to accommodate different starting pitches.\n\nStråhle stated at the conclusion of the article that he had implemented the string scale in the highest three octaves of a clavichord, although it is unclear whether this section would have been strung all with the same gauge wire under equal tension like the monochord which he wrote it resembled, and whose construction he described in more detail. He only described an indirect method of setting its tuning, however, requiring that he first establish reference pitches by transferring the corresponding string lengths to the movable bridges on a keyed thirteen string monochord whose open strings had been previously tuned in unison.\n\nThe article following Stråhle's was a mathematical treatment of it by Jacob Faggot (1699–1777), then secretary of the Academy of Sciences and future director of the Surveying Office, who in the same volume also contributed articles on a weight measure for lye and methods for calculating the volume of barrels. Faggot was one of the first members of the Academy, and had also been member of a special commission on weights and measures. He apparently was not a musician, though Helenius described he was interested in musical topics from a mathematical perspective and documented that he periodically came in contact with musical instrument makers through the Academy. Helenius also presented a theory that Faggot had a more active, if indirect and posthumous influence on the construction of musical instruments in Sweden, claiming that he may have suggested the long tenor strings used in two experimental instruments built by Johan Broman in 1756 which she proposed influenced the type of clavichord built in Sweden in the late eighteenth and early nineteenth centuries.\n\nIn his analysis of Stråhle's article Faggot outlined the trigonometric steps he had used to calculate the sounding lengths of the individual pitches, for the purpose of comparing the new tuning produced by Stråhle's method, against a tuning with pure thirds, fourths and fifths (labeled \"N.1.\" in the table), and equal temperament, which he called only \"an older temperament and [which] is introduced in Mr. Mattheson's \"Critica Musica\"\" (\"N.2.\"), He intended the resulting set of figures to show whether \"the tuning of the pitches, following the previously described invention, satisfies the ear with pleasant sounds and with better evenness, in the \"Musical\" pitches on a keyboard instrument, and therefore teaches understanding better can judge than the old and previously known manner of tuning, when the eye can see what the ear hears.\" \n\nBoth articles were reproduced in a German edition of the Academy's proceedings published in 1751, and a table of Faggot's calculated string lengths was subsequently included by Marpurg on his 1776 \"Versuch über die musikalische Temperatur\", who wrote that he accepted their accuracy but that rather than accomplishing \"Strähle\"'s stated goal, the tuning represented an unequal temperament \"not even of the tolerable type.\"\n\nThe sounding lengths calculated by Faggot are substantially different from what would be produced according to Stråhle's instructions, a fact which appears to have been first published by in \"Dissertatio de Sono et Tono\" in 1779, and the tuning he created includes intervals tuned outside of the range conventionally used in Western art music. Funk is credited the observation of this discrepancy in Gehler's \"Physikalisches Wörterbuch\" in 1791, and Fischer's \"Physikalisches Wörterbuch\" in 1804, and the error was pointed out by Ernst Chladni in \"Die Akustik\" in 1830. No similar comments appear to have been published in Sweden during the same period.\n\nThese works report Faggot's mistake as the result of having used a value from the tangent instead of the sine column from the logarithmic tables. The error itself consisted of making the angle of \"RP\" about seven degrees too great, which caused the effective length of \"QP\" to increase to 8.605. This greatly exaggerated the errors of the temperament compared to the tunings he presented alongside of it, although it is not clear whether Faggot observed these apparent defects as he made no further comments about Stråhle's construction or temperament in the article.\n\nThe tuning produced following Stråhle's instructions is a rational temperament with a range of fifths from 696 to 704 cents, which is from about one cent flatter than a meantone fifth to two cents sharp of just 3:2; the range of major thirds is from 396 cents to 404 cents, or ten cents sharp of just 5/4 to three cents flat of Pythagorean 81/64. These intervals fall within what is considered to have been acceptable but there is no distribution of better thirds to more frequently used keys that characterize what are today the most popular of the tunings published in the seventeenth and eighteenth centuries, which are known as \"well temperaments\". The best fifth is pure in the key of F♯—or the pitch given by \"MB\"—which has a 398 cent third, and the best third is in the key E, which has a 697 cent fifth; the best combination of the two intervals is in the key of F and the worst combination is in the key of B♭.\n\nJ. Murray Barbour brought new attention to Stråhle's construction along with Faggot's treatment of it in the 20th century. Introduced in the context of Marpurg, he included an overview of it alongside the more famous methods of determining string lengths in his 1951 book \"Tuning and Temperament\" where he characterized the tuning as an \"approximation for equal temperament\". He also demonstrated how close Stråhle's construction was to the best approximation the method could provide, which reduces the maximum errors in major thirds and fifths by about half a cent and is accomplished by substituting 7.028 for the length of \"QP\".\n\nBarbour presented a more complete analysis of the construction in \"A Geometrical Approximation to the Roots of Numbers\" published six years later in \"American Mathematical Monthly\". He reviewed Faggot's error and its consequences, and then derived Stråhle's construction algebraically using similar triangles. This takes the generalized form\n\nformula_1\n\nUsing the values from Stråhle's instructions this becomes\n\nformula_2\n\nLetting formula_3 so that formula_4 leads to a form of the first formula that is more useful for calculation\n\nformula_5\n\nBarbour then described a generalized construction using the easily obtained mean proportional for the length of \"MB\" that avoids most of the specific angles and lengths required in the original. For musical applications it is simpler and its results are slightly more uniform than Stråhle's, and it has the advantage of producing the desired string lengths without additional scaling.\n\nHe instructed to first draw the line \"MR\" corresponding to the larger of the two numbers with \"MP\" the smaller, and to construct their mean proportional at \"MB\". The line that will carry the divisions is drawn from \"R\" at any acute angle to \"MR\", and perpendicular to it a line is drawn through \"B\", which intersects the line to be divided at \"A\", and \"RA\" is extended to \"Q\" such that \"RA\"=\"AQ\". A line is drawn from \"Q\" through \"P\", intersecting the line through \"BA\" at \"O\", and a line drawn from \"O\" to \"R\". The construction is completed by dividing \"QR\" and drawing rays from \"O\" through each of the divisions.\n\nBarbour concluded with a discussion of the pattern and magnitude of the errors produced by the generalized construction when used to approximate exponentials of different roots, stating that his method \"is simple and works exceedingly well for small numbers\". For roots from 1 to 2 the error is less than 0.13%—about 2 cents when \"N\"=2— with maxima around \"m\"=0.21 and \"m\"=0.79. The error curve appears roughly sinusoidal and for this range of \"N\" can be approximated by about 99% by fitting the curve obtained for \"N\"=1, formula_6. The error increases rapidly for larger roots, for which Barbour considered the method inappropriate; the error curve resembles the form formula_7 with maxima moving closer to \"m\"= 0 and \"m\"=1 as \"N\" increases.\n\nThe paper was published with two notes added by its referee, Isaac Jacob Schoenberg. He observed that the formula derived by Barbour was a fractional linear transformation and so called for a perspectivity, and that since three pairs of corresponding points on the two lines uniquely determined a projective correspondence Barbour's condition that \"OA\" be perpendicular to \"QR\" was irrelevant. The omission of this step allows a more convenient selection of length for \"QR\", and reduces the number of operations.\n\nSchoenberg also noted that Barbour's equation could be viewed as an interpolation of the exponential curve through the three points \"m\"=0, \"m\"=1/2 and \"m\"=1, which he expanded upon in a short paper titled \"On the Location of the Frets on the Guitar\" published in \"American Mathematical Monthly\" in 1976. This article concluded with a brief discussion of Stråhle's fortuitous use of formula_8 for the half-octave, which is one of the convergents of the continued fraction expansion of the formula_9, and the best rational approximation of it for the size of the denominator.\n\nThe use of fractional approximations of formula_9 in Stråhle's construction was expanded upon by Ian Stewart, who wrote about the construction in \"A Well Tempered Calculator\" in his 1992 book \"Another Fine Math You've Got Me Into...\" as well as \"Faggot's Fretful Fiasco\" included in \"Music and Mathematics\" published in 2006. Stewart considered the construction from the standpoint of projective geometry, and derived the same formulas as Barbour by treating it from the start as a fractional linear function, of the form formula_11, and he pointed out that the approximation for formula_9 implicit in the construction is formula_13, which is the next lower convergent from the half octave it produces. This is the consequence of the function simplifying to formula_14 for \"m\"=0.5 where formula_15 is the generating approximation.\n\nThe geometric and arithmetic methods for dividing monochords as well as musical instrument fretboards compiled by Barbour were for the stated purpose of illustrating the different tunings each represents or implies, and Schoenberg's and Stewart's works retained similar focus and references. Three textbooks on piano building that are not included by them show similar constructions to Stråhle's for designing new instruments but treat the tuning of their pitches independently; both constructions employ a non-perpendicular form as suggested by Schoenberg's observation in Barbour's \"A Geometrical approximation to the Roots of Numbers\", and one achieves optimal results while the other demonstrates an application with a root other than 2.\n\nCarl Kützing, an organ and piano maker in Bern during the middle of the 19th century wrote in his first book on piano design, \"Theoretisch-praktisches Handbuch der Fortepiano-Baukunst\" from 1833, that he devised a simple method of determining the sounding lengths in an octave after reading of the different geometric constructions described in an issue of Marpurg's \"Historisch-kritischen Beitragen zur Aufnahme der Musik\"; he stated that the divisions would be very accurate and that the construction could be used for fretting guitars.\n\nKützing introduced the construction following a description of a large sector to be made for the same purpose. He did not include either method in \"Das Wissenschaftliche der Fortepiano-Baukunst\" published eleven years later, where he calculated lengths using approximately 18:35 ratios between octave lengths and proposed a new method with a non-continuous curve adjusted for actual wire diameters in order to reduce tonal differences from jumps in tension.\n\nKützing instructed to extend a line segment \"bc\"—representing a known sounding length—at 45 degrees to the line ba, and from its octave at point \"d\" located midway between \"b\" and \"c\", to extend a line perpendicular to \"ba\" intersecting it at \"e\", then to divide \"de\" into 12 equal parts. The point \"a\" on \"ab\" is located by transferring the lengths of \"de\", \"db\", from \"e\" away from \"b\", and rays extended from \"a\" through the points dividing \"de\" and intersecting \"bc\" to locate the different endpoints of the string lengths from \"c\".\n\nThis arrangement is equivalent to using the mean proportional to locate \"a\".\n\nA re-labeled diagram with instructions was included in a pamphlet printed by England's largest piano manufacturers John Broadwood & Sons to accompany their display at the 1862 International Exhibition in London, where they described it as \"a practical method of finding the lengths of Strings, for every note of the Octave on equal temperament; so that with wire of the same size the tension on each note shall be the same.\" \n\nIt was also reproduced, alongside a sector, by Giacomo Sievers, a Russian-born piano maker working in Naples, in his 1868 book \"Il Pianoforte\", where he claimed it was the best practical method for determining sounding lengths of strings in a piano. Like Broadwood, Sievers did not describe its source or extent of its use, and did not explain any theory behind it. He also did not suggest it had any use beyond designing pianos.\n\nEnglish piano maker Samuel Wolfenden presented a construction for determining all but the lowest sounding plain string lengths in a piano in \"A Treatise on the Art of Pianoforte Construction\" published in 1916; like Sievers, he did not explain whether this was an original procedure or one in common use, commenting only that it was \"a very practical method of determining string lengths, and in past years I used it altogether\". He added that at the time of writing he found calculating the lengths directly \"somewhat easier\" and had preceded the description with a table of computed lengths for the top five octaves of a piano. He included frequencies in equal temperament, but only published aural tuning instructions in his 1927 supplement.\n\nWolfenden explicitly advocated equalizing the tension of the plain strings which he proposed to accomplish in the upper range by combining a 9:17 ratio between octave lengths with a uniform change in string diameters (achieving slightly more consistent results over the otherwise similar system published by Siegfried Hansing in 1888), in contrast to Sievers scale whose stringing schedule results in higher tension for the thicker, lower sounding pitches.\n\nLike Sievers, Wolfenden constructed all of the sounding lengths on a single segment at 45 degrees from the base lines for the rays, starting with points located for each C in the range designed at 54, 102, 192.5, 364 and 688mm from the upper point. The four vertices for the rays are then located by the intersections of the horizontal base lines extended from the lower C in each octave with a second line angled from the upper starting point for the string line, however, which he specified should both be at 51.5 degrees to the base lines and that the base lines have a 35:13 ratio with the difference between the two octave lengths.\n\nWolfenden's method approximates formula_16 with roughly 1.3775, and is equivalent to formula_17 in Barbour's form. Compensating for its smaller octaves this produces 596 cent half octaves, an error of about 1mm at note F4 (f′) compared with his calculated figures.\n\n\n\"Enligit detta påfund, har jag bygt et \"Monochordium\", i så måtto, at det fullan hafver 13 strängar, ock skulle dy snarare heta \"Tredekachordium\", men som alla strängarna, äro af en \"nummer\", längd ock thon ; så behåller jag det gamla namnet.\n\n\"Til dessa tretton strängar, är lämpadt et vanligit \"Manual\", af en \"Octave\"; men under hvar sträng, sedan de noga äro stämde i \"unison\", sätter jag löfa stallar, å de \"puncter\", ock till de längder fra \"crepinerne\", som min nu beskrefne \"Linea Musica\" det äfkar : derefter hvar sträng undfår sin behöriga thon.\n\n\"Det \"Claver\", som jag här til förfärdigat är jämnväl i de tre högre \"Octaverne\", noga rättadt efter min \"Linea Musica\", til strängarnes längd ock skilnad : ock på det stämningen, må utan besvär, kunna ske ; så är mit \"Monochordium\" så giordt, at det kan ställas ofvan på \"Claveret\", då en \"Octav\" på \"Claveret\" stämmes, thon för thon, mot sina tillhöriga thoner på \"Monochordium\", derefter alla de andra thonerne, å \"Claveret\", stämmas \"Octavs\"-vis ; den stamningen, är ock för örat lättast at värkställa, emedan den bör vara fri för svängningar.\"\n\n"}
{"id": "37063874", "url": "https://en.wikipedia.org/wiki?curid=37063874", "title": "Vizag back-to-back HVDC converter station", "text": "Vizag back-to-back HVDC converter station\n\nThe Vizag back-to-back HVDC station, or Visakhapatnam back-to-back HVDC station, is a back-to-back HVDC connection between the eastern and southern regions in India, located close to the city of Visakhapatnam, and owned by Power Grid Corporation of India.\n\nIt consists of two independent poles, each with a nominal power transmission rating of 500 MW, referred to as Vizag 1 and Vizag 2. \nVizag 1 was built by Alstom between 1996 and 1999 and has nominal DC voltage and current ratings of 205 kV, 2475 A. Its design is very similar to that of the Chandrapur back-to-back HVDC converter station.\n\nVizag 2 was built by ABB between 2002 and 2005 and has nominal DC voltage and current ratings of 176 kV, 2841 A.\n\nBoth Vizag 1 and Vizag 2 use air-insulated, water-cooled thyristor valves.\n\nOn 31 December 2013, the Northern, Eastern and Western grids were synchronised with the Southern regional grid, creating a single synchronous AC grid over the whole of India. As a result, the converter station is no longer required for its original purpose of asynchronously linking the Eastern and Southern grids, although it can still be used as an embedded power flow device to help control power flow within the AC system. The stations could potentially be dismantled and moved to elsewhere to export/import power from other countries. Sometimes the excess power fed to the southern grid by this HVDC link is flowing back to Western region through the 765 KV AC lines between Southern grid and the Western grid which is not desired.\n\n\n"}
{"id": "57000813", "url": "https://en.wikipedia.org/wiki?curid=57000813", "title": "Volkswagen Group MEB platform", "text": "Volkswagen Group MEB platform\n\nThe Volkswagen Group MEB platform () is a modular car platform for electric cars developed by the Volkswagen Group and its subsidiaries. It is used in models of Audi, SEAT, Škoda and Volkswagen. The architecture is aimed to \"consolidate electronic controls and reduce the number of microprocessors, advance the application of new driver-assistance technology and somewhat alter the way cars are built\" by the VW Group.\n\nThe MEB platform is part of a wide strategy to start production of new battery electric vehicles between 2019 and 2025. In 2017, the VW Group announced a gradual transition from combustion engine to battery electric vehicles with all 300 models across 12 brands having an electric version by 2030.\n\nAs of May 2018, the VW Group has committed $48 billion in car battery supplies and plans to outfit 16 factories to build electric cars by the end of 2022. The upcoming Volkswagen-branded production cars will be assembled in VW's Zwickau plant in Germany for the European market from 2020, while two production centers in North America and China are planned to be \"launched at almost the same time\". The Škoda-branded SUV Vision E is to be produced in the Škoda plant Mladá Boleslav, Czech Republic, along with electric motors and electric car batteries.\n\nTwo types of the MEB platform have been developed: one for passenger vehicles and one for utility automobiles that accommodate heavier cargo.\n\nAn unnamed \"small\" SUV.\nFirst fully electric MEB model by SEAT is planned to be produced from 2020 with reported range of 500km.\nTwo unspecified MEB models to be produced before 2025.\n\n\nAudi and Porsche are jointly developing the PPE (Premium Platform Electric) for larger models. It is to be used in next generation of electric cars from 2021 after Porsche's Mission E and Audi's E-tron Quattro SUV, that are planned go into production by 2019 or 2020.\n\n\n"}
{"id": "16048740", "url": "https://en.wikipedia.org/wiki?curid=16048740", "title": "Water supply and sanitation in Pernambuco", "text": "Water supply and sanitation in Pernambuco\n\nWater supply and sanitation in Pernambuco is characterized by high levels of access to water supply in urban areas, but also by poor service quality (intermittent supply), inadequate access to sanitation, and insufficient access to improved water sources and improved sanitation in rural areas.\n\nIn urban areas, where 76% of Pernambuco's 8.4 million inhabitants live, access to house connections for water supply is 91% and access to sewerage is 49%. Concerning service quality, rationing is a constant reality in about 85% of the municipalities, including the Recife Metropolitan Region. This is primarily to a lack of water resources, particularly during the dry season, and frequent drought conditions.\n\nThe State Secretariat of Water Resources (SRH) is in charge of both water resources management and water supply and sanitation policies. The State Water Company (COMPESA) is responsible for water and sanitation service provision in most of the state. COMPESA provides urban water supply and sewerage in 170 and 20 municipalities respectively, out of 189 in the State. The remaining municipalities are served by municipal public providers. COMPESA operates under concession contracts with municipalities and is regulated and supervised by the state-level multi-sector regulatory agency ARPE.\n\nThe city of Recife has pioneered condominial sewers in Brazil.\n\n"}
{"id": "48405028", "url": "https://en.wikipedia.org/wiki?curid=48405028", "title": "Waves4Power", "text": "Waves4Power\n\nWaves4Power is a developer of buoy-based Offshore Wave Energy Converter (OWEC) systems. There are plans to install a demonstration plant in 2015 at the Runde Environmental Centre in Norway. There they will be testing the WaveEL, an offshore buoy. This will be connected via sub-sea cable to the shore based power grid.\n"}
{"id": "19242402", "url": "https://en.wikipedia.org/wiki?curid=19242402", "title": "White Hill Wind Farm", "text": "White Hill Wind Farm\n\nThe White Hill Wind Farm is a wind farm in New Zealand operated by Meridian Energy. It was officially opened in 2007.\n\nIt is located six kilometres south-east of Mossburn in the Southland Region of the South Island. The wind farm covers approximately 24 square kilometers of mainly forestry land.\n\n\n"}
