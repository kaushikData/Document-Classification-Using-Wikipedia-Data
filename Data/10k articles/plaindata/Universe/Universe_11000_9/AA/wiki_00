{"id": "35303279", "url": "https://en.wikipedia.org/wiki?curid=35303279", "title": "1969 Aswan Ilyushin Il-18 crash", "text": "1969 Aswan Ilyushin Il-18 crash\n\nOn 20 March 1969, a United Arab Airlines Ilyushin Il-18 crashed while attempting to land at Aswan Airport. 100 of the 105 passengers and crew on board were killed in the crash.\n\nThe flight was a non-scheduled international passenger service from Jeddah, Saudi Arabia to Aswan, Egypt. The aircraft was carrying home Muslim worshippers who had won a pilgrimage through a lottery. It was dark in the early morning when the flight attempted to land and blowing sand had reduced visibility to 2–3 kilometers. After two unsuccessful attempts to land, the aircraft was making a third try when it banked to the right and hit the left side of the runway. The starboard wing tore off and a fuel spillage followed which caused the crashed aircraft to burst into flames.\n\nThe probable cause was determined to be that the \"Pilot descended below the minimum safe altitude without having the runway lights clearly in sight. A contributory factor was fatigue arising from continuous working hours without suitable rest periods.\"\n\n"}
{"id": "29056235", "url": "https://en.wikipedia.org/wiki?curid=29056235", "title": "2006 European blackout", "text": "2006 European blackout\n\nThe 2006 European blackout was a major blackout which occurred on Saturday, November 4, 2006. More than 15 million clients of the Union for the Co-ordination of Transmission of Electricity (UCTE) did not have access to electricity during about two hours on this date. The continent was in total disarray, dozens of people were trapped in elevators, numerous trains were halted, and the emergency services were receiving an overwhelming amount of calls. The immediate action taken by the Transmission System Operators (TSO) prevented the disturbance from turning into a Europe-wide blackout.\n\nThe cause of this major blackout was a planned routine disconnection of the Ems powerline crossing in Northwest Germany to allow a ship to pass beneath the overhead cables. In September, the shipyard had requested the lines, called Conneforde–Diele red and white, to be shut off starting at 01:00 on 5 November. This change was communicated to the neighboring TSOs and they did simulations to ensure stability. As a result, the planned power flow between TSOs was decreased for 00:00 to 06:00 5 November. On 3 November, the shipyard requested the shut-off to be advanced to 2200 on 4 November. E.ON Netz thought that this would be more favorable and approved the request. However, this change was not communicated to the neighboring TSOs until very late so a full analysis was not done. Also, the transfer capacity had already been sold and it was not possible to change it except for force majeure.\n\nOnce the second circuit was turned off, this caused alarms due to high power flow. Also the Landesbergen–Wehrendorf line was very close to its limit. Over the next half-hour, the power first went down but then it crept back up. E.ON Netz thought that closing a bus tie would decrease this a bit; in fact, it had the opposite effect and once this was performed the line tripped out.\nTwenty eight seconds later, an electrical blackout had cascaded across Europe extending from Poland in the north-east, to the Benelux countries and France in the west, through to Portugal, Spain and Morocco in the south-west, and across to Greece and the Balkans in the south-east.\n\n\nIn total, over a 10 million people in northern Germany, France, Italy, Belgium, and Spain lost power or were affected by the blackout. In northern Germany, more than 100 trains were delayed for more than two hours because of the blackout. Almost all of France was affected except the southeast of the country. In affected areas in France, firefighters were asked to respond to approximately 40 people being stuck in elevators. In Belgium, only the area around Antwerp was seriously affected, as well as Ghent and Liège, leaving the rest of the country with power. Italy, which had experienced a similar blackout in 2003 which left 95% of the country without power, was only affected in a few areas, mainly Piedmont, Liguria in northern Italy, and Puglia in southern Italy. In Spain, the news network Red Electrica was affected, as were the regions of Madrid, Barcelona, Zaragoza, and part of Andalusia.\n\nThe UCTE(Union for the Coordination of the Transmission of Electricity) and TSO(Transmission System Operator) acted swiftly, and were able to restore electricity shortly, however the event highlighted glaring problems. The UCTE and TSO were victims of enormous backlash from the media and the citizens, and there were threats that both corporations might undergo serious managerial overhauls. This forced both companies to go back to the drawing board, and determine what possible improvements could be made to prevent such a problem in the future. For starters, the UCTE strengthened its defense system by using blackout simulations with the help of numerical analysis, and sophisticated technology. Using these simulations, they were able to bring to life realistic scenarios that could impact these regions in the future, and how the TSO could possibly combat the problems. This initiated the tweaking of the N-1 criterion in Policy 3 of the UCTE Operation Handbook. Essentially, the interconnected power systems were decentralized, wherein different border lines were responsible for the power lines running through them, rather than one general body governing the entire system. This process was later known as Resynchronization, and it was able to increase stability within the UCTE if any problems were to arise. Furthermore, \"joint-training\" workshops were established that would assure that regional dispatchers would have the knowledge and skills to operate the power systems, and would have the ability to implement the solutions given by the TSO under any circumstances.\n\nAlthough it may seem an abstract concept, the political system may have played a role in this power outage happening. At the time, many EU policy makers pushed for a more centralised form of government. This would give the government as a whole a greater role in regulating Europe's power grids. When the Blackout of 2006 occurred, these policy makers said this event revealed the fragility of Europe's current power grid system and called on a formal centralised government. On the flip-side, however, because the power disturbances were quickly contained and dealt with, the power sector spokesperson cited this event as a confirmation of the reliability of the current, transnational power grids and praised the decentralised governance model in place at the time.\n\nIt comes as a surprise that, since the 2003 blackout, the security system had gone unchanged. The security system did not account for an increase of liberalization of electric supply which caused an increase in cross-border trades, which are not properly accounted for when reviewing the security of the system. Also, due to the decentralized form of government at the time, the transmission system operator, or TSO, would each control their own area, and exchange little information with other TSOs. This inevitably resulted in a slow response time to contingencies. To repair these fallacies so something like this would not occur in the future, a new mode of coordinated operation for real-time security would be needed. But in order to do so, those implementing this would need to overcome a series of psychology, organizational, and legal challenges. The alternative of this would be to risk yet another major blackout or run the current system very conservatively, which would cause an astronomical cost to the consumers.\n\n\n"}
{"id": "44279876", "url": "https://en.wikipedia.org/wiki?curid=44279876", "title": "ASTRID (reactor)", "text": "ASTRID (reactor)\n\nASTRID (\"Advanced Sodium Technological Reactor for Industrial Demonstration\") is a 600 MW sodium-cooled fast breeder reactor (Generation IV) project, proposed by the Commissariat à l'énergie atomique (CEA). It is proposed to be built on the Marcoule Nuclear Site in France. It is the successor of the three French fast reactors Rapsodie, Phénix and Superphénix. A final decision on construction is to be made in 2019.\n\nThe main goals of ASTRID are the multi-recycling of plutonium, aiming at preserving natural uranium resources, minor actinide transmutation, aiming at reducing nuclear waste, and an enhanced safety comparable to Generation III reactors, such as the EPR. It is envisaged as a 600 MW industrial prototype connected to the grid. A commercial series of 1500 MW SFR reactors is planned to be deployed around 2050.\n\nAs of 2012, the project involves 500 people, with almost half among industrial partners. Those include Électricité de France, Areva, Alstom Power Systems, Comex Nucléaire, Jacobs France, Toshiba and Bouygues Construction.\n\nIn 2014 Japan agreed to cooperate in developing the emergency reactor cooling system, and in a few other areas. As of 2016, France was seeking the full involvement of Japan in ASTRID development.\n\nIf a decision is made to proceed with construction, ASTRID is expected to start operating in the 2030s.\n\n"}
{"id": "5443767", "url": "https://en.wikipedia.org/wiki?curid=5443767", "title": "Aluminium recycling", "text": "Aluminium recycling\n\nAluminium recycling is the process by which scrap aluminium can be reused in products after its initial production. The process involves simply re-melting the metal, which is far less expensive and energy-intensive than creating new aluminium through the electrolysis of aluminium oxide (), which must first be mined from bauxite ore and then refined using the Bayer process. Recycling scrap aluminium requires only 5% of the energy used to make new aluminium from the raw ore. For this reason, approximately 36% of all aluminium produced in the United States comes from old recycled scrap. Used beverage containers are the largest component of processed aluminum scrap, and most of it is manufactured back into aluminium cans.\n\nA common practice since the early 1900s and extensively capitalized during World War II, aluminium recycling is not new. It was, however, a low-profile activity until the late 1960s, when the exploding popularity of aluminium beverage cans finally placed recycling into the public consciousness.\nSources for recycled aluminium include aircraft, automobiles, bicycles, boats, computers, cookware, gutters, siding, wire, and many other products that need a strong lightweight material, or a material with high thermal conductivity. As recycling does not transmute the element, aluminium can be recycled indefinitely and still be used to produce any product for which new aluminium could have been used.\nThe recycling of aluminium generally produces significant cost savings over the production of new aluminium, even when the cost of collection, separation and recycling are taken into account. Over the long term, even larger national savings are made when the reduction in the capital costs associated with landfills, mines, and international shipping of raw aluminium are considered.\n\nRecycling aluminium uses about 5% of the energy required to create aluminium from bauxite; the amount of energy required to convert aluminium oxide into aluminium can be vividly seen when the process is reversed during the combustion of thermite or ammonium perchlorate composite propellant. \n\nAluminium die extrusion is a specific way of getting reusable material from aluminium scraps but does not require a large energy output of a melting process. In 2003, half of the products manufactured with aluminium were sourced from recycled aluminium material.\n\nThe benefit with respect to emissions of carbon dioxide depends on the type of energy used. Electrolysis can be done using electricity from non-fossil-fuel sources, such as nuclear, geothermal, hydroelectric, or solar. Aluminium production is attracted to sources of cheap electricity. Canada, Brazil, Norway, and Venezuela have 61 to 99% hydroelectric power and are major aluminium producers. The use of recycled aluminium also decreases the need for mining bauxite.\n\nThe vast amount of aluminium used means that even small percentage losses are large expenses, so the flow of material is well monitored and accounted for financial reasons. Efficient production and recycling benefits the environment as well.\n\nAluminium beverage cans are usually recycled by the following method:\n\nThe scrap aluminium is separated into a range of categories \"e.g.\" irony aluminium (engine blocks etc.), clean aluminium (alloy wheels). Scraps are classified according to ISRI.\n\nDepending on the specification of the required ingot casting, it will depend on the type of scrap used in the start melt. Generally, the scrap is charged to a reverberatory furnace (other methods appear to be either less economical and/or dangerous) and melted down to form a \"bath\". The molten metal is tested using spectroscopy on a sample taken from the melt to determine what refinements are needed to produce the final casts.\n\nAfter the refinements have been added, the melt may be tested several times to be able to fine-tune the batch to the specific standard.\n\nOnce the correct \"recipe\" of metal is available, the furnace is tapped and poured into ingot moulds, usually via a casting machine. The melt is then left to cool, stacked and sold on as cast silicon–aluminium ingot to various industries for re-use. Mainly, cast alloys like ADC12, LM2, AlSi132, LM24 etc. are produced. These secondary alloys ingots are used in die cast companies.\n\nNowadays, tilting rotary furnaces are used for recycling of aluminum scrap, which give higher recovery compared to reverberatory furnaces (Skelner Furnace).\n\nBrazil recycles 98.2% of its aluminium can production, equivalent to 14.7 billion beverage cans per year, ranking first in the world, more than Japan's 82.5% recovery rate. Brazil has topped the aluminium can recycling charts eight years in a row.\n\nWhite dross, a residue from primary aluminium production and secondary recycling operations, usually classified as waste , still contains useful quantities of aluminium which can be extracted industrially. The process produces aluminium billets, together with a highly complex waste material. This waste is difficult to manage. It reacts with water, releasing a mixture of gases (including, among others, hydrogen, acetylene, and ammonia) which spontaneously ignites on contact with air; contact with damp air results in the release of copious quantities of ammonia gas. Despite these difficulties, however, the waste has found use as a filler in asphalt and concrete.\n\n\n"}
{"id": "2822", "url": "https://en.wikipedia.org/wiki?curid=2822", "title": "Ash", "text": "Ash\n\nAsh or ashes are the solid remains of fires. Specifically, it refers to all non-aqueous, non-gaseous residues that remain after something is burned. In analytical chemistry, in order to analyse the mineral and metal content of chemical samples, ash is the non-gaseous, non-liquid residue after a complete combustion.\n\nAshes as the end product of incomplete combustion will be mostly mineral, but usually still contain an amount of combustible organic or other oxidizable residues. The best-known type of ash is wood ash, as a product of wood combustion in campfires, fireplaces, etc. The darker the wood ashes, the higher the content of remaining charcoal will be due to incomplete combustion.\n\nLike soap, ash is also a disinfecting agent (alkaline). The World Health Organization recommends ash or sand as alternative to soap when it's not available.\n\n\n"}
{"id": "961216", "url": "https://en.wikipedia.org/wiki?curid=961216", "title": "Caning (furniture)", "text": "Caning (furniture)\n\nIn the context of furniture, caning is a method of weaving chair seats and other furniture either while building new chairs or in the process of cane chair repair. In common use, \"cane\" may refer to any plant with a long, thin stem. However, the cane used for furniture is derived from the rattan vine native to Indonesia, the Philippines and Malaysia. The vines typically grow to 100–300 ft in length; most have a diameter less than 1 in. Before export, the rattan stems are cut to uniform lengths and the bark is removed in narrow strips of to  in.\nSugar cane and bamboo (sometimes called \"cane\" in the southern United States) should not be confused with rattan cane.\nRattan vine looks somewhat similar to bamboo but is quite different in that bamboo is hollow and holds itself upright while rattan is a solid flexible vine that needs the support of surrounding structure to elevate itself off the forest floor. It climbs to the top of canopies of the forest to reach sunlight with the help of large rugged thorns that grab hold of surrounding trees. Sometimes much of the length of these rugged vines are draped along the forest floor from tree to tree in search of a suitable structure to climb.\n\nMistakenly some people confuse furniture or chair caning with wicker. To clarify, chair caning is specifically the craft of applying rattan cane or rattan peel to a piece of furniture such as the backs or seats of chairs, whereas wicker or wicker work is a reference to the craft of weaving any number of materials such as willow or rattan reeds as well as man made paper based cords. \n"}
{"id": "189274", "url": "https://en.wikipedia.org/wiki?curid=189274", "title": "Carpentry", "text": "Carpentry\n\nCarpentry is a skilled trade in which the primary work performed is the cutting, shaping and installation of building materials during the construction of buildings, ships, timber bridges, concrete formwork, etc. Carpenters traditionally worked with natural wood and did the rougher work such as framing, but today many other materials are also used and sometimes the finer trades of cabinetmaking and furniture building are considered carpentry. In the United States, 98.5% of carpenters are male, and it was the fourth most male-dominated occupation in the country in 1999. In 2006 in the United States, there were about 1.5 million carpentry positions. Carpenters are usually the first tradesmen on a job and the last to leave. Carpenters normally framed post-and-beam buildings until the end of the 19th century; now this old fashioned carpentry is called timber framing. Carpenters learn this trade by being employed through an apprenticeship training—normally 4 years—and qualify by successfully completing that country's competence test in places such as the United Kingdom, the United States, Australia and South Africa. It is also common that the skill can be learned by gaining work experience other than a formal training program, which may be the case in many places.\n\nThe word \"carpenter\" is the English rendering of the Old French word \"carpentier\" (later, \"charpentier\") which is derived from the Latin \"carpentarius [artifex]\", \"(maker) of a carriage. The Middle English and Scots word (in the sense of \"builder\") was \"wright\" (from the Old English \"wryhta\", cognate with \"work\"), which could be used in compound forms such as \"wheelwright\" or \"boatwright\".\n\nIn the UK, carpentry is more correctly used to describe the skill involved in \"first fixing\" of timber items, such as construction of roofs, floors and timber framed buildings, i.e., those areas of construction that are normally hidden in a finished building. An easy way to envisage this is that first fix work is all that is done before plastering takes place. Second fix is done after plastering takes place. \"Second fix\" work, the construction of items such as skirting boards, architraves, and doors also comes under carpentry. Carpentry is also used to construct the formwork into which concrete is poured during the building of structures such as roads and highway overpasses. In the UK, the skill of making timber formwork for poured, or in situ, concrete, is referred to as \"shuttering\".\n\nCarpentry in the United States is historically defined similarly to the United Kingdom as the \"heavier and stronger\" work distinguished from a joiner \"...who does lighter and more ornamental work than that of a carpenter...\" although the \"...work of a carpenter and joiner are often combined.\" Joiner is less common than the terms \"finish carpenter\" or \"cabinetmaker\". The terms \"housewright\" and \"barnwright\" were used historically, now occasionally used by carpenters who work using traditional methods and materials. Someone who builds custom concrete formwork is a \"form carpenter\".\n\nWood is one of mankind's oldest building materials. The ability to shape wood improved with technological advances from the stone age to the bronze age to the iron age. Some of the oldest archaeological evidence of carpentry are water well casings built using split oak timbers with mortise and tenon and notched corners excavated in eastern Germany dating from about 7,000 years ago in the early neolithic period.\n\nRelatively little information about carpentry is available from pre-history (before written language) or even recent centuries because the knowledge and skills were passed down person to person, rarely in writing, until the printing press was invented in the 15th century and builders began regularly publishing guides and pattern books in the 18th and 19th centuries. The oldest surviving complete architectural text is Vitruvius' ten books collectively titled De architectura, which discuss some carpentry.\n\nSome of the oldest surviving wooden buildings in the world are temples in China such as the Nanchan Temple built in 782, the Greensted Church, parts of which are from the 11th century, and the stave churches in Norway from the 12th and 13th centuries.\n\nBy the 16th century sawmills were coming into use in Europe. The founding of America was partly based on a desire to extract resources from the new continent including wood for use in ships and buildings in Europe. In the 18th century part of the Industrial Revolution was the invention of the steam engine and cut nails. These technologies combined with the invention of the circular saw led to the development of balloon framing which was the beginning of the decline of traditional timber framing.\n\nThe 19th century saw the development of electrical engineering and distribution which allowed the development of hand-held power tools, wire nails and machines to mass-produce screws. In the 20th century, portland cement came into common use and concrete foundations allowed carpenters to do away with heavy timber sills. Also, drywall (plasterboard) came into common use replacing lime plaster on wooden lath. Plywood, engineered lumber and chemically treated lumber also came into use.\n\nFor types of carpentry used in America see American historic carpentry.\n\nCarpentry requires training which involves both acquiring knowledge and physical practice. In formal training a carpenter begins as an \"apprentice\", then becomes a \"journeyman\", and with enough experience and competency can eventually attain the status of a \"master\" carpenter. Today pre-apprenticeship training may be gained through non-union vocational programs such as high school shop classes and community colleges.\n\nInformally a laborer may simply work alongside carpenters for years learning skills by observation and peripheral assistance. While such an individual may obtain journeyman status by paying the union entry fee and obtaining a journeyman's card (which provides the right to work on a union carpentry crew) the carpenter foreman will, by necessity, dismiss any worker who presents the card but does not demonstrate the expected skill level.\n\nCarpenters may work for an employer or be self-employed. No matter what kind of training a carpenter has had, some U. S. states require contractors to be licensed which requires passing a written test and having minimum levels of insurance.\n\nFormal training in the carpentry trade is available in seminars, certificate programs, high-school programs, online classes, in the new construction, restoration, and preservation carpentry fields. Sometimes these programs are called pre-apprenticeship training.\n\nIn the modern British construction industry, carpenters are trained through apprenticeship schemes where general certificates of secondary education (GCSE) in Mathematics, English, and Technology help but are not essential. However, this is deemed the preferred route, as young people can earn and gain field experience whilst training towards a nationally recognized qualification.\n\nThere are two main divisions of training: construction-carpentry and cabinetmaking. During pre-apprenticeship, trainees in each of these divisions spend 30 hours a week for 12 weeks in classrooms and indoor workshops learning mathematics, trade terminology, and skill in the use of hand and power tools. Construction-carpentry trainees also participate in calisthenics to prepare for the physical aspect of the work.\n\nUpon completion of pre-apprenticeship, trainees who have successfully passed the graded curriculum (taught by highly experienced journeyman carpenters) are assigned to a local union and to union carpentry crews at work on construction sites or in cabinet shops as First Year Apprentices. Over the next four years, as they progress in status to Second Year, Third Year, and Fourth Year Apprentice, apprentices periodically return to the training facility every three months for a week of more detailed training in specific aspects of the trade.\n\nTradesmen in countries such as Germany and Australia are required to fulfill a formal apprenticeship (usually three to four years) to work as a professional carpenter. Upon graduation from the apprenticeship, he or she is known as a journeyman carpenter.\n\nUp through the 19th and even the early 20th century, the journeyman traveled to another region of the country to learn the building styles and techniques of that area before (usually) returning home. In modern times, journeymen are not required to travel, and the term now refers to a level of proficiency and skill. Union carpenters in the United States, that is, members of the United Brotherhood of Carpenters and Joiners of America, are required to pass a skills test to be granted official journeyman status, but uncertified professional carpenters may also be known as journeymen based on their skill level, years of experience, or simply because they support themselves in the trade and not due to any certification or formal woodworking education.\n\nProfessional status as a journeyman carpenter in the United States may be obtained in a number of ways. Formal training is acquired in a four-year apprenticeship program administered by the United Brotherhood of Carpenters and Joiners of America, in which journeyman status is obtained after successful completion of twelve weeks of pre-apprenticeship training, followed by four years of on-the-job field training working alongside journeyman carpenters. The Timber Framers Guild also has a formal apprenticeship program for traditional timber framing. Training is also available in groups like the Kim Bồng woodworking village in Vietnam where apprentices live and work to learn woodworking and carpentry skills.\n\nIn Canada, each province sets its own standards for apprenticeship. The average length of time is four years and includes a minimum number of hours of both on-the-job training and technical instruction at a college or other institution. Depending on the number of hours of instruction an apprentice receives, he or she can earn a Certificate of Proficiency, making him or her a journeyman, or a Certificate of Qualification, which allows him or her to practice a more limited amount of carpentry. Canadian carpenters also have the option of acquiring an additional Interprovincial Red Seal that allows them to practice anywhere in Canada. The Red Seal requires the completion of an apprenticeship and an additional examination.\n\nAfter working as a journeyman for a while, a carpenter may go on to study or test as a master carpenter. In some countries, such as Germany and Japan, this is an arduous and expensive process, requiring extensive knowledge (including economic and legal knowledge) and skill to achieve master certification; these countries generally require master status for anyone employing and teaching apprentices in the craft. In others, 'master carpenter' can be a loosely used term to describe any skilled carpenter.\n\nFully trained carpenters and joiners will often move into related trades such as shop fitting, scaffolding, bench joinery, maintenance and system installation.\n\nCarpenters traditionally worked with natural wood which has been prepared by splitting (riving), hewning, or sawing with a pit saw or sawmill called lumber (American English) or timber (British English). Today natural and engineered lumber and many other building materials carpenters may use are typically prepared by others and delivered to the job site. In 2013 the carpenters union in America used the term carpenter for a catch-all position. Tasks performed by union carpenters include installing \"...flooring, windows, doors, interior trim, cabinetry, solid surface, roofing, framing, siding, flooring, insulation, ...acoustical ceilings, computer-access flooring, metal framing, wall partitions, office furniture systems, and both custom or factory-produced materials, ...trim and molding ... ceiling treatments, ... exposed columns and beams, displays, mantels, staircases...metal studs, metal lath, and drywall...\"\n\nCarpentry is often hazardous work. Types of woodworking and carpentry hazards include Machine hazards, flying materials, tool projection, fire and explosion, electrocution, noise, vibration, dust and chemicals.\nIn the United States the Occupational Safety and Health Administration (OSHA) tries to prevent illness, injury and fire through regulations. However, self-employed workers are not covered by the OSHA act. OSHA claims that \"Since 1970, workplace fatalities have been reduced by more than 65 percent and occupational injury and illness rates have declined by 67 percent. At the same time, U.S. employment has almost doubled.\" The leading cause of overall fatalities, called the \"fatal four\", are falls, followed by struck by object, electrocution, and caught-in/between. In general construction \"employers must provide working conditions that are free of known dangers. Keep floors in work areas in a clean and, so far as possible, a dry condition. Select and provide required personal protective equipment at no cost to workers. Train workers about job hazards in a language that they can understand.\" Examples of how to prevent falls includes placing railings and toe-boards at any floor opening which cannot be well covered and elevated platforms and safety harness and lines, safety nets, stair railings and hand rails.\n\nSafety is not just about the workers on the job site. Carpenters work needs to meet the requirements in the Life Safety Code such as in stair building and building codes to promote long term quality and safety for the building occupants.\n\nA finish carpenter (North America), also called a joiner (a traditional name now rare in North America), is one who does finish carpentry, that is, cabinetry, furniture making, fine woodworking, model building, instrument making, parquetry, joinery, or other carpentry where exact joints and minimal margins of error are important. Some large-scale construction may be of an exactitude and artistry that it is classed as finish carpentry.\n\nA carpenter and joiner is one who has a much broader skill ranging from joinery, finishing carpentry, building construction and form work.\n\nA trim carpenter specializes in molding and trim, such as door and window casings, mantels, baseboards, and other types of ornamental work. Cabinet installers may also be referred to as trim carpenters.\n\nA cabinetmaker is a carpenter who does fine and detailed work specializing in the making of cabinets made from wood, wardrobes, dressers, storage chests, and other furniture designed for storage.\n\nA ship's carpenter specializes in shipbuilding, maintenance, repair techniques and carpentry specific to nautical needs in addition to many other on-board tasks; usually the term refers to a carpenter who has a post on a specific ship. Steel warships as well as wooden ones need ship's carpenters, especially for making emergency repairs in the case of battle or storm damage.\n\nA shipwright builds wooden ships on land.\n\nA cooper is someone who makes barrels: wooden staved vessels of a conical form, of greater length than breadth.\n\nA scenic carpenter builds and dismantles temporary scenery and sets in film-making, television, and the theater.\n\nA framer is a carpenter who builds the skeletal structure or wooden framework of buildings, most often in the platform framing method. Historically, balloon framing was used until the 1950s when fire safety concerns made platform framing inherently better. A carpenter who specializes in building with timbers rather than studs is known as a timber framer and does traditional timber framing with wooden joints, including mortise-and-tenon joinery, post and beam work with metal connectors, or pole building framing.\n\nA luthier is someone who makes or repairs stringed instruments. The word luthier comes from the French word for lute, \"luth\".\n\nA log builder builds structures of stacked, horizontal logs including houses, barns, churches, fortifications, and more.\n\nA formwork carpenter creates the shuttering and falsework used in concrete construction.\n\nIn Japanese carpentry, \"daiku\" is the simple term for carpenter, a \"miya-daiku\" (temple carpenter) performs the work of both architect and builder of shrines and temples, and a \"sukiya-daiku\" works on teahouse construction and houses. \"Sashimono-shi\" build furniture and \"tateguya\" do interior finishing work.\n\nA restoration carpenter is a carpenter who works in historic building restoration, someone who restores a structure to a former state.\n\nA conservation carpenter works in architectural conservation, known in the U.S. as a \"preservation carpenter\" who works in historic preservation, someone who keeps structures from changing.\n\nGreen carpentry is the specialization in the use of environmentally friendly, energy-efficient and sustainable sources of building materials for use in construction projects. They also practice building methods that require using less material and material that has the same structural soundness.\n\nRecycled (reclaimed, repurposed) carpentry is carpentry that uses scrap wood and parts of discarded or broken furniture to build new wood products.\n\n\n"}
{"id": "35556420", "url": "https://en.wikipedia.org/wiki?curid=35556420", "title": "Cottbus-Drewitz Solarpark", "text": "Cottbus-Drewitz Solarpark\n\nThe Cottbus-Drewitz Solarpark is a photovoltaic power station, located on a former military airfield. It was completed in 11 weeks using Suntech STP 280 Wp panels.\n\n"}
{"id": "54732036", "url": "https://en.wikipedia.org/wiki?curid=54732036", "title": "EARP Rwanda", "text": "EARP Rwanda\n\nEARP, also known as Electricity Access Rollout Program is a Rwandan Government Program initiated in 2009 to synergise efforts to realize the country's targets for electricity access.\n\nEARP was initiated in 2009 when the Government of Rwanda wanted to bring together all Development partners contribution in the energy sector, especially to increase the access rate in the Rwandan community. At that time, only 6% of Rwandan households were accessing on-grid electricity.\n\nSo far, remarkable strides have been made by this programme under which access to the grid has increased from 364,000 households in June 2012 to more than 850,000 households (35.13 % of the total households in Rwanda) in 2018.\n\nEARP mission goes in line with the Rwandan Government rural electrification policy which has set 100% access target by the year 2024 in Rwanda households and 100% of centers expected to boost the economic development commonly named \"Productive use areas\" by 2022. \n\nAs of September 2018, the cumulative connectivity rate was 48.6% of Rwandan households including 36.5% connected to the national grid and 12.2% accessing through off-grid systems (mainly solar). \n\nDuring the elaboration of the Economic Development targets, the Government of Rwanda took a decision to diversify the sources of electricity from traditional dominant grid to include even off-grid connections. Subsequently, Households far away from the planned national grid coverage have been encouraged to use alternatively cheaper connections such as Mini-grids and Solar Photovoltaics (PVs) to reduce the cost of access to electricity whilst relieving constraints on historical government subsidies.\n\n"}
{"id": "1593775", "url": "https://en.wikipedia.org/wiki?curid=1593775", "title": "Eicosapentaenoic acid", "text": "Eicosapentaenoic acid\n\nEicosapentaenoic acid (EPA; also icosapentaenoic acid) is an omega-3 fatty acid. In physiological literature, it is given the name 20:5(n-3). It also has the trivial name timnodonic acid. In chemical structure, EPA is a carboxylic acid with a 20-carbon chain and five \"cis\" double bonds; the first double bond is located at the third carbon from the omega end.\n\nEPA is a polyunsaturated fatty acid (PUFA) that acts as a precursor for prostaglandin-3 (which inhibits platelet aggregation), thromboxane-3, and leukotriene-5 eicosanoids. EPA is both a precursor and the hydrolytic breakdown product of eicosapentaenoyl ethanolamide (EPEA: CHNO; 20:5,n-3). Studies of fish oil supplements, which contain EPA, have failed to support claims of preventing heart attacks or strokes.\n\nEPA is obtained in the human diet by eating oily fish or fish oil, e.g. cod liver, herring, mackerel, salmon, menhaden and sardine, and various types of edible algae. It is also found in human breast milk.\n\nHowever, fish can either synthesize EPA from fatty acids precursors found in their alimentation or obtain it from the algae they consume. It is available to humans from some non-animal sources (e.g. commercially, from microalgae such as \"Monodus subterraneus\", \"Chlorella minutissima\" and \"Phaeodactylum tricornutum\"., which are being developed as a commercial source). EPA is not usually found in higher plants, but it has been reported in trace amounts in purslane. In 2013, it was reported that a genetically modified form of the plant Camelina produced significant amounts of EPA.\n\nThe human body converts alpha-linolenic acid (ALA) to EPA. ALA is itself an essential fatty acid, an appropriate supply of which must be ensured. The efficiency of the conversion of ALA to EPA, however, is much lower than the absorption of EPA from food containing it. Because EPA is also a precursor to docosahexaenoic acid (DHA), ensuring a sufficient level of EPA on a diet containing neither EPA nor DHA is harder both because of the extra metabolic work required to synthesize EPA and because of the use of EPA to metabolize into DHA. Medical conditions like diabetes or certain allergies may significantly limit the human body's capacity for metabolization of EPA from ALA.\n\nThe US National Institute of Health's MedlinePlus lists medical conditions for which EPA (alone or in concert with other ω-3 sources) is known or thought to be an effective treatment. Most of these involve its ability to lower inflammation.\n\nIntake of large doses (2.0 to 4.0 g/day) of long-chain omega-3 fatty acids as prescription drugs or dietary supplements are generally required to achieve significant (> 15%) lowering of triglycerides, and at those doses the effects can be significant (from 20% to 35% and even up to 45% in individuals with levels greater that 500 mg/dL).\n\nIt appears that both EPA and DHA lower triglycerides, however DHA appears to raise low-density lipoprotein (the variant which drives atherosclerosis; sometimes inaccurately called: \"bad cholesterol\") and LDL-C values (always only a calculated estimate; not measured by labs from person's blood sample for technical and cost reasons), while EPA does not.\n\nThe big difference between fish oil dietary supplement and prescription forms of omega-3 fatty acids is that the prescription variants are concentrated to markedly increase the amount of these key fatty acids per capsule over the many other fats present in \"fish oil\" and the mercury, also present in \"fish oil\", has been removed.\n\nEPA and DHA ethyl esters (all forms) may be absorbed less well, thus work less well, when taken on an empty stomach or with a low-fat meal.\n\nOmega-3 fatty acids, particularly EPA, have been studied for their effect on autistic spectrum disorder (ASD). Some have theorized that, since omega-3 fatty acid levels may be low in children with autism, supplementation might lead to an improvement in symptoms. While some uncontrolled studies have reported improvements, well-controlled studies have shown no statistically significant improvement in symptoms as a result of high-dose omega-3 supplementation.\n\nIn addition, studies have shown that omega-3 fatty acids may be useful for treating depression.\n\n"}
{"id": "38824", "url": "https://en.wikipedia.org/wiki?curid=38824", "title": "Electric power transmission", "text": "Electric power transmission\n\nElectric power transmission is the bulk movement of electrical energy from a generating site, such as a power plant, to an electrical substation. The interconnected lines which facilitate this movement are known as a transmission network. This is distinct from the local wiring between high-voltage substations and customers, which is typically referred to as electric power distribution. The combined transmission and distribution network is known as the \"power grid\" in North America, or just \"the grid\". In the United Kingdom, India, Malaysia and New Zealand, the network is known as the \"National Grid\".\n\nA wide area synchronous grid, also known as an \"interconnection\" in North America, directly connects a large number of generators delivering AC power with the same relative \"frequency\" to a large number of consumers. For example, there are four major interconnections in North America (the Western Interconnection, the Eastern Interconnection, the Quebec Interconnection and the Electric Reliability Council of Texas (ERCOT) grid). In Europe one large grid connects most of continental Europe.\n\nHistorically, transmission and distribution lines were owned by the same company, but starting in the 1990s, many countries have liberalized the regulation of the electricity market in ways that have led to the separation of the electricity transmission business from the distribution business.\n\nMost transmission lines are high-voltage three-phase alternating current (AC), although single phase AC is sometimes used in railway electrification systems. High-voltage direct-current (HVDC) technology is used for greater efficiency over very long distances (typically hundreds of miles). HVDC technology is also used in submarine power cables (typically longer than 30 miles (50 km)), and in the interchange of power between grids that are not mutually synchronized. HVDC links are used to stabilize large power distribution networks where sudden new loads, or blackouts, in one part of a network can result in synchronization problems and cascading failures.\n\nElectricity is transmitted at high voltages (115 kV or above) to reduce the energy loss which occurs in long-distance transmission. Power is usually transmitted through overhead power lines. Underground power transmission has a significantly higher installation cost and greater operational limitations, but reduced maintenance costs. Underground transmission is sometimes used in urban areas or environmentally sensitive locations.\n\nA lack of electrical energy storage facilities in transmission systems leads to a key limitation. Electrical energy must be generated at the same rate at which it is consumed. A sophisticated control system is required to ensure that the power generation very closely matches the demand. If the demand for power exceeds supply, the imbalance can cause generation plant(s) and transmission equipment to automatically disconnect or shut down to prevent damage. In the worst case, this may lead to a cascading series of shut downs and a major regional blackout. Examples include the US Northeast blackouts of 1965, 1977, 2003, and major blackouts in other US regions in 1996 and 2011. Electric transmission networks are interconnected into regional, national, and even continent wide networks to reduce the risk of such a failure by providing multiple redundant, alternative routes for power to flow should such shut downs occur. Transmission companies determine the maximum reliable capacity of each line (ordinarily less than its physical or thermal limit) to ensure that spare capacity is available in the event of a failure in another part of the network.\n\nHigh-voltage overhead conductors are not covered by insulation. The conductor material is nearly always an aluminum alloy, made into several strands and possibly reinforced with steel strands. Copper was sometimes used for overhead transmission, but aluminum is lighter, yields only marginally reduced performance and costs much less. Overhead conductors are a commodity supplied by several companies worldwide. Improved conductor material and shapes are regularly used to allow increased capacity and modernize transmission circuits. Conductor sizes range from 12 mm (#6 American wire gauge) to 750 mm (1,590,000 circular mils area), with varying resistance and current-carrying capacity. For normal AC lines thicker wires would lead to a relatively small increase in capacity due to the skin effect (which causes most of the current to flow close to the surface of the wire). Because of this current limitation, multiple parallel cables (called bundle conductors) are used when higher capacity is needed. Bundle conductors are also used at high voltages to reduce energy loss caused by corona discharge.\n\nToday, transmission-level voltages are usually considered to be 110 kV and above. Lower voltages, such as 66 kV and 33 kV, are usually considered subtransmission voltages, but are occasionally used on long lines with light loads. Voltages less than 33 kV are usually used for distribution. Voltages above 765 kV are considered extra high voltage and require different designs compared to equipment used at lower voltages.\n\nSince overhead transmission wires depend on air for insulation, the design of these lines requires minimum clearances to be observed to maintain safety. Adverse weather conditions, such as high wind and low temperatures, can lead to power outages. Wind speeds as low as can permit conductors to encroach operating clearances, resulting in a flashover and loss of supply.\nOscillatory motion of the physical line can be termed gallop or flutter depending on the frequency and amplitude of oscillation.\n\nElectric power can also be transmitted by underground power cables instead of overhead power lines. Underground cables take up less right-of-way than overhead lines, have lower visibility, and are less affected by bad weather. However, costs of insulated cable and excavation are much higher than overhead construction. Faults in buried transmission lines take longer to locate and repair. \n\nIn some metropolitan areas, underground transmission cables are enclosed by metal pipe and insulated with dielectric fluid (usually an oil) that is either static or circulated via pumps. If an electric fault damages the pipe and produces a dielectric leak into the surrounding soil, liquid nitrogen trucks are mobilized to freeze portions of the pipe to enable the draining and repair of the damaged pipe location. This type of underground transmission cable can prolong the repair period and increase repair costs. The temperature of the pipe and soil are usually monitored constantly throughout the repair period. \n\nUnderground lines are strictly limited by their thermal capacity, which permits less overload or re-rating than overhead lines. Long underground AC cables have significant capacitance, which may reduce their ability to provide useful power to loads beyond . DC cables are not limited in length by their capacitance, however, they do require HVDC converter stations at both ends of the line to convert from DC to AC before being interconnected with the transmission network.\n\nIn the early days of commercial electric power, transmission of electric power at the same voltage as used by lighting and mechanical loads restricted the distance between generating plant and consumers. In 1882, generation was with direct current (DC), which could not easily be increased in voltage for long-distance transmission. Different classes of loads (for example, lighting, fixed motors, and traction/railway systems) required different voltages, and so used different generators and circuits.\n\nDue to this specialization of lines and because transmission was inefficient for low-voltage high-current circuits, generators needed to be near their loads. It seemed, at the time, that the industry would develop into what is now known as a distributed generation system with large numbers of small generators located near their loads.\n\nThe transmission of electric power with alternating current (AC) became possible after Lucien Gaulard and John Dixon Gibbs built what they called the secondary generator, an early transformer provided with 1:1 turn ratio and open magnetic circuit, in 1881.\n\nThe first long distance AC line was long, built for the 1884 International Exhibition of Turin, Italy. It was powered by a 2 kV, 130 Hz Siemens & Halske alternator and featured several Gaulard secondary generators with their primary windings connected in series, which fed incandescent lamps. The system proved the feasibility of AC electric power transmission on long distances.\n\nA very first operative AC line was put into service in 1885 in via dei Cerchi, Rome, Italy, for public lighting. It was powered by two Siemens & Halske alternators rated 30 hp (22 kW), 2 kV at 120 Hz and used 19 km of cables and 200 parallel-connected 2 kV to 20 V step-down transformers provided with a closed magnetic circuit, one for each lamp. Few months later it was followed by the first British AC system, which was put into service at the Grosvenor Gallery, London. It also featured Siemens alternators and 2.4 kV to 100 V step-down transformers – one per user – with shunt-connected primaries.\nWorking from what he considered an impractical Gaulard-Gibbs design, electrical engineer William Stanley, Jr. developed what is considered the first practical series AC transformer in 1885. Working with the support of George Westinghouse, in 1886 he installed demonstration transformer based alternating current lighting system in Great Barrington, Massachusetts. Powered by a steam engine driven 500 V Siemens generator, voltage was stepped down to 100 Volts using the new Stanley transformer to power incandescent lamps at 23 businesses along main street with very little power loss over 4000 feet. This practical demonstration of a transformer and alternating current lighting system would lead Westinghouse to begin installing AC based systems later that year.\n\n1888 saw designs for a functional AC motor, something these systems had lacked up till then. These were induction motors running on polyphase current, independently invented by Galileo Ferraris and Nikola Tesla (with Tesla’s design being licensed by Westinghouse in the US). This design was further developed into the modern practical three-phase form by Mikhail Dolivo-Dobrovolsky and Charles Eugene Lancelot Brown. Practical use of these types of motors would be delayed many years by development problems and the scarcity of poly-phase power systems needed to power them.\n\nThe late 1880s and early 1890s would see a financial merger of many smaller electric companies into a few larger corporations such as Ganz and AEG in Europe and General Electric and Westinghouse Electric in the US. These companies continued to develop AC systems but the technical difference between direct and alternating current systems would follow a much longer technical merger. Due to innovation in the US and Europe, alternating current's economy of scale with very large generating plants linked to loads via long distance transmission was slowly being combined with the ability to link it up with all of the existing systems that needed to be supplied. These included single phase AC systems, poly-phase AC systems, low voltage incandescent lighting, high voltage arc lighting, and existing DC motors in factories and street cars. In what was becoming a \"universal system\", these technological differences were temporarily being bridged via the development of rotary converters and motor-generators that would allow the large number of legacy systems to be connected to the AC grid. These stopgaps would slowly be replaced as older systems were retired or upgraded.\n\nThe first transmission of single-phase alternating current using high voltage took place in Oregon in 1890 when power was delivered from a hydroelectric plant at Willamette Falls to the city of Portland 14 miles downriver. The first three-phase alternating current using high voltage took place in 1891 during the international electricity exhibition in Frankfurt. A 15 kV transmission line, approximately 175 km long, connected Lauffen on the Neckar and Frankfurt.\n\nVoltages used for electric power transmission increased throughout the 20th century. By 1914, fifty-five transmission systems each operating at more than 70 kV were in service. The highest voltage then used was 150 kV.\nBy allowing multiple generating plants to be interconnected over a wide area, electricity production cost was reduced. The most efficient available plants could be used to supply the varying loads during the day. Reliability was improved and capital investment cost was reduced, since stand-by generating capacity could be shared over many more customers and a wider geographic area. Remote and low-cost sources of energy, such as hydroelectric power or mine-mouth coal, could be exploited to lower energy production cost.\n\nThe rapid industrialization in the 20th century made electrical transmission lines and grids a critical infrastructure item in most industrialized nations. The interconnection of local generation plants and small distribution networks was greatly spurred by the requirements of World War I, with large electrical generating plants built by governments to provide power to munitions factories. Later these generating plants were connected to supply civil loads through long-distance transmission.\n\nEngineers design transmission networks to transport the energy as efficiently as feasible, while at the same time taking into account economic factors, network safety and redundancy. These networks use components such as power lines, cables, circuit breakers, switches and transformers. The transmission network is usually administered on a regional basis by an entity such as a regional transmission organization or transmission system operator.\n\nTransmission efficiency is greatly improved by devices that increase the voltage (and thereby proportionately reduce the current), in the line conductors, thus allowing power to be transmitted with acceptable losses. The reduced current flowing through the line reduces the heating losses in the conductors. According to Joule's Law, energy losses are directly proportional to the square of the current. Thus, reducing the current by a factor of two will lower the energy lost to conductor resistance by a factor of four for any given size of conductor.\n\nThe optimum size of a conductor for a given voltage and current can be estimated by Kelvin's law for conductor size, which states that the size is at its optimum when the annual cost of energy wasted in the resistance is equal to the annual capital charges of providing the conductor. At times of lower interest rates, Kelvin's law indicates that thicker wires are optimal; while, when metals are expensive, thinner conductors are indicated: however, power lines are designed for long-term use, so Kelvin's law has to be used in conjunction with long-term estimates of the price of copper and aluminum as well as interest rates for capital.\n\nThe increase in voltage is achieved in AC circuits by using a \"step-up transformer\". HVDC systems require relatively costly conversion equipment which may be economically justified for particular projects such as submarine cables and longer distance high capacity point-to-point transmission. HVDC is necessary for the import and export of energy between grid systems that are not synchronized with each other.\n\nA transmission grid is a network of power stations, transmission lines, and substations. Energy is usually transmitted within a grid with three-phase AC. Single-phase AC is used only for distribution to end users since it is not usable for large polyphase induction motors. In the 19th century, two-phase transmission was used but required either four wires or three wires with unequal currents. Higher order phase systems require more than three wires, but deliver little or no benefit.\n\nThe price of electric power station capacity is high, and electric demand is variable, so it is often cheaper to import some portion of the needed power than to generate it locally. Because loads are often regionally correlated (hot weather in the Southwest portion of the US might cause many people to use air conditioners), electric power often comes from distant sources. Because of the economic benefits of load sharing between regions, wide area transmission grids now span countries and even continents. The web of interconnections between power producers and consumers should enable power to flow, even if some links are inoperative.\n\nThe unvarying (or slowly varying over many hours) portion of the electric demand is known as the \"base load\" and is generally served by large facilities (which are more efficient due to economies of scale) with fixed costs for fuel and operation. Such facilities are nuclear, coal-fired or hydroelectric, while other energy sources such as concentrated solar thermal and geothermal power have the potential to provide base load power. Renewable energy sources, such as solar photovoltaics, wind, wave, and tidal, are, due to their intermittency, not considered as supplying \"base load\" but will still add power to the grid. The remaining or 'peak' power demand, is supplied by peaking power plants, which are typically smaller, faster-responding, and higher cost sources, such as combined cycle or combustion turbine plants fueled by natural gas.\n\nLong-distance transmission of electricity (hundreds of kilometers) is cheap and efficient, with costs of US$0.005–0.02 per kWh (compared to annual averaged large producer costs of US$0.01–0.025 per kWh, retail rates upwards of US$0.10 per kWh, and multiples of retail for instantaneous suppliers at unpredicted highest demand moments). Thus distant suppliers can be cheaper than local sources (e.g., New York often buys over 1000 MW of electricity from Canada). Multiple local sources (even if more expensive and infrequently used) can make the transmission grid more fault tolerant to weather and other disasters that can disconnect distant suppliers.\n\nLong-distance transmission allows remote renewable energy resources to be used to displace fossil fuel consumption. Hydro and wind sources cannot be moved closer to populous cities, and solar costs are lowest in remote areas where local power needs are minimal. Connection costs alone can determine whether any particular renewable alternative is economically sensible. Costs can be prohibitive for transmission lines, but various proposals for massive infrastructure investment in high capacity, very long distance super grid transmission networks could be recovered with modest usage fees.\n\nAt the power stations, the power is produced at a relatively low voltage between about 2.3 kV and 30 kV, depending on the size of the unit. The generator terminal voltage is then stepped up by the power station transformer to a higher voltage (115 kV to 765 kV AC, varying by the transmission system and by the country) for transmission over long distances.\n\nIn the United States, power transmission is, variously, 230 kV to 500 kV, with less than 230 kV or more than 500 kV being local exceptions. For example, the Western System has two primary interchange voltages: 500 kV AC at 60 Hz, and ±500 kV (1,000 kV net) DC from North to South (U.S.-Canada border to U.S.-Mexico border).\n\nThe 287.5 kV (Hoover to Los Angeles line, via Victorville) and 345 kV (APS line) being local standards, both of which were implemented before 500 kV became practical, and thereafter the Western System standard.\n\nTransmitting electricity at high voltage reduces the fraction of energy lost to resistance, which varies depending on the specific conductors, the current flowing, and the length of the transmission line. For example, a span at 765 kV carrying 1000 MW of power can have losses of 1.1% to 0.5%. A 345 kV line carrying the same load across the same distance has losses of 4.2%. For a given amount of power, a higher voltage reduces the current and thus the resistive losses in the conductor. For example, raising the voltage by a factor of 10 reduces the current by a corresponding factor of 10 and therefore the formula_1 losses by a factor of 100, provided the same sized conductors are used in both cases. Even if the conductor size (cross-sectional area) is decreased ten-fold to match the lower current, the formula_1 losses are still reduced ten-fold. Long-distance transmission is typically done with overhead lines at voltages of 115 to 1,200 kV. At extremely high voltages, more than 2,000 kV exists between conductor and ground, corona discharge losses are so large that they can offset the lower resistive losses in the line conductors. Measures to reduce corona losses include conductors having larger diameters; often hollow to save weight, or bundles of two or more conductors.\n\nFactors that affect the resistance, and thus loss, of conductors used in transmission and distribution lines include temperature, spiraling, and the skin effect. The resistance of a conductor increases with its temperature. Temperature changes in electric power lines can have a significant effect on power losses in the line. Spiraling, which refers to the way stranded conductors spiral about the center, also contributes to increases in conductor resistance. The skin effect causes the effective resistance of a conductor to increase at higher alternating current frequencies.\n\nTransmission and distribution losses in the USA were estimated at 6.6% in 1997 and 6.5% in 2007. In general, losses are estimated from the discrepancy between power produced (as reported by power plants) and power sold to the end customers; the difference between what is produced and what is consumed constitute transmission and distribution losses, assuming no utility theft occurs.\n\nAs of 1980, the longest cost-effective distance for direct-current transmission was determined to be . For alternating current it was , though all transmission lines in use today are substantially shorter than this.\n\nIn any alternating current transmission line, the inductance and capacitance of the conductors can be significant. Currents that flow solely in ‘reaction’ to these properties of the circuit, (which together with the resistance define the impedance) constitute reactive power flow, which transmits no ‘real’ power to the load. These reactive currents, however, are very real and cause extra heating losses in the transmission circuit. The ratio of 'real' power (transmitted to the load) to 'apparent' power (the product of a circuit's voltage and current, without reference to phase angle) is the power factor. As reactive current increases, the reactive power increases and the power factor decreases. For transmission systems with low power factor, losses are higher than for systems with high power factor. Utilities add capacitor banks, reactors and other components (such as phase-shifting transformers; static VAR compensators; and flexible AC transmission systems, FACTS) throughout the system help to compensate for the reactive power flow, reduce the losses in power transmission and stabilize system voltages. These measures are collectively called 'reactive support'.\n\nCurrent flowing through transmission lines induces a magnetic field that surrounds the lines of each phase and affects the inductance of the surrounding conductors of other phases. The mutual inductance of the conductors is partially dependent on the physical orientation of the lines with respect to each other. Three-phase power transmission lines are conventionally strung with phases separated on different vertical levels. The mutual inductance seen by a conductor of the phase in the middle of the other two phases will be different than the inductance seen by the conductors on the top or bottom. An imbalanced inductance among the three conductors is problematic because it may result in the middle line carrying a disproportionate amount of the total power transmitted. Similarly, an imbalanced load may occur if one line is consistently closest to the ground and operating at a lower impedance. Because of this phenomenon, conductors must be periodically transposed along the length of the transmission line so that each phase sees equal time in each relative position to balance out the mutual inductance seen by all three phases. To accomplish this, line position is swapped at specially designed transposition towers at regular intervals along the length of the transmission line in various transposition schemes.\n\nSubtransmission is part of an electric power transmission system that runs at relatively lower voltages. It is uneconomical to connect all distribution substations to the high main transmission voltage, because the equipment is larger and more expensive. Typically, only larger substations connect with this high voltage. It is stepped down and sent to smaller substations in towns and neighborhoods. Subtransmission circuits are usually arranged in loops so that a single line failure does not cut off service to a large number of customers for more than a short time. Loops can be \"normally closed\", where loss of one circuit should result in no interruption, or \"normally open\" where substations can switch to a backup supply. While subtransmission circuits are usually carried on overhead lines, in urban areas buried cable may be used. The lower-voltage subtransmission lines use less right-of-way and simpler structures; it is much more feasible to put them underground where needed. Higher-voltage lines require more space and are usually above-ground since putting them underground is very expensive.\n\nThere is no fixed cutoff between subtransmission and transmission, or subtransmission and distribution. The voltage ranges overlap somewhat. Voltages of 69 kV, 115 kV, and 138 kV are often used for subtransmission in North America. As power systems evolved, voltages formerly used for transmission were used for subtransmission, and subtransmission voltages became distribution voltages. Like transmission, subtransmission moves relatively large amounts of power, and like distribution, subtransmission covers an area instead of just point-to-point.\n\nAt the substations, transformers reduce the voltage to a lower level for distribution to commercial and residential users. This distribution is accomplished with a combination of sub-transmission (33 to 132 kV) and distribution (3.3 to 25 kV). Finally, at the point of use, the energy is transformed to low voltage (varying by country and customer requirements – see Mains electricity by country).\n\nHigh-voltage power transmission allows for lesser resistive losses over long distances in the wiring. This efficiency of high voltage transmission allows for the transmission of a larger proportion of the generated power to the substations and in turn to the loads, translating to operational cost savings.\n\nIn a very simplified model, assume the electrical grid delivers electricity from a generator (modelled as an ideal voltage source with voltage formula_3, delivering a power formula_4) to a single point of consumption, modelled by a pure resistance formula_5, when the wires are long enough to have a significant resistance formula_6.\n\nIf the resistance are simply in series without any transformer between them, the circuit acts as a voltage divider, because the same current formula_7 runs through the wire resistance and the powered device. As a consequence, the useful power (used at the point of consumption) is:\nAssume now that a transformer converts high-voltage, low-current electricity transported by the wires into low-voltage, high-current electricity for use at the consumption point. If we suppose it is an ideal transformer with a voltage ratio of formula_9 (i.e., the voltage is divided by formula_9 and the current is multiplied by formula_9 in the secondary branch, compared to the primary branch), then the circuit is again equivalent to a voltage divider, but the transformer-consumption branch has an apparent resistance of formula_12. The useful power is then:\n\nFor formula_14 (i.e. conversion of high voltage to low voltage near the consumption point), a larger fraction of the generator's power is transmitted to the consumption point and a lesser fraction is lost to Joule heating.\n\nOftentimes, we are only interested in the terminal characteristics of the transmission line, which are the voltage and current at the sending and receiving ends. The transmission line itself is then modeled as a \"black box\" and a 2 by 2 transmission matrix is used to model its behavior, as follows:\n\nThe line is assumed to be a reciprocal, symmetrical network, meaning that the receiving and sending labels can be switched with no consequence. The transmission matrix T also has the following properties:\n\nThe parameters \"A\", \"B\", \"C\", and \"D\" differ depending on how the desired model handles the line's resistance (\"R\"), inductance (\"L\"), capacitance (\"C\"), and shunt (parallel, leak) conductance \"G\". The four main models are the short line approximation, the medium line approximation, the long line approximation (with distributed parameters), and the lossless line. In all models described, a capital letter such as \"R\" refers to the total quantity summed over the line and a lowercase letter such as \"c\" refers to the per-unit-length quantity.\n\nThe lossless line approximation is the least accurate model; it is often used on short lines when the inductance of the line is much greater than its resistance. For this approximation, the voltage and current are identical at the sending and receiving ends.\n\nThe characteristic impedance is pure real, which means resistive for that impedance, and it is often called surge impedance for a lossless line. When lossless line is terminated by surge impedance, there is no voltage drop. Though the phase angles of voltage and current are rotated, the magnitudes of voltage and current remain constant along the length of the line. For load > SIL, the voltage will drop from sending end and the line will “consume” VARs. For load < SIL, the voltage will increase from sending end, and the line will “generate” VARs.\n\nThe short line approximation is normally used for lines less than 50 miles long. For a short line, only a series impedance \"Z\" is considered, while \"C\" and \"G\" are ignored. The final result is that A = D = 1 per unit, B = Z Ohms, and C = 0. The associated transition matrix for this approximation is therefore:\n\nThe medium line approximation is used for lines between 50 and 150 miles long. In this model, the series impedance and the shunt (current leak) conductance are considered, with half of the shunt conductance being placed at each end of the line. This circuit is often referred to as a “nominal \"π\" (pi)” circuit because of the shape (\"π\") that is taken on when leak conductance is placed on both sides of the circuit diagram. The analysis of the medium line brings one to the following result:\n\nCounterintuitive behaviors of medium-length transmission lines:\n\n\nThe long line model is used when a higher degree of accuracy is needed or when the line under consideration is more than 150 miles long. Series resistance and shunt conductance are considered as distributed parameters, meaning each differential length of the line has a corresponding differential resistance and shunt admittance. The following result can be applied at any point along the transmission line, where formula_20 is the propagation constant.\n\nTo find the voltage and current at the end of the long line, formula_22 should be replaced with formula_23 (the line length) in all parameters of the transmission matrix.\n\nHigh-voltage direct current (HVDC) is used to transmit large amounts of power over long distances or for interconnections between asynchronous grids. When electrical energy is to be transmitted over very long distances, the power lost in AC transmission becomes appreciable and it is less expensive to use direct current instead of alternating current. For a very long transmission line, these lower losses (and reduced construction cost of a DC line) can offset the additional cost of the required converter stations at each end.\n\nHVDC is also used for long submarine cables where AC cannot be used because of the cable capacitance. In these cases special high-voltage cables for DC are used. Submarine HVDC systems are often used to connect the electricity grids of islands, for example, between Great Britain and continental Europe, between Great Britain and Ireland, between Tasmania and the Australian mainland, between the North and South Islands of New Zealand, between New Jersey and New York City, and between New Jersey and Long Island. Submarine connections up to in length are presently in use.\n\nHVDC links can be used to control problems in the grid with AC electricity flow. The power transmitted by an AC line increases as the phase angle between source end voltage and destination ends increases, but too large a phase angle will allow the systems at either end of the line to fall out of step. Since the power flow in a DC link is controlled independently of the phases of the AC networks at either end of the link, this phase angle limit does not exist, and a DC link is always able to transfer its full rated power. A DC link therefore stabilizes the AC grid at either end, since power flow and phase angle can then be controlled independently.\n\nAs an example, to adjust the flow of AC power on a hypothetical line between Seattle and Boston would require adjustment of the relative phase of the two regional electrical grids. This is an everyday occurrence in AC systems, but one that can become disrupted when AC system components fail and place unexpected loads on the remaining working grid system. With an HVDC line instead, such an interconnection would:\n(and possibly in other cooperating cities along the transmission route). Such a system could be less prone to failure if parts of it were suddenly shut down. One example of a long DC transmission line is the Pacific DC Intertie located in the Western United States.\n\nThe amount of power that can be sent over a transmission line is limited. The origins of the limits vary depending on the length of the line. For a short line, the heating of conductors due to line losses sets a thermal limit. If too much current is drawn, conductors may sag too close to the ground, or conductors and equipment may be damaged by overheating. For intermediate-length lines on the order of , the limit is set by the voltage drop in the line. For longer AC lines, system stability sets the limit to the power that can be transferred. Approximately, the power flowing over an AC line is proportional to the cosine of the phase angle of the voltage and current at the receiving and transmitting ends. This angle varies depending on system loading and generation. It is undesirable for the angle to approach 90 degrees, as the power flowing decreases but the resistive losses remain. Very approximately, the allowable product of line length and maximum load is proportional to the square of the system voltage. Series capacitors or phase-shifting transformers are used on long lines to improve stability. High-voltage direct current lines are restricted only by thermal and voltage drop limits, since the phase angle is not material to their operation.\n\nUp to now, it has been almost impossible to foresee the temperature distribution along the cable route, so that the maximum applicable current load was usually set as a compromise between understanding of operation conditions and risk minimization. The availability of industrial distributed temperature sensing (DTS) systems that measure in real time temperatures all along the cable is a first step in monitoring the transmission system capacity. This monitoring solution is based on using passive optical fibers as temperature sensors, either integrated directly inside a high voltage cable or mounted externally on the cable insulation. A solution for overhead lines is also available. In this case the optical fiber is integrated into the core of a phase wire of overhead transmission lines (OPPC). The integrated Dynamic Cable Rating (DCR) or also called Real Time Thermal Rating (RTTR) solution enables not only to continuously monitor the temperature of a high voltage cable circuit in real time, but to safely utilize the existing network capacity to its maximum. Furthermore, it provides the ability to the operator to predict the behavior of the transmission system upon major changes made to its initial operating conditions.\n\nTo ensure safe and predictable operation, the components of the transmission system are controlled with generators, switches, circuit breakers and loads. The voltage, power, frequency, load factor, and reliability capabilities of the transmission system are designed to provide cost effective performance for the customers.\n\nThe transmission system provides for base load and peak load capability, with safety and fault tolerance margins. The peak load times vary by region largely due to the industry mix. In very hot and very cold climates home air conditioning and heating loads have an effect on the overall load. They are typically highest in the late afternoon in the hottest part of the year and in mid-mornings and mid-evenings in the coldest part of the year. This makes the power requirements vary by the season and the time of day. Distribution system designs always take the base load and the peak load into consideration.\n\nThe transmission system usually does not have a large buffering capability to match the loads with the generation. Thus generation has to be kept matched to the load, to prevent overloading failures of the generation equipment.\n\nMultiple sources and loads can be connected to the transmission system and they must be controlled to provide orderly transfer of power. In centralized power generation, only local control of generation is necessary, and it involves synchronization of the generation units, to prevent large transients and overload conditions.\n\nIn distributed power generation the generators are geographically distributed and the process to bring them online and offline must be carefully controlled. The load control signals can either be sent on separate lines or on the power lines themselves. Voltage and frequency can be used as signalling mechanisms to balance the loads.\n\nIn voltage signaling, the variation of voltage is used to increase generation. The power added by any system increases as the line voltage decreases. This arrangement is stable in principle. Voltage-based regulation is complex to use in mesh networks, since the individual components and setpoints would need to be reconfigured every time a new generator is added to the mesh.\n\nIn frequency signaling, the generating units match the frequency of the power transmission system. In droop speed control, if the frequency decreases, the power is increased. (The drop in line frequency is an indication that the increased load is causing the generators to slow down.)\n\nWind turbines, vehicle-to-grid and other locally distributed storage and generation systems can be connected to the power grid, and interact with it to improve system operation. Internationally, the trend has been a slow move from a heavily centralized power system to a decentralized power system. The main draw of locally distributed generation systems which involve a number of new and innovative solutions is that they reduce transmission losses by leading to consumption of electricity closer to where it was produced.\n\nUnder excess load conditions, the system can be designed to fail gracefully rather than all at once. Brownouts occur when the supply power drops below the demand. Blackouts occur when the supply fails completely.\n\nRolling blackouts (also called load shedding) are intentionally engineered electrical power outages, used to distribute insufficient power when the demand for electricity exceeds the supply.\n\nOperators of long transmission lines require reliable communications for control of the power grid and, often, associated generation and distribution facilities. Fault-sensing protective relays at each end of the line must communicate to monitor the flow of power into and out of the protected line section so that faulted conductors or equipment can be quickly de-energized and the balance of the system restored. Protection of the transmission line from short circuits and other faults is usually so critical that common carrier telecommunications are insufficiently reliable, and in remote areas a common carrier may not be available. Communication systems associated with a transmission project may use:\nRarely, and for short distances, a utility will use pilot-wires strung along the transmission line path. Leased circuits from common carriers are not preferred since availability is not under control of the electric power transmission organization.\n\nTransmission lines can also be used to carry data: this is called power-line carrier, or PLC. PLC signals can be easily received with a radio for the long wave range.\n\nOptical fibers can be included in the stranded conductors of a transmission line, in the overhead shield wires. These cables are known as optical ground wire (\"OPGW\"). Sometimes a standalone cable is used, all-dielectric self-supporting (\"ADSS\") cable, attached to the transmission line cross arms.\n\nSome jurisdictions, such as Minnesota, prohibit energy transmission companies from selling surplus communication bandwidth or acting as a telecommunications common carrier. Where the regulatory structure permits, the utility can sell capacity in extra dark fibers to a common carrier, providing another revenue stream.\n\nSome regulators regard electric transmission to be a natural monopoly and there are moves in many countries to separately regulate transmission (see electricity market).\n\nSpain was the first country to establish a regional transmission organization. In that country, transmission operations and market operations are controlled by separate companies. The transmission system operator is Red Eléctrica de España (REE) and the wholesale electricity market operator is Operador del Mercado Ibérico de Energía – Polo Español, S.A. (OMEL) . Spain's transmission system is interconnected with those of France, Portugal, and Morocco.\n\nThe establishment of RTOs in the United States was spurred by the FERC's Order 888, \"Promoting Wholesale Competition Through Open Access Non-discriminatory Transmission Services by Public Utilities; Recovery of Stranded Costs by Public Utilities and Transmitting Utilities\", issued in 1996.\nIn the United States and parts of Canada, several electric transmission companies operate independently of generation companies, but there are still regions - the Southern United States - where vertical integration of the electric system is intact. In regions of separation, transmission owners and generation owners continue to interact with each other as market participants with voting rights within their RTO. RTOs in the United States are regulated by the Federal Energy Regulatory Commission.\n\nThe cost of high voltage electricity transmission (as opposed to the costs of electric power distribution) is comparatively low, compared to all other costs arising in a consumer's electricity bill. In the UK, transmission costs are about 0.2 p per kWh compared to a delivered domestic price of around 10 p per kWh.\n\nResearch evaluates the level of capital expenditure in the electric power T&D equipment market will be worth $128.9 bn in 2011.\n\nMerchant transmission is an arrangement where a third party constructs and operates electric transmission lines through the franchise area of an unrelated incumbent utility.\n\nOperating merchant transmission projects in the United States include the Cross Sound Cable from Shoreham, New York to New Haven, Connecticut, Neptune RTS Transmission Line from Sayreville, New Jersey to New Bridge, New York, and Path 15 in California. Additional projects are in development or have been proposed throughout the United States, including the Lake Erie Connector, an underwater transmission line proposed by ITC Holdings Corp., connecting Ontario to load serving entities in the PJM Interconnection region.\n\nThere is only one unregulated or market interconnector in Australia: Basslink between Tasmania and Victoria. Two DC links originally implemented as market interconnectors, Directlink and Murraylink, have been converted to regulated interconnectors. NEMMCO\n\nA major barrier to wider adoption of merchant transmission is the difficulty in identifying who benefits from the facility so that the beneficiaries will pay the toll. Also, it is difficult for a merchant transmission line to compete when the alternative transmission lines are subsidized by incumbent utility businesses with a monopolized and regulated rate base. In the United States, the FERC's Order 1000, issued in 2010, attempts to reduce barriers to third party investment and creation of merchant transmission lines where a public policy need is found.\n\nSome large studies, including a large study in the United States, have failed to find any link between living near power lines and developing any sickness or diseases, such as cancer. A 1997 study found that it did not matter how close one was to a power line or a sub-station, there was no increased risk of cancer or illness.\n\nThe mainstream scientific evidence suggests that low-power, low-frequency, electromagnetic radiation associated with household currents and high transmission power lines does not constitute a short or long term health hazard. Some studies, however, have found statistical correlations between various diseases and living or working near power lines. No adverse health effects have been substantiated for people not living close to powerlines.\n\nThe New York State Public Service Commission conducted a study, documented in \"Opinion No. 78-13\" (issued June 19, 1978), to evaluate potential health effects of electric fields. The study's case number is too old to be listed as a case number in the commission's online database, DMM, and so the original study can be difficult to find. The study chose to utilize the electric field strength that was measured at the edge of an existing (but newly built) right-of-way on a 765 kV transmission line from New York to Canada, 1.6 kV/m, as the interim standard maximum electric field at the edge of any new transmission line right-of-way built in New York State after issuance of the order. The opinion also limited the voltage of all new transmission lines built in New York to 345 kV. On September 11, 1990, after a similar study of magnetic field strengths, the NYSPSC issued their \"Interim Policy Statement on Magnetic Fields\". This study established established a magnetic field interim standard of 200 mG at the edge of the right-of-way using the winter-normal conductor rating. This later document can also be difficult to find on the NYSPSC's online database, since it predates the online database system. As a comparison with everyday items, a hair dryer or electric blanket produces a 100 mG - 500 mG magnetic field. An electric razor can produce 2.6 kV/m. Whereas electric fields can be shielded, magnetic fields cannot be shielded, but are usually minimized by optimizing the location of each phase of a circuit in cross-section.\n\nWhen a new transmission line is proposed, within the application to the applicable regulatory body (usually a public utility commission), there is often an analysis of electric and magnetic field levels at the edge of rights-of-way. These analyses are performed by a utility or by an electrical engineering consultant using modelling software. At least one state public utility commission has access to software developed by an engineer or engineers at the Bonneville Power Administration to analyze electric and magnetic fields at edge of rights-of-way for proposed transmission lines. Often, public utility commissions will not comment on any health impacts due to electric and magnetic fields and will refer information seekers to the state's affiliated department of health.\n\nThere are established biological effects for acute \"high\" level exposure to magnetic fields well above 100 µT (1 G) (1,000 mG). In a residential setting, there is \"limited evidence of carcinogenicity in humans and less than sufficient evidence for carcinogenicity in experimental animals\", in particular, childhood leukemia, \"associated with\" average exposure to residential power-frequency magnetic field above 0.3 µT (3 mG) to 0.4 µT (4 mG). These levels exceed average residential power-frequency magnetic fields in homes, which are about 0.07 µT (0.7 mG) in Europe and 0.11 µT (1.1 mG) in North America.\n\nThe Earth's natural geomagnetic field strength varies over the surface of the planet between 0.035 mT and 0.07 mT (35 µT - 70 µT or 350 mG - 700 mG) while the International Standard for the continuous exposure limit is set at 40 mT (400,000 mG or 400 G) for the general public.\n\nTree Growth Regulator and Herbicide Control Methods may be used in transmission line right of ways which may have health effects.\n\nThe Federal Energy Regulatory Commission (FERC) is the primary regulatory agency of electric power transmission and wholesale electricity sales within the United States. It was originally established by Congress in 1920 as the Federal Power Commission and has since undergone multiple name and responsibility modifications. That which is not regulated by FERC, primarily electric power distribution and the retail sale of power, is under the jurisdiction of state authority.\n\nTwo of the more notable U.S. energy policies impacting electricity transmission are Order No. 888 and the Energy Policy Act of 2005.\n\nOrder No. 888 adopted by FERC on 24 April 1996, was “designed to remove impediments to competition in the wholesale bulk power marketplace and to bring more efficient, lower cost power to the Nation’s electricity consumers. The legal and policy cornerstone of these rules is to remedy undue discrimination in access to the monopoly owned transmission wires that control whether and to whom electricity can be transported in interstate commerce.” Order No. 888 required all public utilities that own, control, or operate facilities used for transmitting electric energy in interstate commerce, to have open access non-discriminatory transmission tariffs. These tariffs allow any electricity generator to utilize the already existing power lines for the transmission of the power that they generate. Order No. 888 also permits public utilities to recover the costs associated with providing their power lines as an open access service.\n\nThe Energy Policy Act of 2005 (EPAct) signed into law by congress on 8 August 2005, further expanded the federal authority of regulating power transmission. EPAct gave FERC significant new responsibilities including but not limited to the enforcement of electric transmission reliability standards and the establishment of rate incentives to encourage investment in electric transmission.\n\nHistorically, local governments have exercised authority over the grid and have significant disincentives to encourage actions that would benefit states other than their own. Localities with cheap electricity have a disincentive to encourage making interstate commerce in electricity trading easier, since other regions will be able to compete for local energy and drive up rates. For example, some regulators in Maine do not wish to address congestion problems because the congestion serves to keep Maine rates low. Further, vocal local constituencies can block or slow permitting by pointing to visual impact, environmental, and perceived health concerns. In the US, generation is growing four times faster than transmission, but big transmission upgrades require the coordination of multiple states, a multitude of interlocking permits, and cooperation between a significant portion of the 500 companies that own the grid. From a policy perspective, the control of the grid is balkanized, and even former energy secretary Bill Richardson refers to it as a \"third world grid\". There have been efforts in the EU and US to confront the problem. The US national security interest in significantly growing transmission capacity drove passage of the 2005 energy act giving the Department of Energy the authority to approve transmission if states refuse to act. However, soon after the Department of Energy used its power to designate two National Interest Electric Transmission Corridors, 14 senators signed a letter stating the DOE was being too aggressive.\n\nIn some countries where electric locomotives or electric multiple units run on low frequency AC power, there are separate single phase traction power networks operated by the railways. Prime examples are countries in Europe (including Austria, Germany and Switzerland) which utilize the older AC technology based on 16 \"/\" Hz (Norway and Sweden also use this frequency but use conversion from the 50 Hz public supply; Sweden has a 16 \"/\" Hz traction grid but only for part of the system).\n\nHigh-temperature superconductors (HTS) promise to revolutionize power distribution by providing lossless transmission of electrical power. The development of superconductors with transition temperatures higher than the boiling point of liquid nitrogen has made the concept of superconducting power lines commercially feasible, at least for high-load applications. It has been estimated that the waste would be halved using this method, since the necessary refrigeration equipment would consume about half the power saved by the elimination of the majority of resistive losses. Some companies such as Consolidated Edison and American Superconductor have already begun commercial production of such systems. In one hypothetical future system called a SuperGrid, the cost of cooling would be eliminated by coupling the transmission line with a liquid hydrogen pipeline.\n\nSuperconducting cables are particularly suited to high load density areas such as the business district of large cities, where purchase of an easement for cables would be very costly.\n\nSingle-wire earth return (SWER) or single wire ground return is a single-wire transmission line for supplying single-phase electrical power for an electrical grid to remote areas at low cost. It is principally used for rural electrification, but also finds use for larger isolated loads such as water pumps. Single wire earth return is also used for HVDC over submarine power cables.\n\nBoth Nikola Tesla and Hidetsugu Yagi attempted to devise systems for large scale wireless power transmission in the late 1800s and early 1900s, with no commercial success.\n\nIn November 2009, LaserMotive won the NASA 2009 Power Beaming Challenge by powering a cable climber 1 km vertically using a ground-based laser transmitter. The system produced up to 1 kW of power at the receiver end. In August 2010, NASA contracted with private companies to pursue the design of laser power beaming systems to power low earth orbit satellites and to launch rockets using laser power beams.\n\nWireless power transmission has been studied for transmission of power from solar power satellites to the earth. A high power array of microwave or laser transmitters would beam power to a rectenna. Major engineering and economic challenges face any solar power satellite project.\n\nThe Federal government of the United States admits that the power grid is susceptible to cyber-warfare. The United States Department of Homeland Security works with industry to identify vulnerabilities and to help industry enhance the security of control system networks, the federal government is also working to ensure that security is built in as the U.S. develops the next generation of 'smart grid' networks.\n\n\nNotes\n\n\nMaps\n"}
{"id": "343492", "url": "https://en.wikipedia.org/wiki?curid=343492", "title": "Electrolytic capacitor", "text": "Electrolytic capacitor\n\nAn electrolytic capacitor (abbreviated e-cap) is a polarized capacitor whose anode or positive plate is made of a metal that forms an insulating oxide layer through anodization. This oxide layer acts as the dielectric of the capacitor. A solid, liquid, or gel electrolyte covers the surface of this oxide layer, serving as the (cathode) or negative plate of the capacitor. Due to their very thin dielectric oxide layer and enlarged anode surface, electrolytic capacitors have a much higher capacitance-voltage (CV) product per unit volume compared to ceramic capacitors or film capacitors, and so can have large capacitance values. There are three families of electrolytic capacitor: aluminum electrolytic capacitors, tantalum electrolytic capacitors, and niobium electrolytic capacitors.\n\nThe large capacitance of electrolytic capacitors makes them particularly suitable for passing or bypassing low-frequency signals, and for storing large amounts of energy. They are widely used for decoupling or noise filtering in power supplies and DC link circuits for variable-frequency drives, for coupling signals between amplifier stages, and storing energy as in a flashlamp.\n\nElectrolytic capacitors are polarized components due to their asymmetrical construction, and must be operated with a higher voltage (ie, more positive) on the anode than on the cathode at all times. For this reason the anode terminal is marked with a plus sign and the cathode with a minus sign. Applying a reverse polarity voltage, or a voltage exceeding the maximum rated working voltage of as little as 1 or 1.5 volts, can destroy the dielectric and thus the capacitor. The failure of electrolytic capacitors can be hazardous, resulting in an explosion or fire. Bipolar electrolytic capacitors which may be operated with either polarity are also made, using special constructions with two anodes connected in series.\n\nAs to the basic construction principles of electrolytic capacitors, there are three different types: aluminum, tantalum, and niobium capacitors. Each of these three capacitor families uses non-solid and solid manganese dioxide or solid polymer electrolytes, so a great spread of different combinations of anode material and solid or non-solid electrolytes is available.\n\nLike other conventional capacitors, electrolytic capacitors store the electric energy statically by charge separation in an electric field in the dielectric oxide layer between two electrodes. The non-solid or solid electrolyte in principle is the cathode, which thus forms the second electrode of the capacitor. This and the storage principle distinguish them from electrochemical capacitors or supercapacitors, in which the electrolyte generally is the ionic conductive connection between two electrodes and the storage occurs with statically double-layer capacitance and electrochemical pseudocapacitance.\n\nElectrolytic capacitors use a chemical feature of some special metals, previously called “valve metals”, which on contact with a particular electrolyte form a very thin insulating oxide layer on their surface by anodic oxidation which can function as a dielectric. There are three different anode metals in use for electrolytic capacitors:\n\nTo increase their capacitance per unit volume, all anode materials are either etched or sintered and have a rough surface structure with a much higher surface area compared to a smooth surface of the same area or the same volume. By applying a positive voltage to the above-mentioned anode material in an electrolytic bath an oxide barrier layer with a thickness corresponding to the applied voltage will be formed (formation). This oxide layer acts as dielectric in an electrolytic capacitor. The properties of this oxide layers are given in the following table:\n\nAfter forming a dielectric oxide on the rough anode structure, a counter electrode has to match the rough insulating oxide surface. This is accomplished by the electrolyte, which acts as the cathode electrode of an electrolytic capacitor. There are many different electrolytes in use. Generally they are distinguished into two species, “non-solid” and “solid” electrolytes. As a liquid medium which has ion conductivity caused by moving ions, non-solid electrolytes can easily fit the rough structures. Solid electrolytes which have electron conductivity can fit the rough structures with the help of special chemical processes like pyrolysis for manganese dioxide or polymerization for conducting polymers.\n\nComparing the permittivities of the different oxide materials it is seen that tantalum pentoxide has a permittivity approximately three times higher than aluminum oxide. Tantalum electrolytic capacitors of a given CV value theoretically are therefore smaller than aluminium electrolytic capacitors. In practice different safety margins to reach reliable components makes a comparison difficult.\n\nThe anodically generated insulating oxide layer is destroyed if the polarity of the applied voltage changes.\n\nElectrolytic capacitors are based on the principle of a \"plate capacitor\" whose capacitance increases with larger electrode area A, higher dielectric permittivity ε, and thinner dielectric (d).\n\nThe dielectric thickness of electrolytic capacitors is very small, in the range of nanometers per volt. On the other hand, the voltage strengths of these oxide layers are quite high. With this very thin dielectric oxide layer combined with a sufficiently high dielectric strength the electrolytic capacitors can achieve a high volumetric capacitance. This is one reason for the high capacitance values of electrolytic capacitors compared to conventional capacitors.\n\nAll etched or sintered anodes have a much higher surface area compared to a smooth surface of the same area or the same volume. That increases the capacitance value, depending on the rated voltage, by a factor of up to 200 for non-solid aluminium electrolytic capacitors as well as for solid tantalum electrolytic capacitors. The large surface compared to a smooth one is the second reason for the relatively high capacitance values of electrolytic capacitors compared with other capacitor families.\n\nBecause the forming voltage defines the oxide layer thickness, the desired voltage rating can be produced very simply. Electrolytic capacitors have high volumetric efficiency, the so-called \"CV product\", defined as the product of capacitance and voltage divide by volume.\n\nCombinations of anode materials for electrolytic capacitors and the electrolytes used have given rise to wide varieties of capacitor types with different properties. An outline of the main characteristics of the different types is shown in the table below.\n\nThe non-solid or so-called \"wet\" aluminum electrolytic capacitors were and are the cheapest among all other conventional capacitors. They not only provide the cheapest solutions for high capacitance or voltage values for decoupling and buffering purposes but are also insensitive to low ohmic charging and discharging as well as to low-energy transients. Non-solid electrolytic capacitors can be found in nearly all areas of electronic devices, with the exception of military applications.\n\nTantalum electrolytic capacitors with solid electrolyte as surface-mountable chip capacitors are mainly used in electronic devices in which little space is available or a low profile is required. They operate reliably over a wide temperature range without large parameter deviations. In military and space applications only tantalum electrolytic capacitors have the necessary approvals.\n\nNiobium electrolytic capacitors are in direct competition with industrial tantalum electrolytic capacitors because niobium is more readily available. Their properties are comparable.\n\nThe electrical properties of aluminum, tantalum and niobium electrolytic capacitors have been greatly improved by the polymer electrolyte.\n\nIn order to compare the different characteristics of the different electrolytic capacitor types, capacitors with the same dimensions and of similar capacitance and voltage are compared in the following table. In such a comparison the values for ESR and ripple current load are the most important parameters for the use of electrolytic capacitors in modern electronic equipment. The lower the ESR, the higher the ripple current per volume and better functionality of the capacitor in the circuit. However, better electrical parameters come with higher prices.\n\n) Manufacturer, series name, capacitance/voltage\n\n) calculated for a capacitor 100 µF/10 V,\n\n) from a 1976 data sheet\n\nAluminum electrolytic capacitors form the bulk of the electrolytic capacitors used in electronics because of the large diversity of sizes and the inexpensive production. Tantalum electrolytic capacitors, usually used in the SMD version, have a higher specific capacitance than the aluminum electrolytic capacitors and are used in devices with limited space or flat design such as laptops. They are also used in military technology, mostly in axial style, hermetically sealed. Niobium electrolytic chip capacitors are a new development in the market and are intended as a replacement for tantalum electrolytic chip capacitors.\n\nThe phenomenon that in an electrochemical process, aluminum and such metals as tantalum, niobium, manganese, titanium, zinc, cadmium, etc., can form an oxide layer which blocks an electric current from flowing in one direction but which allows current to flow in the opposite direction, was first observed in 1857 by the German physicist and chemist (1805–1878). It was first put to use in 1875 by the French researcher and founder Eugène Ducretet, who coined the term \"valve metal\" for such metals.\n\nCharles Pollak (born Karol Pollak), a producer of accumulators, found out that the oxide layer on an aluminum anode remained stable in a neutral or alkaline electrolyte, even when the power was switched off. In 1896 he filed a patent for an \"Electric liquid capacitor with aluminum electrodes\" (de: \"Elektrischer Flüssigkeitskondensator mit Aluminiumelektroden\") based on his idea of using the oxide layer in a polarized capacitor in combination with a neutral or slightly alkaline electrolyte.\n\nThe first industrially realized electrolytic capacitors consisted of a metallic box used as the cathode. It was filled with a borax electrolyte dissolved in water, in which a folded aluminum anode plate was inserted. Applying a DC voltage from outside, an oxide layer was formed on the surface of the anode. The advantage of these capacitors was that they were significantly smaller and cheaper than all other capacitors at this time relative to the realized capacitance value. This construction with different styles of anode construction but with a case as cathode and container for the electrolyte was used up to the 1930s and was called a \"wet\" electrolytic capacitor, in the sense of its having a high water content.\n\nThe first more common application of wet aluminum electrolytic capacitors was in large telephone exchanges, to reduce relay hash (noise) on the 48 volt DC power supply. The development of AC-operated domestic radio receivers in the late 1920s created a demand for large-capacitance (for the time) and high-voltage capacitors for the valve amplifier technique, typically at least 4 microfarads and rated at around 500 volts DC. Waxed paper and oiled silk film capacitors were available, but devices with that order of capacitance and voltage rating were bulky and prohibitively expensive.\n\nThe ancestor of the modern electrolytic capacitor was patented by Samuel Ruben in 1925, who teamed with Philip Mallory, the founder of the battery company that is now known as Duracell International. Ruben's idea adopted the stacked construction of a silver mica capacitor. He introduced a separated second foil to contact the electrolyte adjacent to the anode foil instead of using the electrolyte-filled container as the capacitor's cathode. The stacked second foil got its own terminal additional to the anode terminal and the container no longer had an electrical function. This type of electrolytic capacitor combined with an liquid or gel-like electrolyte of a non-aqueous nature, which is therefore dry in the sense of having a very low water content, became known as the \"dry\" type of electrolytic capacitor.\n\nWith Ruben's invention, together with the invention of wound foils separated with a paper spacer 1927 by A. Eckel of Hydra-Werke (Germany), the actual development of e-caps began.\n\nWilliam Dubilier, whose first patent for electrolytic capacitors was filed in 1928, industrialized the new ideas for electrolytic capacitors and started the first large commercial production in 1931 in the Cornell-Dubilier (CD) factory in Plainfield, New Jersey. At the same time in Berlin, Germany, the \"Hydra-Werke\", an AEG company, started the production of e-caps in large quantities.\nIn his 1896 patent Pollak already recognized that the capacitance of the capacitor increases when roughening the surface of the anode foil. Today (2014), electrochemically etched low voltage foils can achieve an up to 200-fold increase in surface area compared to a smooth surface. Advances in the etching process are the reason for the dimension reductions in aluminum electrolytic capacitors over recent decades.\n\nFor aluminum electrolytic capacitors the decades from 1970 to 1990 were marked by the development of various new professional series specifically suited to certain industrial applications, for example with very low leakage currents or with long life characteristics, or for higher temperatures up to 125 °C.\n\nOne of the first tantalum electrolytic capacitors were developed in 1930 by Tansitor Electronic Inc. USA, for military purposes. The basic construction of a wound cell was adopted and a tantalum anode foil was used together with a tantalum cathode foil, separated with a paper spacer impregnated with a liquid electrolyte, mostly sulfuric acid, and encapsulated in a silver case.\n\nThe relevant development of solid electrolyte tantalum capacitors began some years after William Shockley, John Bardeen and Walter Houser Brattain invented the transistor in 1947. It was invented by Bell Laboratories in the early 1950s as a miniaturized, more reliable low-voltage support capacitor to complement their newly invented transistor. The solution found by R. L. Taylor and H. E. Haring of the Bell labs in early 1950 was based on experiences with ceramics. They ground tantalum to a powder, which they pressed into a cylindrical form and then sintered at high a temperature between 1500 and 2000 °C under vacuum conditions to produce a pellet (\"slug\").\n\nThese first sintered tantalum capacitors used a non-solid electrolyte, which does not fit the concept of solid electronics. In 1952 a targeted search at Bell Labs by D. A. McLean and F. S. Power for a solid electrolyte led to the invention of manganese dioxide as a solid electrolyte for a sintered tantalum capacitor.\n\nAlthough fundamental inventions came from Bell Labs, the inventions for manufacturing commercially viable tantalum electrolytic capacitors came from researchers at the Sprague Electric Company. Preston Robinson, Sprague's Director of Research, is considered to be the actual inventor of tantalum capacitors in 1954. His invention was supported by R. J. Millard, who introduced the \"reform\" step in 1955, a significant improvement in which the dielectric of the capacitor was repaired after each dip-and-convert cycle of MnO deposition, which dramatically reduced the leakage current of the finished capacitors.\n\nAlthough solid tantalum capacitors offered capacitors with lower ESR and leakage current values than the aluminum e-caps, 1980 a price shock for tantalum dramatically reduced the applications of Ta-e-caps especially in the entertainment industry. The industry switched back to using aluminum electrolytic capacitors.\n\nThe first solid electrolyte of manganese dioxide developed 1952 for tantalum capacitors had a conductivity 10 times better than all other types of non-solid electrolytes. It also influenced the development of aluminum electrolytic capacitors. In 1964 the first aluminum electrolytic capacitors with solid electrolyte SAL electrolytic capacitor came on the market, developed by Philips.\n\nWith the beginning of digitalization, Intel launched in 1971 its first microcomputer, MCS 4, and in 1972 Hewlett Packard launched one of the first pocket calculators, HP 35. The requirements for capacitors increased in terms of lowering the equivalent series resistance (ESR) for bypass and decoupling capacitors. The manganese dioxide type of electrolyte should be better.\n\nIt was not until 1983 when a new step toward ESR reduction was taken by Sanyo with its \"OS-CON\" aluminum electrolytic capacitors. These capacitors used a solid organic conductor, the charge transfer salt TTF-TCNQ (tetracyanoquinodimethane), which provided an improvement in conductivity by a factor of 10 compared with the manganese dioxide electrolyte.\nThe next step in ESR reduction was the development of conducting polymers by Alan J. Heeger, Alan MacDiarmid and Hideki Shirakawa in 1975. The conductivity of conductive polymers such as polypyrrole (PPy) or PEDOT is better than that of TCNQ by a factor of 100 to 500, and close to the conductivity of metals.\n\nIn 1991 Panasonic came on the market with its \"SP-Cap\", called polymer aluminum electrolytic capacitors. These aluminum electrolytic capacitors with polymer electrolytes reached very low ESR values directly comparable to ceramic multilayer capacitors (MLCCs). They were still less expensive than tantalum capacitors and with their flat design for laptops and cell phones competed with tantalum chip capacitors as well.\n\nTantalum electrolytic capacitors with PPy polymer electrolyte cathode followed three years later. In 1993 NEC introduced its SMD polymer tantalum electrolytic capacitors, called \"NeoCap\". In 1997 Sanyo followed with the \"POSCAP\" polymer tantalum chips.\n\nA new conductive polymer for tantalum polymer capacitors was presented by Kemet at the \"1999 Carts\" conference. This capacitor used the newly developed organic conductive polymer PEDT Poly(3,4-ethylenedioxythiophene), also known as PEDOT (trade name Baytron®) \n\nAnother price explosion for tantalum in 2000/2001 forced the development of niobium electrolytic capacitors with manganese dioxide electrolyte, which have been available since 2002. Niobium is a sister metal to tantalum and serves as valve metal generating an oxide layer during anodic oxidation. Niobium as raw material is much more abundant in nature than tantalum and is less expensive. It was a question of the availability of the base metal in the late 1960s which led to development and implementation of niobium electrolytic capacitors in the former Soviet Union instead of tantalum capacitors as in the West. The materials and processes used to produce niobium-dielectric capacitors are essentially the same as for existing tantalum-dielectric capacitors. The characteristics of niobium electrolytic capacitors and tantalum electrolytic capacitors are roughly comparable.\n\nWith the goal of reducing ESR for inexpensive non-solid e-caps from the mid-1980s in Japan, new water-based electrolytes for aluminum electrolytic capacitors were developed. Water is inexpensive, an effective solvent for electrolytes, and significantly improves the conductivity of the electrolyte. The Japanese manufacturer Rubycon was a leader in the development of new water-based electrolyte systems with enhanced conductivity in the late 1990s. The new series of non-solid e-caps with water-based electrolyte was described in the data sheets as having \"low-ESR\", \"low-impedance\", \"ultra-low-impedance\" or \"high-ripple current\".\n\nA stolen recipe for such a water-based electrolyte, in which important stabilizing substances were absent, led in the years 2000 to 2005 to the problem of mass-bursting capacitors in computers and power supplies, which became known under the term \"capacitor plague\". In these e-caps the water reacts quite aggressively and even violently with aluminum, accompanied by strong heat and gas development in the capacitor, and often led to the explosion of the capacitor.\n\nThe electrical characteristics of capacitors are harmonized by the international generic specification IEC 60384-1. In this standard, the electrical characteristics of capacitors are described by an idealized series-equivalent circuit with electrical components which model all ohmic losses, capacitive and inductive parameters of an electrolytic capacitor:\n\nThe electrical characteristics of electrolytic capacitors depend on the structure of the anode and the electrolyte used. This influences the capacitance value of electrolytic capacitors, which depends on measuring frequency and temperature. Electrolytic capacitors with non-solid electrolytes show a broader aberration over frequency and temperature ranges than do capacitors with solid electrolytes.\n\nThe basic unit of an electrolytic capacitor's capacitance is the microfarad (μF). The capacitance value specified in the data sheets of the manufacturers is called the rated capacitance C or nominal capacitance C and is the value for which the capacitor has been designed.\n\nThe standardized measuring condition for e-caps is an AC measuring method with 0.5 V at a frequency of 100/120 Hz and a temperature of 20 °C. For tantalum capacitors a DC bias voltage of 1.1 to 1.5  V for types with a rated voltage ≤2.5 V, or 2.1 to 2.5 V for types with a rated voltage of >2.5 V, may be applied during the measurement to avoid reverse voltage.\n\nThe capacitance value measured at the frequency of 1 kHz is about 10% less than the 100/120 Hz value. Therefore, the capacitance values of electrolytic capacitors are not directly comparable and differ from those of film capacitors or ceramic capacitors, whose capacitance is measured at 1 kHz or higher.\n\nMeasured with an AC measuring method with 100/120 Hz the capacitance value is the closest value to the electrical charge stored in the e-caps. The stored charge is measured with a special discharge method and is called the DC capacitance. The DC capacitance is about 10% higher than the 100/120 Hz AC capacitance. The DC capacitance is of interest for discharge applications like photoflash.\n\nThe percentage of allowed deviation of the measured capacitance from the rated value is called the capacitance tolerance. Electrolytic capacitors are available in different tolerance series, whose values are specified in the E series specified in IEC 60063. For abbreviated marking in tight spaces, a letter code for each tolerance is specified in IEC 60062.\n\nThe required capacitance tolerance is determined by the particular application. Electrolytic capacitors, which are often used for filtering and bypassing, do not have the need for narrow tolerances because they are mostly not used for accurate frequency applications like in oscillators.\n\nReferring to the IEC/EN 60384-1 standard, the allowed operating voltage for electrolytic capacitors is called the \"rated voltage U\" or \"nominal voltage U\". The rated voltage U is the maximum DC voltage or peak pulse voltage that may be applied continuously at any temperature within the rated temperature range T.\n\nThe voltage proof of electrolytic capacitors decreases with increasing temperature. For some applications it is important to use a higher temperature range. Lowering the voltage applied at a higher temperature maintains safety margins. For some capacitor types therefore the IEC standard specifies a \"temperature derated voltage\" for a higher temperature, the \"category voltage U\". The category voltage is the maximum DC voltage or peak pulse voltage that may be applied continuously to a capacitor at any temperature within the category temperature range T. The relation between both voltages and temperatures is given in the picture at right.\n\nApplying a higher voltage than specified may destroy electrolytic capacitors.\n\nApplying a lower voltage may have a positive influence on electrolytic capacitors. For aluminum electrolytic capacitors a lower applied voltage can in some cases extend the lifetime. For tantalum electrolytic capacitors lowering the voltage applied increases the reliability and reduces the expected failure rate.\nI\n\nThe surge voltage indicates the maximum peak voltage value that may be applied to electrolytic capacitors during their application for a limited number of cycles.\nThe surge voltage is standardized in IEC/EN 60384-1. For aluminum electrolytic capacitors with a rated voltage of up to 315 V, the surge voltage is 1.15 times the rated voltage, and for capacitors with a rated voltage exceeding 315 V, the surge voltage is 1.10 times the rated voltage.\n\nFor tantalum electrolytic capacitors the surge voltage can be 1.3 times the rated voltage, rounded off to the nearest volt. The surge voltage applied to tantalum capacitors may influence the capacitor's failure rate.\n\nAluminum electrolytic capacitors with non-solid electrolyte are relatively insensitive to high and short-term transient voltages higher than surge voltage, if the frequency and the energy content of the transients are low. This ability depends on rated voltage and component size. Low energy transient voltages lead to a voltage limitation similar to a zener diode. An unambiguous and general specification of tolerable transients or peak voltages is not possible. In every case transients arise, the application has to be approved very carefully.\n\nElectrolytic capacitors with solid manganese oxide or polymer electrolyte, and aluminum as well as tantalum electrolytic capacitors can not withstand transients or peak voltages higher than surge voltage. Transients for this type of e-caps may destroy the components.\n\nStandard electrolytic capacitors, and aluminum as well as tantalum and niobium electrolytic capacitors are polarized and generally require the anode electrode voltage to be positive relative to the cathode voltage.\n\nNevertheless, electrolytic capacitors can withstand for short instants a reverse voltage for a limited number of cycles. In detail, aluminum electrolytic capacitors with non-solid electrolyte can withstand a reverse voltage of about 1 V to 1.5 V. This reverse voltage should never be used to determine the maximum reverse voltage under which a capacitor can be used permanently.\n\nSolid tantalum capacitors can also withstand reverse voltages for short periods. The most common guidelines for tantalum reverse voltage are:\nThese guidelines apply for short excursion and should never be used to determine the maximum reverse voltage under which a capacitor can be used permanently.\n\nBut in no case, for aluminum as well as for tantalum and niobium electrolytic capacitors, may a reverse voltage be used for a permanent AC application.\n\nTo minimize the likelihood of a polarized electrolytic being incorrectly inserted into a circuit, polarity has to be very clearly indicated on the case, see the section on \"Polarity marking\" below.\n\nSpecial bipolar aluminum electrolytic capacitors designed for bipolar operation are available, and usually referred to as \"non-polarized\" or \"bipolar\" types. In these, the capacitors have two anode foils with full-thickness oxide layers connected in reverse polarity. On the alternate halves of the AC cycles, one of the oxides on the foil acts as a blocking dielectric, preventing reverse current from damaging the electrolyte of the other one. But these bipolar electrolytic capacitors are not adaptable for main AC applications instead of power capacitors with metallized polymer film or paper dielectric.\n\nIn general, a capacitor is seen as a storage component for electric energy. But this is only one capacitor function. A capacitor can also act as an AC resistor. Especially aluminum electrolytic capacitors in many applications are used as decoupling capacitors to filter or bypass undesired biased AC frequencies to the ground or for capacitive coupling of audio AC signals. Then the dielectric is used only for blocking DC. For such applications the AC resistance, the impedance, is as important as the capacitance value.\n\nThe impedance \"Z\" is the vector sum of reactance and resistance; it describes the phase difference and the ratio of amplitudes between sinusoidally varying voltage and sinusoidally varying current at a given frequency. In this sense impedance is a measure of the ability of the capacitor to pass alternating currents and can be used like Ohm's law.\n\nIn other words, the impedance is a frequency-dependent AC resistance and possesses both magnitude and phase at a particular frequency.\n\nIn data sheets of electrolytic capacitors only the impedance magnitude \"|Z|\" is specified, and simply written as \"\"Z\".\" Regarding the IEC/EN 60384-1 standard, the impedance values of electrolytic capacitors are measured and specified at 10 kHz or 100 kHz depending on the capacitance and voltage of the capacitor.\n\nBesides measuring, the impedance can be calculated using the idealized components of a capacitor's series-equivalent circuit, including an ideal capacitor \"C\", a resistor \"ESR\", and an inductance \"ESL\". In this case the impedance at the angular frequency \"ω\" is given by the geometric (complex) addition of \"ESR\", by a capacitive reactance \"X\"\n\nand by an inductive reactance \"X\" (Inductance)\n\nformula_4.\n\nThen \"Z\" is given by\n"}
{"id": "22191645", "url": "https://en.wikipedia.org/wiki?curid=22191645", "title": "Elektra Birseck Münchenstein", "text": "Elektra Birseck Münchenstein\n\nEBM \"(Cooperative Elektra Birseck, Münchenstein)\" is a Swiss energy supplier with head office in Münchenstein. It was founded as a cooperative under private law in 1897. EBM supplies around 230,000 people with electricity in North-West Switzerland and Alsace. The company operates 167 \"local heat supply\" systems in Switzerland, Alsace and South Germany.\n\nThe engineer Fritz Eckinger and politician Stephan Gschwind founded \"Elektra Birseck Münchenstein\" in 1897 with the intention of introducing electric lighting and the idea of organising a company as a cooperative, which was a new concept at the time. The supply area in the lower part of the Canton of Basel-Country and Birseck-Dorneck, a part of the Canton of Solothurn, was soon expanded. Between 1906 and 1914, EBM connected eleven municipalities and the City of Saint-Louis (Haut Rhin) to the grid. Since 1921, EBM has been supplying a total of 60 municipalities with electricity in the Swiss Cantons of Basel-Landschaft and Solothurn.\n\nIn 1979, it became the first company in Switzerland to support the frugal and rational use of energy. EBM implemented the first tariffs for promoting renewable energies and set up an energy and environmental advice centre for its customers. In the field of decentralised heating supplies, EBM developed the concepts «cogeneration plant and heat pump» at the beginning of the 1980s and constructed the first cogeneration plant in 1982.\n\nSince 1992, the company constructs, supports and participates in photovoltaic plants, to supply solar energy at a favourable price. In 1997, EBM opened an \"electricity museum\" in Münchenstein to celebrate its 100th anniversary.\n\nIn 2009, EBM started producing electricity from wind and solar power. The company founded \"aravis\" («Aravis Energy I LP») to realise large plants in Southern Europe. It is the first «Swiss Limited Partnership» (limited commercial partnership for collective capital investments) that was approved by the Swiss Federal Banking Commission. The fund was used for developing projects with a volume of CHF 200 million. EBM was the main investor with CHF 70 million. EBM’s investments helped to realise wind parks and photovoltaic plants in Italy and Spain with an installed output of around 64 Megawatt.\n\nEBM procures electricity from renewable energies from its own plants, partner plants and long-term agreements. In addition to plants in the region, specific plans are in place to realise large projects in suitable foreign locations.\n\nEBM aims to expand in Europe and Switzerland in the medium terms with the help of the companies «EBM Greenpower AG», «Leading Swiss renewables AG» and its minority share in «Kleinkraftwerk Birseck AG». The electricity generated by foreign plants is not yet supplied to Switzerland but is sold directly abroad at local supply prices or on the foreign electricity markets. The electricity generated from renewable energies rose to 811,703 MWh (as of 31 December 2013) as a result of EBM increasing the number of its plants.\n\n«Leading Swiss renewables AG» (LSR) is an investment company founded by EBM, «Energie Wasser Bern» and «Aravis» in January 2012. The company’s purpose is to develop renewable energy production plants in Europe in the medium term. It has around EUR 100 million in capital at its disposal to meet this target. EBM holds a share of EUR 65 million and «Energie Wasser Bern» a share of EUR 35 million. Plans are for the total installed output to exceed 100 Megawatt in an initial phase. The focus is on wind energy as there is an active market for wind power projects in numerous European countries.\n\nDecentralised power plants have been gaining in importance in recent years as it has become increasingly difficult to construct large power plants in Switzerland and the rest of Europe. EBM has invested in KKB AG, an independent producer of electricity generated from renewable energies. The company acquires and operates water, solar and wind power plants in Switzerland and certain European countries. KKB AG is listed at the stock exchange in Bern under securities number ISIN CH0023777235 listed.\n\nFor more than 30 years, EBM Wärme AG has been operating in the field of heat contracting throughout Switzerland. Its focus has been on the use of renewable energies for many years. In the beginning, EBM mainly realised heating networks in its Swiss grid area. Today, the company is represented in almost all of Switzerland, in the adjoining Alsace by «EBM Thermique SAS» and even in Baden-Wuerttemberg, Germany, by «EBM Wärme GmbH». In 2012, EBM operated a total of 170 plants.\n\nEBM is a cooperative under private law. The cooperative members include legal entities and private persons owning property that is connected to EBM’s grid. The company had over 50,000 cooperative members in 2013.\n\n"}
{"id": "512435", "url": "https://en.wikipedia.org/wiki?curid=512435", "title": "Ferroelectric capacitor", "text": "Ferroelectric capacitor\n\nFerroelectric capacitor is a capacitor based on a ferroelectric material. In contrast, traditional capacitors are based on dielectric materials. Ferroelectric devices are used in digital electronics as part of ferroelectric RAM, or in analog electronics as tunable capacitors (varactors).\n\nIn memory applications, the stored value of a ferroelectric capacitor is read by applying an electric field. The amount of charge needed to flip the memory cell to the opposite state is measured and the previous state of the cell is revealed. This means that the read operation destroys the memory cell state, and has to be followed by a corresponding write operation, in order to write the bit back. This makes it similar to the ferrite core memory. The requirement for a write cycle for each read cycle, together with the high but not infinite write cycle limit, sets a potential problem for some special applications.\n\nIn a short-circuited ferroelectric capacitor with a metal-ferroelectric-metal (MFM) structure, a charge distribution of screening charges forms at the metal-ferroelectric interface so as to screen the electric displacement of the ferroelectric. Due to these screening charges, there is a voltage drop across the ferroelectric capacitor with screening in the electrode layer that can be obtained using the Thomas-Fermi approach as follows:\n\nformula_1\n\nHere formula_2 is the film thickness, formula_3 and formula_4 are the electric fields in the film and electrode at the interface, formula_5 is the spontaneous polarization, formula_6, and formula_7 & formula_8 are the dielectric constants of the film and the metal electrode.\n\nWith perfect electrodes, formula_9 or for thick films, with formula_10 the equation reduces to:\n\nformula_11\n\n\n"}
{"id": "31843107", "url": "https://en.wikipedia.org/wiki?curid=31843107", "title": "Ferroelectric ceramics", "text": "Ferroelectric ceramics\n\nFerroelectric ceramics is a special group of minerals that have ferroelectric properties: the strong dependence of the dielectric constant of temperature, electrical field, the presence of hysteresis and others.\n\nThe first widespread ferroelectric ceramics material, which had ferroelectric properties not only in the form of a single crystal, but in the polycrystalline state, i.e. in the form of ceramic barium titanate was BaO · TiO2, which is important now. Add to it some m-Liv not significantly change its properties. A significant nonlinearity of capacitance capacitor having ferroelectric ceramics materials, so-called varikondy, types of VC-1 VC-2, VC-3 and others.\n\n"}
{"id": "481098", "url": "https://en.wikipedia.org/wiki?curid=481098", "title": "Firewood", "text": "Firewood\n\nFirewood is any wooden material that is gathered and used for fuel. Generally, firewood is not highly processed and is in some sort of recognizable log or branch form, compared to other forms of wood fuel like pellets or chips. Firewood can be seasoned (dry) or unseasoned (fresh/wet). It is generally classified as hardwood or softwood.\n\nFirewood is a renewable resource. However, demand for this fuel can outpace its ability to regenerate on a local or regional level. Good forestry practices and improvements in devices that use firewood can improve local wood supplies.\n\nHarvesting or collecting firewood varies by the region and culture. Some places have specific areas for firewood collection. Other places may integrate the collection of firewood in the cycle of preparing a plot of land to grow food as part of a field rotation process. Collection can be a group, family or an individual activity. The tools and methods for harvesting firewood are diverse.\n\nSome firewood is harvested in \"woodlots\" managed for that purpose, but in heavily wooded areas it is more usually harvested as a byproduct of natural forests. Deadfall that has not started to rot is preferred, since it is already partly seasoned. Standing dead timber is considered better still, for it has less humid organic material on the trunk, allowing tools to stay sharper longer, as well as being both seasoned and less rotten. Harvesting this form of timber reduces the speed and intensity of bushfires, but it also reduces habitat for snag-nesting animals such as owls and some rodents.\n\nHarvesting timber for firewood is normally carried out by hand with chainsaws. Thus, longer pieces – requiring less manual labour, and less chainsaw fuel – are less expensive and only limited by the size of the firebox. In most of the United States, the standard measure of firewood is a cord or , however, firewood can also be sold by weight. The BTU value can affect the price. Prices also vary considerably with the distance from wood lots, and quality of the wood.\n\nBuying and burning firewood that was cut only a short distance from its final destination prevents the accidental spread of invasive tree-killing insects and diseases.\n\nIn most parts of the world, firewood is only prepared for transport at the time it is harvested. Then it is moved closer to the place it will be used as fuel and prepared there. The process of making charcoal from firewood can take place at the place the firewood is harvested.\n\nMost firewood also requires splitting, which also allows for faster seasoning by exposing more surface area. Today most splitting is done with a hydraulic splitting machine, but it can also be split with a splitting maul. More unusual, and dangerous, is a tapered screw-style design, that augers into the wood, splitting it, and can be powered by either a power take-off drive, a dedicated internal combustion engine, or a rugged electric pipe-threading machine, which is safer than the other power sources because the power can be shut off more easily if necessary. Another method is to use a kinetic log splitter, which uses a rack and pinion system powered by a small motor and a large flywheel used for energy storage.\n\nThere are many ways to store firewood. These range from simple piles to free-standing stacks, to specialized structures. Usually the goal of storing wood is to keep water away from it and to continue the drying process.\n\nStacks: The simplest stack is where logs are placed next to and on top of each other, forming a line the width of the logs. The height of the stack can vary, generally depending upon how the ends are constructed. Without constructing ends, the length of the log and length of the pile help determine the height of a free-standing stack.\n\nThere is debate about whether wood will dry more quickly when covered. There is a trade-off between the surface of the wood getting wet vs. allowing as much wind and sun as possible to access the stack. A cover can be almost any material that sheds water – a large piece of plywood, sheet metal, terracotta tiles, or an oiled canvas cloth, even cheap plastic sheeting may also be used. Wood will not dry when \"completely\" enclosed. Ideally pallets or scrap wood should be used to raise the wood from the ground, reducing rot and increasing air flow.\n\nThere are many ways to create the ends of a stack. In some areas, a crib end is created by alternating pairs of logs to help stabilize the end. A stake or pole placed in the ground is another way to end the pile. A series of stacked logs at the end, each with a cord tied to it and the free end of the cord wrapped to log in the middle of the pile, is another way.\n\nUnder a roof: Under a roof, there are no concerns about the wood being subjected to rain, snow or run-off, but ventilation needs to be provided if the wood is stored green so that moisture released from the wood does not recondense inside. The methods for stacking depend on the structure and layout desired. Whether split, or in 'rounds' (flush-cut and unsplit segments of logs), the wood should be stacked lengthwise, which is the most stable and practical method. Again though, if the wood needs further seasoning there should be adequate air flow through the stack.\n\nStoring outdoors: Firewood should be stacked with the bark facing upwards. This allows the water to drain off, and standing frost, ice, or snow to be kept from the wood. Storing wood in close proximity to a dwelling increases the likelihood that insects such as termites can become established indoors.\n\nStoring firewood indoors for any extended period of time is not recommended, for it increases the risk of introducing insects such as termites into the home.\n\nRound stacks can be made many ways. Some are piles of wood with a stacked circular wall around them. Others like the American Holz Hausen are more complicated.\n\nThe moisture content of firewood determines how it burns and how much heat is released. Unseasoned (green) wood moisture content varies by the species; green wood may weigh 70 to 100 percent more than seasoned wood due to water content. Typically, seasoned (dry) wood has 20% to 25% moisture content. Use of the lower heating value is advised as a reasonable standard way of reporting this data.\n\nThe energy content of a measure of wood depends on the tree species. For example, it can range from per cord. The higher the moisture content, the more energy that must be used to evaporate (boil) the water in the wood before it will burn. Dry wood delivers more energy for heating than green wood of the same species.\n\nThe Sustainable Energy Development Office (SEDO), part of the Government of Western Australia states that the energy content of wood is 4.5 kWh/kg or 16.2 gigajoules/tonne (GJ/t).\n\nHere are some examples of energy content of several species of wood:\n\nThe process of kiln drying firewood was invented by Anthony Cutara, for which a successful US patent was filed in 1983. In 1987 the US Department of Agriculture replicated the method and published a detailed procedure for the production of kiln dried firewood, citing the higher heat output and increased combustion efficiency as a key benefit of the process.\n\nUsually firewood is sold by volume. While a specific volume term may be used, there can be a wide variation in what this means and what the measure can produce as a fuel. For example, a cord which is made from logs will not be a cord when it has been cut into 1 foot logs and then split so each piece will fit through a circle. A measure of green unseasoned wood with 65% moisture contains less usable energy than when it has been dried to 20%. Regardless of the term, firewood measurement is best thought of as an estimate.\n\nIn the metric system, firewood is usually sold by the stère, equivalent to a volume of 1 cubic meter (). The most common firewood piece length are 33 cm and 50 cm. Wood can also be sold by the kilogram or by the metric tonne, as in Australia.\n\nIn the United States and Canada, firewood is usually sold by the full cord, face cord or bag.\n\n\nIn Norway, the non-fiction book \"Hel Ved\" (In English: \"Solid Wood: All About Chopping, Drying and Stacking Wood – and the Soul of Wood-Burning\") by Lars Mytting became a bestseller in 2011–2012, selling 150,000 copies. A version of the book has also been published in Sweden, selling 50,000 copies.\n\nIn February 2013, the Norwegian state broadcast NRK sent a 12-hour live program on the topic of woodfire, where a large part of the program consisted of showing firewood burning in a fireplace. More than one million people, 20% of Norway's population, saw part of the program.\n\n\n"}
{"id": "11519537", "url": "https://en.wikipedia.org/wiki?curid=11519537", "title": "Fuel-management systems", "text": "Fuel-management systems\n\nFuel-management systems are used to maintain, control and monitor fuel consumption and stock in any type of industry that uses transport, including rail, road, water and air, as a means of business.\nFuel-management systems are designed to effectively measure and manage the use of fuel within the transportation and construction industries. They are typically used for fleets of vehicles, including railway vehicles and aircraft, as well as any vehicle that requires fuel to operate. They employ various methods and technologies to monitor and track fuel inventories, fuel purchases and fuel dispensed. This information can be then stored in computerized systems and reports generated with data to inform management practices. Online fuel management is provided through the use of web portals to provide detailed fueling data, usually vis a vis the back end of an automated fuel-management system. This enables consumption control, cost analysis and tax accounting for fuel purchases.\nThere are several types of fuel-management systems. Card-based fuel-management systems typically track fuel transactions based on a fueling credit card and the associated driver PIN. Reports can then be generated based on fuel consumption by driver, and data can be directly downloaded. On-site fuel-management systems may employ fleet refueling services or bulk fuel tanks at the site. Fuel is tracked as it is pumped into vehicles, and on-site storage levels can be managed.\nSome fuel companies offer total fuel-management systems whereby they provide elements of a card-based system along with on-site fuel delivery and refueling services. Mobile fuel management refers to a fleet of fuel trucks or tankers which provide fuel supply to commercial fleets of trucks or construction equipment. May involve combining RFID technology to identify equipment and automated fuel management to append the details of each transaction to a unique piece of equipment. By refueling vehicles in the evening when they are not in use, the company can conserve man-hours as the operators do not refuel and the vehicles do not require additional fuel to travel to the refueling station. They may also employ more sophisticated systems that utilize remote data collection to gather specific technical information about the vehicle usage and performance characteristics such as mileage, hours of operation and engine idling time.\n\nThe increasing use of biofuel has introduced another challenge in fuel management. With greater water content, there will be a risk of microbial growth – depending on the storage conditions, the fuel quality will deteriorate over time, leading to clogged filters and loss of productivity.\n\nTank manufacturers have introduced fuel filtering and cleansing packs which recirculate the tank contents through a series of filters and ultraviolet treatment to kill bacteria. Data from fuel quality instrumentation can be streamed to allow remote monitoring over Internet connections.\n\nThere have been, to date, four recognizable generations of fuel-management system:\n\nFirst generation : A bank of a number of electromechanical counters, pulsed by a shaft-driven encoder fitted to the pump. The correct counter is selected by the use of an encoded key. These types of systems were available throughout the 1960s superseded by more sophisticated systems in the late 1970s.\n\nSecond generation : A self-contained, electronic and/or microprocessor-controlled fuel-island control systems which has an ID reader (key, card, RFID etc.) to identify the vehicle and driver, a means of controlling a pump, a means of measuring the fuel delivered, and usually, a means of reporting fuel drawn by a vehicle. The fleet list is usually input using an integral keypad or an office based console. These systems were either fitted with integral printers or permanently hard-wired to back office consoles that provided simple reporting and printouts, these system types were superseded by the proliferation of low cost PC's.\n\nThird generation : A fuel-island control system similar to a second-generation system, which is either periodically, or permanently connected to a PC which is used to report on the fuellings and input the fleet information. These systems also provided the first \"networked\" systems, usually fitted with a dial up modem within the island terminal, networks could be polled around usually at 12pm onwards to download the days transaction to a central PC and controller.\n\nFourth generation : The fuel-island controller is fully connected directly to a central Internet-based server which is updated in real time. All fleet information and transactions are held on the central server. Connection is made from the fuel island to the server using GPRS, or can use the operators own network using a Wi-Fi or Cabled Network Link. Continuous Internet connection can not be guaranteed and hence any fourth-generation system must have a fall-back white/black list, usually built in real time from previous authorisations.\n\nThe principal advantages of a real-time system are that site operation can be monitored in real time, stock figures are always current, and, with integrated tank gauging, fuel theft from tanks and short deliveries can be identified immediately.\n\nThe advent of real-time systems has much reduced the requirements of having printed reports, usually circulated and ignored, in favour of users looking at live and current data presented as and when they need it. Having users interact with live and relevant data, rather than simply viewing lists of out of date information encourages a more active view of fuel management than was previously possible, so that active, timely interventions take place generating fuel savings - which is the entire point of the system. The challenge facing manufacturers currently is to make real time data analysis tools which are relevant to the industry and are quick and simple to use by any operator with no technical background.\n\nTo identify the vehicle/equipment being fuelled, some sort of ID token is normally used. On the most simple systems, this may be an ID or registration number typed in through a keypad, but as this is open to abuse, offering no real fuel security, a physical token is most often used. Some of the most common are listed here :\n\nRFID Tags : By far the most common type of ID token as it is the most reliable in the sometimes harsh environment where fuelling takes place. Using an RFID tag means there are no openings required in the fuel-management terminal and hence best protection from water or dust ingress. RFID tags are low cost and very reliable and the reader requires no ongoing maintenance.\n\nMagnetic Cards : Often seen as a saving over purchasing RFID tags, a fleet already using fuel cards will use these same cards at the fuel-island terminal. The exposed nature of most fuel islands is not the ideal environment for the use of fuel cards so reliability may be compromised and hence perceived savings not achieved. If the readers are regularly cleaned, then an acceptable level of reliability may be achieved.\n\nDallas Touch Keys / IButtons : A popular alternative to RFID tags, these keys have two electrical contacts which need to be touched to a reader. These keys are very reliable with only minimal maintenance of the key reader required.\n\nNozzle Based Technologies : In this system, the fuelling nozzle has a reader mounted on it, or integrated within the nozzle itself. When the refuelling nozzle is inserted into the vehicle filler neck, or connected as part of a dry break fuel system, the vehicle ID is read. This vehicle identification is then transmitted back to the FM terminal using wires or RFID technology. The advantage of this type of system is that the fuelling nozzle must be fully inserted or connected to the vehicle before fuel starts to flow, and fuelling stops if the nozzle is removed, making on-site fuel theft much more difficult. Unfortunately, it does nothing to stop off-site fuel theft, and so should be coupled with anti-syphoning technologies to complete the system.\n\nHand Held Scanners : These are especially useful for Mobile Refuelling Solutions where the fueller is some distance from the pumping unit (eg Bowser) when fuelling takes place. The Hand Held Unit has either an RFID, Near field or Barcode Reader to read a tag or barcode permanently attached to the equipment to be fuelled. The HHU then uses a radio link to transmit the ID data back to the bowser where it is validated and the pump started. The fueller is at the point of delivery and hence can control the hose, minimising spillage issues.\n\nBluetooth technology: Vehicles may be retrofitted with a Bluetooth transponder to allow for both vehicle identification and data transfer from the vehicle's CAN network.\n\nFuel-oil management system (FOMS) is a recent development in the field of electric power by which the fuel oil level in any power plant or any industry can be monitored and controlled using programmable logic controller and supervisory control and data acquisition. TrackME Guam! Fleet and Fuel Management system is a great example of software.\n\nFuel oil\n"}
{"id": "2055966", "url": "https://en.wikipedia.org/wiki?curid=2055966", "title": "Glassy carbon", "text": "Glassy carbon\n\nGlass-like carbon, often called glassy carbon or vitreous carbon, is a non-graphitizing, or nongraphitizable, carbon which combines glassy and ceramic properties with those of graphite. The most important properties are high temperature resistance, hardness (7 Mohs), low density, low electrical resistance, low friction, low thermal resistance, extreme resistance to chemical attack and impermeability to gases and liquids. Glassy carbon is widely used as an electrode material in electrochemistry, as well as for high temperature crucibles and as a component of some prosthetic devices, and can be fabricated as different shapes, sizes and sections.\n\nThe names \"glassy carbon\" and \"vitreous carbon\" have been introduced as trademarks; therefore, IUPAC does not recommend their use as technical terms.\n\nVitreous carbon can also be produced as a foam. It is then called reticulated vitreous carbon (RVC). This foam was first developed in the mid to late 1960s as a thermally insulating, microporous glassy carbon electrode material. RVC foam is a strong, inert, electrically and thermally conductive, and corrosion resistant porous form of carbon with a low resistance to gas and fluid flow. Due to these characteristics, the most widespread scientific use of RVC is as three-dimensional electrode in electrochemistry. Additionally, RVC foams are characterized by an exceptionally high void volume, high surface area, and very high thermal resistance in non-oxidising environments, which allows for heat sterilization and facilitates manipulation in biological applications.\n\nGlassy carbon was first observed in the laboratories of The Carborundum Company, Manchester, UK, in the mid-1950s by Bernard Redfern, a materials scientist and diamond technologist. He noticed that Sellotape he used to hold ceramic (rocket nozzle) samples in a furnace maintained a sort of structural identity after firing in an inert atmosphere.\nHe searched for a polymer matrix to mirror a diamond structure and discovered a resole resin that would, with special preparation, set without a catalyst. Using this phenolic resin, crucibles were produced. Crucibles were distributed to organisations such as UKAEA Harwell.\n\nBernard Redfern left The Carborundum Co., which officially wrote off all interests in the glassy carbon invention. While working at the Plessey Company laboratory (in a disused church) in Towcester, UK, Redfern received a glassy carbon crucible for duplication from UKAEA. He identified it as one he had made from markings he had engraved into the uncured precursor prior to carbonisation. (It is almost impossible to engrave the finished product.) The Plessey Company set up a laboratory first in a factory previously used to make briar pipes, in Litchborough, UK, and then a permanent facility at Caswell, near Blakesly, UK. Caswell became the Plessey Research Centre and then the Allen Clark Research Centre. Glassy carbon arrived at the Plessey Company Limited as a fait accompli. Redfern was assigned J.C. Lewis, as a laboratory assistant, for the production of glassy carbon. F.C. Cowlard was assigned to Redfern's department later, as a laboratory administrator. Cowlard was an administrator who previously had some association with Silane (Silane US Patent assignee 3,155,621 3 Nov 1964). Neither he nor Lewis had any previous connection with glassy carbon.\nThe contribution of Bernard Redfern to the invention and production of glassy / Vitreous carbon is acknowledged by his co-authorship of early articles. But references to Redfern were not obvious in subsequent publications by Cowlard and Lewis. Original boat crucibles, thick section rods and precursor samples exist.\n\nRedfern's UK patent application were filed on 11 January 1960 and Bernard Redfern was the author of US patent US3109712A, granted 5 November 1963, priority date 11 January 1960, filing date 9 January 1961. This came after the rescinded British patent.\nThis prior art is not referenced in US patent 4,668,496, 26 May 1987 for Vitreous Carbon. Patents were filed \"Bodies and shapes of carbonaceous materials and processes for their production\" and the name \"Vitreous Carbon\" presented to the product by the son of Redfern.\n\nGlassy/Vitreous Carbon was under investigation used for components for thermonuclear detonation systems and at least some of the patents surrounding the material were rescinded (in the interests of national security) in the 1960s.\n\nLarge sections of the precursor material were produced as castings, moldings or machined into a predetermined shape. Large crucibles and other forms were manufactured. Carbonisation took place in two stages. Shrinkage during this process is considerable (48.8%) but is absolutely uniform and predictable. A nut and bolt can be made to fit as the polymer, processed separately and subsequently give a perfect fit.\n\nSome of the first ultrapure samples of Gallium Arsenide were zone refined in these crucibles. (Glassy carbon is extremely pure and unreactive to GaAs).\n\nDoped/impure glassy carbon exhibited semiconductor phenomena.\n\nUranium carbide inclusions were fabricated (using U238 carbide at experimental scale).\n\nOn October 11, 2011, research conducted at the Carnegie Geophysical Laboratory led by Stanford’s Wendy L. Mao and her graduate student Yu Lin described a new form of glassy carbon formed under high pressure with hardness equal to diamond, a kind of diamond-like carbon. Unlike diamond, however its structure is that of amorphous carbon so its hardness may be isotropic. Research is ongoing.\n\nThe structure of glassy carbon has long been a subject of debate. Early structural models assumed that both sp- and sp-bonded atoms were present, but it is now known that glassy carbon is 100% sp. More recent research has suggested that glassy carbon has a fullerene-related structure.\n\nNote that glassy carbon should not be confused with amorphous carbon. This from IUPAC: \"Glass-like carbon cannot be described as amorphous carbon because it consists of two-dimensional structural elements and does not exhibit ‘dangling’ bonds.\"\n\nIt exhibits a conchoidal fracture.\n\nGlassy carbon electrode (GCE) in aqueous solutions is considered to be an inert electrode for hydronium ion reduction:\n\nComparable reaction on platinum:\n\nThe difference of 2.1 V is attributed to the properties of platinum which stabilizes a covalent Pt-H bond.\n\nProperties include 'high temperature resistance', hardness (7 Mohs), low density, low electrical resistance, low friction, and low thermal resistance. \n\nDue to their specific surface orientation, glassy carbon is employed as an electrode material for the fabrication of sensors. Glassy carbon paste, glassy carbon, carbon paste etc. electrodes when modified are termed as chemically modified electrodes.\n\n\n"}
{"id": "10584608", "url": "https://en.wikipedia.org/wiki?curid=10584608", "title": "Hematine", "text": "Hematine\n\nHematine (also magnetic hematite, hemalyke or hemalike) is an artificial magnetic material. Hematine is widely used in jewelry. \n\nAlthough it is claimed by many that it is made from ground hematite or iron oxide mixed with a resin, analysis has demonstrated it to be an entirely artificial compound, a barium-strontium ferrite.\n"}
{"id": "46664679", "url": "https://en.wikipedia.org/wiki?curid=46664679", "title": "Hesselbach Wind Farm", "text": "Hesselbach Wind Farm\n\nThe Hesselbach Wind Farm (also known as Windpark Hesselbach) is North Rhine-Westphalia's first forest wind farm. It is situated in the forests of Bad Laasphe-Hesselbach. It consists of eight turbines, each with 3 megawatt (MW) capacity and was opened 2013.\n\n"}
{"id": "57429688", "url": "https://en.wikipedia.org/wiki?curid=57429688", "title": "High-voltage transformer fire barriers", "text": "High-voltage transformer fire barriers\n\nHigh-voltage transformer fire barriers, or transformer firewalls, transformer ballistic firewalls, transformer blast walls, are outdoor countermeasures against cascading failures in a national electric grid. \nThe purpose of these barriers, like common fire barriers in building construction, is compartmentalisation of transformer fires, as well as transformer and bushing explosions where the fuel source of both fires and explosions is the transformer oil. \nWithout compartmentalisation, one ruptured transformer could start its neighbouring transformer on fire and thus create a domino effect that can affect the surrounding electric grid, particularly during peak times. \n\nHigh-voltage transformer fire barriers are typically located in electrical substations, but may also be attached to buildings, such as valve halls or manufacturing plants with large electrical distribution systems, such as pulp and paper mills. \nOutdoor transformer fire barriers that are attached at least on one side to a building are referred to as wing walls.\nAt times, high-voltage transformers can be located immediately outside and sometimes inside of buildings, requiring higher fire-resistance ratings than other fire compartments in a building.\n\nAs early as World War II, substations have been military targets. During WW2, Operation Josephine B the British Special Operations Executive successfully targeted a French substation with high-voltage transformers in order to disrupt the German war effort there.\nOutdoor structures, such as the transformers and fire barriers that separate them, do not constitute buildings, as defined by building codes. Building codes, therefore, do not typically apply to them, unless buildings are in close proximity and may be adversely affected by transformer ruptures. Where building code issues are inapplicable, no typical building permit is required, as would be the case for a house or office building. \nIf no building code applies, then also no fire code applies, as fire codes presume construction and approval per the local building code. \nThis is a crucial factor in terms of regulatory or judicial oversight concerning the construction and maintenance of transformer fire barriers. \n\nBuilding construction and maintenance have the benefit of a defined Authority Having Jurisdiction.\nWhen a building is designed, the architect submits drawings and specifications to the municipal or regional building department, such as the New York City Department of Buildings, along with a fee, which covers plans examinations and inspections.\nThe building department then has a plans examiner check construction documents for compliance with the applicable building code. \nHe or she may require changes before construction may proceed. \nOnce construction is underway, the building department typically has a building inspector checking on progress and compliance. \nOnce a building, or occupancy, is completed and in use, the Authority Having Jurisdiction typically changes from the building department, enforcing the building code that was in effect on the date of the building permit application, over to the fire department, and, specifically the local fire prevention officer, who is tasked to enforce the local fire code, which is based upon the building code. \nA fire prevention officer is actually a law enforcement official, who can criminally charge violators of the fire code, such as home owners or office building or plant owners. While North American outdoor transformer fire barriers use identical fire test standards to qualify fire barriers as are used in buildings (ASTM E119, UL263, CAN/ULC-S101, and, formerly, NFPA 251), and despite the fact that the national electric grid is considered critical national infrastructure, there is no governmental oversight, such as exists for any buildings. \nFor example, in buildings, it is customary to have to prove compliance with codes by means of certification listings, bounding and certification marks. This is the easiest way to communicate to an Authority Having Jurisdiction, that the item tested is identical to the item being sold and installed as intended and within the tolerances indicated in the certification listing, which summarises the test report. \nHowever, in the absence of a code for such outdoor installations, as is the case in North America, there is also no AHJ to make sure that what is being installed is verified by a third party to be fit for purpose, which leaves the property owners to perform this task. In the absence of certification listings that bound the installed configuration, complete with certification marks on the installed fire barrier, which points to the listing number, the end-user must also interpret the test report to resolve bounding issues, which requires knowledge and comprehension of test standards, and the corresponding conditions of acceptance, as well as the raw data provided by the vendor to prove compliance. Personnel familiar with electrical matters, such as high-voltage direct current, electric power transmission and transformers, who may be expected to decide on a tender for transformer fire barriers, can thus benefit from training concerning fire-resistance ratings and fire testing, which are completely different fields, that do not ordinarily cross over into the electrical realm, particularly for outdoor installations .\n\nThe primary North American document that deals with outdoor high-voltage transformer fire barriers is NFPA 850. NFPA 850 – 2015 indicates 3.3.24.2 Fire Resistance Rating. The time, in minutes or hours, that materials or assemblies have withstood a fire exposure as determined by the tests, or methodology based on testing, as mandated in NFPA 5000. NFPA 5000 states under 8.2.1.1, that the fire-resistance ratings of structural elements and building assemblies shall be determined in accordance with the prescriptive requirements of 8.2.2 based on the test procedures set forth in ASTM E 119, Standard Test Methods for Fire Tests of Building Construction and Materials, or UL 263, Standard for Fire Tests of Building Construction and Materials, or other approved test methodology or analytical methods as per §8.2.3., which refer to the use of ASCE/SFPE 29, Standard Calculation Methods for Structural Fire Protection. NFPA 850 further outlines that outdoor oil-insulated transformers should be separated from adjacent structures and from each other by firewalls, spatial separation, or other approved means for the purpose of limiting the damage and potential spread of fire from a transformer failure. 5.1.4.3 states that unless consideration of the factors in 5.1.4.2 indicates otherwise, it is recommended that any oil-insulated transformer containing 500 US gallons (1,893 L) or more of oil be separated from adjacent structures by a 2-hour–rated firewall or by spatial separation in accordance with Table 5.1.4.3. Where a firewall is provided between structures and a transformer, it should extend vertically and horizontally as indicated in Figure 5.1.4.3.\n5.1.4.4 of NFPA 850 states that unless consideration of the factors in 5.1.4.2 indicates otherwise, it is recommended that adjacent oil-insulated transformers containing 500 US gallons (1,893 litres) or more of oil be separated from each other by a 2 hour–rated fire separation or by spatial separation as per Table 5.1.4.3. When the oil containment, as illustrated in Figure 5.1.4.4, consists of a large, flat concrete containment area that holds several transformers and other equipment in it without the typical pit containment areas, specific containment features to keep the oil in one transformer from migrating to any other transformer or equipment should be provided. Subsection 5.5.7 may be used for guidance. Where a firewall is provided between transformers, it should extend at least 30cm (12\") above the top of the transformer casing and oil conservator tank and at least 61cm or 24\" beyond the width of the transformer and cooling radiators, or to the edge of the containment area, whichever is greater.\n5.1.4.5 Where a firewall is provided, it should be designed to withstand the effects of projectiles from exploding transformer bushings or lightning arresters.\nThe above results in the following checklist for compliance against NFPA 850 and its subordinate documents (NFPA 5000, ASTM E119, UL 263, as well as NFPA 251 and CAN/ULC S101, all of which are identical, and represent the sum of North American wall fire testing standards)\n\n\nNFPA 850 recommends the following additional test criterion during a fire event:\nAnecdotal evidence from a transformer fire and explosion at Sinatra station, on the Las Vegas strip indicates flying shrapnel, causing bodily injury as well as debris being thrown onto nearby Interstate 15 as well as porcelain shards being embedded in a fire barrier, which would validate the NFPA 850 requirement above via anecdotal evidence.\n\nNFPA 850 recommends the use of the cellulosic time/temperature curve, which is over 100 years old and is based upon burning wood, as that was a predominant building material and furniture material in buildings at the time. It is a severe fire test. However, the fuel in transformer fires and explosions, is transformer oil. Oil is a hydrocarbon. Hydrocarbons burn more rapidly and release more heat faster than the same quantity of wood. Therefore, a comparison of the ASTM E119 and UL 1709 time/temperature curves suggests the continued use standard wall fire test procedures in ASTM E119, as recommended in NFPA 850, as this applies to non-loadbearing walls, regardless of which time/temperature curve is employed, but to substitute a hydrocarbon time/temperature curve, because transformers are filled with oil, not wood, which explains the different heat signatures.\n\nAny reference to hydrocarbon fire testing, particularly in North America, implies the use of the UL 1709 time/temperature curve, which inherently also implies the use of UL1709’s provisions or furnace instrumentation. In France, and French-influenced countries, it could also mean the use of the modified French hydrocarbon curve, using fast response furnace thermocouples. Where any walls are concerned, including high-voltage transformer fire barrier walls, particularly with the use of the terms \"fire-resistance\" or \"fire-resistant\", what is inherently implied in North America, is the use of one of the identical fire test standards that are used to evaluate non-loadbearing wall assemblies, including and limited to NFPA 251, ASTM E119, UL 263 and/or CAN/ULC-S101. European standards, such as ISO 834, DIN 4102 or BS476, that exclude the mandatory hose stream test and rely on fast response thermocouples for furnace instrumentation, cannot be implied under the terminology of “fire-resistance” or “fire-resistant” for North America, and would have to be very explicitly outlined in order to comply with the Competition Act, Section VII.1 and Title 15 of the Code of Federal Regulations, §45, which states ’’“Unfair methods of competition in or affecting commerce, and unfair or deceptive acts or practices in or affecting commerce, are hereby declared unlawful. Specifically, BS476, Part 20, 1987 makes the “insulation rating” optional (which is never the case in North American wall fire test standards), and also permits the lab’s use of depth gauges to measure maximum gaps in a fire barrier that may develop as a result of fire test exposure. Such practices are not permitted in any North American wall fire test standards. Therefore, a North American high-voltage transformer fire barrier, advertised as a hydrocarbon fire wall or hydrocarbon fire barrier, implies the use of a North American wall fire test standard, whereby the test sample is exposed to the UL 1709 time/temperature curve, substituting the cellulosic curve, using shielded thermocouples, in a calibrated furnace, and satisfying the conditions of acceptance in ASTM E119, NFPA 251, UL 263 and/or CAN/ULC S101 for non-loadbearing walls, which includes mandatory insulation and hose stream test passing performance.\n\nCast concrete is an effective fire barrier in \"building construction\", whose fire exposure is limited to cellulosic time/temperature curve exposures. Evidence of this is plentiful. This does not mean that concrete of \"any\" mix design inherently achieves fire-resistance in all situations without careful consideration of multiple factors. For example, polypropylene fibres have been successfully used to mitigate spalling of concrete when exposed to hydrocarbon fires. The types of aggregate used in concrete can affect fire resistance. The amount of moisture in concrete is known to affect fire-resistance. Also, typically, cementitious products and concrete slabs in fire test laboratories are equipped with moisture probes and ordinarily not subjected to fire testing unless relative humidity is at or below 75%. Further, ASTM E119 indicates in §6.2, that, prior to fire-resistance testing, test specimens are to be conditioned with the objective of providing a moisture condition within the test specimen representative of that in similar construction in buildings. For purposes of standardisation, this condition is established at equilibrium resulting from conditioning in an ambient atmosphere of 50 % relative humidity at 22.8°C or 73°F. There is public domain video evidence on concrete spalling when exposed to high temperatures . Both the amount of free water, which could evaporate out in hot and dry conditions, and the concentration of hydrates play a role in concrete's reaction to fire. The binder used also makes a difference, whereby calcium aluminate cements are broadly used in refractory castables. Just because a calcium aluminate cement is used in a concrete mix design, however, does not mean that there aren't important design considerations, as free water and hydrates are considered here too, to prevent spalling. In fact, it is customary for freshly placed castables to be subjected to a certain heating regimein order to prevent explosive spalling as a result of heat exposure. Apart from moisture-based spalling, there is also the process of \"conversion\", whereby concrete made with calcium aluminate cement first gains significant strength, but then loses some of it in the conversion process. Incorrect use of calcium aluminate cements in construction has led to widespread problems, especially during the third quarter of the 20th century when this type of cement was used because of its faster hardening properties. The problem here was the false assumption that the pre-conversion strength was the final compression or \"cold crushing\" strength. After several years, some of the buildings and structures collapsed due to degradation of the cement and many had to be torn down or reinforced. Heat and humidity accelerate the conversion process; the roof of a swimming pool was one of the first structures to collapse in the UK.. In Madrid, Spain, a large housing block nicknamed Korea (because it was built to house Americans during the Korean War), built 1951~1954 was affected and had to be torn down in 2006. Also in Madrid the Vicente Calderón Stadium was affected and had to be partially rebuilt and reinforced.. While NFPA 850 refers to analytical methods used to determine fire-resistance of concrete structures, one must bear in mind that this refers back to building codes, which are based upon fires in relatively dry buildings. Outdoor transformer fire barrier design based on concrete, regardless whether the binder is Portland cement or calcium aluminate cement, must take into account moisture conditions both in terms of free moisture and hydrates. Simply substituting binders to refractory binders, does not eliminate the ASTM E119 requirements concerning sample conditioning. This means that one rainfall or high humidity, even with heat treatment of CAC bound concrete can affect fire-resistance. Painting the concrete does not stop steam diffusion and may just lock in moisture that can exacerbate spalling. Suppose hypothetically that a design verified an 8\" or 20cm thickness of a certain concrete mix design. There is still the fire-event fragmentation to be considered as well, as its test sequencing. It is possible to overcome the technical challenges, but one must take all factors into account, or use a certification listed system, that can be verified to have taken the aforementioned factors into account.\n\nUtilities at times require ballistic protection of their high-voltage transformer fire barriers. This can have two reasons. First, there have been a number of incidents of reports of shots fired on transformers, for example, in Nova Scotia, Saskatchewan, Alberta, and the Metcalf sniper attack. Shooting at an operating transformer could cause a fire and explosion. This scenario could be tested by exposing the test sample to shots fired before a fire test. Secondly, since NFPA 850 mentions and anecdotal evidence confirms the possibility of fragmentation from bushings and lightning arresters, which form part of the transformer, one could argue that a ballistic test could simulate fire-event fragmentation. Both scenarios have validity, at least until NFPA adds a specific test regime or performance requirement for the fragmentation.\nWhether to defeat ballistic sabotage or transformer fragmentation, either the fire barrier has inherent ballistic resistance properties, or it is added on by means of armour. Vehicle armour traditionally has to consider trade-offs between resistance and weight. For a static wall, keeping the weight low is less important than for fighting vehicles, where every pound of added weight affects range and cargo carrying capacity. Also, a transformer fire barrier is not ordinarily designed to defeat artillery, whereas a main battle tank would be. However, particularly in subsequent installations, where adding the protection is an afterthought, rather than forming part of the original design, there is an incentive to keep barriers as small and modular as possible whilst defeating the given requirements of fire, fragmentation and ballistics, simply due to spatial geometry and the need to perform installation and maintenance work in confined spaces.\nParticularly for added armour, such as high tensile strength steel, or rolled homogeneous armour (RHA), or even combustible armour materials, such as composite material including Kevlar it is important to include such components in fire testing, as they can fundamentally alter fire-resistance performance. In the case of steel armour, this would need to be fireproofed, like all structural steel is for fire-resistant applications. If not, steel first expands and then loses its strength, which can cause damage. In the case of combustible materials added for ballistic resistance, meaning materials that do not pass ASTM E136, these can add fuel to a fire, which is dangerous as part of a fire-resistant assembly, that should not burn from the inside or on the unexposed side, due to heat transmission and autoignition. Therefore, added ballistic resistance without being included in the qualifying fire and hose-stream testing, can have a deleterious effect, defeating the purpose of the barrier.\n\nInternationally, there is a variety of ballistic test standards. For North America, product certification is available from the two primary fire testing and certification organisations using the ballistic test standard UL 752. Those two organisations are UL LLC and Intertek. This combination of organisations that perform both fire and ballistic testing is desirable, simply because the label on the product indicates compliance. When there is no certification mark on the product installed in the field, visible upon inspection of the installed configuration, there is no third-party evidence that what is installed is identical to that which passed testing. UL752 requires ballistic testing in a calibrated range, with samples measuring 12” x 12” or 30 x 30cm, as well as the use of a calibrated gun chronograph. UL publishes such listings via Guide CNEX. Intertek describes its UL752 work online, as performed by its ballistic laboratory in York, Pennsylvania. Legitimate testing per UL 752 requires the use of samples conforming to the standard, sized correctly and, where necessary, tested at three different temperatures, in a calibrated range by a qualified technician. Certification mark use on the product installed in the field enables inspectors and end-users to have verification that the item tested was identical to the item being sold and used. Outdoor test samples shown in advertising, \"outside of the normal shot pattern\", with samples outside of the parameters dictated by the test standard, particularly without the use of the certification mark, visible on the product on the installed configuration, may be indicative.\n\nSince a UL752 ballistic test sample measures 12\" x 12\" or 30 x 30 cm, and any \"legitimate\" wall fire-resistance test sample is minimum 100ft² or 9.3m², and since \"legitimate\", compliant, ballistic testing must occur in a range, into which one cannot fit a full scale fire test sample, and fire test laboratories cannot, for occupational safety and health reasons, permit gunfire, testing of fire-resistance and ballistic resistance must occur on separate samples, or be out of compliance with something. For the combination between fire-resistance materials with ballistic resistance materials to be meaningful, whereby one must not defeat the other, common sense dictates that adding fire-resistive materials to a ballistic material can only improve ballistic resistance, since more material in the way of projectiles, however slight, can only improve ballistic resistance. The opposite, however, is not true. Lightweight armour using covalently bonded composite materials can add fuel to a fire. Metallic armour plating or high tensile strength steel requires fireproofing in order to retain its strength and shape. Therefore, any armour materials, both combustible and non-combustible, added to a fire-resistive assembly must be included at the time of the qualifying fire test to ASTM E119, UL263 or CAN/ULC-S101 in order to provide evidence that the complete system meets the conditions of acceptance for non-loadbearing walls. One may also, subsequently, expose 12\" x 12\" or 30 x 30 cm samples of the combined ballistic and fire-resistive layers in order to take advantage of the added ballistic resistance offered by the fire-resistive materials. If the fire barrier material itself were used to defeat ballistic threats and were reduced in thickness at the points of impact, then the overall thickness and/or configuration should be considered if the barrier must simultaneously defeat fire, fragmentation and ballistics. As an example, if a 2\" or 5cm thick precast concrete panel passed a UL752 ballistic test but were reduced in thickness below the thickness necessary to achieve the mandated fire-resistance rating, then the minimum thickness of that barrier must be evaluated to simultaneously defeat all threats.\n\nSTANAG 4569, level 1, may be used on a full-scale wall sample in advance of fire testing, depending upon access to the test range, which may require some disassembly and re-assembly of wall modules.\n\nThere are arguments for and against doing the fragmentation test before or after the fire test. It is prudent to look at all product testing to see the synergistic effects. For example, a steel-based armour plate may pass a ballistic test but may exhibit plastic deformation at the point of bullet impact. While that plate may pass the ballistic test, the plastic deformation could knock loose fire-resistive material (particularly, though not exclusively, once stressed by fire) it is in contact with and thus degrade or delete the overall system’s fire-resistance, thus defeating the point of the exercise. One of the considerations must be whether the fire-resistive barrier exhibits inherently sufficient fragmentation resistance without requiring an added layer for resisting airborne fragments or projectiles, or whether the fragmentation resistance is added on to a fire-resistive material. Either way, both must be included in fire testing, for the same reasons to do this with ballistic resistance materials, which may be one and the same, such as covalently or ionically bonded impact-resistance products. If a barrier is first exposed to fire, which has a deleterious effect, especially when combined with the mandatory hose stream testing, a metallic armour plate may have lost its strength, if it was not sufficiently fireproofed, meaning that its ballistic rating, once exposed to elevated temperatures, may be seriously degraded and will at the very least be outside of its ballistic listing bounding, which only covers temperature ranges, per UL 752, of -32°C (-25.6°F), +20°C (68°F) and + 49°C (120.2°F) (compared to fire testing, which goes up to ca. 1100°C, depending on the duration of the test). A covalently bound armour material may in fact melt and/or burn as a result of exposure in an ASTM E119 fire test, which could have a disastrous effect on a fire barrier. Combining materials, just because they can individually pass testing to one standard, does not necessarily mean that when combined with another material that has passed a different standard, that the sum of the parts will defeat testing to both standards simultaneously. The conservative approach would be to require a combined ballistic/fragmentation and fire-resistive barrier to undergo fire and hose stream testing first, and then to also defeat bullets and/or flying debris. If materials that are only individually tested, separately concerning fragmentation, ballistics, fire and hose stream, the combination would violate individual certification listings and claims or implications for such systems to meet all sets of requirements without having been tested together may be subject to interpretation in terms of the Competition Act Part VII.1, Deceptive Marketing Practices and/or Title 15 of the Code of Federal Regulations §45, which states \"Unfair methods of competition in or affecting commerce, and unfair or deceptive acts or practices in or affecting commerce, are hereby declared unlawful.\". Concerning the use of structural steel, which is identical to armour plating or high tensile strength steel, in terms of fire-resistance, it is in consideration of the reaction of unprotected steel to fire that ASTM E119 requires the instrumentation of structural steel components in fire-resistive assemblies, to determine the point in time, at which such steel components exceed a heat rise above 538°C or 1,000°F, above ambient before the fire has begun, as this has an effect upon the overall system. Consequently, the point that such temperatures upon steel have been exceeded, which is minutes into a fire test, if there is no fireproofing of the steel, be it armour or otherwise, is the point in time when loaded assemblies have failed. Such a measurement is not required for non-bearing assemblies, such as transformer fire barriers. But that does not mean that steel added on without fire testing can be done without affecting the validity of testing, let alone certification listings. For that reason, ASTM E119 states, identically to NFPA 251, UL 263 and CAN/ULC-S101, in paragraph 5.1, that the test specimen shall be representative of the construction that the test is intended to assess, as to materials, workmanship, and details such as dimensions of parts, and shall be built under conditions representative of those applied in building construction and operation. The physical properties of the materials and ingredients used in the test specimen shall be determined and recorded.\nModularity is offered by some of the North American product vendors in the realm. Modularity enables changes, for example, when one transformer ruptures, damaged wall segments can be replaced, presuming the original equipment vendor is still in business, work can proceed without complete destruction, or, if a transformer were equipped with sensors to enable a monitoring programme to judge the fitness of the transformer and/or its bushings, and the maintenance decision were to replace the transformer, then room can be made for such work by temporarily removing wall segments that are in the way during an outage. As with all such changes, similar to firestops or any passive fire protection systems, it is prudent to be able to demonstrate in advance of the qualifying fire/blast/fragmentation/ballistics testing, that initially installed segments can be demounted and then re-installed, following the manufacturer's instructions and then still pass the required qualifying tests. Typically, manufacturer's instructions accompany test reports and technicians' notes, at laboratories nationally accredited for fire testing and product certification, such as Underwriters Laboratories, Intertek, and Southwest Research Institute, before public domain certification listings are issued. If the intention of modularity exists in advance of the fire test, the instructions for de-mounting and re-assembly are typically followed through before the fire test, particularly if the test is intended to result in a certification listing, which results in the ability to use the laboratory's certification mark on the product, as opposed to testing for information purposes only, which does not entail assurance that the item tested is identical to the item sold and installed, in the absence of the certification label's being visible on the installed configuration as verifiable proof of compliance, pointing to the certification listing upon which the installation and local approval is based.\n\n\n"}
{"id": "37852796", "url": "https://en.wikipedia.org/wiki?curid=37852796", "title": "Home Energy Assistance Target (H.E.A.T.)", "text": "Home Energy Assistance Target (H.E.A.T.)\n\nThe Home Energy Assistance Target (H.E.A.T.) program is the State of Utah’s program through which funds are distributed to the target population. This program is specifically administered by the state and various Associations of Governments (AOG). The Mountain land AOG provides H.E.A.T. assistance to persons in Utah, Wastach, and Summit Counties. MAG receives nearly $2.5 Million annually.\n\nProgram recipients are on the rise. This may be illustrated in the following chart showing the increase in households served by the program in relation to the amount of LIHEAP funds allocated to the State of Utah. Some statistics of note for the State of Utah include:\n\nIn addition to providing matching funds through the Leveraging Incentive Program, LIHEAP strives to coordinate efforts with private utility companies and non-profits where federal funding is not available. In the State of Utah, some of these other sources include Rocky Mountain Power’s Home Electric Lifeline and Lend-a-Hand Programs, Questar’s Energy Assistance Fund and REACH program, Catholic Community Services, American Red Cross, and Murray City Relief Program. H.E.A.T. funding applicants may be referred to these or other private assistance groups if there are not sufficient LIHEAP funds.\n\nPerl, L. (2010). \n"}
{"id": "19334830", "url": "https://en.wikipedia.org/wiki?curid=19334830", "title": "Hydrox (breathing gas)", "text": "Hydrox (breathing gas)\n\nHydrox, a gas mixture of hydrogen and oxygen, was used as a breathing gas in very deep diving. It allows divers to descend several hundred metres.\n\nPrecautions are necessary when using hydrox, since mixtures containing more than a few percent of both oxygen and hydrogen are explosive if ignited. Hydrogen is the lightest gas (half the weight of helium) but still has a narcotic potential and may cause hydrogen narcosis.\n\nAlthough the first reported use of hydrogen seems to be by Antoine Lavoisier (1743–1794), who had guinea pigs breathe it, the actual first uses of this gas in diving are usually attributed to trials by the Swedish engineer, Arne Zetterström in 1945.\n\nZetterström showed that hydrogen was perfectly usable to great depths. Following a fault in using the surface equipment, he died during a demonstration dive. The study of hydrogen was not resumed until several years later by the United States Navy and by the Compagnie maritime d'expertises (Comex), initially during their Hydra I and Hydra II experiments, in 1968 and 1969. Comex subsequently developed procedures allowing dives between 500 and 700 metres (1650 to 2300 feet) in depth, while breathing gas mixtures based on hydrogen, called hydrox (hydrogen-oxygen) or hydreliox (hydrogen-helium-oxygen).\n\nIn July 2012, after about a year of preparation and planning, members of the Swedish Historical Diving Society and the Royal Institute of Technology Diving Club, performed a series of hydrox dives in memory of Arne Zetterström, who was accidentally killed during the ascent from his record dive using hydrox in August 1945. The memorial dives were performed using the same breathing mixture of 96% hydrogen and 4% oxygen as was developed and tested by Zetterström in the 1940s. The dives were made to a depth of , just deep enough to be able to use the oxygen-lean gas mixture. Project Leader Ola Lindh commented that in order to repeat Zetterström's record the team would need to make a dive to , and even today a dive to that depth is considered extreme.\n\nHydrox may be used to combat high pressure nervous syndrome (HPNS), commonly occurring during very deep dives.\n\nThese studies scored a resounding success with a simulated dive to , by Theo Mavrostomos on 20 November 1990 at Toulon, during the COMEX Hydra X decompression chamber experiments. This dive made him \"the deepest diver in the world\".\n\nThe United States Navy has evaluated the use of bacterial flora to speed decompression from hydrox diving.\n\n\n"}
{"id": "13251197", "url": "https://en.wikipedia.org/wiki?curid=13251197", "title": "Ion cyclotron resonance", "text": "Ion cyclotron resonance\n\nIon cyclotron resonance is a phenomenon related to the movement of ions in a magnetic field. It is used for accelerating ions in a cyclotron, and for measuring the masses of an ionized analyte in mass spectrometry, particularly with Fourier transform ion cyclotron resonance mass spectrometers. It can also be used to follow the kinetics of chemical reactions in a dilute gas mixture, provided these involve charged species.\n\nAn ion in a static and uniform magnetic field will move in a circle due to the Lorentz force. The angular frequency of this \"cyclotron motion\" for a given magnetic field strength \"B\" is given by\n\nwhere \"z\" is the number of positive or negative charges of the ion, \"e\" is the elementary charge and \"m\" is the mass of the ion. An electric excitation signal having a frequency \"f\" will therefore resonate with ions having a mass-to-charge ratio \"m/z\" given by\n\nThe circular motion may be superimposed with a uniform axial motion, resulting in a helix, or with a uniform motion perpendicular to the field (e.g., in the presence of an electrical or gravitational field) resulting in a cycloid.\n\nOn March 8, 2013, NASA released an article according to which ion cyclotron waves were identified by its solar probe spacecraft called WIND as the main cause for the heating of the solar wind as it rises from the sun's surface. Before this discovery, it was unclear why the solar wind particles would heat up, instead of cool down, when speeding away from the sun's surface.\n\n"}
{"id": "9326244", "url": "https://en.wikipedia.org/wiki?curid=9326244", "title": "Kantar", "text": "Kantar\n\nA kantar is the official Egyptian weight unit for measuring cotton. It corresponds to the US hundredweight, and is roughly equal to 99.05 pounds, or 45.02 kilograms. It is equal to either 157 kilograms of seed cotton or 50 kilograms of lint cotton.\n"}
{"id": "39040452", "url": "https://en.wikipedia.org/wiki?curid=39040452", "title": "King Abdullah City for Atomic and Renewable Energy", "text": "King Abdullah City for Atomic and Renewable Energy\n\nKing Abdullah City for Atomic and Renewable Energy (also known as K•A•CARE; ) is an independent organization established by Royal Order in April 2010. This city has the responsibility to develop the atomic and renewable energy program in Saudi Arabia. \n\nThe activities in K•A•CARE has been increased rapidly in order to develop and implement the national sustainable energy program. Which aims to create:\n\nK•A•CARE’s team of strategists have made a full evaluation of the alternative energy sources. The result of this evaluation led to the conclusion that hydrocarbons will remain a prime source of energy for a while. They have estimated that by 2032, the nuclear and renewable energy will be distributed as in the following basis: \n\n\n"}
{"id": "17741413", "url": "https://en.wikipedia.org/wiki?curid=17741413", "title": "Kostroma Power Station", "text": "Kostroma Power Station\n\nThe Kostroma Power Station (Kostromskaya GRES) is a gas-fired power station near Volgorechensk in Russia. The station consists of eight 300 MW units and a single 1,200 MW unit. Of which, the 1,200MW unit is the world's largest gas-fired power station unit. The station also has a tall chimney, one of the tallest in the world.\n\n"}
{"id": "27162282", "url": "https://en.wikipedia.org/wiki?curid=27162282", "title": "Laboratory for Energy Conversion", "text": "Laboratory for Energy Conversion\n\nThe Laboratory for Energy Conversion (LEC) formerly known as Turbomachinery Laboratory (LSM) was founded in 1892 by Aurel Boleslav Stodola. As part of the Federal Institute of Technology Zurich (ETH). The laboratory has been headed by some of the most prominent mechanical engineers in the history of turbomachinery.\n\nThe current research projects at LEC cover the fields of:\n\nAmongst many noted achievements, LEC has recently developed the FENT probe. This probe, for the first time, enables measurement of entropy generation in Turbomachinery. The highly rated peer-review journal Measurement Science and Technology recognised the development of this probe as the most outstanding contribution in the field of fluid mechanics in 2008.\n\n\n\n\n"}
{"id": "14460217", "url": "https://en.wikipedia.org/wiki?curid=14460217", "title": "Loir Botor Dingit", "text": "Loir Botor Dingit\n\nLoir Botor Dingit (died 2005) was a rattan farmer and Paramount Chief from Indonesia. He was awarded the Goldman Environmental Prize in 1997 for his efforts on forest protection.\n"}
{"id": "27180252", "url": "https://en.wikipedia.org/wiki?curid=27180252", "title": "Lough Ree Power Station", "text": "Lough Ree Power Station\n\nThe Lough Ree Power Station is a large peat-fired power station in Lanesborough, in the Republic of Ireland. The station generates up of power, ranking as the Third largest peat-fired power station in the country, after West Offaly Power Station at . and Edenderry Power Station at . The power station was constructed as a replacement to the ageing Lanesborough power station.\n\n"}
{"id": "17484911", "url": "https://en.wikipedia.org/wiki?curid=17484911", "title": "Man Meets Dog", "text": "Man Meets Dog\n\nMan Meets Dog is a zoological book for the general audience, written by the Austrian scientist Konrad Lorenz in 1949. The first English-language edition appeared in 1954.\n\nThe original German title is \"So kam der Mensch auf den Hund\", which could be literally translated as \"How man ended up with dog\". The German title is also a play on the phrase \"Auf den Hund kommen\", which is a common idiom in German-speaking countries and probably comes from the old days when farmers with economic problems had to sell their livestock animals and ended up with only the dog.\n\nThe opening chapter \"How it may have started\" deals with theories concerning the question where and when man first domesticated the predecessor of the modern dog. The book has a lot of interesting anecdotes of the author's experiences with dogs, these stories are often illustrated with simple drawings. Lorenz usually owned several dogs and many other animals and lived with them in his house near Vienna. There are also many insights into the behavior of cats and birds, though the focus is of course on the behavior of dogs.\n\nDaniel Pinkwater refers amusingly to \"Man Meets Dog\" in his \"Uncle Boris in the Yukon and other shaggy dog stories\", stating among other things:\n"}
{"id": "16351567", "url": "https://en.wikipedia.org/wiki?curid=16351567", "title": "Mass fraction (chemistry)", "text": "Mass fraction (chemistry)\n\nIn chemistry, the mass fraction formula_1 is the ratio of one substance with mass formula_2 to the mass of the total mixture formula_3, defined as\n\nThe symbol formula_5 is also used to denote mass fraction. The sum of all the mass fractions is equal to 1:\n\nMass fraction can also be expressed, with a denominator of 100, as percentage by mass (in commercial contexts often called \"percentage by weight\", abbreviated \"wt%\"; see mass versus weight). It is one way of expressing the composition of a mixture in a dimensionless size; mole fraction (percentage by moles, mol%) and volume fraction (percentage by volume, vol%) are others.\n\nFor elemental analysis, mass fraction (or \"mass percent composition\") can also refer to the ratio of the mass of one element to the total mass of a compound. It can be calculated for any compound using its empirical formula or its chemical formula.\n\n\"Percent concentration\" does not refer to this quantity. This improper name persists, especially in elementary textbooks. In biology, the unit \"%\" is sometimes (incorrectly) used to denote mass concentration, also called \"mass/volume percentage.\" A solution with 1 g of solute dissolved in a final volume of 100 mL of solution would be labeled as \"1 %\" or \"1 % m/v\" (mass/volume). This is incorrect because the unit \"%\" can only be used for dimensionless quantities. Instead, the concentration should simply be given in units of g/mL. \"Percent solution\" or \"percentage solution\" are thus terms best reserved for \"mass percent solutions\" (m/m = m% = mass solute/mass total solution after mixing), or \"volume percent solutions\" (v/v = v% = volume solute per volume of total solution after mixing). The very ambiguous terms \"percent solution\" and \"percentage solutions\" with no other qualifiers continue to occasionally be encountered.\n\nIn thermal engineering, vapor quality is used for the mass fraction of vapor in the steam.\n\nIn alloys, especially those of noble metals, the term fineness is used for the mass fraction of the noble metal in the alloy.\n\nThe mass fraction is independent of temperature.\n\nThe mixing of two pure components can be expressed introducing the (mass) mixing ratio of them formula_7. Then the mass fractions of the components will be:\n\nThe mass ratio equals the ratio of mass fractions of components:\n\ndue to division of both numerator and denominator by the sum of masses of components.\n\nThe mass fraction of a component in a solution is the ratio of the mass concentration of that component \"ρ\" (density of that component in the mixture) to the density of solution formula_10.\n\nThe relation to molar concentration is like that from above substituting the relation between mass and molar concentration\n\nformula_12\n\nwhere formula_13 is the molar concentration and formula_14 is the molar mass of the component formula_15.\n\nThe mass percentage is sometimes called weight percent (wt%) or weight-weight percentage.\n\nThe mole fraction \"formula_16\" can be calculated using the formula\n\nwhere \"formula_14\" is the molar mass of the component \"formula_15\" and \"formula_20\" is the average molar mass of the mixture.\n\nReplacing the expression of the molar mass-products:\n\nIn a spatially non-uniform mixture, the mass fraction gradient gives rise to the phenomenon of diffusion.\n\n"}
{"id": "2372588", "url": "https://en.wikipedia.org/wiki?curid=2372588", "title": "Massimo Moratti", "text": "Massimo Moratti\n\nMassimo Moratti (born 16 May 1945) is an Italian petroleum tycoon and Chief Executive Officer (CEO) of the Saras Group, founded in 1962 by his father, industrialist Angelo Moratti; the main production site of the Saras Group is the Sarroch refinery located on the island of Sardinia, one of Europe's only six supersites, with a capacity of 300,000 barrels per day, representing 15% of refining capacity in Italy. In recent years, initially to enable independence of the Sarroch refinery from terms of energy, the Saras Group has entered in the production of electricity and is expanding its production of alternative energy sources, particularly in the field of wind energy, through its subsidiaries Sarlux and Sardeolica, the latter of which is controlled indirectly through the company Eolici Ulassai.\n\nFrom 1995 until 2013, Moratti was the chairman of F.C. Internazionale Milano. He is said to have spent around €1.5 billion of his personal fortune in the transfer market, and was famous for signing numerous football superstars.\n\nHe was Inter's honorary chairman, and also a United Nations Goodwill Ambassador. In 2013, he was inducted into the Italian Football Hall of Fame.\n\nMassimo Moratti is the fourth son of industrialist Angelo Moratti, who was the chairman of Football Club Internazionale Milano during the team's \"Golden Age\" from 1955 to 1968. Born in the family villa in the Bosco Chiesanuova, close to Verona, he graduated from Libera Università Internazionale degli Studi Sociali Guido Carli with a master's degree in Political Science.\nLetizia Moratti, his brother's wife, was the Mayor of Milan from 2006 until 2011.\n\nOn his father's death, Massimo Moratti inherited his shares in the Saras Group, engaged in the refining of petroleum, where he is presently C.E.O. Moratti is also the owner of Sarlux, headquartered in Cagliari, which focuses on the production of electricity from the waste oil.\n\nMarried to the environmental activist Emilia Moratti (née Bossi), the couple have five children. On 10 September 2009, Sauro Gori announced that Moratti had been appointed a United Nations Goodwill Ambassador.\n\nIn May 2011, Moratti supported Giuliano Pisapia's bid to become mayor of Milan against his sister-in-law Letizia. His call for 'change' was perceived as an extension of his rivalry with A.C. Milan, Silvio Berlusconi, from football to the political sphere.\n\nMoratti took over as chairman of Inter from Ernesto Pellegrini in 1995, during a period where many considered Inter to be underachievers. During the 18 years of his leadership, Inter has clinched five Scudetti/Serie A in a row from 2006 to 2010 (equalling the all-time record), four Coppa Italia titles (2005, 2006, 2010, 2011), four Supercoppa Italiana titles (2005, 2006, 2008, 2010), one UEFA Champions League (2010), one FIFA Club World Cup (2010) and one UEFA Cup (1998). Inter won the Scudetto, the Coppa Italia and the Champions League in the 2009-2010 season, becoming the first Italian team that managed to achieve the Treble. \n\nMoratti is said to have spent around €1.5 billion of his personal fortune in the transfer market in his time as chairman. His most famous signing was that of Ronaldo from FC Barcelona in the summer of 1997, at a time when he was widely considered the best player in the world. Later, in July 1999, Moratti sanctioned a then world-record €48 million purchase of striker Christian Vieri and has purchased numerous other superstars, including Roberto Carlos, Hernán Crespo, Adriano, Iván Zamorano, Maicon, Roberto Baggio, Zlatan Ibrahimović, Luís Figo, Patrick Vieira, Samuel Eto'o, David Suazo and Wesley Sneijder.\n\nHowever, criticism also been levelled against Moratti, as he fired coaches frequently. Except keeping Giuseppe Baresi and Daniele Bernazzani as backroom staff in the first team or in the youth system, as well as Marco Branca as one of the directors, the team had changed from Rafael Benítez, Leonardo, Gasperini and most recently Ranieri in just 2 seasons. Roberto Mancini and José Mourinho were the only two trophy winning and longest serving coach in recent years. Before Mancini, Massimo employed more than 10 short-lived coaches, including Roy Hodgson, Marcello Lippi, Marco Tardelli, Héctor Cúper and Alberto Zaccheroni.\n\nThe day after Ranieri was dismissed, chief scout of the first team Giovanni Battista Lanfranchi was fired and replaced by the former technical commission of Udinese, Valentino Angeloni. Lanfranchi had served for Inter for 13 years in different positions.\n\nOn 15 November 2013, International Sports Capital took control of 70% of the club. Indonesian businessman Erick Thohir, a part-owner of that company, was elected chairman of Inter, but Moratti remained with the club as the honorary chairman.\n\nOn 28 June 2016 Massimo Moratti's Internazionale Holding S.r.l. sold all its stake in F.C. Internazionale Milano S.p.A. to Erick Thohir's Nusantara Sports Ventures HK Limited for €60 million (and re-sold to Zhang Jindong's Suning Holdings Group). As of 2018, Massimo Moratti's wife, Milly Moratti, remained in the advisory board of Inter. However, Massimo Moratti himself, was no longer the honorary chairman of the club. Other member of Moratti family, Angelomario Moratti, Carlotta Moratti, Giovanni Moratti, remained as the member of the board of a subsidiary of the club, Inter Futura.\n\n\n"}
{"id": "3623194", "url": "https://en.wikipedia.org/wiki?curid=3623194", "title": "Nabucco pipeline", "text": "Nabucco pipeline\n\nThe Nabucco-West pipeline (also referred to as the Turkey–Austria gas pipeline) was a proposed natural gas pipeline from the Turkish-Bulgarian border to Austria. It was a modification of the original Nabucco Pipeline project, which was to run from Erzurum in Turkey to Baumgarten an der March in Austria. The aim of the Nabucco pipeline was to diversify the natural gas suppliers and delivery routes for Europe, thus reducing European dependence on Russian energy. The original project was backed by several European Union member states and by the United States, and was seen as a rival to the South Stream pipeline project. The main supplier was expected to be Iraq, with potential supplies from Azerbaijan, Turkmenistan, and Egypt. The main supply for the Nabucco West was to be Shah Deniz gas through the proposed Trans-Anatolian gas pipeline (TANAP).\n\nThe project was developed by a consortium of six companies. Preparations started in 2002 and the intergovernmental agreement between Turkey, Romania, Bulgaria, Hungary and Austria was signed on 13 July 2009. After an announcement of the construction of TANAP, the consortium submitted the Nabucco-West project. Construction of Nabucco-West depended on the gas export route decision by the Shah Deniz consortium. After Shah Deniz consortium decision to prefer the Trans-Adriatic Pipeline over Nabucco, the Nabucco pipeline plan was finally aborted in June 2013.\n\nThe Nabucco project is backed by the European Union and the United States. In the Trans-European Networks - Energy (TEN - E) programme, the Nabucco pipeline is designated as a project of strategic importance. An objective of the project is to connect the European Union better to the natural gas sources in the Caspian Sea and the Middle East regions. The project has been driven by the intention to diversify its current energy supplies, and to lessen European dependence on Russian energy—the biggest supplier of gas to Europe. The Russia–Ukraine gas disputes have been one of the factors driving the search for alternative suppliers, sources, and routes. Moreover, as per the European Commission, Europe's gas consumption is expected to increase from 502 billion cubic metres, in 2005, to 815 billion cubic metres in 2030, which would mean Russia alone would not be able to meet the demand.\n\nSouth Eastern Europe is important as many of the regions are heavily dependent on Russian gas imports. Nabucco aims to diversify the gas supply to increase competition and security. Simon Pirani, senior research fellow, Oxford Institute for Energy Studies presented to delegates at the Ukrainian Energy Forum in 2013 a list of prices from the Russian newspaper Izvestia: \"What they show is the prices at which Russian gas is being purchased in different European countries, and this tells quite a simple story. If you're in Eastern Europe, and you are quite heavily dependent on Russian gas, you pay more than $500/TCM; if you're in the UK, where we have a pretty much complete domination of gas-to-gas market, you pay $300, or $370+ in Germany, which is somewhere in between.\"\n\nPreparations for the Nabucco project started in February 2002 when first talks took place between Austrian OMV and Turkish BOTAŞ. In June 2002, five companies (OMV of Austria, MOL Group of Hungary, Bulgargaz of Bulgaria, Transgaz of Romania and BOTAŞ of Turkey) signed a protocol of intention to construct the Nabucco pipeline. The protocol followed by the cooperation agreement in October 2002. The name \"Nabucco\" comes from the same famous opera of Giuseppe Verdi, that the five partners had listened to at the Vienna State Opera after this meeting. In December 2003, the European Commission awarded a grant in the amount of 50% of the estimated total eligible cost of the feasibility study including market analysis, and technical, economic and financial studies. On 28 June 2005, the joint venture agreement was signed by five Nabucco partners. The ministerial statement on the Nabucco pipeline was signed on 26 June 2006 in Vienna. On 12 September 2007, Jozias van Aartsen was nominated by the European Commission as the Nabucco project coordinator. In February 2008, German RWE became a shareholder of the consortium.\n\nOn 11 June 2008, the first contract to supply gas from Azerbaijan through the Nabucco pipeline to Bulgaria was signed. The President of Azerbaijan Ilham Aliyev confirmed on 29 January 2009, that Azerbaijan was planning to at least double its gas production in the coming five years to supply the pipeline. On 12 April 2009, the Minister of Energy of Turkey Hilmi Güler confirmed that Turkey is ready to sign a deal, provided that Turkey gets 15% of the natural gas to be carried through the Nabucco pipeline.\n\nOn 27 January 2009, the Nabucco Summit held in Budapest. On 24–25 April 2009, the Nabucco pipeline was discussed, among other energy issues, at the high-level energy summit in Sofia, and on 8 May 2009, at the Southern Corridor Summit in Prague.\n\nThe intergovernmental agreement between Turkey, Romania, Bulgaria, Hungary and Austria was signed by five prime ministers on 13 July 2009 in Ankara. The European Union was represented at the ceremony by the President Jose Manuel Barroso and the Commissioner for Energy Andris Piebalgs, and the United States was represented by Special Envoy for Eurasian Energy Richard Morningstar and Ranking Member of the United States Senate Committee on Foreign Relations Senator Richard Lugar. Hungary ratified the agreement on 20 October 2009. Bulgaria ratified the agreement on 3 February 2010. Romania ratified the agreement on 16 February 2010. Turkey became the final country ratifying the agreement on 4 March 2010.\n\nThe legal framework set up by the intergovernmental agreement was strengthened further with the signing in 2011 of the Project Support Agreements (PSAs) between Nabucco and each of the Transit countries. The main elements of the PSAs are the affirmation of an advantageous regulatory transit regime under EU law; the protection of the Nabucco Pipeline from potential discriminatory changes in the law; and support for legislative and administrative actions for the further implementation of the project.\n\nIn May 2012, the Nabucco consortium submitted a Nabucco-West proposal to the Shah Deniz consortium. On 10 January 2013, Nabucco International and Shah Deniz partners signed a funding agreement. According to the agreement, Shah Deniz partners will take a 50% stake in the project if chosen as an export route for the Shah Deniz gas. On 3 March 2013, Nabucco International signed a memorandum of understanding with the TANAP consortium. However, on 28 June 2013 Shah Deniz consortium announced that it had chosen the Trans Adriatic Pipeline over Nabucco for its gas exports, prompting OMV CEO Gerhard Roiss to regard the Nabucco project as \"over\".\n\nThe original long pipeline was to run from Ahiboz in Turkey via Bulgaria, Romania, and Hungary to Baumgarten an der March, a major natural gas hub in Austria. In Ahiboz, it would be joined with two feeder lines, one connecting to Georgia in the north (South Caucasus Pipeline), and the other connecting to Iraq (pipeline to be built) in the southeast. It would be fed also from the Tabriz–Ankara pipeline. of the pipeline was to be laid in Turkey, in Bulgaria, in Romania, in Hungary, and in Austria.\n\nThe modified Nabucco West is to start from the Turkey–Bulgaria border and further to follow the original route. The total length of Nabucco West is , with the following distances in each of the below countries:\n\n\nFrom Turkey, the original Nabucco pipeline was proposed to enter Bulgaria and after running in parallel to the existing gas system connect to the Bulgarian national gas network at the compressor station of village Lozenets in Yambol Province. After crossing the Balkan Range, the pipeline will head in a northwesterly direction. After reaching the national northern half-ring, it will run in parallel to the existing East-West gas line and continue to northwest before reaching the Danube at Oryahovo. In Bulgaria, Nabucco will have interconnections with the national gas network and will have two off-take systems, compressor stations and pig stations.\n\nIn Romania, the pipeline will be crossing into the country under the Danube. The route on the Romanian territory will go from south-west to north-west, its south-western starting point being located at the Danube-crossing point upstream the Port of Bechet, and the north-western end point being located north of Nădlac. The pipe will follow the south western border of Romania and will travel through the counties of Dolj, Mehedinti, Caras-Severin, Timiş, and Arad. The pipeline will cross 11 protected sites, two national parks, three natural reserves, and 57 watercourses, namely major rivers such as: Jiu, Coşuştea, Cerna, Bela Reca, Timiş, Bega, and Mureş, as well as their tributaries. The terrain is rockier in Romania and mainly constituted of limestone. This section is long.\n\nPolish gas company PGNiG was studying the possibility of building a link from the Nabucco gas pipeline to Poland.\n\nThe Nabucco-West is to be exempt from regulated third party access, including tariff regulation, for 25 years. Its proposal states a capacity of per year. This capacity will be scaled up to to compensate for an anticipated increase in demand. Nabucco West will offer 50% of its transport capacity to third parties outside of the shareholders.\n\nThe Nabucco project is included in the EU Trans-European Energy Network programme and a feasibility study for the Nabucco pipeline has been performed under an EU project grant. The front end engineering and design (FEED) services of the pipeline, including the overall management of the local FEED contractors, the review of the technical feasibility study, route confirmation, preparation of the design basis, hydraulic studies, overall SCADA and telecommunications, GIS and preparation of tender packages for the next phase, was managed by UK-based consultancy Penspen. Starting from 14 December 2011, WorleyParsons was appointed as on owner's engineer.\n\nOn 28 January 2013, it was announced that a re-feed for the Nabucco West project is being conducted by Saipem following the selection of the project as the Central European route by the Shah Deniz consortium in June last year. This work will build upon existing engineering work already completed for the Nabucco classic route.\n\nAccording to Reinhard Mitschek, managing director of Nabucco Gas Pipeline International GmbH, construction of the pipeline was scheduled to begin in 2013 and would become operational by 2017. However, in June 2013, the Shah Deniz Consortium had chosen a rival project, Trans Adriatic Pipeline, that has a route Turkey–Greece-Albania-Italy, and the future of Nabucco project is unclear.\n\nThe pipelines costs are undisclosed, however Reinhard Mitschek said in late 2012 that the costs of Nabucco West would be far lower than €7.9 billion previously suggested. The final investment decision is expected in 2013. The sources of financing of the Nabucco project are not decided yet. As a commercial project, it will be financed 30% by the project's partners and the rest by commercial financial instruments. The European Commission has awarded an EU project grant in the amount of 50% of the estimated total eligible cost of the feasibility study and has also decided to allocate €200 million from the European Economic Recovery Plan. To receive this financing, this grant should be committed by the end 2010.\n\nAt the Nabucco Summit held in Budapest on 27 January 2009, the heads of the European Investment Bank (EIB) and the European Bank for Reconstruction and Development (EBRD) confirmed, that they are prepared to provide financial backing for the project. On 5 February 2010, the EIB vice-president Mathias Kollatz-Ahnensaid that Nabucco consortium is seeking up to €2 billion (20–25% of costs) financing from the bank. The EIB is ready to participate in the financing of this project; however, the precondition is that the partner countries should legally approve the pipeline's transit in their countries.\n\nIn September 2010, the consortium signed an agreement with EIB, EBRD, and the International Finance Corporation (IFC), according to which the banks will conduct due diligence for a financing package of €4 billion. Up to €2 billion will be signed by the EIB, up to €1.2 billion by the EBRD, and up to €800 million by the IFC. All figures listed above relate to the original Nabucco Project. Updated figures for Nabucco West are undisclosed as of June 2013. Reinhard Mitschek, Managing Director of Nabucco said in an interview with Natural Gas Europe in May 2013 that “Nabucco is continuing to cooperate with the International Financial Institutions to ensure the bankability of the project, a large part of the legal due diligence has already been completed. A Letter of Intent has been signed with the IFIs most recently.” In a separate interview in February 2013, Mitschek confirmed that all legal and regulatory framework approved for the original Nabucco project would remain valid for Nabucco West.\n\nThe potential suppliers for original Nabucco project were considered to be Iraq, Azerbaijan, Turkmenistan, and Egypt. At the first stage, of natural gas per year were expected from Iraq. Iraqi gas would be imported via the Arab Gas Pipeline (extension to be built) from the Ekas field. Turkmenistan would provide of gas per year through Iran or across the Caspian Sea via the planned Trans-Caspian Gas Pipeline. OMV and RWE set up a joint venture, named the Caspian Energy Company, to carry out research for a gas pipeline across the Caspian Sea. In the long term, Kazakhstan may become a supplier providing natural gas from the Northern Caspian reserves through the planned Trans-Caspian Gas Pipeline.\n\nEgypt could provide of natural gas through the Arab Gas Pipeline. Prime Minister of Turkey Recep Tayyip Erdoğan has urged Egypt to export natural gas to Europe via the Nabucco pipeline. Iran has also proposed to supply gas to Nabucco pipeline and this was backed by Turkey; however, due the political conditions this is rejected by the EU and the United States.\n\nNabucco-West is designated to carry Azeri gas from the second stage of Shah Deniz through TANAP pipeline. The pipeline is able to transport between 10 – 23 BCM annually from the Shah Deniz gas field. OMV, a shareholder in Nabucco, also suggested that Nabucco will be used to transport gas from its Domino-1 deep-sea offshore well in the Black Sea. The Domino-1 well was OMV’s largest gas find with 1.5 – 3 trillion cubic feet announced in February 2012.\n\nThe project is developed by the Vienna-registered Nabucco Gas Pipeline International GmbH. The managing director of the company is Reinhardt Mitschek.\n\nThe shareholders of the company are:\n\nNabucco International is the owner of the five national Nabucco companies responsible for the operation and maintenance of the pipeline in their respective countries.\n\nRWE left the project and on 1 March 2013 OMV took over all of RWE's shares. On 28 May 2013, it was announced that GDF Suez, a French utilities provider, agreed to buy a 9% stake from OMV.\n\nThe main competitor for the original project was South Stream. In 2006, Gazprom proposed an alternative project, in competition with the Nabucco pipeline, that would involve constructing a second section of the Blue Stream pipeline beneath the Black Sea to Turkey, and extending this up through Bulgaria and Serbia to western Hungary. In 2007, instead the South Stream project through Bulgaria, Serbia, Hungary and Slovenia to Austria and Italy was proposed. On 10 March 2010, CEO of Eni, a partner in South Stream, Paolo Scaroni proposed to merge Nabucco and South Stream projects to \"reduce investments, operational costs and increase overall returns\". This proposal was rejected by energy minister of Russia Sergei Shmatko saying that \"South Stream is more competitive than Nabucco\" and that \"Nabucco and South Stream are far from being competitors\". According to Nobuo Tanaka, former executive director of the International Energy Agency, the Nabucco pipeline would be more effective in increasing Europe's energy security than the South Stream project as it would increase the number of gas suppliers.\n\nEven more important competitor became TANAP which would follow the Nabucco's original route in Turkey. Therefore, Nabucco consortium modified the project and sees the modified Nabucco-West as a prolongation of TANAP into Central Europe. Nabucco West competed with the Trans Adriatic Pipeline and the Interconnector Turkey–Greece–Italy projects.\n\nAlso liquefied natural gas was seen as competitor to Nabucco and to pipeline projects in general. Azerbaijan, Georgia, Romania and Hungary are developing Azerbaijan–Georgia–Romania Interconnector project, which is proposed to transport Azerbaijani gas to Europe in form of LNG. Increasing availability of LNG from large gas-producing countries in the Middle-East and Africa stresses the economic viability of pipelines.\n\nThe Nabucco pipeline will supply only a limited number of countries in South-East and Central Europe. In 2013, it was confirmed by Bulgarian President Rosen Plevneliev that the pipeline would transport gas to a minimum of 16 European countries including the gas hub in Baumgarten, Austria. The project has been criticized as uneconomic because there is no guarantee that there will be sufficient gas supplies to make it profitable. The Nabucco Gas Pipeline project, although initially intending to secure gas from Iraq and Iran has readjusted its intentions given the current political and economic instabilities in the two countries. It will initially transport 10 BCM from the Shah Deniz gas field with the ability to increase its capacity to 23 BCM as demand increases, along with supply. One region that could also supply additional gas is the Black Sea, with OMV and Exxon Mobil announcing an enormous gas discovery in February 2012.\n\nIran's Foreign Minister Manouchehr Mottaki has stated \"speaking about the Nabucco pipeline without Iran's participation would amount to nothing but a pipeline void of gas\". Russian Prime Minister Vladimir Putin has made similar remarks. The deputy chairman of the Russia's State Duma Energy Committee Ivan Grachev has questioned the viability of the Nabucco project and sees it as an attempt to put pressure on Russia. This is supported by Russia's gas deals with Azerbaijan and Turkmenistan, which by some observers has been seen as attempt to reserve potential Nabucco supplies. Azerbaijan has stated that the gas will be transported only through those routes, which would be commercially most attractive. Also the opening of the Central Asia – China gas pipeline and the agreements to build the South Stream pipeline has been seen as the end of Nabucco project.\n\nHowever, before the raise of project's costs and the proposal of modified project, RWE had claimed that the transportation of natural gas through the Nabucco pipeline would be cheaper than through South Stream or other alternative pipelines. According to RWE, the transportation of thousand cubic meters of gas from Shah Deniz field to Europe will cost through the Nabucco pipeline €77 versus €106 through the South Stream pipeline.\nRussian opposition to the pipeline stems from their monopoly over European gas supplies. The Pipeline would lead to cheaper more secure gas supplies for the whole of Europe, due to the decreased influence of the oil linked gas price, this would provide economic benefits to the EU with cheaper energy helping the union become more competitive.\n\nNGOs have also criticized the fact that the pipeline results in effective support of the authoritarian regime in Turkmenistan, which undermines the European Union's policy of human rights promotion.\n\nSome NGOs criticize the EIB and EBRD for their willingness to finance a fossil fuel project, claiming that it goes against the November 2007 resolution on trade and climate change passed in the European Union Parliament. The resolution calls for \"the discontinuation of public support via export credit agencies and public investment banks, for fossil fuel projects.\" Non-governmental organizations also show disapproval, due to the public banks decision to be lenient to Turkmenistan Human and civil rights conditions.\n\nConcerns have been raised about the safety of the project. Gas for the Nabucco pipeline coming from Azerbaijan and Turkmenistan will have to pass near areas of instability in the South Caucasus.\n\n\n"}
{"id": "44889502", "url": "https://en.wikipedia.org/wiki?curid=44889502", "title": "Niobium capacitor", "text": "Niobium capacitor\n\nA niobium electrolytic capacitor is a polarized capacitor whose anode electrode (+) is made of passivated niobium metal or niobium monoxide on which an insulating niobium pentoxide layer acts as the dielectric of the niobium capacitor. A solid electrolyte on the surface of the oxide layer serves as the second electrode (cathode) (-) of the capacitor.\n\nNiobium electrolytic capacitors are passive electronic components and members of the family of electrolytic capacitors.\n\nNiobium capacitors are available as SMD chip capacitors and compete with tantalum chip capacitors in certain voltage and capacitance ratings. They are available with a solid manganese dioxide electrolyte. \nNiobium capacitors are polarized components by manufacturing principle and may only be operated with DC voltage in correct polarity. Reverse voltage or ripple current higher than specified can destroy the dielectric and thus the capacitor. The destruction of the dielectric may have catastrophic consequences. Manufacturers specify special circuit design rules for the safe operation of niobium capacitors.\n\nNiobium capacitors were developed in the United States as well as in the Soviet Union in the 1960s. Since 2002 they have been commercially available in the West to take advantage of the lower cost and better availability of niobium compared with tantalum.\n\nNiobium is a sister metal to tantalum. Niobium has a similar melting point (2744 °C) to tantalum and exhibits similar chemical properties. The materials and processes used to produce niobium-dielectric capacitors are essentially the same as for existing tantalum-dielectric capacitors. However, niobium as a raw material is much more abundant in nature than tantalum and is less expensive. The characteristics of niobium electrolytic capacitors and tantalum electrolytic capacitors are roughly comparable.\n\nNiobium electrolytic capacitors can be made with high purity niobium as the anode but the diffusion of oxygen from the dielectric (NbO) into the niobium anode metal is very high, resulting in leakage current instability or even capacitor failures. There are two possible ways to reduce oxygen diffusion and improve leakage current stability – either by doping metallic niobium powders with nitride into passivated niobium nitride or using niobium oxide (NbO) as anode material. Niobium oxide is a hard ceramic material characterized by high metallic conductivity. Niobium oxide powder can be prepared in a similar structure to that of tantalum powder and can be processed in a similar way to produce capacitors. It also can be oxidized by anodic oxidation (anodizing, forming) to generate the insulating dielectric layer. Thus two types of niobium electrolytic capacitors are marketed, those using a passivated niobium anode and those using a niobium oxide anode. Both types use niobium pentoxide (NbO) as the dielectric layer.\n\nNiobium is a so-called valve metal such as tantalum and aluminum, on which an electrically insulating oxide layer is formed by anodic oxidation, if a positive voltage is applied. Applying a positive voltage to the anode material in an electrolytic bath forms an oxide barrier layer with a thickness corresponding to the applied voltage. This oxide layer acts as dielectric in an electrolytic capacitor.\n\nFor niobium this behavior was known since the beginning of the 20th century. Niobium is more abundant in nature than tantalum and is less expensive but the high melting point of 2744 °C hindered the development of niobium electrolytic capacitors.\n\nIn the 1960s, the better availability of niobium ore compared with tantalum ore prompted research into niobium electrolytic capacitors in the former Soviet Union. Here they took the place that was filled by tantalum capacitors in the West. With the collapse of the Iron Curtain, this know-how has been publicized in the West. In the late 1990s the interest in this technology awoke in the major capacitor manufacturers. The materials and processes used to produce niobium capacitors are essentially the same as for tantalum capacitors. However, a price rise for tantalum in 2000/2001 encouraged the development of niobium electrolytic capacitors with manganese dioxide electrolyte, as well as polymer electrolyte which have been available since 2002.\nEvery electrolytic capacitor in principle forms a \"plate capacitor\" whose capacitance increases with the electrode area (A) and the permittivity (ε) and decreases with the thickness (d) of the dielectric.\n\nThe dielectric thickness of niobium electrolytic capacitors is very thin, in the range of nano-meter per volt. With this very thin dielectric oxide layer, combined with a sufficiently high dielectric strength, the niobium electrolytic capacitors can achieve a high volumetric capacitance comparable to tantalum capacitors. This is one reason for the high capacitance values of electrolytic capacitors compared with other conventional capacitors.\n\nThe niobium anode material is manufactured from a powder sintered into a pellet with a rough surface structure intended to increase the electrode surface A compared to a smooth surface of the same area or the same volume. That increases the later capacitance value, depending on the rated voltage, by the factor of up to 200 for solid niobium electrolytic capacitors. The large surface compared to a smooth one is the second reason for the relatively high capacitance values of niobium electrolytic capacitors.\n\nOne special advantage is given for all electrolytic capacitors. Because the forming voltage defines the oxide layer thickness the voltage proof of the later electrolytic capacitor can be produced very simple for the desired rated value. That makes electrolytic capacitors fit for uses down to 2 V applications in which other capacitor technologies must stay to much higher limits.\n\nThe properties of this niobium pentoxide dielectric layer compared with tantalum pentoxide layer are given in the following table:\n\nThe higher permittivity but lower breakdown voltage of niobium pentoxide in niobium capacitors results in capacitors of similar size to those using tantalum pentoxide in tantalum capacitors.\n\nA typical niobium capacitor is a chip capacitor and consists of niobium or niobium oxide powder pressed and sintered into a pellet as the anode of the capacitor, with the oxide layer of tantalum pentoxide as dielectric, and a solid manganese dioxide electrolyte as the cathode.\n\nThe combination of anode materials for niobium and tantalum electrolytic capacitors and the electrolytes used has formed a wide variety of capacitor types with different properties. An outline of the main characteristics of the different types is shown in the table below.\n\nTantalum electrolytic capacitors with solid electrolyte as surface-mountable chip capacitors are mainly used in electronic devices in which little space is available or a low profile is required. They operate reliable over a wide temperature range without large parameter deviations.\n\nIn order to compare the different characteristics of the different electrolytic chip capacitor types, specimens with the same dimensions and of comparable capacitance and voltage are compared in the following table. In such a comparison the values for ESR and ripple current load are the most important parameters for the use of electrolytic capacitors in modern electronic equipment. The lower the ESR the higher the ripple current per volume the better the functionality of the capacitor in the circuit. \n(1) 100 µF/10 V, unless otherwise specified,\n\n(2) calculated for a capacitor 100 µF/10 V,\n\nThe phenomenon that can electrochemically form an oxide layer on aluminum and metals like tantalum or niobium, blocking an electric current in one direction but allowing it to flow in the other direction, was discovered in 1875 by the French researcher Eugène Ducretet. He coined the term \"valve metal“ for such metals. Charles Pollak (born Karol Pollak) used this phenomenon for an idea of an polarized “Electric liquid capacitor with aluminum electrodes”. In 1896 Pollak obtained a patent for the first electrolytic capacitor. \nThe first tantalum electrolytic capacitors with wound tantalum foils and non-solid electrolyte were developed in 1930 by Tansitor Electronics Inc., USA, and used for military purposes.\n\nThe development of solid electrolyte tantalum capacitors began in the early 1950s as a miniaturized, more reliable low-voltage support capacitor to complement the newly invented transistor. The solution found by R. L. Taylor and H. E. Haring of the Bell Labs was based on experience with ceramics. They ground down tantalum to a powder, pressed this powder into a cylindrical form and then sintered the powder particles into a pellet (“slug”) at high temperatures, between 1500 and 2000 °C, under vacuum conditions. These first sintered tantalum capacitors used a non-solid electrolyte not consistent with the concept of solid state electronics. 1952 a targeted search in the Bell Labs for a solid electrolyte by D. A. McLean and F. S. Power led to the invention of manganese dioxide as a solid electrolyte for a sintered tantalum capacitor.\n\nNiobium electrolytic capacitors as discrete components are not ideal capacitors, they have losses and parasitic inductive parts. All properties can be defined and specified by a series equivalent circuit composed out of an idealized capacitance and additional electrical components which model all losses and inductive parameters of a capacitor. In this series-equivalent circuit the electrical characteristics are defined by:\n\n\nUsing a series equivalent circuit instead of a parallel equivalent circuit is specified by IEC/EN 60384-1.\n\nThe electrical characteristics of niobium electrolytic capacitors depend on structure of the anode and the type of electrolyte. The capacitance value of the capacitor depends on measuring frequency and temperature. The rated capacitance value or nominal value is specified in the data sheets of the manufacturers and is symbolized C C. The standardized measuring condition for electrolytic capacitors an AC measuring method with a frequency of 100/120 Hz. The AC measuring voltage shall not exceed 0,5  V  AC-RMS.\n\nThe percentage of allowed deviation of the measured capacitance from the rated value is called capacitance tolerance. Electrolytic capacitors are available in different tolerance series, whose values are specified in the E series specified in IEC 60063. For abbreviated marking in tight spaces, a letter code for each tolerance is specified in IEC 60062.\n\nReferring to IEC/EN 60384-1 standard the allowed operating voltage for niobium capacitors is called \"rated voltage U \" or \"nominal voltage U\". The rated voltage U is the maximum DC voltage or peak pulse voltage that may be applied continuously at any temperature within the rated temperature range T (IEC/EN 60384-1).\n\nThe voltage proof of electrolytic capacitors decreases with increasing temperature. For some applications it is important to use a higher temperature range. Lowering the voltage applied at a higher temperature maintains safety margins. For some capacitor types therefore the IEC standard specify a \"temperature derated voltage\" for a higher temperature, the \"category voltage U\". The category voltage is the maximum DC voltage or peak pulse voltage that may be applied continuously to a capacitor at any temperature within the category temperature range T. The relation between both voltages and temperatures is given in the picture right.\n\nLower voltage applied may have positive influences for tantalum electrolytic capacitors. Lowering the voltage applied increases the reliability and reduce the expected failure rate.\n\nApplying a higher voltage than specified may destroy electrolytic capacitors.\n\nThe surge voltage indicates the maximum peak voltage value that may be applied to electrolytic capacitors during their application for a limited number of cycles. The surge voltage is standardized in IEC/EN 60384-1. For niobium electrolytic capacitors the surge voltage shall be not higher than round 1.3 times of the rated voltage, rounded off to the nearest volt.\nThe surge voltage applied to niobium capacitors may influence the capacitors failure rate.\n\nLike other electrolytic capacitors, niobium electrolytic capacitors are polarized and require the anode electrode voltage to be positive relative to the cathode voltage.\n\nGeneral information to impedance, ESR, dissipation factor tan δ, ripple current, and leakage current see electrolytic capacitor\n\nFor general information on reliability and failure rate see electrolytic capacitor.\n\nThe life time, service life, load life or useful life of electrolytic capacitors is a special characteristic of non-solid electrolytic capacitors, especially non-solid aluminum electrolytic capacitors which liquid electrolyte can evaporate over the time leading to wear-out failures. Solid niobium capacitors with manganese dioxide electrolyte have no wear-out mechanism so the constant failure rate lasts up to the point all capacitors have failed. They don’t have a life time specification like non-solid aluminum electrolytic capacitors.\n\nHowever, solid polymer niobium electrolytic capacitors do have a life time specification. The polymer electrolyte deteriorates by a thermal degradation mechanism of the conductive polymer. The electrical conductivity decreases, as a function of time, in agreement with a granular structure, in which aging is due to the shrinking of the conductive polymer grains. The life time of polymer electrolytic capacitors is specified in similar terms like non-solid e-caps but its life time calculation follows other rules leading to much longer operational life times.\n\nThe different types of electrolytic capacitors show different behaviors in long-term stability, inherent failure modes and their self-healing mechanisms. Application rules for types with an inherent failure mode are specified to ensure capacitors high reliability and long life.\n\nA rare failure in solid electrolytic capacitors is breakdown of the dielectric caused by faults or impurities. In niobium electrolytic capacitors the dielectric is niobium pentoxide (NbO). Besides this pentoxide there is an additional niobium suboxide, niobium dioxide (NbO). The NbO is a semi-conducting material with a higher conductivity than NbO but much lower than a short. In case of faults or impurities in the dielectric which evokes a partial dielectric breakdown the conducting channel. would be effectively isolated by reduction of NbO into high ohmic NbO if energy is limited.\n\nAs more energy is applied to a faulty solid niobium eventually either the high ohmic NbO channel or the NbO dielectric breaks down and the capacitor exhibits a thermal runaway failure. In comparison to solid tantalum capacitors the thermal runaway of niobium anodes will occur at about three times higher power than of tantalum anodes. This gives a significant reduction (95%) of the ignition failure mode compared to solid tantalum capacitors.\n\nThe dielectric layer NbO of solid niobium electrolytic capacitors has a lower breakdown voltage proof than TaO in tantalum capacitors and therefore grows thicker per applied volt and so operates at lower field strength for a given voltage rating with the lower electrical stress the dielectric. In combination with niobium oxide anodes, which are more stable against oxygen diffusion that results in lower voltage derating rules compared with passivated niobium or tantalum anodes.\n\nElectrolytic capacitor symbols\n\nNiobium capacitors are in general polarized components, with distinctly marked positive terminals. When subjected to reversed polarity (even briefly), the capacitor depolarizes and the dielectric oxide layer breaks down, which can cause it to fail even when later operated with correct polarity. If the failure is a short circuit (the most common occurrence), and current is not limited to a safe value, catastrophic thermal runaway may occur.\n\nThe standardization for all electrical, electronic components and related technologies follows the rules given by the International Electrotechnical Commission (IEC), a non-profit, non-governmental international standards organization. The definition of the characteristics and the procedure of the test methods for capacitors for use in electronic equipment are set out in the generic specification:\n\nUntil now (2014) no IEC detail specification for niobium electrolytic capacitors is available.\n\nFor electronics manufacturers in the United States the EIA publish a standard for niobium and tantalum chip capacitors: \n\n\n\n"}
{"id": "4638199", "url": "https://en.wikipedia.org/wiki?curid=4638199", "title": "Nuclear decommissioning", "text": "Nuclear decommissioning\n\nNuclear decommissioning is the process whereby a nuclear facility is dismantled to the point that it no longer requires measures for radiation protection. \nThe presence of radioactive material necessitates processes that are potentially occupationally hazardous, expensive, time-intensive, and present environmental risks that must be addressed to ensure radioactive materials are either transported elsewhere for storage or stored on-site in a safe manner. \nThe challenge in nuclear decommissioning is not just technical, but also economical and social.\n\nDecommissioning is an administrative and technical process. \nIt includes clean-up of radioactive materials and progressive demolition of the facility. \nOnce a facility is fully decommissioned, no radiological danger should persist. \nThe costs of decommissioning are generally spread over the lifetime of a facility and saved in a decommissioning fund. \nAfter a facility has been completely decommissioned, it is released from regulatory control and the plant licensee is no longer responsible for its nuclear safety. \nDecommissioning may proceed all the way to \"greenfield\" status.\n\nNuclear decommissioning is the administrative and technical process whereby a nuclear facility such as a nuclear power plant (NPP), a research reactor, an isotope production plant, a particle accelerator, or uranium mine is dismantled to the point that it no longer requires measures for radiation protection. \nThe progressive demolition of buildings and removal of radioactive material is potentially occupationally hazardous, expensive, time-intensive, and presents environmental risks that must be addressed to ensure radioactive materials are either transported elsewhere for storage or stored on-site in a safe manner. Decommissioning may proceed all the way to \"greenfield status\". Once a facility is decommissioned no radioactive danger persists and it can be released from regulatory control.\n\nThe International Atomic Energy Agency defines three options for decommissioning:\n\nThe decommission of a nuclear reactor can only take place after the appropriate licence has been granted pursuant to the relevant legislation. As part of the licensing procedure, various documents, reports and expert opinions have to be written and delivered to the competent authority, e.g. safety report, technical documents and an environmental impact study (EIS).\n\nIn the European Union these documents are the basis for the environmental impact assessment (EIA) according to Council Directive 85/337/EEC. A precondition for granting such a licence is an opinion by the European Commission according to Article 37 of the Euratom Treaty. Article 37 obliges every Member State of the European Union to communicate certain data relating to the release of radioactive substances to the Commission. This information must reveal whether and if so what radiological impacts decommissioning – planned disposal and accidental release – will have on the environment, i.e. water, soil or airspace, of the EU Member States. On the basis of these general data, the Commission must be in a position to assess the exposure of reference groups of the population in the nearest neighbouring states.\n\nIn the United States, the NRC recommends that the costs of decommissioning should be spread over the lifetime of a facility and saved in a decommissioning fund. Repository delay seems to be effective in reducing NPP decommissioning costs.\n\nIn France, decommissioning of Brennilis Nuclear Power Plant, a fairly small 70 MW power plant, already cost €480 million (20x the estimate costs) and is still pending after 20 years. \nDespite the huge investments in securing the dismantlement, radioactive elements such as plutonium, caesium-137 and cobalt-60 leaked out into the surrounding lake.\n\nIn the UK, decommissioning of the Windscale Advanced gas cooled reactor (WAGR), a 32 MW prototype power plant, cost €117 million.\nA 2013 estimate by the United Kingdom's Nuclear Decommissioning Authority predicted costs of at least £100 billion to decommission the 19 existing United Kingdom nuclear sites.\n\nIn Germany, decommissioning of Niederaichbach nuclear power plant, a 100 MW power plant, amounted to more than €143 million.\n\nNew methods for decommissioning have been developed in order to minimize the usual high decommissioning costs. \nOne of these methods is in situ decommissioning (ISD), meaning that the reactor is entombed instead of dismantled. \nThis method was implemented at the U.S. Department of Energy Savannah River Site in South Carolina for the closures of the P and R Reactors. \nWith this tactic, the cost of decommissioning both reactors was $73 million. \nIn comparison, the decommissioning of each reactor using traditional methods would have been an estimated $250 million. \nThis results in a 71% decrease in cost by using ISD.\n\nIn 2004, in a meeting in Vienna, the International Atomic Energy Agency estimated the total cost for the decommissioning of all nuclear facilities.\nDecommissioning of all nuclear power reactors in the world would require US$187 billion; US$71 billion for fuel cycle facilities; less than US$7 billion for all research reactors; and US$640 billion for dismantling all military reactors for the production of weapons-grade plutonium, research fuel facilities, nuclear reprocessing chemical separation facilities, etc. \nThe total cost to decommission the nuclear fission industry in the World (from 2001 to 2050) was estimated at around US$1 trillion.\n\nIn Europe there is considerable concern over the funds necessary to finance final decommissioning. In many countries either the funds do not appear sufficient to cover decommissioning and in other countries decommissioning funds are used for other activities, putting decommissioning at risk, and distorting competition with parties who do not have such funds available.\n\nIn 2016 the European Commission assessed that European Union's nuclear decommissioning liabilities were seriously underfunded by about 118 billion euros, with only 150 billion euros of earmarked assets to cover 268 billion euros of expected decommissioning costs covering both dismantling of nuclear plants and storage of radioactive parts and waste. France had the largest shortfall with only 23 billion euros of earmarked assets to cover 74 billion euros of expected costs.\n\nSimilar concerns exist in the United States, where the U.S. Nuclear Regulatory Commission has located apparent decommissioning funding assurance shortfalls and requested 18 power plants to address that issue. The decommissioning cost of Small modular reactors is expected to be twice as much respect to Large Reactors.\n\nOrganizations that promote the international sharing of information, knowledge, and experiences related to nuclear decommissioning include the International Atomic Energy Agency, the Organization for Economic Co-operation and Development's Nuclear Energy Agency and the European Atomic Energy Community. In addition, an online system called the Deactivation and Decommissioning Knowledge Management Information Tool was developed under the United States Department of Energy and made available to the international community to support the exchange of ideas and information. The goals of international collaboration in nuclear decommissioning are to reduce decommissioning costs and improve worker safety.\n\nA wide range of nuclear facilities have been decommissioned so far. The number of decommissioned nuclear reactors out of the List of nuclear reactors is small. As of 2016, 150 nuclear reactors were shut-off, in several early and intermediate stages (cold shut-down, defueling, SAFSTOR, internal demolition), but only 17 have been taken to fully \"greenfield status\". Some of these sites still host spent nuclear fuel in the form of dry casks embedded in concrete filled steel drums.\n\nSeveral nuclear engineering and building demolition companies specialize in nuclear decommissioning, which has become a profitable business. More recently, construction and demolition companies in the UK have also begun to develop nuclear decommissioning services. Due to the radioactivity in the reactor structure (specially with high neutron-flux), decommissioning takes place in stages. Plans for decommissioning reactors have a time frame of decades. The long time frame makes reliable cost estimates difficult and cost overruns are common even for \"quick\" projects.\n\nAs of 2017, most nuclear plants operating in the United States were designed for a life of about 30–40 years and are licensed to operate for 40 years by the US Nuclear Regulatory Commission. The average age of these reactors is 32 years. Many plants are coming to the end of their licensing period and if their licenses are not renewed, they must go through a decontamination and decommissioning process.\n\nMany warships and a few civil ships have used nuclear reactors for propulsion. Former Soviet and American warships have been taken out of service and their power plants removed or scuttled. Dismantling of Russian submarines and ships and American submarines and ships is ongoing. Marine power plants are generally smaller than land-based electrical generating stations.\n\nThe biggest American military nuclear facility for the production of weapons-grade plutonium was Hanford site (in the State of Washington), now defueled, but in a slow and problematic process of decontamination, decommissioning, and demolition. There is \"the canyon\" a giant structure for the chemical extraction of plutonium with the PUREX process. There are also many big containers and underground tanks with a solution of water, hydrocarbons and uranium-plutonium-neptunium-cesium-strontium (all highly radioactive). With all reactors now defueled, some were put in SAFSTOR (with their cooling towers demolished). Several reactors have been declared National Historic Landmarks.\n\n\n"}
{"id": "41453857", "url": "https://en.wikipedia.org/wiki?curid=41453857", "title": "Nuclear energy in Hong Kong", "text": "Nuclear energy in Hong Kong\n\nCurrently, there is no nuclear power plant in Hong Kong. However, a Hong Kong company, Hong Kong Nuclear Investment Company (HKNIC), owns 25% share in Daya Bay Nuclear Power Plant in Guangdong. About 70% of the power plant electricity output is supplied to Hong Kong by their electric utility company China Light and Power Co., Ltd. (CLP) to meet the electricity demand in Hong Kong.\n\nHong Kong currently imports electricity from Mainland China from the Daya Bay Nuclear Power Plant in Guangdong since 1994. The agreement of electricity import lasts until 2034. CLP will also import 17% of electricity from the planned Yangjiang Nuclear Power Station in Guangdong by 2017 under the agreement with China Guangdong Nuclear Power Company.\n\nTo reduce the carbon dioxide emission in Hong Kong, new coal-fired power plants have not been allowed to be built anymore since 1997. The government of Hong Kong have been trying to boost the share of renewable energy, natural gas and nuclear energy for power generation in Hong Kong. The current existing coal-fired power plants will be kept on low utilization as reserve.\n\nThe government itself plan to cut carbon emission up to 50-60% from the current emission by 2020 by gradually replacing coal-based power generation with more nuclear power and other green energy.\n\n"}
{"id": "21961", "url": "https://en.wikipedia.org/wiki?curid=21961", "title": "Nucleon", "text": "Nucleon\n\nIn chemistry and physics, a nucleon is either a proton or a neutron, considered in its role as a component of an atomic nucleus. The number of nucleons in a nucleus defines an isotope's mass number (nucleon number).\n\nUntil the 1960s, nucleons were thought to be elementary particles, not made up of smaller parts. Now they are known to be composite particles, made of three quarks bound together by the so-called strong interaction. The interaction between two or more nucleons is called internucleon interaction or nuclear force, which is also ultimately caused by the strong interaction. (Before the discovery of quarks, the term \"strong interaction\" referred to just internucleon interactions.)\n\nNucleons sit at the boundary where particle physics and nuclear physics overlap. Particle physics, particularly quantum chromodynamics, provides the fundamental equations that explain the properties of quarks and of the strong interaction. These equations explain quantitatively how quarks can bind together into protons and neutrons (and all the other hadrons). However, when multiple nucleons are assembled into an atomic nucleus (nuclide), these fundamental equations become too difficult to solve directly (see lattice QCD). Instead, nuclides are studied within nuclear physics, which studies nucleons and their interactions by approximations and models, such as the nuclear shell model. These models can successfully explain nuclide properties, as for example, whether or not a particular nuclide undergoes radioactive decay.\n\nThe proton and neutron are both baryons and both fermions. The proton carries a positive net charge and the neutron carries a zero net charge; the proton's mass is only about 0.13% less than the neutron's. Thus, they can be viewed as two states of the same nucleon, and together form an isospin doublet (). In isospin space, neutrons can be transformed into protons via SU(2) symmetries, and vice versa. These nucleons are acted upon equally by the strong interaction, which is invariant under rotation in isospin space. According to the Noether theorem, isospin is conserved with respect to the strong interaction.\n\nProtons and neutrons are best known in their role as nucleons, i.e., as the components of atomic nuclei, but they also exist as free particles. Free neutrons are unstable, with a half-life of around 13 minutes, but they are common in nature and have important applications (see neutron radiation and neutron scattering). Singular protons, not bound to other nucleons, are usually regarded as the nuclei of hydrogen atoms or ions, but in some extreme cases (cosmic rays, proton beams), they may be regarded as free protons.\n\nNeither the proton nor neutron is an elementary particle, meaning each is composed of smaller parts, namely three quarks each. A proton is composed of two up quarks and one down quark, while the neutron has one up quark and two down quarks. Quarks are held together by the strong force, or equivalently, by gluons, which mediate the strong force.\n\nAn up quark has electric charge  \"e\", and a down quark has charge  \"e\", so the summed electric charges of proton and neutron are +\"e\" and 0, respectively. Thus, the neutron has a charge of 0 (zero), and therefore is electrically neutral; indeed, the term \"neutron\" comes from the fact that a neutron is electrically neutral.\n\nThe mass of the proton and neutron is quite similar: The proton is or , while the neutron is or . The neutron is roughly 0.13% heavier. The similarity in mass can be explained roughly by the slight difference in masses of up quarks and down quarks composing the nucleons. However, a detailed explanation remains an unsolved problem in particle physics.\n\nThe spin of both protons and neutrons is , which means they are fermions and, like electrons (and unlike bosons), are subject to the Pauli exclusion principle, a very important phenomenon in nuclear physics: protons and neutrons in an atomic nucleus cannot all be in the same quantum state; instead they spread out into nuclear shells analogous to electron shells in chemistry. Also important, this spin (of proton and neutron) is the source of nuclear spin in larger nuclei. Nuclear spin is best known for its crucial role in the NMR/MRI technique for chemical and biochemical analyses.\n\nThe magnetic moment of a proton, denoted μ, is , while the magnetic moment of a neutron is μ = . These parameters are also important in NMR/MRI.\n\nA neutron in free state is an unstable particle, with a half-life around ten minutes. It undergoes decay (a type of radioactive decay) by turning into a proton while emitting an electron and an electron antineutrino. (See the Neutron article for more discussion of neutron decay.) A proton by itself is thought to be stable, or at least its lifetime is too long to measure. This is an important discussion in particle physics, (see Proton decay).\n\nInside a nucleus, on the other hand, combined protons and neutrons (nucleons) can be stable or unstable depending on the nuclide, or nuclear species. Inside some nuclides, a neutron can turn into a proton (producing other particles) as described above; the reverse can happen inside other nuclides, where a proton turns into a neutron (producing other particles) through decay, or electron capture. And inside still other nuclides, both protons and neutrons are stable and do not change form.\n\nBoth nucleons have corresponding antiparticles: the antiproton and the antineutron, which have the same mass and opposite charge as the proton and neutron respectively, and they interact in the same way. (This is generally believed to be \"exactly\" true, due to CPT symmetry. If there is a difference, it is too small to measure in all experiments to date.) In particular, antinucleons can bind into an \"antinucleus\". So far, scientists have created antideuterium and antihelium-3 nuclei.\n\n The masses of the proton and neutron are known with far greater precision in atomic mass units (u) than in MeV/c, due to the relatively poorly known value of the elementary charge. The conversion factor used is 1 u = MeV/c.\nThe masses of their antiparticles are assumed to be identical, and no experiments have refuted this to date. Current experiments show any percent difference between the masses of the proton and antiproton must be less than and the difference between the neutron and antineutron masses is on the order of MeV/c.\n\n† \"The P(939) nucleon represents the excited state of a normal proton or neutron, for example, within the nucleus of an atom. Such particles are usually stable within the nucleus, i.e. Lithium-6.\"\n\nIn the quark model with SU(2) flavour, the two nucleons are part of the ground state doublet. The proton has quark content of \"uud\", and the neutron, \"udd\". In SU(3) flavour, they are part of the ground state octet (8) of spin baryons, known as the Eightfold way. The other members of this octet are the hyperons strange isotriplet , , the and the strange isodoublet . One can extend this multiplet in SU(4) flavour (with the inclusion of the charm quark) to the ground state 20-plet, or to SU(6) flavour (with the inclusion of the top and bottom quarks) to the ground state 56-plet.\n\nThe article on isospin provides an explicit expression for the nucleon wave functions in terms of the quark flavour eigenstates.\n\nAlthough it is known that the nucleon is made from three quarks, , it is not known how to solve the equations of motion for quantum chromodynamics. Thus, the study of the low-energy properties of the nucleon are performed by means of models. The only first-principles approach available is to attempt to solve the equations of QCD numerically, using lattice QCD. This requires complicated algorithms and very powerful supercomputers. However, several analytic models also exist:\n\nThe Skyrmion models the nucleon as a topological soliton in a non-linear SU(2) pion field. The topological stability of the Skyrmion is interpreted as the conservation of baryon number, that is, the non-decay of the nucleon. The local topological winding number density is identified with the local baryon number density of the nucleon. With the pion isospin vector field oriented in the shape of a hedgehog space, the model is readily solvable, and is thus sometimes called the \"hedgehog model\". The hedgehog model is able to predict low-energy parameters, such as the nucleon mass, radius and axial coupling constant, to approximately 30% of experimental values.\n\nThe \"MIT bag model\" confines three non-interacting quarks to a spherical cavity, with the boundary condition that the quark vector current vanish on the boundary. The non-interacting treatment of the quarks is justified by appealing to the idea of asymptotic freedom, whereas the hard boundary condition is justified by quark confinement.\n\nMathematically, the model vaguely resembles that of a radar cavity, with solutions to the Dirac equation standing in for solutions to the Maxwell equations and the vanishing vector current boundary condition standing for the conducting metal walls of the radar cavity. If the radius of the bag is set to the radius of the nucleon, the bag model predicts a nucleon mass that is within 30% of the actual mass.\n\nAlthough the basic bag model does not provide a pion-mediated interaction, it describes excellently the nucleon-nucleon forces through the 6 quark bag s-channel mechanism using the P matrix. \nThe \"chiral bag model\" merges the \"MIT bag model\" and the \"Skyrmion model\". In this model, a hole is punched out of the middle of the Skyrmion, and replaced with a bag model. The boundary condition is provided by the requirement of continuity of the axial vector current across the bag boundary.\n\nVery curiously, the missing part of the topological winding number (the baryon number) of the hole punched into the Skyrmion is exactly made up by the non-zero vacuum expectation value (or spectral asymmetry) of the quark fields inside the bag. , this remarkable trade-off between topology and the spectrum of an operator does not have any grounding or explanation in the mathematical theory of Hilbert spaces and their relationship to geometry. Several other properties of the chiral bag are notable: it provides a better fit to the low energy nucleon properties, to within 5–10%, and these are almost completely independent of the chiral bag radius (as long as the radius is less than the nucleon radius). This independence of radius is referred to as the \"Cheshire Cat principle\", after the fading to a smile of Lewis Carroll's Cheshire Cat. It is expected that a first-principles solution of the equations of QCD will demonstrate a similar duality of quark-pion descriptions.\n\n\n"}
{"id": "14176802", "url": "https://en.wikipedia.org/wiki?curid=14176802", "title": "Pisit Charnsnoh", "text": "Pisit Charnsnoh\n\nPisit Charnsnoh, from Trang Province, Thailand, cofounded the \"Yadfon Association\" in 1985. He was awarded the Goldman Environmental Prize in 2002, for his efforts on protecting the coastal ecosystems of Thailand. Charnsnoh is on the board of the international Seattle-based Mangrove Action Project, and is affiliated with the \"Industrial Shrimp Action Network\".\n"}
{"id": "48883103", "url": "https://en.wikipedia.org/wiki?curid=48883103", "title": "Queensland – New South Wales Interconnector", "text": "Queensland – New South Wales Interconnector\n\nThe Queensland – New South Wales Interconnector (QNI) is a 330 kV AC interconnection between New South Wales and Queensland, Australia. The link was commissioned in 2001. It consisted of double-circuit 330 kV lines between Armidale, Dumaresq, Bulli Creek and Braemar, and a double-circuit 275 kV line between Braemar and Tarong. The original maximum transfer capacity was 300 to 350 MW in both directions. This has been progressively increased to 700 MW from New South Wales to Queensland and 1,200 MW from Queensland to New South Wales. The interconnector is operated by TransGrid and Powerlink Queensland.\n"}
{"id": "6209225", "url": "https://en.wikipedia.org/wiki?curid=6209225", "title": "Rujm el-Hiri", "text": "Rujm el-Hiri\n\nRujm el-Hiri (, \"Rujm al-Hīrī\"; \"Gilgal Refā'īm\" or \"Rogem Hiri\") is an ancient megalithic monument consisting of concentric circles of stone with a tumulus at center. It is located in the Israeli-occupied portion of the Golan Heights, some east of the coast of the Sea of Galilee, in the middle of a large plateau covered with hundreds of dolmens. \nMade up of more than 42,000 basalt rocks arranged in concentric circles, it has a mound tall at its center. Some circles are complete, others incomplete. The outermost wall is in diameter and high. The establishment of the site, and other nearby ancient settlements, is dated by archaeologists to the Early Bronze Age II period (3000–2700 BCE).\n\nSince excavations have yielded very few material remains, Israeli archeologists theorize that the site was not a defensive position or a residential quarter but most likely a ritual center, possibly linked to the cult of the dead.\nHowever, there is no consensus regarding its function, as no similar structure has been found in the Near East.\n\nThe name Rujm el-Hiri, \"stone heap of the wild cat\", was originally taken from Syrian maps. The term \"rujm\" in Arabic (pl. \"rujum\"; Hebrew: \"rogem\") can also refer to a tumulus, a heap of stones underneath which human burial space was located. The name is sometimes romanized as Rujm Hiri or Rujum al-Hiri.\n\nRogem Hiri is a Hebrew version of the Arabic name Rujm el-Hiri. A modern Hebrew name used for the site is \"Gilgal Refā'īm\" or \"Galgal Refā'īm\", \"Wheel of Spirits\" or \"Wheel of Ghosts\" as \"Refa'im\" means \"ghosts\" or \"spirits\".\n\nThe site's size and location, on a wide plateau which is also scattered with hundreds of dolmens, means that an aerial perspective is necessary to see the complete layout. The site was made from Basalt rocks, common in the Golan Heights due to the region's history of volcanic activity. It is made from 37,500 - 40,000 tons of partly worked stone stacked up to high. It was estimated by Freikman that the transportation and building of the massive monument would have required more than 25,000 working days. The site is often referred to as the \"Stonehenge of the Levant.\"\n\nThe remains consist of a large circle (slightly oval) of basalt rocks, containing four smaller concentric circles, each getting progressively thinner; some are complete, others incomplete. The walls of the circles are connected by irregularly placed smaller stone walls perpendicular to the circles.\n\nThe central tumulus is built from smaller rocks, and is thought to have been constructed after the surrounding walls were constructed. Connecting to it are four main stone walls. The first wall, shaped like a semicircle, is 50m in diameter and 1.5m wide. That wall is connected to a second one, an almost complete circle 90m in diameter. The third wall is a full circle, 110m in diameter and 2.6m wide. The fourth and outermost wall is the largest: 150m in diameter and 3.2m wide.\n\nA central tumulus in diameter and high is surrounded by concentric circles, the outermost of which is in diameter and high. Two entrances to the site face the northeast ( wide) and southeast ( wide). The northeast entrance leads to an accessway long leading to the center of the circle which seems to point in the general direction of the June solstice sunrise. The axis of the tomb discovered at the site's center is similarly aligned. Mount Hermon is almost due north and Mount Tabor is close to December solstice sunrise. Geometry and astronomy are visually connected by the temple's design.\n\nThe site was cataloged during an archaeological survey carried out in 1967-1968 by Shmarya Gutman and Claire Epstein. The site is probably the source of the legends about \"a remnant of the giants\" or Rephaim for Og. The surveyors used Syrian maps, and a Syrian triangulation post was found on top of its cairn. After this initial study, serious archaeological excavations commenced in the 1980s under Israeli professors Moshe Kochavi and Yoni Mizrachi, as part of the Land of Geshur Archaeological Project.\n\n\nIn 2007, the site was excavated by Yosef Garfinkel and Michael Freikman of the Hebrew University of Jerusalem. Freikman returned in the summer of 2010 for further investigation of the site's date and function. Freikman believes that the tomb in the center was built at the same time as the rings. Tomb robbers looted the remains, which included jewelry and weapons, but based on the discovery of one Chalcolithic pin dropped in a passageway, Freikman's theory is that the tomb was the centerpiece of the rings.\n\nNew Age movements advocating a return to nature gather at the site on the summer solstice and on the equinox to view the first rays of the sun shine though the rocks. \n\nThe Golan Trail, a marked 130-kilometer walking trail that stretches along the whole length of the Golan Heights, passes Gilgal Refa'im.\n\n"}
{"id": "3979979", "url": "https://en.wikipedia.org/wiki?curid=3979979", "title": "Sabesp", "text": "Sabesp\n\nSabesp is a Brazilian water and waste management company owned by São Paulo state. It provides water and sewage services to residential, commercial and industrial users in São Paulo and in 363 of the 645 municipalities in São Paulo State, typically under 30-year concession contracts. It provides water to 26.7 million customers, or 60% of the population of the state. It is the largest water and waste management company in Latin América. It provides basic sanitation services, which include all phases (abstraction, treatment, processing, distribution) and the collection, treatment and reuse of sewage. The São Paulo Metropolitan Region and the Regional Systems accounted for 74.5% and 25.5% of the sales and services rendered during the year ended December 31, 2004 respectively. Sabesp also supplies water on a bulk basis to municipalities in the São Paulo Metropolitan Area, in which it does not operate water systems to local operators.\n\nIn 2009 Sabesp had 15,103 employees for 7.12 million water connections, corresponding to 2.1 employees per 1,000 connections, indicating a high level of labor productivity.\n\nSABESP was founded in 1973. Its stocks were first floated on the São Paulo stock exchange in 1996. In 2002-2004 the São Paulo Government sold a further equity stakes, including a listing at the New York Stock Exchange. Today 49.8% of its shares are privately owned. In 2006 a law was passed that allowed SABESP to expand its activities into other Brazilian states and internationally. It has signed cooperation agreements in Spain, Israel and Costa Rica. According to its CEO SABESP wants to expand to serve all cities in São Paulo State.\n\n"}
{"id": "1714439", "url": "https://en.wikipedia.org/wiki?curid=1714439", "title": "Samarium–cobalt magnet", "text": "Samarium–cobalt magnet\n\nA samarium–cobalt (SmCo) magnet, a type of rare earth magnet, is a strong permanent magnet made of an alloy of samarium and cobalt. They were developed in the early 1960s based on work done by Karl Strnat and Alden Ray at Wright-Patterson Air Force Base and the University of Dayton, respectively. In particular, Strnat and Ray developed the first formulation of SmCo. They are generally ranked similarly in strength to neodymium magnets, but have higher temperature ratings and higher coercivity. They are brittle, and prone to cracking and chipping. Samarium–cobalt magnets have maximum energy products (BH) that range from 16 megagauss-oersteds (MG·Oe) to 33 MG·Oe, that is approx. 128 kJ/m to 264 kJ/m; their theoretical limit is 34 MG·Oe, about 272 kJ/m. They are available in two \"series\", namely Series 1:5 and Series 2:17.\n\nSintered Samarium Cobalt magnets exhibit magnetic anisotropy, meaning they can only be magnetized in the axis of their magnetic orientation. This is done by aligning the crystal structure of the material during the manufacturing process.\n\nThese samarium–cobalt magnet alloys (generally written as SmCo, or SmCo Series 1:5) have one atom of rare earth samarium per five atoms of cobalt. By weight this magnet alloy will typically contain 36% samarium with the balance cobalt. The energy products of these samarium–cobalt alloys range from 16 MG·Oe to 25 MG·Oe, that is, approx. 128–200 kJ/m. These samarium–cobalt magnets generally have a reversible temperature coefficient of -0.05%/°C. Saturation magnetization can be achieved with a moderate magnetizing field. This series of magnet is easier to calibrate to a specific magnetic field than the SmCo 2:17 series magnets.\n\nIn the presence of a moderately strong magnetic field, unmagnetized magnets of this series will try to align their orientation axis to the magnetic field, thus becoming slightly magnetized. This can be an issue if postprocessing requires that the magnet be plated or coated. The slight field that the magnet picks up can attract debris during the plating or coating process, causing coating failure or a mechanically out-of-tolerance condition.\n\n\"B\" drifts with temperature and it is one of the important characteristics of magnet performance. Some applications, such as inertial gyroscopes and travelling wave tubes (TWTs), need to have constant field over a wide temperature range. The reversible temperature coefficient (RTC) of \"B\" is defined as\n\nTo address these requirements, temperature compensated magnets were developed in the late 1970s. For conventional SmCo magnets, \"B\" decreases as temperature increases. Conversely, for GdCo magnets, \"B\" increases as temperature increases within certain temperature ranges. By combining samarium and gadolinium in the alloy, the temperature coefficient can be reduced to nearly zero.\n\nSmCo magnets have a very high coercivity (coercive force); that is, they are not easily demagnetized. They are fabricated by packing wide-grain lone-domain magnetic powders. All of the magnetic domains are aligned with the easy axis direction. In this case, all of the domain walls are at 180 degrees. When there are no impurities, the reversal process of the bulk magnet is equivalent to lone-domain motes, where coherent rotation is the dominant mechanism. However, due to the imperfection of fabricating, impurities may be introduced in the magnets, which form nuclei. In this case, because the impurities may have lower anisotropy or misaligned easy axes, their directions of magnetization are easier to spin, which breaks the 180° domain wall configuration. In such materials, the coercivity is controlled by nucleation. To obtain much coercivity, impurity control is critical in the fabrication process.\n\nThese alloys (written as SmCo, or SmCo Series 2:17) are age-hardened with a composition of two atoms of rare-earth samarium per 13–17 atoms of transition metals (TM). The TM content is rich in cobalt, but contains other elements such as iron and copper. Other elements like zirconium, hafnium, and such may be added in small quantities to achieve better heat treatment response. By weight, the alloy will generally contain 25% of samarium. The maximum energy products of these alloys range from 20 to 32 MGOe, what is about 160-260 kJ/m. These alloys have the best reversible temperature coefficient of all rare-earth alloys, typically being -0.03%/°C. The \"second generation\" materials can also be used at higher temperatures.\n\nIn SmCo magnets, the coercivity mechanism is based on domain wall pinning. Impurities inside the magnets impede the domain wall motion and thereby resist the magnetization reversal process. To increase the coercivity, impurities are intentionally added during the fabrication process.\n\nThe alloys are typically machined in the unmagnetized state. Samarium–cobalt should be ground using a wet grinding process (water-based coolants) and a diamond grinding wheel. The same type of process is required if drilling holes or other features that are confined. The grinding waste produced must not be allowed to completely dry as samarium–cobalt has a low ignition point. A small spark, such as that produced with static electricity, can easily commence combustion. The fire produced will be extremely hot and difficult to control.\n\nThe reduction/melt method and reduction/diffusion method are used to manufacture samarium–cobalt magnets. The reduction/melt method will be described since it is used for both SmCo and SmCo production. The raw materials are melted in an induction furnace filled with argon gas. The mixture is cast into a mold and cooled with water to form an ingot. The ingot is pulverized and the particles are further milled to further reduce the particle size. The resulting powder is pressed in a die of desired shape, in a magnetic field to orient the magnetic field of the particles. Sintering is applied at a temperature of 1100˚C–1250˚C, followed by solution treatment at 1100˚C–1200˚C and tempering is finally performed on the magnet at about 700˚C–900˚C. It then is ground and further magnetized to increase its magnetic properties. The finished product is tested, inspected and packed.\n\n\n\nSamarium - cobalt magnet has a strong resistance to corrosion and oxidation resistance, usually do not need to be coated can be widely used in high temperature and poor working conditions.\n\nFender used one of designer Bill Lawrence's Samarium Cobalt Noiseless series of electric guitar pickups in Fender's Vintage Hot Rod '57 Stratocaster. These pickups were used in American Deluxe Series Guitars and Basses from 2004 until early 2010. In the mid-1980s some expensive headphones such as the Ross RE-278 used Samarium Cobalt \"Super Magnet\" transducers.\n\nOther uses include:\n\n"}
{"id": "52445983", "url": "https://en.wikipedia.org/wiki?curid=52445983", "title": "Solar power in Armenia", "text": "Solar power in Armenia\n\nSolar energy is widely available in Armenia due to its geographical position and is considered a developing industry.\n\nAccording to the Ministry of Energy Infrastructures and Natural Resources of Armenia, Armenia has an average of about 1720 kilowatt hour (kWh) solar energy flow per square meter of horizontal surface annually and has a potential of 1000 MW power production. In the capital Yerevan, the average solar energy flux is equal to 1642 kWh/m. Armenia's area cannot be considered as homogeneous from the perspective of available solar energy: the difference between the amount of solar energy reaching the ground in different places in the country can be up to 20% in the summer time, and 50% in the winter time.\n\nThe use of solar energy in Armenia is increasing. One example is the American University of Armenia (AUA). According to International Network for Sustainable Energy, solar panels were installed on the rooftop of American University of Armenia in 1999-2002. Also, AUA has solar heaters, which are used to heat up auditoriums, and supply hot water during cold periods. Another example is the United Nations House in Armenia. Its building also uses solar power energy.\n\nThe PV system (Photovoltaics) is used in many countries including Armenia. There are a number of buildings equipped with solar panels, such as the American University of Armenia generating enough power for the elevators and other uses, and the UN House in Armenia, as well as other buildings.\n\nThere are set tariffs for generating electricity using solar energy. In November 2016, Public Services Regulatory Commission of RA made a decision to set the price of electrical energy from photovoltaic systems to 42.645 AMD/kWh. The public can use solar panels of up to 150 kWh power, and any surplus can be sold to Electric Networks of Armenia (ENA). In addition, there are no taxes nor licenses required. By doing so, the Armenian government is trying to encourage its citizens to use renewable energy.\n\nIn Armenia, solar thermal collectors, or water-heaters, are produced in standard sizes (1.38-4.12 square meters). Solar water-heaters can be used for space heating, solar cooling, etc. In order to generate heat, they use solar energy from the Sun. Modern solar water֊heaters can cause water to boil even in winter․\n\nSolar thermal collectors are used throughout the territory of Armenia. One building using solar thermal collectors is AUA, which uses solar cooling and ventilation systems. The biggest solar water-heater in Armenia is located at Diana hotel in Goris, which has 1900 vacuum tubes that provide hot water for a swimming pool with 180 cubic meter volume, and for 40 hotel rooms.\n\nThe main companies that operate in this sector in Armenia are Arpi Solar Ltd, Simartek (in partnership with Solitek Armenia), ArtClima, General Electric Lighting Armenia (Rubinar LLC), Elips Technoline, Hatuk Santekhmontazh CJSC (SanHit Specialized Shop), Shtigen, Technoeco, Solaron (local PV panel manufacturer).\n\nOne of the main factors preventing the development of solar energy in Armenia is the installation cost. The price for a solar photo-voltaic power plant per 1 kW is very high.\n\n\n"}
{"id": "43827616", "url": "https://en.wikipedia.org/wiki?curid=43827616", "title": "South Australian Chamber of Mines and Energy", "text": "South Australian Chamber of Mines and Energy\n\nThe South Australian Chamber of Mines and Energy (SACOME) is the industry body representing companies with interests in the South Australian minerals, energy, extractive, and oil & gas sectors, including those who provide services to these companies. SACOME is a not-for-profit, non-government organisation founded in 1979, and comprises a small team serving the needs of approximately 200 member organisations. SACOME are governed by a council that oversees organisational priorities and policy direction and are guided by many committees that focus on specific aspects of the sector. \n\nOver the years the organisation has published a number of regular periodicals including the \"SA Mines and Energy journal\", has been the subject of several feature articles in the resources sector magazine \"Australia's Paydirt\", and recently has released \"SACOME Priorities: State Election 2018\". \n\nSACOME were founded in 1979.\n\nIn 2014, SACOME launched Dirt TV, a competition to encourage school students in years 7 to 12 to produce short videos creatively promoting the resources sector. The inaugural award was won by high school students James Haskard and Daniel Blake of Concordia College. 14 entries were received and resource company sponsors provided a total prize pool of $10,000.\n\nSACOME believes that the growth of iron ore mining in South Australia has been limited by the state's lack of bulk commodities port infrastructure. In 2011, SACOME's CEO Jason Kuchel publicly supported the chosen location for a future 3 km iron ore export wharf at Port Bonython, northeast of Whyalla in South Australia's upper Spencer Gulf region. The location is controversial due to its close proximity to breeding reef for the Northern Spencer Gulf population of giant Australian cuttlefish. The proposed port's potential environmental impact has been challenged by community groups including Save Point Lowly and the Alternative Port Working Party.\n\nSince then other locations have been considered including the Iron Road Limited proposal at Cape Hardy on the Eyre Peninsula.\n\nSACOME supports the future development of nuclear power in South Australia. Among its members are several companies actively involved in uranium mining and exploration. These include BHP Billiton, Areva Resources Australia, Heathgate Resources and Uranium SA. SACOME's Chief Executive Jason Kuchel believes that small modular nuclear reactors could potentially provide energy to remote resources projects, including at mine sites. Kuchel's advocacy has been acknowledged by Australian Mining magazine as having influenced the establishment of the Nuclear Fuel Cycle Royal Commission in 2016.\n\nSACOME is governed by an elected council that comprises leaders from within the South Australian mining and energy industry\n\nAt January 2018 their members were: \n\n2014:\n\n2015:\n"}
{"id": "7777698", "url": "https://en.wikipedia.org/wiki?curid=7777698", "title": "Synopses of the British Fauna", "text": "Synopses of the British Fauna\n\nSynopses of the British Fauna is a series of identification guides, published by The Linnean Society and The Estuarine and Coastal Sciences Association. Each volume in the series provides and in-depth analysis of a group of animals and is designed to bridge the gap between the standard field guide and more specialised monograph or treatise. The series is now published by The Field Studies Council on behalf of The Linnean Society and The Estuarine and Coastal Sciences Association. \n\nThe series is designed for use in the field and is kept as user friendly as possible with technical terminology kept to a minimum and a glossary of terms provided, although the complexity of the subject matter makes the books more suitable for the more experienced practitioner.\n\nOn 11 March 1943, at a meeting of The Linnean Society in Burlington House, TH Savoy presented his \"Synopsis of the Opiliones\" (Harvestmen). It was so well received that a decision was made there and then to publish it as the first of a series of \"ecological fauna lists\".\n\nRe-launched by Dr Doris Kermack in the mid-1960s, the New Series of \"Synopses of the British Fauna\" went from strength to strength. From number 13, the series had been jointly sponsored by The Estuarine and Coastal Sciences Association and Dr RSK Barnes became co-editor.\n\nFrom 1993, the series has been published by The Field Studies Council and benefits from association with the extensive testing undertaken as part of the AIDGAP project.\n\nThe series contains the following volumes, many of which are out of print. Many of the volumes have been updated and reprinted under slightly different names to reflect either taxonomic changes or advances in the understanding of a group.\n\n\n"}
{"id": "50440388", "url": "https://en.wikipedia.org/wiki?curid=50440388", "title": "Tetrabutylammonium", "text": "Tetrabutylammonium\n\nTetrabutylammonium is a quaternary ammonium cation with the formula [N(CH)]. It is used in the research laboratory to prepare lipophilic salts of inorganic anions. Relative to tetraethylammonium derivatives, tetrabutylammonium salts are more lipophilic but crystallize less readily.\n\nSome tetrabutylammonium salts of simple anions include:\n\nSome tetrabutylammonium salts of more complex examples include:\n"}
{"id": "47365509", "url": "https://en.wikipedia.org/wiki?curid=47365509", "title": "The Messenger (2015 documentary film)", "text": "The Messenger (2015 documentary film)\n\nThe Messenger is a 2015 documentary film written and directed by Su Rynard, focusing on the protection of multiple types of songbirds throughout the world. The film's world premiere took place at the Hot Docs Canadian International Documentary Festival on 2015-04-28.\n\nThe film features German composer and DJ Dominik Eulberg, expert Dr. Bridget Stutchbury, Michael Mesure, Dr. Christy Morrissey, Turkish ecologist Çağan Şekercioğlu, and ornithologist Alejandra Martinez-Salinas.\n\nAt the Hot Docs festival, \"The Messenger\" finished third in the audience balloting.\n\nUS Rights for the film were acquired by Kino Lorber, which started a limited theatrical run beginning in December 2015.\n\n"}
{"id": "2026564", "url": "https://en.wikipedia.org/wiki?curid=2026564", "title": "Tissue paper", "text": "Tissue paper\n\nTissue paper or simply tissue is a lightweight paper or, light crêpe paper. Tissue can be made from recycled paper pulp.\nKey properties are absorbency, basis weight, thickness, bulk (specific volume), brightness, stretch, appearance and comfort. Tissues are also napkins.\n\nTissue paper is produced on a paper machine that has a single large steam heated drying cylinder (Yankee dryer) fitted with a hot air hood. The raw material is paper pulp. The Yankee cylinder is sprayed with adhesives to make the paper stick. Creping is done by the Yankee's doctor blade that is scraping the dry paper off the cylinder surface. The crinkle (crêping) is controlled by the strength of the adhesive, geometry of the doctor blade, speed difference between the Yankee and final section of the paper machine and paper pulp characteristics.\nTissue Paper Converting Machines with Jumbo Rolls attached.\n\nThe highest water absorbing applications are produced with a through air drying (TAD) process. These papers contain high amounts of NBSK and CTMP. This gives a bulky paper with high wet tensile strength and good water holding capacity. The TAD process uses about twice the energy compared with conventional drying of paper.\n\nThe properties are controlled by pulp quality, crêping and additives (both in base paper and as coating). The wet strength is often an important parameter for tissue.\n\nHygienic tissue paper is commonly used for facial tissue (paper handkerchiefs), napkins, bathroom tissue and household towels. Paper has been used for hygiene purposes for centuries, but tissue paper as we know it today was not produced in the United States before the mid-1940s. In Western Europe large scale industrial production started in the beginning of the 1960s.\n\nFacial tissue (paper handkerchiefs) refers to a class of soft, absorbent, disposable paper that is suitable for use on the face. The term is commonly used to refer to the type of facial tissue, usually sold in boxes, that is designed to facilitate the expulsion of nasal mucus although it may refer to other types of facial tissues including napkins and wipes.\n\nThe first tissue handkerchiefs were introduced in the 1920s. They have been refined over the years, especially for softness and strength, but their basic design has remained constant. Today each person in Western Europe uses about 200 tissue handkerchiefs a year, with a variety of 'alternative' functions including the treatment of minor wounds, the cleaning of face and hands and the cleaning of spectacles.\n\nThe importance of the paper tissue on minimising the spread of an infection has been highlighted in light of fears over a swine flu epidemic. In the UK, for example, the Government ran a campaign called “Catch it, bin it, kill it”, which encouraged people to cover their mouth with a paper tissue when coughing or sneezing.\n\nPaper towels are the second largest application for tissue paper in the consumer sector. This type of paper has usually a basis weight of 20 to 24 g/m. Normally such paper towels are two-ply. This kind of tissue can be made from 100% chemical pulp to 100% recycled fibre or a combination of the two. Normally, some long fibre chemical pulp is included to improve strength.\n\nWrapping tissue is a type of thin, translucent tissue paper used for wrapping/packing various articles & cushioning fragile items.\n\nCustom-printed wrapping tissue is becoming a popular trend for boutique retail businesses. There are various on-demand custom printed wrapping tissue paper available online. Sustainably printed custom tissue wrapping paper are printed on FSC-certified, acid-free paper; and only use soy-based inks.\n\nRolls of toilet paper have been available since the end of the 19th century. Today, more than 20 billion rolls of toilet tissue are used each year in Western Europe.\n\nTable napkins can be made of tissue paper. These are made from one up to four plies and in a variety of qualities, sizes, folds, colours and patterns depending on intended use and prevailing fashions. The composition of raw materials varies a lot from deinked to chemical pulp depending on quality.\n\nColored paper napkins can be a source of carcinogenic primary aromatic amines (\"paA\"s) when used as a wrapper for food as a result of degradation of Azo compounds used as paper dyes.\n\nIn the late 1970s and early 1980s, a sound recording engineer named Bob Clearmountain was said to have hung tissue paper over the tweeter of his pair of Yamaha NS-10 speakers to tame the over-bright treble coming from it. \n\nThe phenomenon became the subject of hot debate and an investigation into the sonic effects of many different types of tissue paper. The authors of a study for \"Studio Sound\" magazine suggested that had the speakers' grilles been used in studios, they would have had the same effect on the treble output as the improvised tissue paper filter. Another tissue study found inconsistent results with different paper, but said that tissue paper generally demonstrated an undesirable effect known as \"comb filtering\", where the high frequencies are reflected back into the tweeter instead of being absorbed. The author derided the tissue practice as \"aberrant behavior\", saying that engineers usually fear comb filtering and its associated cancellation effects, suggesting that more controllable and less random electronic filtering would be preferable.\n\nTissue paper, in the form of standard single-ply toilet paper, is commonly used in road repair to protect crack sealants. The sealants require upwards of 40 minutes to cure enough to not stick onto passing traffic. The application of toilet paper removes the stickiness and keeps the tar in place, allowing the road to be reopened immediately and increasing road repair crew productivity. The paper breaks down and disappears in the following days. The use has been credited to Minnesota Department of Transportation employee Fred Muellerleile, who came up with the idea in 1970 after initially trying standard office paper, which worked, but did not disintegrate easily.\n\nApart from above, a range of speciality tissues are also manufactured to be used in the packing industry. These are used for wrapping/packing various items, cushioning fragile items, stuffing in shoes/bags etc. to keep shape intact or, for inserting in garments etc. while packing/folding to keep them wrinkle free and safe. It is generally used printed with the manufacturers brand name or, logo to enhance the look and aesthetic appeal of the product. It is a type of thin, translucent paper generally in the range of grammages between 17 and 40 GSM, that can be rough or, shining, hard or soft, depending upon the nature of use.\n\nIn North America, people are consuming around three times as much tissue as in Europe.\nOut of the world's estimated production of of tissue, Europe produces approximately .\n\nThe European tissue market is worth approximately 10 billion Euros annually and is growing at a rate of around 3%. The European market represents around 23% of the global market. Of the total paper and board market tissue accounts for 10%. An analysis and market research in Europe, Germany was one of the top tissue-consuming countries in Western Europe while Sweden was on top of the per-capita consumption of tissue paper in Western Europe. \nMarket Study.\n\nIn Europe, the industry is represented by the \"European Tissue Symposium\" (ETS), a trade association. The members of ETS represent the majority of tissue paper producers throughout Europe and about 90% of total European tissue production. ETS was founded in 1971 and is based in Brussels since 1992.\n\nIn the U.S., the tissue industry is organized in the AF&PA.\n\nTissue paper production and consumption is predicted to continue to grow because of factors like urbanization, increasing disposable incomes and consumer spending. In 2015, the global market for tissue paper is growing at per annum rates between 8-9% (China, currently 40% of global market) and 2-3% (Europe).\n\nThe largest tissue producing companies by capacity - some of them also global players - in 2015 are (in descending order):\n\nThe paper industry in general has a long history of accusations for being responsible for global deforestation through legal and illegal logging. The WWF has urged Asia Pulp & Paper (APP), \"one of the world's most notorious deforesters\" especially in Sumatran rain forests, to become an environmentally responsible company; in 2012, the WWF launched a campaign to remove a brand of toilet paper known to be made from APP fiber from grocery store shelves. According to the Worldwatch Institute, the world per capita consumption of toilet paper was 3.8 kilograms in 2005. The WWF estimates that \"every day, about 270,000 trees are flushed down the drain or end up as garbage all over the world\", a rate of which about 10% are attributable to toilet paper alone.\n\nMeanwhile, the paper tissue industry, along with the rest of the paper manufacturing sector, has worked to minimise its impact on the environment. Recovered fibres now represent some 46.5% of the paper industry's raw materials. The industry relies heavily on biofuels (about 50% of its primary energy). Its specific primary energy consumption has decreased by 16% and the specific electricity consumption has decreased by 11%, due to measures such as improved process technology and investment in combined heat and power (CHP). Specific carbon dioxide emissions from fossil fuels decreased by 25% due to process-related measures and the increased use of low-carbon and biomass fuels. Once consumed, most forest-based paper products start a new life as recycled material or biofuel\n\nEDANA, the trade body for the non-woven absorbent hygiene products industry (which includes products such as household wipes for use in the home) has reported annually on the industry’s environmental performance since 2005. Less than 1% of all commercial wood production ends up as wood pulp in absorbent hygiene products. The industry contributes less than 0.5% of all solid waste and around 2% of municipal solid waste (MSW) compared with paper and board, garden waste and food waste which each comprise between 18 and 20 percent of MSW.\n\nThere has been a great deal of interest, in particular, in the use of recovered fibres to manufacture new tissue paper products. However, whether this is actually better for the environment than using new fibres is open to question. A Life Cycle Assessment study indicated that neither fibre type can be considered environmentally preferable. In this study both new fibre and recovered fibre offer environmental benefits and shortcomings.\n\nTotal environmental impacts vary case by case, depending on for example the location of the tissue paper mill, availability of fibres close to the mill, energy options and waste utilization possibilities. There are opportunities to minimise environmental impacts when using each fibre type.\n\nWhen using recovered fibres, it is beneficial to:\nWhen using new fibres, it is beneficial to:\nWhen using either fibre type, it is beneficial to:\n\nThe Confederation of European Paper Industries (CEPI) has published reports focusing on the industry’s environmental credentials. In 2002, it noted that “a little over 60% of the pulp and paper produced in Europe comes from mills certified under one of the internationally recognised eco-management schemes”. There are a number of ‘eco-labels’ designed to help consumers identify paper tissue products which meet such environmental standards. Eco-labelling entered mainstream environmental policy-making in the late seventies, first with national schemes such as the German Blue Angel programme, to be followed by the Nordic Swan (1989). In 1992 a European eco-labelling regulation, known as the EU Flower, was also adopted. The stated objective is to support sustainable development, balancing environmental, social and economical criteria.\n\nThere are three types of eco-labels, each defined by ISO (International Organization for Standardization).\n\nType I: ISO 14024\nThis type of eco-label is one where the criteria are set by third parties (not the manufacturer). They are in theory based on life cycle impacts and are typically based on pass/fail criteria. The one that has European application is the EU Flower.\n\nType II: ISO 14021\nThese are based on the manufacturers or retailers own declarations. Well known amongst these are claims of “100% recycled” in relation to tissue/paper.\n\nType III: ISO 14025\nThese claims give quantitative details of the impact of the product based on its life cycle. Sometimes known as EPDs (Environmental Product Declarations), these labels are based on an independent review of the life cycle of the product. The data supplied by the manufacturing companies are also independently reviewed.\n\nThe most well known example in the paper industry is the Paper Profile. You can tell a Paper Profile meets the Type III requirements when the verifiers logo is included on the document.\n\nAn example of an organization that sets standards is the Forest Stewardship Council.\n\n"}
