{"id": "14018485", "url": "https://en.wikipedia.org/wiki?curid=14018485", "title": "1-2-3 (fuel station)", "text": "1-2-3 (fuel station)\n\n1-2-3 is an unmanned fuel station chain in the Nordic and Baltic regions.\n\nIt was created in 2000, as the low cost extension of the Statoil chain, and owned by Statoil Fuel & Retail ASA. The first outlet was opened in Kaunas in December 2000. 65 outlets were planned in the Baltic, later to be supplemented by 107 outlets in Norway and Denmark. Unlike the Statoil stations, there are no franchisees, all stations are vertically integrated.\n"}
{"id": "33564053", "url": "https://en.wikipedia.org/wiki?curid=33564053", "title": "2011 Halloween nor'easter", "text": "2011 Halloween nor'easter\n\nThe 2011 Halloween nor'easter, sometimes referred to as \"Snowtober,\" \"Shocktober,\" \"Storm Alfred,\" and \"Oktoberblast,\" was a large low pressure area that produced unusually early snowfall across the northeastern United States and the Canadian Maritimes. It formed early on October 29 along a cold front to the southeast of the Carolinas. As it moved up the East Coast, its associated snowfall broke records in at least 20 cities for total accumulations, resulting in a rare \"white Halloween\" two days later.\n\nThe storm arrived just two months after Hurricane Irene caused extensive power outages and property damage in the Northeast; with the 2011 New England tornado outbreak also causing damage in Western Massachusetts. It dumped snow on trees that were often still in leaf, adding extra weight, with the ground in some areas still soft from a preceding warm, rainy period that increased the possibility trees could be uprooted. Trees and branches that collapsed caused considerable damage, particularly to power lines, with estimates of storm costs ranging between $1 billion and $3 billion. In all, 3.2 million U.S. residences and businesses in 12 states experienced power outages, with the storm also impacting three Canadian provinces.\n\nSome customers in Connecticut did not get power back until early November; many outages lasted 11 days Many communities chose to postpone celebrations of Halloween from two days to a week later as a result, or cancel them entirely. Delays in restoring power led to the resignation of the chief operating officer of Connecticut Light & Power amid widespread criticism of the company's mishandling of both the nor'easter and Irene.\n\nEarly on October 28, 2011, a ridge over Canada advected an unseasonably cold air mass across the Mid-Atlantic states and New England; at the same time, a surface low-pressure area began developing along the coast of Louisiana. A cold front moved eastward from the Ohio Valley and exited the East Coast of the United States, developing another low pressure area off the coast of the Carolinas on October 29. The remnants of Hurricane Rina had also been absorbed into the developing system. At the same time, an area of precipitation extended from South Carolina through Pennsylvania, mostly falling as rain with some snow observed at higher elevations. By late that morning, the system was producing precipitation over much of the Mid-Atlantic and New England.\n\nAs the system moved to the northeast through the day, it produced widespread snow and winds near hurricane-strength north of the cyclone's warm front over the open waters of the Atlantic Ocean. Winds as high as were observed in Massachusetts, and the National Weather Service, issued a Hurricane Force Wind Warning for the Gulf of Maine and other high seas off New England. Overnight into October 30, the storm passed south of Nantucket, and it moved over Nova Scotia later that day with a barometric pressure of . As it did so, the associated precipitation diminished over New England and moved into Atlantic Canada. As the system moved out into the Atlantic Ocean, it reached a minimum barometric pressure of as it passed to the east of the island of Newfoundland late on October 31. By the early morning of November 1, the system had fully moved out to sea. The storm then hit the UK as Cyclone Quinn.\n\nBefore the storm was at its strongest, local National Weather Service offices issued winter storm warnings from northwestern Virginia through central New England, as well as winter storm watches from central Maryland through central Maine. Officials anticipated peak snowfall totals to be from across much of the region. All warnings were canceled after the storm moved away from the region.\n\nEarly on October 29, the Pennsylvania Department of Transportation activated their fleet of salt trucks. In eastern Pennsylvania, the most recent significant snowstorm during October was in 1972. Utility crews prepared additional crews in the event of power outages. The Delaware Water Gap National Recreation Area closed a road due to the storm's threat. Connecticut governor Dan Malloy opened the state's Emergency Operations Center in Hartford, which included members of the transportation, health, and energy departments. Officials opened 41 shelters in Connecticut. Occupy Wall Street protesters pledged to remain in Lower Manhattan's Zucotti Park despite the weather, obtaining coats and blankets.\n\nPrecipitation began falling in North Carolina and Virginia late on October 28. By early the next day, measurable snowfall had been reported from West Virginia through Maryland, and eventually as far north as Maine. The deepest snowfall reported was , at Peru, Massachusetts. At least 20 cities reported record-breaking totals, and the peak of in West Milford, New Jersey broke that state's record for highest snowfall in October. Newark, New Jersey's largest city, also broke its all-time October snowfall record with . Central Park in Manhattan observed , also a record. Hartford, Connecticut's state capital, observed a record , and the highest total in the state was in Farmington southwest of Hartford; this, too, broke the state record for an October snowfall. As the storm moved into Canada, it dropped rain in Nova Scotia and snow in New Brunswick and Newfoundland. Parts of the inland regions of Newfoundland received close to of snow.\n\nIn Massachusetts, the nor'easter brought wind gusts peaking at in Barnstable and, unofficially, in Provincetown.\nAn automated marine weather station at Mount Desert Rock off the coast of Maine recorded a top gust of . \n\nThe nor'easter storm became the 14th multibillion-dollar weather-related disaster of 2011, breaking the three-year-old record of nine. Across the Northeast, the combination of high winds and wet, heavy snow downed trees, most of which retained their leaves, did extensive damage. In New York City, a thousand trees were estimated to have fallen in Central Park, far more than had been damaged by Hurricane Irene two months earlier, just as had been reported in Connecticut. The New York Botanical Garden in the Bronx reported that 2,200 trees in its old-growth forest, the only one remaining in the city from the era prior to European colonization, were damaged. Downed trees caused widespread power outages, leaving over 3 million people without power. In Central Park workers put a priority on making the park safe for the annual New York City Marathon the next weekend.\n\nTraffic accidents killed at least six people. Two were electrocuted by downed power lines. Overall, there were 39 deaths.\n\nThe storm affected transportation across the Northeast. Two rail services were closed in the New York area, and Amtrak service across the region was either delayed or canceled. NJ Transit suspended service on the Morris and Essex Lines until November 1 due to downed wires and branches, and even then was only able to restore service as far west as Lake Hopatcong. North of New York City, Metro-North suspended commuter rail service on the Harlem Line north of North White Plains, leaving passengers marooned on a train at Southeast for 11 hours when fallen trees blocked the tracks in both directions. Service was also suspended on the Port Jervis Line and the New Canaan, Danbury and Waterbury branches of the New Haven Line. Service on the Port Jervis Line and electrified portions of the Harlem Line was restored on Monday; bus service replaced trains between Southeast and Wassaic and on the Danbury and Waterbury branches for the rest of the week.\n\nThe storm also disrupted air travel from Pennsylvania through Connecticut. Officials at Newark International Airport canceled all flights after 4 pm on October 29, and flights out of New York's two major airports were delayed by up to five hours. Some flights bound for New York were diverted to Hartford. Several JetBlue flights departing from Bradley International Airport there were stranded on the tarmac for up to seven hours due to the hazardous conditions.\n\nSome roads were also affected. Along the Jersey Shore, the nor'easter produced coastal flooding that left Ventnor Heights isolated. Officials closed a portion of the Black Horse Pike in West Atlantic City due to flooding. Further north, the flooding closed five New Jersey state highways in Monmouth and Ocean counties.\n\nSporting events on Saturday, mostly college and high school football games, were also impacted. Penn State officials limited parking at its home game in State College to 1,500 spaces due to the inclement weather. It was the first Nittany Lions home October football game with measurable snow since record keeping began in 1896. At West Point, New York, Army defeated Fordham 55–0 in its first home game played in snow since 1985. On Long Island where a wind swept mixture of heavy snow and rain fell, a match between Plainview – Old Bethpage John F. Kennedy High School and Valley Stream Central High School was cancelled after 15 players were treated for hypothermia, prompting the former school district to reconsider game cancellation policies. At another football game on Long Island 10 players were checked and some treated for the condition. Several players at Farmingdale High School reported extreme fatigue for several weeks following playing in the storm.\n\nMany traditional Halloween activities were affected by the storm. In communities without electricity, where tree limbs and wires were down, trick-or-treating was delayed until days when it was expected to be back and repairs had made the streets safer. This also occurred in communities where electricity was still fully or partially on but the streets still may have been unsafe. In Sleepy Hollow, New York, a popular destination for the holiday since Washington Irving's classic short story, \"The Legend of Sleepy Hollow\" is set there, Halloween events were canceled due to the storm and its aftermath. On the other hand, another popular Halloween destination, Salem, Massachusetts, location of the 1692 witch trials, was unaffected due to its minimal snowfall. \nSome families were able to compensate for the lost Halloween. They took their children to trick-or-treat in other communities that still had electricity. Residents of Glen Rock, New Jersey, organized a \"trunk-or-treat\" party at the local high school's football field, where children went around to parked sport-utility vehicles. Since many schools had snow days, and there was little to distract children without electricity, many parents insisted on going ahead with the holiday. \"You can't cancel Halloween\" said a woman in Fairfield, New Jersey. \"The kids are all hyped up. They had no school because there's no electricity and this and that.\" A boy in Lexington, Massachusetts said he now planned to \"buy some candy and eat it myself.\"\n\nAt UMass Amherst, the storm caused a power outage over a traditional party weekend at the college that lasted throughout most of the night on Saturday, October 29. While power was largely restored by Sunday, October 30, the campus cancelled classes on October 31, and the UMass Campus Center served as a rest location for students and area residents who were still without power. Amherst College saw similar conditions, with the university newspaper reporting that students were taking refuge from power outages in Valentine Dining Hall, with one student describing the response as similar to hurricanes in her home state of Florida.\n\nProposals in some communities to hold Halloween the following weekend, or whenever conditions returned to normal, met with protest from some parents. Some considered the October 31 date to be immutable and nonnegotiable, so children would have to wait for 2012. \"I don't have control over the calendar, so Halloween is on Halloween, which is the 31st\", said Pat Murphy, mayor of New Milford, Connecticut. She noted the town had managed to celebrate the holiday that day on its village green despite considerable storm damage and continued power outages. Others had already allowed their children some trick-or-treating, and did not want them to indulge in candy a second time within the week.\n\nMany school districts were forced by the storm to use up their remaining allotted snow days for the school year, after Hurricane Irene and Tropical Storm Lee had required some be used near the beginning of the year. The Weston, Connecticut, public schools had already used nine snow days as of November 2, five more than its schedule allowed. Since more snow days would inevitably be used during the upcoming winter, they predicted that vacation periods planned for later in the year would have to be shortened or canceled, or the school year would have to be extended.\n\nThe storm came at a critical time for high school seniors preparing college applications for early decision, and 76 colleges and universities moved those deadlines back to compensate. In Connecticut, Weston High School, which had power, opened its library for students wishing to study or work on their applications; movies were shown in the auditorium.\nApproximately 1.7 million customers in the Northeast were still without electricity three days after the storm. Temperatures in the region warmed up to above during the day, but went down to near freezing at night. As powerless houses grew cold, residents bundled up and kept under blankets, went to stay or visit with others who had electricity, or used their car heaters to temporarily warm up. Some, frustrated by long blackouts after other recent storms, considered leaving the region or moving to cities where power lines were underground.\nAbout half a million households in New Jersey lost electricity, prompting a state of emergency declaration from governor Chris Christie. He had himself suffered outages both at his house in Mendham and the governor's mansion, Drumthwacket, near Princeton.\n\nIn Connecticut, Governor Dannel Malloy declared a state of emergency late on October 29, after 830,000 people lost power, breaking the record set after Hurricane Irene. In the Danbury area, outages were so prolonged that seven school districts had to cancel classes for the following week. A state of emergency was also declared in Massachusetts, which allowed for the activation of the state's National Guard as well as other emergency measures. Due to the power outages and downed trees shortly before Halloween, at least three towns in the state advised delaying trick-or-treating. In New Hampshire, officials opened seven shelters for people who lost heating during the storm. The early snowfall allowed for the opening of ski resorts in Vermont and Maine. \nAt a November 1 press conference, Governor Malloy estimated that damages in Connecticut would exceed $3 billion. Two days later, close to 700,000 homes and businesses remained without power. A week after the storm, almost 150,000 customers of the state's two utilities had not yet had power restored. Customers still suffering outages continued to cope as best they could, by sleeping at the homes of friends who had already had their electricity restored, taking showers at work and storing perishable foods outside. Power was not restored to all the customers who had lost it in the storm until November 9.\n\nMany Connecticut residents were angry with the state's electric utilities, particularly Connecticut Light & Power (CL&P), which serves most of the state, for the long delays in restoring service. By the weekend after the storm, in comparison, most customers in other affected states had already gotten their electricity back. Malloy said they had \"missed their own target\" and ordered an investigation into their preparation and restoration efforts to be led by James Lee Witt, director of the Federal Emergency Management Agency during the Clinton Administration. An Avon man complained he had not seen any crews in his area since the storm. Some commentators felt Malloy was being too lenient with CL&P, noting that the company had cut its maintenance budget in the preceding year and that smaller public utilities, such as that serving the city of Norwich, had experienced far less power loss and for far less time despite CL&P's customers paying the highest rates in the contiguous United States. Similar complaints had been made after the company had taken a long time to restore service after Hurricane Irene, and three weeks after the nor'easter, Jeff Butler, the company's CEO, resigned.\n\nA month after the storm, Malloy released Witt's report, which concluded that \"CL&P was not prepared for an event of this size\". The utility had planned for a worst-case scenario in which 100,000 customers lost power, only one-eighth of those actually affected by the nor'easter. While the report noted that such a storm had not hit in 25 years when the company's emergency plan was drawn up in June, it nevertheless faulted CL&P for merely telling its emergency crews to be on call that weekend, instead of having them wait at predetermined locations. As a result, it was harder to mobilize them when the effects turned out to be far worse than they anticipated despite warnings from the NWS using terms such as \"historic\" and \"catastrophic\" prior to the storm.\n\nThe report also said that CL&P did not ask for crews from neighboring states until after the storm, at which point other New England utilities were also seeking help from them. \"Because of that poor preparation, it's not surprising that they didn't, or that they couldn't, respond with enough boots on the ground when the worst-case scenario was compounded by a factor of eight\", said Malloy. The report did praise some aspects of CL&P's response, such as its short call wait times, speedy repairs by crews once they reached their job sites and the absence of death or serious injury among responding utility crews.\n"}
{"id": "56360825", "url": "https://en.wikipedia.org/wiki?curid=56360825", "title": "ATAŞ (Refinery)", "text": "ATAŞ (Refinery)\n\nATAŞ, short for \"Anadolu Tasfiyehanesi Anonim Şirketi\" (literally: Anatolian Refinery Joint-stock Company), is a former oil refinery company in Mersin, southern Turkey. Currently, The facility is used today as an oil storage and terminal.\n\nATAŞ was established in 1958 following a special agreement between the Turkish government and the foreign oil companies Mobil, Royal Dutch Shell, BP and Caltex. Caltex later sold its share to Mobil. Currently, the main shareholder is BP with 68%. Other partners are Shell & Turcas Petrol (27%) and Turcas (5%).\n\nThe refinery was built by the Swiss contractor Foster Wheeler AG. It was put into operation on 30 April 1962. The company's headquarter as well as its facilities are located in Akdeniz district of Mersin at . Its annual production was 3.2 million tonnes. On 7 June 2004, ATAŞ gave up refining crude oil. With the shut down of the refinery after 42-year of production, households and industry facilities in 24 provinces of the Mediterranean and Southeastern Anatolia regions faced fuel oil shortage. Temporary supply from the far away Tüpraş Kırıkkale Oil Refinery caused an increase of oil prices around 11%.\n\nThe facility was converted after investments into a large-scale storage and terminal for fuel oil, diesel oil and gasoline. It underwent a renovation between 2004 and 2006 carried out by Foster Wheeler, the original contractor in the 1960s. The terminal on the Mediterranean Sea coast is suitable for docking of high-capacity tankers. The storage capacity of ATAŞ terminal is .\n\nBeginning in 2015, the local administration in Akdeniz district has been making efforts to convert the campus of ATAŞ Terminal, which stress over and became inactive after the closure of the refinery, into a public park for social and cultural events by preserving its historical characteristic.\n"}
{"id": "40300017", "url": "https://en.wikipedia.org/wiki?curid=40300017", "title": "AdS/CMT correspondence", "text": "AdS/CMT correspondence\n\nIn theoretical physics, anti-de Sitter/condensed matter theory correspondence is the program to apply string theory to condensed matter theory using the AdS/CFT correspondence.\n\nOver the decades, experimental condensed matter physicists have discovered a number of exotic states of matter, including superconductors and superfluids. These states are described using the formalism of quantum field theory, but some phenomena are difficult to explain using standard field theoretic techniques. Some condensed matter theorists including Subir Sachdev hope that the AdS/CFT correspondence will make it possible to describe these systems in the language of string theory and learn more about their behavior.\n\nSo far some success has been achieved in using string theory methods to describe the transition of a superfluid to an insulator. A superfluid is a system of electrically neutral atoms that flows without any friction. Such systems are often produced in the laboratory using liquid helium, but recently experimentalists have developed new ways of producing artificial superfluids by pouring trillions of cold atoms into a lattice of criss-crossing lasers. These atoms initially behave as a superfluid, but as experimentalists increase the intensity of the lasers, they become less mobile and then suddenly transition to an insulating state. During the transition, the atoms behave in an unusual way. For example, the atoms slow to a halt at a rate that depends on the temperature and on Planck's constant, the fundamental parameter of quantum mechanics, which does not enter into the description of the other phases. This behavior has recently been understood by considering a dual description where properties of the fluid are described in terms of a higher dimensional black hole.\n\nWith many physicists turning towards string-based methods to address problems in condensed matter physics, some theorists working in this area have expressed doubts about whether the AdS/CFT correspondence can provide the tools needed to realistically model real-world systems. In a letter to Physics Today, Nobel laureate Philip W. Anderson wrote\n\n\n"}
{"id": "2889394", "url": "https://en.wikipedia.org/wiki?curid=2889394", "title": "Affinity laws", "text": "Affinity laws\n\nThe affinity laws (Also known as the \"Fan Laws\" or \"Pump Laws\") for pumps/fans are used in hydraulics, hydronics and/or HVAC to express the relationship between variables involved in pump or fan performance (such as head, volumetric flow rate, shaft speed) and power. They apply to pumps, fans, and hydraulic turbines. In these rotary implements, the affinity laws apply both to centrifugal and axial flows.\n\nThe laws are derived using the Buckingham π theorem. The affinity laws are useful as they allow prediction of the head discharge characteristic of a pump or fan from a known characteristic measured at a different speed or impeller diameter. The only requirement is that the two pumps or fans are dynamically similar, that is the ratios of the fluid forced are the same.\n\nLaw 1. With impeller diameter (D) held constant:\n\nLaw 1a. Flow is proportional to shaft speed: \nLaw 1b. Pressure or Head is proportional to the square of shaft speed:\n\nLaw 1c. Power is proportional to the cube of shaft speed:\n\nLaw 2. With shaft speed (N) held constant: \n\nLaw 2a. Flow is proportional to the cube of impeller diameter: \nLaw 2b. Pressure or Head is proportional to the square of the impeller diameter:\n\nLaw 2c. Power is proportional to the power 5 of impeller diameter (assuming constant shaft speed):\n\nwhere \n\nThese laws assume that the pump/fan efficiency remains constant i.e. formula_12, which is rarely exactly true, but can be a good approximation when used over appropriate frequency or diameter ranges (i.e., a fan will not move anywhere near 1000 times as much air when spun at 1000 times its designed operating speed, but the air movement may be increased by 99% when the operating speed is only doubled). The exact relationship between speed, diameter, and efficiency depends on the particulars of the individual fan or pump design. Product testing or computational fluid dynamics become necessary if the range of acceptability is unknown, or if a high level of accuracy is required in the calculation. Interpolation from accurate data is also more accurate than the affinity laws. When applied to pumps the laws work well for constant diameter variable speed case (Law 1) but are less accurate for constant speed variable impeller diameter case (Law 2).\nFor radial flow centrifugal pumps, it is common industry practice to reduce the impeller diameter by \"trimming\", whereby the outer diameter of a particular impeller is reduced by machining to alter the performance of the pump. In this particular industry it is also common to refer to the mathematical approximations that relate the volumetric flow rate, trimmed impeller diameter, shaft rotational speed, developed head, and power as the \"affinity laws\". Because trimming an impeller changes the fundamental shape of the impeller (increasing the specific speed), the relationships shown in Law 2 cannot be utilized in this scenario. In this case the industry looks to the following relationships, which is a better approximation of these variables when dealing with impeller trimming.\n\nWith shaft speed (N) held constant and for small variations in impeller diameter via trimming: \n\nThe volumetric flow rate varies directly with the trimmed impeller diameter:\nThe pump developed head (the total dynamic head) varies to the square of the trimmed impeller diameter:\n\nThe power varies to the cube of the trimmed impeller diameter:\n\nwhere \n\n\n"}
{"id": "44717055", "url": "https://en.wikipedia.org/wiki?curid=44717055", "title": "Aluminum electrolytic capacitor", "text": "Aluminum electrolytic capacitor\n\nAluminum electrolytic capacitors are polarized electrolytic capacitors whose anode electrode (+) is made of a pure aluminum foil with an etched surface. The aluminum forms a very thin insulating layer of aluminium oxide by anodization that acts as the dielectric of the capacitor. A non-solid electrolyte covers the rough surface of the oxide layer, serving in principle as the second electrode (cathode) (-) of the capacitor. A second aluminum foil called “cathode foil” contacts the electrolyte and serves as the electrical connection to the negative terminal of the capacitor.\n\nAluminum electrolytic capacitors are divided into three subfamilies by the type of electrolyte: \n\nAluminum electrolytic capacitors with non-solid electrolyte are the most inexpensive type and also those with widest range of sizes, capacitance and voltage values. They are made with capacitance values from 0.1 µF up to 2,700,000 µF (2.7 F), and rated voltages values from 4 V up to 630 V. The liquid electrolyte provides oxygen for re-forming or self-healing of the dielectric oxide layer. However, it can evaporate through a temperature-dependent drying-out process, which causes electrical parameters to drift, limiting the service life time of the capacitors.\n\nDue to their relatively high capacitance values aluminum electrolytic capacitors have low impedance values even at lower frequencies like mains frequency. They are typically used in power supplies, switched-mode power supplies and DC-DC converters for smoothing and buffering rectified DC voltages in many electronic devices as well as in industrial power supplies and frequency converters as DC link capacitors for drives, inverters for photovoltaic, and converters in wind power plants. Special types are used for energy storage, for example in photoflash or strobe applications or for frequency coupling in audio applications.\n\nAluminum electrolytic capacitors are polarized capacitors because of their anodization principle. They can only be operated with DC voltage applied with the correct polarity. Operating the capacitor with wrong polarity or with AC voltage leads to a short circuit and can destroy the component. The exceptions is the bipolar aluminum electrolytic capacitor, which has a back-to-back configuration of two anodes in one case and can be used in AC applications.\n\nElectrolytic capacitors use a chemical feature of some special metals, earlier called \"valve metals\". Applying a positive voltage to the anode material in an electrolytic bath forms an insulating oxide layer with a thickness corresponding to the applied voltage. This oxide layer acts as the dielectric in an electrolytic capacitor. The properties of this aluminum oxide layer compared with tantalum pentoxide dielectric layer are given in the following table:\n\nAfter forming a dielectric oxide on the rough anode structures, a counter-electrode has to match the rough insulating oxide surface. This is provided by the electrolyte, which acts as the cathode electrode of an electrolytic capacitor. Electrolytes may be \"non-solid\" (wet, liquid) or \"solid\". Non-solid electrolytes, as a liquid medium that has an \"ion conductivity\" caused by moving ions, are relatively insensitive to voltage spikes or current surges. Solid electrolytes have an \"electron conductivity\", which makes solid electrolytic capacitors sensitive to voltages spikes or current surges.\n\nThe anodic generated insulating oxide layer is destroyed if the polarity of the applied voltage changes.\nEvery electrolytic capacitor in principle forms a \"plate capacitor\" whose capacitance is greater the larger the electrode area A and the permittivity ε, and the thinner the thickness (d) of the dielectric. \n\nThe capacitance is proportional to the product of the area of one plate multiplied with the permittivity, divided by the thickness of the dielectric.\n\nElectrolytic capacitors obtain their large capacitance values by a large area and small dielectric thickness. The dielectric thickness of electrolytic capacitors is very thin, in the range of nanometers per volt, but the voltage strengths of these oxide layers are quite high. All etched or sintered anodes have a much higher surface compared to a smooth surface of the same area. This increases the capacitance value by a factor of up to 200 for aluminum electrolytic capacitors.\n\nAn aluminum electrolytic capacitor with a non-solid electrolyte always consists of two aluminum foils separated mechanically by a spacer, mostly paper, which is saturated with a liquid or gel-like electrolyte. One of the aluminum foils, the anode, is etched (roughened) to increase the surface and oxidized (formed). The second aluminum foil, called the \"cathode foil\", serves to make electrical contact with the electrolyte. A paper spacer mechanically separates the foils to avoid direct metallic contact. Both foils and the spacer are wound and the winding is impregnated with liquid electrolyte. The electrolyte, which serves as cathode of the capacitor, covers the etched rough structure of the oxide layer on the anode perfectly and makes the increased anode surface effectual. After impregnation the impregnated winding is mounted in an aluminum case and sealed.\n\nBy design, a non-solid aluminum electrolytic capacitor has a second aluminum foil, the so-called cathode foil, for contacting the electrolyte. This structure of an aluminum electrolytic capacitor results in a characteristic result because the second aluminum (cathode) foil is also covered with an insulating oxide layer naturally formed by air. Therefore, the construction of the electrolytic capacitor consists of two single series-connected capacitors with capacitance C of the anode and capacitance C of the cathode. The total capacitance of the capacitor C is thus obtained from the formula of the series connection of two capacitors:\n\nIt follows that the total capacitance of the capacitor C is mainly determined by the anode capacitance C when the cathode capacitance C is very large compared with the anode capacitance C. This requirement is given when the cathode capacitance C is approximately 10 times higher than the anode capacitance C. This can be easily achieved because the natural oxide layer on a cathode surface has a voltage proof of approximately 1.5 V and is therefore very thin.\n\nAlthough the present article only refers in essence to aluminum electrolytic capacitors with non-solid electrolyte, an overview of the different types of aluminum electrolytic capacitors is given here in order to highlight the differences. Aluminum electrolytic capacitors are divided into two sub-types depending on whether they make use of liquid or solid electrolyte systems. Because the different electrolyte systems can be constructed with a variety of different materials, they include further sub-types.\n\nDescription of the materials\n\nThe following table shows an overview over the main characteristics of the different types of aluminum electrolytic capacitors.\n\nAluminum electrolytic capacitors with non-solid electrolyte are the best known and most widely used electrolytic capacitors. These components can be found on almost all boards of electronic equipment. They are characterized by particularly inexpensive and easy to process base materials.\n\nAluminum capacitors with liquid electrolytes based on borax or organic solvents have a large range of types and ratings. Capacitors with water-based electrolytes are often found in digital devices for mass production. Types with solid manganese dioxide electrolyte have served in the past as a \"tantalum replacement\". Polymer aluminum electrolytic capacitors with solid conductive polymer electrolytes are becoming increasingly important, especially in devices with a flat design, such as tablet PCs and flat panel displays. Electrolytic capacitors with hybrid electrolytes are relatively new on the market. With their hybrid electrolyte system they combine the improved conductivity of the polymer with the advantage of liquid electrolytes for better self-healing property of the oxide layer, so that the capacitors have the advantages of both low ESR and low leakage current.\n\nThe basic material of the anode for aluminum electrolytic capacitors is a foil with a thickness of ~ 20–100 µm made of aluminum with a high purity of at least 99.99%. This is etched (roughened) in an electrochemical process to increase the effective electrode surface. By etching the surface of the anode, depending on the required rated voltage, the surface area can be increased by a factor of approximately 200 with respect to a smooth surface.\n\nAfter etching the aluminum anode the roughed surface is \"anodic oxidized\" or \"formed\". An electrically insulating oxide layer AlO is thereby formed on the aluminum surface by application of a current in correct polarity if it is inserted in an electrolytic bath. This oxide layer is the capacitor dielectric.\n\nThis process of oxide formation is carried out in two reaction steps whereby the oxygen for this reaction has to come from the electrolyte. First, a strongly exothermic reaction transforms the metallic aluminum (Al) into aluminum hydroxide, Al(OH):\n\nThis reaction is accelerated by a high electric field and high temperatures, and is accompanied by a pressure buildup in the capacitor housing caused by the released hydrogen gas. The gel-like aluminum hydroxide Al(OH), also called alumina trihydrate (ATH), is converted via a second reaction step (usually slowly over a few hours at room temperature, more rapidly in a few minutes at higher temperatures) into aluminum oxide, AlO:\n\nThe aluminum oxide serves as dielectric and also protects the metallic aluminum against aggressive chemical reactions from the electrolyte. However, the converted layer of aluminum oxide is usually not homogeneous. It forms a complex multilayer structured laminate of amorphous, crystalline and porous crystalline aluminum oxide mostly covered with small residual parts of unconverted aluminum hydroxide. For this reason, in the formation of the anode foil, the oxide film is structured by a special chemical treatment so that either an amorphous oxide or a crystalline oxide is formed. The amorphous oxide variety yields higher mechanical and physical stability and fewer defects, thus increasing the long term stability and lowering the leakage current.\n\nAmorphous oxide has a dielectric ratio of ~ 1.4 nm/V. Compared to crystalline aluminum oxide, which has a dielectric ratio of ~1.0 nm/V, the amorphous variety has a 40% lower capacitance at the same anode surface. The disadvantage of crystalline oxide is its greater sensitivity to tensile stress, which may lead to microcracks when subjected to mechanical (winding) or thermal (soldering) stressors during the post-forming processes.\n\nThe various properties of oxide structures affect the subsequent characteristics of the electrolytic capacitors. Anode foils with amorphous oxide are primarily used for electrolytic capacitors with stable long-life characteristics, for capacitors with low leakage current values, and for e-caps with rated voltages up to about 100 volts. Capacitors with higher voltages, for example photoflash capacitors, usually containing anode foils with crystalline oxide.\n\nBecause the thickness of the effective dielectric is proportional to the forming voltage, the dielectric thickness can be tailored to the rated voltage of the capacitor. For example, for low voltage types a 10 V electrolytic capacitor has a dielectric thickness of only about 0.014 µm, a 100 V electrolytic capacitor of only about 0.14 µm. Thus, the dielectric strength also influences the size of the capacitor. However, due to standardized safety margins the actual forming voltage of electrolytic capacitors is higher than the rated voltage of the component.\n\nAluminum anode foils are manufactured as so-called \"mother rolls\" of about 500 mm in width. They are pre-formed for the desired rated voltage and with the desired oxide layer structure. To produce the capacitors, the anode widths and lengths, as required for a capacitor, have to be cut from the mother roll.\n\nThe second aluminum foil in the electrolytic capacitor, called the \"cathode foil\", serves to make electrical contact with the electrolyte. This foil has a somewhat lower degree of purity, about 99.8%. It is always provided with a very thin oxide layer, which arises from the contact of the aluminum surface with the air in a natural way. In order to reduce the contact resistance to the electrolyte and to make it difficult for oxide formation during discharging, the cathode foil is alloyed with metals such as copper, silicon, or titanium. The cathode foil is also etched to enlarge the surface.\n\nBecause of the extremely thin oxide layer, which corresponds to a voltage proof of about 1.5 V, their specific capacitance is, however, much higher than that of anode foils. To justify the need for a large surface capacitance of the cathode foil see the section on charge/discharge stability below.\n\nThe cathode foils, as the anode foils, are manufactured as so-called \"mother rolls\", from which widths and lengths are cut off, as required, for capacitor production.\n\nThe electrolytic capacitor got its name from the electrolyte, the conductive liquid inside the capacitor. As a liquid it can be adapted to the porous structure of the anode and the grown oxide layer with the same shape and form as a \"tailor-made\" cathode. An electrolyte always consists of a mixture of solvents and additives to meet given requirements. The main electrical property of the electrolyte is its conductivity, which is physically an ion-conductivity in liquids. In addition to the good conductivity of operating electrolytes, various other requirements are, among other things, chemical stability, high flash point, chemical compatibility with aluminum, low viscosity, low environmental impact and low costs. The electrolyte should also provide oxygen for forming and self-healing processes, and all this within a temperature range as wide as possible. This diversity of requirements for the liquid electrolyte results in a wide variety of proprietary solutions.\n\nThe electrolytic systems used today can be roughly summarized into three main groups:\n\n\nSince the amount of liquid electrolyte during the operating time of the capacitors decreases over time through self-healing and by diffusion through the seal, the electrical parameters of the capacitors may be adversely affected, limiting the service life or lifetime of \"wet\" electrolytic capacitors, see the section on lifetime below.\n\nThe anode and cathode foils must be protected from direct contact with each other because such contact, even at relatively low voltages, may lead to a short circuit. In case of direct contact of both foils the oxide layer on the anode surface gives no protection. A spacer or separator made of a special highly absorbent paper with high purity protects the two metal foils from direct contact. This capacitor paper also serves as a reservoir for the electrolyte to extend the lifetime of the capacitor.\n\nThe thickness of the spacer depends on the rated voltage of the electrolytic capacitor. It is up to 100 V between 30 and 75 µm. For higher voltages, several layers of paper (duplex paper) are used to increase the breakdown strength.\n\nThe encapsulation of aluminum electrolytic capacitors is also made of aluminum in order to avoid galvanic reactions, normally with an aluminum case (can, tub). For radial electrolytic capacitors it is connected across the electrolyte with a non-defined resistance to the cathode (ground). For axial electrolytic capacitors, however, the housing is specifically designed with a direct contact to the cathode.\n\nIn case of a malfunction, overload or wrong polarity operating inside the electrolytic capacitor housing, substantial gas pressure can arise. The tubs are designed to open a pressure relief vent and release high pressure gas, including parts of the electrolyte. This vent protects against bursting, explosion or fly away of the metal tub.\n\nFor smaller housings the pressure relief vent is carved in the bottom or the notch of the tub. Larger capacitors like screw-terminal capacitors have a lockable overpressure vent and must be mounted in an upright position.\n\nThe sealing materials of aluminum electrolytic capacitors depend on the different styles. For larger screw-terminal and snap-in capacitors the sealing washer is made of a plastic material. Axial electrolytic capacitors usually have a sealing washer made of phenolic resin laminated with a layer of rubber. Radial electrolytic capacitors use a rubber plug with a very dense structure. All sealing materials must be inert to the chemical parts of the electrolyte and may not contain soluble compounds that could lead to contamination of the electrolyte. To avoid leakage, the electrolyte must not be aggressive to the sealing material.\n\nThe production process starts with mother rolls. First, the etched, roughened and pre-formed anode foil on the mother roll as well as the spacer paper and the cathode foil are cut to the required width. The foils are fed to an automatic winder, which makes a wound section in a consecutive operation involving three sequential steps: terminal welding, winding, and length cutting. In the next production step the wound section fixed at the lead out terminals is soaked with electrolyte under vacuum impregnation. The impregnated winding is then built into an aluminum case, provided with a rubber sealing disc, and mechanically tightly sealed by curling. Thereafter, the capacitor is provided with an insulating shrink sleeve film. This optically ready capacitor is then contacted at rated voltage in a high temperature post-forming device for healing all the dielectric defects resulting from the cutting and winding procedure. After post-forming, a 100% final measurement of capacitance, leakage current, and impedance takes place. Taping closes the manufacturing process; the capacitors are ready for delivery.\n\nAluminum electrolytic capacitors with non-solid electrolyte are available in different styles, see pictures above from left to right:\n\nIn 1875, French researcher Eugène Ducretet discovered that certain \"valve metals\" (aluminum and others) can form an oxide layer that blocks an electric current from flowing in one direction but allows it to flow in the reverse direction.\n\nKarol Pollak, a producer of accumulators, found out that the oxide layer on an aluminum anode remained stable in a neutral or alkaline electrolyte, even when the power was switched off. In 1896 he obtained a patent for an \"Electric liquid capacitor with aluminium electrodes\" (de: \"Elektrischer Flüssigkeitskondensator mit Aluminiumelektroden\") based on the idea of using the oxide layer in a polarized capacitor in combination with a neutral or slightly alkaline electrolyte.\n\nThe first electrolytic capacitors realized industrially consisted of a metallic box used as cathode, filled with a borax electrolyte dissolved in water, in which a folded aluminum anode plate was inserted. Applying a DC voltage from outside, an oxide layer was formed on the surface of the anode. The advantage of these capacitors was that they were significantly smaller and cheaper than all other capacitors at this time with respect to realized capacitance value. This construction with different styles of anode construction but with a case as cathode and a container as the electrolyte was used up to the 1930s and was called a \"wet\" electrolytic capacitor, referring to its high water content.\n\nThe first common application of wet aluminum electrolytic capacitors was in large telephone exchanges, to reduce relay hash (noise) on the 48 volt DC power supply. The development of AC-operated domestic radio receivers in the late 1920s created a demand for large-capacitance (for the time) and high-voltage capacitors for the valve amplifier technique, typically at least 4 microfarads and rated at around 500 volts DC. Waxed paper and oiled silk film capacitors were available, but devices with that order of capacitance and voltage rating were bulky and prohibitively expensive.\n\nThe ancestor of the modern electrolytic capacitor was patented by Samuel Ruben in 1925, who teamed with Philip Mallory, the founder of the battery company that is now known as Duracell International. Ruben's idea adopted the stacked construction of a silver mica capacitor. He introduced a separate second foil to contact the electrolyte adjacent the anode foil instead of using the electrolyte-filled container as the cathode of the capacitor. The stacked second foil got its own terminal additional to the anode terminal and the container had no longer an electrical function. This type of electrolytic capacitor with one anode foil separated from a cathode foil by a liquid or gel-like electrolyte of a non-aqueous nature, which is therefore dry in the sense of having a very low water content, became known as the \"dry\" type of electrolytic capacitor. This invention, together with the invention of wound foils separated with a paper spacer 1927 by A. Eckel, Hydra-Werke (Germany), reduced the size and the price significantly, which helped make the new radios affordable for a broader group of customers.\n\nWilliam Dubilier, whose first patent for electrolytic capacitors was filed in 1928, industrialized the new ideas for electrolytic capacitors and started large-scale commercial production in 1931 in the Cornell-Dubilier (CD) factory in Plainfield, New Jersey. At the same time in Berlin, Germany, the \"Hydra-Werke\", an AEG company, started the production of electrolytic capacitors in large quantities.\n\nAlready in his patent application of 1886 Pollak wrote that the capacitance of the capacitor increased if the surface of the anode foil was roughened. A number of methods have since been developed for roughening the anode surface, mechanical methods like sand blasting or scratching, and chemical etching with acids and acid salts forced by high currents. Some of these methods were developed in the CD factory between 1931 and 1938. Today (2014), electro-chemically etching of low voltage foils can achieve up to a 200 fold increase in surface area compared to a smooth surface. Progress relating to the etching process is the reason for the ongoing reduction in the dimensions of aluminum electrolytic capacitors over the past decades.\n\nThe period after World War II is associated with a rapid development in radio and television technology as well as in industrial applications, which had great influence on production quantities but also on styles, sizes and series diversification of electrolytic capacitors. New electrolytes based on organic liquids reduced leakage currents and ESR, broadened temperature ranges and increased lifetimes. Corrosion phenomena caused by chlorine and water could be avoided by a higher purity manufacturing processes and by using additives in the electrolytes.\n\nThe development of tantalum electrolytic capacitors in the early 1950s with manganese dioxide as solid electrolyte, which has a 10 times better conductivity than all other types of non-solid electrolytes, also influenced the development of aluminum electrolytic capacitors. In 1964 the first aluminum electrolytic capacitors with solid electrolyte (Solid aluminum capacitor (SAL)) appeared on the market, developed by Philips.\n\nThe decades from 1970 to 1990 were marked by the development of various new professional aluminum electrolytic capacitor series with f. e. very low leakage currents or with long life characteristics or for higher temperatures up to 125 °C, which were specifically suited to certain industrial applications. The great diversity of the many series of aluminum electrolytic capacitors with non-solid electrolytes up to now (2014) is an indicator of the adaptability of the capacitors to meet different industrial requirements.\n\nIn 1983 a further reduction of the ESR was achieved by Sanyo with its \"OS-CON\" aluminum electrolytic capacitors. These capacitors use as solid organic conductor the charge transfer salt TTF-TCNQ (tetracyanoquinodimethane), which provided an improvement in conductivity by a factor of 10 with respect to the manganese dioxide electrolyte.\n\nThe ESR values of TCNQ-capacitors were significantly reduced by the discovery of conducting polymers by Alan J. Heeger, Alan MacDiarmid and Hideki Shirakawa. The conductivity of conductive polymers such as polypyrrole or PEDOT are better than that of TCNQ by a factor of 100 to 500, and are close to the conductivity of metals. In 1991 Panasonic put its \"SP-Cap\", a polymer aluminum\nelectrolytic capacitor, on the market. These electrolytic capacitors with polymer electrolytes achieved ESR values low enough to compete with ceramic multilayer capacitors (MLCCs). They were still less expensive than tantalum capacitors and were a short time later used in devices with a flat design, such as laptops and cell phones.\n\nNew water-based electrolytes were developed in Japan from the mid-1980s with the goal of reducing ESR for inexpensive non-solid electrolytic capacitors. Water is inexpensive, an effective solvent for electrolytes, and significantly improves the conductivity of the electrolyte.\n\nThe Japanese manufacturer Rubycon was a leader in the development of new water-based electrolyte systems with enhanced conductivity in the late 1990s. The new series of non-solid capacitors with water-based electrolyte was called in the data sheets \"Low-ESR\", \"Low-Impedance\", \"Ultra-Low-Impedance\" or \"High-Ripple Current\" series.\n\nA stolen recipe of such a water-based electrolyte, in which important stabilizing substances were absent, led in the years 2000 to 2005 to the problem of mass-bursting capacitors in computers and power supplies, which became known under the term \"Capacitor Plague\". In these capacitors the water reacts quite aggressively and even violently with aluminum, accompanied by strong heat and gas development in the capacitor, and often leads to the explosion of the capacitor.\n\nThe electrical characteristics of capacitors are harmonized by the international generic specification IEC 60384-1. In this standard, the electrical characteristics of capacitors are described by an idealized series-equivalent circuit with electrical components that model all ohmic losses, capacitive and inductive parameters of an electrolytic capacitor:\n\n\nThe basic unit of electrolytic capacitors capacitance is the microfarad (μF, or less correctly uF).\n\nThe capacitance value specified in manufacturers' data sheets is called the rated capacitance C or nominal capacitance C and is the value for which the capacitor has been designed. Standardized measuring conditions for electrolytic capacitors are an AC measurement with 0.5 V at a frequency of 100/120 Hz and a temperature of 20 °C.\n\nThe capacitance value of an electrolytic capacitor depends on the measuring frequency and temperature. The value at a measuring frequency of 1 kHz is about 10% less than the 100/120 Hz value. Therefore, the capacitance values of electrolytic capacitors are not directly comparable and differ from those of film capacitors or ceramic capacitors, whose capacitance is measured at 1 kHz or higher.\n\nMeasured with an AC measuring method with 100/120 Hz the measured capacitance value is the closest value to the electrical charge stored in the capacitor. The stored charge is measured with a special discharge method and is called DC capacitance. The DC capacitance is about 10% higher than the 100/120 Hz AC capacitance. The DC capacitance is of interest for discharge applications like photoflash.\n\nThe percentage of allowed deviation of the measured capacitance from the rated value is called capacitance tolerance. Electrolytic capacitors are available in different tolerance series, whose values are specified in the E series specified in IEC 60063. For abbreviated marking in tight spaces, a letter code for each tolerance is specified in IEC 60062.\n\nThe required capacitance tolerance is determined by the particular application. Electrolytic capacitors that are often used for filtering and bypassing capacitors do not need narrow tolerances because they are not used for accurate frequency applications, such as for oscillators.\n\nIn IEC 60384-1 the allowed operating voltage is called the \"rated voltage\" U or the \"nominal voltage\" U. The rated voltage is the maximum DC voltage or peak pulse voltage that may be applied continuously at any temperature within the rated temperature range.\n\nThe voltage proof of electrolytic capacitors, which is directly proportional to the dielectric layer thickness, decreases with increasing temperature. For some applications it is important to use a high temperature range. Lowering the voltage applied at a higher temperature maintains safety margins. For some capacitor types, therefore, the IEC standard specifies a second \"temperature derated voltage\" for a higher temperature range, the \"category voltage\" U. The category voltage is the maximum DC voltage, peak pulse voltage or superimposed AC voltage that may be applied continuously to a capacitor at any temperature within the category temperature range.\n\nAluminum electrolytic capacitors can be applied for a short time with an overvoltage, also called a surge voltage. The surge voltage indicates the maximum voltage value within the temperature range that may be applied during the lifetime at a frequency of 1000 cycles (with a dwell time of 30 seconds and a pause of 5 minutes and 30 seconds in each instance) without causing any visible damage to the capacitor or a capacitance change of more than 15%.\n\nFor capacitors with a rated voltage of ≤ 315 volts the surge voltage is 1.15 times the rated voltage, and for capacitors with a rated voltage exceeding 315 volts the surge voltage is 1.10 times the rated voltage.\n\nAluminum electrolytic capacitors with non-solid electrolyte are relatively insensitive to high and short-term transient voltages higher than the surge voltage, if the frequency and the energy content of the transients is low. This ability depends on the rated voltage and component size. Low energy transient voltages lead to a voltage limitation similar to a zener diode.\n\nThe electrochemical oxide forming processes take place when voltage in correct polarity is applied and generates an additional oxide when transients arise. This formation is accompanied with heat and hydrogen gas generation. This is tolerable if the energy content of the transient is low. However, when a transient peak voltage causes an electric field strength that is too high for the dielectric, it can directly cause a short circuit. An unambiguous and general specification of tolerable transients or peak voltages is not possible. In every case transients arise, the application has to be carefully approved.\n\nElectrolytic capacitors with solid electrolyte cannot withstand transients or peak voltages higher than the surge voltage. Transients for this type of electrolytic capacitor may destroy the component.\n\nElectrolytic capacitors are polarized capacitors and generally require an anode electrode voltage to be positive relative to the cathode voltage. However, the cathode foil of aluminum electrolytic capacitors is provided with a very thin, natural air-originated oxide layer. This oxide layer has a voltage proof of approximately 1 to 1.5 V. Therefore, aluminum electrolytic capacitors with non-solid electrolyte can withstand a very small reverse voltage and, for example, can be measured with an AC voltage of about 0.5 V, as specified in relevant standards.\n\nAt a reverse voltage lower than −1.5 V at room temperature, the cathode aluminum foil begins to build up an oxide layer corresponding to the applied voltage. This is aligned with generating hydrogen gas with increasing pressure. At the same time the oxide layer on the anode foil begins dissolution of the oxide, which weakens the voltage proof. It is now a question of the outside circuit whether the increasing gas pressure from oxidization leads to bursting of the case, or the weakened anode oxide leads to a breakdown with a short circuit. If the outside circuit is high-ohmic the capacitor fails and the vent opens due to high gas pressure. If the outside circuit is low-ohmic, an internal short circuit is more probable. In every case a reverse voltage lower than −1.5 V at room temperature may cause the component to catastrophically fail due to a dielectric breakdown or overpressure, which causes the capacitor to burst, often in a spectacularly dramatic fashion. Modern electrolytic capacitors have a safety vent that is typically either a scored section of the case or a specially designed end seal to vent the hot gas/liquid, but ruptures can still be dramatic.\n\nTo minimize the likelihood of a polarized electrolytic being incorrectly inserted into a circuit, polarity has to be very clearly indicated on the case, see the section headed \"Polarity marking\".\n\nSpecial bipolar capacitors designed for AC operation, usually referred to as \"bipolar\", \"non-polarized\" or \"NP\" types, are available. In these, the capacitors have two anode foils of opposite polarity connected in series. On each of the alternate halves of the AC cycle, one anode acts as a blocking dielectric, preventing reverse voltage from damaging the opposite anode. But these bipolar electrolytic capacitors are not adaptable for main AC applications instead of power capacitors with metallized polymer film or paper dielectric.\n\nIn general, a capacitor is seen as a storage component for electric energy. But this is only one capacitor function. A capacitor can also act as an AC resistor. Especially aluminum electrolytic capacitors are used in many applications as a decoupling capacitors to filter or bypass undesired biased AC frequencies to the ground or for capacitive coupling of audio AC signals. Then the dielectric is used only for blocking DC. For such applications the AC resistance, the impedance is as important as the capacitance value.\n\nThe impedance is the vector sum of reactance and resistance; it describes the phase difference and the ratio of amplitudes between sinusoidally varying voltage and sinusoidally varying current at a given frequency in an AC circuit. In this sense impedance can be used like Ohm's law\n\nIn other words, impedance is a frequency-dependent AC resistance and possesses both magnitude and phase at a particular frequency. \nIn capacitor data sheets, only the impedance magnitude |Z| is specified, and simply written as \"Z\". In this sense the impedance is a measure of the capacitor's ability to pass alternating currents.\n\nImpedance can be calculated using the idealized components of a capacitor's series-equivalent circuit, including an ideal capacitor formula_4,  a resistor formula_5,  and an inductance formula_6.  In this case the impedance at the angular frequency formula_7  is therefore given by the geometric (complex) addition of ESR, by a capacitive reactance (Capacitance)\n\nand by an inductive reactance (Inductance)\n\nThen formula_10 is given by\n"}
{"id": "47295200", "url": "https://en.wikipedia.org/wiki?curid=47295200", "title": "Alupar", "text": "Alupar\n\nAlupar is a Brazilian holding company dedicated to the segments of power generation and transmission. Among all companies in this segment, Alupar is one of the largest in terms of Annual Permitted Revenue and the largest privately held company.\n\nIn 2000 - Alupar started operating in transmission segment. In 2005, the company began operations in hydroelectric generation segment.\n\nIn 2005 - Began operations in hydroelectric generation segment.\n\nIn 2007 - Merger of all transmission and hydroelectric generation companies in the energy sector into the same holding company.\n\nIn 2009 - Private Placement by FI-FGTS\n\nIn 2013 - The company had an initial public offering estimated at BRL 740 million.\n\nIn 2017 - Alupar reported net income of BRL 330.9 million. \nAlupar has infrastructure projects related to the energy sector in Brazil and other countries in Latin America such as Colombia and Peru.\n\nAlupar's shareholder structure consists of a 52% stake held by Guarupart, 12% held by FI- FGTS and 36% held in the public market.\n\nAlupar currently has 29 transmission systems, totalling 7,736 km of transmission lines.\n\n\nAlupar has diversified its electric power matrix by investing in activities of generation plants, such as Hydroelectric Power Plants (HPPs), Small Hydroelectric Power Plants (SHPPs) and Wind Farms.\n\n\n"}
{"id": "751204", "url": "https://en.wikipedia.org/wiki?curid=751204", "title": "Antonio Pacinotti", "text": "Antonio Pacinotti\n\nAntonio Pacinotti (17 June 1841 – 24 March 1912) was an Italian physicist, who was Professor of Physics at the University of Pisa.\n\nPacinotti was born in Pisa, where he also died. He was the son of Luigi Pacinotti and Caterina Catanti, attended the \"istituto arcivescovile Santa Caterina\", and took part in the second war of Italian independence as \"sergente volontario\". He was a student of Carlo Matteucci and graduated in mathematics at Pisa under Riccardo Felici. He was appointed as assistant to the astronomer Giovanni Battista Donati in 1862, professor at the technological institute of Bologna in 1864, professor of physics at the University of Cagliari in 1873, and, finally, successor to his father in 1881 in the chair of technological physics at the University of Pisa. Among his students was Augusto Righi.\n\nPacinotti died in Pisa.\n\nHe is best known for inventing an improved form of direct-current electrical generator, or dynamo, which he built in 1860 and described in a paper published in \"Il Nuovo Cimento\" of 1865. It used a ring armature around which was wrapped a coil of wire, to produce a smoother current than that available from previous types of dynamo. He found that the device could also be used as an electric motor.\n\nIn July 1862, Pacinotti was one of several independent discoverers of the comet 109P/Swift-Tuttle.\n\nLungarno Pacinotti, an embankment of the Arno River in Pisa, is named after him.\n\n"}
{"id": "20353033", "url": "https://en.wikipedia.org/wiki?curid=20353033", "title": "Bada Valley", "text": "Bada Valley\n\nBada Valley or Napu Valley, located in the Lore Lindu National Park in Central Sulawesi, Indonesia, contains hundreds of megaliths going back to the 14th century that are called \"watu\" (\"stone\") in the local Badaic language and \"arca\" (\"statue\") in Indonesian. The purpose of the megaliths and their builders are unknown.\n\n"}
{"id": "15769026", "url": "https://en.wikipedia.org/wiki?curid=15769026", "title": "Baitings Reservoir", "text": "Baitings Reservoir\n\nBaitings Reservoir is a large water supply reservoir operated by Yorkshire Water close to Ripponden in the West Yorkshire Pennines. It lies in the valley of the River Ryburn and is the higher of two reservoirs built to supply Wakefield with water and was completed in 1956. The lower reservoir is Ryburn Reservoir.\n"}
{"id": "36046859", "url": "https://en.wikipedia.org/wiki?curid=36046859", "title": "Barro Blanco Dam", "text": "Barro Blanco Dam\n\nBarro Blanco is a gravity dam currently under construction on the Tabasara River in the Chiriqui Province of Panama. As of 2015, the project is under construction and is 95 percent completed. Although the government had authorized the dam, it suspended construction in February 2015 after protests. The President of the Republic authorized the partial restart of construction in August 2015, but prohibited the filling of the reservoir pending a final agreement. Important issues concerning the dam are controversial, such as the question whether the indigenous communities in the area initially had expressed their support or their opposition to the dam's construction and how well they had been informed about the project and its impacts prior to expressing their views.\n\nThe dam was designed to minimize the impact on the river, never leaving any section of the river dry. The roller-compacted concrete gravity dam is to have a maximum height of and a maximum reservoir surface of , including currently occupied by the Tabasara River and that would be inundated, according to the project design document submitted to the UN Clean Development Mechanism Executive Board. of the inundated land would be located In the indigenous territory (\"Comarca\") of the Ngöbe–Buglé people. The installed capacity of the planned hydropower plant is 28.84 Megawatt.\n\nThe project developer is \"Generadora del Istmo S. A.\" (GENISA), a Panamanian special purpose company created specifically for this project. GENISA is owned by the Kafie family: the public registry lists Luis Kafie (President), Luis Jose Kafie (Treasurer and Director), Shukri Kafie (Director), Eduardo Kafie (Director), Eduardo Kafie Atala (Director) and Christoper Kafie (Director) as member of the board of GENISA. \n\nThe project is financed by loans that have been provided by two European state-owned banks that promote private sector investments in developing countries, the German Investment Corporation (DEG) and the Netherlands Development Finance Company (FMO), as well as by the Central American Bank for Economic Integration (CABEI). CABEI approved a USD 25 million loan to the project developer in 2011. Total funding from the three banks amounts to USD 78.3 million.\n\nThere is a long-standing conflict between the Ngäbe people and the government concerning mining and the construction of dams for hydropower generation in or near their territory. The Ngäbe-Buglé Comarca was created in 1997 for the use of the Ngäbe and Buglé indigenous communities . Despite this during the 1970s the Panamanian government proposed to dam the Tabasara with a 220MW hydroelectric project designed to supply energy to the proposed Colorado Copper Mines. This project was cancelled following widespread protests . They had won the first battle of the Tabasara dams, however, during the 1990s another project was proposed - this time a 48MW dam, they again successfully defeated this proposal. This thereafter resulted in the Panamanian government changing national law in order to repeal requirements related to participation from indigenous communities. \n\nThis long history of conflict regarding their traditional and spiritual lands has resulted in the prolonged social mobilisation of the Ngäbe-Buglé communities.\n\nAccording to GENISA, no indigenous village or houses would be inundated by the reservoir and no one would be resettled. However 5 hectares of stream bed will be flooded during the rainy season. The land to be inundated consists of ravines close to the river that are not suitable for agriculture or livestock grazing. According to another source the area to be flooded is , or 189 hectares. However, critics allege that the livelihoods of some 5000 Ngöbe farmers who rely on the river for potable water, agriculture and fishing will be negatively impacted, and primary forest would be cut down. They also say that the impacted communities have never provided their free, prior and informed consent to the project.\n\nFurthermore, they say that the habitat of the endangered Tabasará rain frog would be destroyed. GENISA says that the Tabasará rain frog lives in several habitats in Panama, including in the Anton Valley hundreds of kilometers away from the site.\n\nIn 2006, under the government of President Martín Torrijos, an international public tender was announced for various hydropower projects in Panama, including Barro Blanco. Among the four participating companies GENISA was selected. The Public Services Authority ASEP authorized GENISA to prepare studies and, subject to obtaining all relevant approvals, to develop the Barro Blanco project.\n\nGENISA commissioned an Environmental Impact Study. As part of this study, in August 2007 in a public forum with the participation of local authorities, villagers close to the project were asked about their view of the dam. At the time, the indigenous M-10 Movement rejected the dam, because it considered indigenous people would lose their land, would not be able to use the river any more and because the environment would be harmed. However, most of the consulted people supported the project, because it would provide jobs, better road access and improved living standards. Some of those who were opposed to the project subsequently sold their land to the project developer. In December 2007, GENISA and representatives of the Ngobe Bugle people signed a cooperation agreement that included safeguards for the fundamental rights of the indigenous people. \n\nIn May 2008 the Panamanian Environmental Authority ANAM approved the project based on the study. In December 2008 GENISA signed a Memorandum of Understanding in which it committed itself to implement a social development plan for the indigenous communities living next to the dam, including \"infrastructure, health and education programs\" during the construction and during the operation of the dam.\n\nIn January 2009 a validation team consisting of the consulting firm AENOR working for the UN CDM Executive Board visited the area and confirmed \"that the most relevant communities involved in the area of the project were consulted, all of them supported the project activity, and project participant (i.e. GENISA) has forecasted several social compensation measurements for the communities involved.\" The International Rivers Network says that the validation report by AENOR was flawed, since allegedly only the non-indigenous population had been consulted and they had failed to take into account all comments received. In 2009 the concession contract between the government and GENISA was signed.\n\nAfter presidential elections in early 2009, in June 2009 Ricardo Martinelli, a businessman who promised to quickly upgrade Panama's infrastructure, took office as President of Panama. In May 2009 GENISA requested a modification of the permit to increase the capacity by 52% to 28.8 Megawatt by moving the turbines to a lower elevation, without increasing the water level in the reservoir. The original environmental study and the permit referred to an installed capacity of only 19 Megawatt. In January 2010 ANAM approved the modification, and in January 2011 the concession contract was modified accordingly. As of 2010, according to the UN CDM project document, 98% of the land to be inundated was owned by GENISA.\n\nAccording to critics, the environmental impact study was flawed, because impacts on biodiversity were assessed superficially and because the capacity of the plant was increased. In late 2010 the European Investment Bank withdrew funding for the dam after an investigation into human rights abuses prompted by NGO protests. In January 2011 DEG and FMO approved their loans for the project.\n\nIn February 2011, the Panamanian government proposed a new mining law that would facilitate the development of mining projects in indigenous areas, while the indigenous people had asked for a law banning mining in their territories. The proposal and the simultaneous launching of bids for a large copper mining project called Cerro Colorado triggered protests of indigenous people, including local communities from the Barro Blanco area, who blocked the Interamerican Highway for four days. The protests were violently suppressed. The conflict was temporarily ended through the San Felix agreement, signed in the village of San Felix. Construction of the dam began in February 2011.\n\nOn June 25, 2011, according to the project developer GENISA, the regional congress of the Kädriri, the local group of Ngäbe people, approved by public majority voting that the construction of the dam should continue, after having been authorized to take a decision by the General Congress of the Ngäbe-Buglé. Also in June 2011 the UN Clean Development Mechanism (CDM) Board approved the carbon finance proposal that was to generate part of the revenue stream for the project. On August 25, 2011, a Compensation and Benefits Agreement was signed between GENISA and the Board of the Regional Congress of the Kädriri.\n\nHowever, the conflict remained unresolved. The government called for a referendum on the dam, but the indigenous leader Silvia Carrera, elected as Cacica General in September 2011, initially rejected it. An international campaign was started to stop construction of the dam and international funding for it. In March 2012 Parliament passed a modified version of the mining law that prohibited mining in the indigenous territory and required the approval of the Ngöbe Buglé General Congress for any future hydroelectric projects. However, the traditional authorities of the Ngöbe criticized Silvia Carrera for having given in. They had asked for a complete ban on hydropower projects to be included in the law.\n\nConstruction had to be halted in May 2012, because the local population occupied the site. The protests were suspended while \"UN inspectors\" were expected to visit the area. In September 2012 an inspection team led by UNDP and consisting of representatives of the Catholic Church, the environmental agency ANAM, the electricity regulatory agency ASEP and the project developer GENISA, visited the area. The purpose of the inspection was to verify on the ground issues that had not been answered satisfactorily in the environmental impact assessment. The inspection team presented its report in December 2012, recommending a water flow simulation to understand the impact of sudden floods as well as a participatory rural appraisal. In March 2013 an indigenous protester against the dam was killed by masked assailants.\n\nIn July 2013, James Anaya, UN special rapporteur on the rights of indigenous people, visited Panama and spoke to the Ngöbe. In his report he concluded that the Ngöbe \"were not properly consulted\". Prior to the visit 12 local and international civil society organizations had asked Anaya to conduct a formal investigation into the human rights impacts of the dam and to call on the government to \"immediately halt the dam’s construction until the threats to the rights of the indigenous Ngӓbe people affected by the project have been fully addressed\".\n\nOn September 6, 2013, the United Nations released three reports on the water flow simulation, a participatory rural appraisal as well as an ecological and economic analysis conducted by two independent international experts, Gonzalo Castro de la Mata and Luis Lopez, showing that the dam had no impact on global biodiversity, but had \"real and important impacts\" on the indigenous populations living in the area. However, the report did not mention that any villages would be inundated. Furthermore, the experts concluded that \"the local population had not been correctly consulted\". They also concluded that the local population had \"a rudimentary and often erroneous knowledge of the project, being a product of rumors, often without foundation\". Together with the insufficient consultation this had created a climate of fear.\n\nIn April 2014 Silvia Carrera, the Cacica General of the Comarca Ngäbe-Buglé, submitted a complaint to the Independent External Panels of DEG and FMO. In May 2015 the joint review of the panels found that the banks had failed to fully comply with the standards to which they had committed themselves. While the banks took significant steps to understand the situation as part of their due diligence process, they accepted an indigenous peoples report that was insufficient for the purpose to approve the credit in January 2011, requiring further investigations as a condition for disbursement, while their own standards would have required a fully satisfactory report at the time of approval. The two banks accepted the conclusions and committed themselves to \"further raise the bar on the required level of information on stakeholder consultation available to (them) at the time of credit approval.\" In the meantime, the government of Panama - since June 2014 led by a new President, Juan Carlos Varela - suspended the construction of the dam in February 2015 on the grounds that the environmental and social impact assessment was faulty pending the outcome of a court case that challenges the assessment. In June 2015 new protests erupted, with protesters shutting down the Interamerican Highway for two days, as a result of which the President sent a riot squad to the area. In August 2015 President Varela and the Cacia General of the indigenous Comarca, Silvia Carrera, signed an agreement to the effect that the civil works of the dam would be completed, but that the electromechanical works would remain suspended and the dam would not be flooded until a final agreement had been reached. GENISA complained that it had been left out of the agreement and that the agreement violated the agreements concluded by the state with the company. In September 2015 the environmental agency ANAM imposed a 775,200 USD fine on GEMISA for having failed to comply with the resettlement and compensation measures under the project. On January 21, 2016 the indigenous movement 10th of April claimed that the agreement had been violated, because tests to fill the reservoir had been made. On January 28, 2016, a technical report was presented to the indigenous communities, showing that the dam was technically safe. Furthermore, a sub-commission consisting of the government and traditional authorities was created to further study the impacts of the project in view of a \"final decision\". Filling of the reservoir began on May 24th, leading to further protests and road blocks. In response, Vice-President and Minister of Foreign Affairs Isabel Saint Malo de Alvarado held talks with indigenous leaders early June 2016. Carrera said that the filling \"was a violation of the rights of indigenous people in the area, and of the contract signed by President Juan Carlos Varela.\"\n\n"}
{"id": "5122105", "url": "https://en.wikipedia.org/wiki?curid=5122105", "title": "Branford Land Trust", "text": "Branford Land Trust\n\nThe Branford Land Trust is a non-profit organization that purchases land to make it non-developable, protected land through land trusts.\n\nFounded in 1967 the Trust aims to protect Branford's natural resources through:\n\nThe Trust owns nearly in over 130 parcels in Branford, Connecticut, U.S., on the shore of Long Island Sound, just east of New Haven. It has worked for the acquisition of several large publicly owned tracts, including the Short Beach Preserve, a parcel of land with rugged rock outcroppings, tall oaks, beeches, groves of mountain laurel, wetlands and a bluff offering views of Talmadge Pond and Long Island Sound.\n\nIn 2002, the Land Trust was awarded the Branford Chamber of Commerce Community Advancement Award which \"recognizes the sustained efforts of an individual organization or company for their efforts towards helping to make Branford a better place to work and live.\"\n\n"}
{"id": "47943132", "url": "https://en.wikipedia.org/wiki?curid=47943132", "title": "Coal Mines Provident Fund Organisation", "text": "Coal Mines Provident Fund Organisation\n\nCoal Mines Provident Fund Organisation is an agency of the Indian government established in 1948 under \"The Coal Mines Provident Fund and Miscellaneous Provisions Act 1948\".\n\nThe authority manages the following schemes:\n"}
{"id": "125293", "url": "https://en.wikipedia.org/wiki?curid=125293", "title": "Copper", "text": "Copper\n\nCopper is a chemical element with symbol Cu (from ) and atomic number 29. It is a soft, malleable, and ductile metal with very high thermal and electrical conductivity. A freshly exposed surface of pure copper has a pinkish-orange color. Copper is used as a conductor of heat and electricity, as a building material, and as a constituent of various metal alloys, such as sterling silver used in jewelry, cupronickel used to make marine hardware and coins, and constantan used in strain gauges and thermocouples for temperature measurement.\n\nCopper is one of the few metals that can occur in nature in a directly usable metallic form (native metals). This led to very early human use in several regions, from c. 8000 BC. Thousands of years later, it was the first metal to be smelted from sulfide ores, c. 5000 BC, the first metal to be cast into a shape in a mold, c. 4000 BC and the first metal to be purposefully alloyed with another metal, tin, to create bronze, c. 3500 BC.\n\nIn the Roman era, copper was principally mined on Cyprus, the origin of the name of the metal, from \"aes сyprium\" (metal of Cyprus), later corrupted to \"сuprum\" (Latin), from which the words derived, \"coper\" (Old English) and \"copper\", first used around 1530.\n\nThe commonly encountered compounds are copper(II) salts, which often impart blue or green colors to such minerals as azurite, malachite, and turquoise, and have been used widely and historically as pigments.\n\nCopper used in buildings, usually for roofing, oxidizes to form a green verdigris (or patina). Copper is sometimes used in decorative art, both in its elemental metal form and in compounds as pigments. Copper compounds are used as bacteriostatic agents, fungicides, and wood preservatives.\n\nCopper is essential to all living organisms as a trace dietary mineral because it is a key constituent of the respiratory enzyme complex cytochrome c oxidase. In molluscs and crustaceans, copper is a constituent of the blood pigment hemocyanin, replaced by the iron-complexed hemoglobin in fish and other vertebrates. In humans, copper is found mainly in the liver, muscle, and bone. The adult body contains between 1.4 and 2.1 mg of copper per kilogram of body weight.\n\nCopper, silver, and gold are in group 11 of the periodic table; these three metals have one s-orbital electron on top of a filled d-electron shell and are characterized by high ductility, and electrical and thermal conductivity. The filled d-shells in these elements contribute little to interatomic interactions, which are dominated by the s-electrons through metallic bonds. Unlike metals with incomplete d-shells, metallic bonds in copper are lacking a covalent character and are relatively weak. This observation explains the low hardness and high ductility of single crystals of copper. At the macroscopic scale, introduction of extended defects to the crystal lattice, such as grain boundaries, hinders flow of the material under applied stress, thereby increasing its hardness. For this reason, copper is usually supplied in a fine-grained polycrystalline form, which has greater strength than monocrystalline forms.\n\nThe softness of copper partly explains its high electrical conductivity (59.6×10 S/m) and high thermal conductivity, second highest (second only to silver) among pure metals at room temperature. This is because the resistivity to electron transport in metals at room temperature originates primarily from scattering of electrons on thermal vibrations of the lattice, which are relatively weak in a soft metal. The maximum permissible current density of copper in open air is approximately 3.1×10 A/m of cross-sectional area, above which it begins to heat excessively.\n\nCopper is one of a few metallic elements with a natural color other than gray or silver. Pure copper is orange-red and acquires a reddish tarnish when exposed to air. The characteristic color of copper results from the electronic transitions between the filled 3d and half-empty 4s atomic shells – the energy difference between these shells corresponds to orange light.\n\nAs with other metals, if copper is put in contact with another metal, galvanic corrosion will occur.\n\nCopper does not react with water, but it does slowly react with atmospheric oxygen to form a layer of brown-black copper oxide which, unlike the rust that forms on iron in moist air, protects the underlying metal from further corrosion (passivation). A green layer of verdigris (copper carbonate) can often be seen on old copper structures, such as the roofing of many older buildings and the Statue of Liberty. Copper tarnishes when exposed to some sulfur compounds, with which it reacts to form various copper sulfides.\n\nThere are 29 isotopes of copper. Cu and Cu are stable, with Cu comprising approximately 69% of naturally occurring copper; both have a spin of . The other isotopes are radioactive, with the most stable being Cu with a half-life of 61.83 hours. Seven metastable isotopes have been characterized; Cu is the longest-lived with a half-life of 3.8 minutes. Isotopes with a mass number above 64 decay by β, whereas those with a mass number below 64 decay by β. Cu, which has a half-life of 12.7 hours, decays both ways.\n\nCu and Cu have significant applications. Cu is used in Cu-PTSM as a radioactive tracer for positron emission tomography.\n\nCopper is produced in massive stars and is present in the Earth's crust in a proportion of about 50 parts per million (ppm). In nature, copper occurs in a variety of minerals, including native copper, copper sulfides such as chalcopyrite, bornite, digenite, covellite, and chalcocite, copper sulfosalts such as tetrahedite-tennantite, and enargite, copper carbonates such as azurite and malachite, and as copper(I) or copper(II) oxides such as cuprite and tenorite, respectively. The largest mass of elemental copper discovered weighed 420 tonnes and was found in 1857 on the Keweenaw Peninsula in Michigan, US. Native copper is a polycrystal, with the largest single crystal ever described measuring 4.4×3.2×3.2 cm.\n\nMost copper is mined or extracted as copper sulfides from large open pit mines in porphyry copper deposits that contain 0.4 to 1.0% copper. Sites include Chuquicamata, in Chile, Bingham Canyon Mine, in Utah, United States, and El Chino Mine, in New Mexico, United States. According to the British Geological Survey, in 2005, Chile was the top producer of copper with at least one-third of the world share followed by the United States, Indonesia and Peru. Copper can also be recovered through the in-situ leach process. Several sites in the state of Arizona are considered prime candidates for this method. The amount of copper in use is increasing and the quantity available is barely sufficient to allow all countries to reach developed world levels of usage.\n\nCopper has been in use at least 10,000 years, but more than 95% of all copper ever mined and smelted has been extracted since 1900, and more than half was extracted the last 24 years. As with many natural resources, the total amount of copper on Earth is vast, with around 10 tons in the top kilometer of Earth's crust, which is about 5 million years' worth at the current rate of extraction. However, only a tiny fraction of these reserves is economically viable with present-day prices and technologies. Estimates of copper reserves available for mining vary from 25 to 60 years, depending on core assumptions such as the growth rate. Recycling is a major source of copper in the modern world. Because of these and other factors, the future of copper production and supply is the subject of much debate, including the concept of peak copper, analogous to peak oil.\n\nThe price of copper has historically been unstable, and its price increased from the 60-year low of US$0.60/lb (US$1.32/kg) in June 1999 to $3.75 per pound ($8.27/kg) in May 2006. It dropped to $2.40/lb ($5.29/kg) in February 2007, then rebounded to $3.50/lb ($7.71/kg) in April 2007. In February 2009, weakening global demand and a steep fall in commodity prices since the previous year's highs left copper prices at $1.51/lb ($3.32/kg).\n\nThe concentration of copper in ores averages only 0.6%, and most commercial ores are sulfides, especially chalcopyrite (CuFeS), bornite (CuFeS) and, to a lesser extent, covellite (CuS) and chalcocite (CuS). These minerals are concentrated from crushed ores to the level of 10–15% copper by froth flotation or bioleaching. Heating this material with silica in flash smelting removes much of the iron as slag. The process exploits the greater ease of converting iron sulfides into oxides, which in turn react with the silica to form the silicate slag that floats on top of the heated mass. The resulting \"copper matte,\" consisting of CuS, is roasted to convert all sulfides into oxides:\nThe cuprous oxide is converted to \"blister\" copper upon heating:\nThe Sudbury matte process converted only half the sulfide to oxide and then used this oxide to remove the rest of the sulfur as oxide. It was then electrolytically refined and the anode mud exploited for the platinum and gold it contained. This step exploits the relatively easy reduction of copper oxides to copper metal. Natural gas is blown across the blister to remove most of the remaining oxygen and electrorefining is performed on the resulting material to produce pure copper:\n\nLike aluminium, copper is recyclable without any loss of quality, both from raw state and from manufactured products. In volume, copper is the third most recycled metal after iron and aluminium. An estimated 80% of all copper ever mined is still in use today. According to the International Resource Panel's Metal Stocks in Society report, the global per capita stock of copper in use in society is 35–55 kg. Much of this is in more-developed countries (140–300 kg per capita) rather than less-developed countries (30–40 kg per capita).\n\nThe process of recycling copper is roughly the same as is used to extract copper but requires fewer steps. High-purity scrap copper is melted in a furnace and then reduced and cast into billets and ingots; lower-purity scrap is refined by electroplating in a bath of sulfuric acid.\n\nNumerous copper alloys have been formulated, many with important uses. Brass is an alloy of copper and zinc. Bronze usually refers to copper-tin alloys, but can refer to any alloy of copper such as aluminium bronze. Copper is one of the most important constituents of silver and carat gold and carat solders used in the jewelry industry, modifying the color, hardness and melting point of the resulting alloys. Some lead-free solders consist of tin alloyed with a small proportion of copper and other metals.\n\nThe alloy of copper and nickel, called cupronickel, is used in low-denomination coins, often for the outer cladding. The US five-cent coin (currently called a \"nickel\") consists of 75% copper and 25% nickel in homogeneous composition. The alloy of 90% copper and 10% nickel, remarkable for its resistance to corrosion, is used for various objects exposed to seawater, though it is vulnerable to the sulfides sometimes found in polluted harbors and estuaries. Alloys of copper with aluminium (about 7%) have a golden color and are used in decorations. \"Shakudō\" is a Japanese decorative alloy of copper containing a low percentage of gold, typically 4–10%, that can be patinated to a dark blue or black color.\n\nCopper forms a rich variety of compounds, usually with oxidation states +1 and +2, which are often called \"cuprous\" and \"cupric\", respectively.\n\nAs with other elements, the simplest compounds of copper are binary compounds, i.e. those containing only two elements, the principal examples being oxides, sulfides, and halides. Both cuprous and cupric oxides are known. Among the numerous copper sulfides, important examples include copper(I) sulfide and copper(II) sulfide.\n\nCuprous halides (with chlorine, bromine, and iodine) are known, as are cupric halides with fluorine, chlorine, and bromine. Attempts to prepare copper(II) iodide yield only cuprous iodide and iodine.\n\nCopper forms coordination complexes with ligands. In aqueous solution, copper(II) exists as [Cu(HO)]. This complex exhibits the fastest water exchange rate (speed of water ligands attaching and detaching) for any transition metal aquo complex. Adding aqueous sodium hydroxide causes the precipitation of light blue solid copper(II) hydroxide. A simplified equation is: \nAqueous ammonia results in the same precipitate. Upon adding excess ammonia, the precipitate dissolves, forming tetraamminecopper(II):\nMany other oxyanions form complexes; these include copper(II) acetate, copper(II) nitrate, and copper(II) carbonate. Copper(II) sulfate forms a blue crystalline pentahydrate, the most familiar copper compound in the laboratory. It is used in a fungicide called the Bordeaux mixture.\nPolyols, compounds containing more than one alcohol functional group, generally interact with cupric salts. For example, copper salts are used to test for reducing sugars. Specifically, using Benedict's reagent and Fehling's solution the presence of the sugar is signaled by a color change from blue Cu(II) to reddish copper(I) oxide. Schweizer's reagent and related complexes with ethylenediamine and other amines dissolve cellulose. Amino acids form very stable chelate complexes with copper(II). Many wet-chemical tests for copper ions exist, one involving potassium ferrocyanide, which gives a brown precipitate with copper(II) salts.\n\nCompounds that contain a carbon-copper bond are known as organocopper compounds. They are very reactive towards oxygen to form copper(I) oxide and have many uses in chemistry. They are synthesized by treating copper(I) compounds with Grignard reagents, terminal alkynes or organolithium reagents; in particular, the last reaction described produces a Gilman reagent. These can undergo substitution with alkyl halides to form coupling products; as such, they are important in the field of organic synthesis. Copper(I) acetylide is highly shock-sensitive but is an intermediate in reactions such as the Cadiot-Chodkiewicz coupling and the Sonogashira coupling. Conjugate addition to enones and carbocupration of alkynes can also be achieved with organocopper compounds. Copper(I) forms a variety of weak complexes with alkenes and carbon monoxide, especially in the presence of amine ligands.\n\nCopper(III) is most often found in oxides. A simple example is potassium cuprate, KCuO, a blue-black solid. The most extensively studied copper(III) compounds are the cuprate superconductors. Yttrium barium copper oxide (YBaCuO) consists of both Cu(II) and Cu(III) centres. Like oxide, fluoride is a highly basic anion and is known to stabilize metal ions in high oxidation states. Both copper(III) and even copper(IV) fluorides are known, KCuF and CsCuF, respectively.\n\nSome copper proteins form oxo complexes, which also feature copper(III). With tetrapeptides, purple-colored copper(III) complexes are stabilized by the deprotonated amide ligands.\n\nComplexes of copper(III) are also found as intermediates in reactions of organocopper compounds. For example, in the Kharasch–Sosnovsky reaction.\n\nA timeline of copper illustrates how the metal has advanced human civilization for the past 11,000 years.\n\nCopper occurs naturally as native metallic copper and was known to some of the oldest civilizations on record. The history of copper use dates to 9000 BC in the Middle East; a copper pendant was found in northern Iraq that dates to 8700 BC. Evidence suggests that gold and meteoric iron (but not smelted iron) were the only metals used by humans before copper. The history of copper metallurgy is thought to follow this sequence: First, cold working of native copper, then annealing, smelting, and, finally, lost-wax casting. In southeastern Anatolia, all four of these techniques appear more or less simultaneously at the beginning of the Neolithic c. 7500 BC.\n\nCopper smelting was independently invented in different places. It was probably discovered in China before 2800 BC, in Central America around 600 AD, and in West Africa about the 9th or 10th century AD. Investment casting was invented in 4500–4000 BC in Southeast Asia and carbon dating has established mining at Alderley Edge in Cheshire, UK, at 2280 to 1890 BC. Ötzi the Iceman, a male dated from 3300–3200 BC, was found with an axe with a copper head 99.7% pure; high levels of arsenic in his hair suggest an involvement in copper smelting. Experience with copper has assisted the development of other metals; in particular, copper smelting led to the discovery of iron smelting. Production in the Old Copper Complex in Michigan and Wisconsin is dated between 6000 and 3000 BC. Natural bronze, a type of copper made from ores rich in silicon, arsenic, and (rarely) tin, came into general use in the Balkans around 5500 BC.\n\nAlloying copper with tin to make bronze was first practiced about 4000 years after the discovery of copper smelting, and about 2000 years after \"natural bronze\" had come into general use. Bronze artifacts from the Vinča culture date to 4500 BC. Sumerian and Egyptian artifacts of copper and bronze alloys date to 3000 BC. The Bronze Age began in Southeastern Europe around 3700–3300 BC, in Northwestern Europe about 2500 BC. It ended with the beginning of the Iron Age, 2000–1000 BC in the Near East, and 600 BC in Northern Europe. The transition between the Neolithic period and the Bronze Age was formerly termed the Chalcolithic period (copper-stone), when copper tools were used with stone tools. The term has gradually fallen out of favor because in some parts of the world, the Chalcolithic and Neolithic are coterminous at both ends. Brass, an alloy of copper and zinc, is of much more recent origin. It was known to the Greeks, but became a significant supplement to bronze during the Roman Empire.\n\nIn Greece, copper was known by the name \"chalkos\" (χαλκός). It was an important resource for the Romans, Greeks and other ancient peoples. In Roman times, it was known as \"aes Cyprium\", \"aes\" being the generic Latin term for copper alloys and \"Cyprium\" from Cyprus, where much copper was mined. The phrase was simplified to \"cuprum\", hence the English \"copper\". Aphrodite (Venus in Rome) represented copper in mythology and alchemy because of its lustrous beauty and its ancient use in producing mirrors; Cyprus was sacred to the goddess. The seven heavenly bodies known to the ancients were associated with the seven metals known in antiquity, and Venus was assigned to copper.\n\nCopper was first used in ancient Britain in about the 3rd or 2nd Century BC. In North America, copper mining began with marginal workings by Native Americans. Native copper is known to have been extracted from sites on Isle Royale with primitive stone tools between 800 and 1600. Copper metallurgy was flourishing in South America, particularly in Peru around 1000 AD. Copper burial ornamentals from the 15th century have been uncovered, but the metal's commercial production did not start until the early 20th century.\n\nThe cultural role of copper has been important, particularly in currency. Romans in the 6th through 3rd centuries BC used copper lumps as money. At first, the copper itself was valued, but gradually the shape and look of the copper became more important. Julius Caesar had his own coins made from brass, while Octavianus Augustus Caesar's coins were made from Cu-Pb-Sn alloys. With an estimated annual output of around 15,000 t, Roman copper mining and smelting activities reached a scale unsurpassed until the time of the Industrial Revolution; the provinces most intensely mined were those of Hispania, Cyprus and in Central Europe.\n\nThe gates of the Temple of Jerusalem used Corinthian bronze treated with depletion gilding. The process was most prevalent in Alexandria, where alchemy is thought to have begun. In ancient India, copper was used in the holistic medical science Ayurveda for surgical instruments and other medical equipment. Ancient Egyptians (~2400 BC) used copper for sterilizing wounds and drinking water, and later to treat headaches, burns, and itching.\n\nThe Great Copper Mountain was a mine in Falun, Sweden, that operated from the 10th century to 1992. It satisfied two thirds of Europe's copper consumption in the 17th century and helped fund many of Sweden's wars during that time. It was referred to as the nation's treasury; Sweden had a copper backed currency.\n\nCopper is used in roofing, currency, and for photographic technology known as the daguerreotype. Copper was used in Renaissance sculpture, and was used to construct the Statue of Liberty; copper continues to be used in construction of various types. Copper plating and copper sheathing were widely used to protect the under-water hulls of ships, a technique pioneered by the British Admiralty in the 18th century. The Norddeutsche Affinerie in Hamburg was the first modern electroplating plant, starting its production in 1876. The German scientist Gottfried Osann invented powder metallurgy in 1830 while determining the metal's atomic mass; around then it was discovered that the amount and type of alloying element (e.g., tin) to copper would affect bell tones. Flash smelting was developed by Outokumpu in Finland and first applied at Harjavalta in 1949; the energy-efficient process accounts for 50% of the world's primary copper production.\n\nThe Intergovernmental Council of Copper Exporting Countries, formed in 1967 by Chile, Peru, Zaire and Zambia, operated in the copper market as OPEC does in oil, though it never achieved the same influence, particularly because the second-largest producer, the United States, was never a member; it was dissolved in 1988.\n\nThe major applications of copper are electrical wire (60%), roofing and plumbing (20%), and industrial machinery (15%). Copper is used mostly as a pure metal, but when greater hardness is required, it is put into such alloys as brass and bronze (5% of total use). For more than two centuries, copper paint has been used on boat hulls to control the growth of plants and shellfish. A small part of the copper supply is used for nutritional supplements and fungicides in agriculture. Machining of copper is possible, although alloys are preferred for good machinability in creating intricate parts.\n\nDespite competition from other materials, copper remains the preferred electrical conductor in nearly all categories of electrical wiring except overhead electric power transmission where aluminium is often preferred. Copper wire is used in power generation, power transmission, power distribution, telecommunications, electronics circuitry, and countless types of electrical equipment. Electrical wiring is the most important market for the copper industry. This includes structural power wiring, power distribution cable, appliance wire, communications cable, automotive wire and cable, and magnet wire. Roughly half of all copper mined is used for electrical wire and cable conductors. Many electrical devices rely on copper wiring because of its multitude of inherent beneficial properties, such as its high electrical conductivity, tensile strength, ductility, creep (deformation) resistance, corrosion resistance, low thermal expansion, high thermal conductivity, ease of soldering, malleability, and ease of installation.\n\nFor a short period from the late 1960s to the late 1970s, copper wiring was replaced by aluminium wiring in many housing construction projects in America. The new wiring was implicated in a number of house fires and the industry returned to copper.\n\nIntegrated circuits and printed circuit boards increasingly feature copper in place of aluminium because of its superior electrical conductivity; heat sinks and heat exchangers use copper because of its superior heat dissipation properties. Electromagnets, vacuum tubes, cathode ray tubes, and magnetrons in microwave ovens use copper, as do waveguides for microwave radiation.\n\nCopper's superior conductivity enhances the efficiency of electrical motors. This is important because motors and motor-driven systems account for 43%–46% of all global electricity consumption and 69% of all electricity used by industry. Increasing the mass and cross section of copper in a coil increases the efficiency of the motor. Copper motor rotors, a new technology designed for motor applications where energy savings are prime design objectives, are enabling general-purpose induction motors to meet and exceed National Electrical Manufacturers Association (NEMA) premium efficiency standards.\n\nCopper has been used since ancient times as a durable, corrosion resistant, and weatherproof architectural material. Roofs, flashings, rain gutters, downspouts, domes, spires, vaults, and doors have been made from copper for hundreds or thousands of years. Copper's architectural use has been expanded in modern times to include interior and exterior wall cladding, building expansion joints, radio frequency shielding, and antimicrobial and decorative indoor products such as attractive handrails, bathroom fixtures, and counter tops. Some of copper's other important benefits as an architectural material include low thermal movement, light weight, lightning protection, and recyclability.\n\nThe metal's distinctive natural green patina has long been coveted by architects and designers. The final patina is a particularly durable layer that is highly resistant to atmospheric corrosion, thereby protecting the underlying metal against further weathering. It can be a mixture of carbonate and sulfate compounds in various amounts, depending upon environmental conditions such as sulfur-containing acid rain. Architectural copper and its alloys can also be 'finished' to take on a particular look, feel, or color. Finishes include mechanical surface treatments, chemical coloring, and coatings.\n\nCopper has excellent brazing and soldering properties and can be welded; the best results are obtained with gas metal arc welding.\n\nCopper is biostatic, meaning bacteria and many other forms of life will not grow on it. For this reason it has long been used to line parts of ships to protect against barnacles and mussels. It was originally used pure, but has since been superseded by Muntz metal and copper-based paint. Similarly, as discussed in copper alloys in aquaculture, copper alloys have become important netting materials in the aquaculture industry because they are antimicrobial and prevent biofouling, even in extreme conditions and have strong structural and corrosion-resistant properties in marine environments.\n\nCopper-alloy touch surfaces have natural properties that destroy a wide range of microorganisms (e.g., \"E. coli\" O157:H7, methicillin-resistant \"Staphylococcus aureus\" (MRSA), \"Staphylococcus\", \"Clostridium difficile\", influenza A virus, adenovirus, and fungi). Some 355 copper alloys were proven to kill more than 99.9% of disease-causing bacteria within just two hours when cleaned regularly. The United States Environmental Protection Agency (EPA) has approved the registrations of these copper alloys as \"antimicrobial materials with public health benefits\"; that approval allows manufacturers to make legal claims to the public health benefits of products made of registered alloys. In addition, the EPA has approved a long list of antimicrobial copper products made from these alloys, such as bedrails, handrails, over-bed tables, sinks, faucets, door knobs, toilet hardware, computer keyboards, health club equipment, and shopping cart handles (for a comprehensive list, see: Antimicrobial copper-alloy touch surfaces#Approved products). Copper doorknobs are used by hospitals to reduce the transfer of disease, and Legionnaires' disease is suppressed by copper tubing in plumbing systems. Antimicrobial copper alloy products are now being installed in healthcare facilities in the U.K., Ireland, Japan, Korea, France, Denmark, and Brazil and in the subway transit system in Santiago, Chile, where copper-zinc alloy handrails will be installed in some 30 stations between 2011 and 2014.\n\nCopper is commonly used in jewelry, and according to some folklore, copper bracelets relieve arthritis symptoms. In one trial for osteoarthritis and one trial for rheumatoid arthritis no differences is found between copper bracelet and control (non-copper) bracelet. No evidence shows that copper can be absorbed through the skin. If it were, it might lead to copper poisoning.\n\nRecently, some compression clothing with inter-woven copper has been marketed with health claims similar to the folk medicine claims. Because compression clothing is a valid treatment for some ailments, the clothing may have that benefit, but the added copper may have no benefit beyond a placebo effect.\n\nTextile fibers are blended with copper to create antimicrobial protective fabrics.\n\n\"Chromobacterium violaceum\" and \"Pseudomonas fluorescens\" can both mobilize solid copper as a cyanide compound. The ericoid mycorrhizal fungi associated with \"Calluna\", \"Erica\" and \"Vaccinium\" can grow in metalliferous soils containing copper. The ectomycorrhizal fungus \"Suillus luteus\" protects young pine trees from copper toxicity. A sample of the fungus \"Aspergillus niger\" was found growing from gold mining solution and was found to contain cyano complexes of such metals as gold, silver, copper, iron, and zinc. The fungus also plays a role in the solubilization of heavy metal sulfides.\n\nCopper proteins have diverse roles in biological electron transport and oxygen transportation, processes that exploit the easy interconversion of Cu(I) and Cu(II). Copper is essential in the aerobic respiration of all eukaryotes. In mitochondria, it is found in cytochrome c oxidase, which is the last protein in oxidative phosphorylation. Cytochrome c oxidase is the protein that binds the O between a copper and an iron; the protein transfers 8 electrons to the O molecule to reduce it to two molecules of water. Copper is also found in many superoxide dismutases, proteins that catalyze the decomposition of superoxides by converting it (by disproportionation) to oxygen and hydrogen peroxide:\n\nThe protein hemocyanin is the oxygen carrier in most mollusks and some arthropods such as the horseshoe crab (\"Limulus polyphemus\"). Because hemocyanin is blue, these organisms have blue blood rather than the red blood of iron-based hemoglobin. Structurally related to hemocyanin are the laccases and tyrosinases. Instead of reversibly binding oxygen, these proteins hydroxylate substrates, illustrated by their role in the formation of lacquers. The biological role for copper commenced with the appearance of oxygen in earth's atmosphere. Several copper proteins, such as the \"blue copper proteins\", do not interact directly with substrates; hence they are not enzymes. These proteins relay electrons by the process called electron transfer.\nA unique tetranuclear copper center has been found in nitrous-oxide reductase.\n\nChemical compounds which were developed for treatment of Wilson's disease have been investigated for use in cancer therapy.\n\nCopper is an essential trace element in plants and animals, but not all microorganisms. The human body contains copper at a level of about 1.4 to 2.1 mg per kg of body mass. Copper is absorbed in the gut, then transported to the liver bound to albumin. After processing in the liver, copper is distributed to other tissues in a second phase, which involves the protein ceruloplasmin, carrying the majority of copper in blood. Ceruloplasmin also carries the copper that is excreted in milk, and is particularly well-absorbed as a copper source. Copper in the body normally undergoes enterohepatic circulation (about 5 mg a day, vs. about 1 mg per day absorbed in the diet and excreted from the body), and the body is able to excrete some excess copper, if needed, via bile, which carries some copper out of the liver that is not then reabsorbed by the intestine.\n\nThe U.S. Institute of Medicine (IOM) updated the estimated average requirements (EARs) and recommended dietary allowances (RDAs) for copper in 2001. If there is not sufficient information to establish EARs and RDAs, an estimate designated Adequate Intake (AI) is used instead. The AIs for copper are: 200 μg of copper for 0–6-month-old males and females, and 220 μg of copper for 7–12-month-old males and females. The RDAs for copper are: 340 μg of copper for 1–3-year-old males, 440 μg of copper for 4–8-year-old males, 700 μg of copper for 9–13-year-old males, 890 μg of copper for 14–18-year-old males, and 900 μg of copper for males that are 19 years old and older. The RDAs for copper are: 340 μg of copper for 1–3-year-old females, 440 μg of copper for 4–8-year-old females, 700 μg of copper for 9–13-year-old females, 890 μg of copper for 14–18-year-old females, and 900 μg of copper for females that are 19 years old and older. The RDAs for copper are: 1,000 μg of copper for 14–50-year-old pregnant females; furthermore, 1,300 μg of copper for 14–50-year-old lactating females. As for safety, the IOM also sets Tolerable upper intake levels (ULs) for vitamins and minerals when evidence is sufficient. In the case of copper the UL is set at 10 mg/day. Collectively the EARs, RDAs, AIs and ULs are referred to as Dietary Reference Intakes.\n\nThe European Food Safety Authority (EFSA) refers to the collective set of information as Dietary Reference Values, with Population Reference Intake (PRI) instead of RDA, and Average Requirement instead of EAR. AI and UL defined the same as in United States. For women and men ages 18 and older the AIs are set at 1.3 and 1.6 mg/day, respectively. AIs for pregnancy and lactation is 1.5 mg/day. For children ages 1–17 years the AIs increase with age from 0.7 to 1.3 mg/day. These AIs are higher than the U.S. RDAs. The European Food Safety Authority reviewed the same safety question and set its UL at 5 mg/day, which is half the U.S. value.\n\nFor U.S. food and dietary supplement labeling purposes the amount in a serving is expressed as a percent of Daily Value (%DV). For copper labeling purposes 100% of the Daily Value was 2.0 mg, but it was revised to 0.9 mg to bring it into agreement with the RDA. A table of the old and new adult Daily Values is provided at Reference Daily Intake. The original deadline to be in compliance was July 28, 2018, but on September 29, 2017 the FDA released a proposed rule that extended the deadline to January 1, 2020 for large companies and January 1, 2021 for small companies.\n\nBecause of its role in facilitating iron uptake, copper deficiency can produce anemia-like symptoms, neutropenia, bone abnormalities, hypopigmentation, impaired growth, increased incidence of infections, osteoporosis, hyperthyroidism, and abnormalities in glucose and cholesterol metabolism. Conversely, Wilson's disease causes an accumulation of copper in body tissues.\n\nSevere deficiency can be found by testing for low plasma or serum copper levels, low ceruloplasmin, and low red blood cell superoxide dismutase levels; these are not sensitive to marginal copper status. The \"cytochrome c oxidase activity of leucocytes and platelets\" has been stated as another factor in deficiency, but the results have not been confirmed by replication.\n\nGram quantities of various copper salts have been taken in suicide attempts and produced acute copper toxicity in humans, possibly due to redox cycling and the generation of reactive oxygen species that damage DNA. Corresponding amounts of copper salts (30 mg/kg) are toxic in animals. A minimum dietary value for healthy growth in rabbits has been reported to be at least 3 ppm in the diet. However, higher concentrations of copper (100 ppm, 200 ppm, or 500 ppm) in the diet of rabbits may favorably influence feed conversion efficiency, growth rates, and carcass dressing percentages.\n\nChronic copper toxicity does not normally occur in humans because of transport systems that regulate absorption and excretion. Autosomal recessive mutations in copper transport proteins can disable these systems, leading to Wilson's disease with copper accumulation and cirrhosis of the liver in persons who have inherited two defective genes.\n\nElevated copper levels have also been linked to worsening symptoms of Alzheimer's disease.\n\nIn the US, the Occupational Safety and Health Administration (OSHA) has designated a permissible exposure limit (PEL) for copper dust and fumes in the workplace as a time-weighted average (TWA) of 1 mg/m. The National Institute for Occupational Safety and Health (NIOSH) has set a Recommended exposure limit (REL) of 1 mg/m, time-weighted average. The IDLH (immediately dangerous to life and health) value is 100 mg/m.\n\nCopper is a constituent of tobacco smoke. The tobacco plant readily absorbs and accumulates heavy metals, such as copper from the surrounding soil into its leaves. These are readily absorbed into the user's body following smoke inhalation. The health implications are not clear.\n\n\n\n"}
{"id": "35836393", "url": "https://en.wikipedia.org/wiki?curid=35836393", "title": "Decline curve analysis", "text": "Decline curve analysis\n\nDecline curve analysis is a means of predicting future oil well or gas well production based on past production history. Production decline curve analysis is a traditional means of identifying well production problems and predicting well performance and life based on measured oil well production. \nBefore the availability of computers, decline curve analysis was performed by hand on semi-log plot paper. Currently, decline curve analysis software on PC computers is used to plot production decline curves for petroleum economics analysis.\n\nOil and gas wells usually reach their maximum output shortly after completion. From that time, other than wells completed in water-drive reservoirs, they decline in production, the rapidity of decline depending on the output of the wells and on other factors governing their productivity. The production decline curve shows the amount of oil and gas produced per unit of time for several consecutive periods; if the conditions affecting the rate of production are not changed, the curve may be fairly regular, and, if projected, will furnish useful knowledge as to the future production of the well. By the aid of this knowledge the value of a property may be judged, and proper depletion and depreciation charges may be made on the books of the operating company.()\n\nProduction decline curve analysis is important in determining the value in oil and gas wells in oil and gas economics. Decline curves are the most common means of forecasting oil and gas production. Decline curves have many advantages: they use data which is easy to obtain, they are easy to plot, they yield results on a time basis, and they are easy to analyze. Decline curves are also one of the oldest methods of predicting oil reserves.()\n\n\n"}
{"id": "17428178", "url": "https://en.wikipedia.org/wiki?curid=17428178", "title": "Deepwater drilling", "text": "Deepwater drilling\n\nDeepwater drilling, or Deep well drilling, is the process of creating holes by drilling rig for oil mining in deep sea. There are approximately 3400 deepwater wells in the Gulf of Mexico with depths greater than 150 meters. \n\nIt has not been technologically and economically feasible for many years, but with rising oil prices, more companies are investing in this area. Major companies working in this sector include Halliburton, Diamond Offshore, TransOcean, Geoservices, and Schlumberger. The deepwater gas and oil market is back on the rise after the 2010 Deepwater Horizon disaster, and total expenditure of around $35 billion per year and a total global CAPEX of $167 billion in the past four years.\n\nRecent industry analysis by Visiongain has estimated that the total expenditure in the global deepwater infrastructure market would reach $145bn in 2011.\n\nIn the Deepwater Horizon oil spill of 2010, a large explosion occurred killing workers and spilling oil into the Gulf of Mexico while a BP oil rig was drilling in deep waters. \n\nSome of the earliest evidence of water wells are located in China. The Chinese discovered and made extensive use of deep drilled groundwater for drinking. The Chinese text \"The Book of Changes\", originally a divination text of the Western Zhou dynasty (1046 -771 BC), contains an entry describing how the ancient Chinese maintained their wells and protected their sources of water. Archaeological evidence and old Chinese documents reveal that the prehistoric and ancient Chinese had the aptitude and skills for digging deep water wells for drinking water as early as 6000 to 7000 years ago. A well excavated at the Hemedu excavation site was believed to have been built during the Neolithic era. The well was cased by four rows of logs with a square frame attached to them at the top of the well. 60 additional tile wells southwest of Beijing are also believed to have been built around 600 BC for drinking and irrigation.\n\nDrilling in deep waters can be performed by two main types of mobile deepwater drilling rigs: semi-submersible drilling rigs and drillships. Drilling can also be performed from a fixed-position installation such as a fixed platform, or a floating platform, such as a spar platform, a tension-leg platform, or a semi-submersible production platform..\n\nOn 20 April 2010, a BP deepwater oil rig (Deepwater Horizon) exploded, killing 11 and releasing 750 000 cubic meters (200 million gallons) of oil into the Gulf of Mexico. With those numbers, many scientists consider this disaster to be one of the worst environmental disasters in the history of the US.\n\nA large number of animal deaths have resulted from the release of the oil. A Center study estimates that over 82,000 birds; about 6,000 sea turtles; and nearly 26,000 marine mammals were killed from either the initial explosion or the oil spill.\n\n\n"}
{"id": "11463156", "url": "https://en.wikipedia.org/wiki?curid=11463156", "title": "Dekatron", "text": "Dekatron\n\nIn electronics, a Dekatron (or Decatron, or generically three-phase gas counting tube or glow-transfer counting tube or cold cathode tube) is a gas-filled decade counting tube. Dekatrons were used in computers, calculators and other counting-related products during the 1950s and 1960s. \"Dekatron,\" now a generic trademark, was the brand name used by Ericsson Telephones Limited (ETL), of Beeston, Nottingham (not to be confused with the Swedish TelefonAB Ericsson of Stockholm).\nThe dekatron was useful for computing, calculating and frequency-dividing purposes because one complete revolution of the neon dot in a dekatron means 10 pulses on the guide electrode(s), and a signal can be derived from one of the ten cathodes in a dekatron to send a pulse, possibly for another counting stage. Dekatrons usually have a maximum input frequency in the high kilohertz (kHz) range – 100 kHz is fast, 1 MHz is around the maximum possible. These frequencies are obtained in hydrogen-filled fast dekatrons. Dekatrons filled with inert gas are inherently more stable and have a longer life, but their counting frequency is limited to 10 kHz (1–2 kHz is more common).\n\nInternal designs vary by the model and manufacturer, but generally a dekatron has ten cathodes and one or two guide electrodes plus a common anode. The cathodes are arranged in a circle with a guide electrode (or two) between each cathode. When the guide electrode(s) is pulsed properly, the neon gas will activate near the guide pins then \"jump\" to the next cathode. Pulsing the guide electrodes (negative going pulses) repeatedly will cause the neon dot to move from cathode to cathode. \n\nHydrogen dekatrons require high voltages ranging from 400 to 600 volts on the anode for proper operation; dekatrons with inert gas usually require ~350 volts. When a dekatron is first powered up, a glowing dot appears at a random cathode; the tube must then be reset to zero state, by driving a negative pulse into the designated starting cathode. The color of the dot depends on the type of gas that is in the tube. Neon-filled tubes display a red-orange dot; argon-filled tubes display a purple dot (and are much dimmer than neon).\n\n\"Counter\" (common-cathode) dekatrons have only one carry/borrow cathode wired to its own socket pin for multistage cascading and the remaining nine cathodes tied together to another pin; therefore they don't need bases with more than 9 pins.\n\n\"Counter/Selector\" (separate-cathode) dekatrons have each cathode wired to its own pin; therefore their bases have at least 13 pins. \"Selectors\" allow for monitoring the status of each cathode or to divide-by-n with the proper reset circuitry. This kind of versatility made such dekatrons useful for numerical division in early calculators.\n\nDekatrons come in various physical sizes, ranging from smaller than a 7-pin miniature vacuum tube to as large as an octal base tube. While most dekatrons are decimal counters, models were also made to count in base-5 and base-12 for specific applications.\n\nThe dekatron fell out of practical use when transistor-based counters became reliable and affordable. Today, dekatrons are used by electronic hobbyists in simple \"spinners\" that run off the mains frequency (50 Hz or 60 Hz) or as a numeric indicator for homemade clocks.\n\n\n"}
{"id": "15230193", "url": "https://en.wikipedia.org/wiki?curid=15230193", "title": "Electric transportation technology", "text": "Electric transportation technology\n\nElectric transportation technology is: \n\n\n\n"}
{"id": "3427155", "url": "https://en.wikipedia.org/wiki?curid=3427155", "title": "Engineered cementitious composite", "text": "Engineered cementitious composite\n\nEngineered Cementitious Composite (ECC), also called Strain Hardening Cement-based Composites (SHCC) or more popularly as bendable concrete, is an easily molded mortar-based composite reinforced with specially selected short random fibers, usually polymer fibers. Unlike regular concrete, ECC has a strain capacity in the range of 3–7%, compared to 0.01% for ordinary portland cement (OPC) paste, mortar or concrete. ECC therefore acts more like a ductile metal like material rather than a brittle glass like material (as does OPC concrete), leading to a wide variety of applications.\n\nECC, unlike common fiber reinforced concrete, is a family of micromechanically designed materials. As long as a cementitious material is designed/developed based on micromechanics and fracture mechanics theory to feature large tensile ductility, it can be called an ECC. Therefore, ECC is not a fixed material design, but a broad range of topics under different stages of research, development, and implementations. The ECC material family is expanding. The development of an individual mix design of ECC requires special efforts by systematically engineering of the material at nano-, micro-, macro- and composite scales.\n\nECC looks similar to ordinary portland cement-based concrete, except that it can deform (or bend) under strain. A number of research groups are developing ECC science, including those at the University of Michigan, University of California, Irvine, Delft University of Technology, the University of Tokyo, the Czech Technical University, University of British Columbia, and Stanford University. Traditional concrete’s lack of durability and failure under strain, both stemming from brittle behavior, have been a pushing factor in the development of ECC.\n\nECC has a variety of unique properties, including tensile properties superior to other fiber-reinforced composites, ease of processing on par with conventional cement, the use of only a small volume fraction of fibers (~ 2%), tight crack width, and a lack of anisotropically weak planes. These properties are due largely to the interaction between the fibers and cementing matrix, which can be custom-tailored through micromechanics design. Essentially, the fibers create many microcracks with a very specific width, rather than a few very large cracks (as in conventional concrete.) This allows ECC to deform without catastrophic failure.\n\nThis microcracking behavior leads to superior corrosion resistance (the cracks are so small and numerous that it is difficult for aggressive media to penetrate and attack the reinforcing steel) as well as to self-healing. In the presence of water (during a rainstorm, for instance) unreacted cement particles recently exposed due to cracking hydrate and form a number of products (Calcium Silicate Hydrate, calcite, etc.) that expand and fill in the crack. These products appear as a white ‘scar’ material filling in the crack. This self-healing behavior not only seals the crack to prevent transport of fluids, but mechanical properties are regained. This self-healing has been observed in a variety of conventional cement and concretes; however, above a certain crack width self healing becomes less effective. It is the tightly controlled crack widths seen in ECC that ensure all cracks thoroughly heal when exposed to the natural environment.\n\nWhen combined with a more conductive material, all cement materials can increase and be used for damage-sensing. This is essentially based on the fact that conductivity will change as damage occurs; the addition of conductive material is meant to raise the conductivity to a level where such changes will be easily identified. Though not a material property of ECC itself, semi-conductive ECC for damage-sensing are being developed.\n\nThere are a number of different varieties of ECC, including:\n\nECC have found use in a number of large-scale applications in Japan, Korea, Switzerland, Australia and the U.S.[3]. These include:\n\n\nNote: FRC=Fiber-Reinforced Cement. HPFRCC=High-Performance Fiber Reinforced Cementitious Composites\n\n\n"}
{"id": "52341031", "url": "https://en.wikipedia.org/wiki?curid=52341031", "title": "Eress", "text": "Eress\n\nThe idea of Eress (European Partnership for Railway Energy Settlement System) was presented for the first time in 1999 by Director of Jernbaneverket Bane Energi Johnny Brevik in a Nordic Railway Conferance in Tampere Finland. It was a new idea therefore it took 4 years before the first agreement between Jernbaneverket, Banverket and Banedanmark was signed in 2004. Johnny Brevik was the first leader of the Board of Directors (2004-2009). \n\nThe European Partnership for Railway Energy Settlement Systems (Eress) is a European Partnership for Railway Energy Settlement Systems. The Eress railway energy settlement system, Erex, provides energy metering services and enables the industry to measure, control and bill for the actual energy consumed by trains. At any given time there will be numerous trains active on the railway network. Trains consume electricity from the grid, for which they need to pay according to their use. The national authority, represented by an infrastructure manager, is responsible for billing this use. The infrastructure manager sends an energy bill to the train companies. In Europe before Eress, most infrastructure managers used estimates to bill train energy consumption.\n\nThe program is based on an earlier idea from the Norwegian National Rail Administration. It was founded in 2004 under the name Nress (Nordic Railway Energy Settlement System), and was a collaboration between Trafikkverket (former Banverket), Banedanmark and Bane NOR (Former Jernbaneverket). In 2008 the name was changed to Eress (European Railway Energy Settlement System). Although the administrative headquarter is in Oslo, Norway, the Eress Board of Directors consists of representatives from each of the 7 existing partners. The program has been shown to result in energy savings and was awarded an award for Energy and CO2 at the 2012 International Union of Railways Sustainability Conference.\n\nThe energy settlement system provided by Eress is called Erex. This, according to Railway Pro (2011), enables infrastructure managers to fulfill requirements for a neutral and non-discriminatory operation, and railway undertakings to understand their use of energy and thereby save energy and costs. The system is composed of the following elements: Advanced energy meters mounted on board trains, an energy measurement system that collects and validates the metered data and a settlement system that performs the settlement, cost distribution, data exchange and billing. According to that article, Erex is unique on the European market in the way that it's the only system that can provide such advanced services to railways.\n\nCurrently (2017), the system is being tested by the French National Railways (SNCF): \"French National Railways (SNCF) is to test the Erex IT energy management and billing system on 50 of its freight locomotives developed by the Eress consortium of five European infrastructure managers and one railway.\"\n\nIn the \"Railway Journal\" on June 16, 2016, Mr. Dyre Martin Gulbrandsen reports that the added accuracy in invoicing by the Eress system creates an incentive to save energy. By billing the actual energy used the train companies start adhere to a more economic driving pattern.\n\nAt least two studies of the Norwegian National Railways (NSB) have documented energy savings resulting from the adoption of Eress. One study found an 18% energy efficiency improvement for NSB between 2004 and 2009. Another from 2016 noted that NSB saved €37m in energy consumption costs since installing Erex in 2004, and that the rail system is using approximately 75% of the energy it was consuming 12 years ago.\n\nDuring the 10th International Union of Railways Sustainability Conference in 2012 in Venice, Eress was awarded an Energy and CO2 award.\nAs of December 2016, Eress partners are: (Initiated by Bane NOR (former Jernbaneverket), Banedanmark and Trafikverket. Others in joining order)\n\nThe Eress Board of Directors consists of representatives from national railway and infrastructure bodies of each of these seven partner countries.\n\n0. https://www.globalrailwayreview.com/article/224/eress-the-new-common-european-standard/\n"}
{"id": "30414731", "url": "https://en.wikipedia.org/wiki?curid=30414731", "title": "Fe'i banana", "text": "Fe'i banana\n\nFe'i bananas (also spelt Fehi or Fei) are cultivated plants in the genus \"Musa\", used mainly for their fruit. They are distinct in appearance and origin from the majority of bananas and plantains currently grown, which derive from different wild species. Found mainly in the islands of the Pacific, particularly French Polynesia, Fe'i bananas have skins which are brilliant orange to red in colour with yellow or orange flesh inside. They are usually eaten cooked and have been an important food for Pacific Islanders, moving with them as they migrated across the ocean. Most are high in beta-carotene (a precursor of vitamin A).\n\nThe scientific name for Fe'i bananas is Musa\" × \"troglodytarum L. Precisely which wild species they are descended from remained unclear .\n\nFe'i bananas are cultivated varieties (cultivars), rather than wild forms. They are distinctly different from the much more common bananas and plantains derived from \"Musa acuminata\" and \"Musa balbisiana\". All members of the genus \"Musa\" are tall herbaceous plants, typically around tall or even more. Although they appear tree-like, the \"trunk\" is actually a pseudostem, formed from the tightly wrapped bases of the leaves. At maturity each pseudostem produces a single flowering stem that grows up inside it, eventually emerging from the top. As it elongates, female flowers appear which go on to form fruit – the bananas. Finally male flowers are produced. In cultivated bananas, the fruit is usually seedless and the male flowers sterile.\n\nFe'i bananas can be distinguished from other kinds of cultivated bananas and plantains in a number of ways. They have highly coloured sap, pink through to bright magenta and dark purple. The bracts of the flowering spike (inflorescence) are bright shiny green rather than dull red or purple. The flowering and fruiting stem is more or less upright (rather than drooping), so that the bunches of bananas are also upright. Ripe fruit has brilliant orange, copper-coloured or red skin with orange or yellow flesh inside. It has prominent ridges, making it squarish in cross-section.\n\nAs with many names in the genus \"Musa\", considerable confusion has existed as to the proper scientific name, if any, for Fe'i bananas. Some authorities have preferred to treat Fe'i bananas as a formal or informal cultivar group rather than employing a Latin binomial, using names like \"Musa\" (Fe'i Group) 'Utafan'.\n\nOne of the earliest detailed accounts of the genus \"Musa\" was by the German-Dutch botanist Georg E. Rumpf (c.1627–1702), usually known by the Latinized name Rumphius. His \"Herbarium amboinensis\" was published in 1747, after his death. His figure and description of a \"species\" under the name \"Musa Uranoscopos\" (meaning \"heaven-looking banana\") is consistent with a Fe'i banana; he refers to the upright flowering spike (although the figure, reproduced here, shows the terminal bud drooping), the coloured sap, and the effect of consumption on urine.\n\nHowever, the starting point for botanical names is the publication of Carl Linnaeus' \"Species Plantarum\" in 1753, so \"Musa uranoscopos\" is not an acceptable name. In the second edition of \"Species Plantarum\", Linnaeus lumped together Rumphius' \"Musa uranoscopos\" and \"Musa\" 'Pissang Batu' under the name \"Musa troglodytarum\", in spite of the fact that Rumphius had noted several distinctions between the two. Linnaeus' treatment has been described as \"beyond understanding\". In 1917, Merrill designated the illustration of Rumphius' \"Musa uranoscopos\" as the lectotype of \"Musa troglodytarum\" L. On this basis, Häkkinen, Väre and Christenhusz concluded in 2012 that \"all Fe'i cultivars, including those featured in Paul Gauguin's famous paintings, should be treated under the name \"M. troglodytarum\" L.\" Other sources also accept this as the scientific name for the group as a whole – for example Rafaël Govaerts in 2004. The name may be written as \"M.\" × \"troglodytarum\" to stress the hybrid origin of Fe'i bananas.\n\nSynonyms of \"M. troglodytarum\" are:\n\nWhereas most cultivated bananas and plantains are derived from species in \"Musa\" section \"Musa\", Fe'i bananas are clearly part of section \"Callimusa\" (in particular the species formerly grouped as section \"Australimusa\"). However, their precise origins are unclear. On the basis of appearance (morphology), \"Musa maclayi\", native to Papua New Guinea, has been proposed as a parent. More recent genetic studies suggest they are close to \"M. lolodensis\" and \"M. peekelii\", both from New Guinea and neighbouring islands. Fe'i bananas may be hybrids between several different wild species. They are generally considered to have originated in New Guinea and then to have been spread eastwards and northwards (as far as the Hawaiian Islands) for use as food.\n\nA few cultivars have been found which appear to be intermediate between Fe'i bananas and the more common \"Musa\" section \"Musa\" bananas and plantains. Although the part of the stem holding the fruit is upright, the rest of the stem then bends over so that the terminal bud faces sideways or downwards. An example is the cultivar 'Tati'a' from Tahiti. Molecular analysis of bananas with this growth habit from Papua New Guinea has shown evidence of genetic input from \"M. acuminata\" and \"M. balbisiana\", the parents of the section \"Musa\" cultivars. Rumphius' illustration of his \"Musa uranoscopos\"\" shows the same morphology, although this might be artistic license.\n\nFe'i bananas are mainly found from the Moluccas in the west to French Polynesia in the east, particularly the Society Islands and the Marquesas. They have been important both as a staple and as a ceremonial food, although their cultivation and use has sharply declined in recent decades. As the Pacific Islanders spread by canoe throughout the Pacific, they took Fe'i bananas with them; cultivation has been traced back to around 250 BC in the Marquesas and to around 800 AD in Tahiti in the Society Islands. They are believed to have originated in the New Guinea area, where cultivars with seeds occur, as do the wild species from which they are thought to be descended.\n\nFe'i bananas are not in commercial cultivation. There are lists of cultivars for different islands, but it is not clear whether these are synonyms, with the same cultivar being known by different names in different locations and languages. Further, it is not clear whether local names apply to cultivars (i.e. distinct cultivated varieties) or to broader groups. Thus Ploetz et al. refer to a banana found in eastern Indonesia by the cultivar name 'Pisang Tongkat Langit'. However, \"pisang tongkat langit\" can be translated as \"sky stick banana\" or \"heaven cane banana\", corresponding to Rumphius' name \"Musa uranoscopos\" (heaven-gazing banana). \"Pisang tongkat langit\" is treated by other sources as referring to \"M.\" × \"troglodytarum\" as a whole rather than to a single cultivar. Significant genetic variation has been reported among bananas from the Moluccas for which this name is used.\n\nThe list below is selective; where many names are given in sources, it concentrates on those with the most description available.\n\n\nFe'i bananas are generally eaten as \"plantains\", i.e. they are usually cooked rather than eaten raw. They have been described as \"delicious and nutritious when baked or boiled, especially if the slices are swathed in fresh coconut cream.\" They have also been described as \"unpleasantly astringent\" unless cooked, having higher proportions of starch and lower proportions of sugar than other kinds of banana. However, in the Federated States of Micronesia, some cultivars, particularly 'Karat Pwehu', 'Karat Pako' and to a lesser extent 'Utin Iap' (='Uht En Yap'), are commonly eaten raw when fully ripe. Karat bananas have a soft texture and a sweet taste and were a traditional weaning food in the Micronesian island of Pohnpei.\n\nIn countries where Fe'i bananas were once a major food item, there has been a shift away from eating traditional foods towards eating imported foods. Bananas with whiter flesh are preferred over traditional varieties with deeply coloured flesh. One issue with Fe'i bananas is that eating them causes the production of yellow coloured urine, thought to be caused by the excretion of excess riboflavin present in the fruit. This effect led people to believe that Fe'i bananas might not be safe to eat, particularly for children. Along with the shift away from traditional foods, there has been a rise in vitamin A deficiency. Fe'i bananas with deeper coloured flesh have been shown to contain high levels of beta-carotene, a precursor of vitamin A. A year-long promotional campaign in Pohnpei in 1999 to encourage the consumption of Karat cultivars had some success in increasing sales.\n\nLevels of beta-carotene vary considerably among Fe'i bananas. In a study of traditional Solomon Islands cultivars, the highest level of beta-carotene found in a Fe'i cultivar was almost 6,000 µg per 100 g of flesh compared to the highest level of 1,300 µg in a non-Fe'i cultivar. However, there was an overlap; some Fe'i cultivars contained less beta-carotene than non-Fe'i cultivars.\n\nFe'i banana plants have many other uses. Like other kinds of banana, the leaves may be used as plates or containers for cooked food. They can also be used as a roofing material, particularly for temporary huts. The fibres of the midrib of the leaves can be used to make ropes, often used to carry bunches of bananas. Other fibrous parts of the leaves can be dried and plaited into mats and similar items. The pseudostems are buoyant, and so can be used to make temporary rafts.\n\nFe'i bananas have distinctive reddish sap which does not readily fade on exposure to light. It is used as a dye, and has also been used to make ink.\n\nThe early European explorers of the Pacific islands produced a few accounts of Fe'i bananas. In 1788, Daniel Solander accompanied Joseph Banks on James Cook's first voyage to the Pacific Ocean aboard the \"Endeavour\". In the account he published later, he noted five kinds of banana or plantain called \"Fe'i\" by the Tahitians. William Ellis lived in the Society Islands in the 1850s. He refers to the name \"Fe'i\", saying that Fe'i bananas were the principal food for the inhabitants of some islands. He also noted that Fe'i banana plants have an upright fruit cluster.\n\nCharles Darwin visited Tahiti in the Society Islands in 1835 and gave an account in \"The Voyage of the Beagle\". Although he does not mention the name \"Fe'i\", he does speak of the \"mountain-banana\": \"On each side of the ravine there were great beds of the mountain-banana, covered with ripe fruit. Many of these plants were from twenty to twenty-five feet high, and from three to four in circumference.\" Fe'i bananas have been noted to grow best in Tahiti on slopes at the base of cliffs.\n\nLaurence H. MacDaniels published a study of the Fe'i banana in 1947. He reported that Fe'i bananas were the staple carbohydrate food of the Society Islanders, and that more than 95% of the bananas on sale were of the Fe'i type. Although some Fe'i banana plants were found in gardens, most bananas were gathered from the \"wild\", thought to have been planted in the past and abandoned.\n\nFe'i bananas are an important component of ceremonial feasts in the Marquesas and the Society Islands. Karat bananas are reported to be one of the few kinds of banana that can be used in ceremonial presentations in Pohnpei, Micronesia. A Samoan legend is that the mountain and the lowland banana fought. The mountain banana – the Fe'i banana – won. Filled with pride at its victory, the mountain banana raised its head high, whereas the defeated lowland banana never raised its head again. (Fe'i bananas have an upright fruiting stem, whereas the fruiting stem droops in other kinds of banana.)\n\nThe bright orange-red colours of Fe'i bananas make them attractive to artists. The French post-impressionist painter Paul Gauguin visited the Society Islands, including Tahiti, towards the end of the 19th century. Three of his works include what are considered to be Fe'i bananas: \"Le Repas\" (The Meal, 1891), \"La Orana Maria\" (Hail Mary, 1891) and \"Paysage de Tahiti\" (Tahitian Landscape, 1891).\n\nFe'i banana cultivars, along with other Pacific crop propagation material, have been saved at the Centre for Pacific Crops and Trees (CePaCT), which catalogs living plants of the Pacific region for conservation. More than 100 samples of Fe'i bananas were collected in French Polynesia, from isolated farms on six different islands. The samples will be conserved in a gene bank in Tahiti, with duplicates kept at CePaCt.\n"}
{"id": "8132718", "url": "https://en.wikipedia.org/wiki?curid=8132718", "title": "Fingrid", "text": "Fingrid\n\nFingrid Oyj is a Finnish national electricity transmission grid operator. It is owned by the Finnish state (53.1 %) and various financial and insurance institutions (46.9 %). In 2011, power companies Fortum Power and Heat Oy and Pohjolan Voima sold their stakes in Fingrid (25% each), because of EU Internal Market Directive in Electricity, which requires that power production and ownership of the transmission systems should be separate. CEO of the company is Jukka Ruusunen.\n\n"}
{"id": "671092", "url": "https://en.wikipedia.org/wiki?curid=671092", "title": "Flixborough disaster", "text": "Flixborough disaster\n\nThe Flixborough disaster was an explosion at a chemical plant close to the village of Flixborough, North Lincolnshire, England on Saturday, 1 June 1974. It killed 28 people and seriously injured 36 out of a total of 72 people on site at the time. The casualty figures could have been much higher, if the explosion had occurred on a weekday, when the main office area would have been occupied. A contemporary campaigner on process safety wrote \"the shock waves rattled the confidence of every chemical engineer in the country\". \n\nThe disaster involved (and may well have been caused by) a hasty modification. There was no on-site senior manager with mechanical engineering expertise (virtually all the plant management had chemical engineering qualifications); mechanical engineering issues with the modification were overlooked by the managers who approved it, nor was the severity of the potential consequences of its failure appreciated. \n\nFlixborough led to a widespread public outcry over process plant safety. Together with the passage of the Health and Safety at Work Act in the same year it led to (and is often quoted in justification of) a more systematic approach to process safety in UK process industries, and – in conjunction with the Seveso disaster and the consequent EU 'Seveso directives' – to explicit UK government regulation of plant processing or storing large inventories of hazardous materials, currently (2014) by the Control of Major Accident Hazards Regulations 1999 (COMAH).\n\nThe chemical works, owned by Nypro UK (a joint venture between Dutch State Mines (DSM) and the British National Coal Board (NCB)) had originally produced fertiliser from by-products of the coke ovens of a nearby steelworks. Since 1967, it had instead produced caprolactam, a chemical used in the manufacture of nylon 6. The caprolactam was produced from cyclohexanone. This was originally produced by hydrogenation of phenol, but in 1972 additional capacity was added, built to a DSM design in which hot liquid cyclohexane was partially oxidised by compressed air. The plant was intended to produce 70,000 tpa (tons per annum) of caprolactam but was reaching a rate of only 47,000 tpa in early 1974. Government controls on the price of caprolactam put further financial pressure on the plant.\n\nIt was a failure of this plant that led to the disaster. A major leak of liquid from the reactor circuit caused the rapid formation of a large cloud of flammable hydrocarbon. When this met an ignition source (probably a furnace at a nearby hydrogen production plant) there was a massive fuel-air explosion. The plant control room collapsed, killing all 18 occupants. Nine other site workers were killed, and a delivery driver died of a heart attack in his cab. Fires started on-site which were still burning ten days later. Around 1,000 buildings within a mile radius of the site (in Flixborough itself and in the neighbouring villages of Burton upon Stather and Amcotts) were damaged, as were nearly 800 in Scunthorpe (three miles away); the blast was heard over thirty miles away in Grimsby and Hull. Images of the disaster were soon shown on television, filmed by BBC and Yorkshire Television filmstock news crews who had been covering the Appleby-Frodingham Gala in Scunthorpe that afternoon.\n\nThe plant was re-built but cyclohexanone was now produced by hydrogenation of phenol (Nypro proposed to produce the hydrogen from LPG; in the absence of timely advice from the Health and Safety Executive (HSE) planning permission for storage of 1200 te LPG at Flixborough was initially granted subject to HSE approval, but HSE objected); as a result of a subsequent collapse in the price of nylon it closed down a few years later. The site was demolished in 1981, although the administration block still remains. The site today is home to the Flixborough Industrial Estate, occupied by various businesses and Glanford Power Station.\n\nThe foundations of properties severely damaged by the blast and subsequently demolished can be found on land between the estate and the village, on the route known as Stather Road. A memorial to those who died was erected in front of offices at the rebuilt site in 1977. Cast in bronze, it showed mallards alighting on water. When the plant was closed, the statue was moved to the pond at the parish church in Flixborough. During the early hours of New Year's Day 1984, the sculpture was stolen. It has never been recovered but the plinth it stood on, with a plaque listing all those who died that day, can still be found outside the church.\n\nThe cyclohexane oxidation process is still operated in much the same plant design in the Far East.\n\nIn the DSM process, cyclohexane was heated to about 155 °C (311 °F) before passing into a series of six reactors. The reactors were constructed from mild steel with a stainless steel lining; when operating they held in total about 145 tonnes of flammable liquid at a working pressure of 8.6 bar gauge (0.86 MPa gauge; 125 psig). In each of the reactors, compressed air was passed through the cyclohexane, causing a small percentage of the cyclohexane to oxidise and produce cyclohexanone, some cyclohexanol also being produced. Each reactor was slightly (approximately 14 inches, 350 mm) lower than the previous one, so that the reaction mixture flowed from one to the next by gravity through nominal 28-inch bore (700mm DN) stub pipes with inset bellows. The inlet to each reactor was baffled so that liquid entered the reactors at a low level; the exiting liquid flowed over a weir whose crest was somewhat higher than the top of the outlet pipe. The mixture exiting reactor 6 was processed to remove reaction products, and the unreacted cyclohexane (only about 6% was reacted in each pass) then returned to the start of the reactor loop.\n\nAlthough the operating pressure was maintained by an automatically controlled bleed valve once the plant had reached steady state, the valve could not be used during start-up, when there was no air feed, the plant being pressurised with nitrogen. During start-up the bleed valve was normally isolated and there was no route for excess pressure to escape; pressure was kept within acceptable limits (slightly wider than those achieved under automatic control) by operator intervention (manual operation of vent valves). A pressure-relief valve acting at gauge was also fitted.\n\nTwo months prior to the explosion, the number 5 reactor was discovered to be leaking. When lagging was stripped from it, a crack extending about was visible in the mild steel shell of the reactor. It was decided to install a temporary pipe to bypass the leaking reactor to allow continued operation of the plant while repairs were made. In the absence of 28-inch nominal bore pipe (700mm DN), 20-inch nominal bore pipe (500mm DN) was used to fabricate the bypass pipe for linking reactor 4 outlet to reactor 6 inlet. The new configuration was tested for leak-tightness at working pressure by pressurisation with nitrogen. For two months after fitting the bypass was operated continuously at temperature and pressure and gave no trouble. At the end of May (by which time the bypass had been lagged) the reactors had to be depressurised and allowed to cool in order to deal with leaks elsewhere. The leaks having been dealt with, early on 1 June attempts began to bring the plant back up to pressure and temperature.\n\nAt about 16:53 on Saturday 1 June 1974, there was a massive release of hot cyclohexane in the area of the missing reactor 5, followed shortly by ignition of the resulting cloud of flammable vapour and a massive explosion in the plant. It virtually demolished the site. Since the accident took place at a weekend there were relatively few people on site: of those on-site at the time, 28 were killed and 36 injured. Fires continued on-site for more than ten days. Off-site there were no fatalities, but 50 injuries were reported and about 2,000 properties damaged.\n\nThe occupants of the works laboratory had seen the release and evacuated the building before the release ignited; most survived. None of the 18 occupants of the plant control room survived, nor did any records of plant readings. The explosion appeared to have been in the general area of the reactors and after the accident only two possible sites for leaks before the explosion were identified: \"the 20 inch bypass assembly with the bellows at both ends torn asunder was found jack-knifed on the plinth beneath\" and there was a 50-inch long split in nearby 8-inch nominal bore stainless steel pipework\".\n\nImmediately after the accident, \"New Scientist\" commented presciently on the normal official response to such events, but hoped that the opportunity would be taken to introduce effective government regulation of hazardous process plants.\nThe Secretary of State for Employment set up a Court of Inquiry to establish the causes and circumstances of the disaster and identify any immediate lessons to be learned, and also an expert committee to identify major hazard sites and advise on appropriate measures of control for them. The Inquiry sat for 70 days in the period September 1974 – February 1975, and took evidence from over 170 witnesses. In parallel, an Advisory Committee on Major Hazards was set up to look at the longer term issues associated with hazardous process plant.\n\nThe report of the court of inquiry was critical of the installation of the bypass pipework on a number of counts: although plant and senior management were chartered engineers (mostly chemical engineers) the post of Works Engineer which had been occupied by a chartered mechanical engineer had been vacant since January 1974 and at the time of the accident there were no professionally qualified engineers in the works engineering department. Nypro had recognised this to be a weakness and identified a senior mechanical engineer in an NCB subsidiary as available to provide advice and support if requested. At a meeting of plant and engineering managers to discuss the failure of Reactor 5, the external mechanical engineer was not present. The emphasis was upon prompt restart and – the inquiry felt – although this did not lead to the deliberate acceptance of hazards, it led to the adoption of a course of action whose hazards (and indeed engineering practicalities) were not adequately considered or understood. The major problem was thought to be getting reactor 5 moved out of the way. Only the plant engineer was concerned about restarting before the reason for the failure was understood, and the other reactors inspected. The difference in elevation between reactor 4 outlet and reactor 6 inlet was not recognised at the meeting. At a working level the offset was accommodated by a dog-leg in the bypass assembly; a section sloping downwards inserted between (and joined with by mitre welds) two horizontal lengths of 20-inch pipe abutting the existing 28-inch stubs. This bypass was supported by scaffolding fitted with supports provided to prevent the bellows having to take the weight of the pipework between them, but with no provision against other loadings. The Inquiry noted on the \"design\" of the assembly:\nThe Inquiry noted further that \"there was no overall control or planning of the design, construction, testing or fitting of the assembly nor was any check made that the operations had been properly carried out\". After the assembly was fitted, the plant was tested for leak-tightness by pressurising with nitrogen to 9 kg/cm; i.e. roughly operating pressure, but below the pressure at which the system relief valve would lift and below the 30% above design pressure called for by the relevant British Standard.\n\nThe 20-inch bypass was therefore clearly not what would have been produced or accepted by a more considered process but controversy developed (and became acrimonious) as to whether its failure was the initiating fault in the disaster (the 20-inch hypothesis, argued by the plant designers (DSM) and the plant constructors; and favoured by the court's technical advisers), or had been triggered by an external explosion resulting from a previous failure of the 8-inch line (argued by experts retained by Nypro and their insurers).\n\nTests on replica bypass assemblies showed that bellows squirm could occur at pressures below the safety valve setting, but that squirm did not lead to a leak (either from damage to the bellows or from damage to the pipe at the mitre welds) until well above the safety valve setting. However theoretical modelling suggested that the expansion of the bellows as a result of squirm would lead to a significant amount of work being done on them by the reactor contents, and there would be considerable shock loading on the bellows when they reached the end of their travel. If the bellows were 'stiff' (resistant to squirm), the shock loading could cause the bellows to tear at pressures below the safety valve setting; it was not impossible that this could occur at pressures experienced during start-up, when pressure was less tightly controlled. (Plant pressures at the time of the accident were unknown since all relevant instruments and records had been destroyed, and all relevant operators killed).\nThe Inquiry concluded that this (\"the 20-inch hypothesis\") was 'a probability' but one 'which would readily be displaced if some greater probability' could be found.\n\nDetailed analysis suggested that the 8-inch pipe had failed due to creep cavitation at a high temperature while the pipe was under pressure. Failure had been accelerated by contact with molten zinc and there were indications that an elbow in the pipe had been at significantly higher temperature than the rest of the pipe. The hot elbow led to a non-return valve held between two pipe flanges by twelve bolts. After the disaster, two of the twelve bolts were found to be loose; the inquiry concluded that they were probably loose before the disaster. Nypro argued that the bolts had been loose, there had consequently been a slow leak of process fluid onto lagging leading eventually to a lagging fire, which had worsened the leak to the point where a flame had played undetected upon the elbow, burnt away its lagging and exposed the line to molten zinc, the line then failing with a bulk release of process fluid which extinguished the original fire, but subsequently ignited giving a small explosion which had caused failure of the bypass, a second larger release and a larger explosion. Tests failed to produce a lagging fire with leaked process fluid at process temperatures; one advocate of the 8-inch hypothesis then argued instead that there had been a gasket failure giving a leak with sufficient velocity to induce static charges whose discharge had then ignited the leak.\n\nThe 8-inch hypothesis was claimed to be supported by eyewitness accounts and by the apparently anomalous position of some debris post-disaster. The inquiry report took the view that explosions frequently throw debris in unexpected directions and eyewitnesses often have confused recollections. The inquiry identified difficulties at various stages of the accident development in the 8-inch hypothesis, their cumulative effect being considered to be such that the report concluded that overall the 20-inch hypothesis involving 'a single event of low probability' was more credible than the 8-inch hypothesis depending upon 'a succession of events, most of which are improbable'.\n\nThe inquiry report identified 'lessons to be learned' which it presented under various headings; 'General observation' (relating to cultural issues underlying the disaster), 'specific lessons' (directly relevant to the disaster, but of general applicability) are reported below; there were also 'general' and 'miscellaneous lessons' of less relevance to the disaster. The report also commented on matters to be covered by the Advisory Committee on Major Hazards.\n\n\nThe disaster was caused by 'a well designed and constructed plant' undergoing a modification that destroyed its technical integrity.\n\nWhen the bypass was installed, there was no works engineer in post and company senior personnel (all chemical engineers) were incapable of recognising the existence of a simple engineering problem, let alone solving it\n\nNo one concerned in the design or construction of the plant envisaged the possibility of a major disaster happening instantaneously. It was now apparent that such a possibility exists where large amounts of potentially explosive material are processed or stored. It was 'of the greatest importance that plants at which there is a risk of instant as opposed to escalating disaster be identified. Once identified measures should be taken both to prevent such a disaster so far as is possible and to minimise its consequences should it occur despite all precautions.' There should be coordination between planning authorities and the Health and Safety Executive, so that planning authorities could be advised on safety issues before granting planning permission; similarly the emergency services should have information to draw up a disaster plan.\n\nThe inquiry summarised its findings as follows:\n\nNypro's advisers had put considerable effort into the 8-inch hypothesis, and the inquiry report put considerable effort into discounting it. The critique of the hypothesis spilled over into criticism of its advocates: 'the enthusiasm for the 8-inch hypothesis felt by its proponents has led them to overlook obvious defects which in other circumstances they would not have failed to realise'. Of one proponent the report noted gratuitously that his examination by the court 'was directed to ensuring that we had correctly appreciated the main steps in the hypothesis some of which appeared to us in conflict with facts which were beyond dispute'. The report thanked him for his work in assembling eyewitness evidence but said his use of it showed 'an approach to the evidence which is wholly unsound'.\n\nThe proponent of the 8-inch gasket failure hypothesis responded by arguing that the 20-inch hypothesis had its share of defects which the inquiry report had chosen to overlook, that the 8-inch hypothesis had more in its favour than the report suggested, and that there were important lessons that the inquiry had failed to identify:\n\nThe Flixborough inquiry findings have not been accorded the normal respect; one critic of them was able to note after a flurry of articles on the 25th anniversary:\n\nThe HSE website currently (2014) says \"During the late afternoon on 1 June 1974 a 20 inch bypass system ruptured, which may have been caused by a fire on a nearby 8-inch pipe\". In the absence of a strong consensus for either hypothesis other possible immediate causes have been suggested.\n\nThe enquiry noted the existence of a small tear in a bellows fragment, and therefore considered the possibility of a small leak from the bypass having led to an explosion bringing the bypass down. It noted this to be not inconsistent with eyewitness evidence, but ruled out the scenario because pressure tests showed the bellows did not develop tears until well above the safety valve pressure. This hypothesis has however been revived, with the tears being caused by fatigue failure at the top of the reactor 4 outlet bellows because of flow-induced vibration of the unsupported bypass line. Finite element analysis has been carried out (and suitable eyewitness evidence adduced) to support this hypothesis.\n\nThe reactors were normally mechanically stirred but reactor 4 had operated without a working stirrer since November 1973; free phase water could have settled out in unstirred reactor 4 and the bottom of reactor 4 would reach operating temperature more slowly than the stirred reactors. It was postulated that there had been bulk water in reactor 4 and a disruptive boiling event had occurred when the interface between it and the reaction mixture reached operating temperature. Abnormal pressures and liquor displacement resulting from this (it was argued) could have triggered failure of the 20-inch bypass..\n\nThe plant design had assumed that the worst consequence of a major leak would be a plant fire and to protect against this a fire detection system had been installed. Tests by the Fire Research Establishment had shown this to be less effective than intended. Moreover, fire detection only worked if the leak ignited at the leak site; it gave no protection against a major leak with delayed ignition, and the disaster had shown this could lead to multiple worker fatalities. The plant \"as designed\" therefore could be destroyed by a single failure and had a much greater risk of killing workers than the designers had intended. Critics of the inquiry report therefore found it hard to accept its characterisation of the plant as 'well-designed'. The HSE (through the Department of Employment) had come up with a 'shopping list' of about 30 recommendations on plant design, many of which had not been adopted (and a few explicitly rejected) by the Inquiry Report; the HSE inspector who acted as secretary to the inquiry spoke afterwards of making sure that the real lessons were acted upon. More fundamentally, Trevor Kletz saw the plant as symptomatic of a general failure to consider safety early enough in process plant design, so that designs were inherently safe – instead processes and plant were selected on other grounds then safety systems bolted on to a design with avoidable hazards and unnecessarily high inventory. 'We keep a lion and build a strong cage to keep it in. But before we do so we should ask if a lamb might do.'\n\nIf the UK public were largely reassured to be told the accident was a one-off and should never happen again, some UK process safety practitioners were less sanguine. Critics felt that the Flixborough explosion was not the result of multiple basic engineering design errors unlikely to coincide again; the errors were rather multiple instances of one underlying cause: a complete breakdown of plant safety procedures (exacerbated by a lack of relevant engineering expertise, but that lack was also a procedural shortcoming).\n\nThe Petrochemicals Division of Imperial Chemical Industries (ICI) operated many plants with large inventories of flammable chemicals at its Wilton site (including one in which cyclohexane was oxidised to cyclohexanone and cyclohexanol). Historically good process safety performance at Wilton had been marred in the late 1960s by a spate of fatal fires caused by faulty isolations/handovers for maintenance work. Their immediate cause was human error but ICI felt that saying that most accidents were caused by human error was no more useful than saying that most falls are caused by gravity. ICI had not simply reminded operators to be more careful, but issued explicit instructions on the required quality of isolations, and the required quality of its documentation. The more onerous requirements were justified as follows:\n\nIn accordance with this view, post-Flixborough (and without waiting for the Inquiry Report), ICI Petrochemicals instituted a review of how it controlled modifications. It found that major projects requiring financial sanction at a high level were generally well-controlled, but for more (financially) minor modifications there was less control and this had resulted in a past history of 'near-misses' and small-scale accidents, few of which could be blamed on chemical engineers. To remedy this, not only were employees reminded of the principal points to consider when making a modification (both on the quality/compliance of the modification itself and on the effect of the modification on the rest of the plant), but new procedures and documentation were introduced to ensure adequate scrutiny. These requirements applied not only to changes to equipment, but also to process changes. All modifications were to be supported by a formal safety assessment. For major modifications this would include an 'operability study'; for minor modifications a checklist-based safety assessment was to be used, indicating what aspects would be affected, and for each aspect giving a statement of the expected effect. The modification and its supporting safety assessment then had to be approved in writing by the plant manager and engineer. Where instruments or electrical equipment were involved signatures would also be needed from the relative specialist (instrument manager or electrical engineer). A Pipework Code of Practice was introduced specifying standards of design construction and maintenance for pipework – all pipework over 3\"nb (DN 75 mm) handling hazardous material would have to be designed by pipework specialists in the design office.\nThe approach was publicised outside ICI; while the Pipework Code of Practice on its own would have combatted the specific fault(s) that led to the Flixborough disaster, the adoption more generally of tighter controls on modifications (and the method by which this was done) were soon recognised to be prudent good practice. In the United Kingdom, the ICI approach became a \"de facto\" standard for high-risk plant (partly because the new (1974) Health and Safety at Work Act went beyond specific requirements on employers to state general duties to keep risks to workers as low as reasonably practicable and to avoid risk to the public so far as reasonably practicable; under this new regime the presumption was that recognised good practice would inherently be 'reasonably practicable' and hence should be adopted, partly because key passages in reports of the Advisory Committee on Major Hazards were clearly supportive).\n\nThe terms of reference of the Court of Inquiry did not include any requirement to comment on the regulatory regime under which the plant had been built and operated, but it was clear that it was not satisfactory. Construction of the plant had required planning permission approval by the local council; while \"an interdepartmental procedure enabled planning authorities to call upon the advice of Her Majesty's Factory Inspectorate when considering applications for new developments which might involve a major hazard\" (there was no requirement for them to do so), since the council had not recognised the hazardous nature of the plant they had not called for advice. As the \"New Scientist\" commented within a week of the disaster:\n\nThe ACMH's terms of reference were to identify types of (non-nuclear) installations posing a major hazard, and advise on appropriate controls on their establishment, siting, layout, design, operation, maintenance and development (including overall development in their vicinity). Unlike the Court of Inquiry, its personnel (and that of its associated working groups) had significant representation of safety professionals, drawn largely from the nuclear industry and ICI (or ex-ICI)\n\nIn its first report (issued as a basis for consultation and comment in March 1976), the ACMH noted that hazard could not be quantified in the abstract, and that a precise definition of 'major hazard' was therefore impossible. Instead installations with an inventory of flammable fluids above a certain threshold or of toxic materials above a certain 'chlorine equivalent' threshold should be ' \"notifiable installations\" '. A company operating a notifiable installation should be required to survey its hazard potential, and inform HSE of the hazards identified and the procedures and methods adopted (or to be adopted) to deal with them.\n\nHSE could then choose to – in some cases (generally involving high risk or novel technology) – require submission of a more elaborate assessment, covering (as appropriate) \"design, manufacture, construction, commissioning, operation and maintenance, as well as subsequent modifications whether of the design or operational procedures or both\". The company would have to show that \"it possesses the appropriate management system, safety philosophy, and competent people, that it has effective methods of identifying and evaluating hazards, that it has designed and operates the installation in accordance with appropriate regulations, standards and codes of practice, that it has adequate procedures for dealing with emergencies, and that it makes use of independent checks where appropriate\"\n\nFor most 'notifiable installations' no further explicit controls should be needed; HSE could advise and if need be enforce improvements under the general powers given it by the 1974 Health and Safety at Work Act (HASAWA), but for a very few sites explicit licensing by HSE might be appropriate; responsibility for safety of the installation remaining however always and totally with the licensee.\n\nHASAWA already required companies to have a safety policy, and a comprehensive plan to implement it. ACMH felt that for major hazard installations the plan should be formal and include\n\nSafety documents were needed both for design and operation. The management of major hazard installations must show that it possessed and used a selection of appropriate hazard recognition techniques, had a proper system for audit of critical safety features, and used independent assessment where appropriate.\n\nThe ACMH also called for tight discipline in the operation of major hazard plants:\n\nThe ACMH's second report (1979) rejected criticisms that since accidents causing multiple fatalities were associated with extensive and expensive plant damage the operators of major hazard sites had every incentive to avoid such accidents and so it was excessive to require major hazard sites to demonstrate their safety to a government body in such detail:\n\nThe approach advocated by the ACMH was largely followed in subsequent UK legislation and regulatory action, but following the release of chlordioxins by a runaway chemical reaction at Seveso in northern Italy in July 1976, 'major hazard plants' became an EU-wide issue and the UK approach became subsumed in EU-wide initiatives (the Seveso Directive in 1982, superseded by the Seveso II Directive in 1996). A third and final report was issued when the ACMH was disbanded in 1983.\n\n\n\n"}
{"id": "19386345", "url": "https://en.wikipedia.org/wiki?curid=19386345", "title": "Gyrokinetics", "text": "Gyrokinetics\n\nGyrokinetics is a theoretical framework to study plasma behavior on perpendicular spatial scales comparable to the gyroradius and frequencies much lower than the particle cyclotron frequencies. \nThese particular scales have been experimentally shown to be appropriate for modeling plasma turbulence. The trajectory of charged particles in a magnetic field is a helix that winds around the field line. This trajectory can be decomposed into a relatively slow motion of the guiding center along the field line and a fast circular motion, called gyromotion. For most plasma behavior, this gyromotion is irrelevant. Averaging over this gyromotion reduces the equations to six dimensions (3 spatial, 2 velocity, and time) rather than the seven (3 spatial, 3 velocity, and time). Because of this simplification, gyrokinetics governs the evolution of charged rings with a guiding center position, instead of gyrating charged particles.\n\nFundamentally, the gyrokinetic model assumes the plasma is strongly magnetized ( formula_1 ), the perpendicular spatial scales are comparable to the gyroradius ( formula_2 ), and the behavior of interest has low frequencies ( formula_3 ). We must also expand the distribution function, formula_4, and assume the perturbation is small compared to the background (formula_5). The starting point is the Fokker–Planck equation and Maxwell's equations. The first step is to change spatial variables from the particle position formula_6 to the guiding center position formula_7. Then, we change velocity coordinates from formula_8 to the velocity parallel formula_9, the magnetic moment formula_10, and the gyrophase angle formula_11. Here parallel and perpendicular are relative to formula_12, the direction of the magnetic field, and formula_13 is the mass of the particle. Now, we can average over the gyrophase angle at constant guiding center position, denoted by formula_14, yielding the gyrokinetic equation.\n\nThe electrostatic gyrokinetic equation, in the absence of large plasma flow, is given by\n\nformula_15.\n\nHere the first term represents the change in the perturbed distribution function, formula_16, with time. The second term represents particle streaming along the magnetic field line. The third term contains the effects of cross-field particle drifts, including the curvature drift, the grad-B drift, and the lowest order E-cross-B drift. The fourth term represents the nonlinear effect of the perturbed formula_17 drift interacting with the distribution function perturbation. The fifth term uses a collision operator to include the effects of collisions between particles. The sixth term represents the Maxwell–Boltzmann response to the perturbed electric potential. The last term includes temperature and density gradients of the background distribution function, which drive the perturbation. These gradients are only significant in the direction across flux surfaces, parameterized by formula_18, the magnetic flux.\n\nThe gyrokinetic equation, together with gyro-averaged Maxwell's equations, give the distribution function and the perturbed electric and magnetic fields. In the electrostatic case we only require Gauss's law (which takes the form of the quasineutrality condition), given by\n\nformula_19.\n\nUsually solutions are found numerically with the help of supercomputers, but in simplified situations analytic solutions are possible.\n\n\n\n"}
{"id": "25536879", "url": "https://en.wikipedia.org/wiki?curid=25536879", "title": "HELMEPA", "text": "HELMEPA\n\nThe Hellenic Marine Environment Protection Association (HELMEPA), established in 1982, is Europe’s first private sector voluntary marine environment protection association.\n\nHELMEPA's aims are to eliminate ship-generated marine pollution and enhance safety at sea.\n\nThe Association trains seafarers and executives so that they are aware of safety and the protection of the marine environment,\" . HELMEPA has also launched several environmental projects and public awareness campaigns such as the environmental education of schoolchildren through HELMEPA Junior. \n\nHELMEPA supports governments in ratifying and implementing international conventions on the protection of the marine environment. \n\nPoliticians and notable business men like Sir Stelios Haji-Ioannou actively support HELMEPA. \n"}
{"id": "38279898", "url": "https://en.wikipedia.org/wiki?curid=38279898", "title": "Kondapalli Toys", "text": "Kondapalli Toys\n\nKondapalli Toys are the toys made of wood in Kondapalli of Krishna district, a village nearby Vijayawada in the Indian state of Andhra Pradesh. \"Bommala Colony\" translates to \"Toys Colony\" in Kondapalli is the place where the art of crafting takes place. It was registered as one of the geographical indication handicraft from Andhra Pradesh as per \"Geographical Indications of Goods (Registration and Protection) Act, 1999\".\n\nThe art of crafting is a 400 year old tradition. The artisans who make the toys are referred as \"Aryakhastriyas\" (also known as Nakarshalu), who have their mention in the \"Brahmanda Purana\". They are said to have migrated from Rajasthan in the 16th century to Kondappali and claims their origin to Muktharishi, a sage endowed with skills in arts and crafts by Lord Shiva.\n\nThe Kondapalli toys are made from soft wood known as \"Tella Poniki\" which are found in nearby Kondapalli Hills. The wood is first carved out and then the edges are smooth finished. The later step involves coloring with either oil and water-colours or vegetable dyes and enamel paints are applied based on the type of the toys. The artisans mainly work on producing figures of mythology, animals, birds, bullock carts, rural life etc., and the most notable one is \"Dasavataram\", dancing dolls etc.\n\nThe art form which has got patronage from the rulers in ancient times is in decline due to lack of profits, time taking to produce toys, influence of western art and younger generations not encouraged towards this art. Lepakshi and \"Lanco Institute of General Humanitarian Trust\" took initiative to keep alive the art of crafting toys.\n"}
{"id": "647683", "url": "https://en.wikipedia.org/wiki?curid=647683", "title": "Lenin (1957 icebreaker)", "text": "Lenin (1957 icebreaker)\n\nLenin () is a Soviet nuclear-powered icebreaker. Launched in 1957, it was both the world's first nuclear-powered surface ship and the first nuclear-powered civilian vessel. \"Lenin\" entered operation in 1959 and worked clearing sea routes for cargo ships along Russia's northern coast. From 1960 to 1965 the ship covered over 85,000 miles during the Arctic navigation season, of which almost 65,000 were through ice. On April 10, 1974 the vessel was awarded the Order of Lenin. She was officially decommissioned in 1989. She was subsequently converted to a museum ship and is now permanently based at Murmansk.\n\nWhen launched in 1957, \"Lenin\" was powered by three OK-150 reactors. In its late-1960s configuration, at full capacity the ship used five to six pounds of uranium-235 per 100 days.\n\nIn the configuration employed from 1970, two OK-900 reactors provided steam for four steam turbines, that were in turn connected to generators, which powered three sets of electric motors to drive the ship's three propellers.\n\nIn February 1965, there was a loss-of-coolant accident. After being shut down for refueling, the coolant was removed from the number two reactor before the spent fuel had been removed. As a result, some of the fuel elements melted and deformed inside the reactor. This was discovered when the spent elements were being unloaded for storage and disposal. 124 fuel assemblies (about 60% of the total) were stuck in the reactor core. It was decided to remove the fuel, control grid, and control rods as a unit for disposal; they were placed in a special cask, solidified, stored for two years, and dumped in Tsivolki Bay (near the Novaya Zemlya archipelago) in 1967.\n\nThe second accident was a cooling system leak which occurred in 1967, shortly after refueling. Finding the leak required breaking through the concrete and metal biological shield with sledgehammers. Once the leak was found, it became apparent that the sledgehammer damage could not be repaired; subsequently, all three reactors were removed by blowing them off the ship with shaped charges above a burial site off Novaya Zemlya, and replaced by two OK-900 reactors. This was completed in early 1970.\n\n\"Lenin\" was decommissioned in 1989, because its hull had worn thin from ice friction. She was laid up at Atomflot, a base for nuclear icebreakers in Murmansk, and according to Pravda.ru, repair and conversion into a museum ship was completed in 2005.\n\n"}
{"id": "41459593", "url": "https://en.wikipedia.org/wiki?curid=41459593", "title": "Liberation cutting", "text": "Liberation cutting\n\nLiberation cutting has similar goals to cleaning, namely the allocation of resources to the most promising trees available on a site. What separates liberation cutting from cleaning is that the overtopping competitors are of a distinctly older age class. Need for liberation cutting often occurs when seedlings of a desired species have been regenerated by a logging operation, but that operation has left older, poor quality or undesired trees that are shading the regeneration and limiting its growth.\n\nLiberation cutting may be superficially similar to an overstory removal cutting. The major difference between these is that in the overstory removal, regeneration was deliberate and the best trees were saved for the final harvest. In the liberation cutting, the worst trees remain and regeneration was likely an afterthought to a logging operation.\n\nHarvesting the undesired trees is not a requirement in liberation operations; the poor quality trees may be killed in place and left as snags, or felled and left to contribute coarse woody debris.\n\n"}
{"id": "5818314", "url": "https://en.wikipedia.org/wiki?curid=5818314", "title": "Neonopolis", "text": "Neonopolis\n\nNeonopolis, a shopping mall, is a $100 million entertainment complex in Las Vegas, Nevada located on top of a $15 million city parking garage. It is located on Fremont Street, at the corner of Las Vegas Boulevard. In keeping with the complex's name, it contains three miles of neon lights.\n\nThree open-air levels surround an outdoor center courtyard with stage, sound and seating. Bands and concerts were scheduled during the summer months during its heyday.\n\nPlans for Neonopolis were announced in December 1997. It opened on May 3, 2002.\n\nIn 2006, Prudential Real Estate sold Neonopolis to a development group led by Rohit Joshi for $25 million. Joshi undertook an abortive rebranding of the center as Fremont Square, but ultimately the Neonopolis name was retained. After temporarily closing in 2010 for redevelopment, Neonopolis underwent renovations in 2011 and reopened with new tenants.\n\n\n\n"}
{"id": "21795385", "url": "https://en.wikipedia.org/wiki?curid=21795385", "title": "Old Partner", "text": "Old Partner\n\nOld Partner (; lit. \"Cowbell Sound\") is a 2008 South Korean documentary film directed by Lee Chung-ryoul. Set in the small rural town of Hanul-ri in Sangun-myeon, Bonghwa County, North Gyeongsang Province, the film focuses on the relationship between a 40-year-old cow and an old farmer in his 80s.\n\nThe film was a surprise success. It attracted over 2.93 million viewers, setting the record for the highest grossing independent film in Korean film history. It won the PIFF Mecenat Award at the Pusan International Film Festival and the Audience Award at the Korean Independent Film Awards. Lee Chung-ryoul became the first independent film director to receive the Best New Director award at the Baeksang Arts Awards.\n\n\n"}
{"id": "30243152", "url": "https://en.wikipedia.org/wiki?curid=30243152", "title": "Pegasus Toroidal Experiment", "text": "Pegasus Toroidal Experiment\n\nThe Pegasus Toroidal Experiment is a plasma confinement experiment relevant to fusion power production, run by the Department of Engineering Physics of the University of Wisconsin-Madison. It is a spherical tokamak, a very low-aspect-ratio version of the tokamak configuration, i.e. the minor radius of the torus is comparable to the major radius.\n\nOther spherical torus experiments:\n\n"}
{"id": "45294937", "url": "https://en.wikipedia.org/wiki?curid=45294937", "title": "Penta-graphene", "text": "Penta-graphene\n\nPenta-graphene is a carbon allotrope composed entirely of carbon pentagons and resembling the Cairo pentagonal tiling. Penta-graphene was proposed in 2014 on the basis of analyses and simulations. Further calculations showed that it is unstable in its pure form, but can be stabilized by hydrogenation. Owing to its atomic configuration, penta-graphene has an unusually negative Poisson’s ratio and very high ideal strength believed to exceed that of a similar material, graphene.\n\nPenta-graphene contains both \"sp\" and \"sp\" hybridized carbon atoms. Contrary to graphene, which is a good conductor of electricity, penta-graphene is an insulator with an indirect band gap of 4.1–4.3 eV. Its hydrogenated form is called penta-graphane. It has a diamond-like structure with \"sp\" and no \"sp\" bonds, and therefore a wider band gap (ca. 5.8 eV) than penta-graphene. Chiral penta-graphene nanotubes have also been studied as metastable allotropes of carbon.\n"}
{"id": "1758788", "url": "https://en.wikipedia.org/wiki?curid=1758788", "title": "Phosphorus pentachloride", "text": "Phosphorus pentachloride\n\nPhosphorus pentachloride is the chemical compound with the formula PCl. It is one of the most important phosphorus chlorides, others being PCl and POCl. PCl finds use as a chlorinating reagent. It is a colourless, water-sensitive and moisture-sensitive solid, although commercial samples can be yellowish and contaminated with hydrogen chloride.\n\nThe structures for the phosphorus chlorides are invariably consistent with VSEPR theory. The structure of PCl depends on its environment. Gaseous and molten PCl is a neutral molecule with trigonal bipyramidal geometry and (\"D\") symmetry. The hypervalent nature of this species (as well as for , see below) can be explained with the inclusion of non-bonding molecular orbitals (molecular orbital theory) or resonance (valence bond theory). This trigonal bipyramidal structure persists in nonpolar solvents, such as CS and CCl. In the solid state PCl is an ionic compound, formulated .\n\nIn solutions of polar solvents, PCl undergoes self-ionization. Dilute solutions dissociate according to the following equilibrium:\n\nAt higher concentrations, a second equilibrium becomes more prevalent:\n\nThe cation and the anion are tetrahedral and octahedral, respectively. At one time, PCl in solution was thought to form a dimeric structure, PCl, but this suggestion is not supported by Raman spectroscopic measurements.\n\nAsCl and SbCl also adopt trigonal bipyramidal structures. The relevant bond distances are 211 pm (As−Cl), 221 pm (As−Cl), 227 pm (Sb−Cl), and 233.3 pm (Sb−Cl). At low temperatures, SbCl converts to the dimer, dioctahedral SbCl, structurally related to niobium pentachloride.\n\nPCl is prepared by the chlorination of PCl. This reaction is used to produce around 10,000 tonnes of PCl per year (as of 2000).\n\nPCl exists in equilibrium with PCl and chlorine, and at 180 °C the degree of dissociation is about 40%. Because of this equilibrium, samples of PCl often contain chlorine, which imparts a greenish coloration.\n\nIn its most characteristic reaction, PCl reacts upon contact with water to release hydrogen chloride and give phosphorus oxides. The first hydrolysis product is phosphorus oxychloride:\n\nIn hot water, hydrolysis proceeds completely to orthophosphoric acid:\n\nIn synthetic chemistry, two classes of chlorination are usually of interest: oxidative chlorinations and substitutive chlorinations. Oxidative chlorinations entail the transfer of Cl from the reagent to the substrate. Substitutive chlorinations entail replacement of O or OH groups with chloride. PCl can be used for both processes.\n\nUpon treatment with PCl, carboxylic acids convert to the corresponding acyl chloride. The following mechanism has been proposed:\n\nIt also converts alcohols to alkyl chlorides. Thionyl chloride is more commonly used in the laboratory because the resultant sulfur dioxide is more easily separated from the organic products than is POCl.\n\nPCl reacts with a tertiary amides, such as dimethylformamide (DMF), to give dimethylchloromethyleneammonium chloride, which is called the Vilsmeier reagent, [(CH)N=CClH]Cl. More typically, a related salt is generated from the reaction of DMF and POCl. Such reagents are useful in the preparation of derivatives of benzaldehyde by formylation and for the conversion of C−OH groups into C−Cl groups.\n\nIt is especially renowned for the conversion of C=O groups to CCl groups. For example, benzophenone and phosphorus pentachloride react to give the diphenyldichloromethane: \n\nThe electrophilic character of PCl is highlighted by its reaction with styrene to give, after hydrolysis, phosphonic acid derivatives.\n\nBoth PCl and PCl convert RCOH groups to the chloride RCCl. The pentachloride is however a source of chlorine in many reactions. It chlorinates allylic and benzylic CH bonds. PCl5 bears a greater resemblance to SOCl, also a source of Cl. For oxidative chlorinations on the laboratory scale, sulfuryl chloride is often preferred over PCl since the gaseous SO by-product is readily separated.\n\nAs for the reactions with organic compounds, the use of PCl has been superseded by SOCl. The reaction of phosphorus pentoxide and PCl produces POCl:\n\nPCl chlorinates nitrogen dioxide to form unstable nitryl chloride:\n\nPCl is a precursor for lithium hexafluorophosphate, LiPF, an electrolyte in lithium ion batteries. is produced by the reaction of with lithium fluoride, with lithium chloride as a side product:\n\nPCl is a dangerous substance as it reacts violently with water.\n\nPhosphorus pentachloride was first prepared in 1808 by the English chemist Humphry Davy. Davy's analysis of phosphorus pentachloride was inaccurate; the first accurate analysis was provided in 1816 by the French chemist Pierre Louis Dulong.\n\n\n"}
{"id": "52242804", "url": "https://en.wikipedia.org/wiki?curid=52242804", "title": "Planetary transits and occultations", "text": "Planetary transits and occultations\n\nIn astronomy, planetary transits and occultations occur when a planet passes in front of another object, as seen by an observer. The occulted object may be a distant star, but in rare cases it may be another planet, in which case the event is called a \"mutual planetary occultation\" or \"mutual planetary transit\", depending on the relative apparent diameters of the objects.\n\nMutual occultations or transits of planets are extremely rare. The most recent event occurred on 3 January 1818, and the next will occur on 22 November 2065. Both involve the same two planets: Venus and Jupiter.\n\nAn occultation of Mars by Venus on 13 October 1590 was observed by the German astronomer Michael Maestlin at Heidelberg. The 1737 event (see list below) was observed by John Bevis at Greenwich Observatory – it is the only detailed account of a mutual planetary occultation. A transit of Mars across Jupiter on 12 September 1170 was observed by the monk Gervase at Canterbury, and by Chinese astronomers.\n\nThe next time a mutual planetary transit or occultation will happen (as seen from Earth) will be on 22 November 2065 at about 12:43 UTC, when Venus near superior conjunction (with an angular diameter of 10.6\") will transit in front of Jupiter (with an angular diameter of 30.9\"); however, this will take place only 8° west of the Sun, and will therefore not be visible to the unaided/unprotected eye. Before transiting Jupiter, Venus will occult Jupiter's moon Ganymede at around 11:24 UTC as seen from some southernmost parts of Earth. Parallax will cause actual observed times to vary by a few minutes, depending on the precise location of the observer.\n\nThere are only 18 mutual planetary transits and occultations as seen from Earth between 1700 and 2200. There is a very long break of events between 1818 and 2065.\n\nTwice during the orbital cycles of Jupiter and Saturn, the equatorial (and satellite) planes of those planets are aligned with Earth's orbital plane, resulting in a series of mutual occultations and eclipses between the moons of these giant planets. The terms \"eclipse\", \"occultation\", and \"transit\" are also used to describe these events. A satellite of Jupiter (for example) may be eclipsed (i.e. made dimmer because it moves into Jupiter's shadow), occulted (i.e. hidden from view because Jupiter lies on our line of sight), or may transit (i.e. pass in front of) Jupiter's disk (see also Solar eclipses on Jupiter).\n\nThis table is another compilation of occultations and transits of bright stars and planets by solar planets. These events are not visible everywhere the occulting body and the occulted body are above the skyline. Some events are barely visible, because they take place in close proximity to the Sun.\n"}
{"id": "23988563", "url": "https://en.wikipedia.org/wiki?curid=23988563", "title": "Postage stamp paper", "text": "Postage stamp paper\n\nPostage stamp paper is the foundation or substrate of the postage stamp to which the ink for the stamp's design is applied to one side and the adhesive is applied to the other. The paper is not only the foundation of the stamp but it has also been incorporated into the stamp's design, has provided security against fraud and has aided in the automation of the postal delivery system.\n\nStamp catalogs like Scott's Standard Postage Stamp Catalog (SC) often document the paper the stamp is printed on to describe a stamp's classification. The same stamp design can appear on several kinds of paper. Stamp collectors and philatelists understand that a stamp's paper not only defines a unique stamp but could also mean the difference between an inexpensive stamp from one that is rare and worth more than its common counterpart.\n\nMaking an accurate determination of the stamp's paper may require special tools such as a micrometer to measure the thickness of a stamp, certain fluid chemicals to reveal hidden features, magnifying glasses or loupes to see fine details, digital microscopes to examine the minutest details of the paper or ultraviolet light to illuminate the paper to reveal its \"glowing\" aspects. Certain paper types may require the services of an expert as the only sure way of knowing the true identity of the stamp's paper.\n\nAll paper is endowed with certain characteristics by its maker. Depending on the purpose of the paper, the craftsman will choose specific materials and apply certain manufacturing processes to achieve the design objectives. Characteristics such as composition, weight, color, size, watermark, surface finish, opacity, hardness and strength all have to be established before the papermaker can begin his work.\n\nThe making of paper can be broken down into three phases; the preparing of the pulp into a suspension of fibers; the forming of the paper on a mould or an endless wire mesh; and lastly the finishing of the paper's surface and drying. From a philatelic interest, it is the second phase, the forming of the paper that yields the most interesting characteristics.\nIn the first phase of papermaking the characteristics such as its composition, color and weight is determined. Paper has as its chief component, a mat of cellulose fibers. Cellulose is the skeleton structure of plant cells and can be separated from the plant for use in paper. Cellulose has several characteristics that make it desirable for paper, the foremost being its strength when formed into a mat or web. When cellulose fibers come in contact with each other in water, a bond is formed. When water is removed from the adjoining fibers, the bond between the fibers strengthens. Pulp, the collection of individual fibers, may be bleached, especially if the paper is to be dyed a different color or the paper is expected to be white. Since most paper is either printed or written upon, fillers are added to the pulp to fill the pores of the paper and sizing is added to make the fibers water resistant, yet both act as fillers. Unsized paper is blotting paper, making it unsuitable for printing. Fillers and sizing are added to the pulp to absorb the ink quickly, unlike pure cellulose. Fillers can be glues made from animal products, starches from rice or wheat, resins or gums, or minerals such as calcium carbonate, titanium dioxide or kaolin. Mineral fillers are the most common as they are very effective as a filler. When all of these ingredients are assembled, they are suspended in water, which may include a color dye, as the \"furnish\" to the second phase of papermaking.\n\nThe paper is formed in the second stage of papermaking. With handmade paper, the \"furnish\" is stored in a vat and the craftsman uses a mould to strain out enough material to form a sheet of paper. The mould determines the dimensions of the finished sheet and its weight, which ultimately establishes the paper’s thickness. The mould is usually a wire mesh that acts as a strainer such that the \"furnish\" is separated out of the water. The water drains off, leaving layers upon layers of fibers or a web of paper on the mould. The texture of the paper is determined by the nature of the mould. Wove paper has a uniform texture while laid paper has a fine-lined texture created by wires that are attached to the wire mesh. If a watermark is part of the paper’s design, it is the mould that creates the watermark, in the same way that the fine lines of laid paper are created. A watermark is a deliberate thinning of the paper by the placing of either wires or metal shapes, called \"bits\", onto the wire mesh of the mould. When the mould is removed from the vat, the water drains causing the pulp to be deposited more between the wires or \"bits\" relative to the top of the \"bits\" or wires. When the dried paper is held up to a light, the thinner paper will appear lighter in contrast to the thicker paper, thus creating a watermark. Numerous countries have used a variety of designs for their watermarks as a means to prevent forgery of stamps, making the watermark of particular philatelic interest.\n\nIn comparison, machine-made paper is made on the Fourdrinier machine by drawing the \"furnish\" out of a vat onto an endless wire mesh. The paper, shortly after being drawn from the vat, is usually pressed with a Dandy roll as the mechanism to imprint a watermark onto the paper. Machine-made paper can produce single sheets of paper or one large continuous web of paper that is collected to form large rolls.\n\nOne characteristic of machine-made paper is that it creates a direction or an alignment of the fibers, which directly impacts its strength. This is of particular importance when tearing the paper, as one would do to separate a stamp for use. When the tear is aligned with the direction of the fibers, the paper will tear evenly. When the tear is opposed to the direction of the fibers, the paper will tear unevenly, in a jagged line. Handmade paper disperses the fibers in unpredictable directions and therefore yields a paper with the most overall strength. A paper’s strength had an influence on the separation methods used for a stamp. For example, a stronger paper may have needed a higher number of perforations per inch to best facilitate the separation of the stamps. Similarly, many stamps have two different standards of perforation for its length and width to optimize the ease of separation while minimizing the cost of manufacturing.\n\nIn the last stage of papermaking, the paper is finished and dried. The finishing of the paper can include the application of a coating that will produce the best effects when printed upon. The coating is a fine layer of special sizing applied to one or both sides of the paper to fill in all of the pores and to smooth out the surface of the paper. A glossy appearance often is a characteristic of coated paper. Once the coating is applied, the paper making process is complete.\n\nCertain postage stamps have been printed on security paper, which is paper that has additional characteristics coated or printed onto the paper to prevent the reuse of the postage stamp and as a means to prevent forgery.\n\nShrinkage is a characteristic of paper because of the nature of cellulose fibers. The cellulose fiber is hygroscopic and acts like a sponge when immersed in water. The fibers expand in their width and not in their length. With handmade paper, because there is no direction associated with the fibers, the paper expands and shrinks unevenly in both the length and width of the finished sheet. With machine-made paper, because there is a direction to the fibers, the paper shrinks unevenly, that is, less in its length (in the direction of the fibers) and more in its width (in the direction opposite of the fibers). This characteristic is important to the printer because certain printing techniques required the paper to be dampened prior to printing. As such, when the paper dried, the uneven shrinkage of machine-made paper would produce an image of different proportions than the die that created it. The differences in appearance between wet and dry printed stamps can sometimes be quite noticeable, with dry printed stamps generally having sharper images on stiffer and thicker paper.\n\nWatermarks are created in the paper by special pieces of bent wire or \"bits\", either attached to the mould or attached to the Dandy roll of machine-made paper. The watermark is inherently created differently by these two methods. In the case of the mould, the watermark is created by the settling of the fibers on the mould, thus creating the intentional thinning of the paper. In the case of the Dandy Roll, the watermark is pressed into the wet pulp. The papermaker closely adjusts the pressure the Dandy Roll exerts on the paper to ensure a proper impression of the watermark design into the paper. Too little pressure and the watermark may not be detectable.\n\nPhilatelically, there are several descriptive terms used to categorize watermarks.\n\nWatermark detection can be as simple as holding the stamp up to a light or by placing it face down on a black surface. If this does not reveal the watermark there are fluids, electrical devices and ink pads that may reveal the thinning of the paper;\n\nWhile the words \"flaws\" and \"errors\" as synonymous, they are used to distinctly describe either a defect in the stamp's paper or when an incorrect paper was used in the printing of the stamp. Flaws describe faults or defects in the paper of the stamp, typical of handling after manufacturing or less frequently during the manufacturing of the paper. Errors describe an incorrect type of paper used to print a stamp, typically the use of watermarked paper when it was not specified for the issue. While a paper fault represents a damaged or defective stamp that devalues its worth, paper errors have the opposite effect and are sought after by collectors.\n\nDuring the manufacturing of paper, flaws can occur in the web. One such flaw is the pinhole. The term is used to designate a small blemish typically characterized by a small hole in the substance of the paper. Pinholes are typical of very thin paper and can be found by holding the stamp up to light.\n\nA rare flaw in paper is when the watermark bit has been improperly repaired on the Dandy roll. Great Britain's \"Emblems\" watermark is composed of two roses, one shamrock and one thistle. A defect was created when the Dandy roll was repaired and instead of a thistle bit, a rose bit was added creating the \"three roses and a shamrock\" flaw. This flaw is found on Great Britain's 1862 3d (plate 2), 1865-67 3 d (plate 4), 6 d (plates 5 and 6), 9d (plate 4) and 1s (plate 4).\n\nAnother paper flaw is a crease. A crease is when the paper becomes an overlapped fold, which subsequently is printed upon. This kind of crease is more of a printing error as it is a paper flaw. Creased stamps can also occur as a result of handling, where it's clear that the stamp has been folded. While the former example is collected by specialists as printing errors, the later simply devalues the stamp.\n\nAnother common handling flaw is a tear. The torn stamp is usually complete but the paper is partially ripped. Tearing a piece off of the stamp, however, is how the Afghan postal clerk cancelled a stamp and so this is not a flaw but evidence that the stamp was probably postally used, especially if the stamp is still on the postal matter.\n\nA stamp can be damaged if it is soaked off of a water-soluble colored paper. For numerous occasions, people send greeting cards in envelopes that are on colored paper. The worst offender is the red envelope. If warm water is used in soaking the stamp from the paper of the envelope, the red dye can and does bleed into the stamp's paper, leaving it tinted red. This is not a stamp variety but simply a damaged stamp. Yellow and blue dyes in colored paper bleed into a stamps too.\n\nA thin is created when a stamp is improperly removed from the paper it was attached. Since paper is created by depositing layers of fibers onto each other to form a web, they can be separated by the layers too. When the paper is finished, the outside layers become the strongest layers. Soaking a stamp in water is the usual way of removing it from the postal matter. Water will dissolve the glue used as the adhesive but it also weakens the bonds of the paper's fibers. Just as the removal of water strengthens the bonding between fibers, adding water weakens them. A failed attempt at removing the stamp from other paper typically results in portion of the stamp's paper being left attached to the postal matter. \"Thins\" can be created in a variety of ways and all result in a damaged stamp. \n\nErrors of paper are created when the incorrect paper is used for the stamp's design rather than a papermaking defect. One example of this is when the watermarked paper intended for use with U.S. revenue stamps was used for the $1 Woodrow Wilson stamp (SC#832). The mistake was made sometime between 1950 and 1951 and some 160,000 to 400,000 copies are estimated to have been printed.\n\nWhen the paper of the stamp is described, stamp catalogs often use words that are relative, such as \"thick\" and \"thin\". This is done to describe the variations of the stamp's paper in a particular issue. Thick may be as much as 0.005 inches and thin as little as 0.001 inches, with medium somewhere in between.\n\nPaper can described as being \"opaque\", \"semi-translucent\" and \"semi-transparent\". The opacity of the stamp describes the ability of light to shine through the paper. If no light shines through the paper, then it is opaque. If some light passes through, in any amount, the paper is semi-translucent. Transparency describes the ability to see an object through the paper or when the paper is placed over printed letters the ability to see the printing through the paper. If the stamp's design can be seen through the back of the stamp, then it is semi-transparent.\n\nAnother comparative set of terms refers to the paper's hardness. \"Hard\", \"stout hard\", and \"soft\" have been used to describe the paper. Experts have described the \"snap\" of the stamp when \"flicked\" as a means to determine if the stamp was printed on hard or soft paper. A sharper snap implied hard paper because the hardness is a characteristic of the amount or kind of sizing used when making the paper.\n\n\"Porous\" paper is used to describe paper as absorbent, usually in contrast to less absorbent paper used in the stamps of the same country. Porosity is a characteristic of paper. Wood fibers are hydrophilic or water loving. Sizing is added to paper to create a resistance to water as well as to fill in the gaps between the fibers. Porosity is a measure of how the paper responds to a liquid.\n\nThere are several popular stamp collecting terms. \"On-paper\" refers to any stamp that is still adhering to another piece of paper. Similarly, \"off-paper\" is used to describe a postally used stamp that is no longer adhering to any other kind of paper. \"Wallpaper\" is the slang name given to the sheets of stamps that have little or no philatelic or monetary value.\n\nPhilatelically, stamp paper can be partitioned into a few large groups. The first that is typically encountered in the stamp catalogs describes the texture of the paper, such as wove or laid. Many stamps have been printed with these different paper textures. Other groups can be formed as: Colored paper, safety or security paper and coated paper.\n\nIn a stamp catalog, a stamp's paper is usually identified at the beginning of the issue. It will also be identified when the paper changes either within the issue or with the next issue. When colored paper is used for the stamp, the ink colors are listed first and then the paper's color is listed next in \"italics\". Sometimes the paper is described as \"ordinary\", which simply means that the common paper of the period was used. Coated paper is usually not listed but may be surmised by the printing process used to print the stamp. For example, photogravure printing yields the best clarity when printed on coated papers.\n\nExamples of different security paper used on postage stamps.\n\n\n\n"}
{"id": "1400408", "url": "https://en.wikipedia.org/wiki?curid=1400408", "title": "Recloser", "text": "Recloser\n\nIn electric power distribution, automatic circuit reclosers (ACRs) are a class of switchgear which is designed for use on overhead electricity distribution networks to detect and interrupt momentary faults. Also known as Reclosers or Autoreclosers, ACRs are essentially high voltage rated circuit breakers with integrated current and voltage sensors and a protection relay, optimized for use as an overhead network distribution protection asset. Commercial ACRs are governed by the ANSI/IEEE C37.60, IEC 62271-111 and IEC 62271-200 standards. The three major classes of operating voltage are 15.5 kV, 27 kV and 38 kV.\n\nFor overhead distribution networks, the majority of faults are transient, such as lightning strike, surges or foreign objects coming into contact with the exposed distribution lines. By this logic, 80% of outages can be resolved by a simple close operation. Reclosers are designed to handle a short close-open duty cycle, where electrical engineers can optionally configure the number of attempted close operations prior to transitioning to a lockout stage.\n\nReclosers were invented in the mid 1900s in the USA. Some of the earliest reclosers were introduced by Kyle Corporation (which was acquired by Cooper Power Systems - part of the Eaton family) in the early 1940s. The brand was the industry leader in reclosers, sectionalizers and switchgear until the 2000s when many other manufacturers entered the market. Reclosers were originally oil filled hydraulic devices with rudimentary mechanical protection relaying capabilities. Modern Automatic Circuit Reclosers are significantly more advanced than the original hydraulic units. The advent of semiconductor based electronic protective relays in the 1980s resulted in increased sophistication, allowing for differing responses to the various cases of abnormal operation or fault on a distribution networks. The high voltage insulation and interrupting device in modern reclosers typically consist of Solid Dielectric Insulation with vacuum interrupters for current interruption and arc quenching.\n\nReclosers are often used as a key component in a smart grid, as they are effectively computer controlled switchgear which can be remotely operated and interrogated using SCADA or other communications. This capability allows utilities to aggregate data about their network performance, and develop automation schemes for power restoration. This automation can either be distributed (executed at the remote recloser level) or centralized (close and open commands issued by a central control room to be executed by remotely controlled ACRs).\n\nIn order to prevent damage, each station along the network is protected with circuit breakers or fuses which will turn off power in the event of a short circuit. This presents a major problem when dealing with transient events. For instance, a tree limb blown off a tree during a windstorm that lands on the power line may cause a short circuit that could cause damage. However, the fault could quickly clear itself as the limb falls to the ground. If the only protection system is provided by breakers at distribution substations, large areas of the distribution network could be blacked out while repair crews reset the breakers. Reclosers are programmed to automate the reset process and allow a more granular approach to service restoration. The result is increased availability of supply.\n\nReclosers address this problem by further dividing up the network into smaller sections. For instance, the city grid example above might be equipped with reclosers at every branch point on the network. Reclosers, because of their upstream position in the network, handle much less power than the breakers at the feeder stations, and therefore can be set to trip at much lower power levels. This means that a single event on the grid will cut off only the section handled by a single recloser, long before the feeder station would notice a problem.\n\nModern recloser installations are often equipped with SCADA communications, allowing the majority of reclosers to be remotely operated by staff in the utility control room. This allows for re-switching the network, as operators can use information provided by the reclosers in the field to reconfigure the distribution network if a fault is detected in the field, or to fix load flow issues. Remote control of reclosers also saves significant operational expenditure, as it can reduce the necessity of field crews to travel to site to reset devices which have transitioned to lockout.\n\nAutoreclosers are made in single-phase and three-phase versions, and use either oil, vacuum, or SF interrupters. Controls for the reclosers range from the original electromechanical systems to digital electronics with metering and SCADA functions. The ratings of reclosers run from 2.4–38 kV for load currents from 10–1200 A and fault currents from 1–16 kA.\n\nOn a 3-phase circuit, a recloser is more beneficial than three separate fuse cutouts. For example, on a wye to delta conversion if cutouts are used on the wye side, and only 1 out of 3 of the cutout fuses open, some customers on the delta side would have a low voltage condition, due to voltage transfer through the transformer windings. Low voltage can cause severe damage to electronic equipment. But if a recloser were used, all three phases will open, eliminating the problem.\n\nWhilst original hydraulic recloser designs had rudimentary protection capabilities, modern semiconductor controlled devices exhibit sophisticated control systems which allow for the configuration of varying responses to different classes of faults on the distribution network. The number of reclose attempts is limited to a maximum of four by recloser Standards. The basic philosophy of reclosing is to actively consider the fault cases and provide an effective response based on the fault type, this is done on a probabilistic methodology in conjunction with the detection of fault type.\n\nThe most common fault type on an overhead distribution network is lightning strike. Lightning surges cause an increase in voltage which can cause localised breakdown of insulation, allow arcing over insulators. Reclosers can detect this as an overcurrent or Earth Fault (depending on the asymmetry of the fault). Lightning surges pass very quickly (reduce in 50ms), so the first reclose operation of a recloser can be configured to both trip and reclose quickly. This first reclose allows for interruption of the arcing caused by lightning, but restores the power quickly.\n\nIf the recloser closes onto a fault, it is likely that the fault is a secondary class of fault, vegetation contact or equipment failure. An overcurrent fault would indicate a line to line class fault, which can be confirmed by negative phase sequence overcurrent protection, whereas an earth fault can indicate a Line to Ground or Double Line to Ground fault. Reclosers can then apply a fuse burning policy, where they remain closed for a short period to allow fuses on lateral lines to burn, isolating the fault. If the fault is not cleared, the recloser trips open again. This same policy can be used to deliver energy to fault sites to burn the fault off the line. This could be a branch crossing between multiple lines, or fauna (birds, snakes, etc.) coming into contact with the conductors.\n\nSensitive Earth Fault protection in reclosers is typically set to immediate lockout. This detection of small leakage currents (less than 1 ampere) on a medium voltage line can indicate insulator failure, broken cables or lines coming into contact with trees. There is no merit in applying reclosing to this scenario, and the industry best practice is not to reclose on Sensitive Earth Fault. Reclosers with sensitive earth fault protection capable of detecting 500mA and below are used as a fire mitigation technique, as they provide an 80% risk reduction in fire starts, however they are never to be used as reclosers in this application, only as single shot distributed circuit breakers which allow for sensitivity to verify the existence of these faults.\n\nTraditional reclosers were designed simply to automate the action of a line crew visiting a remote distribution site to close a tripped circuit breaker and attempt to restore power. With the advanced protection functionality of modern reclosers, these devices are used in a multitude of additional applications\nResidential customers in areas fed by affected overhead power lines can occasionally see the effects of an autorecloser in action. If the fault affects the customer's own distribution circuit, they may see one or several brief, complete outages followed by either normal operation (as the autorecloser succeeds in restoring power after a transient fault has cleared) or a complete outage of service (as the autorecloser exhausts its retries). If the fault is on an adjacent circuit, the customer may see several brief \"dips\" (sags) in voltage as the heavy fault current flows into the adjacent circuit and is interrupted one or more times. A typical manifestation would be the dip, or intermittent black-out, of domestic lighting during an electrical storm. Autorecloser action may result in electronic devices losing time settings, losing data in volatile memory, halting, restarting, or suffering damage due to power interruption. Owners of such equipment may need to protect electronic devices against the consequences of power interruptions and also power surges.\n\nReclosers may cooperate with down-stream protective devices called sectionalizers, usually a disconnector or cutouts equipped with a tripping mechanism triggered by a counter or a timer. A sectionalizer is generally not rated to interrupt fault current however it often has a larger Basic Insulation Level, allowing some sectionalizers to be used as a point of isolation. Each sectionalizer detects and counts fault current interruptions by the recloser (or circuit breaker). After a pre-determined number of interruptions, the sectionalizer will open, thereby isolating the faulty section of the circuit, allowing the recloser to restore supply to the other non-fault sections. Some modern recloser controllers can be configured to have reclosers operate in sectionalizer mode. This is used in applications where protection grading margins are too small to provide effective protection co-ordination between electrical assets.\n\nFire risk is an innate risk of an overhead distribution network. Regardless of the choice of distribution protection switchgear, the fire risk is always higher with overhead conductors than with underground reticulation.\n\nThe Victorian Royal Commission into the 2009 bushfires indicated that reclosing must be disabled on high bushfire risk days, however on low risk days it should be applied for reliability of supply.\n\nIncorrectly configured or old model reclosers have been implicated in the starting or spread of wildfires. Research into the Australian 2009 Black Saturday Bushfires indicated that reclosers operating as single shot circuit breakers with Sensitive Ground Fault protection configured at 500mA would reduce fire start risk by 80%. Any form of reclosing should be removed on high fire risk days, and reclosing in general should not be applied to detected Sensitive Earth Fault faults.\n\nVictorian utilities responded to the Royal Commission by converting some of their overhead network in high risk areas to underground cable, replacing exposed overhead conductors with insulated cables, and replacing old reclosers with modern ACRs with remote communications to ensure that settings can be adjusted on high bushfire risk days.\n\n"}
{"id": "29650829", "url": "https://en.wikipedia.org/wiki?curid=29650829", "title": "Samuel Nguiffo", "text": "Samuel Nguiffo\n\nSamuel Nguiffo is a Cameroonian lawyer. He is manager of the Center for Environment and Development in Yaoundé. He was awarded the Goldman Environmental Prize in 1999, for his efforts on protection of the tropical rainforests of Central Africa.\n"}
{"id": "5497749", "url": "https://en.wikipedia.org/wiki?curid=5497749", "title": "Shovel logging", "text": "Shovel logging\n\nShovel logging, sometimes called Hoe Chucking, uses a log loader to swing logs to the forest road. Shovel logging is one of a number of methods that may be used to move logs from forest to road. Rather than driving out to the log and dragging it back to the landing, the loader moves slowly across the harvest area, grabbing logs/trees within reach, and swinging them around to drop them closer to the road. Logs further from the road can be shoveled to the landing in a few passes back and forth. \nSkidding and cable logging can be more cost efficient for logs further from the road. Shovel logging can make use of the loader between log truck arrivals. It can also reduce soil disturbance, since it requires only a single pass to move all the logs in reach. \n\nThe machine shown here is typical of the 'shovels' used for shovel logging. It is a modified excavator or 'shovel'. The name dates back to a previous generation of equipment when loggers uses power shovels (often with a different boom) to load logs. \n\n"}
{"id": "35546201", "url": "https://en.wikipedia.org/wiki?curid=35546201", "title": "Solarpark Finow Tower", "text": "Solarpark Finow Tower\n\nSolarpark Finow Tower is located in Finowfurth, Northeast of Berlin, Germany and is equipped with Suntech modules.\n\nThe first phase of the project, FinowTower I (24.3 MW) was commissioned in 2010, and the second, FinowTower II (60.4 MW) in 2011.\n\n"}
{"id": "42504185", "url": "https://en.wikipedia.org/wiki?curid=42504185", "title": "Standing Council on Energy and Resources", "text": "Standing Council on Energy and Resources\n\nThe Standing Council on Energy and Resources (SCER) is a council established by the Council of Australian Governments (COAG) in June 2011 to pursue priority issues of national significance in the energy and resources sectors, and progress the key reform elements facing the mineral, petroleum and energy sectors.\n\nThe council's forerunners were the Ministerial Council on Mineral and Petroleum Resources and the Ministerial Council on Energy.\n\nThe members of the council are the Commonwealth, State, Territory and New Zealand Ministers with responsibility for energy and resource matters.\n\n\n"}
{"id": "1675577", "url": "https://en.wikipedia.org/wiki?curid=1675577", "title": "Sulky", "text": "Sulky\n\nA sulky is a lightweight cart having two wheels and a seat for the driver only but usually without a body, generally pulled by horses or dogs, and is used for harness races. The term is also used for a light stroller, an arch mounted on wheels or crawler tracks, used in logging, or other types of vehicle having wheels and usually a seat for the driver, such as a plough, lister or cultivator.\n\nA sulky for horses is a lightweight two-wheeled, single-seat cart that is used as a form of rural transport in many parts of the world. A special development of this is now used in most forms of harness racing in Argentina, Australia, Canada, the United States and New Zealand, including both trotting and pacing races. They are called \"sulkies\" because the driver must prefer to be alone \n\nRace sulkies come in two categories, \n\nAn \"improved sulky\" with pneumatic tires and adjustable height was patented at the United States Patent Office by W.J. Hamill on August 15, 1893. (see Google patents)\nThe asymmetric sulky was patented in Australia in the 1980s and came to prominence in 1987 when a two-year-old gelding named Rowleyalla used one to break the then world record for his category, at 3.4 seconds under the existing mark.\n\nIn 1990 the asymmetric sulky was introduced into North America, winning seven of its first nine starts at Freehold, NJ. Today the great majority of sulky manufacturers in North America are producing asymmetric sulkies.\n\nAn additional sulky type is the \"team-to-pole\" or \"pairs\" sulky, a lightweight single seat sulky designed for draft by two horses abreast.\n\nThese may also be split into two types:\n\nOf the two, the dorsal hitch pairs sulky is the most recent, holding all current world pairs speed records over the mile to July 31, 2005.\n\nThere are two types of sulkies used in harness racing.\n\nWhen it rains, or the track has excessive moisture, trainers and drivers are required to put plastic mud flaps on the back of the wheels. All race bikes must comply with the relevant procedures and standards in order to be approved. In no way can a race bike have any component that will directly interfere with another horse or driver.\n\nSmaller sulkies are also used for dogs, both for racing and as transportation.\n\nThe dog driving sulkies can be divided into two main types: \n\nA further distinction may be made between sulkies with the axles rigidly connected to the vehicle, and those with the axles insulated from the vehicle by springs and dampers. Those with springs and dampers may be further divided into single-axle sprung carts and \"independent suspension\" sprung and damped carts. Needless to say, those with independent suspension by coil springs and dampers tend to be both more expensive and smoother riding.\n\nDriving sulky construction materials run the full gamut from timber, through powder-coated steel tube, aluminum tube, and stainless steel tube. The very latest types (currently undergoing field tests in California) use nanotechnology-based stainless steels of prodigious strength-to-weight ratio.\n\nThe great majority of driving sulkies available have the wheel axles rigidly affixed to the frames. This makes for a rough ride on anything but smooth surfaces such as pavement. But in recent years lightweight, single shaft, independent suspension, driving sulkies have been introduced. These allow safe high speed use in off-road conditions.\n\nThe most recent designs are of the single shaft type, as proponents believe that this type gives the dog(s) greater freedom, less possibility of injury, and a quicker and easier training regime. A single shaft dog sulky, made of stainless steel tube and fitted with independent suspension and disc brakes, weighs a little under 18 kg. The highest speed so far recorded in one of these sulkies being 64.8 kilometers per hour (February 22, 2007, Australia).\n\nHowever, as at August 2005, multiple shaft types are still the most common.\n\nFor off-road use, dog sulkies with sprung and damped independent suspension systems offer greatly improved comfort and safety over traditional unsprung types.\n\n"}
{"id": "889856", "url": "https://en.wikipedia.org/wiki?curid=889856", "title": "Superheated steam", "text": "Superheated steam\n\nSuperheated steam is a steam at a temperature higher than its vaporization (boiling) point at the absolute pressure where the temperature is measured.\n\nThe steam can therefore cool (lose internal energy) by some amount, resulting in a lowering of its temperature without changing state (i.e., condensing) from a gas, to a mixture of saturated vapor and liquid. If unsaturated steam (a mixture which contain both water vapor and liquid water droplets) is heated at constant pressure, its temperature will also remain constant as the vapor quality (think dryness, or percent saturated vapor) increases towards 100%, and becomes dry (i.e., no saturated liquid) saturated steam. Continued heat input will then \"super\" heat the dry saturated steam. This will occur if saturated steam contacts a surface with a higher temperature.\n\nSuperheated steam and liquid water cannot coexist under thermodynamic equilibrium, as any additional heat simply evaporates more water and the steam will become saturated steam. However this restriction may be violated temporarily in dynamic (non-equilibrium) situations. To produce superheated steam in a power plant or for processes (such as drying paper) the saturated steam drawn from a boiler is passed through a separate heating device (a superheater) which transfers additional heat to the steam by contact or by radiation.\n\nSuperheated steam is not suitable for sterilization. This is because the superheated steam is dry. Dry steam must reach much higher temperatures and the materials exposed for a longer time period to have the same effectiveness; or equal F0 kill value. Superheated steam is also not useful for heating. Saturated steam has a much higher wall heat transfer coefficient.\n\nSlightly superheated steam may be used for antimicrobial disinfection of biofilms on hard surfaces.\n\nSuperheated steam’s greatest value lies in its tremendous internal energy that can be used for kinetic reaction through mechanical expansion against turbine blades and reciprocating pistons, that produces rotary motion of a shaft. The value of superheated steam in these applications is its ability to release tremendous quantities of internal energy yet remain above the condensation temperature of water vapor; at the pressures at which reaction turbines and reciprocating piston engines operate.\n\nOf prime importance in these applications is the fact that water vapor containing entrained liquid droplets is generally incompressible at those pressures. If steam doing work in a reciprocating engine or turbine, cools to a temperature at which liquid droplets form; the water droplets entrained in the fluid flow will strike the mechanical parts of engines or turbines, with enough force to bend, crack or fracture them. Superheating and pressure reduction through expansion ensures that the steam flow remains as a compressible gas throughout its passage through a turbine or an engine, preventing damage of the internal moving parts.\n\nSaturated steam is steam that is in equilibrium with heated water at the same pressure, i.e., it has not been heated more than the boiling point for that pressure. This is in contrast to superheated steam, in which the steam (vapor) has been separated from the water droplets then additional heat has been added.\n\nThese condensation droplets are a cause of damage to steam turbine blades, the reason why such turbines rely on a supply of dry, superheated steam.\n\nDry steam is saturated steam that has been very slightly superheated. This is not sufficient to change its energy appreciably, but is a sufficient rise in temperature to avoid condensation problems, given the average loss in temperature across the steam supply circuit. Towards the end of the 19th century, when superheating was still a less-than-certain technology, such steam-drying gave the condensation-avoiding benefits of superheating without requiring the sophisticated boiler or lubrication techniques of full superheating.\n\nBy contrast, water vapor that includes water droplets is described as wet steam. If wet steam is heated further, the droplets evaporate, and at a high enough temperature (which depends on the pressure) all of the water evaporates, the system is in vapor–liquid equilibrium, and it becomes \"saturated steam\".\n\nSaturated steam is advantageous in heat transfer due to the high latent heat of vaporization. It is a very efficient mode of heat transfer. In layman's terms, saturated steam is at its dew point at the corresponding temperature and pressure. The typical latent heat of vaporization (or condensation) is 970 Btu/lb for saturated steam at atmospheric pressure.\n\nSuperheated steam was widely used in main line steam locomotives. Saturated steam has three main disadvantages in a steam engine: it contains small droplets of water which have to be periodically drained from the cylinders; being precisely at the boiling point of water for the boiler pressure in use, it inevitably condenses to some extent in the steam pipes and cylinders outside the boiler, causing a disproportionate loss of steam volume as it does so; and it places a heavy demand on the boiler.\n\nSuperheating the steam dries it effectively, raises its temperature to a point where condensation is much less likely and increases its volume significantly. Added together, these factors increase the power and economy of the locomotive. The main disadvantages are the added complexity and cost of the superheater tubing and the adverse effect that the \"dry\" steam has on lubrication of moving components such as the steam valves. Shunting locomotives did not generally use superheating.\n\nThe normal arrangement involved taking steam after the regulator valve and passing it through long superheater tubes inside specially large firetubes of the boiler. The superheater tubes had a reverse (\"torpedo\") bend at the firebox end so that the steam had to pass the length of the boiler at least twice, picking up heat as it did so.\n\nOther potential uses of superheated steam include: drying, cleaning, layering, reaction engineering, epoxy drying and film use where saturated to highly superheated steam is required at one atmospheric pressure or at high pressure. Ideal for steam drying, steam oxidation and chemical processing. Uses are in surface technologies, cleaning technologies, steam drying, catalysis, chemical reaction processing, surface drying technologies, curing technologies, energy systems and nanotechnologies.\nSuperheated steam is not usually used in a heat exchanger due to low heat transfer co-efficient. In refining and hydrocarbon industries superheated steam is mainly used for stripping and cleaning purposes.\n\nSuperheated steam is used for soil steaming. Steam is induced into the soil which causes almost all organic material to deteriorate. Soil steaming is an effective alternative to chemicals in agriculture.\n\n"}
{"id": "44321440", "url": "https://en.wikipedia.org/wiki?curid=44321440", "title": "Supersonic flow over a flat plate", "text": "Supersonic flow over a flat plate\n\nSupersonic flow over a flat plate is a classical fluid dynamics problem. There is no exact solution to it.\n\nWhen a fluid flow at the speed of sound over a thin sharp flat plate over the leading edge at low incident angle at low Reynolds Number. Then a laminar boundary layer will be developed at the leading edge of the plate. And as there are viscous boundary layer, the plate will have a fictitious boundary layer so that a curved induced shock wave will be generated at the leading edge of the plate.\nThe shock layer is the region between the plate surface and the boundary layer. This shock layer be further subdivided into layer of viscid and inviscid flow, according to the values of Mach number, Reynolds Number and Surface Temperature. However, if the entire layer is viscous, it is called as merged shock layer.\n\nThis Fluid dynamics problem can be solved by different Numerical Methods. However, to solve it with Numerical Methods several assumptions have to be considered. And as a result shock layer properties and shock location is determined. Results vary with one or more than one of viscosity of the fluid, Mach number and angle of incidence changes. Generally for large angles of incidences, the variation of Reynold’s Number has significant effects on the change of the flow variables, whereas the viscous effects are dominant on the upper surface of the plate as well as behind the trailing edge of the plate.\n\nDifferent experimenters get different result as per the assumptions they have made to solve the problem.\n\nThe primary method which is generally used to this problem:\n\nThis method involves using time-dependent Navier-Stokes equation which is advantageous because of its inherent ability to evolve to the correct steady state solution.\nThe continuity, momentum and energy equations and some other situational equations are needed to solve the problem. MacCormack’s time marching technique is applied and then using Taylor series expansion the flow field variables are advanced at each grid point. Then, initial boundary conditions are applied and solving equations will converge to approximated results.\nThese equations can be solved by using different algorithms to get better and efficient results with minimum errors.\n\n1. ^On boundary-layer flow past two-dimensional obstacles By F. T. SMITH, Department of Mathematics, Imperial College, London SW7 2BZ P. W. M. BRIGHTON,? P. S. JACKSONS AND J. c . R. HUNT\nhttp://www.cpom.org/people/jcrh/jfm-113\n\n2. ^A numerical study of the viscous supersonic flow past a flat plate at large angles of incidence By D. Drikakis and F. Durst Lehrstuhlfiir Stromungsmechanik, Universitat Erlangen-Niirnberg, Cauerstrasse. 4, D-91058 Erlangen, Germany\nhttps://www.deepdive.com/search?author=Durst%2C+F.&numPerPage=25\n\n3. ^Receptivity of a supersonic boundary layer over a flat plate. Part 1. Wave structures and interactions By YANBAO MA AND XIAOLIN ZHONG Mechanical and Aerospace Engineering Department, University of California, Los Angeles, CA 90095 USA\nhttp://www.journals.cambridge.org/article_S0022112003004786\n\n5. Computational Fluid Dynamics The Basics with Applications By John D. Anderson, Jr.\n"}
{"id": "30100679", "url": "https://en.wikipedia.org/wiki?curid=30100679", "title": "Swiss International Airlines Flight 850", "text": "Swiss International Airlines Flight 850\n\nSwiss International Air Lines Flight 850 (LX 850) was an international scheduled passenger flight from Basle, Switzerland, to Hamburg, Germany. On 10 July 2002, the flight was unable to land at Fuhlsbüttel Airport due to weather. Attempts were made to divert to other airports at Berlin and Eberswalde before the crew decided to land at Werneuchen. On landing, the aircraft struck an earth bank which ripped off all three undercarriage legs, and came to rest on its belly with an engine on fire. One of the sixteen passengers suffered minor injuries. The aircraft was written off.\n\nThe investigation into the accident by the German Federal Bureau of Aircraft Accident Investigation (BFU) took over eight years to complete. It raised a number of issues, including poor crew resource management, insufficient weather information being passed to the crew of Flight 850 and faulty runway markings at Werneuchen Airfield, where the runway had been reduced in length from to , but the runway markings had not been altered to reflect this.\n\nThe aircraft involved was a Saab 2000, registered HB-IZY, and named \"Doldenhorn\", after a mountain in Switzerland. The aircraft was msn 047 and had first flown on 30 April 1997. At the time of the accident, it had completed 12,303 hours of flight and made 12,069 landings.\n\n\"All times are UTC (Zulu Time), local time was two hours ahead of UTC.\"\n\nFlight 850 was originally scheduled to be operated by an Embraer 145 aircraft. Due to the non-availability of the Embraer 145, a Saab 2000 was substituted, and the briefing for the flight was extended by 15 minutes. Actual departure was at 14:55 UTC, 10 minutes behind schedule. Weather reports indicated a line of thunderstorms, winds up to could be expected at Fuhlsbüttel and the designated alternatives of Hannover and Bremen. A number of SIGMETs were issued about an hour before the flight departed Basle, but the flight crew did not receive these. The SIGMETs indicated a front was developing with thunderstorms reaching FL380 in the Bremen area. It carried four crew and 16 passengers. The Terminal Aerodrome Forecast for Fuhlsbüttel Airport, Hamburg valid from 13:00 to 22:00 was: TAF EDDH 101200Z 101322 31010KT 9999 FEW025 TEMPO 1320 29020G40KT 3000 TSRA BKN013CB\nTempo 1922 4000 RA BKN014.\n\nRunway 23 was the active runway at Fuhlsbüttel. On approach to land, the flight encountered severe turbulence due to a thunderstorm and the crew aborted the approach as the aircraft descended through . It was later established that a derecho had formed. Winds of were recorded, and seven people were killed in the Berlin area. The storm was described as the worst summer storm in 50 years in Berlin. The crew decided to hold while they assessed their alternatives. The designated alternative airport was Bremen Airport, some to the south-west. To reach Bremen would have meant flying through the frontal system. Another aircraft successfully landed on Runway 33 at Hamburg, reporting strong winds. The crew of Flight 850 declined to attempt a landing on Runway 23, and requested a diversion to Langenhagen Airport, Hannover. Air Traffic Control (ATC) did not suggest any other alternatives, nor were they requested by the crew.\n\nEn route, the frontal system prevented the crew from turning towards Hannover. A decision was made to divert to Tegel Airport, Berlin. The Automatic Terminal Information Service at Tegel stated that the weather there was clear and no significant change was expected. Approaching Tegel's Runway 08L, the crew requested priority handling, stating that they had fuel for 40 minutes flight. On approach, severe turbulence was again encountered due to the frontal system having reached Berlin. The approach was abandoned and the crew requested an alternate airfield from ATC. Eberswalde Airfield was suggested and accepted by the crew, who stated \"We'll take anything at this point\". On hearing this remark, ATC treated the aircraft's situation as an emergency. En route to Eberswalde, thunderstorms were observed and alternates were sought from ATC.\n\nHamburg ATC then offered Neubrandenburg Airport, which was rejected by the crew of Flight 850 on receiving the weather information. Werneuchen Airfield was then offered, which was away and offered a runway long. Werneuchen was accepted by the crew. ATC managed to contact the chairman of the flying club based at Werneuchen. He stated that the runway surface was long, but an earth bank stretched across the runway leaving available. Landing on Runway 08 meant that the first part of the runway fell before the earth bank. Almost an hour after aborting the approach to Fuhlsbüttel, Flight 850 began its approach to Werneuchen. The crew reported that they were visual with the runway and were advised by Werneuchen ATC that they needed to land on the eastern part of Runway 08. When Flight 850 turned onto its final approach, the captain remarked that the runway was \"longer than Berne\", and told the first officer to land wherever he wanted. Although the closed off part of the runway had been marked as such, the markings had weathered severely over the years, meaning that the original markings were easier to see than those that actually applied. Fading light and a lack of runway lighting contributed to the inability of the crew to see the earth bank.\n\nThe first officer landed the aircraft at what appeared to be the runway threshold. It then came in contact with the earth bank which ripped off all three undercarriage legs and the aircraft slid to a halt on its belly. The fire alarm for the port engine sounded, and the crew performed fire drills on both engines. One female passenger injured her leg. The wrecked aircraft was initially stored but was later declared as damaged beyond economic repair and was subsequently scrapped.\n\nThe German Federal Bureau of Aircraft Accidents Investigation (BFU) opened an investigation into the accident, which was to take 3,005 days (over eight years) to complete. It found that a combination of factors caused the accident. Had the crew received the SIGMETs, the BFU considers it is likely that the crew would have realised that the thunderstorms were not isolated, but part of a system, and therefore made different decisions to those that they did.\n\nThe METARs for both Tegel and Schönefeld airports showed and NOSIG, the latter element was criticised by the BFU. At 17:50, this METAR was issued at Tegel Airport: EDDT 04001KT CAVOK 30/17 Q1002 A2959 0998 2947 NOSIG. At the time, the cold front was south west of Tegel, and had moved in the previous hour. The BFU was of the opinion that NOSIG should not have been in the METAR, and that a SPECI would have been required. At 18:20, a new METAR was issued at Tegel: EDDT VRB01KT 9999 FEW040CB SCT120 BKN260 29/17 Q1002 A2959 0998 2947 TEMPO 27025G55KT 2000 +TSRA BKN009 BKN015CB COMMENTS: OCNL LTNG AND CB SW OF STN. This METAR was issued two minutes before Flight 850 began its approach to Tegel.\n\nThe decision to abort the approach to Fuhlsbüttel was supported by the BFU, but not the decision to divert to Hannover. The decision to divert to Tegel was supported by the BFU, based on the incorrect information given to the crew of CAVOK and NOSIG at Tegel. On approach to Werneuchen, ATC did not use correct terminology. It also found that the runway markings at Werneuchen did not conform to the required standard.\n\n\n"}
{"id": "38448973", "url": "https://en.wikipedia.org/wiki?curid=38448973", "title": "Tadahiko Mizuno", "text": "Tadahiko Mizuno\n\nHe was also a member of Energy Environmental Institute of Engineering at Hokkaido University until 2009.\n\nMizuno graduated from the Department of Applied Physics, Hokkaido University, Faculty of Engineering in March 1968. In March 1970, he graduated with a master's degree from the Department of Applied Physics, Hokkaido University, Faculty of Engineering. In April 1972 he completed his doctorate degree in Engineering at Hokkaido University, Faculty of Engineering, Department of Engineering. In March 1976, he received his doctorate in Engineering for \"Study on formation process of hydride on the surface of Ti by d, n reaction” Teaching; Atomic Engineering, Corrosion, X-rays analysis, Electron microscope, Exercise: Mathematics, Physical Engineering.\n\nHe was awarded The International Society for Condensed Matter Nuclear Science Prizes (Giuliano Preparata medal) in 2004 from The International Society for Condensed Matter Nuclear Science (ISCMNS). The ISCMNS is the organizer of a conference and a workshop on cold fusion and related topics.\n\n\n\nElectrochemical, metallurgy, nuclear reaction in condensed matter, elucidation of the peculiar behavior of hydrogen in the metal, hydrogen penetration in metals, hydrogen embrittlement, hydrogen production, hydrogen separation and purification, power conversion of hydrogen, elucidation of hydrogen behavior, development of unique methods using hydrogen isotopes, studying the behavior of hydrogen on metal. Mizuno has also written Numerous books representing the interaction between hydrogen and the metals.\n\nMizuno was involved in anti-terrorism measures as part of international safety measures for Hakodate Customs of Ministry of Finance.\n\n\n"}
{"id": "50899203", "url": "https://en.wikipedia.org/wiki?curid=50899203", "title": "Tornadoes of 1964", "text": "Tornadoes of 1964\n\nThis page documents the tornadoes and tornado outbreaks of 1964, primarily in the United States. Most tornadoes form in the U.S., although some events may take place internationally. Tornado statistics for older years like this often appear significantly lower than modern years due to fewer reports or confirmed tornadoes.\n"}
{"id": "8903990", "url": "https://en.wikipedia.org/wiki?curid=8903990", "title": "Wet wing", "text": "Wet wing\n\nA wet wing is an aerospace engineering technique where an aircraft's wing structure is sealed and used as a fuel tank. Wet wings are also called \"integral fuel tanks\". By eliminating the need for fuel bladders, aircraft can weigh less and the wing root bending moment caused by the lift generated by the wings in flight is decreased. This offers further reduction in weight by allowing structural components to be designed lighter as the components do not need to support larger forces.\n\nWet wings are common among most civilian designs, from large transport aircraft, such as airliners, to small general aviation aircraft. Because the tanks are an integral part of the structure, they cannot be removed, and require access panels for routine maintenance and visual inspections.\n\nA disadvantage of the wet wing is that every rivet, bolt, nut plate, hose and tube that penetrates the wing must be sealed to prevent fuel from leaking or seeping around these hardware components. This sealant must allow for expansion and contraction due to rapid temperature changes (such as when cold fuel is pumped into a warm wing tank) and must retain its sealing properties when submerged in fuel and when left dry for long periods of time. Working with this sealant can be difficult and replacing old sealant inside a small wing tank can be harder if the old sealant needs to be removed as well before new sealant can be applied.\n\n"}
