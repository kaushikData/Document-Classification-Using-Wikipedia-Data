{"id": "12788181", "url": "https://en.wikipedia.org/wiki?curid=12788181", "title": "7JP4", "text": "7JP4\n\nThe 7JP4 is an early black and white or monochrome cathode ray tube (also called picture tube and kinescope). It was a popular type used in late 1940s low cost and small table model televisions. The 7JP4 has a 7\" diameter round screen which was often partially masked. Unlike later electromagnetically deflected TV tubes, the 7JP4 is electrostatically deflected like an oscilloscope tube.\n\nThe 7JP4 is part of the 7JPx series of circular face electrostatic cathode ray tubes (CRT). Originally developed for radar applications as a display device for radar display A scopes around 1944. After World War 2 the CRT was adapted for television applications. There are three versions. The 7JP4 (P4 represents the phosphor that glows white and has medium persistence) for television. For oscilloscope applications the 7JP1 was used (P1 phosphor has a green trace and short persistence). Radar applications the 7JP7 was used (P7 phosphor has a blue-white trace with a long persistence). This CRT was produced by multiple manufacturers (RCA, General Electric, Sylvania Electric Products and Tung-sol). Except for the type of phosphor used all three are identical in operation and connection. The screen diagonal is 7 inches (17.8 cm) for 7JP1 and 7JP4, but only 5.5 inches (14 cm) for the 7JP7.\n\nSome General Electrical Characteristics are shown below. Second Anode + Grid 2 (Pin 9) and Plates (Pin 10 & 11 and Pin 7 & 8) have a maximum value limit of 6000 volts dc. Internal arcing can be expected when this voltage is exceeded. Actual values are typically in the 4000 to 5500 volt range, and some sets were operated as low as 2000 volts dc. Grid 1 (Pin 3) can receive either a Negative Bias or a Positive Bias. Pin-2 is the brightness voltage, and pin-3 is the video signal which rides on top of DC, and pin-2 is a DC Level which varies with the Brightness Control. Some sets are backwards and have pin-3 on ground and video on pin-2 along with brightness adjustment.\n\nFrom 1946 to 1951 the 7JP4 was a common CRT (Picture Tube or Kinescope) used in lower priced televisions sold in the United States. These television were popular for portable carry around and small table top sets. These smaller sets were direct view electrostatic deflection designs. This required an extremely large amount of voltage to produce an image across the full display screen. In 1946, RCA influenced manufacturers (with royalty free circuit designs) to move toward electromagnetic deflection type televisions. Electromagnetic deflection uses varying magnetic fields to produce a full screen image. Horizontal and vertical electromagnets are placed at the picture tube neck, called the \"yoke\". This method allowed the image to be viewed on larger screens. The first heavily mass-produced large picture tube to use this newer method of deflection was RCA's 10BP4, introduced in 1946. Soon after electrostatic picture tubes and the television electronic design would be completely replaced.\n\nFor a detailed look at US electrostatic TV design and the 7JP4 Kinescope from an Australian view (by cablehack) go to the external link below involving the Wards Airline TV (Sentinel model 400TV).\n\nThe interest in the 7JPx series of CRT's is in restoration of dead technology. Dead Technology represent products that are no longer mass-produced or seriously used. The technology used in these designs have reach their highest level or have been replaced by a better technology. The restoration of early US made television sets has spurred interest in the 7JP4. Since this particular model is no longer made, only available ones are from old stock or from television sets that cannot be restored. This makes the price of a working CRT very expensive.\n\nBecause it is electrostatically deflected and was obsolete by the mid-1950s, most CRT testers will not test the 7JP4 and thus it is best tested in a working TV set. The Precision CR30, Sencore CR-70 and Jackson 707 are some of the CRT testers that are capable of testing the 7JP4,3KP4 and other electrostatic deflection CRTs. Since the availability of these CRT testers is very limited, the prices for such testers are steep, so many restorers test these CRT's on a working TV set that used electrostatic CRTs. There are many picture tube restoring equipment available for magnetic deflection tubes but there is no way to restore electrostatic tubes. The biggest problem with many picture tubes is the loss of emission or electron production due to contaminated or damaged cathode that surrounds the heater.\n\nThese and other early television sets can be found in the \"Collectors Guide to Vintage Televisions- Identification and Values\", by Bryan Durbal and Glenn Bubenheimer () published by Collector Books, Paducah, KY, USA.\n\n"}
{"id": "35095476", "url": "https://en.wikipedia.org/wiki?curid=35095476", "title": "Aeromexico Flight 230", "text": "Aeromexico Flight 230\n\nAeroméxico Flight 230 experienced a hard landing at Chihuahua Airport on July 27, 1981. Thirty-two people were killed when the McDonnell Douglas DC-9-32 jet aircraft was heavily damaged by impact with the ground and fire on approach in high winds.\n\nThe flight was uneventful until landing at Chihuahua. There were isolated cumulonimbus clouds with strong squalls and showers during approach and landing. Upon touchdown, the aircraft bounced once and struck the ground; the aircraft then slid off the runway, broke up and caught on fire. Thirty-four passengers and crew were able to flee the wreckage; the smoke and fire caused the deaths of those that remained trapped.\n\n\n"}
{"id": "51519180", "url": "https://en.wikipedia.org/wiki?curid=51519180", "title": "Angular Correlation of Electron Positron Annihilation Radiation", "text": "Angular Correlation of Electron Positron Annihilation Radiation\n\nAngular Correlation of Electron Positron Annihilation Radiation (ACAR or ACPAR) is a technique of solid state physics to investigate the electronic structure of metals. It uses positrons which are implanted into a sample and annihilate with the electrons. In the majority of annihilation events, two gamma quanta are created that are, in the reference frame of the electron-positron pair, emitted in exactly opposite directions. In the laboratory frame, there is a small angular deviation from collinearity, which is caused by the momentum of the electron. Hence, measuring the angular correlation of the annihilation radiation yields information about the momentum distribution of the electrons in the solid.\n\nAll the macroscopic electronic and magnetic properties of a solid result from its microscopic electronic structure.\nIn the simple free electron model, the electrons do not interact with each other nor with the atomic cores. The relation between energy formula_1 and momentum formula_2 is given by\nwith the electron mass formula_4. Hence, there is an unambiguous connection between electron energy and momentum. Because of the Pauli exclusion principle the electrons fill all the states up to a maximum energy, the so-called Fermi energy. By the momentum-energy relation, this corresponds to the Fermi momentum formula_5. The border between occupied and unoccupied momentum states, the Fermi surface, is arguably the most significant feature of the electronic structure and has a strong influence on the solid's properties. In the free electron model, the Fermi surface is a sphere.\n\nWith ACAR it is possible to measure the momentum distribution of the electrons. A measurement on a free electron gas for example would give a positive intensity for momenta formula_6 and zero intensity for formula_7. The Fermi surface itself can easily be identified from such a measurement by the discontinuity at formula_5.\n\nIn reality, there \"is\" interaction between the electrons with each other and the atomic cores of the crystal. This has several consequences: For example, the unambiguous relation between energy and momentum of an electronic state is broken and an electronic band structure is formed. Measuring the momentum of one electronic state gives a distribution of momenta which are all separated by reciprocal lattice vectors. Hence, an ACAR measurement on a solid with completely filled bands (i.e. on an insulator) gives a continuous distribution. An ACAR measurement on a metal has discontinuities where bands cross the Fermi level in all Brillouin zones in reciprocal space. This discontinuous distribution is superimposed by a continuous distribution from the entirely filled bands. From the discontinuities the Fermi surface can be extracted.\n\nSince positrons that are created by beta decay possess a longitudinal spin polarization it is possible to investigate the spin-resolved electronic structure of magnetic materials. In this way, contributions from the majority and minority spin channel can be separated and the Fermi surface in the respective spin channels can be measured.\n\nACAR has several advantages and disadvantages compared to other, more well known techniques for the investigation of the electronic structure like ARPES and quantum oscillation: ACAR requires neither low temperatures, high magnetic fields or UHV conditions. Furthermore, it is possible to probe the electronic structure at the surface and in the bulk (formula_9 deep). However, ACAR is reliant on defect free samples as vacancy concentrations of up to per atom can efficiently trap positrons and distort the measurement.\n\nIn an ACAR measurement the angular deviation of many pairs of annihilation radiation is measured. Therefore, the underlying physical observable is often called ‘two photon momentum density’ (TPMD) or formula_10. Quantum mechanically, formula_10 can be expressed as the squared absolute value of the Fourier transform of the multi-particle wave function formula_12 of all the electron and the positron in the solid:\nAs it is not possible to imagine or compute the multi-particle wave function formula_12, it is often written as the sum of the single particle wave functions of the electron formula_15 in the formula_16th state in the formula_17th band and the positron wave function formula_18:\nThe enhancement factor formula_20 accounts for the electron-positron correlation. There exist sophisticated enhancement models to describe the electron-positron correlations, but in the following it will be assumed that formula_21. This approximation is called the independent particle model (IPM).\n\nA very illustrative form of the TPMD can be obtained by the use of the Fourier coefficients for the wave function product formula_22:\nThese Fourier coefficients are distributed over all reciprocal vectors formula_24. If one assumes that the overlap of the electron and the positron wave function is constant for the same band formula_17, summing formula_10 over all reciprocal lattice vectors gives a very instructive result:\nThe function formula_28 is the Heaviside step function and the constant formula_29 . This means, if formula_10 is folded back into the first Brillouin zone, the resulting density is flat except at the Fermi momentum. Therefore, the Fermi surface can be easily identified by looking for this discontinuities in formula_31.\n\nWhen a positron is implanted into a solid it will quickly lose all its kinetic energy and annihilate with an electron. By this process two gamma quanta with each are created which are in the reference frame of the electron positron pair emitted in exactly anti-parallel directions. In the laboratory frame, however, there is a Doppler shift from and an angular deviation from collinearity. Although the full momentum information about the momentum of the electron is encoded in the annihilation radiation, due to technical limitations it cannot be fully recovered. Either one measures the Doppler broadening of the annihilation radiation (DBAR) or the angular correlation of the annihilation radiation (ACAR).\n\nFor DBAR a detector with a high energy resolution like a high purity germanium detector is needed. Such detectors typically do not resolve the position of absorbed photons. Hence only the longitudinal component of the electron momentum formula_32 can be measured. The resulting measurement is a 1D projection of formula_10.\n\nIn ACAR position sensitive detectors, gamma cameras or multi wire proportional chambers, are used. Such detectors have a position resolution of typically but an energy resolution which is just good enough to sort out scattered photons or background radiation. As formula_32 is discarded, a 2D projection of formula_10 is measured. In order to get a high angular resolution of and better, the detectors have to be set up at distances between from each other. Although it is possible to get even better angular resolutions by placing the detectors further apart, this comes at cost of the counting rate. Already with moderate detector distances, the measurement of one projection of formula_10 typically takes weeks.\n\nAs ACAR measures projections of the TPMD it is necessary to reconstruct formula_10 in order to recover the Fermi surface. For such a reconstruction similar techniques as for X-ray computed tomography are used. In contrast to a human body, a crystal has many symmetries which can be included into the reconstruction. This makes the procedure more complex but increases the quality of the reconstruction. Another way to evaluate ACAR spectra is by a quantitative comparison with ab initio calculations.\n\nIn the early years, ACAR was mainly used to investigate the physics of the electron-positron annihilation process. In the 1930s several annihilation mechanism were discussed. Otto Klemperer could show with his angular correlation setup that the electron-positron pairs annihilate mainly into two gamma quanta which are emitted anti-parallel. In the 1950s, it was realized that by measuring the deviation from collinearity of the annihilation radiation information about the electronic structure of a solid can be obtained.\n\nDuring this time mainly setups with ‘long slit geometry’ were used. They consisted of a positron source and a sample in the center, one fixed detector on one side and a second movable detector on the other side of the sample. Each detector was collimated in such a way that the active area was much smaller in one than in the other dimension (thus ‘long slit’). A measurement with a long slit setup yields a 1D projection of the electron momentum density formula_10. Hence, this technique is called 1D-ACAR.\n\nThe development of two-dimensional gamma cameras and multi wire proportional chambers in the 1970s and early 1980s led to the setting up of the first 2D-ACAR spectrometer. This was an improvement to 1D-ACAR in two ways: i) The detection efficiency could be improved and ii) the informational content was greatly increased as the measurement gave a 2D projection of formula_10. An important early example of the use of spin-polarized 2D-ACAR is the proof of half metallicity in the half-Heusler alloy NiMnSb.\n\n"}
{"id": "726885", "url": "https://en.wikipedia.org/wiki?curid=726885", "title": "By-product", "text": "By-product\n\nA by-product is a secondary product derived from a manufacturing process or chemical reaction. It is not the primary product or service being produced. In the context of production, a by-product is the 'output from a joint production process that is minor in quantity and/or net realizable value (NRV) when compared with the main products'. Because they are deemed to have no influence on reported financial results, by-products do not receive allocations of joint costs. By-products also by convention are not inventoried, but the NRV from by-products is typically recognized as 'other income' or as a reduction of joint production processing costs when the by-product is produced. A by-product can be useful and marketable or it can be considered waste. For example, bran is a byproduct of the milling of refined flour, sometimes composted or burned for disposal but in other cases used as a nutritious ingredient in food or feed, and gasoline was once a byproduct of oil refining that later became a desirable commodity as motor fuel.\n\nIEA defines \"by-product\" in the context of life-cycle assessment: \"... main products, co-products (which involve similar revenues to the main product), by-products (which result in smaller revenues), and waste products (which provide little or no revenue).\"\n\nWhile some chemists treat \"by-product\" and \"side-product\" as synonyms in the above sense of a generic secondary (untargeted) product, others find it useful to distinguish the two. When the two terms are distinguished, \"by-product\" is used to refer to a product that is not desired but inevitably results from molecular fragments of starting materials and/or reagents that are not incorporated into the desired product, as a consequence of conservation of mass. In contrast, \"side-product\" is used to refer to a product that is formed from a competitive process that could, in principle, be suppressed by optimization of reaction conditions.\n\n"}
{"id": "44381435", "url": "https://en.wikipedia.org/wiki?curid=44381435", "title": "China Dark Matter Experiment", "text": "China Dark Matter Experiment\n\nThe China Dark Matter Experiment (CDEX) is a search for dark matter WIMP particles at the China Jinping Underground Laboratory. CDEX was the first experiment to be hosted at CJPL, beginning construction of its shield in June 2010, the same month that laboratory construction was completed, and before CJPL's official opening on December 12.\n\nCDEX has p-type point-contact germanium detector surrounded by NaI(Tl) crystals, similar to the CoGeNT experiment. The CDEX-0 prototype was used to develop the current CDEX-1 detector, which has a detector mass of roughly 1 kg. Future plans include scaling to CDEX-10 and CDEX-1T.\n\nCDEX-1 had first low mass results in 2013 and published limits on WIMP masses 6–20 GeV in 2014.\n"}
{"id": "45502648", "url": "https://en.wikipedia.org/wiki?curid=45502648", "title": "Cold Atom Laboratory", "text": "Cold Atom Laboratory\n\nCold Atom Laboratory (CAL) is an experimental instrument being developed that was originally scheduled for launch to the International Space Station (ISS) in June 2017. It was then delayed until a scheduled launch on a SpaceX CRS-12 rocket in August 2017. It launched in May 2018. \n\nThe instrument will create extremely cold conditions in the microgravity environment of the ISS leading to the formation of Bose Einstein Condensates that are a magnitude colder than those that are created in laboratories on Earth. In a space-based laboratory, up to 20 seconds interaction times and as low as 1 picokelvin temperatures are achievable, and it could lead to exploration of unknown quantum mechanical phenomena and test some of the most fundamental laws of physics. NASA's JPL scientists state that the CAL investigation could advance knowledge in the development of extremely sensitive quantum detectors, which could be used for monitoring the gravity of Earth and other planetary bodies, or for building advanced navigation devices.\n\nThe initial mission will have a duration of 12 months with up to five years of extended operation.\n\n"}
{"id": "3185460", "url": "https://en.wikipedia.org/wiki?curid=3185460", "title": "Comgás", "text": "Comgás\n\nComgás is a Brazilian gas distributor focused on São Paulo state. It is Brazil's biggest gas distributor, with around 1,6 million residential, commercial and industrial customers, as of 2016, who receive gas through about of pipelines. Comgás was founded in 1872. The company had been wholly owned by the São Paulo state-owned power generation utility, \"Companhia Energética de São Paulo\" until April 1999, when CESP's stake was sold to the British BG Group and to Royal Dutch Shell. Comgas sells gas under a 30-year franchise, with a potential for a further 20 years.\n\nBrazilian conglomerate Cosan owns 61.73% of Comgas's stock, and Shell 17.12%. The remainder is publicly traded on BM&F Bovespa. Cosan acquired its stake from BG Group in November 2012 for $1.7 billion.\n\n"}
{"id": "30356816", "url": "https://en.wikipedia.org/wiki?curid=30356816", "title": "December 1969 nor'easter", "text": "December 1969 nor'easter\n\nThe December 1969 nor'easter was a strong winter storm that mainly affected the Northeastern United States and southern Quebec between December 25 and December 28, 1969. The multi-faceted storm system included a tornado outbreak, record snow accumulations, a damaging ice storm, and flooding rains.\n\nThe storm developed over Texas by December 25 and advanced eastward, spawning over a dozen tornadoes in Louisiana, Georgia, and Florida. Upon reaching the Eastern Seaboard, the cyclone turned northeastward and intensified into a powerful nor'easter. On December 26 and 27, the storm's forward movement slowed to a drift, causing very heavy snow over Upstate New York, Vermont, and southern Quebec. Warm onshore winds, caused by a storm track close to the shore, allowed precipitation to change to sleet and rain in central and eastern New England. Where precipitation remained as snow, accumulations reached or more, crippling travel. Drifts up to high blocked roadways, isolating some communities and forcing emergency workers to rely on snowmobiles for transportation. At least 20 fatalities were attributed to the storm in New York and New England.\n\nIn central New England, a severe freezing rain event occurred along the boundary between cold air to the west and warmer air to the east. Several inches of glaze ice, accompanied by gale-force winds, caused damage comparable to the aftermath of the 1938 New England hurricane. Throughout the region, the snow and ice—in some cases further weighted by heavy rainfall—caused roofs to cave in. Montreal received of snow in that city's biggest snowstorm on record at the time; officials blamed the storm for 15 deaths in Quebec. In eastern New England, ice jams, poor drainage, and several inches of rain caused flooding that forced people from their homes and submerged roadways. Wind gusts to and strong waves battered the coastline.\n\nThe winter of 1969–1970 continued a long-term El Niño from the previous winter, which featured two significant storms in the Northeastern United States, including the \"Lindsay storm\" in February. North Atlantic Oscillation (NAO) values ranged from weakly negative to weakly positive in the period leading up to the storm, trending more consistently negative in the days surrounding its genesis. NAO values dropped significantly in the wake of the nor'easter, signaling abnormally cold temperatures during January 1970. December 1969 was a month of active weather throughout the Northeast, with frequent light to moderate snow events preceding the late-month nor'easter. A significant storm on December 22–23 dropped of snow in eastern New York and northern New England, as well as significant freezing rain in southern and eastern New England. Many of the same areas would be impacted again just days later.\n\nBefore the cyclone's arrival, a high-pressure area situated north of New England funneled very cold air into the region; the air temperature at Albany, New York, on December 25 still holds the record for the city's coldest December temperature. On December 25 and 26, extensive cold air damming and frontogenesis occurred along the U.S. East Coast. The high-pressure area slowly retreated as the storm approached.\nThe nor'easter originated in a weak area of surface low pressure that formed over northern Texas by December 25. It moved generally eastward over the next day as it crossed the Deep South. A southerly low-level jet (a localized wind stream in the lower levels of the atmosphere) developed by midday over Mississippi and Alabama, and drifted eastward. Precipitation soon blossomed, with heavy snow falling over the Appalachian Mountains by late on December 25. The low turned toward the northeast by early on December 26 to follow the temperature gradient along the East Coast, at which time the storm underwent the first of two periods of rapid intensification. In 12 hours, barometric pressure fell 12 hPa to while the nor'easter accelerated from Georgia to the coast of New Jersey. This bout of strengthening occurred as the associated upper-level trough at the 500-millibar level – several miles into the atmosphere – assumed a negative tilt from northwest to southeast. Heavy precipitation overspread the Mid-Atlantic and New England early on December 26.\n\nThe low-level jet continued to strengthen and delivered warm air to areas east of the storm center, where precipitation quickly transitioned to rain. The jet ultimately reached extremely high velocities of as it wrapped around the northern side of the low, feeding abundant moisture into areas of heavy precipitation. The storm slowed drastically on December 26, and over the next 24 hours, it moved very slowly from Cape May, New Jersey, to near Boston, Massachusetts. During this period, exceptionally heavy snow fell over eastern New York, Vermont, and southern Quebec. While located just east of Long Island on December 27, the nor'easter began its second phase of rapid strengthening that brought central pressure down to . During this period of intensification, snow propagated around the western side of the system, reaching as far south as Long Island.\n\nWith a storm track very close to the shore and a retreating high-pressure area to the northeast, onshore winds allowed temperatures to warm above freezing in eastern and northern New England. A frontal boundary between the warm Atlantic air and a residual wedge of colder air over the interior served as the focus for a severe ice storm in central New England. Thunder and lightning accompanied the precipitation in some areas. By 12:00 UTC on December 27, the attendant upper-level trough matured into a closed cold-core low. The nor'easter continued to drift toward the east-northeast over the next 24 hours.\n\nThe developing extratropical cyclone spawned 16 known tornadoes and waterspouts in Louisiana, Georgia, and Florida, becoming the largest Christmas Day tornado outbreak on record. A short-lived F3 tornado touched down in Vermillion Parish, Louisiana, destroying eight homes near Kaplan and damaging 25 more. One woman was killed and eight were injured, three severely enough to require hospitalization. Another damaging tornado moved through several settlements in southern Iberville Parish just over two hours later; the tornado destroyed seven homes and damaged multiple other buildings, with one injury reported. The tornado was officially classified an F3, though meteorologist Thomas P. Grazulis determined it to have caused damage consistent with an F2 rating. A non-tornadic wind gust destroyed a nearby trailer home. Later that day, a tornado skipped along a path through Catahoula and Concordia parishes, touching down three different times. The F2 tornado damaged about 20 trailer homes in Glade, and caused further damage near the end of its track.\n\nA tornado in southern Georgia, about west of Albany, damaged or destroyed several small homes, killed three head of cattle, toppled numerous trees, and injured seven people. Elsewhere in the state, there were isolated reports of hail and damaging thunderstorm winds. Several tornadoes in the Florida Panhandle caused minor to moderate damage to trees and homes, the most destructive of them occurring in Quincy, Florida. That tornado struck an industrial area and unroofed at least three structures. A tornado in Lee, Florida, destroyed a trailer, damaged farm buildings, and injured one person.\n\nIn addition to widespread rain, parts of northern South Carolina experienced freezing rain that damaged trees and power lines. Freezing rain and sleet fell over the southern Piedmont of North Carolina, resulting in power outages and dangerous travel conditions. At least one fatal traffic accident was reported. Heavy snow covered the northwest Piedmont and Mountain Region of the state, accumulating to around at Asheville; occurring on the heels of another winter storm, this snowfall contributed to depths up to at higher elevations. High winds from the storm left at least 50,000 electric customers in the Richmond, Virginia region without power. In Carroll County, strong winds uprooted trees. Significant snow accumulations occurred throughout the Mid-Atlantic, chiefly away from the coastal plain. Snowfall exceeded in parts of western Virginia, western Maryland, and West Virginia; Washington, D.C. received of snow.\n\nThe cyclone was rated by meteorology researchers Kocin and Uccellini (2004) as a high-end Category 3 on the Northeast Snowfall Impact Scale, equating to a \"major\" winter storm. However, more recent data from the National Climatic Data Center classifies the nor'easter as a low-end \"crippling\" Category 4. By December 30, at least 20 deaths in New York and New England had been attributed to the storm, largely from traffic accidents and physical exhaustion.\n\nAreas of northeastern Pennsylvania, eastern New York, and Vermont received snowfall in excess of , including at Burlington, at Albany, and at Binghamton, New York. Accumulations locally exceeded , with measured at Waitsfield, Vermont and in the small village of East Wallingford, Vermont. The storm remains one of the greatest on record in cities like Albany and Burlington, and contributed to high monthly snowfall totals; in Albany, December 1969 stands as the snowiest documented month, with . Throughout the region, the weight of the snow, combined with gale-force winds, collapsed roofs and brought down power lines. The winds blew snow from this and previous storms into drifts as high as , blocking roads and leaving some communities isolated. In some areas, travel was only possible with snowmobiles, which were used by emergency workers to deliver emergency supplies as needed. Rescuers also used snowmobiles and aircraft to search for stranded residents and travelers. Many cities and counties throughout the Northeast issued snow emergencies. Business and industry suffered extensively in the days following the storm.\n\nWhile major highways were generally cleared by December 29, some local roads reportedly remained covered for up to a month. About of the New York State Thruway—nearly half its total length—was closed for 27 hours. The snowfall also caused widespread property damage, as the roofs of barns, sheds, and other buildings caved in under the weight of the snow. An airport hangar in Oneonta collapsed, destroying five aircraft inside. In the aftermath of the storm, many homes sustained water damage from melting snow. \"Innumerable\" traffic accidents were reported, and many motorists became stranded or were forced to abandon their vehicles. Four people in a stalled automobile died of carbon monoxide poisoning. Also in New York, one man froze to death, and three individuals were killed in a building collapse. In the Capital District, the snow fell with a high water content, occasionally mixing with freezing rain. As a result, it was much denser than normal and proved difficult to manage. The City of Albany spent a record $2 million USD on snow removal, and the city restricted nearly all traffic except for vehicles necessary to maintain emergency supplies. Governor Nelson Rockefeller closed state offices in Albany on December 29 and 30, in an effort to enable efficient snow removal. Mayor Erastus Corning 2nd assessed the situation as a \"dire emergency\".\n\nGovernor Deane C. Davis of Vermont declared a state of emergency and ordered the National Guard to assist with cleanup efforts. Blowing snow continuously blocked roads after being cleared, and snow removal vehicles, not equipped to withstand the excessive snow, began experiencing mechanical issues. The storm left some communities without power or telephone service for up to two days, although isolated outages persisted longer. One person died in Saxtons River. With roads impassable in both New York and New England, tank trucks could not reach dairy farmers, who were in some cases forced to discard thousands of gallons of milk. In other cases, power outages rendered equipment useless, requiring milking to be done manually. One person in New Hampshire died of exhaustion during the storm.\n\nThe influx of warm, moist air over a wedge of cold air near the surface resulted in a severe ice storm across central and northern New England, most notably in the upper Connecticut River Valley. Glaze ice thicker than , and reportedly up to thick in Vermont, built up on trees and power lines, wreaking \"unbelievable\" havoc. In the hardest-hit areas, damage from the ice storm was comparable to that caused by the 1938 New England hurricane. The combined load of snow and ice caused significant damage to many residential and industrial buildings, with several factory and warehouse roofs collapsing. Livestock were killed in multiple rural building collapses throughout the area. Southern Maine endured its second damaging ice storm in a week, with many of the same communities losing power on both occasion. Precipitation also transitioned to freezing rain and sleet across interior Connecticut, where the dangerous conditions induced numerous traffic accidents. Due to the abnormally cold temperatures in the wake of the storm, the glaze persisted for up to six weeks, much longer than the typical three days for ice storms in New England.\n\nThe changeover to heavy rain in southern and eastern New England swelled frozen rivers, creating ice jams which induced severe flooding. Flooding of roadways and homes was compounded by drainage issues from the dense snowcover. Rising floodwaters forced hundreds of families to leave their homes in Massachusetts, New Hampshire, and Maine. Up to of rain fell in the Boston region, where thousands of homes were inundated. Rivers had begun to recede by December 29 after cresting well above flood stage. Up to of rain fell over Rhode Island after of snow, with widespread flooding reported in multiple towns. Frequent automobile accidents and large traffic jams were reported in the state. Winds were strongest in Cape Cod, Massachusetts, where gusts peaked near . The gales were particularly damaging to trees laden with snow and ice. The heavy rainfall in eastern Massachusetts added tremendous weigh to the snow that had already fallen, causing roof collapses and making it difficult to clear the snow. In southeastern New Hampshire, the strong winds toppled trees and blew out windows in Hampton Beach. Although the coastal plain of New England avoided the heaviest snow accumulations, pounding surf and strong winds battered piers, boats, and other coastal installations, while low-lying areas experienced storm surge flooding.\n\nThe storm dropped up to of snow in southern Quebec, where wind gusts up to damaged buildings and brought down power lines. Over a period of 60 hours, of snow fell at Montreal in what was then the city's heaviest snowfall on record. Rail, air, and automobile transportation were paralyzed. One individual in Quebec City was struck and killed by a snowplow, and two more died of exposure to cold while ice fishing on the St. Lawrence River. Several fires broke out in Montreal during the storm, killing at least six people and leaving 23 families homeless. In total, the storm was connected to at least 15 fatalities in the province.\n\n\nGeneral\n\nSpecific\n"}
{"id": "44131920", "url": "https://en.wikipedia.org/wiki?curid=44131920", "title": "Decynium-22", "text": "Decynium-22\n\nDecynium-22 is a cationic derivative of quinoline, and a potent inhibitor of the plasma membrane monoamine transporter (PMAT), as well as all members of the organic cation transporter (OCT) family in both human and rat cells. However, it has little effect on high affinity monoamine transporters such as the dopamine transporter and norepinephrine transporter.\n\nDecynium has been shown to have a very high affinity to organic cation transporters in a variety of species, including human, rat, and pig.\nDecynium-22 has been shown to block the uptake of 1-methyl-4-phenylpyridinium (MPP) via the OCT3 transporter in rat astrocytes.\n\nDecynium-22 emits a low fluorescence yield (around 0.001), and in its monomeric form is a weakly fluorescent. However, aggregated decynium-22 emits a strong superradiant emission with a maximum near 570–580 nm. 480 nm light falls near a short wavelength peak of the excitation spectrum of these aggregates. Decynium-22 fluorescence caused by aggregation can be observed in astrocytes.\n\nDecynium-22 has recently been investigated for its role in increasing extracellular serotonin in the brain in neuropharmacology research. The transportation of the neurotransmitter serotonin is often disrupted in psychiatric disorders characterized by social impairment, such as schizophrenia and depression. Serotonin is primarily taken up by the 5-HT transporter (SERT), although it is also taken up by auxiliary transporters, known as \"uptake 2\", which include OCT and PMAT.\n\nThe most commonly prescribed antidepressant drugs are the selective serotonin reuptake inhibitors (SSRIs), which act by blocking the high affinity SERT. A proposed explanation for the limited efficacy of SSRIs is the presence of the low affinity transporters OCT and PMAT, which limit the ability of SSRIs to increase extracellular serotonin. Decynium-22 blocked serotonin uptake via these auxiliary transporters, and when used in conjunction with SSRIs, decynium-22 enhanced the effects of SSRIs to inhibit serotonin clearance. A similar effect was seen in SERT knock-out mice, which resulted in an improvement of social behavior. When OCT3 was knocked out in mice, however, decynium-22 was ineffectual, indicating that the anti-depressant effects of decynium-22 are dependent upon its blockage of the OCT3.\n"}
{"id": "28176081", "url": "https://en.wikipedia.org/wiki?curid=28176081", "title": "Deep Creek Preserve", "text": "Deep Creek Preserve\n\nDeep Creek Preserve is located in Arcadia, Florida in DeSoto County, Florida. The area serves as a water conservation resource and offers opportunities for a range of activities. It is located at 10797 SW Peace River Street in Arcadia, Florida.\n"}
{"id": "23186554", "url": "https://en.wikipedia.org/wiki?curid=23186554", "title": "Ecosystem respiration", "text": "Ecosystem respiration\n\nEcosystem respiration \n\nEcosystem respiration is the sum of all respiration occurring by the living organisms in a specific ecosystem. \n\nEcosystem respiration is typically measured in the natural environment, such as a forest or grassland field, rather than in the laboratory. Ecosystem respiration is the production portion of carbon dioxide in an ecosystem's carbon flux, while photosynthesis typically accounts for the majority of the ecosystem's carbon consumption.\n\nHow ecosystem respiration works and its importance:\n\n\n"}
{"id": "21821870", "url": "https://en.wikipedia.org/wiki?curid=21821870", "title": "Eximprod Galați Wind Farm", "text": "Eximprod Galați Wind Farm\n\nThe Eximprod Galați Wind Farm is a proposed wind power project in Galați, Galați County, Romania. It will have 35 individual wind turbines with a nominal output of around 2 MW which will deliver up to 70 MW of power, enough to power over 44,253 homes, with a capital investment required of approximately US$80 million.\n"}
{"id": "51171315", "url": "https://en.wikipedia.org/wiki?curid=51171315", "title": "Expanded polyethylene", "text": "Expanded polyethylene\n\nExpanded polyethylene (aka EPE foam) refers to foams made from polyethylene. Typically it is made from expanded pellets ('EPE bead') made with use of a blowing agent, followed by expansion into a mold in a steam chest - the process is similar to that used to make expanded polystyrene foam.\n\nEPE foams are low density, semi-rigid, closed cell foam that are generally somewhere in stiffness/compliance between Expanded polystyrene and Polyurethane. Production of EPE foams is similar to that of expanded polystyrene, but starting with PE beads. Typical densities are with the lower figure being common. Densities as low as can be produced.\n\nBase polymer for EPE foams range from Low-density polyethylene (LDPE) to High-density polyethylene (HDPE).\n\nExpanded polyethylene copolymers (EPC) are also known - such as 50:50 (weight) materials with polystyrene. Though other properties are intermediate between the two bases, toughness for the copolymer exceeds either, with good tensile and puncture resistance. It is particularly applicable for re-usable products.\n\nEPE foams were first manufactured in the 1970s.\n\nProduction of the PE beads is usually by extrusion, followed by chopping, producing a 'pellet'. Autoclave expansion is the most common route the bead foam. Butane or pentane is often used as a blowing agent (before 1992 CFCs may have been used). Depending on the specific process uses the beads may be cross-linked either by electron beam irradiation (see Electron beam processing), or by the addition of a chemical agent such as Dicumyl peroxide.\n\nAn alternate route (JSP Process) to the beads uses carbon dioxide as a blowing agent which is impregenated into the pellets in an autoclave at a temperature close to the plastic's crystalline melting point. The pellets are foamed by \"flashing\" into the (lower pressure) atmosphere to expand.\n\nFinally molding is done by steam chest compression molding; usually the low pressure variant of the process is used, though the high pressure variant may be used for HDPE based EPE foams.\n\nPolyethylene bead foams (including) EPE can be used to replace both polystyrene foam, and both rigid and flexible polyurethane. Uses include cushioning applications, and impact absorption applications including packaging.\n\nConsumption of polyethylene for PE foam was estimated at 114x10 kg in 2001. The majority was used for non-crosslinked foams, but crosslinked PE foams represented a significant (~ one third) fraction of demand. Use in protective packaging represented the largest use sector for such foams.\n\n"}
{"id": "154473", "url": "https://en.wikipedia.org/wiki?curid=154473", "title": "Fermi level", "text": "Fermi level\n\nThe Fermi level chemical potential for electrons (or electrochemical potential for electrons) of a body is the thermodynamic work required to add one electron to the body. It is a thermodynamic quantity usually denoted by \"µ\" or \"E\"\nfor brevity. The Fermi level does not include the work required to remove the electron from wherever it came from.\nA precise understanding of the Fermi level—how it relates to electronic band structure in determining electronic properties, how it relates to the voltage and flow of charge in an electronic circuit—is essential to an understanding of solid-state physics.\n\nIn band structure theory, used in solid state physics to analyze the energy levels in a solid, the Fermi level can be considered to be a hypothetical energy level of an electron, such that at thermodynamic equilibrium this energy level would have a \"50% probability of being occupied at any given time\".\nThe position of the Fermi level with the relation to the band energy levels is a crucial factor in determining electrical properties.\nThe Fermi level does not necessarily correspond to an actual energy level (in an insulator the Fermi level lies in the band gap), nor does it require the existence of a band structure.\nNonetheless, the Fermi level is a precisely defined thermodynamic quantity, and differences in Fermi level can be measured simply with a voltmeter.\n\nSometimes it is said that electric currents are driven by differences in electrostatic potential (Galvani potential), but this is not exactly true.\nAs a counterexample, multi-material devices such as p–n junctions contain internal electrostatic potential differences at equilibrium, yet without any accompanying net current; if a voltmeter is attached to the junction, one simply measures zero volts.\nClearly, the electrostatic potential is not the only factor influencing the flow of charge in a material—Pauli repulsion, carrier concentration gradients, electromagnetic induction, and thermal effects also play an important role.\n\nIn fact, the quantity called \"voltage\" as measured in an electronic circuit has a simple relationship to the chemical potential for electrons (Fermi level).\nWhen the leads of a voltmeter are attached to two points in a circuit, the displayed voltage is a measure of the \"total\" work transferred when a unit charge is allowed to move from one point to the other.\nIf a simple wire is connected between two points of differing voltage (forming a short circuit), current will flow from positive to negative voltage, converting the available work into heat.\n\nThe Fermi level of a body expresses the work required to add an electron to it, or equally the work obtained by removing an electron.\nTherefore, \"V\" − \"V\", the observed difference in voltage between two points, \"A\" and \"B\", in an electronic circuit is exactly related to the corresponding chemical potential difference, \"µ\" − \"µ\", in Fermi level by the formula\nwhere \"−e\" is the electron charge.\n\nFrom the above discussion it can be seen that electrons will move from a body of high \"µ\" (low voltage) to low \"µ\" (high voltage) if a simple path is provided.\nThis flow of electrons will cause the lower \"µ\" to increase (due to charging or other repulsion effects) and likewise cause the higher \"µ\" to decrease.\nEventually, \"µ\" will settle down to the same value in both bodies.\nThis leads to an important fact regarding the equilibrium (off) state of an electronic circuit:\n\nThis also means that the voltage (measured with a voltmeter) between any two points will be zero, at equilibrium.\nNote that thermodynamic equilibrium here requires that the circuit be internally connected and not contain any batteries or other power sources, nor any variations in temperature.\n\nIn the band theory of solids, electrons are considered to occupy a series of bands composed of single-particle energy eigenstates each labelled by \"ϵ\". Although this single particle picture is an approximation, it greatly simplifies the understanding of electronic behaviour and it generally provides correct results when applied correctly.\n\nThe Fermi–Dirac distribution, formula_2, gives the probability that (at thermodynamic equilibrium) a state having energy \"ϵ\" is occupied by an electron:\n\nHere, \"T\" is the absolute temperature and \"k\" is Boltzmann's constant. If there is a state at the Fermi level (\"ϵ\" = \"µ\"), then this state will have a 50% chance of being occupied. The distribution is plotted in the left figure. The closer \"f\" is to 1, the higher chance this state is occupied. The closer \"f\" is to 0, the higher chance this state is empty.\n\nThe location of \"µ\" within a material's band structure is important in determining the electrical behaviour of the material.\n\nIn semiconductors and semimetals the position of \"µ\" relative to the band structure can usually be controlled to a significant degree by doping or gating. These controls do not change \"µ\" which is fixed by the electrodes, but rather they cause the entire band structure to shift up and down (sometimes also changing the band structure's shape). For further information about the Fermi levels of semiconductors, see (for example) Sze.\n\nIf the symbol \"ℰ\" is used to denote an electron energy level measured relative to the energy of the edge of its enclosing band, \"ϵ\", then in general we have \"ℰ\" = \"ϵ\" – \"ϵ\". We can define a parameter \"ζ\" that references the Fermi level with respect to the band edge:\n\nIt follows that the Fermi–Dirac distribution function can be written as\n\nThe band theory of metals was initially developed by Sommerfeld, from 1927 onwards, who paid great attention to the underlying thermodynamics and statistical mechanics. Confusingly, in some contexts the band-referenced quantity \"ζ\" may be called the \"Fermi level\", \"chemical potential\", or \"electrochemical potential\", leading to ambiguity with the globally-referenced Fermi level.\nIn this article, the terms \"conduction-band referenced Fermi level\" or \"internal chemical potential\" are used to refer to \"ζ\".\n\n\"ζ\" is directly related to the number of active charge carriers as well as their typical kinetic energy, and hence it is directly involved in determining the local properties of the material (such as electrical conductivity).\nFor this reason it is common to focus on the value of \"ζ\" when concentrating on the properties of electrons in a single, homogeneous conductive material.\nBy analogy to the energy states of a free electron, the \"ℰ\" of a state is the kinetic energy of that state and \"ϵ\" is its potential energy. With this in mind, the parameter, \"ζ\", could also be labelled the \"Fermi kinetic energy\".\n\nUnlike \"µ\", the parameter, \"ζ\", is not a constant at equilibrium, but rather varies from location to location in a material due to variations in \"ϵ\", which is determined by factors such as material quality and impurities/dopants.\nNear the surface of a semiconductor or semimetal, \"ζ\" can be strongly controlled by externally applied electric fields, as is done in a field effect transistor. In a multi-band material, \"ζ\" may even take on multiple values in a single location.\nFor example, in a piece of aluminum metal there are two conduction bands crossing the Fermi level (even more bands in other materials); each band has a different edge energy, \"ϵ\", and a different \"ζ\".\n\nThe value of \"ζ\" at zero temperature is widely known as the Fermi energy, sometimes written \"ζ\". Confusingly (again), the name \"Fermi energy\" sometimes is used to refer to \"ζ\" at non-zero temperature.\n\nThe Fermi level, \"μ\", and temperature, \"T\", are well defined constants for a solid-state device in thermodynamic equilibrium situation, such as when it is sitting on the shelf doing nothing. When the device is brought out of equilibrium and put into use, then strictly speaking the Fermi level and temperature are no longer well defined. Fortunately, it is often possible to define a quasi-Fermi level and quasi-temperature for a given location, that accurately describe the occupation of states in terms of a thermal distribution. The device is said to be in \"quasi-equilibrium\" when and where such a description is possible.\n\nThe quasi-equilibrium approach allows one to build a simple picture of some non-equilibrium effects as the electrical conductivity of a piece of metal (as resulting from a gradient of \"μ\") or its thermal conductivity (as resulting from a gradient in \"T\"). The quasi-\"μ\" and quasi-\"T\" can vary (or not exist at all) in any non-equilibrium situation, such as:\n\nIn some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution.\nOne cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be \"non-thermalized\". In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of \"μ\" and \"T\" to different bands (conduction band vs. valence band). Even then, the values of \"μ\" and \"T\" may jump discontinuously across a material interface (e.g., p–n junction) when a current is being driven, and be ill-defined at the interface itself.\n\nThe term, \"Fermi level\", is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping.\nIn these contexts, however, one may also see Fermi level used imprecisely to refer to the \"band-referenced Fermi level\", \"µ\" − \"ϵ\", called \"ζ\" above.\nIt is common to see scientists and engineers refer to \"controlling\", \"pinning\", or \"tuning\" the Fermi level inside a conductor, when they are in fact describing changes in \"ϵ\" due to doping or the field effect.\nIn fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is \"always\" fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram).\nA similar ambiguity exists between the terms, \"chemical potential\" and \"electrochemical potential\".\n\nIt is also important to note that Fermi \"level\" is not necessarily the same thing as Fermi \"energy\".\nIn the wider context of quantum mechanics, the term Fermi energy usually refers to \"the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas\".\nThis concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal.\nOn the other hand, in the fields of semiconductor physics and engineering, \"Fermi energy\" often is used to refer to the Fermi level described in this article.\n\nMuch like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences.\nWhen comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained.\nIt can therefore be helpful to explicitly name a common point to ensure that different components are in agreement.\nOn the other hand, if a reference point is inherently ambiguous (such as \"the vacuum\", see below) it will instead cause more problems.\n\nA practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth.\nSuch a conductor can be considered to be in a good thermodynamic equilibrium and so its \"µ\" is well defined.\nIt provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects.\nIt also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.\n\nIn principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies.\nThis approach is not advisable unless one is careful to define exactly where \"the vacuum\" is. The problem is that not all points in the vacuum are equivalent.\n\nAt thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials).\nThe source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum.\nJust outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).\n\nThe parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.\n\nIn cases where the \"charging effects\" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.\n\nIn this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?\nThese chemical potentials are not equivalent, , except in the thermodynamic limit.\nThe distinction is important in small systems such as those showing Coulomb blockade.\nThe parameter, , (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems.\nTo be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.\n"}
{"id": "1243432", "url": "https://en.wikipedia.org/wiki?curid=1243432", "title": "Joseph Tainter", "text": "Joseph Tainter\n\nJoseph Anthony Tainter (born December 8, 1949) is an American anthropologist and historian.\n\nTainter studied anthropology at the University of California, Berkeley and Northwestern University, where he received his Ph.D. in 1975. he holds a professorship in the Department of Environment and Society at Utah State University. His previous positions include Project Leader of Cultural Heritage Research, Rocky Mountain Forest and Range Experiment Station, Albuquerque, New Mexico and Assistant Professor of Anthropology at the University of New Mexico.\n\nTainter has written and edited many articles and monographs. His arguably best-known work, \"The Collapse of Complex Societies\" (1988), examines the collapse of Maya and Chacoan civilizations,\nand of the Western Roman Empire, in terms of network theory, energy economics and complexity theory. Tainter argues that sustainability or collapse of societies follow from the success or failure of problem-solving institutions and that societies collapse when their investments in social complexity and their \"energy subsidies\" reach a point of diminishing marginal returns. He recognizes collapse when a society involuntarily sheds a significant portion of its complexity.\n\nWith Tadeusz Patzek, he is author of \"Drilling Down: The Gulf Oil Debacle and Our Energy Dilemma\", published in 2011.\n\nJoseph Tainter is married to Bonnie Bagley and they have one child, Emmet Bagley Tainter.\n\nAccording to Tainter's \"Collapse of Complex Societies\", societies become more complex as they try to solve problems. Social complexity can be recognized by numerous differentiated and specialised social and economic roles and many mechanisms through which they are coordinated, and by reliance on symbolic and abstract communication, and the existence of a class of information producers and analysts who are not involved in primary resource production. Such complexity requires a substantial \"energy\" subsidy (meaning the consumption of resources, or other forms of wealth).\n\nWhen a society confronts a \"problem,\" such as a shortage of energy, or difficulty in gaining access to it, it tends to create new layers of bureaucracy, infrastructure, or social class to address the challenge. Tainter, who first (ch. 1) identifies seventeen examples of rapid collapse of societies, applies his model to three case studies: The Western Roman Empire, the Maya civilization, and the Chaco culture.\n\nFor example, as Roman agricultural output slowly declined and population increased, per-capita energy availability dropped. The Romans \"solved\" this problem by conquering their neighbours to appropriate their energy surpluses (in concrete forms, as metals, grain, slaves, etc.). However, as the Empire grew, the cost of maintaining communications, garrisons, civil government, etc. grew with it. Eventually, this cost grew so great that any new challenges such as invasions and crop failures could not be solved by the acquisition of more territory.\n\nIntense, authoritarian efforts to maintain cohesion by Domitian and Constantine the Great only led to an ever greater strain on the population. The empire was split into two halves, of which the western soon fragmented into smaller units. The eastern half, being wealthier, was able to survive longer, and did not collapse but instead succumbed slowly and piecemeal, because unlike the western empire it had powerful neighbors able to take advantage of its weakness.\n\nIt is often assumed that the collapse of the western Roman Empire was a catastrophe for everyone involved. Tainter points out that it can be seen as a very rational preference of individuals at the time, many of whom were actually better off. Tainter notes that in the west, local populations in many cases greeted the barbarians as liberators.\n\nTainter begins by categorizing and examining the often inconsistent explanations that have been offered for collapse in the literature. In Tainter's view, while invasions, crop failures, disease or environmental degradation may be the \"apparent\" causes of societal collapse, the ultimate cause is an economic one, inherent in the structure of society rather than in external shocks which may batter them: \"diminishing returns on investments in social complexity\". Finally, Tainter musters modern statistics to show that marginal returns on investments in energy (EROEI), education and technological innovation are diminishing today. The globalised modern world is subject to many of the same stresses that brought older societies to ruin.\n\nHowever, Tainter is not entirely apocalyptic: \"When some new input to an economic system is brought on line, whether a technical innovation or an energy subsidy, it will often have the potential at least temporarily to raise marginal productivity\" (p. 124). Thus, barring continual conquest of your neighbors (which is always subject to diminishing returns), innovation that increases productivity is – in the long run – the only way out of the dilemma of declining marginal returns on added investments in complexity.\n\nAnd, in his final chapters, Tainter discusses why modern societies may not be able to choose to collapse: because surrounding them are other complex societies which will in some way absorb a collapsed region or prevent a general collapse; the Mayan and Chaocan regions had no powerful complex neighbors and so could collapse for centuries or millennia, as could the Western Roman Empire - but the Eastern Roman Empire, bordered as it was by the Parthian/Sassanid Empire, did not have the option of devolving into simpler smaller entities.\n\nHis paper \"Complexity, Problem Solving, and Sustainable Societies\" (1996) focuses on the energy cost of problem solving, and the energy-complexity relation in manmade systems.\n\n\n\n\n"}
{"id": "55537002", "url": "https://en.wikipedia.org/wiki?curid=55537002", "title": "Karadiyana Power Station", "text": "Karadiyana Power Station\n\nThe Karadiyana Power Station (also referred to as the Fairway Power Station after its developers) is a municipal solid waste-fired thermal power station currently under construction at a site in Karadiyana, Sri Lanka. Together with the KCHT Power Station, it is one of two projects that won the bid by the Urban Development Authority, from a pool of 121 bidders. Construction of the facility began on 23 August 2017 with a completion slated for mid-2019. The estimated cost of the project is approximately .\n\nThe power station will be operated by , a subsidiary of the . It will use of waste, with the generated power sold to the state-owned Ceylon Electricity Board at a rate of generated. The residual bottom ash from the process will be used for road construction and other uses, while the unusable fly ash (amounting to 2%) will be disposed of at predesignated locations.\n\n"}
{"id": "19579197", "url": "https://en.wikipedia.org/wiki?curid=19579197", "title": "Kuroda Dam", "text": "Kuroda Dam\n\nThe Kuroda Dam (黒田ダム) is a dam in the city of Toyota in the Aichi Prefecture of Japan.\n"}
{"id": "17556", "url": "https://en.wikipedia.org/wiki?curid=17556", "title": "Laser", "text": "Laser\n\nA laser is a device that emits light through a process of optical amplification based on the stimulated emission of electromagnetic radiation. The term \"laser\" originated as an acronym for \"light amplification by stimulated emission of radiation\". The first laser was built in 1960 by Theodore H. Maiman at Hughes Research Laboratories, based on theoretical work by Charles Hard Townes and Arthur Leonard Schawlow.\n\nA laser differs from other sources of light in that it emits light \"coherently\". Spatial coherence allows a laser to be focused to a tight spot, enabling applications such as laser cutting and lithography. Spatial coherence also allows a laser beam to stay narrow over great distances (collimation), enabling applications such as laser pointers and lidar. Lasers can also have high temporal coherence, which allows them to emit light with a very narrow spectrum, i.e., they can emit a single color of light. Alternatively, temporal coherence can be used to produce pulses of light with a broad spectrum but durations as short as a femtosecond (\"ultrashort pulses\").\n\nLasers are used in optical disk drives, laser printers, barcode scanners, DNA sequencing instruments, fiber-optic and free-space optical communication, laser surgery and skin treatments, cutting and welding materials, military and law enforcement devices for marking targets and measuring range and speed, and in laser lighting displays for entertainment.\n\nLasers are distinguished from other light sources by their coherence. Spatial coherence is typically expressed through the output being a narrow beam, which is diffraction-limited. Laser beams can be focused to very tiny spots, achieving a very high irradiance, or they can have very low divergence in order to concentrate their power at a great distance. Temporal (or longitudinal) coherence implies a polarized wave at a single frequency, whose phase is correlated over a relatively great distance (the coherence length) along the beam. A beam produced by a thermal or other incoherent light source has an instantaneous amplitude and phase that vary randomly with respect to time and position, thus having a short coherence length.\n\nLasers are characterized according to their wavelength in a vacuum. Most \"single wavelength\" lasers actually produce radiation in several \"modes\" with slightly different wavelengths. Although temporal coherence implies monochromaticity, there are lasers that emit a broad spectrum of light or emit different wavelengths of light simultaneously. Some lasers are not single spatial mode and have light beams that diverge more than is required by the diffraction limit. All such devices are classified as \"lasers\" based on their method of producing light, i.e., stimulated emission. Lasers are employed where light of the required spatial or temporal coherence can not be produced using simpler technologies.\n\nThe word \"laser\" started as an acronym for \"light amplification by stimulated emission of radiation\". In this usage, the term \"light\" includes electromagnetic radiation of any frequency, not only visible light, hence the terms \"infrared laser\", \"ultraviolet laser\", \"X-ray laser\" and \"gamma-ray laser\". Because the microwave predecessor of the laser, the maser, was developed first, devices of this sort operating at microwave and radio frequencies are referred to as \"masers\" rather than \"microwave lasers\" or \"radio lasers\". In the early technical literature, especially at Bell Telephone Laboratories, the laser was called an optical maser; this term is now obsolete.\n\nA laser that produces light by itself is technically an optical oscillator rather than an optical amplifier as suggested by the acronym. It has been humorously noted that the acronym LOSER, for \"light oscillation by stimulated emission of radiation\", would have been more correct. With the widespread use of the original acronym as a common noun, optical amplifiers have come to be referred to as \"laser amplifiers\", notwithstanding the apparent redundancy in that designation.\n\nThe back-formed verb \"to lase\" is frequently used in the field, meaning \"to produce laser light,\" especially in reference to the gain medium of a laser; when a laser is operating it is said to be \"lasing.\" Further use of the words \"laser\" and \"maser\" in an extended sense, not referring to laser technology or devices, can be seen in usages such as \"astrophysical maser\" and \"atom laser\".\n\nA laser consists of a gain medium, a mechanism to energize it, and something to provide optical feedback. The gain medium is a material with properties that allow it to amplify light by way of stimulated emission. Light of a specific wavelength that passes through the gain medium is amplified (increases in power).\n\nFor the gain medium to amplify light, it needs to be supplied with energy in a process called pumping. The energy is typically supplied as an electric current or as light at a different wavelength. Pump light may be provided by a flash lamp or by another laser.\n\nThe most common type of laser uses feedback from an optical cavity—a pair of mirrors on either end of the gain medium. Light bounces back and forth between the mirrors, passing through the gain medium and being amplified each time. Typically one of the two mirrors, the output coupler, is partially transparent. Some of the light escapes through this mirror. Depending on the design of the cavity (whether the mirrors are flat or curved), the light coming out of the laser may spread out or form a narrow beam. In analogy to electronic oscillators, this device is sometimes called a \"laser oscillator\".\n\nMost practical lasers contain additional elements that affect properties of the emitted light, such as the polarization, wavelength, and shape of the beam.\n\nElectrons and how they interact with electromagnetic fields are important in our understanding of chemistry and physics.\n\nIn the classical view, the energy of an electron orbiting an atomic nucleus is larger for orbits further from the nucleus of an atom. However, quantum mechanical effects force electrons to take on discrete positions in orbitals. Thus, electrons are found in specific energy levels of an atom, two of which are shown below:\n\nWhen an electron absorbs energy either from light (photons) or heat (phonons), it receives that incident quantum of energy. But transitions are only allowed in between discrete energy levels such as the two shown above.\nThis leads to emission lines and absorption lines.\n\nWhen an electron is excited from a lower to a higher energy level, it will not stay that way forever.\nAn electron in an excited state may decay to a lower energy state which is not occupied, according to a particular time constant characterizing that transition. When such an electron decays without external influence, emitting a photon, that is called \"spontaneous emission\". The phase associated with the photon that is emitted is random. A material with many atoms in such an excited state may thus result in radiation which is very spectrally limited (centered around one wavelength of light), but the individual photons would have no common phase relationship and would emanate in random directions. This is the mechanism of fluorescence and thermal emission.\n\nAn external electromagnetic field at a frequency associated with a transition can affect the quantum mechanical state of the atom. As the electron in the atom makes a transition between two stationary states (neither of which shows a dipole field), it enters a transition state which does have a dipole field, and which acts like a small electric dipole, and this dipole oscillates at a characteristic frequency. In response to the external electric field at this frequency, the probability of the atom entering this transition state is greatly increased. Thus, the rate of transitions between two stationary states is enhanced beyond that due to spontaneous emission. Such a transition to the higher state is called absorption, and it destroys an incident photon (the photon's energy goes into powering the increased energy of the higher state). A transition from the higher to a lower energy state, however, produces an additional photon; this is the process of stimulated emission.\n\nThe gain medium is put into an excited state by an external source of energy. In most lasers this medium consists of a population of atoms which have been excited into such a state by means of an outside light source, or an electrical field which supplies energy for atoms to absorb and be transformed into their excited states.\n\nThe gain medium of a laser is normally a material of controlled purity, size, concentration, and shape, which amplifies the beam by the process of stimulated emission described above. This material can be of any state: gas, liquid, solid, or plasma. The gain medium absorbs pump energy, which raises some electrons into higher-energy (\"excited\") quantum states. Particles can interact with light by either absorbing or emitting photons. Emission can be spontaneous or stimulated. In the latter case, the photon is emitted in the same direction as the light that is passing by. When the number of particles in one excited state exceeds the number of particles in some lower-energy state, population inversion is achieved and the amount of stimulated emission due to light that passes through is larger than the amount of absorption. Hence, the light is amplified. By itself, this makes an optical amplifier. When an optical amplifier is placed inside a resonant optical cavity, one obtains a laser oscillator.\n\nIn a few situations it is possible to obtain lasing with only a single pass of EM radiation through the gain medium, and this produces a laser beam without any need for a resonant or reflective cavity (see for example nitrogen laser). Thus, reflection in a resonant cavity is usually required for a laser, but is not absolutely necessary.\n\nThe optical resonator is sometimes referred to as an \"optical cavity\", but this is a misnomer: lasers use open resonators as opposed to the literal cavity that would be employed at microwave frequencies in a maser.\nThe resonator typically consists of two mirrors between which a coherent beam of light travels in both directions, reflecting back on itself so that an average photon will pass through the gain medium repeatedly before it is emitted from the output aperture or lost to diffraction or absorption.\nIf the gain (amplification) in the medium is larger than the resonator losses, then the power of the recirculating light can rise exponentially. But each stimulated emission event returns an atom from its excited state to the ground state, reducing the gain of the medium. With increasing beam power the net gain (gain minus loss) reduces to unity and the gain medium is said to be saturated. In a continuous wave (CW) laser, the balance of pump power against gain saturation and cavity losses produces an equilibrium value of the laser power inside the cavity; this equilibrium determines the operating point of the laser. If the applied pump power is too small, the gain will never be sufficient to overcome the cavity losses, and laser light will not be produced. The minimum pump power needed to begin laser action is called the \"lasing threshold\". The gain medium will amplify any photons passing through it, regardless of direction; but only the photons in a spatial mode supported by the resonator will pass more than once through the medium and receive substantial amplification.\n\nIn most lasers, lasing begins with stimulated emission amplifying random spontaneously emitted photons present in the gain medium. Stimulated emission produces light that matches the input signal in wavelength, phase, and polarization. This, combined with the filtering effect of the optical resonator gives laser light its characteristic coherence, and may give it uniform polarization and monochromaticity, depending on the resonator's design. Some lasers use a separate injection seeder to start the process off with a beam that is already highly coherent. This can produce beams with a narrower spectrum than would otherwise be possible.\n\nMany lasers produce a beam that can be approximated as a Gaussian beam; such beams have the minimum divergence possible for a given beam diameter. Some lasers, particularly high-power ones, produce multimode beams, with the transverse modes often approximated using Hermite–Gaussian or Laguerre-Gaussian functions. Some high power lasers use a flat-topped profile known as a \"tophat beam\". Unstable laser resonators (not used in most lasers) produce fractal-shaped beams. Specialized optical systems can produce more complex beam geometries, such as Bessel beams and optical vortexes.\n\nNear the \"waist\" (or focal region) of a laser beam, it is highly \"collimated\": the wavefronts are planar, normal to the direction of propagation, with no beam divergence at that point. However, due to diffraction, that can only remain true well within the Rayleigh range. The beam of a single transverse mode (gaussian beam) laser eventually diverges at an angle which varies inversely with the beam diameter, as required by diffraction theory. Thus, the \"pencil beam\" directly generated by a common helium–neon laser would spread out to a size of perhaps 500 kilometers when shone on the Moon (from the distance of the earth). On the other hand, the light from a semiconductor laser typically exits the tiny crystal with a large divergence: up to 50°. However even such a divergent beam can be transformed into a similarly collimated beam by means of a lens system, as is always included, for instance, in a laser pointer whose light originates from a laser diode. That is possible due to the light being of a single spatial mode. This unique property of laser light, spatial coherence, cannot be replicated using standard light sources (except by discarding most of the light) as can be appreciated by comparing the beam from a flashlight (torch) or spotlight to that of almost any laser.\n\nA laser beam profiler is used to measure the intensity profile, width, and divergence of laser beams.\n\nDiffuse reflection of a laser beam from a matte surface produces a speckle pattern with interesting properties.\n\nThe mechanism of producing radiation in a laser relies on stimulated emission, where energy is extracted from a transition in an atom or molecule. This is a quantum phenomenon discovered by Einstein who derived the relationship between the A coefficient describing spontaneous emission and the B coefficient which applies to absorption and stimulated emission. However, in the case of the free electron laser, atomic energy levels are not involved; it appears that the operation of this rather exotic device can be explained without reference to quantum mechanics.\n\nA laser can be classified as operating in either continuous or pulsed mode, depending on whether the power output is essentially continuous over time or whether its output takes the form of pulses of light on one or another time scale. Of course even a laser whose output is normally continuous can be intentionally turned on and off at some rate in order to create pulses of light. When the modulation rate is on time scales much slower than the cavity lifetime and the time period over which energy can be stored in the lasing medium or pumping mechanism, then it is still classified as a \"modulated\" or \"pulsed\" continuous wave laser. Most laser diodes used in communication systems fall in that category.\n\nSome applications of lasers depend on a beam whose output power is constant over time. Such a laser is known as \"continuous wave\" (\"CW\"). Many types of lasers can be made to operate in continuous wave mode to satisfy such an application. Many of these lasers actually lase in several longitudinal modes at the same time, and beats between the slightly different optical frequencies of those oscillations will, in fact, produce amplitude variations on time scales shorter than the round-trip time (the reciprocal of the frequency spacing between modes), typically a few nanoseconds or less. In most cases, these lasers are still termed \"continuous wave\" as their output power is steady when averaged over any longer time periods, with the very high-frequency power variations having little or no impact in the intended application. (However, the term is not applied to mode-locked lasers, where the \"intention\" is to create very short pulses at the rate of the round-trip time.)\n\nFor continuous wave operation, it is required for the population inversion of the gain medium to be continually replenished by a steady pump source. In some lasing media, this is impossible. In some other lasers, it would require pumping the laser at a very high continuous power level which would be impractical or destroy the laser by producing excessive heat. Such lasers cannot be run in CW mode.\n\nPulsed operation of lasers refers to any laser not classified as continuous wave, so that the optical power appears in pulses of some duration at some repetition rate. This encompasses a wide range of technologies addressing a number of different motivations. Some lasers are pulsed simply because they cannot be run in continuous mode.\n\nIn other cases, the application requires the production of pulses having as large an energy as possible. Since the pulse energy is equal to the average power divided by the repetition rate, this goal can sometimes be satisfied by lowering the rate of pulses so that more energy can be built up in between pulses. In laser ablation, for example, a small volume of material at the surface of a work piece can be evaporated if it is heated in a very short time, while supplying the energy gradually would allow for the heat to be absorbed into the bulk of the piece, never attaining a sufficiently high temperature at a particular point.\n\nOther applications rely on the peak pulse power (rather than the energy in the pulse), especially in order to obtain nonlinear optical effects. For a given pulse energy, this requires creating pulses of the shortest possible duration utilizing techniques such as Q-switching.\n\nThe optical bandwidth of a pulse cannot be narrower than the reciprocal of the pulse width. In the case of extremely short pulses, that implies lasing over a considerable bandwidth, quite contrary to the very narrow bandwidths typical of CW lasers. The lasing medium in some \"dye lasers\" and \"vibronic solid-state lasers\" produces optical gain over a wide bandwidth, making a laser possible which can thus generate pulses of light as short as a few femtoseconds (10 s).\n\nIn a Q-switched laser, the population inversion is allowed to build up by introducing loss inside the resonator which exceeds the gain of the medium; this can also be described as a reduction of the quality factor or 'Q' of the cavity. Then, after the pump energy stored in the laser medium has approached the maximum possible level, the introduced loss mechanism (often an electro- or acousto-optical element) is rapidly removed (or that occurs by itself in a passive device), allowing lasing to begin which rapidly obtains the stored energy in the gain medium. This results in a short pulse incorporating that energy, and thus a high peak power.\n\nA mode-locked laser is capable of emitting extremely short pulses on the order of tens of picoseconds down to less than 10 femtoseconds. These pulses will repeat at the round trip time, that is, the time that it takes light to complete one round trip between the mirrors comprising the resonator. Due to the Fourier limit (also known as energy-time uncertainty), a pulse of such short temporal length has a spectrum spread over a considerable bandwidth. Thus such a gain medium must have a gain bandwidth sufficiently broad to amplify those frequencies. An example of a suitable material is titanium-doped, artificially grown sapphire (Ti:sapphire) which has a very wide gain bandwidth and can thus produce pulses of only a few femtoseconds duration.\n\nSuch mode-locked lasers are a most versatile tool for researching processes occurring on extremely short time scales (known as femtosecond physics, femtosecond chemistry and ultrafast science), for maximizing the effect of nonlinearity in optical materials (e.g. in second-harmonic generation, parametric down-conversion, optical parametric oscillators and the like). Due to the large peak power and the ability to generate phase-stabilized trains of ultrafast laser pulses, mode-locking ultrafast lasers underpin precision metrology and spectroscopy applications.\n\nAnother method of achieving pulsed laser operation is to pump the laser material with a source that is itself pulsed, either through electronic charging in the case of flash lamps, or another laser which is already pulsed. Pulsed pumping was historically used with dye lasers where the inverted population lifetime of a dye molecule was so short that a high energy, fast pump was needed. The way to overcome this problem was to charge up large capacitors which are then switched to discharge through flashlamps, producing an intense flash. Pulsed pumping is also required for three-level lasers in which the lower energy level rapidly becomes highly populated preventing further lasing until those atoms relax to the ground state. These lasers, such as the excimer laser and the copper vapor laser, can never be operated in CW mode.\n\nIn 1917, Albert Einstein established the theoretical foundations for the laser and the maser in the paper \"Zur Quantentheorie der Strahlung\" (On the Quantum Theory of Radiation) via a re-derivation of Max Planck's law of radiation, conceptually based upon probability coefficients (Einstein coefficients) for the absorption, spontaneous emission, and stimulated emission of electromagnetic radiation. In 1928, Rudolf W. Ladenburg confirmed the existence of the phenomena of stimulated emission and negative absorption. In 1939, Valentin A. Fabrikant predicted the use of stimulated emission to amplify \"short\" waves. In 1947, Willis E. Lamb and R.C. Retherford found apparent stimulated emission in hydrogen spectra and effected the first demonstration of stimulated emission. In 1950, Alfred Kastler (Nobel Prize for Physics 1966) proposed the method of optical pumping, experimentally confirmed, two years later, by Brossel, Kastler, and Winter.\n\nIn 1951, Joseph Weber submitted a paper on using stimulated emissions to make a microwave amplifier to the June 1952 Institute of Radio Engineers Vacuum Tube Research Conference at Ottawa, Ontario, Canada. After this presentation, RCA asked Weber to give a seminar on this idea, and Charles Hard Townes asked him for a copy of the paper.\n\nIn 1953, Charles Hard Townes and graduate students James P. Gordon and Herbert J. Zeiger produced the first microwave amplifier, a device operating on similar principles to the laser, but amplifying microwave radiation rather than infrared or visible radiation. Townes's maser was incapable of continuous output. Meanwhile, in the Soviet Union, Nikolay Basov and Aleksandr Prokhorov were independently working on the quantum oscillator and solved the problem of continuous-output systems by using more than two energy levels. These gain media could release stimulated emissions between an excited state and a lower excited state, not the ground state, facilitating the maintenance of a population inversion. In 1955, Prokhorov and Basov suggested optical pumping of a multi-level system as a method for obtaining the population inversion, later a main method of laser pumping.\n\nTownes reports that several eminent physicists—among them Niels Bohr, John von Neumann, and Llewellyn Thomas—argued the maser violated Heisenberg's uncertainty principle and hence could not work. Others such as Isidor Rabi and Polykarp Kusch expected that it would be impractical and not worth the effort. In 1964 Charles H. Townes, Nikolay Basov, and Aleksandr Prokhorov shared the Nobel Prize in Physics, \"for fundamental work in the field of quantum electronics, which has led to the construction of oscillators and amplifiers based on the maser–laser principle\".\n\nIn 1957, Charles Hard Townes and Arthur Leonard Schawlow, then at Bell Labs, began a serious study of the infrared laser. As ideas developed, they abandoned infrared radiation to instead concentrate upon visible light. The concept originally was called an \"optical maser\". In 1958, Bell Labs filed a patent application for their proposed optical maser; and Schawlow and Townes submitted a manuscript of their theoretical calculations to the \"Physical Review\", published that year in Volume 112, Issue No. 6.\n\nSimultaneously, at Columbia University, graduate student Gordon Gould was working on a doctoral thesis about the energy levels of excited thallium. When Gould and Townes met, they spoke of radiation emission, as a general subject; afterwards, in November 1957, Gould noted his ideas for a \"laser\", including using an open resonator (later an essential laser-device component). Moreover, in 1958, Prokhorov independently proposed using an open resonator, the first published appearance (in the USSR) of this idea. Elsewhere, in the U.S., Schawlow and Townes had agreed to an open-resonator laser design – apparently unaware of Prokhorov's publications and Gould's unpublished laser work.\n\nAt a conference in 1959, Gordon Gould published the term LASER in the paper \"The LASER, Light Amplification by Stimulated Emission of Radiation\". Gould's linguistic intention was using the \"-aser\" word particle as a suffix – to accurately denote the spectrum of the light emitted by the LASER device; thus x-rays: \"xaser\", ultraviolet: \"uvaser\", et cetera; none established itself as a discrete term, although \"raser\" was briefly popular for denoting radio-frequency-emitting devices.\n\nGould's notes included possible applications for a laser, such as spectrometry, interferometry, radar, and nuclear fusion. He continued developing the idea, and filed a patent application in April 1959. The U.S. Patent Office denied his application, and awarded a patent to Bell Labs, in 1960. That provoked a twenty-eight-year lawsuit, featuring scientific prestige and money as the stakes. Gould won his first minor patent in 1977, yet it was not until 1987 that he won the first significant patent lawsuit victory, when a Federal judge ordered the U.S. Patent Office to issue patents to Gould for the optically pumped and the gas discharge laser devices. The question of just how to assign credit for inventing the laser remains unresolved by historians.\n\nOn May 16, 1960, Theodore H. Maiman operated the first functioning laser at Hughes Research Laboratories, Malibu, California, ahead of several research teams, including those of Townes, at Columbia University, Arthur Schawlow, at Bell Labs, and Gould, at the TRG (Technical Research Group) company. Maiman's functional laser used a solid-state flashlamp-pumped synthetic ruby crystal to produce red laser light, at 694 nanometers wavelength; however, the device only was capable of pulsed operation, because of its three-level pumping design scheme. Later that year, the Iranian physicist Ali Javan, and William R. Bennett, and Donald Herriott, constructed the first gas laser, using helium and neon that was capable of continuous operation in the infrared (U.S. Patent 3,149,290); later, Javan received the Albert Einstein Award in 1993. Basov and Javan proposed the semiconductor laser diode concept. In 1962, Robert N. Hall demonstrated the first \"laser diode\" device, which was made of gallium arsenide and emitted in the near-infrared band of the spectrum at 850 nm. Later that year, Nick Holonyak, Jr. demonstrated the first semiconductor laser with a visible emission. This first semiconductor laser could only be used in pulsed-beam operation, and when cooled to liquid nitrogen temperatures (77 K). In 1970, Zhores Alferov, in the USSR, and Izuo Hayashi and Morton Panish of Bell Telephone Laboratories also independently developed room-temperature, continual-operation diode lasers, using the heterojunction structure.\n\nSince the early period of laser history, laser research has produced a variety of improved and specialized laser types, optimized for different performance goals, including:\nand this research continues to this day.\n\nIn 2017, researchers at TU Delft demonstrated an AC Josephson junction microwave laser. Since the laser operates in the superconducting regime, it is more stable than other semiconductor-based lasers. The device has potential for applications in quantum computing. In 2017, researchers at TU Munich demonstrated the smallest mode locking laser capable of emitting pairs of phase-locked picosecond laser pulses with a repetition frequency up to 200 GHz.\n\nIn 2017, researchers from the Physikalisch-Technische Bundesanstalt (PTB), together with US researchers from JILA, a joint institute of the National Institute of Standards and Technology (NIST) and the University of Colorado Boulder, established a new world record by developing an erbium-doped fiber laser with a linewidth of only 10 millihertz.\n\nFollowing the invention of the HeNe gas laser, many other gas discharges have been found to amplify light coherently.\nGas lasers using many different gases have been built and used for many purposes. The helium–neon laser (HeNe) is able to operate at a number of different wavelengths, however the vast majority are engineered to lase at 633 nm; these relatively low cost but highly coherent lasers are extremely common in optical research and educational laboratories. Commercial carbon dioxide (CO) lasers can emit many hundreds of watts in a single spatial mode which can be concentrated into a tiny spot. This emission is in the thermal infrared at 10.6 µm; such lasers are regularly used in industry for cutting and welding. The efficiency of a CO laser is unusually high: over 30%. Argon-ion lasers can operate at a number of lasing transitions between 351 and 528.7 nm. Depending on the optical design one or more of these transitions can be lasing simultaneously; the most commonly used lines are 458 nm, 488 nm and 514.5 nm. A nitrogen transverse electrical discharge in gas at atmospheric pressure (TEA) laser is an inexpensive gas laser, often home-built by hobbyists, which produces rather incoherent UV light at 337.1 nm. Metal ion lasers are gas lasers that generate deep ultraviolet wavelengths. Helium-silver (HeAg) 224 nm and neon-copper (NeCu) 248 nm are two examples. Like all low-pressure gas lasers, the gain media of these lasers have quite narrow oscillation linewidths, less than 3 GHz (0.5 picometers), making them candidates for use in fluorescence suppressed Raman spectroscopy.\n\nChemical lasers are powered by a chemical reaction permitting a large amount of energy to be released quickly. Such very high power lasers are especially of interest to the military, however continuous wave chemical lasers at very high power levels, fed by streams of gasses, have been developed and have some industrial applications. As examples, in the hydrogen fluoride laser (2700–2900 nm) and the deuterium fluoride laser (3800 nm) the reaction is the combination of hydrogen or deuterium gas with combustion products of ethylene in nitrogen trifluoride.\n\nExcimer lasers are a special sort of gas laser powered by an electric discharge in which the lasing medium is an excimer, or more precisely an exciplex in existing designs. These are molecules which can only exist with one atom in an excited electronic state. Once the molecule transfers its excitation energy to a photon, its atoms are no longer bound to each other and the molecule disintegrates. This drastically reduces the population of the lower energy state thus greatly facilitating a population inversion. Excimers currently used are all ; noble gasses are chemically inert and can only form compounds while in an excited state. Excimer lasers typically operate at ultraviolet wavelengths with major applications including semiconductor photolithography and LASIK eye surgery. Commonly used excimer molecules include ArF (emission at 193 nm), KrCl (222 nm), KrF (248 nm), XeCl (308 nm), and XeF (351 nm).\nThe molecular fluorine laser, emitting at 157 nm in the vacuum ultraviolet is sometimes referred to as an excimer laser, however this appears to be a misnomer inasmuch as F is a stable compound.\n\nSolid-state lasers use a crystalline or glass rod which is \"doped\" with ions that provide the required energy states. For example, the first working laser was a ruby laser, made from ruby (chromium-doped corundum). The population inversion is actually maintained in the dopant. These materials are pumped optically using a shorter wavelength than the lasing wavelength, often from a flashtube or from another laser. The usage of the term \"solid-state\" in laser physics is narrower than in typical use. Semiconductor lasers (laser diodes) are typically \"not\" referred to as solid-state lasers.\n\nNeodymium is a common dopant in various solid-state laser crystals, including yttrium orthovanadate (Nd:YVO), yttrium lithium fluoride () and yttrium aluminium garnet (). All these lasers can produce high powers in the infrared spectrum at 1064 nm. They are used for cutting, welding and marking of metals and other materials, and also in spectroscopy and for pumping dye lasers. These lasers are also commonly frequency doubled, tripled or quadrupled to produce 532 nm (green, visible), 355 nm and 266 nm (UV) beams, respectively. Frequency-doubled diode-pumped solid-state (DPSS) lasers are used to make bright green laser pointers.\n\nYtterbium, holmium, thulium, and erbium are other common \"dopants\" in solid-state lasers. Ytterbium is used in crystals such as Yb:YAG, Yb:KGW, Yb:KYW, Yb:SYS, Yb:BOYS, Yb:CaF, typically operating around 1020–1050 nm. They are potentially very efficient and high powered due to a small quantum defect. Extremely high powers in ultrashort pulses can be achieved with Yb:YAG. Holmium-doped YAG crystals emit at 2097 nm and form an efficient laser operating at infrared wavelengths strongly absorbed by water-bearing tissues. The Ho-YAG is usually operated in a pulsed mode, and passed through optical fiber surgical devices to resurface joints, remove rot from teeth, vaporize cancers, and pulverize kidney and gall stones.\n\nTitanium-doped sapphire (Ti:sapphire) produces a highly tunable infrared laser, commonly used for spectroscopy. It is also notable for use as a mode-locked laser producing ultrashort pulses of extremely high peak power.\n\nThermal limitations in solid-state lasers arise from unconverted pump power that heats the medium. This heat, when coupled with a high thermo-optic coefficient (d\"n\"/d\"T\") can cause thermal lensing and reduce the quantum efficiency. Diode-pumped thin disk lasers overcome these issues by having a gain medium that is much thinner than the diameter of the pump beam. This allows for a more uniform temperature in the material. Thin disk lasers have been shown to produce beams of up to one kilowatt.\n\nSolid-state lasers or laser amplifiers where the light is guided due to the total internal reflection in a single mode optical fiber are instead called fiber lasers. Guiding of light allows extremely long gain regions providing good cooling conditions; fibers have high surface area to volume ratio which allows efficient cooling. In addition, the fiber's waveguiding properties tend to reduce thermal distortion of the beam. Erbium and ytterbium ions are common active species in such lasers.\n\nQuite often, the fiber laser is designed as a double-clad fiber. This type of fiber consists of a fiber core, an inner cladding and an outer cladding. The index of the three concentric layers is chosen so that the fiber core acts as a single-mode fiber for the laser emission while the outer cladding acts as a highly multimode core for the pump laser. This lets the pump propagate a large amount of power into and through the active inner core region, while still having a high numerical aperture (NA) to have easy launching conditions.\n\nPump light can be used more efficiently by creating a fiber disk laser, or a stack of such lasers.\n\nFiber lasers have a fundamental limit in that the intensity of the light in the fiber cannot be so high that optical nonlinearities induced by the local electric field strength can become dominant and prevent laser operation and/or lead to the material destruction of the fiber. This effect is called photodarkening. In bulk laser materials, the cooling is not so efficient, and it is difficult to separate the effects of photodarkening from the thermal effects, but the experiments in fibers show that the photodarkening can be attributed to the formation of long-living color centers.\n\nPhotonic crystal lasers are lasers based on nano-structures that provide the mode confinement and the density of optical states (DOS) structure required for the feedback to take place. They are typical micrometer-sized and tunable on the bands of the photonic crystals.\n\nSemiconductor lasers are diodes which are electrically pumped. Recombination of electrons and holes created by the applied current introduces optical gain. Reflection from the ends of the crystal form an optical resonator, although the resonator can be external to the semiconductor in some designs.\n\nCommercial laser diodes emit at wavelengths from 375 nm to 3500 nm. Low to medium power laser diodes are used in laser pointers, laser printers and CD/DVD players. Laser diodes are also frequently used to optically pump other lasers with high efficiency. The highest power industrial laser diodes, with power up to 20 kW, are used in industry for cutting and welding. External-cavity semiconductor lasers have a semiconductor active medium in a larger cavity. These devices can generate high power outputs with good beam quality, wavelength-tunable narrow-linewidth radiation, or ultrashort laser pulses.\n\nIn 2012, Nichia and OSRAM developed and manufactured commercial high-power green laser diodes (515/520 nm), which compete with traditional diode-pumped solid-state lasers.\n\nVertical cavity surface-emitting lasers (VCSELs) are semiconductor lasers whose emission direction is perpendicular to the surface of the wafer. VCSEL devices typically have a more circular output beam than conventional laser diodes. As of 2005, only 850 nm VCSELs are widely available, with 1300 nm VCSELs beginning to be commercialized, and 1550 nm devices an area of research. VECSELs are external-cavity VCSELs. Quantum cascade lasers are semiconductor lasers that have an active transition between energy \"sub-bands\" of an electron in a structure containing several quantum wells.\n\nThe development of a silicon laser is important in the field of optical computing. Silicon is the material of choice for integrated circuits, and so electronic and silicon photonic components (such as optical interconnects) could be fabricated on the same chip. Unfortunately, silicon is a difficult lasing material to deal with, since it has certain properties which block lasing. However, recently teams have produced silicon lasers through methods such as fabricating the lasing material from silicon and other semiconductor materials, such as indium(III) phosphide or gallium(III) arsenide, materials which allow coherent light to be produced from silicon. These are called hybrid silicon laser. Recent developments have also shown the use of monolithically integrated nanowire lasers directly on silicon for optical interconnects, paving the way for chip level applications. These heterostructure nanowire lasers capable of optical interconnects in silicon are also capable of emitting pairs of phase-locked picosecond pulses with a repetition frequency up to 200 GHz, allowing for on-chip optical signal processing. Another type is a Raman laser, which takes advantage of Raman scattering to produce a laser from materials such as silicon.\n\nLasing without maintaining the medium excited into a population inversion was demonstrated in 1992 in sodium gas and again in 1995 in rubidium gas by various international teams. This was accomplished by using an external maser to induce \"optical transparency\" in the medium by introducing and destructively interfering the ground electron transitions between two paths, so that the likelihood for the ground electrons to absorb any energy has been cancelled.\n\nDye lasers use an organic dye as the gain medium. The wide gain spectrum of available dyes, or mixtures of dyes, allows these lasers to be highly tunable, or to produce very short-duration pulses (on the order of a few femtoseconds). Although these tunable lasers are mainly known in their liquid form, researchers have also demonstrated narrow-linewidth tunable emission in dispersive oscillator configurations incorporating solid-state dye gain media. In their most prevalent form these solid state dye lasers use dye-doped polymers as laser media.\n\nFree-electron lasers, or FELs, generate coherent, high power radiation that is widely tunable, currently ranging in wavelength from microwaves through terahertz radiation and infrared to the visible spectrum, to soft X-rays. They have the widest frequency range of any laser type. While FEL beams share the same optical traits as other lasers, such as coherent radiation, FEL operation is quite different. Unlike gas, liquid, or solid-state lasers, which rely on bound atomic or molecular states, FELs use a relativistic electron beam as the lasing medium, hence the term \"free-electron\".\n\nThe pursuit of a high-quantum-energy laser using transitions between isomeric states of an atomic nucleus has been the subject of wide-ranging academic research since the early 1970s. Much of this is summarized in three review articles. This research has been international in scope, but mainly based in the former Soviet Union and the United States. While many scientists remain optimistic that a breakthrough is near, an operational gamma-ray laser is yet to be realized.\n\nSome of the early studies were directed toward short pulses of neutrons exciting the upper isomer state in a solid so the gamma-ray transition could benefit from the line-narrowing of Mössbauer effect. In conjunction, several advantages were expected from two-stage pumping of a three-level system. It was conjectured that the nucleus of an atom, embedded in the near field of a laser-driven coherently-oscillating electron cloud would experience a larger dipole field than that of the driving laser. Furthermore, nonlinearity of the oscillating cloud would produce both spatial and temporal harmonics, so nuclear transitions of higher multipolarity could also be driven at multiples of the laser frequency.\n\nIn September 2007, the BBC News reported that there was speculation about the possibility of using positronium annihilation to drive a very powerful gamma ray laser. Dr. David Cassidy of the University of California, Riverside proposed that a single such laser could be used to ignite a nuclear fusion reaction, replacing the banks of hundreds of lasers currently employed in inertial confinement fusion experiments.\n\nSpace-based X-ray lasers pumped by a nuclear explosion have also been proposed as antimissile weapons. Such devices would be one-shot weapons.\n\nLiving cells have been used to produce laser light. The cells were genetically engineered to produce green fluorescent protein (GFP). The GFP is used as the laser's \"gain medium\", where light amplification takes place. The cells were then placed between two tiny mirrors, just 20 millionths of a meter across, which acted as the \"laser cavity\" in which light could bounce many times through the cell. Upon bathing the cell with blue light, it could be seen to emit directed and intense green laser light.\n\nWhen lasers were invented in 1960, they were called \"a solution looking for a problem\". Since then, they have become ubiquitous, finding utility in thousands of highly varied applications in every section of modern society, including consumer electronics, information technology, science, medicine, industry, law enforcement, entertainment, and the military. Fiber-optic communication using lasers is a key technology in modern communications, allowing services such as the Internet.\n\nThe first use of lasers in the daily lives of the general population was the supermarket barcode scanner, introduced in 1974. The laserdisc player, introduced in 1978, was the first successful consumer product to include a laser but the compact disc player was the first laser-equipped device to become common, beginning in 1982 followed shortly by laser printers.\n\nSome other uses are:\n\nIn 2004, excluding diode lasers, approximately 131,000 lasers were sold with a value of US$2.19 billion. In the same year, approximately 733 million diode lasers, valued at $3.20 billion, were sold.\n\nLasers have many uses in medicine, including laser surgery (particularly eye surgery), laser healing, kidney stone treatment, ophthalmoscopy, and cosmetic skin treatments such as acne treatment, cellulite and striae reduction, and hair removal.\n\nLasers are used to treat cancer by shrinking or destroying tumors or precancerous growths. They are most commonly used to treat superficial cancers that are on the surface of the body or the lining of internal organs. They are used to treat basal cell skin cancer and the very early stages of others like cervical, penile, vaginal, vulvar, and non-small cell lung cancer. Laser therapy is often combined with other treatments, such as surgery, chemotherapy, or radiation therapy. Laser-induced interstitial thermotherapy (LITT), or interstitial laser photocoagulation, uses lasers to treat some cancers using hyperthermia, which uses heat to shrink tumors by damaging or killing cancer cells. Lasers are more precise than traditional surgery methods and cause less damage, pain, bleeding, swelling, and scarring. A disadvantage is that surgeons must have specialized training. It may be more expensive than other treatments.\n\nMany types of laser can potentially be used as incapacitating weapons, through their ability to produce temporary or permanent vision loss when aimed at the eyes. The degree, character, and duration of vision impairment caused by eye exposure to laser light varies with the power of the laser, the wavelength(s), the collimation of the beam, the exact orientation of the beam, and the duration of exposure. Lasers of even a fraction of a watt in power can produce immediate, permanent vision loss under certain conditions, making such lasers potential non-lethal but incapacitating weapons. The extreme handicap that laser-induced blindness represents makes the use of lasers even as non-lethal weapons morally controversial, and weapons designed to cause permanent blindness have been banned by the Protocol on Blinding Laser Weapons. Weapons designed to cause temporary blindness, known as dazzlers, are used by military and sometimes law enforcement organizations. Incidents of pilots being exposed to lasers while flying have prompted aviation authorities to implement special procedures to deal with such hazards. See Lasers and aviation safety for more on this topic.\n\nLaser weapons capable of directly damaging or destroying a target in combat are still in the experimental stage. The general idea of laser-beam weaponry is to hit a target with a train of brief pulses of light. The rapid evaporation and expansion of the surface causes shockwaves that damage the target. The power needed to project a high-powered laser beam of this kind is beyond the limit of current mobile power technology, thus favoring chemically powered gas dynamic lasers. Example experimental systems include MIRACL and the Tactical High Energy Laser.\nThroughout the 2000s, the United States Air Force worked on the Boeing YAL-1, an airborne laser mounted in a Boeing 747. It was intended to be used to shoot down incoming ballistic missiles over enemy territory. In March 2009, Northrop Grumman claimed that its engineers in Redondo Beach had successfully built and tested an electrically powered solid state laser capable of producing a 100-kilowatt beam, powerful enough to destroy an airplane. According to Brian Strickland, manager for the United States Army's Joint High Power Solid State Laser program, an electrically powered laser is capable of being mounted in an aircraft, ship, or other vehicle because it requires much less space for its supporting equipment than a chemical laser. However, the source of such a large electrical power in a mobile application remained unclear. Ultimately, the project was deemed to be infeasible, and was cancelled in December 2011, with the Boeing YAL-1 prototype being stored and eventually dismantled.\n\nThe United States Navy is developing a laser weapon referred to as the Laser Weapon System or LaWS.\n\nIn recent years, some hobbyists have taken interests in lasers. Lasers used by hobbyists are generally of class IIIa or IIIb (see Safety), although some have made their own class IV types. However, compared to other hobbyists, laser hobbyists are far less common, due to the cost and potential dangers involved. Due to the cost of lasers, some hobbyists use inexpensive means to obtain lasers, such as salvaging laser diodes from broken DVD players (red), Blu-ray players (violet), or even higher power laser diodes from CD or DVD burners.\n\nHobbyists also have been taking surplus pulsed lasers from retired military applications and modifying them for pulsed holography. Pulsed Ruby and pulsed YAG lasers have been used.\n\nDifferent applications need lasers with different output powers. Lasers that produce a continuous beam or a series of short pulses can be compared on the basis of their average power. Lasers that produce pulses can also be characterized based on the \"peak\" power of each pulse. The peak power of a pulsed laser is many orders of magnitude greater than its average power. The average output power is always less than the power consumed.\n\nExamples of pulsed systems with high peak power:\n\nEven the first laser was recognized as being potentially dangerous. Theodore Maiman characterized the first laser as having a power of one \"Gillette\" as it could burn through one Gillette razor blade. Today, it is accepted that even low-power lasers with only a few milliwatts of output power can be hazardous to human eyesight when the beam hits the eye directly or after reflection from a shiny surface. At wavelengths which the cornea and the lens can focus well, the coherence and low divergence of laser light means that it can be focused by the eye into an extremely small spot on the retina, resulting in localized burning and permanent damage in seconds or even less time.\n\nLasers are usually labeled with a safety class number, which identifies how dangerous the laser is:\nThe indicated powers are for visible-light, continuous-wave lasers. For pulsed lasers and invisible wavelengths, other power limits apply. People working with class 3B and class 4 lasers can protect their eyes with safety goggles which are designed to absorb light of a particular wavelength.\n\nInfrared lasers with wavelengths longer than about 1.4 micrometers are often referred to as \"eye-safe\", because the cornea tends to absorb light at these wavelengths, protecting the retina from damage. The label \"eye-safe\" can be misleading, however, as it applies only to relatively low power continuous wave beams; a high power or Q-switched laser at these wavelengths can burn the cornea, causing severe eye damage, and even moderate power lasers can injure the eye.\n\nLasers can be a hazard to both civil and miliatary aviation, due to the potential to temporarily distract or blind pilots. See Lasers and aviation safety for more on this topic.\n\n\n\n"}
{"id": "50880511", "url": "https://en.wikipedia.org/wiki?curid=50880511", "title": "Laúca Dam", "text": "Laúca Dam\n\nThe Lauca Dam is a gravity dam currently being constructed on the Cuanza River in Angola. It is situated on the borders of Cuanza Norte, Cuanza Sul and Malanje Provinces. Construction began in 2012. The first generator of the 2,069.5 MW power station was commissioned on August 4th 2017, when the power plant was officially inaugurated by President José Eduardo dos Santos. The dam and power station will cost US$4.3 billion, funded in part by the Government of Brazil. When complete, it will be the largest power station in the country.\n\nThe tall roller-compacted concrete dam will withhold a reservoir of . The reservoir will supply two hydroelectric power stations, a main and an ancillary. The main will contain six 334 MW Francis turbine-generators and the ancillary a single 65.5 MW Francis unit. The ancillary power station will operate most often, to maintain a minimum ecological flow of the river.\n"}
{"id": "12644763", "url": "https://en.wikipedia.org/wiki?curid=12644763", "title": "Lists of ecoregions by country", "text": "Lists of ecoregions by country\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "15354874", "url": "https://en.wikipedia.org/wiki?curid=15354874", "title": "Long-lived fission product", "text": "Long-lived fission product\n\nLong-lived fission products (LLFPs) are radioactive materials with a long half-life (more than 200,000 years) produced by nuclear fission of uranium and plutonium.\n\nNuclear fission produces fission products, as well as actinides from nuclear fuel nuclei that capture neutrons but fail to fission, and activation products from neutron activation of reactor or environmental materials.\n\nThe high short-term radioactivity of spent nuclear fuel is primarily from fission products with short half-life.\nThe radioactivity in the fission product mixture is mostly short-lived isotopes such as I and Ba, after about four months Ce, Zr/Nb and Sr take the largest share, while after about two or three years the largest share is taken by Ce/Pr, Ru/Rh and Pm.\nNote that in the case of a release of radioactivity from a power reactor or used fuel, only some elements are released. As a result, the isotopic signature of the radioactivity is very different from an open air nuclear detonation where all the fission products are dispersed.\n\nAfter several years of cooling, most radioactivity is from the fission products caesium-137 and strontium-90, which are each produced in about 6% of fissions, and have half-lives of about 30 years. Other fission products with similar half-lives have much lower fission product yields, lower decay energy, and several (Sm, Eu, Cd) are also quickly destroyed by neutron capture while still in the reactor, so are not responsible for more than a tiny fraction of the radiation production at any time. Therefore, in the period from several years to several hundred years after use, radioactivity of spent fuel can be modeled simply as exponential decay of the Cs and Sr. These are sometimes known as medium-lived fission products.\n\nKrypton-85, the 3rd most active MLFP, is a noble gas which is allowed to escape during current nuclear reprocessing; however, its inertness means that it does not concentrate in the environment, but diffuses to a uniform low concentration in the atmosphere. Spent fuel in the U.S. and some other countries is not likely to be reprocessed until decades after use, and by that time most of the Kr will have decayed.\n\nAfter Cs and Sr have decayed to low levels, the bulk of radioactivity from spent fuel come not from fission products but actinides, notably plutonium-239 (half-life 24 ka), plutonium-240 (6.56 ka), americium-241 (432 years), americium-243 (7.37 ka), curium-245 (8.50 ka), and curium-246 (4.73 ka). These can be recovered by nuclear reprocessing (either before or after most Cs and Sr decay) and fissioned, offering the possibility of greatly reducing waste radioactivity in the time scale of about 10 to 10 years. Pu is usable as fuel in existing thermal reactors, but some minor actinides like Am, as well as the non-fissile and less-fertile isotope plutonium-242, are better destroyed in fast reactors, accelerator-driven subcritical reactors, or fusion reactors.\n\nOn scales greater than 10 years, fission products, chiefly Tc, again represent a significant proportion of the remaining, though lower radioactivity, along with longer-lived actinides like neptunium-237 and plutonium-242, if those have not been destroyed.\n\nThe most abundant long-lived fission products have total decay energy around 100-300 keV, only part of which appears in the beta particle; the rest is lost to a neutrino that has no effect. In contrast, actinides undergo multiple alpha decays, each with decay energy around 4-5 MeV.\n\nOnly seven fission products have long half-lives, and these are much longer than 30 years, in the range of 200,000 to 16 million years. These are known as long-lived fission products (LLFP). Two or three have relatively high yields of about 6%, while the rest appear at much lower yields. (This list of seven excludes isotopes with very slow decay and half-lives longer than the age of the universe, which are effectively stable and already found in nature; as well as a few nuclides like technetium-98 and samarium-146 that are \"shadowed\" from beta decay and can only occur as direct fission products, not as beta decay products of more neutron-rich initial fission products. The shadowed fission products have yields on the order of one millionth as much as iodine-129.)\n\nThe first three have similar half-lives, between 200 thousand and 300 thousand years; the last four have longer half-lives, in the low millions of years.\n\nIn total, the other six LLFPs, in thermal reactor spent fuel, initially release only a bit more than 10% as much energy per unit time as Tc-99 for U-235 fission, or 25% as much for 65% U-235+35% Pu-239. About 1000 years after fuel use, radioactivity from the medium-lived fission products Cs-137 and Sr-90 drops below the level of radioactivity from Tc-99 or LLFPs in general. (Actinides, if not removed, will be emitting more radioactivity than either at this point.) By about 1 million years, Tc-99 radioactivity will have declined below that of Zr-93, though immobility of the latter means it is probably still a lesser hazard. By about 3 million years, Zr-93 decay energy will have declined below that of I-129.\n\nNuclear transmutation is under consideration as a disposal method, primarily for Tc-99 and I-129 as these both represent the greatest biohazards and have the greatest neutron capture cross sections, although transmutation is still slow compared to fission of actinides in a reactor. Transmutation has also been considered for Cs-135, but is almost certainly not worthwhile for the other LLFPs.\n"}
{"id": "14909006", "url": "https://en.wikipedia.org/wiki?curid=14909006", "title": "Magnetic structure", "text": "Magnetic structure\n\nThe term magnetic structure of a material pertains to the ordered arrangement of magnetic spins, typically within an ordered crystallographic lattice. Its study is a branch of solid-state physics.\n\nMost solid materials are non-magnetic, that is, they do not display a magnetic structure. Due to the Pauli exclusion principle, each state is occupied by electrons of opposing spins, so that the charge density is compensated everywhere and the spin degree of freedom is trivial. Still, such materials typically do show a weak magnetic behaviour, e.g. due to Pauli paramagnetism or Langevin or Landau diamagnetism. \n\nThe more interesting case is when the material's electron spontaneously break above-mentioned symmetry. For ferromagnetism in the ground state, there is a common spin quantization axis and a global excess of electrons of a given spin quantum number, there are more electrons pointing in one direction than in the other, giving a macroscopic magnetization (typically, the majority electrons are chosen to point up). In the most simple (collinear) cases of antiferromagnetism, there is still a common quantization axis, but the electronic spins are pointing alternatingly up and down, leading again to cancellation of the macroscopic magnetization. However, specifically in the case of frustration of the interactions, the resulting structures can become much more complicated, with inherently three-dimensional orientations of the local spins. Finally, ferrimagnetism as prototypically displayed by magnetite is in some sense an intermediate case: here the magnetization is globally uncompensated as in ferromagnetism, but the local magnetization points in different directions. \n\nThe above discussion pertains to the ground state structure. Of course, finite temperatures lead to excitations of the spin configuration. Here two extreme points of view can be contrasted: in the Stoner picture of magnetism (also called itinerant magnetism), the electronic states are delocalized, and their mean-field interaction leads to the symmetry breaking. In this view, with increasing temperature the local magnetization would thus decrease homogeneously, as single delocalized electrons are moved from the up- to the down-channel. On the other hand, in the local-moment case the electronic states are localized to specific atoms, giving atomic spins, which interact only over a short range and typically are analyzed with the Heisenberg model. Here, finite temperatures lead to a deviation of the atomic spins' orientations from the ideal configuration, thus for a ferromagnet also decreasing the macroscopic magnetization.\n\nSuch ordering can be studied by observing the magnetic susceptibility as a function of temperature and/or the size of the applied magnetic field, but a truly three-dimensional picture of the arrangement of the spins is best obtained by means of neutron diffraction. Neutrons are primarily scattered by the nuclei of the atoms in the structure. At a temperature above the ordering point of the magnetic moments, where the material behaves as a paramagnetic one, neutron diffraction will therefore give a picture of the crystallographic structure only. Below the ordering point, e.g. the Néel temperature of an antiferromagnet or the Curie-point of a ferromagnet the neutrons will also experience scattering from the magnetic moments because they themselves possess spin. The intensities of the Bragg reflections will therefore change. In fact in some cases entirely new Bragg-reflections will occur if the unit cell of the ordering is larger than that of the crystallographic structure. This is a form of superstructure formation. Thus the symmetry of the total structure may well differ from the crystallographic substructure. It needs to be described by one of the 1651 magnetic (Shubnikov) groups rather than one of the non-magnetic space groups.\n\nAlthough ordinary X-ray diffraction is 'blind' to the arrangement of the spins, it has become possible to use a special form of X-ray diffraction to study magnetic structure. If a wavelength is selected that is close to an absorption edge of one of elements contained in the materials the scattering becomes anomalous and this component to the scattering is (somewhat) sensitive to the non-spherical shape of the outer electrons of an atom with an unpaired spin. This means that this type of anomalous X-ray diffraction does contain information of the desired type.\n\nOnly three elements are ferromagnetic at room temperature and pressure: iron, cobalt, and nickel. This is because their Curie temperature, Tc, is higher than room temperature (Tc > 298K). There are also elements which are paramagnetic at room temperature, but become ferromagnetic at low temperatures, including Dysprosium, Erbium, Holmium, Terbium, and Thulium. There is also antiferromagnetic ordering, which becomes disorderd above the Néel temperature. Those elements which become superconductors exhibit superdiamagnetism below a critical temperature.\n"}
{"id": "6135427", "url": "https://en.wikipedia.org/wiki?curid=6135427", "title": "Man-Thing (film)", "text": "Man-Thing (film)\n\nMan-Thing is a 2005 Australian-American horror film directed by Brett Leonard and featuring the Marvel Comics swamp creature Man-Thing created by Stan Lee, Roy Thomas, and Gerry Conway. The plot is based on a storyline by Steve Gerber, who wrote the best-known series of \"Man-Thing\" comics.\nAgents of an oil tycoon vanish while exploring a swamp marked for drilling. The local sheriff investigates and faces a Seminole legend come to life: Man-Thing, a shambling swamp-monster.\nThe film appeared on the Sci Fi Channel in 2005 under the Sci Fi Pictures label. It starred Matthew Le Nevez, Rachael Taylor, and Jack Thompson. The film was released theatrically in a handful of International markets. The film was a box office bomb grossing only $1 million and received generally negative reviews from critics.\n\nAt Dark Waters, a Native American sacred land containing an enigmatic swamp spirit, a teenager is murdered by a plant-like monster. The following day, young replacement sheriff Kyle Williams reaches Bywater and meets with deputy sheriff Fraser, who tells him the previous sheriff is among 47 missing persons since oil tycoon Fred Schist bought the ancient tribal lands from shaman and Seminole chieftain Ted Sallis, the first to disappear. Schist claimed Sallis had sold the lands legally and then escaped with the money. Schist then asked the sheriff for help: Local protestors opposed his perfectly legal activities, and mestizo scoundrel Renee Laroque was sabotaging his facilities. Williams investigates this while trying to find an explanation for the missing people, some of which were found brutally murdered with plants growing from inside their bodies. Photographer Mike Ploog and shaman Pete Horn tell Williams local legends about the guardian spirit, suggesting that it could be real.\n\nAs sabotage and murder continue, Williams investigates the swamp with Fraser and finds the previous sheriff's corpse. Medical examiner Val Mayerik admits that the previous sheriff had ordered him to file the deaths as alligator attacks, even if Mayerik believed otherwise.\n\nWilliams and Fraser try to track Laroque, who lives in the swamp, by using a canoe. At the same time, Schist sends the Thibadeux brothers, local thugs, to track and murder Laroque. The monster in the swamp finds the Thibadeux and kills them. Williams is ensnared by Laroque, who admits having helped Schist buy the lands. Laroque claims that Sallis was opposed to the sale; Laroque insists that the guardian spirit would keep on murdering until Schist stops desecrating the sacred swamp. Fraser tries to help Laroque, but the Man-Thing timely appears and murders Fraser; Laroque knocks Williams down and escapes. Williams wakes up and finds Ploog, who has blurry pictures of the monster; the sheriff seizes the photographs and forbids Ploog to come back to the swamp.\n\nThe following day, Williams interviews Horn and Schist, with the help of schoolteacher Teri Richards' help. Williams starts having romantic feelings for Richards. Horn goes to the swamp and tries to stop the Man-Thing with prayers and sacrificing his own life. The monster kills Horn, but is not otherwise affected by his efforts. That night, Mayerik autopsies the old sheriff and finds a bullet. He tries to tell Williams, but he is back at the swamp, unreachable. Mayerik tells Richards, and she goes to the swamp to tell Williams. Meanwhile, Ploog had returned to the swamp, trying to get a picture of the monster. Instead he startles Fred Schist, who was in the swamp to murder Laroque. Schist shoots and kills Ploog. Soon afterward, Laroque ambushes and defeats Schist's son and minion Jake.\n\nWilliams finds Ploog's corpse and reasons that Schist murdered Ploog. He then meets Richards, who tells him about Mayerik's autopsy. Williams concludes that Schist is guilty of several murders, trying to incriminate Laroque simply to avoid punishment. According to Schist's confession to Laroque, he murdered Sallis and buried him in Dark Waters. Due to the magic embedded in the soil, Sallis returned as the Man-Thing. Richards reveals that she can guide Williams to Laroque's lair, but the Man-Thing starts chasing them. He chases them to the drilling tower at Dark Waters. In the tower, Schist is leveling his weapon at Laroque in an attempt to prevent Laroque from blowing it away with dynamite. Laroque nonetheless tries to detonate his bomb and is shot and wounded by Schist; Schist then wounds Williams.\n\nHowever, the Man-Thing arrives and brutally murders Schist by filling his body with oil. The Man-Thing then moves toward Williams and Richards. Laroque sacrifices himself shouting at the monster and blowing the bomb. The monster survives the flames, but then is absorbed back to the land.\n\nA photo of Stan Lee can be seen on the board of \"Missing People\" who have been presumably killed by Man-Thing.\n\nIn 2000, Marvel Entertainment entered into a joint venture agreement with Artisan Entertainment to turn at least 15 Marvel superhero franchises into live-action films, television series, direct-to-video films and internet projects. These franchises included an adaptation of Man-Thing.\nPlans to make a film about Man-Thing were first announced in 2001. It was variously considered for a direct-to-video release, or a theatrical release. After the success of Bryan Singer's \"X-Men\" (2000), M. Night Shyamalan's \"Unbreakable\" (2000) and Sam Raimi's \"Spider-Man\" (2002) the film was moved to a theatrical release to exploit on the success of superheroes.\n\nOn October 27, 2003 it was reported that Artisan Entertainment, which had partnered with Marvel Enterprises in production The Punisher and Man-Thing films, was being purchase by Lionsgate Films. In February 2004, the film production and distribution company Lionsgate merged with Artisan Entertainment and received the film rights to Iron Fist, Black Widow, Man-Thing and the Punisher. In January 2004, producer Avi Arad said Man-Thing was more of a departure from the original comic than were Marvel's other film character in that it was a horror film with a menacing central character. On April 2004, the film had been completed, with the finished print received and waiting to be tested with audiences, after which an exact release date would be determined. The film was rated R for violence, grisly images, language and some sexuality by the Motion Picture Association of America (MPAA).\n\nThe film had a production budget of $30 million. Avi Arad, then CEO of Marvel Studios, admitted it was a mistake not keeping tabs on the production, as it was being filmed so far away in Australia. He stated \"The one hiccup we had was the one project we didn't micromanage. We were not going to the Outback, there was so much going on. We will never do that again. We should never have trusted anybody that far away without our supervision. Thankfully it was a small movie and not a disaster. If we were there and on top of it, it would have been a[n] amazing movie. I look at the {horror} genre, and I think 'Sh--, I can't believe this'. We've learned our lesson.\"\n\n\"Man-Thing\" was shot completely on location in Sydney, Australia. Shooting locations included Wisemans Ferry, Serenity Cove Studios at Kurnell for exterior swamp scenes and Homebush Bay. The principle photography for \"Man-Thing\" concluded in 2003.\n\nMarvel Studios producer Avi Arad said \"the lead character in the Man-Thing movie would be a combination of prosthetics and computer-generated effects.\nFrom the outset, Man-Thing was intended to be a prosthetic, CG-enhanced creature\", Arad told The Continuum during a visit to Marvel Studios So there was a great deal of R&D... There's positional stuff happening on location, on the set, but at the same time the stuff you don't currently see in camera was always engineered to be enhanced by digital effects. So when you see the movie, hopefully the line is pretty blurry. It's not an all-CG creature.\n\nSpecial effects makeup was by the Make-Up Effects Group of Australia.\nThe Man-Thing was built as a full-size creature suit, portrayed by Mark Stevens, a 7'1\" (216 cm) Australian actor, ex-wrestler and stuntman.\n\nAlthough no full-digital Man-Thing model was made due to budgetary constraints, the suit was combined with digital moving branches and tendrils for certain sequences, also well as digital augmentation for the eyes.\n\nThe band AzUR (DOG Productions' Wayne and Luke, joined with Bec And Freddie) recorded the song The \"Man-Thing Lives Again\" that played over the end credits of the film. It was supposed to be released as a promotional video, but since the film was in a constant state of flux (financial, script, etc...) and was not going to go to theatres (as intended), the music video was pulled for lack of budget. Marvel did not want to leak advance images of the set and creature costume before the film's eventual release.One of the band members has worked on the footage and uploaded a remix on YouTube.\n\nThe \"Man-Thing\" album was composed by Roger Mason and was released on March 17, 2009. The soundtrack consists of 21 tracks. Its duration is over an hour long. The album was released by Nice Spot.\n\"Man-Thing\" had originally been scheduled for release date on August 27, 2004.\nThe US release date was set for Halloween (October 31) 2004, but when Marvel Enterprises released its second quarter financial report, \"Man-Thing\" was included in the 2005 line-up with a release date to be decided. Reportedly, the film was so bad that the test audience walked out before it was finished. So, Marvel put it back on video in the United States, since it would not be bankable in a domestic release. The film was released it internationally in places like Russia and the United Arab Emirates. \"Man-Thing\" was released on April 30, 2005, as a \"Sci-Fi Original\" on the Sci-Fi Channel.\n\nThe character's film rights, along with the other Marvel characters whose film rights were previously acquired by Artisan Entertainment, have reverted to Marvel.\n\nThe film premiered in Singapore on April 21, 2005.\n\nThe film was released on DVD on June 14, 2005 in the United States.\n\nIt was released as a two-disc DVD in Region 2 format.\n\nWhile the film was released direct to television in North America, it played theatrically in three International markets where it accumulated $1,123,136 in box-office grosses. On April 28, 2005, \"Man-Thing\" opened in Russia and four other Post-Soviet states: Armenia, Belarus, Kazakhstan, and Moldova. The film opened on October 26, 2005, in the United Arab Emirates.\nFinally, the film opened in Spain on March 3, 2006.\n\nUpon its release, \"Man-Thing\" received generally negative response and reviews from film critics. On the film-critics aggregator Rotten Tomatoes, it earned 17% positive reviews based on 6 reviews.\n\nFelix Vasquez from \"Cinema Crazed\" gave the film a negative review, writing, \"While the special effects are really good, and the directing is decent, this just ends up becoming a really bad movie botching a really good concept\". David Nusair from \"Reel Film Reviews\" awarded the film 1/4 stars, calling it \"the worst comic book movie ever made\". Jon Condit from Dread Central gave the film a rating of 1.5 out of 5, writing, \"Maybe in more capable hands than Brett Leonard’s this could have been a creepy, albeit cheesy monster movie, but instead it just ends up falling flat.\" David Cornelius from \"eFilmCritic.com\" gave the film 2/5 stars, stating that the film was \" too lame to be genuinely entertaining, not stupid enough throughout to be laughable\". Adam Tyner from DVD Talk awarded the film 2/5 stars, calling it \"thoroughly mediocre\". Andrew Smith of \"Popcorn Pictures\" rated the film a 5/10, calling it \"[a] Wasted effort but watchable anyway\".\n\n"}
{"id": "58197976", "url": "https://en.wikipedia.org/wiki?curid=58197976", "title": "Manny Calonzo", "text": "Manny Calonzo\n\nManny Calonzo is an environmental advocate. Aided by the Ecological Waste Coalition of the Philippines (EcoWaste) – an organization he founded – he was responsible for the Philippine government’s law to nationally ban lead paint (culminating in 85% lead-certified paint there since 2017) and the creation of a third-party certification program for paint manufacturers. His anti-toxic campaign extended to other substances in footwear items which potentially decreased fertility in men and harmed fetuses. In 2018 he was one of seven international Goldman Environmental Prize winners – a decade after he began his national campaign – which resulted in the protection of millions of Filipino children. Calonzo is a consultant on the Global Lead Paint Elimination Campaign at the International Persistent Organic Pollutants Elimination Network.\n"}
{"id": "16056281", "url": "https://en.wikipedia.org/wiki?curid=16056281", "title": "Mayapuri", "text": "Mayapuri\n\nMayapuri is a locality in West Delhi. It used to be a major hub of small scale industries, but following recent government sanctions, most of the heavy metal industries moved out. The place is now a combination of residential flats (DDA and private), light metal factories and automobile service stations.\n\nThe area is mostly residential now, with major localities built by the Delhi Development Authority (DDA). These include Hari Enclave, Vikrant Enclave, MIG Flats and Maya Enclave. There are some major landmarks in this area like the Food Corporation of India, Metal Forging, New Era Public School and Din Dayal Upadhyay Hospital.\n\nVatika Apartments, MIG Flats, is a DDA Colony that was completed and allotted to owners in 1977. There are approximately 432 flats in this society, with A, B, C, D and E streets. Most of B, C and E street have privately held apartments. D Street belongs to the Government of India and the current tenants are serving officers of the Govt. of India. A Street also has governmental holding in several apartments, and these belong to the Customs and Excise Wing of the GoI. Vatika Apartments is a mid-premium gated locality and has its own Mother Dairy (Milk Supply booth) and a Local Shopping Centre.\n\nMayapuri is served by three malls of Rajouri Garden - City Square Mall, TDI Mall and West Gate Mall, and by the upmarket Pacific Mall in Tagore Garden/Subhash Nagar. All these malls are within 3–4 km of Mayapuri.\n\nMayapuri is approximately 15 km from Connaught Place and New Delhi Railway Station, and about 14 km from the Domestic Airport Terminal. It has a Maruti Showroom and workshop, and workshops of Hyundai and Honda as well at a close distance.\n\nMayapuri is also one of the major bus terminals for the Delhi Transport Corporation (DTC). Delhi Metro's Rajouri Garden and Subhash Nagar stations are close by. The petrol pumps at Beriwala Bagh, Police Station and Subhash Nagar are within easy reach. Several schools are in close proximity as are several banks like UCO Bank, Punjab National Bank, Canara Bank, ICIC Bank, HDFC Bank and many more.\n\nPark Plaza, Iris Hometel and Signature Grand are the three premium hotels in Mayapuri. They are situated in Hari Nagar, but that is just as good as Mayapuri. These are about 1 km from Vatika Apartments, Mayapuri.\n\nIn April 2010, the locality of Mayapuri was affected by a serious radiological accident. An AECL Gammacell 220 research irradiator owned by Delhi University since 1968, but unused since 1985, was sold at auction to a scrap metal dealer in Mayapuri on 26 February 2010. The orphan source arrived at a scrap yard in Mayapuri during March, where it was dismantled by workers unaware of the hazardous nature of the device. The cobalt-60 source was cut into eleven pieces. The smallest of the fragments was taken by Ajay Jain who kept it in his wallet, two fragments were moved to a nearby shop, while the remaining eight remained in the scrap yard. All of the sources were recovered by mid-April and transported to the Narora Atomic Power Station, where it was claimed that all radioactive material originally contained within the device was accounted for. The material remains in the custody of the Department of Atomic Energy\n\nEight people were hospitalised in AIIMS as a result of radiation exposure, where one later died. The event was rated level 4 out of 7 on the International Nuclear Events Scale.\n\nOne of the main business at Mayapuri is the recycling of metal scraps and sale of salvage vehicle parts. It is, arguably, the biggest market for used automotive and industrial spare parts in India. Many traders from all over India come here to sell or purchase old auto parts. Many small workshops specialised in different metals are active in the Mayapuri area. The safety of the scrap yards became a concern after the radiological accident which occurred in April 2010. The area is not equipped with radiation detectors or portics, despite being a common practice in steel recycling factories in the US and in most of the European countries. The presence of toxic heavy metals and of harmful chemicals in the waste generated by these activities presents a direct menace for the health of several thousands of people living in the area.\n\nMayapuri is home to the following key landmarks:\n\nFollowing are some key surrounding areas, some are small localities within Mayapuri.\n\n\n\n"}
{"id": "14546072", "url": "https://en.wikipedia.org/wiki?curid=14546072", "title": "Melting-point depression", "text": "Melting-point depression\n\nThe melting temperature of a bulk material is not dependent on its size. However, as the dimensions of a material decrease towards the atomic scale, the melting temperature scales with the material dimensions. The decrease in melting temperature can be on the order of tens to hundreds of degrees for metals with nanometer dimensions.\n\nMelting-point depression is most evident in nanowires, nanotubes and nanoparticles, which all melt at lower temperatures than bulk amounts of the same material. Changes in melting point occur because nanoscale materials have a much larger surface-to-volume ratio than bulk materials, drastically altering their thermodynamic and thermal properties.\n\nThis article focuses on nanoparticles because researchers have compiled a large amount of size-dependent melting data for near spherical nanoparticles. Nanoparticles are easiest to study due their ease of fabrication and simplified conditions for theoretical modeling. The melting temperature of a nanoparticle decreases sharply as the particle reaches critical diameter, usually < 50 nm for common engineering metals. Figure 1 shows the shape of a typical melting curve for a metal nanoparticle as a function of its diameter.\n\nMelting point depression is a very important issue for applications involving nanoparticles, as it decreases the functional range of the solid phase. Nanoparticles are currently used or proposed for prominent roles in catalyst, sensor, medicinal, optical, magnetic, thermal, electronic, and alternative energy applications. Nanoparticles must be in the solid state to function at elevated temperatures in several of these applications.\n\nTwo techniques allow measurement of the melting point of nanoparticle. The electron beam of a transmission electron microscope (TEM) can be used to melt nanoparticles. The melting temperature is estimated from the beam intensity, while changes in the diffraction conditions to indicate phase transition from solid to liquid. This method allows direct viewing of nanoparticles as they melt, making it possible to test and characterize samples with a wider distribution of particle sizes. The TEM limits the pressure range at which melting point depression can be tested.\n\nMore recently, researchers developed nanocalorimeters that directly measure the enthalpy and melting temperature of nanoparticles. Nanocalorimeters provide the same data as bulk calorimeters, however additional calculations must account for the presence of the substrate supporting the particles. A narrow size distribution of nanoparticles is required since the procedure does not allow users to view the sample during the melting process. There is no way to characterize the exact size of melted particles during experiment.\n\nMelting point depression was predicted in 1909 by Pawlow.\n\nTakagi first observed melting point depression of several types of metal nanoparticles in 1954. A variable intensity electron beam from a transmission electron microscope melted metal nanoparticles in early experiments. Diffraction patterns changed from characteristic crystalline patterns to liquid patterns as the small particles melted, allowing Takagi to estimate the melting temperature from the electron beam energy.\n\nNanoparticles have a much greater surface to volume ratio than bulk materials. The increased surface to volume ratio means surface atoms have a much greater effect on chemical and physical properties of a nanoparticle. Surface atoms bind in the solid phase with less cohesive energy because they have fewer neighboring atoms in close proximity compared to atoms in the bulk of the solid. Each chemical bond an atom shares with a neighboring atom provides cohesive energy, so atoms with fewer bonds and neighboring atoms have lower cohesive energy. The average cohesive energy per atom of a nanoparticle has been theoretically calculated as a function of particle size according to Equation 1.\n\nformula_1\n\nWhere: D = nanoparticle size\n\nAs Equation 1 shows, the effective cohesive energy of a nanoparticle approaches that of the bulk material as the material extends beyond atomic size range (D»d).\n\nAtoms located at or near the surface of the nanoparticle have reduced cohesive energy due to a reduced number of cohesive bonds. An atom experiences an attractive force with all nearby atoms according to the Lennard-Jones potential. The Lennard-Jones pair-potential shown in Figure 2 models the cohesive energy between atoms as a function of separation distance.\n\nThe cohesive energy of an atom is directly related to the thermal energy required to free the atom from the solid. According to Lindemann’s criterion, the melting temperature of a material is proportional to its cohesive energy, a (T=Ca). Since atoms near the surface have fewer bonds and reduced cohesive energy, they require less energy to free from the solid phase. Melting point depression of high surface to volume ratio materials results from this effect. For the same reason, surfaces of bulk materials can melt at lower temperatures than the bulk material.\n\nThe theoretical size-dependent melting point of a material can be calculated through classical thermodynamic analysis. The result is the Gibbs–Thomson equation shown in Equation 2.\n\nformula_2\n\nWhere: T = Bulk Melting temperature\n\nA normalized Gibbs–Thomson Equation for gold nanoparticles is plotted in Figure 1, and the shape of the curve is in general agreement with those obtained through experiment.\n\nEquation 2 gives the general relation between the melting point of a metal nanoparticle and its diameter. However, recent work indicates the melting point of semiconductor and covalently bonded nanoparticles may have a different dependence on particle size. The covalent character of the bonds changes the melting physics of these materials. Researchers have demonstrated that Equation 3 more accurately models melting point depression in covalently bonded materials.\n\nformula_3\nWhere: T=Bulk Melting temperature\n\nEquation 3 indicates that melting point depression is less pronounced in covalent nanoparticles due to the quadratic nature of particle size dependence in the melting Equation.\n\nThe specific melting process for nanoparticles is currently unknown. The scientific community currently accepts several mechanisms as possible models of nanoparticle melting. Each of the corresponding models effectively match experimental data for melting of nanoparticles. Three of the four models detailed below derive the melting temperature in a similar form using different approaches based on classical thermodynamics.\n\nThe liquid drop model (LDM) assumes that an entire nanoparticle transitions from solid to liquid at a single temperature. This feature distinguishes the model, as the other models predict melting of the nanoparticle surface prior to the bulk atoms. If the LDM is true, a solid nanoparticle should function over a greater temperature range than other models predict. The LDM assumes that the surface atoms of a nanoparticle dominate the properties of all atoms in the particle. The cohesive energy of the particle is identical for all atoms in the nanoparticle.\n\nThe LDM represents the binding energy of a nanoparticles as function of the free energies of the volume and surface. Equation 4 gives the normalized, size dependent melting temperature of a material according to the liquid-drop model.\n\nformula_4\n\nWhere: σ=solid-vapor interface energy\n\nThe liquid shell nucleation model (LSN) predicts that a surface layer of atoms melts prior to the bulk of the particle. The melting temperature of a nanoparticle is a function of its radius of curvature according to the LSN. Large nanoparticles melt at greater temperatures as a result of their larger radius of curvature.\n\nThe model calculates melting conditions as a function of two competing order parameters using Landau potentials. One order parameter represents a solid nanoparticle, while the other represents the liquid phase. Each of the order parameters is a function of particle radius.\n\nThe parabolic Landau potentials for the liquid and solid phases are calculated at a given temperature, with the lesser Landau a potential assumed to be the equilibrium state at any point in the particle. In the temperature range of surface melting, the results show that the Landau curve of the ordered state is favored near the center of the particle while the Landau curve of the disordered state is smaller near the surface of the particle.\n\nThe Landau curves intersect at a specific radius from the center of the particle. The distinct intersection of the potentials means the LSN predicts a sharp, unmoving interface between the solid and liquid phases at a given temperature. The exact thickness of the liquid layer at a given temperature is the equilibrium point between the competing Landau potentials.\n\nEquation 5 gives the condition at which an entire nanoparticle melts according to the LSN model.\n\nformula_5\nWhere: d=atomic diameter\n\nThe liquid nucleation and growth model (LNG) treats nanoparticle melting as a surface initiated process. The surface melts initially, and the liquid-solid interface quickly advances through the entire nanoparticle. The LNG defines melting conditions through the Gibbs-Duhem relations, yielding a melting temperature function dependent on the interfacial energies between the solid and liquid phases, volumes and surface areas of each phase, and size of the nanoparticle. The model calculations show that the liquid phase forms at lower temperatures for smaller nanoparticles. Once the liquid phase forms, the free energy conditions quickly change and favor melting. Equation 6 gives the melting conditions for a spherical nanoparticle according to the LNG model.\n\nformula_6\n\nThe bond-order-length-strength (BOLS) model employs an atomistic approach to explain melting point depression. The model focuses on the cohesive energy of individual atoms rather than a classical thermodynamic approach. The BOLS model calculates the melting temperature for individual atoms from the sum of their cohesive bonds. As a result, the BOLS predicts the surface layers of a nanoparticle melt at lower temperatures than the bulk of the nanoparticle.\n\nThe BOLS mechanism states that if one bond breaks the remaining neighbouring ones become shorter and stronger. The cohesive energy, or the sum of bond energy, of the less coordinated atoms determines the thermal stability including melting, evaporating and other phase transition. The lowered CN changes the equilibrium bond length between atoms near the surface of the nanoparticle. The bonds relax towards equilibrium lengths, increasing the cohesive energy per bond between atoms, independent of the exact form of the specific interatomic potential. However, the integrated cohesive energy for surface atoms is much lower than bulk atoms due to the reduced coordination number and overall decrease in cohesive energy.\n\nUsing a core–shell configuration, the melting point depression of nanoparticles is dominated by the outermost two atomic layers yet atoms in the core interior remain their bulk nature.\n\nThe BOLS model and the core–shell structure have been applied to other size dependency of nanostructures such as the mechanical strength, chemical and thermal stability, lattice dynamics (optical and acoustic phonons), Photon emission and absorption, electronic colevel shift and work function modulation, magnetism at various temperatures, and dielectrics due to electron polarization etc. Reproduction of experimental observations in the above-mentioned size dependency has been realized. Quantitative information such as the energy level of an isolated atom and the vibration frequency of individual dimer has been obtained by matching the BOLS predictions to the measured size dependency.\n\nNanoparticle shape impacts the melting point of a nanoparticle. Facets, edges and deviations from a perfect sphere all change the magnitude of melting point depression. These shape changes affect the surface to volume ratio, which affects the cohesive energy and thermal properties of a nanostructure. Equation 7 gives a general shape corrected formula for the theoretical melting point of a nanoparticle based on its size and shape.\n\nformula_7\nWhere: c=materials constant\n\nThe shape parameter is 1 for sphere and 3/2 for a very long wire, indicating that melting-point depression is suppressed in nanowires compared to nanoparticles. Past experimental data show that nanoscale tin platelets melt within a narrow range of 10 C of the bulk melting temperature. The melting point depression of these platelets was suppressed compared to spherical tin nanoparticles.\n\nSeveral nanoparticle melting simulations theorize that the supporting substrate affects the extent of melting-point depression of a nanoparticle. These models account for energetic interactions between the substrate materials. A free nanoparticle, as many theoretical models assume, has a different melting temperature (usually lower) than a supported particle due to the absence cohesive energy between the nanoparticle and substrate. However, measurement of the properties of a freestanding nanoparticle remains impossible, so the extent of the interactions cannot be verified through experiment. Ultimately, substrates currently support nanoparticles for all nanoparticle applications, so substrate/nanoparticle interactions are always present and must impact melting point depression.\n\nWithin the size–pressure approximation, which considers the stress induced by the surface tension and the curvature of the particle, it was shown that the size of the particle affects the composition and temperature of a eutectic point (Fe-C) the solubility of C in Fe and Fe:Mo nanoclusters.\nReduced solubility can affect the catalytic properties of nanoparticles. In fact it has been shown that size-induced instability of Fe-C mixtures represents the thermodynamic limit for the thinnest nanotube that can be grown from Fe nanocatalysts.\n\n"}
{"id": "20398", "url": "https://en.wikipedia.org/wiki?curid=20398", "title": "Muonium", "text": "Muonium\n\nMuonium is an exotic atom made up of an antimuon and an electron, which was discovered in 1960 by Vernon W. Hughes\n\nAlthough muonium is short-lived, physical chemists study it using muon spin spectroscopy (μSR), a magnetic resonance technique analogous to nuclear magnetic resonance (NMR) or electron spin resonance (ESR) spectroscopy. Like ESR, μSR is useful for the analysis of chemical transformations and the structure of compounds with novel or potentially valuable electronic properties. Muonium is usually studied by muon spin rotation, in which the Mu atom's spin precesses in a magnetic field applied transverse to the muon spin direction (since muons are typically produced in a spin-polarized state from the decay of pions), and by avoided level crossing (ALC), which is also called level crossing resonance (LCR). The latter employs a magnetic field applied longitudinally to the polarization direction, and monitors the relaxation of muon spins caused by \"flip/flop\" transitions with other magnetic nuclei.\n\nBecause the muon is a lepton, the atomic energy levels of muonium can be calculated with great precision from quantum electrodynamics (QED), unlike in the case of hydrogen, where the precision is limited by uncertainties related to the internal structure of the proton. For this reason, muonium is an ideal system for studying bound-state QED and also for searching for physics beyond the standard model.\n\nNormally in the nomenclature of particle physics, an atom composed of a positively charged particle bound to an electron is named after the positive particle with \"-ium\" appended, in this case \"muium\". The suffix \"-onium\" is mostly used for bound states of a particle with its own antiparticle. The exotic atom consisting of a muon and an antimuon is known as \"true muonium\". It is yet to be observed, but it may have been generated in the collision of electron and positron beams.\n"}
{"id": "21303741", "url": "https://en.wikipedia.org/wiki?curid=21303741", "title": "My Life in Dog Years", "text": "My Life in Dog Years\n\nMy Life in Dog Years is a non-fiction book for children written by the American author Gary Paulsen, together with his wife, Ruth Wright Paulsen. It was published first by Delacorte Press in 1997.\n\nThe book contains a chapter about each different dog in his life. As he goes through each chapter, he delves into the personality of each dog as companions and not just animals.\n\nIt begins with Cookie, a dog which rescued him from a fall through the ice while dog sled racing he Cookie was helped by the other sled dogs gary had. This was ironic since the dog was sick before. How would he have saved him? It next considers Snowball, a dog he owned in the Philippines at age 7 (his first) who was killed in a military truck accident. It then discusses Ike, dog that became attached to him during a hunting trip. Ike continues to accompany him for about a year, but then disappears. Paulsen later discovers that Ike's owner had to leave him when he went to the war. When he returned with an injury, Ike returned to him.\nDirk, next, is a dog that adopted him during his adolescence, and served as a protector during a difficult period. While working on a farm, he met Rex, a rough collie, whom he considered one of the smartest dogs he ever knew after observing his activities for a day. He later adopted an exceptionally large Great Dane, Caesar (pronounced See-Zer) who was easily excited, but a gentle giant that loved hot dogs.\n\nThe author later became a dog sled racer. At one point, he traded one of his best dogs for Quincy, who was later to save his wife from a bear despite being only nine inches tall. The last chapter is about his current dog at the time, Josh. It is clear that Josh is one of the author's favorite dogs, as he has the most to write about him. At the time the book was written, Josh was about 20 years old. Josh was known as the smartest dog in the world.\n\n"}
{"id": "3145631", "url": "https://en.wikipedia.org/wiki?curid=3145631", "title": "Nuclear weapon yield", "text": "Nuclear weapon yield\n\nThe explosive yield of a nuclear weapon is the amount of energy released when that particular nuclear weapon is detonated, usually expressed as a TNT equivalent (the standardized equivalent mass of trinitrotoluene which, if detonated, would produce the same energy discharge), either in kilotons (kt—thousands of tons of TNT), in megatons (Mt—millions of tons of TNT), or sometimes in terajoules (TJ). An explosive yield of one terajoule is 0.239 kt of TNT. Because the accuracy of any measurement of the energy released by TNT has always been problematic, the conventional definition is that one kiloton of TNT is held simply to be equivalent to 10 calories.\n\nThe yield-to-weight ratio is the amount of weapon yield compared to the mass of the weapon. The practical maximum yield-to-weight ratio for fusion weapons (thermonuclear weapons) has been estimated to six megatons of TNT per metric ton of bomb mass (25 TJ/kg). Yields of 5.2 megatons/ton and higher have been reported for large weapons constructed for single-warhead use in the early 1960s. Since then, the smaller warheads needed to achieve the increased net damage efficiency (bomb damage/bomb mass) of multiple warhead systems have resulted in decreases in the yield/mass ratio for single modern warheads.\n\nIn order of increasing yield (most yield figures are approximate):\n\nAs a comparison, the blast yield of the GBU-43 Massive Ordnance Air Blast bomb is 0.011 kt, and that of the Oklahoma City bombing, using a truck-based fertilizer bomb, was 0.002 kt. Most artificial non-nuclear explosions are considerably smaller than even what are considered to be very small nuclear weapons.\n\nThe yield-to-weight ratio is the amount of weapon yield compared to the mass of the weapon. According to nuclear-weapons designer Ted Taylor, the practical maximum yield-to-weight ratio for fusion weapons is about 6 megatons of TNT per metric ton (25 TJ/kg). The \"Taylor limit\" is not derived from first principles, and weapons with yields as high as 9.5 megatons per metric ton have been theorized. The highest achieved values are somewhat lower, and the value tends to be lower for smaller, lighter weapons, of the sort that are emphasized in today's arsenals, designed for efficient MIRV use, or delivery by cruise missile systems.\n\n\n\nLarge single warheads are seldom a part of today's arsenals, since smaller MIRV warheads, spread out over a pancake-shaped destructive area, are far more destructive for a given total yield, or unit of payload mass. This effect results from the fact that destructive power of a single warhead on land scales approximately only as the cube root of its yield, due to blast \"wasted\" over a roughly hemispherical blast volume while the strategic target is distributed over a circular land area with limited height and depth. This effect more than makes up for the lessened yield/mass efficiency encountered if ballistic missile warheads are individually scaled down from the maximal size that could be carried by a single-warhead missile.\n\nYields of nuclear explosions can be very hard to calculate, even using numbers as rough as in the kiloton or megaton range (much less down to the resolution of individual terajoules). Even under very controlled conditions, precise yields can be very hard to determine, and for less controlled conditions the margins of error can be quite large. For fission devices, the most precise yield value is found from \"radiochemical/Fallout analysis\"; that is, measuring the quantity of fission products generated, in much the same way as the chemical yield in chemical reaction products can be measured after a chemical reaction. The radiochemical analysis method was pioneered by Herbert L. Anderson.\n\nFor nuclear explosive devices where the fallout is not attainable or would be misleading, neutron activation analysis is often employed as the second most accurate method, with it having been used to determine the yield of both Little Boy and thermonuclear Ivy Mike's respective yields.\nYields can also be inferred in a number of other remote sensing ways, including scaling law calculations based on blast size, infrasound, fireball brightness (Bhangmeter), seismographic data (CTBTO), and the strength of the shock wave.\n\nEnrico Fermi famously made a (very) rough calculation of the yield of the Trinity test by dropping small pieces of paper in the air and measuring how far they were moved by the blast wave of the explosion; that is, he found the blast pressure at his distance from the detonation in pounds per square inch, using the deviation of the papers' fall away from the vertical as a crude blast gauge/barograph, and then with pressure \"X\" in psi, at distance \"Y\", in miles figures, he extrapolated backwards to estimate the yield of the Trinity device, which he found was about 10 kiloton of blast energy.\n\nFermi later recalled that:\n\nThe surface area (A) and volume (V) of a sphere are:\nformula_1 and formula_2 respectively.\n\nThe blast wave however was likely assumed to grow out as the surface area of the approximately hemispheric near surface burst blast wave of the Trinity gadget.\nThe paper is moved 2.5 meters by the wave - so the effect of the Trinity device is to displace a hemispherical shell of air of volume 2.5 m × 2π(14 km). Multiply by 1 atm to get energy of ~ 80 kT TN.\n\nA good approximation of the yield of the Trinity test device was obtained in 1950 from simple dimensional analysis as well as an estimation of the heat capacity for very hot air, by the British physicist G. I. Taylor. Taylor had initially done this highly classified work in mid-1941, and published a paper which included an analysis of the Trinity data fireball when the Trinity photograph data was declassified in 1950 (after the USSR had exploded its own version of this bomb).\n\nTaylor noted that the radius \"R\" of the blast should initially depend only on the energy \"E\" of the explosion, the time \"t\" after the detonation, and the density ρ of the air. The only equation having compatible dimensions that can be constructed from these quantities is:\n\nformula_3\n\nHere S is a dimensionless constant having a value approximately equal to 1, since it is low order function of the heat capacity ratio or adiabatic index \n\nformula_4\n\nwhich is approximately 1 for all conditions.\n\nUsing the picture of the Trinity test shown here (which had been publicly released by the U.S. government and published in \"Life\" magazine), using successive frames of the explosion, Taylor found that R/t is a constant in a given nuclear blast (especially between 0.38 ms after the shock wave has formed, and 1.93 ms before significant energy is lost by thermal radiation). Furthermore, he estimated a value for S numerically at 1.\n\nThus, with \"t\" = 0.025 s and the blast radius was 140 metres, and taking \"ρ\" to be 1 kg/m (the measured value at Trinity on the day of the test, as opposed to sea level values of approximately 1.3 kg/m) and solving for \"E\", Taylor obtained that the yield was about 22 kilotons of TNT (90 TJ). This does not take into account the fact that the energy should only be about half this value for a hemispherical blast, but this very simple argument did agree to within 10% with the official value of the bomb's yield in 1950, which was (See G. I. Taylor, \"Proc. Roy. Soc. London A\" 200, pp. 235–247 (1950).)\n\nA good approximation to Taylor's constant S for formula_5 below about 2 is:\n\nformula_6\n\nAs it relates to fundamental dimensional analysis, if one expresses all the variables in terms of mass, M, length, L, and time, T :\n\nformula_7\n\n(think of the expression for kinetic energy, formula_8\n\nformula_9\n\nformula_10\n\nformula_11\n\nand then derive an expression for, say, E, in terms of the other variables, by finding values of formula_12, formula_13, and formula_5 in the general relation\n\nformula_15\n\nsuch that the left- and right-hand sides are dimensionally balanced in terms of M, L, and T (i.e., each dimension has the same exponent on both sides).\n\nWhere these data are not available, as in a number of cases, precise yields have been in dispute, especially when they are tied to questions of politics. The weapons used in the atomic bombings of Hiroshima and Nagasaki, for example, were highly individual and very idiosyncratic designs, and gauging their yield retrospectively has been quite difficult. The Hiroshima bomb, \"Little Boy\", is estimated to have been between (a 20% margin of error), while the Nagasaki bomb, \"Fat Man\", is estimated to be between (a 10% margin of error). Such apparently small changes in values can be important when trying to use the data from these bombings as reflective of how other bombs would behave in combat, and also result in differing assessments of how many \"Hiroshima bombs\" other weapons are equivalent to (for example, the Ivy Mike hydrogen bomb was equivalent to either 867 or 578 Hiroshima weapons — a rhetorically quite substantial difference — depending on whether one uses the high or low figure for the calculation). Other disputed yields have included the massive Tsar Bomba, whose yield was claimed between being \"only\" or at a maximum of by differing political figures, either as a way for hyping the power of the bomb or as an attempt to undercut it.\n\n\n"}
{"id": "14659532", "url": "https://en.wikipedia.org/wiki?curid=14659532", "title": "Oil megaprojects (2009)", "text": "Oil megaprojects (2009)\n\nThis page summarizes projects that propose to bring more than of new liquid fuel capacity to market with the first production of fuel beginning in 2009. This is part of the Wikipedia summary of Oil Megaprojects.\n\nTerminology \n"}
{"id": "2046887", "url": "https://en.wikipedia.org/wiki?curid=2046887", "title": "Oxycation", "text": "Oxycation\n\nAn oxycation is a polyatomic ion with a positive charge that contains oxygen.\n\n\nSee for a bigger list.\n\n"}
{"id": "6218947", "url": "https://en.wikipedia.org/wiki?curid=6218947", "title": "P-factor", "text": "P-factor\n\nP-factor, also known as asymmetric blade effect and asymmetric disc effect, is an aerodynamic phenomenon experienced by a moving propeller, that is responsible for the asymmetrical relocation of the propeller's center of thrust when an aircraft is at a high angle of attack. This shift in the location of the center of thrust will exert a yawing moment on the aircraft, causing it to yaw slightly to one side. A rudder input is required to counteract the yawing tendency.\n\nWhen an aircraft is in straight and level flight at cruise speed, the propeller disc is perpendicular to the relative airflow. Each of the propeller blades will contact the air at the same angle and speed and thus the thrust produced is evenly centered across the propeller. As the aircraft's angle of attack increases and the propeller disc rotates toward the horizontal, the airflow will meet the propeller disc at an increasing angle. The propeller blades moving down and forward (for clockwise rotation, from the one o'clock to the six o'clock position when viewed from the front) will have a greater relative wind angle of attack and therefore will produce greater thrust, while propeller blades moving up and back (from the seven o'clock through 12 o'clock position) will have a decreased relative wind angle of attack and therefore decreased thrust. This asymmetry displaces the center of thrust of the propeller disc towards the blade with increased thrust. In an aircraft with two or more propeller engines, P-Factor is what determines which engine is the critical engine.\n\nP-Factor is sometimes erroneously explained with the word \"bite\", as in \"the descending blade has a bigger bite, or angle of attack, than the ascending blade\". This faulty explanation does not take into account the forward motion of the blades as aircraft's angle of attack increases. In order to better understand this concept, imagine a propeller aircraft moving forward with a 90° angle of attack (vertical). This situation is identical to what a helicopter experiences, but a helicopter can reduce or increase the angle of attack of individual blades of the rotor (decreasing the angle of attack on the advancing blade, while increasing the angle of attack on the retreating blade) in order to keep the lift of the rotor disc balanced. Because the force of the air on the blades moving forwards through the arc is greater, they will produce more thrust than the blades that move backwards. If the blades of the rotor were unable to independently change their angle of attack there would be a constant backwards rolling motion due to the increased lift on the side of the rotor disc with the advancing blade. In a fixed-wing aircraft, there is usually no way to adjust the angle of attack of the individual blades of the propellers, therefore the pilot must contend with P-Factor and use the rudder to counteract the shift of thrust.\n\nSingle engine propeller aircraft\n\n(As viewed by the pilot), the aircraft has a tendency to yaw to the left if using a clockwise turning propeller (right hand), and to the right with a counter-clockwise turning propeller (left hand). The clockwise turning propeller is by far the most common. The effect is most noticeable during the climb phase after take off and in flight conditions with high power and high angle of attack.\n\nMulti engine propeller aircraft (clockwise rotation)\n\nAs with single engined aircraft, situations where the aircraft is at high power and has a high angle of attack (such as take off) will cause a slight, but noticeable yawing motion. The engine with the down-moving blades towards the wingtip produces more yaw and roll than the other engine, because the moment (arm) of that engine's thrust about the aircraft center of gravity is greater. Thus, the engine with down-moving blades towards the fuselage will be \"critical\", because its failure and the associated reliance on the other engine will require a larger rudder deflection by the pilot to maintain straight flight than if the other engine had failed. For most aircraft (which have clockwise rotating propellers), this is the left engine.\n\nWith engines rotating in the same direction the P-factor will affect V's, the minimum control speeds of the aircraft in asymmetric powered flight. The published V's are determined while the critical engine is inoperative. The actual V's after failure of any other engine is lower, safer, provided the pilot is aware of the limitations that apply with minimum control speeds.\nAt low speed flight with the left engine failed, the off-centre thrust produced by the right engine creates a larger yawing moment to left than the opposite case. The left engine in this scenario is the critical engine, namely the engine whose failure brings about the more adverse result. In the case of using counter-rotating engines (i.e., not rotating in the same direction) the P-factor is not considered in determining the critical engine (the engines are equally critical).\n\nThe asymmetric blade effect is dependent on thrust, and is proportional to forward velocity (specifically CAS) and, while generally insignificant during the initial ground roll for tail-wheel aircraft, will give a pronounced nose-left tendency during the later stages of the ground roll, particularly if the thrust axis is kept inclined to the flight path vector (i.e. tail-wheel in contact with runway). If a high angle of attack is used during the rotation (or indeed straight and level flight with high power and high angle of attack), then the effect can also be apparent. The effect is not so apparent during the landing, flare and rollout, given the relatively low power setting (propeller RPM). However, should the throttle be suddenly advanced with the tail-wheel in contact with the runway, then anticipation of this nose-left tendency is prudent.\n\n\n"}
{"id": "46587709", "url": "https://en.wikipedia.org/wiki?curid=46587709", "title": "Petroleum Act 1998", "text": "Petroleum Act 1998\n\nThe Petroleum Act 1998 is an Act of the Parliament of the United Kingdom.\nSection 4 has further clauses on licence provisions. Section 50 of the Infrastructure Act 2015 appended this section. It defines 'associated hydraulic fracturing' as more than 1,000 cubic metres of fluid per stage, or more than 10,000 cubic metres of fluid in total. In addition, conditions were attached that mean no fracking can take place at a depth shallower than 1,000 meters, and that soil and air monitoring must be put in place. The regulations state that \"The associated hydraulic fracturing will not take place within protected groundwater source areas\". 'Protected groundwater source area' does not appear to be defined.\n"}
{"id": "35943217", "url": "https://en.wikipedia.org/wiki?curid=35943217", "title": "Polluting Paradise", "text": "Polluting Paradise\n\nPolluting Paradise (, also known as \"Garbage in the Garden of Eden\") is a 2012 German documentary film directed by Fatih Akın. The film was screened in the Special Screenings section at the 2012 Cannes Film Festival. It focuses on the Turkish village of Çamburnu in Sürmene, which has been turned into a rubbish dump by the government.\n"}
{"id": "357565", "url": "https://en.wikipedia.org/wiki?curid=357565", "title": "Potassium sodium tartrate", "text": "Potassium sodium tartrate\n\nPotassium sodium tartrate tetrahydrate, also known as Rochelle salt, is a double salt of tartaric acid first prepared (in about 1675) by an apothecary, Pierre Seignette, of La Rochelle, France. Potassium sodium tartrate and monopotassium phosphate were the first materials discovered to exhibit piezoelectricity. This property led to its extensive use in \"crystal\" gramophone (phono) pick-ups, microphones and earpieces during the post-World War II consumer electronics boom of the mid-20th Century. Such transducers had an exceptionally high output with typical pick-up cartridge outputs as much as 2 volts or more. Rochelle salt is deliquescent so any transducers based on the material deteriorated if stored in damp conditions.\n\nIt has been used medicinally as a laxative. It has also been used in the process of silvering mirrors. It is an ingredient of Fehling's solution (reagent for reducing sugars). It is used in electroplating, in electronics and piezoelectricity, and as a combustion accelerator in cigarette paper (similar to an oxidizer in pyrotechnics).\n\nIn organic synthesis, it is used in aqueous workups to break up emulsions, particularly for reactions in which an aluminium-based hydride reagent was used. Sodium Potassium tartrate is also important in the food industry. \n\nIt is a common precipitant in protein crystallography and is also an ingredient in the Biuret reagent which is used to measure protein concentration. This ingredient maintains cupric ions in solution at an alkaline pH.\n\nThe starting material is tartar with a minimum tartaric acid content 68 %. This is first dissolved in water or in the mother liquor of a previous batch. It is then saponified with hot caustic soda to pH 8, decolorized with activated charcoal, and chemically purified before being filtered. The filtrate is evaporated to 42 °Bé at 100 °C, and passed to granulators in which Seignette's salt crystallizes on slow cooling. The salt is separated from the mother liquor by centrifugation, accompanied by washing of the granules, and is dried in a rotary furnace and sieved before packaging. Commercially marketed grain sizes range from 2000 μm to < 250 μm (powder).\n\nLarger crystals of Rochelle salt have been grown under conditions of reduced gravity and convection on board Skylab .\n\nIn 1824, Sir David Brewster demonstrated piezoelectric effects using Rochelle salts, which led to him naming the effect pyroelectricity.\n"}
{"id": "1615827", "url": "https://en.wikipedia.org/wiki?curid=1615827", "title": "Pulk", "text": "Pulk\n\nA pulk (from Finnish \"pulkka\") is a Nordic short, low-slung small toboggan used in sport or for transport, pulled by a dog or a skier, or in Lapland pulled by reindeer. \n\nThe sled can be used to carry supplies such as a tent or food, or transport a child or other person. In Norway, pulks are often used by families with small children on skiing trips (small children being pulled by the parents). In Finland and Sweden, pulks are often used, mostly by children, as a winter toy for going downhill. Pulks are nowadays made of plastic, which makes them inexpensive.\n\nA larger pulk, designed for transporting larger amounts of goods, is called \"ahkio\" in Finnish. This word is also used by the US Army for a human-drawn snow sled.\n\n\n"}
{"id": "36164804", "url": "https://en.wikipedia.org/wiki?curid=36164804", "title": "Secretary of Energy (Philippines)", "text": "Secretary of Energy (Philippines)\n\nThe Secretary of Energy (Filipino: \"Kalihim ng Enerhiya\") is the member of the Cabinet of the Philippines in charge of the Department of Energy.\n\nThe current secretary is Alfonso Cusi, who assumed office on June 30, 2016.\n\n\"(*) Acting Capacity\"\n\n"}
{"id": "25082219", "url": "https://en.wikipedia.org/wiki?curid=25082219", "title": "Supersonic airfoils", "text": "Supersonic airfoils\n\nA supersonic airfoil is a cross-section geometry designed to generate lift efficiently at supersonic speeds. The need for such a design arises when an aircraft is required to operate consistently in the supersonic flight regime.\n\nSupersonic airfoils generally have a thin section formed of either angled planes or opposed arcs (called \"double wedge airfoils\" and \"biconvex airfoils\" respectively), with very sharp leading and trailing edges. The sharp edges prevent the formation of a detached bow shock in front of the airfoil as it moves through the air. This shape is in contrast to subsonic airfoils, which often have rounded leading edges to reduce flow separation over a wide range of angle of attack. A rounded edge would behave as a blunt body in supersonic flight and thus would form a bow shock, which greatly increases wave drag. The airfoils' thickness, camber, and angle of attack are varied to achieve a design that will cause a slight deviation in the direction of the surrounding airflow.\n\nHowever, since a round leading edge decreases an airfoil's susceptibility to flow separation, a sharp leading edge implies that the airfoil will be more sensitive to changes in angle of attack. Therefore, to increase lift at lower speeds, aircraft that employ supersonic airfoils also use high-lift devices such as leading edge and trailing edge flaps.\n\nAt supersonic conditions, aircraft drag is originated due to:\nTherefore, the Drag coefficient on a supersonic airfoil is described by the following expression:\n\nC= C+ C+ C\n\nExperimental data allow us to reduce this expression to:\n\nC= C + KC\nWhere C is the sum of C) and C, and k for supersonic flow is a function of the Mach number. The skin-friction component is derived from the presence of a viscous boundary layer which is infinitely close to the surface of the aircraft body. At the boundary wall, the normal component of velocity is zero; therefore an infinitesimal area exists where there is no slip. The zero-lift wave drag component can be obtained based on the supersonic area rule which tells us that the wave-drag of an aircraft in a steady supersonic flow is identical to the average of a series of equivalent bodies of revolution. The bodies of revolution are defined by the cuts through the aircraft made by the tangent to the fore Mach cone from a distant point of the aircraft at an azimuthal angle. This average is over all azimuthal angles. The drag due-to lift component is calculated using lift-analysis programs. The wing design and the lift-analysis programs are separate lifting-surfaces methods that solve the direct or inverse problem of design and lift analysis.\n\nYears of research and experience with the unusual conditions of supersonic flow have led to some interesting conclusions about airfoil design. Considering a rectangular wing, the pressure at a point P with coordinates (x,y) on the wing is defined only by the pressure disturbances originated at points within the upstream Mach cone emanating from point P. As result, the wing tips modify the flow within their own rearward Mach cones. The remaining area of the wing does not suffer any modification by the tips and can be analyzed with two-dimensional theory. For an arbitrary planform the supersonic leading and trailing are those portions of the wing edge where the components of the freestream velocity normal to the edge are supersonic. Similarly the subsonic leading and trailing are those portions of the wing edge where the components of the free stream velocity normal to the edge are subsonic.\n\nDelta wings have supersonic leading and trailing edges; in contrast arrow wings have a subsonic leading edge and a supersonic trailing edge.\n\n"}
{"id": "27318462", "url": "https://en.wikipedia.org/wiki?curid=27318462", "title": "T.E.D.D.", "text": "T.E.D.D.\n\nT.E.D.D. (Tornado Electrical Discharge Detection) is a public project to gather and publish RF (Radio Frequency) signals produced by tornadoes to help create additional warning and research instrumentation.\n\nT.E.D.D. is based on the theory that all tornadoes, strong or weak, create a RF (Radio Frequency) footprint or \"signature\". These RF signatures are believed to be created during the formation and life cycle of a tornado, due to tiny and mass amounts of electrical discharges taking place within the funnel.\n\nAlthough the exact origin of RF emissions from a tornado remains uncertain, there are several credible scientific sources and publications citing evidence of their existence.\n\nOne theory explains that the formation of electrical discharges occur when negatively charged condensation from a severe thunderstorm is drawn down within the tornado's funnel, where it then meets positive charges from the ground. When these opposite charges meet, electrical potential is released along with RF emissions that can be detected using tuned equipment.\n\nCurrently, research is being conducted by T.E.D.D. and others to gather more information on this phenomenon.\n\n"}
{"id": "42309821", "url": "https://en.wikipedia.org/wiki?curid=42309821", "title": "Tamizhuku En Ondrai Azhuthavum", "text": "Tamizhuku En Ondrai Azhuthavum\n\nTamizhuku En Ondrai Azhuthavum (\"\"; also referred to as TEOA) is a 2015 Tamil-language thriller film written and directed by Ramprakash Rayappa. Produced by V. Chandran, the film stars Nakul, Attakathi Dinesh, Bindu Madhavi and Aishwarya Dutta. The film received positive reviews from critics.\n\nA deadly bomb will be activated if a computer geek restores a part of Chennai city's mobile networks, which are down due to a solar flare. Four people are linked to each other on a deadly mission.\n\nMukil (Dinesh Ravi) falls in love with Simi (Bindu Madhavi), who is stuck in a pit hole with an 80-tonne gigantic 'Vaasthu' rock under suspension which is ready to fall on her at any time. The only way that Simi can escape from this danger is if her text message reaches Mukil.\n\nIn the meantime, there is a hyperactive science geek Vasanth (Nakul), who is on a mission to activate a dead mobile network signal. then there is a call taxi driver Raja (Sathish), whose car has a remotely operated bomb planted in it by a terrorist outfit waiting to destroy the city once the phone lines are restored. Does the city escape destruction from the bomb and what happens with the four main characters forms the rest of the story.\n\nVLS Rock Cinema, who had made \"Naan Rajavaga Pogiren\" (2013), announced that they had signed actors Nakul, Attakathi Dinesh and Sathish to be a part of a film directed by Ramprakash Rayappa in October 2013. Actresses Bindu Madhavi and Aishwarya Dutta were signed on to play leading female roles in the film. the film began production in December 2013 and the team held a series of workshops to help the actors get into character, before beginning filming in Tiruvottiyur.\n\nThe soundtrack was composed by S. Thaman, and the lyrics were written by Madhan Karky and Yugabharathi.\nThe satellite rights of the film were sold to STAR Vijay.\n\nM. Suganth of The Times of India's gave the film 3.5 stars out of 5 and wrote, \"Ramprakash Rayappa sets up his story effectively and keeps us on the edge of our seats for the most parts. The manner in which he handles this multi-strand narrative without making it chaotic deserves appreciation. The success lies in how he makes each of these sub-plots interesting\", going on to call it a \"solid debut film\". Indo-Asian News Service, while giving the same rating and also describing it as a \"solid social thriller\", wrote, \"(the) details, though minute, make the director stand out from his contemporaries. The writing is fresh and it’s evident from the way the director manages to make the parallelly running stories converge at the end...It’s probably the first Tamil film where a perfect balance is struck between science, romance and comedy interspersed with some fresh thrills\". The New Indian Express wrote, \"The deftness and confidence with which the director moves his narration belies the fact that this is his debut venture. The screenplay is engaging, with the characters well-fleshed out, and there is an element of suspense with humour laced throughout...One of the better scripts to appear in recent times, \"Thamizhuku Enn Ondrai Azhuthavum\" has a lot of positives going for it\". Sify wrote, \"The director has done his paper work with sheer perfection and packaged this out-of-the-box concept in a fairly engaging manner\". Baradwaj Rangan wrote, \"It doesn’t say much about a movie when we walk into a thriller and walk out of what’s mostly a comedy – but with so many laughs, why complain?\", further adding that \"first-time director Ramprakash Rayappa at least has the right ideas\".\n\nIn contrast, Rediff gave the film 2 stars out of 5 and wrote, \"Director Ramprakash has come up with a good idea, but the performance is poor, the romance seems forced, there are too many coincidences and a lame, predictable climax. This spoils what could have been a racy and exciting thriller\".\n"}
{"id": "14300597", "url": "https://en.wikipedia.org/wiki?curid=14300597", "title": "The Steam House", "text": "The Steam House\n\nThe Steam House () is an 1880 Jules Verne novel recounting the travels of a group of British colonists in the Raj in a wheeled house pulled by a steam-powered mechanical elephant. Verne uses the mechanical house as a plot device to have the reader travel in nineteenth-century India. The descriptions are interspersed with historical information and social commentary.\n\nThe book takes place in the aftermath of the Indian Rebellion of 1857 against British rule, with the passions and traumas aroused still very much alive among Indians and British alike. An alternate title by which the book was known - \"The End of Nana Sahib\" - refers to the appearance in the book of the historical figure—Rebel leader Nana Sahib—who disappeared after the crushing of the rebellion, his ultimate fate unknown. Verne offers a fictional explanation to his disappearance.\n\nIn summer 1866, in Aurangabad, British colonial government announces bounty on the head of Nana Sahib, who is supposed to be hiding in that presidency. Nana Sahib, disguised as a sage stalks and kill the man who claims to know face of Nana Sahib. Nana Sahib escapes from Aurangabad same night and taking his brother Bala Rao and followers, hidden in Ajanta and Ellora caves respectively, retreats to Vindhiyanchal mountains to hide from colonial forces.\n\nNana Sahib, along with his brother and followers hides in various small fortresses called \"Pals,\" and mostly inside \"Pal of Tandil.\" His brother Bala Rao, who is extremely similar to Nana sahib in physical appearance, inquires about the inhabitants of fortress and learns from locals that none except local outlaws, insurgents and a mad woman knows about the place. The mad woman is known as Rowing Flame as she carries a burning torch and roams the wilderness in valley of Narmada. The locals respect the mysterious lady and feed and cloth her. From this hiding place, Nana Sahib launches an underground movement and secretly visit local chieftains for persuading them for a rising.\n\nMeanwhile, in Calcutta, a group of Europeans is planning for a voyage through India. The group consist of Banks, a railroad engineer, Maucler, the French adventurer and narrator for most part of the story, Captain Hood; a hunter craving for his \"half century\" of tigers, retired Colonel Sir Edward Munro, whose motive behind joining this expedition is to find and kill Nana Sahib to avenge his Wife, who supposedly died in the Cawnpore massacre. Servants accompanying them are Sergaent McNeil, Munro's faithful servant. Fox is the faithful servant of Captain Hood and fellow hunter, who has killed 37 tigers. Monsieur Parazard; a Negro cook of French origins, Storr, a British Engine driver, Kilouth; an Adivasi coal shover and Gotimi; the faithful Gurkha servant of Colonel Munro.\n\nBanks, the Engineer introduces the machine he invented, a Steam powered mechanical elephant, which pulls two comfortable carriages having all the comforts of a 19th-century house. The machine, can \"walk\" across land and float across rivers using embedded Peddle wheels. The steam elephant is named as Behemoth and together with two carriages, it is called the Steam House. First carriage is used by the gentlemen while the other is reserved for servants. They start from Calcutta, and travelling around the French town of Chandannagar, and Burdawan, Patna and Chitra, reach Gaya, where they visit various Hindu and Buddhist temples and bathing Ghat. On the way to Banaras are interrupted by Hindu fanatics who consider the Steam House to be chariot of their deity. Banks frightens them away by directing steam exhaust at them. In Banaras, Banks and Maucler notice a man spying on them but resolve not to tell Colonel. From Banaras,they travel to Allahabad, where they learn that Nana sahib has been declared dead after an skirmish in the defiles of Satpuda. Colonel Munro is shocked by this news, as he wanted to take revenge himself. After Munro's request, they decides to pass through Kanpur, where an emotional Colonel visits his old house and the well which is supposedly the grave of Mrs Munro and other victims of the massacre. The group decides to journey towards norther forest and pass the Monsoon season there, hunting wild animals. In the way to Terai, they defeat 3 elephants of an arrogant Gujarati Prince in a competition with Behemoth. Near Terai, they are caught in a violent thunderstorm and Gautami narrowly survives after being thunderstruck. The man who was spying on Steam House meets Nana in Bhopal and informs him of further plans of inhabitants of Steam House. Nana orders his faithful follower Kalagni to infiltrate Steam House and lure them near Nana Sahib's hiding place. While returning to their hiding place, Near the Pal of Tandil, they are ambushed by British forces, who were directed unwittingly by the madwoman Rowing Flame. A body matching the description of Nana Sahib is found and he is declared dead by the British authorities.\n\nThe inhabitants of Steam House camp on a plateau in Terai. During a hunting expedition, they rescue Mathias Van Guitt, an Animal purveyor from his own trap. They visit kraal of Van Guitt, where Colonel Munro is saved from a poisonous snake by one of Van Guitt's servant, Kalagni. The Steam House dwellers frequently visit the kraal and invite Van Guitt to the Steam House. Van Guitt try to capture animals, while inhabitants of Steam House hunt animals. One night when they are visiting Kraal,when Tigers and other Predatory animals attack the kraal. Protagonists narrowly escape death as many Indian servants are killed. The buffaloes are either killed by animals or driven away into jungle. Consequently, Van Guitt ask protagonists to drag his caravan of cages to nearest railway station, After reaching station and loading his cargo, Van Guitt and protagonists part their ways. Protagonists employ Kalagni as guide and servants and head for Bobay through Central India. During journey through jungles they encounter a herd of monkeys and a grain transport caravan. Kalagni meet an old acquaintance in the caravan and chat mysteriously with him. On their way to Jabalpur in the jungle, they are cornered and attacked by a herd of elephants which results in loss of second carriage. To escape from the herd, Banks drives Steam House into lake \"Puturia\". All the food and provision is lost with second carriage and after sometime, the fuel is exhausted, resulting in Steam House floating in the middle of the lake. Kalagni volunteers to swim to shore and fetch help. Colonel Munro, suspecting him, sends his faithful servant Gautami with him. Both swim to shore while the steam House slowly drift in the fog. As soon as they reach the shore, Kalagni meets a follower of Nana, Nassim and tries to attack Goumi, who swiftly escape. With morning breeze, the Steam House drifts towards bank. as protagonists land, they are attacked by a group of men led by Kalagni and Nassim attack and kidnap Colonel Munro, leaving others bonded with ropes. Colonel Munro is taken to an abandoned fort, where Nana Sahib shows up and reveals the reality of the news of his death.The dead person who was identified as Nana Sahib was actually his look alike brother Balao Rao. Due to their physical similarity, the British authorities mistook Balao Rao as Nana Sahib. Nana Sahib proclaims death for Colonel Munro To avenge death of his brother, members of the royal family of last Mughal emperor Bahadur Shah II and other victims of British suppuration of Indian Rebellion of 1857. Colonel Munro is tied on the mouth of a large cannon to, to be blown at sunrise. Nana leaves for a meeting in nearby village. Near dawn, Munro is rescued by Goumi, who had hid himself inside the cannon after running away from Lake Puturia and overhearing plans of rebels. As they are escaping, they encounter Rowing Flame. Colonel Munro recognizes her as his wife Lady Munro, but she being lost her sanity, doesn't recognize him and refuses to go with him and sparks from her torch causes the canon to go off. Munro and Goumi escape with Lady Munro while the people in the fort are confused. But soon they are spotted by Kalagni and his men and encounter Nana Sahib on his way back to fort. Goumi and Munro quickly overpowers Nana and his assistant. As they are being chased by the men led by Kalagni, they are rescued by other protagonists riding on Behemoth. They take Nana sahib as prisoner and as they are chased through jungle. Capt Hood and Sgt. McNeil shoot down many of adversaries, including Kalagni. As they near towards a military outpost, Banks supercharges the boiler and protagonists escape the Behemoth, leaving bounded Nana Sahib inside the machine. As the men approach the machine, the boiler bursts, leaving everyone near it dead' although Nana's body is not found. Protagonists are rescued by the stationed regiment as rest of the insurgents fled to inner country. They head for Mumbai via railway and than to Calcutta. In care of Colonel Munro, Lady Munro regains her sanity and memory. When Munro tell Hood that about not being able to achieve his target of killing 50 tigers, Hood replies that Kalagni was his 50th tiger.\n\nThe novel is usually published in two volumes or parts.\n\n\n"}
{"id": "19376406", "url": "https://en.wikipedia.org/wiki?curid=19376406", "title": "Transient astronomical event", "text": "Transient astronomical event\n\nA transient astronomical event, often shortened by astronomers to a transient, is an astronomical object or phenomenon whose duration may be from seconds to days, weeks, or even several years. This is in contrast to the timescale of the millions or billions of years during which the galaxies and their component stars in our universe have evolved. Singularly, the term is used for violent deep-sky events, such as supernovae, novae, dwarf nova outbursts, gamma-ray bursts, and tidal disruption events, as well as gravitational microlensing, transits and eclipses. These events are part of the broader topic of time domain astronomy.\n\nBefore the invention of telescopes, events such as these that were visible to the naked eye, from within or near the Milky Way Galaxy, were very rare, and sometimes hundreds of years apart. However, such events were recorded in antiquity, such as the supernova in 1054 observed by Chinese, Japanese and Arab astronomers, and the event in 1572 known as \"Tycho's Supernova\" after Tycho Brahe, who studied it until it faded after two years. Even though telescopes made it possible to see more distant events, their small fields of view – typically less than 1 square degree – meant that the chances of looking in the right place at the right time were low. Schmidt cameras and other astrographs with wide field were invented in the 20th century, but mostly used to survey the unchanging heavens.\n\nAs the interest in transients has intensified because study of them helps astrophysicists to understand the mechanisms which produced our universe, telescopes with larger fields of view are coming into use. These, such as the Palomar Transient Factory, the spacecraft Gaia and the LSST, are able to spot many more such occurrences. The ability of modern instruments to observe in wavelengths invisible to the human eye (radio waves, infrared, ultraviolet, X-ray) increases the amount of information that may be obtained when a transient is studied. The proposed ULTRASAT satellite will observe a field of more than 200 square degrees continuously in the ultraviolet range. This wavelength is particularly important for detecting supernovae within minutes of their occurrence.\n\n\n\n"}
{"id": "52409731", "url": "https://en.wikipedia.org/wiki?curid=52409731", "title": "Ustilagic acid", "text": "Ustilagic acid\n\nUstilagic acid is an organic compound with the formula CHO. The acid is a cellobiose lipid produced by the corn smut fungus \"Ustilago maydis\" under conditions of nitrogen starvation. The acid was discovered in 1950 and was proved to be an amphipathic glycolipid with surface active properties. The name comes from Latin \"ustus\" which means \"burnt\" and refers to the scorched appearance of the smut fungi.\n\nCellobiose lipids are known as biosurfactants and natural detergents. They can be used in pharmaceutical, cosmetic, and food applications and are known for their strong fungicidal activity on many species.The yeast Pseudozyma fusiformata and Pseudozyma graminicola secrete ustilagic acids, 2-O-3-hydroxyhexanoyl-beta-D-glucopyranosyl-(1→4)-6-O-acetyl-beta-D-glucopyranosyl-(1→16)-2,15,16- trihydroxyhexadecanoic acid. Similar compounds are the extracellular cellobiose lipids of the yeasts Cryptococcus humicola and Trichosporon porosum : 2,3,4-O-triacetyl-beta-D-glucopyranosyl-(1→4)-6-O-acetyl-beta-D-glucopyranosyl -(1→16)-2,16-dihydroxyhexadecanoic acid. These compounds inhibit the growth of quite a number of various species of yeast and fungi, including Candida albicans and Cryptococcus (Filobasidiella) neoformans. The antifungal activity manifested at acidic pH.\n"}
{"id": "32356293", "url": "https://en.wikipedia.org/wiki?curid=32356293", "title": "Woodworking safety", "text": "Woodworking safety\n\nLack of thorough training on the specific system of safety and health signs and signals plays the pivotal role in furnishing the accidents in the woodworking sector. The established contacts with managers from SMEs and companies in Wood Industry also confirmed the current needs for this particular training. SMEs, in particular, have fewer resources to put complex systems of worker protection in place and tend to be more affected by the negative impact of health and safety problems.\n\nWoodworking machines, wood dust, fire and explosion, noise, vibrations, manual handling operations in Wood Industry, hazardous and chemical substances, slips and trips are some of the biggest concerns.\n\nFederal regulators are drawing up new standards for handling industrial and wood dust following a series of fires and explosions related to combustible dust. The Wood Machinery Manufacturers of America's Industrial Dust Task Force has been following the Occupational Safety and Health Administration on the new rules since October 2009. The goal is to ensure that companies follow proper dust collection procedures.\n\nWood Machinery Manufacturers of America (WMMA) fully supports efforts to protect workers against the dangers of combustible dust. However, the organization is encouraging OSHA to avoid creating new economic hardships for woodworking companies and small manufacturers. Their observation says that if the companies find it difficult to meet the regulations, then it will be impossible to achieve the goal which is to protect the workers.\n\nArt Sipple, State of Nevada Safety Consultation & Training Section Supervisor, OSHA asserts flammable dust being the biggest threat to the industry. Not only sawdust which in and of itself poses a fire hazard, but the generation of fine dust particles creates an even bigger hazard. Organic dust particles are highly flammable and under favorable conditions, highly explosive. In addition, some forms of wood release toxic materials when being cut; Western Red Cedar is one example. Another issue is rotating equipment. Guarding is a major issue with all rotating machinery.\n\nSipple says that the regulations are important as they not only ensure the safety of the employees, it further saves extra expenses related to the injuries and illness. He says: “Controls for any industry are practical. The lack of controls causes injuries and illnesses that cost companies a lot of money, not only through insurance claims with workers compensation insurance, but through lost time and production. Generally, $1 spent on controls and safe work practices save employers $3 or more by preventing problems. For a company to run on high performance and high demand, it has to control injuries and illnesses or the company will not be competitive and lose market shares.”\n\nRecent Health and Safety Executive (HSE) accident statistics shows that accidents involving contact with the dangerous parts of machinery or the material being machined accounted for approximately one quarter of all of the fatal injuries recorded in the woodworking industry, and approximately half of all major injury accidents.\n\nThe risks associated with the use of woodworking machinery are high since they rely on high-speed sharp cutters to do the job and in many cases, these are necessarily exposed to enable the machining process to take place. Additionally, many machines are still hand-fed; woodworking is probably the main industry where the hands of the operator are constantly exposed to danger.\n"}
{"id": "27343427", "url": "https://en.wikipedia.org/wiki?curid=27343427", "title": "World Renewable Energy Network", "text": "World Renewable Energy Network\n\nWREN is a major non-profit organization registered in the United Kingdom with charitable status and affiliated to UNESCO, the Deputy Director General of which is its honorary President. It has a Governing Council, an Executive Committee and a Director General. It maintains links with many United Nations, governmental and non-governmental organisations.\n\nEstablished in 1992 during the second World Renewable Energy Congress in Reading, UK, WREN is one of the most effective organizations in supporting and enhancing the utilisation and implementation of renewable energy sources that are both environmentally safe and economically sustainable. This is done through a worldwide network of agencies, laboratories, institutions, companies and individuals, all working together towards the international diffusion of renewable energy technologies and applications. Representing most countries in the world, it aims to promote the communication and technical education of scientists, engineers, technicians and managers in this field and to address itself to the energy needs of both developing and developed countries.\n\nOver two billion dollars have now been allocated to projects dealing with renewable energy and the environment by the World Solar Summit and World Solar Decade along with the World Bank.\n\nThe global activities of the World Renewable Energy Congress / Network encompass:\n\n\n\"With the accelerated approach of the global climate-change point-of-no-return the need to address the pivotal role of renewable energy in the formation of coping strategies, rather than prevention, is more crucial than ever. Sustainability, green buildings, and the development of the large-scale renewable energy industry must be at the top of all development, economic, financial and political agendas. The time for action has arrived. Prevention and questioning how and why we face this great challenge is a luxury we can no longer indulge. We welcome the establishment of the long overdue International Renewable Energy Agency which we hope will work side-by-side with similar intergovernmental agencies striving for the adoption of renewable energies. \"\n\nThe major event organised by WREC/WREN is the biennial congress, normally held during the summer of every even year. The congresses are mostly run and organised by the WREC headquarters which are in Brighton, UK. All members of WREC/WREN are entitled to bid to host the Congress. The WREC/WREN Council meets and decides the location based on: availability of local funding and sponsorship; ease of travel to the location; extent of host government and institutional support; benefits to the local country. All local organisation and services must be provided by the host country.\n\nThe first three congresses were held in the UK (Reading), followed by a move to Denver (United States) and then to Florence (Italy). In the year 2000 the congress returned to the UK (Brighton) with every effort being made to ensure that this event enhanced the recognition of Renewable Energies in the new millennium. In 2002 the congress took place in Cologne (Germany) and 2004 once more in Denver (USA). In 2006 the congress was held in Florence (Italy) and in 2008 in Glasgow (UK). The next congresses will be in Abu Dhabi (UAE) in 2010 and in Denver (USA) in 2012 respectively.\n\nThe following table shows the statistics for the previous WREC conferences:\n\nAt no time in modern history has energy played a more crucial role in the development and well being of nations than at present. The source and nature of energy, the security of supply and the equity of distribution, the environmental impact of its supply and utilization, are all crucial matters to be addressed by suppliers, consumers, governments, industry, academia, and financial institutions.\n\nThe World Renewable Energy Congress (WREC), a major recognised forum for networking between these sectors, addresses these issues through regular meetings and exhibitions, bringing together representatives of all those involved in the supply, distribution, consumption and development of energy sources which are benign, sustainable, accessible and economically viable. WREC enables policy makers, researchers, manufacturers, economists, financiers, sociologists, environmentalists and others to present their views in Plenary and Technical Sessions and to participate in discussions, both formal and informal, thus facilitating the transfer of knowledge between nations, institutions, disciplines and individuals.\n\nThe WREC Renewable Energy Awards were established in 1998, during the 5th edition of the WREC Congress in Florence as a way to recognize outstanding achievement and vision in the global renewable energy sector.\n\nThe WREC Renewable Energy Awards aim at highlighting the worldwide best-implemented policies, projects and research in the following topics:\n\nWREN is a non-profit UK company (reg. no. 1874667) limited by guarantee and not having a share capital, incorporated in 1990 as a registered charity (No. 1009879), with registered offices in England. The aims and objectives of WREC/WREN are as follows:\n\n"}
{"id": "1460862", "url": "https://en.wikipedia.org/wiki?curid=1460862", "title": "Yakov Perelman", "text": "Yakov Perelman\n\nYakov Isidorovich Perelman (; December 4, 1882 – March 16, 1942) was a Russian and Soviet science writer and author of many popular science books, including \"Physics Can Be Fun\" and \"Mathematics Can Be Fun\" (both translated from Russian into English).\n\nPerelman was born in 1882 in the town of Białystok, Congress Poland. He obtained the Diploma in Forestry from the Imperial Forestry Institute (Now Saint Petersburg State Forest Technical University) in Saint Petersburg, in 1909. He was influenced by Ernst Mach and probably the Russian Machist Alexander Bogdanov in his pedagogical approach to popularising science. After the success of \"Physics for Entertainment\", Perelman set out to produce other books, in which he showed himself to be an imaginative populariser of science. Especially popular were \"\"Arithmetic for entertainment\", \"Mechanics for entertainment\", \"Geometry for Entertainment\", \"Astronomy for entertainment\", \"Lively Mathematics\", \" Physics Everywhere\", and \"Tricks and Amusements\".\n\nHis famous books on physics and astronomy were translated into various languages by the erstwhile Soviet Union.\n\nThe scientist Konstantin Tsiolkovsky thought highly of Perelman's talents and creative genius, writing of him in the preface of \"Interplanetary Journeys\": \"The author has long been known by his popular, witty and quite scientific works on physics, astronomy and mathematics, which are, moreover written in a marvelous language and are very readable.\"\n\nPerelman has also authored a number of textbooks and articles in Soviet popular science magazines.\n\nIn addition to his educational and scientific writings, he also worked as an editor of science magazines, including \"Nature and People\" and \"In the Workshop of Nature\".\n\nPerelman died from starvation in 1942, during the German Siege of Leningrad. The siege started at 9 September 1941 and lasted 872 days, until \n27 January 1944. The Siege of Leningrad was one of the longest, most destructive sieges of a major city in modern history and one of the costliest in terms of casualties (1,117,000).\n\nHis older brother Yosif was a writer who published under the pseudonym Osip Dymov. He is not related to the Russian mathematician Grigori Perelman, who was born in 1966 to a different Yakov Perelman. However, Grigori Perelman told The New Yorker that his father gave him \"Physics for Entertainment\", and it inspired his interest in mathematics.\n\n\nHe has also written several books on interplanetary travel (\"Interplanetary Journeys, On a Rocket to Stars, and World Expanses\")\n\nIn 1913, Russian bookshops began carrying \"Physics for Entertainment\". The educationalist's new book attracted young readers seeking answers to scientific questions.\n\n\"Physics for Entertainment\" had a unique layout as well as an instructive style. In the preface (11th ed.) Perelman wrote: \"The main objective of \"Physics for entertainment\" is to arouse the activity of scientific imagination, to teach the reader to think in the spirit of the science of physics and to create in his mind a wide variety of associations of physical knowledge with the widely differing facts of life, with all that he normally comes into contact with.\"\n\nIn the foreword, Perelman describes the contents as “conundrums, brain-teasers, entertaining anecdotes, and unexpected comparisons,” adding, “I have quoted extensively from Jules Verne, H. G. Wells, Mark Twain and other writers, because, besides providing entertainment, the fantastic experiments these writers describe may well serve as instructive illustrations at physics classes.” The 13th edition (1936) would be the last published during the author's lifetime. Among the book's notable topics was the idea of a perpetual machine: a hypothetical machine which could run incessantly performing useful work. The author discusses perpetual motion, highlighting many attempts to build such a machine, and explains why they failed. Other topics included how to jump from a moving car, and why, “according to the law of buoyancy, we would never drown in the Dead Sea.”\n\nRandall Munroe, the creator of the web comic xkcd and author of his own popular science books, wrote: \nThe book is a series of a few hundred examples, no more than one or two pages each, asking a question that illustrates some idea in basic physics.\n\nIt’s neat to see what has and hasn’t changed in the last century or so. Many of the examples he uses seem to be straight out of a modern high school physics textbook, while others were totally new to me. And some of the answers to the questions he poses seem obvious, but others made me stop and think. [This] diagram ... shows a design for a fountain with no pump — it took me a while to get why it works... Later in the book, he explains the physics of that drinking bird toy.\nIt’s written in a fun, engaging, conversational style, as if he’s in the room chatting with you about these neat ideas.\n\n\n"}
