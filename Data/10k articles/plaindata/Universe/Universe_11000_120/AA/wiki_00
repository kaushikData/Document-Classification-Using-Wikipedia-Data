{"id": "14306280", "url": "https://en.wikipedia.org/wiki?curid=14306280", "title": "2007 Zasyadko mine disaster", "text": "2007 Zasyadko mine disaster\n\nThe 2007 Zasyadko mine disaster was a mining accident that happened on November 18, 2007 at the Zasyadko coal mine () in the eastern Ukrainian city of Donetsk. \n\nBy 30 November, 101 miners were reported dead:\nthe worst accident in Ukraine’s history. At the time of the explosion, 457 miners were in the complex.\n\nThe accident was caused by a methane explosion located more than below ground level.\n\nThe Zasyadko Mine is considered one of the most dangerous coal mines in the world, An independent mining expert recently claimed that the company management, linked to a powerful local clan, interferes with hazard-measuring equipment on a permanent basis, in order to present underground situation as being within the safety standards, and so to prevent production from closure by the government inspectors. President Viktor Yushchenko blamed the cabinet for failing to “implement safe mining practices” in the coal industry. A criminal investigation is also underway.\n\nFamilies of the deceased miners will receive compensations totaling 100,000 hryvnias, (approx. $20,000 USD) which constitutes part of the 15 million hryvnias that the Cabinet of Ministers has set aside for renovation of the mine which would prevent future accidents from happening.\n\nOn November 19, 2007, President Viktor Yushchenko signed a decree that calls for investigation into Zasyadko mine disaster as well as prevention of such disasters in the future. The President also signed a decree making November 20 a Day of National Mourning. \n\nWithin the past decade, the frequency of mining accidents has increased in the Donbas coal region. The Zasyadko mine accident is the deadliest ever accident in Ukraine, surpassing the Barakova Mine accident in 2000, which killed at least 80 workers.\n\nThe Zasyadko Mine, Ukraine's largest and most equipped mine, employs 10,000 people and produces up to 10,000 tons of coal per day. Prior to this, four previous major mining accidents at the mine had killed a total of 148 workers combined.\n\nTwelve days later, on December 1, 2007, at 5:55 local time another methane explosion happened in the same mine section injuring 52 miners.\n\nAfter this, at 21:20 local time on December 2, another explosion occurred, killing at least 5 workers and injuring 30 more.\n\n"}
{"id": "7604764", "url": "https://en.wikipedia.org/wiki?curid=7604764", "title": "Beale number", "text": "Beale number\n\nIn mechanical engineering, the Beale number is a parameter that characterizes the performance of Stirling engines. It is often used to estimate the power output of a Stirling engine design. For engines operating with a high temperature differential, typical values for the Beale number range from ( 0.11 ) to ( 0.15 ); where a larger number indicates higher performance.\n\nThe Beale number can be defined in terms of a Stirling engine's operating parameters:\n\nwhere:\n\nTo estimate the power output of an engine, nominal values are assumed for the Beale number, pressure, swept volume and frequency, then the power is calculated as the product of these parameters, as follows:\n\n\n"}
{"id": "32081576", "url": "https://en.wikipedia.org/wiki?curid=32081576", "title": "Black powder rocket motor", "text": "Black powder rocket motor\n\nA black powder rocket motor propels a model rocket using black powder. Black powder rocket propellants consist of charcoal, sulfur, and potassium nitrate. Adjustments can be made to the amount of each component to change the rate at which the black powder burns.\n\nBlack powder rocket motors were created in a primitive form by the Chinese in the early 13th century, and through the years refinements have been made and several uses created. They have been used for weapons and surveillance devices as well as recreation.\n\nBlack powder rocket motors are only produced in small sizes, to reduce the risk of explosion and a loss of efficiency. Black powder rockets are produced in classes 1/4 A through F. Larger sizes of model rocket motors use ammonium perchlorate composite propellant, or other composite fuels that contain ammonium nitrate.\n\nBlack powder is the oldest composite propellant. Its use in rockets preceded its use in guns. The three main components of black powder are charcoal, sulfur, and saltpeter (or potassium nitrate). It is known that by 1045, the Chinese were producing black powder, because many references to the subject were found in The Wu-ching Tsung-Yao (Complete Compendium of Military Classics). In the early thirteenth century the Chinese turned black powder propelled objects, formerly only used for entertainment, into weapons of war. The first recorded use of rockets as military weapons was in 1232. The Chinese ‘arrows of fire’ were fired from a sort of catapult launcher. The black powder was packed in a closed tube that had a hole in one end for escaping hot gases, and a long stick as an elementary stability and guidance system.\n\nBlack powder had a very low specific impulse, however. Refinements in rocket design were made over the next few hundred years, at least on paper. In 1591 a Belgian, Jean Beavie, described and sketched the important idea of multistage rockets.\n\nBy 1600, rockets were being used in various parts of Europe against cavalry. By 1688, rockets weighing over 120 pounds had been built and fired with success in Germany. These German rockets, carrying 16-pound warheads, used wooden powder cases reinforced with linen.\n\nBlack powder rockets reached a new level of performance with the introduction of iron hulls and high-pressure combustion, developed in India by the engineers of Tipu Sultan. With a range of a kilometer, his rocket-propelled grenades and incendiaries took British invaders by surprise during the Anglo-Mysore Wars. Impressed by these weapons, a London lawyer, Sir William Congreve, became fascinated by the challenge of improving rockets. He experimented with propellants and case design. His systematic approach to the problem resulted in improved range, guidance (stabilization), and incendiary capabilities. The British armed forces used Congreve's new rockets to great advantage during the Napoleonic and 1812 Wars.\n\nIn 1939, researchers at the California Institute of Technology seeking to develop a high-performance solid rocket motor to assist aircraft take-off, combined black powder with common road asphalt to produce the first true composite motor. This was the birth of the true composite motor and marked the end of the use of black powder in major rocketry applications.\n\nBlack powder rocket propellant is very similar in makeup to old-fashioned gunpowder. The main difference is the presence of a binder, usually dextrin. The commonly used Estes model rocket engines are made with black powder propellant. Black powder propellant must be pressed very tightly in order to function well. Motors designed with black powder are most often end-burners, due to the fast burn rate of this propellant. A simple dextrin-free version (the most commonly used formulation) incorporates 75% potassium nitrate, 10% sulphur, and 15% charcoal. Dextrin may be added as desired (usually between 0 and 5%). Additional (coarse) charcoal or metal powders (5 - 10%) may be added to obtain an interesting spark trail. However, this may alter slightly the burn rate of the mixture.\n\nThe impulse (area under the thrust-time curve) of a black powder motor is used to determine its class. Motors are divided into classes from 1/4A to E, which covers an impulse range of 0 to 40 N·s (newton-seconds). Other types of model rocket motors can be classified up to an ‘H’, which is up to 320 Ns, and even further in some cases. Each class's upper limit is double the upper limit of the previous class.\n\nFigures from tests of Estes rocket motors are used in the following examples of rocket motor performance.\n\nFor miniature black powder rocket motors (13 mm diameter), the maximum thrust is between 5 and 12 N, the total impulse is between .5 and 2.2 Ns, and the burn time is between .25 and 1 second. For Estes ‘regular size’ rocket motors (18 mm diameter), there are three classes: A, B, and C. The A class 18 mm motors have a maximum thrust between 9.5 and 9.75 N, a total impulse between 2.1 and 2.3 Ns, and a burn time between .5 and .75 seconds. The B class 18 mm motors have a maximum thrust between 12.15 and 12.75 N, a total impulse between 4.2 and 4.35 Ns, and a burn time between .85 and 1 second. The C class 18mm motors have a maximum thrust from 14 – 14.15 N, a total impulse between 8.8 and 9 Ns, and a burn time between 1.85 and 2 seconds.\n\nThere are also 3 classes included in Estes large (24 mm diameter) rocket motors: C, D, and E. The C class 24 mm motors have a maximum thrust between 21.6 and 21.75 N, a total impulse of between 8.8 and 9 Ns, and a burn time between .8 and .85 seconds. The D class 24 mm motors have a maximum thrust between 29.7 and 29.8 N, a total impulse between 16.7 and 16.85 Ns, and a burn time between 1.6 and 1.7 seconds. The E class 24 mm motors have a maximum thrust between 19.4 and 19.5 N, a total impulse between 28.45 and 28.6 Ns, and a burn time between 3 and 3.1 seconds.\n\n"}
{"id": "55660368", "url": "https://en.wikipedia.org/wiki?curid=55660368", "title": "Bujagali–Tororo–Lessos High Voltage Power Line", "text": "Bujagali–Tororo–Lessos High Voltage Power Line\n\nBujagali–Tororo–Lessos High Voltage Power Line is a high voltage electricity power line, under construction, connecting the high voltage substation at Bujagali, in Uganda to another high voltage substation at Lessos, in Kenya.\n\nThe power line starts at Bujagali Hydroelectric Power Station, in Jinja District, as a 220kV high voltage power line. From here, the line travels to the eastern Ugandan town of Tororo, a distance of about . At Tororo, the voltage is stepped up to 400kV, and the line travels in that state, through the town of Eldoret, to Lessos, in Nandi County, a total distance of approximately .\n\nPower transmission line connects the electricity grid of Uganda to that of neighboring Kenya. It is in line with The Nile Equatorial Lakes Subsidiary Action Program (NELSAP), Interconnection of Electric Grids Project, led by Regional Manager, Grania Rubomboras.\n\nThis power line satisfies Uganda's need to export electricity to Kenya. It also satisfies Kenya's need to sell electricity to Uganda and the countries to the west of Kenya, including Rwanda, Burundi and the Democratic Republic of the Congo.\n\nThe Bujagali–Kenya border section is jointly funded by (a) the government of Uganda (GoU), (b) the African Development Bank (AfDB), and (c) the Japan International Cooperation Agency (JICA).\n\nThe Lessos–Uganda border section is jointly funded by the government of Kenya and the African Development Bank, at an initial cost of KSh2.3 billion. Construction is ongoing, with commercial commissioning, expected in December 2019.\n\n\n"}
{"id": "11052965", "url": "https://en.wikipedia.org/wiki?curid=11052965", "title": "Center for Northern Studies", "text": "Center for Northern Studies\n\nThe Center for Northern Studies in Wolcott in the U.S. state of Vermont was founded in 1971 by Steven B. Young as an interdisciplinary study center focused on the natural and human systems of the Circumpolar North, including the cultures and environments of Arctic Canada, Alaska, Scandinavia and Siberia. CNS offered semester and year-long opportunities for undergraduates interested in exploring northern issues, with northern field experiences incorporated into all semester programs. Courses ranged from field biology to cultural anthropology of Circumpolar cultures to northern archaeology. The Center also offered field study courses in Bear Swamp, an extensive tract of boreal forest wetland adjacent to the CNS facilities, which included classrooms, a laboratory, and a specialized Polar research library, which was open to the public. The Center operated as an independent non-profit until 2003 when CNS merged with Sterling College in Craftsbury Common, Vermont. The Center for Northern Studies at Sterling College operated from 2003 until 2010, when Sterling College phased out the focus on Northern Studies. The CNS facilities were sold in 2012, with Sterling College retaining ownership of Bear Swamp.\n"}
{"id": "24580596", "url": "https://en.wikipedia.org/wiki?curid=24580596", "title": "Cerium", "text": "Cerium\n\nCerium is a chemical element with symbol Ce and atomic number 58. Cerium is a soft, ductile and silvery-white metal that tarnishes when exposed to air, and it is soft enough to be cut with a knife. Cerium is the second element in the lanthanide series, and while it often shows the +3 oxidation state characteristic of the series, it also exceptionally has a stable +4 state that does not oxidize water. It is also considered one of the rare-earth elements. Cerium has no biological role and is not very toxic.\n\nDespite always occurring in combination with the other rare-earth elements in minerals such as those of the monazite and bastnäsite groups, cerium is easy to extract from its ores, as it can be distinguished among the lanthanides by its unique ability to be oxidized to the +4 state. It is the most common of the lanthanides, followed by neodymium, lanthanum, and praseodymium. It is the 26th-most abundant element, making up 66 ppm of the Earth's crust, half as much as chlorine and five times as much as lead.\n\nCerium was the first of the lanthanides to be discovered, in Bastnäs, Sweden by Jöns Jakob Berzelius and Wilhelm Hisinger in 1803, and independently by Martin Heinrich Klaproth in Germany in the same year. In 1839 Carl Gustaf Mosander became the first to isolate the metal. Today, cerium and its compounds have a variety of uses: for example, cerium(IV) oxide is used to polish glass and is an important part of catalytic converters. Cerium metal is used in ferrocerium lighters for its pyrophoric properties. Cerium-doped YAG phosphor is used in blue light-emitting diodes to produce white light.\n\nCerium is the second element of the lanthanide series. In the periodic table, it appears between the lanthanides lanthanum to its left and praseodymium to its right, and above the actinide thorium. It is a ductile metal with a hardness similar to that of silver. Its 58 electrons are arranged in the configuration [Xe]4f5d6s, of which the four outer electrons are valence electrons. Immediately after lanthanum, the 4f orbitals suddenly contract and are lowered in energy to the point that they participate readily in chemical reactions; however, this effect is not yet strong enough at cerium and thus the 5d subshell is still occupied. Most lanthanides can use only three electrons as valence electrons, as afterwards the remaining 4f electrons are too strongly bound: cerium is an exception because of the stability of the empty f-shell in Ce and the fact that it comes very early in the lanthanide series, where the nuclear charge is still low enough until neodymium to allow the removal of the fourth valence electron by chemical means.\n\nFour allotropic forms of cerium are known to exist at standard pressure, and are given the common labels of α to δ:\n\nCerium has a variable electronic structure. The energy of the 4f electron is nearly the same as that of the outer 5d and 6s electrons that are delocalized in the metallic state, and only a small amount of energy is required to change the relative occupancy of these electronic levels. This gives rise to dual valence states. For example, a volume change of about 10% occurs when cerium is subjected to high pressures or low temperatures. It appears that the valence changes from about 3 to 4 when it is cooled or compressed.\n\nAt lower temperatures the behavior of cerium is complicated by the slow rates of transformation. Transformation temperatures are subject to substantial hysteresis and values quoted here are approximate. Upon cooling below −15 °C, γ-cerium starts to change to β-cerium, but the transformation involves a volume increase and, as more β forms, the internal stresses build up and suppress further transformation. Cooling below approximately −160 °C will start formation of α-cerium but this is only from remaining γ-cerium. β-cerium does not significantly transform to α-cerium except in the presence of stress or deformation. At atmospheric pressure, liquid cerium is more dense than its solid form at the melting\npoint.\n\nNaturally occurring cerium is made up of four isotopes: Ce (0.19%), Ce (0.25%), Ce (88.4%), and Ce (11.1%). All four are observationally stable, though the light isotopes Ce and Ce are theoretically expected to undergo inverse double beta decay to isotopes of barium, and the heaviest isotope Ce is expected to undergo double beta decay to Nd or alpha decay to Ba. Additionally, Ce would release energy upon spontaneous fission. None of these decay modes have yet been observed, though the double beta decay of Ce, Ce, and Ce has been experimentally searched for. The current experimental limits for their half-lives are:\n\nAll other cerium isotopes are synthetic and radioactive. The most stable of them are Ce with a half-life of 284.9 days, Ce with a half-life of 137.6 days, Ce with a half-life of 33.04 days, and Ce with a half-life of 32.5 days. All other radioactive cerium isotopes have half-lives under four days, and most of them have half-lives under ten minutes. The isotopes between Ce and Ce inclusive occur as fission products of uranium. The primary decay mode of the isotopes lighter than Ce is inverse beta decay or electron capture to isotopes of lanthanum, while that of the heavier isotopes is beta decay to isotopes of praseodymium.\n\nThe great rarity of the proton-rich Ce and Ce is explained by the fact that they cannot be made in the most common processes of stellar nucleosynthesis for elements beyond iron, the s-process (slow neutron capture) and the r-process (rapid neutron capture). This is so because they are bypassed by the reaction flow of the s-process, and the r-process nuclides are blocked from decaying to them by more neutron-rich stable nuclides. Such nuclei are called p-nuclei, and their origin is not yet well understood: some speculated mechanisms for their formation include proton capture as well as photodisintegration. Ce is the most common isotope of cerium, as it can be produced in both the s- and r-processes, while Ce can only be produced in the r-process. Another reason for the abundance of Ce is that it is a magic nucleus, having a closed neutron shell (it has 82 neutrons), and hence it has a very low cross-section towards further neutron capture. Although its proton number of 58 is not magic, it is granted additional stability, as its eight additional protons past the magic number 50 enter and complete the 1 g proton orbital. The abundances of the cerium isotopes may differ very slightly in natural sources, because Ce and Ce are the daughters of the long-lived primordial radionuclides La and Nd, respectively.\n\nCerium tarnishes in air, forming a spalling oxide layer like iron rust; a centimeter-sized sample of cerium metal corrodes completely in about a year. It burns readily at 150 °C to form the pale-yellow cerium(IV) oxide, also known as ceria:\nThis may be reduced to cerium(III) oxide with hydrogen gas. Cerium metal is highly pyrophoric, meaning that when it is ground or scratched, the resulting shavings catch fire. This reactivity conforms to periodic trends, since cerium is one of the first and hence one of the largest lanthanides. Cerium(IV) oxide has the fluorite structure, similarly to the dioxides of praseodymium and terbium. Many nonstoichiometric chalcogenides are also known, along with the trivalent CeZ (Z = S, Se, Te). The monochalcogenides CeZ conduct electricity and would better be formulated as CeZe. While CeZ are known, they are polychalcogenides with cerium(III): cerium(IV) chalcogenides remain unknown.\nCerium is a highly electropositive metal and reacts with water. The reaction is slow with cold water but speeds up with increasing temperature, producing cerium(III) hydroxide and hydrogen gas:\n\nCerium metal reacts with all the halogens to give trihalides:\n\nReaction with excess fluorine produces the stable white tetrafluoride CeF; the other tetrahalides are not known. Of the dihalides, only the bronze diiodide CeI is known; like the diiodides of lanthanum, praseodymium, and gadolinium, this is a cerium(III) electride compound. True cerium(II) compounds are restricted to a few unusual organocerium complexes.\n\nCerium dissolves readily in dilute sulfuric acid to form solutions containing the colorless Ce ions, which exist as a [Ce(HO)] complexes:\n\nThe solubility of cerium is much higher in methanesulfonic acid. Cerium(III) and terbium(III) have ultraviolet absorption bands of relatively high intensity compared with the other lanthanides, as their configurations (one electron more than an empty or half-filled f-subshell respectively) make it easier for the extra f electron to undergo f→d transitions instead of the forbidden f→f transitions of the other lanthanides. Cerium(III) sulfate is one of the few salts whose solubility in water decreases with rising temperature.\nCerium(IV) aqueous solutions may be prepared by reacting cerium(III) solutions with the strong oxidising agents peroxodisulfate or bismuthate. The value of \"E\"(Ce/Ce) varies widely depending on conditions due to the relative ease of complexation and hydrolysis with various anions, though +1.72 V is a usually representative value; that for \"E\"(Ce/Ce) is −2.34 V. Cerium is the only lanthanide which has important aqueous and coordination chemistry in the +4 oxidation state. Due to ligand-to-metal charge transfer, aqueous cerium(IV) ions are orange-yellow. Aqueous cerium(IV) is metastable in water and is a strong oxidising agent that oxidizes hydrochloric acid to give chlorine gas. For example, ceric ammonium nitrate is a common oxidising agent in organic chemistry, releasing organic ligands from metal carbonyls. In the Belousov–Zhabotinsky reaction, cerium oscillates between the +4 and +3 oxidation states to catalyse the reaction. Cerium(IV) salts, especially cerium(IV) sulfate, are often used as standard reagents for volumetric analysis in cerimetric titrations.\n\nThe nitrate complex [Ce(NO)] is the most common cerium complex encountered when using cerium(IV) is an oxidising agent: it and its cerium(III) analogue [Ce(NO)] have 12-coordinate icosahedral molecular geometry, while [Ce(NO)] has 10-coordinate bicapped dodecadeltahedral molecular geometry. Cerium nitrates also form 4:3 and 1:1 complexes with 18-crown-6 (the ratio referring to that between cerium and the crown ether). Halogen-containing complex ions such as , , and the orange are also known. Organocerium chemistry is similar to that of the other lanthanides, being primarily that of the cyclopentadienyl and cyclooctatetraenyl compounds. The cerium(III) cyclooctatetraenyl compound has the uranocene structure.\n\nDespite the common name of cerium(IV) compounds, the Japanese spectroscopist Akio Kotani wrote \"there is no genuine example of cerium(IV)\". The reason for this can be seen in the structure of ceria itself, which always contains some octahedral vacancies where oxygen atoms would be expected to go and could be better considered a non-stoichiometric compound with chemical formula CeO. Furthermore, each cerium atom in ceria does not lose all four of its valence electrons, but retains a partial hold on the last one, resulting in an oxidation state between +3 and +4. Even supposedly purely tetravalent compounds such as CeRh, CeCo, or ceria itself have X-ray photoemission and X-ray absorption spectra more characteristic of intermediate-valence compounds. The 4f electron in cerocene, Ce(CH), is poised ambiguously between being localized and delocalized and this compound is also considered intermediate-valent.\n\nCerium was discovered in Bastnäs in Sweden by Jöns Jakob Berzelius and Wilhelm Hisinger, and independently in Germany by Martin Heinrich Klaproth, both in 1803. Cerium was named by Berzelius after the dwarf planet Ceres, discovered two years earlier. The dwarf planet itself is named after the Roman goddess of agriculture, grain crops, fertility and motherly relationships, Ceres.\n\nCerium was originally isolated in the form of its oxide, which was named \"ceria\", a term that is still used. The metal itself was too electropositive to be isolated by then-current smelting technology, a characteristic of rare-earth metals in general. After the development of electrochemistry by Humphry Davy five years later, the earths soon yielded the metals they contained. Ceria, as isolated in 1803, contained all of the lanthanides present in the cerite ore from Bastnäs, Sweden, and thus only contained about 45% of what is now known to be pure ceria. It was not until Carl Gustaf Mosander succeeded in removing lanthana and \"didymia\" in the late 1830s that ceria was obtained pure. Wilhelm Hisinger was a wealthy mine-owner and amateur scientist, and sponsor of Berzelius. He owned and controlled the mine at Bastnäs, and had been trying for years to find out the composition of the abundant heavy gangue rock (the \"Tungsten of Bastnäs\", which despite its name contained no tungsten), now known as cerite, that he had in his mine. Mosander and his family lived for many years in the same house as Berzelius, and Mosander was undoubtedly persuaded by Berzelius to investigate ceria further.\n\nCerium is the most abundant of all the lanthanides, making up 66 ppm of the Earth's crust; this value is just behind that of copper (68 ppm), and cerium is even more abundant than common metals such as lead (13 ppm) and tin (2.1 ppm). Thus, despite its position as one of the so-called rare-earth metals, cerium is actually not rare at all. Cerium content in the soil varies between 2 and 150 ppm, with an average of 50 ppm; seawater contains 1.5 parts per trillion of cerium. Cerium occurs in various minerals, but the most important commercial sources are the minerals of the monazite and bastnäsite groups, where it makes up about half of the lanthanide content. Monazite-(Ce) is the most common representative of the monazites, with \"-Ce\" being the Levinson suffix informing on the dominance of the particular REE element representative.). Also the cerium-dominant bastnäsite-(Ce) is the most important of the bastnäsites. Cerium is the easiest lanthanide to extract from its minerals because it is the only one that can reach a stable +4 oxidation state in aqueous solution. Because of the decreased solubility of cerium in the +4 oxidation state, cerium is sometimes depleted from rocks relative to the other rare-earth elements and is incorporated into zircon, since Ce and Zr have the same charge and similar ionic radii. In extreme cases, cerium(IV) can form its own minerals separated from the other rare-earth elements, such as cerianite (correctly named cerianite-(Ce)), (Ce,Th)O.\nBastnäsite, LnCOF, is usually lacking in thorium and the heavy lanthanides beyond samarium and europium, and hence the extraction of cerium from it is quite direct. First, the bastnäsite is purified, using dilute hydrochloric acid to remove calcium carbonate impurities. The ore is then roasted in the air to oxidize it to the lanthanide oxides: while most of the lanthanides will be oxidized to the sesquioxides LnO, cerium will be oxidized to the dioxide CeO. This is insoluble in water and can be leached out with 0.5 M hydrochloric acid, leaving the other lanthanides behind.\n\nThe procedure for monazite, (Ln,Th)PO, which usually contains all the rare earths, as well as thorium, is more involved. Monazite, because of its magnetic properties, can be separated by repeated electromagnetic separation. After separation, it is treated with hot concentrated sulfuric acid to produce water-soluble sulfates of rare earths. The acidic filtrates are partially neutralized with sodium hydroxide to pH 3–4. Thorium precipitates out of solution as hydroxide and is removed. After that, the solution is treated with ammonium oxalate to convert rare earths to their insoluble oxalates. The oxalates are converted to oxides by annealing. The oxides are dissolved in nitric acid, but cerium oxide is insoluble in HNO and hence precipitates out. Care must be taken when handling some of the residues as they contain Ra, the daughter of Th, which is a strong gamma emitter.\n\nThe first use of cerium was in gas mantles, invented by the Austrian chemist Carl Auer von Welsbach. In 1885, he had previously experimented with mixtures of magnesium, lanthanum, and yttrium oxides, but these gave green-tinted light and were unsuccessful. Six years later, he discovered that pure thorium oxide produced a much better, though blue, light, and that mixing it with cerium dioxide resulted in a bright white light. Additionally, cerium dioxide also acts as a catalyst for the combustion of thorium oxide. This resulted in great commercial success for von Welsbach and his invention, and created great demand for thorium; its production resulted in a large amount of lanthanides being simultaneously extracted as by-products. Applications were soon found for them, especially in the pyrophoric alloy known as \"mischmetall\" composed of 50% cerium, 25% lanthanum, and the remainder being the other lanthanides, that is used widely for lighter flints. Usually, iron is also added to form an alloy known as ferrocerium, also invented by von Welsbach. Due to the chemical similarities of the lanthanides, chemical separation is not usually required for their applications, such as the mixing of mischmetall into steel to improve its strength and workability, or as catalysts for the cracking of petroleum. This property of cerium saved the life of writer Primo Levi at the Auschwitz concentration camp, when he found a supply of ferrocerium alloy and bartered it for food.\n\nCeria is the most widely used compound of cerium. The main application of ceria is as a polishing compound, for example in chemical-mechanical planarization (CMP). In this application, ceria has replaced other metal oxides for the production of high-quality optical surfaces. Major automotive applications for the lower sesquioxide are as a catalytic converter for the oxidation of CO and NO emissions in the exhaust gases from motor vehicles, Ceria has also been used as a substitute for its radioactive congener thoria, for example in the manufacture of electrodes used in gas tungsten arc welding, where ceria as an alloying element improves arc stability and ease of starting while decreasing burn-off. Cerium(IV) sulfate is used as an oxidising agent in quantitative analysis. Cerium(IV) in methanesulfonic acid solutions is applied in industrial scale electrosynthesis as a recyclable oxidant. Ceric ammonium nitrate is used as an oxidant in organic chemistry and in etching electronic components, and as a primary standard for quantitative analysis.\n\nThe photostability of pigments can be enhanced by the addition of cerium. It provides pigments with light fastness and prevents clear polymers from darkening in sunlight. Television glass plates are subject to electron bombardment, which tends to darken them by creation of F-center color centers. This effect is suppressed by addition of cerium oxide. Cerium is also an essential component of phosphors used in TV screens and fluorescent lamps. Cerium sulfide forms a red pigment that stays stable up to 350 °C. The pigment is a nontoxic alternative to cadmium sulfide pigments.\n\nCerium is used as alloying element in aluminum to create castable eutectic alloys, Al-Ce alloys with 6–16 wt.% Ce, to which Mg and/or Si can be further added; these alloys have excellent high temperature strength.\n\nCerium has no known biological role in humans, but is not very toxic either; it does not accumulate in the food chain to any appreciable extent. Because it often occurs together with calcium in phosphate minerals, and bones are primarily calcium phosphate, cerium can accumulate in bones in small amounts that are not considered dangerous. Cerium, like the other lanthanides, is known to affect human metabolism, lowering cholesterol levels, blood pressure, appetite, and risk of blood coagulation. Cerium nitrate is an effective topical antimicrobial treatment for third-degree burns, although large doses can lead to cerium poisoning and methemoglobinemia. The early lanthanides act as essential cofactors for the methanol dehydrogenase of the methanotrophic bacterium \"Methylacidiphilum fumariolicum\" SolV, for which lanthanum, cerium, praseodymium, and neodymium alone are about equally effective.\n\nLike all rare-earth metals, cerium is of low to moderate toxicity. A strong reducing agent, it ignites spontaneously in air at 65 to 80 °C. Fumes from cerium fires are toxic. Water should not be used to stop cerium fires, as cerium reacts with water to produce hydrogen gas. Workers exposed to cerium have experienced itching, sensitivity to heat, and skin lesions. Cerium is not toxic when eaten, but animals injected with large doses of cerium have died due to cardiovascular collapse. Cerium is more dangerous to aquatic organisms, on account of being damaging to cell membranes, but this is not an important risk because it is not very soluble in water.\n"}
{"id": "32276019", "url": "https://en.wikipedia.org/wiki?curid=32276019", "title": "Coalition for Green Capital", "text": "Coalition for Green Capital\n\nThe Coalition for Green Capital (CGC) is a 501c3 non-profit that works with governments at the international, national, state and local level to establish Green Bank finance institutions to accelerate the deployment of clean energy technology. CGC has designed and created clean energy financing institutions, or “Green Banks.” In the US, Green Banks have collectively sparked over $2 billion in clean energy investments.\n\nThe Coalition for Green Capital’s mission is to accelerate the transition to the clean energy economy by establishing Green Banks at the local, state, federal, and international levels to spur greater private investment in renewables, energy efficiency, and clean transportation.\n\nCGC partners with governments throughout the U.S. and abroad to design and launch Green Banks tailored to the specific needs of each market. Through partnerships with existing Green Banks and research, CGC investigates and develops new ways that Green Banks can grow clean energy markets. CGC also works with policymakers, industry associations, and other key stakeholders to share information about the Green Bank concept, and what Green Banks have already achieved. As a non-profit, CGC is primarily funded by many of the leading climate and energy focused-foundations in the nation.\n\nThe Coalition for Green Capital (CGC) was created as an outgrowth of Reed Hundt and Kenneth Berlin’s efforts with the Obama-Biden Transition Team to promote financing for clean energy and energy efficiency. CGC has since become the nation’s leading expert on Green Bank institution creation and financing.\n\nThe CGC was created coming off of the Obama-Biden Transition Team in 2009, and initially focused on advocating for a federal-level Green Bank. The Green Bank concept supported by the CGC was included in the Waxman-Markey climate change bill that passed the US House of Representatives in 2009. However, climate change legislation was unable to pass the Senate, and the CGC turned its attention to creating a state-level Green Bank.\n\nThe CGC helped newly elected Governor Dannel Malloy (working closely with his newly appointed Commissioner of Energy and Environmental Protection, Dan Esty) re-purpose the Connecticut Clean Energy Fund to create the nation’s first Green Bank. The Connecticut Green Bank has since demonstrated the power of the Green Bank model, using limited public funds to attract over $491.2m of private investment in the Connecticut clean energy economy from 2012-2015.\n\nSince that time, and based on Connecticut’s success, interest in the Green Bank concept has grown significantly. CGC now works in over a dozen states that are at some state of Green Bank development or consideration. CGC continues to work at the federal level, supporting the introduce of the Green Bank Acts of 2014 and 2016. And CGC now works increasingly at the international level, particularly with the formation of the Global Green Bank Network.\n\nCGC lists its on-going work on its website.\n\nCGC provided technical support for the Green Bank Act of 2016, which was introduced in the House by Congressman Chris Van Hollen (D-MD) on July 14, 2016 with seven co-sponsors. CGC also provided technical support for the companion legislation that was introduced in the Senate by Senators Chris Murphy (D-CT), Richard Blumenthal (D-CT) and Sheldon Whitehouse (D-RI) on September 22, 2016.\n\nCGC has played an extensive role in the creation of the Montgomery County Green Bank (MCGB), the first local green bank in the country. CGC provided technical guidance in drafting legislation in line with the County’s needs, worked with County staff to manage the working group process, and is helping stand up and operationalize the new green bank.\n\nCGC and NRDC, along with six founding green bank members, launched the Green Bank Network (GBN) in December 2015. Since then, CGC and NRDC been building the organization. At a White House-sponsored side event of the Clean Energy Ministerial (CEM) in May 2016, the GBN announced the signing of a Memorandum of Understanding among founding members agreeing to principles and mission of the GBN.\n\n----External links\n"}
{"id": "1643492", "url": "https://en.wikipedia.org/wiki?curid=1643492", "title": "Cosmic latte", "text": "Cosmic latte\n\nCosmic latte is a name assigned to the average color of the universe, found by a team of astronomers from Johns Hopkins University. In 2001, Karl Glazebrook and Ivan Baldry determined that the average color of the universe was a greenish white, but they soon corrected their analysis in a 2002 paper in which they reported that their survey of the light from over 200,000 galaxies averaged to a slightly beigeish white. The hex triplet value for cosmic latte is #FFF8E7.\n\nFinding the average color of the universe was not the focus of the study. Rather, the study examined spectral analysis of different galaxies to study star formation. Like Fraunhofer lines, the dark lines displayed in the study's spectral ranges display older and younger stars and allow Glazebrook and Baldry to determine the age of different galaxies and star systems. What the study revealed is that the overwhelming majority of stars formed about 5 billion years ago. Because these stars would have been \"brighter\" in the past, the color of the universe changes over time shifting from blue to red as more blue stars change to yellow and eventually red giants.\n\nAs light from distant galaxies reaches the Earth, the average \"color of the universe\" (as seen from Earth) tends towards pure white, due to the light coming from the stars when they were much younger and bluer.\n\nThe corrected color was initially published on the Johns Hopkins News website and updated on the team's initial announcement. Multiple news outlets, including NPR and BBC, displayed the color in stories and some relayed the request by Glazebrook on the announcement asking for suggestions for names, jokingly adding all were welcome as long as they were not \"beige\".\n\nThese were the results of a vote of the scientists involved based on the new color:\nThough Drum's suggestion of \"cappuccino cosmico\" received the most votes, the researchers favored Drum's other suggestion, \"cosmic latte\". This is because the similar \"Latteo\" means \"Milky\" in Italian, Galileo's native language. It also leads to the similarity to the Italian term for the Milky Way, \"Via Lattea\", and they enjoyed the fact that the color would be similar to the Milky Way's average color as well, as it is part of the sum of the universe. They also claimed to be \"caffeine biased\".\n\nDrum came up with the name while sitting in a Starbucks drinking a latte and reading the \"Washington Post\". Drum noticed that the color of the universe as displayed in the newspaper was the same color as his latte.\n\n"}
{"id": "949651", "url": "https://en.wikipedia.org/wiki?curid=949651", "title": "Criticality accident", "text": "Criticality accident\n\nA criticality accident is an uncontrolled nuclear fission chain reaction. It is sometimes referred to as a critical excursion or a critical power excursion or a divergent chain reaction.\n\nAny such event involves the unintended accumulation or arrangement of a critical mass of fissile material, for example enriched uranium or plutonium. Criticality accidents can release potentially fatal radiation doses, if they occur in an unprotected environment.\n\nUnder normal circumstances, a critical or supercritical fission reaction (one that is self-sustaining in power or increasing in power) should only occur inside a safely shielded location, such as a reactor core or a suitable test environment. A criticality accident occurs if the same reaction is achieved unintentionally, for example in an unsafe environment or during reactor maintenance.\n\nThough dangerous and frequently lethal to humans within the immediate area, the critical mass formed would not be capable of producing a massive nuclear detonation of the type that fission bombs are designed to produce. This is because all the design features needed to make a nuclear warhead cannot arise by chance.\n\nIn some cases, the heat released by the chain reaction will cause the fissile (and other nearby) materials to expand. In such cases, the chain reaction can either settle into a low power steady state or may even become either temporarily or permanently shut down (subcritical).\n\nIn the history of atomic power development, at least 60 criticality accidents have occurred, including 22 in process environments, outside nuclear reactor cores or experimental assemblies, and 38 in small experimental reactors and other test assemblies.\n\nAlthough process accidents occurring outside reactors are characterized by large releases of radiation, the releases are localized. Nonetheless, fatal radiation exposures have occurred to persons close to these events, resulting in 14 fatalities. In a few cases, the energy released has caused significant mechanical damage or even small explosions.\n\nCriticality occurs when sufficient fissile material (a \"critical mass\") is in one place such that each fission of an atom of the material, on average, produces a neutron that in turn strikes another atom causing another fission; this causes the chain reaction to become self-sustaining within the mass of material.\n\nCriticality can be achieved by using metallic uranium or plutonium or by mixing compounds or liquid solutions of these elements. The chain reaction is influenced by range of parameters noted by the acronyms MAGIC MERV (for Mass, Absorption, Geometry, Interaction, Concentration, Moderation, Enrichment, Reflection and Volume) and MERMAIDS (for Mass, Enrichment, Reflection, Moderation, Absorption, Interaction, Density and Shape). Temperature can also be a key factor.\n\nComplex calculations can be performed to predict the conditions needed to arrange materials into a critical state. Where fissile materials are handled in civil and military installations, specially trained personnel are employed to carry out such calculations, and to ensure that all reasonably practicable measures are used to prevent criticality accidents, during both planned normal operations and any potential process upset conditions that cannot be dismissed on the basis of negligible likelihoods.\n\nThe assembly of a critical mass establishes a nuclear chain reaction, resulting in an exponential rate of change in the neutron population over space and time leading to neutron radiation and a neutron flux. This radiation contains both a neutron and gamma ray component and is extremely dangerous to any unprotected nearby life-form. The rate of change of neutron population depends on the neutron generation time, which is characteristic of the neutron population, the state of \"criticality\", and the fissile medium.\n\nA nuclear fission creates approximately 2.5 neutrons per fission event on average. Hence, to maintain a stable, exactly critical chain reaction, 1.5 neutrons per fission event must either leak from the system or be absorbed without causing further fissions.\n\nFor every 1000 neutrons released by fission, a small number, typically no more than about 7, are delayed neutrons which are emitted from the fission product precursors, called \"delayed neutron emitters\". This delayed neutron fraction, on the order of 0.007 for uranium, is crucial for the control of the neutron chain reaction in reactors. It is called one dollar of reactivity. The lifetime of delayed neutrons ranges from fractions of seconds to almost 100 seconds after fission. The neutrons are usually classified in 6 delayed neutron groups. The average neutron lifetime considering delayed neutrons is approximately 0.1 sec, which makes the chain reaction relatively easy to control over time. The remaining 993 prompt neutrons are released very quickly, approximately 1 μs after the fission event.\n\nIn steady state operation, nuclear reactors operate at exact criticality. When at least one dollar of reactivity is added above the exact critical point (where the neutron production rate balances the rate of neutron losses, from both absorption and leakage) then the chain reaction does not rely on delayed neutrons. In such cases, the neutron population can rapidly increase exponentially, with a very small time constant, known as the prompt neutron lifetime. Thus there is a very large increase in neutron population over a very short time frame. Since each fission event contributes approximately 200 MeV per fission, this results in a very large energy burst as a \"prompt critical spike\". This spike can be easily detected by radiation dosimetry instrumentation and \"criticality accident alarm system\" detectors that are properly deployed.\n\nCriticality accidents are divided into one of two categories:\n\n\nExcursion types can be classified into four categories depicting the nature of the evolution over time:\n\n\nThe prompt critical excursion is characterized by a power history with an initial prompt critical spike as previously noted, that either self terminates or continues for an extended period as a tail region that decreases over time. The transient critical excursion is characterized by a continuing or repeating spike pattern (sometimes known as \"chugging\") after the initial prompt critical excursion. The longest of the 22 process accidents occurred at Hanford Works in 1962 and lasted for 37.5 hours. The 1999 Tokaimura nuclear accident remained critical for about 20 hours, until it was shut down by active intervention. The exponential excursion is characterized by a reactivity of less than one dollar added, where the neutron population rises as an exponential over time, until either feedback effects or intervention reduce the reactivity. The exponential excursion can reach a peak power level, then decrease over time, or reach a steady state power level, where the critical state is exactly achieved for a \"steady state\" excursion.\n\nThe steady state excursion is also a state which the heat generated by fission is balanced by the heat losses to the ambient environment. This excursion has been characterized by the Oklo natural reactor that was naturally produced within uranium deposits in Gabon, Africa about 1.7 billion years ago.\n\nAt least sixty criticality accidents have been recorded since 1945. These have caused at least twenty-one deaths: seven in the United States, ten in the Soviet Union, two in Japan, one in Argentina, and one in Yugoslavia. Nine have been due to process accidents, and the others from research reactor accidents.\n\nCriticality accidents have occurred both in the context of nuclear weapons and nuclear reactors.\n\nThere was speculation although not confirmed within criticality accident experts, that Fukushima 3 suffered a criticality accident. Based on incomplete information about the 2011 Fukushima I nuclear accidents, Dr. Ferenc Dalnoki-Veress speculates that transient criticalities may have occurred there. Noting that limited, uncontrolled chain reactions might occur at Fukushima I, a spokesman for the International Atomic Energy Agency (IAEA) “emphasized that the nuclear reactors won’t explode.” By March 23, 2011, neutron beams had already been observed 13 times at the crippled Fukushima nuclear power plant. While a criticality accident was not believed to account for these beams, the beams could indicate nuclear fission is occurring. On April 15, TEPCO reported that nuclear fuel had melted and fallen to the lower containment sections of three of the Fukushima I reactors, including reactor three. The melted material was not expected to breach one of the lower containers, which could cause a massive radioactivity release. Instead, the melted fuel is thought to have dispersed uniformly across the lower portions of the containers of reactors No. 1, No. 2 and No. 3, making the resumption of the fission process, known as a \"recriticality\", most unlikely.\n\nMany criticality accidents have been observed to emit a blue flash of light.\n\nThe blue glow of a criticality accident can result from the fluorescence of the excited ions, atoms and molecules of air (mostly oxygen and nitrogen) falling back to unexcited states, which produces an abundance of blue light. This is also the reason electrical sparks in air, including lightning, appear electric blue. The smell of ozone was said to be a sign of high ambient radioactivity by Chernobyl liquidators.\n\nThis blue flash or \"blue glow\" can also be attributed to Cherenkov radiation, if either water is involved in the critical system or when the blue flash is experienced by the human eye .\n\nIt is a coincidence that the color of Cherenkov light and light emitted by ionized air are a very similar blue; their methods of production are different. Cherenkov radiation does occur in air for high-energy particles (such as particle showers from cosmic rays) but not for the lower energy charged particles emitted from nuclear decay.\n\nIn a nuclear setting, Cherenkov radiation is instead seen in dense media such as water or in a solution such as uranyl nitrate in a reprocessing plant. Cherenkov radiation could also be responsible for the \"blue flash\" experienced in an excursion due to the intersection of particles with the vitreous humour within the eyeballs of those in the presence of the criticality. This would also explain the absence of any record of blue light in video surveillance of the more recent incidents.\n\nSome people reported feeling a \"heat wave\" during a criticality event. It is not known whether this may be a psychosomatic reaction to the terrifying realization of what has just occurred (i.e. the high probability of inevitable impending death from a fatal radiation dose), or if it is a physical effect of heating (or nonthermal stimulation of heat sensing nerves in the skin) due to energy emitted by the criticality event.\n\nA review of all of the criticality accidents with eyewitness accounts indicates that the heat waves were only observed when the fluorescent blue glow (the non-Cherenkov light, see above) was also observed. This would suggest a possible relationship between the two, and indeed, one can be potentially identified. In dense air, over 30% of the emissions lines from nitrogen and oxygen are in the ultraviolet range, and about 45% are in the infrared range. Only about 25% are in the visible range. Since the skin feels light (visible or otherwise) through its heating of the skin surface, it is possible that this phenomenon can explain the heat wave perceptions. However, this explanation has still not been confirmed and may be inconsistent with the intensity of light reported by witnesses compared to the intensity of heat perceived. Further research is hindered by the small amount of data available from the few instances where humans have witnessed these incidents and survived long enough to provide a detailed account of their experiences and observations.\n\n\n\n\n"}
{"id": "27170531", "url": "https://en.wikipedia.org/wiki?curid=27170531", "title": "Croton sylvaticus", "text": "Croton sylvaticus\n\nCroton sylvaticus is a tree in the Euphorbiaceae family. It is commonly known as the forest fever-berry. These trees are distributed in forests from the east coast of South Africa to Tropical Africa. It grows 7–13 m in height, occasionally up to 30 m, in moist forests, thickets and forest edges at altitudes of 350–1800 m.\n\nGreenish cream flowers, up to 3 mm long (all male or female or mixed flowers), in racemes, 10–30 cm long. Fruit, light green when young, turning to orange or red, trilobed, oval in shape, hairy.\n\nUsed as a general timber, for poles, posts and as a fuel.\n\nSap from leaves is used for healing cuts, bark is used in the treatment of malaria, a decoction from the bark of the roots is taken orally as a remedy for tuberculosis, an infusion of the leaves acts as a purgative.\n\n\n"}
{"id": "8483704", "url": "https://en.wikipedia.org/wiki?curid=8483704", "title": "Crown of Thorns (woodworking)", "text": "Crown of Thorns (woodworking)\n\nThe Crown of Thorns (puzzle work) is a woodworking technique of tramp art using interlocking wooden pieces that are notched to intersect at right angles forming joints and self-supporting objects, objects that have a \"prickly\" and transparent quality. Common examples include wreath-shaped picture frames that look similar to Jesus' \"crown of thorns\". \n\nLarger-scale crowns may use the principles of tensegrity structures, where the wooden sticks provide rigidity and separate cables in tension carry the forces that hold them together.\n\n\n"}
{"id": "1862137", "url": "https://en.wikipedia.org/wiki?curid=1862137", "title": "Digital paper", "text": "Digital paper\n\nDigital paper, also known as interactive paper, is patterned paper used in conjunction with a digital pen to create handwritten digital documents. The printed dot pattern uniquely identifies the position coordinates on the paper. The digital pen uses this pattern to store the handwriting and upload it to a computer.\n\nThe dot pattern is a kind of two dimensional barcode; the most common is the proprietary Anoto dot pattern. In the Anoto dot pattern, the paper is divided into a grid with a spacing of about 0.3 mm, a dot is printed near each intersection offset slightly in one of four directions, a camera in the pen typically records a 6 x 6 group of dots. The full pattern is claimed to consist of 669,845,157,115,773,458,169 dots, and to encompass an area exceeding 4.6 million km² (this corresponds to 73 trillion unique sheets of letter-size paper).\n\nThe complete pattern space is divided into various domains. These domains can be used to define paper types, or to indicate the paper's purpose (for example, memo formatting, personal planners, notebook paper, Post-it notes, et cetera).\n\nThe Anoto dot pattern can be printed onto almost any paper, using a standard printing process of at least 600 dpi resolution (some claim a required resolution of 1,000 dpi), and carbon-based black ink. The paper can be any shape or size greater than 2 mm to a side. The ink absorbs infra red light transmitted from the digital pen; the pen contains a receiver which interprets the pattern of light reflected from the paper. Other colors of ink, including non-carbon-based black, can be used to print information which will be visible to the user, and invisible to the pen.\n\n\n"}
{"id": "19887587", "url": "https://en.wikipedia.org/wiki?curid=19887587", "title": "Eco-Agents", "text": "Eco-Agents\n\nEco-Agents () is a member-based environmental organization for children. A subsidiary of the Norwegian Society for the Conservation of Nature, it has 3000 members. The three focus issues are conservation of nature, global warming and consumption.\n\nThe organization was founded in 1992 as \"Blekkulfs venner\", later \"Blekkulfts miljødetektiver\". In 2006 it changed to the current name, and discontinued its cooperation with the mascot \"Blekkulf\", a fictional octopus. The reason was in part to communicate better with older children, and in part because the organization had problems funding the license fees to the creators of the fictional characters.\n\n"}
{"id": "34601448", "url": "https://en.wikipedia.org/wiki?curid=34601448", "title": "Electricity Act 1989", "text": "Electricity Act 1989\n\nThe Electricity Act 1989 provided for the privatisation of the electricity supply industry in Great Britain, replacing the Central Electricity Generating Board in England and Wales and in Scotland by the South of Scotland Electricity Board and the North of Scotland Hydro-Electric Board. The Act also established a licensing regime and a regulator for the industry called the Office of Electricity Regulation (OFFER), which has since become the Office of Gas and Electricity Markets (OFGEM).\n\n\n"}
{"id": "37681343", "url": "https://en.wikipedia.org/wiki?curid=37681343", "title": "Enlil-bānī land grant kudurru", "text": "Enlil-bānī land grant kudurru\n\nThe Enlil-bānī land grant kudurru is an ancient Mesopotamian \"narû ša ḫaṣbi\", or clay stele, recording the confirmation of a beneficial grant of land by Kassite king Kadašman-Enlil I (ca. 1374–1360 BC) or Kadašman-Enlil II (1263-1255 BC) to one of his officials. It is actually a terra-cotta cone, extant with a duplicate, the orientation of whose inscription, perpendicular to the direction of the cone, in two columns and with the top facing the point, indicates it was to be erected upright, (on its now eroded base), like other entitlement documents of the period. \n\nExcavated by Hormuzd Rassam on behalf of the British Museum at Abu Habba, ancient Sippar, accessioned in 1883 and given Museum references BM 91036 and BM 135743, the cones stand around 25 cm in height and have both lost their bases. They commemorate the donation of a sixty field in twenty-three preserved lines on two columns and are without evidence of any of the sculptured religious iconography usually associated with this type of monument. \nThe donor of the original grant is identified as Kurigalzu I, son of Kadašman-Ḫarbe I. The clay cone memorializes the confirmation of this land grant on Enlil-bānī’s son or descendant, possibly his immediate successor to the office of \"nišakku-\"priest, Ninurta-nādin-aḫḫē, by Kadašman-Enlil I, the monarch under whom he attained this office, or alternatively a descendant under the later reign of Kadašman-Enlil II. Brinkman considered that there was no compelling reason for either choice. These kings' names are written with the divine determinative: \"ka-daš-man\" \"en-líl\" and \"ku-ri-gal-zu\", not normally considered a characteristic of Kassite king-names prior to the reign of Kurigalzu II (ca. 1332–1308 BC) although the evidence is scanty. The inscription is important as it was the first to distinguish unambiguously that Kadašman-Ḫarbe and Kadašman-Enlil were two different people and that, although the Kassite deity Ḫarbe was considered equivalent in their pantheon to Enlil, as witnessed on a Kassite-Babylonian vocabulary or synonym list, he was inscribed quite differently.\n\nAn individual by the name of Enlil-bānī is known in the genealogy of several people, such as his grandson, Enlil-kidinnī, who would become the prominent \"šandabakku\" or governor of Nippur, and a descendant, Ninurta-rēṣušu, who was also to enjoy the post of \"nišakku-\"priest during the reign of Nazi-Maruttaš (ca. 1307–1282 BC), where Enlil-bānī is identified as having been the \"rabânum\" of , or mayor of the town later known as Dur-Kurigalzu. If this identification is correct, it would favor a dating of the artifact to the reign of Kadašman-Enlil I.\n\n \n"}
{"id": "14104932", "url": "https://en.wikipedia.org/wiki?curid=14104932", "title": "European Renewable Energy Council", "text": "European Renewable Energy Council\n\nThe European Renewable Energy Council (EREC) was founded in 2000 by the European renewable energy industry, trade and research associations. EREC is located in the Renewable Energy House in Brussels, a monument protected building with 100% renewable energy supply for heating and cooling.\n\nEREC acts as a representative in Brussels of the European Renewable industry and research community and acts as a forum for exchange of information and discussion on issues related to renewables. EREC provides information and consultancy on renewable energies for the political decision makers on local, regional, national and international levels.\n\nIn May 2014 the General Assembly of EREC decided for a voluntary dissolution which led to the liquidation of the association.\n\nEREC is composed of the following non-profit associations and federations: \n\nIn the Greenpeace and EREC's Energy (R)evolution scenario, the world could eliminate fossil fuel use by 2090.\n\nOn the other hand, according to EREC RE-thinking 2050, Europe can be a renewable-energy economy (using only renewable energy) by 2050.\n\n\n\n"}
{"id": "6499752", "url": "https://en.wikipedia.org/wiki?curid=6499752", "title": "Fault (power engineering)", "text": "Fault (power engineering)\n\nIn an electric power system, a fault or fault current is any abnormal electric current. For example, a short circuit is a fault in which current bypasses the normal load. An open-circuit fault occurs if a circuit is interrupted by some failure. In three-phase systems, a fault may involve one or more phases and ground, or may occur only between phases. In a \"ground fault\" or \"earth fault\", current flows into the earth. The prospective short-circuit current of a predictable fault can be calculated for most situations. In power systems, protective devices can detect fault conditions and operate circuit breakers and other devices to limit the loss of service due to a failure.\n\nIn a polyphase system, a fault may affect all phases equally which is a \"symmetrical fault\". If only some phases are affected, the resulting \"asymmetrical fault\" becomes more complicated to analyse. The analysis of these types of faults is often simplified by using methods such as symmetrical components.\n\nThe design of systems to detect and interrupt power system faults is the main objective of power-system protection.\n\nA transient fault is a fault that is no longer present if power is disconnected for a short time and then restored; or an insulation fault which only temporarily affects a device's dielectric properties which are restored after a short time. Many faults in overhead power lines are transient in nature. When a fault occurs, equipment used for power system protection operate to isolate the area of the fault. A transient fault will then clear and the power-line can be returned to service. Typical examples of transient faults include:\nTransmission and distribution systems use an automatic re-close function which is commonly used on overhead lines to attempt to restore power in the event of a transient fault. This functionality is not as common on underground systems as faults there are typically of a persistent nature. Transient faults may still cause damage both at the site of the original fault or elsewhere in the network as fault current is generated.\n\nA persistent fault is present regardless of power being applied. Faults in underground power cables are most often persistent due to mechanical damage to the cable, but are sometimes transient in nature due to lightning.\n\nA symmetric or balanced fault affects each of the three phases equally. In transmission line faults, roughly 5% are symmetric. This is in contrast to an asymmetrical fault, where the three phases are not affected equally.\n\nAn asymmetric or unbalanced fault does not affect each of the three phases equally. Common types of asymmetric faults, and their causes:\n\nOne extreme is where the fault has zero impedance, giving the maximum prospective short-circuit current. Notionally, all the conductors are considered connected to ground as if by a metallic conductor; this is called a \"bolted fault\". It would be unusual in a well-designed power system to have a metallic short circuit to ground but such faults can occur by mischance. In one type of transmission line protection, a \"bolted fault\" is deliberately introduced to speed up operation of protective devices.\n\nA ground fault (earth fault) is any failure that allows unintended connection of power circuit conductors with the earth. Such faults can cause objectionable circulating currents, or may energize the housings of equipment at a dangerous voltage. Some special power distribution systems may be designed to tolerate a single ground fault and continue in operation. Wiring codes may require an insulation monitoring device to give an alarm in such a case, so the cause of the ground fault can be identified and remedied. If a second ground fault develops in such a system, it can result in overcurrent or failure of components. Even in systems that are normally connected to ground to limit overvoltages, some applications require a Ground Fault Interrupter or similar device to detect faults to ground. \n\nRealistically, the resistance in a fault can be from close to zero to fairly high relative to the load resistance. A large amount of power may be consumed in the fault, compared with the zero-impedance case where the power is zero. Also, arcs are highly non-linear, so a simple resistance is not a good model. All possible cases need to be considered for a good analysis.\n\nWhere the system voltage is high enough, an electric arc may form between power system conductors and ground. Such an arc can have a relatively high impedance (compared to the normal operating levels of the system) and can be difficult to detect by simple overcurrent protection. For example, an arc of several hundred amperes on a circuit normally carrying a thousand amperes may not trip overcurrent circuit breakers but can do enormous damage to bus bars or cables before it becomes a complete short circuit. Utility, industrial, and commercial power systems have additional protection devices to detect relatively small but undesired currents escaping to ground. In residential wiring, electrical regulations may now require Arc-fault circuit interrupters on building wiring circuits, to detect small arcs before they cause damage or a fire.\n\nSymmetric faults can be analyzed via the same methods as any other phenomena in power systems, and in fact many software tools exist to accomplish this type of analysis automatically (see power flow study). However, there is another method which is as accurate and is usually more instructive.\n\nFirst, some simplifying assumptions are made. It is assumed that all electrical generators in the system are in phase, and operating at the nominal voltage of the system. Electric motors can also be considered to be generators, because when a fault occurs, they usually supply rather than draw power. The voltages and currents are then calculated for this \"base case\".\n\nNext, the location of the fault is considered to be supplied with a negative voltage source, equal to the voltage at that location in the base case, while all other sources are set to zero. This method makes use of the principle of superposition.\n\nTo obtain a more accurate result, these calculations should be performed separately for three separate time ranges:\n\nAn asymmetric fault breaks the underlying assumptions used in three-phase power, namely that the load is balanced on all three phases. Consequently, it is impossible to \"directly\" use tools such as the one-line diagram, where only one phase is considered. However, due to the linearity of power systems, it is usual to consider the resulting voltages and currents as a superposition of symmetrical components, to which three-phase analysis can be applied.\n\nIn the method of symmetric components, the power system is seen as a superposition of three components:\n\nTo determine the currents resulting from an asymmetrical fault, one must first know the per-unit zero-, positive-, and negative-sequence impedances of the transmission lines, generators, and transformers involved. Three separate circuits are then constructed using these impedances. The individual circuits are then connected together in a particular arrangement that depends upon the type of fault being studied (this can be found in most power systems textbooks). Once the sequence circuits are properly connected, the network can then be analyzed using classical circuit analysis techniques. The solution results in voltages and currents that exist as symmetrical components; these must be transformed back into phase values by using the A matrix.\n\nAnalysis of the prospective short-circuit current is required for selection of protective devices such as fuses and circuit breakers. If a circuit is to be properly protected, the fault current must be high enough to operate the protective device within as short a time as possible; also the protective device must be able to withstand the fault current and extinguish any resulting arcs without itself being destroyed or sustaining the arc for any significant length of time.\n\nThe magnitude of fault currents differ widely depending on the type of earthing system used, the installation's supply type and earthing system, and its proximity to the supply. For example, for a domestic UK 230 V, 60 A TN-S or USA 120 V/240 V supply, fault currents may be a few thousand amperes. Large low-voltage networks with multiple sources may have fault levels of 300,000 amperes. A high-resistance-grounded system may restrict line to ground fault current to only 5 amperes. Prior to selecting protective devices, prospective fault current must be measured reliably at the origin of the installation and at the furthest point of each circuit, and this information applied properly to the application of the circuits.\n\nOverhead power lines are easiest to diagnose since the problem is usually obvious, e.g., a tree has fallen across the line, or a utility pole is broken and the conductors are lying on the ground.\n\nLocating faults in a cable system can be done either with the circuit de-energized, or in some cases, with the circuit under power. Fault location techniques can be broadly divided into terminal methods, which use voltages and currents measured at the ends of the cable, and tracer methods, which require inspection along the length of the cable. Terminal methods can be used to locate the general area of the fault, to expedite tracing on a long or buried cable.\n\nIn very simple wiring systems, the fault location is often found through inspection of the wires. In complex wiring systems (for example, aircraft wiring) where the wires may be hidden, wiring faults are located with a Time-domain reflectometer. The time domain reflectometer sends a pulse down the wire and then analyzes the returning reflected pulse to identify faults within the electrical wire.\n\nIn historic submarine telegraph cables, sensitive galvanometers were used to measure fault currents; by testing at both ends of a faulted cable, the fault location could be isolated to within a few miles, which allowed the cable to be grappled up and repaired. The \"Murray loop\" and the \"Varley loop\" were two types of connections for locating faults in cables\n\nSometimes an insulation fault in a power cable will not show up at lower voltages. A \"thumper\" test set applies a high-energy, high-voltage pulse to the cable. Fault location is done by listening for the sound of the discharge at the fault. While this test contributes to damage at the cable site, it is practical because the faulted location would have to be re-insulated when found in any case.\n\nIn a high resistance grounded distribution system, a feeder may develop a fault to ground but the system continues in operation. The faulted, but energized, feeder can be found with a ring-type current transformer collecting all the phase wires of the circuit; only the circuit containing a fault to ground will show a net unbalanced current. To make the ground fault current easier to detect, the grounding resistor of the system may be switched between two values so that the fault current pulses.\n\nThe prospective fault current of larger batteries, such as deep-cycle batteries used in stand-alone power systems, is often given by the manufacturer.\n\nIn Australia, when this information is not given, the prospective fault current in amperes \"should be considered to be 6 times the nominal battery capacity at the \"C\" A·h rate,\" according to AS 4086 part 2 (Appendix H).\n\n\nGeneral\n"}
{"id": "42256600", "url": "https://en.wikipedia.org/wiki?curid=42256600", "title": "Fu Qiping", "text": "Fu Qiping\n\nFu Qiping is a leader and environmentalist who changed the structure of the economy of a village named Tengtou in China. He received Asia's most prestigious award, the Ramon Magsaysay Award, for his work.\n"}
{"id": "8539679", "url": "https://en.wikipedia.org/wiki?curid=8539679", "title": "Gimmick capacitor", "text": "Gimmick capacitor\n\nA gimmick capacitor is a capacitor made by twisting two pieces of insulated wire together. The capacitance may be varied by loosening or tightening the winding. The capacitance can also be reduced by shortening the twisted pair by cutting. The available capacitance is on the order of 1pF/inch (0.4 pF/cm).\n"}
{"id": "44125", "url": "https://en.wikipedia.org/wiki?curid=44125", "title": "Gyroscope", "text": "Gyroscope\n\nA gyroscope (from Ancient Greek γῦρος \"gûros\", \"circle\" and σκοπέω \"skopéō\", \"to look\") is a device used for measuring or maintaining orientation and angular velocity. It is a spinning wheel or disc in which the axis of rotation is free to assume any orientation by itself. When rotating, the orientation of this axis is unaffected by tilting or rotation of the mounting, according to the conservation of angular momentum.\n\nGyroscopes based on other operating principles also exist, such as the microchip-packaged MEMS gyroscopes found in electronic devices, solid-state ring lasers, fibre optic gyroscopes, and the extremely sensitive quantum gyroscope. \n\nApplications of gyroscopes include inertial navigation systems, such as in the Hubble Telescope, or inside the steel hull of a submerged submarine. Due to their precision, gyroscopes are also used in gyrotheodolites to maintain direction in tunnel mining. Gyroscopes can be used to construct gyrocompasses, which complement or replace magnetic compasses (in ships, aircraft and spacecraft, vehicles in general), to assist in stability (bicycles, motorcycles, and ships) or be used as part of an inertial guidance system.\n\nMEMS gyroscopes are popular in some consumer electronics, such as smartphones.\n\nA gyroscope is a wheel mounted in two or three gimbals, which are pivoted supports that allow the rotation of the wheel about a single axis. A set of three gimbals, one mounted on the other with orthogonal pivot axes, may be used to allow a wheel mounted on the innermost gimbal to have an orientation remaining independent of the orientation, in space, of its support. In the case of a gyroscope with two gimbals, the outer gimbal, which is the gyroscope frame, is mounted so as to pivot about an axis in its own plane determined by the support. This outer gimbal possesses one degree of rotational freedom and its axis possesses none. The inner gimbal is mounted in the gyroscope frame (outer gimbal) so as to pivot about an axis in its own plane that is always perpendicular to the pivotal axis of the gyroscope frame (outer gimbal). This inner gimbal has two degrees of rotational freedom.\n\nThe axle of the spinning wheel defines the spin axis. The rotor is constrained to spin about an axis, which is always perpendicular to the axis of the inner gimbal. So the rotor possesses three degrees of rotational freedom and its axis possesses two.\nThe wheel responds to a force applied to the input axis by a reaction force to the output axis.\n\nThe behaviour of a gyroscope can be most easily appreciated by consideration of the front wheel of a bicycle. If the wheel is leaned away from the vertical so that the top of the wheel moves to the left, the forward rim of the wheel also turns to the left. In other words, rotation on one axis of the turning wheel produces rotation of the third axis.\n\nA gyroscope flywheel will roll or resist about the output axis depending upon whether the output gimbals are of a free or fixed configuration. Examples of some free-output-gimbal devices would be the attitude reference gyroscopes used to sense or measure the pitch, roll and yaw attitude angles in a spacecraft or aircraft.\n\nThe centre of gravity of the rotor can be in a fixed position. The rotor simultaneously spins about one axis and is capable of oscillating about the two other axes, and it is free to turn in any direction about the fixed point (except for its inherent resistance caused by rotor spin). Some gyroscopes have mechanical equivalents substituted for one or more of the elements. For example, the spinning rotor may be suspended in a fluid, instead of being mounted in gimbals. A control moment gyroscope (CMG) is an example of a fixed-output-gimbal device that is used on spacecraft to hold or maintain a desired attitude angle or pointing direction using the gyroscopic resistance force.\n\nIn some special cases, the outer gimbal (or its equivalent) may be omitted so that the rotor has only two degrees of freedom. In other cases, the centre of gravity of the rotor may be offset from the axis of oscillation, and thus the centre of gravity of the rotor and the centre of suspension of the rotor may not coincide.\n\nEssentially, a gyroscope is a top combined with a pair of gimbals. Tops were invented in many different civilizations, including classical Greece, Rome, and China. Most of these were not utilized as instruments.\n\nThe first known apparatus similar to a gyroscope (the \"Whirling Speculum\" or \"Serson's Speculum\") was invented by John Serson in 1743. It was used as a level, to locate the horizon in foggy or misty conditions.\n\nThe first instrument used more like an actual gyroscope was made by Johann Bohnenberger of Germany, who first wrote about it in 1817. At first he called it the \"Machine\". Bohnenberger's machine was based on a rotating massive sphere. In 1832, American Walter R. Johnson developed a similar device that was based on a rotating disc. The French mathematician Pierre-Simon Laplace, working at the École Polytechnique in Paris, recommended the machine for use as a teaching aid, and thus it came to the attention of Léon Foucault. In 1852, Foucault used it in an experiment involving the rotation of the Earth. It was Foucault who gave the device its modern name, in an experiment to see (Greek \"skopeein\", to see) the Earth's rotation (Greek \"gyros\", circle or rotation), which was visible in the 8 to 10 minutes before friction slowed the spinning rotor.\n\nIn the 1860s, the advent of electric motors made it possible for a gyroscope to spin indefinitely; this led to the first prototype heading indicators, and a rather more complicated device, the gyrocompass. The first functional gyrocompass was patented in 1904 by German inventor Hermann Anschütz-Kaempfe. American Elmer Sperry followed with his own design later that year, and other nations soon realized the military importance of the invention—in an age in which naval prowess was the most significant measure of military power—and created their own gyroscope industries. The Sperry Gyroscope Company quickly expanded to provide aircraft and naval stabilizers as well, and other gyroscope developers followed suit.\n\nIn 1917, the Chandler Company of Indianapolis, created the \"Chandler gyroscope\", a toy gyroscope with a pull string and pedestal. Chandler continued to produce the toy until the company was purchased by TEDCO inc. in 1982. The chandler toy is still produced by TEDCO today.\n\nIn the first several decades of the 20th century, other inventors attempted (unsuccessfully) to use gyroscopes as the basis for early black box navigational systems by creating a stable platform from which accurate acceleration measurements could be performed (in order to bypass the need for star sightings to calculate position). Similar principles were later employed in the development of inertial navigation systems for ballistic missiles.\n\nDuring World War II, the gyroscope became the prime component for aircraft and anti-aircraft gun sights. After the war, the race to miniaturize gyroscopes for guided missiles and weapons navigation systems resulted in the development and manufacturing of so-called midget gyroscopes that weighed less than and had a diameter of approximately . Some of these miniaturized gyroscopes could reach a speed of 24,000 revolutions per minute in less than 10 seconds.\n\nGyroscopes continue to be an engineering challenge. For example, the axle bearings have to be extremely accurate. A small amount of friction is deliberately introduced to the bearings, since otherwise an accuracy of better than formula_1 of an inch would be required.\n\nThree-axis MEMS-based gyroscopes are also being used in portable electronic devices such as tablets, smartphones, and smartwatches. This adds to the 3-axis acceleration sensing ability available on previous generations of devices. Together these sensors provide 6 component motion sensing; acceleration for X,Y, and Z movement, and gyroscopes for measuring the extent and rate of rotation in space (roll, pitch and yaw). Some devices (e.g. the iPhone) additionally incorporate a magnetometer to provide absolute angular measurements relative to the Earth's magnetic field. Newer MEMS-based inertial measurement units incorporate up to all nine axes of sensing in a single integrated circuit package, providing inexpensive and widely available motion sensing.\n\nA Steadicam rig was employed during the filming of Return of the Jedi, in conjunction with two gyroscopes for extra stabilization, to film the background plates for the speeder bike chase. Steadicam inventor Garrett Brown operated the shot, walking through a redwood forest, running the camera at one frame per second. When projected at 24 frames per second, it gave the impression of flying through the air at perilous speeds.\n\nThe heading indicator or directional gyro has an axis of rotation that is set horizontally, pointing north. Unlike a magnetic compass, it does not seek north. When being used in an airliner, for example, it will slowly drift away from north and will need to be reoriented periodically, using a magnetic compass as a reference.\n\nUnlike a directional gyro or heading indicator, a gyrocompass seeks north. It detects the rotation of the Earth about its axis and seeks the \"true\" north, rather than the \"magnetic\" north. Gyrocompasses usually have built-in damping to prevent overshoot when re-calibrating from sudden movement.\n\nBy determining an object's acceleration and integrating over time, the velocity of the object can be calculated. Integrating again, position can be determined. The simplest accelerometer is a weight that is free to move horizontally, which is attached to a spring and a device to measure the tension in the spring. This can be improved by introducing a counteracting force to push the weight back and to measure the force needed to prevent the weight from moving. A more complicated design consists of a gyroscope with a weight on one of the axes. The device will react to the force generated by the weight when it is accelerated, by integrating that force to produce a velocity.\n\nA gyrostat consists of a massive flywheel concealed in a solid casing. Its behaviour on a table, or with various modes of suspension or support, serves to illustrate the curious reversal of the ordinary laws of static equilibrium due to the gyrostatic behaviour of the interior invisible flywheel when rotated rapidly. The first gyrostat was designed by Lord Kelvin to illustrate the more complicated state of motion of a spinning body when free to wander about on a horizontal plane, like a top spun on the pavement, or a bicycle on the road. Kelvin also made use of gyrostats to develop mechanical theories of the elasticity of matter and of the ether.. In modern continuum mechanics there is a variety of these models, based on ideas of Lord Kelvin. They represent a specific type of Cosserat theories (suggested for the first time by Eugène Cosserat and François Cosserat), which can be used for description of artificially made smart materials as well as of other complex media. One of them, so-called Kelvin's medium, has the same equations as magnetic insulators near the state of magnetic saturation in the approximation of quasimagnetostatics .\n\nIn modern times, the gyrostat concept is used in the design of attitude control systems for orbiting spacecraft and satellites. For instance, the Mir space station had three pairs of internally mounted flywheels known as \"gyrodynes\" or \"control moment gyros\".\n\nIn physics, there are several systems whose dynamical equations resemble the equations of motion of a gyrostat. Examples include a solid body with a cavity filled with an inviscid, incompressible, homogeneous liquid, the static equilibrium configuration of a stressed elastic rod in elastica theory, the polarization dynamics of a light pulse propagating through a nonlinear medium, the Lorenz system in chaos theory, and the motion of an ion in a Penning trap mass spectrometer.\n\nA microelectromechanical systems (MEMS) gyroscope are miniaturized gyroscope found in electronic devices. It takes the idea of the Foucault pendulum and uses a vibrating element.\n\nThe hemispherical resonator gyroscope (HRG), also called wine-glass gyroscope or mushroom gyro, makes using a thin solid-state hemispherical shell, anchored by a thick stem. This shell is driven to a flexural resonance by electrostatic forces generated by electrodes which are deposited directly onto separate fused-quartz structures that surround the shell. Gyroscopic effect is obtained from the inertial property of the flexural standing waves.\n\nA vibrating structure gyroscope (VSG), also called a Coriolis vibratory gyroscope (CVG), uses a resonator made of different metallic alloys. It takes a position between the low-accuracy, low-cost MEMS gyroscope and the higher-accuracy and higher-cost fiber optic gyroscope. Accuracy parameters are increased by using low-intrinsic damping materials, resonator vacuumization, and digital electronics to reduce temperature dependent drift and instability of control signals.\n\nHigh quality wine-glass resonators are used for precise sensors like HRG.\n\nA dynamically tuned gyroscope (DTG) is a rotor suspended by a universal joint with flexure pivots. The flexure spring stiffness is independent of spin rate. However, the dynamic inertia (from the gyroscopic reaction effect) from the gimbal provides negative spring stiffness proportional to the square of the spin speed (Howe and Savet, 1964; Lawrence, 1998). Therefore, at a particular speed, called the tuning speed, the two moments cancel each other, freeing the rotor from torque, a necessary condition for an ideal gyroscope.\n\nA ring laser gyroscope relies on the Sagnac effect to measure rotation by measuring the shifting interference pattern of a beam split into two halves, as the two halves move around the ring in opposite directions.\n\nWhen the Boeing 757-200 entered service in 1983, it was equipped with the first suitable ring laser gyroscope. This gyroscope took many years to develop, and the experimental models went through many changes before it was deemed ready for production by the engineers and managers of Honeywell and Boeing. It was an outcome of the competition with mechanical gyroscopes, which kept improving. The reason Honeywell, of all companies, chose to develop the laser gyro was that they were the only one that didn't have a successful line of mechanical gyroscopes, so they wouldn't be competing against themselves. The first problem they had to solve was that with laser gyros rotations below a certain minimum could not be detected at all, due to a problem called \"lock-in\", whereby the two beams act like coupled oscillators and pull each other's frequencies toward convergence and therefore zero output. The solution was to shake the gyro rapidly so that it never settled into lock-in. Paradoxically, too regular of a dithering motion produced an accumulation of short periods of lock-in when the device was at rest at the extremities of its shaking motion. This was cured by applying a random white noise to the vibration. The material of the block was also changed from quartz to a new glass ceramic Cer-Vit, made by Owens Corning, because of helium leaks.\n\nA fiber optic gyroscope also uses the interference of light to detect mechanical rotation. The two halves of the split beam travel in opposite directions in a coil of fiber optic cable as long as 5 km. Like the ring laser gyroscope, it makes use of the Sagnac effect.\n\nA London moment gyroscope relies on the quantum-mechanical phenomenon, whereby a spinning superconductor generates a magnetic field whose axis lines up exactly with the spin axis of the gyroscopic rotor. A magnetometer determines the orientation of the generated field, which is interpolated to determine the axis of rotation. Gyroscopes of this type can be extremely accurate and stable. For example, those used in the Gravity Probe B experiment measured changes in gyroscope spin axis orientation to better than 0.5 milliarcseconds (1.4 degrees, or about ) over a one-year period. This is equivalent to an angular separation the width of a human hair viewed from away.\n\nThe GP-B gyro consists of a nearly-perfect spherical rotating mass made of fused quartz, which provides a dielectric support for a thin layer of niobium superconducting material. To eliminate friction found in conventional bearings, the rotor assembly is centered by the electric field from six electrodes. After the initial spin-up by a jet of helium which brings the rotor to 4,000 RPM, the polished gyroscope housing is evacuated to an ultra-high vacuum to further reduce drag on the rotor. Provided the suspension electronics remain powered, the extreme rotational symmetry, lack of friction, and low drag will allow the angular momentum of the rotor to keep it spinning for about 15,000 years.\n\nA sensitive DC SQUID that can discriminate changes as small as one quantum, or about 2 Wb, is used to monitor the gyroscope. A precession, or tilt, in the orientation of the rotor causes the London moment magnetic field to shift relative to the housing. The moving field passes through a superconducting pickup loop fixed to the housing, inducing a small electric current. The current produces a voltage across a shunt resistance, which is resolved to spherical coordinates by a microprocessor. The system is designed to minimize Lorentz torque on the rotor.\n\nIn addition to being used in compasses, aircraft, computer pointing devices, etc., gyroscopes have been introduced into consumer electronics. The first usage or application of gyroscope in consumer electronics was popularized by Steve Jobs in Apple iPhone. \n\nSince the gyroscope allows the calculation of orientation and rotation, designers have incorporated them into modern technology. The integration of the gyroscope has allowed for more accurate recognition of movement within a 3D space than the previous lone accelerometer within a number of smartphones. Gyroscopes in consumer electronics are frequently combined with accelerometers (acceleration sensors) for more robust direction- and motion-sensing. Examples of such applications include smartphones such as the Samsung Galaxy Note 4, HTC Titan, Nexus 5, iPhone 5s, Nokia 808 PureView and Sony Xperia, game console peripherals such as the PlayStation 3 controller and the Wii Remote, and virtual reality sets such as the Oculus Rift.\n\nNintendo has integrated a gyroscope into the Wii console's Wii Remote controller by an additional piece of hardware called \"Wii MotionPlus\". It is also included in the 3DS and the Wii U GamePad, which detects movement when turning.\n\nCruise ships use gyroscopes to level motion-sensitive devices such as self-leveling pool tables.\n\nAn electric powered flywheel gyroscope inserted in a bicycle wheel is being sold as a training wheel alternative. \n\n\n"}
{"id": "917891", "url": "https://en.wikipedia.org/wiki?curid=917891", "title": "Hundredth monkey effect", "text": "Hundredth monkey effect\n\nThe hundredth monkey effect is a hypothetical phenomenon in which a new behaviour or idea is claimed to spread rapidly by unexplained means from one group to all related groups once a critical number of members of one group exhibit the new behaviour or acknowledge the new idea.\n\nOne of the primary factors in the promulgation of the story is that many authors quote secondary, tertiary or post-tertiary sources which have themselves misrepresented the original observations.\n\nA behavioural study was conducted in the 1950's of a troupe of \"Macaca fuscata\" (Japanese monkeys) on Kōjima island. An unanticipated byproduct of the study was that the scientists witnessed several evolutionary behavioural changes by the troupe, two of which were orchestrated by one young female, and the others by her sibling or contemporaries.\n\nThe account of only one of these behavioural changes (sweet potato washing) was propagated into a phenomenon and the story loosely published by Lawrence Blair and Lyall Watson in the mid-to-late 1970s.\n\nThe story of the hundredth monkey effect was published in Lyall Watson's foreword to Lawrence Blair's \"Rhythms of Vision\" in 1975, and spread with the appearance of Watson's 1979 book \"Lifetide\". The account is that unidentified scientists were conducting a study of macaque monkeys on the Japanese island of Kōjima in 1952. These scientists observed that some of these monkeys learned to wash sweet potatoes, and gradually this new behaviour spread through the younger generation of monkeys—in the usual fashion, through observation and repetition. Watson then concluded that the researchers observed that once a critical number of monkeys was reached, i.e., the hundredth monkey, this previously learned behaviour instantly spread across the water to monkeys on nearby islands.\n\nThis story was further popularised by Ken Keyes Jr. with the publication of his book \"The Hundredth Monkey\". Keys's book was about the devastating effects of nuclear war on the planet. Keys presented the hundredth monkey effect story as an inspirational parable, applying it to human society and the effecting of positive change. Unfortunately, Keyes combined two items of truth, one that the Koshima monkeys learned to wash sweet potatoes, and two that the phenomenon was observed on neighbouring islands. He did not provide substantiating evidence for his claims, diluting the importance of both studies and potentially discrediting the scientists involved. Combining this science with his political views may also have damaged the research credibility, leading to many reporters attempting to 'debunk' the Japanese team's research without doing sufficient research themselves.\n\nThe original Koshima research was undertaken by a team of scientists as a secondary consequence of 1948 research on semiwild horses in Japan. The Koshima troupe was identified as segregated from other monkeys and, from 1950, used as a closed study group to observe wild Japanese monkey behaviour. While studying the group the team would drop sweet potatoes and wheat on the beach and observe the troupe's behaviour. In 1954 a paper was published indicating the first observances of one monkey, Imo, washing her sweet potatoes in the water. Her changed behaviour led to several feeding behaviour changes over the course of the next few years, all of which was of great benefit in understanding the process of teaching and learning in animal behaviour. A brief account of the behavioural changes can be seen below:\n\n\nThe study does not indicate a catalyst ratio at which all the Koshima monkeys started washing sweet potatoes, or a correlation to other monkey studies where similar behaviour started. To the contrary, it indicated that certain age groups in Koshima would not learn the behaviour.\n\nSeparate papers make mention that, from 1960 onward, similar sweet potato washing behaviours were noticed in other parts of the world, however this is not directly attributed to Koshima. Claims are made that a monkey swam from one island to another where he taught the resident monkeys how to wash sweet potatoes. No mention of the other behavioural improvements are made. No indication of how the monkey swam is made either - it must be noted that the Koshima monkeys cannot swim. Therefore, although the question must be asked how the swimming monkey learned the sweet potato washing behaviour if not from Koshima, no indication is made as to where the monkey learned the behaviour.\n\nIn 1985, Elaine Myers re-examined the original published research in \"The Hundredth Monkey Revisited\" in the journal \"In Context\". In her review she found that the original research reports by the Japan Monkey Centre in Vol. 2, 5, and 6 of the journal \"Primates\" are insufficient to support Watson's story. In short, she is suspicious of the existence of a hundredth monkey phenomenon; the published articles describe how the sweet potato washing behaviour gradually spread through the monkey troupe and became part of the set of learned behaviours of young monkeys, but Myers does not agree that it serves as evidence for the existence of a critical number at which the idea suddenly spread to other islands.\n\nThe story as told by Watson and Keyes is popular among New Age authors and personal growth gurus and has become an urban legend and part of New Age mythology. Also, Rupert Sheldrake has cited that a phenomenon like the hundredth monkey effect would be evidence of morphic fields bringing about non-local effects in consciousness and learning. As a result, the story has also become a favourite target of the Committee for the Scientific Investigation of Claims of the Paranormal, and was used as the title essay in \"The Hundredth Monkey and Other Paradigms of the Paranormal\", published by the Committee in 1990.\n\nIn his book \"Why People Believe Weird Things\", Michael Shermer explains how the urban legend started, was popularised, and has been discredited.\n\nThe original research continues to prove useful in the study of cultural transmission in animals.\n\nAn analysis of the appropriate literature by Ron Amundson, published by The Skeptics Society, revealed several key points that challenge the supposed effect.\n\nUnsubstantiated claims that there was a sudden and remarkable increase in the proportion of washers in the first population were exaggerations of a much slower, more mundane effect. Rather than all monkeys mysteriously learning the skill it was noted that it was predominantly a learned skill, which is widespread in the animal kingdom; older monkeys who did not know how to wash tended not to learn. As the older monkeys died and younger monkeys were born the proportion of washers naturally increased. The time span between observations by the Japanese scientists was on the order of years so the increase in the proportion was not observed to be sudden.\n\nClaims that the practice spread suddenly to other isolated populations of monkeys may be called into question given the fact that the monkeys had the researchers in common. Amundson also notes that the sweet potato was not available to the monkeys prior to human intervention. The number of monkeys in the colony was counted as 59 in 1962 indicating that even in numbers no \"hundredth monkey\" existed.\n\n\n\n"}
{"id": "32052633", "url": "https://en.wikipedia.org/wiki?curid=32052633", "title": "Hydroelectricity in Albania", "text": "Hydroelectricity in Albania\n\nAlbania is the biggest a producer of hydroelectricity in the world by percentage (90% as of 2011) and by own production (100%). Albania aims to increase its production to 100%. Some of the projects underway include Skavica, up to 350 MW, Devolli 400 MW, Vjosa up to 400 MW, Kalivaci and Ashta 48 MW hydropower plants, Valbona and Tropojë, 40 MW.\n"}
{"id": "2809486", "url": "https://en.wikipedia.org/wiki?curid=2809486", "title": "James River bateau", "text": "James River bateau\n\nThe James River Bateau was a shallow draft river craft used during the period from 1775 to 1840 to transport tobacco and other cargo on the James River and its tributaries in the Commonwealth of Virginia. It was flat bottomed and pointed at both ends. The length of the bateau varied greatly, 58 feet (17.5 m) being a common length. The bateau was propelled by bateaumen pushing with long sturdy poles. Alternate spellings of bateau include batteau, batoe and the plurals bateaux, batoes, and batteaux. Bateau is the French word for \"boat\". In the colonial days, bateaus were used extensively in rivers throughout the eastern part of the United States, but the coverage of this article is confined to those that plied the James River in the Commonwealth of Virginia.\n\nAnthony Rucker the Elder was the original inventor and constructor of the James River Bateau in 1775. It was a boat essentially different from any before that time used on the rivers of Virginia. Rucker's design was successfully patented many years after its development. The earliest known reference to the bateau comes from Thomas Jefferson's account book, dated April 19, 1775. Jefferson had been present at the first launching, and forty-six years later he was witness to the successful patenting of the bateau by heirs of Rucker. George Washington also mentioned the bateau in his diary entry, dated April 7, 1791. Unfortunately, none of the original bateaus exist. Some remains were uncovered by construction workers at the site of the James River and Kanawha Canal Basin.\n\nThe five Rucker brothers were among the tobacco planters in Amherst County, Virginia. Anthony Rucker was a tobacco inspector for the county. The need to transport large hogshead of tobacco to the port at Richmond, Virginia, likely motivated the Rucker brothers to develop the bateau. It was just wide enough to accommodate standard hogsheads (barrels) across the floor. The tobacco hogshead became standardized by the 1760s and measured 48 inches (120 cm) long and 30 inches (76 cm) in diameter at the head. They held about 1,000 pounds (450 kg) of tightly packed tobacco. Larger bateaus could transport 10 or more hogsheads, depending on river conditions. Tobacco was a very profitable crop, and because of cheap slave labor vast amounts were produced by planters along the James River basin.\n\nThe bateau became such a useful craft that it was also used for other cargo as well as passenger transportation. During the period of 1820 to 1840, at least 500 bateaus and 1,500 bateaumen operated on the James River between Lynchburg, Virginia and Richmond. Boatmen were nearly all Slave and Free African Americans. The use of the bateau sharply declined after 1840 when the James and Kanawha River Canal reached Lynchburg. The packet boat and rail took over the cargo.\n\nThe James River bateau was designed for freight and for ease of navigation in the shallow rocky waters of the Upper James. Thomas Jefferson, in 1775, recorded the purchase of a bateau in his account book, stating, \"Apr. 29. Rucker's battoe (sic) is 50. f. long. 4.f. wide in the bottom & 6.f. at top. she carries 11. hhds & draws 13½ I. water.\" Typical bateaus were thought to be about 58 feet (17.5 m) long, some shorter, some longer. They had no keel to interfere with navigating river rapids and were well adapted to shallow water, having a draft of about 12–18 inches (30–45 cm) when loaded. They measured 6–8 feet (1.8–2.4 m) at the beam. The sides varied from 18–24 inches (45–60 cm) in height. Very long planks, fastened to ribs, formed the sides and bottom. The nose cones were built and attached separately to facilitate maintenance since the ends of the bateau received abuse from the river rocks. The bateau had no rudder and was guided by long sweeps that engaged notches formed in the tip of the nose cones. The cargo versions had no seats. Passenger versions had a canopy and some had oar locks.\n\nEven though the working bateau is no longer used, historians and river enthusiasts still keep the memory alive. Replicas of the bateaus have been built around the country and bateau river cruises are available in a number of states. Since 1985, the James River Batteau Festival has promoted a bateau run from Lynch's Landing in Lynchburg to Maiden's Landing in Powhatan, a distance of about 120 river miles (200 km). Seventeen batteaux crews and many canoeists participated in the 2005 festival. The number of Batteaus on the river during the festival has increased to 25 in 2009.\n\n\n"}
{"id": "309930", "url": "https://en.wikipedia.org/wiki?curid=309930", "title": "Lift coefficient", "text": "Lift coefficient\n\nThe lift coefficient (C) is a dimensionless coefficient that relates the lift generated by a lifting body to the fluid density around the body, the fluid velocity and an associated reference area. A lifting body is a foil or a complete foil-bearing body such as a fixed-wing aircraft. \"C\" is a function of the angle of the body to the flow, its Reynolds number and its Mach number. The lift coefficient \"c\" refers to the dynamic lift characteristics of a two-dimensional foil section, with the reference area replaced by the foil chord.\n\nThe lift coefficient \"C\" is defined by\n\nwhere \nformula_2 is the lift force, formula_3 is the relevant surface area and formula_4 is the fluid dynamic pressure, in turn linked to the fluid density formula_5, and to the flow speed formula_6. The choice of the reference surface should be specified since it is arbitrary. For example, for cylindric profiles (the 3D extrusion of an airfoil in the spanwise direction) it is always oriented in the spanwise direction, but while in aerodynamics and thin airfoil theory the second axis generating the surface is commonly the chordwise direction:\n\nresulting in a coefficient:\n\nwhile for thick airfoils and in marine dynamics, the second axis is sometimes taken in the thickness direction:\n\nresulting in a different coefficient:\n\nThe ratio between these two coefficients is the thickness ratio:\n\nThe lift coefficient can be approximated using the lifting-line theory, numerically calculated or measured in a wind tunnel test of a complete aircraft configuration.\n\nLift coefficient may also be used as a characteristic of a particular shape (or cross-section) of an airfoil. In this application it is called the section lift coefficient formula_12. It is common to show, for a particular airfoil section, the relationship between section lift coefficient and angle of attack. It is also useful to show the relationship between section lift coefficient and drag coefficient.\n\nThe section lift coefficient is based on two-dimensional flow over a wing of infinite span and non-varying cross-section so the lift is independent of spanwise effects and is defined in terms of formula_13, the lift force per unit span of the wing. The definition becomes\n\nwhere L is the reference length that should always be specified: in aerodynamics and airfoil theory usually the airfoil chord formula_15 is chosen, while in marine dynamics and for struts usually the thickness formula_16 is chosen. Note this is directly analogous to the drag coefficient since the chord can be interpreted as the \"area per unit span\".\n\nFor a given angle of attack, \"c\" can be calculated approximately using the thin airfoil theory, calculated numerically or determined from wind tunnel tests on a finite-length test piece, with end-plates designed to ameliorate the three-dimensional effects. Plots of \"c\" versus angle of attack show the same general shape for all airfoils, but the particular numbers will vary. They show an almost linear increase in lift coefficient with increasing angle of attack with a gradient known as the lift slope. For a thin airfoil of any shape the lift slope is π/90 ≃ 0.11 per degree. At higher angles a maximum point is reached, after which the lift coefficient reduces. The angle at which maximum lift coefficient occurs is the stall angle of the airfoil, which is approximately 10 to 15 degrees on a typical airfoil.\n\nSymmetric airfoils necessarily have plots of c versus angle of attack symmetric about the \"c\" axis, but for any airfoil with positive camber, i.e. asymmetrical, convex from above, there is still a small but positive lift coefficient with angles of attack less than zero. That is, the angle at which \"c\" = 0 is negative. On such airfoils at zero angle of attack the pressures on the upper surface are lower than on the lower surface.\n\n\n"}
{"id": "1373487", "url": "https://en.wikipedia.org/wiki?curid=1373487", "title": "Liquid smoke", "text": "Liquid smoke\n\nLiquid smoke is a water-soluble yellow to red liquid used for flavoring. It is used as a substitute for cooking with wood smoke while retaining a similar flavor. It can be used to flavor any meat or vegetable. It is generally made by concentrating the smoke from wood, but can contain any number of food additives.\n\nPyrolysis or thermal decomposition of wood in a low oxygen manner originated prehistorically to produce charcoal. Condensates of the vapors eventually were made and found useful as preservatives. The term wood vinegar for centuries was the popular term used to describe the water based condensates of wood smoke. Presumably, this is due to its utilization as food vinegar. Pliny the Elder recorded in one of his ten volumes of \"Natural History\" the use of wood vinegar as an embalming agent, declaring it superior to other treatments he used. Widely recognized as the father of chemical engineering, another naturalist documentarian Johann Rudolf Glauber outlined in \"Furni Novi Philosophici\" the methods to produce wood vinegar during charcoal making. Further, he described the use of the water insoluble tar fraction as a wood preservative and documented the freezing of the wood vinegar to concentrate it. Use of the French derivation, pyroligneous acid as a widely used term for wood vinegar emerged by 1788.\n\nIn the United States, the commercial distribution era of pyroligneous acid under a new term, liquid smoke that subsumed it began with E.H.Wright in 1895. Among Wright’s innovations were the standardization of the product, marketing and distribution. Wright’s Liquid Smoke and its modern-day successors have always been the subject of controversy about what they are and how they are made. But in 1913 Wright, prevailed in a federal misbranding case. Case judge Van Valkenburg wrote:\n\nThe Government, in trying to show that this is not smoke produced by combustion, has shown that it is produced in exactly the same kind of way that is stated on that label. The fact is that they have produced something here which they say has something of the flavor and properties similar to the curative properties of smoke; they get it out of wood and they get it by distillation and it turns out to be a substance like, if not exactly identical with pyroligneous acid. Well, nobody could be deceived into thinking it was specifically what the indictment charges they are being deceived with. It is a thing which is produced in such a manner from the art and methods employed in it, that the application of the term “smoke” to it seems to me to be apt or applicable instead of deceptive, and it does not deceive in the sense this statute implies.\n\nHistorically, all pyroligneous acid products, Wright’s product and many other condensates have been made as byproducts of charcoal manufacturing which was of greater value. Chemicals such as methanol, acetic acid and acetone have been isolated from these condensates and sold. But with the advent of lower cost fossil fuel sources, today these and other wood derived chemicals retain only small niches. In 1959 the era of modern condensed smoke based products began with the establishment of Red Arrow Products Company in Manitowoc, Wisconsin. The important distinction marking this era from the past is the production of modern condensates to be used industrially as a replacement for smoking food directly with non-condensed smoke. Today there are many manufacturing locations around the world, most of which pyrolyze wood primarily to generate condensates which are further processed to make hundreds of derivative products. These are now referred to less so as liquid smoke products rather as smoke flavourings, smoke flavors, and natural condensed smokes.\n\nLiquid smoke and pyroligneous acid are terms used to describe the condensed products from the destructive distillation of wood. There are no standards of identity, prescribed production methods, or tests which distinguish between liquid smoke and pyroligneous acid; they can be considered to be the same. However, the numerous variables that are manipulated during pyrolysis do lead to a wide range of compositions of the condensates. In addition, implementation of many further processing steps by concentration, dilution, distillation, extraction, and use of food additives has led to the many hundreds of unique products on the market worldwide.\n\nWood, particularly hardwood, is by far the most widely used biomass pyrolyzed to make liquid smoke. Commercial products are made using both batch and continuous methods. Commercial products are made using a range of reactors from rotary calciners, heated screws, batch charcoal kilns, to fast pyrolysis reactors. The process type and conditions of processing lead to greater variances between the condensates than the differences between the common wood types that are in use. Variables such as feed rate, vapor residence time, particle size, oxygen infiltration, and temperature can have substantial effects on yield and composition of the condensates. Wide ranges of chemical composition are reported throughout the literature and unless the process and conditions are cited, there is limited utility of such results. Commercial manufacturers strive to control their manufacturing variables in order to standardize product compositions.\n\nWater is added either during condensation or after to cause separation of three fractions. Once water is added, the aqueous phase becomes the largest and most useful fraction. It contains wood derived chemical compounds of higher chemical polarity such as those found in carboxylic acid, aldehyde, and phenol chemical classes. Many compounds together are responsible for the flavor, browning, antioxidant, and antimicrobial effects of smoke and liquid smoke. The smallest condensed fraction is the lowest-polarity upper phase which a mix of phytosterols and other oily, waxy substances. The lower phase is commonly referred to as tar. It is an intermediate-polarity mixture of phenolic polymers, secondary and tertiary reaction products, some of the water-soluble polar compounds partitioned in the amount of which is governed by individual partition coefficients, water and the bulk of the polycyclic aromatic hydrocarbons. Wood tar has been used as a preservative, water repellent, and antiseptic. Tar from birch was produced as a commodity product on large scale in northern Europe. Today commercial liquid smoke products are still prepared from this phase.\n\nLiquid smoke condensates are made commercially for the global meat industry in the U.S. and Europe and are regulated by those governments. Liquid smoke is still referenced as wood vinegar and is being made and used indigenously in many other locations such as Japan, China, Indonesia, Malaysia, Brazil, and Southeast Asia. The food regulatory regimes in these locations are either non-existent or not known outside of their jurisdictions.\n\nThe application of liquid smoke to food has grown to encompass a wide variety of methods employing thousands of commercial formulations worldwide. By far the widest use of liquid smoke is for the purpose of replacing direct smoking of food with onsite-generated smoke from wood. To impart the desired functional effects of smoke, liquid smoke preparations must be applied topically. In addition to flavor, reaction color, anti-microbial, and texture effects are the functionalities that can only be obtained by topical addition followed by thermal processing. Dipping products in diluted solutions or soaking them in brines containing liquid smoke followed by heating was done long before the modern industrial era using Wright’s liquid smoke and pyroligneous acid precursors. Allen patented a method of regenerating smoke using air atomization. It remains the leading technology for using condensed smoke products to treat processed meat, cheese, fish, and other foods in batch smokehouses. As the meat-processing industry has consolidated, continuous processes have evolved and direct applications of solutions of liquid smoke via showering or drenching systems installed on continuous lines have grown to be the largest type of application method. In North America there are more than thirty-five processed-meat plants utilizing bulk tanks to receive tankers of liquid smoke for topical application as an alternative to direct wood smoking. Also noteworthy is the method of topical application by impregnation of fibrous, laminated, and plastic casings. Meat products are subsequently stuffed into these casings and thermally processed. The use of natural condensed smoke preparations internally in food is another important means of imparting smoke flavor. It is used when other technical functions of smoke are not imperative to be expressed in a finished food. This can be done directly by adding into blenders with meat or other foods or injecting whole muscle meat. Incorporation into sauces such as barbeque or dry seasonings and compounding with other flavors are other important ways in which the flavors are used. Further utility of aqueous smoke solutions is gained by the use of more complex food-grade processing such as extraction into oil, spray drying using maltodextrin carriers, or plating onto foods and food ingredients such as malt flour, yeast, or salt.\n\nExtensive references to beneficial uses of pyroligneous acid in plants for seed germination, pest control, microbial control, plant structural enhancements are reported. Livestock benefits such as antimicrobial preservation of feed, nutrient digestability, and other claims are found. Scientific agricultural studies can be found in peer reviewed journals, but many agricultural benefits such as soil quality improvement, better seed germination, and healthier foliage are widely promoted without attribution. Broad claims of medical benefits to humans in digestive ailments, dental infections, liver, heart, skin ailments, ears, eyes are found, but the literature is devoid of accepted scientific studies for such testimonial claims in humans.\n\nThe first government sanctioned assessment of liquid smoke was undertaken by the US Food and Drug Administration (FDA). In 1981, the committee commissioned by FDA to evaluate information on the products concluded there was no evidence demonstrating the products were a hazard to the public the way they were being used. Today these products stand as Generally Recognized as Safe in the US and may be used at levels necessary to produce the intended technical effects. Manufacturing plants where liquid smoke is made are regulated and inspected by the FDA.\n\nThe European Parliament and the Council established Community procedures for the safety assessment and the authorization of smoke flavorings used or intended for use in or on foods in 2003. The European Food Safety Authority (EFSA) was charged with evaluating information on primary condensate smoke flavorings. Information on twelve products from ten applicants were evaluated by EFSA. Opinions were published on all twelve. The products considered were what each applicant considered their own primary product prior to any further processing or derivatization. All twelve products were determined to be genotoxic positive by in vitro methods, but when evaluated by in vivo methods ten were found to not be of concern by EFSA. The AM-01 product was judged inconclusive and FF-B was considered weakly genotoxic. Based upon the NOAEL determinations for each product and supplemental information supplied by some manufacturers usage limits for most products have been established and are conveyed by manufacturers to users. Most of these primary products and their derivatives remain in commercial use. Only products which are the subjects of these evaluations are authorized to be used in commerce within the European Union.\n\n"}
{"id": "48849776", "url": "https://en.wikipedia.org/wiki?curid=48849776", "title": "List of wind farms in Jordan", "text": "List of wind farms in Jordan\n\nAs of 2018, there are Five operational wind power plants at Ibrahimyah, Hofa, Maan & Tafila. The Ibrahimyah plant, located approximately 80 km north of Amman, consists of 4 wind turbines with capacity 0.08 MW for each. The Hofa plant, located approximately 92 km north of Amman, consists of 5 wind turbines with capacity 0.225 MW for each. The Tafila Wind Farm is located in Tafilah Governorate in south-west Jordan. In Maan two Wind Farms ,Maan Wind Farm was inaugaurated in 2016 in south Jordan with a capacity of 80 MW.,and Rajef Wind Farm reached the commercial Operation Date by October 2018 ,with a capacity of 86 MW .\n\n"}
{"id": "10113138", "url": "https://en.wikipedia.org/wiki?curid=10113138", "title": "MacCready Solar Challenger", "text": "MacCready Solar Challenger\n\nThe Solar Challenger was a solar-powered electric aircraft designed by Paul MacCready's AeroVironment. The aircraft was designed as an improvement on the Gossamer Penguin, which in turn was a solar-powered variant of the human-powered Gossamer Albatross. It was powered entirely by the photovoltaic cells on its wing and stabilizer, without even reserve batteries, and was the first such craft capable of long-distance flight. In 1981, it successfully completed a 163-mile (262 km) demonstration flight from France to England.\n\nThe Solar Challenger was designed by a team led by Paul MacCready as a more airworthy improvement on the Gossamer Penguin, directly incorporating lessons learned from flight testing the earlier aircraft. As with the Gossamer Penguin, construction was sponsored by DuPont in exchange for publicity for the company's patented materials incorporated in the design. AstroFlight, Inc. supplied the motors and solar panels, designed by Robert Boucher. The plane's wings carried 16,128 solar cells yielding a maximum solar power of 3,800 watts. It was flight tested in Western USA in winter 1980-1981.\n\nOn July 7, 1981, the aircraft flew 163 miles from Pontoise – Cormeilles Aerodrome, north of Paris, France to Manston Royal Air Force Base in Manston, United Kingdom, staying aloft 5 hours and 23 minutes, with pilot Stephen Ptacek at the controls. Currently the plane is owned by the Smithsonian Institution's Air and Space Museum.\n\nThe Solar Challenger was designed to be sturdier, more powerful, and more maneuverable than the Gossamer Penguin so as to be able to withstand sustained high-altitude flight and normal turbulence. It was over three times as heavy (without pilot) as the Gossamer Penguin and had a shorter wingspan, but was proportionately more powerful, with electricity supplied by 16,128 solar cells powering two three-horsepower motors. The solar panels were directly affixed to the wing and large horizontal stabilizer, both of which had to be flat on top to accommodate them. The two motors, each 3 inches wide and 17 inches long and incorporating samarium-cobalt permanent magnets, operated in tandem on a common shaft to drive a single, controllable-pitch propeller. The design incorporated advanced synthetic materials with very high strength to weight ratios, including Kevlar, Nomex, Delrin, Teflon, and Mylar, all supplied by the aircraft's sponsor, Dupont.\n\n"}
{"id": "32942873", "url": "https://en.wikipedia.org/wiki?curid=32942873", "title": "Melo HVDC Back-to-back station", "text": "Melo HVDC Back-to-back station\n\nThe Melo HVDC Back-to-back station is the central part of the EHV-interconnection between Uruguay and Brazil, which is only possible by means of a frequency converter, as the frequency of the power grid in Uruguay is 50 Hz and that in Brazil is 60 Hz. The station, which is situated east of Melo, Uruguay, is interconnected by the San Carlos substation with a 283 kilometres long 500 kV line. It is capable of transferring 500 MW, and was built by AREVA in 31 months from 2009 to 2011.\n\nFrom the Melo station a 128 kilometres long 525 kV powerline, of which 65 km are situated in Uruguay, runs to a newly built 525 kV/230 kV substation close to the Candiota power station, which contains some harmonic filters.\n\n"}
{"id": "5006878", "url": "https://en.wikipedia.org/wiki?curid=5006878", "title": "Nicholas Georgescu-Roegen", "text": "Nicholas Georgescu-Roegen\n\nNicholas Georgescu-Roegen (born Nicolae Georgescu, 4 February 1906 – 30 October 1994) was a Romanian American mathematician, statistician and economist. He is best known today for his 1971 magnum opus on \"The Entropy Law and the Economic Process\", in which he argued that all natural resources are irreversibly degraded when put to use in economic activity. A progenitor and a paradigm founder in economics, Georgescu-Roegen's work was seminal in establishing ecological economics as an independent academic sub-discipline in economics.\n\nSeveral economists have hailed Georgescu-Roegen as a man who lived well ahead of his time, and some historians of economic thought have proclaimed the ingenuity of his work. In spite of such appreciation, Georgescu-Roegen was never awarded the Nobel Prize in Economics, although benefactors from his native Romania were lobbying for it on his behalf. After Georgescu-Roegen's death, his work was praised by a surviving friend of the highest rank: Prominent Keynesian economist and Nobel Prize laureate Paul Samuelson professed that he would be delighted if the fame Georgescu-Roegen did not fully realise in his own lifetime were granted by posterity instead.\n\nIn the history of economic thought, Georgescu-Roegen was the first economist of some standing to theorise on the premise that all of earth's mineral resources will eventually be exhausted at some point. In his paradigmatic magnum opus, Georgescu-Roegen argues that economic scarcity is rooted in physical reality; that all natural resources are irreversibly degraded when put to use in economic activity; that the carrying capacity of earth – that is, earth's capacity to sustain human populations and consumption levels – is bound to decrease some time in the future as earth's finite stock of mineral resources is being extracted and put to use; and consequently, that the world economy as a whole is heading towards an inevitable future collapse, ultimately bringing about human extinction. Due to the radical pessimism inherent in his work, based on the physical concept of entropy, the theoretical position of Georgescu-Roegen and his followers was later termed 'entropy pessimism'.\n\nEarly in his life, Georgescu-Roegen was the student and protégé of Joseph Schumpeter, who taught that irreversible evolutionary change and 'creative destruction' are inherent in capitalism. Later in his life, Georgescu-Roegen was the teacher and mentor of Herman Daly, who then went on to develop the concept of a steady-state economy to impose permanent government restrictions on the flow of natural resources through the (world) economy.\n\nAs he brought natural resource flows into economic modelling and analysis, Georgescu-Roegen's work was seminal in establishing ecological economics as an independent academic sub-discipline in economics in the 1980s. In addition, the degrowth movement that formed in France and Italy in the early-2000s recognises Georgescu-Roegen as the main intellectual figure influencing the movement. Taken together, by the 2010s Georgescu-Roegen has educated, influenced and inspired at least three generations of people, including his contemporary peers, younger ecological economists, still younger degrowth organisers and activists, and others throughout the world.\n\nThe inability or reluctance of most mainstream economists to recognise Georgescu-Roegen's work has been ascribed to the fact that much of his work reads like applied physics rather than economics, as this latter subject is generally taught and understood today.\n\nGeorgescu-Roegen's work was blemished somewhat by mistakes caused by his insufficient understanding of the physical science of thermodynamics. These mistakes have since generated some controversy, involving both physicists and ecological economists.\n\nThe life of Nicholas Georgescu-Roegen (born Nicolae Georgescu) spanned most of the 20th century, from 1906 to 1994. In his native Romania, he lived through two world wars and three dictatorships before he fled the country. Living in political exile in the US in the second half of his life, he witnessed at a distance the rise and fall of socialism in Romania. He made many important contributions to mainstream neoclassical economics before he finally turned against it and published his paradigmatic magnum opus on \"The Entropy Law and the Economic Process\". Although this work was seminal in establishing ecological economics as an independent academic sub-discipline in economics, Georgescu-Roegen died disappointed and bitter that his paradigmatic work did not receive the appreciation he had expected for it in his own lifetime.\n\nNicolae Georgescu was born in Constanţa, Romania in 1906 to a family of simple origins. His father, of Greek descent, was an army officer. His mother, an ethnic Romanian, was a sewing teacher at a girls school. The father spent time teaching his son how to read, write and calculate, and planted in the boy the seed of intellectual curiosity. By her living example, the mother taught her son the value of hard work. After having lost his position in the army for disciplinary reasons, the father died when Nicolae was only eight years old.\nConstanţa was then a small Black Sea port with some 25,000 inhabitants. The mix of various cultures and ethnic groups in the town shaped Nicolae's cosmopolitan spirit from his earliest years. In primary school, Nicolae excelled at mathematics, and he was encouraged by a teacher to apply for a scholarship at a secondary school, the \"Lyceum Mânăstirea Dealu\" (\"Lycée of the Monastery of the Hill\"), a new military prep school in the town. Nicolae won a scholarship there in 1916, but his attendance was delayed by Romania's entry into World War I. His widowed mother fled with the family to Bucharest, the country's capital, where they stayed with Nicolae's maternal grandmother during the rest of the war. In these times of hardship, Nicolae had traumatic boyhood experiences of the agonies of war. He wanted to become a mathematics teacher, but he could barely keep up his schoolwork.\n\nAfter the war, Nicolae returned to his home town to attend the \"lyceum\". Teaching standards were high, and many of the teachers later went on to become university professors, but the discipline was regimented, with mock-military physical exercises and wearing uniforms. Students were not permitted to leave the school except in summer and briefly during Christmas and Easter. Nicolae proved to be an excellent student, especially in mathematics. He later credited the five years of secondary education he received at the lyceum for providing him with an extraordinary education that would serve him well later in his career, but he also blamed the discipline and the monastic isolation of the place for having stunted his social abilities, something that would put him at odds with acquaintances and colleagues throughout his life.\n\nAt the lyceum, it turned out that Nicolae Georgescu had a namesake. In order to avoid any confusion, he decided to create an addendum to his family name, made up of the first and the last letter of his first name, plus the first four letters of his last name, all six letters put in the reverse order: NicolaE GEORgescu → '-Roegen'. Georgescu-Roegen would retain this addendum for the rest of his life. Later in his life, he also changed his first name to its French and English form, 'Nicholas'.\nGeorgescu-Roegen received his diploma from the lyceum in 1923. Thanks to a scholarship awarded to children from poor families, he was soon after accepted at the University of Bucharest for further studies in mathematics. The curriculum there was conventional, and the teaching methods were much the same as those that had prevailed at the lyceum. At the university, he met the woman who would later become his wife for the rest of his life, Otilia Busuioc. To sustain himself during his studies, he gave private lessons and taught in a grammar school outside the city. After his graduation \"cum laude\" in 1926, he took the examination to qualify as a secondary school teacher and then accepted a teaching post for another year in his former lyceum in Constanţa.\n\nAt the university, Georgescu-Roegen became closely acquainted with one of his professors, Traian Lalescu, a renowned mathematician of the day who had taken a special interest in applying mathematical methods to economic reality using statistics. Lalescu was concerned with the lack of adequate data needed to analyse Romania's economy, so he encouraged Georgescu-Roegen to pursue this line of research in further studies abroad. Georgescu-Roegen soon followed this piece of advice: In 1927 he went to France to study at the \"Institute de Statistique, Sorbonne\" in Paris.\n\nGeorgescu-Roegen's stay in Paris broadened his field of study well beyond pure mathematics. Not only did he attend the lectures of the best statistics and economics professors in France, he also immersed himself in the philosophy of science, especially the works of Blaise Pascal, Ernst Mach, and Henri Bergson. Daily life was not easy for a poor foreign student in a great city. The meager means he received from Romania could barely support even his most basic necessities, and French students habitually referred to all foreign students by the derogatory term \"métèques\", 'strangers'. But his studies progressed splendidly: In 1930, Georgescu-Roegen defended his doctoral dissertation on how to discover the cyclical components of a phenomenon. He passed with extraordinary honour. Émile Borel, one of Georgescu-Roegen's professors, thought so highly of the dissertation that he had it published in full as a special issue of a French academic journal.\n\nWhile studying in Paris, Georgescu-Roegen learned of the work of Karl Pearson at University College in London. Pearson was a leading English scholar of the time, with a field of interests that coincided with Georgescu-Roegen's own, namely mathematics, statistics, and philosophy of science. Georgescu-Roegen made arrangements to lodge with the family of a young Englishman he had met in Paris and left for England in 1931. During his stay in London, his hosts not only accepted Georgescu-Roegen as their paying guest, but also taught him the basics of the English language, in preparation for his studies in the country.\nWhen he approached Pearson and the English university system, Georgescu-Roegen was amazed with the informality and openness he found. There was no more feeling like a \"métèque\", a stranger. Studying with Pearson for the next two years and reading Pearson's work on the philosophy of science, titled \"The Grammar of Science\", further shaped Georgescu-Roegen's scientific methodology and philosophy. The two became friends, and Pearson encouraged Georgescu-Roegen to carry on with his studies in mathematical statistics. They co-pioneered research on the so-called \"problem of moments\", one of the most difficult topics in statistics at the time, but neither was able to solve the problem. This was a great disappointment to Pearson, but Georgescu-Roegen was pleased by their joint effort nonetheless.\n\nWhile studying in London, Georgescu-Roegen was contacted by a representative of the US-based Rockefeller Foundation. Due to his past academic achievements, the foundation wanted to grant Georgescu-Roegen a research fellowship in the US. Georgescu-Roegen had earlier learned of the use of time series analyses by the then famous Harvard Economic Barometer at Harvard University, so he accepted the grant. The trip was put off for about a year, however, as he had more pressing obligations in Romania: He needed to conclude his first national editorial project, a 500 page manual on \"Metoda Statistică\", and he had to care for his aging widowed mother who was in bad health.\n\nIn autumn 1934, Georgescu-Roegen went to the US. On arriving at Harvard University, he learned that the Economic Barometer had been shut down years before: The project had completely failed to predict the Wall Street Crash of 1929, and was soon abandoned altogether. After several failed attempts to find another sponsor for his research, Georgescu-Roegen finally managed a meeting with the professor at the university teaching business cycles to see if there were any other opportunities available to him. This professor happened to be Joseph Schumpeter.\nMeeting Schumpeter at this point completely changed the direction of Georgescu-Roegen's life and career. Schumpeter warmly welcomed Georgescu-Roegen to Harvard, and soon introduced him to the now famous 'circle', one of the most remarkable groups of economists ever working at the same institution, including Wassily Leontief, Oskar Lange, Fritz Machlup, and Nicholas Kaldor, among others. Georgescu-Roegen was now in a stimulating intellectual environment with weekly evening gatherings and informal academic discussions, where Schumpeter himself presided as the 'ringmaster' of the circle. In Schumpeter, Georgescu-Roegen had found a competent and sympathetic mentor. Although Georgescu-Roegen never formally enrolled in any economics classes, this was how he became an economist: \"Schumpeter turned me into an economist... My only degree in economics is from \"Universitas Schumpeteriana\".\"\n\nWhile at Harvard, Georgescu-Roegen published four important papers, laying the foundations for his later theories of production and of consumer preferences. The scholarly quality of these articles impressed Schumpeter.\n\nGeorgescu-Roegen's trip to the US was not all spent at Harvard. He managed to obtain a modest stipend for himself and his wife Otilia that enabled them to travel about the country, journeying as far as California. Through Schumpeter's contacts, Georgescu-Roegen had the opportunity to meet Irving Fisher, Harold Hotelling, and other leading economists of the day. He also met Albert Einstein at Princeton University.\n\nDuring his stay, Georgescu-Roegen's relationship with Schumpeter developed. Realising that Georgescu-Roegen was a promising young scholar, Schumpeter wanted to keep him at Harvard. He offered Georgescu-Roegen a position with the economics faculty, and asked him to work with him on an economics treatise as a joint effort, just the two of them, but Georgescu-Roegen declined. He wanted to go back to Romania in order to serve his backward fatherland that had sponsored most of his education so far; besides, his return was expected at home. Later in his life, Georgescu-Roegen would regret having turned down Schumpeter's generous offer at this point in his career.\n\nIn spring 1936, Georgescu-Roegen left the US. His voyage back to Romania came to last almost a year in itself, as he paid a long visit to Friedrich Hayek and John Hicks at the London School of Economics on the way home. He was in no hurry to return.\n\nFrom 1937 to 1948, Georgescu-Roegen lived in Romania, where he witnessed all the turmoils and excesses of World War II and the subsequent rise to power of the communists in the country. During the war, Georgescu-Roegen lost his only brother due to a fatal reaction to a tuberculosis vaccine.\nUpon his return from the US to Bucharest, Georgescu-Roegen was soon appointed to several government posts. His doctoral dissertation from \"Sorbonne\" as well as his other academic credentials earned him a respectable reputation everywhere, and his fine French and English skills were needed in the foreign affairs department. He became vice-director of the Central Statistical Institute, responsible for compiling data on the country's foreign trade on a daily basis; he also served on the National Board of Trade, settling commercial agreements with the major foreign powers; he even participated in the diplomatic negotiations concerning the reassignment of Romania's national borders with Hungary.\n\nGeorgescu-Roegen engaged himself in politics and joined the pro-monarchy National Peasants' Party. The country's economy was still underdeveloped and had a large agrarian base, where the mass of the peasantry lived in backwardness and poverty. Substantial land reforms were called for if the most appalling inequalities between the rural and the urban parts of the population were to be evened out. Georgescu-Roegen put a persuasive effort into this work and was soon elevated to the higher ranks of the party, becoming member of the party's National Council.\n\nGeorgescu-Roegen did only little academic work during this period of his life. Apart from co-editing the national encyclopedia, the \"Enciclopedia României\", and reporting on the country's economic situation in some minor statistics publications, he published nothing of scholarly significance. Although he did reside in his native country, Georgescu-Roegen would later refer to this period of his life as his Romanian 'exile': The exile was an intellectual one for him.\nBy the end of the war, Romania was occupied by the Soviet Union. A trusted government official and a leading member of an influential political party, Georgescu-Roegen was appointed general secretary of the Armistice Commission, responsible for negotiating the conditions for peace with the occupying power. The negotiations dragged out for half a year and came to involve long and stressful discussions: During most of the war, Romania had been an Axis power allied with Nazi Germany, so the Soviet representatives treated the commission as nothing but a vehicle for levying the largest possible amount of war reparations on the Romanian people.\n\nAfter the war, political forces in the country began encroaching on Georgescu-Roegen. Before and during the war, Romania had already passed through three successive dictatorships, and the fourth one was now imminent. Plenty of items on Georgescu-Roegen's track record were suitable for antagonising both the native Romanian communists and the Soviet authorities that still occupied the country: His top membership of the Peasants' Party, in open opposition to the Communist Party; his chief negotiating position in the Armistice Commission, defending Romania's sovereignty against the occupying power; and his earlier affiliation with capitalist US as a Rockefeller research fellow at Harvard University. Political repression in the country intensified as the rise to power of the communists was completing, and Georgescu-Roegen finally realised it was time to get away: \"... I had to flee Romania before I was thrown into a jail from which no one has ever come out alive.\" By the aid of the Jewish community – he had earlier risked his neck by helping the Jews during the Romanian part of the Holocaust – Georgescu-Roegen and his wife got hold of counterfeit identity cards that secured them the passage out of the country, surrounded by bribed smugglers and stowed away in the hold of a freighter heading for Turkey.\n\nHaving visited Turkey before on official business, Georgescu-Roegen was able to use his contacts there to notify Schumpeter and Leontief at Harvard University in the US about his flight. Leontief offered Georgescu-Roegen a position at Harvard, and made the necessary arrangements for the couple in advance of their arrival there.\n\nAfter a journey from Turkey through continental Europe, Georgescu-Roegen and his wife reached Cherbourg in France, from where they crossed the Atlantic by ship. Georgescu-Roegen's arrival at Harvard in summer 1948 was something of a return for him there. Only now, the circumstances were very different from what they had been in the 1930s: He was no longer a promising young scholar on a trip abroad, supported and sponsored by his native country; instead, he was a middle-aged political refugee who had fled a communist dictatorship behind the Iron Curtain. Yet, he was welcomed at Harvard just the same, obtaining employment as a lecturer and research associate, collaborating with Wassily Leontief on the Harvard Economic Research Project and other subjects. This was not a permanent employment, however.\n\nWhile working at Harvard, Georgescu-Roegen was approached by Vanderbilt University, who offered him a permanent academic chair as economics professor. Georgescu-Roegen accepted the offer and moved to Vanderbilt in Nashville, Tennessee in 1949. It has been argued that Georgescu-Roegen's decision to move from Harvard to the permanence and stability of the less prestigious Vanderbilt was motivated by his precarious wartime experiences and his feeling of insecurity as a political refugee in his new country. It has also been argued that Joseph Schumpeter had at this point lost most of his former influence that could have secured Georgescu-Roegen a permanent position at Harvard (Schumpeter died in 1950). Georgescu-Roegen remained at Vanderbilt until his retirement in 1976 at age 70. Except for short trips, he would never leave Nashville again.\nDuring his years at Vanderbilt University, Georgescu-Roegen pursued an impressive academic career. He held numerous visiting appointments and research fellowships across the continents, and served as editor of a range of academic journals, including the \"Econometrica\". He received several academic honours, including the distinguished Harvie Branscomb Award, presented in 1967 by his employer, Vanderbilt University. In 1971, the very same year his magnum opus was published, he was honoured as Distinguished Fellow of the American Economic Association.\n\nIn the early-1960s, Georgescu-Roegen had Herman Daly as a student. Daly later went on to become a leading ecological economist as well as the economists profession's most faithful, persistent and influential proponent of the economics of Georgescu. However, Georgescu-Roegen, for his part, would later turn critical of his student's work (see below).\n\nThe publication of Georgescu-Roegen's magnum opus in 1971 did not trigger any immediate debates in the mainstream of the economics profession, and the only review in a leading mainstream journal warned the readers against the \"incorrect statements and philosophical generalisations\" made by the author; but Georgescu-Roegen did receive four favourable reviews from heterodox, evolutionary economists.\n\nThrough the 1970s, Georgescu-Roegen had a short-lived cooperation with the Club of Rome. Whereas Georgescu-Roegen's own magnum opus went largely unnoticed by mainstream (neoclassical) economists, the report on \"The Limits to Growth\", published in 1972 by the Club of Rome, created something of a stir in the economics profession. In the heated controversies that followed the report, Georgescu-Roegen found himself largely on the same side as the club, and opposed to the mainstream economists. Teaming up with a natural ally, he approached the club and became a member there. Georgescu-Roegen's theoretical work came to influence the club substantially. One other important result of the cooperation was the publication of the pointed and polemical article on \"Energy and Economic Myths\", where Georgescu-Roegen took issue with mainstream economists and various other debaters. This article found a large audience through the 1970s. Later, the cooperation with the club waned: Georgescu-Roegen reproached the club for not adopting a definite anti-growth political stance; he was also sceptical of the club's elitist and technocratic fashion of attempting to monitor and guide global social reality by building numerous abstract computer simulations of the world economy, and then publish all the findings to the general public. In the early-1980s, the parties finally split up.\nIn continental Europe, Georgescu-Roegen and his work gained influence from the 1970s. When Georgescu-Roegen delivered a lecture at the University of Geneva in Switzerland in 1974, he made a lasting impression on the young and newly graduated French historian and philosopher . The ensuing cooperation and friendship between the two resulted in the French translation of a selection of Georgescu-Roegen's articles entitled \"Demain la décroissance: Entropie – Écologie – Économie\" (\"Tomorrow, the Decline: Entropy – Ecology – Economy\"), published in 1979. Similar to his involvement with the Club of Rome (see above), Georgescu-Roegen's article on \"Energy and Economic Myths\" came to play a crucial role in the dissemination of his views among the later followers of the degrowth movement. In the 1980s, Georgescu-Roegen met and befriended Catalan agricultural economist and historian of economic thought Juan Martínez-Alier, who would soon after become a driving force in the formation of both the International Society for Ecological Economics and the degrowth movement. Since the degrowth movement formed in France and Italy in the early-2000s, leading French champion of the movement Serge Latouche has credited Georgescu-Roegen for being a \"main theoretical source of degrowth.\" Likewise, Italian degrowth theorist Mauro Bonaiuti has considered Georgescu-Roegen's work to be \"one of the analytical cornerstones of the degrowth perspective.\"\n\nApart from his involvement with the Club of Rome and a few European scholars, Georgescu-Roegen remained a solitary man throughout the years at Vanderbilt. He rarely discussed his ongoing work with colleagues and students, and he collaborated in very few joint projects during his career. In addition, several independent sources confirm the observation that Georgescu-Roegen's uncompromising personality and bad temper made him a rather unpleasant acquaintance to deal with. His blunt and demanding behaviour tended to offend most people in academia and elsewhere, thereby undermining his influence and standing.\n\nOn Georgescu-Roegen's formal retirement in 1976, a symposium in his honour was organised by three of his colleagues at Vanderbilt, and the papers presented there were later published as an anthology. No fewer than four Nobel Prize laureates were among the contributing economists; but none of the colleagues from Georgescu-Roegen's department at Vanderbilt participated, a fact that has since been taken as evidence of his social and academic isolation at the place.\n\nAfter Georgescu-Roegen's formal retirement from Vanderbilt in 1976, he continued to live and work as an emeritus in his home in Nashville until his death in 1994. Through these later years, he wrote several articles and papers, expanding on and developing his views. He also corresponded extensively with his few friends and former colleagues.\n\nIn 1988, Georgescu-Roegen was invited to join the editorial board of the newly established academic journal \"Ecological Economics\", published by the International Society for Ecological Economics; but although most of the people organising the journal and the society recognised and admired Georgescu-Roegen's work, he turned down the invitation: He regarded both the journal and the society as nothing but vehicles for promoting concepts like sustainable development and steady-state economics, concepts he himself dismissed as misdirected and wrong (see below, both here and here). Georgescu-Roegen had more ambitious goals in mind: He wanted to overturn and replace the prevailing, but flawed, mainstream paradigm of neoclassical economics with his own 'bioeconomics' (see below); to downscale (degrow) the economy as soon as possible (see below); and \"not\" merely be relegated to some arcane and insignificant – so he believed – economics sub-discipline such as ecological economics.\n\nGeorgescu-Roegen lived long enough to survive the communist dictatorship in Romania he had fled earlier in his life (see above). He even received some late recognition from his fatherland: In the wake of the fall of the Berlin Wall and the subsequent Romanian Revolution in 1989, Georgescu-Roegen was elected to the Romanian Academy in Bucharest. He was pleased by his election.\n\nHis last years were marked by seclusion and withdrawal from the world. By now, Georgescu-Roegen was an old man. Although he had a productive and successful academic career behind him, he was disappointed that his work had not received the dissemination and recognition he had expected for it in his own lifetime. He believed he had long been running against a current. As he likened himself to one unlucky heretic and legendary martyr of science out of the Italian Renaissance, Georgescu-Roegen grumbled and exclaimed: \"\"E pur si muove\" is ordinarily attributed to Galileo, although those words were the last ones uttered by Giordano Bruno on the burning stake!\" He came to realise that he had failed in his life's work to warn the general public and change people's minds about the looming mineral resource exhaustion he himself was very concerned about. He finally grasped that philosophical pessimism may well be a stance favoured by a few solitary intellectuals like himself, but such a stance is normally shunned like a taboo in wider human culture: \"[A] considered pessimist is looked upon as a bearer of bad news and ... is not welcomed ever... \", he lamented. Yet, in spite of his deep disappointment and frustration, he continued to write down and propagate his views as long as he was physically able to do so.\n\nBy the end, his health deteriorated. He was becoming rather deaf, and complications caused by his diabetes rendered him unable to climb stairs. In his final years, he isolated himself completely. He cut off all human contact, even to those of his former colleagues and students who appreciated his contribution to economics. He died bitter and (almost) lonely in his home at the age of 88. His wife Otilia survived him by some four years. The couple had no children.\n\nIn his obituary essay on Georgescu-Roegen, Herman Daly wrote admirably of his deceased teacher and mentor, concluding that \"He demanded a lot, but he gave more.\"\n\nIn his work as an economist, Georgescu-Roegen was influenced by the philosophy of Ernst Mach and the later school of logical positivism derived from Mach. Georgescu-Roegen found that two of his other main sources of inspiration, namely Karl Pearson and Albert Einstein, also had a largely Machian outlook. \"My philosophy is in spirit Machian: it is ... mainly [concerned] with the problem of valid analytical representations of the relations among facts.\" Much of his criticism of both neoclassical economics and of Marxism was based on this outlook.\n\nComing to the US after World War II, Georgescu-Roegen's background soon put him at odds with the dominant theoretical school of neoclassical economics in the country. Having lived in Romania, an underdeveloped and peasant-dominated economy, he realised that neoclassical economics could explain only those social conditions that prevailed in advanced capitalist economies, but not in other institutional settings. He was also critical of the increasing use of abstract algebraic formalism grounded in no facts of social reality. Both of these issues made him attentive to social phenomena that were either overlooked or misrepresented by mainstream neoclassical economic analysis.\n\nIt has been argued that an unbroken path runs from Georgescu-Roegen's work in pure theory in the early years, through his writings on peasant economies in the 1960s, leading to his preoccupation with entropy and bioeconomics in the last 25 years of his life.\n\nAccording to Georgescu-Roegen's own recollection, the ideas presented in his paradigmatic magnum opus were worked out in his mind over a period of twenty years or so before the final publication. The three most important sources of inspiration for his work were Émile Borel's monograph on thermodynamics he had read while studying in Paris (see above); Joseph Schumpeter's view that irreversible evolutionary change are inherent in capitalism; and the Romanian historical record of the large oil refineries in Ploieşti becoming target of strategic military attacks in both world wars, proving the importance of natural resources in social conflict.\n\nGeorgescu-Roegen outlines that both main streams of economic thought having dominated the world since the end of the 19th century – namely neoclassical economics and Marxism – share the shortcoming of not taking into account the importance of natural resources in man's economy. Hence, Georgescu-Roegen engages himself in an intellectual battle with two fronts.\n\nThe physical theory of thermodynamics is based on two laws: The first law states that energy is neither created nor destroyed in any isolated system (a conservation principle). The second law of thermodynamics – also known as the entropy law – states that energy tends to be degraded to ever poorer qualities (a degradation principle).\n\nGeorgescu-Roegen argues that the relevance of thermodynamics to economics stems from the physical fact that man can neither create nor destroy matter or energy, only transform it. The usual economic terms of 'production' and 'consumption' are mere verbal conventions that tend to obscure that nothing is created and nothing is destroyed in the economic process – everything is being transformed.\n\nThe science of thermodynamics features a cosmology of its own predicting the heat death of the universe: Any transformation of energy – whether in nature or in human society – is moving the universe closer towards a final state of inert physical uniformity and maximum entropy. According to this cosmological perspective, all of man's economic activities are only speeding up the general march against a future planetary heat death locally on earth, Georgescu-Roegen submits. This view on the economy was later termed 'entropy pessimism'. Some of Georgescu-Roegen's followers and interpreters have elaborated on this view.\n\nIntroducing the term 'low entropy' for valuable natural resources, and the term 'high entropy' for valueless waste and pollution, Georgescu-Roegen explains that all the economic process does from a physical point of view is to irreversibly transform low entropy into high entropy, thereby providing a flow of natural resources for people to live on. The irreversibility of this economic process is the reason why natural resources are scarce: Recycling of material resources is possible, but only by using up some energy resources plus an additional amount of other material resources; and energy resources, in turn, cannot be recycled at all, but are dissipated as waste heat (according to the entropy law).\nGeorgescu-Roegen points out that the earth is a closed system in the thermodynamic sense of the term: the earth exchanges energy, but not matter (practically) with the rest of the universe. Hence, mainly two sources of low entropy are available to man, namely the stock of mineral resources in the crust of the earth; and the flow of radiation, received from the sun. Since the sun will continue to shine for billions of years to come, the earth's mineral stock is the scarcer one of these two main sources of low entropy. Whereas the stock of minerals may be extracted from the crust of the earth at a rate of our own choosing (practically), the flow of solar radiation arrives at the surface of the earth at a constant and fixed rate, beyond human control, Georgescu-Roegen maintains. This natural 'asymmetry' between man's access to the stock of minerals and the flow of solar energy accounts for the historical contrast between urban and rural life: The busy urban life, on the one hand, is associated with industry and the impatient extraction of minerals; the tranquil rural life, on the other hand, is associated with agriculture and the patient reception of the fixed flow of solar energy. Georgescu-Roegen argues that this 'asymmetry' helps explain the historical subjection of the countryside by the town since the dawn of civilisation, and he criticises Karl Marx for not taking this subjection properly into account in his theory of historical materialism.\nGeorgescu-Roegen explains that modern mechanised agriculture has developed historically as a result of the growing pressure of population on arable land; but the relief of this pressure by means of mechanisation has only substituted a scarcer source of input for the more abundant input of solar radiation: Machinery, chemical fertilisers and pesticides all rely on mineral resources for their operation, rendering modern agriculture – and the industrialised food processing and distribution systems associated with it – almost as dependent on earth's mineral stock as the industrial sector has always been. Georgescu-Roegen cautions that this situation is a major reason why the carrying capacity of earth is decreasing. In effect, overpopulation on earth is largely a dynamic long run phenomenon, being a by-product of ever more constraining mineral scarcities.\n\nGeorgescu-Roegen's model of the economy grew out of his dissatisfaction with neoclassical production theory as well as the input-output model of the economy, developed by Nobel Prize laureate Wassily Leontief. Georgescu-Roegen realised that production cannot be adequately described by stocks of equipment and inventories only, or by flows of inputs and outputs only. It was necessary to combine these two descriptions. In order to complete the picture, it was also necessary to add the new concept of a 'fund'.\n\nIn Georgescu-Roegen's flow-fund model of production, a fund factor is either labour power, farmland, or man-made capital providing a useful service at any point in time. A 'stock' factor is a material or energy input that can be decumulated at will; a 'flow' factor is a stock spread out over a period of time. The fund factors constitute the agents of the economic process, and the flow factors are used or acted upon by these agents. Unlike a stock factor, a fund factor cannot be used (decumulated) at will, as its rate of utilisation depends on the distinct physical properties of the fund (labour power and farmland, for instance, may run the risk of overuse and exhaustion if proper care is not taken).\nContrary to neoclassical production theory, Georgescu-Roegen identifies nature as the exclusive primary source of all factors of production. According to the first law of thermodynamics, matter and energy are neither created nor destroyed in the economy (the conservation principle). According to the second law of thermodynamics – the entropy law – what happens in the economy is that all matter and energy is transformed from states available for human purposes to states unavailable for human purposes (the degradation principle). This transformation constitutes a unidirectional and irreversible process. Consequently, valuable natural resources ('low entropy') are procured by the input end of the economy; the resources flow through the economy, being transformed and manufactured into goods along the way; and unvaluable waste and pollution ('high entropy') eventually accumulate by the output end. Mankind lives in, by, and of nature, and we return our residues to nature. By so doing, the entropy of the combined nature-economy system steadily increases.\n\nThe presence of natural resource flows in Georgescu-Roegen's model of production (production function) differentiates the model from those of both Keynesian macroeconomics, neoclassical economics, as well as classical economics, including most – though not all – variants of Marxism. Only in ecological economics are natural resource flows positively recognised as a valid theoretical basis for economic modelling and analysis.\n\nLater, Georgescu-Roegen's production model formed the basis of his criticism of neoclassical economics (see below).\n\nIn his social theory, Georgescu-Roegen argues that man's economic struggle to work and earn a livelihood is largely a continuation and extension of his biological struggle to sustain life and survive. This biological struggle has prevailed since the dawn of man, and the nature of the struggle was not altered by the invention of money as a medium of exchange. Unlike animals, man has developed exosomatic instruments, that is, tools and equipment. These instruments are produced by man and are not a part of his body. At the same time, production is a social, and not an individual, undertaking. This situation has turned man's struggle to sustain life and survive into a social conflict which is unique when compared to animals. Contrasting his own view with those of Karl Marx, Georgescu-Roegen asserts: \n\nWhen man (some men) attempts to radically change the distribution of access to material resources in society, this may result in wars or revolutions, Georgescu-Roegen admits; but even though wars and revolutions may bring about the intended redistributions, man's economic struggle and the social conflict will remain. There will be rulers and ruled in any social order, and the ruling is largely a continuation of the biological struggle of sustaining life and survive, Georgescu-Roegen claims. Under these material conditions, the ruling classes of past and present have always resorted to force, ideology and manipulation to defend their privileges and maintain the acquiescence of the ruled. This historical fact does not end with communism, Georgescu-Roegen points out; quite the opposite, it goes on during communism, and beyond it as well. It would be contrary to man's biological nature to organise himself otherwise.\n\nLater, Georgescu-Roegen introduced the term 'bioeconomics' (short for 'biological economics') to describe his view that man's economic struggle is a continuation of the biological struggle. In his final years, he planned to write a book on the subject of bioeconomics, but due to old age, he was unable to complete it. He did manage to write a sketch on it, though.\n\nGeorgescu-Roegen takes a dismal view of the future of mankind. On the one hand, his general argument is that the carrying capacity of earth – that is, earth's capacity to sustain human populations and consumption levels – is decreasing as earth's finite stock of mineral resources are being extracted and put to use; but on the other hand, he finds that restraining ourselves collectively on a permanent and voluntary basis for the benefit of future generations runs counter to our biological nature as a species. We cannot help ourselves. Consequently, the world economy will continue growing until its inevitable and final collapse. From that point on, ever deepening scarcities will cause widespread misery, aggravate social conflict throughout the globe, and intensify man's economic struggle to work and earn a livelihood. A prolonged 'biological spasm' of our species will follow, ultimately spelling the end of mankind itself, as man has already become completely and irreversibly dependent on the industrial economy for his biological existence. We are not going to make it. We are doomed to downfall, destruction, and demise. Predicts Georgescu-Roegen: \n\nGeorgescu-Roegen's radically pessimistic 'existential risk' perspective on global mineral resource exhaustion was later countered by Robert Ayres (see below).\n\nIn the years following the publication of his magnum opus in 1971 and until his death in 1994, Georgescu-Roegen published a number of articles and essays where he further expanded on and developed his views.\n\nCriticising neoclassical economics, Georgescu-Roegen argues that neoclassical production theory is false when representing the economy as a mechanical, circular and closed system, with no inlets and no outlets. A misrepresentation such as this fails to take into account the exhaustion of mineral resources at the input end, and the building up of waste and pollution at the output end. In Georgescu-Roegen's view, the economy is represented more accurately by his own flow-fund model of production (see above).\nIn addition, Georgescu-Roegen finds that neoclassical economics tends to overlook, or, at best, to misrepresent the problem of how to allocate the exhaustible mineral resources between present and future generations. Georgescu-Roegen points out that the market mechanisms of supply and demand are systematically unable to work out the intergenerational allocation problem in a satisfactory way, since future generations are not, and cannot be, present on today's market. This anomaly of the market mechanisms – or ecological market failure – is described by Georgescu-Roegen as 'a dictatorship of the present over the future'. On this issue, notable economists and Nobel Prize laureates Robert Solow and Joseph Stiglitz, Georgescu-Roegen's two main adversaries in academia in the 1970s, have stated their account of the mainstream neoclassical approach to the economics of exhaustible resources: They both claim that across the board substitutability of man-made capital for natural capital constitutes a real possibility. Hence, any concern with intergenerational allocation of the mineral stock should be relaxed somewhat (according to Solow); or even ignored altogether (according to Stiglitz).\n\nThe position of Solow and Stiglitz (as well as other, like-minded theorists in the neoclassical tradition) was later termed 'weak sustainability' by environmental economist Kerry Turner.\nIn response to the position of Solow and Stiglitz, Georgescu-Roegen argues that neoclassical economists generally fail to realise the important difference between material resources and energy resources in the economic process. This is where his flow-fund model of production comes into play (see above). Georgescu-Roegen's point is that only material resources can be transformed into man-made capital. Energy resources, on the other hand, cannot be so transformed, as it is physically impossible to turn energy into matter, and matter is what man-made capital is made up of physically. The only possible role to be performed by energy resources is to assist – usually as fuel or electricity – in the process of transforming material resources into man-made capital. In Georgescu-Roegen's own terminology, energy may have the form of either a stock factor (mineral deposits in nature), or a flow factor (resources transformed in the economy); but never that of a fund factor (man-made capital in the economy). Hence, substituting man-made capital for energy resources is physically impossible.\n\nFurthermore, not all material resources are transformed into man-made capital; instead, some material resources are manufactured directly into consumer goods having only a limited durability. Finally, in the course of time, all man-made capital depreciates, wears out and needs replacement; but both old and new man-made capital is made out of material resources to begin with. All in all, the economic process is indeed a process with steadily increasing entropy, and the 'mechanical' notion of across the board substitutability prevalent in neoclassical economics is untenable, Georgescu-Roegen submits.\n\nContrary to the neoclassical position, Georgescu-Roegen argues that flow factors and fund factors (that is, natural resources and man-made capital) are essentially complementary, since both are needed in the economic process in order to have a working economy. Georgescu-Roegen's conclusion, then, is that the allocation of exhaustible mineral resources between present and future generations is a large problem that cannot, and should not, be relaxed or ignored: \"There seems to be no way to do away with the dictatorship of the present over the future, although we may aim at making it as bearable as possible.\" Georgescu-Roegen's followers and interpreters have since been discussing the existential impossibility of allocating earth's finite stock of mineral resources evenly among an unknown number of present and future generations. This number of generations is likely to remain unknown to us, as there is no way – or only little way – of knowing in advance if or when mankind will ultimately face extinction. In effect, \"any\" conceivable intertemporal allocation of the stock will inevitably end up with universal economic decline at some future point. This approach to mankind's prospects is absent in neoclassical economics.\nThe position of Georgescu-Roegen, including his criticism of neoclassical economics, was later termed 'strong sustainability' by Kerry Turner. Later still, Turner's taxonomy of 'weak' and 'strong' sustainability was integrated into ecological economics. However, contrary to the widely established use of Turner's simplifying taxonomy, Georgescu-Roegen never referred to his own position as 'strong sustainability' or any other variant of sustainability. Quite the opposite. Georgescu-Roegen flatly dismissed any notion of sustainable development as only so much 'snake oil' intended to deceive the general public. In his last years, he even denounced the notion bitterly as \"one of the most toxic recipes for mankind\": There can be no such thing as a 'sustainable' rate of extraction and use of a finite stock of non-renewable mineral resources – \"any\" rate will diminish the remaining stock itself. Consequently, the Industrial Revolution has brought about unsustainable economic development in the world (see below).\n\nLeading ecological economist and steady-state theorist Herman Daly is a former student and protégé of Georgescu-Roegen. In the 1970s, Daly developed the concept of a steady-state economy, by which he understands an economy made up of a constant stock of physical wealth (man-made capital) and a constant stock of people (population), both stocks to be maintained by a minimal flow of natural resources (or 'throughput', as he terms it). Daly argues that this steady-state economy is both necessary and desirable in order to keep human environmental impact within biophysical limits (however defined), and to create more allocational fairness between present and future generations with regard to mineral resource use. In several articles, Georgescu-Roegen criticised his student's concept of a steady-state economy.\nGeorgescu-Roegen argues that Daly's steady-state economy will provide no ecological salvation for mankind, especially not in the longer run. Due to the geologic fact that mineral ores are deposited and concentrated very unevenly in the crust of the earth, prospecting for and extraction of mineral resources will sooner or later be faced with the principle of diminishing returns, whereby extraction activities are pushed to still less accessible sites and still lower grades of ores. In the course of time, then, extraction costs and market prices of the incremental amount of resources will tend to increase. Eventually, all minerals will be exhausted, but the \"economic\" exhaustion will manifest itself long before the \"physical\" exhaustion provides the ultimate backstop for further activity: There will still be deposits of resources left in the crust, but the geologic concentration of these deposits will remain below the critical cutoff grade; hence, continued extraction will no longer pay off, and the market for these resources will then collapse. This long-term dynamics will work itself through any economic (sub-)system, regardless of the system's geographical location, its size and its state of development (whether a progressive, a steady or a declining state). In effect, the arguments advanced by Daly in support of his steady-state economy apply with even greater force in support of a declining-state economy, Georgescu-Roegen points out: When the overall purpose is to ration and stretch mineral resource use for as long time into the future as possible, zero economic growth is more desirable than growth is, true; but negative growth is better still! In this context, Georgescu-Roegen also criticises Daly for not specifying \"at what levels\" man-made capital and human population are to be kept constant in the steady-state.\nInstead of Daly's steady-state economics, Georgescu-Roegen proposed his own so-called 'minimal bioeconomic program', featuring quantitative restrictions even more severe than those propounded by Daly.\nHerman Daly on his part has readily accepted his teacher's judgement on this subject matter: In order to compensate for the principle of diminishing returns in mineral resource extraction, an ever greater share of capital and labour in the economy will gradually have to be transferred to the mining sector, thereby skewing the initial structure of any steady-state system. Even more important is it that the steady-state economy will serve only to postpone, and not to prevent, the inevitable mineral resource exhaustion anyway. \"A steady-state economy cannot last forever, but neither can a growing economy, nor a declining economy\", Daly concedes in his response to Georgescu-Roegen's criticism. In the same turn, Daly confirms Georgescu-Roegen's general argument that earth's carrying capacity is decreasing as mankind is extracting the finite mineral stock.\n\nLikewise, several other economists in the field besides Georgescu-Roegen and Daly have agreed that a steady-state economy does not by itself constitute a long-term solution to the 'entropy problem' facing mankind.\n\nIn his technology assessments, Georgescu-Roegen puts thermodynamic principles to use in a wider historical context, including the future of mankind.\n\nAccording to Georgescu-Roegen's terminology, a technology is 'viable' only when it is able to return an energy surplus sufficiently large to maintain its own operation, \"plus\" some additional energy left over for other use. If this criterion is not met, the technology in question is only 'feasible' (if workable at all), but not 'viable'. Both viable and feasible technologies depend on a steady flow of natural resources for their operation.\nGeorgescu-Roegen argues that the first viable technology in the history of man was fire. By controlling fire, it was possible for man to burn a forest, or all forests. It was also possible to cook food and to obtain warmth and protection. Inspired by the ancient Greek myth of Prometheus, the Titan who stole fire from the gods and gave it to man, Georgescu-Roegen terms fire 'the first Promethean recipe'. According to Georgescu-Roegen, a later important Promethean recipe (technology) of the same (first) kind was animal husbandry, feeding on grass and other biomass (like fire does).\n\nMuch later in the history of man, the steam engine came about as the crucial Promethean recipe of the \"second\" kind, feeding on coal. The invention of the steam engine made it possible to drain the groundwater flooding the mine shafts, and the mined coal could then be used as fuel for other steam engines in turn. This technology propelled the Industrial Revolution in Britain in the second half of the 18th century, whereby man's economy has been thrust into a long, never-to-return overshoot-and-collapse trajectory with regard to the earth's mineral stock. Georgescu-Roegen lists the internal combustion engine and the nuclear fission reactor as other, later examples of Promethean recipes of the second kind, namely heat engines feeding on a mineral fuel (oil and uranium (plus thorium), respectively).\nBy a Promethean recipe of the \"third\" kind, Georgescu-Roegen understands a solar collector returning a net energy output sufficiently large to supply all the energy input needed to manufacture an additional solar collector of the same kind, thereby constituting a full serial reproduction with regard to solar energy only. The fact that solar collectors of various kinds had been in operation on a substantial scale for more than a century without providing a breakthrough in energy efficiency brought Georgescu-Roegen to the conclusion that no Promethean recipe was yet around in the world in his day. Only feasible recipes for solar collectors were available, functioning like what he labelled 'parasites' with regard to the terrestrial inputs of energy for their manufacture and operation – and like any other parasite, these recipes cannot survive their host (the 'host' being the sources of the terrestrial inputs). Georgescu-Roegen believed that for a worldwide solar-powered economy to be truly energy self-supporting, a Promethean kind of solar collector had yet to be invented. Later, some scholars have argued that the efficiency of solar collectors has increased considerably since Georgescu-Roegen made these assessments.\n\nGeorgescu-Roegen further points out that regardless of the efficiency of any particular kind of solar collector, the major drawback of solar power \"per se\" when compared to terrestrial fossil fuels and uranium (plus thorium) is the diffuse, low-intensity property of solar radiation. Hence, a lot of material equipment is needed as inputs at the surface of the earth to collect, concentrate and (when convenient) store or transform the radiation before it can be put to use on a larger industrial scale. This necessary material equipment adds to the 'parasitical' operation of solar power, Georgescu-Roegen maintains.\nAssessing fusion power as a possible future source of energy, Georgescu-Roegen ventured the opinion that, regarding magnetic confinement fusion, no reactor will ever be built to be large enough to effectively withstand and confine the vehement thermal pressure of the plasmic deuterium/tritium fusion processes through an extended period of time. He did not assess the other one of the two major fusion power technologies being researched in his day – and still being researched – namely inertial confinement fusion.\nAll of these technology assessments have to do with energy resources only, and not with material resources. Georgescu-Roegen stressed the point that even with the proliferation of solar collectors throughout the surface of the globe, or the advent of fusion power, or both, any industrial economy will still depend on a steady flow of material resources extracted from the crust of the earth, notably metals. He repeatedly argued his case that in the (far) future, it will be scarcity of terrestrial material resources, and not of energy resources, that will prove to impose the most binding constraint on man's economy on earth. As he held no space advocacy views, Georgescu-Roegen failed to assess the (still) emerging technology of asteroid mining or any other known type of space colonisation as potentials for compensating for this future scarcity constraint facing mankind: He was convinced that throughout its entire span of existence, our species will remain confined solely to earth for all practical purposes. His paradigmatic vision concluded thereby.\n\nGeorgescu-Roegen's work was blemished somewhat by mistakes caused by his insufficient understanding of the physical science of thermodynamics. While working on \"The Entropy Law and the Economic Process\" (see above), Georgescu-Roegen had the firm understanding that the entropy law applies equally well to both energy resources and to material resources, and much of the reasoning in the opus rests on this understanding; but, regrettably for Georgescu-Roegen, this understanding was – and still is – false: In thermodynamics proper, the entropy law does apply to energy, but not to matter of macroscopic scale (that is, not to material resources). \nGeorgescu-Roegen himself was not confident about this tentative solution to the problem. He remained embarrassed that he had misinterpreted, and consequently, overstretched the proper application of the physical law that formed part of the title of his magnum opus. He conceded that he had entered into the science of thermodynamics as something of a bold novice. Dedicated to interdisciplinarity, he was worried that physicists would dismiss all of his work as amateurism on this count. The predicament would trouble him for the rest of his life. In one of his last published articles before his death, Georgescu-Roegen described his encouragement when he had once earlier come across the concept of 'matter dissipation' used by German physicist and Nobel Prize laureate Max Planck to account for the existence of irreversible physical processes where no simultaneous transformation of energy was taking place. Georgescu-Roegen found consolation in the belief that the concept of 'matter dissipation' used by a physicist of Planck's authoritative standing would decisively substantiate his own fourth law and his own concept of material entropy.\n\nGeorgescu-Roegen's formulation of a fourth law of thermodynamics and the concept of material entropy soon generated a prolonged controversy, involving both physicists and ecological economists; but some twelve to fifteen years after Georgescu-Roegen's death, a consensus on this subject matter finally emerged along the following lines:\n\n\nIn the same vein as this consensus, a full chapter on the economics of Georgescu-Roegen has approvingly been included in one elementary physics textbook on the historical development of thermodynamics, and the details (Georgescu-Roegen's mistakes) about the fourth law and material entropy are omitted there.\n\nModelling a possible future economic system for mankind, Robert Ayres has countered Georgescu-Roegen's position on the impossibility of complete and perpetual recycling of material resources. According to Ayres, it is possible to develop what he conceptualises as a 'spaceship economy' on earth on a stable and permanent basis, provided that a sufficient flow of energy is available to support it (for example, by an ample supply of solar energy). In this spaceship economy, all waste materials will be temporarily discarded and stored in inactive reservoirs – or what he calls 'waste baskets' – before being recycled and returned to active use in the economic system at some later point in time. It will not be necessary, or even possible, for materials recycling to form its own separate and continuous flow to be of use – only, the waste baskets in question have to be large enough to compensate for the rate and the efficiency of the recycling effort. In effect, complete and perpetual recycling of material resources will be possible in a future spaceship economy of this kind specified, thereby rendering obsolete Georgescu-Roegen's proposed fourth law of thermodynamics, Ayres submits. In a later article, Ayres restated his case for a spaceship economy.\nIn ecological economics, Ayres' contribution vis-à-vis Georgescu-Roegen's proposed fourth law was since described as yet another instance of the so-called 'energetic dogma': Earlier, Georgescu-Roegen had attached the label 'energetic dogma' to various theorists holding the view that only energy resources, and not material resources, are the constraining factor in all economic activity. Ayres appears to be the odd man out on this subject matter: Whatever the scientific status and validity of Georgescu-Roegen's fourth law may be, several other economists in the field besides Georgescu-Roegen deny the practical possibility of ever having complete and perpetual recycling of all material resources in any type of economic system, regardless of the amount of energy, time and information to be assigned to the recycling effort.\n\nEach year since 1987, the Georgescu-Roegen Prize has been awarded by the Southern Economic Association for the best academic article published in the Southern Economic Journal.\n\nIn 2012, two awards in honour of Georgescu-Roegen's life and work were established by The Energy and Resources Institute in New Delhi, India: The Georgescu-Roegen Annual Awards. The awards were officially announced on Georgescu-Roegen's 106th birth anniversary. The awards have two categories: The award for 'unconventional thinking' is presented for scholarly work in academia, and the award for 'bioeconomic practice' is presented for initiatives in politics, business and grassroots organisations.\n\nJapanese ecological economist Kozo Mayumi, a student of Georgescu-Roegen in 1984-88, was the first to receive the award in the 'unconventional thinking' category. Mayumi was awarded for his work on energy analysis and hierarchy theory.\n\n\n"}
{"id": "30016181", "url": "https://en.wikipedia.org/wiki?curid=30016181", "title": "Nonadecylic acid", "text": "Nonadecylic acid\n\nNonadecylic acid, or nonadecanoic acid, is a 19-carbon long saturated fatty acid with the chemical formula CH(CH)COOH. It forms salts called \"nonadecylates\". Nonadecylic acid can be found in fats and vegetable oils. It is also used by insects as pheromones.\n\nn-nonadecanoic acid has found applications in the field of metal lubrication.\n\n"}
{"id": "39224928", "url": "https://en.wikipedia.org/wiki?curid=39224928", "title": "OGDCL Institute of Science and Technology", "text": "OGDCL Institute of Science and Technology\n\nOGDCL Institute of Science and Technology also referred to as OIST is postgraduate institute affiliated with Quaid-i-Azam University located in Islamabad. OIST commenced classes in May, 2013 with an intake of 30 students, in affiliation with Quaid-i-Azam University.\n\nThe Oil and Gas Development Company Limited (OGDCL) has decided to start MS Petroleum Engineering Programme in its Institute of Science and Technology. According to the spokesperson, Adviser to the Prime Minister on Petroleum and Natural Resources Dr Asim Hussain, in this regard, has directed the company to promote higher education and technology in the country. Following the instructions, the official said, the OGDCL Institute of Science and Technology affiliated with Quaid-e-Azam University, Islamabad has initiated its MS Petroleum Engineering Programme for 30 students\n\nOIST offering MS program for 7 different field of engineering i.e. Mechanical, Electrical, Geological Engineering, Geology etc. Zero semester offered for all 7 field.\n\n"}
{"id": "251399", "url": "https://en.wikipedia.org/wiki?curid=251399", "title": "Observable universe", "text": "Observable universe\n\nThe observable universe is a spherical region of the Universe comprising all matter that can be observed from Earth at the present time, because electromagnetic radiation from these objects has had time to reach Earth since the beginning of the cosmological expansion. There are at least 2 trillion galaxies in the observable universe. Assuming the Universe is isotropic, the distance to the edge of the observable universe is roughly the same in every direction. That is, the observable universe has a spherical volume (a ball) centered on the observer. Every location in the Universe has its own observable universe, which may or may not overlap with the one centered on Earth.\n\nThe word \"observable\" in this sense does not refer to the capability of modern technology to detect light or other information from an object, or whether there is anything to be detected. It refers to the physical limit created by the speed of light itself. Because no signals can travel faster than light, any object farther away from us than light could travel in the age of the Universe (estimated around years) simply cannot be detected, as they have not reached us yet. Sometimes astrophysicists distinguish between the \"visible\" universe, which includes only signals emitted since recombination—and the \"observable\" universe, which includes signals since the beginning of the cosmological expansion (the Big Bang in traditional physical cosmology, the end of the inflationary epoch in modern cosmology).\n\nAccording to calculations, the current \"comoving distance\"—proper distance, which takes into account that the universe has expanded since the light was emitted—to particles from which the cosmic microwave background radiation (CMBR) was emitted, which represent the radius of the visible universe, is about 14.0 billion parsecs (about 45.7 billion light-years), while the comoving distance to the edge of the observable universe is about 14.3 billion parsecs (about 46.6 billion light-years), about 2% larger. The radius of the observable universe is therefore estimated to be about 46.5 billion light-years and its diameter about 28.5 gigaparsecs (93 billion light-years, ). The total mass of ordinary matter in the universe can be calculated using the critical density and the diameter of the observable universe to be about 1.5×10 kg.\n\nSince the expansion of the universe is known to accelerate and will become exponential in the future, the light emitted from all distant objects, past some time dependent on their current redshift, will never reach the Earth. In the future all currently observable objects will slowly freeze in time while emitting progressively redder and fainter light. For instance, objects with the current redshift \"z\" from 5 to 10 will remain observable for no more than 4–6 billion years. In addition, light emitted by objects currently situated beyond a certain comoving distance (currently about 19 billion parsecs) will never reach Earth.\n\nSome parts of the universe are too far away for the light emitted since the Big Bang to have had enough time to reach Earth, and so lie outside the observable universe. In the future, light from distant galaxies will have had more time to travel, so additional regions will become observable. However, due to Hubble's law, regions sufficiently distant from the Earth are expanding away from it faster than the speed of light (special relativity prevents nearby objects in the same local region from moving faster than the speed of light with respect to each other, but there is no such constraint for distant objects when the space between them is expanding; see uses of the proper distance for a discussion) and furthermore the expansion rate appears to be accelerating due to dark energy. Assuming dark energy remains constant (an unchanging cosmological constant), so that the expansion rate of the universe continues to accelerate, there is a \"future visibility limit\" beyond which objects will \"never\" enter our observable universe at any time in the infinite future, because light emitted by objects outside that limit would never reach the Earth. (A subtlety is that, because the Hubble parameter is decreasing with time, there can be cases where a galaxy that is receding from the Earth just a bit faster than light does emit a signal that reaches the Earth eventually.) This future visibility limit is calculated at a comoving distance of 19 billion parsecs (62 billion light-years), assuming the universe will keep expanding forever, which implies the number of galaxies that we can ever theoretically observe in the infinite future (leaving aside the issue that some may be impossible to observe in practice due to redshift, as discussed in the following paragraph) is only larger than the number currently observable by a factor of 2.36.\n\nThough in principle more galaxies will become observable in the future, in practice an increasing number of galaxies will become extremely redshifted due to ongoing expansion, so much so that they will seem to disappear from view and become invisible. An additional subtlety is that a galaxy at a given comoving distance is defined to lie within the \"observable universe\" if we can receive signals emitted by the galaxy at any age in its past history (say, a signal sent from the galaxy only 500 million years after the Big Bang), but because of the universe's expansion, there may be some later age at which a signal sent from the same galaxy can \"never\" reach the Earth at any point in the infinite future (so, for example, we might never see what the galaxy looked like 10 billion years after the Big Bang), even though it remains at the same comoving distance (comoving distance is defined to be constant with time—unlike proper distance, which is used to define recession velocity due to the expansion of space), which is less than the comoving radius of the observable universe. This fact can be used to define a type of cosmic event horizon whose distance from the Earth changes over time. For example, the current distance to this horizon is about 16 billion light-years, meaning that a signal from an event happening \"at present\" can eventually reach the Earth in the future if the event is less than 16 billion light-years away, but the signal will never reach the Earth if the event is more than 16 billion light-years away.\n\nBoth popular and professional research articles in cosmology often use the term \"universe\" to mean \"observable universe\". This can be justified on the grounds that we can never know anything by direct experimentation about any part of the universe that is causally disconnected from the Earth, although many credible theories require a total universe much larger than the observable universe. No evidence exists to suggest that the boundary of the observable universe constitutes a boundary on the universe as a whole, nor do any of the mainstream cosmological models propose that the universe has any physical boundary in the first place, though some models propose it could be finite but unbounded, like a higher-dimensional analogue of the 2D surface of a sphere that is finite in area but has no edge. It is plausible that the galaxies within our observable universe represent only a minuscule fraction of the galaxies in the universe. According to the theory of cosmic inflation initially introduced by its founder, Alan Guth (and by D. Kazanas ), if it is assumed that inflation began about 10 seconds after the Big Bang, then with the plausible assumption that the size of the universe before the inflation occurred was approximately equal to the speed of light times its age, that would suggest that at present the entire universe's size is at least 3×10 times the radius of the observable universe. There are also lower estimates claiming that the entire universe is in excess of 250 times larger than the observable universe and also higher estimates implying that the universe is at least 10 times larger than the observable universe.\n\nIf the universe is finite but unbounded, it is also possible that the universe is \"smaller\" than the observable universe. In this case, what we take to be very distant galaxies may actually be duplicate images of nearby galaxies, formed by light that has circumnavigated the universe. It is difficult to test this hypothesis experimentally because different images of a galaxy would show different eras in its history, and consequently might appear quite different. Bielewicz et al. claims to establish a lower bound of 27.9 gigaparsecs (91 billion light-years) on the diameter of the last scattering surface (since this is only a lower bound, the paper leaves open the possibility that the whole universe is much larger, even infinite). This value is based on matching-circle analysis of the WMAP 7 year data. This approach has been disputed.\n\nThe comoving distance from Earth to the edge of the observable universe is about 14.26 gigaparsecs (46.5 billion light-years or ) in any direction. The observable universe is thus a sphere with a diameter of about 28.5 gigaparsecs (93 billion light-years or ). Assuming that space is roughly flat (in the sense of being a Euclidean space), this size corresponds to a comoving volume of about ( or ).\n\nThe figures quoted above are distances \"now\" (in cosmological time), not distances \"at the time the light was emitted\". For example, the cosmic microwave background radiation that we see right now was emitted at the time of photon decoupling, estimated to have occurred about years after the Big Bang, which occurred around 13.8 billion years ago. This radiation was emitted by matter that has, in the intervening time, mostly condensed into galaxies, and those galaxies are now calculated to be about 46 billion light-years from us. To estimate the distance to that matter at the time the light was emitted, we may first note that according to the Friedmann–Lemaître–Robertson–Walker metric, which is used to model the expanding universe, if at the present time we receive light with a redshift of \"z\", then the scale factor at the time the light was originally emitted is given by\n\nformula_1.\n\nWMAP nine-year results combined with other measurements give the redshift of photon decoupling as \"z\" = , which implies that the scale factor at the time of photon decoupling would be . So if the matter that originally emitted the oldest CMBR photons has a \"present\" distance of 46 billion light-years, then at the time of decoupling when the photons were originally emitted, the distance would have been only about 42 \"million\" light-years.\n\nMany secondary sources have reported a wide variety of incorrect figures for the size of the visible universe. Some of these figures are listed below, with brief descriptions of possible reasons for misconceptions about them.\n\n\n\n\n\n\n\nSky surveys and mappings of the various wavelength bands of electromagnetic radiation (in particular 21-cm emission) have yielded much information on the content and character of the universe's structure. The organization of structure appears to follow as a hierarchical model with organization up to the scale of superclusters and filaments. Larger than this (at scales between 30 and 200 megaparsecs), there seems to be no continued structure, a phenomenon that has been referred to as the \"End of Greatness\".\n\nThe organization of structure arguably begins at the stellar level, though most cosmologists rarely address astrophysics on that scale. Stars are organized into galaxies, which in turn form galaxy groups, galaxy clusters, superclusters, sheets, walls and filaments, which are separated by immense voids, creating a vast foam-like structure sometimes called the \"cosmic web\". Prior to 1989, it was commonly assumed that virialized galaxy clusters were the largest structures in existence, and that they were distributed more or less uniformly throughout the universe in every direction. However, since the early 1980s, more and more structures have been discovered. In 1983, Adrian Webster identified the Webster LQG, a large quasar group consisting of 5 quasars. The discovery was the first identification of a large-scale structure, and has expanded the information about the known grouping of matter in the universe. In 1987, Robert Brent Tully identified the Pisces–Cetus Supercluster Complex, the galaxy filament in which the Milky Way resides. It is about 1 billion light-years across. That same year, an unusually large region with a much lower than average distribution of galaxies was discovered, the Giant Void, which measures 1.3 billion light-years across. Based on redshift survey data, in 1989 Margaret Geller and John Huchra discovered the \"Great Wall\", a sheet of galaxies more than 500 million light-years long and 200 million light-years wide, but only 15 million light-years thick. The existence of this structure escaped notice for so long because it requires locating the position of galaxies in three dimensions, which involves combining location information about the galaxies with distance information from redshifts.\nTwo years later, astronomers Roger G. Clowes and Luis E. Campusano discovered the Clowes–Campusano LQG, a large quasar group measuring two billion light-years at its widest point which was the largest known structure in the universe at the time of its announcement. In April 2003, another large-scale structure was discovered, the Sloan Great Wall. In August 2007, a possible supervoid was detected in the constellation Eridanus. It coincides with the 'CMB cold spot', a cold region in the microwave sky that is highly improbable under the currently favored cosmological model. This supervoid could cause the cold spot, but to do so it would have to be improbably big, possibly a billion light-years across, almost as big as the Giant Void mentioned above.\n\nAnother large-scale structure is the SSA22 Protocluster, a collection of galaxies and enormous gas bubbles that measures about 200 million light-years across.\n\nIn 2011, a large quasar group was discovered, U1.11, measuring about 2.5 billion light-years across. On January 11, 2013, another large quasar group, the Huge-LQG, was discovered, which was measured to be four billion light-years across, the largest known structure in the universe at that time. In November 2013, astronomers discovered the Hercules–Corona Borealis Great Wall, an even bigger structure twice as large as the former. It was defined by the mapping of gamma-ray bursts.\n\nThe \"End of Greatness\" is an observational scale discovered at roughly 100 Mpc (roughly 300 million light-years) where the lumpiness seen in the large-scale structure of the universe is homogenized and isotropized in accordance with the Cosmological Principle. At this scale, no pseudo-random fractalness is apparent.\nThe superclusters and filaments seen in smaller surveys are randomized to the extent that the smooth distribution of the universe is visually apparent. It was not until the redshift surveys of the 1990s were completed that this scale could accurately be observed.\n\nAnother indicator of large-scale structure is the 'Lyman-alpha forest'. This is a collection of absorption lines that appear in the spectra of light from quasars, which are interpreted as indicating the existence of huge thin sheets of intergalactic (mostly hydrogen) gas. These sheets appear to be associated with the formation of new galaxies.\n\nCaution is required in describing structures on a cosmic scale because things are often different from how they appear. Gravitational lensing (bending of light by gravitation) can make an image appear to originate in a different direction from its real source. This is caused when foreground objects (such as galaxies) curve surrounding spacetime (as predicted by general relativity), and deflect passing light rays. Rather usefully, strong gravitational lensing can sometimes magnify distant galaxies, making them easier to detect. Weak lensing (gravitational shear) by the intervening universe in general also subtly changes the observed large-scale structure. \n\nThe large-scale structure of the universe also looks different if one only uses redshift to measure distances to galaxies. For example, galaxies behind a galaxy cluster are attracted to it, and so fall towards it, and so are slightly blueshifted (compared to how they would be if there were no cluster) On the near side, things are slightly redshifted. Thus, the environment of the cluster looks a bit squashed if using redshifts to measure distance. An opposite effect works on the galaxies already within a cluster: the galaxies have some random motion around the cluster center, and when these random motions are converted to redshifts, the cluster appears elongated. This creates a \"finger of God\"—the illusion of a long chain of galaxies pointed at the Earth.\n\nAt the centre of the Hydra-Centaurus Supercluster, a gravitational anomaly called the Great Attractor affects the motion of galaxies over a region hundreds of millions of light-years across. These galaxies are all redshifted, in accordance with Hubble's law. This indicates that they are receding from us and from each other, but the variations in their redshift are sufficient to reveal the existence of a concentration of mass equivalent to tens of thousands of galaxies.\n\nThe Great Attractor, discovered in 1986, lies at a distance of between 150 million and 250 million light-years (250 million is the most recent estimate), in the direction of the Hydra and Centaurus constellations. In its vicinity there is a preponderance of large old galaxies, many of which are colliding with their neighbours, or radiating large amounts of radio waves.\n\nIn 1987, astronomer R. Brent Tully of the University of Hawaii's Institute of Astronomy identified what he called the Pisces–Cetus Supercluster Complex, a structure one billion light-years long and 150 million light-years across in which, he claimed, the Local Supercluster was embedded.\n\nThe mass of the observable universe is often quoted as 10 tonnes or 10 kg. In this context, mass refers to ordinary matter and includes the interstellar medium (ISM) and the intergalactic medium (IGM). However, it excludes dark matter and dark energy. This quoted value for the mass of ordinary matter in the universe can be estimated based on critical density. The calculations are for the observable universe only as the volume of the whole is unknown and may be infinite.\nCritical density is the energy density for which the universe is flat. If there is no dark energy, it is also the density for which the expansion of the universe is poised between continued expansion and collapse. From the Friedmann equations, the value for formula_2 critical density, is:\n\nwhere \"G\" is the gravitational constant and H = \"H\" is the present value of the Hubble constant. The current value for \"H\", due to the European Space Agency's Planck Telescope, is \"H\" = 67.15 kilometers per second per mega parsec. This gives a critical density of (commonly quoted as about 5 hydrogen atoms per cubic meter). This density includes four significant types of energy/mass: ordinary matter (4.8%), neutrinos (0.1%), cold dark matter (26.8%), and dark energy (68.3%). Note that although neutrinos are Standard Model particles, they are listed separately because they are difficult to detect and so different from ordinary matter. The density of ordinary matter, as measured by Planck, is 4.8% of the total critical density or . To convert this density to mass we must multiply by volume, a value based on the radius of the \"observable universe\". Since the universe has been expanding for 13.8 billion years, the comoving distance (radius) is now about 46.6 billion light-years. Thus, volume (\"πr\") equals and the mass of ordinary matter equals density () times volume () or .\n\nAssuming the mass of ordinary matter is about (refer to previous section) and assuming all atoms are hydrogen atoms (which in reality make up about 74% of all atoms in our galaxy by mass, see Abundance of the chemical elements), calculating the estimated total number of atoms in the observable universe is straightforward. Divide the mass of ordinary matter by the mass of a hydrogen atom ( divided by ). The result is approximately 10 hydrogen atoms.\n\nThe most distant astronomical object yet announced as of 2016 is a galaxy classified GN-z11. In 2009, a gamma ray burst, GRB 090423, was found to have a redshift of 8.2, which indicates that the collapsing star that caused it exploded when the universe was only 630 million years old. The burst happened approximately 13 billion years ago, so a distance of about 13 billion light-years was widely quoted in the media (or sometimes a more precise figure of 13.035 billion light-years), though this would be the \"light travel distance\" (\"see\" Distance measures (cosmology)) rather than the \"proper distance\" used in both Hubble's law and in defining the size of the observable universe (cosmologist Ned Wright argues against the common use of light travel distance in astronomical press releases on this page, and at the bottom of the page offers online calculators that can be used to calculate the current proper distance to a distant object in a flat universe based on either the redshift \"z\" or the light travel time). The proper distance for a redshift of 8.2 would be about 9.2 Gpc, or about 30 billion light-years. Another record-holder for most distant object is a galaxy observed through and located beyond Abell 2218, also with a light travel distance of approximately 13 billion light-years from Earth, with observations from the Hubble telescope indicating a redshift between 6.6 and 7.1, and observations from Keck telescopes indicating a redshift towards the upper end of this range, around 7. The galaxy's light now observable on Earth would have begun to emanate from its source about 750 million years after the Big Bang.\n\nThe limit of observability in our universe is set by a set of cosmological horizons which limit—based on various physical constraints—the extent to which we can obtain information about various events in the universe. The most famous horizon is the particle horizon which sets a limit on the precise distance that can be seen due to the finite age of the universe. Additional horizons are associated with the possible future extent of observations (larger than the particle horizon owing to the expansion of space), an \"optical horizon\" at the surface of last scattering, and associated horizons with the surface of last scattering for neutrinos and gravitational waves.\n\n\n"}
{"id": "34301968", "url": "https://en.wikipedia.org/wiki?curid=34301968", "title": "OpenVReg", "text": "OpenVReg\n\nOpenVReg stands for Open Voltage Regulator. OpenVReg is NVIDIA power supply specifications. It is an industrial first attempt to standardize both the package and pinout for low voltage DC-DC switching regulators.\n\nThe goal of OpenVReg is to enables packaging and pinout compatibility across multiple sources of similar devices with a single common PCB footprint and layout. OpenVReg defines the basic operation, the minimum set of features, the electrical interface and the mechanical requirements necessary to implement compliant devices.\n\nOpenVReg compliant devices have:\n\n\nThe current OpenVreg development focuses exclusively on DC-DC regulators and controllers. Future version may add definition for other types of devices. The current three OpenVReg regulator types are Type 0, Type 2 and Type2+1.\n\nOpenVReg Type 0 is a step down DC-DC converter with integrated power stage.\n\nType 0 defines three subtypes targeting different applications:\n\nNote: Regulators must be stable with either polymer or MLCC input and output capacitor.\n\nOpenVReg Type 2 is a dual phase DC-DC controller. Type 2 defines two subtypes targeting different applications:\nOpenVReg Type 2+1 is a multi-phasePWM buck switching regulator controller with two integrated gate drivers\n\nOpenVreg Specifications\n\nTexes Instruments<br>\nON Semiconductor<br>\nRichtek<br>\n<br>more...\n\n"}
{"id": "52660479", "url": "https://en.wikipedia.org/wiki?curid=52660479", "title": "Open energy system databases", "text": "Open energy system databases\n\nOpen energy system database projects employ open data methods to collect, clean, and republish energy-related datasets for open use. The resulting information is then available, given a suitable open license, for statistical analysis and for building numerical energy system models, including open energy system models. Permissive licenses like Creative Commons CC0 and are preferred, but some projects will house data made public under market transparency regulations and carrying unqualified copyright.\n\nThe databases themselves may furnish information on national power plant fleets, renewable generation assets, transmission networks, time series for electricity loads, dispatch, spot prices, and cross-border trades, weather information, and similar. They may also offer other energy statistics including fossil fuel imports and exports, gas, oil, and coal prices, emissions certificate prices, and information on energy efficiency costs and benefits.\n\nMuch of the data is sourced from official or semi-official agencies, including national statistics offices, transmission system operators, and electricity market operators. Data is also crowdsourced using public wikis and public upload facilities. Projects usually also maintain a strict record of the provenance and version histories of the datasets they hold. Some projects, as part of their mandate, also try to persuade primary data providers to release their data under more liberal licensing conditions.\n\nTwo drivers favor the establishment of such databases. The first is a wish to reduce the duplication of effort that accompanies each new analytical project as it assembles and processes the data that it needs from primary sources. And the second is an increasing desire to make public policy energy models more transparent to improve their acceptance by policymakers and the public. Better transparency dictates the use of open information, able to be accessed and scrutinized by third-parties, in addition to releasing the source code for the models in question.\n\nIn the mid-1990s, energy models used structured text files for data interchange but efforts were being made to migrate to relational database management systems for data processing. These early efforts however remained local to a project and did not involve online publishing or open data principles.\n\nThe first energy information portal to go live was OpenEI in late 2009, followed by reegle in 2011.\n\nA 2012 paper marks the first scientific publication to advocate the crowdsourcing of energy data. The 2012 PhD thesis by Chris Davis also discusses the crowdsourcing of energy data in some depth. A 2016 thesis surveyed the spatial (GIS) information requirements for energy planning and finds that most types of data, with the exception of energy expenditure data, are available but nonetheless remain scattered and poorly coordinated.\n\nIn terms of open data, a 2017 paper concludes that energy research has lagged behind other fields, most notably physics, biotechnology, and medicine. The paper also lists the benefits of open data and open models and discusses the reasons that many projects nonetheless remain closed. A one-page opinion piece from 2017 advances the case for using open energy data and modeling to build public trust in policy analysis. The article also argues that scientific journals have a responsibility to require that data and code be submitted alongside text for peer review.\n\nData models are central to the design and organization of databases. Open energy database projects generally try to develop and adhere to well resolved data models, using defacto and published standards where applicable. Some projects attempt to coordinate their data models in order to harmonize their data and improve its utility. Defining and maintaining suitable metadata is also a key issue. The life-cycle management of data includes, but is not limited to, the use of version control to track the provenance of incoming and cleansed data. Some sites allow users to comment on and rate individual datasets.\n\nIssues surrounding copyright remain at the forefront with regard to open energy data. As noted, most energy datasets are collated and published by official or semi-official sources. But many of the publicly available energy datasets carry no license, limiting their reuse in numerical and statistical models, open or otherwise. Copyright protected material cannot lawfully be circulated, nor can it be modified and republished.\n\nMeasures to enforce market transparency have not helped much because the associated information is again not licensed to enable modification and republication. Transparency measures include the 2013 European energy market transparency regulation 543/2013. Indeed, 543/2013 \"is only an obligation to publish, not an obligation to license\". Notwithstanding, 543/2013 does enable downloaded data to be computer processed with legal certainty.\n\nEnergy databases with hardware located with the European Union are protected under a general database law, irrespective of the legal status of the information they hold.\nDatabase rights not waived by public sector providers significantly restrict the amount of data a user can lawfully access.\n\nA December 2017 submission by energy researchers in Germany and elsewhere highlighted a number of concerns over the re-use of public sector information within the Europe Union.\nThe submission drew heavily on a recent legal opinion covering electricity data.\n\nNational and international energy statistics are published regularly by governments and international agencies, such as the IEA. In 2016 the United Nations issued guidelines for energy statistics. While the definitions and sectoral breakdowns are useful when defining models, the information provided is rarely in sufficient detail to enable its use in high-resolution energy system models.\n\nThere are few published standards covering the collection and structuring of high-resolution energy system data. The IEC Common Information Model (CIM) defines data exchange protocols for low and high voltage electricity networks.\n\nEnergy system models are data intensive and normally require detailed information from a number of sources. Dedicated projects to collect, collate, document, and republish energy system datasets have arisen to service this need. Most database projects prefer open data, issued under free licenses, but some will accept datasets with proprietary licenses in the absence of other options.\n\nThe OpenStreetMap project, which uses the Open Database License (ODbL), contains geographic information about energy system components, including transmission lines. Wikipedia itself has a growing set of information related to national energy systems, including descriptions of individual power stations.\n\nThe following table summarizes projects that specifically publish open energy system data. Some are general repositories while others (for instance, oedb) are designed to interact with open energy system models in real-time.\n\nThe Energy Research Data Portal for South Africa is being developed by the Energy Research Centre, University of Cape Town, Cape Town, South Africa. Coverage includes South Africa and certain other African countries where the Centre undertakes projects. The website uses the CKAN open source data portal software. A number of data formats are supported, including CSV and XLSX. The site also offers an API for automated downloads. , the portal contained 65datasets.\n\nThe energydata.info project from the World Bank Group, Washington, DC, USA is an energy database portal designed to support national development by improving public access to energy information. As well as sharing data, the platform also offers tools to visualize and analyze energy data. Although the World Bank Group has made available a number of dataset and apps, external users and organizations are encouraged to contribute. The concepts of open data and open source development are central to the project. energydata.info uses its own fork of the CKAN open source data portal as its web-based platform. The Creative Commons CC BY 4.0 license is preferred for data but other open licenses can be deployed. Users are also bound by the terms of use for the site.\n\n, the database held 131datasets, the great majority related to developing countries. The datasets are tagged and can be easily filtered. A number of download formats, including GIS files, are supported: CSV, XLS, XLSX, ArcGIS, Esri, GeoJSON, KML, and SHP. Some datasets are also offered as HTML. Again, , four apps are available. Some are web-based and run from a browser.\n\nThe semantic wiki-site and database Enipedia lists energy systems data worldwide. Enipedia is maintained by the Energy and Industry Group, Faculty of Technology, Policy and Management, Delft University of Technology, Delft, the Netherlands. A key tenet of Enipedia is that data displayed on the wiki is not trapped within the wiki, but can be extracted via SPARQL queries and used to populate new tools. Any programming environment that can download content from a URL can be used to obtain data. Enipedia went live in March 2011, judging by traffic figures quoted by Davis.\n\nA 2010 study describes how community driven data collection, processing, curation, and sharing is revolutionizing the data needs of industrial ecology and energy system analysis. A 2012 chapter introduces a system of systems engineering (SoSE) perspective and outlines how agent-based models and crowdsourced data can contribute to the solving of global issues.\n\nThe OpenEnergy Platform (OEP) is a collaborative versioned dataset repository for storing open energy system model datasets. A dataset is presumed to be in the form of a database table, together with metadata. Registered users can upload and download datasets manually using a web-interface or programmatically via an API using HTTP POST calls. Uploaded datasets are screened for integrity using deterministic rules and then subject to confirmation by a moderator. The use of versioning means that any prior state of the database can be accessed (as recommended in this 2012 paper). Hence, the repository is specifically designed to interoperate with energy system models. The backend is a PostgreSQL object-relational database under subversion version control. Open source licenses are specific to each dataset. Unlike other database projects, users can download the current version (the public tables) of the entire PostgreSQL database or any previous version. Initial development is being lead by the Reiner Lemoine Institute, Berlin, Germany.\n\nThe Open Data Energy Networks ( or ) portal is run by eight partners, led by the French national transmission system operator (TSO) Réseau de Transport d'Électricité (RTE). The portal was previously known as Open Data RTE. The site offers electricity system datasets under a Creative Commons compatible license, with metadata, an RSS feed for notifying updates, and an interface for submitting questions. of information obtained from the site can also register third-party URLs (be they publications or webpages) against specific datasets.\n\nThe portal uses the French Government Licence Ouverte license and this is explicitly compatible with the United Kingdom Open Government Licence (OGL), the Creative Commons license (and thereby later versions), and the Open Data Commons license.\n\nThe site hosts electricity, gas, and weather information related to France.\n\nThe Open Power System Data (OPSD) project seeks to characterize the German and western European power plant fleets, their associated transmission network, and related information and to make that data available to energy modelers and analysts. The platform was originally implemented by the University of Flensburg, DIW Berlin, the Technical University of Berlin, and the energy economics consultancy Neon Neue Energieökonomik, all from Germany. The first phase of the project, from August 2015 to July 2017, was funded by the Federal Ministry for Economic Affairs and Energy (BMWi) for . The project later received funding for a second phase, from January 2018 to December 2020, with ETH Zurich replacing Flensburg University as a partner.\n\nDevelopers collate and harmonize data from a range of government, regulatory, and industry sources throughout Europe. The website and the metadata utilize English, whereas the original material can be in any one of 24languages. Datasets follow the emerging frictionless data package standard being developed by Open Knowledge International (OKI). The website was launched on 28October 2016. , the project offers the following primary packages, for Germany and other European countries:\n\n\nIn addition, the project hosts selected contributed packages:\n\n\nTo facilitate analysis, the data is aggregated into large structured files (in CSV format) and loaded into data packages with standardized machine-readable metadata (in JSON format). The same data is usually also provided as XLSX (Excel) and SQLite files. The datasets can be accessed in real-time using stable URLs. The Python scripts deployed for data processing are available on GitHub and carry an MIT license. The licensing conditions for the data itself depends on the source and varies in terms of openness. Previous versions of the datasets and scripts can be recovered in order to track changes or replicate earlier studies. The project also engages with energy data providers, such as transmission system operators (TSO) and ENTSO-E, to encourage them to make their data available under open licenses (for instance, Creative Commons and ODbL licenses).\n\nA number of published electricity market modeling analyses are based on OPSD data.\n\nIn 2017, the Open Power System Data project won the Schleswig-Holstein Open Science Award and the Germany Land of Ideas award.\n\nOpen Energy Information (OpenEI) is a collaborative website, run by the US government, providing open energy data to software developers, analysts, users, consumers, and policymakers. The platform is sponsored by the United States Department of Energy (DOE) and is being developed by the National Renewable Energy Laboratory (NREL). OpenEI launched on 9December 2009. While much of its data is from US government sources, the platform is intended to be open and global in scope.\n\nOpenEI provides two mechanisms for contributing structured information: a semantic wiki (using MediaWiki and the Semantic MediaWiki extension) for collaboratively-managed resources and a dataset upload facility for contributor-controlled resources. US government data is distributed under a CC0 public domain dedication, whereas other contributors are free to select an open data license of their choice. Users can rate data using a five-star system, based on accessibility, adaptability, usefulness, and general quality. Individual datasets can be manually downloaded in an appropriate format, often as CSV files. Scripts for processing data can also be shared through the site. In order to build a community around the platform, a number of forums are offered covering energy system data and related topics.\n\nMost of the data on OpenEI is exposed as linked open data (LOD) (described elsewhere on this page). OpenEI also uses LOD methods to populate its definitions throughout the wiki with real-time connections to DBPedia, reegle, and Wikipedia.\n\nOpenEI has been used to classify geothermal resources in the United States. And to publicize municipal utility rates, again within the US.\n\nOpenGridMap employs crowdsourcing techniques to gather detailed data on electricity network components and then infer a realistic network structure using methods from statistics and graph theory. The scope of the project is worldwide and both distribution and transmission networks can be reverse engineered. The project is managed by the Chair of Business Information Systems, TUM Department of Informatics, Technical University of Munich, Munich, Germany. The project maintains a website and a Facebook page and provides an Android mobile app to help the public document electrical devices, such as transformers and substations. The bulk of the data is being made available under a Creative Commons license. The processing software is written primarily in Python and MATLAB and is hosted on GitHub.\n\nOpenGridMap provides a tailored GIS web application, layered on OpenStreetMap, which contributors can use to upload and edit information directly. The same database automatically stores field recordings submitted by the mobile app. Subsequent classification by experts allows normal citizens to document and photograph electrical components and have them correctly identified. The project is experimenting with the use of hobby drones to obtain better information on associated facilities, such as photovoltaic installations. Transmission line data is also sourced from and shared with OpenStreetMap. Each component record is verified by a moderator.\n\nOnce sufficient data is available, the transnet software is run to produce a likely network, using statistical correlation, Voronoi partitioning, and minimum spanning tree (MST) algorithms. The resulting network can be exported in CSV (separate files for nodes and lines), XML, and CIM formats. CIM models are well suited for translation into software-specific data formats for further analysis, including power grid simulation. Transnet also displays descriptive statistics about the resulting network for visual confirmation.\n\nThe project is motivated by the need to provide datasets for high-resolution energy system models, so that energy system transitions (like the German \"Energiewende\") can be better managed, both technically and policy-wise. The rapid expansion of renewable generation and the anticipated uptake of electric vehicles means that electricity system models must increasingly represent distribution and transmission networks in some detail.\n\n, OpenGridMap techniques have been used to estimate the low voltage network in the German city of Garching and to estimate the high voltage grids in several other countries.\n\nreegle is a clean energy information portal covering renewable energy, energy efficiency, and climate compatible development topics. reegle was launched in 2006 by REEEP and REN21 with funding from the Dutch (VROM), German (BMU), and UK (Defra) environment ministries. Originally released as a specialized internet search engine, reegle was relaunched in 2011 as an information portal.\n\nreegle offers and utilizes linked open data (LOD) (described elsewhere on this page). Sources of data include UN and World Bank databases, as well as dedicated partners around the world. reegle maintains a comprehensive structured glossary (driven by an LOD-compliant thesaurus) of energy and climate compatible development terms to assist with the tagging of datasets. The glossary also facilitates intelligent web searches.\n\nreegle offers country profiles which collate and display energy data on a per-country basis for most of the world. These profiles are kept current automatically using LOD techniques.\n\nRenewables.ninja is a website that can calculate the hourly power output from solar photovoltaic installations and wind farms located anywhere in the world. The website is a joint project between the Department of Environmental Systems Science, ETH Zurich, Zürich, Switzerland and the Centre for Environmental Policy, Imperial College London, London, United Kingdom. The website went live during September 2016. The resulting time series are provided under a Creative Commons license and the underlying power plant models are published using a BSD-new license. , only the solar model, written in Python, has been released.\n\nThe project relies on weather data derived from meteorological reanalysis models and weather satellite images. More specifically, it uses the 2016 MERRA-2 reanalysis dataset from NASA and satellite images from CM-SAF SARAH. For locations in Europe, this weather data is further \"corrected\" by country so that it better fits with the output from known PV installations and windfarms. Two 2016 papers describe the methods used in detail in relation to Europe. The first covers the calculation of PV power. And the second covers the calculation of wind power.\n\nThe website displays an interactive world map to aid the selection of a site. Users can then choose a plant type and enter some technical characteristics. , only year 2014 data can be served, due to technical restrictions. The results are automatically plotted and are available for download in hourly CSV format with or without the associated weather information. The site offers an API for programmatic dataset recovery using token-based authorization. Examples deploying cURL and Python are provided.\n\nA number of studies have been undertaking using the power production datasets underpinning the website (these studies predate the launch of the website), with the bulk focusing on energy options for Great Britain.\n\nThe SMARD site (pronounced \"smart\") serves electricity market data from Germany, Austria, and Luxembourg and also provides visual information. The electricity market plots and their underlying time series are released under a permissive CC BY 4.0 license. The site itself was launched on 3July 2017 in German and an English translation followed shortly. The data portal is mandated under the German Energy Industry Act (\"\" or \"EnWG\") section §111d, introduced as an amendment on 13October 2016. Four table formats are offered: CSV, XLS, XML, and PDF. The maximum sampling resolution is . Market data visuals or plots can be downloaded in PDF, SVG, PNG, and JPG formats. Representative output is shown in the thumbnail (on the left), in this case mid-winter dispatch over two days for the whole of Germany. The horizontal ordering by generation type is first split into renewable and conventional generation and then based on merit.\n\n\n\n"}
{"id": "21857455", "url": "https://en.wikipedia.org/wiki?curid=21857455", "title": "PGE Polska Grupa Energetyczna", "text": "PGE Polska Grupa Energetyczna\n\nPGE Polska Grupa Energetyczna S.A. (PGE SA or PGE Group, the name can be translated as \"Polish Energy Group\") is a state-owned public power company and the largest power producing company in Poland. PGE is listed on the Warsaw Stock Exchange and is a constituent of the WIG30 index.\n\nThe group is largely controlled by the Polish State Treasury who as of 9 July 2014 owns 58.39% of the public limited company. In addition to the activities of its core businesses of central and holdings companies in the generation and distribution of electricity, the group also trades electricity and other relevant products on the market. The total company revenue for 2015 was 28.542 billion złoty and the company made a net income loss of 3.032 billion złoty.\n\nThe PGE Group was founded as Polskie Sieci Elektroenergetyczne S.A. (translates as \"Polish Electrical Power Lines S.A.\") in 1990. In 2007 the Transmission System Operator division (PSE-Operator) was separated from the PSE S.A. group. On 9 May 2007 Polska Grupa Energetyczna was established by the merger of PSE, PGE Energia SA and BOT Górnictwo i Energetyka S.A.\n6 November 2009 IPO in Warsaw Stock Exchange.\n19 March 2010 included to WIG20.\n\nThe PGE Group operates two large lignite mines and more than 40 power stations, including the Bełchatów Power Station. Power stations are fueled mainly by hard coal and lignite. The company consists of eight distribution system operator companies, eight electricity retail sales companies, an electricity wholesale company and enterprises operating in other industries (including the telecommunications). PGE holds a 38% market share in generation of electricity and an estimated 30% market share in supply in electricity for the year 2015. \n\nOn 15 January 2009, the company announced a plan to build two nuclear power stations in Poland. It also participates in the Visaginas Nuclear Power Plant project.\n\nThe company bought the naming rights for the PGE Arena Gdańsk, a football stadium in Gdańsk, Poland, for 35 million złoty (about €8.5 million) for a duration of five years. The 2010 Speedway World Cup is named according to PGE.\nPolska Grupa Energetyczna bought PGE Skra Bełchatów volleyball club.\n\n"}
{"id": "22860615", "url": "https://en.wikipedia.org/wiki?curid=22860615", "title": "Plantibody", "text": "Plantibody\n\nA plantibody is an antibody that is produced by plants that have been genetically engineered with animal DNA. An antibody (also known as an immunoglobulin) is a complex protein within the body that recognizes antigens on viruses and other dangerous compounds in order to alert the immune system that there are pathogens within the body. The transgenic plants become transformed with the DNA and produce antibodies that are similar to those inserted. The term plantibody and the concept are trademarked by the company Biolex.\n\nA plantibody is produced by insertion of genes encoding antibodies into a transgenic plant. The plantibodies are then modified by intrinsic plant mechanisms (N-glycosylation). Plantibodies are purified through processes such as filtration, chromatography, and diafiltration. It is less costly to produce antibodies in transgenic plants than in transgenic animals. \nTransgenic plants offer an attractive method for large-scale production of antibodies for immunotherapy. Antibodies produced in plants have many advantage that are beneficial to humans, plants, and the economy as well. They can be purified cheaply and in large numbers. The many seeds of plants allow for ample storage, and they have no risk of transmitting diseases to humans because the antibodies are produced without the need of the antigen or infectious microorganisms. Plants could be engineered to produce antibodies which fight off their own plant diseases and pests, for example, nematodes, and eliminate the need for toxic pesticides.\n\nAntibodies generated by plants are cheaper, easier to manage, and safer to use than those obtained from animals. The applications are increasing because recombinant DNA is very useful in creating proteins that are identical when exposed into a plant's. A recombinant DNA is an artificial DNA that is created by combining two or more sequences that would not normally come together. In this way, DNA injected into a plant is turned into recombinant DNA and manipulated. The favorable properties of plants are likely to make the plant systems a useful alternative for small, medium and large scale production throughout the development of new antibody-based pharmaceuticals.\n\nThe main reason plants are being used to produce antibodies is for treatment of illnesses such as immune disorders, cancer, and inflammatory diseases, given the fact that the plantibodies also have no risk of spreading diseases to humans.\nIn the past 2 decades, research has shown that plant-derived antibodies have become easier to produce.\n\nPlantibodies are close to passing clinical trials and becoming approved commercially because of key points. Plants are more economical than most forms of creating antibodies and the technology for harvesting and maintaining them is already present. Plants also reduce the chance of coming in contact with pathogens, making their antibodies safer to use. Plantibodies can be made at an affordable cost and easier manufacturing due to the availability and relatively easy manipulation of genetic information in crops such as potatoes, soybean, alfalfa, rice, wheat and tobacco.\n\nCommercial use is not yet legalized, but clinical trials are underway to implement the use of plantibodies for humans as injections. So far, companies have started conducting human tests of pharmaceutical products, creating plantibodies that include:\nBy being able to genetically alter plants to create specific antibodies, it is easier to produce antibodies that will fight diseases not only for plants but for human as well. For that reason, plantibody applications will move more towards the medicinal field.\n\n"}
{"id": "1394398", "url": "https://en.wikipedia.org/wiki?curid=1394398", "title": "Reinforced carbon–carbon", "text": "Reinforced carbon–carbon\n\nCarbon fibre reinforced carbon (CFRC), \ncarbon–carbon (C/C), \nor reinforced carbon–carbon (RCC) \nis a composite material consisting of carbon fiber reinforcement in a matrix of graphite. It was developed for the nose cones of intercontinental ballistic missiles, and is most widely known as the material for the nose cone and wing leading edges of the Space Shuttle orbiter. It has been used in the brake systems of Formula One racing cars since 1976; carbon–carbon brake discs and pads are a standard component of Formula One brake systems.\n\nCarbon–carbon is well-suited to structural applications at high temperatures, or where thermal shock resistance and/or a low coefficient of thermal expansion is needed. While it is less brittle than many other ceramics, it lacks impact resistance; Space Shuttle \"Columbia\" was destroyed during atmospheric re-entry after one of its RCC panels was broken by the impact of a piece of foam insulation from the Space Shuttle External Tank. \n\nThe material is made in three stages:\n\nFirst, material is laid up in its intended final shape, with carbon filament and/or cloth surrounded by an organic binder such as plastic or pitch. Often, coke or some other fine carbon aggregate is added to the binder mixture.\n\nSecond, the lay-up is heated, so that pyrolysis transforms the binder to relatively pure carbon. The binder loses volume in the process, causing voids to form; the addition of aggregate reduces this problem, but does not eliminate it.\n\nThird, the voids are gradually filled by forcing a carbon-forming gas such as acetylene through the material at a high temperature, over the course of several days. This long heat treatment process also allows the carbon to form into larger graphite crystals, and is the major reason for the material's high cost. The gray \"Reinforced Carbon–Carbon (RCC)\" panels on the space shuttle's wing leading edges and nose cone cost NASA $100,000/sq ft to produce, although much of this cost was a result of the advanced geometry and research costs associated with the panels.\n\nC/C is a hard material that can be made highly resistant to thermal expansion, temperature gradients, and thermal cycling, depending on how the fibre scaffold is laid up and the quality/density of the matrix filler.\n\nThe strength of carbon–carbon with unidirectional reinforcement fibres is up to 700 MPa. Carbon–carbon materials retain their properties above 2000 °C. This temperature may be exceeded with the help of protective coatings to prevent oxidation.\nThe material has a density between 1.6–1.98 g/cm.\n\nCarbon fibre-reinforced silicon carbide (C/SiC) is a development of pure carbon–carbon, and can be used in automotive applications, such as components of brake systems on high performance road cars, namely the brake disc and brake pads. C/SiC utilises silicon carbide with carbon fibre, and this compound is thought to be more durable than pure carbon-carbon. However, it is heavier and hence not used in Formula 1 racing.\n\nApplications initially included the Mercedes-Benz C215 Coupe F1 edition, and are standard fitment on the Bugatti Veyron and certain current Bentleys, Ferraris, Porsches, Corvette ZR1, ZO6 and Lamborghinis. They are also offered as an \"optional upgrade\" on certain high performance Audi cars, including the D3 S8, B7 RS4, C6 S6 and RS6, and the R8.\n\nCarbon brakes became widely available for commercial airplanes in the 1980s having been first used on the Concorde supersonic transport.\n\nA related non-ceramic carbon composite with uses in high tech racing automotives is the carbotanium carbon–titanium composite used in the Zonda R and Huayra supercars made by the Italian motorcar company Pagani.\n\n"}
{"id": "5907903", "url": "https://en.wikipedia.org/wiki?curid=5907903", "title": "Rinky Dink", "text": "Rinky Dink\n\nRinky Dink is a mobile musical sound system that operates on power provided by two bicycles and solar panels. It tours the world as part of many musical festivals and parties and is an example of how green electricity can be generated and used to power things. As well as being powered by bicycle, the system itself is moved around using specially converted bicycles. It has been featured on the BBC programmes \"Newsnight\" and \"Panorama\".\n\nThe Rinky Dink was also responsible for powering the first bicycle-powered digital recording in history—\"Live & Pedal-Powered\" (1995) by Baka Beyond.\n\nThe system was named after the expression \"rinky-dink\" which originally meant \"rip-off\", but came to mean anything that was poorly put together, amateurish, or shoddy.\n\n\n"}
{"id": "44817299", "url": "https://en.wikipedia.org/wiki?curid=44817299", "title": "Sack (unit)", "text": "Sack (unit)\n\nThe sack (abbreviation: sck.) was an English unit of weight or mass used for coal and wool. It has also been used for other commodities by weight, commodities by volume, and for both weight and volume in the United States.\n\nThe wool sack or woolsack ( or \"\") was standardized as 2 wey of 14 stone each, with each stone 12½ merchants' pounds each (350 lbs. or about 153 kg), by the time of the Assize of Weights and Measures . 12 such sacks formed the wool last.\n\nThe coal sack was standardized as an imperial hundredweight of 112 avoirdupois pounds (approx. 51 kg).\n\nThe large sack was a UK unit of weight for coal. It was introduced by the London, Westminster and Home Counties Coal Trade Act of 1831 (2 Will 4 c lxxvi), which required coal to be sold by weight rather than volume.\n\nThe Royal Navy used large sacks holding two hundredweight for coaling its ships. These sacks were made of jute bound with Manila rope. They were filled in the hold of a collier using a scoop and then a wire cable was run through two iron rings at the mouth of the sack to close and hoist it over to the warship, twelve sacks at a time. A sack truck would then be used to take each sack to the chute of the warship's coal bunker where they would be emptied. These sacks were large and heavy, weighing at least sixteen pounds when empty, and costing 11 shillings and sixpence before the First World War.\n\nThe large sack was defined as 224 pounds.\n\n1 large sack ≡ 2 sacks, equivalent to 2 cwt, 224 lb, or about 102 kg\n\nThe sack has also been used as a unit of volume. In the American oil industry, a sack represents the amount of portland cement that occupies , and in most cases weighs . Other uses in the US include the measurement by volume of salt, where one sack is , cotton where one sack is and flour, where one sack is just . It has also been used as a measure of volume for dry goods in Britain, with one sack being equivalent to .\n\nIn British usage, a sack of flour was equivalent to 20 stones, or one-eighth of a long ton. A sack of coal was 16 stones, or , while the weight of a sack of wool depended on who was selling it. A sack of grower's wool was 3.25 hundredweight or , whereas a sack of dealer's wool was considerably lighter, at .\n\n"}
{"id": "5664906", "url": "https://en.wikipedia.org/wiki?curid=5664906", "title": "Slip melting point", "text": "Slip melting point\n\nThe Slip melting point (SMP) or \"slip point\" is one conventional definition of the melting point of a waxy solid. It is determined by casting a 10 mm column of the solid in a glass tube with an internal diameter of about 1 mm and a length of about 80 mm, and then\nimmersing it in a temperature-controlled water bath. The slip point is\nthe temperature at which the column of the solid begins to rise in the tube\ndue to buoyancy, and because the outside surface of the solid is molten.\n\nThis is a popular method for fats and waxes, because they tend to be mixtures of compounds with a range of molecular masses, without well-defined melting points.\n"}
{"id": "338511", "url": "https://en.wikipedia.org/wiki?curid=338511", "title": "Stearic acid", "text": "Stearic acid\n\nStearic acid ( , ) is a saturated fatty acid with an 18-carbon chain and has the IUPAC name octadecanoic acid. It is a waxy solid and its chemical formula is CHCOH. Its name comes from the Greek word στέαρ \"stéar\", which means tallow. The salts and esters of stearic acid are called stearates. As its ester, stearic acid is one of the most common saturated fatty acids found in nature following palmitic acid. The triglyceride derived from three molecules of stearic acid is called stearin.\n\nStearic acid is obtained from fats and oils by the saponification of the triglycerides using hot water (about 100 °C). The resulting mixture is then distilled. Commercial stearic acid is often a mixture of stearic and palmitic acids, although purified stearic acid is available. \n\nFats and oils rich in stearic acid are more abundant in animal fat (up to 30%) than in vegetable fat (typically <5%). The important exceptions are cocoa butter and shea butter, where the stearic acid content (as a triglyceride) is 28–45%.\n\nIn terms of its biosynthesis, stearic acid is produced from carbohydrates via the fatty acid synthesis machinery wherein acetyl-CoA contributes two-carbon building blocks.\n\nIn general, the applications of stearic acid exploit its bifunctional character, with a polar head group that can be attached to metal cations and a nonpolar chain that confers solubility in organic solvents. The combination leads to uses as a surfactant and softening agent. Stearic acid undergoes the typical reactions of saturated carboxylic acids, a notable one being reduction to stearyl alcohol, and esterification with a range of alcohols. This is used in a large range of manufactures, from simple to complex electronic devices.\n\nStearic acid is mainly used in the production of detergents, soaps, and cosmetics such as shampoos and shaving cream products. Soaps are not made directly from stearic acid, but indirectly by saponification of triglycerides consisting of stearic acid esters. Esters of stearic acid with ethylene glycol, glycol stearate, and glycol distearate are used to produce a pearly effect in shampoos, soaps, and other cosmetic products. They are added to the product in molten form and allowed to crystallize under controlled conditions. Detergents are obtained from amides and quaternary alkylammonium derivatives of stearic acid.\n\nIn view of the soft texture of the sodium salt, which is the main component of soap, other salts are also useful for their lubricating properties. Lithium stearate is an important component of grease. The stearate salts of zinc, calcium, cadmium, and lead are used to soften PVC. Stearic acid is used along with castor oil for preparing softeners in textile sizing. They are heated and mixed with caustic potash or caustic soda. Related salts are also commonly used as release agents, e.g. in the production of automobile tires.\n\nBeing inexpensive and chemically benign, stearic acid finds many niche applications. As an example, it can be used to make castings from a plaster \"piece mold\" or \"waste mold\", and to make a mold from a shellacked clay original. In this use, powdered stearic acid is mixed in water and the suspension is brushed onto the surface to be parted after casting. This reacts with the calcium in the plaster to form a thin layer of calcium stearate, which functions as a release agent. When reacted with zinc it forms zinc stearate, which is used as a lubricant for playing cards (fanning powder) to ensure a smooth motion when fanning. Stearic acid is a common lubricant during injection molding and pressing of ceramic powders. It is also used as a mold release for foam latex that is baked in stone molds.\n\nStearic acid is used as a negative plate additive in the manufacture of lead-acid batteries. It is added at the rate of 0.6 g per kg of the oxide while preparing the paste. It is believed to enhance the hydrophobicity of the negative plate, particularly during dry-charging process. It also reduces the extension of oxidation of the freshly formed lead (negative active material) when the plates are kept for drying in the open atmosphere after the process of tank formation. As a consequence, the charging time of a dry uncharged battery during initial filling and charging (IFC) is comparatively lower, as compared to a battery assembled with plates which do not contain stearic acid additive.\n\nFatty acids are classic components of candle-making. Stearic acid is used along with simple sugar or corn syrup as a hardener in candies. In fireworks, stearic acid is often used to coat metal powders such as aluminium and iron. This prevents oxidation, allowing compositions to be stored for a longer period of time.\n\nAn isotope labeling study in humans concluded that the fraction of dietary stearic acid that oxidatively desaturates to oleic acid is 2.4 times higher than the fraction of palmitic acid analogously converted to palmitoleic acid.\nAlso, stearic acid is less likely to be incorporated into cholesterol esters.\nIn epidemiologic and clinical studies, stearic acid was found to be associated with lowered LDL cholesterol in comparison with other saturated fatty acids.\n\n\n"}
{"id": "42490664", "url": "https://en.wikipedia.org/wiki?curid=42490664", "title": "Storm Data", "text": "Storm Data\n\nStorm Data and Unusual Weather Phenomena (SD) is a monthly NOAA publication with comprehensive listings and detailed summaries of severe weather occurrences in the United States. Included is information on tornadoes, high wind events, hail, lightning, floods and flash floods, tropical cyclones (hurricanes), ice storms, snow, extreme temperatures such as heat waves and cold waves, droughts, and wildfires. Photographs of weather and attendant damage are used as much as possible. Maps of significant weather are also included.\n\n\"Storm Data\" was started by the Weather Bureau, predecessor to the National Weather Service (NWS), in 1959. It is updated continuously on a monthly basis with a lag of a few months from the present. This delay is because the data is compiled and verified by local NWS offices and sent to the National Centers for Environmental Information (NCEI) which does further refinements and publishes \"Storm Data\" in reports covering the entire country. The local NWS offices initially gather the data, starting when a severe weather event unfolds and continuing until sufficient information is obtained. The initial data, considered preliminary, is sent in real-time to the Storm Prediction Center (SPC) which does limited quality control as new information becomes available and enters it into its (and its predecessor the National Severe Storms Forecast Center) Storm Events Database that begins in 1950. Original sources of the data include but are not limited to local law enforcement, local, state, and federal emergency management, storm spotter and storm chaser reports, the news media, insurance industry data, NWS damage surveys, and reports from the general public.\n\nSPC is interested in tornado, convective wind, and hail data. The tornado portion of the database, the National Tornado Database, is one of three authoritative tornado databases. Another is the DAPPL (short for Damage Area Per Path Length) database that was headed by Ted Fujita at the University of Chicago and concerns the period from 1916-1992. The most comprehensive historical database was compiled by Tom Grazulis of the Tornado Project and exhaustively covers known significant tornadoes for the period from 1680-1995. Both the Storms Event Database and \"Storm Data\" are official records. The database and the publication are from the same source but the database is more easily searchable. Delayed reports are added to both the database as well as the publication as new information becomes available in the \"Late Reports and Corrections\" section.\n\nUntil 2012 \"Storm Data\" was available to anyone but for a charge. Now it is a free publication downloadable from the NCEI website.\n\n\"Storm Data\" publishes chronological tabulations, narratives, and images for a calendar month, as well as updates to previous publications. The format has undergone various changes throughout publication history but consists of reports separated by state subdivided by regions within a state. Type of occurrence, location (including municipality and county as well as estimated latitude and longitude), date and time, magnitude of event (i.e. wind speed, Fujita scale rating, Saffir-Simpson Hurricane Scale rating, hail size, storm surge or river crest height, etc.), fatalities and injuries, monetary damages of property and agricultural crops, and descriptions are included. For tornadoes, Fujita scale or Enhanced Fujita scale rating is included as well as path length in miles and path width in yards. Average path width is listed from 1950-1994 and maximum path width is listed from 1995 to present. For fatalities, demographic information such as age and sex are gathered when possible as is the type of location (frame house, mobile home, apartment, outside, vehicle, church, school or other public building, etc.) and/or activity (boating, camping, playing sports, golfing, swimming, bathing, telephoning, construction work, etc.) at time of death.\n\nSignificant waterspouts, funnel cloud sightings, dense fog, dense smoke, dust storms, dust devils, debris flows (such as landslides), avalanches, tsunami and other surf and tide events, volcanic ash, as well as other extreme or unusual weather may also be listed. Annual summaries of selected event types are listed in the December issue for older years and in a separate issue for recent years.\n\n\n\n"}
{"id": "18487170", "url": "https://en.wikipedia.org/wiki?curid=18487170", "title": "Task 40", "text": "Task 40\n\nIEA Bioenergy Task 40: Sustainable International Bioenergy Trade, commonly abbreviated Task 40, was established under the International Energy Agency (IEA) Bioenergy Implementing Agreement in December 2003 with the aim of focusing on international bioenergy technology potential, barriers, and trade as well as its wider implications. Task 40 develops sustainable biomass markets, leading to a stable global commodity market in biomass energy.\n\nhttp://task40.ieabioenergy.com/\n\n"}
{"id": "8348984", "url": "https://en.wikipedia.org/wiki?curid=8348984", "title": "Volkswagen Golf Mk5", "text": "Volkswagen Golf Mk5\n\nThe Volkswagen Golf Mk5 (codenamed \"Typ 1K\") is a compact car, the fifth generation of the Volkswagen Golf and the successor to the Volkswagen Golf Mk4. Built on the Volkswagen Group A5 (PQ35) platform, it was unveiled at the Frankfurt Motor Show in October 2003 and went on sale in Europe one month later. A compact MPV version of the car was produced as the Golf Plus.\n\nThe Golf Mk5 was replaced in 2009 by the Volkswagen Golf Mk6.\n\nThe fifth generation had revised suspension changes and chassis tuning and increased cargo volume corresponding to a minor increase in size over the outgoing model. Its cargo volume is roughly more.\n\nIts replacement, the Mk6, was moved forward from the previously stated 2009 in Europe to the autumn of 2008, right after its official premiere at the Paris Motor Show in September 2008.\n\nOptions for engines and transmissions vary from country to country, but the Golf Mk5 is available with 4-cylinder, 5-cylinder, and 6-cylinder petrol engines, and a new Pumpe Duse unit injector Turbocharged Direct Injection (TDI) diesel engine. Transmission options include manual, automatic, Tiptronic, and Direct-Shift Gearbox (DSG).\n\nThe GTI comes with VW's 4-cylinder 2.0L Turbo Fuel Stratified Injection (FSI) which makes and torque. Transmissions include a 6-speed manual or 6-speed DSG.\n\nIn September 2005, the Golf Mk5 GT was announced, which featured a choice of either 1.4 L petrol engine in twincharger (TSI) configuration, or a 2.0 litre TDI. Both are available as versions; while the diesel also is available as a variant in the UK. The diesel has of torque, which is more than the range topping R32.\n\nThe new Twincharger (TSI) petrol engine uses Fuel Stratified Injection (FSI), along with a pair of chargers forcing the induction of the air. The chargers are a single supercharger that disengages after a specified rev-range, at which point charging of the air is handled by a single turbocharger. This system benefits from the pumping efficiency of the supercharger at lower revs and the fuel efficiency of the turbocharger at high revs. This results in more constant power delivery through the rev range, and better fuel efficiency. Both petrol and diesel versions are also available with DSG (Direct-Shift Gearbox). Performance figures for the petrol vehicle are 0- in 6.9s (6 speed) and 6.9s (DSG), with the diesel taking 8.2s, and both reaching top speed of .\n\nUnited States and Canada base specification Rabbits use the same 2.5L five-cylinder gasoline engine that powers the Jetta and New Beetle in these markets, making and in 2006-2007 models, and and from 2008 onward. North American transmission choices include a 5-speed manual or 6-speed automatic with Tiptronic for the Rabbit. Diesel engines have been unavailable on Rabbits, though they were offered through 2006 on the Jetta until tightening emissions regulations in the U.S. lead to their temporary unavailability.\n\nVolkswagen has no plans to sell the GT version in the US or Canada, though the VR6-powered R32 range-topping model was available.\n\nVolkswagen marketed the three-box sedan variant of the Mk5 Golf in 2004, as the Jetta in North America and Europe, replacing the \"Bora\" name of the previous Mk320 Golf saloon. The Jetta name was also introduced to Australia with the Mk5.\n\nAs with its predecessor the Mk5 Jetta features unique front wings, front doors and rear doors, so the only external panel shared with the Golf hatchback is the bonnet. As with all Golf-based saloons, the Jetta features a unique chrome grille, and its GLI variant has the Golf GTI's front end. Front lights were now shared with the Golf.\n\nIn the US market, the Jetta outsells the Golf by a ratio of 4 to 1.\n\nThere was no Cabriolet (convertible) version of the Golf Mk5, so the Volkswagen Eos coupé convertible (introduced in Spring 2006) was marketed as a separate model, and the New Beetle convertible makes a Golf Cabrio redundant. The Eos does not share body panels with any other Volkswagen model, although it is based on the A5 Golf/Jetta platform.\nThe fifth generation estate car/wagon debuted at the International Geneva Motor Show (8–18 March 2007) and was marketed as the Golf Variant in the German domestic market and in North America as the Jetta Sportwagen. It was facelifted in late 2009, with changes including the front clip and interior from the sixth generation Golf, remaining based on the Golf Mk V, and was marketed as the Golf Wagon and Variant in the Canadian and Mexican market.\nIn December 2004, Volkswagen announced the Golf Plus variant of the Golf Mk5. It is taller than the standard Golf, and shorter than the other compact MPV of the marque, the seven-seater Volkswagen Touran.\n\nAt the 2006 Paris Motor Show Volkswagen released the CrossGolf version, which is essentially a version of the Golf Plus with crossover-style body elements. It was developed by the Volkswagen Individual division, which also developed the Golf R32 and the Volkswagen CrossPolo. The CrossGolf is only available in front-wheel drive configuration (like the CrossPolo), and is powered by two petrol engines, 1.6 and 1.4 TSI, and two diesel engines, 1.9 TDI and 2.0 TDI, with outputs ranging from to . In the UK this model is badged as \"Golf Plus Dune\" and sold with the 1.9 TDI outputting .\n\nIn December 2008, the facelifted version was revealed at the Bologna Motor Show, featuring a revised front end, more similar to the Volkswagen Golf Mk6, but retaining a largely similar design of the rear end and the interior.\n\nThe Golf Mk5 GT features a choice of either 1.4 L petrol engine in twincharger (TSI) configuration, or a 2.0 litre TDI diesel engine.plus a 2.0 (fsi) direct injection petrol engine has 150 bhp. TSI petrol and diesels are available as versions. The 125kW diesel engine has 350 N·m (258 ft·lbf) of torque, which is more than the range topping R32. The petrol engined offering contains the new TSI engine, which is based on the recent Fuel Stratified Injection (FSI), but with a pair of chargers forcing the induction of the air. The chargers are a single supercharger that disengages after a specified rev-range, at which point charging of the air is handled by a single turbocharger. This system benefits from both of the efficiency of the supercharger in the lower rev ranges, with the longevity of the turbocharger higher in the rev range. This results in little turbo lag, constant power delivery along the rev range, and better fuel efficiency than similarly powered 2.4 L V6 engine due to its small size. However, the power delivery of the petrol TSI engine was criticised as being very jerky by Jeremy Clarkson.\n\nIn the UK the GT sport badge was marketed as offering both high power and low emissions, sparking some controversy. The diesel offers 156g/km and returns 47.9 mpg combined with the petrol equivalent offering 175g/km and 38.2 mpg respectively.\n\nBoth petrol and diesel versions are also available with Direct-Shift Gearbox (DSG). Performance figures for the petrol vehicle are 0- in 7.9 seconds (6-speed manual) and 7.7 seconds (DSG), with the diesel taking 8.2 seconds, and both reaching top speed of .\n\nThe Golf GT features the same brakes as the Golf GTI, with ventilated front discs, and solid rears. It has also lowered suspension, which lowers its centre of gravity, it borrows the GTI's suspension/damper settings, uses 7Jx17\" \"ClassiXs\" alloy wheels fitted with wide 225/45 R17 tyres, and has twin exhaust outlets.\n\nThe Golf GTI features a 2.0 litre turbocharged inline 4-cylinder petrol engine with Fuel Stratified Injection (FSI) direct-injection technology, which produces 200PS (147 kW/197 bhp). It is available in both 3-door and 5-door hatchback body shapes, and comes with a choice of either 6-speed manual or a 6-speed Direct-Shift Gearbox (DSG) which greatly reduces shift time to only 8ms.\n\nThe concept GTI was first shown to the public at the Frankfurt Motor Show in 2003. The first production model was initially unveiled at the Mondial de l'Automobile in Paris in September 2004, and went on sale around the world shortly thereafter. At the Los Angeles Auto Show in January 2006, the GTI made its long-awaited North American debut in 3-door guise (a 5-door variant was eventually made available), where it is marketed solely under the 'GTI' moniker, with no reference to the Rabbit. The new GTI has a considerable price increase over the previous model, mainly due to the features mentioned above, and the fact that the exterior itself had not seen such a dramatic design change in years. The price is further raised because it is built in Germany, unlike the Mk4 some of which were built in Brazil. The innovative DSG transmission and the TFSI engine all helped raise the retail price of the car. The Mk5 GTI was named 2007 Automobile of the Year by \"Automobile Magazine\", in December 2006.\n\nThis generation marked the only generation in Canada to have the GTI as a separate nameplate rather than a trim of the Golf. When Volkswagen announced the revival of the Golf in the United States & Canada for the 2010 model year, Volkswagen reverted the GTI nameplate as a Golf trim, although the GTI remains a separate nameplate in the United States.\n\nIn late September 2007, the Mk5 R32 went on sale in Europe. It features an updated 3.2-litre VR6 engine of that fitted to the previous Mk4 version, with an extra due to a reworked inlet manifold. Maximum power is now at 6,300 rpm; torque is unchanged at . It reaches an electronically governed top speed of . Going from 0 to will take 6.5 s, reduced to 6.2 s with the Direct-Shift Gearbox.\n\nCompared with the previous Mk4 R32, it is 0.1 seconds faster for the manual version, while the newer R32 is about heavier. As with the previous R32; there is the Haldex Traction-based 4motion part-time four-wheel drive, now through 18\" Zolder 20-spoke alloy wheels. Stopping the R32 comes in the form of blue-painted brake calipers with discs at the front and disks at the rear.\n\nThe Mk5 R32 was released in the US in August 2007.\n\nFollowing Volkswagen's successful 20th anniversary edition GTI (1996 in Europe, and 2003 for the North American market), and the 25th anniversary GTI (in 2001 for Europe only) models, Volkswagen marked the GTI's 30th anniversary by producing the GTI Edition 30.\n\nGoing on sale in November 2006 from £22,295 RRP, with an initial goal of a limited production run of only 1500 (Europe models), the Edition 30 was available in 6 colours; Tornado Red, Black, Candy White, Reflex Silver (Metallic), Steel Grey (Metallic) and finally Diamond Black (Pearl). Due to strong demand, 2280 cars were eventually built with a small continuing into the 2009 model year. The changes over the standard production model included a modified engine that produced an extra more than the standard version, raising the output to , giving rumour that it was faster in the dry and more powerful than the R32. Slight changes to the body work included body coloured side skirts and Votex front spoiler, colour-keyed rear bumper and tinted rear lights from the R32. Changes to the interior included a return for the golf ball shaped gear knob and silver \"Edition 30\" logo'd sill plates. Edition 30 seats were also decked out in the distinctive red stitching on 'Vienna' leather and 'Interlagos' fabric mix. Red stitching was also added to the leather-covered steering wheel. Finally, dependent on the market and the options available the Edition 30 was available with 18\" BBS originated 'Pescara' alloy wheels, or black versions of the 18\" 'Monza II' alloy wheels.\n\nPerformance was marginally improved: with 0- coming at 6.8 seconds (6.6 seconds for DSG-equipped models), and a top speed of (manual) or (DSG).\n\nIn October 2006, Volkswagen debuted a new Fahrenheit Edition of their GTI and GLI models at the Playboy Mansion. These new models were the first special-edition versions of the GTI and GLI made available in North America, and the first of the new models arrived in dealers in the early March 2007.\n\nFahrenheit models of the GTI were distinguished by their Magma Orange paint job, special Fahrenheit badging, a commemorative plate placed on the steering wheel, body-coloured interior panels, orange stitching on the DSG boot, steering wheel, park brake handle and floor mats (from which the red GTI logo had been removed), as well as special gunmetal-colored 18\" \"Charleston\" wheels. The Fahrenheit also came with a European tuned suspension.\n\nThe Fahrenheit GTI was available with Volkswagen's DSG transmission or 6-speed manual. Only 1200 Fahrenheit GTI models in Magma Orange were produced for the US (150 in Canada) and 1200 GLIs in yellow (not available in Canada)(600 DSG 600 manual). US pricing started at $27,665.\n\nThe Golf Speed study, with its Lamborghini finish colors and many accessories has been causing a stir for months. It comes from young Volkswagen apprentices, who skillfully refined the Golf and with that not only convinced their supervisors. The response from drivers, that have seen the \"orange speed\" and \"yellow speed\" colored Golf-studies at trade shows, in the Car city and in the Potsdamer Platz in Berlin, was so overwhelming, that a limited edition of one hundred Golf yellow speed and Golf orange speed each is being manufactured.\n\nMulticolor plays a main role: exclusive colors from our subsidiary Lamborghini (\"giallo midas\" and \"arancio borealis\") ensure that the Golf yellow speed and Golf orange speed can be noticed as special members of the Golf family, even from far away. Four-color coats, applied by hand, ensure a one of a kind multi-faceted brilliance, which is particularly emphasized in the sunlight and from different angles.\n\nAdditionally, there are almost endless details: just like the Golf R32 the Golf Speed has an exhaust system with a double end pipe in the middle. In the front the shining chrome grille and chrome outside mirrors also stand out. The dynamic overall impression is underlined even more through the rear spoiler and the broadened body colored sillboards as well as the 18\" alloy wheels.\n\nThe side and rear windows are tinted. The later is additionally cleaned, so for optical reasons the rear windshield wiper with spray nozzle was omitted. Just like with the R32, the rear lights are darker than in the series and stand out in a cherry red from the particular paint color.\n\nThe interior has a modified top sport seat system like the GTI, but here with seat covers made of leather and alcantara. The exquisite leather decor is furnished with coordinated decor stitching, which guarantees an even more exclusive impression. Also taken from the GTI is the sport steering wheel in leather. Next to the decor stitching, the steering wheel clasp is a special eye-catcher, in the respective color the name yellow speed or orange speed as well as the exclusivity guaranteeing production number is engraved.\n\nThe black roof liner as well as the aluminum application in the instrument cluster, the footrest and the pedals round off the sporty-exquisite impression of the interior. Furthermore on the technical side there are a sport chassis lowered by ca. 15 mm, fog lights and heatable front seats that come standard.\n\nThe Golf Speed is either powered by a 2,0-FSI-Aggregate FSI with 110 kW / 150 hp or the 2,0 TDI with 103 kW / 140 hp with 6-gear transmission. The turbo-diesel direct-fuel-injection can be ordered optional with the sporty double clutch system DSG.\n\nVolkswagen offers a specialty for pick-up customers. You can see how the Golf Speed is made by hand. In the plant the clients are handed the steering wheel clasp with the corresponding number between 1 and 100. The engraved clasp is installed upon its delivery in the Autostadt, so when a Golf Speed receives its individual identification it is witnessed by its owner.\n\nThis is a special edition GTI, developed by Volkswagen Individual. It was given the 230PS (169 kW/226 hp) Edition 30 engine, instead of the 200PS (147 kW/197 hp) in the standard Mk5 GTI. It is equipped with 225/40R18 Pirelli P-Zero tyres on titanium colored alloy wheels. It is available in 6-speed manual or an optional DSG gearbox.\n\nIt features leather sport seats in \"San Remo” microfiber with embossed Pirelli tyre tread pattern down the centre. It also has yellow stitching on the seats, steering wheel and gear shift. There is also a Pirelli logo on the head restraints. The exterior and valences are painted sunflower yellow. Other colours are also available.\n\nVolkswagen unveiled the GTI W12-650 at the GTI Festival in Wörthersee, Austria, in May 2007. It was designed as a concept car, and only one is known to exist. Unlike most concept cars, it is mechanically functional to the extent that it can be driven. Due to the rushed build time of the car (8 weeks), however, not all of its features function fully. The steering-wheel mounted paddle-shifters are not linked to the transmission, the hazard lights do not function, the stereo system doesn't work, and the heating and air-conditioning system of the car does not function due to the dashboard controls never being linked to the unit.\n\nThe car features a 6.0L W12 bi-turbo engine from the Bentley Continental GT delivering , of torque, 0- in 3.7 seconds, and a top speed of . The W12 differs from the standard GTI in several ways. It features 19-inch wheels that resembled the GTI's. It is lower and wider, the rear seats have been removed to accommodate the mid-engine design, and the roof is made from carbon-fibre composite, front brakes are from the Audi RS4, and rear brakes and axle are from a Lamborghini Gallardo. The W12-650 achieved a time of 1:29.6 on BBC Top Gear’s \"Power Lap\" feature. Jeremy Clarkson showed that the car had some trouble with high-speed cornering but was extremely fast in the straight sections of the track.\n\nA VW Golf TDI Hybrid concept was shown at the March 2008 Geneva motor show. The concept vehicle shown had a three-cylinder TDI engine - probably the 1.4 litre used in the Volkswagen Polo BlueMotion - mated to a electric motor, and a seven-speed double-clutch DSG transmission. The electric power system is a Nickel-metal hydride battery in the boot, and a regenerative braking system. An \"energy monitor\" display on the dashboard keeps tabs on what the powertrain is doing, and provides both a stop/start capability and a full-electric mode at low speed. The design also includes concepts introduced via BlueMotion, with smaller grill and thinner low-resistance tyres. According to Germany's Auto Bild, the car will get 69.9 mpg, and emit 90 g/km of carbon dioxide, less than the 104 g/km emitted by the Toyota Prius and 116 emitted by the Honda Civic Hybrid. The TDI Hybrid was expected to be marketed in Europe from mid-2009.\n\nVW CEO Martin Winterkorn announced Volkswagen Golf Twin Drive plug-in hybrid vehicle based on Mark V Golf, which uses 2.0L turbodiesel and electric motor with lithium-ion batteries. The car can run about 50 kilometres on battery power. The combined power is .\n\nVolkswagen never developed the Twin Drive system with 8 German partners and is planning a trial fleet of 20 Golfs outfitted with the system in 2010.\n\nThe production version was expected to be based on Mark VI Golf featuring a 1.5L turbodiesel engine and electric motor, with estimated arrival date of 2015.\n\nIn 2007, VW built a concept car based on the Golf Mk5 featuring a 6.0-litre W12 engine from a Bentley Continental GT with a Volkswagen Phaeton gearbox. The engine was twin-turbocharged and tuned to provide an extra , giving . Volkswagen claimed that the car could accelerate from 0–100 km/h (0-62 mph) in 3.7 seconds. The Golf GTI W12 was rear wheel drive, using a six-speed automatic gearbox.\nThe body was extensively modified to carry the engine, widened and given a rear spoiler to improve handling.\nThe W12 engine was placed in the middle of the car to improve the car's grip.\n\nThe engines used are the same as for many other Volkswagen Group cars:\nIn 2004, the Mk5 received a 5-star Euro NCAP rating. The 2010 edition of Monash University's \"Used Car Safety Ratings\", found that the Golf Mk5 provides an \"excellent\" (five out of five stars) level of occupant safety protection in the event of an accident.\n\nIn auto racing, APR Motorsport has led two MKV VW GTI's to victory in the Grand-Am KONI Sports Car Challenge and Continental Tire Sports Car Challenge Street Tuner (ST) class.\n\n\n\n\n"}
{"id": "5149305", "url": "https://en.wikipedia.org/wiki?curid=5149305", "title": "Waste exchange", "text": "Waste exchange\n\nWaste exchange is where the waste product of one process becomes the raw materials for a second process. This is similar to using pre-consumer recycling material in a product. This represents a way of reducing waste disposal through reuse for that which cannot be eliminated. In this way waste exchange practices are high on the waste hierarchy.\n\nThere are free online services for businesses and other organisations that help to keep reusable items in circulation and out of landfill.\n\n"}
{"id": "39597430", "url": "https://en.wikipedia.org/wiki?curid=39597430", "title": "Wave spring", "text": "Wave spring\n\nA wave spring, also known as \"coiled wave spring\" or \"scrowave spring\", is a spring made up of pre-hardened flat wire in a process called \"on-edge coiling\" (also known as edge-winding). During this process, waves are added to give it a spring effect. The number of turns and waves can be easily adjusted to accommodate stronger force or meet specific requirements.\n\nA wave spring has advantages over a traditional coiled spring or a washer:\n\nMultiple types of wave spring are available: Single-turn wave springs include gap single-turn and overlap single-turn type. Multi-turn wave spring types, include shim-end and plain-end types. The nested wave spring incorporates smaller waves within larger ones.\nSingle-turns are best for applications with short deflection and low to medium forces. The number of waves and material thickness can be changed to accommodate stronger forces. It is used for bearing pre-load.\n\nA multi-turn wave spring can decrease the needed axial space. It is suited for applications with large deflection and a small spring rate. A wide range of forces can be accommodated.\n\nEliminates the need to stack springs to accommodate higher loads. It produces high force while maintaining the precision of a circular-grain wave spring. It replaces a stack of belleville washers where a high but accurate force is needed.\n\n\n\n"}
{"id": "15220286", "url": "https://en.wikipedia.org/wiki?curid=15220286", "title": "Yttria-stabilized zirconia", "text": "Yttria-stabilized zirconia\n\nYttria-stabilized zirconia (YSZ) is a ceramic in which the cubic crystal structure of zirconium dioxide is made stable at room temperature by an addition of yttrium oxide. These oxides are commonly called \"zirconia\" (ZrO) and \"yttria\" (YO), hence the name.\n\nPure zirconium dioxide undergoes a phase transformation from monoclinic (stable at room temperature) to tetragonal (at about 1173 °C) and then to cubic (at about 2370 °C), according to the scheme:\n\nmonoclinic (1173 °C) formula_1 tetragonal (2370 °C) formula_1 cubic (2690 °C) formula_1 melt\n\nObtaining stable sintered zirconia ceramic products is difficult because of the large volume change accompanying the transition from tetragonal to monoclinic (about 5%). Stabilization of the cubic polymorph of zirconia over wider range of temperatures is accomplished by substitution of some of the Zr ions (ionic radius of 0.82 Å, too small for ideal lattice of fluorite characteristic for the tetragonal zirconia) in the crystal lattice with slightly larger ions, e.g., those of Y (ionic radius of 0.96 Å). The resulting doped zirconia materials are termed \"stabilized zirconias\".\n\nMaterials related to YSZ include calcia-, magnesia-, ceria- or alumina-stabilized zirconias, or partially stabilized zirconias (PSZ). Hafnia stabilized Zirconia is also known.\n\nAlthough 8-9 mol% YSZ is known to not be completely stabilized in the pure cubic YSZ phase up to temperatures above 1000 °C (Ref. and publications therein), commonly used abbreviations in conjunction with yttria-stabilized zirconia are:\n\nThe thermal expansion coefficients depends on the modification of zirconia as follows:\n\nBy the addition of yttria to pure zirconia (e.g., fully stabilized YSZ) Y ions replace Zr on the cationic sublattice. Thereby, oxygen vacancies are generated due to charge neutrality:\n\nformula_4 with formula_5,\n\nmeaning two Y ions generate one vacancy on the anionic sublattice. This facilitates moderate conductivity of yttrium stabilized zirconia for O ions (and thus electrical conductivity) at elevated and high temperature. This ability to conduct O ions makes yttria-stabilized zirconia well suited for application as solid electrolye in solid oxide fuel cells.\n\nFor low dopant concentrations, the ionic conductivity of the stabilized zirconias increases with increasing YO content. It has a maximum around 8-9 mol% almost independent of the temperature (800-1200 °C). Unfortunately, 8-9 mol% YSZ (8YSZ, 8YDZ) also turned out to be situated in the 2-phase field (c+t) of the YSZ phase diagram at these temperatures, which causes the material's decomposition into Y-enriched and depleted regions on the nm-scale and, consequently, the electrical degradation during operation. The microstructural and chemical changes on the nm-scale are accompanied by the drastic decrease of the oxygen-ion conductivity of 8YSZ (degradation of 8YSZ) of about 40% at 950 °C within 2500 hrs. Traces of impurities like Ni, dissolved in the 8YSZ, e.g., due to fuel-cell fabrication, can have a severe impact on the decomposition rate (acceleration of inherent decomposition of the 8YSZ by orders of magnitude) such that the degradation of conductivity even becomes problematic at low operation temperatures in the range of 500-700 °C.\n\nNowadays, more complex ceramics like co-doped Zirconia (e.g., with Scandia, ...) are in use as solid electrolytes.\n\nYSZ has a number of applications:\n\n"}
