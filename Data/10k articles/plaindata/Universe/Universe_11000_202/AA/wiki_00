{"id": "10745656", "url": "https://en.wikipedia.org/wiki?curid=10745656", "title": "Adamantine (veneer)", "text": "Adamantine (veneer)\n\nAdamantine is a veneer developed by The Celluloid Manufacturing Company of New York City, covered by U.S. Patent number 232,037, dated September 7, 1880. Seth Thomas Clock Company purchased the right to use the adamantine veneer in 1880. This veneer is sometimes referred to as celluloid and is found on clocks in a wide variety of colors that simulate marble or alabaster.\n\nSeth Thomas clocks used Adamantine to create a Marbaline veneer for their clocks.\n"}
{"id": "15844940", "url": "https://en.wikipedia.org/wiki?curid=15844940", "title": "African Energy Commission", "text": "African Energy Commission\n\nThe African Energy Commission (Afrec) is a continental African structure with the responsibility to ensure, co-ordinate and harmonise the protection, preservation, development and the national exploitation, marketing and integration of the energy resources of the African continent.\n\nIt was launched on the weekend of 11 July 2001 as the African Union's (AU) answer to the Organisation of Petroleum Exporting Countries (OPEC).\n\nThe commission was set up with the intention of coordinating policy for the energy-rich continent, and was launched after a three-day meeting of AU energy ministers in Algiers, Algeria.\n\nAfrec will be the official energy arm of the AU, and includes such African nations such as Algeria, Angola, Libya and Nigeria among its founders, nations who are also members of OPEC.\n\nAccording to Chakib Khelil, Algerian energy minister and OPEC President, \"Afrec is a framework of co-operation and co-ordination, but it is also a framework of action for Africans in the energy sector.\" \n\n"}
{"id": "14553266", "url": "https://en.wikipedia.org/wiki?curid=14553266", "title": "Allotropes of oxygen", "text": "Allotropes of oxygen\n\nThere are several known allotropes of oxygen. The most familiar is molecular oxygen (O), present at significant levels in Earth's atmosphere and also known as dioxygen or triplet oxygen. Another is the highly reactive ozone (O). Others include:\n\nAtomic oxygen, denoted O(P) or O(3P), is very reactive, as the single atoms of oxygen tend to quickly bond with nearby molecules. On Earth's surface, it does not exist naturally for very long, but in outer space, the presence of plenty of ultraviolet radiation results in a low Earth orbit atmosphere in which 96% of the oxygen occurs in atomic form.\n\nAtomic oxygen has been detected at planet Mars by Mariner, Viking, and the SOFIA observatory.\n\nThe common allotrope of elemental oxygen on Earth, , is generally known as oxygen, but may be called \"dioxygen\", \"diatomic oxygen\", \"molecular oxygen\", or \"oxygen gas\" to distinguish it from the element itself. As a major component (about 21% by volume) of Earth's atmosphere, elemental oxygen is most commonly encountered in this form. Aerobic organisms utilize atmospheric dioxygen as the terminal oxidant in cellular respiration. The ground state of dioxygen is known as triplet oxygen, O, because it has two unpaired electrons. The first excited state, singlet oxygen, \"\"O has no unpaired electrons and is metastable.\n\nThe ground state of has a bond length of 121 pm and a bond energy of 498 kJ/mol. It is a colourless gas with a boiling point of . It can be condensed from air by cooling with liquid nitrogen, which has a boiling point of . Liquid oxygen is pale blue in colour, and is quite markedly paramagnetic due to the unpaired electrons; liquid oxygen contained in a flask suspended by a string is attracted to a magnet.\n\nSinglet oxygen is the common name used for the two metastable states of molecular oxygen (O) with higher energy than the ground state triplet oxygen. Because of the differences in their electron shells, singlet oxygen has different chemical properties than triplet oxygen, including absorbing and emitting light at different wavelengths. It can be generated in a photosensitized process by energy transfer from dye molecules such as rose bengal, methylene blue or porphyrins, or by chemical processes such as spontaneous decomposition of hydrogen trioxide in water or the reaction of hydrogen peroxide with hypochlorite.\n\nTriatomic oxygen (ozone, O), is a very reactive allotrope of oxygen that is destructive to materials like rubber and fabrics and is also damaging to lung tissue. Traces of it can be detected as a sharp, chlorine-like smell, coming from electric motors, laser printers, and photocopiers. It was named \"ozone\" by Christian Friedrich Schönbein, in 1840, from the Greek word ὠζώ (ozo) for smell.\n\nOzone is thermodynamically unstable toward the more common dioxygen form, and is formed by reaction of O with atomic oxygen produced by splitting of O by UV radiation in the upper atmosphere. Ozone absorbs strongly in the ultraviolet and functions as a shield for the biosphere against the mutagenic and other damaging effects of solar UV radiation (see ozone layer). Ozone is formed near the Earth's surface by the photochemical disintegration of nitrogen dioxide from the exhaust of automobiles. Ground-level ozone is an air pollutant that is especially harmful for senior citizens, children, and people with heart and lung conditions such as emphysema, bronchitis, and asthma. The immune system produces ozone as an antimicrobial (see below). Liquid and solid O have a deeper blue color than ordinary oxygen and they are unstable and explosive.\n\nOzone is a pale blue gas condensable to a dark blue liquid. It is formed whenever air is subjected to an electrical discharge, and has the characteristic pungent odour of new-mown hay, or for those living in urban environments, of subways – the so-called 'electrical odour'.\n\nTetraoxygen had been suspected to exist since the early 1900s, when it was known as oxozone. It was identified in 2001 by a team led by Fulvio Cacace at the University of Rome. The molecule was thought to be in one of the phases of solid oxygen later identified as . Cacace's team suggested that probably consists of two dumbbell-like molecules loosely held together by induced dipole dispersion forces.\n\nThere are six known distinct phases of solid oxygen. One of them is a dark-red cluster. When oxygen is subjected to a pressure of 96 GPa, it becomes metallic, in a similar manner as hydrogen, and becomes more similar to the heavier chalcogens, such as tellurium and polonium, both of which show significant metallic character. At very low temperatures, this phase also becomes superconducting.\n\n"}
{"id": "4624848", "url": "https://en.wikipedia.org/wiki?curid=4624848", "title": "Asymmetric price transmission", "text": "Asymmetric price transmission\n\nAsymmetric price transmission (sometimes abbreviated as APT and informally called \"rockets and feathers\" ) refers to pricing phenomenon occurring when downstream prices react in a different manner to upstream price changes, depending on the characteristics of upstream prices or changes in those prices.\n\nThe simplest example is when prices of ready products increase promptly whenever prices of inputs increase, but take time to decrease after input price decreases.\n\nIn business terms, \"price transmission\" means the process in which upstream prices affect downstream prices. \"Upstream prices\" should be thought of in terms of main inputs prices (for processing / manufacturing, etc.) or prices quoted on higher market levels (e.g. wholesale markets). Accordingly, \"downstream prices\" should be thought of in terms of output prices (for processing / manufacturing, etc.) or prices quoted on lower market levels (e.g. retail markets).\n\nSince (by definition) upstream and downstream prices are related:\n\nPrice transmission is best illustrated by an example. Assume that:\n\nGiven the above, one might expect that:\nSuch behaviour, predicted by all canonical industry / market pricing models (perfect competition, monopoly) is called Symmetric Price Transmission. \nIn contrast to Symmetric price transmission, Asymmetric Price Transmission is said to exist when the adjustment of prices is not homogeneous with respect to characteristics external or internal to the system. As an example of Asymmetric Price Transmission consider a situation when:\n\nOne should remember that the size asymmetry cannot occur on its own. If that had been the case the upstream prices and downstream prices would drift apart. Since downstream prices and upstream prices are by definition related to each other, this cannot be the case. Accordingly, size asymmetry can occur only together with time asymmetry and only when the long-run relationship between prices is restored after the impulse shock to upstream prices.\nThe issue of Asymmetric Price Transmission received a considerable attention in economic literature because of two reasons. \n\nFirstly, its presence is not in line with predictions of the canonical economic theory (e.g. perfect competition and monopoly), which expect that under some regularity assumptions (such as non-kinked, convex/concave demand function) downstream responses to upstream changes should be symmetric in terms of absolute size and timing.\nSecondly, because of the size of the some markets in which Asymmetric Price Transmission takes place (such as petroleum markets), global dependence on some products (again oil) and the share of income spent by average household on some products (again petroleum products), Asymmetric Price Transmission is important from the welfare point of view. One must remember that APT implies a welfare redistribution from agents downstream to agents upstream (presumably consumers to large energy companies); it has serious political and social consequences.\n\n"}
{"id": "6608403", "url": "https://en.wikipedia.org/wiki?curid=6608403", "title": "Australian HPV Super Series", "text": "Australian HPV Super Series\n\nThe Australian HPV Super Series is an annual championship held in South Australia and Western Australia featuring velomobiles racing around enclosed circuits for a period between 6 and 24 hours.\n\nThe largest event of its kind anywhere in the world, it attracts teams from all around Australia, and even overseas. As of 2018, the championship consists of six races taking place at Mount Gambier, Loxton, Adelaide twice, Busselton, and Murray Bridge respectively.\n\nFrom the 2016 and 2017 seasons, in all races apart from Murray Bridge, race days were separated for Category 4 (Saturday) and the remaining categories (Sunday). This was scaled back to only include the two Victoria Park races from the 2018 season onward.\n\nRaces at Loxton, Adelaide, and Murray Bridge are all live streamed on the AHPVSS Facebook and YouTube pages.\n\nIn 1985, what would become the inaugural Pedal Prix race was held in the car park of what was at the time the Underdale Campus of The University of South Australia on Holbrooks Road. There were less than a dozen participating teams. This event marks the start of the Australian HPV Super Series and at the time it generated tremendous interest. Vehicles varied considerably in sophistication and quality but the potential to get students involved in designing, making and testing the vehicles was readily apparent.\n\nIn 1986 the event was moved to the Road Safety Centre on Oaklands Road in Marion (currently being turned into a wetlands) to cater for the increased number of teams. Rules and standards were developed to guide teams in building vehicles so that they were safer. This site was considerably more complex with many corners and a hill to test riders and their vehicles.\n\nThe popularity of the event continued to grow and it soon became apparent that the number of entries was growing beyond the capacity of the Road Safety Centre. As a result, in 1992 the event was moved to the Adelaide International Raceway at Virginia, SA. This site easily catered for the increased number of entries. The wider, flatter track saw records for the distance travelled in the 24 hour endurance race increase. A major disadvantage of this site has been its openness and exposure to weather. Wind, dust and an uninteresting track layout had the committee looking for alternatives. 1996 marked the last time that petrol driven hybrids were allowed to participate. At various stages throughout the history of the 24 hour race there have been categories for petrol and solar hybrids as well as a commuter category where more than one rider was in the vehicle.\n\n1997 saw the event moved to Sturt Reserve, Murray Bridge, where for the first time public roads were sealed off specially for the event. It was felt that the new venue would comfortably accommodate the number of entries anticipated, provide a greater challenge for teams and provide a better atmosphere for all competitors, spectators and visitors. The field at the first Murray Bridge totalled 90 teams. The record size for the competing field at Murray Bridge was 228 set in 2009.\n\nIn 2003, The HPV Super Series began, with a championship season that spanned 4 races. They included two 3 hour sprints on the same day and later a 6-hour race all at Victoria Park and then concluded with the 24 hour Murray Bridge event. The two 3 hour sprints were then replaced with a single 6 hour race. A 9-hour race was trialed once during 2009 for round 2 at Victoria Park. The current series format is four 6 hour races and a single 24 hour final race.\n\nThe 2013 Murray Bridge event was the first to include teams representing five states/territories with teams from South Australia, Victoria, Western Australia, New South Wales and the Northern Territory present.\n\nIn 2014, a street circuit in Loxton, South Australia was introduced as the new opening race of a four-round championship. In addition, the McNamara Park Circuit near Mount Gambier, South Australia and a street circuit in Busselton, Western Australia were introduced as non-series events. The Murray Bridge street circuit remained as the final series round and the two Victoria Park rounds also remained.\n\nIn 2017, the McNamara Park Circuit was upgraded to a series event as the opening race of the series. In 2018, the race in Busseltion was also upgraded to a series event as the 5th round.\n\nOn December 14th 2017, it was announced by the Australian International Pedal Prix and the Victorian HPV Grand Prix Series that National Vehicle Specifications had been adopted for the 2018 season. These are specifications that competitors' vehicles must abide by in order to pass scrutineering and therefore race in any Australian HPV Super Series or Victorian HPV Series event. Before these national specifications were put into effect, teams who were competing across different series had two different specification sets to comply with. With the new system, there is no need to change anything with the vehicle in order to meet specifications for both series. This is one of a number of steps being taken to eventually set up a national competition.\n\n\nThe four categories are divided under two classifications of \"School Categories\" and \"Community Categories\". Introduced in 2006, all school and community categories have further sub-categories for All Female teams.\n\nThe following category criteria is accurate as of the 2018 season.\n\nFrom 2014 season each teams best two rounds from the 6 hour races are added to their result from the 24 hour race to determine their Series Championship total. From 2016, points are allocated by category result instead of overall result.\n\nThis race, currently held in April is eight hours long and is held at McNamara Park, just outside of Mount Gambier, South Australia, on a 2.4 km closed circuit. The track made its debut in 2014 as an 8-hour non-series race before being integrated into the main series in 2017.\n\nThis race, currently held in May, is six hours long and take place at Loxton, South Australia on a 1.37 km street circuit that incorporates Loxton's large center roundabout. The track made its debut for the 2014 season as a six-hour race. This track heavily favors lighter bikes due to the climb on the southern side of the track.\n\nThis race, currently held in June and July respectively, are six hours long and take place at Victoria Park, Adelaide on a closed-criterium track which is 1.354 km long.\nThe fastest teams can achieve distances of over 250 km during these races.\nThe Victoria Park track is now in its third incarnation. Prior to 2009 the track included an uphill segment of Wakefield Rd.\nFrom 2009 to 2012 the track used the southern hairpin and start line of the Adelaide 500 track and a new section running parallel to Wakefield St.\nLate in 2012 it was confirmed by the Adelaide City Council that the redevelopment of Victoria Park had been given the green light. The redevelopment now includes an extension to the short track to take it past the heritage grandstand present on the site. This track heavily favors the most aerodynamic bikes due to the lack of slow corners.\n\nIn 2014, a new race on an 850 m street circuit in Busselton, Western Australia was announced. It is currently the only event in the AHPVSS outside of South Australia. It was initially a non-series round used to promote HPV racing in Western Australia. This was upgraded into a series event in 2018. The track is located mainly in a carpark on the foreshore of Busselton. It is tight and twisty with three hairpins. Since 2014 it has been a 6 hour race, but the Busselton City Council is in talks with the AIPP about the possibility of holding a 24 hour event.\n\nThe Australian HPV Super Series concludes in September with what is considered to be the premier HPV event in the country, the 24-hour, final race at Sturt Reserve in Murray Bridge. The event now attracts over 30,000 spectators and participants annually, becoming an economic boom for the town. It attracts the best teams from all over the country to what is considered the toughest and most competitive race. The closed-circuit track is, as of 2016, 1.7 km long containing a series of left and right hand corners, fast straights and challenging corners. When flooded with over 200 teams it makes for genuinely tricky and exciting racing, The elite teams may cover over during the race. The race starts at 12:00 pm on Saturday and concludes 24 hours later. (note: In 2007 the race was stopped early due to gale-force winds, and in 2017 the race started 4 hours later than the scheduled start also due to strong winds).\n\nDuring the first race at Murray Bridge in 1997, the track followed the roads that bordered Sturt Reserve in a closed circuit. In 2000, the Northern Hairpin along Olympic Drive and Janesh Road was added, lengthening the track by 366 metres. In 2004 the corner leading up to the main straight was transformed to its current shape (shortening the track by 40m). This corner has had various sponsors names associated with it, and is currently called 'Belotti Corner,' but it is known popularly among riders and spectators alike as \"Crash Corner\". The Southern Straight was resurfaced in 2011, removing the roughest section of the track. In 2016, the track was shortened to its current form, by-passing the Northern Hairpin due to the mills on that section of Janesh Road having to remain open during the event.\n\nThis track requires bikes to have good aerodynamics, minimal weight and decent handling.\n\nIn 2005, a timed Friday night practice session was introduced. This session is used to determine the grid positions for the start of the race the following day. This also included the introduction of a Top 12 Shootout where the three fastest teams from each category were given the opportunity to set a lap time on a clear track on Saturday morning before the start of the race to determine the top 12 positions on the grid.\n\nIn 2008, the shootout was expanded to a Top 15 shootout to include the fastest all female team from categories 1, 2 and 3.\nThe shootout has quickly become a crowd favourite since its introduction. Large crowds gather around the track to cheer on the fastest teams as they are given the opportunity to push their trikes to the limit on a clear track. Top teams exceed 70 km/h on the main straight during their flying lap.\n\nSpecial Note \"Bendigo Youth Racings victory over Team Ballistic was the closest in the events history with only 10 seconds separating them after 24 hours.\"\n\nSpecial Note \"The 2007 race was only 23 hours long as it had to be cut short by an hour due to gale force winds.\"\n\nSpecial Note \"Due to the mill now operating during the race, a different track was used, removing the hairpin at the north of the course.\"\n\n\n"}
{"id": "29970746", "url": "https://en.wikipedia.org/wiki?curid=29970746", "title": "Aușeu-Borod Wind Farm", "text": "Aușeu-Borod Wind Farm\n\nThe Auşeu-Borod Corugea Wind Farm is a proposed wind power project in Bihor County, Romania. It will have 26 individual wind turbines with a nominal output of around 2.5 MW each, which will deliver up to 65 MW of power, enough to power over 25,700 homes, with a capital investment required of approximately €85 million.\n"}
{"id": "4704921", "url": "https://en.wikipedia.org/wiki?curid=4704921", "title": "Bagnold number", "text": "Bagnold number\n\nThe Bagnold number (Ba) is the ratio of grain collision stresses to viscous fluid stresses in a granular flow with interstitial Newtonian fluid, first identified by Ralph Alger Bagnold. \n\nThe Bagnold number is defined by \n\nwhere formula_2 is the particle density, formula_3 is the grain diameter, formula_4 is the shear rate and formula_5 is the dynamic viscosity of the interstitial fluid. The parameter formula_6 is known as the linear concentration, and is given by\n\nwhere formula_8 is the solids fraction and formula_9 is the maximum possible concentration (see random close packing).\n\nIn flows with small Bagnold numbers (Ba < 40), viscous fluid stresses dominate grain collision stresses, and the flow is said to be in the 'macro-viscous' regime. Grain collision stresses dominate at large Bagnold number (Ba > 450), which is known as the 'grain-inertia' regime. A transitional regime falls between these two values.\n\n\n"}
{"id": "3865400", "url": "https://en.wikipedia.org/wiki?curid=3865400", "title": "Biscayne Aquifer", "text": "Biscayne Aquifer\n\nThe Biscayne Aquifer, named after Biscayne Bay, is a surficial aquifer. It is a shallow layer of highly permeable limestone under a portion of South Florida. The area it underlies includes Broward County, Miami-Dade County, Monroe County, and Palm Beach County, a total of about .\n\nThe water-absorbing layers of rock underlying south Florida divide into three layers. The Biscayne Aquifer is closest to the surface and because of this it directly interacts with natural and man-made bodies of surface water, such as streams, lakes, canals and reservoirs. The ground water and the aquifer currently are managed as an integrated water system.\n\nBecause the top part of the Biscayne aquifer is the water table, this aquifer is known as an unconfined aquifer. Since it merges with the floor of Biscayne Bay and with the Atlantic Ocean, it is also a coastal aquifer. Both of these factors contribute to its potential contamination. Lowered water tables, primarily from over-pumping, could allow salt water intrusion without man-made interventions such as dam-like structures that control fresh and salt water levels in canals. Because the aquifer is so close to the surface, it is extremely vulnerable to surface contaminants.\n\nThe South Florida Water Management District controls an extensive system of canals and other control systems and pumping stations along with the Biscayne Aquifer, Lake Okeechobee and three other large water conservation areas as it monitors and controls the storage and release of the water in the district. It must take into account the danger of salt water intrusion and monitor water demand while it manages surplus flood water and maintains water table levels and adequate water supplies.\n\nThe Biscayne Aquifer supplies South Florida metropolitan area with its primary source of fresh water. This area includes most of south Florida (Miami-Dade, Monroe, and parts of Broward Counties) as well was other urban areas stretching from Homestead, Florida to Boca Raton, Florida. Further, water from the Biscayne Aquifer is piped to the Florida Keys.\n\n"}
{"id": "41179697", "url": "https://en.wikipedia.org/wiki?curid=41179697", "title": "Buried Valley Aquifer System", "text": "Buried Valley Aquifer System\n\nThe Buried Valley Aquifer System is a buried valley aquifer in the central basin of the Passaic River watershed in New Jersey, as defined by the U.S. Army Corps of Engineers and U.S. Environmental Protection Agency. This region impacts drinking water sources for twenty-six municipalities in four northern New Jersey counties: Morris, Union, Essex, and Somerset. \n\nThe borders of this area is defined as:\n\"on the north by Hook Mountain (a basaltic lava flow) and by a line which roughlybisects the Town of Montville. The western boundary is defined by the trace of the Ramapo Fault and the beginning of the Highlands Physiographic Province. The boundary to the south and the east is formed by the Second Watchung Mountain range, including portions of Union, Essex and Somerset Counties, New Jersey.\"\n\nThis area includes the following municipalities:\n"}
{"id": "151481", "url": "https://en.wikipedia.org/wiki?curid=151481", "title": "Columbite", "text": "Columbite\n\nColumbite, also called niobite, niobite-tantalite and columbate [(Fe, Mn)NbO], is a black mineral group that is an ore of niobium. It has a submetallic luster and a high density and is a niobate of iron and manganese. This mineral group was first found in Haddam, Connecticut, in the United States. It forms a series with the tantalum-dominant analogue ferrotantalite and one with the manganese-dominant analogue manganocolumbite. The iron-rich member of the columbite group is ferrocolumbite. Some tin and tungsten may be present in the mineral. Yttrocolumbite is the yttrium-rich columbite with the formula (Y,U,Fe)(Nb,Ta)O. It is a radioactive mineral found in Mozambique.\n\nColumbite has the same composition and crystal symmetry (orthorhombic) as tantalite. In fact, the two are often grouped together as a semi-singular mineral series called columbite-tantalite or coltan in many mineral guides. However, tantalite has a much greater specific gravity than columbite, more than 8.0 compared to columbite's 5.2.\n\nColumbite is also very similar to tapiolite. Those minerals have same chemical composition but different crystal symmetry: orthorhombic for columbite and tetragonal for tapiolite. The largest documented single crystal of columbite consisted of plates thick measuring .\n\nThe occurrence of columbite in the United States was made known from a specimen presumably stemming from John Winthrop (1606–1676), first Governor of the Connecticut Colony and avid mineral collector. Amidst 600 other samples, it was donated by his namesake and grandson, John Winthrop (1681–1747) to Hans Sloane, President of the Royal Society of London, upon becoming a Fellow of the Royal Society in 1737.\n\nIn 1801 Charles Hatchett discovered the element niobium in this specimen, which he named columbium in honour of explorer Christopher Columbus.\n\n"}
{"id": "43098291", "url": "https://en.wikipedia.org/wiki?curid=43098291", "title": "Davenport chained rotations", "text": "Davenport chained rotations\n\nIn physics and engineering, Davenport chained rotations are three chained intrinsic rotations about body-fixed specific axes. Euler rotations and Tait–Bryan rotations are particular cases of the Davenport general rotation decomposition. The angles of rotation are called Davenport angles because the general problem of decomposing a rotation in a sequence of three was studied first by Paul B. Davenport.\n\nThe non-orthogonal rotating coordinate system may be imagined to be rigidly attached to a rigid body. In this case, it is sometimes called a \"local\" coordinate system. Being rotation axes are solidary with the moving body, the generalized rotations can be divided into two groups (here \"x\", \"y\" and \"z\" refer to the non-orthogonal moving frame):\n\n\nMost of the cases belong to the second group, being the generalized Euler rotations are a degenerated case in which first and third axes are overlapping.\n\nThe general problem of decomposing a rotation into three composed movements about intrinsic axes was studied by P. Davenport, under the name \"generalized Euler angles\", but later these angles were named \"Davenport angles\" by M. Shuster and L. Markley.\n\nThe general problem consists of obtaining the matrix decomposition of a rotation given the three known axes. In some cases one of the axes is repeated. This problem is equivalent to a decomposition problem of matrices\n\nDavenport proved that any orientation can be achieved by composing three elemental rotations using non-orthogonal axes. The elemental rotations can either occur about the axes of the fixed coordinate system (extrinsic rotations) or about the axes of a rotating coordinate system, which is initially aligned with the fixed one and modifies its orientation after each elemental rotation (intrinsic rotations).\n\nAccording to the Davenport theorem, a unique decomposition is possible if and only if the second axis is perpendicular to the other two axes. Therefore, axes 1 and 3 must be in the plane orthogonal to axis 2.\n\nTherefore, decompositions in Euler chained rotations and Tait–Bryan chained rotations are particular cases of this. The Tait–Bryan case appears when axes 1 and 3 are perpendicular, and the Euler case appears when they are overlapping.\n\nA set of Davenport rotations is said to be complete if it is enough to generate any rotation of the space by composition. Speaking in matrix terms, it is complete if it can generate any orthonormal matrix of the space, whose determinant is +1. Due to the non-commutativity of the matrix product, the rotation system must be ordered.\n\nSometimes the order is imposed by the geometry of the underlying problem. For example, when used for vehicles, which have a special axis pointing to the \"forward\" direction, only one of the six possible combinations of rotations is useful. The interesting composition is the one able to control the heading and the elevation of the aircraft with one independent rotation each.\n\nIn the adjacent drawing, the yaw, pitch and roll (YPR) composition allows adjustment of the direction of an aircraft with the two first angles. A different composition like YRP would allow establishing the direction of the wings axis, which is obviously not useful in most cases.\n\nTait–Bryan rotations are a special case in which the first and third axes are perpendicular among them. Assuming a reference frame <\"x\",\"y\",\"z\"> with a convention as in image 2, and a plane with <yaw, pitch, roll> axes like in the image 1, lying horizontal on the plane <x,y> in the beginning, after performing intrinsic rotations Y, P and R in the yaw, pitch and roll axes (in this order) we obtain something similar to image 3.\n\nIn the beginning :\n\n\nThe rotations are applied in order yaw, pitch and roll. In these conditions, the Heading (angle on the horizontal plane) will be equal to the yaw applied, and the Elevation will be equal to the pitch.\n\nMatrix expressions for the three Tait–Bryan rotations in 3 dimensions are:\n\nThe matrix of the composed rotations is\n\nOf the six possible combinations of yaw, pitch and roll, this combination is the only one in which the heading (direction of the roll axis) is equal to one of the rotations (the yaw), and the elevation (angle of the roll axis with the horizontal plane) is equal to other of the rotations (to the pitch).\n\nEuler rotations appear as the special case in which the first and third axes of rotations are overlapping. These Euler rotations are related to the proper Euler angles, which were thought to study the movement of a rigid body such as a planet. The angle to define the direction of the roll axis is normally named \"longitude of the revolution axis\" or \"longitude of the line of nodes\" instead of \"heading\", which makes no sense for a planet.\n\nAnyway, Euler rotations can still be used when speaking about a vehicle, though they will have a weird behavior. As the vertical axis is the origin for the angles, it is named \"inclination\" instead of \"elevation\". As before, describing the attitude of a vehicle, there is an axis considered pointing forward, and therefore only one out of the possible combinations of rotations will be useful.\n\nThe combination depends on how the axis are taken and what the initial position of the plane is. Using the one in the drawing, and combining rotations in such a way that an axis is repeated, only roll–pitch–roll will allow controlling the longitude and the inclination with one rotation each.\n\nThe three matrices to multiply are:\n\nIn this convention Roll imposes the \"heading\", Pitch is the \"inclination\" (complementary of the elevation), and Roll imposes the \"tilt\".\n\nDavenport rotations are usually studied as an intrinsic rotation composition, because of the importance of the axes fixed to a moving body, but they can be converted to an extrinsic rotation composition, in case it could be more intuitive.\n\nAny extrinsic rotation is equivalent to an intrinsic rotation by the same angles but with inverted order of elemental rotations, and vice versa. For instance, the intrinsic rotations \"x-y’-z″\" by angles \"α\", \"β\", \"γ\" are equivalent to the extrinsic rotations \"z-y-x\" by angles \"γ\", \"β\", \"α\". Both are represented by a matrix\n\nif \"R\" is used to pre-multiply column vectors, and by a matrix\nif \"R\" is used to post-multiply row vectors. See Ambiguities in the definition of rotation matrices for more details.\n\nIntrinsic rotations are elemental rotations that occur about the axes of the rotating coordinate system \"XYZ\", which changes its orientation after each elemental rotation. The \"XYZ\" system rotates, while \"xyz\" is fixed. Starting with \"XYZ\" overlapping \"xyz\", a composition of three intrinsic rotations can be used to reach any target orientation for \"XYZ\". The Euler or Tait-Bryan angles (\"α\", \"β\", \"γ\") are the amplitudes of these elemental rotations. For instance, the target orientation can be reached as follows:\n\n\nThe above-mentioned notation allows us to summarize this as follows: the three elemental rotations of the XYZ-system occur about \"z\", \"x\"’ and \"z\"″. Indeed, this sequence is often denoted \"z-x’-z″\". Sets of rotation axes associated with both proper Euler angles and Tait-Bryan angles are commonly named using this notation (see above for details). Sometimes, the same sequence is simply called \"z-x-z\", \"Z-X-Z\", or \"3-1-3\", but this notation may be ambiguous as it may be identical to that used for extrinsic rotations. In this case, it becomes necessary to separately specify whether the rotations are intrinsic or extrinsic.\n\nRotation matrices can be used to represent a sequence of intrinsic rotations. For instance, \nrepresents a composition of intrinsic rotations about axes \"x-y’-z″\", if used to pre-multiply column vectors, while\nrepresents exactly the same composition when used to post-multiply row vectors. See Ambiguities in the definition of rotation matrices for more details.\n\nExtrinsic rotations are elemental rotations that occur about the axes of the fixed coordinate system \"xyz\". The \"XYZ\" system rotates, while \"xyz\" is fixed. Starting with \"XYZ\" overlapping \"xyz\", a composition of three extrinsic rotations can be used to reach any target orientation for \"XYZ\". The Euler or Tait-Bryan angles (\"α\", \"β\", \"γ\") are the amplitudes of these elemental rotations. For instance, the target orientation can be reached as follows:\n\n\nIn sum, the three elemental rotations occur about \"z\", \"x\" and \"z\". Indeed, this sequence is often denoted \"z-x-z\" (or 3-1-3). Sets of rotation axes associated with both proper Euler angles and Tait–Bryan angles are commonly named using this notation (see above for details).\n\nRotation matrices can be used to represent a sequence of extrinsic rotations. For instance, \nrepresents a composition of extrinsic rotations about axes \"x-y-z\", if used to pre-multiply column vectors, while\nrepresents exactly the same composition when used to post-multiply row vectors. See Ambiguities in the definition of rotation matrices for more details.\n\nAny extrinsic rotation is equivalent to an intrinsic rotation by the same angles but with inverted order of elemental rotations, and vice versa. For instance, the intrinsic rotations \"x-y’-z″\" by angles \"α\", \"β\", \"γ\" are equivalent to the extrinsic rotations \"z-y-x\" by angles \"γ\", \"β\", \"α\". Both are represented by a matrix\n\nif \"R\" is used to pre-multiply column vectors, and by a matrix\nif \"R\" is used to post-multiply row vectors. See Ambiguities in the definition of rotation matrices for more details.\n\nThe rotation matrix of the intrinsic rotation sequence \"x-y’-z″\" can be obtained by the sequential intrinsic element rotations from the right to the left:\n\nformula_16\n\nIn this process there are three frames related in the intrinsic rotation sequence. Let's denote the frame 0 as the initial frame, the frame 1 after the first rotation around the \"x axis\", the frame 2 after the second rotation around the \"y’\" axis and the frame 3 as the third rotation around \"z″\" axis.\n\nSince a rotation matrix can be represented among these three frames, let's use the left shoulder index to denote the representation frame. The following notation means the rotation matrix that transforms the frame \"a\" to the frame \"b\" and that is represented in the frame \"c\" :\n\nformula_17\n\nAn intrinsic element rotation matrix represented in that frame where the rotation happens has the same value as that of the corresponding extrinsic element rotation matrix:\n\nformula_18\n\nThe intrinsic element rotation matrix \"Y’\" and \"Z″\" represented in the frame 0 can be expressed as other forms:\n\nformula_19\n\nformula_20\n\nThe two equations above are substituted to the first equation:\n\nformula_21\n\nTherefore, the rotation matrix of an intrinsic element rotation sequence is the same as that of the inverse extrinsic element rotation sequence:\n\nformula_22\n\n"}
{"id": "15865362", "url": "https://en.wikipedia.org/wiki?curid=15865362", "title": "Dead Sea salt", "text": "Dead Sea salt\n\nDead Sea salt refers to salt and other mineral deposits extracted or taken from the Dead Sea. The composition of this material differs significantly from oceanic salt.\n\nDead Sea salt was used by the peoples of Ancient Egypt and it has been utilized in various unguents, skin creams, and soaps since then.\n\nThe Dead Sea's mineral composition varies with season, rainfall, depth of deposit, and ambient temperature. Most oceanic salt is approximately 85% sodium chloride (the same salt as table salt) while Dead Sea salt is only 30.5% of this, with the remainder composed of other dried minerals and salts. Major ion concentrations in the water of the Dead Sea have given the following results (which do not necessarily reflect those of Dead Sea \"salt\"):\nThe major minerals present in Dead Sea mud—the result of runoff streams flowing into the Dead Sea and bringing large deposits during the Holocene era—have been shown to be the following:\n\nDead Sea salts have been claimed to treat the following conditions:\n\n\"Rheumatologic Conditions\" – in balneotherapy of rheumatoid arthritis, psoriatic arthritis, and osteoarthritis. The minerals are absorbed while soaking, stimulating blood circulation.\n\n\"Common skin ailments\" – for treating skin disorders such as acne and psoriasis which may be relieved by regularly soaking the affected area in water with added Dead Sea salt. The National Psoriasis Foundation recommends Dead Sea and Dead Sea salts as effective treatments for psoriasis. High concentration of magnesium in Dead Sea salt may be helpful in improving skin hydration and reducing inflammation, although Epsom salt is a much-less-expensive salt that also contains high amounts of magnesium and therefore may be equally as useful for this purpose.\n\n\"Allergies\" – The high concentration of bromide and magnesium in the Dead Sea salt may help relieve allergic reactions of the skin by reducing inflammation.\n\n\"Skin ageing\" – Dead Sea salt may reduce the depth of skin wrinkling.\n\n"}
{"id": "1221457", "url": "https://en.wikipedia.org/wiki?curid=1221457", "title": "Delayed neutron", "text": "Delayed neutron\n\nIn nuclear engineering, a delayed neutron is a neutron emitted after a nuclear fission event, by one of the fission products (or actually, a fission product daughter after beta decay), any time from a few milliseconds to a few minutes after the fission event. Neutrons born within 10 seconds of the fission are termed \"prompt neutrons\".\n\nIn a nuclear reactor large nuclides fission into two neutron-rich fission products (i.e. unstable nuclides). Many of these fission products then undergo radioactive decay (usually beta decay) and the resulting nuclides are left in an excited state. These usually immediately undergo gamma decay but a small fraction of them are excited enough to be able to decay by emitting a neutron in addition. The moment of beta decay of the precursor nuclides - which are the precursors of the delayed neutrons - happens orders of magnitude later compared to the emission of the prompt neutrons. Hence the neutron that originates from the precursor's decay is termed a delayed neutron. However, the \"delay\" in the neutron emission is due to the delay in beta decay, since neutron emission, like gamma emission, happens almost immediately after the beta decay. The various half lives of these decays that finally result in neutron emission, are thus the beta decay half lives of the precursor radionuclides.\n\nDelayed neutrons play an important role in nuclear reactor control and safety analysis.\n\nDelayed neutrons are associated with the beta decay of the fission products. After prompt fission neutron emission the residual fragments are still neutron rich and undergo a beta decay chain. The more neutron rich the fragment, the more energetic and faster the beta decay. In some cases the available energy in the beta decay is high enough to leave the residual nucleus in such a highly excited state that neutron emission instead of gamma emission occurs.\n\nUsing U-235 as an example, this nucleus absorbs thermal neutrons, and the immediate mass products of a fission event are two large fission fragments, which are remnants of the formed U-236 nucleus. These fragments emit, on average, two or three free neutrons (in average 2.47), called \"prompt\" neutrons. A subsequent fission fragment occasionally undergoes a stage of radioactive decay (which is a beta minus decay) that yields a new nucleus (the emitter nucleus) in an excited state that emits an additional neutron, called a \"delayed\" neutron, to get to ground state. These neutron-emitting fission fragments are called delayed neutron precursor atoms.\n\nDelayed Neutron Data for Thermal Fission in U-235\n\nThe standard deviation of the final kinetic energy distribution as a function of mass of final fragments from low energy fission of uranium 234 and uranium 236, presents a peak around light fragment masses region and another on heavy fragment masses region. Simulation by Monte Carlo method of these experiments suggests that those peaks are produced by prompt neutron emission. This effect of prompt neutron emission does not permit to obtain primary mass and kinetic distribution which is important to study fission dynamics from saddle to scission point.\n\nIf a nuclear reactor happened to be prompt critical - even very slightly - the number of neutrons would increase exponentially at a high rate, and very quickly the reactor would become uncontrollable by means of cybernetics. The control of the power rise would then be left to its intrinsic physical stability factors, like the thermal dilatation of the core, or the increased resonance absorptions of neutrons, that usually tend to decrease the reactor's reactivity when temperature rises; but the reactor would run the risk of being damaged or destroyed by heat.\n\nHowever, thanks to the delayed neutrons, it is possible to leave the reactor in a subcritical state as far as only prompt neutrons are concerned: the delayed neutrons come a moment later, just in time to sustain the chain reaction when it is going to die out. In that regime, neutron production overall still grows exponentially, but on a time scale that is governed by the delayed neutron production, which is slow enough to be controlled (just as an otherwise unstable bicycle can be balanced because human reflexes are quick enough on the time scale of its instability). Thus, by widening the margins of non-operation and supercriticality and allowing more time to regulate the reactor, the delayed neutrons are essential to inherent reactor safety and even in reactors requiring active control.\n\nThe lower percentage of delayed neutrons makes the use of large percentage of plutonium in nuclear reactors more challenging.\n\nThe factor β is defined as:\n\nand it is equal to 0.0064 for U-235.\n\nThe delayed neutron fraction (DNF) is defined as:\n\nThese two factors, β and \"DNF\", are not the same thing in case of a rapid change in the number of neutrons in the reactor.\n\nAnother concept, is the \"effective fraction of delayed neutrons\", which is the fraction of delayed neutrons weighted (over space, energy, and angle) on the adjoint neutron flux. This concept arises because delayed neutrons are emitted with an energy spectrum more thermalized relative to prompt neutrons. For low enriched uranium fuel working on a thermal neutron spectrum, the difference between the average and effective delayed neutron fractions can reach 50 pcm.\n\n\n"}
{"id": "14005026", "url": "https://en.wikipedia.org/wiki?curid=14005026", "title": "Diffusiophoresis and diffusioosmosis", "text": "Diffusiophoresis and diffusioosmosis\n\nDiffusiophoresis is the spontaneous motion of colloidal particles or molecules in a fluid, induced by a concentration gradient of a different substance. In other words, it is motion of one species, A, in response to a concentration gradient in another species, B. Typically, A is colloidal particles which are in aqueous solution in which B is a dissolved salt such as sodium chloride, and so the particles of A are much larger than the ions of B. But both A and B could be polymer molecules, and B could be a small molecule. For example, concentration gradients in ethanol solutions in water move 1 μm diameter colloidal particles with diffusiophoretic velocities formula_1 of order 0.1 to 1 μm/s, the movement is towards regions of the solution with lower ethanol concentration (and so higher water concentration). Both species A and B will typically be diffusing but diffusiophoresis is distinct from simple diffusion: in simple diffusion a species A moves down a gradient in its own concentration. \n\nDiffusioosmosis, also referred to as capillary osmosis, is flow of a solution relative to a fixed wall or pore surface, where the flow is driven by a concentration gradient in the solution. This is distinct from flow relative to a surface driven by a gradient in the hydrostatic pressure in the fluid. In diffusioosmosis the hydrostatic pressure is uniform and the flow is due to a concentration gradient.\n\nDiffusioosmosis and diffusiophoresis are essentially the same phenomenon. They are both relative motion of a surface and a solution, driven by a concentration gradient in the solution. This motion is called diffusiophoresis when the solution is considered static with particles moving in it due to relative motion of the fluid at the surface of these particles. The term diffusioosmosis is used when the surface is viewed as static, and the solution flows.\n\nA well studied example of diffusiophoresis is the motion of colloidal particles in an aqueous solution of an electrolyte solution, where a gradient in the concentration of the electrolyte causes motion of the colloidal particles. Colloidal particles may be hundred of nanometres or larger in diameter, while the interfacial double layer region at the surface of the colloidal particle will be of order the Debye length wide, and this is typically only nanometres. So here, the interfacial width is much smaller than the size of the particle, and then the gradient in the smaller species drives diffusiophoretic motion of the colloidal particles largely through motion in the interfacial double layer.\n\nDiffusiophoresis was first studied by Derjaguin and coworkers in 1947.\n\nDiffusiophoresis, by definition, moves colloidal particles, and so the applications of diffusiophoresis are to situations where we want to move colloidal particles. Colloidal particles are typically between 10 nanometres and a few micrometres in size. Simple diffusion of colloids is fast on lengthscales of a few micrometres, and so diffusiophoresis would not be useful, whereas on lengthscales larger than millimetres, diffusiophoresis may be slow as its speed decreases with decreasing size of the solute concentration gradient. Thus, typically diffusiophoresis is employed on lengthscales approximately in the range a micrometre to a millimetre. Applications include moving particles into or out of pores of that size, and helping or inhibiting mixing of colloidal particles. \n\nIn addition, solid surfaces that are slowly dissolving will create concentration gradients near them, and these gradients may drive movement of colloidal particles towards or away from the surface. This was studied by Prieve in the context of latex particles being pulled towards, and coating, a dissolving steel surface.\n\nDiffusiophoresis is an analogous phenomenon to thermophoresis, where a species A moves in response to a temperature gradient. Both diffusiophoresis and thermophoresis are governed by Onsager reciprocal relations. Simply speaking, a gradient in any thermodynamic quantity, such as the concentration of any species, or temperature, will drive motion of all thermodynamic quantities, i.e., motion of all species present, and a temperature flux. Each gradient provides a thermodynamic force that moves the species present, and the Onsager reciprocal relations govern the relationship between the forces and the motions.\n\nDiffusiophoresis is a special case of multicomponent diffusion. Multicomponent diffusion is diffusion in mixtures, and diffusiophoresis is the special case where we are interested in the movement of one species that is usually a colloidal particle, in a gradient of a much smaller species, such as dissolved salt such as sodium chloride in water. or a miscible liquid, such as ethanol in water. Thus diffusiophoresis always occurs in a mixture, typically a three-component mixture of water, salt and a colloidal species, and we are interested in the cross-interaction between the salt and the colloidal particle.\n\nIt is the very large difference in size between the colloidal particle, which may be 1μm across, and the size of the ions or molecules, which are less than 1 nm across, that makes diffusiophoresis closely related to diffusioosomosis at a flat surface. In both cases the forces that drive the motion are largely localised to the interfacial region, which is a few molecules across and so typically of order a nanometer across. Over distances of order a nanometer, there is little difference between the surface of a colloidal particle 1 μm across, and a flat surface.\n\nDiffusioosmosis is flow of a fluid at a solid surface, or in other words, flow at a solid/fluid interface. The Marangoni effect is flow at a fluid/fluid interface. So the two phenomena are analogous with the difference being that in diffusioosmosis one of the phases is a solid. Both diffusioosmosis and the Marangoni effect are driven by gradients in the interfacial free energy, i.e., in both cases the induced velocities are zero if the interfacial free energy is uniform in space, and in both cases if there are gradients the velocities are directed along the direction of increasing interfacial free energy.\n\nIn diffusioosmosis, for a surface at rest the velocity increases from zero at the surface to the diffusioosmotic velocity, over the width of the interface between the surface and the solution. Beyond this distance, the diffusioosmotic velocity does not vary with distance from the surface. The driving force for diffusioosmosis is thermodynamic, i.e., it acts to reduce the free energy if the system, and so the direction of flow is away from surface regions of low surface free energy, and towards regions of high surface free energy. For a solute that adsorbs at surface, diffusioosmotic flow is away from regions of high solute concentration, while for solutes that are repelled by the surface, flow is away from regions of low solute concentration.\n\nFor gradients that are not-too-large, the diffusioosmotic slip velocity, i.e., the relative flow velocity far from the surface will be proportional to the gradient in the concentration gradient\n\nformula_2\n\nwhere formula_3 is a diffusioosmotic coefficient, and formula_4 is the solute concentration. When the solute is ideal and interacts with a surface in the formula_5 plane at formula_6 via a potential formula_7, the coefficient formula_3 is given by\n\nformula_9\n\nwhere formula_10 is Boltzmann's constant, formula_11 is the absolute temperature, and formula_12 is the viscosity in the interfacial region, assumed to be constant in the interface. This expression assumes that the fluid velocity for fluid in contact with the surface is forced to be zero, by interaction between the fluid and the wall. This is called the no-slip condition.\n\nTo understand these expressions better, we can consider a very simple model, where the surface simply excludes an ideal solute from an interface of width formula_13, this is would be the Asakura-Oosawa model of an ideal polymer against a hard wall. Then the integral is simplyformula_14 and the diffusioosmotic slip velocity\n\nformula_15\n\nNote that the slip velocity is directed towards increasing solute concentrations. \n\nA particle much larger than formula_13 moves with a diffusiophoretic velocity formula_17 relative to the surrounding solution. So diffusiophoresis moves particles towards lower solute concentrations, in this case.\n\nIn this simple model, formula_18 can also derived directly from the expression for fluid flow in the Stokes limit for an incompressible fluid, which is\n\nformula_19\n\nfor formula_20 the fluid flow velocity and formula_21 the pressure. We consider an infinite surface in the formula_5 plane at formula_6, and enforce stick boundary conditions there, i.e., formula_24. We take the concentration gradient to be along the formula_25 axis, i.e., formula_26. Then the only non-zero component of the flow velocity formula_20 is along x, formula_28, and it depends only on height formula_29. So the only non-zero component of the Stokes' equation is\n\nformula_30\n\nIn diffusioosmosis, in the bulk of the fluid (i.,e., outside the interface) the hydrostatic pressure is assumed to be uniform (as we expect any gradients to relax away by fluid flow) and so in bulk\n\nformula_31\n\nfor formula_32the solvent's contribution to the hydrostatic pressure, and formula_33 the contribution of the solute, called the osmotic pressure. Thus in the bulk the gradients obey\n\nformula_34\n\nAs we have assumed the solute is ideal, formula_35, and so\n\nformula_36\n\nOur solute is excluded from a region of width formula_13 (the interfacial region) from the surface, and so in interface formula_38, and so there formula_39. Assuming continuity of the solvent contribution into the interface we have a gradient of the hydrostatic pressure in the interface \n\nformula_40\n\ni.e., in the interface there is a gradient of the hydrostatic pressure equal to the negative of the bulk gradient in the osmotic pressure. It is this gradient in the interface in the hydrostatic pressure formula_21 that creates the diffusioosmotic flow. Now that we have formula_42, we can substitute into the Stokes equation, and integrate twice, then\n\nformula_43\n\nformula_44\n\nwhere formula_45, formula_46, formula_47 and formula_48 are integration constants. Far from the surface the flow velocity must be a constant, so formula_49. We have imposed zero flow velocity at formula_6, so formula_51. Then imposing continuity where the interface meets the bulk, i.e., forcing formula_52 and formula_53 to be continuous at formula_54 we determine formula_45 and formula_48, and so get\n\nformula_57\n\nformula_58\n\nWhich gives, as it should, the same expression for the slip velocity, as above. This result is for a specific and very simple model, but it does illustrate general features of diffusioosmoisis: 1) the hydrostatic pressure is, by definition (flow induced by pressure gradients in the bulk is a common but separate physical phenomenon) uniform in the bulk, but there is a gradient in the pressure in the interface, 2) this pressure gradient in the interface causes the velocity to vary in the direction perpendicular to the surface, and this results in a slip velocity, i.e., for the bulk of the fluid to move relative to the surface, 3) away from the interface the velocity is constant, this type of flow is sometimes called plug flow. \n\nIn many applications of diffusiophoresis, the motion is driven by gradients in the concentration of a salt (electrolyte) concentration, such as sodium chloride in water. Colloidal particles in water are typically charged, and there is an electrostatic potential, called a zeta potential at their surface. This charged surface of the colloidal particle interacts with a gradient in salt concentration, and this gives rise to diffusiophoretic velocity formula_59 given by\n\nformula_60\n\nwhere formula_61 is the permittivity of water, formula_12 is the viscosity of water, formula_63 is the zeta potential of the colloidal particle in the salt solution, formula_64is the reduced difference between the diffusion constant of the positively charged ion, formula_65, and the diffusion constant of the negatively charged ion, formula_66, and formula_67 is the salt concentration. formula_68is the gradient, i.e., rate of change with position, of the logarithm of the salt concentration, which is equivalent to the rate of change of the salt concentration, divided by the salt concentration - it is effectively one over the distance over which the concentration decreases by a factor of e. The above equation is approximate, and only valid for 1:1 electrolytes such as sodium chloride.\n\nNote that there are two contributions to diffusiophoresis of a charged particle in a salt gradient, which give rise to the two terms in the above equation for formula_59. The first is due to the fact that whenever there is a salt concentration gradient, then unless the diffusion constants of the positive and negative ions are exactly equal to each other, there is an electric field, i.e., the gradient acts a little like a capacitor. This electric filed generated by the salt gradient drives electrophoresis of the charged particle, just as an externally applied electric field does. This gives rise to the first term in the equation above, i.e., diffusiophoresis at a velocity formula_70.\n\nThe second part is due to the surface free energy of the surface of a charged particle, decreasing with increasing salt concentration, this is a similar mechanism to that found in diffusiophoresis in gradients of neutrial substances. This gives rise to the second part of the diffusiophoretic velocity formula_71. Note that this simple theory predicts that this contribution to the diffusiophoretic motion is always up a salt concentration gradient, it always moves particles towards higher salt concentration. By contrast, the sign of the electric-field contribution to diffusiophoresis depends on the sign of formula_72. So for example, for a negatively charged particle, formula_73, and if the positively charged ions diffuse faster than the negatively charged ones, then this term will push particles down a salt gradient, but if it is the negatively charged ions that diffuse faster, then this term pushes the particles up the salt gradient.\n\nOne inexpensive method for making water potable is to use CO2. In the paper, \"Membraneless water filtration using CO2\" by Dr. Howard Stone of Princeton University and associates, an innovative method of using diffusiophoresis to take contaminated water by using CO2 and air to create carbonic acid to split the water into a waste stream and a potable water stream. This allows for easy ionic separation of suspended particles. This has huge energy cost and time savings opportunity to make drinking water safe compared to traditional water filtration methods for dirty water sources.\n\n"}
{"id": "61580", "url": "https://en.wikipedia.org/wiki?curid=61580", "title": "Electrical resistivity and conductivity", "text": "Electrical resistivity and conductivity\n\nElectrical resistivity (also known as specific electrical resistance, or volume resistivity) is a fundamental property of a material that quantifies how strongly that material opposes the flow of electric current. A low resistivity indicates a material that readily allows the flow of electric current. Resistivity is commonly represented by the Greek letter \"ρ\" (rho). The SI unit of electrical resistivity is the ohm-metre (Ω⋅m). As an example, if a solid cube of material has sheet contacts on two opposite faces, and the resistance between these contacts is 1 Ω, then the resistivity of the material is 1 Ω⋅m.\n\nElectrical conductivity or specific conductance is the reciprocal of electrical resistivity, and measures a material's ability to conduct an electric current. It is commonly represented by the Greek letter σ (sigma), but κ (kappa) (especially in electrical engineering) or γ (gamma) are also occasionally used. Its SI unit is siemens per metre (S/m).\n\nIn an ideal case, cross-section and physical composition of the examined material are uniform across the sample, and the electric field and current density are both parallel and constant everywhere. Many resistors and conductors do in fact have a uniform cross section with a uniform flow of electric current, and are made of a single material, so that this is a good model. (See the adjacent diagram.) When this is the case, the electrical resistivity \"ρ\" (Greek: rho) can be calculated by:\n\nwhere\n\nBoth \"resistance\" and \"resistivity\" describe how difficult it is to make electrical current flow through a material, but unlike resistance, resistivity is an \"intrinsic property\". This means that all pure copper wires (which have not been subjected to distortion of their crystalline structure etc.), irrespective of their shape and size, have the same \"resistivity\", but a long, thin copper wire has a much larger \"resistance\" than a thick, short copper wire. Every material has its own characteristic resistivity. For example, rubber has a far larger resistivity than copper.\n\nIn a hydraulic analogy, passing current through a high-resistivity material is like pushing water through a pipe full of sand—while passing current through a low-resistivity material is like pushing water through an empty pipe. If the pipes are the same size and shape, the pipe full of sand has higher resistance to flow. Resistance, however, is not \"solely\" determined by the presence or absence of sand. It also depends on the length and width of the pipe: short or wide pipes have lower resistance than narrow or long pipes.\nThe above equation can be transposed to get Pouillet's law (named after Claude Pouillet):\n\nThe resistance of a given material is proportional to the length, but inversely proportional to the cross-sectional area. Thus resistivity can be expressed using the SI unit \"ohm metre\" (Ω⋅m) (i.e ohms divided by metres (for the length) and then multiplied by square metres (for the cross-sectional area)}.\n\nFor example, if \"A\" = formula_3 = (forming a cube with perfectly conductive contacts on opposite faces), then the resistance of this element in ohms is numerically equal to the resistivity of the material it is made of in Ω⋅m.\n\nConductivity, σ, is the inverse of resistivity:\n\nConductivity has SI units of \"siemens per metre\" (S/m).\n\nFor less ideal cases, such as more complicated geometry, or when the current and electric field vary in different parts of the material, it is necessary to use a more general expression in which the resistivity at a particular point is defined as the ratio of the electric field to the density of the current it creates at that point:\n\nwhere\n\nin which formula_10 and formula_11 are inside the conductor.\n\nConductivity is the inverse (reciprocal) of resistivity. Here, it is given by:\n\nFor example, rubber is a material with large \"ρ\" and small \"σ\"—because even a very large electric field in rubber makes almost no current flow through it. On the other hand, copper is a material with small \"ρ\" and large \"σ\"—because even a small electric field pulls a lot of current through it.\n\nAs shown below, this expression simplifies to a single number when the electric field and current density are constant in the material.\n\nWhen the resistivity of a material has a directional component, the most general definition of resistivity must be used. It starts from the tensor-vector form of Ohm's law which relates the electric field inside a material to the electric current flow. This equation is completely general, meaning it is valid in all cases, including those mentioned above. However, this definition is the most complicated, so it is only directly used in anisotropic cases, where the more simple definitions cannot be applied. If the material is not anisotropic, it is safe to ignore the tensor-vector definition, and use a simpler expression instead.\n\nHere, anisotropic means that the material has different properties in different directions. For example, a crystal of graphite consists microscopically of a stack of sheets, and current flows very easily through each sheet, but much less easily from one sheet to the adjacent one. In such cases, the current does not flow in exactly the same direction as the electric field. Thus, the appropriate equations are generalized to the three-dimensional tensor form:\n\nwhere the conductivity σ and resistivity ρ are rank-2 tensors, and electric field E and current density J are vectors. These tensors can be represented by 3×3 matrices, the vectors with 3x1 matrices, with matrix multiplication used on the right side of these equations. In matrix form, the resistivity relation is given by:\n\nwhere\n\nEquivalently, resistivity can be given in the more compact Einstein notation:\n\nIn either case, the resulting expression for each electric field component is:\n\nSince the choice of the coordinate system is free, the usual convention is to simplify the expression by choosing an x-axis parallel to the current direction, so J=J=0. This leaves:\n\nConductivity is defined similarly:\n\nor\n\nBoth resulting in:\n\nLooking at the two expressions, formula_18 and formula_33 are the matrix inverse of each other. However, in the most general case, the individual matrix elements are not necessarily reciprocals of one another; for example, \"σ\" may not be equal to 1/\"ρ\". This can be seen in the Hall effect, where formula_34 is nonzero. In the Hall effect, due to rotational invariance about the z-axis, formula_35 and formula_36, so the relation between resistivity and conductivity simplifies to:\n\nIf the electric field is parallel to the applied current, formula_34 and formula_40 are zero. When they are zero, one number, formula_41, is enough to describe the electrical resistivity. It is then written as simply formula_9, and this reduces to the simpler expression.\n\nAccording to elementary quantum mechanics, an electron in an atom or crystal can only have certain precise energy levels; energies between these levels are impossible. When a large number of such allowed energy levels are spaced close together (in energy-space)—i.e. have similar (minutely differing) energies— we can talk about these energy levels together as an \"energy band\". There can be many such energy bands in a material, depending on the atomic number {number of electrons (if the atom is neutral)} and their distribution (besides the size of the crystal and external factors like environmental modification of the energy bands).\n\nThe material's electrons seek to minimize the total energy in the material by going to low energy states; however, the Pauli exclusion principle means that only one can exist in each such state. So the electrons \"fill up\" the band structure starting from the bottom. The characteristic energy level up to which the electrons have filled is called the Fermi level. The position of the Fermi level with respect to the band structure is very important for electrical conduction: only electrons in energy levels near the Fermi level are free to move around, since the electrons can easily jump among the partially occupied states in that region. In contrast, the low energy states are rigidly filled with a fixed number of electrons at all times, and the high energy states are empty of electrons at all times.\n\nElectric current consists of a flow of electrons. In metals there are many electron energy levels near the Fermi level, so there are many electrons available to move. This is what causes the high electronic conductivity of metals.\n\nAn important part of band theory is that there may be forbidden bands of energy: energy intervals that contain no energy levels. In insulators and semiconductors, the number of electrons is just the right amount to fill a certain integer number of low energy bands, exactly to the boundary. In this case, the Fermi level falls within a band gap. Since there are no available states near the Fermi level, and the electrons are not freely movable, the electronic conductivity is very low.\n\nA metal consists of a lattice of atoms, each with an outer shell of electrons that freely dissociate from their parent atoms and travel through the lattice. This is also known as a positive ionic lattice. This 'sea' of dissociable electrons allows the metal to conduct electric current. When an electrical potential difference (a voltage) is applied across the metal, the resulting electric field causes electrons to drift towards the positive terminal. The actual drift velocity of electrons is typically small, on the order of magnitude of meters per hour. However, due to the sheer number of moving electrons, even a slow drift velocity results in a large current density. The mechanism is similar to transfer of momentum of balls in a Newton's cradle but the rapid propagation of an electric energy along a wire is not due to the mechanical forces, but the propagation of an energy-carrying electromagnetic field guided by the wire.\n\nMost metals have electrical resistance. In simpler models (non quantum mechanical models) this can be explained by replacing electrons and the crystal lattice by a wave-like structure. When the electron wave travels through the lattice, the waves interfere, which causes resistance. The more regular the lattice is, the less disturbance happens and thus the less resistance. The amount of resistance is thus mainly caused by two factors. First, it is caused by the temperature and thus amount of vibration of the crystal lattice. The temperature causes bigger vibrations, which act as irregularities in the lattice. Second, the purity of the metal is relevant as a mixture of different ions is also an irregularity.\n\nIn metals, the Fermi level lies in the conduction band (see Band Theory, above) giving rise to free conduction electrons. However, in semiconductors the position of the Fermi level is within the band gap, about halfway between the conduction band minimum (the bottom of the first band of unfilled electron energy levels) and the valence band maximum (the top of the band below the conduction band, of filled electron energy levels). That applies for intrinsic (undoped) semiconductors. This means that at absolute zero temperature, there would be no free conduction electrons, and the resistance is infinite. However, the resistance decreases as the charge carrier density (i.e., without introducing further complications, the density of electrons) in the conduction band increases. In extrinsic (doped) semiconductors, dopant atoms increase the majority charge carrier concentration by donating electrons to the conduction band or producing holes in the valence band. (A \"hole\" is a position where an electron is missing; such holes can behave in a similar way to electrons.) For both types of donor or acceptor atoms, increasing dopant density reduces resistance. Hence, highly doped semiconductors behave metallically. At very high temperatures, the contribution of thermally generated carriers dominates over the contribution from dopant atoms, and the resistance decreases exponentially with temperature.\n\nIn electrolytes, electrical conduction happens not by band electrons or holes, but by full atomic species (ions) traveling, each carrying an electrical charge. The resistivity of ionic solutions (electrolytes) varies tremendously with concentration – while distilled water is almost an insulator, salt water is a reasonable electrical conductor. Conduction in ionic liquids is also controlled by the movement of ions, but here we are talking about molten salts rather than solvated ions. In biological membranes, currents are carried by ionic salts. Small holes in cell membranes, called ion channels, are selective to specific ions and determine the membrane resistance.\n\nThe electrical resistivity of a metallic conductor decreases gradually as temperature is lowered. In ordinary conductors, such as copper or silver, this decrease is limited by impurities and other defects. Even near absolute zero, a real sample of a normal conductor shows some resistance. In a superconductor, the resistance drops abruptly to zero when the material is cooled below its critical temperature. An electric current flowing in a loop of superconducting wire can persist indefinitely with no power source.\n\nIn 1986, researchers discovered that some cuprate-perovskite ceramic materials have much higher critical temperatures, and in 1987 one was produced with a critical temperature above . Such a high transition temperature is theoretically impossible for a conventional superconductor, so the researchers named these conductors \"high-temperature superconductors\". Liquid nitrogen boils at 77 K, cold enough to activate high-temperature superconductors, but not nearly cold enough for conventional superconductors. In conventional superconductors, electrons are held together in pairs by an attraction mediated by lattice phonons. The best available model of high-temperature superconductivity is still somewhat crude. There is a hypothesis that electron pairing in high-temperature superconductors is mediated by short-range spin waves known as paramagnons.\n\nPlasmas are very good conductors and electric potentials play an important role.\n\nThe potential as it exists on average in the space between charged particles, independent of the question of how it can be measured, is called the \"plasma potential\", or \"space potential\". If an electrode is inserted into a plasma, its potential generally lies considerably below the plasma potential, due to what is termed a Debye sheath. The good electrical conductivity of plasmas makes their electric fields very small. This results in the important concept of \"quasineutrality\", which says the density of negative charges is approximately equal to the density of positive charges over large volumes of the plasma (\"n\" = <Z>\"n\"), but on the scale of the Debye length there can be charge imbalance. In the special case that \"double layers\" are formed, the charge separation can extend some tens of Debye lengths.\n\nThe magnitude of the potentials and electric fields must be determined by means other than simply finding the net charge density. A common example is to assume that the electrons satisfy the Boltzmann relation:\n\nDifferentiating this relation provides a means to calculate the electric field from the density:\n\nIt is possible to produce a plasma that is not quasineutral. An electron beam, for example, has only negative charges. The density of a non-neutral plasma must generally be very low, or it must be very small. Otherwise, the repulsive electrostatic force dissipates it.\n\nIn astrophysical plasmas, Debye screening prevents electric fields from directly affecting the plasma over large distances, i.e., greater than the Debye length. However, the existence of charged particles causes the plasma to generate, and be affected by, magnetic fields. This can and does cause extremely complex behavior, such as the generation of plasma double layers, an object that separates charge over a few tens of Debye lengths. The dynamics of plasmas interacting with external and self-generated magnetic fields are studied in the academic discipline of magnetohydrodynamics.\n\nPlasma is often called the \"fourth state of matter\" after solid, liquids and gases. It is distinct from these and other lower-energy states of matter. Although it is closely related to the gas phase in that it also has no definite form or volume, it differs in a number of ways, including the following:\n\n\nThe degree of doping in semiconductors makes a large difference in conductivity. To a point, more doping leads to higher conductivity. The conductivity of a solution of water is highly dependent on its concentration of dissolved salts, and other chemical species that ionize in the solution. Electrical conductivity of water samples is used as an indicator of how salt-free, ion-free, or impurity-free the sample is; the purer the water, the lower the conductivity (the higher the resistivity). Conductivity measurements in water are often reported as \"specific conductance\", relative to the conductivity of pure water at . An EC meter is normally used to measure conductivity in a solution. A rough summary is as follows:\n\nThis table shows the resistivity (ρ), conductivity and temperature coefficient of various materials at 20 °C (68 °F, 293 K)\n\nThe effective temperature coefficient varies with temperature and purity level of the material. The 20 °C value is only an approximation when used at other temperatures. For example, the coefficient becomes lower at higher temperatures for copper, and the value 0.00427 is commonly specified at .\n\nThe extremely low resistivity (high conductivity) of silver is characteristic of metals. George Gamow tidily summed up the nature of the metals' dealings with electrons in his popular science book \"One, Two, Three...Infinity\" (1947):\n\nMore technically, the free electron model gives a basic description of electron flow in metals.\n\nWood is widely regarded as an extremely good insulator, but its resistivity is sensitively dependent on moisture content, with damp wood being a factor of at least worse insulator than oven-dry. In any case, a sufficiently high voltage – such as that in lightning strikes or some high-tension powerlines – can lead to insulation breakdown and electrocution risk even with apparently dry wood.\n\nThe electrical resistivity of most materials changes with temperature. If the temperature \"T\" does not vary too much, a linear approximation is typically used:\nwhere formula_46 is called the \"temperature coefficient of resistivity\", formula_47 is a fixed reference temperature (usually room temperature), and formula_48 is the resistivity at temperature formula_47. The parameter formula_46 is an empirical parameter fitted from measurement data. Because the linear approximation is only an approximation, formula_46 is different for different reference temperatures. For this reason it is usual to specify the temperature that formula_46 was measured at with a suffix, such as formula_53, and the relationship only holds in a range of temperatures around the reference. When the temperature varies over a large temperature range, the linear approximation is inadequate and a more detailed analysis and understanding should be used.\n\nIn general, electrical resistivity of metals increases with temperature. Electron–phonon interactions can play a key role. At high temperatures, the resistance of a metal increases linearly with temperature. As the temperature of a metal is reduced, the temperature dependence of resistivity follows a power law function of temperature. Mathematically the temperature dependence of the resistivity ρ of a metal is given by the Bloch–Grüneisen formula:\n\nwhere formula_55 is the residual resistivity due to defect scattering, A is a constant that depends on the velocity of electrons at the Fermi surface, the Debye radius and the number density of electrons in the metal. formula_56 is the Debye temperature as obtained from resistivity measurements and matches very closely with the values of Debye temperature obtained from specific heat measurements. n is an integer that depends upon the nature of interaction:\n\n\nIf more than one source of scattering is simultaneously present, Matthiessen's Rule (first formulated by Augustus Matthiessen in the 1860s) states that the total resistance can be approximated by adding up several different terms, each with the appropriate value of \"n\".\n\nAs the temperature of the metal is sufficiently reduced (so as to 'freeze' all the phonons), the resistivity usually reaches a constant value, known as the residual resistivity. This value depends not only on the type of metal, but on its purity and thermal history. The value of the residual resistivity of a metal is decided by its impurity concentration. Some materials lose all electrical resistivity at sufficiently low temperatures, due to an effect known as superconductivity.\n\nAn investigation of the low-temperature resistivity of metals was the motivation to Heike Kamerlingh Onnes's experiments that led in 1911 to discovery of superconductivity. For details see History of superconductivity.\n\nIn general, intrinsic semiconductor resistivity decreases with increasing temperature. The electrons are bumped to the conduction energy band by thermal energy, where they flow freely, and in doing so leave behind holes in the valence band, which also flow freely. The electric resistance of a typical intrinsic (non doped) semiconductor decreases exponentially with temperature:\n\nAn even better approximation of the temperature dependence of the resistivity of a semiconductor is given by the Steinhart–Hart equation:\n\nwhere \"A\", \"B\" and \"C\" are the so-called Steinhart–Hart coefficients.\n\nThis equation is used to calibrate thermistors.\n\nExtrinsic (doped) semiconductors have a far more complicated temperature profile. As temperature increases starting from absolute zero they first decrease steeply in resistance as the carriers leave the donors or acceptors. After most of the donors or acceptors have lost their carriers, the resistance starts to increase again slightly due to the reducing mobility of carriers (much as in a metal). At higher temperatures, they behave like intrinsic semiconductors as the carriers from the donors/acceptors become insignificant compared to the thermally generated carriers.\n\nIn non-crystalline semiconductors, conduction can occur by charges quantum tunnelling from one localised site to another. This is known as variable range hopping and has the characteristic form of\n\nwhere \"n\" = 2, 3, 4, depending on the dimensionality of the system.\n\nWhen analyzing the response of materials to alternating electric fields (dielectric spectroscopy), in applications such as electrical impedance tomography, it is convenient to replace resistivity with a complex quantity called impedivity (in analogy to electrical impedance). Impedivity is the sum of a real component, the resistivity, and an imaginary component, the reactivity (in analogy to reactance). The magnitude of impedivity is the square root of sum of squares of magnitudes of resistivity and reactivity.\n\nConversely, in such cases the conductivity must be expressed as a complex number (or even as a matrix of complex numbers, in the case of anisotropic materials) called the \"admittivity\". Admittivity is the sum of a real component called the conductivity and an imaginary component called the susceptivity.\n\nAn alternative description of the response to alternating currents uses a real (but frequency-dependent) conductivity, along with a real permittivity. The larger the conductivity is, the more quickly the alternating-current signal is absorbed by the material (i.e., the more opaque the material is). For details, see Mathematical descriptions of opacity.\n\nEven if the material's resistivity is known, calculating the resistance of something made from it may, in some cases, be much more complicated than the formula formula_60 above. One example is spreading resistance profiling, where the material is inhomogeneous (different resistivity in different places), and the exact paths of current flow are not obvious.\n\nIn cases like this, the formulas\nmust be replaced with\nwhere E and J are now vector fields. This equation, along with the continuity equation for J and the Poisson's equation for E, form a set of partial differential equations. In special cases, an exact or approximate solution to these equations can be worked out by hand, but for very accurate answers in complex cases, computer methods like finite element analysis may be required.\n\nIn some applications where the weight of an item is very important, resistivity density products are more important than absolute low resistivity – it is often possible to make the conductor thicker to make up for a higher resistivity; and then a low resistivity density product material (or equivalently a high conductivity to density ratio) is desirable. For example, for long distance overhead power lines, aluminium is frequently used rather than copper (Cu) because it is lighter for the same conductance.\n\nSilver, although it is the least resistive metal known, has a high density and performs similarly to copper by this measure, but is much more expensive. Calcium and the alkali metals have the best resistivity-density products, but are rarely used for conductors due to their high reactivity with water and oxygen (and lack of physical strength). Aluminium is far more stable. Two other important attributes, price and toxicity, exclude the (otherwise) best choice: beryllium. (Pure beryllium is also brittle.) Thus, aluminium is usually the metal of choice when the weight or cost of a conductor is the driving consideration.\n\n\n"}
{"id": "5081545", "url": "https://en.wikipedia.org/wiki?curid=5081545", "title": "Electromechanical coupling coefficient", "text": "Electromechanical coupling coefficient\n\nElectromechanical coupling coefficient is a numerical measure of the conversion efficiency between electrical and acoustic energy in piezoelectric materials.\n\nQualitatively the electromechanical coupling coefficient, k, can be determined as:\n\n"}
{"id": "27746978", "url": "https://en.wikipedia.org/wiki?curid=27746978", "title": "Energy efficiency implementation", "text": "Energy efficiency implementation\n\nThe Energy efficiency implementation industry branch comprises firms which \"retrofit\" or \"replace\" inefficient equipment with more efficient parts or equipment, with the goal of reducing energy consumption.\nRetrofitting enhances existing equipment by making it expend less energy, complete replacement of equipment may be more costly, but can reduce the implementation complexity. The common goal is to save kilowatts (kW) and kilowatt hours (kWh). The difference between these two measurements is that one is a power rating (kW) and the other is a measurement of energy actually consumed (kWh).\n\nPublic Utility Commissions in many states mandate that their utilities design and implement energy efficiency programs. The funding for these can be reflected in their rates or are collected through a surcharge in monthly customer bills. Some utilities design their own programs and implement them using a rebate form or other application, and administer the programs using their own staff. Most major utilities hire implementation contractors who are responsible for the design and implementation, and some implement programs already designed and approved by their PUC. Some programs require a co-pay by the customer, some are installed at no-cost.\n\nUtilities invest in energy efficiency for the following reasons:\nThe end result is that utility companies have more energy to sell, or in other words, they are able to sell their excess capacity to more customers in the area without increasing their production capacity.\n\nThe results of energy efficiency implementation are all beneficial for the energy consumer. It reduces operational costs, reduces carbon footprint, and it can even improve quality of life. Energy efficiency implementation can also play a role in increased revenue when consumers choose a “greener” product over another that is not.\nEnergy efficiency implementation can be extremely beneficial to large market segments like small businesses, schools, cement processing plants; basically any area that uses large amounts of energy. Small changes here add up to large savings.\n\nImplementing energy efficiency measures in a home or business can also lead to behavioral changes. When an energy efficiency change has been made and the energy consumer sees the benefits, more changes are often wanted to further their savings. These small changes create awareness and can be as simple as turning of lights when a room is not in use, or as complex as adding window glazing or installing demand-control ventilation.\n\nEnergy sector regulators might have wide discretion in the implementation and/or monitoring energy efficiency (EE) initiatives. The most likely roles involve giving technical advice to the agency developing EE initiatives, since changes in demand patterns will have implications for the operations and investment plans of utilities (and for costs, security of supply and quality of service) . Particularly when the EE outlays are by the utility, the energy sector regulator needs to monitor outcomes to ensure that the resources are being utilized in ways that are consistent with overarching public policies. Furthermore, interactions of utility initiatives with other EE policies need to be taken into account when evaluating whether the scale and scope of existing utility-based demand-side management programs. Utilities are in a position to analyze bills and conduct on-premises energy audits to identify areas of saving. Regulators could require utilities to undertake costly audit programs. A high tech approach to improving operations and the customer interface involves smart meters and information systems that enable the utility to track system performance in real time.\n\nThe costs of implementing such systems need to be balanced against the benefits, including the possibility that outlays on other projects might be more cost effective. Thus, the role of regulators primarily involves providing technical input into the development of EE policies initiated by other agencies or via legislated tax programs. In addition, the Regulator must determine (unless specified in law) which benefit-cost test is appropriate for evaluating utility-based EE programs. The regulatory tests include the participant cost test (will participants benefit over the measure's life?), the program administrator cost test (will utility bills increase?), the ratepayer impact measure (will utility prices increase?), the total resource cost test (will the total costs of energy decrease?) and the societal cost test (is the utility, state or nation better off, including environmental impacts?). The IEA Energy Efficiency Governance Handbook goes into much more detail on the importance of having a coherent system for developing, incentivizing, and evaluating energy efficiency programs.\n\nBy making appliances, lighting or the HVAC system more efficient, energy consumption levels drop, which leads to a lower utility bill for utility customers.\n\nBy reducing energy consumption we are able to conserve natural resources and thus have a less dramatic impact on our environment.\n\nWorking in natural or well-lit areas contributes to performance and productivity. Comfortable temperature levels also factor into how well a person functions. This fact can be applied to any operation that requires the use of lighting or an HVAC system.\n\nEnergy efficiency projects can be implemented by commercial property managers or by energy service companies or contractors.\n\nCommercial property managers that plan and manage energy efficiency projects generally use a software platform to perform energy audits and to collaborate with contractors to understand their full range of options.\n\nImplementation companies normally focus on specific equipment or appliances that they specialize in retrofitting or replacing, or they provide this service by sector: residential, commercial or industrial. Energy efficiency implementation is a complex field, and in order to implement effectively, the implementer must be multi-faceted and have extensive experience in many areas of energy efficiency.\n\nThere are many systems, machines and methods that assist in creating energy savings including: gas, electricity, HVAC, lighting, daylighting, motion detection, insulated glazing, refrigerator strip curtains, revolving doors, anterooms, thermostat controls, demand-control ventilation and voltage optimisation\n\n"}
{"id": "1458687", "url": "https://en.wikipedia.org/wiki?curid=1458687", "title": "Flathead engine", "text": "Flathead engine\n\nA flathead engine, otherwise sidevalve engine, is an internal combustion engine with its poppet valves contained within the engine block, instead of in the cylinder head, as in an overhead valve engine. \n\nFlatheads are an early design concept that has mostly fallen into disuse, but they are currently experiencing a revival in low-revving aero-engines such as the D-Motor. \n\nThe valve gear comprises a camshaft sited low in the cylinder block which operates the poppet valves via tappets and short pushrods (or sometimes with no pushrods at all). The flathead system obviates the need for further valvetrain components such as lengthy pushrods, rocker arms, overhead valves or overhead camshafts. The sidevalves are typically adjacent, sited on one side of the cylinder(s); but some flatheads employ the less common \"crossflow\" \"T-head\" variant. In a T-head engine, the exhaust gases leave on the opposite side of the cylinder from the intake valve. \n\nThe sidevalve engine's combustion chamber is not above the piston (as in an ohv engine) but to the side, above the valves. The spark plug may be sited over the piston (as in an ohv engine) or above the valves; but aircraft designs with two plugs per cylinder may use either or both positions.\n\n\"Pop-up pistons\" may be used with compatible heads to increase compression ratio and improve the combustion chamber's shape to prevent knocking. \"Pop-up\" pistons are so called because, at tdc, they protrude above the top of the cylinder block.\n\nThe advantages of a sidevalve engine include: simplicity, reliability, low part count, low cost, low weight, compactness, responsive low-speed power, low mechanical engine noise, and insensitivity to low-octane fuel. The absence of a complicated valvetrain allows a compact engine that is cheap to manufacture, since the cylinder head may be little more than a simple metal casting. These advantages explain why side valve engines were used for economy cars, trucks, and agricultural engines for many years, while OHV designs came to be specified only for high-performance applications such as aircraft, luxury cars, sports cars, and some motorcycles.\n\nAt top dead centre, the piston gets very close to the flat portion of the cylinder head above, and the resultant squish turbulence produces excellent fuel/air mixing. A feature of the sidevalve design (particularly beneficial for an aero-engine) is that if a valve should seize in its guide and remain partially open, the piston would not be damaged, and the engine would continue operating safely on its other cylinders.\n\nThe main disadvantages of a sidevalve engine are poor gas flow, poor combustion chamber shape, and low compression ratio, all of which result in a low-revving engine with low power output.\n\nIn a sidevalve engine, intake and exhaust gases follow a circuitous route, with low volumetric efficiency, or \"poor breathing\", not least because the exhaust gases interfere with the incoming charge. Because the exhaust follows a lengthy path to leave the engine, there is a tendency for the engine to overheat. Although a sidevalve engine can safely operate at high speed, its volumetric efficiency swiftly deteriorates, so that high power outputs are not feasible at speed. High volumetric efficiency was less important for early cars because their engines rarely sustained extended high speeds, but designers seeking higher power outputs had to abandon the sidevalve. A compromise used by the Willys Jeep, Rover, Landrover, and Rolls-Royce in the 1950s was the \"F-head\" (or \"intake-over-exhaust\" valving), which has one sidevalve and one overhead valve per cylinder.\n\nThe flathead's elongated combustion chamber is prone to preignition (or \"knocking\") if compression ratio is increased, but improvements such as laser ignition or microwave enhanced ignition might help prevent knocking. Turbulence grooves may increase swirl inside the combustion chamber, thus increasing torque, especially at low rpm. Better mixing of the fuel/air charge improves combustion and helps to prevent knocking.\n\nAn advance in flathead technology resulted from experimentation in the 1920s by Sir Harry Ricardo, who improved their efficiency after studying the gas-flow characteristics of sidevalve engines.\n\nThe difficulty in designing a high-compression-ratio flathead means that most tend to be spark-ignition designs; and flathead diesels are virtually unknown.\n\nThe sidevalve arrangement was once the most common across all motor industries (automotive, agricultural, marine, aviation, and others), but has since about 1930 it fallen from favor in most multicylinder applications, such as automotive and aviation, having been displaced by overhead valve designs. Sidevalve designs are still common for many small single-cylinder or twin-cylinder engines, such as lawnmowers, rotavators, two-wheel tractors and other basic farm machinery. American LaFrance powered their fire engines with T-head engines from the 1920s to the 1950s; and early Stutz engines were T heads.\n\nMulticylinder flathead engines were used for cars such as the Ford Model T, the Ford flathead V8 engine and the Ford Sidevalve engine. Cadillac produced V-16 flathead engines for their Series 90 luxury cars from 1938–1940.\nAfter WWII, flathead designs began to be superseded by ohv designs. Flatheads were no longer common in cars, but they continued in more rudimentary vehicles such as off-road military Jeeps. In US custom andhot rod circles, restored examples of early Ford flathead V8s are still seen.\n\nThe simplicity, lightness, compactness and reliability might seem ideal for an aero-engine, but because of their low efficiency, early flathead engines were deemed unsuitable, a notable exception being the American Aeronca E-107 opposed twin aero engine of 1930. \n\nTwo modern flatheads are the Belgian D-Motor flat-fours and flat-sixes. These are extremely oversquare and compact aero-engines with direct drive to a propeller. As these engines were designed to produce peak power at only 2800 rpm, their very low engine speed meant the designers could dispense with the complexity and weight of an ohv valvetrain. \n\nFlathead designs have been used on a number of early pre-war motorcycles, in particular US V-twins such as Harley-Davidson and Indian, some British singles, BMW flat twins and Russian copies thereof. The Cleveland Motorcycle Manufacturing Company produced a T-head four-cylinder in-line motorcycle engine in the 1920s.\n\n"}
{"id": "22003056", "url": "https://en.wikipedia.org/wiki?curid=22003056", "title": "German Solar Industry Association", "text": "German Solar Industry Association\n\nThe German Solar Industry Association (ger: e.V. [BSW-Solar]) is the interest group of the German solar energy industry. \n\nWith more than 800 member companies BSW-Solar acts as an informant, intermediary and communication channel between the solar businesses and the German government. BSW-Solar represents the common commercial interests of businesses within the solar energy value chain and exerts decisive influence on creating and securing a suitable policy framework for stable growth, and thus on ensuring investment security throughout the solar industry. It is the association’s objective to establish solar energy as a firm pillar of a global energy supply.\n\nBSW-Solar assists its members with legal issues, informs about support policies, and secures stable framework conditions for the industry. The aim is to support the implementation of solar energy as a crucial part of the energy mix and to strengthen the competitiveness of the solar technologies compared to other energy sources. BSW-Solar also initiates internal market research and feasibility studies to provide first hand information to its members. It delivers international business contacts, publishes guidelines and provides policy advice on how to develop new solar markets. \n\nAccording to its own reference the association tries to establish a positive corporate- and in-dustry identity. BSW-Solar has initiated the largest national solar campaign “Week of the sun (ger: Woche der Sonne)” to enhance public awareness. The campaign is supported by the Federal Ministry for the Environment (ger: Bundesministerium für Umwelt, Naturschutz und Reaktorsicherheit) and distributes comprehensive informative material in many German cities and local communities as well as to manufactures, companies and installers.\n\n"}
{"id": "1453254", "url": "https://en.wikipedia.org/wiki?curid=1453254", "title": "Huron Wind", "text": "Huron Wind\n\nHuron Wind is the first commercial wind farm in Ontario. It is located in the village of Inverhuron, Ontario near Tiverton, and consists of 5 Vestas V80-1.8MW wind turbines. It is next to the Bruce Power Visitor Centre, within sight of the Bruce Nuclear Generating Station, and adjacent to the larger Enbridge Ontario Wind Farm. Groundbreaking was on July 11, 2002 and it was declared officially in service on December 1, 2002.\n\nThe project is owned by a consortium of BPC Generation Infrastructure Trust, Cameco Corporation and TransCanada PipeLines Limited.\n\n\n"}
{"id": "8164293", "url": "https://en.wikipedia.org/wiki?curid=8164293", "title": "Insecta Britannica Diptera", "text": "Insecta Britannica Diptera\n\nInsecta Britannica Diptera is a seminal work of entomology by Francis Walker. The work spans three volumes; a fourth volume was never published. Parts of the work were credited by Walker to Alexander Henry Haliday, including the characters and synoptic tables of the Empididae, Syrphidae, and Dolichopodidae and addenda and corrigenda intended for volume 4.\n\nThe precise dates of early entomological publications are frequently hard to establish.\n\n\nA digitized version of the work can be found at the Internet Archive:\n"}
{"id": "8343529", "url": "https://en.wikipedia.org/wiki?curid=8343529", "title": "Kurgan stelae", "text": "Kurgan stelae\n\nKurgan stelae (Mongolian: ; Russian: ; Ukrainian: \"stone babas\"; ) or Balbals ( \"balbal\", most probably from a Turkic word \"\" meaning \"ancestor\" or \"grandfather\" or the Mongolic word \"barimal\" which means \"handmade statue\") are anthropomorphic stone stelae, images cut from stone, installed atop, within or around kurgans (i.e. tumuli), in kurgan cemeteries, or in a double line extending from a kurgan. The stelae are also described as \"obelisks\" or \"statue menhirs\".\n\nSpanning more than three millennia, they are clearly the product of various cultures. The earliest are associated with the Pit Grave culture of the Pontic-Caspian steppe (and therefore with the Proto-Indo-Europeans according to the mainstream Kurgan hypothesis). The Iron Age specimens are identified with the Scythians and medieval examples with Turkic peoples.\n\nSuch stelae are found in large numbers in Southern Russia, Ukraine, Prussia, southern Siberia, Central Asia, Turkey and Mongolia.\n\nAnthropomorphic stelae were probably memorials to the honoured dead. They are found in the context of burials and funeral sanctuaries from the Eneolithic through to the Middle Ages. Ivanovovsky reported that Tarbagatai Torgouts (Kalmyks) revered kurgan obelisks in their country as images of their ancestors, and that when a bowl was held by the statue, it was to deposit a part of the ashes after the cremation of the deceased, and another part was laid under the base of the statue.\n\nWhen used architecturally, stelae could act as a system of stone fences, frequently surrounded by a moat, with sacrificial hearths, sometimes tiled on the inside.\n\nThe earliest anthropomorphic stelae date to the 4th millennium BC, and are associated with the early Bronze Age Yamna Horizon, in particular with the Kemi Oba culture of the Crimea and adjacent steppe region. Those in Ukraine number around three hundred, most of them very crude stone slabs with a simple schematic protruding head and a few features such as eyes or breasts carved into the stone. Some twenty specimens, known as statue menhirs, are more complex, featuring ornaments, weapons, human or animal figures.\n\nThe simple, early type of anthropomorphic stelae are also found in the Alpine region of Italy, southern France and Portugal. Examples have also been found in Bulgaria at Plachidol, Ezerovo, and Durankulak. The example illustrated above was found at Hamangia-Baia, Romania.\n\nThe distribution of later stelae is limited in the west by the Odessa district, Podolsk province, Galicia, Kalisz province, Prussia; in the south by Kacha River, Crimea; in the south-east by Kuma River in the Stavropol province and Kuban region; in the north by Minsk province and Oboyan district of the Kursk province (in some opinions even the Ryazan province), Ahtyr district in the Kharkov province, Voronezh province, Balash and Atkar districts in the Saratov province to the banks of Samara River in Buzuluk districts in the Samara province, in the east they are spread in the Kyrgyz (Kazakh) steppe to the banks of the Irtysh River and to Turkestan (near Issyk Kul, Tokmak district), then in upper courses of rivers Tom and Yenisei, in Sagai steppe in Mongolia (according to Potanin and Yadrintseva).\n\nThe Cimmerians of the early 1st millennium BC left a small number (about ten are known) of distinctive stone stelae. Another four or five \"deer stones\" dating to the same time are known from the northern Caucasus.\n\nFrom the 7th century BC, Scythian tribes began to dominate the Pontic steppe. They were in turn displaced by the Sarmatians from the 2nd century BC, except in Crimea, where they persisted for a few centuries longer. These peoples left carefully crafted stone stelae, with all features cut in deep relief.\n\nEarly Slavic stelae are again more primitive. There are some thirty sites of the middle Dniestr region where such anthropomorphic figures were found. The most famous of these is the Zbruch Idol (c. 10th century), a post measuring about 3 meters, with four faces under a single pointed hat (c.f. Svetovid). Boris Rybakov argued for identification of the faces with the gods Perun, Makosh, Lado and Veles.\n\nBronze Age anthropomorphic funerary stelae have been found in Saudi Arabia. There are similarities to the Kurgan type in the handling of the slab-like body with incised detail, though the treatment of the head is rather more realistic.\n\nThe anthropomorphic stelae so far found in Anatolia appear to post-date those of the Kemi Oba culture on the steppe and are presumed to derive from steppe types. A fragment of one was found in the earliest layer of deposition at Troy, known as Troy I.\n\nThirteen stone stelae, of a type similar to those of the Eurasian steppes, were found in 1998 in their original location at the centre of Hakkâri, a city in the south eastern corner of Turkey, and are now on display in the Van Museum. The stelae were carved on upright flagstone-like slabs measuring between 0.7 m to 3.10 m in height. The stones contain only one cut surface, upon which human figures have been chiseled. The theme of each stele reveals the fore view of an upper human body. Eleven of the stelae depict naked warriors with daggers, spears, and axes-masculine symbols of war. They always hold a drinking vessel made of skin in both hands. Two stelae contain female figures without arms. The earliest of these stelae are in the style of bas relief while the latest ones are in a linear style. They date from the 15th to the 11th century BC and may represent the rulers of the kingdom of Hubushkia, perhaps derived from a Eurasian steppe culture that had infiltrated into the Near East.\n\nEuropean traveler William of Rubruck mentioned them for the first time in the 13th century, seeing them on kurgans in the Cuman (Kipchak) country, he reported that Cumans installed these statues on tombs of their deceased. These statues are also mentioned in the 17th-century \"Large Drawing Book\", as markers for borders and roads, or orientation points. In the 18th century information about some kurgan stelae was collected by Pallas, Falk, Guldenshtedt, Zuev, Lepekhin, and in the first half of the 19th century by Klaprot, Duboa-de-Montpere and Spassky (\"Siberian obelisks\"). Count Aleksey Uvarov, in the 1869 ‘‘Works of the 1st Archeological Congress in Moscow\" (vol. 2), assembled all available at that time data about kurgan obelisks, and illustrated them with drawings of 44 statues.\n\nLater in the 19th century, data about these statues was gathered by A.I. Kelsiev, and in Siberia, Turkestan and Mongolia by Potanin, Pettsold, Poyarkov, Vasily Radlov, Ivanov, Adrianov and Yadrintsev, in Prussia by Lissauer and Gartman.\n\nThe Historical museum in Moscow has 30 specimens (in the halls and in the courtyard); others are in Kharkov, Odessa, Novocherkassk, etc. These are only a small part of examples dispersed in various regions of Eastern Europe, of which multitudes were already destroyed and used as construction material for buildings, fences, etc.\n\nIn the 1850s Piskarev, summing all information about kurgan obelisks available in literature, counted 649 items, mostly in Ekaterinoslav province (428), in Taganrog (54), in Crimea province (44), in Kharkov (43), in the Don Cossacks land (37), in Yenisei province, Siberia (12), in Poltava (5), in Stavropol (5), etc.; but many statues remained unknown to him.\n\nScythian balbals, later Kuman, commonly depict a warrior holding a drinking horn in their upraised right hand.\nMany also show a sword or dagger suspended on the warrior's belt.\n\nWriting about Altai kurgans, L.N. Gumilev states: \"To the east from the tombs are standing chains of balbals, crudely sculpted stones implanted in the ground. Number of balbals at the tombs I investigated varies from 0 to 51, but most often there are 3–4 balbals per tomb\". Similar numbers are also given by L. R. Kyzlasov. They are memorials to the feats of the deceased, every balbal represents an enemy killed by him. Many tombs have no balbals. Apparently, there are buried ashes of women and children.\n\nBalbals have two clearly distinct forms: conic and flat, with shaved top. Considering the evidence of Orkhon inscriptions that every balbal represented a certain person, such distinction cannot be by chance. Likely here is marked an important ethnographic attribute, a headdress. The steppe-dwellers up until present wear a conic 'malahai', and the Altaians wear flat round hats. The same forms of headdresses are recorded for the 8th century.\nAnother observation of Lev Gumilev: \"From the Tsaidam salt lakes to the Kül-tegin monument leads a three-kilometer chain of balbals. To our time survived 169 balbals, apparently there were more. Some balbals are given a crude likeness with men, indicated are hands, a hint of a belt. Along the moat toward the east runs a second chain of balbals, which gave I. Lisi a cause to suggest that they circled the fence wall of the monument. However, it is likely that it is another chain belonging to another deceased buried earlier\".\n\nSome kurgan obelisks are found still standing on kurgans, others were found buried in the slopes. Not always can be stated if they were contemporary with the kurgans on which they stand, existed earlier, or were carved later and lifted onto the kurgan. Kurgan obelisks are of sandstone, limestone, granite, etc. Their height is from 3.5 m to 0.7 m, but more often 1.5–2 m. Some of them are simple stone columns, with a rough image of a human face, on others the head (with the narrowed neck) is clearly depicted; in most cases not only the head is depicted, but also body, arms, and frequently both legs, and headdress, and dress. On more crude statues is impossible to dissern sex, but mostly it is expressed clearly: men are with moustaches (sometimes with beard, one bearded kurgan obelisk is in the courtyard of the Historical Museum in Moscow), in a costume with metal breastplates and belts, sometimes with a sword, etc.; women are with bared breasts, wearing peculiar headdresses, with girdles or necklaces on the neck, etc.\n\nOther obelisks show figures completely naked and usually only their head is covered, and legs are shod. Kurgan statues are sitting (frequently females), and standing (mostly males); in both cases the legs are not depicted. If the legs are depicted, they are either barefoot, or more often shoed, in high or low boots ('bashmaks'), sometimes with distinguishable trousers with ornaments. Many female kurgan obelisks (and some male) are naked above the belt, but below a belt and dress are visible, sometimes two dresses, one longer underneath, and another on the top, as a semi-'kaftan' or a short furcoat, with appliques and inserts (the ornaments of inserts consist of geometrical lines, double spirals, etc., or even cuirass). Others have stripes on the shoulders, many have two stripes (seldom three, or one wide across), plates (apparently, metal) on the breast attached to a belt or, more often, to two belts. On the belt sometimes is possible to distinguish a buckle in the middle or thongs hanging from it with sometimes attached bag, a round metal pocket mirror, knife, comb, sometimes also is shown (male statues) a dagger or a straight sword, a bow, a ‘kolchan’ (quiver), a hook, an axe. On the neck the men wear a metal band, women wear a necklace of beads or scales, sometimes even 2 or 3 are visible, some have a wide tape or a belt dropping from the necklace, ending with a 4-corner cloth. On the hands, wrists and shoulders (especially for nude figures) are bracelets (rings) and cuffs, in the ears, for women and men, are earrings, on the head (forehead) sometimes is an ornamental bandage or a diadem. The female braids can not always be distinguished from ribbons or bandages, they also are depicted for men. In some cases the male hat undoubtedly represents a small helmet (‘misyurka’), sometimes with crossing metal strips. The female headdress is more diverse, like a hat with curved brims, ‘bashlyk’, Kyrgyz (Kazakh) hat, etc.\n\nThe type of the face is not always depicted clearly. The vast majority of women join hands on the navel or at the bottom of the stomach, and hold a vessel, frequently cylindrical, like a cup or a glass. Sometimes it is so blurred that it can be taken for a folded scarf. One male figurine holds a bowl in the left hand, and a sword in the right; and another has hands simply joined together, without a bowl, one female figurine holds a ring, some hold a rhyton (drinking horn).\n\n\n"}
{"id": "10918148", "url": "https://en.wikipedia.org/wiki?curid=10918148", "title": "Lightning-prediction system", "text": "Lightning-prediction system\n\nA lightning prediction system is a type of lightning detection equipment that determines when atmospheric conditions likely to produce lightning strikes and sounds an alarm, warning those nearby that lightning is imminent and giving them the chance to find safety before the storm arrives in the area. Lightning protection systems are often installed in outdoor areas which are often congested with people, lack sufficient shelter, and are difficult to evacuate quickly (such as water parks, college campuses, and large swimming pool or athletic field complexes). These locations are particularly dangerous during lightning storms. Prediction systems are prone to false alarms as they respond to conditions that are not always attributed to a developing thunderstorm. Electric field data is typically used in conjunction with detection information to limit false positives. \n\nThe detection equipment is designed to constantly survey atmospheric electrical activity and potential for lightning occurrence via radar and other methods. Storms are scanned by radar to determine the degree of electrification and potential for lightning occurrence.\n\nThe method used by such systems includes the stationing of at least three receivers at known locations in order to triangulate their data. When any of the receivers detects a strong electrical disturbance, the location is shared with other receivers in the area for corroboration, and then (presuming the data has passed the filters), encoded and transmitted to a central facility and thereafter processed for deriving the position of the lightning strike. By detecting thunderstorm electrical fields, a track can be predicted to allow warnings as early as 30 minutes before lightning strikes the protected area. The system is synchronized with the U.S. Coast Guard LORAN navigation network, and includes various features which permit a more accurate analysis of lightning position.\n\nWhile some systems require manual remote activation of the siren from the central monitoring facility, others work automatically. These systems can also sound an \"All Clear\" tone when electrical activity in the monitored area has receded to safe levels. This feature eliminates both the need for skilled monitoring of the system as well as the chance for human error in interpreting the data.\n\n"}
{"id": "3409543", "url": "https://en.wikipedia.org/wiki?curid=3409543", "title": "Maria van der Hoeven", "text": "Maria van der Hoeven\n\nMaria Josephina Arnoldina van der Hoeven (born 13 September 1949) is a Dutch politician of the Christian Democratic Appeal (CDA) party. She served as Executive Director of the International Energy Agency from 1 September 2011 to 31 August 2015.\n\nVan der Hoeven served as a Member of the House of Representatives from 11 June 1991 to 22 July 2002, when she became Minister of Education, Culture and Science, serving until 22 February 2007 in the first, second and third Balkenende cabinets. She again returned to the House of Representatives for two short periods, after the general elections in 2003 and 2006, serving from 30 January to 27 May 2003 and from 30 November 2006 to 22 February 2007. She was Minister of Economic Affairs from 22 February 2007 to 14 October 2010 in the fourth Balkenende cabinet.\n\nAfter completing her secondary education she trained as a primary-school teacher in Maastricht. She went on to gain a secondary teaching certificate in English, after which she attended courses in higher management for non-profit organisations at the Institute of Social Sciences and business management at the Open University in Heerlen. From 1969 she taught at home economics schools and from 1971 at a junior secondary commercial school, where she later became a school counsellor. Until 1987 she was head of the Adult Commercial Vocational Training Centre in Maastricht, after which she served as the head of the Limburg Technology Centre until 1991.\n\nFrom 1985 to 1991, Van der Hoeven was a member of the municipal council of Maastricht. From 1991 to 2002, Member of the House of Representatives and Minister of Education, Culture and Science from 2002 to 2007. In 2005, she caused an uproar in a debate about the teaching of Intelligent Design in the country's schools. Van der Hoeven said that Charles Darwin's theories were incomplete and that new things had been discovered by proponents of intelligent design. The then Dutch Minister of Education later announced that she did not intend to introduce the creationist ideas into the school curricula but only wanted to confront their adherents with the supporters of the theory of evolution.\n\nMinister of Economic Affairs from 2007 to 2010, she has held a variety of social and cultural posts, including membership of the governing board of the Domstad Primary Teacher Training College in Utrecht and the Southern Dutch Opera Association, and membership of the ‘’t Vervolg’ theatre group.\n\nOn 11 March 2011, Van der Hoeven was appointed Executive Director of the International Energy Agency. Her opponents have voiced concerns that she lacks expertise on energy matters, while her supporters point out that her work as Minister of Economic Affairs included many energy issues, and that she has extensive contacts with major OPEC members. She took over from Nobuo Tanaka on 1 September 2011. On 1 September 2015, she was succeeded by Fatih Birol.\n\nSince October 2016, van der Hoeven has been Vice Chairwoman of the High-level Panel of the European Decarbonisation Pathways Initiative within the European Commission. In addition, he holds several board memberships. \n\n\nVan der Hoeven is married to Lou Buytendijk, who was diagnosed with Alzheimer's disease in 2005. Because of her husband's illness she is active in the Dutch Alzheimer's Foundation and currently serves as its president.\n\n\n\n"}
{"id": "15109232", "url": "https://en.wikipedia.org/wiki?curid=15109232", "title": "Martin Copley", "text": "Martin Copley\n\nMartin Copley (1940 – 30 July 2014) was a British-born Australian conservationist and philanthropist who established the Australian Wildlife Conservancy (AWC), an organisation which purchases and manages large areas of land, mainly former pastoral properties, as nature reserves (called 'sanctuaries') for the conservation of biodiversity.\n\nCopley was a British financier and insurance underwriter. He first visited Australia in 1966. In 1991 he purchased a property containing a large area of natural bushland at Chidlow, Western Australia, now the Karakamia Sanctuary, for conservation purposes, effectively founding what was to become the AWC. In 1994 he moved to Australia permanently. In 2001 the AWC became a public charitable organisation. Copley died of cancer on 30 July 2014. \n"}
{"id": "268367", "url": "https://en.wikipedia.org/wiki?curid=268367", "title": "Methoxide", "text": "Methoxide\n\nMethoxides are organic salts and the simplest alkoxides. Sodium methoxide and potassium methoxide have widespread use, though other metal-cation variants such as lithium methoxide, rubidium methoxide, and caesium methoxide exist as well.\n\nThe methoxide ion has the formula of CHO and is the conjugate base of methanol. It is a strong organic base, even stronger than the inorganic hydroxide ion. As such, methoxide solutions must be kept free of water; otherwise, the methoxide will remove a proton from a water molecule, yielding methanol and hydroxide.\n\nSodium methoxide, also called sodium methylate and sodium methanolate, is a white powder when pure. It is used as an initiator of an anionic addition polymerization with ethylene oxide, forming a polyether with high molecular weight. Both sodium methoxide and its counterpart prepared with potassium are frequently used as catalysts for commercial-scale production of biodiesel. In this process, vegetable oils or animal fats, which chemically are fatty acid triglycerides, are transesterified with methanol to give fatty acid methyl esters (FAMEs).\n\nSodium methoxide is produced on an industrial scale and available from a number of chemical companies.\nPotassium methoxide is commonly used as a catalyst for transesterification in the production of biodiesel.\n"}
{"id": "1663157", "url": "https://en.wikipedia.org/wiki?curid=1663157", "title": "N.V. Elmar", "text": "N.V. Elmar\n\nN.V. Elmar is the sole provider of electricity on the island of Aruba.\n\nN.V. Elmar is an abbreviation for the full company name in Dutch: \"Naamloze Vennootschap Electriciteit-Maatschappij Aruba\".\n\nThe \"Naamloze Vennootschap\", abbreviated as N.V. is the traditional Limited Liability Company with its capital divided into shares and may be compared with the Spanish S.A. and the U.S. Corporation.\n\n"}
{"id": "3620906", "url": "https://en.wikipedia.org/wiki?curid=3620906", "title": "Neon Museum", "text": "Neon Museum\n\nThe Neon Museum in Las Vegas, Nevada, United States, features signs from old casinos and other businesses displayed outdoors on 2.62 acres. The museum features a restored lobby shell from the defunct La Concha Motel as its visitor center, which officially opened on October 27, 2012.\n\nFor many years, the Young Electric Sign Company (YESCO) stored many of these old signs in their \"boneyard.\" The signs were slowly being destroyed by exposure to the elements.\n\nThe signs are considered by Las Vegas locals, business owners and government organizations to be not only artistically, but also historically, significant to the culture of the city. Each of the restored signs in the collection holds a story about who created it and why it is important.\n\nThe Neon Museum was founded in 1996 as a partnership between the Allied Arts Council of Southern Nevada and the City of Las Vegas. Today, it is an independent non-profit. Located on Las Vegas Boulevard and Bonanza, the Neon Museum includes the new Neon Boneyard Park, which is adjacent to the former YESCO Boneyard.\n\nThe impetus behind the museum was the loss of the iconic sign from The Sands; after it closed in 1995, there was no place to store the massive sign, and it was scrapped. To mark its official opening in November 1996, the Neon Museum restored and installed the \"Hacienda Horse & Rider\" sign at the intersection of Las Vegas Boulevard and Fremont Street. However, access to the collection was provided by appointment only. Annual attendance was approximately 12–20,000 during this time.\n\nIn 2005, the historic La Concha lobby was donated to the museum, which moved and reassembled the building north along Las Vegas Boulevard after cutting it into eight pieces. It now serves as the museum's visitors' center and headquarters. Although it cost nearly $3 million to move and restore the La Concha, the plans to open a museum became concrete after the donation of the building, drawing a number of public and private grants and donations. In total, approximately $6.5 million was raised for the visitors' center, headquarters, a new park, and restoration of 15 major signs.\n\nIn November 2009, the Neon Museum restored and installed the famous Silver Slipper sign across from its welcome center, and two more restored vintage signs were installed near the northern end of Las Vegas Boulevard to mark its designation as a National Scenic Byway. \n\nPaid public admission commenced on October 27, 2012, replacing the prior appointment-only basis. Attendance during the first year was 60,461, exceeding the early estimate of 45–50,000 visitors.\n\nAfter outgrowing its space in the former La Concha lobby shell, the museum moved its headquarters to old City Hall in 2016 and converted the offices into a museum store. In 2017, the museum purchased land for its first expansion since opening to the public in 2012. For its fifth anniversary, the Neon Museum offered free admission on October 28, 2017. In 2018, the Neon Museum administrative staff moved again to a space on the campus of the Las Vegas-Review Journal and opened a programming space there called Ne10 Studio.\n\nThe Neon Museum is located on Las Vegas Boulevard and Bonanza Road, South of Cashman Center and along the Las Vegas downtown museum corridor. The museum has exhibits in three main areas: restored and installed neon signs in both the Fremont Street area and along the Las Vegas Strip; and in the Neon Boneyard.\n\nThe Neon Museum maintains several restored signs throughout Downtown Las Vegas and along the Las Vegas Strip. The cost of restoring signs is estimated to range from $10,000 for small pieces to $100,000 for the largest signs.\n\nThe Neon Boneyard Park was installed in 2012, with \"NEON\" spelled out using letters shaped like those on signs for the Golden Nugget (\"N\"), Caesars Palace (\"E\"), Binion's Horseshoe (\"O\"), and Desert Inn (\"N\"). The letters are set on a grid inspired by the sign for The Sands, and the sign is decorated with stars like those from the Stardust and a starburst like the Welcome to Fabulous Las Vegas sign designed by Betty Willis.\n\nPieces in the boneyard include signage from the Stardust, Riviera, Desert Inn and Caesars Palace as well as many others. The Neon Museum also houses fiberglass sculptures including a giant skull from the Treasure Island Hotel and Casino among others.\n\nIn 2018, the Neon Museum began \"Brilliant!\", a 30-minute night show designed by Craig Winslow which uses multiple projectors to reanimate defunct signs, set to vintage and contemporary music.\n\n"}
{"id": "2103852", "url": "https://en.wikipedia.org/wiki?curid=2103852", "title": "Orthomode transducer", "text": "Orthomode transducer\n\nAn orthomode transducer (OMT) is a waveguide component. It is commonly referred to as a \"polarisation duplexer\". Orthomode transducers serve either to combine or to separate two orthogonally polarized microwave signal paths. One of the paths forms the uplink, which is transmitted over the same waveguide as the received signal path, or downlink path. Such a device may be part of a VSAT antenna feed or a terrestrial microwave radio feed; for example, OMTs are often used with a feed horn to isolate orthogonal polarizations of a signal and to transfer transmit and receive signals to different ports.\n\nFor VSAT modems the transmission and reception paths are at 90° to each other, or in other words, the signals are orthogonally polarized with respect to each other. This orthogonal shift between the two signal paths provides approximately an isolation of 40 dB in the K band and K band radio frequency bands.\n\nHence this device serves in an essential role as the junction element of the outdoor unit (ODU) of a VSAT modem. It protects the receiver front-end element (the low-noise block converter, LNB) from burn-out by the power of the output signal generated by the block up converter (BUC). The BUC is also connected to the feed horn through a wave guide port of the OMT junction device.\n\nOrthomode transducers are used in dual-polarized Very small aperture terminals (VSATs), in sparsely populated areas, radar antennas, radiometers, and communications links. They are usually connected to the antenna's down converter or LNB and to the High Power Amplifier (HPA) attached to a transmitting antenna.\n\nWherever there are two polarizations of radio signals \"(Horizontal and Vertical)\", the transmitted and received radio signal to and from the antenna are said to be “orthogonal”. This means that the modulation planes of the two radio signal waves are at 90 degrees angles to each other. The OMT device is used to separate two equal frequency signals, of high and low signal power.\nProtective separation is essential as the transmitter unit would seriously damage the very sensitive low micro-voltage (µV), front-end receiver amplifier unit at the antenna.\n\nThe transmission signal of the up-link, of relatively high power (1, 2,or 5 watts for common VSAT equipment) originating from BUC \"(block up converter)\", and the very low power received signal power (µV) coming from the antenna (aerial) to the LNB receiver unit, in this case are at an angle of 90° relative to each other, are both coupled together at the feed-horn focal-point of the Parabolic antenna. The device that unites both up-link and down-link paths, which are at 90° to each other, is known as an Orthogonal Mode Transducer \"OMT\".\n\nIn the VSAT K band of operation case, a typical \"OMT\" Orthomode Transducer provides a 40 dB isolation between each of the connected radio ports to the feed horn that faces the parabolic dish reflector (40 dB means that only 0.01% of the transmitter's output power is cross-fed into the receiver's wave guide port).\nThe port facing the parabolic reflector of the antenna is a circular polarizing port so that horizontal and vertical polarity coupling of inbound and outbound radio signal is easily achieved.\n\nThe 40 dB isolation provides essential protection to the very sensitive receiver amplifier against burn out from the relatively high-power signal of the transmitter unit. Further isolation may be obtained by means of selective radio frequency filtering to achieve an isolation of 100 dB (100 dB means that only a 10 fraction of the transmitter's output power is cross-fed into the wave guide port of the receiver).\n\nThe second image demonstrates two types of outdoor units, a 1-watt Hughes unit and a composite configuration of a 2-watt BUC/OMT/LNB Andrew, Swedish Microwave units.\n\nThe following images show a Portenseigne & Hirschmann K band configuration, that highlights the horizontal the vertical, and circular polarized wave-guide ports that join to the Feed-horn, the LNB or BUC elements of an outdoor unit.\n\nAn ortho-mode transducer is also a component commonly found on high capacity terrestrial microwave radio links. In this arrangement, two parabolic reflector dishes operate in a point to point microwave radio path (4 GHz to 85 GHz) with four radios, two mounted on each end. On each dish a T-shaped ortho-mode transducer is mounted at the rear of the feed, separating the signal from the feed into two separate radios, one operating in the horizontal polarity, and the other in the vertical polarity. This arrangement is used to increase the aggregate data throughput between two dishes on a point to point microwave path, or for fault-tolerance redundancy. Certain types of outdoor microwave radios have integrated orthomode transducers and operate in both polarities from a single radio unit, performing cross-polarization interference cancellation (XPIC) within the radio unit itself.\nAlternatively, the orthomode transducer may be built into the antenna, and allow connection of separate radios, or separate ports of the same radio, to the antenna.\nAn ortho-mode transducer can be modelled as a 4-port device, 2 of these (H and V) representing the single-polarization ports and the remaining (h, v) embodied by the degenerate modes in the dual-polarized port.\n\nThe scattering parameters can be gathered in a 4×4 scattering matrix formula_1, which is symmetrical for a reciprocal OMT (i.e. not including circulators, isolators or active components), thus leaving 10 independent terms for a general lossy device:\n\nformula_2\n\nOf these:\nAn ideal OMT exhibits perfect matching (null terms on the diagonal), unitary direct transmission terms and infinite XPD and isolation (null corresponding scattering parameters):\n\nformula_13\n\nCharacterization of a manufactured OMT (considered the device under test, DUT) is usually a delicate matter for both mechanical and theoretical reasons.\n\nConceptually, if an ideal OMT is available as part of the measurement setup, often named \"golden sample\", its dual-polarized port can be connected to its counterpart on the DUT, resulting in a 4-port equivalent device with 4 single-polarization ports. The ideal OMT splits the two polarizations at the dual-polarized port into two standard single-polarized ports and such arrangement allows the direct measurement of all the scattering parameters of the DUT (either by using a 4-port vector network analyzer (VNA) or a 2-port one with 2 single-polarized loads used in several combinations).\n\nSuch ideal setup is only prone to mechanical uncertainties related to the physical placement and alignment of the dual-polarized ports. A simple misalignment angle formula_14 introduces an artificial path from each polarization to the opposite proportional to formula_15. The phasorial combination of the leakage formula_10 (or formula_9) due to the XPDs of DUT and this artificial loss formula_15 is the actual external measured quantity. If, by proper phase recombination, the two contributions tend to cancel each other, the actual measured XPD can increase to infinity (possible only if formula_19), thus resulting in a huge estimation error.\n\nDepending on the expected XPD of the DUT, mechanical countermeasures should be introduced to guarantee that the artificial measurement uncertainty can be neglected.\n\nAny deviation from this ideal setup, however, introduces errors and uncertainties.\n\nIf a dual-polarization matched load is available in place of the ideal OMT, this allows 2×2 measurements from the single-polarization ports, yielding only 2 of the reflection terms (formula_3 and formula_3) and one IPI (formula_11). Other measurements aimed at gaining estimations of the other scattering parameters of the DUT involve the dual-polarized port and require additional components, such as dual-polarized to single-polarized transitions or tapers, which are often not matched on at least one of the two polarizations: this creates undesired reflections which propagate through the OMT and combine at the VNA ports thus preventing direct measurements. These issues add to mechanical factors and enhance uncertainties in the measurement procedure.\n\nDue to the increasing demand for high-capacity data links, the exploitation of dual-polarization has fostered research in design and characterization of OMTs to overcome the practical difficulties. The literature concerning OMT modelling and practical characterization consists of works both by academic organizations such as the National Research Council (Italy), Marche Polytechnic University and European Space Agency and likewise by industrial teams such as CommScope and Siae Microelettronica with immediate impact on products for modern dual-polarized telecommunication systems, for instance in terrestrial microwave backhauling.\n\n\n"}
{"id": "10431489", "url": "https://en.wikipedia.org/wiki?curid=10431489", "title": "Paresh Narayan", "text": "Paresh Narayan\n\nProfessor Paresh Kumar Narayan (born 1977), is an academic of Fiji Indian origin, who was Australia's youngest Professor of Finance and is now the chair of finance at Deakin University in Melbourne.\n\nThe eldest of three brothers, the others being Nilesh Narayan and Nitesh Narayan, Narayan was born in a farming community in Navua. He attended Vashist Muni Primary School, in Navua, then Mahatma Gandhi Memorial primary and secondary schools and finally, Suva Muslim College. On completing high school, he enrolled at the University of the South Pacific (USP) on a Government scholarship in a Bachelor of Arts program, majoring in economics and geography. After graduating, he worked as a tutor in economics at USP, while pursuing post- graduate studies in this field. In 1998, he was awarded a Japanese Government scholarship for a master's degree in development studies. He graduated in 1999 from USP with a gold medal for the having the best thesis. He then worked as a planning officer with the Fiji Islands Trade and Investment Bureau while he continued to teach at USP and the Central Queensland University international campus in Suva.\n\nIn 2001, he was awarded a Monash graduate school scholarship to study in Australia for three years. Professor Narayan completed his PhD in 18 months. His doctorate thesis, An Econometric Model of Tourism Demand and a Computable General Equilibrium Analysis of the Impact of Tourism: The Case of Fiji Islands was assessed to be the most outstanding work, earning him the Mollie Hollman Medal in 2004 at Monash University.\n\nIn addition, he received a Monash University postgraduate travel grant in 2002; the Australian Population Association Borrie Prize for an essay titled, Determinants of Female Fertility in Taiwan, 1966–2001: Evidence from Cointegration and Variance Decomposition Analysis, a postgraduate Publications Award in 2003 and numerous research grants.\n\nParesh is married to Seema Dhar. Seema is completed her PhD thesis in the area of current account sustainability for OECD countries at Monash University. Seema is also a gold medalist in Economics. Paresh and Seema have two sons. \n\nOn completion of his doctoral studies, Professor Paresh was appointed a lecturer at the Griffith Business School, on the Gold Coast, Australia, and was promoted to senior lecturer in 2005. In 2006, he became associate professor.\n\nParesh Kumar Narayan is presently an Alfred Deakin Professor at the Deakin Business School. He is the Director of the Centre for Economics & Financial Econometrics Research at Deakin University.\n\nHe has published more than 130 papers in renowned journals all over the world.\n\nHe has written chapters in books like South Asia in the Era of Globalisation: Trade, Industrialisation and Welfare; Economic Impact of Fiji's Sugar Industry, CGE Modelling and Behaviour of J-Curve.\n\nHe is an advisor to Econtech, one of Australia's leading economic consultancy firms, and two years ago he won the prestigious Excellence in Research Award, which he received from the Emerald Institute, a leading publishing institute in United Kingdom.\n\nIn 2014, Professor Narayan received the Scopus Young Researcher award for the best three authors in Australia in the Social Science category under the age of forty. In 2015, Professor Narayan was awarded the Mahatma Gandhi Pravasi Samman Award for non-resident Indians who have made substantial contributions to the profession, including contributions to public policy. In 2015 he also received the Gold Medal and Citation by the Indian Econometric Society.\n\nNarayan blamed the ousted Qarase government and its failure to implement macroeconomic management and stability for the post coup economic decline in Fiji. Earlier he had criticized the Qarase Government's 2007 budget as being \"economically sick\".\n\nProfessor Narayan has been a consultant to a number of esteemed institutions, such as the Asian Development Bank, the United Nations, the Commonwealth Secretariat, the International labour Organisation, and AusAID amongst others.\n\nProfessor Narayan is a co Editor-in-Chief of Economic Modelling, Associate Editor of Finance Research Letters and Studies in Economics & Finance, Subject Editor of Journal of International Financial Markets Institutions and Money and Guest Editor of the Journal of Banking & Finance and Energy Economics.\n"}
{"id": "15743602", "url": "https://en.wikipedia.org/wiki?curid=15743602", "title": "Pellet baskets", "text": "Pellet baskets\n\nA pellet basket is a small metal basket that sits inside a woodstove or fireplace and holds wood pellets. Pellet baskets allow a person to heat their home using existing stoves or fireplaces; thus eliminating the need for electric and natural gas. Pellet baskets are lower-cost alternatives to pellet stoves.\n\nBaskets range in size from 10 to 26 inches and hold from 10 to 30 lbs. of pellet fuel. A fully loaded basket can burn unattended for 2 to 12 hours.\n\nNo fan is needed to circulate air within the stove because there is enough air flow through the basket itself to allow pellets to burn with a flame. Wood pellets can be added on top of the hot pellets with a small metal scoop when needed. They may smolder for a few minutes until they ignite. Readjust the air settings for a safe working temperature.\n\nRemove ashes from the bottom of the pellet basket once daily for maximum air flow. Always remember proper wood stove or fireplace maintenance. This includes proper chimney care in order to keep creosote buildup in check. Remove the pellet basket from the wood stove or fireplace to remove ashes.\n\n"}
{"id": "21595073", "url": "https://en.wikipedia.org/wiki?curid=21595073", "title": "Photovoltaic thermal hybrid solar collector", "text": "Photovoltaic thermal hybrid solar collector\n\nPhotovoltaic thermal hybrid solar collectors, sometimes known as hybrid PV/T systems or PVT, are systems that convert solar radiation into thermal and electrical energy. These systems combine a solar cell, which converts sunlight into electricity, with a solar thermal collector, which captures the remaining energy and removes waste heat from the PV module. and thus be more overall energy efficient than solar photovoltaic (PV) or solar thermal alone. A significant amount of research has gone into developing PVT technology since the 1970s.\n\nPhotovoltaic cells suffer from a drop in efficiency with the rise in temperature due to increased resistance. Such systems can be engineered to carry heat away from the PV cells thereby cooling the cells and thus improving their efficiency by lowering resistance. Although this is an effective method, it causes the thermal component to under-perform compared to a solar thermal collector.\n\nA number of PV/T collectors in different categories are commercially available and can be divided into the following categories:\n\n\nThe basic water-cooled design uses a channel to direct fluid flow using piping of various materials or plates attached to the back of a PV module. The fluid flow arrangement through the cooling element will determine which systems the panels are most suited to.\n\nIn a standard fluid-based system, a working fluid, typically water, glycol or mineral oil is then piped through these pipes or plate chillers. The heat from the PV cells is conducted through the metal and absorbed by the working fluid (presuming that the working fluid is cooler than the operating temperature of the cells). In closed-loop systems this heat is either exhausted (to cool it), or transferred at a heat exchanger, where it flows to its application. In open-loop systems, this heat is used, or exhausted before the fluid returns to the PV cells. It is also possible to disperse nanoparticles in the liquid to create a liquid filter for PV/T applications. The basic advantage of this type of split configuration is that the thermal collector and the photovoltaic collector can operate at different temperatures.\n\nThe basic air-cooled design uses a hollow, conductive metal housing to mount the photo-voltaic (PV) panels.\nHeat is radiated from the panels into the enclosed space, where the air is either circulated into a building HVAC system to recapture heat energy, or rises and is vented from the top of the structure.\n\nWhile energy transfer to air is not as efficient as a liquid collector, the infrastructure required has lower cost and complexity; basically a shallow metal box. Placement of PV panels can be vertical or angled.\n\nA concentrator system has the advantage to reduce the amount of photovoltaic (PV) cells needed, such that somewhat more expensive and efficient multi-junction photovoltaic cells can be used that will maximize the ratio of produced high-value electrical power versus lower-value thermal power. A major limitation of high-concentrator (i.e. HCPV and HCPVT) systems is that they maintain their advantage over conventional c-Si/mc-Si collectors only in regions that remain consistently free of atmospheric aerosol contaminants (e.g. light clouds, smog, etc.). Concentrator system performance is especially degraded because 1) radiation is reflected and scattered outside of the small (often less than 1°-2°) acceptance angle of the collection optics, and 2) absorption of specific components of the solar spectrum causes one or more series junctions within the MJ cells to underperform.\n\nConcentrator systems also require reliable control systems to accurately track the sun and to protect the PV cells from damaging over-temperature conditions. Under ideal conditions, about 75% of the sun's power directly incident upon such systems can be gathered as electricity and heat. For more details, see the discussion of CPVT within the article for concentrated photovoltaics.\n\n"}
{"id": "1610513", "url": "https://en.wikipedia.org/wiki?curid=1610513", "title": "Polyunsaturated fat", "text": "Polyunsaturated fat\n\nPolyunsaturated fats are fats in which the constituent hydrocarbon chain possesses two or more carbon–carbon double bonds. Polyunsaturated fat can be found mostly in nuts, seeds, fish, seed oils, and oysters. \"Unsaturated\" refers to the fact that the molecules contain less than the maximum amount of hydrogen (if there were no double bonds). These materials exist as \"cis\" or \"trans\" isomers depending on the geometry of the double bond.\n\nSaturated fats have hydrocarbon chains which can be most readily aligned. The hydrocarbon chains in trans fats align more readily than those in cis fats, but less well than those in saturated fats. In general, this means that the melting points of fats increase from cis to trans unsaturated and then to saturated. See the section about the chemical structure of fats for more information.\n\nThe position of the carbon-carbon double bonds in carboxylic acid chains in fats is designated by Greek letters. The carbon atom closest to the carboxyl group is the \"alpha\" carbon, the next carbon is the \"beta\" carbon and so on. In fatty acids the carbon atom of the methyl group at the end of the hydrocarbon chain is called the \"omega\" carbon because \"omega\" is the last letter of the Greek alphabet. Omega-3 fatty acids have a double bond three carbons away from the methyl carbon, whereas omega-6 fatty acids have a double bond six carbons away from the methyl carbon. The illustration below shows the omega-6 fatty acid, linoleic acid.\n\nWhile it is the \"nutritional\" aspects of polyunsaturated fats that are generally of greatest interest, these materials also have non-food applications. Drying oils, which polymerize on exposure to oxygen to form solid films, are polyunsaturated fats. The most common ones are linseed (flax seed) oil, tung oil, poppy seed oil, perilla oil, and walnut oil. These oils are used to make paints and varnishes.\n\nIn preliminary research, omega-3 fatty acids in algal oil, fish oil, fish and seafood have been shown to lower the risk of heart attacks. Other preliminary research indicates that omega-6 fatty acids in sunflower oil and safflower oil may also reduce the risk of cardiovascular disease.\n\nAmong omega-3 fatty acids, neither long-chain nor short-chain forms were consistently associated with breast cancer risk. High levels of docosahexaenoic acid (DHA), however, the most abundant omega-3 polyunsaturated fatty acid in erythrocyte (red blood cell) membranes, were associated with a reduced risk of breast cancer. The DHA obtained through the consumption of polyunsaturated fatty acids is positively associated with cognitive and behavioral performance. In addition DHA is vital for the grey matter structure of the human brain, as well as retinal stimulation and neurotransmission.\n\nDietary intake of polyunsaturated fatty acids is under preliminary research to assess the risk of developing amyotrophic lateral sclerosis (motor neurone disease).\n\nThe importance of the ratio of omega-6/omega-3 essential fatty acids as established by comparative studies shows an omega-6:omega-3 ratio under 4:1 may influence health.\n\nContrary to conventional advice, an evaluation of evidence from 1966-1973 pertaining to the health impacts of replacing dietary saturated fat with linoleic acid found that participants in the group doing so had \"increased\" rates of death from all causes, coronary heart disease, and cardiovascular disease. Although this evaluation was disputed by many scientists, it fueled debate over worldwide dietary advice to substitute polyunsaturated fats for saturated fats.\n\nPolyunsaturated fat supplementation does not decrease the incidence of pregnancy-related disorders, such as hypertension or preeclampsia, but may increase the length of gestation slightly and decreased the incidence of early premature births.\n\nExpert panels in the United States and Europe recommend that pregnant and lactating women consume higher amounts of polyunsaturated fats than the general population to enhance the DHA status of the fetus and newborn.\n\nResults from observational clinical trials on polyunsaturated fat intake and cancer have been inconsistent and vary by numerous factors of cancer incidence, including gender and genetic risk. Some studies have shown associations between higher intakes and/or blood levels of polyunsaturated fat omega-3s and a decreased risk of certain cancers, including breast and colorectal cancer, while other studies found no associations with cancer risk.\n\nFood sources of polyunsaturated fats include:\n"}
{"id": "22745487", "url": "https://en.wikipedia.org/wiki?curid=22745487", "title": "Precursor (physics)", "text": "Precursor (physics)\n\nPrecursors are characteristic wave patterns caused by dispersion of an impulse's frequency components as it propagates through a medium. Classically, precursors precede the main signal, although in certain situations they may also follow it. Precursor phenomena exist for all types of waves, as their appearance is only predicated on the prominence of dispersion effects in a given mode of wave propagation. This non-specificity has been confirmed by the observation of precursor patterns in different types of electromagnetic radiation (microwaves, visible light, and terahertz radiation) as well as in fluid surface waves and seismic waves.\n\nPrecursors were first theoretically predicted in 1914 by Arnold Sommerfeld for the case of electromagnetic radiation propagating through a neutral dielectric in a region of normal dispersion. Sommerfeld's work was expanded in the following years by Léon Brillouin, who applied the saddle point approximation to compute the integrals involved. However, it was not until 1969 that precursors were first experimentally confirmed for the case of microwaves propagating in a waveguide, and much of the experimental work observing precursors in other types of waves has only been done since the year 2000. This experimental lag is mainly due to the fact that in many situations, precursors have a much smaller amplitude than the signals that give rise to them (a baseline figure given by Brillouin is six orders of magnitude smaller). As a result, experimental confirmations could only be done after technology became available to detect precursors.\n\nAs a dispersive phenomenon, the amplitude at any distance and time of a precursor wave propagating in one dimension can be expressed by the Fourier integral\n\nwhere formula_2 is the Fourier transform of the initial impulse and the complex exponential formula_3 represents the individual component wavelets summed in the integral. To account for the effects of dispersion, the phase of the exponential must include the dispersion relation (here, the formula_4 factor) for the particular medium in which the wave is propagating. \n\nThe integral above can only be solved in closed form when idealized assumptions are made about the initial impulse and the dispersion relation, as in Sommerfeld's derivation below. In most realistic cases, numerical integration is required to compute the integral.\n\nAssuming the initial impulse takes the form of a sinusoid turned on abruptly at time formula_5,\n\nthen we can write the general-form integral given in the previous section as \n\nFor simplicity, we assume the frequencies involved are all in a range of normal dispersion for the medium, and we let the dispersion relation take the form \n\nwhere formula_9, formula_10 being the number of atomic oscillators in the medium, formula_11 and formula_12 the charge and mass of each one, formula_13 the natural frequency of the oscillators, and formula_14 the vacuum permittivity. This yields the integral\n\nTo solve this integral, we first express the time in terms of the retarded time formula_16, which is necessary to ensure that the solution does not violate causality by propagating faster than formula_17. We also treat formula_18 as large and ignore the formula_19 term in deference to the second-order formula_20 term. Lastly, we substitute formula_21, getting\n\nRewriting this as \n\nand making the substitutions\n\nallows the integral to be transformed into\n\nwhere formula_26 is simply a dummy variable, and, finally\n\nwhere formula_28 is a Bessel function of the first kind. This solution, which is an oscillatory function with amplitude and period that both increase with increasing time, is characteristic of a particular type of precursor known as the Sommerfeld precursor.\n\nThe stationary phase approximation can be used to analyze the form of precursor waves without solving the general-form integral given in the Basic Theory section above. The stationary phase approximation states that for any speed of wave propagation formula_29 determined from any distance formula_30 and time formula_31, the dominant frequency formula_32 of the precursor is the frequency whose group velocity equals formula_29:\n\nTherefore, one can determine the approximate period of a precursor waveform at a particular distance and time by calculating the period of the frequency component that would arrive at that distance and time based on its group velocity. In a region of normal dispersion, high-frequency components have a faster group velocity than low-frequency ones, so the front of the precursor should have a period corresponding to that of the highest-frequency component of the original impulse; with increasing time, components with lower and lower frequencies arrive, so the period of the precursor becomes longer and longer until the lowest-frequency component arrives. As more and more components arrive, the amplitude of the precursor also increases. The particular type of precursor characterized by increasing period and amplitude is known as the high-frequency Sommerfeld precursor.\n\nIn a region of anomalous dispersion, where low-frequency components have faster group velocities than high-frequency ones, the opposite of the above situation occurs: the onset of the precursor is characterized by a long period, and the period of the signal decreases with time. This type of precursor is called a low-frequency Sommerfeld precursor.\n\nIn certain situations of wave propagation (for instance, fluid surface waves), two or more frequency components may have the same group velocity for particular ranges of frequency; this is typically accompanied by a local extremum in the group velocity curve. This means that for certain values of time and distance, the precursor waveform will consist of a superposition of both low- and high-frequency Sommerfeld precursors. Any local extrema only correspond to single frequencies, so at these points there will be a contribution from a precursor signal with a constant period; this is known as a Brillouin precursor.\n"}
{"id": "43263353", "url": "https://en.wikipedia.org/wiki?curid=43263353", "title": "Presidential Task Force on Power", "text": "Presidential Task Force on Power\n\nThe Presidential Task Force on Power (PTFP) was established by the President of the Federal Republic of Nigeria, Dr. Goodluck Ebele Jonathan's administration, in June 2010, to drive the implementation of the reform of Nigeria's power sector. It brings together all the agencies that have a role to play in removing legal and regulatory obstacles to private sector investment in the power industry. It also has the mandate to monitor the planning and execution of various short-term projects in generation, transmission, distribution and fuel-to-power that are critical to meeting the stated service delivery targets of the power reform roadmap.\n\nThe PTFP collaborates closely with various ministries and agencies that have specific contributions to the reform process, including the Federal Ministry of Power, the Federal Ministry of Finance, Ministry of Petroleum Resources, the Bureau of Public Enterprises (BPE), the Nigerian Electricity Regulatory Commission (NERC), the Nigerian National Petroleum Corporation (NNPC), the Bureau of Public Procurement, National Gas Company Limited (NGC) and the Power Holding Company of Nigeria (PHCN) to mention a few.\n\nPresident of the Federal Republic of Nigeria, Dr. Goodluck Ebele Jonathan, reconstituted the PTFP Board, with Engr. Reynolds Bekinbo Dagogo-Jack as Chairman, on September 5, 2012.\n\nThe Presidential Task Force Board of Directors is charged with setting and maintaining the direction of the Task Force. They are responsible for implementing PTFP's mandate while providing overall leadership and its strategic direction. All Board Members have been instrumental in setting policy for the Task Force as well as ensuring it has all the necessary resources and capabilities to achieve its objective.\n"}
{"id": "2211437", "url": "https://en.wikipedia.org/wiki?curid=2211437", "title": "Pusher trailer", "text": "Pusher trailer\n\nA pusher trailer is a device attached to the rear of a vehicle or bike that provides force to assist the vehicle. \n\nElectric pusher trailers use energy stored in a battery, typically of lithium ion or sealed lead acid chemistry, to provide power. Two wheel and one wheel designs are common.\n\nGas powered pusher trailers typically employ a two or four stroke internal combustion engine to provide power.\n\nPusher trailers are gasoline, diesel or electric fueled trailers with a traditional internal combustion engine (petroleum engines) and transmission which can be hitched up to battery electric vehicles and run from the cockpit to give the vehicle increased range.\n\nThe trailer provides ground traction through the wheels to push the trailer forward, and by default, the electric vehicle as well. In this way, a trip beyond the normal range of the EV can be undertaken without stopping for recharging.\n\nSome types of articulated bus have the engine (and propulsion) in the rear section.\n\n\n"}
{"id": "21600882", "url": "https://en.wikipedia.org/wiki?curid=21600882", "title": "Radiator (heating)", "text": "Radiator (heating)\n\nRadiators and convectors are heat exchangers designed to transfer thermal energy from one medium to another for the purpose of space heating. \n\nDenison Olmsted of New Haven, Connecticut, appears to have been the earliest person to use the term 'radiator' to mean a heating appliance in an 1834 patent for a stove with a heat exchanger which then radiated heat. In the patent he wrote that his invention was \"a peculiar kind of apparatus, which I call a radiator\". The heating radiator was invented by Franz San Galli in 1855, a Prussian-born Russian businessman living in St. Petersburg. In the late 1800s, companies, such as the American Radiator Company, promoted cast iron radiators over previous fabricated steel designs in order to lower costs and expand the market.\n\nIn practice, the term \"radiator\" refers to any of a number of devices in which a fluid circulates through exposed pipes (often with fins or other means of increasing surface area), notwithstanding that such devices tend to transfer heat mainly by convection and might logically be called convectors.\n\nThe term convection heater or \"convector\" refers to a class of devices in which the source of heat is not directly exposed. As domestic safety and the supply from water heaters keeps temperatures relatively low, radiation is inefficient in comparison to convection. Convection heaters also work differently to electric radiators in that they disperse heat differently.\n\nThe international standard for energy efficient consumer products Energy Star recommends placing heat-resistant reflectors between radiators and exterior walls to help retain heat in a room.\n\nA hot-water radiator consists of a sealed hollow metal container filled with hot water by gravity feed, a pressure pump, or convection. As it gives out heat, the hot water cools and sinks to the bottom of the radiator and is forced out of a pipe at the other end. Anti-hammer devices are often installed to prevent or minimize knocking in hot water radiator pipes.\n\nTraditional cast iron radiators are no longer common in new construction, replaced mostly with forced hot water baseboard style radiators. They consist of copper pipes which have aluminium fins to increase their surface area. These conduction boiler systems use conduction to transfer heat from the water into the metal radiators or convectors.\n\nThe radiators are designed to heat the air in the room using convection to transfer heat from the radiators to the surrounding air. They do this by drawing cool air in at the bottom, warming the air as it passes over the radiator fins, and discharging the heated air at the top. This sets up convective loops of air movement within a room. If the register is blocked either from above or below, this air movement is prevented, and the heater will not work. Baseboard heating systems are sometimes fitted with moveable covers to allow the resident to fine-tune heating by room, much like air registers in a central air system.\n\nSteam has the advantage of flowing through the pipes under its own pressure without the need for pumping. For this reason, it was adopted earlier, before electric motors and pumps became available. Steam is also far easier to distribute than hot water throughout large, tall buildings like skyscrapers. However, the higher temperatures at which steam systems operate make them inherently less efficient, as unwanted heat loss is inevitably greater.\n\nSteam pipes and radiators are prone to producing banging sounds called steam hammer. The bang is created when some of the steam condenses into water in a horizontal section of the steam piping. Subsequently, steam picks up the water, forms a \"slug\" and hurls it at high velocity into a pipe fitting, creating a loud hammering noise and greatly stressing the pipe. This condition is usually caused by a poor condensate drainage strategy and is often caused by buildings settling and the resultant pooling of condensate in pipes and radiators that no longer tilt slightly back towards the boiler.\n\nA fan-assisted radiator contains a heat exchanger fed by hot water from the heating system. A thermostatic switch energises an electric fan which blows air over the heat exchanger to circulate it in a room. Its advantages are small relative size and even distribution of heat. Disadvantages are fan noise and the need for both a source of heat and a separate electrical supply.\n\nAlso known as \"radiant heat\", underfloor heating uses a network of pipes, tubing or heating cables, buried in or attached beneath a floor to allow heat to rise into the room. Best results are achieved with conductive flooring materials such as tile. The large surface area of such room-sized radiators allows them to be kept just a few degrees above desired room temperature, minimizing convection. Underfloor heating is more expensive in new construction than less efficient systems. It also is generally difficult to retrofit into existing buildings.\n\nThe Roman hypocaust employed a similar principle of operation.\n\nSkirting-board radiators are a form of heating which involves placing radiators inside a skirting board. \nHot water is piped though the system, usually taken directly from the central heating system.\n\nSimilar in configuration to forced hot water baseboard—low profile units running along the base of a wall with a central heating element surrounded by radiating fins—electric baseboard heaters are inexpensive to produce and install. They offer instant heat and great reliability, but may be more or less cost-effective relative to other forms of heat depending on electricity prices.\n\nElectrically-powered portable radiators come in two basic forms:\n\n"}
{"id": "42177042", "url": "https://en.wikipedia.org/wiki?curid=42177042", "title": "Red rosin paper", "text": "Red rosin paper\n\nRed rosin paper (red building paper, brown rosin paper, slip sheet paper, rosin-sized sheathing paper, and building paper) is a 100% recycled heavy duty felt paper used in construction such as underlayment under flooring and siding. The name \"rosin-sized sheathing paper\", commonly used to describe the material, comes from the rosin used in the paper, the process of sizing it to add the rosin, and its use by builders. \"Alum-rosin size was invented by Moritz Friedrich Illig in Germany in 1807...\" and is known to have been used as a building paper by 1850.\n\nIn building construction red rosin paper is used to reduce air and moisture flow through a wall or floor, create a \"slip sheet\" so different materials can slip by each other as they expand and contract, keep dust from working down through a floor, minimize squeaking, and sometimes as part of built-up roofs. Rosin paper is also temporarily used to protect a work site during construction. Rosin paper may have a polyurethane coating to improve moisture resistance and tearing. It is not acid free particularly containing abietic acid and comes in many size rolls up to wide.\n\nPapers from the Middle Ages were sized with gelatine but the invention of the paper-making machine in the late 18th century demanded a better size resulting in the creation of the rosin size.\n\nA modern equivalent would be Synthetic Resin Bonded Paper (SRBP).\n\n"}
{"id": "362677", "url": "https://en.wikipedia.org/wiki?curid=362677", "title": "Rock candy", "text": "Rock candy\n\nRock candy or sugar candy (in British English), also called rock sugar, is a type of confection composed of relatively large sugar crystals. This candy is formed by allowing a supersaturated solution of sugar and water to crystallize onto a surface suitable for crystal nucleation, such as a string, stick, or plain granulated sugar. Heating the water before adding the sugar allows more sugar to dissolve thus producing larger crystals. Crystals form after 6–7 days. Food coloring may be added to the mixture to produce colored candy.\n\nEtymologically, \"sugar candy\" derives from late 13th century English, in the meaning \"crystallized sugar,\" from Old French \"çucre candi\" \"sugar candy,\" ultimately from Arabic \"qandi\", from Persian \"qand\" \"cane sugar,\" probably from Sanskrit \"khanda\" \"piece (of sugar),\" perhaps from Dravidian (compare Tamil \"kantu\" \"candy,\" \"kattu\" \"to harden, condense\"). The sense gradually broadened (especially in the U.S.A.) to mean by the late 19th century \"any confection having sugar as its basis.\" In Britain these are sweets, and \"candy\" tends to be restricted to sweets made only from boiled sugar and striped in bright colors.\n\nThe modern American term \"rock candy\" (referring to brittle large natural sugar crystals) should not be confused with the British term rock (referring to an amorphous and opaque boiled sugar product, initially hard but then chewy at mouth temperature).\n\nCandied sugar has its origins in Iran. Islamic writers in the first half of the 9th century described the production of candy sugar, where crystals were grown result of cooling supersaturated sugar solutions. In order to accelerate crystallization, confectioners later learned to immerse small twigs in the solution for the crystals to grow on. The sugar solution was colored with cochineal and indigo and scented with ambergris or flower essence.\n\nRock candy is often dissolved in tea. It is an important part of the tea culture of East Frisia, where a lump of rock sugar is placed at the bottom of the cup. Rock candy consumed with tea is also the most common and popular way of drinking tea in Iran. In Iran it is called \"nabat\", and the most popular nabat is saffron.\n\nIn China, it is used to sweeten Chrysanthemum tea as well as Cantonese dessert soups and the liquor \"baijiu\". In some Chinese provinces, it is used as a part of traditional Chinese medicine. It is a common ingredient in Chinese cooking, and many households have rock candy available to marinate meats and add to stir fry. Rock candy is also regarded as having medicinal properties and is used to prepare food such as yao shan. In less modern times, rock sugar was a luxury only for the wealthy.\n\nRock candy is a common ingredient in Tamil cuisine, particularly in the Sri Lankan city of Jaffna.\n\nIn the Friesland province of the Netherlands, bits of rock candy are baked in the luxury white bread \"Fryske Sûkerbôle\".\n\nIn Mexico it is used during the Day of the Dead to make sugar skulls, often highly decorated. Sugar skulls are given to children so that they will not fear death; they are also offered to the dead.\n\nIn the US, rock candy comes in many colors and flavors, and is slightly hard to find, due to it being considered old-fashioned.\n\nMisri (, , ) refers to crystallized sugar lumps, and a type of confectionery mineral, which has its origins in India and Persia, also known as rock sugar elsewhere. It is used in India as a type of candy, or used to sweeten milk or tea.\n\nIn Hinduism, mishri may be offered to a deity as \"bhog\" and distributed as \"prasad\". The god Krishna is said to be fond of \"makkhan\" (butter) and misri. In many devotional songs written in Brajbhoomi in praise of Krishna, the words \"makkhan\" and \"misri\" are often used in combination. In Northern Karnataka people serve mishri along with water to visitors in the Summer season.\n\nAmong Indian misri dishes are \"mishri-mawa\" (\"kalakand\"), \"mishri-peda\", which are more commonly eaten in Northern-Western India, Uttar Pradesh, Delhi, Rajasthan, Gujarat, Punjab, Orissa, North coastal of Andhra Pradesh and many other states and parts of India.\n\nThe Ghantewala Halwai of Delhi, who started his career by selling \"Misari mawa\" in 1790 is famous for Misarimawa and sells forty varieties of sweets made from Misari.\n\nRock and rye is a term used both for alcoholic liqueurs and cocktails using rye whiskey and rock candy, as well as for non-alcoholic beverages made in imitation thereof, such as the \"Rock & Rye\" flavor of soda pop made by Faygo.\n\n\n"}
{"id": "12589148", "url": "https://en.wikipedia.org/wiki?curid=12589148", "title": "S. David Freeman", "text": "S. David Freeman\n\nS. David Freeman (born January 14, 1926) is an American engineer, attorney, and author, born in Chattanooga, Tennessee, who has had many key roles in energy policy.\n\nFreeman has been termed an \"eco-pioneer\" for his environmentally-oriented leadership of the Sacramento Municipal Utility District (SMUD). He had done a similar job at the Tennessee Valley Authority (TVA), after President Jimmy Carter appointed him to head the TVA board in 1977, changing the TVA focus from growth to conservation. Freeman decided to stop construction on several nuclear projects that may have contributed to several rate increases with TVA.Today, TVA’s generation portfolio is 37% nuclear, 24% coal, 20% natural gas, 9% hydro, 3% wind + solar and 7% energy efficiency. He also headed other major energy organizations, including the New York Power Authority and the Los Angeles Department of Water and Power (LADWP).\n\nFreeman became general manager of the Sacramento Municipal Utility District in 1990. He has said that SMUD was an embarrassment at that time, and the district was \"reeling from two decades of rate hikes, construction cost overruns, operating failures, equipment outages, worker injuries, poor morale and management scandals\". Freeman left SMUD in 1994 and as of 2012, SMUD is considered a model of efficiency, service and innovation. The turnaround began in 1989 when the people of Sacramento voted to close the Rancho Seco Nuclear Generating Station.\n\nAccording to Freeman, since opening in 1971, Rancho Seco had \"suffered dozens of emergencies, shutdowns, releases of radioactive material and accidents\". Freed of this costly, unreliable and dangerous nuclear power station, SMUD went on to earn a worldwide reputation for its affordable, clean, renewable energy programs.The largest power source for Sacramento is the 500 MW Cosumnes gas plant. \n\nFreeman has authored several books, including \"Energy: The New Era\" (1974), \"Winning Our Energy Independence\" (2007), An \"All-Electric America: A Climate Solution and the Hopeful Future (2016)\" and an autobiography, \"The Green Cowboy: An Energetic Life\" (2016). Other books that contain his works include \"Oral history of the Tennessee Valley Authority: Interview with S. David Freeman, March 30, 1984\" and \"Speeches by S. David Freeman\" (1997).\n\nIn \"Winning Our Energy Independence\", S. David Freeman says we have the renewable energy we need to wean ourselves from the Three Poisons: foreign oil, dirty coal and dangerous nuclear power.\n\n"}
{"id": "11271138", "url": "https://en.wikipedia.org/wiki?curid=11271138", "title": "Sewage pumping", "text": "Sewage pumping\n\nSmall-scale sewage pumping is normally done by a submersible pump.\n\nThis became popular in the early 1960s, when a guide-rail system was developed to lift the submersible pump out of the pump station for repair, and ended the dirty and sometimes dangerous task of sending people into the sewage or wet pit. Growth of the submersible pump for sewage pumping since has been dramatic, as an increasing number of specifiers and developers learned of their advantages.\n\nThree classes of submersible pumps exist:\n\nSubmersible pumps are normally used in a packaged pump station where drainage by gravity is not possible.\n\nVertical type sewage pumps have also been used for many years. They have the motor above the floor so work on the motor can be done without entering the sewage pit.\n\n"}
{"id": "51550431", "url": "https://en.wikipedia.org/wiki?curid=51550431", "title": "Singida Wind Power Station", "text": "Singida Wind Power Station\n\nSingida Wind Power Station, also Singida Wind Farm, is a potential wind-powered electricity power station, under construction in Tanzania.\n\nThe power station is located approximately , by road, south-east of Singida, the capital and largest city in the Singida Region of central Tanzania, about equidistant between Singida and Puma. This about , by road, west of Dar es Salaam, the commercial capital of Tanzania. The coordinates of the power station are 04°53'58.0\"S, 34°47'22.0\"E (Latitude:-4.899450; Longitude:34.789432).\n\nThe power station is privately owned by Wind East Africa Limited. The power generated is projected at 100 megawatts and will be sold to the Tanzanian power company Tanesco for integration into the national power grid. The station construction cost is budgeted at US$285 million, and the station is expected to be ready in December 2017.\n\nThe power station is owned by a consortium that consists of Six Telecoms, a Tanzanian company, Aldwych International Limited of the United Kingdom, and the International Finance Corporation, based in Washington, D. C. Wind East Africa Limited is the special purpose vehicle formed by the consortium to develop, own, and operate the power station.\n\n\n"}
{"id": "23715165", "url": "https://en.wikipedia.org/wiki?curid=23715165", "title": "Solid unbleached board", "text": "Solid unbleached board\n\nSolid unbleached board, also known as SUB, is a grade of paperboard typically made of unbleached chemical pulp. Most often it comes with two to three layers of mineral or synthetic pigment coating on the top and one layer on the reverse side. Recycled fibres are sometimes used to replace the unbleached chemical pulp.\n\nThe main end use for this type of board is for packaging of frozen or chilled food, beverage carriers, detergent cereals, shoes, toys and others.\n"}
{"id": "57293402", "url": "https://en.wikipedia.org/wiki?curid=57293402", "title": "Storm Emma (2018)", "text": "Storm Emma (2018)\n\nStorm Emma, also called Ulrike, was an event that was part of the 2018 Great Britain and Ireland cold wave and the 2017–18 European Windstorm Season. The Low Pressure System interacted with Cold air that had been firmly established over Western Europe for a few days, leading to heavy snow falls of up to . It also brought a renewed push of cold air to much of the United Kingdom with temperatures falling as low as in Nairnshire . The worst affected areas were South west England and Southern Wales where an estimated 16 people died in connection with the storm.\n\nAlthough the maximum snow fall was , most places affected reported a general total of . Snowfall was reported along the coast of Italy and French Riviera for the first time since 2010. Snow also fell in Barcelona, a rare occurrence for the region.\n\nThroughout the period of the storm, the temperatures were very suppressed with Tredegar recording a daytime high of however more generally there was a maximum of .\n"}
{"id": "2957972", "url": "https://en.wikipedia.org/wiki?curid=2957972", "title": "Tall oil", "text": "Tall oil\n\nTall oil, also called \"liquid rosin\" or tallol, is a viscous yellow-black odorous liquid obtained as a by-product of the Kraft process of wood pulp manufacture when pulping mainly coniferous trees. The name originated as an anglicization of the Swedish \"tallolja\" (\"pine oil\"). Tall oil is the third largest chemical by-product in a Kraft mill after lignin and hemicellulose; the yield of crude tall oil from the process is in the range of 30–50 kg / ton pulp. It may contribute to 1.0–1.5% of the mill's revenue if not used internally.\n\nIn the Kraft Process, high alkalinity and temperature converts the esters and carboxylic acids in rosin into soluble sodium soaps of lignin, rosin, and fatty acids. The spent cooking liquor is called weak black liquor and is about 15% dry content. The black liquor is concentrated in a multiple effect evaporator and after the first stage the black liquor is about 20–30%. At this stage it is called intermediate liquor. Normally the soaps start to float in the storage tank for the weak or intermediate liquors and are skimmed off and collected. A good soap skimming operation reduces the soap content of the black liquor down to 0.2–0.4% w/w of the dry residue. The collected soap is called \"raw rosin soap\" or \"rosinate\". The raw rosin soap is then allowed to settle or is centrifuged to release as much as possible of the entrained black liquor. The soap goes then to the \"acidulator\" where it is heated and acidified with sulfuric acid to produce \"crude tall oil\" (CTO).\n\nThe soap skimming and acidulator operation can be improved by addition of flocculants. A flocculant will shorten the separation time and give a cleaner soap with lower viscosity. This makes the acidulator run smoother as well.\n\nMost pines give a soap yield of 5–25 kg/ton pulp, while Scots pine gives 20–50 kg/ton. Scots pine grown in northern Scandinavia give a yield of even more than 50 kg/ton. Globally about 2 mill ton/year of CTO are refined.\n\nThe composition of crude tall oil varies a great deal, depending on the type of wood used. A common quality measure for tall oil is acid number. With pure pines it is possible to have acid numbers in the range 160–165, while mills using a mix of softwoods and hardwoods might give acid numbers in the range of 125–135.\n\nNormally crude tall oil contains rosins (which contains resin acids (mainly abietic acid and its isomers), fatty acids (mainly palmitic acid, oleic acid and linoleic acid) and fatty alcohols, unsaponifiable sterols (5–10%), some sterols, and other alkyl hydrocarbon derivates.\n\nBy fractional distillation \"tall oil rosin\" is obtained, with rosin content reduced to 10–35%. By further reduction of the rosin content to 1–10%, \"tall oil fatty acid\" (\"TOFA\") can be obtained, which is cheap, consists mostly of oleic acid, and is a source of volatile fatty acids.\n\nThe tall oil rosin finds use as a component of adhesives, rubbers, and inks, and as an emulsifier. The pitch is used as a binder in cement, an adhesive, and an emulsifier for asphalt.\n\nTOFA is a low-cost and vegetarian lifestyle-friendly alternative to tallow fatty acids for production of soaps and lubricants. When esterified with pentaerythritol, it is used as a compound of adhesives and oil-based varnishes. When reacted with amines, polyamidoamines are produced which may be used as epoxy resin curing agents . \n\nTall oil is also used in oil drilling as a component of drilling fluids.\n"}
{"id": "4085687", "url": "https://en.wikipedia.org/wiki?curid=4085687", "title": "Thermoplastic elastomer", "text": "Thermoplastic elastomer\n\nThermoplastic elastomers (TPE), sometimes referred to as thermoplastic rubbers, are a class of copolymers or a physical mix of polymers (usually a plastic and a rubber) which consist of materials with both thermoplastic and elastomeric properties. While most elastomers are thermosets, thermoplastics are in contrast relatively easy to use in manufacturing, for example, by injection molding. Thermoplastic elastomers show advantages typical of both rubbery materials and plastic materials. The benefit of using thermoplastic elastomers is the ability to stretch to moderate elongations and return to its near original shape creating a longer life and better physical range than other materials. The principal difference between thermoset elastomers and thermoplastic elastomers is the type of cross-linking bond in their structures. In fact, crosslinking is a critical structural factor which imparts high elastic properties.\n\nThere are six generic classes of commercial TPEs (designations acc. to ISO 18064):\n\nExamples of TPE materials that come from block copolymers group are amongst others CAWITON® , THERMOLAST® K, THERMOLAST® M, Arnitel, Hytrel, Dryflex, Mediprene, Kraton, Pibiflex, Sofprene, and Laprene. Out of these styrenic block copolymers (TPE-s) are CAWITON®, THERMOLAST® K, THERMOLAST® M, Sofprene, Dryflex and Laprene. Desmopan or Elastollan are examples of Thermoplastic polyurethanes (TPU). Santoprene, Termoton, Solprene, THERMOLAST® V, Vegaprene, or Forprene are examples of TPV materials. Examples of Thermoplastic olefin elastomers (TPO) compound are For-Tec E or Engage. Ninjaflex used for 3D printing.\n\nIn order to qualify as a thermoplastic elastomer, a material must have these three essential characteristics:\n\nIt was not until the 1950s, when thermoplastic polyurethane polymers became available, that TPE became a commercial reality. During the 1960s styrene block copolymer became available, and in the 1970s a wide range of TPEs came on the scene. The worldwide usage of TPEs (680,000 tons/year in 1990) is growing at about nine percent per year. The styrene-butadiene materials possess a two-phase microstructure due to incompatibility between the polystyrene and polybutadiene blocks, the former separating into spheres or rods depending on the exact composition. With low polystyrene content, the material is elastomeric with the properties of the polybutadiene predominating. Generally they offer a much wider range of properties than conventional cross-linked rubbers because the composition can vary to suit final construction goals.\n\nBlock copolymers are interesting because they can \"microphase separate\" to form periodic nanostructures, as in the styrene-butadiene-styrene (SBS) block copolymer shown at right. The polymer is known as Kraton and is used for shoe soles and adhesives. Owing to the microfine structure, the transmission electron microscope or TEM was needed to examine the structure. The butadiene matrix was stained with osmium tetroxide to provide contrast in the image. The material was made by living polymerization so that the blocks are almost monodisperse, so helping to create a very regular microstructure. The molecular weight of the polystyrene blocks in the main picture is 102,000; the inset picture has a molecular weight of 91,000, producing slightly smaller domains. The spacing between domains has been confirmed by small-angle X-ray scattering, a technique which gives information about microstructure.\nSince most polymers are incompatible with one another, forming a block polymer will usually result in phase separation, and the principle has been widely exploited since the introduction of the SBS block polymers, especially where one of the block is highly crystalline. One exception to the rule of incompatibility is the material Noryl, where polystyrene and polyphenylene oxide or PPO form a continuous blend with one another.\n\nOther TPEs have crystalline domains where one kind of block co-crystallizes with other block in adjacent chains, such as in copolyester rubbers, achieving the same effect as in the SBS block polymers. Depending on the block length, the domains are generally more stable than the latter owing to the higher crystal melting point. That point determines the processing temperatures needed to shape the material, as well as the ultimate service use temperatures of the product. Such materials include Hytrel, a polyester-polyether copolymer and Pebax, a nylon or polyamide-polyether copolymer.\n\nTPE materials have the potential to be recyclable since they can be molded, extruded and reused like plastics, but they have typical elastic properties of rubbers which are not recyclable owing to their thermosetting characteristics. They can also be ground up and turned into 3D printing filament with a recyclebot. TPE also require little or no compounding, with no need to add reinforcing agents, stabilizers or cure systems. Hence, batch-to-batch variations in weighting and metering components are absent, leading to improved consistency in both raw materials and fabricated articles. Depending on the environment, TPEs have outstanding thermal properties and material stability when exposed to a broad range of temperatures and non-polar materials. TPEs consume less energy to produce, can be colored easily by most dyes, and allow economical quality control.\n\nThe two most important manufacturing methods with TPEs are extrusion and injection molding. TPEs can now be 3D printed and have been shown to be economically advantageous to make products using distributed manufacturing. Compression molding is seldom, if ever, used. Fabrication via injection molding is extremely rapid and highly economical. Both the equipment and methods normally used for the extrusion or injection molding of a conventional thermoplastic are generally suitable for TPEs. TPEs can also be processed by blow molding, melt calendaring, thermoforming, and heat welding.\n\nTPEs are used where conventional elastomers cannot provide the range of physical properties needed in the product. These materials find large application in the automotive sector and in household appliances sector. In 2014 the world market for TPEs reached a volume of ca. 16.7 billion US dollars. About 40% of all TPE products are used in the manufacturing of vehicles. For instance copolyester TPEs are used in snowmobile tracks where stiffness and abrasion resistance are at a premium. Thermoplastic olefins (TPO) are increasingly used as a roofing material. TPEs are also widely used for catheters where nylon block copolymers offer a range of softness ideal for patients. Thermoplastic silicone and olefin blends are used for extrusion of glass run and dynamic weatherstripping car profiles. Styrene block copolymers are used in shoe soles for their ease of processing, and widely as adhesives. Owing to their unrivaled abilities in two-component injection molding to various thermoplastic substrates, engineered TPS materials also cover a broad range of technical applications ranging from automotive market to consumer and medical products. Examples of those are soft grip surfaces, design elements, back-lit switches and surfaces, as well as sealings, gaskets, or damping elements. TPE is commonly used to make suspension bushings for automotive performance applications because of its greater resistance to deformation when compared to regular rubber bushings. Thermoplastics have experienced growth in the heating, ventilation, and air conditioning (HVAC) industry due to the function, cost effectiveness and adaptability to modify plastic resins into a variety of covers, fans and housings. TPE may also be used in medical devices and is also finding more and more uses as an electrical cable jacket and inner insulation. You'll also be able to find TPE used in some headphone cables.\n\n"}
{"id": "4952755", "url": "https://en.wikipedia.org/wiki?curid=4952755", "title": "ZM-87", "text": "ZM-87\n\nThe ZM-87 Portable Laser Disturber is a Chinese electro-optic countermeasure neodymium laser device. The ZM-87 was primarily intended to blind humans but was also reported to damage the photo-electric elements in laser rangefinders, videocameras and missile seekers. Roughly 22 of the devices were produced by the company Norinco before production ceased in 2000 as a result of the 1995 United Nations Protocol on Blinding Laser Weapons ban.\n\nThe ZM-87 is notable as one of only a few laser weapons ever produced. Controversy has also surrounded the possible recent use of the weapon by Russian, Chinese, and North Korean armed forces.\n\nA battery supplies a portable electric energy converter which through a cable feeds a beam emitter long mounted on a tripod. It has a gunsight. It resembles a heavy machine gun. A portable variant was also produced, resembling a QBZ-95 bullpup assault rifle with a telescopic sight attached.\n\nDevelopment of the ZM-87 began in the late 1980s. The device was first publicly revealed at a defense exhibition in the Philippines in May 1995 and, soon after, in Abu Dhabi, where the weapon gained publicity. In October 1995, during the United Nations Convention on Certain Conventional Weapons, Protocol IV, banning blinding laser weapons, was passed, making the ZM-87 illegal. In April 1997 a United States Naval officer sustained a retinal injury consistent with exposure to this sort of laser fired from the Russian freighter \"Kapitan Man\" at a Canadian Forces helicopter in which he was a passenger. This became known as the Strait of Juan de Fuca laser incident. By December 2000, known production of ZM-87 has ceased. However, in 2003 North Korea is reported to have used the ZM-87 to illuminate two United States Army Apache helicopters.\n\n\n"}
