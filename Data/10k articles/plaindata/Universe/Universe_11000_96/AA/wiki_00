{"id": "19292103", "url": "https://en.wikipedia.org/wiki?curid=19292103", "title": "Albert J. Meyer (economist)", "text": "Albert J. Meyer (economist)\n\nAlbert Julius Meyer (14 May 1919 – 31 October 1983) was an American economist who taught at Harvard University for 28 years. Meyers specialized in the economies of the Middle East.\n\nMeyer was born in Hawarden, Iowa. obtained his bachelors and master's degrees at the University of California at Los Angeles. In 1947 he received his doctorate from Johns Hopkins University. Among his seminal papers was \"Entrepreneurship the missing link in the Arab states?\" In 1955, he started teaching at Harvard. While at Harvard he produced two major books:\n\nMyers was chief of mission for the Special U.S. Economic Mission to Saudi Arabia, June 13–30, 1962, that led to closer U.S. ties with the kingdom.\n\nMeyers was married to Anne Avantaggio and they had three children. He died of leukemia in Boston and his papers are maintained at the Harvard University Archives.\n\n"}
{"id": "50938864", "url": "https://en.wikipedia.org/wiki?curid=50938864", "title": "Albert Road gas holder", "text": "Albert Road gas holder\n\nThe Albert Road gas holder in New Barnet, north London, is a disused gas holder on the site of the former New Barnet Gas Works with a capacity of 2,000,000 cubic feet of gas. It was built for the Barnet & District Gas and Water Company and came into use in 1934. It was decommissioned around 2009.\n\nThe holder is to the east of Albert Road on the north part of the former New Barnet Gas Works site adjacent to Victoria Park, Barnet.\n\nThe site was opened for the East Barnet Gas and Water Company (formed 1866) which in 1872 took over two local gas companies and became the Barnet & District Gas and Water Company. The company was nationalised in 1949 and became part of the Eastern Gas Board. Some time after 2000, Asda supermarkets acquired the site but plans for housing and a supermarket were not approved and they sold the area to the social housing provider One Housing.\n\nGas holders have been present on the southern part of the site since at least the 1890s but the current, and last remaining, holder was not brought into use until 1934. It is column-guided with four concentric lifts and a capacity of 2,000,000 cubic feet of gas. After the exploitation of North Sea Gas began in the 1960s the gas works on the rest of the site was demolished and a high pressure pipeline installed with the remaining holder retained just for low pressure storage to cope with diurnal peak demand on the system. It was decommissioned around 2009, and does not form part of the infrastructure network for the supply of gas.\n"}
{"id": "31618101", "url": "https://en.wikipedia.org/wiki?curid=31618101", "title": "Ap Lei Chau Power Station", "text": "Ap Lei Chau Power Station\n\nThe Ap Lei Chau Power Station () was an oil-fired power station in Ap Lei Chau, Hong Kong.\n\nThe power station was commissioned in 1968 and was the largest power station in British Hong Kong at that time. The power station was decommissioned starting in 1984 and eventually closed in 1989. Its generators were then moved to Lamma Power Station. The area around the power station is now surrounded by the South Horizons residential area.\n\nThe power station provided electricity to the western area of Hong Kong.\n\n"}
{"id": "18393379", "url": "https://en.wikipedia.org/wiki?curid=18393379", "title": "Aygaz", "text": "Aygaz\n\nAygaz A.Ş. is Turkey’s 14th largest industrial organization according to the listing by Istanbul Chamber of Commerce. It is majority-owned by the Koç Group, and partly floated on the Istanbul Stock Exchange.\n\nAygaz brings together the energy firms of the Koç Group outside the oil and refinery sector. Its subsidiary portfolio ranges from LPG trade to electricity and natural gas. Having been the leader of the LPG sector in Turkey since its foundation in 1961 , Aygaz manufactures and sells LPG devices in addition to distributing LPG as auto gas, cylinder gas and bulk gas. Serving customers in 81 provinces with more than 3,800 cylinder gas dealers and auto gas stations, Aygaz also exports LPG devices to 22 countries in Europe, Africa and the Middle East. Aygaz cylinders are delivered to over 100,000 homes every day, while over one million vehicles travel with Aygaz’s auto gas product, Aygaz Euro LPG+, every day. With more than 15.000 personnel working at the headquarters, facilities and dealerships.\n\nIn 2011, the consolidated sales revenue of Aygaz reached 5,456 million TL and its net profit raised to 380 million TL.\n\n\nList of companies of Turkey \n\n"}
{"id": "19984125", "url": "https://en.wikipedia.org/wiki?curid=19984125", "title": "Borsod Power Plant", "text": "Borsod Power Plant\n\nThe Borsod Power Plant is one of Hungary's largest biomass power plants having an installed heat capacity of 140 MW and electric capacity of 40 MW.\n\n"}
{"id": "867770", "url": "https://en.wikipedia.org/wiki?curid=867770", "title": "Brabus", "text": "Brabus\n\nBRABUS GmbH, founded 1977 in Bottrop (Ruhr Area), Germany, is a high-performance aftermarket tuning company which specializes in Mercedes-Benz, Smart and Maybach vehicles. Brabus became the largest Mercedes tuner, other than Mercedes-AMG which became a DaimlerChrysler affiliate in the 1990s.\n\nThe company began with Bodo Buschmann wanting to customize his cars, but discovered most existing customizers could not meet his thought or requirement, so he decided to start his own brand. After Buschmann had decided to start his own brand, BRABUS GmbH was originally registered in 1977 at Germany with Buschmann's friend Klaus Brackman because German law required a company must be established with at least two people. The company name was derived from the first three characters of the founders' surnames (Brackman, Buschmann). Following the company's foundation, Brackman did not show interests in modifying vehicles or running a business, so he sold his company shares to Buschmann for 100 euros.\n\nIn 2012, BRABUS GmbH had started building its third factory in Bottrop.\n\nBrabus's primary focus is to achieve maximum engine performance through the increase of horsepower and torque. Customers can either buy cars from Brabus, or send in their car to be customized and/or overhauled. Customers ordering a car directly involves Brabus purchasing a particular model from Mercedes and then modifying it according to the customer's requests. Brabus is known for providing expensive tuning and custom solutions.\n\nBrabus also offers cosmetic modifications including low profile spoilers, bodykits, carbon fiber splitters, and multi-piece alloy wheels. Other upgrades include racing LSDs, open racing exhaust systems, twelve-piston disc brakes, and engine remapping. Customers can also have complete engine overhauls, or have new crate engines from AMG modified for them.\n\nBrabus engines range from small K4 blocks for SLK roadsters and CLK-Class to the twin-turbo blocks for the S-Class. The company also provides improvements to the interior from custom upholstery, gauges, shiftknobs, pedals, and trim to various electronics such as wider LCD screens for the Maybach.\n\n\nThe company has built special editions of Mercedes-Benz, Maybach, and Smart vehicles.\n\n\n\n\n\n\n\n\n\nThe Brabus K8 performance kit for the SL model (SL55) includes modifications to the supercharger incorporating a custom vibration damper and pulley, high-performance metal catalysts, and an auxiliary circulation pump with opposing radiator. Power is increased from a standard to at 6,200 rpm. The maximum torque is at 3,000 rpm. The roadster is claimed reach in 4.1 seconds, passes after 13.6 seconds and reaches an electronically limited top speed of .\n\n\n\n\n\n\n\nThe 2012 Brabus Rocket 800 is powered by a V12 engine which provides 800 hp and 1,420Nm of torque, which has been limited to 1,100Nm. The Brabus Rocket accelerates from 0 to 100 km/h (62 mph)in 3.7 seconds, 23.8 seconds to 300 km/h (186 mph) and has a top speed of 370 km/h (230 mph).\n\n"}
{"id": "7410474", "url": "https://en.wikipedia.org/wiki?curid=7410474", "title": "Buchonia", "text": "Buchonia\n\nBuchonia is a region in Hesse, a state of Germany, where one of the first forestry planning systems was developed by Georg Ludwig Hartig (1764-1837). It was called \"Flächenfachwerk\". He also wrote in 1791 \"Anweisung zur Holzzucht für Förster\" \nGeorg Ludwig Hartig: \"\"Anweisung zur Holzzucht für Förster\" \"\n"}
{"id": "2065712", "url": "https://en.wikipedia.org/wiki?curid=2065712", "title": "Chirotechnology", "text": "Chirotechnology\n\nChirotechnology in materials science is the chemistry and technology of production and separation of enantiomers.\n"}
{"id": "2861460", "url": "https://en.wikipedia.org/wiki?curid=2861460", "title": "Cooling curve", "text": "Cooling curve\n\nA cooling curve is a line graph that represents the change of phase of matter, typically from a gas to a solid or a liquid to a solid. The independent variable (X-axis) is time and the dependent variable (Y-axis) is temperature. Below is an example of a cooling curve used in castings.\n\nThe initial point of the graph is the starting temperature of the matter, here noted as the \"pouring temperature\". When the phase change occurs there is a \"thermal arrest\", that is the temperature stays constant. This is because the matter has more internal energy as a liquid or gas than in the state that it is cooling to. The amount of energy required for a phase change is known as latent heat. The \"cooling rate\" is the slope of the cooling curve at any point. \n"}
{"id": "15360756", "url": "https://en.wikipedia.org/wiki?curid=15360756", "title": "Dvaravati sila", "text": "Dvaravati sila\n\nThe Dvararvati Sila is a type of Sila or coral stone obtained from the Gomati river (Gomti River) in Dvaraka. Dvaraka is located in the Jamnagar District of Gujarat at the mouth of the Gomati River as it debouches into the Gulf of Kutch. The city lies in the westernmost part of India. In ancient Sanskrit literature Dvaraka was called Dvarawati and was rated as one of the seven most prehistoric cities in the country. Thus, the Sila or the stone obtained at the mouth of the Gomati river is called the Dvaravati Sila and is worshipped. \n\nDvaraka Silas are coral with chakra (wheel) markings and the chakra-mark is the most distinguishing feature of these stones, and hence they are called ‘chakrankita-sila’. \n\nAniconic representation of God is by a symbol rather than an image. Indian art overwhelmingly prefers the iconic image, but some aniconism does occur in folk worship, in early Hinduism in the form of Vishnu's Saligrama Sila (murthi) (fossil stone), Dvaravati Sila (coral stone), Govardhana Sila (stone from the Govardhan hill), etc. They have solar significance, and their use in worship is very common among all sects of Vaishnavites of Hindu religion.\n\nThe legendary city of Dvaraka in Hindu history was the dwelling place of Krishna. Dwarka or Dvaraka is derived from 'Dwar', a door, and in ancient times its flourishing port was considered to be the gateway to the main land. As 'Ka' means 'Brahma' meaning gateway to Moksha (salvation). It is also called Dwarkamati and Dwarkavati or Dvaravati. The famous Nageswar Jyotirlinga near Dwaraka is made of a large Dwaraka Sila.\n\nThe Hindu scriptures prescribe that stones obtained from Dvaraka only for worship even though geologically it may be found in other places too.\n\nThere are several schools of thought among the learned acharyas of the Vaishnava sect on the worship of this Sila. This Sila is worshipped, along with or independent of Sila (murthi) or Saligrama Sila, in some parts of the country (among Vaishnavites of Saurashtra, Bengal and Maharashtra; the Madhva sect in Karnataka), particularly in the Vaishnava tradition. It is worshipped along with the Sila| Saligrama Sila (stone) since scriptures consider it auspicious to do so. According to Skanda Purana, wherever Dvaraka Sila is placed in front of the Sila|, every class of magnificence goes on increasing unlimitedly. Skanda Purana also says that one who daily worships Dvaraka Sila along with twelve Saligrama Silas will be honored even in Vaikuntha. \n\nThe chakra-mark is the most distinguishing feature of the Dvaravati stones, and hence they are called \"chakrankita-sila\". According to Garuda Purana, there are twelve varieties of this stone, owing to the number of chakras (wheels), colours and forms (Sanskrit sloka in this regard states:‘dasadha cha prabhinnas ta varnakrti-vibhedatah’). When there is only one chakra, the stone is called Devesa; when there are two chakras, it is Sudarshana; three chakras represent the deity Ananta. When there are four chakras, the stone is Janardana. Vasudeva is represented by the stone having five chakras, Pradyumna by six chakras, Bala-bhadra by seven, Purushottama by eight, Nava-vyuha by nine, Dasavatara by ten, Aniruddha by eleven and Dvadastma by twelve. Nava-vyuha represents the collection of nine forms of Vishnu: Vasudeva, Samkarshana, Pradyumna, Aniruddha, Narayana, Hayagriva, Vishnu, Nrsimha and Varaha. The first four forms are well known as ‘chatur-vyuha’. The twelve major forms of Vishnu are derived from these nine forms, according to the Tantra siddhanta, a division of Pancharatra. \n\nPrahlada Samhita, quoted in Salagrama-pariksha (by Anupasimha) gives the first few names differently. The Dvaravati Sila with only one chakra is called Sudarsana, with two chakras 'Lakshmi-narayana' and with three chkras 'Trivikrama'. The rest of the names are the same as given above. The name Ananta is given to stones which have more than twelve chakras. The name for Dasavatara in the above list is given here as 'Dasamurti'. When the chakras are more than twelve, only even numbered chakras are to be preferred, according to Galava-smrtir.\n\nThese Silas also have distinct personalities like the Saligrama Sila that are identified by their size, colure, texture markings; these are explained below.\n\nI.Sudarshana: one chakra - salvation \n\nII.Lakshmi-Narayana: two chakras- salvation\nIII.Trivikrama: three chakras - freedom from the fear of births and deaths\nIV.Janardana: four chakras - fulfillment of desires \n\nV.Vasudeva: five chakras - obtainment of prosperity and elimination of enemies \n\nVI.Pradyumna: six chakras - wealth and lustre \n\nVII.Baladeva: seven chakras - continuation of progeny and celebrity \n\nVIII.Purushottama: eight chakras - satisfaction of all that one aspires for \n\nIX.Navavyuha (the collection of nine forms of Vishnu): nine chakras - rewards, which are difficult, even for the gods to obtain \n\nX.Dashmurti (the ten incarnations of Vishnu): ten chakras - sovereignty and prosperity \n\nXI.Aniruddha: eleven chakras - lordship \n\nXII.Dvadasatmaka: twelve chakras - final emancipation \n\nXIII.Ananta: more than twelve chakras - fulfills one’s desires (only even numbered chakras are to be preferred)\n\nThe colour and the shape of the sila is said to give following effects.\n\nØ White stones are considered most suitable for worship and will make for a worldly prosperity in all aspects and spiritual welfare \n\nØ Dark (blue-black) stones forebode death \n\nØ Tawny ones cause anxiety\n\nØ Multi-coloured ones bring about disease and sorrow Ø Yellow ones take away wealth\n\nØ Smoke coloured ones produce loss of wealth\n\nØ Blue stones will bring about obstacles to any undertaking \n\nØ Round in shape or square auspicious \n\nØ Triangular or uneven in shape inauspicious\n\n"}
{"id": "19509955", "url": "https://en.wikipedia.org/wiki?curid=19509955", "title": "Earth in culture", "text": "Earth in culture\n\nThe cultural perspective on Earth, or the world, varies by society and time period. Religious beliefs often include a creation belief as well as personification in the form of a deity. The exploration of the world has modified many of the perceptions of the planet, resulting in a viewpoint of a globally integrated ecosystem. Unlike the remainder of the planets in the Solar System, mankind didn't perceive the Earth as a planet until the sixteenth century.\n\nUnlike the other planets in the Solar System, in English, Earth does not directly share a name with an ancient Roman deity. The name \"Earth\" derives from the eighth century Anglo-Saxon word \"erda\", which means ground or soil. It became \"eorthe\" later, and then \"erthe\" in Middle English. These words are all cognates of Jörð, the name of the giantess of Norse myth. Earth was first used as the name of the sphere of the Earth in the early fifteenth century. The planet's name in Latin, used academically and scientifically in the West during the Renaissance, is the same as that of Terra Mater, the Roman goddess, which translates to English as Mother Earth.\n\nThe standard astronomical symbol of the Earth consists of a cross circumscribed by a circle. This symbol is known as the wheel cross, sun cross, Odin's cross or Woden's cross. Although it has been used in various cultures for different purposes, it came to represent the compass points, earth and the land. Another version of the symbol is a cross on top of a circle; a stylized globus cruciger that was also used as an early astronomical symbol for the planet Earth.\n\nEarth has often been personified as a deity, in particular a goddess. In many cultures the mother goddess is also portrayed as a fertility deity. To the Aztec, Earth was called Tonantzin—\"our mother\"; to the Incas, Earth was called Pachamama—\"mother earth\". The Chinese Earth goddess Hou Tu is similar to Gaia, the Greek goddess personifying the Earth. To Hindus it is called Bhuma Devi, the Goddess of Earth. (See also Graha.) The Tuluva people of Tulunadu in Southern India celebrate a Three Day \"Earth Day\" called Keddaso. This festival comes in usually on 10th,12th,13 February every Calendar year. In Norse mythology, the Earth giantess Jörð was the mother of Thor and the daughter of Annar. Ancient Egyptian mythology is different from that of other cultures because Earth is male, Geb, and sky is female, Nut.\n\nCreation myths in many religions recall a story involving the creation of the world by a supernatural deity or deities. A variety of religious groups, often associated with fundamentalist branches of Protestantism or Islam, assert that their interpretations of the accounts of creation in sacred texts are literal truth and should be considered alongside or replace conventional scientific accounts of the formation of the Earth and the origin and development of life. Such assertions are opposed by the scientific community as well as other religious groups. A prominent example is the creation-evolution controversy.\n\nIn the ancient past there were varying levels of belief in a flat Earth, with the Mesopotamian culture portraying the world as a flat disk afloat in an ocean. The spherical form of the Earth was suggested by early Greek philosophers; a belief espoused by Pythagoras. By the Middle Ages—as evidenced by thinkers such as Thomas Aquinas—European belief in a spherical Earth was widespread.\n\nThe technological developments of the latter half of the 20th century are widely considered to have altered the public's perception of the Earth. Before space flight, the popular image of Earth was of a green world. Science fiction artist Frank R. Paul provided perhaps the first image of a cloudless \"blue\" planet (with sharply defined land masses) on the back cover of the July 1940 issue of \"Amazing Stories\", a common depiction for several decades thereafter.\n\nEarth was first photographed from space by Explorer 6 in 1959. Yuri Gagarin became the first human to view Earth from space in 1961. The crew of the Apollo 8 was the first to view an Earth-rise from lunar orbit in 1968. In 1972 the crew of the Apollo 17 produced the famous \"Blue Marble\" photograph of the planet Earth from cislunar space. This became an iconic image of the planet as a marble of cloud-swirled blue ocean broken by green-brown continents. NASA archivist Mike Gentry has speculated that \"The Blue Marble\" is the most widely distributed image in human history. Inspired by \"The Blue Marble\" poet-diplomat Abhay K has penned an Earth Anthem describing the planet as a \"Cosmic Blue Pearl\". A photo taken of a distant Earth by \"Voyager 1\" in 1990 inspired Carl Sagan to describe the planet as a \"Pale Blue Dot.\"\n\nSince the 1960s, Earth has also been described as a massive \"Spaceship Earth,\" with a life support system that requires maintenance, or, in the Gaia hypothesis, as having a biosphere that forms one large organism.\n\nOver the past two centuries a growing environmental movement has emerged that is concerned about humankind's effects on the Earth. The key issues of this socio-political movement are the conservation of natural resources, elimination of pollution, and the usage of land. Although diverse in interests and goals, environmentalists as a group tend to advocate sustainable management of resources and stewardship of the environment through changes in public policy and individual behavior. Of particular concern is the large-scale exploitation of non-renewable resources. Changes sought by the environmental movements are sometimes in conflict with commercial interests due to the additional costs associated with managing the environmental impact of those interests.\n\n"}
{"id": "42961694", "url": "https://en.wikipedia.org/wiki?curid=42961694", "title": "Electric rickshaw", "text": "Electric rickshaw\n\nElectric rickshaws (also known as electric tuk-tuks or e-rickshaws) have been becoming more popular in some cities since 2008 as an alternative to auto rickshaws and pulled rickshaws because of their low fuel cost, and less human effort compared to pulled rickshaws. They are being widely accepted as an alternative to petrol/diesel/CNG auto rickshaws. They are 3 wheelers pulled by an electric motor ranging from 650-1400  Watts. They are mostly manufactured in India and China, only a few other countries manufacture these vehicles. Battery-run rickshaws could be a low-emitter complementary transport for the low-income people, who suffer most from a lack of transport facility, if introduced in a systematic manner according to experts.\n\nThe electric automobile did not easily develop into a viable means of transportation. Research waned from 1920-1960 until environmental issues of pollution and diminishing natural resources reawakened the need of a more environmentally friendly means of transportation. Technologies that support a reliable battery and the weight of the needed number of batteries elevated the price of making an electric vehicle. In 1837, Robert Davidson of Scotland appears to have been the builder of the first electric car, but it wasn't until the 1890s that electric cars were manufactured and sold in Europe and America. During the late 1890s, United States roads were populated by more electric automobiles than those with internal combustion engines.\n\nThese rickshaws have a M.S(Mild Steel) tubular Chassis, consist of 3 wheels with a differential mechanism at rear wheels. The motor is brushless DC motor manufactured mostly in India and China. The electrical system used in Indian version is 48V and Bangladesh is 60V. The body design from most popular Chinese version is of very thin iron or aluminum sheets. Vehicles made in fiber are also popular because of their strength and durability, resulting in low maintenance, especially in India. Body design is varied from load carriers, passenger vehicles with no roof, to full body with windshield for drivers comfort It consist of a controller unit.They are sold on the basis of voltage supplied and current output, also the number of mosfet(metal oxide field effect transistor) used. The battery used is mostly lead acid battery with life of 6–12 months. Deep discharge batteries designed for electric vehicles are rarely used. Weight of the electric car has also been a recurring design difficulty in them.\n\nThese are load carrying versions of these rickshaws differ in their upper body, load carrying capacity, motor power, controller and other structural aspects, sometimes motor power is also increased in order to carry loads up to 500–1000 kg.\n\nThere are two types of solar vehicles:\n\nIn practice, the term \"solar rickshaw\" is most commonly used to describe battery-electric rickshaws whose batteries are indirectly solar-charged (i.e. independently from the vehicle) prior to use. This is usually facilitated by removing batteries in need of charging from the vehicle, and exchanging them for batteries which have already been charged. Alternatively, batteries can be charged in-situ while the vehicle is parked, although this may limit daytime usage. The exact same battery replacement and in-situ methods are of course also used for non-'solar' batteries and vehicles.\n\nElectric rickshaw are most popular in Asia. The low cost Chinese version being the first to show up on streets. They are mostly used in China, India, Bangladesh and Nepal, also in low numbers other parts of Asia they have been showing up. China, Japan, India, and European countries (Switzerland, France, Germany) have researched and developed electric tricycles for commercial transport and are attempting to capture the growing market in Asia. Government has made efforts though to run them and made plans to issue licences on a fee of 1.5tk but there has been no action on this matter to date.\n\nBangladesh imports electric rickshaws directly from China or via other countries, the well established cities prefer them as cheaper and better means of transport. The government in an inter-ministerial meeting on May 5 banned import and assembly of the vehicles and decided to send off-road those already plying, primarily on the ground that it consumes electricity mostly through illegal connections.\n\nChina is the largest manufacturer of electric rickshaws in the world, due to low labor cost, high production rates and encouraging government policies on foreign trade they import a large number on daily basis. There are hundreds of electric rickshaw manufacturers and thousands of parts producers. and now main market for small city or town which bus is not popular.\n\nOne of the first attempt to design electric rickshaws was done by Nimbkar Agricultural Research Institute in late 1990s. They modified the cycle rickshaw and then converted to an electric one. In India they are popularly known as e-rickshaws and are widely spread all over India. They started to gain popularity in India since 2011.The design is now much different from cycle rickshaws. They have provided with service to city and has also contribution in providing livelihood to people in India. Due to their low cost and high efficiency they are accepted on the Indian streets, but government policies have been threatening the e-rickshaw and banned them in the capital city Delhi, but due to increase in number failed to put them off the streets. They are still widely used in Delhi and other parts of India. In Delhi, as per government official's figures in April 2012, the number was over 100,000.\n\nIn India almost all claimed to be manufacturing the vehicle are merely importing it from China and assembling them. Though the manufacturers in India are less in number, manufacturers claim that in the vehicle production is less and cost is a little at higher but they offer higher quality products and also offer services and warranty, these manufacturers market the product as an Indian make and are also popular because of uniqueness in their product and providing a branded better quality product.\n\nThe FRP body e-rickshaws are also popular in India and are manufactured in India due to high shipment cost from China they are cheaper to Indian manufacturers, where a Chinese version of FRP Rickshaw will cost 1.5 times more than an Indian make.\n\nThere are issues with services due to lack of established companies and just about everyone importing and selling them from China, resulting in problems to their customers, this is the reason consumers have started gaining knowledge and prefer more durable versions from well-established companies and Indian manufacturers.\n\nInitially e-rickshaws were unregulated by any central law in India. However, the Delhi High Court, banned running of e-rickshaws in Delhi on 31 July 2014, over safety concerns raised through a public interest litigation. In a rally held for regularization of e rickshaws in Delhi transport minister Nitin Gadkari said that \"municipal corporations would regularize e-rickshaws by registering them for a fee of just Rs.100. After registering the e-rickshaw, corporations will have to issue identity cards to drivers so they can earn their livelihood easily.\" Once the policy was in place, the corporation, along with traffic police, would have fix the amount of fine to be imposed for violation of the policy. However, the policy was never implemented. Certain states like Tripura had regularised the e-rickshaws through municipal bylaws or through state legislation. In March 2015, the Indian Parliament passed an amendment to the Motor Vehicles (Amendment) Bill, 2015 legalizing E-Rickshaws. By July 2015 Battery Rickshaw are available for travel in many cities, now certified to ply with Registration No. plate by R.T.O. with insurance.\n\n"}
{"id": "3500739", "url": "https://en.wikipedia.org/wiki?curid=3500739", "title": "Emulsifying wax", "text": "Emulsifying wax\n\nEmulsifying wax is a cosmetic emulsifying ingredient. The ingredient name is often followed by the initials NF, indicating that it conforms to the specifications of the National Formulary.\n\nEmulsifying wax is created when a wax material (either a vegetable wax of some kind or a petroleum-based wax) is treated with a detergent (typically sodium dodecyl sulfate or polysorbates) to cause it to make oil and water bind together into a smooth emulsion. It is a white waxy solid with a low fatty alcohol odor.\n\nThe ingredients for Emulsifying Wax NF are: Cetearyl Alcohol, and a polyoxyethylene derivative of a fatty acid ester of sorbitan (a polysorbate).\n\nIn a cosmetic product, if the emulsifying wax used meets the standards for the National Formulary, it may be listed in the ingredient declaration by the term \"emulsifying wax NF\". Otherwise, the emulsifier is considered a blended ingredient and the individual components must be listed individually in the ingredient declaration, placed appropriately in descending order of predominance in the whole.\n"}
{"id": "478933", "url": "https://en.wikipedia.org/wiki?curid=478933", "title": "Energy conservation", "text": "Energy conservation\n\nEnergy conservation is the effort made to reduce the consumption of energy by using less of an energy service. This can be achieved either by using energy more efficiently (using less energy for a constant service) or by reducing the amount of service used (for example, by driving less). Energy conservation is a part of the concept of eco-sufficiency. Energy conservation reduces the need for energy services and can result in increased environmental quality, national security, personal financial security and higher savings.\nIt is at the top of the sustainable energy hierarchy.\nIt also lowers energy costs by preventing future resource depletion.\n\nEnergy can be conserved by reducing wastage and losses, improving efficiency through technological upgrades and improved operation and maintenance. On a global level energy use can also be reduced by the stabilisation of population growth.\n\nEnergy can only be transformed from one form to other, such as heat energy to motive power in cars, or kinetic energy of water flow to electricity in hydroelectric power plants. However machines are required to transform energy from one form to other. The wear and friction of the components of these machine while running cause loss of quadrillions of BTU and $500 billions in industries only in USA. It is possible to minimize these losses by adopting green engineering practices to improve life cycle of the components.(Ref.R.Chattopadhyay:Green Tribology, Green Surface Engineering and Global Warming', ASM International,USA, 2014. Also 3 books by same author published earlier on wear, surface engineering and global warming.) \n\nSome countries employ energy or carbon taxes to motivate energy users to reduce their consumption. Carbon taxes can force consumption to shift to nuclear power and other energy sources that carry different sets of environmental side effects and limitations. On the other hand, taxes on all energy consumption can reduce energy use across the board while reducing a broader array of environmental consequences arising from energy production. The state of California employs a tiered energy tax whereby every consumer receives a baseline energy allowance that carries a low tax. As usage increases above that baseline, the tax increases drastically. Such programs aim to protect poorer households while creating a larger tax burden for high energy consumers.\n\nOne of the primary ways to improve energy conservation in buildings is to perform an energy audit. An energy audit is an inspection and analysis of energy use and flows for energy conservation in a building, process or system with an eye toward reducing energy input without negatively affecting output. This is normally accomplished by trained professionals and can be part of some of the national programs discussed above. Recent development of smartphone apps enables homeowners to complete relatively sophisticated energy audits themselves.\n\nBuilding technologies and smart meters can allow energy users, both commercial and residential, to visualize the impact their energy use can have in their workplace or homes. Advanced real-time energy metering can help people save energy by their actions.\nIn passive solar building design, windows, walls, and floors are made to collect, store, and distribute solar energy in the form of heat in the winter and reject solar heat in the summer. This is called passive solar design or climatic design because, unlike active solar heating systems, it does not involve the use of mechanical and electrical devices.\n\nThe key to designing a passive solar building is to best take advantage of the local climate. Elements to be considered include window placement and glazing type, thermal insulation, thermal mass, and shading. Passive solar design techniques can be applied most easily to new buildings, but existing buildings can be retrofitted.\n\nIn the United States, suburban infrastructure evolved during an age of relatively easy access to fossil fuels, which has led to transportation-dependent systems of living. Zoning reforms that allow greater urban density as well as designs for walking and bicycling can greatly reduce energy consumed for transportation. The use of telecommuting by major corporations is a significant opportunity to conserve energy, as many Americans now work in service jobs that enable them to work from home instead of commuting to work each day.\n\nConsumers are often poorly informed of the savings of energy efficient products. A prominent example of this is the energy savings that can be made by replacing an incandescent light bulb with a more modern alternative. When purchasing light bulbs, many consumers opt for cheap incandescent bulbs, failing to take into account their higher energy costs and lower lifespans when compared to modern compact fluorescent and LED bulbs. Although these energy-efficient alternatives have a higher upfront cost, their long lifespan and low energy use can save consumers a considerable amount of money. The price of LED bulbs has also been steadily decreasing in the past five years due to improvements in semiconductor technology. Many LED bulbs on the market qualify for utility rebates that further reduce the price of purchase to the consumer. Estimates by the U.S. Department of Energy state that widespread adoption of LED lighting over the next 20 years could result in about $265 billion worth of savings in United States energy costs.\n\nThe research one must put into conserving energy is often too time consuming and costly for the average consumer when there are cheaper products and technology available using today's fossil fuels. Some governments and NGOs are attempting to reduce this complexity with ecolabels that make differences in energy efficiency easy to research while shopping.\n\nTo provide the kind of information and support people need to invest money, time and effort in energy conservation, it is important to understand and link to people's topical concerns. For instance, some retailers argue that bright lighting stimulates purchasing. However, health studies have demonstrated that headache, stress, blood pressure, fatigue and worker error all generally increase with the common over-illumination present in many workplace and retail settings. It has been shown that natural daylighting increases productivity levels of workers, while reducing energy consumption.\n\nIn warm climates where air conditioning is used, any household device that gives off heat will result in a larger load on the cooling system. Items such as stoves, dish washers, clothes dryers, hot water and incandescent lighting all add heat to the home. Low-power or insulated versions of these devices give off less heat for the air conditioning to remove. The air conditioning system can also improve in efficiency by using a heat sink that is cooler than the standard air heat exchanger, such as geothermal or water.\n\nIn cold climates, heating air and water is a major demand on household energy use. Significant energy reductions are possible by using different technologies. Heat pumps are a more efficient alternative to electrical resistance heaters for warming air or water. A variety of efficient clothes dryers are available, and the clothes lines requires no energy- only time. Natural-gas condensing boilers and hot-air furnaces increase efficiency over standard hot-flue models. New construction implementing heat exchangers can capture heat from waste water or exhaust air in bathrooms, laundry and kitchens.\n\nIn both warm and cold climate extremes, airtight thermal insulated construction is the largest factor determining the efficiency of a home. Insulation is added to minimize the flow of heat to or from the home, but can be labor-intensive to retrofit to an existing home.\n\nDespite the vital role energy efficiency is expected to play in cost-effectively cutting energy demand, only a small part of its economic potential is exploited in the Asia. Governments have implemented a range of subsidies such as cash grants, cheap credit, tax exemptions, and co-financing with public-sector funds to encourage a range of energy-efficiency initiatives across several sectors. Governments in the Asia-Pacific region have implemented a range of information provision and labeling programs for buildings, appliances, and the transportation and industrial\nsectors. Information programs can simply provide data, such as fuel-economy labels, or actively seek to encourage behavioral changes, such as Japan's Cool Biz campaign that encourages setting air conditioners at 28-degrees Celsius and allowing employees to dress casually in the summer.\n\nAt the end of 2006, the European Union (EU) pledged to cut its annual consumption of primary energy by 20% by 2020. The 'European Union Energy Efficiency Action Plan' is long-awaited. Directive 2012/27/EU is on energy efficiency.\n\nAs part of the EU's SAVE Programme, aimed at promoting energy efficiency and encouraging energy-saving behavior, the Boiler Efficiency Directive specifies minimum levels of efficiency for boilers utilizing liquid or gaseous fuels.\n\nThe Petroleum Conservation Research Association (PCRA) is an Indian governmental body created in 1978 that engages in promoting energy efficiency and conservation in every walk of life. In the recent past PCRA has done mass-media campaigns in television, radio, and print media. An impact-assessment survey by a third party revealed that due to these larger campaigns by PCRA, the public's overall awareness level has gone up leading to saving of fossil fuels worth crores of rupees, besides reducing pollution.\n\nThe Bureau of Energy Efficiency is an Indian government organization created in 2001 that is responsible for promoting energy efficiency and conservation.\n\nSince the 1973 oil crisis, energy conservation has been an issue in Japan. All oil-based fuel is imported, so domestic sustainable energy is being developed.\n\nThe Energy Conservation Center promotes energy efficiency in every aspect of Japan. Public entities are implementing the efficient use of energy for industries and research. It includes projects such as the Top Runner Program. In this project, new appliances are regularly tested on efficiency, and the most efficient ones are made the standard.\n\nIn Lebanon and since 2002 The Lebanese Center for Energy Conservation (LCEC) has been promoting the development of efficient and rational uses of energy and the use of renewable energy at the consumer level. It was created as a project financed by the International Environment Facility (GEF) and the Ministry of Energy Water (MEW) under the management of the United Nations Development Programme (UNDP) and gradually established itself as an independent technical national center although it continues to be supported by the United Nations Development Programme (UNDP) as indicated in the Memorandum of Understanding (MoU) signed between MEW and UNDP on 18 June 2007.\n\nUntil recently, Nepal has been focusing on the exploitation of its huge water resources to produce hydro power. Demand side management and energy conservation was not in the focus of government action. In 2009, bilateral Development Cooperation between Nepal and the Federal Republic of Germany, has agreed upon the joint implementation of \"Nepal Energy Efficiency Programme\". The lead executing agencies for the implementation are the Water and Energy Commission Secretariat (WECS). The aim of the programme is the promotion of energy efficiency in policy making, in rural and urban households as well as in the industry.\nDue to the lack of a government organization that promotes energy efficiency in the country, the Federation of Nepalese Chambers of Commerce and Industry (FNCCI) has established the Energy Efficiency Centre under his roof to promote energy conservation in the private sector. The Energy Efficiency Centre is a non-profit initiative that is offering energy auditing services to the industries. The Centre is also supported by Nepal Energy Efficiency Programme of Deutsche Gesellschaft für Internationale Zusammenarbeit.\nA study conducted in 2012 found out that Nepalese industries could save 160,000 Megawatt hours of electricity and 8,000 Terajoule of thermal energy (like diesel, furnace oil and coal) every year. These savings are equivalent to annual energy cost cut of up to 6.4 Billion Nepalese Rupees.\nAs a result of Nepal Economic Forum 2014, an economic reform agenda in the priority sectors was declared focusing on energy conservation among others. In the energy reform agenda the government of Nepal gave the commitment to introduce incentive packages in the budget of the fiscal year 2015/16 for industries that practices energy efficiency or use efficient technologies (incl. cogeneration).\n\nIn New Zealand the Energy Efficiency and Conservation Authority is the Government Agency responsible for promoting energy efficiency and conservation. The Energy Management Association of New Zealand is a membership based organization representing the New Zealand energy services sector, providing training and accreditation services with the aim of ensuring energy management services are credible and dependable.\n\nIn Nigeria, the Lagos State Government is encouraging Lagosians to imbibe an energy conservation culture. The Lagos State Electricity Board (LSEB) is spearheading an initiative tagged \"Conserve Energy, Save Money\" under the Ministry of Energy and Mineral Resources. The initiative is designed to sensitize Lagosians around the theme of energy conservation by connecting with and influencing their behavior through do-it-yourself tips and exciting interaction with prominent personalities. In September 2013, Governor Babatunde Raji Fashola of Lagos State and rapper Jude 'MI' Abaga (campaign ambassador)() participated in the Governor's first ever Google+ Hangout on the topic of energy conservation.\n\nIn addition to the hangout, during the month of October (the official energy conservation month in the state), LSEB hosted experience centers in malls around Lagos State where members of the public were encouraged to calculate their current household energy consumption and discover ways to save money using the 1st-ever consumer-focused energy app in sub-saharan Africa. To get Lagosians started on energy conservation, Solar Lamps and Phillips Energy-saving bulbs were also given out at each experience center.\nPictures from the experience centers: (part of Lagos state government energy initiatives)\n\nSri Lanka currently consumes fossil fuels, hydro power, wind power, solar power and dendro power for their day to day power generation. The Sri Lanka Sustainable Energy Authority is playing a major role regarding energy management and energy conservation. Today, most of the industries are requested to reduce their energy consumption by using renewable energy sources and optimizing their energy usage.\n\nTurkey aims to decrease by at least 20% the amount of energy consumed per GDP of Turkey by the year 2023 (energy intensity).\n\nThe United States is currently the second largest single consumer of energy, following China. The U.S. Department of Energy categorizes national energy use in four broad sectors: transportation, residential, commercial, and industrial.\n\nEnergy usage in transportation and residential sectors, about half of U.S. energy consumption, is largely controlled by individual consumers. Commercial and industrial energy expenditures are determined by businesses entities and other facility managers. National energy policy has a significant effect on energy usage across all four sectors.\n\nAnother aspect of energy conversation is using Leadership in Energy and Environmental Design. (LEED) This program is not mandatory, it is voluntary. This program has many categories, Energy and Atmosphere Prerequisite, applies to energy conservation. This section focuses on energy performance, renewable energy, energy performance, and many more. This program is designed to promote energy efficiency and be a green building, which is part of conservation. As mention above “energy conservation are efforts made to reduce the consumption of energy.” \n<br>\n\n\n"}
{"id": "156817", "url": "https://en.wikipedia.org/wiki?curid=156817", "title": "Gridiron pendulum", "text": "Gridiron pendulum\n\nThe gridiron pendulum was a temperature-compensated clock pendulum invented by British clockmaker John Harrison around 1726 and later modified by John Ellicott. It was used in precision clocks. In ordinary clock pendulums, the pendulum rod expands and contracts with changes in temperature. The period of the pendulum's swing depends on its length, so a pendulum clock's rate varied with changes in ambient temperature, causing inaccurate timekeeping. The gridiron pendulum consists of alternating parallel rods of two metals with different thermal expansion coefficients, such as steel and brass. The rods are connected by a frame in such a way that their different thermal expansions (or contractions) compensate for each other, so the overall length of the pendulum, and thus its period, stays constant with temperature.\n\nThe gridiron pendulum was used during the Industrial Revolution period in regulator clocks, precision clocks employed as time standards in factories, laboratories, office buildings, and post offices to schedule work and set other clocks. The gridiron became so associated with quality timekeeping that to this day many clocks have pendulums with decorative fake gridirons, which have no temperature compensating qualities.\n\nIts simplest and later form consists of five rods. A central iron rod runs up from the bob to a point immediately below the suspension. At that point a cross-piece (middle bridge) extends from the central rod and connects to two zinc rods, one on each side of the central rod, which reach down to, and are fixed to, the bottom bridge just above the bob. The bottom bridge clears the central rod and connects to two further iron rods which run back up to the top bridge attached to the suspension. As the iron rods expand in heat, the bottom bridge drops relative to the suspension, and the bob drops relative to the middle bridge. However, the middle bridge rises relative to the bottom one because the greater expansion of the zinc rods pushes the middle bridge, and therefore the bob, upwards to match the combined drop caused by the expanding iron.\nIn simple terms, the upwards expansion of the zinc counteracts the combined downwards expansion of the iron (which has a greater total length). The rod lengths are calculated so that the effective length of the zinc rods multiplied by zinc's thermal expansion coefficient equals the effective length of the iron rods multiplied by iron's expansion coefficient thereby keeping the pendulum the same length.\n\nHarrison's original construction using brass (pure zinc not being available then) is more complex since brass does not expand as much as zinc does. A further set of rods and bridges is needed giving nine rods in all, five iron and four brass. The exact degree of compensation can be adjusted by having a section of the central rod which is partly brass and partly iron. These overlap (like a sandwich) and are joined by a pin which passes through both metals. A number of holes for the pin are made in both parts and moving the pin up or down the rod changes how much of the combined rod is brass and how much is iron. In the late 19th century the Dent company marketed a further development of the zinc gridiron in which the four outer rods were replaced by two concentric tubes which were linked by a tubular nut which could be screwed up and down to alter the degree of compensation.\n\nScientists in the 1800s found that the gridiron pendulum had disadvantages that made it unsuitable for the highest precision clocks. The friction of the rods sliding in the holes in the frame caused the rods to adjust to temperature changes in a series of tiny jumps, rather than with a smooth motion. This caused the rate of the pendulum, and therefore the clock, to change suddenly with each jump. Later it was found that zinc is not very stable dimensionally; it is subject to creep. Therefore, another type of temperature-compensated pendulum, the mercury pendulum, was used in the highest precision clocks. \n\nBy 1900, the highest precision astronomical regulators used pendulums of low thermal expansion materials like invar and fused quartz.\n"}
{"id": "28580984", "url": "https://en.wikipedia.org/wiki?curid=28580984", "title": "Halânga Power Station", "text": "Halânga Power Station\n\nThe Halânga Power Station is a large thermal power plant located in Halânga, having 7 generation groups, 4 of 25 MW, 2 of 50 MW and 1 of 47 MW having a total electricity generation capacity of 247 MW. The American AES Corporation is also interested in constructing a new 400 MW unit at the station's site worth around US$1.6 billion.\n\n\n"}
{"id": "6516528", "url": "https://en.wikipedia.org/wiki?curid=6516528", "title": "Honda Civic GX", "text": "Honda Civic GX\n\nThe Honda Civic GX was the only car factory-built to run on compressed natural gas (CNG) in the U.S. available to non-fleet customers. The GX was based on the Honda Civic and available for fleet sales in all 50 states in the US. It was previously available for retail sales in four states (California, New York, Utah and Oklahoma), but later was made available to retail consumers in 35 states throughout the U.S. The GX was manufactured in Honda's Greensburg, Indiana plant together with the production of conventional Civics from late 2009. It was previously produced in East Liberty, Ohio.\n\nThe third generation GX was awarded the 2012 Green Car of the Year by the Green Car Journal in November 2011 at the Los Angeles Auto Show. For eight years up to 2011, the Civic GX was rated first by the American Council for an Energy-Efficient Economy in the \"Greenest Vehicle of the Year\" list (excluding the years 2001, 2002, 2003, and 2006 when the Honda Insight hybrid topped the list) For 2012 the GX was surpassed by the Mitsubishi i-MiEV. For 2014, the GX ranked 10th, after several hybrids and electric vehicles. 2015 was the last model year for the Civic GX.\n\nThe Honda Civic GX first appeared in 1998 as a factory-modified Civic LX that had been designed to run exclusively on CNG (compressed natural gas). In 1998 the Civic GX cost $4500 more than a comparable Civic LX. The car looked and drove just like a contemporary Honda Civic LX, but did not run on gasoline. In 2001, the Civic GX was rated the cleanest-burning internal combustion engine in the world by the EPA.\n\nThe GX was first leased to the City of Los Angeles to be used by parking enforcement officers and other city employees as a live beta test. The GX followed the same model year design changes as the Civic LX model, until the model year 2001 when a CVT (continuously variable transmission) was introduced in place of the 4 speed automatic transmission. In the 2006 year model, the GX again was equipped with the automatic 5 speed transmission, which increased its mileage and extended its range to 250 miles. In 1998 the GX was available for special order in some states to consumers (California and Colorado in particular).\n\nIn 2006, the Civic went through its greatest modification since its inception. All variants of the Civic were awarded the 2006 \"Motor Trend\" \"Car of the Year\" award. The 8th generation Civic remained unchanged from 2006 and was also available in the GX model. The GX was a very limited-availability car with fewer than a thousand units per year being produced by the factory.\n\nIn October 2006, the 2007 Civic GX became available in New York. In July 2009, the GX became available to the public in Utah. In April 2010 the GX became available to the public in Oklahoma. The CNG Civic in this market related to favorable natural gas costs and the numerous high pressure filling stations. The promotion of CNG conversions by natural gas producers headquartered in Oklahoma provided incentive for Honda to market the Civic GX there. State of Oklahoma incentives were a factor that led United Parcel Service to convert part of their delivery truck fleet to CNG. After December 2010, the GX was available for fleet sales in all 50 states. Retail sales were expanded to 35 states in the fall of 2011.\n\n2015 was the last model year for the Civic CNG. The company said it had sold about 16,000 natural-gas vehicles since the model was introduced, mainly to taxi and commercial fleets. American Honda Motor Company executive vice president John Mendel commented that Honda was phasing out efforts to develop natural-gas powered vehicles and would instead focus on hybrids and electric vehicles. He cited the lack of a CNG fueling infrastructure in the United States as the main reason for the decision to stop producing the Civic CNG. \"The infrastructure, while it improved, just wasn't as convenient as petrol,\" Mendel said. \"We gave it a pretty long run and we tried and tried and tried.\"\n\nThe GX was originally introduced with a 1.6 liter I4 engine. The 2001 model make-over carried a 1.7 liter engine. Beginning in model-year 2006, the 1.8 liter inline four-cylinder engine was introduced to the civic lineup. The\ncompression ratio in the Civic GX is 12.5:1, higher than that of most US pump gasoline-powered automobiles. The significantly higher compression ratio is usable without detonation due to the 120-octane natural gas that powers the car. Acceleration of the 2012 Civic Natural Gas is less than that of the comparable 4-door 2012 LX model due to both lower power (110 hp vs. 140 hp) and heavier weight (2848 lbs vs 2705 lbs). Zero-to-sixty times have been clocked at 12.6 seconds.\n\nThe CNG cylinder (fuel tank) is carried in the trunk of the car and holds 8.0 gasoline gallon equivalent (GGE) at 3600 psi.\n\nRange on a full 3600 psi fill is variable, depending on driving conditions and driving technique. While Honda claims an estimated 225–250 miles from a full CNG tank, independent tests have found lower ranges of 180–200 miles and \"just over 200 miles\" (about 300 km). There were improvements in the 2012 EPA fuel economy as the range increased to 225 ~ 250 miles. The EPA rates the 2009 Honda Civic GX at 24 equivalent MPG city and 36 equivalent MPG highway. Independent tests with mixed driving usage found rates of \"nearly 32\" and 26.8 equivalent MPG. The estimated fuel cost for this vehicle to drive 25 miles in a combination of city and highway driving is $1.47 using CNG, based on an average fuel price of $1.93 per gasoline equivalent gallon (121.5 cubic feet).\nThe GX qualifies for HOV Lane access in California, Arizona, Utah, and other states.\n\nHome refueling is available for the GX with the addition of the Phill Home Refueling Appliance. This unit attaches to a home or commercial natural gas source, and compresses the gas into the car's tank through an attached hose. The unit requires a 240 V power source, and uses 800 W when in operation.\n\nHonda did not recommend utilizing home refueling options due to possible moisture and chemical contamination of some natural gas supplies. Honda reserved the right to void the warranty of a car needing service based on inspection of the fuel system for contamination.\n\nIn 2000 the Civic GX rated first in the \"Greenest Vehicle of the Year\" list by the American Council for an Energy-Efficient Economy. It ranked cleaner than the GM EV1. It was dethroned by the SULEV-rated Honda Insight hybrid a year later. (See http://www.greenercars.org).\n\nThe GX again ranked #1 after the Insight was discontinued (2007–11). In 2012 it was beaten by the new i-MiEV from Mitsubishi. In 2014, the GX ranked 10th, after several hybrids and electric vehicles.\n\nThe 2012 Honda Civic GX was awarded the 2012 Green Car of the Year by the Green Car Journal in November 2011 at the Los Angeles Auto Show.\n\n\n"}
{"id": "51776", "url": "https://en.wikipedia.org/wiki?curid=51776", "title": "Hydrostatic equilibrium", "text": "Hydrostatic equilibrium\n\nIn fluid mechanics, a fluid is said to be in hydrostatic equilibrium or hydrostatic balance when it is at rest, or when the flow velocity at each point is constant over time. This occurs when external forces such as gravity are balanced by a pressure-gradient force. For instance, the pressure-gradient force prevents gravity from collapsing Earth's atmosphere into a thin, dense shell, whereas gravity prevents the pressure gradient force from diffusing the atmosphere into space.\n\nHydrostatic equilibrium is the current distinguishing criterion between dwarf planets and small Solar System bodies, and has other roles in astrophysics and planetary geology. This qualification typically means that the object is symmetrically rounded into a spheroid or ellipsoid shape, where any irregular surface features are due to a relatively thin solid crust. There are 32 observationally confirmed such objects (apart from the Sun), sometimes called \"planemos,\" in the Solar System, seven more that are virtually certain, and a hundred or so more that are likely.\n\nNewton's laws of motion state that a volume of a fluid that is not in motion or that is in a state of constant velocity must have zero net force on it. This means the sum of the forces in a given direction must be opposed by an equal sum of forces in the opposite direction. This force balance is called a hydrostatic equilibrium.\n\nThe fluid can be split into a large number of cuboid volume elements; by considering a single element, the action of the fluid can be derived.\n\nThere are 3 forces: the force downwards onto the top of the cuboid from the pressure, P, of the fluid above it is, from the definition of pressure,\nSimilarly, the force on the volume element from the pressure of the fluid below pushing upwards is\n\nFinally, the weight of the volume element causes a force downwards. If the density is ρ, the volume is V and g the standard gravity, then:\nThe volume of this cuboid is equal to the area of the top or bottom, times the height — the formula for finding the volume of a cube.\n\nBy balancing these forces, the total force on the fluid is\nThis sum equals zero if the fluid's velocity is constant. Dividing by A,\nOr,\nP − P is a change in pressure, and h is the height of the volume element—a change in the distance above the ground. By saying these changes are infinitesimally small, the equation can be written in differential form.\nDensity changes with pressure, and gravity changes with height, so the equation would be:\n\nNote finally that this last equation can be derived by solving the three-dimensional Navier–Stokes equations for the equilibrium situation where\nThen the only non-trivial equation is the formula_11-equation, which now reads\nThus, hydrostatic balance can be regarded as a particularly simple equilibrium solution of the Navier–Stokes equations.\n\nBy plugging the energy momentum tensor for a perfect fluid\ninto the Einstein field equations\nand using the conservation condition\none can derive the Tolman–Oppenheimer–Volkoff equation for the structure of a static, spherically symmetric relativistic star in isotropic coordinates:\nIn practice, \"Ρ\" and \"ρ\" are related by an equation of state of the form \"f\"(\"Ρ\",\"ρ\")=0, with \"f\" specific to makeup of the star. \"M\"(\"r\") is a foliation of spheres weighted by the mass density \"ρ\"(\"r\"), with the largest sphere having radius \"r\":\nPer standard procedure in taking the nonrelativistic limit, we let \"c\"→∞, so that the factor\nTherefore, in the nonrelativistic limit the Tolman–Oppenheimer–Volkoff equation reduces to Newton's hydrostatic equilibrium:\n(we have made the trivial notation change \"h\"=\"r\" and have used \"f\"(\"Ρ\",\"ρ\")=0 to express \"ρ\" in terms of \"P\"). A similar equation can be computed for rotating, axially symmetric stars, which in its gauge independent form reads:\nUnlike the TOV equilibrium equation, these are two equations (for instance, if as usual when treating stars, one chooses spherical coordinates as basis coordinates formula_21, the index \"i\" runs for the coordinates \"r\" and formula_22).\n\nThe hydrostatic equilibrium pertains to hydrostatics and the principles of equilibrium of fluids. A hydrostatic balance is a particular balance for weighing substances in water. Hydrostatic balance allows the discovery of their specific gravities.\n\nIn any given layer of a star, there is a hydrostatic equilibrium between the outward thermal pressure from below and the weight of the material above pressing inward. The isotropic gravitational field compresses the star into the most compact shape possible. A rotating star in hydrostatic equilibrium is an oblate spheroid up to a certain (critical) angular velocity. An extreme example of this phenomenon is the star Vega, which has a rotation period of 12.5 hours. Consequently, Vega is about 20% larger at the equator than at the poles. A star with an angular velocity above the critical angular velocity becomes a Jacobi (scalene) ellipsoid, and at still faster rotation it is no longer ellipsoidal but piriform or oviform, with yet other shapes beyond that, though shapes beyond scalene are not stable.\n\nIf the star has a massive nearby companion object then tidal forces come into play as well, distorting the star into a scalene shape when rotation alone would make it a spheroid. An example of this is Beta Lyrae.\n\nHydrostatic equilibrium is also important for the intracluster medium, where it restricts the amount of fluid that can be present in the core of a cluster of galaxies.\n\nWe can also use the principle of hydrostatic equilibrium to estimate the velocity dispersion of dark matter in clusters of galaxies. Only baryonic matter (or, rather, the collisions thereof) emits X-ray radiation. The absolute X-ray luminosity per unit volume takes the form formula_23 where formula_24 and formula_25 are the temperature and density of the baryonic matter, and formula_26 is some function of temperature and fundamental constants. The baryonic density satisfies the above equation formula_27:\nThe integral is a measure of the total mass of the cluster, with formula_29 being the proper distance to the center of the cluster. Using the ideal gas law formula_30 (formula_31 is Boltzmann's constant and formula_32 is a characteristic mass of the baryonic gas particles) and rearranging, we arrive at\nMultiplying by formula_34 and differentiating with respect to formula_29 yields\nIf we make the assumption that cold dark matter particles have an isotropic velocity distribution, then the same derivation applies to these particles, and their density formula_37 satisfies the non-linear differential equation\nWith perfect X-ray and distance data, we could calculate the baryon density at each point in the cluster and thus the dark matter density. We could then calculate the velocity dispersion formula_39 of the dark matter, which is given by \nThe central density ratio formula_41 is dependent on the redshift formula_11 of the cluster and is given by\nwhere formula_22 is the angular width of the cluster and formula_45 the proper distance to the cluster. Values for the ratio range from .11 to .14 for various surveys.\n\nThe concept of hydrostatic equilibrium has also become important in determining whether an astronomical object is a planet, dwarf planet, or small Solar System body. According to the definition of planet adopted by the International Astronomical Union in 2006, one defining characteristic of planets and dwarf planets is that they are objects that have sufficient gravity to overcome their own rigidity and assume hydrostatic equilibrium. Such a body will normally have the differentiated interior and geology of a world (a planemo), though near-hydrostatic bodies such as the proto-planet 4 Vesta may also be differentiated. Sometimes the equilibrium shape is an oblate spheroid, as is the case with Earth. However, in the cases of moons in synchronous orbit, nearly unidirectional tidal forces create a scalene ellipsoid. Also, the dwarf planet is scalene due to its rapid rotation.\n\nIt had been thought that icy objects with a diameter larger than roughly 400 km are usually in hydrostatic equilibrium, whereas those smaller than that are not. Icy objects need less mass for hydrostatic equilibrium than rocky objects. The smallest object that is known to have an equilibrium shape is the icy moon Mimas at 397 km, whereas the largest object known to have an obviously non-equilibrium shape is the rocky asteroid Pallas at 532 km (582 × 556 × 500 ± 18 km). However, Mimas is not actually in hydrostatic equilibrium for its current rotation. The smallest body confirmed to be in hydrostatic equilibrium is the dwarf planet Ceres, at 945 km, whereas the largest body known to not be in hydrostatic equilibrium is the icy moon Iapetus, at 1,470 km.\n\nBecause the terrestrial planets and dwarf planets (and likewise the larger satellites, like the Moon and Io) have irregular surfaces, this definition evidently has some flexibility, but a specific means of quantifying an object's shape by this standard has not yet been announced. Local irregularities may be consistent with global equilibrium. For example, the massive base of the tallest mountain on Earth, Mauna Kea, has deformed and depressed the level of the surrounding crust, so that the overall distribution of mass approaches equilibrium. The amount of leeway afforded the definition could affect the classification of the asteroid Vesta, which may have solidified while in hydrostatic equilibrium but was subsequently significantly deformed by large impacts (now 572.6 × 557.2 × 446.4 km).\n\nIn the atmosphere, the pressure of the air decreases with increasing altitude. This pressure difference causes an upward force called the pressure-gradient force. The force of gravity balances this out, keeping the atmosphere bound to Earth and maintaining pressure differences with altitude.\n\n\n"}
{"id": "7429730", "url": "https://en.wikipedia.org/wiki?curid=7429730", "title": "Intercontinental Exchange", "text": "Intercontinental Exchange\n\nIntercontinental Exchange (ICE) is an American company that owns exchanges for financial and commodity markets, and operates 12 regulated exchanges and marketplaces. This includes ICE futures exchanges in the United States, Canada and Europe, the Liffe futures exchanges in Europe, the New York Stock Exchange equity options exchanges and OTC energy, credit and equity markets.\n\nICE also owns and operates 6 central clearing houses: ICE Clear U.S., ICE Clear Europe, ICE Clear Singapore, ICE Clear Credit, ICE Clear Netherlands and ICE NGX. ICE has offices in Atlanta, New York, London, Chicago, Houston, Winnipeg, Amsterdam, Calgary, Washington, D.C., San Francisco, Tel Aviv and Singapore.\n\nJeffrey C. Sprecher was a power plant developer who spotted a need for a seamless market in natural gas used to power generators. In the late 1990s, Sprecher acquired Continental Power Exchange, Inc. with the objective of developing an Internet-based platform to provide a more transparent and efficient market structure for OTC energy commodity trading.\n\nIn May 2000, ICE was founded by Sprecher and backed by Goldman Sachs, Morgan Stanley, BP, Total, Shell, Deutsche Bank and Société Générale who represent some of the world's largest energy traders.\n\nThe new exchange offered the trading community better price transparency, more efficiency, greater liquidity and lower costs than manual trading. While the company's original focus was energy products (crude and refined oil, natural gas, power, and emissions), acquisitions have expanded its activity into soft commodities (sugar, cotton and coffee), foreign exchange and equity index futures.\n\nIn a response to US financial crisis in 2008, Sprecher formed ICE US Trust based in New York, now called ICE Clear Credit LLC, to serve as a limited-purpose bank, a clearing house for credit default swaps. Sprecher worked closely with the Federal Reserve to serve as its over-the-counter (OTC) derivatives clearing house. \"US regulators were keen on the kind of clearing house for opaque over-the-counter (OTC) derivatives as a risk management device. In the absence of a central counterparty - which would guarantee pay-outs should a trading party be unable to do so - there was a high risk of massive market disruption\".\n\nThe principal backers for ICE US Trust were the same financial institutions most affected by the crisis, the top ten of the world's largest banks (Goldman Sachs, Bank of America, Citi, Credit Suisse, Deutsche Bank, JPMorgan, Merrill Lynch, Morgan Stanley and UBS). Sprecher's clearing house cleared their global credit default swaps (CDS) in exchange for sharing profits with these banks. By 30 September 2008 the Financial Post warned that the \"$54000bn credit derivatives market faced its biggest test in October 2008 as billions of dollars worth of contracts on now-defaulted derivatives would be auctioned by the International Swaps and Derivatives Association . In his article in the Financial Post, he described ICE as a \"US-based electronic futures exchange\" which raised the stakes on October 30, 2008 in its effort to expand in the $54000 bn credit derivatives market.\n\nBy 2010, Intercontinental Exchange had cleared more than $10 trillion in credit default swaps (CDS) through its subsidiaries, ICE Trust CDS (now ICE Clear Credit).\n\nBy 2017 Intercontinental Exchange had been named to the Fortune Future 50 determining the top 50 companies that are best positioned to adapt and deliver growth in a complex environment. ICE was also named to the Fortune 500 in June 2017 and is the only exchange operator included in the ranking.\n\nThe Intercontinental Exchange has had a policy to grow through the acquisition of other exchanges, a number of these have been successful while others have failed due to concerns by regulators or others that the new company would have created a monopoly situation. The major acquisition and attempted acquisitions have included:\n\nIn June 2001, ICE expanded its business into futures trading by acquiring the London-based International Petroleum Exchange (IPE), now ICE Futures Europe, which operated Europe's leading open-outcry energy futures exchange. Since 2003, ICE has partnered with the Chicago Climate Exchange (CCX) to host its electronic marketplaces. In April 2005, the entire ICE portfolio of energy futures became fully electronic and ICE closed International Petroleum Exchange's high profile and historic trading floor.\n\nICE became a publicly traded company on November 16, 2005, and was added to the Russell 1000 Index on June 30, 2006. The company expanded rapidly in 2007, acquiring the New York Board of Trade (NYBOT), and ChemConnect (a chemical commodity market).\n\nIn March 2007 ICE made an unsuccessful $9.9 billion bid for the Chicago Board of Trade, which was instead acquired by the CME Group.\n\nIntercontinentalExchange Inc., the \"upstart Atlanta-based energy bourse\" purchased the privately held 120-year-old Winnipeg Commodity Exchange, known for its canola futures contract, for $40 million.\n\nThe Winnipeg Commodity Exchange (WCE) was renamed ICE Futures Canada as of January 1, 2008.\nIn 2004, the Winnipeg Commodity Exchange had \"closed its open-outcry trading floor\" becoming \"the first North American agricultural futures exchange to trade exclusively on an electronic platform\" by trading via the \"Chicago Board of Trade's electronic platform, and [using] clearing services from the Kansas City Board of Trade. IntercontinentalExchange converted Winnipeg Commodity Exchange contracts to the IntercontinentalExchange platform. IntercontinentalExchange maintained an office and \"small core staff\" in Winnipeg, Manitoba. The Manitoba Securities Commission oversee its operations.\n\nIn June 2008, ICE announced that it had entered into a definitive merger agreement to acquire Creditex Group Inc. (Creditex). The transaction consideration totaled $625 million comprising approximately $565 million in ICE common stock and $60 million in cash, as well as a working capital adjustment to be finalized at closing. Upon the closing of the transaction, Creditex Group became a wholly owned subsidiary of ICE, operating under the Creditex name.\n\nIn January 2008, ICE partnered with Canada's TSX Group's Natural Gas Exchange, expanding their offering to clearing and settlement services for physical OTC natural gas contracts.\n\nIn April 2010, ICE acquired Climate Exchange PLC for 395 million pounds ($622 million) and European Climate Exchange (ECX) as part of its purchase. Exchange-traded emissions products were first offered by the European Climate Exchange (ECX), which was established in 2005, by listing products on the ICE Futures Europe's trading platform. ICE Futures Europe is the leading market for carbon dioxide (CO2) emissions. ICE's ECX products comply with the requirements of the European Union Emission Trading Scheme.\n\nIn February 2011, in the wake of an announced merger of NYSE Euronext with Deutsche Börse, speculation developed that ICE and Nasdaq could mount a counter-bid of their own for NYSE Euronext. ICE was thought to be looking to acquire the American exchange's derivatives business, Nasdaq its cash equities business. As of the time of the speculation, \"NYSE Euronext’s market value was $9.75 billion. Nasdaq was valued at $5.78 billion, while ICE was valued at $9.45 billion.\" Late in the month, Nasdaq was reported to be considering asking either ICE or the Chicago Mercantile Exchange (CME) to join in what would probably be an $11–12 billion counterbid for NYSE. On April 1, ICE and Nasdaq made an $11.3 billion offer which was rejected April 10 by NYSE. Another week later, ICE and Nasdaq sweetened their offer, including a $.17 increase per share to $42.67 and a $350 million breakup fee if the deal were to encounter regulatory trouble. The two said the offer was a $2 billion (21%) premium over the Deutsche offer and that they had fully committed financing of $3.8 billion from lenders to finance the deal.\n\nThe Justice Department, also in April, \"initiated an antitrust review of the proposal, which would have brought nearly all U.S. stock listings under a merged Nasdaq-NYSE.\" In May, saying it \"became clear that we would not be successful in securing regulatory approval,\" the Nasdaq and ICE withdrew their bid. The European Commission then blocked the Deutsche merger on 1 February 2012, citing the fact that the merged company would have a near monopoly.\n\nIn December 2012, ICE announced it would buy NYSE Euronext (this time without the involvement of Nasdaq) for $8.2 billion, pending regulatory approval. Jeffrey Sprecher will retain his position as Chairman and CEO. The boards of directors of both ICE and NYSE Euronext approved the acquisition.\n\nIn September 2014, ICE announced that it had entered into a definitive agreement to acquire SuperDerivatives, a provider of risk management analytics, financial market data and valuation services. Terms of the all-cash transaction included a purchase price of approximately $350 million. Completion of the transaction was subject to regulatory approval and other customary closing conditions. The transaction successfully completed on 7 October 2014.\n\nIn October 2015, ICE announced that it had entered into a definitive agreement to acquire Interactive Data Corporation (IDC), a provider of financial market data, analytics and related trading solutions, from Silver Lake, involved in technology investing, and Warburg Pincus, a private equity firm focused on growth investing. The acquisition was valued at approximately $5.2 billion, including $3.65 billion in cash and $1.55 billion in ICE common stock, and builds on ICE’s global market data growth strategy by expanding the markets served, adding technology platforms and increasing new data and valuation services. Completion of the transaction was subject to regulatory approval and other customary closing conditions. The transaction completed on December 14, 2015.\n\nIn December 2015, ICE acquired Trayport for $650 million from GFI Group. Trayport provides a trading technology platform that serves brokers, exchanges, clearing houses and trading participants, primarily in the European utility markets. Approximately 70%-80% of European utility trades flow through its platform.\n\nFollowing the acquisition, the Competition and Markets Authority (CMA) called in the merger for review. In October 2016 it announced its decision to require ICE to sell Trayport, having ruled that the merger could lead to a substantial lessening of competition. ICE challenged the decision in the Competition Appeal Tribunal (CAT), but the CAT upheld the CMA's decision. As a result, ICE sold Trayport to TMX Group in October 2017, in exchange for certain TMX Group assets and cash of £350 million.\n\nThe CMA's ruling against ICE was the first time CMA had required that a company sell an asset that it had already bought.\n\nIn March 2016, ICE announced that it had entered into a definitive agreement to acquire Standard & Poor's Securities Evaluations, Inc. (SPSE), a leading provider of fixed income evaluated pricing, and Credit Market Analysis (CMA), a leading provider of independent data for the over-the-counter (OTC) markets, two assets under the S&P Global Market Intelligence business unit, from McGraw Hill Financial (NYSE: MHFI). When completed, the acquisition will enable ICE to offer customers new data and valuation services. Under the terms of the agreement, ICE can elect to satisfy its payment of the purchase price due at the close of the transaction in either cash or shares of ICE’s common stock. All other terms of the agreement were not disclosed. In October 2016 it had completed its all-cash acquisition of S&P Global’s (NYSE: SPGI) Standard & Poor’s Securities Evaluations (SPSE) and Credit Market Analysis (CMA).\n\nIn February 2017, ICE announced it had entered an agreement to acquire TMX Atrium, an extranet and wireless services business from TMX Group. Terms of the agreement were not disclosed, and the transaction is expected to close within 90 days, subject to regulatory approvals. The financial impact of the transaction will be immaterial and was included in ICE’s financial guidance for 2017.\n\nIn February 2017, ICE announced that it had entered into a definitive agreement to acquire the Global Research division’s index platform from Bank of America Merrill Lynch. The BofAML indices are the second most used fixed income indices by assets under management (AUM) globally, and upon closing, the AUM benchmarked against the combined fixed income index business of ICE will be nearly $1 trillion. Upon closing, the indices will be re-branded as the ICE BofAML indices. The terms of the agreement were not disclosed, and the transaction is expected to be completed in the second half of 2017. The financial impact of the transaction is expected to be immaterial in 2017. In October 2017 ICE announced it has completed its acquisition.\n\nIn October 2017, ICE announced it had acquired a 4.7% stake in Euroclear for EUR 275 million. ICE anticipates having one representative join the Board of Euroclear. Euroclear is a leading provider of post-trade services, including settlement, central securities depositories and related services for cross-border transactions across asset classes. It is understood ICE increased its stake to 10% in early 2018.\n\nIn October 2017, ICE announced it had entered into an agreement to acquire Virtu BondPoint from Virtu Financial for $400 million in cash. The acquisition was reportedly completed on January 2, 2018 as announced by ICE. BondPoint is an integrated, automated platform offering additional fixed income execution services and one of the broadest sets of fixed income instruments.\n\nIn April 2018, ICE announced that it had entered into an agreement to acquire the Chicago Stock Exchange (CHX), a full-service stock exchange, including trading, data and corporate listings services. The transaction is expected to close in the second quarter of 2018, subject to regulatory approvals. Terms of the transaction were not disclosed, and the financial impact will not be material to ICE or impact capital return plans.\n\nIn May 2018, ICE announced that it had entered into an agreement to acquire TMC Bonds LLC for $685 million in cash. Established in 2000, TMC Bonds is a fixed income marketplace, supporting anonymous trading in various asset classes including Municipals, Corporates, Treasuries, Agencies and Certificates of Deposit. The transaction is expected to close in the second half of 2018, subject to customary regulatory and anti-trust approvals, and is not expected to materially impact 2018 financial results or capital returns.\n\nIn October 2018, ICE announced that it had acquired the remaining equity of MERSCORP Holding, Inc., owner of Mortgage Electronic Registration Systems, Inc. (MERS). MERSCORP owns and operates the MERS System, a national electronic registry that tracks the changes in servicing rights and beneficial ownership interests in U.S.-based mortgage loans. ICE had owned a majority equity interest in MERS since 2016. Earlier this month, ICE successfully moved the MERS System infrastructure to the ICE Mahwah data center, an integral requirement for completing the final acquisition of the business. Price and terms of the transaction were not disclosed and will not be material to ICE’s earnings or have an impact on capital return plans.\n\nICE provides exchange trading and clearing services in a number of different markets. Its main products include:\n\n\nThe company is split into the following subsidiaries:\n\n\nIn June 2016 Intercontinental Exchange introduced the expanded ICE Data Services, bringing together proprietary exchange data, valuations, analytics, desktop tools and connectivity solutions from across ICE, New York Stock Exchange (NYSE), SuperDerivatives and Interactive Data (IDC).\n\nICE originally formed its ICE Data subsidiary in 2003, recognizing the rising demand for exchange data as markets became increasingly automated. ICE continues to invest in its data services to address evolving customer needs driven by regulatory reform, market fragmentation, passive investing and indexation, along with increased demand for data capacity and security, and independent valuations. Their customers include global financial institutions, asset managers, commercial hedging firms, risk managers, corporate issuers and individual investors.\n\nICE Data Services has offices in California, New York, Chicago, Bedford MA, London, Dublin, Tel Aviv, Hong Kong, Singapore, Tokyo and Melbourne.\n\nIn August 2018 Intercontinental Exchange announced that it planned to form a new company, Bakkt, which is intended to leverage Microsoft cloud solutions to create an open and regulated, global ecosystem for digital assets. The new company will work with a marquee group of organizations including Boston Consulting Group (BCG), Microsoft, Starbucks, and others, to create an integrated platform that enables consumers and institutions to buy, sell, store and spend digital assets on a seamless global network.\n\nThe Bakkt ecosystem is expected to include federally regulated markets and warehousing along with merchant and consumer applications. Its first use cases will be for trading and conversion of Bitcoin versus fiat currencies, as Bitcoin is today the most liquid digital currency. The effort is designed to address evolving needs in the estimated $270 billion digital asset marketplace.\n\nAs an initial component of the Bakkt offering, Intercontinental Exchange’s U.S.-based futures exchange and clearing house plan to launch a 1-day physically delivered Bitcoin contract along with physical warehousing in November 2018, subject to CFTC review and approval. These regulated venues will establish new protocols for managing the specific security and settlement requirements of digital currencies. In addition, the clearing house plans to create a separate guarantee fund that will be funded by Bakkt.\n\nIn addition to Intercontinental Exchange and M12, Microsoft’s venture capital arm, investors in Bakkt are expected to include, among others, an affiliate of Fortress Investment Group, Eagle Seven, Galaxy Digital, Horizons Ventures, Alan Howard, Pantera Capital, Protocol Ventures, and Susquehanna International Group, LLP.\n\n\n"}
{"id": "28013617", "url": "https://en.wikipedia.org/wiki?curid=28013617", "title": "Jordan Radioactive Storage Facility", "text": "Jordan Radioactive Storage Facility\n\nThe Jordan Radioactive Storage Facility is a proposed national storage facility in Amman for Jordan's radioactive waste and nuclear materials.\n\nThe agreement between Jordan Atomic Energy Commission (JAEC) and the US Department of Energy's (DoE's) Pacific Northwest National Laboratory (PNNL) was signed by Ned Xoubi, nuclear fuel cycle commissioner at JAEC, and Daniel Rutherford, contract manager at PNNL.\n\nThe project, which is expected to be completed by the end of 2009, will comprise approximately four thousands square feet of storage facility that will, in turn, host Jordan's radioactive waste and nuclear sources in a safe and secure environment for the next five decades. All radioactive waste will be managed, stored and monitored in strict accordance with the best international standards and the International Atomic Energy Agency (IAEA) guidelines.\n\nThe facility is the first of its kind in Jordan and is being designed and constructed by Jordanian engineers with the help of American experts. JAEC will provide the project manager Ned Xoubi, and the DoE will provide JAEC with $370,000 for the construction of the central storage facility.\n"}
{"id": "525062", "url": "https://en.wikipedia.org/wiki?curid=525062", "title": "Jožef Stefan Institute", "text": "Jožef Stefan Institute\n\nThe Jožef Stefan Institute (IJS) () is the largest research institute in Slovenia. The main research areas are physics, chemistry, molecular biology, biotechnology, information technologies, reactor physics, energy and environment. At the beginning of the 2013 the institute had 962 employees, 404 of them were Ph.D scientists.\n\nThe mission of the Jožef Stefan Institute is the accumulation and dissemination of knowledge at the frontiers of natural science and technology to the benefit of society at large through the pursuit of education, learning, research, and development of high technology at the highest international levels of excellence.\n\nThe institute was founded by Yugoslav State Security in 1949 for atomic weapons research. Initially, the Vinča Nuclear Institute in Belgrade was established in 1948, followed by Rudjer Boskovic in Zagreb in 1949 and the Jožef Stefan Institute as an Institute for Physics in the Slovenian Academy of Sciences and Arts. It is named after the distinguished 19th-century physicist Jožef Stefan, best known for his work on the Stefan-Boltzmann law of black-body radiation.\n\nIJS is today involved in a wide variety of fields of scientific and economic interest. After close to 60 years of scientific achievement, the institute has become part of the image of Slovenia.\n\nOver the last 60 years it has created a number of important institutions, such as the University of Nova Gorica, the Jožef Stefan International Postgraduate School and the Ljubljana Technology park.\n\n\nThe institute has facilities in two locations. The main facilities and the headquarters are on Jamova 39 in Ljubljana, the other location is the Institute's Reactor Center Podgorica located in Dol near Ljubljana.\n\n"}
{"id": "1402016", "url": "https://en.wikipedia.org/wiki?curid=1402016", "title": "Linear aeration", "text": "Linear aeration\n\nLinear aeration is a relatively new aeration process; it allows water to penetrate the soil and to be retained in the proper amounts. Linear aeration also adds organic nutrition, soil softeners (humus, topsoil, compost, sand, clay, etc.) if necessary.\n\nIn linear aeration, the necessary organic matter is added on top of the soil surface, between plantings. The grooves are then cut into the soil to let the amendments and additives enter the soil; and recovered in the same passage.\n\nLinear aeration also alleviates excessive water in lawn areas. In linear aeration, the necessary organic matter is added on top of the turf. Next, grooves are cut into the turf to let the soil additives enter the soil; the grooves are recovered in the same passage.\n\n\n"}
{"id": "2196648", "url": "https://en.wikipedia.org/wiki?curid=2196648", "title": "Lithium hydride", "text": "Lithium hydride\n\nLithium hydride is an inorganic compound with the formula LiH. This alkali metal hydride is a colorless solid, although commercial samples are grey. Characteristic of a salt-like (ionic) hydride, it has a high melting point, and it is not soluble but reactive with all organic and protic solvents. It is soluble and nonreactive with certain molten salts such as lithium fluoride, lithium borohydride, and sodium hydride. With a molecular mass of slightly less than 8.0, it is the lightest ionic compound.\n\nLiH is a diamagnetic and an ionic conductor with a conductivity gradually increasing from at 443 °C to 0.18 Ωcm at 754 °C; there is no discontinuity in this increase through the melting point. The dielectric constant of LiH decreases from 13.0 (static, low frequencies) to 3.6 (visible-light frequencies). LiH is a soft material with a Mohs hardness of 3.5. Its compressive creep (per 100 hours) rapidly increases from < 1% at 350 °C to > 100% at 475 °C, meaning that LiH can't provide mechanical support when heated.\n\nThe thermal conductivity of LiH decreases with temperature and depends on morphology: the corresponding values are 0.125 W/(cm·K) for crystals and 0.0695 W/(cm·K) for compacts at 50 °C, and 0.036 W/(cm·K) for crystals and 0.0432 W/(cm·K) for compacts at 500 °C. The linear thermal expansion coefficient is 4.2/°C at room temperature.\n\nLiH is produced by treating lithium metal with hydrogen gas:\n\nThis reaction is especially rapid at temperatures above 600 °C. Addition of 0.001–0.003% carbon, or/and increasing temperature or/and pressure, increases the yield up to 98% at 2-hour residence time. However, the reaction proceeds at temperatures as low as 29 °C. The yield is 60% at 99 °C and 85% at 125 °C, and the rate depends significantly on the surface condition of LiH.\n\nLess common ways of LiH synthesis include thermal decomposition of lithium aluminium hydride (200 °C), lithium borohydride (300 °C), n-butyllithium (150 °C), or ethyllithium (120 °C), as well as several reactions involving lithium compounds of low stability and available hydrogen content.\n\nChemical reactions yield LiH in the form of lumped powder, which can be compressed into pellets without a binder. More complex shapes can be produced by casting from the melt. Large single crystals (about 80 mm long and 16 mm in diameter) can be then grown from molten LiH powder in hydrogen atmosphere by the Bridgman–Stockbarger technique. They often have bluish color owing to the presence of colloidal Li. This color can be removed by post-growth annealing at lower temperatures (~550 °C) and lower thermal gradients. Major impurities in these crystals are Na (20–200 parts per million, ppm), O (10–100 ppm), Mg (0.5–6 ppm), Fe (0.5-2 ppm) and Cu (0.5-2 ppm).\nBulk cold-pressed LiH parts can be easily machined using standard techniques and tools to micrometer precision. However, cast LiH is brittle and easily cracks during processing.\n\nLiH powder reacts rapidly with air of low humidity, forming LiOH, and . In moist air the powder ignites spontaneously, forming a mixture of products including some nitrogenous compounds. The lump material reacts with humid air, forming a superficial coating, which is a viscous fluid. This inhibits further reaction, although the appearance of a film of \"tarnish\" is quite evident. Little or no nitride is formed on exposure to humid air. The lump material, contained in a metal dish, may be heated in air to slightly below 200 °C without igniting, although it ignites readily when touched by an open flame. The surface condition of LiH, presence of oxides on the metal dish, etc., have a considerable effect on the ignition temperature. Dry oxygen does not react with crystalline LiH unless heated strongly, when an almost explosive combustion occurs.\n\nLiH is highly reactive toward water and other protic reagents:\n\nLiH is less reactive with water than Li and thus is a much less powerful reducing agent for water, alcohols, and other media containing reducible solutes. This is true for all the binary saline hydrides.\n\nLiH pellets slowly expand in moist air, forming LiOH; however, the expansion rate is below 10% within 24 hours in a pressure of 2 Torr of water vapor. If moist air contains carbon dioxide, then the product is lithium carbonate. LiH reacts with ammonia, slowly at room temperature, but the reaction accelerates significantly above 300 °C. LiH reacts slowly with higher alcohols and phenols, but vigorously with lower alcohols.\n\nLiH reacts with sulfur dioxide:\nthough above 50 °C the product is lithium dithionite.\n\nLiH reacts with acetylene to form lithium carbide and hydrogen. With anhydrous organic acids, phenols and acid anhydrides LiH reacts slowly, producing hydrogen gas and the lithium salt of the acid. With water-containing acids, LiH reacts faster than with water. Many reactions of LiH with oxygen-containing species yield LiOH, which in turn irreversibly reacts with LiH at temperatures above 300 °C:\n\nWith a hydrogen content in proportion to its mass three times that of NaH, LiH has the highest hydrogen content of any hydride. LiH is periodically of interest for hydrogen storage, but applications have been thwarted by its stability to decomposition. Thus removal of H requires temperatures above the 700 °C used for its synthesis, such temperatures are expensive to create and maintain. The compound was once tested as a fuel component in a model rocket.\n\nLiH is not usually a hydride-reducing agent, except in the synthesis of hydrides of certain metalloids. For example, silane is produced in the reaction of lithium hydride and silicon tetrachloride by the Sundermeyer process:\n\nLithium hydride is used in the production of a variety of reagents for organic synthesis, such as lithium aluminium hydride (LiAlH) and lithium borohydride (LiBH). Triethylborane reacts to give superhydride (LiBHEt).\n\nLithium hydride (LiH) is sometimes a desirable material for the shielding of nuclear reactors, with the isotope lithium-7 (Li-7), and it can be fabricated by casting.\n\nLithium deuteride, in the form of lithium-7 deuteride, is a good moderator for nuclear reactors, because deuterium (H-2) has a lower neutron absorption cross-section than ordinary hydrogen (H-1) does, and the cross-section for Li-7 is also low, decreasing the absorption of neutrons in a reactor. Lithium-7 is preferred for a moderator because it has a lower neutron capture cross-section, and it also forms less tritium (H-3) under bombardment with neutrons.\n\nThe corresponding lithium-6 deuteride, Li</sup>H, or LiD, is the primary fusion fuel in thermonuclear weapons. In hydrogen warheads of the Teller–Ulam design, a nuclear fission trigger explodes to heat and compress the lithium-6 deuteride, and to bombard the LiD with neutrons to produce tritium in an exothermic reaction: Li-6 + n --> He-4 + H-3. The deuterium and tritium (both isotopes of hydrogen) then fuse to produce helium-4, one neutron, and 17.59 MeV of free energy in the form of gamma rays, kinetic energy, etc. The helium is an inert byproduct.\n\nBefore the Castle Bravo nuclear weapons test in 1954, it was thought that only the less common isotope lithium-6 would breed tritium when struck with fast neutrons. The Castle Bravo test showed (accidentally) that the more plentiful lithium-7 also does so under extreme conditions, albeit by an endothermic reaction.\n\nLiH reacts violently with water to give hydrogen gas and LiOH, which is caustic. Consequently, LiH dust can explode in humid air, or even in dry air due to static electricity. At concentrations of in air the dust is extremely irritating to the mucous membranes and skin and may cause an allergic reaction. Because of the irritation, LiH is normally rejected rather than accumulated by the body.\n\nSome lithium salts, which can be produced in LiH reactions, are toxic. LiH fire should not be extinguished using carbon dioxide, carbon tetrachloride, or aqueous fire extinguishers; they should be smothered by covering with a metal object or graphite or dolomite powder. Sand is less suitable, as it can explode when mixed with burning LiH, especially if not dry. LiH is normally transported in oil, using containers made of ceramic, certain plastics or steel, and is handled in an atmosphere of dry argon or helium. Nitrogen can be used, but not at elevated temperatures, as it reacts with lithium. LiH normally contains some metallic lithium, which corrodes steel or silica containers at elevated temperatures.\n\n"}
{"id": "544255", "url": "https://en.wikipedia.org/wiki?curid=544255", "title": "Ludwig Boltzmann", "text": "Ludwig Boltzmann\n\nLudwig Eduard Boltzmann (; February 20, 1844 – September 5, 1906) was an Austrian physicist and philosopher whose greatest achievement was in the development of statistical mechanics, which explains and predicts how the properties of atoms (such as mass, charge, and structure) determine the physical properties of matter (such as viscosity, thermal conductivity, and diffusion).\n\nBoltzmann coined the term ergodic while he was working on a problem in statistical mechanics.\n\nBoltzmann was born in Vienna, the capital of the Austrian Empire. His father, Ludwig Georg Boltzmann, was a revenue official. His grandfather, who had moved to Vienna from Berlin, was a clock manufacturer, and Boltzmann's mother, Katharina Pauernfeind, was originally from Salzburg. He received his primary education from a private tutor at the home of his parents. Boltzmann attended high school in Linz, Upper Austria. When Boltzmann was 15, his father died.\n\nBoltzmann studied physics at the University of Vienna, starting in 1863. Among his teachers were Josef Loschmidt, Joseph Stefan, Andreas von Ettingshausen and Jozef Petzval. Boltzmann received his PhD degree in 1866 working under the supervision of Stefan; his dissertation was on the kinetic theory of gases. In 1867 he became a Privatdozent (lecturer). After obtaining his doctorate degree, Boltzmann worked two more years as Stefan's assistant. It was Stefan who introduced Boltzmann to Maxwell's work.\n\nIn 1869 at age 25, thanks to a letter of recommendation written by Stefan, he was appointed full Professor of Mathematical Physics at the University of Graz in the province of Styria. In 1869 he spent several months in Heidelberg working with Robert Bunsen and Leo Königsberger and in 1871 with Gustav Kirchhoff and Hermann von Helmholtz in Berlin. In 1873 Boltzmann joined the University of Vienna as Professor of Mathematics and there he stayed until 1876.\n\nIn 1872, long before women were admitted to Austrian universities, he met Henriette von Aigentler, an aspiring teacher of mathematics and physics in Graz. She was refused permission to audit lectures unofficially. Boltzmann advised her to appeal, which she did, successfully. On July 17, 1876 Ludwig Boltzmann married Henriette; they had three daughters and two sons. Boltzmann went back to Graz to take up the chair of Experimental Physics. Among his students in Graz were Svante Arrhenius and Walther Nernst. He spent 14 happy years in Graz and it was there that he developed his statistical concept of nature.\n\nBoltzmann was appointed to the Chair of Theoretical Physics at the University of Munich in Bavaria, Germany in 1890.\n\nIn 1894, Boltzmann succeeded his teacher Joseph Stefan as Professor of Theoretical Physics at the University of Vienna.\n\nBoltzmann spent a great deal of effort in his final years defending his theories. He did not get along with some of his colleagues in Vienna, particularly Ernst Mach, who became a professor of philosophy and history of sciences in 1895. That same year Georg Helm and Wilhelm Ostwald presented their position on \"Energetics\" at a meeting in Lübeck. They saw energy, and not matter, as the chief component of the universe. Boltzmann's position carried the day among other physicists who supported his atomic theories in the debate. In 1900, Boltzmann went to the University of Leipzig, on the invitation of Wilhelm Ostwald. After the retirement of Mach due to bad health, Boltzmann returned to Vienna in 1902. In 1903 he founded the Austrian Mathematical Society together with Gustav von Escherich and Emil Müller. His students included Karl Přibram, Paul Ehrenfest and Lise Meitner.\n\nIn Vienna, Boltzmann taught physics and also lectured on philosophy. Boltzmann's lectures on natural philosophy were very popular and received considerable attention. His first lecture was an enormous success. Even though the largest lecture hall had been chosen for it, the people stood all the way down the staircase. Because of the great successes of Boltzmann's philosophical lectures, the Emperor invited him for a reception at the Palace.\n\nIn the year 1906 his mental condition became so bad that he had to resign his position. He committed suicide on September 5, 1906 by hanging himself while on vacation with his wife and daughter in Duino, near Trieste (then Austria). He is buried in the Viennese Zentralfriedhof; his tombstone bears the inscription of the entropy formula:\n\nBoltzmann's kinetic theory of gases seemed to presuppose the reality of atoms and molecules, but almost all German philosophers and many scientists like Ernst Mach and the physical chemist Wilhelm Ostwald disbelieved their existence. During the 1890s Boltzmann attempted to formulate a compromise position which would allow both atomists and anti-atomists to do physics without arguing over atoms. His solution was to use Hertz's theory that atoms were \"Bilder\", that is, models or pictures. Atomists could think the pictures were the real atoms while the anti-atomists could think of the pictures as representing a useful but unreal model, but this did not fully satisfy either group. Furthermore, Ostwald and many defenders of \"pure thermodynamics\" were trying hard to refute the kinetic theory of gases and statistical mechanics because of Boltzmann's assumptions about atoms and molecules and especially statistical interpretation of the second law of thermodynamics.\n\nAround the turn of the century, Boltzmann's science was being threatened by another philosophical objection. Some physicists, including Mach's student, Gustav Jaumann, interpreted Hertz to mean that all electromagnetic behavior is continuous, as if there were no atoms and molecules, and likewise as if all physical behavior were ultimately electromagnetic. This movement around 1900 deeply depressed Boltzmann since it could mean the end of his kinetic theory and statistical interpretation of the second law of thermodynamics.\n\nAfter Mach's resignation in Vienna in 1901, Boltzmann returned there and decided to become a philosopher himself to refute philosophical objections to his physics, but he soon became discouraged again. In 1904 at a physics conference in St. Louis most physicists seemed to reject atoms and he was not even invited to the physics section. Rather, he was stuck in a section called \"applied mathematics\", he violently attacked philosophy, especially on allegedly Darwinian grounds but actually in terms of Lamarck's theory of the inheritance of acquired characteristics that people inherited bad philosophy from the past and that it was hard for scientists to overcome such inheritance.\n\nIn 1905 Boltzmann corresponded extensively with the Austro-German philosopher Franz Brentano with the hope of gaining a better mastery of philosophy, apparently, so that he could better refute its relevancy in science, but he became discouraged about this approach as well.\n\nBoltzmann's most important scientific contributions were in kinetic theory, including the Maxwell–Boltzmann distribution for molecular speeds in a gas. In addition, Maxwell–Boltzmann statistics and the Boltzmann distribution over energies remain the foundations of classical statistical mechanics. They are applicable to the many phenomena that do not require quantum statistics and provide a remarkable insight into the meaning of temperature.\nMuch of the physics establishment did not share his belief in the reality of atoms and molecules — a belief shared, however, by Maxwell in Scotland and Gibbs in the United States; and by most chemists since the discoveries of John Dalton in 1808. He had a long-running dispute with the editor of the preeminent German physics journal of his day, who refused to let Boltzmann refer to atoms and molecules as anything other than convenient theoretical constructs. Only a couple of years after Boltzmann's death, Perrin's studies of colloidal suspensions (1908–1909), based on Einstein's theoretical studies of 1905, confirmed the values of Avogadro's number and Boltzmann's constant, and convinced the world that the tiny particles really exist.\n\nTo quote Planck, \"The logarithmic connection between entropy and probability was first stated by L. Boltzmann in his kinetic theory of gases\". This famous formula for entropy \"S\" is\n\nwhere \"k\" is Boltzmann's constant, and \"ln\" is the natural logarithm. \"W\" is \"Wahrscheinlichkeit\", a German word meaning the probability of occurrence of a macrostate or, more precisely, the number of possible microstates corresponding to the macroscopic state of a system — number of (unobservable) \"ways\" in the (observable) thermodynamic state of a system can be realized by assigning different positions and momenta to the various molecules. Boltzmann's paradigm was an ideal gas of \"N\" \"identical\" particles, of which \"N\" are in the \"i\"th microscopic condition (range) of position and momentum. \"W\" can be counted using the formula for permutations\n\nwhere \"i\" ranges over all possible molecular conditions. (formula_4 denotes factorial.) The \"correction\" in the denominator is because identical particles in the same condition are indistinguishable.\n\nBoltzmann was also one of the founders of quantum mechanics due to his suggestion in 1877 that the energy levels of a physical system could be discrete.\n\nThe equation for \"S\" is engraved on Boltzmann's tombstone at the Vienna Zentralfriedhof — his second grave.\n\nThe Boltzmann equation was developed to describe the dynamics of an ideal gas.\n\nwhere \"ƒ\" represents the distribution function of single-particle position and momentum at a given time (see the Maxwell–Boltzmann distribution), \"F\" is a force, \"m\" is the mass of a particle, \"t\" is the time and \"v\" is an average velocity of particles.\n\nThis equation describes the temporal and spatial variation of the probability distribution for the position and momentum of a density distribution of a cloud of points in single-particle phase space. (See Hamiltonian mechanics.) The first term on the left-hand side represents the explicit time variation of the distribution function, while the second term gives the spatial variation, and the third term describes the effect of any force acting on the particles. The right-hand side of the equation represents the effect of collisions.\n\nIn principle, the above equation completely describes the dynamics of an ensemble of gas particles, given appropriate boundary conditions. This first-order differential equation has a deceptively simple appearance, since \"ƒ\" can represent an arbitrary single-particle distribution function. Also, the force acting on the particles depends directly on the velocity distribution function \"ƒ\". The Boltzmann equation is notoriously difficult to integrate. David Hilbert spent years trying to solve it without any real success.\n\nThe form of the collision term assumed by Boltzmann was approximate. However, for an ideal gas the standard Chapman–Enskog solution of the Boltzmann equation is highly accurate. It is expected to lead to incorrect results for an ideal gas only under shock wave conditions.\n\nBoltzmann tried for many years to \"prove\" the second law of thermodynamics using his gas-dynamical equation — his famous H-theorem. However the key assumption he made in formulating the collision term was \"molecular chaos\", an assumption which breaks time-reversal symmetry as is necessary for \"anything\" which could imply the second law. It was from the probabilistic assumption alone that Boltzmann's apparent success emanated, so his long dispute with Loschmidt and others over Loschmidt's paradox ultimately ended in his failure.\n\nFinally, in the 1970s E. G. D. Cohen and J. R. Dorfman proved that a systematic (power series) extension of the Boltzmann equation to high densities is mathematically impossible. Consequently, nonequilibrium statistical mechanics for dense gases and liquids focuses on the Green–Kubo relations, the fluctuation theorem, and other approaches instead.\n\nThe idea that the second law of thermodynamics or \"entropy law\" is a law of disorder (or that dynamically ordered states are \"infinitely improbable\") is due to Boltzmann's view of the second law of thermodynamics.\n\nIn particular, it was Boltzmann's attempt to reduce it to a stochastic collision function, or law of probability following from the random collisions of mechanical particles. Following Maxwell, Boltzmann modeled gas molecules as colliding billiard balls in a box, noting that with each collision nonequilibrium velocity distributions (groups of molecules moving at the same speed and in the same direction) would become increasingly disordered leading to a final state of macroscopic uniformity and maximum microscopic disorder or the state of maximum entropy (where the macroscopic uniformity corresponds to the obliteration of all field potentials or gradients). The second law, he argued, was thus simply the result of the fact that in a world of mechanically colliding particles disordered states are the most probable. Because there are so many more possible disordered states than ordered ones, a system will almost always be found either in the state of maximum disorder – the macrostate with the greatest number of accessible microstates such as a gas in a box at equilibrium – or moving towards it. A dynamically ordered state, one with molecules moving \"at the same speed and in the same direction\", Boltzmann concluded, is thus \"the most improbable case conceivable...an infinitely improbable configuration of energy.\" \n\nBoltzmann accomplished the feat of showing that the second law of thermodynamics is only a statistical fact. The gradual disordering of energy is analogous to the disordering of an initially ordered pack of cards under repeated shuffling, and just as the cards will finally return to their original order if shuffled a gigantic number of times, so the entire universe must some-day regain, by pure chance, the state from which it first set out. (This optimistic coda to the idea of the dying universe becomes somewhat muted when one attempts to estimate the timeline which will probably elapse before it spontaneously occurs.) The tendency for entropy increase seems to cause difficulty to beginners in thermodynamics, but is easy to understand from the standpoint of the theory of probability. Consider two ordinary dice, with both sixes face up. After the dice are shaken, the chance of finding these two sixes face up is small (1 in 36); thus one can say that the random motion (the agitation) of the dice, like the chaotic collisions of molecules because of thermal energy, causes the less probable state to change to one that is more probable. With millions of dice, like the millions of atoms involved in thermodynamic calculations, the probability of their all being sixes becomes so vanishingly small that the system \"must\" move to one of the more probable states. However, mathematically the odds of all the dice results not being a pair sixes is also as hard as the ones of all of them being sixes, and since statistically the data tend to balance, one in every 36 pairs of dice will tend to be a pair of sixes, and the cards -when shuffled- will sometimes present a certain temporary sequence order even if in its whole the deck was disordered.\n\nIn 1885 he became a member of the Imperial Austrian Academy of Sciences and in 1887 he became the President of the University of Graz. He was elected a member of the Royal Swedish Academy of Sciences in 1888 and a Foreign Member of the Royal Society (ForMemRS) in 1899. The following are named in his honour:\n\n\n\n\n"}
{"id": "37875353", "url": "https://en.wikipedia.org/wiki?curid=37875353", "title": "MER-6A Emergency Rocket Communications System", "text": "MER-6A Emergency Rocket Communications System\n\nThe Mobile Electronics Rocket-6A or MER-6A, Emergency Rocket Communications System (ERCS) provided a reliable and survivable emergency communications method for the United States National Command Authority, using a UHF repeater placed atop a Blue Scout rocket.\n\nThe Blue Scout version of ERCS (Program 279) was deployed to three sites near Wisner, West Point, and Tekamah, Nebraska.\n\n"}
{"id": "29885279", "url": "https://en.wikipedia.org/wiki?curid=29885279", "title": "Maria Elena Foronda Farro", "text": "Maria Elena Foronda Farro\n\nMaria Elena Foronda Farro is a Peruvian sociologist and environmentalist. She was awarded the Goldman Environmental Prize in 2003, for her campaigns of improving waste treatment from the country's fishmeal industry.\n"}
{"id": "5435827", "url": "https://en.wikipedia.org/wiki?curid=5435827", "title": "Michael Sladek", "text": "Michael Sladek\n\nMichael Sladek (born 1947) is a German doctor and bearer of the Bundesverdienstkreuz.\n\nHe became famous by realising a grid-independent system for producing electricity, by distributed little power plants. \nFor this he was awarded 1996 by the German magazine \"Capital\" with the Capital/ WWF - Umweltpreis. In 1999 he and his wife Ursula Sladek were awarded with the Nuclear-Free Future Award. In January 2004 the Sladek couple was awarded the highest order in Germany, the Bundesverdienstkreuz, for their great engagement for the environment.\n\nWith his system that combines an efficiency-strategy with a power saving strategy it became possible to satisfy the power consumption of the community Schönau in the Black-Forest. \nFollowing his engagement supported by his wife and many friends the first German green power-provider came into existence, the EWS Schönau.\nThe community of Schönau was the first community on Earth in a Western civilised country that became independent of the national power grid and could decide how the power will be produced.\n\n"}
{"id": "8883812", "url": "https://en.wikipedia.org/wiki?curid=8883812", "title": "Minister of Innovation, Energy and Mines (Manitoba)", "text": "Minister of Innovation, Energy and Mines (Manitoba)\n\nThe Minister of Science, Technology, Energy and Mines was a cabinet minister in the province of Manitoba, Canada.\n\nThe position was created in September 2006, and incorporated responsibilities from the former portfolios of Industry, Economic Development and Mines and Energy, Science and Technology. In November 2009, the portfolio was renamed Innovation, Energy and Mines. In October 2013, the functions of this department were redistributed between the Minister of Mineral Resources, the Minister of Jobs and the Economy and the Minister of Municipal Government (Energy division).\n"}
{"id": "12268191", "url": "https://en.wikipedia.org/wiki?curid=12268191", "title": "Ontario Forest Research Institute", "text": "Ontario Forest Research Institute\n\nOntario Forest Research Institute (OFRI) is a division of the Ministry of Natural Resources and Forestry (MNRF) located in Sault Ste. Marie, Ontario, Canada. This institute is composed of research scientists, specialists, statisticians, technicians, management, and administrative staff. OFRI research helps provide sustainable management of Ontario Forests and Natural Resources.\n"}
{"id": "38183925", "url": "https://en.wikipedia.org/wiki?curid=38183925", "title": "Organogels", "text": "Organogels\n\nAn organogel is a class of gel composed of a liquid organic phase within a three-dimensional, cross-linked network. Organogel networks can form in two ways. The first is classic gel network formation via polymerization. This mechanism converts a precursor solution of monomers with various reactive sites into polymeric chains that grow into a single covalently-linked network. At a critical concentration (the gel point), the polymeric network becomes large enough so that on the macroscopic scale, the solution starts to exhibit gel-like physical properties: an extensive continuous solid network, no steady-state flow, and solid-like rheological properties. However, organogels that are “low molecular weight gelators” can also be designed to form gels via self-assembly. Secondary forces, such as van der Waals or hydrogen bonding, cause monomers to cluster into a non-covalently bonded network that retains organic solvent, and as the network grows, it exhibits gel-like physical properties. Both gelation mechanisms lead to gels characterized as organogels.\n\nGelation mechanism greatly influences the typical organogel properties. Since precursors with multiple functional groups polymerize into networks of covalent C-C bonds (on average 85 kcal/mol), networks formed by self-assembly, which relies on secondary forces (generally less than 10 kcal/mol), are less stable. Theorists also have difficulties predicting characteristic gelation parameters, such as gel point and gelation time, with a single and simple equation. Gel point, the transition point from a polymer solution to gel, is a function of the extent of reaction or the fraction of functional groups reacted. Gelation time is the time interval between the onset of reaction– by heating, addition of catalyst into a liquid system, etc.– and gel point. Kinetic and statistical mathematical theories have had moderate success in predicting gelation parameters; a simple, accurate, and widely applicable theory has not yet been developed.\n\nThis article will first discuss the details of organogels formation and the variables of the characteristic gelation parameters as they relate to organogels. Then, various methods used to characterize organogels will be explained. Finally, we will review the use of organogels in various industries.\n\nThe formulation of an accurate theory of gel formation that correctly predicts gelation parameters (such as time, rate, and structure) of a broad range of materials is highly sought after for both commercial and intellectual reasons. As noted earlier, researchers often judge gel theories based upon their ability to accurately predict gel points. The kinetic and statistical methods model gel formation with different mathematical approaches. most researchers used statistical methods, as the equations derived thereby are less cumbersome and contain variables to which specific physical meanings can be attached, thus aiding in the analysis of gel formation theory. Below, we present the classical Flory-Stockmayer (FS) statistical theory for gel formation. This theory, despite its simplicity, has found widespread use. This is due in large part to small increases in accuracy provided by the use of more complicated methods, and to its being a general model which can be applied to many gelation systems. Other gel formation theories cased on different chemical approximations have also been derived. However, the FS model has better simplicity, wide applicability, and accuracy, and remains the most used.\n\nThe kinetic (or coagulation) approach preserves the integrity of any and all structures created during network formation. Thus, an infinite set of differential rate equations (one for each possible structure, of which there essentially infinite) must be created in order to treat gel systems kinetically. Consequently, exact solutions for kinetic theories can be obtained for only the most basic systems.\n\nHowever, numerical answers to kinetic systems can be given via Monte Carlo methods. In general, kinetic treatments of gelation result in large, unwieldy, and dense sets of equations that give answers not discernibly better than those given by the statistical approach. A major drawback of the kinetic approach is that it treats the gel as essentially one giant, rigid molecule, and cannot actively simulate characteristic structures of gels such as elastic and dangling chains. Kinetic models have mostly fallen out of use given how clumsy the equations become in everyday use. Interested readers however, are directed to the following papers for further reading on a specific kinetic model.\n\nThe statistical approach views the phase change from liquid to gel as a uniform process throughout the fluid. That is, polymerization reactions are occurring all throughout the solution, with each reaction having an equal chance of occurring. Statistical theories try to determine the fraction of the total possible bonds that need to be made before an infinite polymer network can appear. The classic statistical theory first developed by Flory rested on two critical assumptions.\n\n\nUsing the above assumptions, let us examine a homopolymerization reaction starting from a single monomer with z-functional groups with a fraction p of all possible bonds already having been formed. The polymer we create follows the form of a Cayley tree or Bethe lattice – known from the field of statistical mechanics. The number of branches from each node is determined by the amount of functional groups, z, on our monomer. As we follow the tree’s branches we want there to always be at least one path that leads onwards, as this is the condition of an infinite network polymer. At each node, there are z-1 possible paths, since one functional group was used to create the node. The probability that at least one of the possible paths has been created is (z-1)p. Since we want an infinite network, we require on average that (z-1)p ≥ 1 to ensure an infinitely long path. Therefore, the FS model predicts the critical point (p) to be:\n\nPhysically, p is the fraction of all possible bonds that can be made. So a p of ½ means that the first point in time that an infinite network will be able to exist will be when ½ of all possible bonds have been made by the monomers.\n\nThis equation is derived for the simple case of a self-reacting monomer with a single type of reacting group A. The Flory model was further refined by Stockmayer to include multifunctional monomers. However, the same two assumptions were kept. Thus, the classical statistical gel theory has come to be known as the Flory-Stockmayer (FS). The FS model gives the following equations for a bifunctional polymer system, and can be generalized to branch units of any amount of functionality following the steps laid out by Stockmayer.\n\nWhere p and p are the fraction of all possible A and B bonds respectively and r (which must be less than 1) is the ratio of reactive sites of A and B on each monomer. If the starting concentrations of A and B reactive sites are the same, then pp can be condensed to p and values for the fraction of all bonds at which an infinite network will form can be found.\n\nf and f are defined as above, where N are the number of moles of Ai containing f functional groups for each type of A functional molecule.\n\nTypically, gels are synthesized via sol-gel processing, a wet-chemical technique involving a colloidal solution (sol) that acts as the precursor for an integrated network (gel). There are two possible mechanisms whereby organogels form depending on the physical intermolecular inter-actions, namely the fluid-filled fiber and the solid fiber mechanism. The main difference is in the starting materials, i.e. surfactant in apolar solvent versus solid organogelator in apolar solvent. Surfactant or surfactant mixture forms reverse micelles when mixed with an apolar solvent. The fluid-fiber matrix forms when a polar solvent (e.g. water) is added to the reverse micelles to encourage the formation of tubular reverse micelle structures. As more polar solvent is added, the reverse micelles elongate and entangle to form organogel. Gel formation via solid-fiber matrix, on the other hand, forms when the mixture of organogelators in apolar solvent is heated to give apolar solution of organogelator and then cooled down below the solubility limit of the organogelators. The organogelators precipitate out as fibers, forming a 3-dimensional network which then immobilizes the apolar solvent to produce organogels. Table 1 lists the type of organogelators and the properties of the organogels synthesized.\n\nGelation times vary depending on the organogelators and medium. One can promote or delay gelation by influencing the molecular self-assembly of organogelators in a system.\nMolecular self-assembly is a process by which molecules adopt a defined arrangement without guidance or management from an external source. The organogelators may undergo physical or chemical interactions so as to form self-assembled fibrous structures in which they become entangled with each other, resulting in the formation of a three-dimensional network structure. It is believed that the self-assembly is governed by non-covalent interactions, such as hydrogen bonding, hydrophobic forces, van der Waals forces, π-π interactions etc. Although molecular self-assembly is not fully understood so far, researchers have demonstrated by adjusting certain aspects of the system, one is able to promote or inhibit self-assembly in organogelator molecules.\n\nOrganogelators can be divided into two groups based on whether or not they form hydrogen bonds. Hydrogen bond forming organogelators include, amino acids/amides/urea moieties and carbohydrates whereas non-hydrogen bond forming organogelators (e.g. π-π stacking) include anthracene-, anthraquinone- and steroid-based molecules.\nSolubility and/or solvent-molecule interactions play an important role in promoting organogelator self-assembly. Hirst et al. showed that the solubility of the gelators in media can be modified by tuning the peripheral protecting groups of the gelators, which in turn controls the gel point and the concentrations at which crosslinking takes place (See Table 2 for data). Gelators that have higher solubility in medium show less preference for crosslinking. These gelators (Figure 1) are less effective and require higher total concentrations to initiate the process. In addition, solvent-molecule interactions also modulate the level of self-assembly. This was shown by Hirst et al. in the NMR binding model as well as in SAXS/SANS results.\nGarner et al. explored the importance of organogelator structures using 4-tertbutyl-1-aryl cy-clohexanol derivatives showing that a phenyl group in an axial configuration induces gelation, unlike derivatives with the phenyl group in equatorial configuration. Polymeric organogelators can induce gelation even at very low concentrations (less than 20 g/L) and the self-assembly capability could be customized by modifying the chemical structure of the polymer backbone.\n\nBy manipulating the solvent-molecule interactions, one can promote molecular self-assembly of the organogelator and therefore gelation. Although this is the traditionally used approach, it has limitations. There are still no reliable models that describe the gelation for all kinds of organogelators in all media. An alternate approach is to promote self-assembly by triggering changes in intermolecular interactions, i.e. cis-trans isomerization, hydrogen bonding, donor-acceptor π-π stacking interaction, electrostatic interactions etc. Matsumoto et al. and Hirst et al. have reported gelation using light-induced isomerization and by incorporating additives into the system to influence molecular packing, respectively.\n\nMatsumoto et al. used UV light to trigger trans–cis photoisomerization of fumaric amide units causing self-assembly or disassembly to a gel or the corresponding sol, respectively (See Figure 2). Hirst et al., on the other hand, introduced a two-component system, where inserting a second component into the system changed the gelator’s behavior. This had effectively controlled the molecular self-assembly process. \n\nChen et al. designed a system that would undergo self-assembly by triggering changes in intermolecular interactions. They used an oxidation-induced planarization to trigger gelator self-assembly and gelation through donor-acceptor π-stacking interaction. The interesting part is that both strong oxidants such as cerium(IV) ammonium nitrate and weak oxidants like nitric oxide, NO can induce gelation. Figure 3 shows the oxidation of dihydropyridine catalyzed/induced by NO. NO has been used as an analyte or biomarker for disease detection, and the discovery of NO’s role in analyte-triggered gelation system no doubt has opened new doors to the world of chemical sensing. \n\nGels are characterized from two different perspectives. First, the physical structure of the gel is determined. This is followed by a characterization of the gel’s mechanical properties. The former generally affects the mechanical properties of gels.\n\nThis is a reliable technique for measuring the strength of the intermolecular interactions in gels. Gel network strength is proportional to the magnitude of enthalpy change (ΔH). A higher ΔH means a more tightly bonded network while a smaller enthalpy value means a network made of weaker bonds.\n\nThere are numerous microscopy methods for defining gel structures which include SEM and TEM. Use of microscopic techniques can directly determine the physical parameters of the gel matrix. These include measurements of pore diameter, wall thickness and shape of the gel network. Use of SEM can distinguish between gels that have a fibrous network as opposed to those that have a three-dimensional cross linked structure. It must be noted that microscopy techniques may not yield quantitatively accurate results. If a high vacuum is used during imaging, the liquid solvent can be removed from the gel matrix-inducing strain to the gel which leads to physical deformation. Use of an environmental SEM, which operates at higher pressures, can yield higher quality imaging.\n\nTwo scattering techniques for indirectly measuring gel parameters are small angle X-ray scattering (SARS/SAXS) and small angle neutron scattering (SANS). SARS works exactly like X-ray scattering (XRD) except small angles (0.1-10.0 °) are used. The challenge with small angles is in separating the scattering pattern from the main beam. In SANS, the procedure is the same as SARS except that a neutron beam is used instead of an x-ray beam. One advantage of using a neutron beam as opposed to an x-ray beam is an increased signal to noise ratio. It also provides the ability for isotope labeling because the neutrons interact with the nuclei instead of the electrons. By analyzing the scattering pattern direct information about the size of the material can be obtained. Both SARS and SANS provide useful data on the atomic scale at 50-250 and 10-1000 Å respectively. These distances are perfectly suited for studying the physical parameters of gels.\n\nThere are numerous methods to characterize the material properties of a gel. These are briefly summarized below.\n\nHardness or stiffness of the gel is measured by placing a metal ball on top of the material and the hardness of the material depends on the amount of indentation caused by the ball.\n\nThis technique utilizes a similar approach when compared to ball indentation only on a significantly small scale. The tip is lowered into the sample and a laser reflecting off the cantilever allows for precise measurements to be obtained.\n\nIn this technique, the tensile strength of the gel is measured in one direction. The two important measurements to make include the force applied per unit area and the amount of elongation under a known applied force. This test provides information for how a gel will respond when an external force is applied.\n\nDue to varying degrees of cross-linkage in a gel network, different gels display different visocoelastic properties. A material containing viscoelastic properties undergoes both viscous and elastic changes when a deformation occurs. Viscosity can be thought of as a time dependent process of a material deforming to a more relaxed state while elasticity is an instantaneous process. The viscoelastic properties of gels mean that they undergo time dependent structural changes in response to a physical deformation. Two techniques for measuring viscoelasticity are broadband viscoelastic spectroscopy (BVS) and resonant ultrasound spectroscopy (RUS). In both techniques, a damping mechanism is resolved with both differing frequency and time in order to determine the viscoelastic properties of the material.\n\nOrganogels are useful in applications such as:\n\nAn undesirable example of organogel formation is wax crystallization in petroleum.\n"}
{"id": "27964795", "url": "https://en.wikipedia.org/wiki?curid=27964795", "title": "Pentadienyl", "text": "Pentadienyl\n\nIn chemistry, pentadienyl refers to the organic radical, anion, or cation with the formula [CHCHCHCHCH], where \"z\" = 0, −1, +1, respectively.\n\nIn organometallic chemistry, the pentadienyl anion is a ligand, the acyclic analogue of the more common cyclopentadienyl. It is generated by deprotonation of pentadiene. A number of complexes are known, including the analogue of ferrocene, Fe(CH). Only few pentadienyl complexes feature CH ligands. More common is the dimethyl analogue 2,4-MeCH. Additionally, many pentadienyl ligands are cyclic, being derived from the addition of hydride to \"η\"-arene complexes or hydride abstraction from cyclohexadiene complexes.\n\nThe first pentadienyl complex to be reported was derived from protonolysis of a complex of pentadienol:\nTreatment of this cation with sodium borohydride gives the pentadiene complex:\n\nIn organic chemistry, the pentadienyl radical, , is of some significance as an especially stabilized radical. The radical is delocalized over five carbon centers. Consequently the C−H bond in the diene (C\"H\"(CH=CH)) is especially weak. Fats derivatives containing this \"doubly allylic\" group are collectively called drying oils. They tend to polymerize in a useful way upon exposure to air. \n\nCyclooxygenases (\"COX\") are enzymes that generate prostanoids, including thromboxane and prostaglandins such as prostacyclin. Aspirin and ibuprofen exert their effects through inhibition of COX.\n"}
{"id": "21896171", "url": "https://en.wikipedia.org/wiki?curid=21896171", "title": "Power optimizer", "text": "Power optimizer\n\nA power optimizer is a DC to DC converter technology developed to maximize the energy harvest from solar photovoltaic or wind turbine systems. They do this by individually tuning the performance of the panel or wind turbine through maximum power point tracking, and optionally tuning the output to match the performance of the string inverter. Power optimizers are especially useful when the performance of the power generating components in a distributed system will vary widely, such as due to differences in equipment, shading of light or wind, or being installed facing different directions or widely separated locations.\n\nPower optimizers for solar applications can be similar to microinverters in that both systems attempt to isolate individual panels in order to improve overall system performance. A smart module is a power optimizer integrated into a solar module. A microinverter essentially combines a power optimizer with a small inverter in a single enclosure that is used on every panel, while the power optimizer leaves the inverter in a separate box and uses only one inverter for the entire array. The claimed advantage to this \"hybrid\" approach is lower overall system costs, avoiding the distribution of electronics.\n\nMost energy production or storage devices have a complex relationship between the power they produce, the load placed on them, and the efficiency of the delivery. A conventional battery, for instance, stores energy in chemical reactions in its electrolytes and plates. These reactions take time to occur, which limits the rate at which the power can be efficiently drawn from the cell. For this reason, large batteries used for power storage generally list two or more capacities, normally the \"2 hour\" and \"20 hour\" rates, with the 2 hour rate often being around 50% of the 20 hour rate.\n\nSolar panels have similar issues due to the speed at which the cell can convert solar photons into electrons, ambient temperature, and a host of other issues. In this case there is a complex non-linear relationship between voltage, current and the total amount of power being produced, the \"I-V curve\". In order to optimize collection, modern solar arrays use a technique known as \"maximum power point tracking\" (MPPT) to monitor the total output of the array and continually adjust the presented load to keep the system operation at its peak efficiency point.\n\nTraditionally, solar panels produce voltages around 30 V. This is too low to be effectively converted into AC to feed to the power grid. To address this, panels are strung together in series to increase the voltage to something more appropriate for the inverter being used, typically about 600 V.\n\nThe drawback to this approach is that MPPT system can only be applied to the array as a whole. Because the I-V curve is non-linear, a panel that is even slightly shadowed can have dramatically lower output, and greatly increase its internal resistance. As the panels are wired in series, this would cause the output of the entire string to be reduced due to the increased total resistance. This change in performance causes the MPPT system to change the operation point, moving the rest of the panels away from their best performance.\n\nBecause of their sequential wiring, power mismatch between PV modules within a string can lead to a drastic and disproportionate loss of power from the entire solar array, in some cases leading to complete system failure. Shading of as little as 9% of the entire surface array of a PV system can, in some circumstances, lead to a system-wide power loss of as much as 54%. Although this problem is most notable with \"large\" events like a passing shadow, even the tiniest differences in panel performance, due to dirt, differential aging or tiny differences during manufacturing, can make the array as a whole operate away from its best MPPT point. \"Panel matching\" is an important part of solar array design.\n\nThese problems have led to a number of different potential solutions that isolate panels individually or into much smaller groups (2 to 3 panels) in an effort to provide MPPT that avoids the problems of large strings.\n\nOne solution, the microinverter, places the entire power conversion system directly on the back of each panel. This allows the system to track the MPPT for each panel, and directly output AC power that matches the grid. The panels are then wired together in parallel, so even the failure of one of the panels or microinverters will not lead to a loss of power from the string. However, this approach has the disadvantage of distributing the power conversion circuitry, which, in theory, is the expensive part of the system. Microinverters, at least as late as early 2011, had significantly higher price per watt.\n\nThis leads, naturally, to the power optimizer concept, where only the MPPT system is distributed to the panels. In this case the conversion from DC to AC takes place in a single inverter, one that lacks the MPPT hardware or has it disabled. Advanced solutions are able to work correctly with all solar inverters, to make possible optimisation of already installed plants. According to its supporters, this \"hybrid\" approach produces the lowest-cost solution overall, while still maintaining the advantages of the microinverter approach.\n\nPower optimizers are essentially DC-DC converters, taking the DC power from a solar panel at whatever voltage and current is optimal (via MPPT), then converting that to a different voltage and current that best suits the central / string inverter.\n\nSome power optimizers are designed to work in conjunction with a central inverter from the same manufacturer, which allows the inverter to communicate with the optimizers to ensure that the inverter always receives the same total voltage from the panel string. In this situation, if there is a string of panels in series and a single panel's output drops due to shade, its voltage will drop so that it can deliver the same amount of current (amps). This would cause the string voltage to drop as well, except that the central inverter adjusts all the other optimizers so that their output voltage increases slightly, maintaining the fixed string voltage required at the inverter (just at reduced available amperage while the single panel is shaded). The down side of this type of optimizer is that it requires a central inverter from the same manufacturer as the optimizers, so it is not possible to gradually retrofit these in an existing installation unless the inverter is also replaced, as well as optimizers installed on all panels at the same time.\n\nAccording to an IMS Research report from September 2012, BMSolar, Tigo Energy, SolarEdge Technologies Inc. and Enphase Energy accounted for more than 90 percent of micro inverter and power optimizer shipments in 2011. According to Greentech Media, another emerging player in the field of DC-DC optimizers for utility scale solar applications is Philadelphia-area based Alençon Systems LLC with its SPOT family of products.\n\n"}
{"id": "26129602", "url": "https://en.wikipedia.org/wiki?curid=26129602", "title": "Puertollano Photovoltaic Park", "text": "Puertollano Photovoltaic Park\n\nThe Puertollano Photovoltaic Park is the fourth largest photovoltaic power station in the world, with a nominal capacity of 47.6 MW. The facility is located in Puertollano, Spain. 476 individual plants with a nominal power of 100 kWp, Suntech and Solaria modules. Fixed structure oriented at 33° south with a total of 231,653 panels. \n\n"}
{"id": "2282444", "url": "https://en.wikipedia.org/wiki?curid=2282444", "title": "Rare-earth magnet", "text": "Rare-earth magnet\n\nRare-earth magnets are strong permanent magnets made from alloys of rare-earth elements. Developed in the 1970s and 1980s, rare-earth magnets are the strongest type of permanent magnets made, producing significantly stronger magnetic fields than other types such as ferrite or alnico magnets. The magnetic field typically produced by rare-earth magnets can exceed 1.4 teslas, whereas ferrite or ceramic magnets typically exhibit fields of 0.5 to 1 tesla. There are two types: neodymium magnets and samarium–cobalt magnets. Magnetostrictive rare-earth magnets such as Terfenol-D also have applications, e.g. in loudspeakers. Rare-earth magnets are extremely brittle and also vulnerable to corrosion, so they are usually plated or coated to protect them from breaking, chipping, or crumbling into powder.\n\nThe development of rare-earth magnets began around 1966, when K. J. Strnat and G. Hoffer of the US Air Force Materials Laboratory discovered that an alloy of yttrium and cobalt, YCo, had by far the largest magnetic anisotropy constant of any material then known. The term \"rare earth\" can be misleading, as these metals are not particularly rare or precious; they are about as abundant as tin or lead. However rare earth ores are unevenly distributed, with the major source being China, which has led countries to classify rare earth metals as strategically important. Recent Chinese export restrictions on these materials have led other countries to initiate research programs to develop strong magnets that do not require them. \n\nThe rare-earth (lanthanide) elements are metals that are ferromagnetic, meaning that like iron they can be magnetized to become permanent magnets, but their Curie temperatures (the temperature above which their ferromagnetism disappears) are below room temperature, so in pure form their magnetism only appears at low temperatures. However, they form compounds with the transition metals such as iron, nickel, and cobalt, and some of these compounds have Curie temperatures well above room temperature. Rare-earth magnets are made from these compounds.\n\nThe greater strength of rare-earth magnets is mostly due to two factors. First, their crystalline structures have very high magnetic anisotropy. This means that a crystal of the material preferentially magnetizes along a specific crystal axis but is very difficult to magnetize in other directions. Like other magnets, rare-earth magnets are composed of microcrystalline grains, which are aligned in a powerful magnetic field during manufacture, so their magnetic axes all point in the same direction. The resistance of the crystal lattice to turning its direction of magnetization gives these compounds a very high magnetic coercivity (resistance to being demagnetized).\n\nSecond, atoms of rare-earth elements can have high magnetic moments because their orbital electron structure contains many unpaired electrons, in contrast to other elements, in which almost all of the electrons exist in pairs with opposite spins, so their magnetic fields cancel out. This is a consequence of incomplete filling of the f-shell, which can contain up to 7 unpaired electrons. In a magnet it is the unpaired electrons, aligned so they spin in the same direction, which generate the magnetic field. This gives the materials high remanence (saturation magnetization \"J\"). The maximal energy density \"BH\" is proportional to \"J\", so these materials have the potential for storing large amounts of magnetic energy. The magnetic energy product \"BH\" of neodymium magnets is about 18 times greater than \"ordinary\" magnets by volume. This allows rare-earth magnets to be smaller than other magnets with the same field strength.\n\nSome important properties used to compare permanent magnets are: remanence (\"B\"), which measures the strength of the magnetic field; coercivity (\"H\"), the material's resistance to becoming demagnetized; energy product (\"BH\"), the density of magnetic energy; and Curie temperature (\"T\"), the temperature at which the material loses its magnetism. Rare-earth magnets have higher remanence, much higher coercivity and energy product, but (for neodymium) lower Curie temperature than other types. The table below compares the magnetic performance of the two types of rare-earth magnets, neodymium (NdFeB) and samarium-cobalt (SmCo), with other types of permanent magnets.\n\nSource: \n\nSamarium–cobalt magnets (chemical formula: SmCo), the first family of rare-earth magnets invented, are less used than neodymium magnets because of their higher cost and lower magnetic field strength. However, samarium–cobalt has a higher Curie temperature, creating a niche for these magnets in applications where high field strength is needed at high operating temperatures. They are highly resistant to oxidation, but sintered samarium-cobalt magnets are brittle and prone to chipping and cracking and may fracture when subjected to thermal shock.\n\nNeodymium magnets, invented in the 1980s, are the strongest and most affordable type of rare-earth magnet. They are made of an alloy of neodymium, iron, and boron (NdFeB), sometimes abbreviated as NIB. Neodymium magnets are used in numerous applications requiring strong, compact permanent magnets, such as electric motors for cordless tools, hard disk drives, magnetic holddowns, and jewelry clasps. They have the highest magnetic field strength and have a higher coercivity (which makes them magnetically stable), but they have a lower Curie temperature and are more vulnerable to oxidation than samarium–cobalt magnets. Corrosion can cause unprotected magnets to spall off a surface layer or to crumble into a powder. Use of protective surface treatments such as gold, nickel, zinc, and tin plating and epoxy-resin coating can provide corrosion protection.\n\nOriginally, the high cost of these magnets limited their use to applications requiring compactness together with high field strength. Both the raw materials and the patent licenses were expensive. However, since the 1990s, NIB magnets have become steadily less expensive, and the low cost has inspired new uses such as magnetic construction toys.\n\nThe greater force exerted by rare-earth magnets creates hazards that are not seen with other types of magnet. Magnets larger than a few centimeters are strong enough to cause injuries to body parts pinched between two magnets or a magnet and a metal surface, even causing broken bones. Magnets allowed to get too near each other can strike each other with enough force to chip and shatter the brittle material, and the flying chips can cause injuries. There have even been cases where young children who have swallowed several magnets have had a fold of the digestive tract pinched between the magnets, causing injury and in one case intestinal perforations, sepsis and death. \n\nThe U.S. Consumer Product Safety Commission passed a rule restricting rare-earth magnet size in consumer products, but it was vacated by a US Federal court decision in November 2016.\n\nSince their prices became competitive in the 1990s, neodymium magnets have been replacing Alnico and ferrite magnets in the many applications in modern technology requiring powerful magnets. Their greater strength allows smaller and lighter magnets to be used for a given application.\n\nCommon applications of rare-earth magnets include:\n\nOther applications of rare-earth magnets include:\n\nThe United States Department of Energy has identified a need to find substitutes for rare-earth metals in permanent-magnet technology and has begun funding such research. The Advanced Research Projects Agency-Energy (ARPA-E) has sponsored a Rare Earth Alternatives in Critical Technologies (REACT) program, to develop alternative materials. In 2011, ARPA-E awarded 31.6 million dollars to fund Rare-Earth Substitute projects.\n\n\n"}
{"id": "11866002", "url": "https://en.wikipedia.org/wiki?curid=11866002", "title": "Reduced moderation water reactor", "text": "Reduced moderation water reactor\n\nThe Reduced-Moderation Water Reactor (RMWR), also referred to as the Resource-renewable BWR, is a proposed type of light water moderated nuclear power reactor, featuring some characteristics of a fast neutron reactor, thereby combining the established and proven technology of light water reactors with the desired features of fast neutron reactors. The RMWR concept builds upon the Advanced Boiling Water Reactor and is under active development in theoretical studies, particularly in Japan. Hitachi and the Japan Atomic Energy Agency are both involved in research.\n\nEven in Generation II PWRs, the neutron spectrum is not fully thermalised. The goal of the RMWR is to depart further from the thermal neutron spectrum in order to achieve a breeder ratio of slightly greater than one, so that after the initial fuel charge no enrichment of the uranium input to the fuel cycle is required. The RMWR concept is dependent on nuclear fuel reprocessing in order to achieve its objective of a resource renewable fuel cycle. Hitachi has proposed the FLUOREX process as reprocessing technology for this purpose, instead of the more conventional Purex technology. \n\nIn contrast to regular light water reactors and in order to achieve a harder neutron spectrum, which is optimal for breeding purposes, the RMWR uses hexagonal fuel assemblies and Y-shaped control rods. The fuel is MOX (Mixed Oxide), consisting of 18% plutonium, which is surrounded by depleted uranium in the blanket region. \n\nThe RMWR has an electric output of 1356 MW, or 3926 MW.\n\nAnother RMWR breeder design intends on closing the nuclear fuel cycle by mixing thorium with reprocessed transuranics, which include plutonium, in a thorium containing MOX fuel. The neutron speed would be in the spectrum that could purportedly transmutate the long-lived fission products like Tc-99 & I-127 and as the neutron spectrum is hard/fast enough, to also be capable of burning the usually troublesome minor actinides quite efficiently.\n\nHitachi announced a collaboration with three U.S. universities on the development of the RBWR/RMWR in 2014.\n\n\n"}
{"id": "29461662", "url": "https://en.wikipedia.org/wiki?curid=29461662", "title": "San Antonio de Ureca", "text": "San Antonio de Ureca\n\nSan Antonio de Ureca, also known as Ureka or Ureca is a village in Bioko Sur, Equatorial Guinea, south of Malabo on the island of Bioko.\nThe town of Ureka is included among the wettest areas in the world; it receives about 10,450 millimeters (418 ins) of rainfall annually. It is the wettest place in Africa.\n"}
{"id": "10980696", "url": "https://en.wikipedia.org/wiki?curid=10980696", "title": "Separator (oil production)", "text": "Separator (oil production)\n\nThe term separator in oilfield terminology designates a pressure vessel used for separating well fluids produced from oil and gas wells into gaseous and liquid components. A separator for petroleum production is a large vessel designed to separate production fluids into their constituent components of oil, gas and water. A separating vessel may be referred to in the following ways: Oil and gas separator, Separator, Stage separator, Trap, Knockout vessel (Knockout drum, knockout trap, water knockout, or liquid knockout), Flash chamber (flash vessel or flash trap), Expansion separator or expansion vessel, Scrubber (gas scrubber), Filter (gas filter). These separating vessels are normally used on a producing lease or platform near the wellhead, manifold, or tank battery to separate fluids produced from oil and gas wells into oil and gas or liquid and gas. An oil and gas separator generally includes the following essential components and features:\n\n1. A vessel that includes (a) primary separation device and/or section, (b) secondary “gravity” settling (separating) section, (c) mist extractor to remove small liquid particles from the gas, (d) gas outlet, (e) liquid settling (separating) section to remove gas or vapor from oil (on a three-phase unit, this section also separates water from oil), (f) oil outlet, and (g) water outlet (three-phase unit).\n\n2. Adequate volumetric liquid capacity to handle liquid surges (slugs) from the wells and/or flowlines.\n\n3. Adequate vessel diameter and height or length to allow most of the liquid to separate from the gas so that the mist extractor will not be flooded.\n\n4. A means of controlling an oil level in the separator, which usually includes a liquid-level controller and a diaphragm motor valve on the oil outlet.\n\n5. A back pressure valve on the gas outlet to maintain a steady pressure in the vessel.\n\n6. Pressure relief devices.\n\nSeparators work on the principle that the three components have different densities, which allows them to stratify when moving slowly with gas on top, water on the bottom and oil in the middle. Any solids such as sand will also settle in the bottom of the separator. The functions of oil and gas separators can be divided into the primary and secondary functions which will be discussed later on.\n\nOil and gas separators can have three general configurations: vertical, horizontal, and spherical.\nVertical separators can vary in size from 10 or 12 inches in diameter and 4 to 5 feet seam to seam (S to S) up to 10 or 12 feet in diameter and 15 to 25 feet S to S. Horizontal separators may vary in size from 10 or 12 inches in diameter and 4 to 5 feet S to S up to 15 to 16 feet in diameter and 60 to 70 feet S to S. Spherical separators are usually available in 24 or 30 inch up to 66 to 72 inch in diameter.\nHorizontal oil and gas separators are manufactured with monotube and dual-tube shells. Monotube units have one cylindrical shell, and dual-tube units have two cylindrical parallel shells with one above the other. Both types of units can be used for two-phase and three-phase service. A monotube horizontal oil and gas separator is usually preferred over a dual-tube unit. The monotube unit has greater area for gas flow as well as a greater oil/gas interface area than is usually available in a dual-tube separator of comparable price. The monotube separator will usually afford a longer retention time because the larger single-tube vessel retains a larger volume of oil than the dual-tube separator. It is also easier to clean than the dualtube unit.\nIn cold climates, freezing will likely cause less trouble in the monotube unit because the liquid is usually in close contact with the warm stream of gas flowing through the separator. The monotube design normally has a lower silhouette than the dual-tube unit, and it is easier to stack them for multiple-stage separation on offshore platforms where space is limited. It was illustrated by Powers \"et al\" (1990) that vertical separators should be constructed such that the flow stream enters near the top and passes through a gas/liquid separating chamber even though they are not competitive alternatives unlike the horizontal separators.\n\nThe three configurations of separators are available for two-phase operation and three-phase operation. In the two-phase units, gas is separated from the liquid with the gas and liquid being discharged separately. Oil and gas separators are mechanically designed such that the liquid and gas components are separated from the hydrocarbon steam at specific temperature and pressure according to Arnold \"et al\" (2008). In three-phase separators, well fluid is separated into gas, oil, and water with the three fluids being discharged separately. The gas-liquid separation section of the separator is determined by the maximum removal droplet size using the Souders–Brown equation with an appropriate K factor. The oil-water separation section is held for a retention time that is provided by laboratory test data, pilot plant operating procedure, or operating experience. In the case where the retention time is not available, the recommended retention time for three-phase separator in API 12J is used. The sizing methods by K factor and retention time give proper separator sizes. According to Song \"et al\" (2010), engineers sometimes need further information for the design conditions of downstream equipment, i.e., liquid loading for the mist extractor, water content for the crude dehydrator/desalter or oil content for the water treatment.\n\nOil and gas separators can operate at pressures ranging from a high vacuum to 4,000 to 5,000 psi. Most oil and gas separators operate in the pressure range of 20 to 1,500 psi.Separators may be referred to as low pressure, medium pressure, or high pressure. Low-pressure separators\nusually operate at pressures ranging from 10 to 20 up to 180 to 225 psi. Medium-pressure separators usually operate at pressures ranging from 230 to 250 up to 600 to 700 psi. High-pressure separators generally operate in the wide pressure range from 750 to 1,500 psi.\n\nOil and gas separators may be classified according to application as test separator, production separator, low temperature separator, metering separator, elevated separator, and stage separators (first stage, second stage, etc.).\n\n\nA test separator is used to separate and to meter the well fluids. The test separator can be referred to as a well tester or well checker. Test separators can be vertical, horizontal, or spherical. They can be two-phase or three-phase. They can be permanently installed or portable (skid or trailer mounted). Test separators can be equipped with various types of meters for measuring the oil, gas, and/or water for potential tests, periodic production tests, marginal well tests, etc.\n\n\nA production separator is used to separate the produced well fluid from a well, group of wells, or a lease on a daily or continuous basis. Production separators can be vertical, horizontal, or spherical. They can be two-phase or three-phase. Production separators range in size from 12 in. to 15 ft in diameter, with most units ranging from 30 in. to 10 ft in diameter. They range in length from 6 to 70 ft, with most from 10 to 40 ft long.\n\n\nA low-temperature separator is a special one in which high-pressure well fluid is jetted into the vessel through a choke or pressure reducing valve so that the separator temperature is reduced appreciably below the well-fluid temperature. The temperature reduction is obtained by the Joule–Thomson effect of expanding well fluid as it flows through the pressure-reducing choke or valve into the separator. The lower operating temperature in the separator causes condensation of vapors that otherwise would exit the separator in the vapor state. Liquids thus recovered require stabilization to prevent excessive evaporation in the storage tanks.\n\n\nThe function of separating well fluids into oil, gas, and water and metering the liquids can be accomplished in one vessel. These vessels are commonly referred to as metering separators and are available for two-phase and three-phase operation. These units are available in special models that make them suitable for accurately metering foaming and heavy viscous oil.\n\nSeparation of oil from gas may begin as the fluid flows through the producing formation into the well bore and may progressively increase through the tubing, flow lines, and surface handling equipment. Under certain conditions, the fluid may be completely separated into liquid and gas before it reaches the oil and gas separator. In such cases, the separator vessel affords only an \"enlargement\" to permit gas to ascend to one outlet and liquid to descend to another.\n\nDifference in density of the liquid and gaseous hydrocarbons may accomplish acceptable separation in an oil and gas separator. However, in some instances, it is necessary to use mechanical devices commonly referred to as \"mist extractors\" to remove liquid mist from the gas before\nit is discharged from the separator. Also, it may be desirable or necessary to use some means to remove non solution gas from the oil before the oil is discharged from the separator.\n\nThe physical and chemical characteristics of the oil and its conditions of pressure and temperature determine the amount of gas it will contain in solution. The rate at which the gas is liberated from a given oil is a function of change in pressure and temperature. The volume of gas that an oil and gas separator will remove from crude oil is dependent on (1) physical and chemical characteristics of the crude, (2) operating pressure, (3) operating temperature, (4) rate of throughput, (5) size and configuration of the separator, and (6) other factors.\n\nAgitation, heat, special baffling, coalescing packs, and filtering materials can assist in the removal of nonsolution gas that otherwise may be retained in the oil because of the viscosity and surface tension of the oil. Gas can be removed from the top of the drum by virtue of being gas. Oil and water are separated by a baffle at the end of the separator, which is set at a height close to the oil-water contact, allowing oil to spill over onto the other side, while trapping water on the near side. The two fluids can then be piped out of the separator from their respective sides of the baffle. The produced water is then either injected back into the oil reservoir, disposed of, or treated. The bulk level (gas–liquid interface) and the oil water interface are determined using instrumentation fixed to the vessel. Valves on the oil and water outlets are controlled to ensure the interfaces are kept at their optimum levels for separation to occur. The separator will only achieve bulk separation. The smaller droplets of water will not settle by gravity and will remain in the oil stream. Normally the oil from the separator is routed to a coalescer to further reduce the water content.\n\nThe production of water with oil continues to be a problem for engineers and the oil producers. Since 1865 when water was coproduced with hydrocarbons, separation of valuable hydrocarbons from disposable water has challenged and frustrated the oil industry. According to Rehm \"et al\" (1983), innovation over the years has led from the skim pit to installation of the stock tank, to the gunbarrel, to the freewater knockout, to the hay-packed coalescer and most recently to the Performax Matrix Plate Coalescer, an enhanced gravity settling separator. The history of water treating for the most part has been sketchy and spartan. There is little economic value to the produced water, and it represents an extra cost for the producer to arrange for its disposal. Today oil fields produce greater quantities of water than they produce oil. Along with greater water production are emulsions and dispersions which are more difficult to treat. The separation process becomes interlocked with a myriad of contaminants as the last drop of oil is being recovered from the reservoir. In some instances it is preferable to separate and to remove water from the well fluid before it flows through pressure reductions, such as those caused by chokes and valves. Such water removal may prevent difficulties that could be caused downstream by the water, such as corrosion which can be referred to as being a chemical reactions that occurs whenever a gas or liquid chemically attacks an exposed metallic surface. Corrosion is usually accelerated by warm temperatures and likewise by the presence of acids and salts. Other factors that affect the removal of water from oil include hydrate formation and the formation of tight emulsion that may be difficult to resolve into oil and water. The water can be separated from the oil in a three-phase separator by use of chemicals and gravity separation. If the three-phase separator is not large enough to separate\nthe water adequately, it can be separated in a free-water knockout vessel installed upstream or downstream of the separators.\n\nFor an oil and gas separator to accomplish its primary functions, pressure must be maintained in the separator so that the liquid and gas can be discharged into their respective processing or gathering systems. Pressure is maintained on the separator by use of a gas backpressure valve on each separator or with one master backpressure valve that controls the pressure on a battery of two or more separators. The optimum pressure to maintain on a separator is the pressure that will result in the highest economic yield from the sale of the liquid and gaseous hydrocarbons.\n\nTo maintain pressure on a separator, a liquid seal must be effected in the lower portion of the vessel. This liquid seal prevents loss of gas with the oil and requires the use of a liquid-level controller and a valve.\n\nEffective oil-gas separation is important not only to ensure that the required export quality is achieved but also to prevent problems in downstream process equipment and compressors. Once the bulk liquid has been knocked out, which can be achieved in many ways, the remaining liquid droplets are separated from by a demisting device. Until recently the main technologies used for this application were reverse-flow cyclones, mesh pads and vane packs. More recently new devices with higher gas-handling have been developed which have enabled potential reduction in the scrubber vessel size. There are several new concepts currently under development in which the fluids are degassed upstream of the primary separator. These systems are based on centrifugal and turbine technology and have additional advantages in that they are compact and motion insensitive, hence ideal for floating production facilities. Below are some of the ways in which oil is separated from gas in separators.\n\nNatural gas is lighter than liquid hydrocarbon. Minute particles of liquid hydrocarbon that are temporarily suspended in a stream of natural gas will, by density difference or force of gravity, settle out of the stream of gas if the velocity of the gas is sufficiently slow. The larger droplets of hydrocarbon will quickly settle out of the gas, but the smaller ones will take longer. At standard conditions of pressure and temperature, the droplets of liquid hydrocarbon may have a density 400 to 1,600 times that of natural gas. However, as the operating pressure and temperature increase, the difference in density decreases. At an operating pressure of 800 psig, the liquid hydrocarbon may be only 6 to 10 times as dense as the gas. Thus, operating pressure materially affects the size of the separator and the size and type of mist extractor required to separate adequately the liquid and gas. The fact that the liquid droplets may have a density 6 to 10 times that of the gas may indicate that droplets of liquid would quickly settle out of and separate from the gas. However, this may not occur because the particles of liquid may be so small that they tend to \"float\" in the gas and may not settle out of the gas stream in the short period of time the gas is in the oil and gas separator. As the operating pressure on a separator increases, the density difference between the liquid and gas decreases. For this reason, it is desirable to operate oil and gas separators at as low a pressure as is consistent with other process variables, conditions, and requirements.\n\nIf a flowing stream of gas containing liquid, mist is impinged against a surface, the liquid mist may adhere to and coalesce on the surface. After the mist coalesces into larger droplets, the droplets will gravitate to the liquid section of the vessel. If the liquid content of the gas is high, or if the mist particles are extremely fine, several successive impingement surfaces may be required to effect satisfactory removal of the mist.\n\nWhen the direction of flow of a gas stream containing liquid mist is changed abruptly, inertia causes the liquid to continue in the original direction of flow. Separation of liquid mist from the gas thus can be effected because the gas will more readily assume the change of flow direction and will flow away from the liquid mist particles. The liquid thus removed may coalesce on a surface or fall to the liquid section below.\n\nSeparation of liquid and gas can be effected with either a sudden increase or decrease in gas velocity. Both conditions use the difference in inertia of gas and liquid. With a decrease in velocity, the higher inertia of the liquid mist carries it forward and away from the gas. The liquid may then coalesce on some surface and gravitate to the liquid section of the separator. With an increase in gas velocity, the higher inertia of the liquid causes the gas to move away from the liquid, and the liquid may fall to the liquid section of the vessel.\n\nIf a gas stream carrying liquid mist flows in a circular motion at sufficiently high velocity, centrifugal force throws the liquid mist outward against the walls of the container. Here the liquid coalesces into progressively larger droplets and finally gravitates to the liquid section below. Centrifugal force is one of the most effective methods of separating liquid mist from gas. However, according to Keplinger (1931), some separator designers have pointed out a disadvantage in that a liquid with a free surface rotating as a whole will have its surface curved around its lowest point lying on the axis of rotation. This created false level may cause difficulty in regulating the fluid level control on the separator. This is largely overcome by placing vertical quieting baffles which should extend from the bottom of the separator to above the outlet. Efficiency of this type of mist extractor increases as the velocity of the gas stream increases. Thus for a given rate of throughput, a smaller centrifugal separator will suffice.\n\nBecause of higher prices for natural gas, the widespread reliance on metering of liquid hydrocarbons, and other reasons, it is important to remove all nonsolution gas from crude oil during field processing. Methods used to remove gas from crude oil in oil and gas separators are discussed below:\n\nModerate, controlled agitation which can be defined as movement of the crude oil with sudden force is usually helpful in removing nonsolution gas that may be mechanically locked in the oil by surface tension and oil viscosity. Agitation usually will cause the gas bubbles to coalesce and to separate from the oil in less time than would be required if agitation were not used.\n\nHeat as a form of energy that is transferred from one body to another results in a difference in temperature. This reduces surface tension and viscosity of the oil and thus assists in releasing gas that is hydraulically retained in the oil. The most effective method of heating crude oil is to pass it through a heated-water bath. A spreader plate that disperses the oil into small streams or rivulets increases the effectiveness of the heated-water bath. Upward flow of the oil through the water bath affords slight agitation, which is helpful in coalescing and separating entrained gas from the oil. A heated-water bath is probably the most effective method of removing foam bubbles from foaming crude oil. A heated-water bath is not practical in most oil and gas separators, but heat can be added to the oil by direct or indirect fired heaters and/or heat exchangers, or heated free-water knockouts or emulsion treaters can be used to obtain a heated-water bath.\n\nCentrifugal force which can be defined as a fictitious force, peculiar to a particle moving on a circular path, that has the same magnitude and dimensions as the force that keeps the particle on its circular path (the centripetal force) but points in the opposite direction is effective in separating gas from oil. The heavier oil is thrown outward against the wall of the vortex retainer while the gas occupies the inner portion of the vortex. A properly shaped and sized vortex will allow the gas to ascend while the liquid flows downward\nto the bottom of the unit.\n\nThe direction of flow in and around a separator along with other flow instruments are usually illustrated on the Piping and instrumentation diagram, (P&ID). Some of these flow instruments include the Flow Indicator (FI), Flow Transmitter (FT) and the Flow Controller (FC). Flow is of paramount importance in the oil and gas industry because flow, as a major process variable is essentially important in that its understanding helps engineers come up with better designs and enables them to confidently carry out additional research. Mohan \"et al\" (1999) carried out a research into the design and development of separators for a three-phase flow system. The purpose of the study was to investigate the complex multiphase hydrodynamic flow behaviour in a three-phase oil and gas separator. A mechanistic model was developed alongside a computational fluid dynamics (CFD) simulator. These were then used to carry out a detailed experimentation on the three-phase separator. The experimental and CFD simulation results were suitably integrated with the mechanistic model. The simulation time for the experiment was 20 seconds with the oil specific gravity as 0.885, and the separator lower part length and diameter were 4-ft and 3-inches respectively. The first set of experiment became a basis through which detailed investigations were used to carry out and to conduct similar simulation studies for different flow velocities and other operating conditions as well.\n\nAs earlier stated, flow instruments that function with the separator in an oil and gas environment include the flow indicator, flow transmitter and the flow controller. Due to maintenance (which will be discussed later) or due to high usage, these flowmeters do need to be calibrated from time to time. Calibration can be defined as the process of referencing signals of known quantity that has been predetermined to suit the range of measurements required. Calibration can also be seen from a mathematical point of view in which the flowmeters are standardized by determining the deviation from the predetermined standard so as to ascertain the proper correction factors. In determining the deviation from the predetermined standard, the actual flowrate is usually first determined with the use of a master meter which is a type of flowmeter that has been calibrated with a high degree of accuracy or by weighing the flow so as to be able to obtain a gravimetric reading of the mass flow. Another type of meter used is the transfer meter. However, according to Ting \"et al\" (1989), transfer meters have been proven to be less accurate if the operating conditions are different from its original calibrated points. According to Yoder (2000), the types of flowmeters used as master meters include turbine meters, positive displacement meters, venturi meters, and Coriolis meters. In the U.S., master meters are often calibrated at a flow lab that has been certified by the National Institute of Standards and Technology, (NIST). NIST certification of a flowmeter lab means that its methods have been approved by NIST. Normally, this includes NIST traceability, meaning that the standards used in the flowmeter calibration process have been certified by NIST or are causally linked back to standards that have been approved by NIST. However, there is a general belief in the industry that the second method which involves the gravimetric weighing of the amount of fluid (liquid or gas) that actually flows through the meter into or out of a container during the calibration procedure is the most ideal method for measuring the actual amount of flow. Apparently, the weighing scale used for this method also has to be traceable to the National Institute of Standards and Technology (NIST) as well.\nIn ascertaining a proper correction factor, there is often no simple hardware adjustment to make the flowmeter start reading correctly. Instead, the deviation from the correct reading is recorded at a variety of flowrates. The data points are plotted, comparing the flowmeter output to the actual flowrate as determined by the standardized National Institute of Standards and Technology master meter or weigh scale.\n\nThe controls required for oil and gas separators are liquid level controllers for oil and oil/water interface (three-phase operation) and gas back-pressure control valve with pressure controller. Although the use of controls is expensive making the cost of operating fields with separators so high, installations has resulted in substantial savings in the overall operating expense as in the case of the 70 gas wells in the Big Piney, Wyo sighted by Fair (1968). The wells with separators were located above 7,200 ft elevation, ranging upward to 9,000 ft. Control installations were sufficiently automated such that the field operations around the controllers could be operated from a remote-control station at the field office using the Distributed Control System. All in all, this improved the efficiency of personnel and the operation of the field, with a corresponding increase in production from the area.\n\nThe valves required for oil and gas separators are oil discharge control valve, water-discharge control valve (three-phase operation), drain valves, block valves, pressure relief valves, and Emergency Shutdown valves (ESD). ESD valves typically stay in open position for months or years awaiting a command signal to operate. Little attention is paid to these valves outside of scheduled turnarounds. The pressures of continuous production often stretch these intervals even longer. This leads to build up or corrosion on these valves that prevents them from moving. For safety critical applications, it must be ensured that the valves operate upon demand.\n\nThe accessories required for oil and gas separators are pressure gauges, thermometers, pressure-reducing regulators (for control gas), level sight glasses, safety head with rupture disk, piping, and tubing.\n\nOil and gas separators should be installed at a safe distance from other lease equipment. Where they are installed on offshore platforms or in close proximity to other equipment, precautions should be taken to prevent injury to personnel and damage to surrounding equipment in case the\nseparator or its controls or accessories fail. The following safety features are recommended for most oil and gas separators.\n\n\nHigh- and low liquid-level controls normally are float-operated pilots that actuate a valve on the inlet to the separator, open a bypass around the separator, sound a warning alarm, or perform some other pertinent function to prevent damage that might result from high or low liquid levels in the separator.\n\n\nHigh- and low pressure controls are installed on separators to prevent excessively high or low pressures from interfering with normal operations. These high- and low-pressure controls can be mechanical, pneumatic, or electric and can sound a warning, actuate a shut-in valve, open a bypass, or perform other pertinent functions to protect personnel, the separator, and surrounding equipment.\n\n\nTemperature controls may be installed on separators to shut in the unit, to open or to close a bypass to a heater, or to sound a warning should the temperature in the separator become too high or too low. Such temperature controls are not normally used on separators, but they may be appropriate in special cases. According to Francis (1951), low-temperature controls in separators is another tools used by gas producers which finds its application in the high-pressure gas fields, usually referred to as \"vapour-phase\" reservoirs. Low temperatures obtainable from the expansion of these high-pressure gas streams are utilized to a profitable advantage. A more efficient recovery of the hydrocarbon condensate and a greater degree of dehydration of the gas as compared to the conventional heater and separator installation is a major advantage of low-temperature controls in oil and gas separators.\n\n\nA spring-loaded safety relief valve is usually installed on all oil and gas separators. These valves normally are set at the design pressure of the vessel. Safety relief valves serve primarily as a warning, and in most instances are too small to handle the full rated fluid capacity of the separator. Full-capacity safety relief valves can be used and are particularly recommended when no safety head (rupture disk) is used on the separator.\n\n\nA safety head or rupture disk is a device containing a thin metal membrane that is designed to rupture when the pressure in the separator exceeds a predetermined value. This is usually from 1 1/4 to 1% times the design pressure of the separator vessel. The safety head disk is usually selected so that it will not rupture until the safety relief valve has opened and is incapable of preventing excessive pressure buildup in the separator.\n\nOver the life of a production system, the separator is expected to process a wide range of produced fluids. With break through from water flood and expanded gas lift circulation, the produced fluid water cut and gas-oil ratio is ever changing. In many instances, the separator fluid loading may exceed the original design capacity of the vessel. As a result, many operators find their separator no longer able to meet the required oil and water effluent standards, or experience high liquid carry-over in the gas according to Power \"et al\" (1990). Some operational maintenance and considerations are discussed below:\n\nIn refineries and processing plants, it is normal practice to inspect all pressure vessels and piping periodically for corrosion and erosion. In the oil fields, this practice is not generally followed (they are inspected at a predetermined frequency, normally decided by an RBI assessment) and equipment is replaced only after actual failure. This policy may create hazardous conditions for operating personnel and surrounding equipment. It is recommended that periodic inspection schedules for all pressure equipment be established and followed to protect against undue failures.\n\nAll safety relief devices should be installed as close to the vessel as possible and in such manner that the reaction force from exhausting fluids will not break off, unscrew, or otherwise dislodge the safety device. The discharge from safety devices should not endanger personnel\nor other equipment.\n\nSeparators should be operated above hydrate-formation temperature. Otherwise hydrates may form in the vessel and partially or completely plug it thereby reducing the capacity of the separator. In some instances when the liquid or gas outlet is plugged or restricted, this causes the safety valve to open or the safety head to rupture. Steam coils can be installed in the liquid section of oil and gas separators to melt hydrates that may form there. This is especially appropriate on low-temperature separators.\n\nA separator handling corrosive fluid should be checked periodically to determine whether remedial work is required. Extreme cases of corrosion may require a reduction in the rated working pressure of the vessel. Periodic hydrostatic testing is recommended, especially if the fluids being handled are corrosive. Expendable anode can be used in separators to protect them against electrolytic corrosion. Some operators determine separator shell and head thickness with ultrasonic thickness indicators and calculate the maximum allowable working pressure from the remaining metal thickness. This should be done yearly offshore and every two to four years onshore.\n\n\n"}
{"id": "38976222", "url": "https://en.wikipedia.org/wiki?curid=38976222", "title": "Solar Hero", "text": "Solar Hero\n\nSolar Hero is a 2012 Canadian documentary film directed by Matt Keay. The documentary follows Team Alberta (Alberta Solar Decathlon Project) as they journey to in the 2009 US DOE Solar Decathlon Competition in Washington, DC in October 2009.\n\n\n\n"}
{"id": "32747596", "url": "https://en.wikipedia.org/wiki?curid=32747596", "title": "Structural coloration", "text": "Structural coloration\n\nStructural coloration is the production of colour by microscopically structured surfaces fine enough to interfere with visible light, sometimes in combination with pigments. For example, peacock tail feathers are pigmented brown, but their microscopic structure makes them also reflect blue, turquoise, and green light, and they are often iridescent.\n\nStructural coloration was first observed by English scientists Robert Hooke and Isaac Newton, and its principle – wave interference – explained by Thomas Young a century later. Young described iridescence as the result of interference between reflections from two or more surfaces of thin films, combined with refraction as light enters and leaves such films. The geometry then determines that at certain angles, the light reflected from both surfaces interferes constructively, while at other angles, the light interferes destructively. Different colours therefore appear at different angles.\n\nIn animals such as on the feathers of birds and the scales of butterflies, interference is created by a range of photonic mechanisms, including diffraction gratings, selective mirrors, photonic crystals, crystal fibres, matrices of nanochannels and proteins that can vary their configuration. Some cuts of meat also show structural coloration due to the exposure of the periodic arrangement of the muscular fibres. Many of these photonic mechanisms correspond to elaborate structures visible by electron microscopy. In the few plants that exploit structural coloration, brilliant colours are produced by structures within cells. The most brilliant blue coloration known in any living tissue is found in the marble berries of \"Pollia condensata\", where a spiral structure of cellulose fibrils produces Bragg's law scattering of light. The bright gloss of buttercups is produced by thin-film reflection by the epidermis supplemented by yellow pigmentation, and strong diffuse scattering by a layer of starch cells immediately beneath.\n\nStructural coloration has potential for industrial, commercial and military application, with biomimetic surfaces that could provide brilliant colours, adaptive camouflage, efficient optical switches and low-reflectance glass.\n\nIn his 1665 book \"Micrographia\", Robert Hooke described the \"fantastical\" colours of the peacock's feathers:\n\nIn his 1704 book \"Opticks\", Isaac Newton described the mechanism of the colours other than the brown pigment of peacock tail feathers. Newton noted that\n\nThomas Young (1773–1829) extended Newton's particle theory of light by showing that light could also behave as a wave. He showed in 1803 that light could diffract from sharp edges or slits, creating interference patterns.\n\nIn his 1892 book \"Animal Coloration\", Frank Evers Beddard (1858–1925) acknowledged the existence of structural colours:\n\nBut Beddard then largely dismissed structural coloration, firstly as subservient to pigments: \"in every case the [structural] colour needs for its display a background of dark pigment;\" and then by asserting its rarity: \"By far the commonest source of colour in invertebrate animals is the presence in the skin of definite pigments\", though he does later admit that the Cape golden mole has \"structural peculiarities\" in its hair that \"give rise to brilliant colours\".\n\nStructural coloration is caused by interference effects rather than by pigments. Colours are produced when a material is scored with fine parallel lines, formed of one or more parallel thin layers, or otherwise composed of microstructures on the scale of the colour's wavelength.\n\nStructural coloration is responsible for the blues and greens of the feathers of many birds (the bee-eater, kingfisher and roller, for example), as well as many butterfly wings, beetle wing-cases (elytra) and (while rare among flowers) the gloss of buttercup petals. These are often iridescent, as in peacock feathers and nacreous shells such as of pearl oysters (Pteriidae) and \"Nautilus\". This is because the reflected colour depends on the viewing angle, which in turn governs the apparent spacing of the structures responsible. Structural colours can be combined with pigment colours: peacock feathers are pigmented brown with melanin, while buttercup petals have both carotenoid pigments for yellowness and thin films for reflectiveness.\n\nIridescence, as explained by Thomas Young in 1803, is created when extremely thin films reflect part of the light falling on them from their top surfaces. The rest of the light goes through the films, and a further part of it is reflected from their bottom surfaces. The two sets of reflected waves travel back upwards in the same direction. But since the bottom-reflected waves travelled a little further – controlled by the thickness and refractive index of the film, and the angle at which the light fell – the two sets of waves are out of phase. When the waves are one or more whole wavelength apart – in other words at certain specific angles, they add (interfere constructively), giving a strong reflection. At other angles and phase differences, they can subtract, giving weak reflections. The thin film therefore selectively reflects just one wavelength – a pure colour – at any given angle, but other wavelengths – different colours – at different angles. So, as a thin-film structure like a butterfly's wing or bird's feather moves, it seems to change colour.\n\nA number of fixed structures can create structural colours, by mechanisms including diffraction gratings, selective mirrors, photonic crystals, crystal fibres and deformed matrices. Structures can be far more elaborate than a single thin film: films can be stacked up to give strong iridescence, to combine two colours, or to balance out the inevitable change of colour with angle to give a more diffuse, less iridescent effect. Each mechanism offers a specific solution to the problem of creating a bright colour or combination of colours visible from different directions.\nA diffraction grating constructed of layers of chitin and air gives rise to the iridescent colours of various butterfly wing scales as well as to the tail feathers of birds such as the peacock. Hooke and Newton were correct in their claim that the peacock's colours are created by interference, but the structures responsible, being close to the wavelength of light in scale (see micrographs), were smaller than the striated structures they could see with their light microscopes. Another way to produce a diffraction grating is with tree-shaped arrays of chitin, as in the wing scales of some of the brilliantly coloured tropical \"Morpho\" butterflies (see drawing). Yet another variant exists in \"Parotia lawesii\", Lawes's parotia, a bird of paradise. The barbules of the feathers of its brightly coloured breast patch are V-shaped, creating thin-film microstructures that strongly reflect two different colours, bright blue-green and orange-yellow. When the bird moves the colour switches sharply between these two colours, rather than drifting iridescently. During courtship, the male bird systematically makes small movements to attract females, so the structures must have evolved through sexual selection.\n\nPhotonic crystals can be formed in different ways. In \"Parides sesostris\", the emerald-patched cattleheart butterfly, photonic crystals are formed of arrays of nano-sized holes in the chitin of the wing scales. The holes have a diameter of about 150 nanometres and are about the same distance apart. The holes are arranged regularly in small patches; neighbouring patches contain arrays with differing orientations. The result is that these emerald-patched cattleheart scales reflect green light evenly at different angles instead of being iridescent. In \"Lamprocyphus augustus\", a weevil from Brazil, the chitin exoskeleton is covered in iridescent green oval scales. These contain diamond-based crystal lattices oriented in all directions to give a brilliant green coloration that hardly varies with angle. The scales are effectively divided into pixels about a micrometre wide. Each such pixel is a single crystal and reflects light in a direction different from its neighbours.\nSelective mirrors to create interference effects are formed of micron-sized bowl-shaped pits lined with multiple layers of chitin in the wing scales of \"Papilio palinurus\", the emerald swallowtail butterfly. These act as highly selective mirrors for two wavelengths of light. Yellow light is reflected directly from the centres of the pits; blue light is reflected twice by the sides of the pits. The combination appears green, but can be seen as an array of yellow spots surrounded by blue circles under a microscope.\n\nCrystal fibres, formed of hexagonal arrays of hollow nanofibres, create the bright iridescent colours of the bristles of \"Aphrodita\", the sea mouse, a non-wormlike genus of marine annelids. The colours are aposematic, warning predators not to attack. The chitin walls of the hollow bristles form a hexagonal honeycomb-shaped photonic crystal; the hexagonal holes are 0.51 μm apart. The structure behaves optically as if it consisted of a stack of 88 diffraction gratings, making \"Aphrodita\" one of the most iridescent of marine organisms.\nDeformed matrices, consisting of randomly oriented nanochannels in a spongelike keratin matrix, create the diffuse non-iridescent blue colour of \"Ara ararauna\", the blue-and-yellow macaw. Since the reflections are not all arranged in the same direction, the colours, while still magnificent, do not vary much with angle, so they are not iridescent.\nSpiral coils, formed of helicoidally stacked cellulose microfibrils, create Bragg reflection in the \"marble berries\" of the African herb \"Pollia condensata\", resulting in the most intense blue coloration known in nature. The berry's surface has four layers of cells with thick walls, containing spirals of transparent cellulose spaced so as to allow constructive interference with blue light. Below these cells is a layer two or three cells thick containing dark brown tannins. \"Pollia\" produces a stronger colour than the wings of \"Morpho\" butterflies, and is one of the first instances of structural coloration known from any plant. Each cell has its own thickness of stacked fibres, making it reflect a different colour from its neighbours, and producing a pixellated or pointillist effect with different blues speckled with brilliant green, purple and red dots. The fibres in any one cell are either left-handed or right-handed, so each cell circularly polarizes the light it reflects in one direction or the other. \"Pollia\" is the first organism known to show such random polarization of light, which, nevertheless does not have a visual function, as the seed-eating birds that visit this plant species are not able to perceive polarised light. Spiral microstructures are also found in scarab beetles where they produce iridescent colours.\nThin film with diffuse reflector, based on the top two layers of a buttercup's petals. The brilliant yellow gloss derives from a combination, rare among plants, of yellow pigment and structural coloration. The very smooth upper epidermis acts as a reflective and iridescent thin film; for example, in \"Ranunculus acris\", the layer is 2.7 micrometres thick. The unusual starch cells form a diffuse but strong reflector, enhancing the flower's brilliance. The curved petals form a paraboloidal dish which directs the sun's heat to the reproductive parts at the centre of the flower, keeping it some degrees Celsius above the ambient temperature.\n\nSurface gratings, consisting on ordered surface features due exposure of ordered muscle cells on cuts of meat. The structural coloration on meat cuts appears only after the ordered pattern of muscle fibrils is exposed and light is diffracted by the proteins in the fibrils. The coloration or wavelength of the diffracted light depends on the angle of observation and can be enhanced by covering the meat with translucent foils. Roughening the surface or removing water content by drying causes the structure to collapse, thus, the structural coloration to disappear.\n\nSome animals including cephalopods like squid are able to vary their colours rapidly for both camouflage and signalling. The mechanisms include reversible proteins which can be switched between two configurations. The configuration of reflectin proteins in chromatophore cells in the skin of the \"Doryteuthis pealeii\" squid is controlled by electric charge. When charge is absent, the proteins stack together tightly, forming a thin, more reflective layer; when charge is present, the molecules stack more loosely, forming a thicker layer. Since chromatophores contain multiple reflectin layers, the switch changes the layer spacing and hence the colour of light that is reflected.\n\nBlue-ringed octopuses spend much of their time hiding in crevices whilst displaying effective camouflage patterns with their dermal chromatophore cells. If they are provoked, they quickly change colour, becoming bright yellow with each of the 50-60 rings flashing bright iridescent blue within a third of a second. In the greater blue-ringed octopus (\"Hapalochlaena lunulata\"), the rings contain multi-layer iridophores. These are arranged to reflect blue–green light in a wide viewing direction. The fast flashes of the blue rings are achieved using muscles under neural control. Under normal circumstances, each ring is hidden by contraction of muscles above the iridophores. When these relax and muscles outside the ring contract, the bright blue rings are exposed.\n\nGabriel Lippmann won the Nobel Prize in Physics in 1908 for his work on a structural coloration method of colour photography, the Lippmann plate. This used a photosensitive emulsion fine enough for the interference caused by light waves reflecting off the back of the glass plate to be recorded in the thickness of the emulsion layer, in a monochrome (black and white) photographic process. Shining white light through the plate effectively reconstructs the colours of the photographed scene.\nIn 2010, the dressmaker Donna Sgro made a dress from Teijin Fibers' Morphotex, an undyed fabric woven from structurally coloured fibres, mimicking the microstructure of \"Morpho\" butterfly wing scales. The fibres are composed of 61 flat alternating layers, between 70 and 100 nanometres thick, of two plastics with different refractive indices, nylon and polyester, in a transparent nylon sheath with an oval cross-section. The materials are arranged so that the colour does not vary with angle. The fibres have been produced in red, green, blue, and violet.\n\nStructural coloration could be further exploited industrially and commercially, and research that could lead to such applications is under way. A direct parallel would be to create active or adaptive military camouflage fabrics that vary their colours and patterns to match their environments, just as chameleons and cephalopods do. The ability to vary reflectivity to different wavelengths of light could also lead to efficient optical switches that could function like transistors, enabling engineers to make fast optical computers and routers.\n\nThe surface of the compound eye of the housefly is densely packed with microscopic projections that have the effect of reducing reflection and hence increasing transmission of incident light. Similarly, the eyes of some moths have antireflective surfaces, again using arrays of pillars smaller than the wavelength of light. \"Moth-eye\" nanostructures could be used to create low-reflectance glass for windows, solar cells, display devices, and military stealth technologies. Antireflective biomimetic surfaces using the \"moth-eye\" principle can be manufactured by first creating a mask by lithography with gold nanoparticles, and then performing reactive-ion etching.\n\n\n\n\n"}
{"id": "19901232", "url": "https://en.wikipedia.org/wiki?curid=19901232", "title": "Sunjoy Monga", "text": "Sunjoy Monga\n\nSunjoy Monga is a conservationist, photographer, and naturalist based in Mumbai in India. He was born in Masjid Bunder.\n\nSunjoy authored book 'City Forest'. He is associated with the Bombay Natural History Society written columns in newspapers on the need for conservation, poaching, decreases in the city's flamingoe population, and the man-animal conflict in the Sanjay Gandhi National Park.\n\nHe has initiated an environmental awareness drive 'Young Rangers' amongst schools and school children across India. He has also been chosen on the Tumbhi advisory panel.\n\nHe has also written a field guide titled 'Birds of Mumbai' that documents more than 350 species of birds seen in Mumbai, Maharashtra, India.\n\n"}
{"id": "1561900", "url": "https://en.wikipedia.org/wiki?curid=1561900", "title": "Switchgear", "text": "Switchgear\n\nIn an electric power system, switchgear is the combination of electrical disconnect switches, fuses or circuit breakers used to control, protect and isolate electrical equipment. Switchgear is used both to de-energize equipment to allow work to be done and to clear faults downstream. This type of equipment is directly linked to the reliability of the electricity supply.\n\nThe earliest central power stations used simple open knife switches, mounted on insulating panels of marble or asbestos. Power levels and voltages rapidly escalated, making opening manually operated switches too dangerous for anything other than isolation of a de-energized circuit. Oil-filled equipment allowed arc energy to be contained and safely controlled. By the early 20th century, a switchgear line-up would be a metal-enclosed structure with electrically operated switching elements, using oil circuit breakers. Today, oil-filled equipment has largely been replaced by air-blast, vacuum, or SF equipment, allowing large currents and power levels to be safely controlled by automatic equipment.\n\nHigh-voltage switchgear was invented at the end of the 19th century for operating motors and other electric machines. The technology has been improved over time and can now be used with voltages up to 1,100 kV.\n\nTypically, switchgear in substations are located on both the high- and low-voltage sides of large power transformers. The switchgear on the low-voltage side of the transformers may be located in a building, with medium-voltage circuit breakers for distribution circuits, along with metering, control, and protection equipment. For industrial applications, a transformer and switchgear line-up may be combined in one housing, called a unitized substation (USS).\n\nA switchgear has 2 types of components:\n\nOne of the basic functions of switchgear is protection, which is interruption of short-circuit and overload fault currents while maintaining service to unaffected circuits. Switchgear also provides isolation of circuits from power supplies. Switchgear is also used to enhance system availability by allowing more than one source to feed a load.\n\nSwitchgears are as old as electricity generation. The first models were very primitive: all components were simply fixed to a wall. Later they were mounted on wooden panels. For reasons of fire protection, the wood was replaced by slate or marble. This led to a further improvement, because the switching and measuring devices could be attached to the front, while the wiring was on the back.\n\nSwitchgear for lower voltages may be entirely enclosed within a building. For higher voltages (over about 66 kV), switchgear is typically mounted outdoors and insulated by air, although this requires a large amount of space. Gas-insulated switchgear saves space compared with air-insulated equipment, although the equipment cost is higher. Oil insulated switchgear presents an oil spill hazard.\n\nSwitches may be manually operated or have motor drives to allow for remote control.\n\nA switchgear may be a simple open-air isolator switch or it may be insulated by some other substance. An effective although more costly form of switchgear is the gas-insulated switchgear (GIS), where the conductors and contacts are insulated by pressurized sulfur hexafluoride gas (SF). Other common types are oil or vacuum insulated switchgear.\n\nThe combination of equipment within the switchgear enclosure allows them to interrupt fault currents of thousands of amps. A circuit breaker (within a switchgear enclosure) is the primary component that interrupts fault currents. The quenching of the arc when the circuit breaker pulls apart the contacts (disconnects the circuit) requires careful design. Circuit breakers fall into these six types:\n\nOil circuit breakers rely upon vaporization of some of the oil to blast a jet of oil along the path of the arc. The vapor released by the arcing consists of hydrogen gas.\nMineral oil has better insulating property than air. Whenever there is a separation of current carrying contacts in the oil, the arc in circuit breaker is initialized at the moment of separation of contacts, and due to this arc the oil is vaporized and decomposed in mostly hydrogen gas and ultimately creates a hydrogen bubble around the electric arc. This highly compressed gas bubble around the arc prevents re-striking of the arc after current reaches zero crossing of the cycle. The oil circuit breaker is one of the oldest type of circuit breakers.\n\nAir circuit breakers may use compressed air (puff) or the magnetic force of the arc itself to elongate the arc. As the length of the sustainable arc is dependent on the available voltage, the elongated arc will eventually exhaust itself. Alternatively, the contacts are rapidly swung into a small sealed chamber, the escaping of the displaced air thus blowing out the arc.\n\nCircuit breakers are usually able to terminate all current flow very quickly: typically between 30 ms and 150 ms depending upon the age and construction of the device.\n\nGas (SF) circuit breakers sometimes stretch the arc using a magnetic field, and then rely upon the dielectric strength of the SF gas to quench the stretched arc.\n\nHybrid switchgear is a type which combines the components of traditional air-insulated switchgear (AIS) and SF gas-insulated switchgear (GIS) technologies. It is characterized by a compact and modular design, which encompasses several different functions in one module.\n\nCircuit breakers with vacuum interrupters have minimal arcing characteristics (as there is nothing to ionize other than the contact material), so the arc quenches when it is stretched by a small amount (<2–8 mm). Near zero current the arc is not hot enough to maintain a plasma, and current ceases; the gap can then withstand the rise of voltage. Vacuum circuit breakers are frequently used in modern medium-voltage switchgear to 40,500 volts. Unlike the other types, they are inherently unsuitable for interrupting DC faults. The reason vacuum circuit breakers are unsuitable for breaking high DC voltages is that with DC there is no \"current zero\" period. The plasma arc can feed itself by continuing to gasify the contact material.\n\nBreakers that use carbon dioxide as the insulating and arc extinguishing medium work on the same principles as a sulfur hexafluoride (SF) breaker. Because SF is a greenhouse gas more potent than CO, by switching from SF to CO it is possible to reduce the greenhouse gas emissions by 10 tons during the product lifecycle.\n\nCircuit breakers and fuses disconnect when current exceeds a predetermined safe level. However they cannot sense other critical faults, such as unbalanced currents—for example, when a transformer winding contacts ground. By themselves, circuit breakers and fuses cannot distinguish between short circuits and high levels of electrical demand.\n\nDifferential protection depends upon Kirchhoff's current law, which states that the sum of currents entering or leaving a circuit node must equal zero. Using this principle to implement differential protection, any section of a conductive path may be considered a node. The conductive path could be a transmission line, a winding of a transformer, a winding in a motor, or a winding in the stator of an alternator. This form of protection works best when both ends of the conductive path are physically close to each other. This scheme was invented in Great Britain by Charles Hesterman Merz and Bernard Price.\n\nTwo identical current transformers are used for each winding of a transformer, stator, or other device. The current transformers are placed around opposite ends of a winding. The current through both ends should be identical. A protective relay detects any imbalance in currents, and trips circuit breakers to isolate the device. In the case of a transformer, the circuit breakers on both the primary and secondary would open.\n\nA short circuit at the end of a long transmission line appears similar to a normal load, because the impedance of the transmission line limits the fault current. A distance relay detects a fault by comparing the voltage and current on the transmission line. A large current along with a voltage drop indicates a fault.\n\nSeveral different classifications of switchgear can be made:\n\n\nA single line-up may incorporate several different types of devices, for example, air-insulated bus, vacuum circuit breakers, and manually operated switches may all exist in the same row of cubicles.\n\nRatings, design, specifications and details of switchgear are set by a multitude of standards. In North America mostly IEEE and ANSI standards are used, much of the rest of the world uses IEC standards, sometimes with local national derivatives or variations.\n\nTo help ensure safe operation sequences of switchgear, trapped key interlocking provides predefined scenarios of operation. For example, if only one of two sources of supply are permitted to be connected at a given time, the interlock scheme may require that the first switch must be opened to release a key that will allow closing the second switch. Complex schemes are possible.\n\nIndoor switchgear can also be type tested for internal arc containment (e.g., IEC 62271-200). This test is important for user safety as modern switchgear is capable of switching large currents.\n\nSwitchgear is often inspected using thermal imaging to assess the state of the system and predict failures before they occur. Other methods include partial discharge (PD) testing, using either fixed or portable testers, and acoustic emission testing using surface-mounted transducers (for oil equipment) or ultrasonic detectors used in outdoor switchyards. Temperature sensors fitted to cables to the switchgear can permanently monitor temperature build-up. SF equipment is invariably fitted with alarms and interlocks to warn of loss of pressure, and to prevent operation if the pressure falls too low.\n\nThe increasing awareness of dangers associated with high fault levels has resulted in network operators specifying closed-door operations for earth switches and racking breakers. Many European power companies have banned operators from switch rooms while operating. Remote racking systems are available which allow an operator to rack switchgear from a remote location without the need to wear a protective arc flash hazard suit. Switchgear systems require continuous maintenance and servicing to remain safe to use and fully optimized to provide such high voltages.\n\n\n"}
{"id": "48208041", "url": "https://en.wikipedia.org/wiki?curid=48208041", "title": "T. G. K. Menon", "text": "T. G. K. Menon\n\nThacheril Govindan Kutty Menon is an Indian social worker and environmentalist. His contributions are reported in the introduction of environmentally friendly irrigation and farming techniques under the aegis of Kasturbagram in Indore, Madhya Pradesh. He is known to have promoted bio-dynamic agriculture in India. He received the Jamnalal Bajaj Award in 1989. The Government of India awarded him the fourth highest civilian honour of the Padma Shri in 1991.\n"}
{"id": "612238", "url": "https://en.wikipedia.org/wiki?curid=612238", "title": "Tree house", "text": "Tree house\n\nA tree house, tree fort or treeshed is a platform or building constructed around, next to or among the trunk or branches of one or more mature trees while above ground level. Tree houses can be used for recreation, work space, habitation, and observation.\n\nBuilding tree platforms or nests as a shelter from dangers on the ground is a habit of all the great apes, and may have been inherited by humans. It is true that evidence of prehistoric man-made tree houses have never been found by paleoanthropologists, but remains of wooden tree houses would not remain. However, evidence for cave accommodation, terrestrial man-made rock shelters, and bonfires should be possible to find if they had existed, but are scarce from earlier than 40000 years ago. This has led to a hypothesis that archaic humans may have lived in trees until about 40000 years ago.\n\nEven today, treehouses are built by some indigenous people in order to escape the danger and adversity on the ground in some parts of the tropics. It has been claimed that the majority of the Korowai clans, a Papuan tribe in the southeast of Irian Jaya, live in tree houses on their isolated territory as protection against a tribe of neighbouring head-hunters, the Citak. The BBC revealed in 2018 that the Korowai had constructed tree houses \"for the benefit of overseas programme makers\" and did not actually live in them. However, the Korowai people still build tree houses, but not elevated on stilts as in the BBC scene, but fastened to trees in the tree trunks of tall trees, to protect occupants and store food from scavenging animals.\n\nModern tree houses are usually built as a hut for children or for leisure purposes. Modern tree houses may also be integrated into existing hotel facilities.\n\nAlong with subterranean and ground level houses, tree houses are an option for building eco-friendly houses in remote forest areas, because they do not require a clearing of a certain area of forest. However, the wildlife, climate and illumination on ground level in areas of dense close-canopy forest is not desirable to some people.\n\nThere are numerous techniques to fasten the structure to the tree which seek to minimize tree damage.\n\nThe construction of modern tree houses usually starts with the creation of a rigid platform, on which the house will be placed; the platform will lean (possibly on the corners) on the branches. In case there aren’t enough suitable supports, the methods to support the platform are:\nStruts and stilts are used for relieving weights on a lower elevation or straight to the ground; Tree houses supported by stilts weigh much less on the tree and help to prevent stress, potential strain, and injury caused by puncture holes. Stilts are typically anchored into the ground with concrete although new designs, such as the “Diamond Pier”, accelerates installation time and they are less invasive for the root system. Stilts are considered the easiest method of supporting larger tree houses, and can also increase structural support and safety.\nStay rods are used for relieving weights on a higher elevation. These systems are particularly useful to control movements caused by wind or tree growth, however they are the used less often, due to the natural limits of the systems. Higher elevation and more branches tailing off decreases capacity and increases wind sensibility.\nAs building materials for hanging are used ropes, wire cables, tension fasteners, springs etc.\n\nFriction and tension fasteners are the most common noninvasive methods of securing tree houses. They do not use nails, screws or bolts, but instead grip the beams to the trunk by means of counter-beam, threaded bars, or tying.\nInvasive methods are all methods that use nails, screws, bolts, kingpins, etc. Because these methods require punctures in the tree, they must be planned properly in order to minimize stress. Not all species of plants suffer from puncture in the same way, depending partly on whether the sap conduits run in the pith or in the bark. Nails are generally not recommended. A special kind of bolt developed in the 1990s called a treehouse attachment bolt (TAB) can support greater weights than earlier methods.\n\nSince the mid-1990s, recreational tree houses have enjoyed a rise in popularity in countries such as the United States and parts of Europe. This has been due to increased disposable income, better technology for builders, research into safe building practices and an increased interest in environmental issues, particularly sustainable living. This growing popularity is also reflected in a rise of social media channels, websites, and television shows specially dedicated to featuring remarkable tree houses around the world.\nIncreased popularity has, in turn, given rise to demand for businesses covering all building and design work for clients. There are over 30 businesses in Europe and the USA specializing in the construction of tree houses of various degrees of permanence and sophistication, from children's play structures to fully functioning homes.\n\nPopularity of tree house hotels is equally growing, with a number of booking websites offering accommodation in tree houses.\n\nMany areas of the world have no specific planning laws for tree houses, so the legal issues can be confusing to both the builder and the local planning departments only. Treehouses can be exempt, partially regulated or fully regulated - depending on the locale.\n\nIn some cases, tree houses are exempted from standard building regulations, as they're considered outside of the regulations specification. An exemption may be given to a builder if the tree house is in a remote or non-urban location. Alternatively, a tree house may be included in the same category as structures such as garden sheds, sometimes called a \"temporary structure\". There may be restrictions on height, distance from boundary and privacy for nearby properties. There are various grey areas in these laws, as they were not specifically designed for tree-borne structures. A very small number of planning departments have specific regulations for tree houses, which set out clearly what may be built and where. For safety during the tree house construction, it is usually best to do as much work as possible on the ground, taking long-term viability into consideration.\n\nThe tree house has been central to various environmental protest communities around the world, in a technique, popularized, known as tree sitting. This method may be used in protests against proposed road building or old growth forestry operations. Tree houses are used as a method of defence from which it is difficult and costly to safely evict the protesters and begin work. Julia Butterfly Hill is a particularly well known tree sitter who occupied a Californian Redwood for 738 days, saving the tree and others in the immediate area. Her accommodation consisted of two 3m (29 sq ft) platforms 60 m (200 ft) above the ground.\n\n"}
{"id": "1226063", "url": "https://en.wikipedia.org/wiki?curid=1226063", "title": "Treenail", "text": "Treenail\n\nA treenail, also trenail, trennel, or trunnel, is a wooden peg, pin, or dowel used to fasten pieces of wood together, especially in timber frames, covered bridges, wooden shipbuilding and boat building. It is driven into a hole bored through two (or more) pieces of structural wood (mortise and tenon).\n\nThe use of wood as a tenon can be traced back over 7,000 years, as archaeologist have found traces of wood nails in the excavation of early Germanic sites. Trenails are notoriously economical and readily available, making them a common early building material. Honey Locust is a favorite wood when making trunnels in shipbuilding due to its strength and rot resistance, while red oak is typical in buildings. Traditionally treenails and pegs were made by splitting bolts of wood with a froe and shaping them with a drawknife on a shaving horse. Treenails are cut from a single piece of wood and perform well because of the natural grain. The grain of the treenail runs perpendicular to the grain of the receiving mortises which adds structural strength. Treenails are typically 1.25\"-1.5\" (32-38 mm) in diameter and are hand whittled with rough facets. The mortise is drilled 1/16\" (2 mm) smaller than the treenail to create a tight fit and take advantage of friction in the mortise. In cases where the treenail is or longer, the treenail should be shaped 1/8 inch (3 mm) smaller than the other half. In the same case the mortise is drilled in two parts, with a smaller auger for the smaller part of the treenail and a typical auger for the standard part. Other trenails are tapered with the large end being 1/8\" longer than the mortise. After treenails are hammered into the mortise, they can be trimmed, split, and wedged with a small piece of oak that increases friction force. As an alternative to the wedge, the treenail can receive a plug or a punch to the center that expands the entire circumference. While this method prevents leaks by reducing gaps, plugs and punches are more likely to fall out in cold temperatures. Ideally, the nose of the treenail is driven 4–5 cm clear of the timber before being trimmed. Unlike metal nails, trenails can not be removed (without great effort) or reused. As the wood shrinks or expand the fibers create a friction that interlocks it into the mortise snugly. If a treenail breaks or fails but the wood it is fastening remains intact the remaining trenail can be cut out and replaced with a larger treenail that fits snugly. In addition, treenails have the ability to move over time and retain structural integrity. Because both the mortise and the tenon are wood, the trenail does not stress the mortise to the point of failure during movement including seismic forces and grade settlement.\n\nEarly mortise and tenon trusses with spans of less than 30 feet used trenail fasteners. When used in a truss, the connecting mortises are drilled off center such that when the treenail is inserted it creates a tighter joint. Because of the large number of trenails required in a truss, the treenails can be turned on a lathe with a head and a tapered end, often kept extra-long for the tightest fit. The bottom chord often requires 2-3 pegs and is the weakest part of the truss. Hence the treenail can not prevent failure in spans of over 30 feet. In cases where significant shrinkage may occur, it may be necessary to use iron U-straps or reinforcements.\n\nAncient shipbuilding used treenails to bind the boat together. They had the advantage of not giving rise to \"nail-sickness\", a term for decay accelerated and concentrated around metal fasteners. Increased water content causes wood to expand, so that treenails gripped the planks tighter as they absorbed water. However, when the treenail was a different wood species than the planking it usually caused rot. Treenails and iron nails were most common until the 1780s when copper nails over copper sheathing became more popular. As late as the 1870s, the merchant navy ships used treenails and iron bolts, while the higher class ships used the copper and yellow metal bolts and dumps. In the 1870s tradition, treenails were typically used in a ratio of four treenails to one bolt with the exception that sometimes the number of bolts was increased. In later corvettes the ratio was changed to two treenails to one bolt.\n\nSimilar wooden trenail fastenings were used as alternatives to metal spikes to secure railroad rail-support \"chairs\" to wooden sleepers (ties) in early Victorian times. Treenails were extensively used constructing railroads in North England.\n"}
{"id": "9730335", "url": "https://en.wikipedia.org/wiki?curid=9730335", "title": "Uniflow steam engine", "text": "Uniflow steam engine\n\nThe uniflow type of steam engine uses steam that flows in one direction only in each half of the cylinder. Thermal efficiency is increased in the compound and multiple expansion types of steam engine by separating expansion into steps in separate cylinders; in the uniflow design, thermal efficiency is achieved by having a temperature gradient along the cylinder. Steam always enters at the hot ends of the cylinder and exhausts through ports at the cooler centre. By this means, the relative heating and cooling of the cylinder walls is reduced.\n\nSteam entry is usually controlled by poppet valves (which act similarly to those used in internal combustion engines) that are operated by a camshaft. The inlet valves open to admit steam when minimum expansion volume has been reached at the start of the stroke. For a period of the crank cycle, steam is admitted, and the poppet inlet is then closed, allowing continued expansion of the steam during the stroke, driving the piston. Near the end of the stroke, the piston will uncover a ring of exhaust ports mounted radially around the centre of the cylinder. These ports are connected by a manifold and piping to the condenser, lowering the pressure in the chamber below that of the atmosphere causing rapid exhausting. Continued rotation of the crank moves the piston. From the animation, the features of a uniflow engine can be seen, with a large piston almost half the length of the cylinder, poppet inlet valves at either end, a camshaft (whose motion is derived from that of the driveshaft) and a central ring of exhaust ports.\n\nUniflow engines potentially allow greater expansion in a single cylinder without the relatively cool exhaust steam flowing across the hot end of the working cylinder and steam ports of a conventional \"counterflow\" steam engine during the exhaust stroke. This condition allows higher thermal efficiency. The exhaust ports are only open for a small fraction of the piston stroke, with the exhaust ports closed just after the piston begins traveling toward the admission end of the cylinder. The steam remaining within the cylinder after the exhaust ports are closed is trapped, and this trapped steam is compressed by the returning piston. This is thermodynamically desirable as it preheats the hot end of the cylinder before the admission of steam. However, the risk of excessive compression often results in small auxiliary exhaust ports being included at the cylinder heads. Such a design is called a semi-uniflow engine.\n\nEngines of this type usually have multiple cylinders in an in-line arrangement, and may be single- or double-acting. A particular advantage of this type is that the valves may be operated by the effect of multiple camshafts, and by changing the relative phase of these camshafts, the amount of steam admitted may be increased for high torque at low speed, and may be decreased at cruising speed for economy of operation. Alternatively, designs using a more-complex cam surface allowed the varying of timing by shifting the entire camshaft longitudinally compared to its follower, allowing the admission timing to be varied. (The camshaft could be shifted by mechanical or hydraulic devices.) And, by changing the absolute phase, the engine's direction of rotation may be changed. The uniflow design also maintains a constant temperature gradient through the cylinder, avoiding passing hot and cold steam through the same end of the cylinder.\n\nIn practice, the uniflow engine has a number of operational shortcomings. The large expansion ratio requires a large cylinder volume. To gain the maximum potential work from the engine a high reciprocation rate is required, typically 80% faster than a double-acting counterflow type engine. This causes the opening times of the inlet valves to be very short, putting great strain on a delicate mechanical part. In order to withstand the huge mechanical forces encountered, engines have to be heavily built and a large flywheel is required to smooth out the variations in torque as the steam pressure rapidly rises and falls in the cylinder. Because there is a thermal gradient across the cylinder, the metal of the wall expands to different extents. This requires the cylinder bore to be machined wider in the cool center than at the hot ends. If the cylinder is not heated correctly, or if water enters, the delicate balance can be upset causing seizure mid-stroke or, potentially, destruction.\n\nThe uniflow engine was first used in Britain in 1827 by Jacob Perkins and was patented in 1885 by Leonard Jennett Todd. It was popularised by German engineer Johann Stumpf in 1909, with the first commercial stationary engine produced a year previously in 1908.\n\nThe uniflow principle was mainly used for industrial power generation, but was also tried in a few railway locomotives in England, such as the North Eastern Railway uniflow locomotives No 825 of 1913, and No 2212 of 1919, and the Midland Railway Paget locomotive. Experiments were also made in France, Germany, the United States and Russia. In no case were the results encouraging enough for further development to be undertaken.\n\nThe first large-scale utilization of a Uniflow engine was in Atkinson steam wagons, in 1918.\n\nThe final commercial evolution of the uniflow engine occurred in the United States during the late 1930s and 1940s by the Skinner Engine Company with the development of the Compound Unaflow Marine Steam Engine. This engine operates in a steeple compound configuration and provides efficiencies approaching contemporary diesels. Many car ferries on the Great Lakes were so equipped, one of which is still operating, of 1952. The , the most prolific aircraft carrier design in history, used two 5-cylinder Skinner Unaflow engines, but these were not steeple compounds. A non-compound Skinner Uniflow remained in service until 2013 in the Great Lakes cement carrier , installed when the vessel was re-powered in 1950.\n\nIn small sizes (less than about ), reciprocating steam engines are much more efficient than steam turbines. White Cliffs Solar Power Station used a three-cylinder uniflow engine with \"Bash\"-type admission valves to generate about 25 kW electrical output.\n\nThe single-acting uniflow steam engine configuration closely resembles that of a two-stroke internal combustion engine, and it is possible to convert a two-stroke engine to a uniflow steam engine by feeding the cylinder with steam via a \"bash valve\" fitted in place of the spark plug. As the rising piston nears the top of its stroke, it knocks open the bash valve to admit a pulse of steam. The valve closes automatically as the piston descends, and the steam is exhausted through the existing cylinder porting. The inertia of the flywheel then carries the piston back to the top of its stroke against the compression, as it does in the original form of the engine. Also like the original, the conversion is not self-starting and must be turned over by an external power source to start. An example of such a conversion is the steam-powered moped, which is started by pedalling.\n\n\n\n"}
{"id": "52856410", "url": "https://en.wikipedia.org/wiki?curid=52856410", "title": "Våg", "text": "Våg\n\nA våg (plural \"våger\") or vog is an old Scandinavian unit of mass.\n\nThe standardized \"landsvåg\", which was introduced in Norway with the new system of weights and measures in 1875, corresponded to three \"bismerpund\", or . The \"våg\" was used in Eastern Norway, Western Norway, and Northern Norway, but it varied in weight. Previously, it was often reckoned as 72 marks or approximately . In Sunnmøre the \"våg\" was equivalent to three \"lispund\" or about , but in Sunnhordland it was reckoned as three \"spann\" or 90 marks; that is, about .\n\n"}
{"id": "2511451", "url": "https://en.wikipedia.org/wiki?curid=2511451", "title": "Wave Dragon", "text": "Wave Dragon\n\nWave Dragon is a floating slack-moored energy converter of the overtopping type, developed by the Danish company Wave Dragon Aps. Wave Dragon is a joint EU research project, including partners from Austria, Denmark, Germany, Ireland, Portugal, Sweden, and the UK. It was the world's first offshore wave energy converter.\n\nThe 237 tons prototype Wave Dragon was towed in March 2003 to the first test site at the Danish Wave Energy Test Center in Nissum Bredning fjord. It was tested until January 2005. In 2006 a modified prototype was deployed to another test site with more energetic wave climate. The prototype was scrapped in 2011.\n\nWave Dragon is a floating, slack-moored energy converter of the 'overtopping' type which can be deployed as a single unit, or in arrays of up to 200 units; the output of such an array would have a capacity comparable to traditional fossil-fuel power plants.\n\nThe first prototype was connected to the power grid in 2003 and is currently deployed in Nissum Bredning, Denmark. Long term testing is under way to determine system performance; i.e. availability and power production under different weather and tide conditions. A multi-MW deployment is expected in 2009.\n\nThe Wave Dragon concept combines existing, mature offshore and hydro turbine technology. In the Wave Dragon, the Kaplan turbine is being tested at the Technical University of Munich. This turbine uses a siphon inlet whereas the next 6 turbines to be installed will be equipped with a cylinder gate to start and stop water inlet to the turbine.\n\nWave Dragon uses principles from traditional hydropower plants in an offshore floating platform to use wave energy.\n\nThe Wave Dragon consists of two wave reflectors that direct the waves towards a ramp. Behind the ramp, a large reservoir collects the directed water, and temporarily stores the water. The reservoir is held above sea level. The water leaves the reservoir through hydro turbines.\n\nThree-step energy conversion:\n\nOvertopping (absorption) -> Storage (reservoir) -> Power-take-off (low-head turbines)\n\nMain components of a Wave Dragon:\n\nWave energy converters make use of the mechanical motion or fluid pressure. Wave Dragon does not have any conversion, e.g. oscillating water/air columns, hinged rafts, and gyroscopic/hydraulic devices. The Wave Dragon directly utilises the energy of the water's motion.\n\nThe Wave Dragon is of heavy, durable construction and has only one kind of moving parts: the turbines. This is essential for any device bound for operations offshore, where extreme conditions and fouling, etc., seriously affect any moving parts.\n\nWave Dragon model testing has been used in order to:\n\nThe main body to or platform consists of one large floating reservoir. To reduce rolling and keep the platform stable, the Wave Dragon must be large and heavy. The prototype used in Nissum is of a traditional (ship-like) plate construction of plates of 8 mm steel. The total steel weight of the main body plus the ramp is 150 tons, so that 87 tons of water must be added to achieve the 237 tons total weight needed for stable continuous operation.\n\n\n"}
{"id": "1076403", "url": "https://en.wikipedia.org/wiki?curid=1076403", "title": "Woodturning", "text": "Woodturning\n\nWoodturning is the craft of using the wood lathe with hand-held tools to cut a shape that is symmetrical around the axis of rotation. Like the potter's wheel, the wood lathe is a simple mechanism which can generate a variety of forms. The operator is known as a turner, and the skills needed to use the tools were traditionally known as turnery. In pre-industrial England, these skills were sufficiently difficult to be known as 'the misterie' of the turners guild. The skills to use the tools by hand, without a fixed point of contact with the wood, distinguish woodturning and the wood lathe from the machinists lathe, or metal-working lathe.\n\nItems made on the lathe include tool handles, candlesticks, egg cups, knobs, lamps, rolling pins, cylindrical boxes, Christmas ornaments, bodkins, knitting needles, needle cases, thimbles, pens, chessmen, spinning tops; legs, spindles and pegs for furniture; balusters and newel posts for architecture; baseball bats, hollow forms such as woodwind musical instruments, urns, sculptures; bowls, platters, and chair seats. Industrial production has replaced many of these products from the traditional turning shop. However, the wood lathe is still used for decentralized production of limited or custom turnings. A skilled turner can produce a wide variety of objects with five or six simple tools. The tools can be reshaped easily for the task at hand. \n\nIn many parts of the world, the lathe has been a portable tool that goes to the source of the wood, or adapts to temporary workspaces. 21st-century turners restore furniture, continue folk-art traditions, produce custom architectural work, and create fine craft for galleries. Woodturning appeals to people who like to work with their hands, find pleasure in problem-solving, or enjoy the tactile and visual qualities of wood.\n\nWood lathes work with either reciprocating or continuous revolution. The reciprocating lathe is powered by a bow or a spring, rotating the wood first in one direction, and then in the other. The turner cuts on just one side of the rotation, as with the pole lathe. The reciprocating lathe may be human-powered with a bow, as well as with spring mechanisms. The reciprocating lathe, while primitive technology requiring considerable dexterity to operate, is capable of excellent results in skilled hands. For example, reciprocating bow lathes are still used to turn beads for the Arabian lattice windows called Meshrebeeyeh that so charmed Holtzapffel in the 1880s .\n\nContinuous revolution of the workpiece can be human-powered with a treadle wheel, or achieved with water, steam, or electric power. The style of cutting does not have the pause required by the reciprocating lathe's rotation. Even with continuous revolution, however, the turner controls the contact of tool and wood entirely by hand. The cutters are not fixed, nor advanced automatically, as with the metal-working lathe.\n\nThe nature of wood defines woodturning techniques. The orientation of the wood grain, relative to the axis of the lathe, affects the tools and techniques used by the woodturner. In spindle turning, the grain runs lengthwise along the lathe bed, as if a log were mounted in the lathe. Grain is thus always perpendicular to the direction of rotation under the tool. In bowl turning, the grain runs at right angles to the axis, as if a plank were mounted across the chuck. When a bowl blank rotates, the angle that the grain makes with the cutting tool continually changes between the easy cuts to two places per rotation where the tool is cutting across the grain and even upwards across it. This varying grain angle limits some of the tools that may be used and requires additional skill from the turner.\n\nMoisture content affects both the ease of cutting wood and the final shape of the work when it dries. Wetter wood cuts easily with a continuous ribbon of shavings that are relatively dust-free. However, the wet wood moves as it dries. shrinking less along the grain. These variable changes may add the illusion of an oval bowl, or draw attention to features of the wood. Dry wood is necessary for turnings that require precision, as in the fit of a lid to a box, or in forms where pieces are glued together.\n\nThe character of the wood creates other challenges for the woodturner. Turners of hardwoods and ivory select different tools than those used for cutting softwoods. Voids in the wood require higher lathe speeds, fillers, or extra safety precautions. Although other woodworkers value tight, straight grain, woodturners often search out the unusual wood from roots, defects, or diseased portions of trees.\n\nThe craft of woodturning is preserved and advanced by a community of practitioners. Until the 1970s, an apprentice system in the U.K., and Industrial Arts education in the U.S., preserved many of the traditional skills of the craft. Between 1975 and 1985, industrial arts teachers, hobbyists, artists, collectors, and tool suppliers developed the symposium format for exchange of information about the craft. This community was a kind of prototype for the artisan-based maker culture active in the 21st century. The community organizes regional, national, and international symposiums, publishes journals, and hosts travelling experts at club events. Most publications and DVDs are of the DIY variety, including numerous YouTube videos.\n\nThe archaeological record of woodturning is limited to illustrations because wood is a fiber prone to rot. Egyptian monuments illustrate a strap used by a helper to rotate the lathe while another worker cut the wood. Early bow lathes and strap lathes were developed and used in Egypt and Rome. The Chinese, Persians, and Arabs had their own variations of the bow lathe. Early lathe workers would sometimes use their bare feet to hold cutting tools in place while using their hand to power the lathe. Bow lathes continue in use right up to the present day, and much of our information about them comes from watching turners use them. Between 500 and 1500 A.D., turned wooden vessels served as the everyday bowls and cups of most of the population of Europe. Our knowledge of these humble vessels comes from bowls excavated from shipwrecks, such as the Mary Rose and the Oseberg burial ship, or dug out of deep wells, where they were preserved in a nonaerobic environment. Much of this ware was turned from green wood on a spring pole lathe. Finely crafted drinking bowls, known as mazers, were produced in very limited quantities from dry wood, then decorated with silver-gilt central bosses and rims.\n\nAs early as 1568, a separate fly wheel powered a lathe via a drive belt. A master would cut the wood while an apprentice turned the crank on a huge wheel, often several feet in diameter. This was a continuous revolution lathe, which led to adaptation to external power sources such as water, steam, and electricity. This lathe evolved into the 'queen of machine tools' which made it possible to turn parts for other machinery. The Holtzapffels developed ornamental turning lathes from the continuous revolution lathe combined with metal-working innovations like the automatic slide rest. These lathes worked from geared patterns to cut designs in hardwoods such as ebony. They were favored as a hobby by European princes, meriting a mention by Tolstoy in War and Peace (1869).\n\nWoodturners in London organized into a guild as early as 1310 on Wood Street. By 1347, the Turners Company was assigned responsibility for regulating weights and measures by the Mayor. By 1591, they built their own Hall. The Company governed the apprentice system, and established pricing for goods. In 1604, they were incorporated as the Worshipful Company of Turners of London. Outside of London, the craft was decentralized and unregulated. Itinerant turners known as Bodgers set up temporary pole lathes near the source of wood for turning furniture parts.\n\nIn the 19th and early 20th century, woodturners in England worked in Turning Shops, usually within the master-apprentice system. In Germany and Russia, woodturning was concentrated in villages which had a specialty, such as turning toys. Bow lathes and pole lathes continued in use for decentralized, one-man production of architectural elements and bowls in many parts of the world. In the US, woodturning was part of the curriculum of industrial arts taught in public schools—often a prerequisite for classes in building furniture. The 'problems' from textbooks included both tool management skills, and assignments to turn objects such as gavels, darning eggs, boxes, trays, candlesticks, lamps, and legs for furniture.\n\nWoodturning skills were used by patternmakers in the making of prototypes and shapes for casting molds used in foundries during the 19th and 20th century. They worked very slowly to achieve precision, using enormous patternmaker lathes and slow-cutting scraping tools.\n\nWoodturning has always had a strong hobbyist presence. In the 1970s, an explosion of interest in hobby woodturning in the English-speaking world sparked a revival in the craft. Dr. Dale Nish travelled to England to recruit teachers, tools, and techniques from the last of the apprentice-trained woodturners. A few years later, Canadian Stephen Hogbin spent a year in Australia, pushing the limits of the craft through changes in scale and design. Industrial arts teachers used their institutional affiliation to create seminars, publish books, and foster research. The tool industry identified a new market for lathes and turning tools. A small group of serious collectors invested in the increasingly sculptural explorations of woodturners. It is unusual that woodturning never established a strong foothold in university departments of art and design. Instead, practitioners of the craft have become adept at learning from demonstrations, private classes, regional meetings, their own published journals, and internet technologies. Some artists began as woodturners, and moved into more sculptural work, experimenting with super object forms and other fine craft concepts. The Center for Art in Wood, founded in 1986 as The Wood Turning Center, houses a collection in Philadelphia with over 1,000 objects from international artists as well as a research library and gallery. Other turners have chosen an artisan-based focus on traditional work, custom work, and the pleasure of studio practice.\n\nComplex forms made on a wood lathe develop from surprisingly few types of cuts: parting, planing, bead, cove, and hollowing. Parting separates the wood from the holding device, or establishes depth cuts. Planing is done with a tool in which the bevel below the cutting edge supports wood fibers, just as in a typical wood planer. Beads are a convex shape relative to the cylinder, and coves are a concave shape. Hollowing techniques are a combination of drilling and scooping out materials. The woodturner is at liberty to choose from a variety of tools for all of these techniques, and the quality of the cuts improves with practice wielding the tool selected. Turners rely upon three points of contact making any type of cut: the tool presses down on the tool rest, and against the woodturner's body before contacting the surface of the wood, most often with a bevel edge riding the surface of the wood. The objective is to position the tool correctly so that the wood comes around to the cutting edge, generating a thin shaving without chipping or tearing out sections of the wood. Woodturners prefer to use very clean cuts to minimize the time spent with abrasives. When it is necessary to sand the piece, they do so on the lathe, using abrasives held by hand, in an inertial sander which revolves with the wood's own rotation, or with power tools—drills or right-angle drills. The lathe also becomes a useful holding device for carving, burning, texturing, coloring, and finishing the form.\n\nThe wood rotates between the headstock of the lathe which includes the drive mechanism and the tailstock support, which only rotates if its center is 'live' or supported by a rotating holding device. The headstock end may use points or spurs which are driven into the wood. This type of turning is described as 'between centers.' The headstock spindle may also use a cup, collet, or a scroll chuck to hold a tenon on the workpiece which will be removed in the finished product. The wood can also be screwed or glued to a faceplate—a strong disk that is threaded to mount on the headstock's spindle. The use of a chuck or faceplate allows the woodturner to forego tailstock support for the rotating wood. This type of secure holding system is essential for hollowing bowls or hollow forms.\n\nTurning tools are generally made from three different types of steel; carbon steel, high speed steel (HSS), and more recently powdered metal. Comparing the three types, high speed steel tools maintain their edge longer, requiring less frequent sharpening than carbon steel, but not as long as powdered metal tools. The harder the type of high speed steel used, the longer the edge will maintain sharpness. Powdered steel is even harder than HSS, but takes more effort to obtain an edge as sharp as HSS, just as HSS is harder to get as sharp as carbon steel.\n\nWoodturning tools must be sharpened more frequently than other edged woodworking tools to maintain a clean cut because the wood passes at great speed. Sharpening is usually accomplished with the aid of mechanical devices such as powered sharpening wheels and abrasives. This sharpening process requires either skill of the craftsman, or one of the many available sharpening jigs, which facilitate maintaining a specific bevel on the tool. As with any mechanical sharpening method, overheating or blueing is a danger to be avoided as it will ruin the steel's temper, rendering the steel too soft to maintain a sharp edge. When this happens, the blued area must then be ground away to expose fresh steel and the tool must then have the bevel reestablished and the edge re-honed. High speed steel is not prone to blueing (overheating) whereas carbon steel blues easily, requiring frequent quenching in water or oil to avoid losing temper.\n\n\nWhen woodturning, it is important to wear certain personal protective equipment (PPE). Loose clothing should not be worn, all jewellery should be removed, and long hair should be tied back. Wood shavings generated during turning will also need to be periodically removed.\n\nA good way to check the safety before starting the lathe is 'SAFER':\n\n\nSafe usage of a lathe also depends on the operator's choice of proper techniques for the lathe, tools, and wood. For example, using a high spindle speed with an unbalanced wooden blank may cause the lathe to vibrate dangerously. Spinning a large turning blank too fast may cause it to explode. Inappropriate use of tools such as gouges and skew chisels can cause a \"catch\", where the tool bites suddenly and aggressively into the wood in an uncontrolled manner. This exerts very large forces on the wood, the tool, the lathe and the operator, often causing the wood to break apart or tear free from the lathe. Lathe accidents often pull the tool out of the operator's hand and throw them in unexpected directions. Particular care is also required for wooden shapes that are not circular, such as off-center work, or bowls with wings or square rims. Portions of the turning extend farther from the axis of rotation, and are sometimes more difficult to see than the bulk of the wooden blank.\n\n\n\n"}
