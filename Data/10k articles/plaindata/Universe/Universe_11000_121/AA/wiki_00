{"id": "1263365", "url": "https://en.wikipedia.org/wiki?curid=1263365", "title": "Acetylide", "text": "Acetylide\n\nAcetylide refers to chemical compounds with the chemical formulas MC≡CH and MC≡CM, where M is a metal. The term is used loosely and can refer to substituted acetylides having the general structure RC≡CM (where R is an organic side chain). Acetylides are reagents in organic synthesis. The calcium acetylide commonly called calcium carbide is a major compound of commerce.\n\nAlkali metal and alkaline earth metal acetylides of the general formula MC≡CM are salt-like Zintl phase compounds, containing ions. Evidence for this ionic character can be seen in the ready hydrolysis of these compounds to form acetylene and metal oxides, there is also some evidence for the solubility of ions in liquid ammonia. The ion has a closed shell ground state of Σ, making it isoelectronic to a neutral molecule N, which may afford it some stability.\n\nAnalogous acetylides prepared from other metals, particularly transition metals, show covalent character and are invariably associated with their metal centers. This can be seen in their general stability to water (i.e. silver acetylide, copper acetylide) and radically different chemical applications.\n\nAcetylides of the general formula RC≡CM (where R = H or alkyl) generally show similar properties to their doubly substituted analogues. In the absence of additional ligands, metal acetylides adopt polymeric structures wherein the acetylide groups are bridging ligands.\n\nTerminal alkynes are weak acids:\n\nTo generate acetylides from acetylene and alkynes relies on the use of organometallic or inorganic superbases in solvents which are less acidic than the terminal alkyne. In early studies liquid ammonia was employed, but etherial solvents are more common.\n\nLithium amide, LiHMDS, or organolithium reagents, such as butyllithium, are frequently used form lithium acetylides:\n\nSodium or potassium acetylides can be prepared from various inorganic reagents (e.g. sodium amide) or from their elemental metals, often at room temperature and atmospheric pressure.\n\nCopper(I) acetylide can be prepared by passing acetylene through an aqueous solution of copper(I) chloride because of a low solubility equilibrium. Similarly, silver acetylides can be obtained from silver nitrate.\n\nCalcium carbide is prepared by heating carbon with lime CaO at approximately 2,000 °C. A similar process is used to produce lithium carbide.\n\nAcetylides of the type RCM are widely used in alkynylations in organic chemistry. They are nucleophiles that add to a variety of electrophilic and unsaturated substrates. A classic application is the Favorskii reaction.\n\nIllustrative is the sequence shown below, ethyl propiolate is deprotonated by \"n\"-butyllithium to give the corresponding acetylide. This acetylide adds to the carbonyl center of cyclopentanone. Hydrolytic workup liberate the alkynyl alcohol.\n\nAcetylides are sometimes intermediates in coupling reactions. Examples include Sonogashira coupling, Cadiot-Chodkiewicz coupling, Glaser coupling and Eglinton coupling.\n\nSome acetylides are notoriously explosive. Formation of acetylides poses a risk in handling of gaseous acetylene in presence of metals such as mercury, silver or copper, or alloys with their high content (brass, bronze, silver solder).\n\n"}
{"id": "33184799", "url": "https://en.wikipedia.org/wiki?curid=33184799", "title": "Adirondack League Club", "text": "Adirondack League Club\n\nThe Adirondack League Club is an organization that owns a large piece of wilderness land in the New York State Adirondack Park. The club was founded in the 1890s as a club for hunting and fishing. The club was involved in a protracted legal dispute with the Sierra Club over the right of public access to streams and other waterways.. The Adirondack League Club is known for its socially and politically influential members, including gilded era industrialists and several past Presidents of the United States.\n\n\n"}
{"id": "14383139", "url": "https://en.wikipedia.org/wiki?curid=14383139", "title": "Allotropes of sulfur", "text": "Allotropes of sulfur\n\nThe element sulfur exists as many allotropes. In terms of large number of allotropes, sulfur is second only to carbon. In addition to the allotropes, each allotrope often exists in polymorphs, delineated by Greek prefixes (α, β, etc.).\n\nFurthermore, because elemental sulfur has been an item of commerce for centuries, its various forms are given traditional names. Early workers identified some forms that have later proved to be single or mixtures of allotropes. Some forms have been named for their appearance, e.g. \"mother of pearl sulfur\", or alternatively named for a chemist who was pre-eminent in identifying them, e.g. \"Muthmann's sulfur I\" or \"Engel's sulfur\".\n\nThe most commonly encountered form of sulfur is the orthorhombic polymorph of , which adopts a puckered ring – or \"crown\" – structure. Two other polymorphs are known, also with nearly identical molecular structures. In addition to S, sulfur rings of 6, 7, 9–15, 18 and 20 atoms are known. At least five allotropes are uniquely formed at high pressures, two of which are metallic.\n\nThe number of sulfur allotropes reflects the relatively strong S−S bond of 265 kJ/mol. Furthermore, unlike most elements, the allotropes of sulfur can be manipulated in solutions of organic solvents and is amenable to analysis by HPLC.\n\nThe pressure-temperature (P-T) phase diagram for sulfur is complex (see image). The region labeled I (a solid region), is α-sulfur. See the legend for further identifications and information, and see the section on high pressure forms for information on these phases.\n\nIn a high-pressure study at ambient temperatures, four new solid forms, termed II, III, IV, V have been characterized, where α-sulfur is form I. Solid forms II and III are polymeric, while IV and V are metallic (and are superconductive below 10 K and 17 K, respectively). Laser irradiation of solid samples produces three sulfur forms below 200–300 kbar (20–30 GPa).\n\nTwo methods exist for the preparation of the \"cyclo\"-sulfur allotropes. One of the methods, which is most famous for preparing hexasulfur, is to react hydrogen polysulfides with polysulfur dichloride:\n\nA second strategy uses titanocene pentasulfide as a source of the S unit. This complex is easily made from polysulfide solutions:\n\nThen the resulting pentasulfur-titanocene complex is allowed to react with polysulfur dichloride to give the desired \"cyclo\"-sulfur in the series:\n\nThis has not been isolated, but has been detected in the vapour phase.\n\nThis was first prepared by M. R. Engel in 1891 by reacting HCl with thiosulfate, HSO. Cyclo-S is orange-red and forms a rhombohedral crystal. It is called ρ-sulfur, ε-sulfur, Engel's sulfur and Aten's sulfur. Another method of preparation involves reacting a polysulfane with sulfur monochloride:\n\nThe sulfur ring in cyclo-S has a \"chair\" conformation, reminiscent of the chair form of cyclohexane. All of the sulfur atoms are equivalent.\n\nIt is a bright yellow solid. Four (α-, β-, γ-, δ-) forms of cyclo-heptasulfur are known. Two forms (γ-, δ-)have been characterized. The cyclo-S ring has an unusual range of bond lengths of 199.3–218.1 pm. It is said to be the least stable of all of the sulfur allotropes.\n\nα-Sulfur, see image, is the form most commonly found in nature. When pure it has a greenish-yellow colour (traces of cyclo-S in commercially available samples make it appear yellower). It is practically insoluble in water and is a good electrical insulator with poor thermal conductivity. It is quite soluble in carbon disulfide: 35.5 g/100 g solvent at 25 °C. It has an orthorhombic crystal structure. This is the predominant form found in \"flowers of sulfur\", \"roll sulfur\" and \"milk of sulfur\". It contains S puckered rings, alternatively called a crown shape. The S-S bond lengths are all 203.7 pm and the S-S-S angles are 107.8° with a dihedral angle of 98°. At 95.3 °C, α-sulfur converts to β-sulfur.\n\nThis is a yellow solid with a monoclinic crystal form and is less dense than α-sulfur. Like the α- form it contains puckered S rings and only differs from it in the way the rings are packed in the crystal. It is unusual because it is only stable above 95.3 °C, below this it converts to α-sulfur. It can be prepared by crystallising at 100 °C and cooling rapidly to slow down formation of α-sulfur. It has a melting point variously quoted as 119.6 °C and 119.8 °C but as it decomposes to other forms at around this temperature the observed melting point can vary from this. The 119 °C melting point has been termed the \"ideal melting point\" and the typical lower value (114.5 °C) when decomposition occurs, the \"natural melting point\".\n\nThis form, first prepared by F.W. Muthmann in 1890, is sometimes called \"nacreous sulfur\" or \"mother of pearl sulfur\" because of its appearance. It crystallises in pale yellow monoclinic needles. It contains puckered S rings like α-sulfur and β-sulfur and only differs from them in the way that these rings are packed. It is the densest form of the three. It can be prepared by slowly cooling molten sulfur that has been heated above 150 °C or by chilling solutions of sulfur in carbon disulfide, ethyl alcohol or hydrocarbons. It is found in nature as the mineral rosickyite.\n\nThese allotropes have been synthesised by various methods for example, reacting titanocene pentasulfide and a dichlorosulfane of suitable sulfur chain length, SCl:\n\nor alternatively reacting a dichlorosulfane, SCl and a polysulfane, HS:\n\nS, S and S can also be prepared from S. With the exception of cyclo-S, the rings contain S-S bond lengths and S-S-S bond angle that differ one from another.\n\nCyclo-S is the most stable cyclo-allotrope. Its structure can be visualised as having sulfur atoms in three parallel planes, 3 in the top, 6 in the middle and three in the bottom.\n\nTwo forms (α-, β-) of cyclo-S are known, one of which has been characterized.\n\nTwo forms of cyclo-S are known where the conformation of the ring is different. To differentiate these structures, rather than using the normal crystallographic convention of α-, β-, etc., which in other cyclo-S compounds refer to different packings of essentially the same conformer, these two conformers have been termed endo- and exo-.\n\nThis is produced from a solution of cyclo-S and cyclo-S in CS. It has a density midway between cyclo-S and cyclo-S. The crystal consists of alternate layers of cyclo-S and cyclo-S. For all the elements this may be the only allotrope which contains molecules of different sizes.\n\nThe production of pure forms of catena-sulfur has proved to be extremely difficult. Complicating factors include the purity of the starting material and the thermal history of the sample.\n\nThis form, also called fibrous sulfur or ω1-sulfur, has been well characterized. It has a density of 2.01 g·cm (α-sulfur 2.069 g·cm) and decomposes around its melting point of 104 °C. It consists of parallel helical sulfur chains. These chains have both left and right-handed \"twists\" and a radius of 95 pm. The S-S bond length is 206.6 pm, the S-S-S bond angle is 106° and the dihedral angle is 85.3°, (comparable figures for α-sulfur are 203.7 pm, 107.8° and 98.3°).\n\nThis has not been well characterized but is believed to consist of criss-crossed helices. It is also called χ-sulfur or ω2-sulfur.\n\nThe naming of the different forms is very confusing and care has to be taken to determine what is being described as the same names are used interchangeably.\n\nThis is the quenched product of sulfur melts above 160 °C (at this point the properties of the liquid melt change remarkably, e.g. large increase in viscosity). Its form changes from an initial plastic form gradually to a glassy form, hence its other names of plastic, glassy or vitreous sulfur. It is also called χ-sulfur. It contains a complex mixture of catena-sulfur forms mixed with cyclo-forms.\n\nThis is obtained by washing quenched liquid sulfur with CS. It is sometimes called polymeric sulfur, μ-S or ω-S.\n\nThis is a mixture of the allotropic ψ- form and γ-cycloS.\n\nThis is a commercially available product prepared from amorphous sulfur that has not been stretched prior to extraction of soluble forms with CS. It sometimes called \"white sulfur of Das\" or supersublimated sulfur. It is a mixture of ψ-sulfur and lamina sulfur. The composition depends on the exact method of production and the samples history. One well known commercial form is \"Crystex\". ω-sulfur is used in the vulcanization of rubber.\n\nThis name is given to the molten sulfur immediately after melting, cooling this gives predominantly β-sulfur.\n\nThis name is applied to solid insoluble sulfur and the melt prior to quenching.\n\nDark-coloured liquid formed when λ-sulfur is left to stay molten. Contains mixture of S rings.\n\nThis term is applied to biradical catena- chains in sulfur melts or the chains in the solid.\n\nAllotropes are in Bold. \n\nDisulfur, S, is the predominant species in sulfur vapour above 720 °C (a temperature above that shown in the phase diagram); at low pressure (1 mmHg) at 530 °C, it comprises 99% of the vapor. It is a triplet diradical (like dioxygen and sulfur monoxide), with an S-S bond length of 188.7 pm. The blue colour of burning sulfur is due to the emission of light by the S molecule produced in the flame.\n\nThe S molecule has been trapped in the compound [SI][EF] (E = As, Sb) for crystallographic measurements, produced by reacting elemental sulfur with excess iodine in liquid sulfur dioxide. The [SI] cation has an \"open-book\" structure, in which each [I] ion donates the unpaired electron in the π molecular orbital to a vacant orbital of the S molecule.\n\nS is found in sulfur vapour, comprising 10% of vapour species at 440 °C and 10 mmHg. It is cherry red in colour, with a bent structure, similar to ozone, O.\n\nThis has been detected in the vapour phase but has not been fully characterized; various forms, (e.g. chains, branched chains and rings) have been proposed. A primary research result based on theoretical calculations suggests that it has a ring structure.\n\n"}
{"id": "33791719", "url": "https://en.wikipedia.org/wiki?curid=33791719", "title": "Aphyocharacidium", "text": "Aphyocharacidium\n\nAphyocharacidium is a genus of characin found in tropical South America, with two currently described species:\n"}
{"id": "1130688", "url": "https://en.wikipedia.org/wiki?curid=1130688", "title": "Areal density (computer storage)", "text": "Areal density (computer storage)\n\nAreal density is a measure of the quantity of information bits that can be stored on a given length of track, area of surface, or in a given volume of a computer storage medium. Generally, higher density is more desirable, for it allows greater volumes of data to be stored in the same physical space. Density therefore has a direct relationship to storage capacity of a given medium. Density also generally has a fairly direct effect on the performance within a particular medium, as well as price.\n\nHard disk drives store data in the magnetic polarization of small patches of the surface coating on a disk. The maximum areal density is defined by the size of the magnetic particles in the surface, as well as the size of the \"head\" used to read and write the data. The areal density of disk storage devices has increased dramatically since IBM introduced the IBM 350 disk storage, the first hard disk drive in 1956 at an areal density of 2,000 bit/in. The growth rate has matched Moore's Law with the density reaching 1 Tbit/in in 2014./ In 2015, Seagate introduced a hard drive at a density of 1.34 Tbit/in, about 600 million times that of the first disk drive. It is expected that current recording technology can be \"feasibly\" expected to scale to at least 5 Tbit/in in the near future. New technologies like Heat-assisted magnetic recording (HAMR) and Microwave Assisted Magnetic Recording (MAMR) are under development and are expected to continue magnetic areal density progress.\n\nCompact discs (CDs) store data in small pits in a plastic surface that is then covered with a thin layer of reflective metal. The standard defines pits that are 0.83 micrometers long and 0.5 micrometers wide, arranged in tracks spaced 1.6 micrometers apart, offering a density of about 0.90 Gbit/in.\n\nDVD disks are essentially a higher-density CD, using more of the disk surface, smaller pits (0.64 micrometers), and tighter tracks (0.74 micrometers), offering a density of about 2.2 Gbit/in. Single-layer HD DVD and Blu-ray disks offer densities around 7.5 Gbit/in and 12.5 Gbit/in, respectively.\n\nWhen introduced in 1982 CDs had considerably higher densities than hard disk drives, but hard disk drives have since advanced much more quickly and eclipsed optical media in both areal density and capacity per device.\n\nThe first magnetic tape drive, the Univac Uniservo, recorded at the density of 128 bit/in on a half-inch magnetic tape, resulting in the areal density of 256 bit/in. In 2015, IBM and Fujifilm claimed a new record for the magnetic tape areal density of 1.23 Gbit/in, while LTO-6, the highest-density production tape shipping in 2015, provides an areal density of 0.84 Gbit/in.\n\nA number of technologies are attempting to surpass the densities of existing media.\n\nIBM's Millipede memory was attempting to commercialize a system at 1 Tbit/in in 2007 (800 Gbit/in was demonstrated in 2005). This is about the same capacity at which perpendicular hard drives are expected to \"top out\"; Millipede technology has, so far, been losing the density race with hard drives. Development since mid-2006 appears to be moribund, although the latest demonstrator with 2.7 Tbit/in seemed promising. A newer IBM technology, racetrack memory, uses an array of many small nanoscopic wires arranged in 3D, each holding numerous bits to improve density. Although exact numbers have not been mentioned, IBM news articles talk of \"100 times\" increases.\n\nVarious holographic storage technologies are also attempting to leapfrog existing systems, but they too have been losing the race, and are estimated to offer 1 Tbit/in as well, with about 250 GB/in being the best demonstrated to date- for non-quantum holography systems.\n\nOther experimental technologies offer even higher densities. Molecular polymer storage has been shown to store 10 Tbit/in. By far the densest type of memory storage experimentally to date is electronic quantum holography. By superimposing images of different wavelengths into the same hologram, a Stanford research team was able to achieve a bit density of 35 bit/electron (approximately 3 exabytes/in). This was demonstrated using electron microscopes and a copper medium as reported in the Stanford Report on January 28, 2009.\n\nIn 2012, DNA was successfully used as an experimental data storage medium, but required a DNA synthesizer and DNA microchips for the transcoding. , DNA holds the record for highest-density storage medium. In March 2017, scientists at Columbia University and the New York Genome Center published a method known as DNA Fountain which allows perfect retrieval of information from a density of 215 petabytes per gram of DNA, 85% of the theoretical limit.\n\nWith the notable exception of NAND Flash memory, increasing storage density of a medium is generally associated with improved transfer speed at which that medium can operate. This is most obvious when considering various disk-based media, where the storage elements are spread over the surface of the disk and must be physically rotated under the \"head\" in order to be read or written. Higher density means more data moves under the head for any given mechanical movement.\n\nConsidering the floppy disk as a basic example, we can calculate the effective transfer speed by determining how fast the bits move under the head. A standard 3½-inch floppy disk spins at 300 rpm, and the innermost track about 66 mm long (10.5 mm radius). At 300 rpm the linear speed of the media under the head is thus about 66 mm × 300 rpm = 19800 mm/minute, or 330 mm/s. Along that track the bits are stored at a density of 686 bit/mm, which means that the head sees 686 bit/mm × 330 mm/s = 226,380 bit/s (or 28.3 KiB/s).\n\nNow consider an improvement to the design that doubles the density of the bits by reducing sample length and keeping the same track spacing. This would immediately result in a doubling of transfer speed because the bits would be passing under the head twice as fast. Early floppy disk interfaces were originally designed with 250 kbit/s transfer speeds in mind, and were already being outperformed with the introduction of the \"high density\" 1.44 MB (1,440 KiB) floppies in the 1980s. The vast majority of PCs included interfaces designed for high density drives that ran at 500 kbit/s instead. These too were completely overwhelmed by newer devices like the LS-120, which were forced to use higher-speed interfaces such as IDE.\n\nAlthough the effect on performance is most obvious on rotating media, similar effects come into play even for solid-state media like Flash RAM or DRAM. In this case the performance is generally defined by the time it takes for the electrical signals to travel though the computer bus to the chips, and then through the chips to the individual \"cells\" used to store data (each cell holds one bit).\n\nOne defining electrical property is the resistance of the wires inside the chips. As the cell size decreases, through the improvements in semiconductor fabrication that lead to Moore's Law, the resistance is reduced and less power is needed to operate the cells. This, in turn, means that less electric current is needed for operation, and thus less time is needed to send the required amount of electrical charge into the system. In DRAM in particular the amount of charge that needs to be stored in a cell's capacitor also directly affects this time.\n\nAs fabrication has improved, solid-state memory has improved dramatically in terms of performance. Modern DRAM chips had operational speeds on the order of 10 ns or less. A less obvious effect is that as density improves, the number of DIMMs needed to supply any particular amount of memory decreases, which in turn means less DIMMs overall in any particular computer. This often leads to improved performance as well, as there is less bus traffic. However, this effect is generally not linear.\n\nStorage density also has a strong effect on the price of memory, although in this case the reasons are not so obvious.\n\nIn the case of disk-based media, the primary cost is the moving parts inside the drive. This sets a fixed lower limit, which is why the average selling price for both of the major HDD manufacturers has been $45–75 US since 2007. That said, the price of high-end drives has fallen rapidly, and this is indeed an effect of density. In this case the only way to make a higher capacity drive is to use more platters, essentially individual hard drives within the case. As the density increases the number of platters needed to supply any given amount of storage falls, leading to lower costs due to the reduction of mechanical parts inside. It is worth observing dollars per gigabyte (GB) for hard drives.\n\nThe fact that overall price has remained fairly steady has led to the common measure of the price/performance ratio in terms of cost per bit. In these terms the increase in density of hard drives becomes much more obvious. IBM's RAMAC from 1956 supplied 5 MB for $50,000, or $10,000 per megabyte. In 1989, a typical 40 MB hard drive from Western Digital retailed for $1199.00, or $30/MB. Drives broke the $1/MB in 1994, and in early 2000 were about 2¢/MB. By 2004, the 250 GB Western Digital Caviar SE listed for $249.99, approaching $1/GB, an improvement of 36 \"thousand\" times since 1989, and 10 million since the RAMAC. As of 2011, 2 TB drives are selling for less than $70, or 3.5¢/GB, an improvement of 1 million times since 1989, and 280 million since the RAMAC. This is all without adjusting for inflation, which adds another factor of about seven times since 1956.\n\nSolid-state storage has seen similar dramatic reductions in cost per bit. In this case the primary determinant of cost is \"yield\", the number of working chips produced in a unit time. Chips are produced in batches printed on the surface of a single large silicon wafer, which is then cut up and non-working examples are discarded. To improve yield, modern fabrication has moved to ever-larger wafers, and made great improvements in the quality of the production environment. Other factors include packaging the resulting wafer, which puts a lower limit on this process of about $1 per completed chip.\n\nThe relationship between information density and cost per bit can be illustrated as follows: a memory chip that is half the physical size means that twice as many units can be produced on the same wafer, thus halving the price of each one. As a comparison, DRAM was first introduced commercially in 1971, a 1 kbit part that cost about $50 in large batches, or about 5 cents per bit. 64 Mbit parts were common in 1999, which cost about 0.00002 cents per bit (20 microcents/bit).\n\n"}
{"id": "3679872", "url": "https://en.wikipedia.org/wiki?curid=3679872", "title": "Automatic watch", "text": "Automatic watch\n\nAn automatic or self-winding watch is a mechanical watch in which the natural motion of the wearer provides energy to run the watch, making manual winding unnecessary. A mechanical watch which is neither self-winding nor electrically driven is called a \"manual watch\".\n\nIn a mechanical watch the watch's gears are turned by a spiral spring called a mainspring. In a \"manual watch\" energy is stored in the mainspring by turning a knob, the \"crown\" on the side of the watch, winding the mainspring. Then the energy from the mainspring powers the watch movement until it runs down, requiring the spring to be wound again.\n\nA self-winding watch movement has a mechanism which winds the mainspring using the natural motions of the wearer's body. The watch contains an oscillating weight that turns on a pivot. The normal movements of the watch in the user's pocket or on the user's arm cause the rotor to pivot on its staff, which is attached to a ratcheted winding mechanism. The motion of the watch is thereby translated into circular motion of the weight which, through a series of reverser and reducing gears, eventually winds the mainspring. There are many different designs for modern self-winding mechanisms. Some designs allow winding of the watch to take place while the weight swings in only one direction while other, more advanced, mechanisms have two ratchets and wind the mainspring during both clockwise and anti-clockwise weight motions.\n\nThe fully wound mainspring in a typical watch can store enough energy reserve for roughly two days, allowing the watch to keep running through the night while stationary. In many cases automatic wristwatches can also be wound manually by turning the crown, so the watch can be kept running when not worn, and in case the wearer's wrist motions are not sufficient to keep it wound automatically.\n\nSelf-winding mechanisms continue working even after the mainspring is fully wound up. If a simple mainspring was used, this would put excessive tension on the mainspring. This could break the mainspring, and even if it did not, it can cause a problem called \"knocking\" or \"banking\". The excessive drive force applied to the watch movement gear train can make the balance wheel rotate with excessive amplitude, causing the impulse pin to hit the back of the pallet fork horns. This will make the watch run fast, and could break the impulse pin. To prevent this, a slipping clutch device is used on the mainspring so it cannot be overwound.\n\nThe \"slipping mainspring\" device was patented by Adrien Philippe, one of the founders of Patek Philippe, on 16 June 1863, long before self-winding wristwatches. In an ordinary watch mainspring barrel, the outer end of the spiral mainspring is attached to the inside of the barrel. In the slipping barrel, the mainspring is attached to a circular steel expansion spring, often called the 'bridle', which presses against the inside wall of the barrel, which has serrations or notches to hold it.\n\nAs long as the mainspring is less than fully wound, the bridle holds the mainspring by friction to the barrel wall, allowing the mainspring to be wound. When the mainspring reaches full wind, its force is stronger than the bridle spring, and further winding pulls the bridle loose from the notches and it simply slides along the wall, preventing the mainspring from being wound further. The bridle must grip the barrel wall with just the right force to allow the mainspring to wind fully but not overwind. If it grips too loosely, the bridle will begin to slip before the mainspring is fully wound, a defect known as 'mainspring creep' which results in a shortened reserve power time.\n\nA further advantage of this device is that the mainspring cannot be broken by excessive manual winding. This feature is often described in watch company advertising as an \"unbreakable mainspring\".\n\nThe earliest reference to self-winding watches is at the end of 1773 when a newspaper reported that Joseph Tlustos had invented a watch that did not need to be wound. But his idea was probably based on the myth of perpetual motion, and it is unlikely that it was a practical solution to the problem of self-winding watches. In 1776 Joseph Gallmayr also stated that he had made a self-winding watch, but there is no evidence to support this claim.\n\nThe earliest credible evidence for a successful design is the watch made by the Swiss watchmaker Abraham-Louis Perrelet, who lived in Le Locle. In late 1776 or early 1777, he invented a self-winding mechanism for pocket watches using an oscillating weight inside the watch that moved up and down. The Geneva Society of Arts, reporting on this watch in 1777, stated that 15 minutes walking was necessary to fully wind the watch. \n\nIn 1777 Abraham-Louis Breguet also became interested in the idea and his first attempts led him to make a self-winding mechanism with a barrel remontoire. Although a successful design, it was too complex and expensive for it to be manufactured and sold.\n\nAbout the end of 1777 or early 1778, Hubert Sarton () designed a watch with a rotor mechanism. Towards the end of 1778 he sent a watch to the French Academy of Sciences and a report was written which, together with a drawing, gave a detailed description of the mechanism. Sarton's design is similar to those used in modern wrist watches, although there is no evidence linking the 18th-century design to 20th-century developments.\n\nAbout the beginning of 1779, Abraham-Louis Breguet became aware of Perrelet's watches, probably through Louis Recordon, who travelled from Geneva to London via Le Locle and Paris. Breguet studied and improved the design, and made many self-winding watches from then to about 1810. \n\nAlthough a few self-winding watches and patents for them were made from 1780 on, for more than one hundred years these watches were rare, until the advent of the wrist watch.\n\nDuring the years 1776 to 1810 four different types of weight were used:\n\nAs noted above, some watches used bidirectional winding and others used unidirectional winding. The latter is sufficient and all of Breguet's watches used unidirectional winding. \n\nBefore the invention of the slipping mainspring, automatic watches had a system for locking the weight. Most common, as in the 1780 drawing, when the mainspring was fully wound a lever K was raised that entered a hole N in the weight to prevent it from moving until the mainspring had unwound enough to lower the lever. Different methods were used in side-weight, rotor and center-weight mechanisms.\n\nThe advent of the wrist watch after World War I led to renewed interest in self-winding mechanisms, and all four types listed above were used:\n\nInvented by John Harwood, a watch repairer from Bolton, England, who took out a UK patent with his financial backer, Harry Cutts, on 7 July 1923, and obtained a corresponding Swiss patent on 16 October 1923. The Harwood system used a pivoting weight which swung as the wearer moved, winding the mainspring. The ratchet mechanism wound the mainspring only when moving in one direction. The weight did not rotate a full 360°; spring bumpers limited its swing to about 180°, to encourage a back and forth motion. This early type of self-winding mechanism is now referred to as a 'hammer' or 'bumper'.\n\nLike its 18th century counterparts, Harwood's watch also had a problem with jerking because \"the brass weight hit too sharply against the banking pins as it pivoted\".\n\nWhen fully wound, Harwood's watch would run for 12 hours autonomously. It did not have a conventional stem winder, so the hands were moved manually by rotating a bezel around the face of the watch. The watches were first produced with the help of Swiss watch manufacturer Fortis and went on sale in 1928. 30,000 were made before the Harwood Self-Winding Watch Company collapsed in 1931 in the Great Depression. 'Bumper' watches were the first commercially successful automatic watches; they were made by several high grade watch manufacturers during the 1930s and 1940s.\n\nThe Rolex Watch Company improved Harwood's design in 1930 and used it as the basis for the Rolex Oyster Perpetual, in which the centrally mounted semi-circular weight could rotate through a full 360° rather than the about 200° of the 'bumper' winder. Rolex's version also increased the amount of energy stored in the mainspring, allowing it to run autonomously for up to 35 hours.\n\nInformation about 18th century rotor watches was not published until 1949. Although the Oyster Perpetual was probably an original invention, the company may have known of Coviot's 1893 patent that re-invented the 18th century design. \n\nThe next development for automatic watches came in 1948 from Eterna Watch. To wind a watch effectively, one of the chief requirements of a rotor is heft. Until this point, the best bearing used in any watch was a jewel bearing, which perfectly suits the small gears of a watch. A rotor, on the other hand, requires a different solution. In 1948, Eterna introduced the solution that is still in use today: ball bearings. Ball bearings provide robust support for a heavy object to rotate smoothly and reliably even under abnormal stress, such as if the watch were dropped.\n\nBy the 1960s, automatic winding had become standard in quality mechanical watches. Because the rotor weight needed in an automatic watch takes up a lot of space in the case, increasing its thickness, some manufacturers of quality watches, such as Patek Philippe, continue to design manually wound watches, which can be as thin as 1.77 millimeters.\n\nHowever, in 2007 Carl F. Bucherer implemented a new approach without a rotor, a peripherally mounted power source, where a geared ring made of tungsten encircles the entire mechanism, rotating on carbon rollers whenever the watch moves. A system of clutch wheels captures power. No rotor means thinner watches and an ultradense weight swinging around a greater radius means a better chance of achieving a greater power reserve with same amount of arm movement.\n\n\n"}
{"id": "46538081", "url": "https://en.wikipedia.org/wiki?curid=46538081", "title": "Bio bus", "text": "Bio bus\n\nBio-Bus is the UK's first bus powered entirely by human faeces and food waste.\n\nThe bus, which entered service in south west England in 2014, is powered by biomethane. The gas is made from human sewage and food waste which is processed at a plant in Avonmouth which is run by GENeco a subsidiary of Wessex Water. It has a Scania K270UB chassis and an Enviro300 body with 40 seats. A special registration was issued: YT13YUK.\n\nThe bus initially served a Bath Bus Company route between Bristol Airport and Bath. The neighbouring city of Bristol was European Green Capital in 2015 and, in connection with this, the bus operated in Bristol on the First Bristol number 2 route.\n\nSimilar buses have operated in Oslo.\n\nWhen compared with the diesel engines normally used to power buses, the Bio bus produces 20-30% less carbon dioxide, 80% fewer nitrogen oxides and is low in particulates. One tank of the gas will power the bus for ; however this means that the bus has to make a significant journey to refuel, making it less economic to run.\n"}
{"id": "6285002", "url": "https://en.wikipedia.org/wiki?curid=6285002", "title": "CIIC 504", "text": "CIIC 504\n\nCIIC 504 is an Ogham inscription. It was discovered at Ballaqueeney (Ballaquine) Isle of Man in 1874 by Reverend F. B. Grant during the process of excavating dirt for use as railroad ballast. \n\nThe stone was kept in a garden in Ballaqueeney from its discovery until about 1885. This caused the stone surface to deteriorate, due to being exposed to wind and rain. It is known that two characters can no longer be read. \n\nArcheologists note that \"the grooves are finely cut with a V-shaped section\" and \"the scores forming the consonants for the most part are 2 inches in length...there are no traces of divisional points; but there are longer spaces between the words than between the letters.\"\n\nThe stones are now located in the Manx Museum in Douglas, Isle of Man.\n\n\"Bivaidonas\", a person's name, is believed to suggest that the person named was related to the Conailli Muirtheimne of north Louth and south Down.\n\n\n"}
{"id": "42563057", "url": "https://en.wikipedia.org/wiki?curid=42563057", "title": "Composite repairs", "text": "Composite repairs\n\nComposite repairs. Composite materials are used in a wide range of applications in aerospace, marine, automotive, surface transport and sports equipment markets. Damage to composite components is not always visible to the naked eye and the extent of damage is best determined for structural components by suitable Non Destructive Test (NDT) methods. The concept for composite repair of composite or metallic structures is simple. The bonded repair reduces stresses in the damaged region and keeps the cracks from opening and therefore from growing.\n\nThe composite structures of interest are mainly components composed of laminated plies or sandwich structures as shown in Figure 1. Laminate structures are assembled so that the fibre orientation provides most of the desired mechanical properties and the matrix largely determines the environmental performance. In sandwich structures thin, high strength skins are separated by, and bonded to, lightweight honeycomb cores; the thicker the core the stiffer the panel with minimal weight increase.\n\nThe most important damage to fibre reinforced composites is the result of impact incidents. Low velocity and high velocity impact can result in significantly different damage patterns for a given composite configuration. In metals the impact energy is dissipated through elastic and plastic deformations and still the structure retains a good margin of structural integrity. In fibre reinforced composite materials however, the damage is usually more extensive than that seen on the surface. Some typical damage cases for composite structures are shown in Figure 2.\n\nIn monolithic laminates the underlying damage can extend to a much greater extent than the barely visible evidence on its surface. Other type of damage is laminate splitting. Here the damage does not extend through the full length of the part. The effects on the mechanical performance depend on the length of split relative to the component thickness.\n\nIn sandwich structures, impact results in dents of various sizes and depending on the energy levels, puncture damage is not unusual. In this case both skins may be damaged. Other common damage types include heat damage and bolt hole damage. The heat damage is caused by the exposure at high temperature which causes a local fracture with separation of surface plies. The bolt hole damage is cause by the bearing stresses at the contact surfaces of the composite structure with a bolt or rivet used for joining purposes. This could result in elongation of the hole causing laminate splitting, or damage to the upper plies. In any case, the effect on the mechanical performance depends on the thickness of the damaged part.\n\nFurther implications, apart from the initial impact damage, come from the exposure of the damaged area to moisture and other degrading factors like chemicals, lubricants, fuel, hydraulic fluids, etc. The presence of such environments may result in further deterioration of the mechanical performance.\n\nWhen performing a composite repair there are certain steps that should be followed. In Figure 3, a typical composite repair flowchart is given. The first step in the procedure should be a careful damage assessment. Some damage to composites is obvious and easily assessed but in many cases the damage may first appear quite small, although the real damage is very much greater. Impact damage to a fibre can appear as a small dent on the reinforced composite surface but the underlying damage can be much more extensive. The decision to repair or scrap is determined by considering the extent of repair needed to replace the original structural performance of the composite. Other considerations are the repair costs, the position and accessibility of the damage and the availability of suitable repair materials.\n\nThe initial assessment will determine the repair type to be performed. Easy repairs are usually small or do not affect the structural integrity of the component. These repairs are made by following the simple guidelines indicated for laminate or sandwich panels. Complex repairs are needed when the damage is extensive and needs to replace the structural performance of the component. The best choice of materials would be to use the original fibres, fabrics and matrix resin. Any alternative would need careful consideration of the service environment of the repaired composite, i.e. hot, wet and mechanical performance. The proposed repair scheme should meet all the original design requirements for the structure.\n\nSome repairs need the specialist equipment of the workshop and some form of improvised repair is needed to return the component to a suitable repair workshop. A temporary repair, usually in the form of a patch, can be fixed to the component to ensure safety until the component can be repaired at a later date. For a permanent repair all the approved general guidelines for laminate and sandwich repairs should be followed. These repair operations should be carried out in controlled workshop areas to ensure high quality. Operating in a controlled environment and attention to the detail will ensure success.\n\nBefore returning to service the quality check is always required. For comprehensive inspection of repaired parts a number of Non Destructive Tests (NDT) can be used. Special attention should be given to the quality of the repaired area and more specifically to the interface between the original part and the repaired area. Usual inspection methods include some form of ultrasonic or X-ray inspection.\n\nA typical composite repair usually starts after damage detection either by unarmed eye or various other NDT-techniques. After evaluating the extent of the damage, the damage zone is prepared for reparation. This is done by removing the composite material around the damage zone 1. Three techniques are known to be utilised, being slightly different depending on the nature of the composite, as shown in Figure 4.\nWhen a composite repair is applied the proper surface treatment is essential for a successful result. The above-mentioned repairs can be time consuming and often require high skills and experience to be performed. This is why current developments tend to focus on automating this process either by advanced mechanical milling or alternate technologies like nanosecond-pulsed lasers 2-3. After the damage zone has been completely excavated, surfaces are cleaned and further prepared for the final repair by patches. This can be done by plasma burning of surface contaminants, exposing fibres by removing matrix material through laser radiation or improving the surface wettability for adhesives by photochemical reactions induced by UV-laser light 4.\n\nIn a typical repair, the patch is applied under vacuum and at temperatures high enough for the curing of the adhesive. For these purposes a portable hot bonder device may be used for in the field repairs. For more complex and higher quality repairs an autoclave should be used. A hot bonder unit is shown in Figure 5.\nIn any case, the application of a vacuum bag is a necessary step for high quality repairs. Vacuum bag processing is suited to components with thin sections and large sandwich structures. The vacuum bag technique involves the placing and sealing of a flexible bag over a composite lay-up and evacuating all the air from under the bag as schematically shown in Figure 6.\nThe removal of air forces the bag down onto the lay-up with consolidation pressure of 1 atmosphere (1 bar). The completed assembly, with vacuum still applied, is heated up to the desired temperature for curing . This can be achieved by using heating mat or by placing the assembly inside an oven with good air circulation. For thicker sections and high quality bonding, the use of an autoclave with regulated temperature and additional overpressure should be sought.\n\nThe most important technical challenges in the implementation of a successful composite patch repair are: (a) proper design of the repair patch and the procedures that will be followed, (b) selection of the most suitable materials and tools for the application, (c) careful surface preparation (d) implementation of the composite patch repair and careful application of the cure cycle, (e) non destructive evaluation of the repair by suitable methodology and (f) monitoring of the structural integrity of the repair either at specific time intervals either continuously.\n\n\n"}
{"id": "22637838", "url": "https://en.wikipedia.org/wiki?curid=22637838", "title": "Copper indium gallium selenide solar cells", "text": "Copper indium gallium selenide solar cells\n\nA copper indium gallium selenide solar cell (or CIGS cell, sometimes CI(G)S or CIS cell) is a thin-film solar cell used to convert sunlight into electric power. It is manufactured by depositing a thin layer of copper, indium, gallium and selenide on glass or plastic backing, along with electrodes on the front and back to collect current. Because the material has a high absorption coefficient and strongly absorbs sunlight, a much thinner film is required than of other semiconductor materials.\n\nCIGS is one of three mainstream thin-film PV technologies, the other two being cadmium telluride and amorphous silicon. Like these materials, CIGS layers are thin enough to be flexible, allowing them to be deposited on flexible substrates. However, as all of these technologies normally use high-temperature deposition techniques, the best performance normally comes from cells deposited on glass, even though advances in low-temperature deposition of CIGS cells have erased much of this performance difference. CIGS outperforms polysilicon at the cell level, however its module efficiency is still lower, due to a less mature upscaling.\n\nThin-film market share is stagnated at around 15 percent, leaving the rest of the PV market to conventional solar cells made of crystalline silicon. In 2013, the market share of CIGS alone was about 2 percent and all thin-film technologies combined fell below 10 percent. CIGS cells continue being developed, as they promise to reach silicon-like efficiencies, while maintaining their low costs, as is typical for thin-film technology. Prominent manufacturers of CIGS photovoltaics were the now-bankrupt companies Nanosolar and Solyndra. Current market leader is the Japanese company Solar Frontier, Global Solar and GSHK Solar producing solar modules free of any heavy metals such as cadmium or lead.\n\nCIGS is a I-III-VI compound semiconductor material composed of copper, indium, gallium, and selenium. The material is a solid solution of copper indium selenide (often abbreviated \"CIS\") and copper gallium selenide, with a chemical formula of CuInGaSe, where the value of x can vary from 1 (pure copper indium selenide) to 0 (pure copper gallium selenide). It is a tetrahedrally bonded semiconductor, with the chalcopyrite crystal structure. The bandgap varies continuously with \"x\" from about 1.0 eV (for copper indium selenide) to about 1.7 eV (for copper gallium selenide).\n\nCIGS has an exceptionally high absorption coefficient of more than 10/cm for 1.5 eV and higher energy photons. CIGS solar cells with efficiencies around 20% have been claimed by the National Renewable Energy Laboratory (NREL), the Swiss Federal Laboratories for Materials Science and Technology (Empa), and the German \"Zentrum für Sonnenenergie und Wasserstoff Forschung\" (ZSW) (\"translated: Center for Solar Energy and Hydrogen Research\"), which is the record to date for any thin film solar cell.\n\nThe most common device structure for CIGS solar cells is shown in the diagram \"(see Figure 1: Structure of a CIGS device)\". Soda-lime glass of about of 1–3 milimetres thickness is commonly used as a substrate, because the glass sheets contains sodium, which has been shown to yield a substantial open-circuit voltage increase, notably through surface and grain boundary defects passivation. However, many companies are also looking at lighter and more flexible substrates such as polyimide or metal foils. A molybdenum (Mo) metal layer is deposited (commonly by sputtering) which serves as the back contact and reflects most unabsorbed light back into the CIGS absorber. Following molybdenum deposition a p-type CIGS absorber layer is grown by one of several unique methods. A thin n-type buffer layer is added on top of the absorber. The buffer is typically cadmium sulfide (CdS) deposited via chemical bath deposition. The buffer is overlaid with a thin, intrinsic zinc oxide layer (i-ZnO) which is capped by a thicker, aluminum (Al) doped ZnO layer. The i-ZnO layer is used to protect the CdS and the absorber layer from sputtering damage while depositing the ZnO:Al window layer, since the latter is usually deposited by DC sputtering, known as a damaging process. The Al doped ZnO serves as a transparent conducting oxide to collect and move electrons out of the cell while absorbing as little light as possible.\nThe CuInSe-based materials that are of interest for photovoltaic applications include several elements from groups I, III and VI in the periodic table. These semiconductors are especially attractive for solar applications because of their high optical absorption coefficients and versatile optical and electrical characteristics, which can in principle be manipulated and tuned for a specific need in a given device.\n\nCIGS is mainly used in the form of polycrystalline thin films. The best efficiency achieved as of September 2014 was 21.7%. A team at the National Renewable Energy Laboratory achieved 19.9%, a record at the time, by modifying the CIGS surface and making it look like CIS. These examples were deposited on glass, which meant the products were not mechanically flexible. In 2013, scientists at the Swiss Federal Laboratories for Materials Science and Technology developed CIGS cells on flexible polymer foils with a new record efficiency of 20.4%. These display both the highest efficiency and greatest flexibility.\n\nThe U.S. National Renewable Energy Laboratory confirmed 13.8% module efficiency of a large-area (meter-square) production panel, and 13% total-area (and 14.2% aperture-area) efficiency with some production modules. In September 2012 the German Manz AG presented a CIGS solar module with an efficiency of 14.6% on total module surface and 15.9% on aperture, which was produced on a mass production facility. MiaSolé obtained a certified 15.7% aperture-area efficiency on a 1m production module, and Solar Frontier claimed a 17.8% efficiency on a 900 cm module.\n\nHigher efficiencies (around 30%) can be obtained by using optics to concentrate the incident light. The use of gallium increases the optical band gap of the CIGS layer as compared to pure CIS, thus increasing the open-circuit voltage. Gallium's relative abundance, compared to indium, lowers costs.\n\nUnlike conventional crystalline silicon cells based on a homojunction, the structure of CIGS cells is a more complex heterojunction system. A direct bandgap material, CIGS has very strong light absorption and a layer of only 1–2 micrometers (µm) is enough to absorb most of the sunlight. By comparison, a much greater thickness of about 160–190 µm is required for crystalline silicon.\n\nThe active CIGS-layer can be deposited in a polycrystalline form directly onto molybdenum (Mo) coated on a variety of several different substrates such as glass sheets, steel bands and plastic foils made of polyimide. This uses less energy than smelting large amounts of quartz sand in electric furnaces and growing large crystals, necessary for conventional silicon cells, and thus reduces its energy payback time significantly. Also unlike crystalline silicon, these substrates can be flexible.\n\nIn the highly competitive PV industry, pressure increased on CIGS manufacturers, leading to the bankruptcy of several companies, as prices for conventional silicon cells declined rapidly in recent years. However, CIGS solar cells have become as efficient as multicrystalline silicon cells—the most common type of solar cells. CIGS and CdTe-PV remain the only two commercially successful thin-film technologies in a globally fast-growing PV market.\n\nIn photovoltaics \"thinness\" generally is in reference to so-called \"first generation\" high-efficiency silicon cells, which are manufactured from bulk wafers hundreds of micrometers thick. Thin films sacrifice some light gathering efficiency but use less material. In CIGS the efficiency tradeoff is less severe than in silicon. The record efficiencies for thin film CIGS cells are slightly lower than that of CIGS for lab-scale top performance cells. In 2008, CIGS efficiency was by far the highest compared with those achieved by other thin film technologies such as cadmium telluride photovoltaics (CdTe) or amorphous silicon (a-Si). CIS and CGS solar cells offer total area efficiencies of 15.0% and 9.5%, respectively. In 2015, the gap with the other thin film technologies has been closed, with record cell efficiencies in laboratories of 21.5% for CdTe (FirstSolar) and 21.7% for CIGS (ZSW). \"(See also .)\"\n\nAll high performance CIGS absorbers in solar cells have similarities independent of production technique. First, they are polycrystalline α-phase which has the chalcopyrite crystal structure shown in Figure 3. The second property is an overall Cu deficiency. Cu deficiency increases the majority carrier (hole) concentration by increasing the number of (electron-accepting) Cu vacancies. When CIGS films are In rich (Cu deficient) the film's surface layer forms an ordered defect compound (ODC) with a stoichiometry of Cu(In,Ga)Se. The ODC is n-type, forming a p-n homojunction in the film at the interface between the α phase and the ODC. The recombination velocity at the CIGS/CdS interface is decreased by the homojunction's presence. The drop in interface recombination attributable to ODC formation is demonstrated by experiments which have shown that recombination in the bulk of the film is the main loss mechanism in Cu deficient films, while in Cu rich films the main loss is at the CIGS/CdS interface.\n\nSodium incorporation is necessary for optimal performance. Ideal Na concentration is considered to be approximately 0.1%. Na is commonly supplied by the soda-lime glass substrate, but in processes that do not use this substrate the Na must be deliberately added. Na's beneficial effects include increases in p-type conductivity, texture, and average grain size. Furthermore, Na incorporation allows for performance to be maintained over larger stoichiometric deviations. Simulations have predicted that Na on an In site creates a shallow acceptor level and that Na serves to remove In on Cu defects (donors), but reasons for these benefits are controversial. Na is also credited with catalyzing oxygen absorption. Oxygen passivates Se vacancies that act as compensating donors and recombination centers.\nAlloying CIS (CuInSe) with CGS (CuGaSe) increases the bandgap. To reach the ideal bandgap for a single junction solar cell, 1.5 eV, a Ga/(In+Ga) ratio of roughly 0.7 is optimal. However, at ratios above ~0.3, device performance drops off. Industry currently targets the 0.3 Ga/(In+Ga) ratio, resulting in bandgaps between 1.1 and 1.2 eV. The decreasing performance has been postulated to be a result of CGS not forming the ODC, which is necessary for a good interface with CdS.\n\nThe highest efficiency devices show substantial texturing, or preferred crystallographic orientation. A (204) surface orientation is observed in the best quality devices. A smooth absorber surface is preferred to maximize the ratio of the illuminated area to the area of the interface. The area of the interface increases with roughness while illuminated area remains constant, decreasing open circuit voltage (V). Studies have also linked an increase in defect density to decreased V. Recombination in CIGS has been suggested to be dominated by non-radiative processes. Theoretically, recombination can be controlled by engineering the film, and is extrinsic to the material.\n\nThe most common vacuum-based process is to co-evaporate or co-sputter copper, gallium, and indium onto a substrate at room temperature, then anneal the resulting film with a selenide vapor. An alternative process is to co-evaporate copper, gallium, indium and selenium onto a heated substrate.\n\nA non-vacuum-based alternative process deposits nanoparticles of the precursor materials on the substrate and then sinters them in situ. Electroplating is another low cost alternative to apply the CIGS layer.\n\nThe following sections outline the various techniques for precursor deposition processing, including sputtering of metallic layers at low temperatures, printing of inks containing nanoparticles, electrodeposition, and a technique inspired by wafer-bonding.\n\nThe Se supply and selenization environment is important in determining the properties and quality of the film. When Se is supplied in the gas phase (for example as HSe or elemental Se) at high temperatures, the Se becomes incorporated into the film by absorption and subsequent diffusion. During this step, called chalcogenization, complex interactions occur to form a chalcogenide. These interactions include formation of Cu-In-Ga intermetallic alloys, formation of intermediate metal-selenide binary compounds and phase separation of various stoichiometric CIGS compounds. Because of the variety and complexity of the reactions, the properties of the CIGS film are difficult to control.\n\nThe Se source affects the resulting film properties. HSe offers the fastest Se incorporation into the absorber; 50 at% Se can be achieved in CIGS films at temperatures as low as 400 °C. By comparison, elemental Se only achieves full incorporation with reaction temperatures above 500 °C. Films formed at lower temperatures from elemental Se were Se deficient, but had multiple phases including metal selenides and various alloys. Use of HSe provides the best compositional uniformity and the largest grain sizes. However, HSe is highly toxic and is classified as an environmental hazard.\n\nIn this method a metal film of Cu, In and Ga is sputtered at or near room temperature and reacted in a Se atmosphere at high temperature. This process has higher throughput than coevaporation and compositional uniformity can be more easily achieved.\n\nSputtering a stacked multilayer of metal – for example a Cu/In/Ga/Cu/In/Ga... structure – produces a smoother surface and better crystallinity in the absorber compared to a simple bilayer (Cu-Ga alloy/In) or trilayer (Cu/In/Ga) sputtering. These attributes result in higher efficiency devices, but forming the multilayer is a more complicated deposition process and did not merit the extra equipment or the added process complexity. Additionally, the reaction rates of Cu/Ga and Cu/In layers with Se are different. If the reaction temperature is not high enough, or not held long enough, CIS and CGS form as separate phases. \nCompanies currently that used similar processes include Showa Shell, Avancis (now an affiliate of Saint-Gobain Group), Miasolé, Honda Soltec, and Energy Photovoltaics (EPV). Showa Shell sputtered a Cu-Ga alloy layer and an In layer, followed by selenization in HSe and sulfurization in HS. The sulfurization step appears to passivate the surface in a way similar to CdS in most other cells. Thus, the buffer layer used is Cd-free, eliminating any environmental impact of Cd. Showa Shell reported a maximum module efficiency of 13.6% with an average of 11.3% for 3600 cm substrates. Shell Solar uses the same technique as Showa Shell to create the absorber; however, their CdS layer comes from chemical vapor deposition. Modules sold by Shell Solar claim 9.4% module efficiency.\nMiasole had procured venture capital funds for its process and scale up. However, little is known about their process beyond their stated efficiency of 9 to 10% for modules.\nEPV uses a hybrid between coevaporation and sputtering in which In and Ga are evaporated in a Se atmosphere. This is followed by Cu sputtering and selenization. Finally, In and Ga are again evaporated in the presence of Se. Based on Hall measurements, these films have a low carrier concentration and relatively high mobility. EPV films have a low defect concentration.\n\nIn this method, metal or metal-oxide nanoparticles are used as the precursors for CIGS growth. These nanoparticles are generally suspended in a water based solution and then applied to large areas by various methods, such as printing. The film is then dehydrated and, if the precursors are metal-oxides, reduced in a H/N atmosphere. Following dehydration, the remaining porous film is sintered and selenized at temperatures greater than 400 °C.\n\nNanosolar and International Solar Electric Technology (ISET) unsuccessfully attempted to scale up this process. ISET uses oxide particles, while Nanosolar did not discuss its ink. The advantages of this process include uniformity over large areas, non-vacuum or low-vacuum equipment and adaptability to roll-to-roll manufacturing. When compared to laminar metal precursor layers, sintered nanoparticles selenize more rapidly. The increased rate is a result of the greater surface area associated with porosity. Porosity produces rougher absorber surfaces. Use of particulate precursors allows for printing on a large variety of substrates with materials utilization of 90% or more. Little research and development supported this technique.\nNanosolar reported a cell (not module) efficiency of 14%, however this was not verified by any national laboratory testing, nor did they allow onsite inspections. In independent testing ISET's absorber had the 2nd lowest efficiency at 8.6%. However, all the modules that beat ISET's module were coevaporated, a process which has manufacturing disadvantages and higher costs. ISET's sample suffered most from low V and low fill factor, indicative of a rough surface and/or a high number of defects aiding recombination. Related to these issues, the film had poor transport properties including a low Hall mobility and short carrier lifetime.\n\nPrecursors can be deposited by electrodeposition. Two methodologies exist: deposition of elemental layered structures and simultaneous deposition of all elements (including Se). Both methods require thermal treatment in a Se atmosphere to make device quality films. Because electrodeposition requires conductive electrodes, metal foils are a logical substrate. Electrodeposition of elemental layers is similar to the sputtering of elemental layers. \nSimultaneous deposition employs a working electrode (cathode), a counter electrode (anode), and a reference electrode as in Figure 4. A metal foil substrate is used as the working electrode in industrial processes. An inert material provides the counter electrode, and the reference electrode measures and controls the potential. The reference electrode allows the process to be performed potentiostatically, allowing control of the substrate's potential.\n\nSimultaneous electrodeposition must overcome the fact that the elements' standard reduction potentials are not equal, causing preferential deposition of a single element. This problem is commonly alleviated by adding countering ions into solution for each ion to be deposited (Cu, Se, In, and Ga), thus changing that ion's reduction potential. Further, the Cu-Se system has a complicated behavior and the film's composition depends on the Se/Cu ion flux ratio which can vary over the film surface. This requires the precursor concentrations and deposition potential to be optimized. Even with optimization, reproducibility is low over large areas due to composition variations and potential drops along the substrate.\nThe resulting films have small grains, are Cu-rich, and generally contain CuSe phases along with impurities from the solution. Annealing is required to improve crystallinity. For efficiencies higher than 7%, a stoichiometry correction are required. The correction was originally done via high temperature physical vapor deposition, which is not practical in industry.\nSolopower is currently producing cells with >13.7% conversion efficiency as per NREL.\n\nIn this process, two different precursor films are deposited separately on a substrate and a superstrate. The films are pressed together and heated to release the film from the reusable superstrate, leaving a CIGS absorber on the substrate (Figure 5). Heliovolt patented this procedure and named it the FASST process. In principle, the precursors can be deposited at low temperature using low-cost deposition techniques, lowering module cost. However, the first generations of products use higher temperature PVD methods and do not achieve full cost cutting potential. Flexible substrates could eventually be used in this process. \nTypical film characteristics are not known outside of the company, as no research has been conducted by independently funded laboratories. However, Heliovolt claimed a top cell efficiency of 12.2%.\n\nCoevaporation, or codeposition, is the most prevalent CIGS fabrication technique. Boeing's coevaporation process deposits bilayers of CIGS with different stoichiometries onto a heated substrate and allows them to intermix.\n\nNREL developed another process that involves three deposition steps and produced the current CIGS efficiency record holder at 20.3%. The first step in NREL's method is codeposition of In, Ga, and Se. This is followed by Cu and Se deposited at a higher temperature to allow for diffusion and intermixing of the elements. In the final stage In, Ga, and Se are again deposited to make the overall composition Cu deficient.\nWürth Solar began producing CIGS cells using an inline coevaporation system in 2005 with module efficiencies between 11% and 12%. They opened another production facility and continued to improve efficiency and yield. Other companies scaling up coevaporation processes include Global Solar and Ascent Solar. Global Solar used an inline three stage deposition process. In all of the steps Se is supplied in excess in the vapor phase. In and Ga are first evaporated followed by Cu and then by In and Ga to make the film Cu deficient. These films performed quite favorably in relation to other manufacturers and to absorbers grown at NREL and the Institute for Energy Conversion (IEC). However, modules of Global Solar’s films did not perform as well. The property in which the module most obviously under-performed was a low V, which is characteristic of high defect density and high recombination velocities. Global Solar’s absorber layer outperformed the NREL absorber in carrier lifetime and hall mobility. However, as completed cells the NREL sample performed better. This is evidence of a poor CIGS/CdS interface, possibly due to the lack of an ODC surface layer on the Global Solar film.\nDisadvantages include uniformity issues over large areas and the related difficulty of coevaporating elements in an inline system. Also, high growth temperatures raise the thermal budget and costs. Additionally, coevaporation is plagued by low material utilization (deposition on chamber walls instead of the substrate, especially for selenium) and expensive vacuum equipment. A way to enhance Se utilisation is via a thermal or plasma-enhanced selenium-cracking process, which can be coupled with an ion beam source for ion beam assisted deposition.\n\nChemical vapor deposition (CVD) has been implemented in multiple ways for the deposition of CIGS. Processes include atmosphere pressure metal organic CVD (AP-MOCVD), plasma-enhanced CVD (PECVD), low-pressure MOCVD (LP-MOCVD), and aerosol assisted MOCVD (AA-MOCVD). Research is attempting to switch from dual-source precursors to single-source precursors. Multiple source precursors must be homogeneously mixed and the flow rates of the precursors have to be kept at the proper stoichiometry. Single-source precursor methods do not suffer from these drawbacks and should enable better control of film composition.\nAs of 2014 CVD was not used for commercial CIGS synthesis. CVD produced films have low efficiency and a low V, partially a result of a high defect concentration. Additionally, film surfaces are generally quite rough which serves to further decrease the V. However, the requisite Cu deficiency has been achieved using AA-MOCVD along with a (112) crystal orientation.\nCVD deposition temperatures are lower than those used for other processes such as co-evaporation and selenization of metallic precursors. Therefore, CVD has a lower thermal budget and lower costs. Potential manufacturing problems include difficulties converting CVD to an inline process as well as the expense of handling volatile precursors.\n\nCIS films can be produced by electrospray deposition. The technique involves the electric field assisted spraying of ink containing CIS nano-particles onto the substrate directly and then sintering in an inert environment. The main advantage of this technique is that the process takes place at room temperature and it is possible to attach this process with some continuous or mass production system like roll-to-roll production mechanism.\n\n\n"}
{"id": "41550247", "url": "https://en.wikipedia.org/wiki?curid=41550247", "title": "Double H Pipeline", "text": "Double H Pipeline\n\nDouble H Pipeline is a planned crude oil pipeline from Dore, North Dakota to Guernsey, Wyoming. It is supposed to carry 100,000 barrels (50,000 initially) of crude oil from the North Dakota and Bakken formation shale plays as well as Wyoming's oil fields.\n\nThe project was proposed by Hiland Crude, LLC a subsidiary of Hiland Partners that is owned by the Harold Hamm family from Enid, Oklahoma.\n\nThe 12-inch line was scheduled to begin operating after completion in January 2015. It would connect to the Pony Express Pipeline owned by Tallgrass Energy to connect with the crude oil hub of Cushing, Oklahoma and access lucrative oil markets.\n\nIn January 2015, it was reported that Hamm was selling the Bakken pipeline network.\n\n\n"}
{"id": "57029791", "url": "https://en.wikipedia.org/wiki?curid=57029791", "title": "Earl Bell Gilmore", "text": "Earl Bell Gilmore\n\nEarl Bell Gilmore (1887–1964) was the son of Arthur Fremont Gilmore and took control of the family businesses, including the Gilmore Oil Company, in 1918.\n"}
{"id": "1754814", "url": "https://en.wikipedia.org/wiki?curid=1754814", "title": "Ecology (journal)", "text": "Ecology (journal)\n\nEcology is a scientific journal that publishes research and synthesizes papers in the field of ecology. It was founded in 1920, and is published by the Ecological Society of America. According to the \"Journal Citation Reports,\" it is currently ranked 15th out of 136 journals in the Ecology category.\n"}
{"id": "9479", "url": "https://en.wikipedia.org/wiki?curid=9479", "title": "Einsteinium", "text": "Einsteinium\n\nEinsteinium is a synthetic element with symbol Es and atomic number 99. A member of the actinide series, it is the seventh transuranic element.\n\nEinsteinium was discovered as a component of the debris of the first hydrogen bomb explosion in 1952, and named after Albert Einstein. Its most common isotope einsteinium-253 (half-life 20.47 days) is produced artificially from decay of californium-253 in a few dedicated high-power nuclear reactors with a total yield on the order of one milligram per year. The reactor synthesis is followed by a complex process of separating einsteinium-253 from other actinides and products of their decay. Other isotopes are synthesized in various laboratories, but at much smaller amounts, by bombarding heavy actinide elements with light ions. Owing to the small amounts of produced einsteinium and the short half-life of its most easily produced isotope, there are currently almost no practical applications for it outside basic scientific research. In particular, einsteinium was used to synthesize, for the first time, 17 atoms of the new element mendelevium in 1955.\n\nEinsteinium is a soft, silvery, paramagnetic metal. Its chemistry is typical of the late actinides, with a preponderance of the +3 oxidation state; the +2 oxidation state is also accessible, especially in solids. The high radioactivity of einsteinium-253 produces a visible glow and rapidly damages its crystalline metal lattice, with released heat of about 1000 watts per gram. Difficulty in studying its properties is due to einsteinium-253's decay to berkelium-249 and then californium-249 at a rate of about 3% per day. The isotope of einsteinium with the longest half-life, einsteinium-252 (half-life 471.7 days) would be more suitable for investigation of physical properties, but it has proven far more difficult to produce and is available only in minute quantities, and not in bulk. Einsteinium is the element with the highest atomic number which has been observed in macroscopic quantities in its pure form, and this was the common short-lived isotope einsteinium-253.\n\nLike all synthetic transuranic elements, isotopes of einsteinium are very radioactive and are considered highly dangerous to health on ingestion.\n\nEinsteinium was first identified in December 1952 by Albert Ghiorso and co-workers at the University of California, Berkeley in collaboration with the Argonne and Los Alamos National Laboratories, in the fallout from the \"Ivy Mike\" nuclear test. The test was carried out on November 1, 1952 at Enewetak Atoll in the Pacific Ocean and was the first successful test of a hydrogen bomb. Initial examination of the debris from the explosion had shown the production of a new isotope of plutonium, , which could only have formed by the absorption of six neutrons by a uranium-238 nucleus followed by two beta decays.\nAt the time, the multiple neutron absorption was thought to be an extremely rare process, but the identification of indicated that still more neutrons could have been captured by the uranium nuclei, thereby producing new elements heavier than californium.\nGhiorso and co-workers analyzed filter papers which had been flown through the explosion cloud on airplanes (the same sampling technique that had been used to discover ). Larger amounts of radioactive material were later isolated from coral debris of the atoll, which were delivered to the U.S. The separation of suspected new elements was carried out in the presence of a citric acid/ammonium buffer solution in a weakly acidic medium (pH ≈ 3.5), using ion exchange at elevated temperatures; fewer than 200 atoms of einsteinium were recovered in the end. Nevertheless, element 99 (einsteinium), namely its Es isotope, could be detected via its characteristic high-energy alpha decay at 6.6 MeV. It was produced by the capture of 15 neutrons by uranium-238 nuclei followed by seven beta-decays, and had a half-life of 20.5 days. Such multiple neutron absorption was made possible by the high neutron flux density during the detonation, so that newly generated heavy isotopes had plenty of available neutrons to absorb before they could disintegrate into lighter elements. Neutron capture initially raised the mass number without changing the atomic number of the nuclide, and the concomitant beta-decays resulted in a gradual increase in the atomic number:\n^{238}_{92}U ->[\\ce{+15n}][6 \\beta^-] ^{253}_{98}Cf ->[\\beta^-] ^{253}_{99}Es\n</chem>\n\nSome U atoms, however, could absorb two additional neutrons (for a total of 17), resulting in Es, as well as in the Fm isotope of another new element, fermium. The discovery of the new elements and the associated new data on multiple neutron capture were initially kept secret on the orders of the U.S. military until 1955 due to Cold War tensions and competition with Soviet Union in nuclear technologies. However, the rapid capture of so many neutrons would provide needed direct experimental confirmation of the so-called r-process multiple neutron absorption needed to explain the cosmic nucleosynthesis (production) of certain heavy chemical elements (heavier than nickel) in supernova explosions, before beta decay. Such a process is needed to explain the existence of many stable elements in the universe.\n\nMeanwhile, isotopes of element 99 (as well as of new element 100, fermium) were produced in the Berkeley and Argonne laboratories, in a nuclear reaction between nitrogen-14 and uranium-238, and later by intense neutron irradiation of plutonium or californium:\n\nThese results were published in several articles in 1954 with the disclaimer that these were not the first studies that had been carried out on the elements. The Berkeley team also reported some results on the chemical properties of einsteinium and fermium. The \"Ivy Mike\" results were declassified and published in 1955.\nIn their discovery of the elements 99 and 100, the American teams had competed with a group at the Nobel Institute for Physics, Stockholm, Sweden. In late 1953 – early 1954, the Swedish group succeeded in the synthesis of light isotopes of element 100, in particular Fm, by bombarding uranium with oxygen nuclei. These results were also published in 1954. Nevertheless, the priority of the Berkeley team was generally recognized, as its publications preceded the Swedish article, and they were based on the previously undisclosed results of the 1952 thermonuclear explosion; thus the Berkeley team was given the privilege to name the new elements. As the effort which had led to the design of \"Ivy Mike\" was codenamed Project PANDA, element 99 had been jokingly nicknamed \"Pandamonium\" but the official names suggested by the Berkeley group derived from two prominent scientists, Albert Einstein and Enrico Fermi: \"We suggest for the name for the element with the atomic number 99, einsteinium (symbol E) after Albert Einstein and for the name for the element with atomic number 100, fermium (symbol Fm), after Enrico Fermi.\" Both Einstein and Fermi died between the time the names were originally proposed and when they were announced. The discovery of these new elements was announced by Albert Ghiorso at the first Geneva Atomic Conference held on 8–20 August 1955. The symbol for einsteinium was first given as \"E\" and later changed to \"Es\" by IUPAC.\n\nEinsteinium is a synthetic, silvery-white, radioactive metal. In the periodic table, it is located to the right of the actinide californium, to the left of the actinide fermium and below the lanthanide holmium with which it shares many similarities in physical and chemical properties. Its density of 8.84 g/cm is lower than that of californium (15.1 g/cm) and is nearly the same as that of holmium (8.79 g/cm), despite atomic einsteinium being much heavier than holmium. The melting point of einsteinium (860 °C) is also relatively low – below californium (900 °C), fermium (1527 °C) and holmium (1461 °C). Einsteinium is a soft metal, with the bulk modulus of only 15 GPa, which value is one of the lowest among non-alkali metals.\n\nContrary to the lighter actinides californium, berkelium, curium and americium which crystallize in a double hexagonal structure at ambient conditions, einsteinium is believed to have a face-centered cubic (\"fcc\") symmetry with the space group \"Fm\"\"m\" and the lattice constant \"a\" = 575 pm. However, there is a report of room-temperature hexagonal einsteinium metal with \"a\" = 398 pm and \"c\" = 650 pm, which converted to the \"fcc\" phase upon heating to 300 °C.\n\nThe self-damage induced by the radioactivity of einsteinium is so strong that it rapidly destroys the crystal lattice, and the energy release during this process, 1000 watts per gram of Es, induces a visible glow. These processes may contribute to the relatively low density and melting point of einsteinium. Further, owing to the small size of the available samples, the melting point of einsteinium was often deduced by observing the sample being heated inside an electron microscope. Thus the surface effects in small samples could reduce the melting point value.\n\nThe metal is divalent and has a noticeably high volatility. In order to reduce the self-radiation damage, most measurements of solid einsteinium and its compounds are performed right after thermal annealing. Also, some compounds are studied under the atmosphere of the reductant gas, for example HO+HCl for EsOCl so that the sample is partly regrown during its decomposition.\n\nApart from the self-destruction of solid einsteinium and its compounds, other intrinsic difficulties in studying this element include scarcity – the most common Es isotope is available only once or twice a year in sub-milligram amounts – and self-contamination due to rapid conversion of einsteinium to berkelium and then to californium at a rate of about 3.3% per day:\n^{253}_{99}Es ->[\\alpha][20 \\ce{d}] ^{249}_{97}Bk ->[\\beta^-][314 \\ce{d}] ^{249}_{98}Cf\n</chem>\n\nThus, most einsteinium samples are contaminated, and their intrinsic properties are often deduced by extrapolating back experimental data accumulated over time. Other experimental techniques to circumvent the contamination problem include selective optical excitation of einsteinium ions by a tunable laser, such as in studying its luminescence properties.\n\nMagnetic properties have been studied for einsteinium metal, its oxide and fluoride. All three materials showed Curie–Weiss paramagnetic behavior from liquid helium to room temperature. The effective magnetic moments were deduced as for EsO and for the EsF, which are the highest values among actinides, and the corresponding Curie temperatures are 53 and 37 K.\n\nLike all actinides, einsteinium is rather reactive. Its trivalent oxidation state is most stable in solids and aqueous solution where it induces a pale pink color. The existence of divalent einsteinium is firmly established, especially in the solid phase; such +2 state is not observed in many other actinides, including protactinium, uranium, neptunium, plutonium, curium and berkelium. Einsteinium(II) compounds can be obtained, for example, by reducing einsteinium(III) with samarium(II) chloride. The oxidation state +4 was postulated from vapor studies and is yet uncertain.\n\nNineteen nuclides and three nuclear isomers are known for einsteinium with atomic weights ranging from 240 to 258. All are radioactive and the most stable nuclide, Es, has a half-life of 471.7 days. Next most stable isotopes are Es (half-life 275.7 days), Es (39.8 days) and Es (20.47 days). All of the remaining isotopes have half-lives shorter than 40 hours, and most of them decay within less than 30 minutes. Of the three nuclear isomers, the most stable is Es with half-life of 39.3 hours.\n\nEinsteinium has a high rate of nuclear fission that results in a low critical mass for a sustained nuclear chain reaction. This mass is 9.89 kilograms for a bare sphere of Es isotope, and can be lowered to 2.9 by adding a 30-centimeter-thick steel neutron reflector, or even to 2.26 kilograms with a 20-cm-thick reflector made of water. However, even this small critical mass greatly exceeds the total amount of einsteinium isolated thus far, especially of the rare Es isotope.\n\nBecause of the short half-life of all isotopes of einsteinium, any primordial einsteinium — that is, einsteinium that could possibly have been present on the Earth during its formation — has long since decayed. Synthesis of einsteinium from naturally-occurring actinides uranium and thorium in the Earth's crust requires multiple neutron capture, which is an extremely unlikely event. Therefore, all terrestrial einsteinium is produced in scientific laboratories, high-power nuclear reactors, or in nuclear weapons tests, and is present only within a few years from the time of the synthesis. The transuranic elements from americium to fermium, including einsteinium, occurred naturally in the natural nuclear fission reactor at Oklo, but no longer do so. Einsteinium was observed in Przybylski's Star in 2008.\n\nEinsteinium is produced in minute quantities by bombarding lighter actinides with neutrons in dedicated high-flux nuclear reactors. The world's major irradiation sources are the 85-megawatt High Flux Isotope Reactor (HFIR) at the Oak Ridge National Laboratory in Tennessee, U.S., and the SM-2 loop reactor at the Research Institute of Atomic Reactors (NIIAR) in Dimitrovgrad, Russia, which are both dedicated to the production of transcurium (\"Z\" > 96) elements. These facilities have similar power and flux levels, and are expected to have comparable production capacities for transcurium elements, although the quantities produced at NIIAR are not widely reported. In a \"typical processing campaign\" at Oak Ridge, tens of grams of curium are irradiated to produce decigram quantities of californium, milligram quantities of berkelium (Bk) and einsteinium and picogram quantities of fermium.\n\nThe first microscopic sample of Es sample weighing about 10 nanograms was prepared in 1961 at HFIR. A special magnetic balance was designed to estimate its weight. Larger batches were produced later starting from several kilograms of plutonium with the einsteinium yields (mostly Es) of 0.48 milligrams in 1967–1970, 3.2 milligrams in 1971–1973, followed by steady production of about 3 milligrams per year between 1974 and 1978. These quantities however refer to the integral amount in the target right after irradiation. Subsequent separation procedures reduced the amount of isotopically pure einsteinium roughly tenfold.\n\nHeavy neutron irradiation of plutonium results in four major isotopes of einsteinium: Es (α-emitter with half-life of 20.03 days and with a spontaneous fission half-life of 7×10 years); Es (β-emitter with half-life of 38.5 hours), Es (α-emitter with half-life of about 276 days) and Es (β-emitter with half-life of 24 days). An alternative route involves bombardment of uranium-238 with high-intensity nitrogen or oxygen ion beams.\n\nEinsteinium-247 (half-life 4.55 minutes) was produced by irradiating americium-241 with carbon or uranium-238 with nitrogen ions. The latter reaction was first realized in 1967 in Dubna, Russia, and the involved scientists were awarded the Lenin Komsomol Prize.\n\nThe isotope Es was produced by irradiating Cf with deuterium ions. It mainly decays by emission of electrons to Cf with a half-life of minutes, but also releases α-particles of 6.87 MeV energy, with the ratio of electrons to α-particles of about 400.\n\nThe heavier isotopes Es, Es, Es and Es were obtained by bombarding Bk with α-particles. One to four neutrons are liberated in this process making possible the formation of four different isotopes in one reaction.\n\nEinsteinium-253 was produced by irradiating a 0.1–0.2 milligram Cf target with a thermal neutron flux of (2–5)×10 neutrons·cm·s for 500–900 hours:\n\nThe analysis of the debris at the 10-megaton \"Ivy Mike\" nuclear test was a part of long-term project. One of the goals of which was studying the efficiency of production of transuranium elements in high-power nuclear explosions. The motivation for these experiments was that synthesis of such elements from uranium requires multiple neutron capture. The probability of such events increases with the neutron flux, and nuclear explosions are the most powerful man-made neutron sources, providing densities of the order 10 neutrons/cm within a microsecond, or about 10 neutrons/(cm·s). In comparison, the flux of the HFIR reactor is 5 neutrons/(cm·s). A dedicated laboratory was set up right at Enewetak Atoll for preliminary analysis of debris, as some isotopes could have decayed by the time the debris samples reached the mainland U.S. The laboratory was receiving samples for analysis as soon as possible, from airplanes equipped with paper filters which flew over the atoll after the tests. Whereas it was hoped to discover new chemical elements heavier than fermium, none of these were found even after a series of megaton explosions conducted between 1954 and 1956 at the atoll.\n\nThe atmospheric results were supplemented by the underground test data accumulated in the 1960s at the Nevada Test Site, as it was hoped that powerful explosions conducted in confined space might result in improved yields and heavier isotopes. Apart from traditional uranium charges, combinations of uranium with americium and thorium have been tried, as well as a mixed plutonium-neptunium charge, but they were less successful in terms of yield and was attributed to stronger losses of heavy isotopes due to enhanced fission rates in heavy-element charges. Product isolation was problematic as the explosions were spreading debris through melting and vaporizing the surrounding rocks at depths of 300–600 meters. Drilling to such depths to extract the products was both slow and inefficient in terms of collected volumes.\n\nAmong the nine underground tests that were carried between 1962 and 1969, the last one was the most powerful and had the highest yield of transuranium elements. Milligrams of einsteinium that would normally take a year of irradiation in a high-power reactor, were produced within a microsecond. However, the major practical problem of the entire proposal was collecting the radioactive debris dispersed by the powerful blast. Aircraft filters adsorbed only about 4 of the total amount, and collection of tons of corals at Enewetak Atoll increased this fraction by only two orders of magnitude. Extraction of about 500 kilograms of underground rocks 60 days after the Hutch explosion recovered only about 1 of the total charge. The amount of transuranium elements in this 500-kg batch was only 30 times higher than in a 0.4 kg rock picked up 7 days after the test which demonstrated the highly non-linear dependence of the transuranium elements yield on the amount of retrieved radioactive rock. Shafts were drilled at the site before the test in order to accelerate sample collection after explosion, so that explosion would expel radioactive material from the epicenter through the shafts and to collecting volumes near the surface. This method was tried in two tests and instantly provided hundreds kilograms of material, but with actinide concentration 3 times lower than in samples obtained after drilling. Whereas such method could have been efficient in scientific studies of short-lived isotopes, it could not improve the overall collection efficiency of the produced actinides.\n\nAlthough no new elements (apart from einsteinium and fermium) could be detected in the nuclear test debris, and the total yields of transuranium elements were disappointingly low, these tests did provide significantly higher amounts of rare heavy isotopes than previously available in laboratories.\n\nSeparation procedure of einsteinium depends on the synthesis method. In the case of light-ion bombardment inside a cyclotron, the heavy ion target is attached to a thin foil, and the generated einsteinium is simply washed off the foil after the irradiation. However, the produced amounts in such experiments are relatively low. The yields are much higher for reactor irradiation, but there, the product is a mixture of various actinide isotopes, as well as lanthanides produced in the nuclear fission decays. In this case, isolation of einsteinium is a tedious procedure which involves several repeating steps of cation exchange, at elevated temperature and pressure, and chromatography. Separation from berkelium is important, because the most common einsteinium isotope produced in nuclear reactors, Es, decays with a half-life of only 20 days to Bk, which is fast on the timescale of most experiments. Such separation relies on the fact that berkelium easily oxidizes to the solid +4 state and precipitates, whereas other actinides, including einsteinium, remain in their +3 state in solutions.\n\nSeparation of trivalent actinides from lanthanide fission products can be done by a cation-exchange resin column using a 90% water/10% ethanol solution saturated with hydrochloric acid (HCl) as eluant. It is usually followed by anion-exchange chromatography using 6 molar HCl as eluant. A cation-exchange resin column (Dowex-50 exchange column) treated with ammonium salts is then used to separate fractions containing elements 99, 100 and 101. These elements can be then identified simply based on their elution position/time, using α-hydroxyisobutyrate solution (α-HIB), for example, as eluant.\n\nSeparation of the 3+ actinides can also be achieved by solvent extraction chromatography, using bis-(2-ethylhexyl) phosphoric acid (abbreviated as HDEHP) as the stationary organic phase, and nitric acid as the mobile aqueous phase. The actinide elution sequence is reversed from that of the cation-exchange resin column. The einsteinium separated by this method has the advantage to be free of organic complexing agent, as compared to the separation using a resin column.\n\nEinsteinium is highly reactive and therefore strong reducing agents are required to obtain the pure metal from its compounds. This can be achieved by reduction of einsteinium(III) fluoride with metallic lithium:\n\nHowever, owing to its low melting point and high rate of self-radiation damage, einsteinium has high vapor pressure, which is higher than that of lithium fluoride. This makes this reduction reaction rather inefficient. It was tried in the early preparation attempts and quickly abandoned in favor of reduction of einsteinium(III) oxide with lanthanum metal:\n\nEinsteinium(III) oxide (EsO) was obtained by burning einsteinium(III) nitrate. It forms colorless cubic crystals, which were first characterized from microgram samples sized about 30 nanometers. Two other phases, monoclinic and hexagonal, are known for this oxide. The formation of a certain EsO phase depends on the preparation technique and sample history, and there is no clear phase diagram. Interconversions between the three phases can occur spontaneously, as a result of self-irradiation or self-heating. The hexagonal phase is isotypic with lanthanum(III) oxide where the Es ion is surrounded by a 6-coordinated group of O ions.\n\nEinsteinium halides are known for the oxidation states +2 and +3. The most stable state is +3 for all halides from fluoride to iodide.\n\nEinsteinium(III) fluoride (EsF) can be precipitated from einsteinium(III) chloride solutions upon reaction with fluoride ions. An alternative preparation procedure is to exposure einsteinium(III) oxide to chlorine trifluoride (ClF) or F gas at a pressure of 1–2 atmospheres and a temperature between 300 and 400 °C. The EsF crystal structure is hexagonal, as in californium(III) fluoride (CfF) where the Es ions are 8-fold coordinated by fluorine ions in a bicapped trigonal prism arrangement.\n\nEinsteinium(III) chloride (EsCl) can be prepared by annealing einsteinium(III) oxide in the atmosphere of dry hydrogen chloride vapors at about 500 °C for some 20 minutes. It crystallizes upon cooling at about 425 °C into an orange solid with a hexagonal structure of UCl type, where einsteinium atoms are 9-fold coordinated by chlorine atoms in a tricapped trigonal prism geometry. Einsteinium(III) bromide (EsBr) is a pale-yellow solid with a monoclinic structure of AlCl type, where the einsteinium atoms are octahedrally coordinated by bromine (coordination number 6).\n\nThe divalent compounds of einsteinium are obtained by reducing the trivalent halides with hydrogen:\n\nEinsteinium(II) chloride (EsCl), einsteinium(II) bromide (EsBr), and einsteinium(II) iodide (EsI) have been produced and characterized by optical absorption, with no structural information available yet.\n\nKnown oxyhalides of einsteinium include EsOCl, EsOBr and EsOI. They are synthesized by treating a trihalide with a vapor mixture of water and the corresponding hydrogen halide: for example, EsCl + HO/HCl to obtain EsOCl.\n\nThe high radioactivity of einsteinium has a potential use in radiation therapy, and organometallic complexes have been synthesized in order to deliver einsteinium atoms to an appropriate organ in the body. Experiments have been performed on injecting einsteinium citrate (as well as fermium compounds) to dogs. Einsteinium(III) was also incorporated into beta-diketone chelate complexes, since analogous complexes with lanthanides previously showed strongest UV-excited luminescence among metallorganic compounds. When preparing einsteinium complexes, the Es ions were 1000 times diluted with Gd ions. This allowed reducing the radiation damage so that the compounds did not disintegrate during the period of 20 minutes required for the measurements. The resulting luminescence from Es was much too weak to be detected. This was explained by the unfavorable relative energies of the individual constituents of the compound that hindered efficient energy transfer from the chelate matrix to Es ions. Similar conclusion was drawn for other actinides americium, berkelium and fermium.\n\nLuminescence of Es ions was however observed in inorganic hydrochloric acid solutions as well as in organic solution with di(2-ethylhexyl)orthophosphoric acid. It shows a broad peak at about 1064 nanometers (half-width about 100 nm) which can be resonantly excited by green light (ca. 495 nm wavelength). The luminescence has a lifetime of several microseconds and the quantum yield below 0.1%. The relatively high, compared to lanthanides, non-radiative decay rates in Es were associated with the stronger interaction of f-electrons with the inner Es electrons.\n\nThere is almost no use for any isotope of einsteinium outside basic scientific research aiming at production of higher transuranic elements and transactinides.\n\nIn 1955, mendelevium was synthesized by irradiating a target consisting of about 10 atoms of Es in the 60-inch cyclotron at Berkeley Laboratory. The resulting Es(α,n)Md reaction yielded 17 atoms of the new element with the atomic number of 101.\n\nThe rare isotope einsteinium-254 is favored for production of ultraheavy elements because of its large mass, relatively long half-life of 270 days, and availability in significant amounts of several micrograms. Hence einsteinium-254 was used as a target in the attempted synthesis of ununennium (element 119) in 1985 by bombarding it with calcium-48 ions at the superHILAC linear accelerator at Berkeley, California. No atoms were identified, setting an upper limit for the cross section of this reaction at 300 nanobarns.\nEinsteinium-254 was used as the calibration marker in the chemical analysis spectrometer (\"alpha-scattering surface analyzer\") of the Surveyor 5 lunar probe. The large mass of this isotope reduced the spectral overlap between signals from the marker and the studied lighter elements of the lunar surface.\n\nMost of the available einsteinium toxicity data originate from research on animals. Upon ingestion by rats, only about 0.01% einsteinium ends in the blood stream. From there, about 65% goes to the bones, where it remains for about 50 years, 25% to the lungs (biological half-life about 20 years, although this is rendered irrelevant by the short half-lives of einsteinium isotopes), 0.035% to the testicles or 0.01% to the ovaries – where einsteinium stays indefinitely. About 10% of the ingested amount is excreted. The distribution of einsteinium over the bone surfaces is uniform and is similar to that of plutonium.\n\n\n"}
{"id": "52956972", "url": "https://en.wikipedia.org/wiki?curid=52956972", "title": "Electric vehicles in Hong Kong", "text": "Electric vehicles in Hong Kong\n\nElectric vehicles are those using electric motors as their main drive unit, with rechargeable batteries onboard and charged by external source. The benefits of using EVs include low noise pollution, guarantee of energy security, zero roadside emission and high energy efficiency. There are mainly three kinds of EVs in the market: pure electric, petrol/diesel hybrid, and plug-in hybrid electric. The Hong Kong government recognizes all three kinds of EVs to be environmentally friendly and eligible for concessions. As at end of October 2017, there are 11,039 EVs in Hong Kong, up from less than 100 in 2010. At present, 73 EV models from 8 countries have been approved by the Transport Department. These include 51 models for private cars and motorcycles, 22 models for public transport and commercial vehicles.\n\nIn March 1994, the government has waived the First Registration Tax for EVs. It has been extended several times, and the latest one lasts until end of March 2017. However, EVs are yet to be popular in Hong Kong. Since 2009-10 Policy Address, a series of schemes and policies have been implemented by the government to actively promote and encourage the use of EVs. In order to coordinate the work in promoting EVs, the Steering Committee on the Promotion of Electric Vehicles was set up in April 2012. The committee is chaired by the Financial Secretary with 2-year terms, along with 16 other members from professionals or government departments. The ultimate objective of the Government is to have zero emission buses running across the territory. The HK Government allocated $180 million to purchase 36 single-deck electric buses, which includes 28 battery-electric buses and 8 supercapacitor buses. 18 battery-electric buses and 2 supercapacitor buses have commenced operation. Most of the remaining electric buses are expected to be put into service progressively in later 2017 or early 2018.\n\nThe 2014 Hong Kong Emission Inventory Report indicates that the road transport sector is the largest local air pollution source, especially 60% of carbon monoxide emission comes from transportation. Through replacing burning gasoline with using electricity among the EVs, the roadside air quality is improved after reducing carbon emissions from exhausted gases. Besides minimising various pollution problems and lowering the risk of respiratory illnesses, noises and vibrations are also eliminated when no chemical combustion is involved as in the traditional engines. Under the assumption of annual mileage as 20,000 km and consideration over the emissions from generations, the estimated carbon dioxide emitted is around 1,880 kg, 33% and 47% less than that of the hybrid cars and the petrol vehicles respectively.\nThe ultimate objective of the Government is to have zero emission buses running across the territory. To this end, the Government allocated $180 million for franchised bus companies to purchase 36 single-deck electric buses, include 28 battery-electric buses and 8 supercapacitor buses, for trial runs to assess their operational efficiency and performance under the local conditions. 18 battery-electric buses and 2 supercapacitor buses have commenced operation. Most of the remaining electric buses are expected to be put into service progressively in later 2017 or early next year.\n\nEVs have a higher energy efficiency than traditional petrol vehicles, allowing an 80% conversion of input energy into usable power, as a 15%-20% improvement than the latter. With technological advancements, EVs have higher battery capacity that allows 100 km travelling distance after a single charge, far higher than the original 40 km at the early development. By comparing EVs with petrol vehicles over the energy cost, the price of the latter is around 13.6 times higher in the case from Tuen Mun to Mong Kok (30 km; HKD4.7:HKD36.7). The 2011-12 Financial Budget highlighted the value brought by the green development has increased by 12.4% with the 3.6% surge of employees. With the recent emphasis on green economy and smart city in the 2015-16 Financial Budget, the Government believes that the high cost- effectiveness of EVs allows further creation of business and employment opportunities like the complementary accessories of EVs and R&D for further scientific innovations in the ever-expanding market.\n\nCompared to 28 local EVs in 2009, 5,042 registered EVs with 4,899 as private vehicles are recorded for road use as at April 30, 2016. As at end of October 2017, there are 11,039 EVs for road use, up from less than 100 in end 2010. At present, 73 EV models from 8 countries have been type-approved by the Transport Department. These include 51 models for private cars and motorcycles, 22 models for public transport and commercial vehicles. Meanwhile, there is an around 3% annual growth of the new local EVs registrations, which is far higher than other developed automobile markets. From January to May in 2015, more than 1,110 longer-range models of Teslas EVs are sold, accounting for nearly 95% in the EVs market.\n\nOver 1,300 public chargers are spread throughout all 18 local districts, whereas 1 public charger is available for usage by 4 EVs on average. Among them, 921 are standard chargers, 239 are medium chargers, 15 are CHAdeMO quick chargers and 145 are quick chargers with transferrable charging standards (top up the batteries of EVs within 30 minutes when compared to 8 hours previously). The Government and other stakeholders are determined in enhancing the charging efficiency of these public charging infrastructures through various ongoing infrastructure-led collaborations. For example, Charge Hong Kong, a non-profit organization, is working to increase charging facilities in public areas and upgrade the existing public standard chargers to the medium or multi-standard quick type.\n\nUnder Pilot Green Transport Fund, 92 EVs are included for trial under a total subsidy of around $96 million in total. The 2 models of electric vans as interim trial reflected that 50% to 80% of the fuel costs are saved with no negative and uncertain outcomes in terms of their batteries and performance. These reflected that EVs are compatible with local operating conditions with huge green innovative development potentials in the future.\n\nThe government is committed to promoting the use of EVs within different sectors in the society. Hence, a number of favourable measures are adopted to boost the use of EVs.\n\nGiving economic incentives is regarded as a favourable policy that encourages the use of EVs as it lowers the costs. Exemption of the first registration tax on EVs was first introduced in 1997 to promote the use of greener vehicles. The Financial Secretary extended the waiver of the first registration tax on purchasing EVs in 2009 Budget until the end of March 2017. Moreover, enterprises purchasing EVs will enjoy another tax deduction, which is a full “profits tax reduction for the capital expenditure on the EVs in the first year of procurement”.\n\nTo encourage the public transport sector to use EVs, the government offers the industry some financial assistance programs, for example, “Pilot Green Transport Fund”(See the above section). Such a fund targets public transport service providers and attracts them to use innovative and green technologies for trial. Furthermore, other financial assistances are put in place like the one-off grant to PLB owners and the allocation of $180 million HKD for franchised bus runners to purchase electric buses. The former aims to upgrade PLB emission standard to that of EV while the latter serves to test the use of EVs and promote the use of EVs among the community.\n\nWith regard to the insufficient charging facilities and immature charging technologies, not only does the government devote to increasing the number of charging sockets while some of them are free, but also to collaborate with EV manufacturing firms for the advancement of charging facilities. Apart from infrastructure support, the government also issues a set of guidelines on the safe use of EV chargers and provides hotline service for information and technical support.\n\nThe government takes the lead to use EVs by giving priority to replace government vehicles with EVs. It is estimated that over 200 government vehicles are EVs. Not only does the government take the lead to use EVs, it also helps organize an event that arouses the public interest to use EVs, namely the “FIA Formula E Championship”. It is found that the government is determined to provide comprehensive support, ranging from introducing preferential policies to holding promotional activity, in an effort to promote the use of EV in Hong Kong.\n\nAs at April 2016, there are 1320 public chargers in Hong Kong, distributed unequally in different territories, for example, there are nearly 500 public chargers in Hong Kong Island, whereas only less than 400 are located in New Territories and Island Districts, which cover the most area of Hong Kong (see Diagram 1 below). Therefore, the insufficiency of chargers in rural areas is a great concern to some EV drivers. Some drivers opined that it is not ideal to rely on public chargers, in particular when one could hardly find a parking space in Hong Kong. Some even complained that the public parking space for EVs are often occupied by non-EVs.\n\nAnother criticism is the formidability for individuals to install chargers in private car parks. The installation of chargers in private buildings has to undergo an array of procedures. According to the guidelines from the government, individual owners of EVs have to consult the owner of the car park space, the dealer of the EV, the property management company and the registered electrical contractor. Apart from lowering the feasibility of charging the EV at work and at home, this also hinders the incentive of using EVs in the long run.\n\nThere are 4 types of charging sockets (UK, Europe, US and Japan) for EV in Hong Kong, inter alia, only the Japanese socket could serve a fast charge (in 20 minutes). However, 76% of the chargers in Hong Kong are slow chargers which takes 5 to 12 hours for an EV to be fully charged.\n\nThe battery in an EV has to be replaced every three to four years, and the price could amount to 1/3 of the EV itself. Report says that this might place a high recurrent expenditure on EV drivers. It is also uncertain on how would the abandoned batteries be processed.\n\nUsing EV is not necessarily reducing carbon dioxide emission. It depends on the carbon intensity of electricity generation. Coal is used for over half of the power generation in Hong Kong, and research finds that an EV might emit more carbon dioxide than non-EV over 150000-kilometre lifetime after taking into account the carbon intensity of electricity generation and the subsequent production of car batteries.\n\nThe emphasis of Singaporean government’s EVs policy is placed on the cooperation with vehicle manufacturers and technology companies. By launching EV test-bed and forming agreement with Chinese companies to introduce EV into their public transportation, the determination of the government to utilize EVs as environmental policy to combat air pollution is clearly shown. However, surprisingly, as opposed to Hong Kong, the most popular EV model in Singapore is not Tesla. One may suggest the biggest obstacles come from the international oil company, which considers Singapore as the hub of oil refinement and trading center, hence acting as the tripping stone for the government to continuously promote the use of EVs.\n\nThe British government took initiative and launched “green car revolution” programme introducing EVs into the market. This program included a range of incentives, government support, legislation and education programs, together suggesting by 2020 all new cars sold in the UK should be either hybrid or EVs. Apart from large scale subsidy and tax reduction to enhance incentive in purchasing EVs, the government has launched “Plugged-in-Places” scheme to increase vehicle recharging points and convenience, as well as 100% discount for London Congestion Charge. A comprehensive policy planning encouraged people to switch to EVs, resulting in over 20-folds increase of EVs registration in UK from 2011 to 2015.\n\nIn response to progress in promoting EVs, Hong Kong Productivity Council listed out 13 suggested measures for the government to increase its utility rate. The first step the government shall take is to lay out a comprehensive EVs development plan, together with stages and its corresponding goals. The government is also advised to explore possibilities in promoting EVs by improving their chargers, for instance replace the speed of the EVs charger from normal standard to medium/high standard, introduce a central information bank containing all EVs charging station in Hong Kong, as well as adopt commercial mode of running EVs charging station by increasing its coverage in private housing estate and commercial building. Apart from hardware development, the government is urged to devote resources in cultivating technicians and engineers specialized in EVs to prepare for the upcoming demand in human resources. The Environmental Bureau is now organising a competition to invite public to brainstorm some innovative idea of alternative use of disposed EV battery, on one hand engaging the public in policy formation so as supporting the EV battery recycling industry. As evidenced by the world trend, it is believed EV will continue to be in Hong Kong government’s agenda to combat air pollution.\n\n"}
{"id": "4959757", "url": "https://en.wikipedia.org/wiki?curid=4959757", "title": "Energy Policy and Conservation Act", "text": "Energy Policy and Conservation Act\n\nThe Energy Policy and Conservation Act of 1975 (EPCA) () is a United States Act of Congress that responded to the 1973 oil crisis by creating a comprehensive approach to federal energy policy. The primary goals of EPCA are to increase energy production and supply, reduce energy demand, provide energy efficiency, and give the executive branch additional powers to respond to disruptions in energy supply. Most notably, EPCA established the Strategic Petroleum Reserve, the Energy Conservation Program for Consumer Products, and Corporate Average Fuel Economy regulations.\n\nThe need for a national oil storage reserve had been recognized for at least three decades. Secretary of the Interior Harold L. Ickes advocated the stockpiling of emergency crude oil in 1944. President Harry S Truman's Minerals Policy Commission proposed a strategic oil supply in 1952. President Dwight Eisenhower suggested an oil reserve after the 1956 Suez Crisis. The Cabinet Task Force on Oil Import Control recommended a similar reserve in 1970.\n\nBut few events so dramatically underscored the need for a strategic oil reserve as the 1973-74 oil embargo. The cutoff of oil flowing into the United States from OPEC sent economic shockwaves throughout the nation. In the aftermath of the oil crises, the United States established the SPR.\n\nThe EPCA declared it to be U.S. policy to establish a reserve of petroleum, setting the Strategic Petroleum Reserve (SPR) into motion, and extended the Emergency Petroleum Allocation Act of 1973 (EPAA). A number of existing storage sites were acquired in 1977. Construction of the first surface facilities began in June 1977. On July 21, 1977, the first oil—approximately of Saudi Arabian light crude—was delivered to the SPR. Fill was suspended in FY 1995 to devote budget resources to refurbishing the SPR equipment and extending the life of the complex. The current SPR sites are expected to be usable until around 2025. Fill was resumed in 1999.\n\nPart A of Title III of the EPCA established the Corporate Average Fuel Economy standards for automobiles. The average fuel economy for model years, 1978, 1979, and 1980 were set at 18, 19, and 20 miles per gallon, respectively, and by 1985 the average economy was required to be 27.5 mpg Furthermore, automobiles were required to be labeled with their fuel economies, estimated fuel costs, and the range of fuel economy for comparable vehicles after the 1976 model year. The National Highway Traffic Safety Administration was given the authority to regulate fuel economies for automobiles and light trucks.\n\nPart B of Title III of the EPCA established the Energy Conservation Program, which gives the Department of Energy the \"authority to develop, revise, and implement minimum energy conservation standards for appliances and equipment.\" As currently implemented, the Department of Energy currently enforces test procedures and minimum standards for more than 50 products covering residential, commercial and industrial, lighting, and plumbing applications.\n\nThe law has banned crude oil exports, with the U.S. Commerce Department able to grant exceptions for certain types of oil. In 1980, crude oil exports peaked at 104 million barrels, dropping to 43.8 million barrels in 2013. \nThe exceptional export licenses were for oil from Cook Inlet, oil flowing through the Trans-Alaskan Pipeline System, oil exported to Canada, heavy oil from California, certain trades with Mexico, and some exceptions for re-exporting foreign oil.\nWhen oil is processed, e.g. distillation, it can be exported without a license.\nThe export ban has been quoted as a reason why crude oil has cost $10 below the world price from early 2014 throughout 2015, as measured by the West Texas Intermediate benchmark.\n\nOil producing companies and oil producing states, such as Texas, Alaska and North Dakota lobbied to lift the ban.\nOil refineries have been against lifting the export ban, because their raw material, the sweet, light domestic crude was available at a low price.\n\nIn June 2015, the Obama administration had permitted the export of sweet, light oil for the import of heavy, sour oil from Mexico. Environmental groups have opposed lifting the ban because it would mean more oil sales, more drilling and more oil production with all its environmental impacts, increasing emissions of carbon dioxide and other pollutants.\n\nOn December 18, 2015 Congress lifted the 41 year old ban. Republicans favored lifting the ban and in return agreed to not block a $500m payment to the UN Green Climate Fund and tax breaks for solar and wind power.\n\nThe EPCA contained several policies to encourage the production of domestic energy sources. It authorized a program to promote coal production that would guarantee qualifying underground coal mining operations up to $30 million per project. The qualifying requirements are tailored to promote more environmentally friendly development and smaller coal producers. Recipients of the loan guarantees are required to have a contract with a customer who is certified by the Environmental Protection Agency to operate their plant in compliance with the Clean Air Act. At least 80% of the total guarantee amount must finance low-sulfur coal development. Finally, large coal or oil companies are prohibited from receiving loan guarantees.\n\nComplementary to the increased coal production goals of the legislation, the EPCA also provided mechanisms to allow the government to ensure that natural gas and petroleum based fuels are available to consumers in times of fuel shortages or crises. The Federal Energy Administration's authority to require power plants to burn coal instead of natural gas or petroleum based fuels was extended through 1977. This mechanism would reduce the use of these fuels for power generation and free them for use by other consumers. Furthermore, the President was given authority to order maximum domestic oil and gas production, and the President was directed to submit plans for energy conservation and energy rationing in case of a fuel shortage.\n\n"}
{"id": "37555", "url": "https://en.wikipedia.org/wiki?curid=37555", "title": "Enriched uranium", "text": "Enriched uranium\n\nEnriched uranium is a type of uranium in which the percent composition of uranium-235 has been increased through the process of isotope separation. Natural uranium is 99.284% U isotope, with U only constituting about 0.711% of its mass. U is the only nuclide existing in nature (in any appreciable amount) that is fissile with thermal neutrons.\n\nEnriched uranium is a critical component for both civil nuclear power generation and military nuclear weapons. The International Atomic Energy Agency attempts to monitor and control enriched uranium supplies and processes in its efforts to ensure nuclear power generation safety and curb nuclear weapons proliferation.\n\nDuring the Manhattan Project enriched uranium was given the codename oralloy, a shortened version of Oak Ridge alloy, after the location of the plants where the uranium was enriched. The term oralloy is still occasionally used to refer to enriched uranium. There are about 2,000 tonnes (t, Mg) of highly enriched uranium in the world, produced mostly for nuclear power, nuclear weapons, naval propulsion, and smaller quantities for research reactors.\n\nThe U remaining after enrichment is known as depleted uranium (DU), and is considerably less radioactive than even natural uranium, though still very dense and extremely hazardous in granulated form – such granules are a natural by-product of the shearing action that makes it useful for armor-penetrating weapons and radiation shielding. At present, 95 percent of the world's stocks of depleted uranium remain in secure storage.\n\nUranium as it is taken directly from the Earth is not suitable as fuel for most nuclear reactors and requires additional processes to make it usable. Uranium is mined either underground or in an open pit depending on the depth at which it is found. After the uranium ore is mined, it must go through a milling process to extract the uranium from the ore. This is accomplished by a combination of chemical processes with the end product being concentrated uranium oxide, which is known as \"yellowcake\", contains roughly 60% uranium whereas the ore typically contains less than 1% uranium and as little as 0.1% uranium (Henderson 2000). After the milling process is complete, the uranium must next undergo a process of conversion, \"to either uranium dioxide, which can be used as the fuel for those types of reactors that do not require enriched uranium, or into uranium hexafluoride, which can be enriched to produce fuel for the majority of types of reactors\". Naturally-occurring uranium is made of a mixture of U-235 and U-238. The U-235 is fissile meaning it is easily split with neutrons while the remainder is U-238, but in nature, more than 99% of the extracted ore is U-238. Most nuclear reactors require enriched uranium, which is uranium with higher concentrations of U-235 ranging between 3.5% and 4.5%. There are two commercial enrichment processes: gaseous diffusion and gas centrifugation. Both enrichment processes involve the use of uranium hexafluoride and produce enriched uranium oxide.\n\n\"Reprocessed uranium\" (RepU) is a product of nuclear fuel cycles involving nuclear reprocessing of spent fuel. RepU recovered from light water reactor (LWR) spent fuel typically contains slightly more U-235 than natural uranium, and therefore could be used to fuel reactors that customarily use natural uranium as fuel, such as CANDU reactors. It also contains the undesirable isotope uranium-236, which undergoes neutron capture, wasting neutrons (and requiring higher U-235 enrichment) and creating neptunium-237, which would be one of the more mobile and troublesome radionuclides in deep geological repository disposal of nuclear waste.\n\n\"Low enriched uranium\" (LEU) has a lower than 20% concentration of U; for instance, in commercial light water reactors (LWR), the most prevalent power reactors in the world, uranium is enriched to 3 to 5% U. Fresh LEU used in research reactors is usually enriched 12% to 19.75% U-235, the latter concentration being used to replace HEU fuels when converting to LEU.\n\n\"Highly enriched uranium\" (HEU) has a 20% or higher concentration of U. The fissile uranium in nuclear weapon primaries usually contains 85% or more of U known as weapons-grade, though theoretically for an implosion design, a minimum of 20% could be sufficient (called weapon(s)-usable) although it would require hundreds of kilograms of material and \"would not be practical to design\"; even lower enrichment is hypothetically possible, but as the enrichment percentage decreases the critical mass for unmoderated fast neutrons rapidly increases, with for example, an infinite mass of 5.4% U being required. For criticality experiments, enrichment of uranium to over 97% has been accomplished.\n\nThe very first uranium bomb, Little Boy, dropped by the United States on Hiroshima in 1945, used 64 kilograms of 80% enriched uranium. Wrapping the weapon's fissile core in a neutron reflector (which is standard on all nuclear explosives) can dramatically reduce the critical mass. Because the core was surrounded by a good neutron reflector, at explosion it comprised almost 2.5 critical masses. Neutron reflectors, compressing the fissile core via implosion, fusion boosting, and \"tamping\", which slows the expansion of the fissioning core with inertia, allow nuclear weapon designs that use less than what would be one bare-sphere critical mass at normal density. The presence of too much of the U isotope inhibits the runaway nuclear chain reaction that is responsible for the weapon's power. The critical mass for 85% highly enriched uranium is about , which at normal density would be a sphere about in diameter.\n\nLater US nuclear weapons usually use plutonium-239 in the primary stage, but the jacket or tamper secondary stage, which is compressed by the primary nuclear explosion often uses HEU with enrichment between 40% and 80%\nalong with the fusion fuel lithium deuteride. For the secondary of a large nuclear weapon, the higher critical mass of less-enriched uranium can be an advantage as it allows the core at explosion time to contain a larger amount of fuel. The U is not fissile but still fissionable by fusion neutrons.\n\nHEU is also used in fast neutron reactors, whose cores require about 20% or more of fissile material, as well as in naval reactors, where it often contains at least 50% U, but typically does not exceed 90%. The Fermi-1 commercial fast reactor prototype used HEU with 26.5% U. Significant quantities of HEU are used in the production of medical isotopes, for example molybdenum-99 for technetium-99m generators.\n\nIsotope separation is difficult because two isotopes of the same element have very nearly identical chemical properties, and can only be separated gradually using small mass differences. (U is only 1.26% lighter than U.) This problem is compounded by the fact that uranium is rarely separated in its atomic form, but instead as a compound (UF is only 0.852% lighter than UF.)\nA cascade of identical stages produces successively higher concentrations of U. Each stage passes a slightly more concentrated product to the next stage and returns a slightly less concentrated residue to the previous stage.\n\nThere are currently two generic commercial methods employed internationally for enrichment: gaseous diffusion (referred to as \"first\" generation) and gas centrifuge (\"second\" generation), which consumes only 2% to 2.5% as much energy as gaseous diffusion (at least a \"factor of 20\" more efficient). Some work is being done that would use nuclear resonance; however there is no reliable evidence that any nuclear resonance processes have been scaled up to production.\n\nGaseous diffusion is a technology used to produce enriched uranium by forcing gaseous uranium hexafluoride (\"hex\") through semi-permeable membranes. This produces a slight separation between the molecules containing U and U. Throughout the Cold War, gaseous diffusion played a major role as a uranium enrichment technique, and as of 2008 accounted for about 33% of enriched uranium production, but in 2011 was deemed an obsolete technology that is steadily being replaced by the later generations of technology as the diffusion plants reach their ends-of-life. In 2013, the Paducah facility in the US ceased operating, it was the last commercial U gaseous diffusion plant in the world.\n\nThermal diffusion utilizes the transfer of heat across a thin liquid or gas to accomplish isotope separation. The process exploits the fact that the lighter U gas molecules will diffuse toward a hot surface, and the heavier U gas molecules will diffuse toward a cold surface. The S-50 plant at Oak Ridge, Tennessee was used during World War II to prepare feed material for the EMIS process. It was abandoned in favor of gaseous diffusion.\n\nThe gas centrifuge process uses a large number of rotating cylinders in series and parallel formations. Each cylinder's rotation creates a strong centripetal force so that the heavier gas molecules containing U move tangentially toward the outside of the cylinder and the lighter gas molecules rich in U collect closer to the center. It requires much less energy to achieve the same separation than the older gaseous diffusion process, which it has largely replaced and so is the current method of choice and is termed \"second generation\". It has a separation factor per stage of 1.3 relative to gaseous diffusion of 1.005, which translates to about one-fiftieth of the energy requirements. Gas centrifuge techniques produce close to 100% of the world's enriched uranium.\n\nThe Zippe centrifuge is an improvement on the standard gas centrifuge, the primary difference being the use of heat. The bottom of the rotating cylinder is heated, producing convection currents that move the U up the cylinder, where it can be collected by scoops. This improved centrifuge design is used commercially by Urenco to produce nuclear fuel and was used by Pakistan in their nuclear weapons program.\n\nLaser processes promise lower energy inputs, lower capital costs and lower tails assays, hence significant economic advantages. Several laser processes have been investigated or are under development. Separation of Isotopes by Laser Excitation (SILEX) is well advanced and licensed for commercial operation in 2012.\n\n\"Atomic vapor laser isotope separation\" employs specially tuned lasers to separate isotopes of uranium using selective ionization of hyperfine transitions. The technique uses lasers tuned to frequencies that ionize U atoms and no others. The positively charged U ions are then attracted to a negatively charged plate and collected.\n\n\"Molecular laser isotope separation\" uses an infrared laser directed at UF, exciting molecules that contain a U atom. A second laser frees a fluorine atom, leaving uranium pentafluoride, which then precipitates out of the gas.\n\n\"Separation of isotopes by laser excitation\" is an Australian development that also uses UF. After a protracted development process involving U.S. enrichment company USEC acquiring and then relinquishing commercialization rights to the technology, GE Hitachi Nuclear Energy (GEH) signed a commercialization agreement with Silex Systems in 2006. GEH has since built a demonstration test loop and announced plans to build an initial commercial facility. Details of the process are classified and restricted by intergovernmental agreements between United States, Australia, and the commercial entities. SILEX has been projected to be an order of magnitude more efficient than existing production techniques but again, the exact figure is classified. In August, 2011 Global Laser Enrichment, a subsidiary of GEH, applied to the U.S. Nuclear Regulatory Commission (NRC) for a permit to build a commercial plant. In September 2012, the NRC issued a license for GEH to build and operate a commercial SILEX enrichment plant, although the company had not yet decided whether the project would be profitable enough to begin construction, and despite concerns that the technology could contribute to nuclear proliferation.\n\nAerodynamic enrichment processes include the Becker jet nozzle techniques developed by E. W. Becker and associates using the LIGA process and the vortex tube separation process. These aerodynamic separation processes depend upon diffusion driven by pressure gradients, as does the gas centrifuge. They in general have the disadvantage of requiring complex systems of cascading of individual separating elements to minimize energy consumption. In effect, aerodynamic processes can be considered as non-rotating centrifuges. Enhancement of the centrifugal forces is achieved by dilution of UF with hydrogen or helium as a carrier gas achieving a much higher flow velocity for the gas than could be obtained using pure uranium hexafluoride. The Uranium Enrichment Corporation of South Africa (UCOR) developed and deployed the continuous Helikon vortex separation cascade for high production rate low enrichment and the substantially different semi-batch Pelsakon low production rate high enrichment cascade both using a particular vortex tube separator design, and both embodied in industrial plant. A demonstration plant was built in Brazil by NUCLEI, a consortium led by Industrias Nucleares do Brasil that used the separation nozzle process. However all methods have high energy consumption and substantial requirements for removal of waste heat; none are currently still in use.\n\nIn the electromagnetic isotope separation process (EMIS), metallic uranium is first vaporized, and then ionized to positively charged ions. The cations are then accelerated and subsequently deflected by magnetic fields onto their respective collection targets. A production-scale mass spectrometer named the Calutron was developed during World War II that provided some of the U used for the Little Boy nuclear bomb, which was dropped over Hiroshima in 1945. Properly the term 'Calutron' applies to a multistage device arranged in a large oval around a powerful electromagnet. Electromagnetic isotope separation has been largely abandoned in favour of more effective methods.\n\nOne chemical process has been demonstrated to pilot plant stage but not used for production. The French CHEMEX process exploited a very slight difference in the two isotopes' propensity to change valency in oxidation/reduction, utilising immiscible aqueous and organic phases. An ion-exchange process was developed by the Asahi Chemical Company in Japan that applies similar chemistry but effects separation on a proprietary resin ion-exchange column.\n\nPlasma separation process (PSP) describes a technique that makes use of superconducting magnets and plasma physics. In this process, the principle of ion cyclotron resonance is used to selectively energize the U isotope in a plasma containing a mix of ions. The French developed their own version of PSP, which they called RCI. Funding for RCI was drastically reduced in 1986, and the program was suspended around 1990, although RCI is still used for stable isotope separation.\n\n\"Separative work\" – the amount of separation done by an enrichment process – is a function of the concentrations of the feedstock, the enriched output, and the depleted tailings; and is expressed in units that are so calculated as to be proportional to the total input (energy / machine operation time) and to the mass processed. Separative work is \"not\" energy. The same amount of separative work will require different amounts of energy depending on the efficiency of the separation technology. Separative work is measured in \"Separative work units\" SWU, kg SW, or kg UTA (from the German \"Urantrennarbeit\" – literally \"uranium separation work\")\n\nIn addition to the separative work units provided by an enrichment facility, the other important parameter to be considered is the mass of natural uranium (NU) that is needed to yield a desired mass of enriched uranium. As with the number of SWUs, the amount of feed material required will also depend on the level of enrichment desired and upon the amount of U that ends up in the depleted uranium. However, unlike the number of SWUs required during enrichment, which increases with decreasing levels of U in the depleted stream, the amount of NU needed will decrease with decreasing levels of U that end up in the DU.\n\nFor example, in the enrichment of LEU for use in a light water reactor it is typical for the enriched stream to contain 3.6% U (as compared to 0.7% in NU) while the depleted stream contains 0.2% to 0.3% U. In order to produce one kilogram of this LEU it would require approximately 8 kilograms of NU and 4.5 SWU if the DU stream was allowed to have 0.3% U. On the other hand, if the depleted stream had only 0.2% U, then it would require just 6.7 kilograms of NU, but nearly 5.7 SWU of enrichment. Because the amount of NU required and the number of SWUs required during enrichment change in opposite directions, if NU is cheap and enrichment services are more expensive, then the operators will typically choose to allow more U to be left in the DU stream whereas if NU is more expensive and enrichment is less so, then they would choose the opposite.\n\nThe opposite of enriching is downblending; surplus HEU can be downblended to LEU to make it suitable for use in commercial nuclear fuel.\n\nThe HEU feedstock can contain unwanted uranium isotopes: U is a minor isotope contained in natural uranium; during the enrichment process, its concentration increases but remains well below 1%. High concentrations of U are a byproduct from irradiation in a reactor and may be contained in the HEU, depending on its manufacturing history. HEU reprocessed from nuclear weapons material production reactors (with an U assay of approx. 50%) may contain U concentrations as high as 25%, resulting in concentrations of approximately 1.5% in the blended LEU product. U is a neutron poison; therefore the actual U concentration in the LEU product must be raised accordingly to compensate for the presence of U.\n\nThe blendstock can be NU, or DU, however depending on feedstock quality, SEU at typically 1.5 wt% U may used as a blendstock to dilute the unwanted byproducts that may be contained in the HEU feed. Concentrations of these isotopes in the LEU product in some cases could exceed ASTM specifications for nuclear fuel, if NU, or DU were used. So, the HEU downblending generally cannot contribute to the waste management problem posed by the existing large stockpiles of depleted uranium.\n\nA major downblending undertaking called the Megatons to Megawatts Program converts ex-Soviet weapons-grade HEU to fuel for U.S. commercial power reactors. From 1995 through mid-2005, 250 tonnes of high-enriched uranium (enough for 10,000 warheads) was recycled into low-enriched-uranium. The goal is to recycle 500 tonnes by 2013. The decommissioning programme of Russian nuclear warheads accounted for about 13% of total world requirement for enriched uranium leading up to 2008.\n\nThe United States Enrichment Corporation has been involved in the disposition of a portion of the 174.3 tonnes of highly enriched uranium (HEU) that the U.S. government declared as surplus military material in 1996. Through the U.S. HEU Downblending Program, this HEU material, taken primarily from dismantled U.S. nuclear warheads, was recycled into low-enriched uranium (LEU) fuel, used by nuclear power plants to generate electricity.\n\nThe following countries are known to operate enrichment facilities: Argentina, Brazil, China, France, Germany, India, Iran, Japan, the Netherlands, North Korea, Pakistan, Russia, the United Kingdom, and the United States. Belgium, Iran, Italy, and Spain hold an investment interest in the French Eurodif enrichment plant, with Iran's holding entitling it to 10% of the enriched uranium output. Countries that had enrichment programs in the past include Libya and South Africa, although Libya's facility was never operational. Australia has developed a laser enrichment process known as SILEX, which it intends to pursue through financial investment in a U.S. commercial venture by General Electric. It has also been claimed that Israel has a uranium enrichment program housed at the Negev Nuclear Research Center site near Dimona.\n\n\n"}
{"id": "22891389", "url": "https://en.wikipedia.org/wiki?curid=22891389", "title": "Ferromagnetic material properties", "text": "Ferromagnetic material properties\n\nThe article Ferromagnetic material properties is intended to contain a glossary of terms used to describe (mainly quantitatively) ferromagnetic materials, and magnetic cores.\n\n\nTo describe a soft ferromagnetic material for technical use, the following parameters are specified:\n\n\n\nThese parameters used e.g. in Philips' handbook and Magnetic Materials Producers Association \"Soft Ferrites, A Users Guide\".\n\nNote: possibly there are some errors - need verification from another source; see discussion page.\n"}
{"id": "14161069", "url": "https://en.wikipedia.org/wiki?curid=14161069", "title": "Gallium manganese arsenide", "text": "Gallium manganese arsenide\n\nGallium manganese arsenide is a magnetic semiconductor. It is based on the world's second favorite semiconductor, GaAs, and as such is readily compatible with existing semiconductor technologies. Differently from other dilute magnetic semiconductors (DMSs), such as the majority of those based on II-VI semiconductors, it is not paramagnetic\nbut ferromagnetic, and hence exhibits hysteretic magnetization behavior. This memory effect is of importance for the creation of persistent devices. In (Ga,Mn)As, the manganese atoms provide a magnetic moment, and each also acts as an acceptor, making it a \"p\"-type material. The presence of carriers allows the material to be used for spin-polarized currents. In contrast, many other ferromagnetic DMSs are strongly insulating\nand so do not possess free carriers. (Ga,Mn)As is therefore a candidate as a spintronic material.\n\nLike other DMSs, (Ga,Mn)As is formed by doping a standard semiconductor with magnetic elements. This is done using the growth technique molecular beam epitaxy (MBE), whereby crystal structures can be grown with atom layer precision. In (Ga,Mn)As the manganese substitute into gallium sites in the GaAs crystal and provide a magnetic moment. Because manganese has a low solubility in GaAs, incorporating a sufficiently high concentration for ferromagnetism to be achieved proves challenging. In standard MBE growth, to ensure that a good structural quality is obtained, the temperature the substrate is heated to, known as the growth temperature, is normally high, typically ~600 °C. However, if a large flux of manganese is used in these conditions, instead of being incorporated, segregation occurs where the manganese accumulate on the surface and form complexes with elemental arsenic atoms.\nThis problem was overcome using the technique of low-temperature MBE. It was found, first in (In,Mn)As\nand then later used for (Ga,Mn)As,\nthat by utilising non-equilibrium crystal growth techniques larger dopant concentrations could be successfully incorporated. At lower temperatures, around 250 °C, there is insufficient thermal energy for surface segregation to occur but still sufficient for a good quality single crystal alloy to form.\n\nIn addition to the substitutional incorporation of manganese, low-temperature MBE also causes the inclusion of other impurities. The two other common impurities are interstitial manganese\nand arsenic antisites.\nThe former is where the manganese atom sits between the other atoms in the zinc-blende lattice structure and the latter is where an arsenic atom occupies a gallium site. Both impurities act as double donors, removing the holes provided by the substitutional manganese, and as such they are known as compensating defects. The interstitial manganese also bond antiferromagnetically to substitutional manganese, removing the magnetic moment. Both these defects are detrimental to the ferromagnetic properties of the (Ga,Mn)As, and so are undesired.\n\nThe temperature below which the transition from paramagnetism to ferromagnetism occurs is known as the Curie temperature, \"T\". Theoretical predictions based on the Zener model suggest that the Curie temperature scales with the quantity of manganese, so \"T\" above 300 K is possible if manganese doping levels as high as 10% can be achieved.\nAfter its discovery by Ohno \"et al.\", the highest reported Curie temperatures in (Ga,Mn)As rose from 60 K to 110 K. However, despite the predictions of room-temperature ferromagnetism, no improvements in \"T\" were made for several years.\n\nAs a result of this lack of progress, predictions started to be made that 110 K was in fact a fundamental limit for (Ga,Mn)As. The self-compensating nature of the defects would limit the possible hole concentrations, preventing further gains in \"T\".\nThe major breakthrough came from improvements in post-growth annealing. By using annealing temperatures comparable to the growth temperature it was possible to pass the 110 K barrier.\nThese improvements have been attributed to the removal of the highly mobile interstitial manganese.\n\nCurrently, the highest reported values of \"T\" in (Ga,Mn)As are around 173 K,\nstill well below the much sought room-temperature. As a result, measurements on this material must be done at cryogenic temperatures, currently precluding any application outside of the laboratory. Naturally, considerable effort is being spent in the search for an alternative DMS that does not share this limitation.\nIn addition to this, as MBE techniques and equipment are refined and improved it is hoped that greater control over growth conditions will allow further incremental advances in the Curie temperature of (Ga,Mn)As.\n\nRegardless of the fact that room-temperature ferromagnetism has not yet been achieved, DMS materials such as (Ga,Mn)As, have shown considerable success. Thanks to the rich interplay of physics inherent to DMSs a variety of novel phenomena and device structures have been demonstrated. It is therefore instructive to make a critical review of these main developments.\n\nA key result in DMS technology is gateable ferromagnetism, where an electric field is used to control the ferromagnetic properties. This was achieved by Ohno \"et al.\"\nusing an insulating-gate field-effect transistor with (In,Mn)As as the magnetic channel. The magnetic properties were inferred from magnetization dependent Hall measurements of the channel. Using the gate action to either deplete or accumulate holes in the channel it was possible to change the characteristic of the Hall response to be either that of a paramagnet or of a ferromagnet. When the temperature of the sample was close to its \"T\" it was possible to turn the ferromagnetism on or off by applying a gate voltage which could change the \"T\" by ±1 K.\n\nA similar (In,Mn)As transistor device was used to provide further examples of gateable ferromagnetism.\nIn this experiment the electric field was used to modify the coercive field at which magnetization reversal occurs. As a result of the dependence of the magnetic hysteresis on the gate bias the electric field could be used to assist magnetization reversal or even demagnetize the ferromagnetic material.\nThe combining of magnetic and electronic functionality demonstrated by this experiment is one of the goals of spintronics and may be expected to have a great technological impact.\n\nAnother important spintronic functionality that has been demonstrated in DMSs is that of spin injection. This is where the high spin polarization inherent to these magnetic materials is used to transfer spin polarized carriers into a non-magnetic material.\nIn this example, a fully epitaxial heterostructure was used where spin polarized holes were injected from a (Ga,Mn)As layer to an (In,Ga)As quantum well where they combine with unpolarized electrons from an \"n\"-type substrate. A polarization of 8% was measured in the resulting electroluminescence. This is again of potential technological interest as it shows the possibility that the spin states in non-magnetic semiconductors can be manipulated without the application of a magnetic field.\n\n(Ga,Mn)As offers an excellent material to study domain wall mechanics because the domains can have a size of the order of 100 µm.\nSeveral studies have been done in which lithographically defined lateral constrictions\nor other pinning points\nare used to manipulate domain walls. These experiments are crucial to understanding domain wall nucleation and propagation which would be necessary for the creation of complex logic circuits based on domain wall mechanics.\nMany properties of domain walls are still not fully understood and one particularly outstanding issue is of the magnitude and size of the resistance associated with current passing through domain walls. Both positive\nand negative\nvalues of domain wall resistance have been reported, leaving this an open area for future research.\n\nAn example of a simple device that utilizes pinned domain walls is provided by reference.\nThis experiment consisted of a lithographically defined narrow island connected to the leads via a pair of nanoconstrictions. While the device operated in a diffusive regime the constrictions would pin domain walls, resulting in a giant magnetoresistance (GMR) signal. When the device operates in a tunnelling regime another magnetoresistance (MR) effect is observed, discussed below.\n\nA furtherproperty of domain walls is that of current induced domain wall motion. This reversal is believed to occur as a result of the spin-transfer torque exerted by a spin polarized current.\nIt was demonstrated in reference\nusing a lateral (Ga,Mn)As device containing three regions which had been patterned to have different coercive fields, allowing the easy formation of a domain wall. The central region was designed to have the lowest coercivity so that the application of current pulses could cause the orientation of the magnetization to be switched. This experiment showed that the current required to achieve this reversal in (Ga,Mn)As was two orders of magnitude lower than that of metal systems. It has also been demonstrated that current-induced magnetization reversal can occur across a (Ga,Mn)As/GaAs/(Ga,Mn)As vertical tunnel junction.\n\nAnother novel spintronic effect, which was first observed in (Ga,Mn)As based tunnel devices, is tunnelling anisotropic magnetoresistance (TAMR). This effect arises from the intricate dependence of the tunnelling density of states on the magnetization, and can result in MRs of several orders of magnitude. This was demonstrated first in vertical tunnelling structures\nand then later in lateral devices.\nThis has established TAMR as a generic property of ferromagmetic tunnel structures. Similarly, the dependence of the single electron charging energy on the magnetization has resulted in the obersvation of another dramatic MR effect in a (Ga,Mn)As device, the so-called Coulomb blockade anisotropic magnetoresistance (CBAMR).\n\nThere are many excellent review articles about the properties and applications of DMSs and (Ga,Mn)As in particular. If further information is required on the topic, several reviews are recommended:\n"}
{"id": "867011", "url": "https://en.wikipedia.org/wiki?curid=867011", "title": "Groundhog Day gale of 1976", "text": "Groundhog Day gale of 1976\n\nThe Groundhog Day gale was a severe winter storm that hit the northeastern United States and southeastern Canada on February 2 (Groundhog Day), 1976.\n\nAn upper-level low was stationary across the Desert Southwest of the United States, on January 28. A system in the northern branch of the Westerlies known as a Saskatchewan Screamer, similar to an Alberta clipper but originating as a frontal wave in the next Canadian province to the east, moved east-southeast across Canada beginning on January 30, luring the system in the United States eastward. The cyclones merged by February 2, becoming a significant storm over New England before lifting northward through Quebec into the Davis Strait. At this time, maximum sustained winds reached 164 kilometers per hour (102 mph) in coastal areas (equal to a Category 2 hurricane on the Saffir-Simpson hurricane scale), with wind gusts of up to 188 kilometers per hour (116 mph). By February 6, this extratropical cyclone was absorbed by another system in the northern Canadian archipelago.\n\nCaribou, Maine, recorded one of its lowest pressures on record, with much of New England recording its lowest values for the month of February,with a reading of . Winds gusted to 60 knots (69 mph) in Rockland and 100 knots (115 mph) at Southwest Harbor. Blizzard conditions were experienced for a few hours as the cyclone moved up into Canada. The storm caused extensive damage in many areas. Although many trees were blown down by the storm, many more were killed after large amounts of seawater were blown inland. Coastal flooding was seen from Brunswick to Eastport. A tidal surge went up the Penobscot River, flooding Bangor, Maine, for three hours around midday. At 11:15 am, waters began rising on the river and within 15 minutes had risen a total of flooding downtown. About 200 cars were submerged and office workers were stranded until waters receded. There were no reported deaths during this unusual flash flood.\n\nBoston, Massachusetts, set their lowest February pressure on record, with a reading of .\n\nCyclonic flow and cold air around the backside of this system led to significant Lake effect snows for areas downwind of the Great Lakes.\n\nBurlington set a daily snow record on February 2 when 6.5 inches fell.\n\nSignificant damage occurred in southern New Brunswick, especially to the city of Saint John. Saint John experienced winds of 188 km/h (116 mph). Southwest Nova Scotia and southern New Brunswick experienced coastal flooding of up to deep causing extensive damage to wharves, coastal buildings, boats and vessels. Power and communications lines were also knocked out. The tides along the coast were increased due to the convergence of anomalistic, synodical, and tropical monthly tidal cycles peaking simultaneously (known as Saros); a once in 18-year event. Damage was estimated in the tens of millions of dollars. Offshore New Brunswick, 12-m (39 ft) waves with swells of were reported in the high seas. The lighthouse at Fish Fluke Point was wrecked and subsequently abandoned. The aftermath of this storm was worsened by a severe cold snap that followed the day after.\n\n"}
{"id": "21871219", "url": "https://en.wikipedia.org/wiki?curid=21871219", "title": "HYDROSOL", "text": "HYDROSOL\n\nHYDROSOL (short for Solar hydrogen via water splitting in advanced monolithic reactors for future solar power plants) is a series of European Union funded projects for the promotion of renewable energy. Its aim is the production of hydrogen using concentrated solar power with a specific thermochemical cycle.\n\nThe Fifth Framework Programme for Research and Technological Development (FP5) project HYDROSOL started in December 2002 with a budget of €2.6 million. A pilot-scale solar reactor was designed, built and operated at the German Aerospace Center with a solar furnace facility in Cologne (Germany), continuously producing \"solar hydrogen\".\n\nThe FP6 HYDROSOL II is a pilot reactor of 100 kW scale for solar thermochemical hydrogen production at the Plataforma Solar de Almería in Spain, which started in November 2005 and has been in operation since 2008.\n\nThe FP7 HYDROSOL-3D project, started on January 1, 2010 and ran until January 1, 2013. The Hydrosol series projects were conceived and coordinated by the Aerosol and Particle Technology Laboratory of the Centre for Research and Technology-Hellas and Ciemat. In 2006, the Hydrosol project was awarded the Descartes Prize by the European Commission for Collaborative Scientific Research.\n\nIn early 2017, the Synlight project at the German Aerospace Centre (DLR) created an artificial sun in the lab. In an effort to better optimise solar hydrogen production at scale, the electrically powered 'sun' is able to provide focussed temperatures approaching 3,000°C, far above the temperatures currently reached by commercial concentrated solar power stations.\n\n\n"}
{"id": "13873019", "url": "https://en.wikipedia.org/wiki?curid=13873019", "title": "Household hazardous waste", "text": "Household hazardous waste\n\nHousehold hazardous waste (HHW), sometimes called retail hazardous waste or \"home generated special materials', is post-consumer waste which qualifies as hazardous waste when discarded. It includes household chemicals and other substances for which the owner no longer has a use, such as consumer products sold for home care, personal care, automotive care, pest control and other purposes. These products exhibit many of the same dangerous characteristics as fully regulated hazardous waste due to their potential for reactivity, ignitability, corrosivity, toxicity, or persistence. Examples include drain cleaners, oil paint, motor oil, antifreeze, fuel, poisons, pesticides, herbicides and rodenticides, fluorescent lamps, lamp ballasts, smoke detectors, medical waste, some types of cleaning chemicals, and consumer electronics (such as televisions, computers, and cell phones).\n\nCertain items such as batteries and fluorescent lamps can be returned to retail stores for disposal. The Rechargeable Battery Recycling Corporation (RBRC) maintains a list of battery recycling locations and your local environmental organization should have list of fluorescent lamp recycling locations. The classification \"household hazardous waste\" has been used for decades and does not accurately reflect the larger group of materials that during the past several years have become known as \"household hazardous wastes\". These include items such as latex paint, non-hazardous household products and other items that do not generally exhibit hazardous characteristics which are routinely included in \"household hazardous waste\" disposal programs. The term \"home generated special materials\" more accurately identifies a broader range of items that public agencies are targeting as recyclable and/or should not be disposed of into a landfill.\n\nHHW is not regulated by the EPA. Many states and local solid waste management departments have created and funded Household Hazardous Waste collection programs to offer safe disposal options. These programs may include home collection service, permanent facilities and one day collection events. \n\nAlthough most U.S. states and federal regulations continue to permit homeowner disposal of some household hazardous waste into the solid waste stream, state agencies are becoming more stringent in enforcing existing hazardous waste regulations at the retail level.\n\nThe most extensive overview of this topic including history, policy and technical issues is contained in the 2008 book Handbook on Household Hazardous Waste, Amy Cabaniss, Editor. An additional HHW overview resource is in Chapter 10 of the Handbook of Solid Waste Management, George Tchobanoglous and Frank Kreith, Editors.\n\nThe professional organization most focused on HHW issues is the North American Hazardous Materials Management Association, NAHMMA. NAHMMA has chapters in many states, holds an annual conference, provides training and offers professional publications. In collaboration with the Solid Waste Association of North America (SWANA) NAHMMA offers certification to HHW collection professionals.\n\nIn Florida, and in other United States states, responsibility for proper disposal of retail hazardous waste falls upon the generator. Depending on the state, \"proper disposal\" either encourage, or in some states require, that small business hazardous waste not be disposed of through the solid waste stream, but must only be transported in accordance with DOTlicensed carriers and EPA (RCRA) regulations to a properly permitted hazardous waste TSDRF (Treatment Storage Disposal and/or Recycling Facility).\n\nSome states allow collection of small business hazardous wastes at the same location as household hazardous wastes. However, it is more common for public collection facilities to limit hazardous waste collection to households. In 1992 the US EPA issued a policy that allowed states the option to collect and mix household hazardous wastes with conditionally exempt hazardous wastes from small businesses. This has encouraged a trend of local collection programs evolving from household hazardous waste only to also include small business hazardous waste collection.\n\nCalifornia has introduced an Electronic Waste Recycling Act. While most states recognize the exemption for home generated hazardous waste in 40 CFR, California has established Section 25218 of the Health and Safety Code to regulate all aspects of home generated special materials (HHW). 25218 details the types of programs e.g. Door-to-Door, Permanent HHWCF, Curbside, Mobile etc. Public agencies must sponsor (as the generator) all HHW programs as their EPA ID number is used. All HHW programs are monitored by DTSC and/or the local CUPA organization. A Permit-by-Rule must be obtained from DTSC or the CUPA before implementing most HHW collection activities.\n\nPennsylvania has introduced the Covered Device Recycling Act.\n\nSimilar regulations, such as the Waste Electrical and Electronic Equipment Directive are being introduced in the countries of the European Union.\n\n\n"}
{"id": "39256418", "url": "https://en.wikipedia.org/wiki?curid=39256418", "title": "Human virome", "text": "Human virome\n\nThe human virome is the total collection of viruses in and on the human body. Viruses in the human body may infect both human cells and other microbes such as bacteria (as with bacteriophages). Some viruses cause disease, while others may be asymptomatic. Certain viruses are also integrated into the human genome as proviruses or endogenous viral elements.\n\nViruses evolve rapidly and hence the human virome changes constantly. Every human being has a unique virome with a unique balance of species. Lifestyle, age, geographic location, and even the season of the year can affect an individual's exposure to viruses, and one's susceptibility to any disease that might be caused by those viruses is also affected by pre-existing immunity and both viral and human genetics.\n\nThe human virome is far from being completely explored and new viruses are discovered frequently. Unlike the roughly 40 trillion bacteria in a typical human microbiome, an estimate of the number of viral particles in a healthy adult human is not yet available, although virions generally outnumber individual bacteria 10:1 in nature. Studying the virome is thought to provide an understanding of microbes in general and how they affect human health and disease.\n\nMultiple methods are available for the isolation and study of human viruses:\n\n\nThe human virome is not stable and may change over time. In fact, new viruses are discovered constantly. With an increasing number of known viruses, diagnosis and treatment of novel viral-associated conditions will become easier as well. Studying the virome could help improve drug development and limit antibiotic usage.\n\nOne of the first studies that used high-throughput DNA sequencing to describe the diversity of eukaryotic dsDNA viruses in normal individuals included 706 samples from 102 subjects. This study detected an average of 5.5 viral genera in each individual and these viruses included herpesviruses, papillomaviruses, polyomaviruses, adenoviruses, anelloviruses, parvoviruses, and circoviruses.\n\nEach individual had a distinct viral profile, demonstrating the high interpersonal diversity of the virome. One to 15 viral genera (average 5.5) were detected in 92% of the 102 individuals sampled (Figure 2). Figure 3 illustrates the viromes of the 102 individuals defined by sampling up to five major body habitats, showing that a broad range of viruses was detected in healthy people (Figure 3).\n\nThe 102 individuals carried seven distinct families of human DNA viruses (Figure 4A). Sequences were detected predominantly in the nose and skin, similarity to 17 papillomavirus genera(Figure 4B). Roseoloviruses, predominantly HHV-7 and to a lesser extent HHV-6, were present among 98% of the individuals who provided mouth samples.\n\nIn addition, the same viruses were prevalent in multiple body habitats within individuals. For instance, the beta- and gamma-papillomaviruses were the viruses most commonly found in the skin and the nose (anterior nares; see Figure 4A,B), which may reflect proximity and similarities in microenvironments that support infection with these viruses.\n\nWhole-genome sequencing data of blood from 8,240 individuals without any clear infectious disease revealed 94 different viruses in 42% of the study participants. The sequences included 19 human DNA viruses, proviruses and RNA viruses (herpesviruses, anelloviruses, papillomaviruses, three polyomaviruses, adenovirus, HIV, HTLV, hepatitis B, hepatitis C, parvovirus B19, and influenza virus). Of possible relevance to transfusion medicine, this study identified Merkel cell polyomavirus in 49 individuals, papillomavirus in blood of 13 individuals, parvovirus B19 in 6 individuals, and the presence of herpesvirus 8 in 3 individuals.\n\nThe human virome is a part of our bodies and will not always cause harm. Many latent and asymptomatic viruses are present in the human body all the time. Viruses infect all life forms; therefore the bacterial, plant, and animal cells and material in our gut also carry viruses. When viruses cause harm by infecting the cells in the body, a symptomatic disease may develop. Contrary to common belief, harmful viruses may be in the minority compared to benign viruses in the human body. It is much harder to identify viruses than it is to identify bacteria, therefore our understanding of benign viruses in the human body is very rudimentary.\n\nThe health effects of viruses on an individual are dependent on the individual's immune system.\n\nRecent research has linked the emerging idea of the hygiene hypothesis to viruses. This hypothesis attempts to explain and justify some of the high incidences of diseases such as asthma and eczema in the Western world to Western society's overuse of antibiotic and antiviral agents. This overuse potentially disrupts not only the bacteria of the gut but also the viruses that have long lived in harmony with the human body and now play a role in regulation of human health. This hypothesis generally refers to microorganisms but is now being extended to include airborne viruses and common viral infections of childhood that are becoming increasingly less common.\n\nDiverse viruses colonize the human skin and differ by skin site. These skin virome includes human viruses (i.e. human papillomavirus) and bacteriophages (bacterial viruses) that infect commensal skin bacteria such as Staphylococci. Virus communities differ by moisture levels and degree of protection from the external environment.\n\nMany studies have demonstrated that the bacteria and viruses in the human gut (the gut microbiome) can be altered by changes in diet. One study that focused on bacterial viruses, called bacteriophages, in the gut found a significant relationship between diet and the type of bacteriophages present. This was done by comparing the distance between bacteriophage gut communities in individuals both before and after they started a controlled diet. The results were that the distance between the bacteriophage gut communities of individuals on the same diet was significantly smaller at the end of their dietary treatment than it was at the start, while there was no increase in community similarity for individuals on different diets over time.\n"}
{"id": "41641823", "url": "https://en.wikipedia.org/wiki?curid=41641823", "title": "Hydrophobic light-activated adhesive", "text": "Hydrophobic light-activated adhesive\n\nHydrophobic light-activated adhesive (HLAA) is a type of glue that sets in seconds, but only after exposure to ultraviolet light. One biocompatible, biodegradable HLAA is under consideration for use in human tissue repair as a replacement for sutures, staples and other approaches.\n\nThe glue was developed in a collaboration between Boston Children's Hospital, MIT and Harvard-affiliated Brigham and Women's Hospital. It was inspired by the viscous, water-repellant fluids secreted by animals such as slugs, sandcastle worms and insect footpads.\n\nHLAA has been used experimentally to repair holes in pig hearts. It provides a hemostatic seal that adheres to the heart tissue despite immersion in liquid blood. It is not rejected by the body and is sufficiently adhesive and elastic that it is not pulled loose or damaged by the contractions of the heart muscle. It harmlessly biodegrades over time. The lack of stitching or stapling implies that procedures for applying glue-treated patches are potentially considerably less invasive than the alternatives. The polymer becomes physically entangled with collagen and other proteins on the tissue surface rather than adhering via a chemical reaction.\n\nSutures can damage heart tissue and take too long to apply. Staples can also damage heart tissue. Existing surgical adhesives can be toxic, and they can become unstuck in wet, dynamic environments such as the heart. As a result, infants often require subsequent operations to \"replug\" the hole. One other surgical adhesive cures when exposed to water.\n"}
{"id": "1115289", "url": "https://en.wikipedia.org/wiki?curid=1115289", "title": "Infinite Energy (magazine)", "text": "Infinite Energy (magazine)\n\nInfinite Energy is a bi-monthly magazine published in New Hampshire that details theories and experiments concerning alternative energy, new science and new physics. The magazine was founded by the late Eugene Mallove, and is owned by the non-profit New Energy Foundation. It was established in 1994 as \"Cold Fusion\" magazine and changed its name in March 1995.\n\nTopics of interest include \"new hydrogen physics,\" also called cold fusion; vacuum energy, or zero point energy; and so-called \"environmental energy\" which they define as the attempt to violate the Second Law of Thermodynamics, for example with a perpetual motion machine. This is done in pursuit of the founder's commitment to \"unearthing new sources of energy and new paradigms in science.\" The magazine has also published articles and book reviews that are critical of the big bang theory that describes the origin of the universe.\n\nThe magazine has a print run of 3,000, and is available on U.S. newsstands. The issues range in size from 48 to 100 pages.\n"}
{"id": "14066275", "url": "https://en.wikipedia.org/wiki?curid=14066275", "title": "Inversion temperature", "text": "Inversion temperature\n\nThe inversion temperature in thermodynamics and cryogenics is the critical temperature below which a non-ideal gas (all gases in reality) that is expanding at constant enthalpy will experience a temperature decrease, and above which will experience a temperature increase. This temperature change is known as the Joule-Thomson effect, and is exploited in the liquefaction of gases. Inversion temperature depends on the nature of the gas. \n\nFor a van der Waals gas we can calculate the enthalpy formula_1 using statistical mechanics as\n\nwhere formula_3 is the number of molecules, formula_4 is volume, formula_5 is temperature (in the Kelvin scale), formula_6 is Boltzmann's constant, and formula_7 and formula_8 are constants depending on intermolecular forces and molecular volume, respectively.\n\nFrom this equation, we note that if we keep enthalpy constant and increase volume, temperature must change depending on the sign of formula_9. Therefore, our inversion temperature is given where the sign flips at zero, or\n\nwhere formula_11 is the critical temperature of the substance. So for formula_12, an expansion at constant enthalpy increases temperature as the work done by the repulsive interactions of the gas is dominant, and so the change in energy is negative. But for formula_13, expansion causes temperature to decrease because the work of attractive intermolecular forces dominates, giving a positive change in energy.\n\n\n\n<br>\n"}
{"id": "32059363", "url": "https://en.wikipedia.org/wiki?curid=32059363", "title": "Jharkhand State Electricity Board", "text": "Jharkhand State Electricity Board\n\nJharkhand State Electricity Board is a Government of Jharkhand enterprise, entrusted with the generation and distribution of electrical power in the state of Jharkhand, India. It suffers a loss of more than 1,000 crore (USD 200mn) every year.\n\nIn 2011, it was revealed that in the last ten years, as much as 80 crores in outstanding dues to various units had been waived. For example, ex-Chief Minister Madhu Koda, in prison for graft since 2008, waived of a 10 crore bill from a company owned by fellow minister Vinod Sinha, who is also in jail on graft charges.\n\n\n\n"}
{"id": "31028494", "url": "https://en.wikipedia.org/wiki?curid=31028494", "title": "Lake Sonfon", "text": "Lake Sonfon\n\nLake Sonfon, also known as Lake Confon, is a fresh water mountain lake in Diang, Sierra Leone of religious and cultural significance. The nearest towns are Kabala that is 60 km to the north and Benugu that is 40 km to the south. It is located in the hills of the Sula Mountains at an altitude of above sea level. Sonfon drains from its southern end, which forms the start of the Pampana River, and is fed by seven small streams with its water level varying considerably during the year. The Lake has a maximum depth of and with an area of is Sierra Leone's largest inland lake.\n\nGold deposits are found in the rocks of Lake Sonfon and in alluvial deposits in the area. Only the alluvial deposits are being worked employing 15,000 miners around the Lake. This mining is causing the level of water in the lake to decrease.\n\nAlthough the Sonfon is not well surveyed 105 species of birds have been identified at the lake including the iris glossy-starling, the Dybowski's twinspot, the splendid sunbird, the red-faced pytilia and the pied-winged swallow. Animals that live at the lake include endangered pygmy hippopotamus, black duikers and Maxwell's duikers. In the dry season the lake is completely covered with vegetation. The hill environment around the lake consist of forests, wooded savanah, grassland and farmbush. The Lake is a key conservation area and a proposed protected area but as of 2011 there is no protection in place.\n\nLake Sonfon is considered sacred in traditional belief with local people carrying out cultural ceremonies along its shore. Offerings, including rice and food, are floated into the lake on calabashes. In traditional belief the lake is symbolically intermittent, as well as being intermittent in terms of the amount of water in the dry season and a powerful Djinn lives in the lake.\n\n"}
{"id": "49272511", "url": "https://en.wikipedia.org/wiki?curid=49272511", "title": "Marguerite Fund", "text": "Marguerite Fund\n\nThe 2020 European Fund for Energy, Climate Change and Infrastructure, known as the Marguerite Fund, is a pan-European equity fund that invests in renewables, energy and transport. It is based in Luxembourg.\n\nThe Marguerite Fund was established in 2010 by the European Investment Bank, Caisse des dépôts et consignations, Cassa Depositi e Prestiti, Instituto de Crédito Oficial, Kreditanstalt für Wiederaufbau and Powszechna Kasa Oszczędności Bank Polski as part of the European Economic Recovery Plan.\n\nThe Marguerite Fund has invested in the Butendiek Wind Farm off Germany, the Tychovo and the Kukinia wind farms in Poland,]]. the Thorntonbank Wind Farm off Belgium, Chirnogeni Wind Farm in Romania, Massangis 1 and Toul-Rosières 2 photovoltaic plants in France, the Poznań incineration plant in Poland, the Zagreb International Airport in Croatia, the N17/N18 Motorway project in Ireland, and the Autovía de Arlanzon Motorway in Spain. In January 2016, the Marguerite Fund became the second largest investor in the Latvian gas company Latvijas Gāze.\n"}
{"id": "9336144", "url": "https://en.wikipedia.org/wiki?curid=9336144", "title": "Marinisation", "text": "Marinisation\n\nMarinisation (also marinization) is design, redesign, or testing of products specifically for use and long-term survival in the harsh marine environment. This is done by many manufacturing industries worldwide including many military organisations (especially navies).\nThe main challenge is for the engineers who design the product from the scratch. Every part needs to be designed in order to fit to the marine environment and then to resist its \"natural attacks\".\n\nThere are three main factors that need to be considered for a product to be truly marinised.\n\nThese three factors are a constant in the marine environment, and are present even on a dead calm day. These factors apply also in fresh water. \n\nMarinised electronics use one or more of the following protection methods. In most cases more than one method is used:\n\n\nMarinised metals include some of the following examples:\n\n\nMarinised batteries are usually gel batteries or sealed maintenance-free batteries. Not using marinised batteries in salt water can be deadly in an enclosed environment for many reasons:\n\n\n"}
{"id": "11732979", "url": "https://en.wikipedia.org/wiki?curid=11732979", "title": "Michigan Alternative and Renewable Energy Center", "text": "Michigan Alternative and Renewable Energy Center\n\nThe Michigan Alternative and Renewable Energy Center (MAREC) was a facility located in Muskegon, Michigan that promoted research, education and business development in alternative and renewable energy technologies. In May 2016, the Center was renamed the Muskegon Innovation Center and the organization refocused on supporting innovation and entrepreneurship.\n\nDevelopment and planning for the center began in 1999 when a group of Grand Valley State University faculty and Muskegon business people proposed a research and development facility focused on alternative energy. Subsequent partnerships between the business, community, and the private sector resulted in groundbreaking for the center in late 2002, and its completion in 2003.\n\nThe facility was powered, in part, by a fuel cell and a micro turbine, which turned natural gas into electricity. In addition, the building's photovoltaic solar roof tiles harnessed the solar power generated by the sun to create useful energy. MAREC then used a nickel metal hydride battery system to store some of the energy produced by these sources for use during peak energy consumption periods.\n\nThe building was also constructed using many alternative and renewable building materials including flooring surfaces produced from fast-growing bamboo and recycled tires, and rigid wall surfaces made from pressed wheat. These materials were used to conserve and recycle valuable natural resources.\n\nMAREC was part of the Muskegon Lakeshore SmartZone, a joint venture with the Michigan Economic Development Corporation, the city of Muskegon, and Grand Valley State University. The Muskegon Lakeshore SmartZone is intended to promote and attract high technology business development in Muskegon and the region.\n\nMAREC had of space devoted to incubating businesses that would research and develop alternative energy sources and uses. The focus on alternative energy was expected to be a catalyst for economic development and job growth in the area. Research and development initiatives was also intended to fuel business expansion at Edison Landing, the SmartZone being transformed into a multi-use office, retail, and residential center. Overall SmartZone development was expected to complement the array of human, physical, and capital investments made at MAREC.\n\n\n"}
{"id": "16516069", "url": "https://en.wikipedia.org/wiki?curid=16516069", "title": "Minera San Xavier", "text": "Minera San Xavier\n\nMinera San Xavier is a subsidiary of the Canadian company New Gold Inc. that operates a gold and silver mine near Cerro de San Pedro, a municipality of San Luis Potosí just 12 kilometers away from the capital city of the state. The city of San Luis Potosí was founded after gold was discovered in the hills near Cerro de San Pedro in the 15th century.\n\nThe company has defied legal resolutions against its operation by arranging economically with the Mexican ecological authority, the Secretariat of the Environment and Natural Resources (Secretaría de Medio Ambiente y Recursos Naturales, Semarnat), pushing them to disregard ecological research done by some independent institutions and to instead accept most other independent as well as company-funded and government required studies. Continuous internal and external monitoring of mining activities has not produced any evidence of contamination to date.\n\nOn 5 February 2007, Pro San Luis Ecológico, A.C., filed a submission before the Commission for Environmental Cooperation under Article 14(1) of the North American Agreement on Environmental Cooperation. The submitter asserted that Mexico was failing to effectively enforce its environmental laws with respect to the authorization of the Cerro de San Pedro project in San San Luis Potos. In submission SEM-07-001 (Minera San Xavier), the submitter asserted that in April 2006 Semarnat violated a Mexican court ruling by authorizing Minera San Xavier project for a second time. On 15 July 2009, the CEC Secretariat decided not to recommend the development of a factual investigation.\n\n\n"}
{"id": "11064146", "url": "https://en.wikipedia.org/wiki?curid=11064146", "title": "Outlook On Renewable Energy In America", "text": "Outlook On Renewable Energy In America\n\nOutlook On Renewable Energy In America is a comprehensive two volume report, published in 2007 by The American Council on Renewable Energy (ACORE), about the future of renewable energy in the United States. It has been said that this report exposes a \"new reality for renewable energy in America\".\n\nVolume One of the report presents background information on government research conducted by the National Renewable Energy Laboratory, Energy Information Administration, Electric Power Research Institute and other institutions. It also collates information from industry associations such as the American Wind Energy Association, Solar Energy Industries Association, National Hydropower Association, as well as some non-profit organizations.\n\n\"Joint Outlook on Renewable Energy in America\" is the second volume of the report which presents a scenario compiled by the American renewable energy community. ACORE makes it clear that this report is not a forecast—it is a scenario of what is achievable if the country wants renewables to reach their full potential, and is willing to embrace the public policies to make that happen.\n\nACORE released an update of the report in March 2014, which assesses the marketplace and forecasts the future of each renewable energy technology sector from the perspectives of U.S. renewable energy trade associations. Each sector forecast is accompanied by a list of the trade association’s specific policy recommendations that they believe might encourage continued industry growth.\n\nAccording to experts, renewable energy could provide up to 635 gigawatts (GW) of new electricity generating capacity by 2025 – a substantial contribution and potentially more than the nation’s need for new capacity. According to this scenario, this capacity could come from a wide array of new technologies utilizing the full range of our renewable resources:\n\n\nIn addition, the scenario shows that renewable fuels could meet a large portion of U.S. liquid fuel needs. Recent studies show that biofuels could supply 30% to 40% of U.S. petroleum products by 2030. Ethanol fuel alone could reach per year by the end of the first quarter of 2009, a significant contribution to the approximately of gasoline consumed annually.\n\nTo make the transition to the renewable energy future that is outlined in the scenario, the report argues that the energy policy of the United States needs to be built on a range of principles, which include:\n\n\nThe report argues that America needs renewable energy, for many reasons:\n\nThe report quotes President Bush on the need to diversify America's energy supply:\n\nACORE suggests that these reports dispel the commonly held notion that renewable energy cannot supply the energy needs of a growing American economy. But for this to happen, the government would have to commit to long term policies that promote renewable energy.\n\n\n"}
{"id": "22566483", "url": "https://en.wikipedia.org/wiki?curid=22566483", "title": "Park &amp; Charge", "text": "Park &amp; Charge\n\nPark & Charge is a European infrastructure for charging electric vehicles.\n\nAt reserved parking spaces for electric vehicles a simple, locked charging station with all necessary sockets and safety facilities is installed to which only \"Park & Charge\" users have access. Electric vehicles are in generally energy efficient, and therefore at a \"Park & Charge\" chargers requires relatively small quantities of electricity. By not having complicated metering technology and with the simplest organizational measures such as a (flat rate) cost, the administrative costs are kept low.\n\nPark & Charge runs an open database \"LEMnet\" since 1998 that lists its own charging stations as well as that of other infrastructure providers in Europe.\n\nIn 1992, the first four locations of Park & Charge were established under the \"pilot and demonstration project P & D\" of the Federal Office of Energy in Bern. After a three-year experimental phase, the project went into regular operation. The Electromobile Club of Switzerland (ECS), which has accompanied the project, took over the financial and legal responsibility for Park & Charge. In the subsequent period Park & Charge became so popular that it needed to be legally formalised. At the end of 1997 Park & Charge was founded as an independent association (President: Wilfried Blum, Managing Director: Eduard Stolz).\n\nPark & Charge stations became available beyond the borders as well. The German Solar Mobility Association founded an organization for the operation in Germany in 1997. Since 1999 there are Park & Charge stations in Italy and Austria. As for Austria VLOTTE has taken over the operations since 2010 which is a project by the \"Vorarlberger Elektroautomobil Planungs- und Beratungs GmbH\" in cooperation with the VKW Utility Company Vorarlberg (Vorarlberger Kraftwerke AG). More charging points became available in Liechtenstein, France and the Netherlands.\n\nIn the Canton Ticino the federal \"P & D\" project was continued with studies for „Veicoli Elettrici Leggeri“ (light electric vehicles) running from 1995 to 2001 (VEL1) and from 2001 to 2005 (VEL2). With the help of those projects an electric vehicle network of charging stations was deployed which is operated by RiParTI (Ricariche e Parcheggi in Ticino – Charging and Parking in Ticino).\n\nThe key, giving access to all the Park & Charge Charging in Europe.\n\nThe uniform logo for the whole of Europe and is used by law enforcement bodies for the simple identification of electric vehicles, and is dated 1 December of the previous year contribution period to 31 January the following year. Entitles the owners of Electric vehicles to use the specially marked and for reserved parking spaces.\n\nThe yearly subscription entitles the user use of all the Park & Charge Charging points in Europe at a (flat rate). The prices of the subscription is set by the respective national organizations.\n\nToday allows Park & Charge is at nearly 500 locations in Switzerland, Germany, Austria, the Netherlands and Italy, for drivers of electric vehicles, it is a safe and easy way to charge their car's battery.\n\nThe locations of the charging stations of Park & Charge were assembled in a list for its members. The list did also include charging sports of third parties as far as they were usable by the vehicle owners.\n\nAfter the reorganization the new association created an internet database in 1998 to publish the list of available charging stations. The name LEMnet is derived from the main vehicle type of Park & Charge members at the time being light electric vehicles (LEV - in German \"Leichtelektromobile\" LEM) being electric quadricycles that are often summarized in English in the Neighborhood Electric Vehicle class. Those only need an electrical outlet to be charged overnight so that LEMnet did also list inns that cooperate with electric vehicle owners.\n\nWith the of electric vehicles the LEMnet database served as a central overview mapping the available charging stations across Europe. The providers of commercial charging station networks began to provide their data to LEMnet - in 2012 more than 30 providers cooperated with LEMnet. On 26 March 2012 it was decided form a new association to run the LEMnet database - the \"LEMnet Europe e.V.\"' is supported by Park & Charge and the E-Mobility Cluster, Mitteldeutschland.\n\n\n Getting connected, Electromobility and infrastructure. http://www.opi2020.com/images/4_Getting_connected_GB_040912.pdf\n\n"}
{"id": "271396", "url": "https://en.wikipedia.org/wiki?curid=271396", "title": "Pennyweight", "text": "Pennyweight\n\nA pennyweight (abbreviated dwt, from denarius weight) is a unit of mass that is equal to 24 grains, of a troy ounce, of a troy pound, approximately 0.054857 avoirdupois ounce and exactly 1.55517384 grams.\n\nIn the Middle Ages, a British penny's weight was literally, as well as monetarily, of an ounce and of a pound of sterling silver. At that time, the pound in use was the Tower pound, equal to 7,680 Tower grains (also known as wheat grains). The medieval English pennyweight was thus equal to 32 Tower grains. When Troy weights replaced Tower weights in 1527, the Troy weights were defined in such a way that the old Tower pound came out to exactly 5400 Troy grains (also known as barleycorns), the Tower pennyweight Troy grains (and thus approximately 1.46 grams). After 1527, the English pennyweight was the Troy pennyweight. of 24 Troy grains. Thus the Troy pound, ounce, and pennyweight, with their definitions given in terms of the Troy grain instead of in terms of the Tower grain, were 1/15 or 6.667% more than the Tower equivalents.\n\nThe Troy pound and the pennyweight lost their official status in the United Kingdom in the Weights and Measures Act of 1878; only the Troy ounce and its decimal subdivisions remained official. The Troy ounce enjoys a specific legal exemption from metrication in the UK.\n\nThe pennyweight is the common weight used in the valuation and measurement of precious metals. Jewellers use the pennyweight in calculating the amount and cost of precious metals used in fabricating or casting jewellery. Similarly, dentists and dental labs still use the pennyweight as the measure of precious metals in dental crowns and inlays.\n\nPennyweight and grains are still used to weigh gooseberries in competitions in Cheshire, northwest UK. Over the Pennines in Yorkshire the alternative drams and grains measurement has been used since a new set of scales was purchased by the Egton Bridge Old Gooseberry Society in 1937. As of 2018, the world record for the heaviest gooseberry of 41 dwt 11 gr was held by Kelvin Archer of Cheshire.\n\nThe most common abbreviation for pennyweight is \"dwt\"; \"d\", for the Roman \"denarius\", was the abbreviation for \"penny\" before Decimalization of the British monetary system. Alternate abbreviations are \"pwt\" and \"PW\".\n\nAlthough the abbreviations are the same, the pennyweight bears no relation to the weight of the American penny nail. That name is derived from the price for a hundred nails in 15th century England: the larger the nail, the higher the cost per hundred.\nThe pennyweight also bears no relation to the weight of the American \"penny\" (1 cent) coin, which weighs 2.5 g (for those minted after 1982).\n"}
{"id": "40783503", "url": "https://en.wikipedia.org/wiki?curid=40783503", "title": "Plumage League", "text": "Plumage League\n\nA Plumage League to campaign against the excessive use of birds' feathers and plumage in ladies fashions was formed by Reverend Francis Orpen Morris and Lady Mount Temple in December 1885. This almost immediately amalgamated to form the Plumage Section of the Selborne Society for the Preservation of Birds, Plants and Pleasant Places in the following January.\n\nAnother plumage league was later formed by Emily Williamson in Didsbury. This was organised by Mancunian women unable to join the male British Ornithological Union as the Society for Protection of Birds which subsequently became the RSPB.\n"}
{"id": "355155", "url": "https://en.wikipedia.org/wiki?curid=355155", "title": "Polar orbit", "text": "Polar orbit\n\nA polar orbit is one in which a satellite passes above or nearly above both poles of the body being orbited (usually a planet such as the Earth, but possibly another body such as the Moon or Sun) on each revolution. It therefore has an inclination of (or very close to) 90 degrees to the body's equator. A satellite in a polar orbit will pass over the equator at a different longitude on each of its orbits.\n\nPolar orbits are often used for earth-mapping, earth observation, capturing the earth as time passes from one point, reconnaissance satellites, as well as for some weather satellites. The Iridium satellite constellation also uses a polar orbit to provide telecommunications services. The disadvantage to this orbit is that no one spot on the Earth's surface can be sensed continuously from a satellite in a polar orbit.\n\nNear-polar orbiting satellites commonly choose a Sun-synchronous orbit, meaning that each successive orbital pass occurs at the same local time of day. This can be particularly important for applications such as remote sensing atmospheric temperature, where the most important thing to see may well be \"changes\" over time which are not aliased onto changes in local time. To keep the same local time on a given pass, the time period of the orbit must be kept as short as possible, this is achieved by keeping the orbit lower around Earth. However, very low orbits of a few hundred kilometers rapidly decay due to drag from the atmosphere. Commonly used altitudes are between 700 and 800 km, producing an orbital period of about 100 minutes. The half-orbit on the Sun side then takes only 50 minutes, during which local time of day does not vary greatly.\n\nTo retain the Sun-synchronous orbit as Earth revolves around the Sun during the year, the orbit of the satellite must precess at the same rate, which is not possible if the satellite were to pass directly over the pole. Because of Earth's equatorial bulge, an orbit inclined at a slight angle is subject to a torque, which causes precession. An angle of about 8° from the pole produces the desired precession in a 100-minute orbit.\n\n\n"}
{"id": "699689", "url": "https://en.wikipedia.org/wiki?curid=699689", "title": "Polaron", "text": "Polaron\n\nA polaron is a quasiparticle used in condensed matter physics to understand the interactions between electrons and atoms in a solid material. The polaron concept was first proposed by Lev Landau in 1933 to describe an electron moving in a dielectric crystal where the atoms move from their equilibrium positions to effectively screen the charge of an electron, known as a phonon cloud. This lowers the electron mobility and increases the electron's effective mass.\n\nThe general concept of a polaron has been extended to describe other interactions between the electrons and ions in metals that result in a bound state, or a lowering of energy compared to the non-interacting system. Major theoretical work has focused on solving Fröhlich and Holstein Hamiltonians. This is still an active field of research to find exact numerical solutions to the case of one or two electrons in a large crystal lattice, and to study the case of many interacting electrons.\n\nExperimentally, polarons are important to the understanding of a wide variety of materials. The electron mobility in semiconductors can be greatly decreased by the formation of polarons. Organic semiconductors are also sensitive to polaronic effects, which is particularly relevant in the design of organic solar cells that effectively transport charge. The electron phonon interaction that forms Cooper pairs in low-T superconductors (type-I superconductors) can also be modeled as a polaron, and two opposite spin electrons may form a bipolaron sharing a phonon cloud. This has been suggested as a mechanism for Cooper pair formation in high-T superconductors (type-II superconductors). Polarons are also important for interpreting the optical conductivity of these types of materials.\n\nThe polaron, a fermionic quasiparticle, should not be confused with the polariton, a bosonic quasiparticle analogous to a hybridized state between a photon and an optical phonon.\n\nThe energy spectrum of an electron moving in a periodical potential of rigid crystal lattice is called the Bloch spectrum, which consists of allowed bands and forbidden bands. An electron with energy inside an allowed band moves as a free electron but has an effective mass that differs from the electron mass in vacuum. However, a crystal lattice is deformable and displacements of atoms (ions) from their equilibrium positions are described in terms of phonons. Electrons interact with these displacements, and this interaction is known as electron-phonon coupling. One of possible scenarios was proposed in the seminal 1933 paper by Lev Landau, which includes the production of a lattice defect such as an F-center and a trapping of the electron by this defect. A different scenario was proposed by Solomon Pekar that envisions dressing the electron with lattice deformation (a cloud of virtual phonons). Such an electron with the accompanying deformation moves freely across the crystal, but with increased effective mass. Pekar coined for this charge carrier the term polaron.\n\nL. D. Landau and S. I. Pekar formed the basis of polaron theory. A charge placed in a polarizable medium will be screened. Dielectric theory describes the phenomenon by the induction of a polarization around the charge carrier. The induced polarization will follow the charge carrier when it is moving through the medium. The carrier together with the induced polarization is considered as one entity, which is called a polaron (see Fig. 1).\n\nWhile polaron theory was originally developed for electrons as dressed charges in a crystal field, there is no fundamental reason held against any other charged particle which might interact with phonons. Therefore, also other charged particles such as (electron) holes and ions should generally follow the polaron theory.\n\nRecently, the proton polaron was identified in experimental work on ceramic electrolytes after hypothesis of its existence.\nA. Braun, Q. Chen A. L. Samgin \n\nA conduction electron in an ionic crystal or a polar semiconductor is the prototype of a polaron. Herbert Fröhlich proposed a model Hamiltonian for this polaron through which its dynamics are treated quantum mechanically (Fröhlich Hamiltonian).\nThis model assumes that electron wavefunction is spread out over many ions which are all somewhat displaced from their equilibrium positions, or the continuum approximation. The strength of the electron-phonon interaction is expressed by a dimensionless coupling constant α introduced by Fröhlich. In Table 1 the Fröhlich coupling constant is given for a few solids. The Fröhlich Hamiltonian for a single electron in a crystal using second quantization notation is: \n\nThe exact form of γ depends on the material and the type of phonon being used in the model. A detailed advanced discussion of the variations of the Fröhlich Hamiltonian can be found in J. T. Devreese and A. S. Alexandrov The terms Fröhlich polaron and large polaron are sometimes used synonymously, since the Fröhlich Hamiltonian includes the continuum approximation and long range forces. There is no known exact solution for the Fröhlich Hamiltonian with longitudinal optical (LO) phonons and linear formula_5 (the most commonly considered variant of the Fröhlich polaron) despite extensive investigations.\n\nDespite the lack of an exact solution, some approximations of the polaron properties are known.\n\nThe physical properties of a polaron differ from those of a band-carrier. A polaron is characterized by its \"self-energy\" formula_6, an \"effective mass\" formula_7 and by its characteristic \"response\" to external electric and magnetic fields (e. g. dc mobility and optical absorption coefficient).\n\nWhen the coupling is weak (formula_8 small), the self-energy of the polaron can be approximated as:\n\nand the polaron mass formula_10, which can be measured by cyclotron resonance experiments, is larger than the band mass m of the charge carrier without self-induced polarization:\n\nWhen the coupling is strong (α large), a variational approach due to Landau and Pekar indicates that the self-energy is proportional to α² and the polaron mass scales as \"α\"⁴. The Landau–Pekar variational calculation \n\nyields an upper bound to the polaron self-energy formula_12, valid\nfor \"all\" \"α\", where formula_13 is a constant determined by solving an integro-differential equation. It was an open question for many years whether this\nexpression was asymptotically exact as α tends to infinity. Finally,\nDonsker and Varadhan, applying large deviation theory to Feynman's\npath integral formulation for the self-energy, showed the large α exactitude \nof this Landau–Pekar formula. Later, Lieb and Thomas \nand with explicit bounds on the lower order corrections to the \nLandau–Pekar formula.\n\nFeynman introduced a variational principle for path integrals to study the polaron. He simulated the interaction between the electron and the polarization modes by a harmonic interaction between a hypothetical particle and the electron. The analysis of an exactly solvable (\"symmetrical\") 1D-polaron model, Monte Carlo schemes and other numerical schemes demonstrate the remarkable accuracy of Feynman's path-integral approach to the polaron ground-state energy. Experimentally more directly accessible properties of the polaron, such as its mobility and optical absorption, have been investigated subsequently.\n\nIn the strong coupling limit, formula_14, the spectrum of excited states of a polaron begins with polaron-phonon bound states with energies less than formula_15, where formula_16 is the frequency of optical phonons.\n\nThe expression for the magnetooptical absorption of a polaron is:\n\nHere, formula_18 is the cyclotron frequency for a rigid-band electron. The magnetooptical absorption Γ(Ω) at the frequency Ω takes the form Σ(Ω) is the so-called \"memory function\", which describes the dynamics of the polaron. Σ(Ω) depends also on α, β and formula_19.\n\nIn the absence of an external magnetic field (formula_20) the optical absorption spectrum (3) of the polaron at weak coupling is determined by the absorption of radiation energy, which is reemitted in the form of LO phonons. At larger coupling, formula_21, the polaron can undergo transitions toward a relatively stable internal excited state called the \"relaxed excited state\" (RES) (see Fig. 2). The RES peak in the spectrum also has a phonon sideband, which is related to a Franck–Condon-type transition.\n\nA comparison of the DSG results with the optical conductivity spectra given by approximation-free numerical and approximate analytical approaches is given in ref.\n\nCalculations of the optical conductivity for the Fröhlich polaron performed within the Diagrammatic Quantum Monte Carlo method, see Fig. 3, fully confirm the results of the path-integral variational approach at formula_22 In the intermediate coupling regime formula_23 the low-energy behavior and the position of the maximum of the optical conductivity spectrum of ref. follow well the prediction of ref. There are the following qualitative differences between the two approaches in the intermediate and strong coupling regime: in ref., the dominant peak broadens and the second peak does not develop, giving instead rise to a flat shoulder in the optical conductivity spectrum at formula_24. This behavior can be attributed to the optical processes with participation of two or more phonons. The nature of the excited states of a polaron needs further study.\n\nThe application of a sufficiently strong external magnetic field allows one to satisfy the resonance condition formula_25, which {(for formula_26)} determines the polaron cyclotron resonance frequency. From this condition also the polaron cyclotron mass can be derived. Using the most accurate theoretical polaron models to evaluate formula_27, the experimental cyclotron data can be well accounted for.\n\nEvidence for the polaron character of charge carriers in AgBr and AgCl was obtained through high-precision cyclotron resonance experiments in external magnetic fields up to 16 T. The all-coupling magneto-absorption calculated in ref., leads to the best quantitative agreement between theory and experiment for AgBr and AgCl. This quantitative interpretation of the cyclotron resonance experiment in AgBr and\nAgCl by the theory of ref. provided one of the most convincing and clearest demonstrations of Fröhlich polaron features in solids.\n\nExperimental data on the magnetopolaron effect, obtained using far-infrared photoconductivity techniques, have been applied to study the energy spectrum of shallow donors in polar semiconductor layers of CdTe.\n\nThe polaron effect well above the LO phonon energy was studied through cyclotron resonance measurements, e. g., in II–VI semiconductors, observed in ultra-high magnetic fields. The resonant polaron effect manifests itself when the cyclotron frequency approaches the LO phonon energy in sufficiently high magnetic fields.\n\nThe great interest in the study of the two-dimensional electron gas (2DEG) has also resulted in many investigations on the properties of polarons in two dimensions. A simple model for the 2D polaron system consists of an electron confined to a plane, interacting via the Fröhlich interaction with the LO phonons of a 3D surrounding medium. The self-energy and the mass of such a 2D polaron are no longer described by the expressions valid in 3D; for weak coupling they can be approximated as:\n\nIt has been shown that simple scaling relations exist, connecting the physical properties of polarons in 2D with those in 3D. An example of such a scaling relation is:\n\nwhere formula_31 (formula_32) and formula_33 (formula_34) are, respectively, the polaron and the electron-band masses in 2D (3D).\n\nThe effect of the confinement of a Fröhlich polaron is to enhance the \"effective\" polaron coupling. However, many-particle effects tend to counterbalance this effect because of screening.\n\nAlso in 2D systems cyclotron resonance is a convenient tool to study polaron effects. Although several other effects have to be taken into account (nonparabolicity of the electron bands, many-body effects, the nature of the confining potential, etc.), the polaron effect is clearly revealed in the cyclotron mass. An interesting 2D system consists of electrons on films of liquid He. In this system the electrons couple to the ripplons of the liquid He, forming \"ripplopolarons\". The effective coupling can be relatively large and, for some values of the parameters, self-trapping can result. The acoustic nature of the ripplon dispersion at long wavelengths is a key aspect of the trapping.\nFor GaAs/AlGaAs quantum wells and superlattices, the polaron effect is found to decrease the energy of the shallow donor states at low magnetic fields and leads to a resonant splitting of the energies at high magnetic fields. The energy spectra of such polaronic systems as shallow donors (\"bound polarons\"), e. g., the D and D centres, constitute the most complete and detailed polaron spectroscopy realised in the literature.\n\nIn GaAs/AlAs quantum wells with sufficiently high electron density, anticrossing of the cyclotron-resonance spectra has been observed near the GaAs transverse optical (TO) phonon frequency rather than near the GaAs LO-phonon frequency. This anticrossing near the TO-phonon frequency was explained in the framework of the polaron theory.\n\nBesides optical properties, many other physical properties of polarons have been studied, including the possibility of self-trapping, polaron transport, magnetophonon resonance, etc.\n\nSignificant are also the extensions of the polaron concept: acoustic polaron, piezoelectric polaron, electronic polaron, bound polaron, trapped polaron, spin polaron, molecular polaron, solvated polarons, polaronic exciton, Jahn-Teller polaron, small polaron, bipolarons and many-polaron systems. These extensions of the concept are invoked, e. g., to study the properties of conjugated polymers, colossal magnetoresistance perovskites, high-formula_35 superconductors, layered MgB superconductors, fullerenes, quasi-1D conductors, semiconductor nanostructures.\n\nThe possibility that polarons and bipolarons play a role in high-formula_35 superconductors has renewed interest in the physical properties of many-polaron systems and, in particular, in their optical properties. Theoretical treatments have been extended from one-polaron to many-polaron systems.\n\nA new aspect of the polaron concept has been investigated for semiconductor nanostructures: the exciton-phonon states are not factorizable into an adiabatic product Ansatz, so that a \"non-adiabatic\" treatment is needed. The \"non-adiabaticity\" of the exciton-phonon systems leads to a strong enhancement of the phonon-assisted transition probabilities (as compared to those treated adiabatically) and to multiphonon optical spectra that are considerably different from the Franck–Condon progression even for small values of the electron-phonon coupling constant as is the case for typical semiconductor nanostructures.\n\nIn biophysics Davydov soliton is a propagating along the protein α-helix self-trapped amide I excitation that is a solution of the Davydov Hamiltonian. The mathematical techniques that are used to analyze Davydov's soliton are similar to some that have been developed in polaron theory. In this context the Davydov soliton corresponds to a \"polaron\" that is (i) \"large\" so the continuum limit approximation in justified, (ii) \"acoustic\" because the self-localization arises from interactions with acoustic modes of the lattice, and (iii) \"weakly coupled\" because the anharmonic energy is small compared with the phonon bandwidth.\n\nIt has been shown that the system of an impurity in a Bose–Einstein condensate is also a member of the polaron family. This allows the hitherto inaccessible strong coupling regime to be studied, since the interaction strengths can be externally tuned through the use of a Feshbach resonance. This was recently realized experimentally by two research groups.\nThe existence of the polaron in a Bose–Einstein condensate was demonstrated for both attractive and repulsive interactions, including the strong coupling regime.\n\n"}
{"id": "28686876", "url": "https://en.wikipedia.org/wiki?curid=28686876", "title": "Rothenburg Solar Park", "text": "Rothenburg Solar Park\n\nThe Rothenburg Solar Park is a photovoltaic power station in Rothenburg, Oberlausitz in Germany. It has a capacity of 20.5 megawatts (MW). The solar park is equipped with 273,240  CdTe-modules from First Solar, and 11 Siemens central inverters. The project was commissioned in 2009.\n\n"}
{"id": "24515182", "url": "https://en.wikipedia.org/wiki?curid=24515182", "title": "Spectralon", "text": "Spectralon\n\nSpectralon is a fluoropolymer, which has the highest diffuse reflectance of any known material or coating over the ultraviolet, visible, and near-infrared regions of the spectrum. It exhibits highly Lambertian behavior, and can be machined into a wide variety of shapes for the construction of optical components such as calibration targets, integrating spheres, and optical pump cavities for lasers.\n\nSpectralon's reflectance is generally >99% over a range from 400 to 1500 nm and >95% from 250 to 2500 nm. However, grades are available with added carbon to achieve various gray levels. Surface or subsurface contamination may lower the reflectance at the extreme upper and lower ends of the spectral range. The material is also highly lambertian at wavelengths from 257 nm to 10600 nm, although reflectivity decreases at wavelengths beyond the near infrared. Spectralon exhibits absorbances at 2800 nm, then absorbs strongly (<20% reflectance) from 5400 to 8000 nm. Although the diffused reflectance has been shown to increase overall laser efficiency, the material has a fairly low damage threshold of 4 joules per square centimeter, limiting its use to lower powered applications.\n\nThe Lambertian reflectance arises from the material's surface and immediate subsurface structure. The porous network of thermoplastic produces multiple reflections in the first few tenths of a millimeter. Spectralon can partially depolarize the light it reflects, but this effect decreases at high incidence angles. Although it is extremely hydrophobic, this open structure readily absorbs non-polar solvents, greases and oils. Impurities are difficult to remove from Spectralon; thus, the material should be kept free from contaminants to maintain its reflectance properties.\n\nThe material has a hardness roughly equal to that of high-density polyethylene and is thermally stable to > 350 °C. It is chemically inert to all but the most powerful bases such as sodium amide and organo-sodium or lithium compounds. The material is extremely hydrophobic. Gross contamination of the material or marring of the optical surface can be remedied by sanding under a stream of running water. This surface refinishing both restores the original topography of the surface and returns the material to its original reflectance. Weathering tests on the material show no damage upon exposure to atmospheric UV flux. The material shows no sign of optical or physical degradation after long-term immersion testing in sea water.\n\nThree grades of Spectralon reflectance material are available: optical grade, laser grade and space grade. Optical-grade Spectralon is characterized by a high reflectance and Lambertian behavior and is primarily used as a reference standard or target for calibration of spectrophotometers. Laser-grade Spectralon offers the same physical characteristics as optical-grade material but is a different formulation of resin that gives enhanced performance when used in laser pump cavities. Spectralon is used in a variety of \"side pumped\" lasers. Space-grade Spectralon combines high reflectance with an extremely lambertian reflectance profile and is used for terrestrial remote sensing applications.\n\nSpectralon's optical properties make it ideal as a reference surface in remote sensing and spectroscopy. For instance, it is used to obtain leaf reflectance and bidirectional reflectance distribution function (BRDF) in the laboratory. It can also be applied to obtain vegetation fluorescence using the Fraunhofer lines.\nBasically Spectralon allows removing the contributions in the emitted light that are not directly linked to the surface (leaf) properties but to geometrical factors.\n\nSpectralon was developed by Labsphere and has been available since 1986.\n\n"}
{"id": "1359349", "url": "https://en.wikipedia.org/wiki?curid=1359349", "title": "Suez Canal overhead powerline crossing", "text": "Suez Canal overhead powerline crossing\n\nThe Suez Canal overhead powerline crossing is a major electrical power line built across the Suez Canal in 1998, located near Suez, Egypt. It is designed for two 500 kV circuits.\n\nBecause the required clearance over the Suez Canal is , the overhead line has two high pylons (one on either side of the crossing) in spite of its small span width of . The pylons each have four crossarms: three for the conductors and one for catching the conductors in case of an insulator string failure.\n\nThe crossing was part of a major drive to develop the areas surrounding the Suez Canal, including other projects such as the Ahmed Hamdi Tunnel under the Suez Canal (completed in 1981), the El Ferdan Railway Bridge, and the Suez Canal Bridge (completed in 2001).\n\nIt was constructed by a consortium between STFA Enerkom and Siemens.\n\n"}
{"id": "54409548", "url": "https://en.wikipedia.org/wiki?curid=54409548", "title": "Tapered element oscillating microbalance", "text": "Tapered element oscillating microbalance\n\nA tapered element oscillating microbalance (TEOM) is an instrument used for real-time detection of aerosol particles by measuring their mass concentration. It makes use of a small vibrating glass tube whose oscillation frequency changes when aerosol particles are deposited on it increasing its inertia. TEOM-based devices have been approved by the U.S. Environmental Protection Agency for environmental air quality monitoring, and by the U.S. Mine Safety and Health Administration for monitoring coal dust exposure for miners to prevent several respiratory diseases.\n\nThe TEOM uses a hollow glass tube as a microbalance. Incoming particles are deposited on a filter at the tip of the tube, and the added mass causes a change in its oscillation frequency which is detected electronically. The element is periodically cycled to return it to its natural frequency. The inlet to the device only allows particles of the desired size range to enter. TEOM devices operate continuously and do not need filter changes as frequently as high-volume air samplers.\n\nMechanical noise and dramatic temperature fluctuations can interfere with the operation of a TEOM device. In addition, water droplets cannot be distinguished from particle mass, so the device must adjust the incoming air temperature to cause water droplets to evaporate, or contain a dryer or humidity sensor to adjust the readings. Under ideal conditions, TEOM is just as accurate as the standard reference method, but its sensitivity presents complications for use for environmental monitoring in urban areas.\n\nA filter dynamic measurement system (FDMS) can be used to adjust for the volatile component of the mass. TEOM has poor sensitivity to semi-volatile particles due to the temperature and humidity conditions used. TEOM instruments with FDMS alternate between a base cycle and a reference cycle, the latter of which measures the mass loss of the filter when clean air is passed through it, allowing the mass loss during the base cycle to be estimated. It is important that the air conditioning system not cycle over the same period as the TEOM instrument, because this can cause aliasing.\n\nInstruments using TEOM have been designated as Federal Equivalent Methods by the U.S. Environmental Protection Agency for environmental air quality monitoring of both coarse and fine particulate matter (PM, PM, and PMc). TEOM instruments are faster than and avoid difficulties with beta attenuation and quartz crystal microbalance methods.\n\nTEOM is the basis for a continuous personal dust monitor (CPDM) for coal dust in mines, to protect workers from exposure to coal mine dust which leads to black lung disease and progressive massive fibrosis. Prior to the introduction of CPDMs, dust particles collected on a filter needed to be analyzed in a laboratory, leading to a delay of weeks in obtaining results. Continuous monitoring allows miners to take corrective action such as moving to another area or changing their activities if dust levels exceed exposure limits. In one study this led to a 90% reduction in samples exceeding the dust exposure limit. In February 2016, the U.S. Mine Safety and Health Administration (MSHA) mandated the use of CPDMs on working sections of underground coal mines, and for workers who have evidence of the development of pneumoconiosis. As of 2017, the only CPDM instrument approved by MSHA uses a TEOM.\n\nAs of 2013, TEOM was not considered suitable for workplace monitoring of nanomaterials due to its cut-off particle sizes of 10, 2.5, or 1 μm, and the physical size of the instrument.\n\nTEOM is a proprietary technology developed by Rupprecht and Patashnick Co., Inc. of Albany, New York, whose successor company is Thermo Fisher Scientific. \"TEOM\" is a registered trademark. It was originally developed as a fixed-site environmental particulate mass monitor, and TEOM aerosol detectors were available in 1981.\n\nDevelopment of the continuous personal dust monitor was performed by Thermo Fisher under contract from the U.S. National Institute for Occupational Safety and Health with input from other government, labor, and industry organizations. Machine-mounted continuous dust monitors have been available since 1997.\n"}
{"id": "2024566", "url": "https://en.wikipedia.org/wiki?curid=2024566", "title": "Teltron tube", "text": "Teltron tube\n\nA teltron tube (named for Teltron Inc., which is now owned by 3B Scientific Ltd.) is a type of cathode ray tube used to demonstrate the properties of electrons. There were several different types made by Teltron including a diode, a triode, a Maltese Cross tube, a simple deflection tube with a fluorescent screen, and one which could be used to measure the charge-to-mass ratio of an electron. The latter two contained an electron gun with deflecting plates. The beams can be bent by applying voltages to various electrodes in the tube or by holding a magnet close by. The electron beams are visible as fine bluish lines. This is accomplished by filling the tube with low pressure helium (He) or Hydrogen (H) gas. A few of the electrons in the beam collide with the helium atoms, causing them to fluoresce and emit light.\n\nThey are usually used to teach electromagnetic effects because they show how an electron beam is affected by electric fields and by magnetic fields like the Lorentz force.\n\nCharged particles in a uniform electric field follow a parabolic trajectory, since the electric field term (of the Lorentz force which acts on the particle) is the product of the particle's charge and the magnitude of the electric field, (oriented in the direction of the electric field). In a uniform magnetic field however, charged particles follow a circular trajectory due to the cross product in the magnetic field term of the Lorentz force. (That is, the force from the magnetic field acts on the particle in a direction perpendicular to the particle's direction of motion. See: Lorentz force for more details.)\n\nThe 'teltron' apparatus consists of a Teltron type electron deflection tube, a Teltron stand, EHT power supply (, variable).\n\nIn an evacuated glass bulb some hydrogen gas (H) is filled, so that the tube has a hydrogen atmosphere at low pressure of about is formed. The pressure is such that the electrons are decelerated by collisions as little as possible (change in kinetic energy), the number of collisions are few but sufficient to emit visible light. Inside the bulb there is an electron gun. This consists of a heating spiral, a cathode and an anode hole. From the cathode (-) electrons are emitted and accelerated by the electric field towards the positively charged anode (+). Through a hole in the anode, the electrons leave the beam forming system and the Wehnelt cylinder bundles.\n\nWhen the heater is energized, the heating coil will cause electrons to emerge from it due to thermionic emission. In the electric field between anode and cathode, the electric field acts on the electrons, which accelerate to a high velocity, such that the electrons leave through a small opening in the anode as an electron beam. Only when the coil current is turned on will a force act on the beam and change its direction. Otherwise it will retain its velocity. If, however, the coil current is switched on, the Lorentz force will direct the electrons into a circular orbit.\n\nThe higher the coil current, the stronger magnetic field and thus smaller radius of the circular path of the electrons. The strength of the magnetic field and the Lorentz force are proportional to each other, such that when the Lorentz force increases. A larger Lorentz force will deflect the electrons more strongly, so the orbit will be smaller. The Lorentz force formula_1 is always perpendicular to the instantaneous direction of movement and allows a centripetal formula_2 circular motion. The magnitude of the velocity and hence the kinetic energy can not change:\n\nFrom this we get the amount of specific electron charge\n\nThe determination of the velocity is performed using the energy conservation law\n\nThis is finally followed by\n\nThe specific electron charge has the value\n\nSince the charge of an electron is available from the Millikan experiment, the study of electrons in an magnetic field is the determination of its mass in accordance with:\n\nSimilar concepts for the weighing of charged particles can be found in the mass spectrometer.\n\n"}
{"id": "1016017", "url": "https://en.wikipedia.org/wiki?curid=1016017", "title": "Thiocyanate", "text": "Thiocyanate\n\nThiocyanate (also known as rhodanide) is the anion [SCN]. It is the conjugate base of thiocyanic acid. Common derivatives include the colourless salts potassium thiocyanate and sodium thiocyanate. Organic compounds containing the functional group SCN are also called thiocyanates. Mercury(II) thiocyanate was formerly used in pyrotechnics.\n\nThiocyanate is analogous to the cyanate ion, [OCN], wherein oxygen is replaced by sulfur. [SCN] is one of the pseudohalides, due to the similarity of its reactions to that of halide ions. Thiocyanate used to be known as rhodanide (from a Greek word for rose) because of the red colour of its complexes with iron. Thiocyanate is produced by the reaction of elemental sulfur or thiosulfate with cyanide:\nThe second reaction is catalyzed by thiosulfate sulfurtransferase, a hepatic mitochondrial enzyme, and by other sulfur transferases, which together are responsible for around 80% of cyanide metabolism in the body.\n\nOrganic and transition metal derivatives of the thiocyanate ion can exist as \"linkage isomers\". In thiocyanates, the organic group (or metal ion) is attached to sulfur: R−S−C≡N has a S–C single bond and a C≡N triple bond. In isothiocyanates, the substituent is attached to nitrogen: R−N=C=S has a S=C double bond and a C=N double bond:\n\nOrganic thiocyanates are valuable building blocks in organic chemistry and they allow to access efficiently various sulfur containing functional groups and scaffolds.\n\nSeveral synthesis routes exist, the most basic being the reaction between alkyl halides and alkali thiocyanate in aqueous media.\nOrganic thiocyanates are hydrolyzed to thiocarbamates in the Riemschneider thiocarbamate synthesis.\nThiocyanate is known to be an important part in the biosynthesis of hypothiocyanite by a lactoperoxidase. Thus the complete absence of thiocyanate or reduced thiocyanate in the human body, (e.g., cystic fibrosis) is damaging to the human host defense system.\n\nThiocyanate is a potent competitive inhibitor of the thyroid sodium-iodide symporter. Iodine is an essential component of thyroxine. Since thiocyanates will decrease iodide transport into the thyroid follicular cell, they will decrease the amount of thyroxine produced by the thyroid gland. As such, foodstuffs containing thiocyanate are best avoided by Iodide deficient hypothyroid patients.\n\nIn the early 20th century, thiocyanate was used in the treatment of hypertension, but it is no longer used because of associated toxicity. Sodium nitroprusside, a metabolite of which is thiocyanate, is however still used for the treatment of a hypertensive emergency. Rhodanese catalyzes the reaction of sodium nitroprusside with thiosulfate to form the metabolite thiocyanate.\n\nThiocyanate shares its negative charge approximately equally between sulfur and nitrogen. As a consequence, thiocyanate can act as a nucleophile at either sulfur or nitrogen — it is an ambidentate ligand. [SCN] can also bridge two (M−SCN−M) or even three metals (>SCN− or −SCN<). Experimental evidence leads to the general conclusion that class A metals (hard acids) tend to form \"N\"-bonded thiocyanate complexes, whereas class B metals (soft acids) tend to form \"S\"-bonded thiocyanate complexes. Other factors, e.g. kinetics and solubility, are sometimes involved, and linkage isomerism can occur, for example [Co(NH)(NCS)]Cl and [Co(NH)(SCN)]Cl.\nIf [SCN] is added to a solution containing iron(III) ions (Fe), a blood red solution is formed due to the formation of [Fe(SCN)(HO)].\n\nSimilarly, Co gives a blue complex with thiocyanate. Both the iron and cobalt complexes can be extracted into organic solvents like diethyl ether or amyl alcohol. This allows the determination of these ions even in strongly coloured solutions. The determination of Co(II) in the presence of Fe(III) is possible by adding KF to the solution, which forms uncoloured, very stable complexes with Fe(III), which no longer react with SCN.\n\nPhospholipids or some detergents aid the transfer of thiocyanatoiron into chlorinated solvents like chloroform and can be determined in this fashion.\n"}
{"id": "79103", "url": "https://en.wikipedia.org/wiki?curid=79103", "title": "Thoosa", "text": "Thoosa\n\nIn Greek mythology, Thoosa or Thoösa (, ) was a sea nymph whose name derives from the word \"thoos\", meaning \"swift\". She and the god Poseidon are the parents of the cyclops Polyphemus. According to Homer, Thoosa was the daughter of the sea god Phorcys, but no mother is mentioned.\n\n"}
{"id": "7299174", "url": "https://en.wikipedia.org/wiki?curid=7299174", "title": "Tomb of Genghis Khan", "text": "Tomb of Genghis Khan\n\nThe location of the tomb of Genghis Khan (died August 18th, 1227) has been the object of much speculation and research. The site remains undiscovered.\n\nGenghis Khan asked to be buried without markings or any sign. After he died, his body was returned to Mongolia and presumably to his birthplace in the Khentii Aimag, where many assume he is buried somewhere close to the Onon River. According to one legend, the funeral escort killed anyone and anything that crossed their path, in order to conceal where he was finally buried. After the tomb was completed, the slaves who built it were massacred, and then the soldiers who killed them were also killed. The Genghis Khan Mausoleum is his memorial, but not his burial site. Folklore says that a river was diverted over his grave to make it impossible to find (echoing the manner of burial of the Sumerian King Gilgamesh of Uruk or of the Visigoth leader Alaric). Other tales state that his grave was stampeded over by many horses, that trees were then planted over the site, and that the permafrost also played its part in the hiding of the burial site. The \"Erdeni Tobchi\" (1662) claims that Genghis Khan's coffin may have been empty when it arrived in Mongolia. Similarly, the \"Altan Tobchi\" (1604) maintains that only his shirt, tent and boots were buried in the Ordos (Ratchnevsky, p. 143f.).\nTurnbull (2003, p. 24) tells another legend in which the grave was re-discovered 30 years after Genghis Khan's death. According to this tale, a young camel was buried with the Khan, and the camel's mother was later found weeping at the grave of its young.\n\nMarco Polo wrote that, even by the late 13th century, the Mongols did not know the location of the tomb. \"The Secret History of the Mongols\" has the year of Genghis Khan's death but no information concerning his burial. In the \"Travels of Marco Polo\" he writes that \"It has been an invariable custom, that all the grand khans, and chiefs of the race of Genghis-khan, should be carried for interment to a certain lofty mountain named Altaï, and in whatever place they may happen to die, although it should be at the distance of a hundred days' journey, they are nevertheless conveyed thither.\"\n\nMarco Polo writes of Genghis Khan's death: \nOther sources name the area of the Burkhan Khaldun mountain as his burial site (roughly ). The area near the Burkhan Khaldun was called the Ikh Khorig, or Great Taboo. This 240 square-kilometre area was sealed off by the Mongols, with trespassing being punishable by death. Only within the last 20 years has the area been open to Western archaeologists. \n\nAccording to the tradition of the Yuan dynasty, the part of the Mongol empire that ruled over China, all the great khans of Mongols are buried around the area of Genghis Khan's tomb. The site's name in Chinese was Qinian valley (). However, the concrete location of the valley is never mentioned in any documents.\n\nThere were rumours concerning a standard containing clues to the site that had been removed by the Soviets from a Buddhist monastery in 1937, and rumours concerning a curse leading to the death of two French archaeologists (comparable to the curse of the tomb of Tamerlane, Gur-e Amir).\n\nOn 6 October 2004, Genghis Khan's palace was discovered, which may make it possible to find his burial site.\n\nAmateur archaeologist Maury Kravitz dedicated 40 years to his search for the tomb. In a 15th-century account of a French Jesuit, he found a reference to an early battle where Genghis Khan, at the time still known as Temüjin, won a decisive victory. According to this source, he selected the confluence of the Kherlen and \"Bruchi\" rivers, with Burkhan Khaldun over his right shoulder, and after his victory, Temüjin said that this place would be forever his favourite. Kravitz, convinced that Temüjin's grave would be near that battlefield, attempted to find the \"Bruchi\" river, which turned out to be unknown to cartographers. He did, however, discover a toponym; \"Baruun Bruch\" (\"West \"Bruch\"\") in the area in question and as of 2006 was conducting excavations there, roughly 100 km east of the Burkhan Khaldun (, the wider area of Bayanbulag). Maury Kravitz died in 2012, without finding the tomb.\n\nAlbert Yu-Min Lin leads an international crowdsourcing effort: The \"Valley of the Khan Project\" attempts to discover the tomb of Genghis Khan allegedly using non-invasive technology on this area. His team uses technology platforms for ground, aerial, and satellite-based remote sensing. Their protection of a region of Mongolia through investigation earned him the \"National Geographic Adventure\" magazine’s “2010 Readers Choice Adventurer of the Year.”\n\nIn January 2015 the University of California, San Diego set up a project asking anyone interested to tag potential sites of the burial through images taken from space. \n\nNew searches are being conducted by using drones.\n\nIn 2015 and 2016, two expeditions led by French archeologist Pierre-Henri Giscard, a specialist in Mongolian archeology, and Raphaël Hautefort, a specialist in scientific imaging, around the Khentii mountains (North East of Mongolia) support the theory of a tumulus at the top of the Burkhan Khaldun mountain. Their non-invasive analysis, carried out with drones, shows that the 250m-long tumulus is of human origin and probably built on the model of the Chinese imperial tombs present in Xi'an. In addition, the expedition notes that this mound is still the subject of religious rites and pilgrimages of the surrounding population. This expedition did not give rise to any scientific publication by Pierre-Henri Giscard because it was made without authorization, and without informing local authorities. Indeed, besides the fact that the access to the area around Burkhan Khaldun is strictly controlled, the sacredness of the tomb for the Mongolian government and the population makes it impossible to explore. Pierre-Henri Giscard mentioned that more details about his researches may be published posthumously.\n\n\n"}
{"id": "1386651", "url": "https://en.wikipedia.org/wiki?curid=1386651", "title": "Transite", "text": "Transite\n\nTransite originated as a brand that Johns-Manville created in 1929 for a line of asbestos-cement products, including boards and pipes. In time it became a generic term for other companies' similar asbestos-cement products, and later an even more generic term for a hard, fireproof composite material, fibre cement boards, typically used in wall construction. It can also be found in insulation, siding, roof gutters, and cement wallboard. The more prevalent transite found in wall construction and roofing tiles for example, will last anywhere from 50 years to over 100 years.\n\nThe use of asbestos to manufacture transite was phased out in the 1980s. Previously transite was made of cement, with varying amounts (12-50%) of asbestos fiber to provide tensile strength (similar to the steel in reinforced concrete), and other materials. It was frequently used for such purposes as furnace flues, roof shingles, siding, soffit and fascia panels, and wallboard for areas where fire retardancy is particularly important. It was also used in walk-in coolers made in large supermarkets in the 1960s, 1970s and even the 1980s. Other uses included roof drain piping, water piping, sanitary sewer drain piping, laboratory fume hood panels, ceiling tiles, landscape edging, and HVAC ducts. Because cutting, breaking, and machining asbestos-containing transite releases carcinogenic asbestos fibers into the air, its use has fallen out of favor. Despite asbestos-containing transite being phased out, it is still not banned in the United States; some 230,000 deaths have been attributed to it. Demolition of older buildings containing transite materials, particularly siding made from transite requires special precautions and disposal techniques to protect workers and the public.\n\nThe transite that is produced today is made without asbestos. Transite HT and Transite 1000 are currently available fiber cement boards that contain no asbestos. Instead they contain crystalline silica, which the International Agency for Research on Cancer has classified as being carcinogenic to humans (Class 1). Crystalline silica is also known to cause silicosis, a non-cancerous lung disease.\n\n"}
{"id": "3224804", "url": "https://en.wikipedia.org/wiki?curid=3224804", "title": "United Nations Humanitarian Response Depot", "text": "United Nations Humanitarian Response Depot\n\nThe United Nations Humanitarian Response Depot (UNHRD) is an international network of six humanitarian support hubs located strategically around the world, that provide supply chain solutions to the international humanitarian community. The hubs are located in Brindisi (Italy), Dubai (UAE), Accra (Ghana), Panama City (Panama), Kuala Lumpur (Malaysia) and Las Palmas (Spain).\n\nIt is managed by the World Food Programme and currently serves 86 partners, such us UN organizations, Government agencies and NGOs. It enables these partners to assist people affected by natural disasters or other complex emergencies by prepositioning vital relief items and allowing them to be dispatched rapidly to critical areas. In addition, the UNHRD network offers services and knowledge that allow various humanitarian partners to fulfill their missions rapidly and effectively.\n\nToday’s UNHRD Network dates back to an initiative by the Italian government in the mid-80s: to pre-position relief items and support equipment for humanitarian operations at its military airport facilities. Expanding on the success of this method and envisioning a global network, WFP transformed the humanitarian depot into a logistic hub for emergency preparedness and response. In line with the United Nations Reforms for better coordinated development system and more effective humanitarian structures, UNHRD enhances the efficiency and effectiveness of humanitarian assistance, with the specific mandate to ‘assist the population living in countries affected by natural disasters or complex emergencies, through a prepositioning of relief and survival items and their rapid demobilization to the affected countries.’\n\nThe original UNHRD depot was inaugurated in Brindisi in the year 2000 to replace the United Nations Supply Depot (UNSD) in Pisa, then managed by the Office of Coordination of Humanitarian Affairs (OCHA). Upon decision of the Inter Agency Standing Committee (IASC) the mandate to provide rapid and accessible stockpiling and demobilisation services to partners inside and outside the United Nations was transferred from OCHA to WFP. Subsequently, the hub was moved to the military airport at Brindisi in an effort to set up a new and robust logistics platform, available to all partners as a “shared resource”.\n\nIn 2006, based on its own requirements and that of its partners, WFP replicated the successful Brindisi model by setting up further emergency response facilities in strategic locations worldwide, creating a network of HRDs in Africa, the Middle East, South East Asia and Latin America. Each of these locations has been selected to provide easy access to an airport, port and road systems, making access to a wide range of transportation methods available and allowing for consistently low response times of 24-48 hours.\n\nWhen Governments, UN agencies and NGOs look to respond quickly and efficiently to a disaster, they call on emergency supplies that are immediately available in UNHRD warehouses. By prepositioning relief items, the humanitarian community can support affected people at the very beginning of an emergency, often saving lives within the first 24 – 48 hours.\nUNHRD offers a range of supply chain solutions to relief organizations around the globe, acting as a ‘one-stop shop’ to its partners for storage, procurement, transport, handling, stock borrowing, technical field assistance and training centre facilities.\n\nOriginally, five humanitarian organisations relied on the UNHRD Network, including WFP itself. This number has since grown to 86 in 2017. Partner organisations that join the network are diverse, spanning UN agencies, governmental and non-governmental organisations.\n\nIn 2017, the UNHRD Network managed 575 shipments in 95 countries supporting 36 partners in responding to humanitarian emergencies, including Hurricane Irma, Rohingya Refugees Crisis in Bangladesh and South Sudanes Refugee Crisis in Uganda.\n\n\n"}
