{"id": "13244943", "url": "https://en.wikipedia.org/wiki?curid=13244943", "title": "Acerinox accident", "text": "Acerinox accident\n\nThe Acerinox accident was an incident of radioactive contamination in Cádiz (Spain). In May 1998, a caesium-137 source managed to pass through the monitoring equipment in an Acerinox scrap metal reprocessing plant in Los Barrios, Spain. When melted, the caesium-137 caused the release of a radioactive cloud. The Acerinox chimney detectors failed to detect it, but it was eventually detected in France, Italy, Switzerland, Germany and Austria. The radioactive levels measured were up to 1000 times higher than normal.\n\nThe accident contaminated the scrap metal reprocessing plant, plus two other steel mills where it sent its waste for decontamination. According to independent laboratories, the ashes produced by the Acerinox factory had between 640 and 1420 becquerels per gram (the Euratom norm is 10 Bq/g), high enough to be a threat to the public. \n\nOn the radiological consequences of this event, six people were exposed to slight levels of caesium-137 contamination. The estimated total costs for clean-up, waste storage, and lost production in the factory were around 26 million US dollars (most of it due to the lost production).\n\n\n"}
{"id": "26111274", "url": "https://en.wikipedia.org/wiki?curid=26111274", "title": "Aerolíneas Argentinas Flight 707", "text": "Aerolíneas Argentinas Flight 707\n\nAerolíneas Argentinas Flight 707 was an international Asunción–Formosa–Corrientes–Rosario–Buenos Aires passenger service operated with an Avro 748-105 Srs. 1, registration LV-HGW, named \"Ciudad de Bahía Blanca\", that crashed on 4 February 1970 near the city of Loma Alta, Chaco, Argentina.\n\nWhile en route on its third leg between Camba Puntá Airport and Islas Malvinas International Airport the aircraft flew into a cumulonimbus cloud; the pilots lost control of the aircraft after it encountered severe turbulence, and it crashed into the ground. All 37 occupants of the aircraft —33 of them passengers— perished in the accident.\n\nLoss of control of the airplane and collision with terrain when encountering a zone with adverse meteorological conditions and severe turbulence.\n\n\n"}
{"id": "41938141", "url": "https://en.wikipedia.org/wiki?curid=41938141", "title": "Aluminum polymer composite", "text": "Aluminum polymer composite\n\nAn aluminum polymer composite (APC) material combines aluminum with a polymer to create materials with interesting characteristics. In 2014 researchers used a 3d laser printer to produce a polymer matrix. When coated with a 50-100 nanometer layer of aluminum oxide, the material was able to withstand loads of as much as 280 megapascals, stronger than any other known material whose density was less than , that of water.\n\nSpherical aluminum foam pieces bonded by polymers produced foams that were 80-95% metal. Such foams were test=manufactured on an automated assembly line and are under consideration as automobile parts.\n\nExperimentally determined thermal conductivity of specific APCs matched both the Agari and Bruggeman models provide a good estimation for thermal conductivity. The experimental values of both thermal conductivity and diffusivity have shown a better heat transport for the composite filled with large particles.\n\n"}
{"id": "6077244", "url": "https://en.wikipedia.org/wiki?curid=6077244", "title": "Amaranthus acanthochiton", "text": "Amaranthus acanthochiton\n\nAmaranthus acanthochiton, the greenstripe, is an annual plant species of the genus \"Amaranthus\" in the Amaranthaceae family. It is native to the southwestern United States (Arizona, New Mexico, Texas, and Utah) and northern Mexico (Chihuahua), growing at elevations of 1000–2000 m where it is uncommon.\n\nIt is a dioecious plant growing to 10–80 cm tall. The leaves are slender, 2–8 cm long and 2–12 mm broad. The flowers are pale green, produced in dense terminal spikes. The seeds are brown, 1-1.3 mm diameter, contained in a 2-2.5 mm achene.\n\nIt is critically endangered in Utah, and endangered in Arizona (though no status has been set).\n\nThe seeds and young leaves were used by the Hopi Indians as a food source. The seeds were cooked as a form of porridge, while the leaves were used as greens.\n\n"}
{"id": "34013579", "url": "https://en.wikipedia.org/wiki?curid=34013579", "title": "Argonectes", "text": "Argonectes\n\nArgonectes is a genus of halftooths from tropical South America.\n\nThere are currently two recognized species in this genus:\n"}
{"id": "5023057", "url": "https://en.wikipedia.org/wiki?curid=5023057", "title": "Arsenic triselenide", "text": "Arsenic triselenide\n\nArsenic triselenide (AsSe) is an inorganic chemical compound, a selenide of arsenic.\n\nAmorphous arsenic triselenide is used as a chalcogenide glass for infrared optics, as it transmits light with wavelengths between 870 nm and 17.2 µm [reference needed].\n\nArsenic triselenide is covalently bonded. Even so, the arsenic has a formal oxidation state of +3.\n\nThin film selenide glasses have emerged as an important material for integrated photonics due to its high refractive index, mid-IR transparency and high non-linear optical indices. High-quality AsSe glass films can be deposited from spin coating method from ethylenediamine solutions.\n\nArsenic triselenide should be kept away from strong bases in contact with aluminium or zinc and strong acids.\n"}
{"id": "20233807", "url": "https://en.wikipedia.org/wiki?curid=20233807", "title": "Awhitu Wind Farm", "text": "Awhitu Wind Farm\n\nThe Awhitu Wind Farm is a renewable energy project in New Zealand initially planned by Tilt Renewables. The development is proposed to be located on the Awhitu Peninsula near Waiuku on the west coast south of Auckland. As of 2016 the project was being privately pursued.\n\nThe project was initially developed by Genesis Energy with plans for up to 18 wind turbines with a capacity of up to 25 MW. Maximum height to the tip of the blades was 90m.\n\nIn 2004, the wind farm received carbon credits from the New Zealand government, under a scheme to promote renewable energy for electricity generation. However, local councils denied the application for resource consents. This project was notable in being one of the few power projects to be denied resource consents.\n\nGenesis Energy appealed to the Environment Court, which, in 2005, granted the resource consents until the year 2015, overturning the decision of the local councils. Genesis subsequently sold development rights to a landowner, who approached Trustpower to progress the project. In 2016, Tilt Renewables demerged from Trustpower. By 2016 the project was being pursued by private development.\n\n"}
{"id": "32748130", "url": "https://en.wikipedia.org/wiki?curid=32748130", "title": "Biomass Energy Centre", "text": "Biomass Energy Centre\n\nBiomass Energy Centre is a biomass fired CHP power station located in the town of Chilton in County Durham. Opened in 2011, the plant was developed, and then owned and operated by Veolia Energy-Dalkia.\n\nIn May 2009, Veolia Energy-Dalkia, who had a base in nearby Cramlington, Northumberland and had previously provided cogeneration (CHP) plants for the Newcastle General, Freeman and Royal Victoria Infirmary hospitals in Newcastle, announced their plans to build a biomass fueled CHP power station in the County Durham town of Chilton. The site chosen had previously been occupied by a feed mill and silo, which had been highly visible throughout the town and had come to be known locally as \"Chilton Cathedral\". The new plant was expected to cost £40 million, half of which was to be spent within the North East region, on materials and components for the plant, aiding the local economy.\n\nChilton Town Council voted unanimously in support of the plant, saying it would be a catalyst for the regeneration of the area, and this was supported by a 340 name petition from the local people. Planning permission was granted later that June and construction commenced in January 2010. 50 jobs were created during the construction of the plant, with 17 permanent jobs created following completion. Commissioning of the plant began in June 2011, taking three to four weeks to complete.\n\n115,000 to 120,000 tonnes of wood are burned in the station per year, providing electricity for up to 20,000 homes through the National Grid. The plant is one of the first in the UK to burn domestic waste wood to provide electricity for the National Grid, rather than for an industrial site. It is the first of a number of similar sites which Dalkia are planning in the UK.\n\nThe feedstock for the plant is wood which has reached the end of its useful life such as old shipping pallets, manufacturing offcuts, wood from the construction and demolition industries, and material from civic amenity sites. This wood is collected through a network set up by Dalkia The fuel would other wise be sent to landfill.\n\n50,000 tonnes of wood pellets are produced on site each year, for use elsewhere.\n\nThe plant has a generating capacity of 17.5 megawatts.\n\n"}
{"id": "24295719", "url": "https://en.wikipedia.org/wiki?curid=24295719", "title": "Blood in the Mobile", "text": "Blood in the Mobile\n\nBlood in the Mobile is a 2010 documentary film by Danish film director Frank Piasecki Poulsen. The film addresses the issue of conflict minerals by examining illegal cassiterite mining in the North-Kivu province in eastern DR Congo. In particular, it focuses on the cassiterite mine in Bisie.\n\nThe film is co-financed by Danish, German, Finnish, Hungarian and Irish television, as well as the Danish National film board.\n\nThe film premiered in Denmark on September 1, 2010. During the making of the film Frank Piasecki Poulsen is working with communications professional and new media entrepreneur Mikkel Skov Petersen on the online campaign of the same name.\n\nThe campaign is addressing Poulsen and Petersens notion of the responsibility of the manufacturers of mobile phones on the situation in war torn eastern Congo. The project is collaborating with NGOs like Dutch-based Make It Fair and British-based Global Witness who are also engaged in changing the conduct of Western companies regarding the industrial use of minerals of unknown origin.\n\nThe cassiterite dug out in the illegal mines in North-Kivu is according to Danish corporate monitor organization Danwatch primarily purchased as tin by the electronics industry after processing in East Asia.\n\nApart from trying to raise awareness of the issue of illegal mining and alleged lack of corporate social responsibility from the mobile phone industry, the campaign is an attempt to experiment with new ways of building an audience and create additional funding for documentary films.\n\nThe production of the film and the campaign is run in association with Danish new media company Spacesheep, founded in 2009 by Poulsen and Petersen in association with major Danish independent TV and film production company Koncern.\n\n"}
{"id": "23774943", "url": "https://en.wikipedia.org/wiki?curid=23774943", "title": "Centre for Environmental Studies", "text": "Centre for Environmental Studies\n\nThe Centre for Environmental Studies (CES) was an environmental think-tank in the United Kingdom. It was established in 1967 by the second Wilson government as an independent charitable trust for the purpose of advancing education and research in the planning and design of the physical environment. It began with $750,000 funding from the Ford Foundation and grants from the British government.\n\nThe first director was A. H. Chilver, Professor of Civil Engineering in the University of London. The centre began to assess the needs for research in planning the environment for human living, and to consider how and where this research could most effectively be undertaken.\n\nFunding for the centre was withdrawn by the Thatcher government in 1979. The centre continued for some years as an independent body, closing in the 1980s.\n"}
{"id": "45256982", "url": "https://en.wikipedia.org/wiki?curid=45256982", "title": "Chlorotrifluorosilane", "text": "Chlorotrifluorosilane\n\nChlorotrifluorosilane is an organic gaseous compound with formula SiClF composed of silicon, fluorine and chlorine.\n\nBy heating a mixture of anhydrous aluminium chloride and sodium hexafluorosilicate to between 190 and 250°C a mixture of gases containing chlorotrifluorosilane is given off. These are condensed at -196°C degrees and fractionally distilled at temperatures up to -78°C.\n\nSiClF can be made by reacting silicon tetrachloride and silicon tetrafluoride gases at 600°C, producing a mixture of fluorochloro silanes including about one quarter SiClF.\n\nSiClF can be made by reacting silicon tetrachloride with antimony trifluoride. An antimony pentachloride catalyst assists. The products are distilled to separate it out from tetrafluorosilane and dichlorodifluorosilane.\n\nAt high temperatures above 500°C silicon tetrafluoride can react with phosphorus trichloride to yield some SiClF. This is unusual because SiF is very stable.\n\nSilicon tetrachloride can react with trifluoro(trichloromethyl)silane to yield SiClF and CClSiCl.\n\n2-Chloroethyltrifluorosilane or 1,2-dichloroethyltrifluorosilane can be disassociated by an infrared laser to yield SiClF and CH (ethylene) or vinyl chloride. By tuning the laser to a vibration frequency of a particular isotope of silicon, different isotomers can be selectively broken up in order to have a product that only concentrates one isotope of silicon. So silicon-30 can be increased to 80% by using the 934.5 cm line in a CO laser.\n\nThe first published preparation of SiClF by Schumb and Gamble was by exploding hexafluorodisilane in chlorine: SiF + Cl → 2SiClF. Other products of this explosion may include amorphous silicon, SiClF and SiF.\n\nChlorine reacts with silicon tetrafluoride in the presence of aluminium chips at 500-600°C to make mostly silicon tetra chloride and some SiClF.\n\nMercuric chloride when heated with SiFCo(CO) breaks the bond to form a 90% yield of SiClF.\n\nThe combination of SiF and dimethylchlorophosphine and yield some SiClF.\n\nTrifluorosilane SiHF reacts with gaseous chlorine to yield SiClF and HCl.\n\nBond length for Si–Cl is 1.996 Å and for Si–F is 1.558 Å. The bond angle ∠FSiCl = 110.2° and ∠FSiF = 108.7°. The bond length between silicon and chlorine is unusually short, indicating a 31% double bond. This can be explained by the more ionic fluoride bonds withdrawing some charge allowing a partial positive charge on the chlorine.\n\nThe molecular dipole moment is 0.636 Debye.\n\nBetween 129.18 and 308.83 K the vapour pressure in mm Hg at temperature T in K is given by log P = 102.6712 -2541.6/T -43.347 log T + 0.071921T -0.000045231 T.\n\nThe heat of formation of chlorotrifluorosilane is -315.0 kcal/mol at 298K.\n\nChlorotrifluorosilane is hydrolysed by water to produce silica.\n\nChlorotrifluorosilane reacts with trimethylstannane ((CH)SnH) at room temperature to make trifluorosilane in about 60 hours.\n\nProposed uses include a dielectric gas with a high breakdown voltage, and low global warming potential, a precursor for making fluorinated silica soot, and a vapour deposition gas.\nChlorotrifluorosilane can form an addition compound with pyridine with formula SiClF.2py (py=pyridine) An addition compound with trimethylamine exists. This addition compound is made by mixing trimethylamine vapour with Chlorotrifluorosilane and condensing out a solid at -78°C. If this was allowed to soak in trimethylamine liquid for over eight hours, a diamine complex formed (2MeN·SiClF). At 0° the disassociation pressure of the monoamine complex was 23 mm Hg.\n\nSiClF is a trigonal bipyramidal shape with a Cl and F atom on the axis. It is formed when gamma rays hit the neutral molecule.\n\nChlorotetrafluorosilicate (IV) (SiClF) can form a stable a pale yellow crystalline compound tetraethylammonium tlorotetrafluorosilicate.\n\n"}
{"id": "23877012", "url": "https://en.wikipedia.org/wiki?curid=23877012", "title": "Coefficient of moment", "text": "Coefficient of moment\n\nThe coefficients used for moment are similar to coefficients of lift, drag, and thrust, and are likewise dimensionless; however, these must include a characteristic length, in addition to the area; the span is used for rolling or yawing moment, and the chord is used for pitching moment.\n\n100%"}
{"id": "26467469", "url": "https://en.wikipedia.org/wiki?curid=26467469", "title": "Dark Matter Time Projection Chamber", "text": "Dark Matter Time Projection Chamber\n\nThe Dark Matter Time Projection Chamber (DMTPC) is an experiment for direct detection of weakly interacting massive particles (WIMPs), one of the most favored candidates for dark matter. The experiment uses a low-pressure time projection chamber in order to extract the original direction of potential dark matter events. The collaboration includes physicists from the Massachusetts Institute of Technology (MIT), Boston University (BU), and Brandeis University. Several prototype detectors have been built and tested in laboratories at MIT and BU. The group took its first data in an underground laboratory at the Waste Isolation Pilot Plant (WIPP) site near Carlsbad, New Mexico in Fall, 2010.\n\nThe DMTPC detector consists of a TPC filled with low pressure CF gas. Charged particles incident on the gas are slowed and eventually stopped, leaving a trail of free electrons and ionized molecules. The electrons are drifted by an electric field toward an amplification region. Instead of using MWPC endplates for amplification and event readout, as in the traditional TPC design, the DMTPC amplification region consists of a metal wire mesh separated from a copper anode with a high electric field between them. This creates a more uniform electric field in order to preserve the shape of the original track during amplification. The avalanche of electrons also creates a great deal of scintillation light, which passes through the wire mesh. Some of this light is collected by a CCD camera located outside the main detector volume. This results in a two dimensional image of the ionization signal of the track as it appeared on the amplification plane. Information about the charged particle, including its direction of motion within the detector, can be reconstructed from the CCD readout. Additional track information is obtained from readout of the charge signal on the anode plane.\nThe largest existing prototype detectors each have a total of 20 L of CF gas within the drift region, where measurable events will occur. The group also plans to eventually construct a detector with a volume of 1 m.\n\nIn a proposed dark matter event, a WIMP enters the detector volume and interacts with one of the atoms in the CF, typically fluorine. While the WIMP does not directly leave a track, the momentum transfer of the interaction causes the atom to recoil, and its ionization track, with a typical range of a few millimeters, may be detected. CF gas is used because the most common fluorine isotope, F, is believed to be an excellent target nucleus for setting spin-dependent WIMP-nucleon scattering. If the recoiling ion is energetic enough, the direction of the incoming WIMP may be extrapolated from the direction of the recoil.\n\nBecause of the motion of the solar system around the center of the galaxy, many physicists believe that the particles comprising the dark matter halo will appear to originate from a particular direction in the sky roughly corresponding to the position of the constellation Cygnus. If this is true, the DMTPC group hope to be able to use the directional track information to statistically confirm the existence of dark matter, even in the presence of non-dark matter backgrounds which are believed to have a different directional signal. Several other groups developing low pressure TPC dark matter detectors with directional sensitivity exist, including DRIFT, NEWAGE, and MIMAC. Additionally, dark matter searches such as COUPP and NEWAGE also use fluorine as the principal target nucleus for spin-dependent interactions.\n\nDMTPC published first results from a surface run in 2010, setting a spin-dependent cross section limit.\n\n\n\n"}
{"id": "1367595", "url": "https://en.wikipedia.org/wiki?curid=1367595", "title": "Elemental analysis", "text": "Elemental analysis\n\nElemental analysis is a process where a sample of some material (e.g., soil, waste or drinking water, bodily fluids, minerals, chemical compounds) is analyzed for its elemental and sometimes isotopic composition. Elemental analysis can be qualitative (determining what elements are present), and it can be quantitative (determining how much of each are present). Elemental analysis falls within the ambit of analytical chemistry, the set of instruments involved in deciphering the chemical nature of our world.\n\nFor organic chemists, elemental analysis or \"EA\" almost always refers to CHNX analysis—the determination of the mass fractions of carbon, hydrogen, nitrogen, and heteroatoms (X) (halogens, sulfur) of a sample. This information is very important to help determine the structure of an unknown compound, as well as to help ascertain the structure and purity of a synthesized compound. In present day organic chemistry spectroscopic technics (like NMR, both H and C), mass spectrometry and chromatographic procedures have replaced EA as the primary technique for structural determination, although it still gives very useful complementary information. It is also the fastest and most inexpensive method to determine sample purity.\n\nAntoine Lavoisier is regarded as the inventor of elemental analysis as a quantitate, experimental tool to assess the chemical composition of a compound. At the time elemental analysis was based on gravimetric determination of specific adsorbant materials before and after selective adsorption of the combustion gases. Today fully automated systems based on thermal conductivity or infrared spectroscopy detection of the combustion gases, or other spectroscopic methods are used.\n\nThe most common form of elemental analysis, CHNS analysis, is accomplished by combustion analysis. In this technique, a sample is burned in an excess of oxygen and various traps, collecting the combustion products: carbon dioxide, water, and nitric oxide. The masses of these combustion products can be used to calculate the composition of the unknown sample. Modern elemental analyzers are also capable of simultaneous determination of sulfur along with CHN in the same measurement run.\n\nQuantitative analysis is the determination of the mass of each element or compound present. Other quantitative methods include:\n\nTo qualitatively determine which elements exist in a sample, the methods are:\n\nThe analysis of results is performed by determining the ratio of elements from within the sample, and working out a chemical formula that fits with those results. This process is useful as it helps determine if a sample sent is a desired compound and confirms the purity of a compound. The accepted deviation of elemental analysis results from the calculated is 0.3%.\n\n"}
{"id": "49300117", "url": "https://en.wikipedia.org/wiki?curid=49300117", "title": "Energy efficient clay brick project", "text": "Energy efficient clay brick project\n\nFunded by the Swiss Agency for Development and Cooperation’s (SDC) Global Programme Climate Change, and implemented by Swisscontact, the Energy Efficient Clay Brick project (EECB) aims at increasing energy efficiency and reducing greenhouse gas emission in the South African clay brick sector. The project represents phase II of the 4 year Vertical Shaft Brick Kiln (VSBK) project and was launched in November 2013 with a duration of another 4 years.\n\nThe Swiss Agency for Development and Cooperation (SDC) funded “Energy Efficient Clay Brick (EECB)” Project aims to foster sustained and broad-based Energy Efficiency (EnE) in the Clay Brick Sector, by targeting the entire market system. \nBased on this market system approach, the EECB Project – which has evolved from and forms the second phase of the Vertical Shaft Brick Kiln Project (2009 – 2013) – invests in a diverse set of stakeholders within the Clay Brick Sector of South Africa, by supporting and capacitating clay brick makers and industry service providers in adopting a systematic approach to energy management. The Project not only contributes to technology transfer, but also addresses sector constraints within the enabling environment as well as the demand levels of the clay brick market system.\n\nThe overall objective of the EECB Project is to facilitate EnE within the Clay Brick Sector, reducing CO emissions and thereby contributing towards the country’s overall targets in increasing EnE and lowering carbon emissions. By October 2017, the EECB Project expects to reach 50 producers who reduce their CO emissions by 320,000 tCO annually and reach a 10 percent reduction in overall input energy.\n\nOn the supply side, the project engages actively with clay brick producers and related service providers, equipping them with the necessary capacity and knowledge needed to shift to more energy efficient production practices including the Vertical Shaft Brick Kiln (VSBK) and Habla Zig Zag Kiln (HZZK) technologies. With regards to a more enabling environment, EECB facilitates a close cooperation between private and public stakeholders and facilitates the access to specific financial products and funding schemes.\n\nOn the demand side of the bricks sub-sector, the project promotes the use of energy efficient clay brick among the building sector in an effort to grow market share and create a pull effect for the main drivers of change - the buyers.\n\nSince its implementation in November 2013, the EECB has gained recognition amongst the South African Clay Brick Sector through its close partnership with the South African Clay Brick Association (CBA) as well as the Energy Efficiency Sector.\n\nNotable Successes\n\n"}
{"id": "3687862", "url": "https://en.wikipedia.org/wiki?curid=3687862", "title": "Equivalence of direct radiation", "text": "Equivalence of direct radiation\n\nEquivalence of direct radiation (EDR) is a standardized comparison method for estimating the output ability of space-heating radiators and convectors.\n\nMeasured in square feet, the reference standard for EDR is the mattress radiator invented by Stephen J. Gold in the mid 19th century.\n\nOne square foot of EDR is able to liberate 240 BTU per hour when surrounded by air and filled with steam of approximately temperature and 1 psi of pressure.\n\nEDR was originally a measure of the actual surface area of radiators. As radiator (and later convector) design became more complicated and compact, the relationship of actual surface area to EDR became arbitrary. Laboratory methods based on the condensation of steam allowed for very accurate measurements.\n\nWhile now somewhat archaic, EDR is still computed and used for sizing steam boilers and radiators, and for modifying and troubleshooting older heating systems using steam or hot water. \n\n"}
{"id": "4482505", "url": "https://en.wikipedia.org/wiki?curid=4482505", "title": "Frascati Tokamak Upgrade", "text": "Frascati Tokamak Upgrade\n\nThe Frascati Tokamak Upgrade is a tokamak operating at Frascati, Italy. Building on the Frascati Tokamak experiment, FTU is a compact, high-magnetic-field tokamak (B = 8 T). It began operation in 1990 and has since achieved operating goals of 1.6 MA at 8 T and average electron density greater than 4 per cubic meter. \nThe poloidal section of FTU is circular, with a limiter.\n\n"}
{"id": "2407249", "url": "https://en.wikipedia.org/wiki?curid=2407249", "title": "Geitonogamy", "text": "Geitonogamy\n\nGeitonogamy (from Greek \"geiton\" (γείτων) = neighbor + \"gamein\" (γαμεῖν) = to marry) is a type of self-pollination. Geitonogamous pollination is sometimes distinguished from the fertilizations that can result from it, geitonogamy. If a plant is self-incompatible, geitonogamy can reduce seed production.\n\nGeitonogamy is when pollen is exported using a vector (pollinator or wind) out of one flower but only to another flower on the same plant. It is a form of self-fertilization.\n\nIn flowering plants, pollen is transferred from a flower to another flower on the same plant, and in animal pollinated systems this is accomplished by a pollinator visiting multiple flowers on the same plant. Geitonogamy is also possible within species that are wind-pollinated, and may actually be a quite common source of self-fertilized seeds in self-compatible species. It also occurs in monoecious gymnosperms. Although geitonogamy is functionally cross-pollination involving a pollinating agent, genetically it is similar to autogamy since the pollen grains come from the same plant.\n\nMonoecious plants like maize show geitonogamy. Geitonogamy is not possible for strictly dioecious plants.\n\n"}
{"id": "323207", "url": "https://en.wikipedia.org/wiki?curid=323207", "title": "George Constantinescu", "text": "George Constantinescu\n\nGeorge \"Gogu\" Constantinescu (, first name's diminutive is Gogu, last name also Constantinesco; 4 October 1881 – 11 December 1965) was a Romanian scientist, engineer and inventor. During his career, he registered over 130 inventions. He is the creator of the \"theory of sonics\", a new branch of continuum mechanics, in which he described the transmission of mechanical energy through vibrations.\n\nBorn in Craiova in \"the Doctor's House\" near the Mihai Bravu Gardens, he was influenced by his father George, born in 1844 (a professor of mathematics and engineering science, specialized in mathematics at the Sorbonne University). Gogu Constantinescu settled in the United Kingdom in 1912. He was an honorary member of the Romanian Academy.\n\nHe married Alexandra (Sandra) Cocorescu in Richmond, London, in December 1914. The couple moved to Wembley and, after their son Ian was born, they moved to Weybridge. The marriage broke down in the 1920s and ended in divorce. He then married Eva Litton and the couple moved to Oxen House, beside Lake Coniston. Eva had two children, Richard and Michael, by a previous marriage.\n\nHis hydraulic machine gun synchronization gear allowed airplane-mounted guns to shoot between the spinning blades of the propeller. The Constantinesco synchronization gear (or \"\"CC\" gear\") was first used operationally on the D.H.4s of No. 55 squadron R.F.C. from March 1917, during World War I, and rapidly became standard equipment, replacing a variety of mechanical gears. It continued to be used by the Royal Air Force until World War II – the Gloster Gladiator being the last British fighter to be equipped with \"CC\" gear.\n\nIn 1918, he published the book \"A treatise on transmission of power by vibrations\" in which he described his Theory of sonics. The theory is applicable to various systems of power transmission but has mostly been applied to hydraulic systems. Sonics differs from hydrostatics, being based on waves, rather than pressure, in the liquid. Constantinescu argued that, contrary to popular belief, liquids are compressible. Transmission of power by waves in a liquid (e.g. water or oil) required a generator to produce the waves and a motor to use the waves to do work, either by percussion (as in rock drills) or by conversion to rotary motion.\n\nHe had several patents for improvements to carburetors, for example US1206512. He also devised a hydraulic system (patent GB133719) for operating both the valves and the fuel injectors for diesel engines.\n\nHe invented a mechanical torque converter actuated by a pendulum. This was applied to the \"Constantinesco\", a French-manufactured car. It was also tried on rail vehicles. A 250 hp petrol engined locomotive with a Constantinescu torque converter was exhibited at the 1924 Wembley Exhibition. The system was not\nadopted on British railways but it was applied to some railcars on the Romanian State Railways.\n\nOther inventions included a \"railway motor wagon\". The latter ran on normal flanged steel wheels but the drive used a road vehicle powertrain with rubber tyres pressed against the rails. This is similar to the system used on many modern road-rail vehicles. He also designed the Constanţa Mosque (a project completed by the architect Victor Ştefănescu).\n\nResearch on a \"sonic asynchronous motor for vehicle applications\" (based on Constantinescu's work) has been done at the Transilvania University of Brașov. The date of the paper is believed to be 5 October 2010.\n\nHe died at Oxen House, beside Coniston Water on 11/12 December 1965, and is buried in the churchyard at Lowick, Cumbria.\n\nThe Dimitrie Leonida Technical Museum in Bucharest has exhibits relating to George Constantinescu.\n\n"}
{"id": "13255", "url": "https://en.wikipedia.org/wiki?curid=13255", "title": "Hydrogen", "text": "Hydrogen\n\nHydrogen is a chemical element with symbol H and atomic number 1. With a standard atomic weight of , hydrogen is the lightest element on the periodic table. Its monatomic form (H) is the most abundant chemical substance in the Universe, constituting roughly 75% of all baryonic mass. Non-remnant stars are mainly composed of hydrogen in the plasma state. The most common isotope of hydrogen, termed \"protium\" (name rarely used, symbol H), has one proton and no neutrons.\n\nThe universal emergence of atomic hydrogen first occurred during the recombination epoch. At standard temperature and pressure, hydrogen is a colorless, odorless, tasteless, non-toxic, nonmetallic, highly combustible diatomic gas with the molecular formula H. Since hydrogen readily forms covalent compounds with most nonmetallic elements, most of the hydrogen on Earth exists in molecular forms such as water or organic compounds. Hydrogen plays a particularly important role in acid–base reactions because most acid-base reactions involve the exchange of protons between soluble molecules. In ionic compounds, hydrogen can take the form of a negative charge (i.e., anion) when it is known as a hydride, or as a positively charged (i.e., cation) species denoted by the symbol H. The hydrogen cation is written as though composed of a bare proton, but in reality, hydrogen cations in ionic compounds are always more complex. As the only neutral atom for which the Schrödinger equation can be solved analytically, study of the energetics and bonding of the hydrogen atom has played a key role in the development of quantum mechanics.\n\nHydrogen gas was first artificially produced in the early 16th century by the reaction of acids on metals. In 1766–81, Henry Cavendish was the first to recognize that hydrogen gas was a discrete substance, and that it produces water when burned, the property for which it was later named: in Greek, hydrogen means \"water-former\".\n\nIndustrial production is mainly from steam reforming natural gas, and less often from more energy-intensive methods such as the electrolysis of water. Most hydrogen is used near the site of its production, the two largest uses being fossil fuel processing (e.g., hydrocracking) and ammonia production, mostly for the fertilizer market. Hydrogen is a concern in metallurgy as it can embrittle many metals, complicating the design of pipelines and storage tanks.\n\nHydrogen gas (dihydrogen or molecular hydrogen, also called diprotium when consisting specifically of a pair of protium atoms) is highly flammable and will burn in air at a very wide range of concentrations between 4% and 75% by volume. The enthalpy of combustion is −286 kJ/mol:\n\nHydrogen gas forms explosive mixtures with air in concentrations from 4–74% and with chlorine at 5–95%. The explosive reactions may be triggered by spark, heat, or sunlight. The hydrogen autoignition temperature, the temperature of spontaneous ignition in air, is . Pure hydrogen-oxygen flames emit ultraviolet light and with high oxygen mix are nearly invisible to the naked eye, as illustrated by the faint plume of the Space Shuttle Main Engine, compared to the highly visible plume of a Space Shuttle Solid Rocket Booster, which uses an ammonium perchlorate composite. The detection of a burning hydrogen leak may require a flame detector; such leaks can be very dangerous. Hydrogen flames in other conditions are blue, resembling blue natural gas flames.\n\nThe destruction of the Hindenburg airship was a notorious example of hydrogen combustion and the cause is still debated. The visible orange flames in that incident were the result of a rich mixture of hydrogen to oxygen combined with carbon compounds from the airship skin.\n\nH reacts with every oxidizing element. Hydrogen can react spontaneously and violently at room temperature with chlorine and fluorine to form the corresponding hydrogen halides, hydrogen chloride and hydrogen fluoride, which are also potentially dangerous acids.\n\nThe ground state energy level of the electron in a hydrogen atom is −13.6 eV, which is equivalent to an ultraviolet photon of roughly 91 nm wavelength.\n\nThe energy levels of hydrogen can be calculated fairly accurately using the Bohr model of the atom, which conceptualizes the electron as \"orbiting\" the proton in analogy to the Earth's orbit of the Sun. However, the atomic electron and proton are held together by electromagnetic force, while planets and celestial objects are held by gravity. Because of the discretization of angular momentum postulated in early quantum mechanics by Bohr, the electron in the Bohr model can only occupy certain allowed distances from the proton, and therefore only certain allowed energies.\n\nA more accurate description of the hydrogen atom comes from a purely quantum mechanical treatment that uses the Schrödinger equation, Dirac equation or even the Feynman path integral formulation to calculate the probability density of the electron around the proton. The most complicated treatments allow for the small effects of special relativity and vacuum polarization. In the quantum mechanical treatment, the electron in a ground state hydrogen atom has no angular momentum at all—illustrating how the \"planetary orbit\" differs from electron motion.\nThere exist two different spin isomers of hydrogen diatomic molecules that differ by the relative spin of their nuclei. In the orthohydrogen form, the spins of the two protons are parallel and form a triplet state with a molecular spin quantum number of 1 (+); in the parahydrogen form the spins are antiparallel and form a singlet with a molecular spin quantum number of 0 (–). At standard temperature and pressure, hydrogen gas contains about 25% of the para form and 75% of the ortho form, also known as the \"normal form\". The equilibrium ratio of orthohydrogen to parahydrogen depends on temperature, but because the ortho form is an excited state and has a higher energy than the para form, it is unstable and cannot be purified. At very low temperatures, the equilibrium state is composed almost exclusively of the para form. The liquid and gas phase thermal properties of pure parahydrogen differ significantly from those of the normal form because of differences in rotational heat capacities, as discussed more fully in \"spin isomers of hydrogen\". The ortho/para distinction also occurs in other hydrogen-containing molecules or functional groups, such as water and methylene, but is of little significance for their thermal properties.\n\nThe uncatalyzed interconversion between para and ortho H increases with increasing temperature; thus rapidly condensed H contains large quantities of the high-energy ortho form that converts to the para form very slowly. The ortho/para ratio in condensed H is an important consideration in the preparation and storage of liquid hydrogen: the conversion from ortho to para is exothermic and produces enough heat to evaporate some of the hydrogen liquid, leading to loss of liquefied material. Catalysts for the ortho-para interconversion, such as ferric oxide, activated carbon, platinized asbestos, rare earth metals, uranium compounds, chromic oxide, or some nickel compounds, are used during hydrogen cooling.\n\n\nWhile H is not very reactive under standard conditions, it does form compounds with most elements. Hydrogen can form compounds with elements that are more electronegative, such as halogens (e.g., F, Cl, Br, I), or oxygen; in these compounds hydrogen takes on a partial positive charge. When bonded to fluorine, oxygen, or nitrogen, hydrogen can participate in a form of medium-strength noncovalent bonding with the hydrogen of other similar molecules, a phenomenon called hydrogen bonding that is critical to the stability of many biological molecules. Hydrogen also forms compounds with less electronegative elements, such as metals and metalloids, where it takes on a partial negative charge. These compounds are often known as hydrides.\n\nHydrogen forms a vast array of compounds with carbon called the hydrocarbons, and an even vaster array with heteroatoms that, because of their general association with living things, are called organic compounds. The study of their properties is known as organic chemistry and their study in the context of living organisms is known as biochemistry. By some definitions, \"organic\" compounds are only required to contain carbon. However, most of them also contain hydrogen, and because it is the carbon-hydrogen bond which gives this class of compounds most of its particular chemical characteristics, carbon-hydrogen bonds are required in some definitions of the word \"organic\" in chemistry. Millions of hydrocarbons are known, and they are usually formed by complicated synthetic pathways that seldom involve elementary hydrogen.\n\nCompounds of hydrogen are often called hydrides, a term that is used fairly loosely. The term \"hydride\" suggests that the H atom has acquired a negative or anionic character, denoted H, and is used when hydrogen forms a compound with a more electropositive element. The existence of the hydride anion, suggested by Gilbert N. Lewis in 1916 for group 1 and 2 salt-like hydrides, was demonstrated by Moers in 1920 by the electrolysis of molten lithium hydride (LiH), producing a stoichiometry quantity of hydrogen at the anode. For hydrides other than group 1 and 2 metals, the term is quite misleading, considering the low electronegativity of hydrogen. An exception in group 2 hydrides is , which is polymeric. In lithium aluminium hydride, the anion carries hydridic centers firmly attached to the Al(III).\n\nAlthough hydrides can be formed with almost all main-group elements, the number and combination of possible compounds varies widely; for example, more than 100 binary borane hydrides are known, but only one binary aluminium hydride. Binary indium hydride has not yet been identified, although larger complexes exist.\n\nIn inorganic chemistry, hydrides can also serve as bridging ligands that link two metal centers in a coordination complex. This function is particularly common in group 13 elements, especially in boranes (boron hydrides) and aluminium complexes, as well as in clustered carboranes.\n\nOxidation of hydrogen removes its electron and gives H, which contains no electrons and a nucleus which is usually composed of one proton. That is why is often called a proton. This species is central to discussion of acids. Under the Brønsted–Lowry acid–base theory, acids are proton donors, while bases are proton acceptors.\n\nA bare proton, , cannot exist in solution or in ionic crystals because of its unstoppable attraction to other atoms or molecules with electrons. Except at the high temperatures associated with plasmas, such protons cannot be removed from the electron clouds of atoms and molecules, and will remain attached to them. However, the term 'proton' is sometimes used loosely and metaphorically to refer to positively charged or cationic hydrogen attached to other species in this fashion, and as such is denoted \"\" without any implication that any single protons exist freely as a species.\n\nTo avoid the implication of the naked \"solvated proton\" in solution, acidic aqueous solutions are sometimes considered to contain a less unlikely fictitious species, termed the \"hydronium ion\" (). However, even in this case, such solvated hydrogen cations are more realistically conceived as being organized into clusters that form species closer to H. Other oxonium ions are found when water is in acidic solution with other solvents.\n\nAlthough exotic on Earth, one of the most common ions in the universe is the ion, known as protonated molecular hydrogen or the trihydrogen cation.\n\nNASA has investigated the use of atomic hydrogen as a rocket propellant. It could be stored in liquid helium to prevent it from recombining into molecular hydrogen. When the helium is vaporized, the atomic hydrogen would be released and combine back to molecular hydrogen. The result would be an intensely hot stream of hydrogen and helium gas. The liftoff weight of rockets could be reduced by 50% by this method.\n\nMost interstellar hydrogen is in the form of atomic hydrogen because the atoms can seldom collide and combine. They are the source of the important 21 cm hydrogen line in astronomy at 1420 MHz.\n\nHydrogen has three naturally occurring isotopes, denoted , and . Other, highly unstable nuclei ( to ) have been synthesized in the laboratory but not observed in nature.\n\nHydrogen is the only element that has different names for its isotopes in common use today. During the early study of radioactivity, various heavy radioactive isotopes were given their own names, but such names are no longer used, except for deuterium and tritium. The symbols D and T (instead of and ) are sometimes used for deuterium and tritium, but the corresponding symbol for protium, P, is already in use for phosphorus and thus is not available for protium. In its nomenclatural guidelines, the International Union of Pure and Applied Chemistry (IUPAC) allows any of D, T, , and to be used, although and are preferred.\n\nThe exotic atom muonium (symbol Mu), composed of an antimuon and an electron, is also sometimes considered as a light radioisotope of hydrogen, due to the mass difference between the antimuon and the electron. Muonium was discovered in 1960. During the muon's lifetime, muonium can enter into compounds such as muonium chloride (MuCl) or sodium muonide (NaMu), analogous to hydrogen chloride and sodium hydride respectively.\n\nIn 1671, Robert Boyle discovered and described the reaction between iron filings and dilute acids, which results in the production of hydrogen gas. In 1766, Henry Cavendish was the first to recognize hydrogen gas as a discrete substance, by naming the gas from a metal-acid reaction \"inflammable air\". He speculated that \"inflammable air\" was in fact identical to the hypothetical substance called \"phlogiston\" and further finding in 1781 that the gas produces water when burned. He is usually given credit for the discovery of hydrogen as an element. In 1783, Antoine Lavoisier gave the element the name hydrogen (from the Greek ὑδρο- \"hydro\" meaning \"water\" and -γενής \"genes\" meaning \"creator\") when he and Laplace reproduced Cavendish's finding that water is produced when hydrogen is burned.\n\nLavoisier produced hydrogen for his experiments on mass conservation by reacting a flux of steam with metallic iron through an incandescent iron tube heated in a fire. Anaerobic oxidation of iron by the protons of water at high temperature can be schematically represented by the set of following reactions:\n\nMany metals such as zirconium undergo a similar reaction with water leading to the production of hydrogen.\n\nHydrogen was liquefied for the first time by James Dewar in 1898 by using regenerative cooling and his invention, the vacuum flask. He produced solid hydrogen the next year. Deuterium was discovered in December 1931 by Harold Urey, and tritium was prepared in 1934 by Ernest Rutherford, Mark Oliphant, and Paul Harteck. Heavy water, which consists of deuterium in the place of regular hydrogen, was discovered by Urey's group in 1932. François Isaac de Rivaz built the first de Rivaz engine, an internal combustion engine powered by a mixture of hydrogen and oxygen in 1806. Edward Daniel Clarke invented the hydrogen gas blowpipe in 1819. The Döbereiner's lamp and limelight were invented in 1823.\n\nThe first hydrogen-filled balloon was invented by Jacques Charles in 1783. Hydrogen provided the lift for the first reliable form of air-travel following the 1852 invention of the first hydrogen-lifted airship by Henri Giffard. German count Ferdinand von Zeppelin promoted the idea of rigid airships lifted by hydrogen that later were called Zeppelins; the first of which had its maiden flight in 1900. Regularly scheduled flights started in 1910 and by the outbreak of World War I in August 1914, they had carried 35,000 passengers without a serious incident. Hydrogen-lifted airships were used as observation platforms and bombers during the war.\n\nThe first non-stop transatlantic crossing was made by the British airship \"R34\" in 1919. Regular passenger service resumed in the 1920s and the discovery of helium reserves in the United States promised increased safety, but the U.S. government refused to sell the gas for this purpose. Therefore, H was used in the \"Hindenburg\" airship, which was destroyed in a midair fire over New Jersey on 6 May 1937. The incident was broadcast live on radio and filmed. Ignition of leaking hydrogen is widely assumed to be the cause, but later investigations pointed to the ignition of the aluminized fabric coating by static electricity. But the damage to hydrogen's reputation as a lifting gas was already done and commercial hydrogen airship travel ceased. Hydrogen is still used, in preference to non-flammable but more expensive helium, as a lifting gas for weather balloons.\n\nIn the same year the first hydrogen-cooled turbogenerator went into service with gaseous hydrogen as a coolant in the rotor and the stator in 1937 at Dayton, Ohio, by the Dayton Power & Light Co.; because of the thermal conductivity of hydrogen gas, this is the most common type in its field today.\n\nThe nickel hydrogen battery was used for the first time in 1977 aboard the U.S. Navy's Navigation technology satellite-2 (NTS-2). For example, the ISS, Mars Odyssey and the Mars Global Surveyor are equipped with nickel-hydrogen batteries. In the dark part of its orbit, the Hubble Space Telescope is also powered by nickel-hydrogen batteries, which were finally replaced in May 2009, more than 19 years after launch and 13 years beyond their design life.\n\nBecause of its simple atomic structure, consisting only of a proton and an electron, the hydrogen atom, together with the spectrum of light produced from it or absorbed by it, has been central to the development of the theory of atomic structure. Furthermore, study of the corresponding simplicity of the hydrogen molecule and the corresponding cation brought understanding of the nature of the chemical bond, which followed shortly after the quantum mechanical treatment of the hydrogen atom had been developed in the mid-1920s.\n\nOne of the first quantum effects to be explicitly noticed (but not understood at the time) was a Maxwell observation involving hydrogen, half a century before full quantum mechanical theory arrived. Maxwell observed that the specific heat capacity of H unaccountably departs from that of a diatomic gas below room temperature and begins to increasingly resemble that of a monatomic gas at cryogenic temperatures. According to quantum theory, this behavior arises from the spacing of the (quantized) rotational energy levels, which are particularly wide-spaced in H because of its low mass. These widely spaced levels inhibit equal partition of heat energy into rotational motion in hydrogen at low temperatures. Diatomic gases composed of heavier atoms do not have such widely spaced levels and do not exhibit the same effect.\n\nAntihydrogen () is the antimatter counterpart to hydrogen. It consists of an antiproton with a positron. Antihydrogen is the only type of antimatter atom to have been produced .\n\nHydrogen, as atomic H, is the most abundant chemical element in the universe, making up 75% of normal matter by mass and more than 90% by number of atoms. (Most of the mass of the universe, however, is not in the form of chemical-element type matter, but rather is postulated to occur as yet-undetected forms of mass such as dark matter and dark energy.) This element is found in great abundance in stars and gas giant planets. Molecular clouds of H are associated with star formation. Hydrogen plays a vital role in powering stars through the proton-proton reaction and the CNO cycle of nuclear fusion.\n\nThroughout the universe, hydrogen is mostly found in the atomic and plasma states, with properties quite different from those of molecular hydrogen. As a plasma, hydrogen's electron and proton are not bound together, resulting in very high electrical conductivity and high emissivity (producing the light from the Sun and other stars). The charged particles are highly influenced by magnetic and electric fields. For example, in the solar wind they interact with the Earth's magnetosphere giving rise to Birkeland currents and the aurora. Hydrogen is found in the neutral atomic state in the interstellar medium. The large amount of neutral hydrogen found in the damped Lyman-alpha systems is thought to dominate the cosmological baryonic density of the Universe up to redshift \"z\"=4.\n\nUnder ordinary conditions on Earth, elemental hydrogen exists as the diatomic gas, H. However, hydrogen gas is very rare in the Earth's atmosphere (1 ppm by volume) because of its light weight, which enables it to escape from Earth's gravity more easily than heavier gases. However, hydrogen is the third most abundant element on the Earth's surface, mostly in the form of chemical compounds such as hydrocarbons and water. Hydrogen gas is produced by some bacteria and algae and is a natural component of flatus, as is methane, itself a hydrogen source of increasing importance.\n\nA molecular form called protonated molecular hydrogen () is found in the interstellar medium, where it is generated by ionization of molecular hydrogen from cosmic rays. This charged ion has also been observed in the upper atmosphere of the planet Jupiter. The ion is relatively stable in the environment of outer space due to the low temperature and density. is one of the most abundant ions in the Universe, and it plays a notable role in the chemistry of the interstellar medium. Neutral triatomic hydrogen H can exist only in an excited form and is unstable. By contrast, the positive hydrogen molecular ion () is a rare molecule in the universe.\n\n is produced in chemistry and biology laboratories, often as a by-product of other reactions; in industry for the hydrogenation of unsaturated substrates; and in nature as a means of expelling reducing equivalents in biochemical reactions.\n\nThe electrolysis of water is a simple method of producing hydrogen. A low voltage current is run through the water, and gaseous oxygen forms at the anode while gaseous hydrogen forms at the cathode. Typically the cathode is made from platinum or another inert metal when producing hydrogen for storage. If, however, the gas is to be burnt on site, oxygen is desirable to assist the combustion, and so both electrodes would be made from inert metals. (Iron, for instance, would oxidize, and thus decrease the amount of oxygen given off.) The theoretical maximum efficiency (electricity used vs. energetic value of hydrogen produced) is in the range 88-94%.\n\nWhen determining the electrical efficiency of PEM (proton exchange membrane) electrolysis, the higher heat value (HHV) is used. This is because the catalyst layer interacts with water as steam. As the process operates at 80 °C for PEM electrolysers the waste heat can be redirected through the system to create the steam, resulting in a higher overall electrical efficiency. The lower heat value (LHV) must be used for alkaline electrolysers as the process within these electrolysers requires water in liquid form and uses alkalinity to facilitate the breaking of the bond holding the hydrogen and oxygen atoms together. The lower heat value must also be used for fuel cells, as steam is the output rather than input.\n\nHydrogen is often produced using natural gas, which involves the removal of hydrogen from hydrocarbons at very high temperatures, with about 95% of hydrogen production coming from steam reforming around year 2000. Commercial bulk hydrogen is usually produced by the steam reforming of natural gas. At high temperatures (1000–1400 K, 700–1100 °C or 1300–2000 °F), steam (water vapor) reacts with methane to yield carbon monoxide and .\n\nThis reaction is favored at low pressures but is nonetheless conducted at high pressures (2.0  MPa, 20 atm or 600 inHg). This is because high-pressure is the most marketable product and pressure swing adsorption (PSA) purification systems work better at higher pressures. The product mixture is known as \"synthesis gas\" because it is often used directly for the production of methanol and related compounds. Hydrocarbons other than methane can be used to produce synthesis gas with varying product ratios. One of the many complications to this highly optimized technology is the formation of coke or carbon:\n\nConsequently, steam reforming typically employs an excess of . Additional hydrogen can be recovered from the steam by use of carbon monoxide through the water gas shift reaction, especially with an iron oxide catalyst. This reaction is also a common industrial source of carbon dioxide:\n\nOther important methods for production include partial oxidation of hydrocarbons:\n\nand the coal reaction, which can serve as a prelude to the shift reaction above:\n\nHydrogen is sometimes produced and consumed in the same industrial process, without being separated. In the Haber process for the production of ammonia, hydrogen is generated from natural gas. Electrolysis of brine to yield chlorine also produces hydrogen as a co-product.\n\nIn the laboratory, is usually prepared by the reaction of dilute non-oxidizing acids on some reactive metals such as zinc with Kipp's apparatus.\n\nAluminium can also produce upon treatment with bases:\n\nAn alloy of aluminium and gallium in pellet form added to water can be used to generate hydrogen. The process also produces alumina, but the expensive gallium, which prevents the formation of an oxide skin on the pellets, can be re-used. This has important potential implications for a hydrogen economy, as hydrogen can be produced on-site and does not need to be transported.\n\nThere are more than 200 thermochemical cycles which can be used for water splitting, around a dozen of these cycles such as the iron oxide cycle, cerium(IV) oxide–cerium(III) oxide cycle, zinc zinc-oxide cycle, sulfur-iodine cycle, copper-chlorine cycle and hybrid sulfur cycle are under research and in testing phase to produce hydrogen and oxygen from water and heat without using electricity. A number of laboratories (including in France, Germany, Greece, Japan, and the USA) are developing thermochemical methods to produce hydrogen from solar energy and water.\n\nUnder anaerobic conditions, iron and steel alloys are slowly oxidized by the protons of water concomitantly reduced in molecular hydrogen (). The anaerobic corrosion of iron leads first to the formation of ferrous hydroxide (green rust) and can be described by the following reaction:\n\nIn its turn, under anaerobic conditions, the ferrous hydroxide () can be oxidized by the protons of water to form magnetite and molecular hydrogen.\nThis process is described by the Schikorr reaction:\n\nThe well crystallized magnetite () is thermodynamically more stable than the ferrous hydroxide ().\n\nThis process occurs during the anaerobic corrosion of iron and steel in oxygen-free groundwater and in reducing soils below the water table.\n\nIn the absence of atmospheric oxygen (), in deep geological conditions prevailing far away from Earth atmosphere, hydrogen () is produced during the process of serpentinization by the anaerobic oxidation by water protons (H) of the ferrous (Fe) silicate present in the crystal lattice of fayalite (, the olivine iron-endmember). The corresponding reaction leading to the formation of magnetite (), quartz (Si) and hydrogen () is the following:\n\nThis reaction closely resembles the Schikorr reaction observed in anaerobic oxidation of ferrous hydroxide in contact with water.\nthe\n\nFrom all the fault gases formed in power transformers, hydrogen is the most common and is generated under most fault conditions; thus, formation of hydrogen is an early indication of serious problems in the transformer's life cycle.\n\nLarge quantities of are needed in the petroleum and chemical industries. The largest application of is for the processing (\"upgrading\") of fossil fuels, and in the production of ammonia. The key consumers of in the petrochemical plant include hydrodealkylation, hydrodesulfurization, and hydrocracking. has several other important uses. is used as a hydrogenating agent, particularly in increasing the level of saturation of unsaturated fats and oils (found in items such as margarine), and in the production of methanol. It is similarly the source of hydrogen in the manufacture of hydrochloric acid. is also used as a reducing agent of metallic ores.\n\nHydrogen is highly soluble in many rare earth and transition metals and is soluble in both nanocrystalline and amorphous metals. Hydrogen solubility in metals is influenced by local distortions or impurities in the crystal lattice. These properties may be useful when hydrogen is purified by passage through hot palladium disks, but the gas's high solubility is a metallurgical problem, contributing to the embrittlement of many metals, complicating the design of pipelines and storage tanks.\n\nApart from its use as a reactant, has wide applications in physics and engineering. It is used as a shielding gas in welding methods such as atomic hydrogen welding. H is used as the rotor coolant in electrical generators at power stations, because it has the highest thermal conductivity of any gas. Liquid H is used in cryogenic research, including superconductivity studies. Because is lighter than air, having a little more than of the density of air, it was once widely used as a lifting gas in balloons and airships.\n\nIn more recent applications, hydrogen is used pure or mixed with nitrogen (sometimes called forming gas) as a tracer gas for minute leak detection. Applications can be found in the automotive, chemical, power generation, aerospace, and telecommunications industries. Hydrogen is an authorized food additive (E 949) that allows food package leak testing among other anti-oxidizing properties.\n\nHydrogen's rarer isotopes also each have specific applications. Deuterium (hydrogen-2) is used in nuclear fission applications as a moderator to slow neutrons, and in nuclear fusion reactions. Deuterium compounds have applications in chemistry and biology in studies of reaction isotope effects. Tritium (hydrogen-3), produced in nuclear reactors, is used in the production of hydrogen bombs, as an isotopic label in the biosciences, and as a radiation source in luminous paints.\n\nThe triple point temperature of equilibrium hydrogen is a defining fixed point on the ITS-90 temperature scale at 13.8033 Kelvin.\n\nHydrogen is commonly used in power stations as a coolant in generators due to a number of favorable properties that are a direct result of its light diatomic molecules. These include low density, low viscosity, and the highest specific heat and thermal conductivity of all gases.\n\nHydrogen is not an energy resource, except in the hypothetical context of commercial nuclear fusion power plants using deuterium or tritium, a technology presently far from development. The Sun's energy comes from nuclear fusion of hydrogen, but this process is difficult to achieve controllably on Earth. Elemental hydrogen from solar, biological, or electrical sources requires more energy to make than is obtained by burning it, so in these cases hydrogen functions as an energy carrier, like a battery. Hydrogen may be obtained from fossil sources (such as methane), but these sources are unsustainable.\n\nThe energy density per unit \"volume\" of both liquid hydrogen and compressed hydrogen gas at any practicable pressure is significantly less than that of traditional fuel sources, although the energy density per unit fuel \"mass\" is higher. Nevertheless, elemental hydrogen has been widely discussed in the context of energy, as a possible future \"carrier\" of energy on an economy-wide scale. For example, sequestration followed by carbon capture and storage could be conducted at the point of production from fossil fuels. Hydrogen used in transportation would burn relatively cleanly, with some NO emissions, but without carbon emissions. However, the infrastructure costs associated with full conversion to a hydrogen economy would be substantial. Fuel cells can convert hydrogen and oxygen directly to electricity more efficiently than internal combustion engines.\n\nHydrogen is employed to saturate broken (\"dangling\") bonds of amorphous silicon and amorphous carbon that helps stabilizing material properties. It is also a potential electron donor in various oxide materials, including ZnO, SnO, CdO, MgO, ZrO, HfO, LaO, YO, TiO, SrTiO, LaAlO, SiO, AlO, ZrSiO, HfSiO, and SrZrO.\n\nH is a product of some types of anaerobic metabolism and is produced by several microorganisms, usually via reactions catalyzed by iron- or nickel-containing enzymes called hydrogenases. These enzymes catalyze the reversible redox reaction between H and its component two protons and two electrons. Creation of hydrogen gas occurs in the transfer of reducing equivalents produced during pyruvate fermentation to water. The natural cycle of hydrogen production and consumption by organisms is called the hydrogen cycle.\n\nWater splitting, in which water is decomposed into its component protons, electrons, and oxygen, occurs in the light reactions in all photosynthetic organisms. Some such organisms, including the alga \"Chlamydomonas reinhardtii\" and cyanobacteria, have evolved a second step in the dark reactions in which protons and electrons are reduced to form H gas by specialized hydrogenases in the chloroplast. Efforts have been undertaken to genetically modify cyanobacterial hydrogenases to efficiently synthesize H gas even in the presence of oxygen. Efforts have also been undertaken with genetically modified alga in a bioreactor.\n\nHydrogen poses a number of hazards to human safety, from potential detonations and fires when mixed with air to being an asphyxiant in its pure, oxygen-free form. In addition, liquid hydrogen is a cryogen and presents dangers (such as frostbite) associated with very cold liquids. Hydrogen dissolves in many metals, and, in addition to leaking out, may have adverse effects on them, such as hydrogen embrittlement, leading to cracks and explosions. Hydrogen gas leaking into external air may spontaneously ignite. Moreover, hydrogen fire, while being extremely hot, is almost invisible, and thus can lead to accidental burns.\n\nEven interpreting the hydrogen data (including safety data) is confounded by a number of phenomena. Many physical and chemical properties of hydrogen depend on the parahydrogen/orthohydrogen ratio (it often takes days or weeks at a given temperature to reach the equilibrium ratio, for which the data is usually given). Hydrogen detonation parameters, such as critical detonation pressure and temperature, strongly depend on the container geometry.\n\n\n\n"}
{"id": "39512129", "url": "https://en.wikipedia.org/wiki?curid=39512129", "title": "Hydrosal Gel", "text": "Hydrosal Gel\n\nHydrosal Gel is a brand name for a first-line topical gel treatment for excessive sweating. The product is a registered trademark of Valeo Pharma Inc. Hydrosal Gel contains 15% aluminum chloride hexahydrate, an ingredient often used in strong antiperspirants, as well as a hydroalcoholic salicylic acid gel base.\n\nHydrosal Gel has a low risk of irritation due to its proprietary gel base. Clinical studies have demonstrated that the gel vehicle is better tolerated than an alcohol solution most commonly found in other products for excessive sweating, resulting in less irritation. In addition to decreasing perspiration, Hydrosal Gel reduces odours. Clinical studies support the efficacy and low incidence of irritation of the 15% aluminum chloride and 2% salicylic acid gel base formula.\n"}
{"id": "7651212", "url": "https://en.wikipedia.org/wiki?curid=7651212", "title": "Institute for Energy and Transport", "text": "Institute for Energy and Transport\n\nThe Institute for Energy and Transport (IET) is one of the seven scientific Institutes of the Joint Research Centre (JRC), a Directorate General of the European Commission (EC). It is based both in Petten, the Netherlands and Ispra, Italy, and has a multidisciplinary team of around 300 academic, technical, and support staff.\n\nThe mission of the IET is to provide support to European Union policies and technology innovation to ensure sustainable, safe, secure and efficient energy production, distribution and use and to foster sustainable and efficient transport in Europe.\n\nIET is doing so by carrying out research in both nuclear and non-nuclear energy domains, with partners from the Member States and beyond. In state-of-the-art experimental facilities, IET carries out key scientific activities in the following fields: renewable energies including solar, photovoltaics and biomass; sustainable & safe nuclear energy for current & future reactor systems; energy infrastructures and security of supply; sustainable transport, fuels and technologies including hydrogen and fuel cells as well as clean fossil fuel; energy techno/economic assessment; bioenergy including biofuels; energy efficiency in buildings, industry, transport and end-use.\n\n\n\n"}
{"id": "44142424", "url": "https://en.wikipedia.org/wiki?curid=44142424", "title": "Investigator Group Wilderness Protection Area", "text": "Investigator Group Wilderness Protection Area\n\nInvestigator Group Wilderness Protection Area is a protected area located in the Investigator Group of islands off the west coast of Eyre Peninsula in South Australia between to south-west of Elliston. \nThe wilderness protection area was proclaimed in August 2011 under the \"Wilderness Protection Act 1992\" in order to protect ‘important haul-out areas for the Australian sea lion and New Zealand fur seal’ and habitat for species such as White-faced storm petrels, Cape Barren geese and mutton birds and the Pearson Island black-footed rock-wallaby. The wilderness protection area was created from land excised from the Investigator Group Conservation Park. It consists of the Ward Islands, the Top Gallant Isles, and the Pearson Isles which consist of Dorothee Island, Pearson Island and Veteran Isles with exception of a portion of land on Pearson Island which is held by the Australian Maritime Safety Authority for ‘lighthouse purposes’. The Wilderness Protection Area is classified as an IUCN Category Ib protected area.\n\n"}
{"id": "20722789", "url": "https://en.wikipedia.org/wiki?curid=20722789", "title": "Last Drinks: The Impact of the Northern Territory Intervention", "text": "Last Drinks: The Impact of the Northern Territory Intervention\n\nLast Drinks: The Impact of the Northern Territory Intervention is a 2008 Quarterly Essay by Australian journalist Paul Toohey. Toohey critiques the Australian government’s intervention in remote indigenous communities in the Northern Territory, which began in 2007, and aimed to protect indigenous children from a “national emergency” of child sexual abuse. \"Last Drinks\" won the 2008 Walkley Award for \"Coverage of Indigenous Affairs\".\n"}
{"id": "6899646", "url": "https://en.wikipedia.org/wiki?curid=6899646", "title": "Lead-bismuth eutectic", "text": "Lead-bismuth eutectic\n\nLead-Bismuth Eutectic or LBE is a eutectic alloy of lead (44.5%) and bismuth (55.5%) used as a coolant in some nuclear reactors, and is a proposed coolant for the lead-cooled fast reactor, part of the Generation IV reactor initiative.\nIt has a melting point of 123.5 °C/255.3 °F (pure lead melts at 327 °C/621 °F, pure bismuth at 271 °C/520 °F) and a boiling point of 1,670 °C/3,038 °F.\n\nLead-bismuth alloys with between 30% and 75% bismuth all have melting points below 200 °C/392 °F.\nAlloys with between 48% and 63% bismuth have melting points below 150 °C/302 °F.\n\nWhile lead expands slightly on melting and bismuth contracts slightly on melting, LBE has negligible change in volume on melting.\n\nThe Soviet Alfa-class submarines used LBE as a coolant for their nuclear reactors throughout the Cold War.\n\nThe Russians are the acknowledged experts in lead-bismuth cooled reactors, with OKB Gidropress (the Russian developers of the VVER-type Light-water reactors) having special expertise in their development. The SVBR-75/100, a modern design of this type, is one example of the extensive Russian experience with this technology.\n\nGen4 Energy (formerly Hyperion Power Generation), a United States firm connected with Los Alamos National Laboratory, announced plans in 2008 to design and deploy a uranium nitride fueled small modular reactor cooled by lead-bismuth eutectic for commercial power generation, district heating, and desalinization. The proposed reactor, called the Gen4 Module, is planned as a 70 MW reactor of the sealed modular type, factory assembled and transported to site for installation, and transported back to factory for refueling.\n\nAs compared to sodium-based liquid metal coolants such as liquid sodium or NaK, lead-based coolants have significantly higher boiling points, meaning a reactor can be operated without risk of coolant boiling at much higher temperatures. This improves thermal efficiency and could potentially allow hydrogen production through thermochemical processes.\n\nLead and LBE also do not react readily with water or air, in contrast to sodium and NaK which ignite spontaneously in air and react explosively with water. This means that lead- or LBE-cooled reactors, unlike sodium-cooled designs, would not need an intermediate coolant loop, which reduces the capital investment required for a plant.\n\nBoth lead and bismuth are also an excellent radiation shield, blocking gamma radiation while simultaneously being virtually transparent to neutrons. In contrast, sodium will form the potent gamma emitter sodium-24 (half-life 15 hours) following intense neutron radiation, requiring a large radiation shield for the primary cooling loop.\n\nAs heavy nuclei, lead and bismuth can be used as spallation targets for non-fission neutron production, as in Accelerator Transmutation of Waste (see energy amplifier).\n\nBoth lead-based and sodium-based coolants have the advantage of relatively high boiling points as compared to water, meaning it is not necessary to pressurise the reactor even at high temperatures. This improves safety as it reduces the probability of a loss of coolant accident dramatically, and allows for passively safe designs.\n\nLead and LBE coolant are more corrosive to steel than sodium, and this puts an upper limit on the velocity of coolant flow through the reactor due to safety considerations. Furthermore, the higher melting points of lead and LBE (327 °C and 123.5 °C respectively) may mean that solidification of the coolant may be a greater problem when the reactor is operated at lower temperatures.\n\nFinally, upon neutron radiation bismuth-209, the main stable isotopes of bismuth present in LBE coolant, undergo neutron capture and subsequent beta decay, forming polonium-210, a potent alpha emitter. The presence of radioactive polonium in the coolant would require special precautions to control alpha contamination during refueling of the reactor and handling components in contact with LBE.\n\n"}
{"id": "4653469", "url": "https://en.wikipedia.org/wiki?curid=4653469", "title": "Lifeboat ethics", "text": "Lifeboat ethics\n\nLifeboat ethics is a metaphor for resource distribution proposed by the ecologist Garrett Hardin in 1974.\n\nHardin's metaphor describes a lifeboat bearing 50 people, with room for ten more. The lifeboat is in an ocean surrounded by a hundred swimmers. The \"ethics\" of the situation stem from the dilemma of whether (and under what circumstances) swimmers should be taken aboard the lifeboat.\n\nHardin compared the lifeboat metaphor to the Spaceship Earth model of resource distribution, which he criticizes by asserting that a spaceship would be directed by a single leadera captainwhich the Earth lacks. Hardin asserts that the spaceship model leads to the tragedy of the commons. In contrast, the lifeboat metaphor presents individual lifeboats as rich nations and the swimmers as poor nations.\n\nOther issues which can be raised include:\n\nThe third point regarding low supply of food had happened in reality before. A British court, in the ruling of \"R v Dudley and Stephens\" ruled that necessity is not a defense of murder.\n\nLifeboat ethics is closely related to environmental ethics, utilitarianism, and issues of resource depletion. Hardin uses lifeboat ethics to question policies such as foreign aid, immigration, and food banks.\n\n\n"}
{"id": "22032029", "url": "https://en.wikipedia.org/wiki?curid=22032029", "title": "Magnetic separation", "text": "Magnetic separation\n\nMagnetic Separation is which separating components of mixtures by using magnets to attract magnetically susceptible materials. This separation technique can be useful in mining iron as it is attracted to a magnet. \nIn mines where wolframite was mixed with cassiterite, such as South Crofty and East Pool mine in Cornwall or with bismuth such as at the Shepherd and Murphy mine in Moina, Tasmania, magnetic separation is used to separate the ores. At these mines, a device called a Wetherill's Magnetic Separator (invented by John Price Wetherill, 1844–1906) was used. In this machine, the raw ore, after calcination was fed onto a conveyor belt which passed underneath two pairs of electromagnets under which further belts ran at right angles to the feed belt. The first pair of balls was weakly magnetized and served to draw off any iron ore present. The second pair were strongly magnetized and attracted the wolframite, which is weakly magnetic. These machines were capable of treating 10 tons of ore a day.\n\nIt is also used in electromagnetic cranes that separate magnetic material from scraps and unwanted substances.\n\nOne other application, not widely known but very important, is to use magnets in process industries to remove metal contaminants from product streams. This takes a lot of importance in food or pharma industries.\n\n"}
{"id": "44033820", "url": "https://en.wikipedia.org/wiki?curid=44033820", "title": "Metastable inner-shell molecular state", "text": "Metastable inner-shell molecular state\n\nMetastable Innershell Molecular State (MIMS) is a class of ultra-high-energy short-lived molecules have the binding energy up to 1,000 times larger and bond length up to 100 times smaller than typical molecules. MIMS is formed by inner-shell electrons that are normally resistant to molecular formation. However, in stellar conditions, the inner-shell electrons become reactive to form molecular structures (MIMS) from combinations of all elements in the periodic table. \nMIMS upon dissociation can emit x-ray photons with energies up to 100 keV at extremely high conversion efficiencies from compression energy to photon energy. MIMS is predicted to exist and dominate radiation processes in extreme astrophysical environments, such as large planet cores, star interiors, and black hole and neutron star surroundings. There, MIMS is predicted to enable highly energy-efficient transformation of the stellar compression energy into the radiation energy.\n\nThe right schematic illustration shows the proposed four stages of the K-shell MIMS (K-MIMS) formation and x-ray generation process. Stage I: Individual atoms are subjected to the stellar compression and ready for absorbing the compression energy. Stage II: The outer electron shells fuse together under increasing \"stellar\" pressure. Stage III: At the peak pressure, via pressure ionization K-shell orbits form the K-MIMS, which is vibrationally hot and encapsulated by a Rydberg-like pseudo-L-Shell structure. Stage IV: The K-MIMS cools down by ionizing (\"boiling-off\") a number of pseudo-L-shell electrons and subsequent optical decay by emitting an x-ray photon. The dissociated atoms return their original atoms states and are ready for absorbing the compression energy.\n\nMIMS also can be readily produced in laboratory and industrial environments, such as hypervelocity particle impact, laser fusion and z-machine. MIMS can be exploited for highly energy-efficient production of high intensity x-ray beams for a wide range of innovative applications, such as photolithography, x-ray lasers, and inertial fusion.\n\nThe inner-shell-bound metastable quasimolecules were proposed to exist in the x-ray generating scattering process since the work by Mott in the 1930s. The existence of high energy quasimolecules in highly compressed matter (or strongly coupled plasma) was theoretically predicted in the ab initio quantum calculation by Younger et al. in the late 1980s. In 2008, from the\nresult obtained by Mueller, Rafelski and Greiner for quasimolecules in atomic collisions at high impact velocity, Winterberg predicted the existence of inner-shell bound metastable keV molecules under high pressure and their use for the ignition of\nthermonuclear reactions. Metastable Innsershell Molecular State (MIMS) that is homologous to the rare gas excimers was proposed by Bae in 2008 to interpret the mysterious anomalous x-ray signals observed by Bae and his colleagues at Brookhaven National Lab (BNL) in the 1990s. For more details, refer to the last section of this article, \"Other models for inner-shell-bound\nmolecules.\"\n\nTo search for many-body effects in the highly compressed stellar materials, Bae and his colleagues at BNL generated and studied such materials by impacting various bio and water nanoparticle at hypervelocities (v~100 km/s) on various targets. In their study, anomalous signals were discovered, when the nanoparticles were directly impacted on and detected by Si particle detectors that had windows sufficiently thick enough to block the penetration of the nanoparticles completely. By exploiting the discovered anomalous signals, the feasibility of generating highly compressed \"stellar\" matter at shock pressures on the order of 10 TPa (100 Mbar) with the nanoparticle impact in a non-destructive laboratory setup was proven. However, the nature of the signals and the underlying physics of their generation mechanism had not been understood for 15 years.\n\nIt was not until 2008 that Bae was able to unlock the mystery of the anomalous BNL signals owing to emerging sciences of the stellar materials. In the analysis of the BNL signals, Bae discovered that a new class of ultra-high-energy metastable molecules that are bound by inner-shell electrons was responsible for the signals and named the molecules Metastable Innershell Molecular State (MIMS). Further, Bae discovered that the observed energy conversion efficiency via MIMS from the nanoparticle kinetic energy to the radiation energy was as high as 40%, thus proposed that MIMS can enable a new generation of ultra-high efficiency compact x-ray generators.\n\nIn 2012, Bae independently confirmed the BNL results with buckyball ions (C) impacting on an Al target in an independent tabletop apparatus that is orders of magnitude more compact than that at BNL. The result also demonstrated the potential of scaling up of x-ray generation with nanoparticle impact by exploiting C ions, of which currents can be readily scaled up to an industrial quantity in a tabletop apparatus. Bae also proposed a more elaborated MIMS model that is homologous to rare gas excimer molecules was developed and predicted that all elements in periodic table are subjected to the MIMS formation. Up until then, the observed MIMS was proposed to be formed with L-shell electrons.\n\nIn MIMS researches that involve a wide range of x-ray generation phenomena in 2013–2014, Bae discovered that the manifestations of MIMS that was formed with K-shell electrons have existed in extensive experimental data in the x-ray generating heavy ion collision process by numerous researchers for several decades. In his papers, Bae proposed that the quasimolecule is a manifestation of MIMS during the collisional process, a special circumstance for producing MIMS. After extensive analyses and theoretical modeling of these data, which involve a wide range of elements in the periodic table, a successful integration of the data into the frame of the unified MIMS model was demonstrated. Thereby, the MIMS model was firmly established and confirmed for any combinations of all elements in the periodic table.\n\nSpecifically, the extensive analyses of the data that relate to hard x-ray generating collisions have resulted in a universal law (Z–dependency) of the binding energy of the homonucleus MIMS bound by K-shell electrons (K-MIMS). Here Z is the atomic number of the constituent atoms of the K-MIMS. Bae further developed a unified theory to elucidate the Z-dependent behavior of the homonucleus K-MIMS, which behaves much like the helium excimer molecule: He*. The MIMS theory also predicted a 1/Z dependency law for the bond length of the homonucleus K-MIMS. Based on the MIMS theory the uranium K-MIMS, for example, is predicted to have a 100 times smaller bond length, a 2,000 times larger binding energy, and a 5,000 times larger characteristic x-ray energy than the He excimer molecule. The predicted bond lengths of the bismuth and uranium K-MIMS are in excellent agreement with that estimated from the experimental results by researchers at the GSI Helmholtz Centre for Heavy Ion Research in Darmstadt, Germany \n\nIn typical environment, the ground state of rare gas atom dimers is electronically non-binding, but if their closed outershell electrons are excited, dimers can readily form transient bound molecules, excimers. For example, the ground He state (1sσ1sσ: XΣ) is electronically repulsive, but excitation of an electron can lead to Rydberg states (for example, the metastable, 1sσ1sσ2sσ: aΣ) with the Hecore. The low-lying metastable excited states of the dimer (the He excimer) are strongly covalently bound. The metastable excimer can radiate to the free repulsive ground state.\n\nThe excimer formation is a critical step in energy-efficient conversion of the atomic electron excitation/ionization energy into the radiation energy in rare gas plasma for the excimer lasers. Without forming the excimers, the energy of the excited atoms would be rapidly lost by non-radiative collisional decay processes in the high pressure environment. In other words, the excimer formation is a crucial step needed for efficient transformation of the atomic excitation energy to the radiation energy in an ultra-high pressure environment. Analogously, the MIMS formation is a crucial step required for efficiently transforming the atomic core-excitation energy into the radiation energy.\n\nBased on the MIMS model, a schematic potential curve was proposed for the K-shell MIMS (K-MIMS) and illustrated in the right figure. In the \"electronically-cold\" highly compressed plasma, a K-shell core ion with a hole, [K], collides with another core ion without a hole, [K], to form a near-dissociative K-shell MIMS (K-MIMS) and its excess energy transforms into the vibration energy. Such a vibrationally hot K-MIMS in the plasma is denoted here by <nowiki></nowiki>K + K<nowiki></nowiki>. Subsequently, a Rydberg-like pseudo-L-shell forms around the K-MIMS core ions that are proposed to go through a rapid relaxation into the lowest vibrational state by ionizing a number of electrons in the pseudo-L-shell. The vibrationally cold K-MIMS without an L-shell hole is denoted by [KKL]. The vibrational energy is quenched by ionizing the bound [KKL] MIMS into a higher ionic state [KKL] with n holes in the pseudo-L-shell, where 1 ≤ n ≤ 8. Because the n holes can be distributed among 2s and 2p orbits, thus the maximum value of n is 8. The statistically distributed K-MIMS decays into the lower L-shell MIMS (L-MIMS), [KKL],\nby emitting an x-ray photon, and subsequently the L-MIMS dissociates into two atomic ions.\n\nBased on the previously proposed MIMS model, a wide range of the K-MIMS binding energies were extracted from the extensive ranges of experimental data from He* excimer to UAu* MIMS (for 2 ≤ Z ≤ 92). The right shows these data plotted against the constituent atom atomic number Z. The lowest bound state (aΣ) of He* excimer has a binding energy of 1.96 eV, which is plotted in the right figure. These data are plotted against Z in the figure.\n\nThe overall data trend shows a Z dependency. Overall fitting is excellent, thus indicates that the K-MIMS can be modeled with a He*-like excimer with core K-shell ions with an atomic number Z to the first order. Note that the binding energies are about one order of magnitude smaller than the corresponding K-shell satellite x-ray energies, which follow the (Z-1) dependency of the Moseley's law.\n\nAn intuitive analytical theory has been developed and is presented here to illuminate the universal Z-dependent behavior of K-MIMS by Bae. In this theory, the homologous molecular characteristics of K-MIMS is proposed to be primarily determined by the 1sσ bonding molecular orbit of the homologous core molecular ion under the assumption that other effects of surrounding electrons and atoms can be considered as a minor perturbation. The quantum characteristics of excimer can be described by a stable core molecular ion surrounded by Rydberg-like electron orbits. The lowest metastable electronic state of He* (aΣ) is 1sσ1sσ2sσ, thus the proposed K-MIMS model should have a similar molecular orbital. In this case the bonding of K-MIMS is expected to be primarily dominated by the 1sσ orbital. Intuitively, the 1sσ orbital can be approximated in the frame of the LCAO model by a linear combination of the two 1s atomic orbitals, of which size is proportional to 1/Z. Therefore, the K-MIMS size, thus the bond length, is predicted to be proportional to 1/Z. Therefore, to the first order the K-MIMS bond length is predicted to be proportional to 1/Z. The right figure shows the predicted K-MIMS bond length as a function of the constituent atom atomic number, Z. The bond length of He* (aΣ) is ~1.05 A, and the solid line represents a 1/Z curve that is extrapolated from the bond length of He* (aΣ).\n\nCurrently, there is no other direct experimental data or theoretical results on the bond length of the K-MIMS. However, in the recent work by Mokler's group, the sizes of quasimolecules were estimated from the x-ray cross-section enhancement of both projectile x-rays and target x-rays in H-like ion impact. The enhancement was interpreted to result from an extensive K-K electron sharing (transition of a hole) between the projectile and target ions due to the quasimolecule effect during the collision. Based on the K-MIMS theory as illustrated in the right figure, the K-K sharing distance can approximate the K-MIMS bond length. The two data points are the K-K sharing distances of U ion impact on Au and Bi ion impact on Au, which were estimated from the cross sections. The Z values were approximated by the (Z +Z)/2, where Z is the atomic number of the projectile ion and Z is that of the target atom. The predicted 1/Z curve is in excellent agreement with these data points as shown in the right figure.\n\nMIMS can be also formed with two different elements. Currently, such heteronucleus MIMS formed with H and He with other elements are proposed to be observed in H and He impact on a range of solids. Based on Equation of States (EOS) of materials, it can be predicted that pressures required to form homonucleus L-shell MIMS are on the order of 100 Mbar (10 TPa), while homonucleus K-shell MIMS on the order of 10 – 100 Gbar (1,000 – 10,000 TPa). Pressures required to form heteronucleus MIMS are predicted to be considerably smaller than that for homonucleus MIMS.\n\nIn the field of x-ray generation by heavy ion impact on solids, the search for the x-ray signatures of the inner-shell-bound quasimolecule in the x-ray generating scattering process can be traced down to the work by Mott in the 1930s. The quasimolecule can be considered as a collisional complex that is a manifestation of MIMS during collision processes. However, the actual experimental searches for such x-ray signatures of the quasimolecule in x-ray generation in heavy ion impact started much later, in the 1970s. One of the primary motivations of these researches was to explore a super-heavy quasimolecule/quasiatom with a combined atomic number exceeding 100, which was predicted to behave like a transuranium atom in the united atom frame.\n\nIn the early heavy ion collision researches, the cross-sections for carbon K x-ray production were measured for a wide range of ions incident on a carbon target, at energies from 20 to 80 keV. The carbon Kα x-ray generation cross sections for the heavy ions, such as Ar and Xe, were discovered to be several orders of magnitude larger than those by light ions, such as H and He, which were consistent with the values predicted by the direct Coulomb scattering theory. The anomalous cross sections for heavy ions were qualitatively interpreted in terms of the electron-promotion mechanism of the molecular orbital theory. In his papers, Bae proposed that the production of the shocked regions that are able to bear abundant MIMS by the heavy ion impacts as in the nanoparticle impact can be another major factor for the observed x-ray yield enhancement.\n\nMore importantly, there had been extensive theoretical and experimental researches on intense K-shell satellites above the Kα line of various solids that were bombarded by heavy ions with kinetic energies on the order of 10 MeV. In the researches, the K-shell satellites were interpreted to result from radiative decays of atoms with multiple holes, one of which is in the K-shell and others in the L-shell. Striking differences between the satellite spectra obtained with light ions, such as electrons and protons, and the spectra with heavy ions were discovered. In addition to the intensity difference, the x-ray spectra obtained with heavy ions showed many more peaks with multiple L-shell holes than those with light ions. The x-ray energies of the satellites were consistent with those obtained by the ab initio calculations based on the atomic model x-ray emissions that involve varying number of L-shell holes. However, the satellite data for larger number of L-shell holes significantly deviated from the calculation results. Based on the MIMS model, the x-ray satellite structures are now understood to result from the interaction between the vibrational state of K-MIMS and the L-shell electron ionization channels.\n\nOverall, Younger et al. demonstrated the possibility of forming transient molecular states with closed-shell electrons in ab initio calculations for the first time for frozen or slow moving ion systems that can be approximated with the Born-Oppenheimer approximation.\n\n"}
{"id": "5667589", "url": "https://en.wikipedia.org/wiki?curid=5667589", "title": "Nanocomposite", "text": "Nanocomposite\n\nNanocomposite is a multiphase solid material where one of the phases has one, two or three dimensions of less than 100 nanometers (nm), or structures having nano-scale repeat distances between the different phases that make up the material.\n\nThe idea behind Nanocomposite is to use building blocks with dimensions in nanometre range to design and create new materials with unprecedented flexibility and improvement in their physical properties.\n\nIn the broadest sense this definition can include porous media, colloids, gels and copolymers, but is more usually taken to mean the solid combination of a bulk matrix and nano-dimensional phase(s) differing in properties due to dissimilarities in structure and chemistry. The mechanical, electrical, thermal, optical, electrochemical, catalytic properties of the nanocomposite will differ markedly from that of the component materials. Size limits for these effects have been proposed,\n\nNanocomposites are found in nature, for example in the structure of the abalone shell and bone. The use of nanoparticle-rich materials long predates the understanding of the physical and chemical nature of these materials. Jose-Yacaman \"et al.\" investigated the origin of the depth of colour and the resistance to acids and bio-corrosion of Maya blue paint, attributing it to a nanoparticle mechanism. From the mid-1950s nanoscale organo-clays have been used to control flow of polymer solutions (e.g. as paint viscosifiers) or the constitution of gels (e.g. as a thickening substance in cosmetics, keeping the preparations in homogeneous form). By the 1970s polymer/clay composites were the topic of textbooks, although the term \"nanocomposites\" was not in common use.\n\nIn mechanical terms, nanocomposites differ from conventional composite materials due to the exceptionally high surface to volume ratio of the reinforcing phase and/or its exceptionally high aspect ratio. The reinforcing material can be made up of particles (e.g. minerals), sheets (e.g. exfoliated clay stacks) or fibres (e.g. carbon nanotubes or electrospun fibres). The area of the interface between the matrix and reinforcement phase(s) is typically an order of magnitude greater than for conventional composite materials. The matrix material properties are significantly affected in the vicinity of the reinforcement. Ajayan \"et al.\" note that with polymer nanocomposites, properties related to local chemistry, degree of thermoset cure, polymer chain mobility, polymer chain conformation, degree of polymer chain ordering or crystallinity can all vary significantly and continuously from the interface with the reinforcement into the bulk of the matrix.\n\nThis large amount of reinforcement surface area means that a relatively small amount of nanoscale reinforcement can have an observable effect on the macroscale properties of the composite. For example, adding carbon nanotubes improves the electrical and thermal conductivity. Other kinds of nanoparticulates may result in enhanced optical properties, dielectric properties, heat resistance or mechanical properties such as stiffness, strength and resistance to wear and damage. In general, the nano reinforcement is dispersed into the matrix during processing. The percentage by weight (called \"mass fraction\") of the nanoparticulates introduced can remain very low (on the order of 0.5% to 5%) due to the low filler percolation threshold, especially for the most commonly used non-spherical, high aspect ratio fillers (e.g. nanometer-thin platelets, such as clays, or nanometer-diameter cylinders, such as carbon nanotubes). The orientation and arrangement of asymmetric nanoparticles, thermal property mismatch at the interface, interface density per unit volume of nanocomposite, and polydispersity of nanoparticles significantly affect the effective thermal conductivity of nanocomposites.\n\nCeramic matrix composites (CMCs) consist of ceramic fibers embedded in a ceramic matrix. The matrix and fibers can consist of any ceramic material, including carbon and carbon fibers. The ceramic occupying most of the volume is often from the group of oxides, such as nitrides, borides, silicides, whereas the second component is often a metal. Ideally both components are finely dispersed in each other in order to elicit particular optical, electrical and magnetic properties as well as tribological, corrosion-resistance and other protective properties.\n\nThe binary phase diagram of the mixture should be considered in designing ceramic-metal nanocomposites and measures have to be taken to avoid a chemical reaction between both components. The last point mainly is of importance for the metallic component that may easily react with the ceramic and thereby lose its metallic character. This is not an easily obeyed constraint because the preparation of the ceramic component generally requires high process temperatures. The safest measure thus is to carefully choose immiscible metal and ceramic phases. A good example of such a combination is represented by the ceramic-metal composite of TiO and Cu, the mixtures of which were found immiscible over large areas in the Gibbs’ triangle of Cu-O-Ti.\n\nThe concept of ceramic-matrix nanocomposites was also applied to thin films that are solid layers of a few nm to some tens of µm thickness deposited upon an underlying substrate and that play an important role in the functionalization of technical surfaces. Gas flow sputtering by the hollow cathode technique turned out as a rather effective technique for the preparation of nanocomposite layers. The process operates as a vacuum-based deposition technique and is associated with high deposition rates up to some µm/s and the growth of nanoparticles in the gas phase. Nanocomposite layers in the ceramics range of composition were prepared from TiO and Cu by the hollow cathode technique that showed a high mechanical hardness, small coefficients of friction and a high resistance to corrosion.\n\nMetal matrix nanocomposites can also be defined as reinforced metal matrix composites. This type of composites can be classified as continuous and non-continuous reinforced materials. One of the more important nanocomposites is Carbon nanotube metal matrix composites, which is an emerging new material that is being developed to take advantage of the high tensile strength and electrical conductivity of carbon nanotube materials. Critical to the realization of CNT-MMC possessing optimal properties in these areas are the development of synthetic techniques that are (a) economically producible, (b) provide for a homogeneous dispersion of nanotubes in the metallic matrix, and (c) lead to strong interfacial adhesion between the metallic matrix and the carbon nanotubes. In addition to carbon nanotube metal matrix composites, boron nitride reinforced metal matrix composites and carbon nitride metal matrix composites are the new research areas on metal matrix nanocomposites.\n\nA recent study, comparing the mechanical properties (Young's modulus, compressive yield strength, flexural modulus and flexural yield strength) of single- and multi-walled reinforced polymeric (polypropylene fumarate—PPF) nanocomposites to tungsten disulfide nanotubes reinforced PPF nanocomposites suggest that tungsten disulfide nanotubes reinforced PPF nanocomposites possess significantly higher mechanical properties and tungsten disulfide nanotubes are better reinforcing agents than carbon nanotubes. Increases in the mechanical properties can be attributed to a uniform dispersion of inorganic nanotubes in the polymer matrix (compared to carbon nanotubes that exist as micron sized aggregates) and increased crosslinking density of the polymer in the presence of tungsten disulfide nanotubes (increase in crosslinking density leads to an increase in the mechanical properties). These results suggest that inorganic nanomaterials, in general, may be better reinforcing agents compared to carbon nanotubes.\n\nAnother kind of nanocomposite is the energetic nanocomposite, generally as a hybrid sol–gel with a silica base, which, when combined with metal oxides and nano-scale aluminum powder, can form \"superthermite\" materials.\n\nIn the simplest case, appropriately adding nanoparticulates to a polymer matrix can enhance its performance, often dramatically, by simply capitalizing on the nature and properties of the nanoscale filler (these materials are better described by the term nanofilled polymer composites ). This strategy is particularly effective in yielding high performance composites, when uniform dispersion of the filler is achieved and the properties of the nanoscale filler are substantially different or better than those of the matrix. The uniformity of the dispersion is in all nanocomposites is counteracted by thermodynamically driven phase separation. Clustering of nanoscale fillers produces aggregates that serve as structural defects and result in failure. Layer-by-layer (LbL) assembly when nanometer scale layers of nanoparticulates and a polymers are added one by one. LbL composites display performance parameters 10-1000 times better that the traditional nanocomposites made by extrusion or batch-mixing. \n\nNanoparticles such as graphene , carbon nanotubes, molybdenum disulfide and tungsten disulfide are being used as reinforcing agents to fabricate mechanically strong biodegradable polymeric nanocomposites for bone tissue engineering applications. The addition of these nanoparticles in the polymer matrix at low concentrations (~0.2 weight %) cause significant improvements in the compressive and flexural mechanical properties of polymeric nanocomposites. Potentially, these nanocomposites may be used as a novel, mechanically strong, light weight composite as bone implants. The results suggest that mechanical reinforcement is dependent on the nanostructure morphology, defects, dispersion of nanomaterials in the polymer matrix, and the cross-linking density of the polymer. In general, two-dimensional nanostructures can reinforce the polymer better than one-dimensional nanostructures, and inorganic nanomaterials are better reinforcing agents than carbon based nanomaterials. In addition to mechanical properties, polymer nanocomposites based on carbon nanotubes or graphene have been used to enhance a wide range of properties, giving rise to functional materials for a wide range of high added value applications in fields such as energy conversion and storage, sensing and biomedical tissue engineering. For example, multi-walled carbon nanotubes based polymer nanocomposites have been used for the enhancement of the electrical conductivity.\n\nNanoscale dispersion of filler or controlled nanostructures in the composite can introduce new physical properties and novel behaviors that are absent in the unfilled matrices. This effectively changes the nature of the original matrix (such composite materials can be better described by the term genuine nanocomposites or hybrids ). Some examples of such new properties are fire resistance or flame retardancy, and accelerated biodegradability.\n\nA range of polymeric nanocomposites are used for biomedical applications such as tissue engineering, drug delivery, cellular therapies. Due to unique interactions between polymer and nanoparticles, a range of property combinations can be engineered to mimic native tissue structure and properties. A range of natural and synthetic polymers are used to design polymeric nanocomposites for biomedical applications including starch, cellulose, alginate, chitosan, collagen, gelatin, and fibrin, poly(vinyl alcohol) (PVA), poly(ethylene glycol) (PEG), poly(caprolactone) (PCL), poly(lactic-co-glycolic acid) (PLGA), and poly(glycerol sebacate) (PGS). A range of nanoparticles including ceramic, polymeric, metal oxide and carbon-based nanomaterials are incorporated within polymeric network to obtain desired property combinations.\n\nNanocomposites that can respond to an external stimulus are of increased interest due to the fact that, due to the large amount of interaction between the phase interfaces, the stimulus response can have a larger affect on the composite as a whole. The external stimulus can take many forms, such as a magnetic, electrical, or mechanical field. Specifically, magnetic nanocomposites are useful for use in these applications due to the nature of magnetic material's ability to respond both to electrical and magnetic stimuli. The penetration depth of a magnetic field is also high, leading to an increased area that the nanocomposite is affected by and therefore an increased response. In order to respond to a magnetic field, a matrix can be easily loaded with nanoparticles or nanorods The different morphologies for magnetic nanocomposite materials are vast, including matrix dispersed nanoparticles, core-shell nanoparticles,\ncolloidal crystals, macroscale spheres, or janus-type nanostructures.\n\nMagnetic nanocomposites can be utilized in a vast number of applications, including catalytic, medical, and technical. For example, palladium is a common transition metal used in catalysis reactions. Magnetic nanoparticle-supported palladium complexes can be used in catalysis to increase the efficiency of the palladium in the reaction .\n\nMagnetic nanocomposites can also be utilized in the medical field, with magnetic nanorods embedded in a polymer matrix can aid in more precise drug delivery and release. Finally, magnetic nanocomposites can be used in high frequency/high temperature applications. For example, multi-layer structures can be fabricated for use in electronic applications. An electrodeposited Fe/Fe oxide multi-layered sample can be an example of this application of magnetic nanocomposites.\n\n"}
{"id": "3090064", "url": "https://en.wikipedia.org/wiki?curid=3090064", "title": "Oil!", "text": "Oil!\n\nOil! is a novel by Upton Sinclair, first published in 1926–27 and told as a third-person narrative, with only the opening pages written in the first person. The book was written in the context of the Harding administration's Teapot Dome Scandal and takes place in Southern California. It is a social and political satire skewering the human foibles of all its characters.\n\nThe main character is James Arnold Ross Jr., nicknamed Bunny, son of an oil tycoon. Bunny's sympathetic feelings toward oilfield workers and socialists provoke arguments with his father throughout the story.\n\nThe novel served as a loose inspiration for the 2007 film \"There Will Be Blood\".\n\n\nJames Arnold \"Dad\" Ross and his son, James Jr. (\"Bunny\") are introduced as they drive through southern California to meet with the Watkins family, who are leasing out some oil property they own. They find out that the family is deadlocked about how the properties and proceeds should be divided. While Dad and Bunny go quail hunting on the Watkins' goat ranch, they find oil. At Bunny's urging, Dad tries to prevent the elder Watkins from beating his daughter Ruth; trying to convince them that he has received a \"third revelation\" which prohibits parents from beating their children. The plan backfires when Eli, Ruth's brother, interjects himself into the discussion and claims that \"he\" has received the revelation.\n\nAs drilling begins at the Watkins ranch, Bunny begins to realize his father's business methods are not entirely ethical. After a worker is killed in an accident and an oil well is destroyed in a blowout, Dad's workforce goes on strike. Bunny is torn between loyalty to Dad and his friendship to Ruth and her rebellious brother Paul, who support the workers. Paul is drafted into World War I and, when the conflict is over, remains in Siberia to fight the rising Bolsheviks. Back home, Bunny enrolls in college, and he becomes increasingly involved with socialism through a classmate, Rachel Menzies. Paul returns home and tells of his travels, explaining he has become a communist.\n\nBunny accompanies Dad to the seaside mansion of his business associate Vernon Roscoe. Dad and Roscoe flee the country to avoid being subpoenaed by Congress in the Teapot Dome scandal. Before Dad goes away, Bunny proposes parting ways with his father and earning his own way in the world; Dad is confused and hurt, but not unsupportive. Overseas, Dad meets and marries Mrs. Olivier, a widow and Spiritualist, but soon passes away from pneumonia. Bunny decides to dedicate his life and inheritance to social justice while Roscoe moves to get control of the bulk of Dad's estate. Bunny and his sister Bertie are swindled out of most of their inheritance by Roscoe and Mrs. Olivier. \n\nBunny marries Rachel and they dedicate themselves to establishing a socialist institution of learning; Eli, by now a successful evangelist, falsely claims that Paul underwent a deathbed conversion to Christianity.\n\nThe book is loosely based on the life of Edward L. Doheny (and the company he co-founded, Pan American Petroleum & Transport Company, the California assets of which became Pan American Western Petroleum Company), and also the strategic alliance Union-Independent Producers Agency, a consortium created in 1910 to bring oil via pipeline from Kern County to the Pacific Coast facilities of Union Oil Company at Port Harford (now called Port San Luis just west of Avila Beach).\n\nNumerous parallels exist between the opening setting of the novel, Beach City, and the city of Huntington Beach. Huntington Beach was originally called \"Pacific City\", for which Beach City is a play off of both names. The novel states that the area had street names like \"Telegraph\" and \"Beach City Blvd\". Telegraph Road would be the last street crossed before getting off the highway onto Beach Blvd in the town of Buena Park to travel south to Huntington Beach. James Arnold Ross and Bunny stay in a hotel at the intersection of Beach City Blvd and Coast Drive, similar to Beach Blvd and what would later develop into Pacific Coast Highway, where a hotel and water resort once resided in the early 1900s. In the novel, Beach City is covered in beet and cabbage fields. Huntington Beach historically was covered in beet and celery fields. In the novel, the primary oil field found is on \"Prospect Hill\". The first confirmed oil wells in Huntington Beach were located on a series of bluffs.\n\nThe character of Eli Watkins is loosely based on the famous evangelist Aimee McPherson.\n\n\"Oil!\" was banned in Boston for its motel sex scene. Sinclair's publisher printed 150 copies of a \"fig-leaf edition\" with the offending nine pages blacked out. Sinclair protested the banning and hoped to bring an obscenity case to trial. He did not do so, but the controversy helped make the book a bestseller.\n\nThe 2007 feature film \"There Will Be Blood\", directed by Paul Thomas Anderson, is inspired by the novel, but the story is too different to be considered an adaptation. Unlike the novel, \"There Will Be Blood\" focused on the father, with his son being a supporting character. Paul Thomas Anderson said that he only incorporated the first 150 pages of the book into his film, so the rest of the film and novel are nearly entirely different.\n\nAnderson based his composite lead character Daniel Plainview on Edward L. Doheny and several men. He was inspired by the oil museums in Kern County, California and the libraries and museums in the area around Silver City, New Mexico, as well as the period photography, which played a large part in shaping his screenplay and the film.\n"}
{"id": "1493898", "url": "https://en.wikipedia.org/wiki?curid=1493898", "title": "Open Access Same-Time Information System", "text": "Open Access Same-Time Information System\n\nThe Open Access Same-Time Information System (OASIS), is an Internet-based system for obtaining services related to electric power transmission in North America. It is the primary means by which high-voltage transmission lines are reserved for moving wholesale quantities of electricity. The OASIS concept was originally conceived with the Energy Policy Act of 1992, and formalized in 1996 through Federal Energy Regulatory Commission (FERC) Orders 888 and 889.\n\nElectric utility systems in North America developed over time as regulated monopolies, jurisdictional utilities given rights to own and operate transmission and distribution networks in a given geographical area along with the responsibility to serve all loads in that same area. At first, utility companies generally served their own system load demand by building local power generation facilities within their systems. Social, economic, and ecological influences later led to new arrangements where a utility company might enter into long-term power purchase or sale agreements with neighboring utility companies, or locate new generation facilities outside of their system and enter into long-term agreements for transmission rights to deliver that energy to their own system. In the short-term world of day-to-day operations, utility companies would agree to \"preschedule\" (day ahead) or \"real time\" (same day or next hour) energy transactions with adjacent companies to supplement their own generation asset capabilities.\n\nAs utility companies began integrating their operations in more complex ways with their neighbors, they evolved into a vertical organizational structure with three tiers: Generation, Transmission, and Scheduling. On a day-to-day or day-ahead operational level these functions might be performed by three or more people at large utilities, but might be combined into a single employee's job at a small utility. The size of the back office support for each function varies greatly depending upon the size of the utility.\n\nThe generation group manages the maintenance and operations of generation assets, with an eye on the future regarding when, where and how much generation assets will need to be developed to keep up with future demand.\n\nThe transmission group concerns itself with maintaining the high voltage transmission system and lower voltage distribution system. As load demands increase or new generation assets come online in their systems, they upgrade existing facilities or construct new transmission corridors to maintain the reliable delivery of energy.\n\nThe scheduling group is responsible for ensuring that there is adequate power supply to meet the demand of the customer load on a day-to-day and hour-to-hour basis, and also for procuring resources to meet long term needs. These resources can be procured through the generation group, or through purchases and sales with other companies.\n\nUnlike a typical residential or commercial/industrial customer, large bulk users of electricity such as mills, mines and large factories generally have the opportunity to negotiate the rates they will pay their supplier for electricity. In some cases they might even have their own generation assets as well. If they chose to use their supplier's generation instead of their own, they might also be required to pay a fee for the transmission to deliver it, since that transmission might be built specifically to serve their needs. Fees for services provided by the transmission group were defined in a pro forma tariff, a document the transmission group supplied that detailed requirements and responsibilities for the purchaser and provider, and definitions and costs of the types of transmission services available.\n\nThe Energy Policy Act of 1992 (EPAct) laid the initial foundation for the eventual deregulation of the North American electricity market. This Act called for utility companies to allow external entities fair access to the electric transmission systems in North America. The act's intent was to allow large customers (and in theory, every customer) to choose their electricity supplier and subsequently pay for the transmission to deliver it from the generation to serve their load.\n\nBased on the premise that new generating facilities would be allowed fair access to their regional transmission system, and precipitated by the EPAct of 1992, construction of new independently owned generation assets began in response to the development of the North American electricity market. Recognizing competition was coming, electric utility companies began modifying their scheduling functions by forming affiliated Power Marketing departments. Similarly, financial trading interests and existing energy companies (outside of electricity) saw the opportunities in the emerging electricity market and began to organize unaffiliated power marketing divisions. With open access, anyone with the proper resources and/or creditworthiness could purchase the rights to generation, move it across the transmission network (provided adequate capacity was available), and deliver it to a place of higher demand.\n\nFollowing passage of the EPAct of 1992, independent generation owner/operators (also called independent power producers or IPPs) and unaffiliated power marketers lodged frequent complaints with FERC about unfair treatment under the new open access requirements. The complaints generally followed the same theme: vertically integrated electric utility companies would favor their own affiliated power marketing division over external parties trying to move power on the system. In many cases, the power marketers operated side by side with the transmission operators (or it might even be the same person) and there were no rules to prevent unfair treatment of external transmission system users.\n\nTo protect and promote generation competition and also enforce fair treatment of external users of the transmission system, FERC issued Order 888 and Order 889 on April 24, 1996. The EPAct of 1992 was the beginning of electric deregulation in North America, but Orders 888 and 889 marked the point where the trading of electricity gained a firm foothold.\n\nOrder 888's primary objective was to establish and promote competition in the generation market, by ensuring fair access and market treatment of transmission customers. FERC outlined six points to accomplish this goal:\n\nOne fairly immediate result of this order was the functional separation and isolation of the power schedulers and power marketers within vertically integrated utilities from their company's area of transmission operations. Affiliated power marketers could no longer work alongside the transmission operators who were charged with treating them and external parties equally, and at the same time affiliated power marketers would no longer have any \"inside information\" on the availability of the transmission system nor the transactions being scheduled on it.\n\nOrder 889 went to great lengths to detail exactly how all participants in the electricity market should interact with transmission providers. It laid out the structure and function of what became known as OASIS \"nodes,\" which are secure, web-based interfaces to each transmission system's market offerings and transmission availability announcements. Each OASIS node was to be the single point of information dissemination to the market as well as the customer portal for transmission service requests (TSR), even for affiliated power marketers wanting access to their own parent company's transmission.\n\nOASIS nodes are entirely web-based, and public access is limited. Power marketers that become signatories to a transmission provider's OATT gain more complete access so they can view existing transmission and service availability and existing service requests made by other parties. There are also market observers who have read-only access, who may view activity but not request services.\n\nTransmission facilities have power transfer limits that must be maintained to allow the power grid to operate reliably. Transmission operators perform system studies in various future time frames to determine how much transfer capacity is required to serve their own \"native load\", and how much capacity must remain as a buffer to prevent unscheduled or accidental overflows that can damage high voltage equipment. The difference between the capacity needed to serve load and to maintain safe flow margins can be made available for purchase on the OASIS node.\n\nUnplanned outages and other system emergencies can adversely impact the total power transfer capability across transmission systems, and it sometimes becomes necessary for transmission providers to curtail power flows across the system by revoking transmission rights given to buyers on the OASIS. Some transmission buyers are willing to pay higher rates to avoid having their transactions curtailed, and as such transmission companies offer different priorities of transmission service at varying rates. The least expensive type of transmission is generally \"non-firm\" and purchased on an hour to hour basis. Daily non-firm is a slightly higher priority (because the buyer committed to purchasing all day), and increments go up from there to weekly, monthly, seasonally, yearly, or longer with the cost for each also rising incrementally. \"Firm\" transmission services are even more expensive, but are the last transactions to be curtailed.\n\nEven before the appearance of the OASIS nodes, many groups of transmission owners had already turned over operational control of their collective bulk transmission systems to Independent System Operators of various forms. These ISO's offered OASIS access to their collective systems very early on, so that it was often possible to make a single OASIS transmission service request that could cross multiple transmission systems. Since the inception of OASIS, and under the prodding of FERC to move transmission assets under the control of ISO's, the number of OASIS nodes is decreasing as ISO's assume control of transmission systems and consolidate their related OASIS functions.\n\nAfter the doors opened to allow power marketers to move their electricity purchases across multiple transmission systems, many transmission operators saw their transmission systems loaded to much higher levels. Even though transmission services are generally obtained \"point-to-point\", in actuality power flows divide among numerous paths according to the properties of electricity and thus the actual energy flows follow the path of least resistance.\n\nA result of the long distance electricity transactions being scheduled was the impact of \"loop flows\" caused by energy flowing on these alternate paths. Transmission system operators were faced with a dilemma: The problems were often being caused by external influences, and the only way available to them to reduce the stress on the transmission system was to curtail their own transmission sales. This resulted in a loss of revenue and still did not always solve the overloading problems.\n\nThe North American Electric Reliability Corporation (NERC) stepped in to address this new problem that threatened the North American power grid by introducing the NERC Tagging application. NERC Tags captured entire transactions from beginning to end. This let them string together all the transmission legs obtained on various OASIS nodes, and then determine how the total schedule impacted transmission systems, and what priorities of transmission were used in the schedule. This let them determine which schedules should be curtailed to relieve loading on transmission systems.\n\nNERC also assumed initial control of the Transmission System Information Networks (TSIN), a database of electric power system data. In 2012 the North American Energy Standards Board (NAESB), an industry council, assumed responsibility for TSIN. It is now the \"OATI Web Registry\" and requires that users be registered. The registry is a web-based database containing a comprehensive listing of generation points, transmission facilities and delivery points as well as transmission and generation priority definitions with regard to the applications that use it (the various OASIS nodes as well as the NERC Tagging application).\n\n\n"}
{"id": "23441208", "url": "https://en.wikipedia.org/wiki?curid=23441208", "title": "Organization of Petroleum Importing Countries", "text": "Organization of Petroleum Importing Countries\n\nThe Organization of Petroleum Importing Countries was a cartel (or a proposal for a cartel) of countries that primarily consume petroleum, to counter the influence of OPEC, the Organization of Petroleum Exporting Countries.\n\nIt was reported in the early 1980s that Costa Rica had led the establishment of a group of countries under this name.\n\nThere were also calls in the early 21st century for an organization fulfilling such a role. It was argued that it was in the interests of developing countries and that it would also act to the benefit of developed countries.\n"}
{"id": "39740453", "url": "https://en.wikipedia.org/wiki?curid=39740453", "title": "PFC Energy", "text": "PFC Energy\n\nPFC Energy is a global energy research and consultancy group. Its services include upstream and gas, midstream and downstream oil, markets and country strategies, and financial sector consulting, as well as scenario planning and country economic modeling. The company also provides consulting services related to clean energy, alternative fuels, and natural gas. Its clients are oil and gas operators, national oil companies, service companies, investors, governments and other stakeholders. PFC Energy has focused exclusively on the energy sector for since 1984, covering all phases of the energy value chain, and the assets and activities of key countries and companies. An independent partnership with more than 130 professional staff, PFC Energy is based in Washington, D.C. and also maintains offices in Houston, Kuala Lumpur, Moscow, Paris, Beijing and Singapore.\n\nThe company was originally known as Petroleum Finance Company. On January 27, 2007, it changed its name to PFC Energy.\n\nOn June 8, 2011, PFC Energy and Guggenheim Securities formed a strategic alliance to provide research and analytical services to institutional investors and to collaborate on investment banking activities for corporate clients interested in the energy sector. On June 20, 2013, IHS Inc. acquired PFC Energy.\n\nEvery January, PFC Energy publishes the \"PFC Energy 50\", the definitive ranking of the World's Largest Listed Energy Firms. PFC Energy 50 ranks the world's leading publicly traded energy companies by market capitalization. The listing includes companies from nine sectors: International Oil Companies; National Oil Companies; Exploration & Production; Midstream/Infrastructure, Refining & Marketing; Gas/Utilities; Oilfield & Drilling Services; Equipment, Engineering & Construction; and Alternative Energy.\n\n• Robin West, Chairman\n\n"}
{"id": "23319", "url": "https://en.wikipedia.org/wiki?curid=23319", "title": "Palladium", "text": "Palladium\n\nPalladium is a chemical element with symbol Pd and atomic number 46. It is a rare and lustrous silvery-white metal discovered in 1803 by William Hyde Wollaston. He named it after the asteroid Pallas, which was itself named after the epithet of the Greek goddess Athena, acquired by her when she slew Pallas. Palladium, platinum, rhodium, ruthenium, iridium and osmium form a group of elements referred to as the platinum group metals (PGMs). These have similar chemical properties, but palladium has the lowest melting point and is the least dense of them.\n\nMore than half the supply of palladium and its congener platinum is used in catalytic converters, which convert as much as 90% of the harmful gases in automobile exhaust (hydrocarbons, carbon monoxide, and nitrogen dioxide) into less noxious substances (nitrogen, carbon dioxide and water vapor). Palladium is also used in electronics, dentistry, medicine, hydrogen purification, chemical applications, groundwater treatment, and jewelry. Palladium is a key component of fuel cells, which react hydrogen with oxygen to produce electricity, heat, and water.\n\nOre deposits of palladium and other PGMs are rare. The most extensive deposits have been found in the norite belt of the Bushveld Igneous Complex covering the Transvaal Basin in South Africa; the Stillwater Complex in Montana, United States; the Sudbury Basin and Thunder Bay District of Ontario, Canada; and the Norilsk Complex in Russia. Recycling is also a source, mostly from scrapped catalytic converters. The numerous applications and limited supply sources result in considerable investment interest.\n\nPalladium belongs to group 10 in the periodic table, but the configuration in the outermost electrons are in accordance with Hund's rule. Electrons in the s-shell migrate to fill the d orbitals because they have less energy. \n\nPalladium is a soft silver-white metal that resembles platinum. It is the least dense and has the lowest melting point of the platinum group metals. It is soft and ductile when annealed and is greatly increased in strength and hardness when cold-worked. Palladium dissolves slowly in concentrated nitric acid, in hot, concentrated sulfuric acid, and when finely ground, in hydrochloric acid. It dissolves readily at room temperature in aqua regia.\n\nPalladium does not react with oxygen at standard temperature (and thus does not tarnish in air). Palladium heated to 800 °C will produce a layer of palladium(II) oxide (PdO). It tarnishes lightly in a moist atmosphere containing sulfur.\n\nPalladium films with defects produced by alpha particle bombardment at low temperature exhibit superconductivity having T=3.2 K.\n\nNaturally occurring palladium is composed of seven isotopes, six of which are stable. The most stable radioisotopes are Pd with a half-life of 6.5 million years (found in nature), Pd with 17 days, and Pd with 3.63 days. Eighteen other radioisotopes have been characterized with atomic weights ranging from 90.94948(64) u (Pd) to 122.93426(64) u (Pd). These have half-lives of less than thirty minutes, except Pd (half-life: 8.47 hours), Pd (half-life: 13.7 hours), and Pd (half-life: 21 hours).\n\nFor isotopes with atomic mass unit values less than that of the most abundant stable isotope, Pd, the primary decay mode is electron capture with the primary decay product being rhodium. The primary mode of decay for those isotopes of Pd with atomic mass greater than 106 is beta decay with the primary product of this decay being silver.\n\nRadiogenic Ag is a decay product of Pd and was first discovered in 1978 in the Santa Clara meteorite of 1976. The discoverers suggest that the coalescence and differentiation of iron-cored small planets may have occurred 10 million years after a nucleosynthetic event. Pd versus Ag correlations observed in bodies, which have been melted since accretion of the solar system, must reflect the presence of short-lived nuclides in the early solar system.\n\nPalladium compounds primarily exist in the 0 and +2 oxidation state. Other less common states are also recognized. Generally the compounds of palladium are more similar to those of platinum than those of any other element.\nPalladium(II) chloride is the principal starting material for other palladium compounds. It arises by the reaction of palladium with chlorine. It is used to prepare heterogeneous palladium catalysts such as palladium on barium sulfate, palladium on carbon, and palladium chloride on carbon. Solutions of PdCl in nitric acid react with acetic acid to give palladium(II) acetate, also a versatile reagent. PdCl reacts with ligands (L) to give square planar complexes of the type PdClL. One example of such complexes is the benzonitrile derivative PdX(PhCN).\nThe complex bis(triphenylphosphine)palladium(II) dichloride is a useful catalyst.\n\nPalladium forms a range of zerovalent complexes with the formula PdL, PdL and PdL. For example, reduction of a mixture of PdCl(PPh) and PPh gives tetrakis(triphenylphosphine)palladium(0):\n\nAnother major palladium(0) complex, tris(dibenzylideneacetone)dipalladium(0) (Pd(dba)), is prepared by reducing sodium tetrachloropalladate in the presence of dibenzylideneacetone. \n\nPalladium(0), as well as palladium(II), are catalysts in coupling reactions, as has been recognized by the 2010 Nobel Prize in Chemistry to Richard F. Heck, Ei-ichi Negishi, and Akira Suzuki. Such reactions are widely practiced for the synthesis of fine chemicals. Prominent coupling reactions include the Heck, Suzuki, Sonogashira coupling, Stille reactions, and the Kumada coupling. Palladium(II) acetate, tetrakis(triphenylphosphine)palladium(0) (Pd(PPh), and tris(dibenzylideneacetone)dipalladium(0) (Pd(dba)) serve either as catalysts or precatalysts.\n\nAlthough Pd(IV) compounds are comparatively rare, one example is sodium hexachloropalladate(IV), Na[PdCl]. A few compounds of palladium(III) are also known. Palladium(VI) was claimed in 2002, but subsequently disproven.\n\nMixed valence palladium complexes exist, e.g. Pd(CO)(OAc)Pd(acac) forms an infinite Pd chain structure, with alternatively interconnected Pd(CO)(OAc) and Pd(acac) units.\n\nWilliam Hyde Wollaston noted the discovery of a new noble metal in July 1802 in his lab-book and named it palladium in August of the same year. Wollaston purified enough of the material and offered it, without naming the discoverer, in a small shop in Soho in April 1803. After harsh criticism from Richard Chenevix that palladium is an alloy of platinum and mercury, Wollaston anonymously offered a reward of 20 British pounds for 20 grains of synthetic palladium \"alloy\". Chenevix received the Copley Medal in 1803 after he published his experiments on palladium. Wollaston published the discovery of rhodium in 1804 and mentions some of his work on palladium. He disclosed that he was the discoverer of palladium in a publication in 1805.\n\nIt was named by Wollaston in 1802 after the asteroid 2 Pallas, which had been discovered two months earlier. Wollaston found palladium in crude platinum ore from South America by dissolving the ore in aqua regia, neutralizing the solution with sodium hydroxide, and precipitating platinum as ammonium chloroplatinate with ammonium chloride. He added mercuric cyanide to form the compound palladium(II) cyanide, which was heated to extract palladium metal.\n\nPalladium chloride was at one time prescribed as a tuberculosis treatment at the rate of 0.065 g per day (approximately one milligram per kilogram of body weight). This treatment had many negative side-effects, and was later replaced by more effective drugs.\n\nMost palladium is used for catalytic converters in the automobile industry. In the run up to year 2000, the Russian supply of palladium to the global market was repeatedly delayed and disrupted for political reasons, the export quota was not granted on time. The ensuing market panic drove the price to an all-time high of $1100 per troy ounce in January 2001. Around that time, the Ford Motor Company, fearing that automobile production would be disrupted by a palladium shortage, stockpiled the metal. When prices fell in early 2001, Ford lost nearly US$1 billion.\n\nWorld demand for palladium increased from 100 tons in 1990 to nearly 300 tons in 2000. The global production of palladium from mines was 222 tonnes in 2006 according to the United States Geological Survey. Many were concerned about a steady supply of palladium in the wake of Russia's military maneuvers in Ukraine, partly as sanctions could hamper Russian palladium exports; any restrictions on Russian palladium exports would exacerbate what is already expected to be a large palladium deficit in 2014. Those concerns pushed palladium prices to their highest level since 2001. In September 2014 they soared above the $900 per ounce mark. In 2016 however palladium cost around $614 per ounce as Russia managed to maintain stable supplies. In January 2018 palladium futures climbed past $1,100 per ounce for the first time on record, mainly due to the strong demand from the automotive industry.\n\nAs overall mine production of palladium reached 208,000 kilograms in 2016, Russia was the top producer with 82,000 kilograms, followed by South Africa, Canada and the U.S. Russia's company Norilsk Nickel ranks first among the largest palladium producers globally, it accounts for 39% of the world’s production. \n\nPalladium can be found as a free metal alloyed with gold and other platinum-group metals in placer deposits of the Ural Mountains, Australia, Ethiopia, North and South America. For the production of palladium, these deposits play only a minor role. The most important commercial sources are nickel-copper deposits found in the Sudbury Basin, Ontario, and the Norilsk–Talnakh deposits in Siberia. The other large deposit is the Merensky Reef platinum group metals deposit within the Bushveld Igneous Complex South Africa. The Stillwater igneous complex of Montana and the Roby zone ore body of the Lac des Îles igneous complex of Ontario are the two other sources of palladium in Canada and the United States. Palladium is found in the rare minerals cooperite and polarite. Many more Pd minerals are known, but all of them are very rare.\n\nPalladium is also produced in nuclear fission reactors and can be extracted from spent nuclear fuel (see synthesis of precious metals), though this source for palladium is not used. None of the existing nuclear reprocessing facilities are equipped to extract palladium from the high-level radioactive waste.\n\nThe largest use of palladium today is in catalytic converters. Palladium is also used in jewelry, dentistry, watch making, blood sugar test strips, aircraft spark plugs, surgical instruments, and electrical contacts. Palladium is also used to make professional transverse (concert or classical) flutes. As a commodity, palladium bullion has ISO currency codes of XPD and 964. Palladium is one of only four metals to have such codes, the others being gold, silver and platinum. Because it absorbs hydrogen, palladium is a key component of the controversial cold fusion experiments that began in 1989.\n\nWhen it is finely divided, as with palladium on carbon, palladium forms a versatile catalyst; it speeds heterogeneous catalytic processes like hydrogenation, dehydrogenation, and petroleum cracking. Palladium is also essential to the Lindlar catalyst, also called Lindlar's Palladium. A large number of carbon–carbon bonding reactions in organic chemistry are facilitated by palladium compound catalysts. For example:\n\n\nWhen dispersed on conductive materials, palladium is an excellent electrocatalyst for oxidation of primary alcohols in alkaline media. Palladium is also a versatile metal for homogeneous catalysis, used in combination with a broad variety of ligands for highly selective chemical transformations.\n\nIn 2010, palladium-catalysed organic reactions were recognized by the Nobel Prize in Chemistry. A 2008 study showed that palladium is an effective catalyst for carbon-fluoride bonds. \n\nPalladium catalysis is primarily employed in organic chemistry and industrial applications, although its use is growing as a tool for synthetic biology; in 2017, effective \"in vivo\" catalytic activity of palladium nanoparticles was demonstrated in mammals to treat disease.\n\nThe second greatest application of palladium in electronics is in multilayer ceramic capacitors in which palladium (and palladium-silver alloy) is used for electrodes. Palladium (sometimes alloyed with nickel) is used for component and connector plating in consumer electronics and in soldering materials. The electronic sector consumed 1.07 million troy ounces (33.2 tonnes) of palladium in 2006, according to a Johnson Matthey report.\n\nHydrogen easily diffuses through heated palladium, and membrane reactors with Pd membranes are used in the production of high purity hydrogen. Palladium is used in palladium-hydrogen electrodes in electrochemical studies. Palladium(II) chloride readily catalyzes carbon monoxide gas to carbon dioxide and is useful in carbon monoxide detectors.\n\nPalladium readily absorbs hydrogen at room temperatures, forming palladium hydride PdH with x less than 1. While this property is common to many transition metals, palladium has a uniquely high absorption capacity and does not lose its ductility until x approaches 1. This property has been investigated in designing an efficient, inexpensive, and safe hydrogen fuel storage medium, though palladium itself is currently prohibitively expensive for this purpose. The content of hydrogen in palladium can be linked to magnetic susceptibility, which decreases with the increase of hydrogen and becomes zero for PdH. At any higher ratio, the solid solution becomes diamagnetic.\n\nPalladium is used in small amounts (about 0.5%) in some alloys of dental amalgam to decrease corrosion and increase the metallic lustre of the final restoration.\n\nPalladium has been used as a precious metal in jewelry since 1939 as an alternative to platinum in the alloys called \"white gold\", where the naturally white color of palladium does not require rhodium plating. Palladium is much less dense than platinum. Similar to gold, palladium can be beaten into leaf as thin as 100 nm ( in). Unlike platinum, palladium may discolor at temperatures above ; it is relatively brittle.\n\nPalladium is one of the three most popular alloying metals in white gold (nickel and silver can also be used). Palladium-gold is more expensive than nickel-gold, but seldom causes allergic reactions (though certain cross-allergies with nickel may occur).\n\nWhen platinum became a strategic resource during World War II, many jewelry bands were made out of palladium. Up to as recently as September 2001, palladium was more expensive than platinum and rarely used in jewelry because of the technical difficulty of casting. Currently, the casting problem has been resolved and use in jewelry has increased because platinum has increased in price while palladium decreased.\n\nPrior to 2004, the principal use of palladium in jewelry was the manufacture of white gold. In early 2004, when gold and platinum prices rose steeply, China began fabricating volumes of palladium jewelry, consuming 37 tonnes in 2005. Changes in the relative price of platinum after 2008 lowered demand for palladium to 17.4 tonnes in 2009.\n\nIn January 2010, hallmarks for palladium were introduced by assay offices in the United Kingdom, and hallmarking became mandatory for all jewelry advertising pure or alloyed palladium. Articles can be marked as 500, 950, or 999 parts of palladium per thousand of the alloy.\nFountain pen nibs made from gold are sometimes plated with palladium when a silver (rather than gold) appearance is desired. Sheaffer has used palladium plating for decades, either as an accent on otherwise gold nibs or covering the gold completely.\n\nIn the platinotype printing process, photographers make fine-art black-and-white prints using platinum or palladium salts. Often used with platinum, palladium provides an alternative to silver.\n\nPalladium is a metal with low toxicity. It is poorly absorbed by the human body when ingested. Plants such as the water hyacinth are killed by low levels of palladium salts, but most other plants tolerate it, although tests show that, at levels above 0.0003%, growth is affected. High doses of palladium could be poisonous; tests on rodents suggest it may be carcinogenic, though no clear evidence indicates the element harms humans.\n\nLike other platinum-group metals, bulk Pd is quite inert. Although contact dermatitis has been reported, data on the effects are limited. It has been shown that people with an allergic reaction to palladium also react to nickel, making it advisable to avoid the use of dental alloys containing palladium on those so allergic.\n\nSome palladium is emitted with the exhaust gases of cars with catalytic converters. Between 4 and 108 ng/km of palladium particulate is released by such cars, while the total uptake from food is estimated to be less than 2 µg per person a day. The second possible source of palladium is dental restoration, from which the uptake of palladium is estimated to be less than 15 µg per person per day. People working with palladium or its compounds might have a considerably greater uptake. For soluble compounds such as palladium chloride, 99% is eliminated from the body within 3 days.\n\nThe median lethal dose (LD) of soluble palladium compounds in mice is 200 mg/kg for oral and 5 mg/kg for intravenous administration.\n\n\n"}
{"id": "88444", "url": "https://en.wikipedia.org/wiki?curid=88444", "title": "Phosphor", "text": "Phosphor\n\nA phosphor, most generally, is a substance that exhibits the phenomenon of luminescence. Somewhat confusingly, this includes both phosphorescent materials, which show a slow decay in brightness (> 1 ms), and fluorescent materials, where the emission decay takes place over tens of nanoseconds. Phosphorescent materials are known for their use in radar screens and glow-in-the-dark materials, whereas fluorescent materials are common in cathode ray tube (CRT) and plasma video display screens, fluorescent lights, sensors, and white LEDs.\n\nPhosphors are often transition-metal compounds or rare-earth compounds of various types. The most common uses of phosphors are in CRT displays and fluorescent lights. CRT phosphors were standardized beginning around World War II and designated by the letter \"P\" followed by a number.\n\nPhosphorus, the chemical element named for its light-emitting behavior, emits light due to chemiluminescence, not phosphorescence.\n\nA material can emit light either through incandescence, where all atoms radiate, or by luminescence, where only a small fraction of atoms, called \"emission centers\" or \"luminescence centers\", emit light. In inorganic phosphors, these inhomogeneities in the crystal structure are created usually by addition of a trace amount of dopants, impurities called \"activators\". (In rare cases dislocations or other crystal defects can play the role of the impurity.) The wavelength emitted by the emission center is dependent on the atom itself and on the surrounding crystal structure.\n\nThe scintillation process in inorganic materials is due to the electronic band structure found in the crystals. An incoming particle can excite an electron from the valence band to either the conduction band or the exciton band (located just below the conduction band and separated from the valence band by an energy gap). This leaves an associated hole behind, in the valence band. Impurities create electronic levels in the forbidden gap. The excitons are loosely bound electron–hole pairs that wander through the crystal lattice until they are captured as a whole by impurity centers. The latter then rapidly de-excite by emitting scintillation light (fast component). In case of inorganic scintillators, the activator impurities are typically chosen so that the emitted light is in the visible range or near-UV, where photomultipliers are effective. The holes associated with electrons in the conduction band are independent from the latter. Those holes and electrons are captured successively by impurity centers exciting certain metastable states not accessible to the excitons. The delayed de-excitation of those metastable impurity states, slowed down by reliance on the low-probability forbidden mechanism, again results in light emission (slow component).\n\nMany phosphors tend to lose efficiency gradually by several mechanisms. The activators can undergo change of valence (usually oxidation), the crystal lattice degrades, atoms – often the activators – diffuse through the material, the surface undergoes chemical reactions with the environment with consequent loss of efficiency or buildup of a layer absorbing either the exciting or the radiated energy, etc.\n\nThe degradation of electroluminescent devices depends on frequency of driving current, the luminance level, and temperature; moisture impairs phosphor lifetime very noticeably as well.\n\nHarder, high-melting, water-insoluble materials display lower tendency to lose luminescence under operation.\n\nExamples:\n\nPhosphors are usually made from a suitable host material with an added activator. The best known type is a copper-activated zinc sulfide and the silver-activated zinc sulfide (\"zinc sulfide silver\").\n\nThe host materials are typically oxides, nitrides and oxynitrides, sulfides, selenides, halides or silicates of zinc, cadmium, manganese, aluminium, silicon, or various rare-earth metals. The activators prolong the emission time (afterglow). In turn, other materials (such as nickel) can be used to quench the afterglow and shorten the decay part of the phosphor emission characteristics.\n\nMany phosphor powders are produced in low-temperature processes, such as sol-gel and usually require post-annealing at temperatures of ~1000 °C, which is undesirable for many applications. However, proper optimization of the growth process allows to avoid the annealing.\n\nPhosphors used for fluorescent lamps require a multi-step production process, with details that vary depending on the particular phosphor. Bulk material must be milled to obtain a desired particle size range, since large particles produce a poor-quality lamp coating, and small particles produce less light and degrade more quickly. During the firing of the phosphor, process conditions must be controlled to prevent oxidation of the phosphor activators or contamination from the process vessels. After milling the phosphor may be washed to remove minor excess of activator elements. Volatile elements must not be allowed to escape during processing. Lamp manufacturers have changed composition of phosphors to eliminate some toxic elements, such as beryllium, cadmium, or thallium, formerly used.\n\nThe commonly quoted parameters for phosphors are the wavelength of emission maximum (in nanometers, or alternatively color temperature in kelvins for white blends), the peak width (in nanometers at 50% of intensity), and decay time (in seconds).\n\nPhosphor layers provide most of the light produced by fluorescent lamps, and are also used to improve the balance of light produced by metal halide lamps. Various neon signs use phosphor layers to produce different colors of light. Electroluminescent displays found, for example, in aircraft instrument panels, use a phosphor layer to produce glare-free illumination or as numeric and graphic display devices. White LED lamps consist of a blue or ultra-violet emitter with a phosphor coating that emits at longer wavelengths, giving a full spectrum of visible light. Unfocused and undeflected cathode ray tubes were used as stroboscope lamps since 1958.\n\nPhosphor thermometry is a temperature measurement approach that uses the temperature dependence of certain phosphors. For this, a phosphor coating is applied to a surface of interest and, usually, the decay time is the emission parameter that indicates temperature. Because the illumination and detection optics can be situated remotely, the method may be used for moving surfaces such as high speed motor surfaces. Also, phosphor may be applied to the end of an optical fiber as an optical analog of a thermocouple.\n\n\nIn these applications, the phosphor is directly added to the plastic used to mold the toys, or mixed with a binder for use as paints.\n\nZnS:Cu phosphor is used in glow-in-the-dark cosmetic creams frequently used for Halloween make-ups.\nGenerally, the persistence of the phosphor increases as the wavelength increases. \nSee also lightstick for chemiluminescence-based glowing items.\n\nPhosphor banded stamps first appeared in 1959 as guides for machines to sort mail. Around the world many varieties exist with different amounts of banding. Postage stamps are sometimes collected by whether or not they are \"tagged\" with phosphor (or printed on luminescent paper).\n\nZinc sulfide phosphors are used with radioactive materials, where the phosphor was excited by the alpha- and beta-decaying isotopes, to create luminescent paint for dials of watches and instruments (radium dials). Between 1913 and 1950 radium-228 and radium-226 were used to activate a phosphor made of silver doped zinc sulfide (ZnS:Ag), which gave a greenish glow. The phosphor is not suitable to be used in layers thicker than 25 mg/cm², as the self-absorption of the light then becomes a problem. Furthermore, zinc sulfide undergoes degradation of its crystal lattice structure, leading to gradual loss of brightness significantly faster than the depletion of radium. ZnS:Ag coated spinthariscope screens were used by Ernest Rutherford in his experiments discovering atomic nucleus.\n\nCopper doped zinc sulfide (ZnS:Cu) is the most common phosphor used and yields blue-green light. Copper and magnesium doped zinc sulfide (ZnS:Cu,Mg) yields yellow-orange light.\n\nTritium is also used as a source of radiation in various products utilizing tritium illumination.\n\nElectroluminescence can be exploited in light sources. Such sources typically emit from a large area, which makes them suitable for backlights of LCD displays. The excitation of the phosphor is usually achieved by application of high-intensity electric field, usually with suitable frequency. Current electroluminescent light sources tend to degrade with use, resulting in their relatively short operation lifetimes.\n\nZnS:Cu was the first formulation successfully displaying electroluminescence, tested at 1936 by Georges Destriau in Madame Marie Curie laboratories in Paris.\n\nPowder or AC electroluminescence is found in a variety of backlight and night light applications. Several groups offer branded EL offerings (e.g. IndiGlo used in some Timex watches) or \"Lighttape\", another trade name of an electroluminescent material, used in electroluminescent light strips. The Apollo space program is often credited with being the first significant use of EL for backlights and lighting.\n\nWhite light-emitting diodes are usually blue InGaN LEDs with a coating of a suitable material. Cerium(III)-doped YAG (YAG:Ce, or YAlO:Ce) is often used; it absorbs the light from the blue LED and emits in a broad range from greenish to reddish, with most of output in yellow. This yellow emission combined with the remaining blue emission gives the “white” light, which can be adjusted to color temperature as warm (yellowish) or cold (blueish) white. The pale yellow emission of the Ce:YAG can be tuned by substituting the cerium with other rare-earth elements such as terbium and gadolinium and can even be further adjusted by substituting some or all of the aluminium in the YAG with gallium. However, this process is not one of phosphorescence. The yellow light is produced by a process known as scintillation, the complete absence of an afterglow being one of the characteristics of the process.\n\nSome rare-earth-doped Sialons are photoluminescent and can serve as phosphors. Europium(II)-doped β-SiAlON absorbs in ultraviolet and visible light spectrum and emits intense broadband visible emission. Its luminance and color does not change significantly with temperature, due to the temperature-stable crystal structure. It has a great potential as a green down-conversion phosphor for white LEDs; a yellow variant also exists. For white LEDs, a blue LED is used with a yellow phosphor, or with a green and yellow SiAlON phosphor and a red CaAlSiN-based (CASN) phosphor.\n\nWhite LEDs can also be made by coating near ultraviolet (NUV) emitting LEDs with a mixture of high efficiency europium based red and blue emitting phosphors plus green emitting copper and aluminium doped zinc sulfide (ZnS:Cu,Al). This is a method analogous to the way fluorescent lamps work.\n\nSome newer white LEDs use a yellow and blue emitter in series, to approximate white; this technology is used in some Motorola phones such as the Blackberry as well as LED lighting and the original version stacked emitters by using GaN on SiC on InGaP but was later found to fracture at higher drive currents.\n\nMany white LEDs used in general lighting systems can be used for data transfer, for example, in systems that modulate the LED to act as a beacon.\n\nCathode ray tubes produce signal-generated light patterns in a (typically) round or rectangular format. Bulky CRTs were used in the black-and-white household television (\"TV\") sets that became popular in the 1950s, as well as first-generation, tube-based color TVs, and most earlier computer monitors. CRTs have also been widely used in scientific and engineering instrumentation, such as oscilloscopes, usually with a single phosphor color, typically green. Phosphors for such applications may have long afterglow, for increased image persistence.\n\nThe phosphors can be deposited as either thin film, or as discrete particles, a powder bound to the surface. Thin films have better lifetime and better resolution, but provide less bright and less efficient image than powder ones. This is caused by multiple internal reflections in the thin film, scattering the emitted light.\n\nWhite (in black-and-white): The mix of zinc cadmium sulfide and zinc sulfide silver, the ZnS:Ag+(Zn,Cd)S:Ag is the white P4 phosphor used in black and white television CRTs. Mixes of yellow and blue phosphors are usual. Mixes of red, green and blue, or a single white phosphor, can also be encountered.\n\nRed: Yttrium oxide-sulfide activated with europium is used as the red phosphor in color CRTs. The development of color TV took a long time due to the search for a red phosphor. The first red emitting rare-earth phosphor, YVO:Eu, was introduced by Levine and Palilla as a primary color in television in 1964. In single crystal form, it was used as an excellent polarizer and laser material.\n\nYellow: When mixed with cadmium sulfide, the resulting zinc cadmium sulfide (Zn,Cd)S:Ag, provides strong yellow light.\n\nGreen: Combination of zinc sulfide with copper, the P31 phosphor or ZnS:Cu, provides green light peaking at 531 nm, with long glow.\n\nBlue: Combination of zinc sulfide with few ppm of silver, the ZnS:Ag, when excited by electrons, provides strong blue glow with maximum at 450 nm, with short afterglow with 200 nanosecond duration. It is known as the P22B phosphor. This material, zinc sulfide silver, is still one of the most efficient phosphors in cathode ray tubes. It is used as a blue phosphor in color CRTs.\n\nThe phosphors are usually poor electrical conductors. This may lead to deposition of residual charge on the screen, effectively decreasing the energy of the impacting electrons due to electrostatic repulsion (an effect known as \"sticking\"). To eliminate this, a thin layer of aluminium (about 100 nm) is deposited over the phosphors, usually by vacuum evaporation, and connected to the conductive layer inside the tube. This layer also reflects the phosphor light to the desired direction, and protects the phosphor from ion bombardment resulting from an imperfect vacuum.\n\nTo reduce the image degradation by reflection of ambient light, contrast can be increased by several methods. In addition to black masking of unused areas of screen, the phosphor particles in color screens are coated with pigments of matching color. For example, the red phosphors are coated with ferric oxide (replacing earlier Cd(S,Se) due to cadmium toxicity), blue phosphors can be coated with marine blue (CoO·\"n\") or ultramarine (). Green phosphors based on ZnS:Cu do not have to be coated due to their own yellowish color.\n\nThe black-and-white television screens require an emission color close to white. Usually, a combination of phosphors is employed.\n\nThe most common combination is ZnS:Ag+(Zn,Cd)S:Cu,Al (blue+yellow). Other ones are ZnS:Ag+(Zn,Cd)S:Ag (blue+yellow), and ZnS:Ag+ZnS:Cu,Al+YOS:Eu (blue + green + red – does not contain cadmium and has poor efficiency). The color tone can be adjusted by the ratios of the components.\n\nAs the compositions contain discrete grains of different phosphors, they produce image that may not be entirely smooth. A single, white-emitting phosphor, (Zn,Cd)S:Ag,Au,Al overcomes this obstacle. Due to its low efficiency, it is used only on very small screens.\n\nThe screens are typically covered with phosphor using sedimentation coating, where particles suspended in a solution are let to settle on the surface.\n\nFor displaying of a limited palette of colors, there are a few options.\n\nIn beam penetration tubes, different color phosphors are layered and separated with dielectric material. The acceleration voltage is used to determine the energy of the electrons; lower-energy ones are absorbed in the top layer of the phosphor, while some of the higher-energy ones shoot through and are absorbed in the lower layer. So either the first color or a mixture of the first and second color is shown. With a display with red outer layer and green inner layer, the manipulation of accelerating voltage can produce a continuum of colors from red through orange and yellow to green.\n\nAnother method is using a mixture of two phosphors with different characteristics. The brightness of one is linearly dependent on electron flux, while the other one's brightness saturates at higher fluxes—the phosphor does not emit any more light regardless of how many more electrons impact it. At low electron flux, both phosphors emit together; at higher fluxes, the luminous contribution of the nonsaturating phosphor prevails, changing the combined color.\n\nSuch displays can have high resolution, due to absence of two-dimensional structuring of RGB CRT phosphors. Their color palette is, however, very limited. They were used e.g. in some older military radar displays.\n\nThe phosphors in color CRTs need higher contrast and resolution than the black-and-white ones. The energy density of the electron beam is about 100 times greater than in black-and-white CRTs; the electron spot is focused to about 0.2 mm diameter instead of about 0.6 mm diameter of the black-and-white CRTs. Effects related to electron irradiation degradation are therefore more pronounced.\n\nColor CRTs require three different phosphors, emitting in red, green and blue, patterned on the screen. Three separate electron guns are used for color production.\n\nThe composition of the phosphors changed over time, as better phosphors were developed and as environmental concerns led to lowering the content of cadmium and later abandoning it entirely. The (Zn,Cd)S:Ag,Cl was replaced with (Zn,Cd)S:Cu,Al with lower cadmium/zinc ratio, and then with cadmium-free ZnS:Cu,Al.\n\nThe blue phosphor stayed generally unchanged, a silver-doped zinc sulfide. The green phosphor initially used manganese-doped zinc silicate, then evolved through silver-activated cadmium-zinc sulfide, to lower-cadmium copper-aluminium activated formula, and then to cadmium-free version of the same. The red phosphor saw the most changes; it was originally manganese-activated zinc phosphate, then a silver-activated cadmium-zinc sulfide, then the europium(III) activated phosphors appeared; first in an yttrium vanadate matrix, then in yttrium oxide and currently in yttrium oxysulfide. The evolution of the phosphors was therefore:\n\nFor projection televisions, where the beam power density can be two orders of magnitude higher than in conventional CRTs, some different phosphors have to be used.\n\nFor blue color, ZnS:Ag,Cl is employed. However, it saturates. (La,Gd)OBr:Ce,Tb can be used as an alternative that is more linear at high energy densities.\n\nFor green, a terbium-activated GdOTb; its color purity and brightness at low excitation densities is worse than the zinc sulfide alternative, but it behaves linear at high excitation energy densities, while zinc sulfide saturates. However, it also saturates, so YAlO:Tb or YSiO:Tb can be substituted. LaOBr:Tb is bright but water-sensitive, degradation-prone, and the plate-like morphology of its crystals hampers its use; these problems are solved now, so it is gaining use due to its higher linearity.\n\nYOS:Eu is used for red emission.\n\nSome other phosphors commercially available, for use as X-ray screens, neutron detectors, alpha particle scintillators, etc., are:\n\n\n"}
{"id": "6754266", "url": "https://en.wikipedia.org/wiki?curid=6754266", "title": "Quarter sawing", "text": "Quarter sawing\n\nQuarter sawing also quarter-cut is a type of cut in the rip-sawing of logs into lumber. The resulting lumber is called \"quartersawn\" (quarter-sawn), \"quartered\", and \"radially-sawn\". There is widespread confusion between the terms quartersawn and \"riftsawn\" with both words defined with opposite meanings and as synonyms.\n\nQuarter-sawn boards have greater stability of form and size with less cupping, shrinkage across the width, shake and splitting, and other good qualities. In some woods, such as oak, the grain produces a decorative effect which shows a prominent ray fleck and sapele is likely to produce a ribbon figure.\n\nWhen boards are cut from a log they are usually rip cut along the length (axis) of the log. This can be done in three ways: plain-sawing (most common, also known as flat-sawn, bastard-sawn, through and through, and tangent-sawn), quarter-sawing (less common), or rift sawing (rare).\n\nIn flat-sawing the log is passed through the blade cutting off plank after plank without changing the orientation of the blade or log. The resulting planks have different annual ring orientations when viewed from the end. The relative angle that form the rings and the surface go from almost zero degrees in the external planks to almost ninety degrees at the core of the log.\n\nQuarter sawing gets its name from the fact that the log is first quartered lengthwise, resulting in wedges with a right angle ending at approximately the center of the original log. Each quarter is then cut separately by tipping it up on its point and sawing boards successively along the axis. That results in boards with the annual rings mostly perpendicular to the faces. Quarter sawing yields boards with straight striped grain lines, greater stability than flatsawn wood, and a distinctive ray and fleck figure. It also yields narrower boards, because the log is first quartered, which is more wasteful.\n\nQuartersawn boards can also be produced by cutting a board from one flat face of the quarter, flipping the wedge onto the other flat face to cut the next board, and so on.\n\nThe William Ritter Lumber Company (1890–1960), famous for its Appalachian oak flooring and other products, used a modified technique to reduce waste: (1) bark and a few boards were removed from two opposite sides of the log; (2) the log was cut in half \"(possibly, four quarters)\"; (3) each piece was placed on the flat side and \"quartersawn\" lumber was cut. \"(Note: no reference is made in this source to the notion of \"flipping the wedge\" as described in the preceding paragraph. Apparently, the cuts were made without \"flipping.\")\"\n\nQuarter sawing is sometimes confused with the much less common \"rift sawing.\" In quartersawn wood, only the center board of the quarter-log is cut with the growth rings truly perpendicular to the surface of the board. The smaller boards cut from either side have grain increasingly skewed. Riftsawn wood has every board cut along a radius of the original log, so each board has a perpendicular grain, with the growth rings oriented at right angles to the surface of the board. However, since this produces a great deal of waste (in the form of wedge-shaped scraps from between the boards) rift-sawing is very seldom used. Quartersawn wood is thus seen as an acceptable compromise between economical but less-stable flatsawn wood (which, especially in oak, will often display the distinct \"cathedral window\" grain) and the expensively-wasteful rift sawn wood, which has the straightest grain and thus the greatest stability.\n\nQuartersawn boards have two advantages: they are more resistant against warping with changes in moisture and, while shrinkage can occur, it is less troublesome.\n\nIn high-end string instruments, the neck and fretboards can be made from quartersawn wood since they must remain stable throughout the life of the instrument, to keep the tone as invariable as possible. In acoustic guitars, quartersawn wood is also often used for the sides which must be steam bent to produce compound curves. This is partly for structural reasons, but also for the aesthetics of highly figured timbers being highlighted when sawn this way. On high-end electric guitars and bass guitars quartersawn wood is often used as the base material for the neck of the guitar, since this makes for a stronger and straighter neck which aids tuning and setup stability.\n\nThe second advantage of quartersawn wood is the decorative pattern on the board, although this depends on the timber species. Flat sawn wood (especially oak) will often display a prominent wavy grain (sometimes called a cathedral-window pattern) caused by the saw cutting at a tangent to a growth ring; since in quartersawn wood the saw cuts across the growth rings, the visible grain is much straighter; it is this evenness of the grain that gives quartersawn wood its greater stability.\n\nIn addition to the grain, quartersawn wood (particularly oak) will also often display a pattern of medullary rays, seen as subtle wavy ribbon-like patterns across the straight grain. Medullary rays grow in a radial fashion in the living tree, so while flat-sawing would cut across the rays, quarter-sawing puts them on the face of the board. This ray pattern has made quartersawn wood especially desirable for furniture and decorative panelling.\n\nQuartersawn oak was a key feature of the decorative style of the American Arts and Crafts movement, particularly the work of Gustav Stickley, who said \"The quartersawing method of cutting... renders quartersawn oak structurally stronger, also finer in grain, and, as shown before, less liable to warp and check than when sawn in any other way.\" Cheaper copies of Stickley's furniture were sometimes made with the less-expensive ash stained to resemble oak, but it can be identified by its lack of rays.\n\nWood cut in this way is prized for certain applications, but it will tend to be more expensive as well. In cutting a log, quarter sawn boards can be produced in several ways, but if a log is cut for maximum yield it will produce only a few quarter sawn boards among the total; if a log is cut to produce only quarter sawn boards there will be considerable waste.\n\nThe process indicated in the US as \"quarter sawing\" yields a few boards that are quartersawn, but mostly rift sawn boards.\n\n"}
{"id": "26422414", "url": "https://en.wikipedia.org/wiki?curid=26422414", "title": "SDE Sea Waves Power Plant", "text": "SDE Sea Waves Power Plant\n\nSDE Sea Wave Power Plant is a type of renewable energy power plant utilizing sea wave energy for the production of electricity.\n\nThe device is made of horizontal buoys, one end of which is attached to a breakwater, or on some other sea-based structure, which create a vertical motion, according to the frequency of the sea wave. The buoys' movement presses on a hydraulic liquid, which is regulated by systems that convert the energy into circular systems. These systems operate an electricity generator and the process culminates in electricity production. The system's innovation is based on its self-correcting mechanism whereby, should a large wave overwhelm the buoy, it would flip over and then \"wait\" for a lower tide to flip back. The system's high survivability capability is based on the fact that only 10% of its components are submerged in sea water.\n\nThe device is developed by Israeli company S.D.E. Energy LTD. S.D.E. has built and tested twelve different models of its system, culminating with a full-scale model that operated and tested in Jaffa Port near Tel Aviv in Israel and produced 40 kW for a period of a year.\n\nIn March, 2010, S.D.E. is looking at building a 250 kW model also in the port of Jaffa near Tel Aviv, and planning to construct a 100 MW power plant as one of its projects in the islands of Kosrae, Micronesia and Zanzibar.\n\n"}
{"id": "24281365", "url": "https://en.wikipedia.org/wiki?curid=24281365", "title": "Sand geyser", "text": "Sand geyser\n\nA sand geyser, sand fountain or sand blow is a geologic phenomena which occurs in association with earthquakes and other seismic events. In the geologic record, these are seen as clastic dikes. It is described as \"a geyser of sand and water that shoots from the ground during a major earthquake.\" A quake can cause underlying sand to liquefy while pressure forces the eruption of the sand mixture to the surface. The mixture of sand and water can also contain dissolved gases such as methane and carbon dioxide. \n\nSome investigators have located soil formations that indicate the existence of past sand geysers in earthquake prone areas.\n\nNASA has proposed that the existence of sand geysers on the surface of Mars explains some of the seasonal variations of light and dark areas. Plume-like markings that begin to appear during the martian spring may be caused by solid CO transforming explosively into its gaseous state causing an eruption of soil materials.\n\nA 2008 video recorded one eruption in Saudi Arabia. Another lay commentator attributed a similar event to differences in air temperature between underground pockets of air and the air above the ground. The phenomenon was observed during an earthquake in New Zealand in 1987. \n\n \n"}
{"id": "236827", "url": "https://en.wikipedia.org/wiki?curid=236827", "title": "Shekel", "text": "Shekel\n\nShekel (Akkadian: \"šiqlu\" or \"siqlu\"; , . shekels or sheqalim) is any of several ancient units of weight or of currency. Since it was a coin that represented a claim on a weight of barley held in the city warehouse, the term \"shekel\" was likely used in both contexts: 1) As the name of the coin, and; 2) To describe the measure of barley. This coin weighed about 180 grains (11 grams or .35 troy ounces).\n\nThe Hebrew word \"shekel\" is based on the Semitic verbal root for \"weighing\" (), cognate to the Akkadian \"šiqlu\" or \"siqlu\", a unit of weight equivalent to the Sumerian \"gin2\". Use of the word was first attested in c. 2150 BC during the Akkadian Empire under the reign of Naram-Sin, and later in c. 1700 BC in the Code of Hammurabi. The root is found in the Hebrew words for \"to weigh\" (\"shaqal\"), \"weight\" (\"mishqal\") and \"consideration\" (\"shiqqul\"), and is related to the root in Aramaic and the root in Arabic, such as the words \"thiqal\" (the weight) or \"Mithqal\" (unit of weight). The famous writing on the wall in the Biblical Book of Daniel includes a cryptic use of the word in Aramaic: \"Mene, mene, teqel, u-farsin\". The word \"shekel\" came into the English language via the Hebrew Bible, where it is first used in the Book of Genesis.\n\nThe earliest shekels were a unit of weight, used as other units such as grams and troy ounces for trading before the advent of coins. The shekel was common among western Semitic peoples. Moabites, Edomites, and Phoenicians used the shekel, although proper coinage developed very late. Carthaginian coinage was based on the shekel and may have preceded its home town of Tyre in issuing proper coins. \n\nCoins were used and may have been invented by the early Anatolian traders who stamped their marks to avoid weighing each time used. Herodotus states that the first coinage was issued by Croesus, King of Lydia, spreading to the golden Daric (worth 20 \"sigloi\" or shekel), issued by the Persian Empire and the silver Athenian obol and drachma. Early coins were money stamped with an official seal to certify their weight. Silver ingots, some with markings were issued. Later authorities decided who designed coins. \n\nAs with many ancient units, the shekel had a variety of values depending on era, government and region; weights between 7 and 17 grams and values of 11, 14, and 17 grams are common. When used to pay laborers, recorded wages in the ancient world range widely. The Code of Hammurabi (circa 1800 BC) sets the value of unskilled labor at approximately ten shekels per year of work. Later, records within the Persian Empire (539-333 BC) give ranges from a minimum of two shekels per month for unskilled labor, to as high as seven to ten shekels per month in some records. A survival wage for an urban household during the Persian period would require at least 22 shekels of income per year.\n\n notes that the measures of the ingredients for the holy anointing oil were to be calculated using the Shekel of the Sanctuary (see also , and similarly at for payment for the redemption of 273 first-born males and at for the offerings of the leaders of the tribes of Israel), suggesting that there were other common measures of shekel in use, or at least that the Temple authorities defined a standard for the shekel to be used for Temple purposes.\n\nAccording to Jewish law, whenever a census of the Jewish people was to be conducted, every person that was counted was required to pay the \"half-Shekel\" for his atonement ().\n\nThe Aramaic \"tekel\", similar to the Hebrew \"shekel\", used in the writing on the wall during the feast of Belshazzar according to the Book of Daniel and defined as weighed, shares a common root with the word shekel and may even additionally attest to its original usage as a weight.\n\nDuring the Second Temple period, it was customary among Jews to annually offer the \"half-Shekel\" into the Temple treasury, for the upkeep and maintenance of the Temple precincts, as also used in purchasing public animal-offerings. This practice not only applied to Jews living in the Land of Israel, but also to Jews living outside the Land of Israel. Archaeological excavations conducted at Horvat 'Ethry in Israel from 1999 to 2001 by Boaz Zissu and Amir Ganor of the Israeli Antiquities Authority (IAA) have yielded important finds, the most-prized of which being a \"half-Shekel\" coin minted in the 2nd century CE, upon which are embossed the words \"Half-Shekel\" in paleo-Hebrew (), and which same coin possesses a silver content of 6.87 grams. According to the Jewish historian Josephus, the annual monetary tribute of the half-Shekel to the Temple at Jerusalem was equivalent to two Athenian drachmæ, each Athenian or Attic drachma weighing a little over 4.3 grams.\n\nThe Jerusalem shekel was issued from  66 to 70 amid the First Jewish Revolt as a means of emphasizing the independence of Judaea from Roman rule.\n\nThe Bar Kochba shekel was issued from  132 to 135 amid the Bar Kokhba Revolt for similar reasons.\n\nThe Carthaginian or Punic shekel was typically around 7.2 grams in silver and 7.5 grams in gold (suggesting an exchange rate of 12:1). They were apparently first developed on Sicily during the mid-4th century . They were particularly associated with the payment of Carthage's mercenary armies and were repeatedly debased over the course of each of the Punic Wars, although the Carthaginian Empire's expansion into Spain under the Barcids before the Second and recovery under Hannibal before the Third permitted improving the amount and quality of the currency. Throughout, it was more common for Carthage's holdings in North Africa to employ bronze or no coinage except when paying mercenary armies and for most of the specie to circulate in Spain, Sardinia, and Sicily.\n\nThe Tyrian shekel began to be issued  . Owing to the relative purity of their silver, they were the preferred medium of payment for the Temple tax in Jerusalem despite their royal and pagan imagery. The money changers assaulted by Jesus in the New Testament are those who exchanged worshippers' baser common currency for such shekels and they have been suggested as a possible coin used as the \"30 pieces of silver\" in the New Testament.\n\nThe Israeli shekel (properly sheqel) replaced the Israeli lira or pound in 1980. Its currency sign was , although it was more commonly denominated as S or IS. It was subdivided into 100 new agoras or agorot. It suffered from hyperinflation and was quickly replaced.\n\nThe new shekel replaced it in 1985. Its currency sign is although it is often denominated as S or NS. It is subdivided into 100 agoras or agorot. Both Israeli shekels are purely units of currency and not weight. With the 2014 series of notes, the Bank of Israel abandoned the transcriptions \"Sheqel\" and \"Sheqalim\" in favor of the standard English forms \"Shekel\" and \"Shekels\".\n\n"}
{"id": "4395461", "url": "https://en.wikipedia.org/wiki?curid=4395461", "title": "Shuuto", "text": "Shuuto\n\nThe or shootball is a baseball pitch. It is commonly thrown by right-handed Japanese pitchers such as Hiroki Kuroda, Noboru Akiyama, Kenjiro Kawasaki, Daisuke Matsuzaka, Yu Darvish and Masumi Kuwata. The most renowned \"shuuto\" pitcher in history was Masaji Hiramatsu, whose famous pitch was dubbed the \"razor\" \"shuuto\" because it seemed to \"\"\"cut the air\" when thrown.\n\nThe pitch is mainly designed to break down and in on right-handed batters, to prevent them from making solid contact with the ball. It can be thrown to left-handers to keep them off balance. Good \"shuuto\" pitches often break the bats of right-handed hitters because they get jammed when trying to swing at this pitch. It could be said that the \"shuuto\" has a somewhat similar break and purpose as the screwball. If the \"shuuto\" was thrown off the outside part of the plate, it would tail back over the outside border of the strike zone. Conversely, if it was thrown on the inside part of the plate, it would move even further inside.\n\nThe shuuto is often described in English as a \"reverse slider\", but this is not strictly the case. The shuuto generally has more velocity and less break than a slider. The two-seam fastball, the sinker, and the screwball, in differing degrees, move down and in towards a right-handed batter when thrown, or in the opposite manner of a curveball and a slider.\n\nThe \"shuuto\" is often confused with the gyroball, perhaps because of an article by Will Carroll that erroneously equated the two pitches. Although Carroll later corrected himself, the confusion persists.\n\nAccording to baseball analyst Mike Fast, the shuuto \"can describe any pitch that tails to the pitcher's arm side, including the two-seam fastball, the circle change-up, the screwball, and the split-finger fastball\".\n\nThe \"shuuto\" is mentioned in the movie \"Mr. Baseball\". This is the type of pitch that Tom Selleck's character is continually unable to hit, even though he is a left-handed batter. The \"shuuto\" is described as \"the great equalizer\".\n\nIn the third edition of The Dickson Baseball Dictionary \"shoot\" is explained as:\nMany Japanese pitchers utilize the pitching style of the shot due to the varying results of the ball twisting or sinking in flight. This is done in an attempt to outwit the batter and cause them to either miss the ball when swinging or increase the chance of a foul ball or easily fielded ball. A shot is more difficult to hit compared to a straight pitch because the batter must compensate for the eccentric movement of the ball between the time the ball leaves the pitcher’s hand and crosses home plate. American baseball utilizes terms such as slider, screwball, breaking ball, changeup or knuckleball instead of the Japanese term.\n\nThe pitch can be thrown with the same grip as Tsushima Fast Ball. Depending on the pitcher, the ball is grasped as shown.\n\nTwisting the wrist/forearm using an inward rotation burdens the elbow and can easily lead to injury. Kudo naturally rotated inward after beginning with his forearm outwardly rotated. He released it with a push of the ball using the middle finger without straining the elbow. By contrast Hiramatsu did not put his fingers on the seams. \n\nA baseball cliche states, \"Good pitchers of the curve slider have poor shots, good pitching pitchers have poor curves\". This is thought to be due to the difference in hand shape and the difference in pitching form. Aoyama Noboru stated, \"In Japan 's history of baseball, both curves and shoots were top notches is about Takehiko Bessho.\"\n\nShoots with good sharpness are called \"razor shoot\" or \"high speed shoot\". Hisamfumi Kawamura, Ryutaro Imanishi, Akiyama Climbing, Hiramatsu's shoots are called razor shoots, Hiramatsu's shot demonstrated its power especially to the right batter. Shoots such as Morita Yukihi and Kobayashi Masahide are sometimes referred to as high-speed shoots because ball speeds exceeding 150  km/h have emerged.\n\nIf the pitcher throws a straight ball, or if the release point shifts, the ball may have a rotation similar to the shoot, which is called shoot rotation or natural shoot. It is a mistake to throw a shot with the intention of throwing a straight ball. This requires a shift in the release point and the rotation become loose, the lateral change is also small, and the control is often lost. When a straight ball intended to be thrown aimed on a diagonal (crossfire) shoot rotates, it heads towards the center of the strike zone, so it may be easy to hit. \n\nIt is recognized that this method is not stable, but some pitchers use this as a weapon. Teruhiro Kuroki says \"put power in the middle finger\" against the alternative \"put power in the index finger\" of other pitchers when throwing the slider.\n"}
{"id": "29005742", "url": "https://en.wikipedia.org/wiki?curid=29005742", "title": "Son of Town Hall", "text": "Son of Town Hall\n\nSon of Town Hall was a junk raft which made a Transatlantic crossing in 1998, built by Poppa Neutrino.\n\nWriter Alec Wilkinson gave a vivid description of \"Son of Town Hall\" in his book \"The Happiest Man in the World\", saying: \"The raft looked like a specter, a ghost ship, as if made from rags and rope and lumber, a vessel from the end of the world, or something medieval, a flagship of nothingness, the Armada of the Kingdom of Oblivion.\"\n\n\n"}
{"id": "270806", "url": "https://en.wikipedia.org/wiki?curid=270806", "title": "Surgical stainless steel", "text": "Surgical stainless steel\n\nSurgical stainless steel is a grade of stainless steel used in biomedical applications. The most common \"surgical steels\" are austenitic 316 stainless and martensitic 440 and 420 stainless steels. There is no formal definition on what constitutes a \"surgical stainless steel\", so product manufacturers and distributors apply the term to refer to any grade of corrosion resistant steel.\n\n316 stainless steel, also referred to as marine grade stainless steel, is a chromium, nickel, molybdenum alloy of steel that exhibits relatively good strength and corrosion resistance. Along with the titanium alloy Ti6Al4V, 316 stainless is a common choice of material for biomedical implants. Although Ti6Al4V provides greater strength per weight and corrosion resistance, 316 stainless components can be more economical to produce. However, immune system reaction to nickel is a potential complication of 316. \nImplants and equipment that are put under pressure (bone fixation screws, prostheses, body piercing jewelry) are made out of austenitic steel, often 316L and 316LVM compliant to ASTM F138.\n316 surgical steel is used in the manufacture and handling of food and pharmaceutical products where it is often required in order to minimize metallic contamination. \nASTM F138-compliant steel is also used in the manufacture of body piercing jewellery and body modification implants.\n\n440 and 420 stainless steels, known also by the name \"Cutlery Stainless Steel\", are high carbon steels alloyed with chromium. They have very good corrosion resistance compared to other cutlery steels, but their corrosion resistance is inferior to 316 stainless. Biomedical cutting instruments are often made from 440 or 420 stainless due to its high hardness coupled with acceptable corrosion resistance. This type of stainless steel may be slightly magnetic.\n\n"}
{"id": "37128795", "url": "https://en.wikipedia.org/wiki?curid=37128795", "title": "Talcher–Kolar HVDC system", "text": "Talcher–Kolar HVDC system\n\nThe Talcher–Kolar HVDC system, otherwise known as the East–South interconnection II is a 1450 km HVDC transmission connection between the eastern and southern regions in India connecting four states namely Odisha, Andhra Pradesh, Tamil Nadu and Karnataka. The system has a transmission voltage of ±500 kV and was originally put into service in March 2003, with a rated power of 2000 MW. In 2007 the scheme was upgraded to 2500 MW.\n\nThe Talcher–Kolar HVDC system is owned by Power Grid Corporation of India and the converter stations were built by Siemens.\n\nThe scheme is a conventional bipolar system with overhead lines for the high-voltage conductors and ground return for the neutral current.\n\nThe scheme includes two ground electrodes, of ring construction, located at Rohila, 28.5 km northeast of the Talcher converter station and at Chikkadasarahalli, 30 km northwest of the Kolar converter station.\n\nHowever, the soil conditions at some distance from the electrode at Chikkadasarahalli were later found to have higher than expected soil resistivity. These geological effects gave rise to problems with DC currents flowing into the neutral connections of transformers at nearby AC substations, which required the addition of DC blocking devices.\n\n"}
{"id": "2116830", "url": "https://en.wikipedia.org/wiki?curid=2116830", "title": "Trajectory optimization", "text": "Trajectory optimization\n\nTrajectory optimization is the process of designing a trajectory that minimizes (or maximizes) some measure of performance while satisfying a set of constraints. Generally speaking, trajectory optimization is a technique for computing an open-loop solution to an optimal control problem. It is often used for systems where computing the full closed-loop solution is either impossible or impractical.\n\nAlthough the idea of trajectory optimization has been around for hundreds of years (calculus of variations, brachystochrone problem), it only became practical for real-world problems with the advent of the computer. Many of the original applications of trajectory optimization were in the aerospace industry, computing rocket and missile launch trajectories. More recently, trajectory optimization has also been used in a wide variety of industrial process and robotics applications.\n\nTrajectory optimization first showed up in 1697, with the introduction of the Brachystochrone problem: find the shape of a wire such that a bead sliding along it will move between two points in the minimum time. The interesting thing about this problem is that it is optimizing over a curve (the shape of the wire), rather than a single number. The most famous of the solutions was computed using calculus of variations.\n\nIn the 1950s, the digital computer started to make trajectory optimization practical for solving real-world problems. The first optimal control approaches grew out of the calculus of variations, based on the research of Gilbert Ames Bliss and Bryson in America, and Pontryagin in Russia. Pontryagin's maximum principle is of particular note. These early researchers created the foundation of what we now call indirect methods for trajectory optimization.\n\nMuch of the early work in trajectory optimization was focused on computing rocket thrust profiles, both in a vacuum and in the atmosphere. This early research discovered many basic principles that are still used today. \nAnother successful application was the climb to altitude trajectories for the early jet aircraft. Because of the high drag associated with the transonic drag region and the low thrust of early jet aircraft, trajectory optimization was the key to maximizing climb to altitude performance. Optimal control based trajectories were responsible for some of the world records. In these situations, the pilot followed a Mach versus altitude schedule based on optimal control solutions.\n\nOne of the important early problems in trajectory optimization was that of the singular arc, where Pontryagin's maximum principle fails to yield a complete solution. An example of a problem with singular control is the optimization of the thrust of a missile flying at a constant altitude and which is launched at low speed. Here the problem is one of a bang-bang control at maximum possible thrust until the singular arc is reached. Then the solution to the singular control provides a lower variable thrust until burnout. At that point bang-bang control provides that the control or thrust go to its minimum value of zero. This solution is the foundation of the boost-sustain rocket motor profile widely used today to maximize missile performance.\n\nThere are a wide variety of applications for trajectory optimization, primarily in robotics: industry, manipulation, walking, path-planning, and aerospace. It can also be used for modeling and estimation.\n\nTrajectory optimization is often used to compute trajectories for quadrotor helicopters. These applications typically used highly specialized algorithms.\nOne interesting application shown by the U.Penn GRASP Lab is computing a trajectory that allows a quadrotor to fly through a hoop as it is thrown. Another, this time by the ETH Zurich Flying Machine Arena, involves two quadrotors tossing a pole back and forth between them, with it balanced like an inverted pendulum.\n\nTrajectory optimization is used in manufacturing, particularly for controlling chemical processes (such as in \n\n) or computing the desired path for robotic manipulators (such as in\n\nThere are a variety of different applications for trajectory optimization within the field of walking robotics. For example, one paper used trajectory optimization of bipedal gaits on a simple model to show that walking is energetically favorable for moving at a low speed and running is energetically favorable for moving at a high speed.\n\nLike in many other applications, trajectory optimization can be used to compute a nominal trajectory, around which a stabilizing controller is built.\n\nTrajectory optimization can be applied in detailed motion planning complex humanoid robots, such as Atlas.\n\nFinally, trajectory optimization can be used for path-planning of robots with complicated dynamics constraints, using reduced complexity models.\nFor tactical missiles, the flight profiles are determined by the thrust and lift histories. These histories can be controlled by a number of means including such techniques as using an angle of attack command history or an altitude/downrange schedule that the missile must follow. Each combination of missile design factors, desired missile performance, and system constraints results in a new set of optimal control parameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe techniques to any optimization problems can be divided into two categories: indirect and direct. An indirect method works by analytically constructing the necessary and sufficient conditions for optimality, which are then solved numerically. A direct method attempts a direct numerical solution by constructing a sequence of continually improving approximations to the optimal solution. Direct and indirect methods can be blended by an application of the covector mapping principle of Ross and Fahroo.\n\nThe optimal control problem is an infinite-dimensional optimization problem, since the decision variables are functions, rather than real numbers. All solution techniques perform transcription, a process by which the trajectory optimization problem (optimizing over functions) is converted into a constrained parameter optimization problem (optimizing over real numbers). Generally, this constrained parameter optimization problem is a non-linear program, although in special cases it can be reduced to a quadratic program or linear program.\n\nSingle shooting is the simplest type of trajectory optimization technique. The basic idea is similar to how you would aim a cannon: pick a set of parameters for the trajectory, simulate the entire thing, and then check to see if you hit the target. The entire trajectory is represented as a single segment, with a single constraint, known as a defect constraint, requiring that the final state of the simulation match the desired final state of the system. Single shooting is effective for problems that are either simple or have an extremely good initialization. Both the indirect and direct formulation tend to have difficulties otherwise.\nMultiple shooting is a simple extension to single shooting that renders it far more effective. Rather than representing the entire trajectory as a single simulation (segment), the algorithm breaks the trajectory into many shorter segments, and a defect constraint is added between each. The result is large sparse non-linear program, which tends to be easier to solve than the small dense programs produced by single shooting.\n\nDirect collocation methods work by approximating the state and control trajectories using polynomial splines. These methods are sometimes referred to as direct transcription. Trapezoidal collocation is a commonly used low-order direct collocation method. The dynamics, path objective, and control are all represented using linear splines, and the dynamics are satisfied using trapezoidal quadrature. Hermite-Simpson Collocation is a common medium-order direct collocation method. The state is represented by a cubic-Hermite spline, and the dynamics are satisfied using Simpson quadrature.\n\nOrthogonal collocation is technically a subset of direct collocation, but the implementation details are so different that it can reasonably be considered its own set of methods. Orthogonal collocation differs from direct collocation in that it typically uses high-order splines, and each segment of the trajectory might be represented by a spline of a different order. The name comes from the use of orthogonal polynomials in the state and control splines.\n\nPseudospectral collocation, also known as global collocation, is a subset of orthogonal collocation in which the entire trajectory is represented by a single high-order orthogonal polynomial. As a side note: some authors use orthogonal collocation and pseudospectral collocation interchangeably. When used to solve a trajectory optimization problem whose solution is smooth, a pseudospectral method will achieve spectral (exponential) convergence.\n\nDifferential dynamic programming, is a bit different than the other techniques described here. In particular, it does not cleanly separate the transcription and the optimization. Instead, it does a sequence of iterative forward and backward passes along the trajectory. Each forward pass satisfies the system dynamics, and each backward pass satisfies the optimality conditions for control. Eventually, this iteration converges to a trajectory that is both feasible and optimal.\n\nThere are many techniques to choose from when solving a trajectory optimization problem. There is no best method, but some methods might do a better job on specific problems. This section provides a rough understanding of the trade-offs between methods.\n\nWhen solving a trajectory optimization problem with an indirect method, you must explicitly construct the adjoint equations and their gradients. This is often difficult to do, but it gives an excellent accuracy metric for the solution. Direct methods are much easier to set up and solve, but do not have a built-in accuracy metric. As a result, direct methods are more widely used, especially in non-critical applications. Indirect methods still have a place in specialized applications, particularly aerospace, where accuracy is critical.\n\nOne place where indirect methods have particular difficulty is on problems with path inequality constraints. These problems tend to have solutions for which the constraint is partially active. When constructing the adjoint equations for an indirect method, the user must explicitly write down when the constraint is active in the solution, which is difficult to know a priori. One solution is to use a direct method to compute an initial guess, which is then used to construct a multi-phase problem where the constraint is prescribed. The resulting problem can then be solved accurately using an indirect method.\n\nSingle shooting methods are best used for problems where the control is very simple (or there is an extremely good initial guess). For example, a satellite mission planning problem where the only control is the magnitude and direction of an initial impulse from the engines.\n\nMultiple shooting tends to be good for problems with relatively simple control, but complicated dynamics. Although path constraints can be used, they make the resulting nonlinear program relatively difficult to solve.\n\nDirect collocation methods are good for problems where the accuracy of the control and the state are similar. These methods tend to be less accurate then others (due to their low-order), but are particularly robust for problems with difficult path constraints.\n\nOrthogonal collocation methods are best for obtaining high-accuracy solutions to problems where the accuracy of the control trajectory is important. Some implementations have trouble with path constraints. These methods are particularly good when the solution is smooth.\n\nIt is common to solve a trajectory optimization problem iteratively, each time using a discretization with more points. A h-method for mesh refinement works by increasing the number of trajectory segments along the trajectory, while a p-method increases the order of the transcription method within each segment.\n\nDirect collocation methods tend to exclusively use h-method type refinement, since each method is a fixed order. Shooting methods and orthogonal collocation methods can both use h-method and p-method mesh refinement, and some use a combination, known as hp-adaptive meshing. It is best to use h-method when the solution is non-smooth, while a p-method is best for smooth solutions.\n\nExamples of trajectory optimization programs include:\n\nA collection of low thrust trajectory optimization tools, including members of the Low Thrust Trajectory Tool (LTTT) set, can be found here: LTTT Suite Optimization Tools.\n"}
{"id": "32544581", "url": "https://en.wikipedia.org/wiki?curid=32544581", "title": "Uniformity tape", "text": "Uniformity tape\n\nUniformity tape is a microstructured thin-film mechanism for mixing and diffusing the light generated by light-emitting diodes (LEDs) in edge-lit digital displays, including monitors, televisions and signage.\n\nCompared to other sources of illumination, such as fluorescent and incandescent bulbs, LEDs are energy efficient and increasingly inexpensive. As hard-point light sources, however, LEDs have several significant limitations in edge-lit digital displays.\n\nFirst, the light generated by LEDs must be spread evenly to all parts of the display by a light guide (typically a plate of poly(methyl methacrylate)), which transports light by total internal reflection (TIR). Extraction patterns on the surface of the light-guide meter out the light and generate a uniform brightness distribution.\n\nEven with a light guide, dark zones can be noticeable along the injection edge closest to the LEDs. The existence of these dark zones requires that the injection edge be shielded by a border or bezel. The necessity of incorporating a bezel limits design options and the space available for displaying information.\n\nDark zones also influence the spacing between LEDs, which is typically between about 9 mm and 11 mm; placing LEDS farther apart creates more pronounced dark zones. This, in turn, limits the ability of designers to reduce the number of LEDs in a display despite the cost advantages and increased energy efficiency of such a reduction. Furthermore, the poor light-mixing of current light guides makes digital displays highly sensitive to variations in LED color and brightness. Closely packed LEDs can also create thermal management issues.\n\nThe first microstructured light-mixing tape was scheduled to be introduced in late 2011. 3M™ Uniformity Tape is designed to be applied directly to the injection edge of a light guide for the purpose of defusing the light generated by LEDs. The microstructure consists of linear aspheric prisms (Figure 1) that are aligned perpendicular to the plane of the guide. The size of the microstructure is small compared to the size of the LEDs, typically on the order of 12-50 μm. As a result, no registration of the tape with the LEDs is required.\n\nFigure 1.\nSchematic of Tape on Light Guide and Tape Microstructure\n\nMicrostructured Uniformity Tape greatly reduces dark zones by inducing lateral light-spreading in the plane of the light guide.\n\nThe effect of the 3M Uniformity Tape on the injection of LED light from a nominally lambertian LED is shown in Figure 2a-c. The left image in Figure 2a shows the injection cone angle of the LED into a light guide under normal TIR conditions (+/-42 degrees in a conventional PMMA light guide with an optically flat entrance face). The right image shows a close-up view of the left image. Figures 2b and 2c (and their details on the right) show the effects of two variations of 3M Uniformity Tape, with refractive index equal to 1.54 and 1.62, respectively.\n\nFigure 2.\nDemonstration of the Light Spreading Enabled by Uniformity Tape\nResearch conducted by 3M indicates that the tape enables a 50 percent reduction in LED count. Additional redesign of the light guide's extraction pattern could lead to an LED reduction approaching two-thirds of current levels. The tape also opens opportunities for digital displays with lower cost, higher performance and new design flexibility, including the development of a robust backlight that will retain a uniform brightness despite multiple LED failures.\n\nThese benefits can be achieved with little effect on overall system efficiency. Injection losses can be as low as 1-2 percent with a properly designed light guide.\n\nApplication of Uniformity Tape also appears to provide ancillary benefits. When applied to the input edge of the light guide, the optically clear adhesive will wet out and conform to the surface roughness of the light guide edge so that the film and light guide were optically coupled (i.e., no air is trapped between them). As a result of this optical coupling, the PMMA plate does not require the level of polishing required in a conventional light guide; this suggests that Uniformity Tape could lead to a reduction in manufacturing steps and lower costs.\n\n\n"}
{"id": "997664", "url": "https://en.wikipedia.org/wiki?curid=997664", "title": "Used good", "text": "Used good\n\nA secondhand or used good is a piece of personal property that is being purchased by or otherwise transferred to a second or later end user. A \"used\" good can also simply mean it is no longer in the same condition as it was when transferred to the current owner. When the term \"used\" means that an item has expended its purpose (such as a used diaper), it is typically called garbage, instead. \n\nUsed goods may be transferred informally between friends and family for free as hand-me-downs. They may be sold for a fraction of their original price at garage sales, in bazaar-style fundraisers, in privately owned consignment shops, or through online auctions. Some things are typically sold in specialized shops, such as a car dealership that specializes in the sale of used vehicles or a used bookstore that sells used books. In other cases, such as a charity shop, a wide variety of used goods might be handled by the same establishment. High-value used luxury goods, such as antique furniture, jewelry, watches, and artwork, might be sold through a generic auction house such as Sotheby's or a more specialized niche like Bob's Watches.\n\nGovernments require some used goods to be sold through regulated markets, as in the case of items which have safety and legal issues, such as used firearms or cars. For these items, government licensing bodies require certification and registration of the sale, to prevent the sale of stolen, unregistered, or unsafe goods. For some high-value used goods, such as cars and motorcycles, governments regulate sales of used goods to ensure that the government gets its sales tax revenue from the sale.\n\nSecondhand goods can benefit the purchaser as the price paid is lower than that of the same items bought new. If the reduction in price more than compensates for the possibly shorter remaining lifetime, lack of warranty, and so on, there is a net benefit.\n\nSelling unwanted goods secondhand instead of discarding them obviously benefits the seller.\n\nRecycling goods through the secondhand market reduces use of resources in manufacturing new goods, and diminishes waste which must be disposed of, both of which are significant environmental benefits. However, manufacturers who profit from sales of new goods lose corresponding sales. Scientific research shows that buying used goods reduces carbon footprint and CO2 emissions significantly compared to the complete product life cycle, because of less production, raw material sourcing and logistics. Often the relative carbon footprint of production, raw material sourcing and the supply chain is unknown. A scientific methodology has been made to analyze how much CO2 emissions are reduced when buying used goods like secondhand hardware versus new hardware.\n\nQuality secondhand goods can be more durable than equivalent new goods.\n\nSecondhand goods may have faults which are not apparent even if examined; purchasing sight unseen, for example, from an Internet auction site, has further unknowns. Goods may cause problems beyond their value; for example, furniture may have not easily seen bedbugs, which may cause an infestation which is difficult and expensive to eradicate. Faulty electrical and mechanical goods can be hazardous and dangerous. This is especially a big issue if sold to countries that do not have recycling facilities for these devices, which has led to an issue with electronic waste.\n\nMany items that are considered obsolete and worthless in developed countries, such as decade-old hand tools and clothes, are useful and valuable in impoverished communities in the country or in developing countries. Underdeveloped countries like Zambia are extremely welcoming to donated secondhand clothing. At a time when the country's economy was in severe decline, the used goods provided jobs by keeping \"many others busy with repairs and alterations\". It has created a type of spin-off economy at a time when many Zambians were out of work. The used garments and materials that were donated to the country also allowed for the production of \"a wide range of fabrics\" whose imports had been previously restricted. The trade is essentially executed by women who operate their small business based on local associations and networks. Not only does this provide self-employment, but it also increases household income and enhances the economy. But while many countries would be welcoming of secondhand goods, it is also true that there are countries in need who refuse donated items. Countries like Poland, Philippines and Pakistan have been known to reject secondhand items for \"fear of venereal disease and risk to personal hygiene\". Similar to these countries, India also refuses the import of secondhand clothing but will accept the import of wool fibers, including \"mutilated hosiery\" which is a term meaning \"woollen garments shredded by machine in the West prior to export\". Through the production of \"shoddy\" (recycled wool), most of which is produced in Northern India today, unused clothing can be recycled into fibres that are spun into yarn for reuse in 'new' used goods. \n\nUnited States taxpayers can deduct donations of used goods to charitable organizations. Both Goodwill Industries and the Salvation Army websites have lists of items with their estimated range of values. Another way that people transfer used goods is by giving them to friends or relatives. When a person gives an item of some value that they have used to someone else, such as a used car or a winter coat, it is sometimes referred to as a \"hand-me-down\".\n\nUsed items can often be found for sale in thrift stores and pawnshops, auctions, garage sales, and in more recent times online auctions. Some stores sell both new and used goods (e.g. car dealerships), while others only sell new goods but may take used items in exchange for credit toward the purchase of newer goods. For example, some musical instrument stores and high-end audio stores only sell new gear, but they will accept good quality used items as trade ins towards the purchase of new items; after the store purchases the used items, they then sell them using online auctions or other services.\n\nWhen an item is no longer of use to a person they may sell or \"pawn\" it, especially when they are in need of money. Items can also be sold (or taken away free of cost) as scrap (e.g. a broken-down old car will be towed away for free for its scrap metal value). Owners may sell the good themselves or to a dealer who then sells it on for a profit. They may also choose to give it away to another person this is often referred to as freecycling. However, because the process takes some effort on part of the owner they may simply keep possession of it or dump it at a landfill instead of going to the trouble of selling it. It has been common to buy secondhand or used good on markets or bazaars for long time. When the web became popular, it became common with websites such as eBay and Yahoo! Classifieds.\n\nThe strategy of buying used items is employed by some to save money, as they are typically worth less than the equivalent new items. Purchasing used items for reuse prevents them from becoming waste and saves costly production of equivalent new goods. Motivations for purchase include conserving natural resources and protecting the environment, and may form part of a simple living plan.\n\nUsed cars are especially notable for depreciating in value much faster than many other items. Used cars may have been bought or leased by their previous user, and may be purchased directly from the previous owner or through a dealer. George Akerlof published a paper entitled \"The Market for Lemons\", examining the effects of information asymmetry on the used car market. Used cars may require more maintenance or have fewer features than later equivalent models.\n\nUsed books are often re-sold through a used bookstore. They may also be given away, perhaps as part of a program such as the Little Free Library's programs. Used bookstores may also sell secondhand music recordings or videos.\n\nIn developed countries, unwanted used clothing is often donated to charities that sort and sell it. Some of these distribute some of the clothing to people on low incomes for free or a very low price. Others sell all of the collected clothing in bulk to a commercial used clothing redistributor, and then use the raised funds to finance their activities. In the U.S., almost 5 billion pounds of clothing is donated to charity shops each year. Only about 10% of it can be re-sold by the charity shops. About a third of the donated clothing is bought, usually in bulk and at a heavy discount, by commercial dealers and fabric recyclers, who export it to other countries. Some of the used clothes are also smuggled into Mexico.\n\nUsed clothing unsuitable for sale in an affluent market may still find a buyer or end-user in another market, such as a student market or a less affluent region of a developing country. In developing countries, such as Zambia, secondhand clothing is sorted, recycled and sometimes redistributed to other nations. Some of the scraps are kept and used to create unique fashions which enable the locals to construct identity. Not only does the trade represent a great source of employment for women as well as men, it also supports other facets of the economy: the merchants buy timber and other materials for their stands, metal hangers to display clothing, and food and drinks for customers. Carriers also find work as they transport the garments from factories to various locations. The secondhand clothing trade is central to the lives of many citizens dwelling in such countries. A dress agent will often deal with a buyer and seller directly, taking unwanted clothes that still have value, and reselling them in a shop.\n\nImportation of used clothing is sometimes opposed by the textile industry in developing countries. They are concerned that fewer people will buy the new clothes that they make when it is cheaper to buy imported used clothing. Nearly all the clothes made in Mexico are intended for export, and the Mexican textile industry opposes the importation of used clothes.\n\nThe Sierra Club, an environmental organization, argues that secondhand purchasing of furniture is the \"greenest\" way of furnishing a home. \n\nVintage guitars also became increasingly desired objects among musicians and collectors during the nineties and afterwards. Some music stores specialize in selling used musical instruments, used copies of printed music, and related paraphernalia.\n\nUsed Mobile Phones – A Large and Growing Business \n\n"}
{"id": "424335", "url": "https://en.wikipedia.org/wiki?curid=424335", "title": "Vostok Station", "text": "Vostok Station\n\nVostok Station (, , literally \"Station East\") is a Russian research station in inland Princess Elizabeth Land, Antarctica. Founded by the Soviet Union in 1957, the station lies at the southern Pole of Cold, with the lowest reliably measured natural temperature on Earth of . Research includes ice core drilling and magnetometry. Vostok (Russian for \"east\") was named after \"Vostok\", the lead ship of the First Russian Antarctic Expedition captained by Fabian von Bellingshausen (the second ship \"Mirny\" captained by Mikhail Lazarev became the namesake for Mirny Station).\n\nVostok Research Station is about 1,300 km (≈800 mi) from the Geographic South Pole, at the center of the East Antarctic Ice Sheet. \n\nVostok is located near the Southern Pole of Inaccessibility and the South Geomagnetic Pole, making it one of the optimal places to observe changes in the Earth's magnetosphere. Other studies include actinometry, geophysics, medicine and climatology.\n\nThe station is at above sea level and is one of the most isolated established research stations on the Antarctic continent. The station was supplied from Mirny Station on the Antarctic coast. The station typically contains 25 scientists and engineers in the summer. In winter, their number drops to 13.\n\nThe only permanent research station located farther south is the Amundsen–Scott South Pole Station, operated by the United States at the geographic South Pole. The Chinese Kunlun Station is farther south than Vostok but only occupied during summers.\n\nSome of the challenges faced by those living on the station were described in Vladimir Sanin's books such as \"Newbie in the Antarctic\" (1973), \"72 Degrees Below Zero\" (1975), and others.\n\nVostok Station was established on 16 December 1957 (during the International Geophysical Year) by the 2nd Soviet Antarctic Expedition and was operated year-round for more than 37 years. The station was temporarily closed from February to November 1994.\n\nIn 1974, when British scientists in Antarctica performed an airborne ice-penetrating radar survey and detected strange radar readings at the site, the presence of a liquid, freshwater lake below the ice did not instantly spring to mind. In 1991, Jeff Ridley, a remote-sensing specialist with the Mullard Space Science Laboratory at University College London, directed a European satellite called ERS-1 to turn its high-frequency array toward the center of the Antarctic ice cap. It confirmed the 1974 discovery, but it was not until 1993 that the discovery was published in the \"Journal of Glaciology\". Space-based radar revealed that the sub-glacial body of fresh water was one of the largest lakes in the world – and one of some 140 subglacial lakes in Antarctica. Russian and British scientists delineated the lake in 1996 by integrating a variety of data, including airborne ice-penetrating radar imaging observations and spaceborne radar altimetry. Lake Vostok lies some 4,000 meters (13,000 ft) below the surface of the central Antarctic ice sheet and covers an area of 14,000 km² (5,400 sq mi).\n\nVostok Station Tractor: Heavy tractor АТТ 11 which participated in the first traverse to the South Geomagnetic Pole, along with a plaque to commemorate the opening of the station in 1957, has been designated a Historic Site or Monument (HSM 11) following a proposal by Russia to the Antarctic Treaty Consultative Meeting.\n\nProfessor Kudryashov's Drilling Complex Building: The drilling complex building stands close to Vostok Station at an elevation of 3488 m. It was constructed in the summer season of 1983–1984. Under the leadership of Professor Boris Kudryashov, ancient ice core samples were obtained. The building has been designated a Historic Site or Monument (HSM 88), following a proposal by Russia to the Antarctic Treaty Consultative Meeting.\n\nVostok Station has an ice cap climate (\"EF\"), with subzero temperatures year round, typical as with much of Antarctica. Annual precipitation is only (all occurring as snow), making it one of the driest places on Earth. On average, Vostok station receives 26 days of snow per year. Average summer temperature is and average winter temperature is . It is one of the coldest spots on earth, with the lowest recorded temperature being .\n\nVostok Station is not the coldest known location on Earth. The now inactive Plateau Station, located on the central Antarctic plateau, recorded an average yearly temperature that was consistently lower than that of Vostok Station during the 37-month period that it was active in the late 1960s, with its average for the coldest month being several degrees lower than the same statistic for Vostok. Plateau Station never reached the record low set at Vostok. However, temperatures at Plateau Station were only recorded during the 37 months that it was active. Had a lower temperature than the Vostok record occurred there at a later date, it would never have been recorded.\n\nThe highest recorded temperature at Vostok was . It is also one of the sunniest places on Earth, despite having no sunshine at all between May and August; there are more hours of sunshine per year than even the sunniest places in South Africa. Vostok has the highest sunshine total for any calendar month on Earth, at an average of 708.8 hours of sunshine in December, or 22.9 hours daily. It also has the lowest sunshine for any calendar month, with an absolute maximum of 0 hours of sunshine per month during polar night.\n\nVostok is one of the coldest places on Earth. The average temperature of the cold season (from April to October) is about , while the average temperature of the warm season (from November to March) is about .\n\nThe lowest reliably measured temperature on Earth of was in Vostok on 21 July 1983 at 05:45 Moscow Time, which was 07:45 for Vostok's time zone, and 01:45 UTC (See \"List of weather records\"). This beat the station's former record of on 24 August 1960. Lower temperatures occurred higher up towards the summit of the ice sheet as temperature decreases with height along the surface.\n\nThe coldest wind chill was on 24 August 2005 with a real temperature of .\n\nThough unconfirmed, it has been reported that Vostok reached a temperature of on 28 July 1997.\n\nThe warmest recorded temperature at Vostok is , which occurred on 5 January 1974.\n\nThe coldest month was August 1987 with a mean temperature of and the warmest month was December 1989 with a mean temperature of .\n\nIn addition to the extremely cold temperatures, other factors make Vostok one of the most difficult places on Earth for human habitation:\n\nAcclimatization to such conditions can take from a week to two months and is accompanied by headaches, eye twitches, ear pains, nose bleeds, perceived suffocation, sudden rises in blood pressure, loss of sleep, reduced appetite, vomiting, joint and muscle pain, arthritis, and weight loss of 3–5 kg (7–11 lb) (sometimes as high as 12 kg (26 lb)).\n\nIn the 1970s the Soviet Union drilled a set of cores 500–952 m deep. These have been used to study the oxygen isotope composition of the ice, which showed that ice of the last glacial period was present below about 400 metres' depth. Then three more holes were drilled: in 1984, Hole 3G reached a final depth of 2202 m; in 1990, Hole 4G reached a final depth of 2546 m; and in 1993 Hole 5G reached a depth of 2755 m; after a brief closure, drilling continued during the winter of 1995. In 1996 it was stopped at depth 3623 m, by the request of the Scientific Committee on Antarctic Research that expressed worries about possible contamination of Lake Vostok. This ice core, drilled collaboratively with the French, produced a record of past environmental conditions stretching back 420,000 years and covering four previous glacial periods. For a long time it was the only core to cover several glacial cycles; but in 2004 it was exceeded by the EPICA core which, whilst shallower, covers a longer time span. In 2003 drilling was permitted to continue, but was halted at the estimated distance to the lake of only 130 m.\n\nThe ancient lake was finally breached on 5 February 2012 when scientists stopped drilling at the depth of 3,770 metres and reached the surface of the sub-glacial lake.\n\nThe brittle zone is approximately between 250 and 750 m and corresponds to the Last Glacial Maximum, with the end of the Holocene climatic optimum at or near the 250-metre depth.\n\nAlthough the Vostok core reached a depth of 3623 m the usable climatic information does not extend down this far. The very bottom of the core is ice refrozen from the waters of Lake Vostok and contains no climate information. The usual data sources give proxy information down to a depth of 3310 m or 414,000 years. Below this there is evidence of ice deformation. It has been suggested that the Vostok record may be extended down to 3345 m or 436,000 years, to include more of the interesting MIS11 period, by inverting a section of the record. This then produces a record in agreement with the newer, longer EPICA record, although it provides no new information.\n\n\n"}
