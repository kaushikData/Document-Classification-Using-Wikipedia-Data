{"id": "17742457", "url": "https://en.wikipedia.org/wiki?curid=17742457", "title": "1865 Viroqua tornado", "text": "1865 Viroqua tornado\n\nThe 1865 Viroqua tornado moved through western Wisconsin on Thursday, June 29, 1865. With at least 22 fatalities, it was one of the first deadly tornadoes recorded in Wisconsin after it became a state 17 years prior.\n\nApproaching Viroqua, Wisconsin, the funnel was accompanied by a \"branch whirl holding on like a parasite.\" Multiple vortex \"branches\" and \"eddies\" were observed as the tornado passed through the town. \"Death rode upon that sulphury siroc\" as \"the angry elements at the beck of an invisible power lay waste the fairest portion of the village.\" Ten people were killed on one street. A death toll of 17 is usually given for this tornado, but according to the History of Vernon County at least 13 people were killed in Viroqua and as many as 12 others may have died later from injuries. The tornado moved at an estimated 60 mph as it approached a schoolhouse 2 miles east of Viroqua containing a teacher and 24 students. The building was lifted into the air \"and dashed to ground\", killing the teacher and eight students. Foot-square timbers were carried \"long distances; tree tops were filled with feather beds, chairs, and clothing. All kinds of livestock were either dead or writhing on the points of broken branches; 20-ton rocks were rolled, lifted, and broken\".\n\n"}
{"id": "17127473", "url": "https://en.wikipedia.org/wiki?curid=17127473", "title": "6N14P", "text": "6N14P\n\nThe 6N14P (Russian: 6Н14П) is a miniature Russian-made medium gain dual triode vacuum tube, intended for service as a low-noise cascode amplifier at HF through VHF frequencies. It is a direct equivalent of ECC84 and 6CW7 vacuum tubes.\n\nUf = 6.3V, If = 350 mA uM = 25 Ia = 10.5 mA S = 6.8 mA/V Pa = 1.5 W\n\nIn the Soviet Union 6N14P found application in special equipment and to a lesser extent in more-expensive FM receivers and television sets in the 1960s. However, in the majority of medium and low-grade Soviet-produced FM radios a more general purpose 6N3P (2C51) vacuum tube was utilized, as the lower frequency OIRT FM band used in the USSR (65.8 to 74 MHz) allowed for less stringent noise figure requirements towards the receiver design and its components than the higher frequency Western FM broadcast band (87.5-108 MHz) used in most of the rest of the world.\n\nCurrently, like many other vacuum tubes, 6N14P has found some use with DIY audio enthusiasts.\n\n\n"}
{"id": "56179620", "url": "https://en.wikipedia.org/wiki?curid=56179620", "title": "Adranosite", "text": "Adranosite\n\nAdranosite is a mineral discovered in the La Fossa crater, Vulcano, Aeolian Islands, Italy, with the formula (NH)NaAl(SO)Cl(OH). Adranosite-(Fe) is the Fe3+ analogue of adranosite, with the formula (NH)NaFe(SO)Cl(OH).\n"}
{"id": "58353181", "url": "https://en.wikipedia.org/wiki?curid=58353181", "title": "Aitchison geometry", "text": "Aitchison geometry\n\nAitchison geometry is a framework focused on the analysis of data in which each data point is a tuple of nonnegative numbers whose sum is 1. These numbers may be proportions, percentages, probabilities, or concentrations. These tuples are referred to as compositions. At the heart of the framework is the characterization of the Aitchison simplex, where each element of the space is a composition. \n\nLinear operations can be defined on compositions, known as \"perturbation\" and \"powering operations\". These operations are linear in the Aitchison simplex and can be transformed into traditional addition and multiplication operations in Euclidean space through the use of log-ratio transformations. Inner products can be defined in the Aitchison simplex, giving rise to the distance metrics such as the Aitchison distance. It can also be shown that the Aitchison simplex forms a finite-dimensional Hilbert space.\n\nThe Aitchison simplex is formally defined for D species as follows\nwhere formula_2 can be any positive real-valued constant. \n\nThere are three core axioms that the Aitchison simplex, namely\n\n\nThe Aitchison simplex has the following operators defined using the closure operation as follows\n\n\n\n\nUnder these these operations alone, it is sufficient to show that the Aitchison simplex forms a Euclidean vector space.\n\nSince the Aitchison simplex forms a finite Hilbert space, it is possible to construct orthonormal bases in the simplex. Every composition can be decomposed as follows\n\nWhere formula_8 forms an orthonormal basis in the simplex.\n\nThere are 3 well-characterized isomorphisms that transform from the Aitchison simplex to real space. All of these transforms satisfy linearity and as given below\n\nThe additive log ratio (alr) transform is an where formula_9. This is given by\n\nThe choice of denominator component is arbituary, and could be any specified component.\nThis transform is commonly used in chemistry with measurements such as pH. In addition, this is the transform most commonly used for Multinomial logistic regression. The alr transform is not an isometry, meaning that distances on transformed values will not be equivalent to distances on the original compositions in the simplex.\n\nThe center log ratio (clr) tranform is both an isomorphism and an isometry where formula_11\n\nThe inverse of this function is also known as the Softmax function commonly used in neural networks.\n\nThe isometric log ratio (ilr) tranform is both an isomorphism and an isometry where formula_13\n\nThere are multiple ways to construct orthonormal bases, including using the Gram–Schmidt orthogonalization or Singular-value decomposition of clr transformed data. \nAnother alternative is to construct log contrasts from a bifurcating tree. If are given a bifurcating tree, we can construct a basis from the internal nodes in the tree.\n\nEach vector in the basis would be determined as follows\n\nThe elements within each vector are given as follows\n\nwhere formula_17 are the respective number of tips in the corresponding subtrees shown in the figure. It can be shown that the resulting basis is orthonormal\n\nOnce the basis formula_18 is built, the ilr transform can be calculated as follows\n\nwhere each element in the ilr transformed data is of the following form\n\nwhere formula_21 and formula_22 are the set of values corresponding to the tips in the subtrees formula_23 and formula_24\n"}
{"id": "31944088", "url": "https://en.wikipedia.org/wiki?curid=31944088", "title": "BatteryMAX (idle detection)", "text": "BatteryMAX (idle detection)\n\nBatteryMAX is an idle detection system used for computer power management under operating system control developed at Digital Research, Inc.'s European Development Centre (EDC) in Hungerford, UK. It was created to address the new genre of portable personal computers (laptops) which ran from battery power. As such, it was also an integral part of Novell's PalmDOS 1.0 operating system tailored for early palmtops in 1992.\n\nPower saving in laptop computers traditionally relied on hardware inactivity timers to determine whether a computer was idle. It would typically take several minutes before the computer could identify idle behavior and switch to a lower power consumption state. By monitoring software applications from within the operating system, BatteryMAX is able to reduce the time taken to detect idle behavior from minutes to microseconds. Moreover, it can switch power states around 18 times a second between a user's keystrokes. The technique was named \"Dynamic Idle Detection\" and includes halting, or stopping the CPU for periods of just a few microseconds until a hardware event occurs to restart it.\n\nDR DOS 5.0 was the first Personal Computer operating system to incorporate an Idle Detection System for power management. It was invented by British engineers Roger Gross and John Constant in August 1989. A US patent describing the idle detection system was filed on 9 March 1990 and granted on 11 October 1994.\n\nDespite taking an early lead and having the protection of a patent, BatteryMAX did not enjoy significant commercial success having been sidelined after the disarray that followed the integration of Digital Research into Novell, Inc. in 1991. It was not until 1992, some three years after the invention, that software power management under operating system control became ubiquitous following the launch of Advanced Power Management (APM) by Microsoft and Intel.\n\nBatteryMAX uses the technique of Dynamic Idle Detection to provide power savings by detecting what the application is doing (whether it is idle), and switching power states (entering low power mode) therefore extending the battery life of the product.\n\nBatteryMAX employs a layered model of detection software encapsulated into an DOS character device driver called $IDLE$ that contains all the hardware-dependent code to support Dynamic Idle Detection. It can either be linked into the DR-DOS operating system BIOS or loaded dynamically using the CONFIG.SYS DEVICE directive, overloading the built-in default driver. All versions of DR-DOS since version 5.0 have contained Dynamic Idle Detection support inside the operating system kernel. When the operating system believes an application is idle, it calls the $IDLE$ BIOS/driver layer, which executes custom code written by the computer manufacturer or third parties to verify the request and switch power states. Using the device driver concept, BatteryMAX can be integrated with hardware-related power management facilities, which might be provided by the underlying hardware, including interfacing with APM or ACPI system BIOSes.\n\nPower states are computer dependent and will vary from manufacturer to manufacturer. Power savings can be made in a number of ways including slowing/stopping the processor clock speed or shutting off power to complete sub-systems.\n\nBefore switching power states, the $IDLE$ driver uses any available hardware assistance to detect if the application has been accessing other components in the system. For example, the application may be polling a serial port, or updating a graphics screen. If this is the case, the device driver determines that the application is not in fact idle and overrides the kernel's call to switch power states by passing information back up the layers and allowing application execution to resume.\n\nAn application is idle if it is waiting for some external event to occur, for example for a keystroke or a mouse movement, or for a fixed amount of time to pass. The DR-DOS kernel monitors all DOS API calls building up a profile of the applications behavior. Certain combinations of API calls suggest that the application is idle.\n\nThe $IDLE$ driver is able to make the subtle distinction between a program that is genuinely idle, for instance one that is polling the keyboard in a tight loop, and one that is active but also polling the keyboard, to test for an abort key to be pressed. The driver makes this distinction by monitoring the time taken to go idle. If the time is within a specified period, the driver assumes that the program is idle, e.g. polling in a tight loop for a key to be pressed. If the time is outside the specified limit, the driver assumes that some processing has occurred in between polling the keyboard, and allows application execution to resume without switching power states. A local variable, IDLE_CNTDN, specifies the time against which the actual time taken to go idle is compared. The value for this variable is dynamically calculated at initialization and recalculated periodically.\n\nThe Idle Detection technique was first used to improve multi-tasking of single-tasking DOS applications in Digital Research's multi-tasking/multi-user Concurrent DOS/386 (CDOS386) operating system.\n\nPrograms written for single-tasking operating systems such as MS-DOS/PC DOS can go into endless loops until interrupted; for example when waiting for a user to press a key. Whilst this is not a problem where there is no other process waiting to run, it wastes valuable processor time that could be used by other programs in a multi-tasking/multi-user environment like CDOS386. Applications designed for a multi-tasking environment use API calls to \"sleep\" when they are idle for a period of time but normal DOS applications do not do this so Idle Detection must be used.\n\nThe Concurrent DOS/386 release included an Idle Detection function in the operating system kernel which monitored DOS API calls to determine whether the application was doing useful work or in fact idle. If it was idle, the process was suspended allowing the dispatcher to schedule another process for execution.\n\nBatteryMAX and the \"idle detection\" patent played an important role in an alleged patent infringement relating to software power management under operating system control.\nOn May 15, 2009, St. Clair Intellectual Property Consultants, Inc. filed civil action No. 09-354 in the United States District Court D. Delaware, against defendants Acer, Inc., Dell Inc., Gateway, Inc. and Lenovo Group, Inc. and on September 18, 2009 filed civil action No. 09-704 against Apple Inc., and Toshiba Corporation, Inc. The actions alleged infringement of several U.S. patents that they owned relating to software power management under operating system control. \n\nSt. Clair asserted that Henry Fung had invented software power management under operating system control and alleged that these companies had infringed St. Clair's patents and therefore owed St. Clair royalty payments. Microsoft intervened on behalf of the defendants and filed a declaratory judgment against St. Clair on April 7, 2010, seeking judgments of non-infringement and invalidity of the Fung patents. (D.I. 1, C.A. No. 10-282). Intel Corporation filed an intervention on behalf of the defendants and this was granted on June 4, 2010 (D.I. 178, C.A. No. 09-354).\n\nSeattle law firm Perkins Coie, LLP acting for the defendants discovered BatteryMax and Gross’s Idle Detection Patent during a prior art search. Gross’s patent had an earlier priority date than Fung’s patents which if proven would undermine St. Clair's case. On February 28, 2011, Gross was hired by Intel as a Subject Matter Expert to provide expert witness testimony for the defendants in the case. Gross provided evidence in his expert report that he, not Fung, had invented software power management under operating system control and sited the Idle Detection patent and the existence of BatteryMax as proof of this.\n\nSt Clair filed a Motion to Exclude Opinions Concerning BatteryMax, in an attempt to have Gross’s expert report dismissed but on March 29, 2013 the district court denied St. Clair’s motion declaring Gross’s testimony for the defendants as admissible, stating that \"The Court agrees with Defendants that there is sufficient corroborating evidence that BatteryMax was available to the public prior to the Fung patents' priority date. Further, the Court concludes that even if BatteryMax did not predate the Fung patents, Mr. Gross's testimony … would be relevant and helpful to the fact finder in an obviousness inquiry”.\n\n"}
{"id": "815969", "url": "https://en.wikipedia.org/wiki?curid=815969", "title": "Buckling", "text": "Buckling\n\nIn science, buckling is a mathematical instability that leads to a failure mode.\n\nWhen a structure is subjected to compressive stress, buckling may occur. Buckling is characterized by a sudden sideways deflection of a structural member. This may occur even though the stresses that develop in the structure are well below those needed to cause failure of the material of which the structure is composed. As an applied load is increased on a member, such as a column, it will ultimately become large enough to cause the member to become unstable and it is said to have buckled. Further loading will cause significant and somewhat unpredictable deformations, possibly leading to complete loss of the member's load-carrying capacity. If the deformations that occur after buckling do not cause the complete collapse of that member, the member will continue to support the load that caused it to buckle. If the buckled member is part of a larger assemblage of components such as a building, any load applied to the buckled part of the structure beyond that which caused the member to buckle will be redistributed within the structure.\n\nIn a mathematical sense, buckling is a bifurcation in the solution to the equations of static equilibrium. At a certain point, under an increasing load, any further load is able to be sustained in one of two states of equilibrium: a purely compressed state (with no lateral deviation) or a laterally-deformed state.\n\n The ratio of the effective length of a column to the least radius of gyration of its cross section is called the slenderness ratio (sometimes expressed with the Greek letter lambda, λ). This ratio affords a means of classifying columns and their failure mode. The slenderness ratio is important for design considerations. All the following are approximate values used for convenience.\n\nIf the load on a column is applied through the center of gravity (centroid) of its cross section, it is called an axial load. A load at any other point in the cross section is known as an eccentric load. A short column under the action of an axial load will fail by direct compression before it buckles, but a long column loaded in the same manner will fail by springing suddenly outward laterally (buckling) in a bending mode. The buckling mode of deflection is considered a failure mode, and it generally occurs before the axial compression stresses (direct compression) can cause failure of the material by yielding or fracture of that compression member. However, intermediate-length columns will fail by a combination of direct compressive stress and bending.\n\nIn particular:\n\nThe theory of the behavior of columns was investigated in 1757 by mathematician Leonhard Euler. He derived the formula, the Euler formula, that gives the maximum axial load that a long, slender, ideal column can carry without buckling. An ideal column is one that is perfectly straight, made of a homogeneous material, and free from initial stress. When the applied load reaches the Euler load, sometimes called the critical load, the column comes to be in a state of unstable equilibrium. At that load, the introduction of the slightest lateral force will cause the column to fail by suddenly \"jumping\" to a new configuration, and the column is said to have buckled. This is what happens when a person stands on an empty aluminum can and then taps the sides briefly, causing it to then become instantly crushed (the vertical sides of the can understood as an infinite series of extremely thin columns). The formula derived by Euler for long slender columns is given below.\n\nTo get the mathematical demonstration read: Euler's critical load\n\nwhere\n\nExamination of this formula reveals the following facts with regard to the load-bearing ability of slender columns.\n\nA conclusion from the above is that the buckling load of a column may be increased by changing its material to one with a higher modulus of elasticity (E), or changing the design of the column's cross section so as to increase its moment of inertia. The latter can be done without increasing the weight of the column by distributing the material as far from the principal axis of the column's cross section as possible. For most purposes, the most effective use of the material of a column is that of a tubular section.\n\nAnother insight that may be gleaned from this equation is the effect of length on critical load. Doubling the unsupported length of the column quarters the allowable load. The restraint offered by the end connections of a column also affects its critical load. If the connections are perfectly rigid (does not allowing rotation of its ends), the critical load will be four times that for a similar column where the ends are pinned (allowing rotation of its ends).\n\nSince the radius of gyration is defined as the square root of the ratio of the column's moment of inertia about an axis to its cross sectional area, the above Euler formula may be reformatted by substituting the radius of gyration A·r for I:\n\nwhere formula_13 is the stress that causes buckling the column, and formula_14 is the slenderness ratio.\n\nSince structural columns are commonly of intermediate length, the Euler formula has little practical application for ordinary design. Issues that cause deviation from the pure Euler column behaviour include imperfections in geometry of the column in combination with plasticity/non-linear stress strain behaviour of the column's material. Consequently, a number of empirical column formulae have been developed that agree with test data, all of which embody the slenderness ratio. Due to the uncertainty in the behavior of columns, for design, appropriate safety factors are introduced into these formulae. One such formula is the Perry Robertson formula which estimates the critical buckling load based on an assumed small initial curvature, hence an eccentricity of the axial load. The Rankine Gordon formula (Named for William John Macquorn Rankine and Perry Hugesworth Gordon (1899 – 1966)) is also based on experimental results and suggests that a column will buckle at a load F given by:\nwhere F is the Euler maximum load and F is the maximum compressive load. This formula typically produces a conservative estimate of F.\n\nTo get the mathematical demonstration read: Self-buckling\n\nA free-standing, vertical column, with density formula_16, Young's modulus formula_3, and cross-sectional area formula_18, will buckle under its own weight if its height exceeds a certain critical value:\n\nwhere \"g\" is the acceleration due to gravity, \"I\" is the second moment of area of the beam cross section, and \"B\" is the first zero of the Bessel function of the first kind of order -1/3, which is equal to 1.86635086…\n\nUsually buckling and instability are associated with compression, but buckling and instability can also occur in elastic structures subject to dead tensile load.\n\nAn example of a single-degree-of-freedom structure is shown in Fig. 2, where the critical load is also indicated.\nAnother example involving flexure of a structure made up of beam elements governed by the equation of the Euler's elastica is shown in Fig.3.\nIn both cases, there are no elements subject to compression. The instability and buckling in tension are related to the presence of the slider, the junction between the two rods, allowing only relative sliding between the connected pieces. Watch a movie for more details.\n\nBuckling of an elastic structure strongly depends on the curvature of the constraints against which the ends of the structure are prescribed to move (see Bigoni, Misseroni, Noselli and Zaccaria, 2012). In fact, even a single-degree-of-freedom system (see Fig.3) may exhibit a tensile (or a compressive) buckling load as related to the fact that one end has to move along the circular profile labeled 'Ct' (labelled 'Cc').\nThe two circular profiles can be arranged in a 'S'-shaped profile, as shown in Fig.4; in that case a discontinuity of the constraint's curvature is introduced, leading to multiple bifurcations. Note that the single-degree-of-freedom structure shown in Fig.4 has two buckling loads (one tensile and one compressive). Watch a movie for more details.\n\nStructures subject to a follower (nonconservative) load may suffer instabilities which are not of the buckling type and therefore are not detectable with a static approach. For instance, the so-called 'Ziegler column' is shown in Fig.5.\n\nThis two-degree-of-freedom system does not display a quasi-static buckling, but becomes dynamically unstable.\nTo see this, we note that the equations of motion are\n\nand their linearized version is\n\nAssuming a time-harmonic solution in the form\n\nwe find the critical loads for flutter (formula_23) and divergence (formula_24),\n\nwhere formula_26 and formula_27.\nFlutter instability corresponds to a vibrational motion of increasing amplitude and is shown in Fig.6 (upper part) together with the divergence instability (lower part) consisting in an exponential growth.\n\nRecently, Bigoni and Noselli (2011) have experimentally shown that flutter and divergence instabilities can be directly related to dry friction, watch the movie for more details.\n\nBuckling is a state which defines a point where an equilibrium configuration becomes unstable under a parametric change of load and can manifest itself in several different phenomena. All can be classified as forms of bifurcation.\n\nThere are four basic forms of bifurcation associated with loss of structural stability or buckling in the case of structures with a single degree of freedom. These comprise two types of pitchfork bifurcation, one saddle-node bifurcation (often referred to as a limit point) and one transcritical bifurcation. The pitchfork bifurcations are the most commonly studied forms and include the buckling of columns, sometimes known as Euler buckling; the buckling of plates, sometimes known as local buckling, which is well known to be relatively safe (both are supercritical phenomena) and the buckling of shells, which is well-known to be a highly dangerous (subcritical phenomenon). Using the concept of potential energy, equilibrium is defined as a stationary point with respect to the degree(s) of freedom of the structure. We can then determine whether the equilibrium is stable, as in the case where the stationary point is a local minimum; or unstable, as in the case where the stationary point is a maximum point of inflection or saddle point (for multiple-degree-of-freedom structures only) – see animations below.\n\nIn Euler buckling, when the applied load is increased by a small amount beyond the critical load, the structure deforms into a buckled configuration which is adjacent to the original configuration. For example, the Euler column pictured will start to bow when loaded slightly above its critical load, but will not suddenly collapse.\n\nIn structures experiencing limit point instability, if the load is increased infinitesimally beyond the critical load, the structure undergoes a large deformation into a different stable configuration which is not adjacent to the original configuration. An example of this type of buckling is a toggle frame (pictured) which 'snaps' into its buckled configuration.\n\nA plate is a 3-dimensional structure defined as having a width of comparable size to its length, with a thickness is very small in comparison to its other two dimensions. Similar to columns, thin plates experience out-of-plane buckling deformations when subjected to critical loads; however, contrasted to column buckling, plates under buckling loads can continue to carry loads, called local buckling. This phenomenon is incredibly useful in numerous systems, as it allows systems to be engineered to provide greater loading capacities.\n\nFor a rectangular plate, supported along every edge, loaded with a uniform compressive force per unit length, the derived governing equation can be stated by:\n\nwhere\n\nThe solution to the deflection can be expanded into two harmonic functions shown:\n\nwhere \n\nThe previous equation can be substituted into the earlier differential equation where formula_36 equals 1. formula_30 can be separated providing the equation for the critical compressive loading of a plate:\n\nwhere\n\nThe buckling coefficient is influenced by the aspect of the specimen, formula_37 / formula_45, and the number of lengthwise curvatures. For an increasing number of such curvatures, the aspect ratio produces a varying buckling coefficient; but each relation provides a minimum value for each formula_35. This minimum value can then be used as a constant, independent from both the aspect ratio and formula_35.\n\nGiven stress is found by the load per unit area, the following expression is found for the critical stress:\n\nFrom the derived equations, it can be seen the close similarities between the critical stress for a column and for a plate. As the width formula_38 shrinks, the plate acts more like a column as it increases the resistance to buckling along the plate’s width. The increase of formula_37 allows for an increase of the number of sine waves produced by buckling along the length, but also increases the resistance from the buckling along the width. This creates the preference of the plate to buckle in such a way to equal the number of curvatures both along the width and length. Due to boundary conditions, when a plate is loaded with a critical stress and buckles, the edges perpendicular to the load cannot deform out-of-plane and will therefore continue to carry the stresses. This creates a non-uniform compressive loading along the ends, where the stresses are imposed on half of the effective width on either side of the specimen, given by the following:\n\nwhere\n\nAs the loaded stress increase, the effective width continues to shrink; if the stresses on the ends ever reaches the yield stress, the plate will fail. This is what allows the buckled structure to continue supporting loadings. When the axial load over the critical load is plotted against the displacement, the fundamental path is shown. It demonstrates the plate's similarity to a column under buckling; however, past the buckling load, the fundamental path bifurcates into a secondary path that curves upward, providing the ability to be subjected to higher loads past the critical load.\n\nA conventional bicycle wheel consists of a thin rim kept under high compressive stress by the (roughly normal) inward pull of a large number of spokes. It can be considered as a loaded column that has been bent into a circle. If spoke tension is increased beyond a safe level, the wheel spontaneously fails into a characteristic saddle shape (sometimes called a \"taco\" or a \"pringle\") like a three-dimensional Euler column. This is normally a purely elastic deformation and the rim will resume its proper plane shape if spoke tension is reduced slightly.\n\nBuckling is also a failure mode in pavement materials, primarily with concrete, since asphalt is more flexible. Radiant heat from the sun is absorbed in the road surface, causing it to expand, forcing adjacent pieces to push against each other. If the stress is great enough, the pavement can lift up and crack without warning. Going over a buckled section can be very jarring to automobile drivers, described as running over a speed hump at highway speeds.\n\nSimilarly, rail tracks also expand when heated, and can fail by buckling, a phenomenon called sun kink. It is more common for rails to move laterally, often pulling the underlain railroad ties (sleepers) along.\n\nThe buckling force in the track due to warming up is a function of the rise in temperature only and is independent of the track length:\n\nDerivation of buckling force function:\n\nThe linear thermal expansion due to heating of the track is found using\nwhere\n\nAccording to Hooke's law the extension due to a force (in the rail) is\nwhere \n\nPutting these together gives\n\nThese accidents were deemed to be sun kink related (\"more information available at List of rail accidents (2000–2009)\"):\n\nOften it is very difficult to determine the exact buckling load in complex structures using the Euler formula, due to the difficulty in determining the constant K. Therefore, maximum buckling load is often approximated using energy conservation and referred to as an energy method in structural analysis.\n\nThe first step in this method is to assume a displacement mode and a function that represents that displacement. This function must satisfy the most important boundary conditions, such as displacement and rotation. The more accurate the displacement function, the more accurate the result.\n\nThe method assumes that the system (the column) is a conservative system in which energy is not dissipated as heat, hence the energy added to the column by the applied external forces is stored in the column in the form of strain energy.\n\nIn this method, there are two equations used (for small deformations) to approximate the \"strain\" energy (the potential energy stored as elastic deformation of the structure) and \"applied\" energy (the work done on the system by external forces).\n\nwhere formula_60 is the displacement function and the subscripts formula_61 and formula_62 refer to the first and second derivatives of the displacement. Energy conservation yields:\n\nFlexural-torsional buckling can be described as a combination of bending and twisting response of a member in compression. Such a deflection mode must be considered for design purposes. This mostly occurs in columns with \"open\" cross-sections and hence have a low torsional stiffness, such as channels, structural tees, double-angle shapes, and equal-leg single angles. Circular cross sections do not experience such a mode of bucking.\n\nWhen a simply supported beam is loaded in flexure, the top side is in compression, and the bottom side is in tension. If the beam is not supported in the lateral direction (i.e., perpendicular to the plane of bending), and the flexural load increases to a critical limit, the beam will experience a lateral deflection of the compression flange as it buckles locally. The lateral deflection of the compression flange is restrained by the beam web and tension flange, but for an open section the twisting mode is more flexible, hence the beam both twists and deflects laterally in a failure mode known as \"lateral-torsional buckling\". In wide-flange sections (with high lateral bending stiffness), the deflection mode will be mostly twisting in torsion. In narrow-flange sections, the bending stiffness is lower and the column's deflection will be closer to that of lateral bucking deflection mode.\n\nThe use of closed sections such as square hollow section will mitigate the effects of lateral-torsional buckling virtue of their high torsional rigidity. \n\n\"C\" is a modification factor used in the equation for nominal flexural strength when determining lateral-torsional buckling. The reason for this factor is to allow for non-uniform moment diagrams when the ends of a beam segment are braced. The conservative value for \"C\" can be taken as 1, regardless of beam configuration or loading, but in some cases it may be excessively conservative. \"C\" is always equal to or greater than 1, never less. For cantilevers or overhangs where the free end is unbraced, C is equal to 1. Tables of values of \"C\" for simply supported beams exist.\n\nIf an appropriate value of \"C\" is not given in tables, it can be obtained via the following formula:\n\nwhere\n\nThe result is the same for all unit systems.\n\nThe buckling strength of a member is less than the elastic buckling strength of a structure if the material of the member is stressed beyond the elastic material range and into the non-linear (plastic) material behavior range. When the compression load is near the buckling load, the structure will bend significantly and the material of the column will diverge from a linear stress-strain behavior. The stress-strain behavior of materials is not strictly linear even below the yield point, hence the modulus of elasticity decreases as stress increases, and significantly so as the stresses approach the material's yield strength. This reduced material rigidity reduces the buckling strength of the structure and results in a bucking load less than that predicted by the assumption of linear elastic behavior.\n\nA more accurate approximation of the buckling load can be had by the use of the tangent modulus of elasticity, E, which is less than the elastic modulus, in place of the elastic modulus of elasticity. The tangent is equal to the elastic modulus and then decreases beyond the proportional limit. The tangent modulus is a line drawn tangent to the stress-strain curve at a particular value of strain (in the elastic section of the stress-strain curve, the tangent modulus is equal to the elastic modulus). Plots of the tangent modulus of elasticity for a variety of materials are available in standard references.\n\nIf a column is loaded suddenly and then the load released, the column can sustain a much higher load than its static (slowly applied) buckling load. This can happen in a long, unsupported column used as a drop hammer. The duration of compression at the impact end is the time required for a stress wave to travel along the column to the other (free) end and back down as a relief wave. Maximum buckling occurs near the impact end at a wavelength much shorter than the length of the rod, and at a stress many times the buckling stress of a statically-loaded column. The critical condition for buckling amplitude to remain less than about 25 times the effective rod straightness imperfection at the buckle wavelength is\n\nwhere formula_69 is the impact stress, formula_5 is the length of the rod, formula_71 is the elastic wave speed, and formula_72 is the smaller lateral dimension of a rectangular rod. Because the buckle wavelength depends only on formula_69 and formula_72, this same formula holds for thin cylindrical shells of thickness formula_72.\n\nSolutions of Donnell's eight order differential equation gives the various buckling modes of a thin cylinder under compression. But this analysis, which is in accordance with the small deflection theory gives much higher values than shown from experiments. So it is customary to find the critical buckling load for various structures which are cylindrical in shape from empirically based design curves wherein the critical buckling load F is plotted against the ratio R/t, where R is the radius and t is the thickness of the cylinder for various values of L/R, L the length of the cylinder. If cut-outs are present in the cylinder, critical buckling loads as well as pre-buckling modes will be affected. Presence or absence of reinforcements of cut-outs will also affect the buckling load.\n\nPipes and pressure vessels subject to external overpressure, caused for example by steam cooling within the pipe and condensing into water with subsequent massive pressure drop, risk buckling due to compressive hoop stresses. Design rules for calculation of the required wall thickness or reinforcement rings are given in various piping and pressure vessel codes.\n\nThe mechanisms of cortical gyrification are just beginning to be understood. Mechanical buckling forces due to the expanding brain tissue probably cause the cortical surface to fold. This is an example of how pattern formation in nature can also take place due to elastic instabilities instead of the classical reaction-diffusion mechanism first proposed by Alan Turing.\n\n\n\n"}
{"id": "35553189", "url": "https://en.wikipedia.org/wiki?curid=35553189", "title": "Cellino San Marco Solar Park", "text": "Cellino San Marco Solar Park\n\nCellino San Marco Solar Park is a 42.692 MW solar photovoltaic (PV) plant in Southern Italy, near Cellino San Marco using 600,000 First Solar modules.\n\n"}
{"id": "15666340", "url": "https://en.wikipedia.org/wiki?curid=15666340", "title": "Consolidated Safety-Valve Co. v. Crosby Steam Gauge &amp; Valve Co.", "text": "Consolidated Safety-Valve Co. v. Crosby Steam Gauge &amp; Valve Co.\n\nConsolidated Safety-Valve Co. v. Crosby Steam Gauge & Valve Co., 113 U.S. 157 (1885), was a patent case to determine validity of patent No. 58,294, granted to George W. Richardson September 25, 1866, for an improvement in steam safety valves.\n\nRichardson was the first person who made a safety valve which, while it automatically relieved the pressure of steam in the boiler, did not, in effecting that result, reduce the pressure to such an extent as to make the use of the relieving apparatus practically impossible because of the expenditure of time and fuel necessary to bring up the steam again to the proper working standard.\n\nHis valve was the first which had the strictured orifice to retard the escape of the steam and enable the valve to open with increasing power against the spring and close suddenly, with small loss of pressure in the boiler.\n\nThe direction given in the patent that the flange or lip is to be separated from the valve seat by about one sixty-fourth of an inch for an ordinary spring, with less space for a strong spring and more space for a weak spring, to regulate the escape of steam as required, is a sufficient description as matter of law, and it is not shown to be insufficient as a matter of fact.\n\nLetters patent No. 85,963, granted to said Richardson January 19, 1869, for an improvement in safety valves for steam boilers or generators, are valid.\n\nThe patents of Richardson were infringed by a valve which produces the same effects in operation by the means described in Richardson's claims, although the valve proper is an annulus and the extended surface is a disc inside of the annulus, the Richardson valve proper being a disc and the extended surface an annulus surrounding the disc, and although the valve proper has two ground joints, and only the steam which passes through one of them goes through the stricture, while, in the Richardson valve, all the steam which passes into the air goes through the stricture, and although the huddling chamber is at the center instead of the circumference, and is in the seat of the valve, under the head, instead of in the head, and the stricture is at the circumference of the seat of the valve instead of being at the circumference of the head.\n\nThe fact that the prior patented valves were not used and the speedy and extensive adoption of Richardson's valve support the conclusion as to the novelty of the latter.\n\nSuits in equity having been begun in 1879 for the infringement of the two patents, and the circuit court having dismissed the bills, this Court in reversing the decrees after the first patent had expired but not the second, awarded accounts of profits and damages as to both patents, and a perpetual injunction as to the second patent.\n\n"}
{"id": "37904380", "url": "https://en.wikipedia.org/wiki?curid=37904380", "title": "Copper in renewable energy", "text": "Copper in renewable energy\n\nRenewable energy sources such as solar, wind, tidal, hydro, biomass, and geothermal have become significant sectors of the energy market. The rapid growth of these sources in the 21st century has been prompted by increasing costs of fossil fuels as well as their environmental impact issues that significantly lowered their use. \n\nCopper plays an important role in these renewable energy systems. In fact, copper usage averages up to five times more in renewable energy systems than in traditional power generation, such as fossil fuel and nuclear. Since copper is an excellent thermal and electrical conductor among the engineering metals (second only to silver), power systems that utilize copper generate and transmit energy with high efficiency and with minimum environmental impacts. \n\nWhen choosing electrical conductors, facility planners and engineers factor capital investment costs of the materials against operational savings due to their electrical energy efficiencies over their useful lives, plus maintenance costs. Copper often fairs well in these calculations. One pertinent factor, called \"copper usage intensity,” is a measure of the number of pounds of copper necessary to install one megawatt of new power-generating capacity. \n\nWhen planning for a new renewable power facility, engineers and product specifiers seek to avoid supply shortages of selected conductor materials. According to the United States Geological Survey, in-ground copper reserves have increased more than 700% since 1950, from almost 100 million tonnes to 720 million tonnes today, despite the fact that world refined usage has more than tripled in the last 50 years. Copper resources are estimated to exceed 5,000 million tonnes. Bolstering the annual supply is the fact that more than 30 percent of copper installed during the last decade came from recycled sources.\n\nRegarding the sustainability of renewable energy systems, it is worthy to note that in addition to copper's high electrical and thermal conductivity, its recycling rate is higher than any other metal.\n\nThis article discusses the role of copper in various renewable energy generation systems.\n\nCopper plays a larger role in renewable energy generation than in conventional thermal power plants in terms of tonnage of copper per unit of installed power. The copper usage intensity of renewable energy systems is four to six times higher than in fossil fuel or nuclear plants. So for example, while conventional power requires approximately 1 tonne of copper per installed megawatt (MW), renewable technologies such as wind and solar require four to six times more copper per installed MW. This is because copper is spread over much larger land areas, particularly in solar and wind energy power plants, and there is a need for long runs of power and grounding cables to connect components that are widely dispersed, including to energy storage systems and to the main electrical grid. \n\nWind and solar photovoltaic energy systems have the highest copper content of all renewable energy technologies. A single wind farm can contain between 4 million and 15 million pounds of copper. A photovoltaic solar power plant contains approximately 5.5 tons of copper per megawatt of power generation. A single 660-kW turbine is estimated to contain some 800 pounds of copper. \n\nThe total amount of copper used in renewable-based and distributed electricity generation in 2011 was estimated to be 272 kilotonnes (kt). Cumulative copper use through 2011 was estimated to be 1,071 kt.\n\nCopper conductors are used in major electrical renewable energy components, such as turbines, generators, transformers, inverters, electrical cables, power electronics, and information cable. Copper usage is approximately the same in turbines/generators, transformers/inverters, and cables. Much less copper is used in power electronics.\n\nSolar thermal heating and cooling energy systems rely on copper for their thermal energy efficiency benefits. Copper is also used as a special corrosion-resistant material in renewable energy systems in wet, humid, and saline corrosive environments.\n\nCopper is a sustainable material that is 100% recyclable. The recycling rate of copper is higher than any other metal. At the end of the useful life of the renewable energy power plant or its electrical or thermal components, the copper can be recycled with no loss of its beneficial properties.\n\nOf the 20,000 TWh of power consumed globally in a single year, approximately 90 TWh are generated from solar PV systems. While this is only a very small percentage of global energy consumption (0.6% of total installed electricity generating capacity worldwide), it is nevertheless sufficient to power the needs of more than 10 million people living at the standard of living in a developed country.\n\nVarious overlapping statistics regarding the growth of solar PVs have been cited. Solar PVs have been cited to have a 40% annual growth rate, which may grow even faster as the cost of the technology continues to decline. Another source cites operating capacity to have increased by an average of 58% annually from year end-2006 through 2011. Installed capacity estimates to 2020 suggest a rapid rise in solar PV generation, growing by a factor of five between 2010 and 2020.\n\nHousehold PV systems are able to generate their own electricity and use the electrical grid for support and reliability.\n\nFor these reasons, policy initiatives are taking place to enhance the deployment of solar photovoltaic energy installations. This would boost the steady expansion of PV markets by reducing the competitiveness gap of PVs compared to fossil fuel technologies. The goal at this point is to reach grid parity, where the cost of producing energy from rooftop panels over the course of their 25-year lifetime equates to the cost of retail electricity generated by conventional sources. This achievement has already been accomplished in some regions.\n\nThere is eleven to forty times more copper per unit of generation in photovoltaic systems than in conventional fossil fuel plants. The usage of copper in photovoltaic systems averages around 4-5 tonnes per MW or higher if conductive ribbon strips that connect individual PV cells are considered.\n\nCopper is used in: 1) small wires that interconnect photovoltaic modules; 2) earthing grids in electrode earth pegs, horizontal plates, naked cables, and wires; 3) DC cables that connect photovoltaic modules to inverters; 4) low-voltage AC cables that connect inverters to metering systems and protection cabinets; 5) high-voltage AC cables; 6) communication cables; 7) inverters/power electronics; 8) ribbons; and 9) transformer windings.\n\nCopper used in photovoltaic systems in 2011 was estimated to be 150 kt. Cumulative copper usage in photovoltaic systems through 2011 was estimated to be 350 kt.\n\nSolar photovoltaic (PV) systems are highly scalable, ranging from small rooftop systems to large photovoltaic power station with capacities of hundreds of megawatts. In residential systems, copper intensity appears to be linearly scalable with the capacity of the electrical generation system. Residential and community-based systems generally range in capacity from 10 kW to 1 MW.\n\nPV cells are grouped together in solar modules. These modules are connected to panels and then into PV arrays. In grid-connected photovoltaic power system, arrays can form sub-fields from which electricity is collected and transported towards the grid connection.\n\nCopper solar cables connect modules (module cable), arrays (array cable), and sub-fields (field cable). Whether a system is connected to the grid or not, electricity collected from the PV cells needs to be converted from DC to AC and stepped up in voltage. This is done by solar inverters which contain copper windings, as well as with copper-containing power electronics.\n\nThe photovoltaic industry uses several different semiconducting materials for the production of solar cells and often groups them into first and second generation technologies, while the third generation includes a number of emerging technologies that are still in the research and development phase. Solar cells typically convert 20% of incident sunlight into electricity, allowing the generation of 100 - 150 kWh per square meter of panel per year.\n\nConventional first-generation crystalline silicon (c-Si) technology includes monocrystalline silicon and polycrystalline silicon. In order to reduce costs of this wafer-based technology, copper-contacted silicon solar cells are emerging as an important alternative to silver as the preferred conductor material. Challenges with solar cell metallization lie in the creation of a homogenous and qualitatively high-value layer between silicon and copper to serves as a barrier against copper diffusion into the semiconductor. Copper-based front-side metallization in silicon solar cells is a significant step towards lower cost.\n\nThe second-generation technology includes thin film solar cells. Despite having a slightly lower conversion efficiency than conventional PV technology, the overall cost-per-watt is still lower. Commercially significant thin film technologies include copper indium gallium selenide solar cells (CIGS) and cadmium telluride photovoltaics (CdTe), while amorphous silicon (a-Si) and micromorphous silicon (m-Si) tandem cells are slowly being outcompeted in recent years.\n\nCIGS, which is actually copper (indium-gallium) diselenide, or Cu(InGa)Se, differs from silicon in that it is a heterojunction semiconductor. It has the highest solar energy conversion efficiency (~20%) among thin film materials. Because CIGS strongly absorbs sunlight, a much thinner film is required than with other semiconductor materials.\n\nA photovoltaic cell manufacturing process has been developed that makes it possible to print CIGS semi-conductors. This technology has the potential to reduce the price per solar watt delivered.\n\nWhile copper is one of the components in CIGS solar cells, the copper content of the cell is actually small: about 50 kg of copper per MW of capacity.\n\nMono-dispersed copper sulfide nanocrystals are being researched as alternatives to conventional single crystals and thin films for photovoltaic devices. This technology, which is still in its infancy, has potential for dye-sensitized solar cells, all-inorganic solar cells, and hybrid nano-crystal-polymer composite solar cells.\n\nSolar generation systems cover large areas. There are many connections among modules and arrays, and connections among arrays in sub-fields and linkages to the network. Solar cables are used for wiring solar power plants. The amount of cabling involved can be substantial. Typical diameters of copper cables used are 4–6 mm for module cable, 6–10 mm for array cable, and 30–50 mm for field cable.\n\nEnergy efficiency and renewable energy are twin pillars of a sustainable energy future. However, there is little linking of these pillars despite their potential synergies. The more efficiently energy services are delivered, the faster renewable energy can become an effective and significant contributor of primary energy. The more energy is obtained from renewable sources, the less fossil fuel energy is required to provide that same energy demand. This linkage of renewable energy with energy efficiency relies in part on the electrical energy efficiency benefits of copper.\n\nIncreasing the diameter of a copper cable increases its electrical energy efficiency \"(see: Copper wire and cable)\". Thicker cables reduce resistive (IR) loss, which affects lifetime profitability of PV system investments. Complex cost evaluations, factoring extra costs for materials, the amount of solar radiation directed towards solar modules per year (accounting for diurnal and seasonal variations, subsidies, tariffs, payback periods, etc.) are necessary to determine whether higher initial investments for thicker cables are justified.\n\nDepending upon circumstances, some conductors in PV systems can be specified with either copper or aluminum. As with other electrical conducting systems, there are advantages to each \"(see: Copper wire and cable)\". Copper is the preferred material when high electrical conductivity characteristics and flexibility of the cable are of paramount importance. Also, copper is more suitable for small roof facilities, in smaller cable trays, and when ducting in steel or plastic pipes.\n\nCable ducting is not needed in smaller power facilities where copper cables are less than 25mm. Without duct work, installation costs are lower with copper than with aluminum.\n\nData communications networks rely on copper, optical fiber, and/or radio links. Each material has its advantages and disadvantages. Copper is more reliable than radio links. Signal attenuation with copper wires and cables can be resolved with signal amplifiers.\n\nThe Sun’s solar energy can also be harnessed for its heat. When the Sun’s energy heats a fluid in a closed system, its pressure and temperature rise. When introduced to a turbine, the fluid expands, turning the turbine and producing electrical power.\n\nConcentrating solar power (CSP), also known as solar thermal electricity (STE), uses arrays of mirrors that concentrate the sun’s rays to temperatures between 400C -1000C. Electrical power is produced when the concentrated light is converted to heat, which drives a heat engine (usually a steam turbine) connected to an electrical power generator.\n\nCSP facilities can produce large-scale power and hold much promise in areas with plenty of sunshine and clear skies. Poised to make Sun-powered grids a reality, CSP is currently capable of providing power and dispatchability on a scale similar to that of fossil fuel or nuclear electrical power plants.\n\nThe electrical output of CSP facilities match shifting daily demand for electricity in places where air conditioning systems are spreading. When backed by thermal storage facilities and combustible fuel, CSP offers utilities electricity that can be dispatched when required, enabling it to be used for base, shoulder and peak loads.\n\nIndustry groups have estimated that the technology could generate a quarter of the world’s electricity needs by 2050. For this reason, plans for future CSP facilities are ambitious. A timeline of CSP deployment around the world is available. Total installed power is forecasted to increase exponentially through 2025, creating as much as 130,000 jobs.\n\nIn 2010, Spain, the world leader of CSP technology, was constructing or planning to build some 50 large CSP plants. That nation has a total installed base of 1581 MW of power plus an additional 774 MW nearing completion for installation. Other countries in southern Europe also have CSP facilities, as do countries in emerging markets, such as Chile, India, Morocco, Saudi Arabia, South Africa, and the United Arab Emirates.\n\nUnlike wind energy, photovoltaics, and most distributed power, the main advantage of CSP is its thermal storage capability and hybridization possibilities. Storage systems range from 4 hours in the most typical plants to more than 20 hours when base load is required. This can complement variable generation of other renewable power sources.\n\nCSP systems are sometimes combined with fossil fueled steam turbine generation, but interest is growing in pure CSP technology. Further information on concentrating solar power is available from the Global Solar Thermal Energy Council.\n\nA CSP system consists of: 1) a concentrator or collector containing mirrors that reflect solar radiation and deliver it to the receiver; 2) a receiver that absorbs concentrated sunlight and transfers heat energy to a working fluid (usually a mineral oil, or more rarely, molten salts, metals, steam or air); 3) a transport and storage system that passes the fluid from the receiver to the power conversion system; and 4) a steam turbine that converts thermal power to electricity on demand.\n\nCopper is used in field power cables, grounding networks, and motors for tracking and pumping fluids, as well as in the main generator and high voltage transformers. Typically, there is about 200 tonnes copper for a 50 MW power plant.\n\nIt has been estimated that copper usage in concentrated solar thermal power plants was 2 kt in 2011. Cumulative copper usage in these plants through 2011 was estimated to be 7 kt.\n\nThere are four main types of CSP technologies, each containing a different amount of copper: parabolic trough plants, tower plants, distributed linear absorber systems including linear Fresnel plants, and dish Stirling plants. The use of copper in these plants is described here.\n\nParabolic trough plants are the most common CSP technology, representing about 94% of power installed in Spain. These plants collect solar energy in parabolic trough concentrators with linear collector tubes. The heat transfer fluids are typically synthetic oil that circulates through tubes at inlet outlet/temperatures of 300 °C to 400 °C. The typical storage capacity of a 50 MW facility is 7 hours at nominal power. A plant of this size and storage capacity can generate 160 GWh/year in a region like Spain.\n\nIn parabolic trough plants, copper is specified in the solar collector field (power cables, signals, earthing, electrical motors); steam cycle (water pumps, condenser fans, cabling to consumption points, control signal and sensors, motors), electricity generators (alternator, transformer), and storage systems (circulating pumps, cabling to consumption points). A 50 MW plant with 7.5 hours of storage contains approximately 196 tonnes of copper, of which 131,500 kg are in cables and 64,700 kg are in various equipment (generators, transformers, mirrors, and motors). This translates to about 3.9 tonnes/MW, or, in other terms, 1.2 tonnes/GWh/year. A plant of the same size without storage can have 20% less copper in the solar field and 10% less in the electronic equipment. A 100 MW plant will have 30% less relative copper content per MW in the solar field and 10% less in electronic equipment.\n\nCopper quantities also vary according to design. The solar field of a typical 50 MW power plant with 7 hours of storage capacity consists of 150 loops and 600 motors, while a similar plant without storage uses 100 loops and 400 motors. Motorized valves for mass flow control in the loops rely on more copper. Mirrors use a small amount of copper to provide galvanic corrosion protection to the reflective silver layer. Changes in the size of the plants, size of collectors, efficiencies of heat transfer fluids will also affect material volumes.\n\nTower plants, also called central tower power plants, may become the preferred CSP technology in the future. They collect solar energy concentrated by the heliostat field in a central receiver mounted at the top of the tower. Each heliostat tracks the Sun along two axes (azimuth and elevation). Therefore, two motors per unit are required.\n\nCopper is required in the heliostat field (power cables, signal, earthing, motors), receiver (trace heating, signal cables), storage system (circulating pumps, cabling to consumption points), electricity generation (alternator, transformer), steam cycle (water pumps, condenser fans), cabling to consumption points, control signal and sensors, and motors.\n\nA 50 MW solar tower facility with 7.5 hours of storage uses about 219 tonnes of copper. This translates to 4.4 tonnes of copper/MW, or, in other terms, 1.4 tonnes/GWh/year. Of this amount, cables account for approximately 154,720 kg. Electronic equipment, such as generators, transformers, and motors, account for approximately 64,620 kg of copper. A 100 MW plant has slightly more copper per MW in the solar field because the efficiency of the heliostat field diminishes with the size. A 100 MW plant will have somewhat less copper per MW in process equipment.\n\nLinear Fresnel plants use linear reflectors to concentrate the Sun’s rays in an absorber tube similar to parabolic trough plants. Since the concentration factor is less than in parabolic trough plants, the temperature of the heat transfer fluid is lower. This is why most plants use saturated steam as the working fluid in both the solar field and the turbine.\n\nA 50 MW linear Fresnel power plant requires about 1,960 tracking motors. The power required for each motor is much lower than the parabolic trough plant. A 50 MW lineal Fresnel plant without storage will contain about 127 tonnes of copper. This translates to 2.6 tonnes of copper/MW, or in other terms, 1.3 tonnes of copper/GWh/year. Of this amount, 69,960 kg of copper are in cables from process area, solar field, earthing and lightning protection and controls. Another 57,300 kg of copper is in equipment (transformers, generators, motors, mirrors, pumps, fans).\n\nThese plants are an emerging technology that has potential as a solution for decentralized applications. The technology does not require water for cooling in the conversion cycle. These plants are non-dispatchable. Energy production ceases when clouds pass overhead. Research is being conducted on advanced storage and hybridization systems.\n\nThe largest dish Sterling installation has a total power of 1.5 MW. Relatively more copper is needed in the solar field than other CSP technologies because electricity is actually generated there. Based on existing 1.5 MW plants, the copper content is 4 tonnes/MW, or, in other terms, 2.2 tonnes of copper/GWh/year. A 1.5 MW power plant has some 6,060 kg of copper in cables, induction generators, drives, field and grid transformers, earthing and lightning protection.\n\nSolar water heaters can be a cost-effective way to generate hot water for homes. They can be used in any climate. The fuel they use, sunshine, is free.\n\nSolar hot water collectors are used by more than 200 million households as well as many public and commercial buildings worldwide. The total installed capacity of solar thermal heating and cooling units in 2010 was 185 GW-thermal.\n\nSolar heating capacity increased by an estimated 27% in 2011 to reach approximately 232 GWth, excluding unglazed swimming pool heating. Most solar thermal is used for water heating, but solar space heating and cooling are gaining ground, particularly in Europe.\n\nThere are two types of solar water heating systems: active, which have circulating pumps and controls, and passive, which don't. Passive solar techniques do not require working electrical or mechanical elements. They include the selection of materials with favorable thermal properties, designing spaces that naturally circulate air, and referencing the position of a building to the Sun.\n\nCopper is an important component of solar thermal heating and cooling systems because of its high heat conductivity, resistance to atmospheric and water corrosion, sealing and joining by soldering, and mechanical strength. Copper is used both in receivers and primary circuits (pipes and heat exchangers for water tanks). For the absorber plate, aluminum is sometimes used as it is cheaper, yet when combined with copper piping, there may be problems in regards to allow the absorber plate to transfer its heat to the piping suitably. An alternative material that is currently used is PEX-AL-PEX but there may be similar problems with the heat transfer between the absorber plate and the pipes as well. One way around this is to simply use the same material for both the piping and the absorber plate. This material can be copper off course but also aluminum or PEX-AL-PEX.\n\nThree types of solar thermal collectors are used for residential applications: flat plate collectors, integral collector-storage, and solar thermal collector: Evacuated tube collectors; They can be direct circulation (i.e., heats water and brings it directly to the home for use) or indirect circulation (i.e., pumps heat a transfer fluid through a heat exchanger, which then heats water that flows into the home) systems.\n\nIn an evacuated tube solar hot water heater with an indirect circulation system, evacuated tubes contain a glass outer tube and metal absorber tube attached to a fin. Solar thermal energy is absorbed within the evacuated tubes and is converted into usable concentrated heat. Copper heat pipes transfer thermal energy from within the solar tube into a copper header. A thermal transfer fluid (water or glycol mixture) is pumped through the copper header. As the solution circulates through the copper header, the temperature rises. The evacuated glass tubes have a double layer. The outer layer is fully transparent to allow solar energy to pass through unimpeded. The inner layer is treated with a selective optical coating that absorbs energy without reflection. The inner and outer layers are fused at the end, leaving an empty space between the inner and outer layers. All air is pumped out of the space between the two layers (evacuation process), thereby creating the thermos effect which stops conductive and convective transfer of heat that might otherwise escape into the atmosphere. Heat loss is further reduced by the low-emissivity of the glass that is used. Inside the glass tube is the copper heat pipe. It is a sealed hollow copper tube that contains a small amount of proprietary liquid, which under low pressure boils at a very low temperature. Other components include a solar heat exchanger tank and a solar pumping station, with pumps and controllers.\n\nWind power is the conversion of wind energy into a useful form of energy, such as using wind turbines to make electricity, windmills for mechanical power, windpumps for water pumping or drainage, or sails to propel ships. In a wind turbine, the wind's kinetic energy is converted into mechanical energy to drive a generator, which in turn generates electricity.\n\nWind energy is one of the fastest growing energy technologies. Wind power capacity increased from a very small base of around 0.6 GW in 1996 to around 160 GW in 2009. It has also been reported that wind power capacity increased by 20% in 2011 to approximately 238 GW by 2012. This was the largest addition in capacity of any of the renewable energy technologies. It is anticipated that the growth of wind energy will continue to rise dramatically. Moderate estimates for global capacity by 2020 are 711 GW.\n\nSome 50 countries operated wind power facilities in 2010.\n\nTraditionally, wind power has been generated on land. But higher wind speeds are available offshore compared to land. Technologies are being improved to exploit the potential of wind power in offshore environments. The offshore wind power market is expanding with the use of larger turbines and installations farther from shore.\n\nOffshore installation, as yet, is a comparatively small market, probably accounting for little more than 10% of installation globally. The location of new wind farms increasingly will be offshore, especially in Europe. Offshore wind farms are normally much larger, often with over 100 turbines with ratings up to 3 MW and above per turbine. The harsh environment means that the individual components need to be more rugged and corrosion protected than their onshore components. Increasingly long connections to shore with subsea MV and HV cables are required at this time. The need for corrosion protection favors copper nickel cladding as the preferred alloy for the towers.\n\nWind power installations vary in scale and type. Large wind farm installations linked to the electrical grid are at one end of the spectrum. These may be located either onshore or offshore. At the other end of the spectrum are small individual turbines that provide electricity to individual premises or electricity-using installations. These are often in rural and grid-isolated sites.\n\nThe basic components of a wind power system consist of a tower with rotating blades containing an electricity generator and a transformer to step up voltage for electricity transmission to a substation on the grid. Cabling and electronics are also important components.\n\nCopper is an important conductor in wind power generation. Wind farms can contain several hundred-thousand feet of copper weighing between 4 million to 15 million pounds, mostly in wiring, cable, tubing, generators and step-up transformers.\n\nCopper usage intensity is high because turbines in wind generation farms are spread over large areas. In land-based wind farms, copper intensity can range between 5,600 and 14,900 pounds per MW, depending on whether the step-up transformers have copper or aluminum conductors. In the off-shore environment, copper intensity is much higher: approximately 21,000 pounds per MW, which includes submarine cables to shore. In both onshore and offshore environments, additional copper cabling is used to connect wind farms to main electrical grids.\n\nThe amount of copper used for wind energy systems in 2011 was estimated to be 120 kt. The cumulative amount of copper installed through 2011 was estimated to be 714 kt.\n\nFor wind farms with three-stage gearbox doubly fed 3 MW induction generators, approximately 2.7 t per MW is needed with standard windmills. For windmills with LV/MV transformers in the nacelle, 1.85 t per MW is needed. \n\nCopper is primarily used in coil windings in the stator and rotor portions of generators (which convert mechanical energy into electrical energy), in high voltage and low voltage cable conductors including the vertical electrical cable that connects the nacelle to the base of the wind turbine, in the coils of transformers (which steps up low voltage AC to high voltage AC compatible with the grid), in gearboxes (which convert the slow revolutions per minute of the rotor blades to faster rpms) and in wind farm electrical grounding systems. Copper may also be used in the nacelle (the housing of the wind turbine that rests on the tower containing all the main components), auxiliary motors (motors used to rotate the nacelle as well as control the angle of the rotor blades), cooling circuits (cooling configuration for the entire drive train), and power electronics (which enable the wind turbine systems to perform like a power plant).\n\nIn the coils of wind generators, electric current suffers from losses that are proportional to the resistance of the wire that carries the current. This resistance, called copper losses, causes energy to be lost by heating up the wire. In wind power systems, this resistance can be reduced with thicker copper wire and with a cooling system for the generator, if required.\n\nEither copper or aluminum conductors can be specified for generator cables. Copper has the higher electrical conductivity and therefore the higher electrical energy efficiency. It is also selected for its safety and reliability. The main consideration for specifying aluminum is its lower capital cost. Over time, this benefit is offset by higher energy losses over years of power transmission. Deciding which conductor to use is determined during a project's planning phase when utility teams discuss these matters with turbine and cable manufacturers. \n\nRegarding copper, its weight in a generator will vary according to the type of generator, power rating, and configuration. Its weight has an almost linear relationship to the power rating.\n\nGenerators in direct-drive wind turbines contain more copper, as the generator itself is bigger due to the absence of a gearbox.\n\nA generator in a direct drive configuration could be 3.5 times to 6 times heavier than in a geared configuration, depending on the type of generator.\n\nFive different types of generator technologies are used in wind generation:\nThe amount of copper in each of these generator types is summarized here.\n\nDirect-drive configurations of the synchronous type machines contain the most copper. Conventional synchronous generators (CSG) direct-drive machines have the highest per-unit copper content. The share of CSGs will increase from 2009 to 2020, especially for direct drive machines. DFAGs accounted for the most unit sales in 2009.\n\nThe variation in the copper content of CSG generators depends upon whether they are coupled with single-stage (heavier) or three-stage (lighter) gearboxes. Similarly, the difference in copper content in PMSG generators depends on whether the turbines are medium speed, which are heavier, or high-speed turbines, which are lighter.\n\nThere is increasing demand for synchronous machines and direct-drive configurations. CSG direct and geared DFAGs will lead the demand for copper. The highest growth in demand is expected to be the direct PMSGs, which is forecast to account for 7.7% of the total demand for copper in wind power systems in 2015. However, since permanent magnets that contain the rare earth element neodymium may not be able to escalate globally, direct drive synchronous magnet (DDSM) designs may be more promising. The amount of copper required for a 3 MW DDSM generator is 12.6 t.\n\nLocations with high-speed turbulent winds are better suited for variable-speed wind turbine generators with full-scale power converters due to the greater reliability and availability they offer in such conditions. Of the variable-speed wind turbine options, PMSGs could be preferred over DFAGs in such locations. In conditions with low wind speed and turbulence, DFAGs could be preferred to PMSGs.\n\nGenerally, PMSGs deal better with grid-related faults and they could eventually offer higher efficiency, reliability, and availability than geared counterparts. This could be achieved by reducing the number of mechanical components in their design. Currently, however, geared wind turbine generators have been more thoroughly field-tested and are less expensive due to the greater volumes produced.\n\nThe current trend is for PMSG hybrid installations with a single-stage or two-stage gearbox. The most recent wind turbine generator by Vestas is geared drive. The most recent wind turbine generator by Siemens is a hybrid. Over the medium term, if the cost of power electronics continues to decrease, direct-drive PMSG are expected to become more attractive.\nHigh-temperature superconductors (HTSG) technology is currently under development. It is expected that these machines will be able to attain more power than other wind turbine generators. If the offshore market follows the trend of larger unit machines, offshore could be the most suitable niche for HTSGs.\n\nFor a 2 MW turbine system, the following amounts of copper were estimated for components other than the generator:\n\nCabling is the second largest copper-containing component after the generator. A wind tower system with the transformer next to the generator will have medium-voltage (MV) power cables running from the top to the bottom of the tower, then to a collection point for a number of wind towers and on to the grid substation, or direct to the substation. The tower assembly will incorporate wire harnesses and control/signal cables, while low-voltage (LV) power cables are required to power the working parts throughout the system.\n\nFor a 2 MW wind turbine, the vertical cable could range from 1,000-1,500 kg of copper, depending upon its type. Copper is the dominant material in underground cables.\n\nCopper is vital to the electrical grounding system for wind turbine farms. Grounding systems can either be all-copper (solid or stranded copper wires and copper bus bars) often with an American gauge rating of 4/0 but perhaps as large as 250 thousands of circular mils or copper-clad steel, a lower cost alternative.\n\nTurbine masts attract lightning strikes, so they require lightning protection systems. When lightning strikes a turbine blade, current passes along the blade, through the blade hub in the nacelle (gearbox/ generator enclosure) and down the mast to a grounding system. The blade incorporates a large cross-section copper conductor that runs along its length and allows current to pass along the blade without deleterious heating effects. The nacelle is protected by a lightning conductor, often copper. The grounding system, at the base of the mast, consists of a thick copper ring conductor bonded to the base or located within a meter of the base. The ring is attached to two diametrically opposed points on the mast base. Copper leads extend outward from the ring and connect to copper grounding electrodes. The grounding rings at turbines on wind farms are inter-connected, providing a networked system with an extremely small aggregate resistance.\n\nSolid copper wire has been traditionally deployed for grounding and lightning equipment due to its excellent electrical conductivity. However, manufacturers are moving towards less expensive bi-metal copper clad or aluminum grounding wires and cables. Copper-plating wire is being explored. Current disadvantages of copper plated wire include lower conductivity, size, weight, flexibility and current carrying capability.\n\nAfter generators and cable, minor amounts of copper are used in the remaining equipment. In yaw and pitch auxiliary motors, the yaw drive uses a combination of induction motors and multi-stage planetary gearboxes with minor amounts of copper. Power electronics have minimal amounts of copper compared to other equipment. As turbine capacities increase, converter ratings also increase from low voltage (<1 kV) to medium voltage (1–5 kV). Most wind turbines have full power converters, which have the same power rating as the generator, except the DFAG that has a power converter that is 30% of the rating of the generator. Finally, minor amounts of copper are used in air/oil and water cooled circuits on gearboxes or generators.\n\nClass 5 copper power cabling is exclusively used from the generator through the loop and tower interior wall. This is due to its ability to withstand the stress from 15,000 torsion cycles for 20 years of service life.\n\nSuperconducting materials are being tested within and outside of wind turbines. They offer higher electrical efficiencies, the ability to carry higher currents, and lighter weights. These materials are, however, much more expensive than copper at this time.\n\nThe amount of copper in offshore wind farms increases with the distance to the coast. Copper usage in offshore windmills is on the order of 10.5 t per MW. The Borkum 2 offshore wind farm in Denmark uses 5,800 t for a 400 MW, 200 kilometer connection to the external grid, or approximately 14.5 t of copper per MW. The Horns wind farm uses 8.75 tons of copper per MW to transmit 160 MW 21 kilometers to the grid.\n"}
{"id": "25896348", "url": "https://en.wikipedia.org/wiki?curid=25896348", "title": "Copper–tungsten", "text": "Copper–tungsten\n\nCopper–tungsten (tungsten–copper, CuW, or WCu) is a mixture of copper and tungsten. As copper and tungsten are not mutually soluble, the material is composed of distinct particles of one metal dispersed in a matrix of the other one. The microstructure is therefore rather a metal matrix composite instead of a true alloy.\n\nThe material combines the properties of both metals, resulting in a material that is heat-resistant, ablation-resistant, highly thermally and electrically conductive, and easy to machine.\n\nParts are made from the CuW composite by pressing the tungsten particles into the desired shape, sintering the compacted part, then infiltrating with molten copper. Sheets, rods, and bars of the composite mixture are available as well.\n\nCommonly used copper tungsten mixtures contains 10–50 wt.% of copper, the remaining portion being mostly tungsten. The typical properties is dependent on its composition. The mixture with less wt.% of copper has higher density, higher hardness, and higher resistivity. The typical density of CuW90, with 10% of copper, is 16.75 g/cm and 11.85 g/cm for CuW50 . CuW90 has higher hardness and resistivity of 260 HB kgf/mm and 6.5 µΩ.cm than CuW50.\n\nTypical properties of commonly used copper tungsten composition\nCuW composites are used where the combination of high heat resistance, high electrical and thermal conductivity, and low thermal expansion are needed. Some of the applications are in electric resistance welding, as electrical contacts, and as heat sinks. As contact material, the composite is resistant to erosion by electric arc. WCu alloys are also used in electrodes for electrical discharge machining and electrochemical machining.\n\nThe CuW75 composite, with 75% tungsten, is widely used in chip carriers, substrates, flanges, and frames for power semiconductor devices. The high thermal conductivity of copper together with the low thermal expansion of tungsten allows thermal expansion matching to silicon, gallium arsenide, and some ceramics. Other materials for this applications are CuMo alloy, AlSiC, and Dymalloy.\n\nComposites with 70–90% of tungsten are used in liners of some specialty shaped charges. The penetration is enhanced by factor 1.3 against copper for homogeneous steel target, as both the density and the break-up time are increased. Tungsten powder based shaped charge liners are especially suitable for oil well completion. Other ductile metals can be used as a binder in place of copper as well. Graphite can be added as lubricant to the powder.\n\nCuW can also be used as a contact material in a vacuum. When the contact is very fine grained (VFG) the electrical conductivity is much higher than a normal piece of Copper Tungsten. Copper tungsten is a good choice for a vacuum contact due to its low cost, resistance to arc erosion, good conductivity, and resistance to mechanical wear and contact welding. CuW is usually a contact for vacuum, oil, and gas systems. It is not a good contact for air since the surface will oxidize when exposed. CuW is less likely to erode in air when the concentration of copper is higher in the material. The uses of CuW in the air are as an arc tip, arc plate, and an arc runner.\n\nCopper tungsten materials are often used for arcing contacts in SF circuit breakers that require medium and high voltages. The copper tungsten used in these circuit breakers is subject to temperatures above 20,000K, which is reached during arcing. Copper tungsten arc contacts resist arc erosion.\n\nThe Spark Erosion (EDM) process calls for copper tungsten. Usually, this process is used with graphite, but as tungsten has a high melting point (3420 C) this allows the CuW electrodes to have a longer service life than the graphite electrodes. This is crucial when the electrodes have been processed with complex machining. Since the electrodes are susceptible to wear the electrodes provide more geometrical accuracy than the other electrodes. These properties also let the rods and tubes manufactured for spark erosion be made smaller in diameter and have a longer length since the material is less likely to chip and warp. \nđ\n\nThe electrical and thermal properties of the composites vary with different proportions. An increase in copper increases the thermal conductivity, which plays a huge part when being used in circuit breakers. Electrical resistivity increases with an increase in the percentage of tungsten present in the composite, ranging from 3.16 at 55% tungsten to 6.1 when the composite contains 90% tungsten. An increase in tungsten leads to an increase in ultimate tensile strength up until the alloy reaches 80% tungsten and 20% copper with an ultimate tensile strength of 663 MPa. After this mixture of copper and tungsten, the ultimate tensile strength then begins to decrease fairly rapidly.\n"}
{"id": "9019923", "url": "https://en.wikipedia.org/wiki?curid=9019923", "title": "Cullingey", "text": "Cullingey\n\nA cullingey is an obsolete unit of mass that was used in the southern region in state of Karnataka in India. One cullingey was approximately equal to 81.25 troy grains (5.265 g). After metrification in the mid-20th century, the unit became obsolete.\n\n"}
{"id": "29923378", "url": "https://en.wikipedia.org/wiki?curid=29923378", "title": "EDP Sarichioi Wind Farm", "text": "EDP Sarichioi Wind Farm\n\nThe EDP Sarichioi Wind Farm is a proposed wind power project in Sarichioi, Tulcea County, Romania. It will have 11 individual Vestas V90 wind turbines with a nominal output of around 3 MW which will deliver up to 33 MW of power, enough to power over 21,500 homes, with a capital investment required of approximately US$50 million.\n"}
{"id": "38911457", "url": "https://en.wikipedia.org/wiki?curid=38911457", "title": "Egyptian Petroleum Research Institute", "text": "Egyptian Petroleum Research Institute\n\nThe Egyptian Petroleum Research Institution (EPRI) is a governmental organization in Egypt founded by the presidential decree 541 in 1974. It is under the umbrella of the Ministry of Scientific Research and Technology, to help advance the development of studies and applications within the oil sector, and to find solutions to both long and short term technical problems.\n\nTo effectively carry out this role the Ministry of petroleum is authorized for up to 50% of the EPRI's board of director shares; the Minister of Petroleum and Mineral Resources and Development is the head of the EPRI board, other members who are appointed by him to support and back the EPRI mission.\n\n"}
{"id": "34365252", "url": "https://en.wikipedia.org/wiki?curid=34365252", "title": "Estimated ultimate recovery", "text": "Estimated ultimate recovery\n\nEstimated ultimate recovery or Expected ultimate recovery (EUR) of a resource is the sum of the proven reserves at a specific time and the cumulative production up to that time.\n"}
{"id": "598708", "url": "https://en.wikipedia.org/wiki?curid=598708", "title": "Facultative aerobic organism", "text": "Facultative aerobic organism\n\nA facultative anaerobe is an organism that makes ATP by aerobic respiration if oxygen is present, but is capable of switching to fermentation if oxygen is absent.\n\nSome examples of facultatively anaerobic bacteria are \"Staphylococcus\" spp., \"Streptococcus\" spp., \"Escherichia coli\", \"Salmonella\", \"Listeria\" spp. and \"Shewanella oneidensis\". Certain eukaryotes are also facultative anaerobes, including fungi such as \"Saccharomyces cerevisiae\" and many aquatic invertebrates such as Nereid (worm) polychaetes.\n\n\n"}
{"id": "17033195", "url": "https://en.wikipedia.org/wiki?curid=17033195", "title": "Fauna of South America", "text": "Fauna of South America\n\nThe fauna of South America consists of a huge variety of unique animals some of which evolved in relative isolation. The isolation of South America had an abrupt end some few million years ago when the Isthmus of Panama was formed allowing small scale migration of animals that would result in the Great American Interchange. South America is the continent with the largest number or recorded bird species.\n\nFour examples of animals in South America appear below:\n"}
{"id": "19985012", "url": "https://en.wikipedia.org/wiki?curid=19985012", "title": "Fier Power Station", "text": "Fier Power Station\n\nThe Fier Power Station was Albania's largest thermal power plant having 6 identical groups of 31 MW each thus totalling a capacity of 186 MW.\n\nThe plant was decommissioned in 2007.\n"}
{"id": "6037443", "url": "https://en.wikipedia.org/wiki?curid=6037443", "title": "Friction tape", "text": "Friction tape\n\nFriction tape is a type of adhesive tape made from cloth impregnated with a rubber-based adhesive, mainly used to insulate splices in electric wires and cables. Because the adhesive is impregnated in the cloth, friction tape is sticky on both sides. The rubber-based adhesive makes it an electrical insulator and provides a degree of protection from liquids and corrosion. In the past, friction tape was widely used by electricians, but PVC electrical tape has replaced it in most applications today. The frictional properties of the tape come from the cloth material, which is usually made from cotton, while the fabric base protects electrical splices against punctures and abrasion.\n\nAside from its original purpose as an electrical insulating tape, a common use for friction tape is to wrap it around the blade of a hockey stick to improve puck control. Another use is to wrap it around the handle of a softball or baseball bat to provide a better grip. It is also useful as a base when wrapping a bicycle handlebar with handlebar tape.\n\n"}
{"id": "19221161", "url": "https://en.wikipedia.org/wiki?curid=19221161", "title": "Fuente Álamo Solar Power Plant", "text": "Fuente Álamo Solar Power Plant\n\nPlanta Solar Fuente Álamo is a photovoltaic power station in Fuente-Álamo, Murcia in Spain. It covers an area of . The power station has a capacity of 26 megawatts and its annual output is 44 GWh, equivalent to supply electricity to 13,000 households.\n\nThe project was developed and constructed by Gestamp Solar. It was later acquired by FRV. The power station cost nearly €200 million and it was commissioned in 2008.\n\n\n"}
{"id": "56088643", "url": "https://en.wikipedia.org/wiki?curid=56088643", "title": "Gernot Wagner", "text": "Gernot Wagner\n\nGernot Wagner (1980 in Austria) is an Austrian-American economist and author. He holds an AB and a PhD in political economy and government from Harvard University, as well as an MA in economics from Stanford University. He is a lecturer and researcher at Harvard University and is the co-author, with Martin L. Weitzman, of \"Climate Shock\", a Top 15 Financial Times-McKinsey Business Book of the Year 2015. He is married to Siripanth Nippita, a gynecologist at Harvard Medical School.\n\nWagner was an economist at the Environmental Defense Fund from 2008 to 2014 and lead senior economist from 2014 to 2016. While there he was a member of the faculty of the School of International and Public Affairs, Columbia University, and he wrote \"Climate Shock\" (2015), a book emphasizing the importance of risk and uncertainty for prompting action on climate change. Wagner was a member of the six-person lead author team, including Suzi Kerr, that wrote the World Bank's Emissions Trading in Practice : A Handbook on Design and Implementation.\n\n\"Risk\" and \"uncertainty\" in climate change are often mentioned as reasons to delay action. Wagner's \"Climate Shock\", joint with Martin Weitzman, emphasizes that the \"known unknowns\" and potential \"unknown unknowns\" instead increase the need for action. This contrasts with work done, for example, by economists Bill Nordhaus, Richard Tol, and others. Nordhaus, in turn, favorably reviewed Wagner and Weitzman's book in the New York Review of Books. Wagner's latest academic work on this topic, joint with Kent Daniel of Columbia University and Bob Litterman of Kepos Capital further emphasizes the importance of pricing climate risk and uncertainty.\n\nWagner is the co-director, joint with David Keith, of Harvard's Solar Geoengineering Research Program founded in 2017. His geoengineering research focuses on economics, governance, policy, and public perception, including the chemtrails conspiracy theory. Together with Dustin Tingley, Wagner finds that in a U.S. public opinion survey conducted in October 2016, 30 to 40% of the U.S. public believed in a version of the conspiracy. The paper also describes what the authors call a \"community of conspiracy\" in online discourse, in particular on Twitter and other anonymous social media.\n\nOn November 23rd, 2018, Wagner published an open-access article on \"Stratospheric aerosol injection tactics and costs in the first 15 years of deployment.\" The article was picked up in a questionable report by CNN, claiming that: \"Scientists are proposing an ingenious but as-yet-unproven way to tackle climate change: spraying sun-dimming chemicals into the Earth's atmosphere.\"\n\nGernot Wagner has written three books:\n\n"}
{"id": "48613487", "url": "https://en.wikipedia.org/wiki?curid=48613487", "title": "Gommateshwara statue", "text": "Gommateshwara statue\n\nGommateshwara Statue ಗೊಮ್ಮಟೇಶ್ವರ is a high monolithic statue located on Vindyagiri at Shravanbelagola in the Indian state of Karnataka. Vindyagiri Hill is one of the two hills in Shravanabelagola; the other is Chandragiri, which is also a seat of several ancient Jain centers, much older than Gommateshwara statue.\n\nThe Gommateshwara statue is dedicated to the Jain god Bahubali. It was built around 983 A.D. and is one of the largest free standing statues in the world. The construction of the statue was commissioned by the Ganga dynasty minister and commander, Chavundaraya. Neighboring areas have Jain temples known as \"basadis\" and several images of the Tirthankaras. Chandragiri is dedicated to the Jain God Bharat, the brother of Bahubali and the son of the first tirthankara Adinatha.\n\nOne can have a beautiful view of the surrounding areas from the top of the hill. An event known as Mahamastakabhisheka attracts devotees from all over the world. The Mahamastakabhisheka festival is held once in 12 years, when the Gommateshwara statue is anointed with milk, saffron, ghee, sugarcane juice (ishukrasa) etc. from the top of the statue Heinrich Zimmer attributed this anointment as the reason for the statue's freshness. The next abhisheka will be in 2030.\n\nIn 2007, the statue was voted as the first of Seven Wonders of India in a \"Times of India\" poll; 49% of the total votes went in favor of it.\n\nThe statue depicts the prolonged meditation of \"Bahubali\". The motionless contemplation in kayotsarga (standing still) posture led to the growth of climbing vines around his legs. The image of Gommateshwara has curly hair ringlets and large ears. The eyes are open as if he is viewing the world with detachment. His facial features are perfectly chiseled with a faint touch of a smile at the corner of the lips that embodies a calm inner peace and vitality. His shoulders are broad, the arms stretch straight down and the figure has no support from the thigh upwards.\n\nThere is an anthill in the background which signifies his incessant penance. From this anthill, emerge a snake and a creeper which twine around both the legs and arms culminating as a cluster of flowers and berries at the upper portion of the arms. The entire figure stands on an open lotus signifying the totality attained in installing this unique statue. On either side of Gommateshwara stand two tall and majestic chauri bearers in the service of the Lord. One of them is a yaksh and the other one is a yakshini. These richly ornamented and beautifully carved figures complement the main figure. Carved on the rear side of the anthill is also a trough for collecting water and other ritual ingredients used for the sacred bath of the statue.\n\nIn the introduction to his English translation of the \"Gommatsāra\", J. L. Jaini writes:\n\nThe event has been attended by multiple political personalities including Krishna-Rajendra Wodeyar in 1910, and Narendra Modi and Ramnath Kovind in 2018.\n\n\n"}
{"id": "1349150", "url": "https://en.wikipedia.org/wiki?curid=1349150", "title": "Heating element", "text": "Heating element\n\nA heating element converts electrical energy into heat through the process of resistive or Joule heating. Electric current passing through the element encounters resistance, resulting in heating of the element. Unlike the Peltier effect, this process is independent of the direction of current flow.\n\n\n\nResistive heaters can be made of conducting PTC rubber materials where the resistivity increases exponentially with increasing temperature. Such a heater will produce high power when it is cold, and rapidly heat up itself to a constant temperature. Due to the exponentially increasing resistivity, the heater can never heat itself to warmer than this temperature. Above this temperature, the rubber acts as an electrical insulator. The temperature can be chosen during the production of the rubber. Typical temperatures are between .\n\nIt is a point-wise self-regulating heater and self-limiting heater. Self-regulating means that every point of the heater independently keeps a constant temperature without the need of regulating electronics. Self-limiting means that the heater can never exceed a certain temperature in any point and requires no overheat protection.\n\n\n\n"}
{"id": "39319467", "url": "https://en.wikipedia.org/wiki?curid=39319467", "title": "Human viruses in water", "text": "Human viruses in water\n\nViruses are a major cause of human waterborne and water-related diseases. Waterborne diseases are caused by water that is contaminated by human and animal urine and feces that contain pathogenic microorganisms. A subject can get infected through contact with or consumption of the contaminated water. Viruses affect all living organisms from single cellular plants, bacteria and animal to the highest forms of plants and animals including human beings. Viruses can interact with their host through several methods and mechanisms; some viruses can be host specific (HIV) and some can be less host specific (influenza) viruses. Different viruses can have different routes of transmission; for example, HIV is directly transferred by contaminated body fluids from an infected host into the tissue or bloodstream of a new host while influenza is airborne and transmitted through inhalation of contaminated air containing viral particles by a new host. Enteric viruses primarily infect the intestinal tract through ingestion of food and water contaminated with viruses of fecal origin. Some viruses can be transmitted through all three routes of transmission.\n\nWater virology started about half a century ago when scientists attempted to detect the polio virus in water samples. Since then, other pathogenic viruses that are responsible for gastroenteritis, hepatitis, and many other virus strains have replaced enteroviruses as the main aim for detection in the water environment.\n\nWater virology was born after a large hepatitis outbreak transmitted through water was confirmed in New Delhi between December 1955 and January 1956.\n\nViruses can cause massive human mortality. The smallpox virus killed an estimated 10 to 15 million human beings per year until 1967. Smallpox was finally eliminated in 1977 by extinction of the virus through vaccination, and the impact of viruses such as influenza, poliomyelitis and measles are mainly controlled by vaccination.\n\nDespite advances in vaccination and prevention of viral diseases, it is estimated that in the 1980s a child died approximately every six seconds from diarrhea confirmed by WHO. Many cases of hepatitis A and/or E, both of which are enteric viruses, are typically transmitted by food and water. Extreme examples include the outbreak of 300,000 cases of hepatitis A and 25,000 cases of gastroenteritis in 1988 in Shanghai caused by shellfish harvested from a sewage polluted estuary. In 1991, an outbreak of 79,000 cases of hepatitis E in Kanpur was ascribed to drinking polluted water.\n\nA more recent outbreak of Hepatitis E in South Sudan killed 88 people. Medecins Sans Frontieres (MSF) said it had treated almost 4,000 patients since the outbreak was identified in South Sudan in July 2012. In this outbreak, Hepatitis E, which causes liver infections, and was thought to be spread by drinking water contaminated with feces.\n\nSewage contaminated water contains many viruses, over one hundred species are reported and can lead to diseases that affect human beings. For example, hepatitis, gastroenteritis, meningitis, fever, rash, and conjunctivitis can all be spread through contaminated water. More viruses are being discovered in water because of new detection and characterization methods, although only some of these viruses are human pathogens.\n\nViruses need a suitable environment to survive in. There are many characteristics that control the survival of viruses in water such as temperature, light, pH, salinity, organic matter, suspended solids or sediments, and air–water interfaces.\n\nTemperature has the highest effect on virus’s survival in water since lower temperatures are the key to longer virus survival. The rate of protein, nucleic acid denaturation and chemical reactions that destroy the viral capsid are increased at higher temperatures, thus viruses will survive best at low temperatures. Hepatitis A, adenoviruses and parvoviruses have the highest survival rate in low temperatures amongst enteric viruses.\n\nUltraviolet light (UV) is the light in sunlight and can inactivate viruses by causing cross-linking of the nucleotides in the viral genome. Many viruses in water are exterminated in the presence of sunlight. The combination of higher temperatures and more UV in the summer time corresponds to shorter viral survival in summer compared to winter. Double stranded DNA viruses like adenoviruses are more resistant to UV light inactivation than enteroviruses because they can use their host cell to repair the damage caused by the UV light.\n\nVisible light can also affect virus survival by a process called photodynamic inactivation but the length and intensity of the light exposure can change the inactivation rate.\n\nThe pH of most natural water is between 5-9. Enteric viruses are stable in these conditions. On the other hand, many enteric viruses are more stable at pH 3-5 than at pH 9 and 12. Enteroviruses can survive at pH 11-11.5 and 1-2, but for only short periods. Adenoviruses and rotaviruses are delicate to a pH of 10 or greater and leads to inactivation.\n\nWater that is intended for drinking should go through some treatment to reduce pathogenic viral and bacterial concentrations. As the density of the human population has increased the incidence of sewage contamination of water has increased as well, thus the risk to humans from pathogenic viruses will increase if precautions are not taken.\n\nScientific studies suggest that the most common viruses found are caliciviruses, astroviruses and enteric viruses. Laboratories are still looking for improved methods to detect these pathogenic viruses. Reducing the amount of viruses in drinking water is accomplished by various treatments that are typically part of drinking water treatment systems in developed countries.\n\nWater purification of surface water (water from lakes, rivers, or reservoirs) typically utilizes four treatment stages: coagulation and flocculation, sedimentation, filtration, and disinfection. The first three stages remove mainly dirt and larger particles, although filtration does reduce the number of viruses and bacteria in the water the number of pathogens present after filtration is still considered too high for drinking water. Purification of water from underground aquifers, called ground water, may skip some of these steps as ground water tends to have fewer contaminants than surface water. The last step, disinfection, is primarily responsible for the reduction of pathogenic viruses to safe levels in all drinking water sources. The most common disinfectants used are chlorine and chloramine. Ozone and UV light can also be used to treat large volumes of water to remove pathogens.\n\nThe quality of drinking water is ensured through a framework of water safety plans that ensures the safe disposal of human waste so that drinking water supplies are not contaminated. Improving the water supply, sanitation, hygiene and management of our water resources could prevent ten percent of total global disease.\n\nHalf of the hospital beds occupied in the world are related to the lack of safe drinking water. Unsafe water leads to the 88% of the global cases of diarrhea and 90% of the deaths of diarreaheal diseases in children under five years old. Most of these deaths occur in developing countries due to poverty and the high cost of safe water.\n\nApproximately 1.1 billion people do not have access to improved water and 2.4 billion people do not have access to sanitation facilities. This situation leads to 2 million preventable deaths each year.\n\n"}
{"id": "11026373", "url": "https://en.wikipedia.org/wiki?curid=11026373", "title": "Hydraulic engine house, Bristol Harbour", "text": "Hydraulic engine house, Bristol Harbour\n\nThe Hydraulic engine house is part of the \"Underfall Yard\" in Bristol Harbour in Bristol, England.\n\nThe octagonal brick and terracotta chimney of the engine house dates from 1888, and is grade II* listed, as is the hydraulic engine house itself. It replaced the original pumping house which is now The Pump House public house. It is built of red brick with a slate roof and originally contained two steam engines made by the Worthington Corporation. These were compound surface condensing cylinders of diameter. These were replaced in 1907 by the current machines from Fullerton, Hodgart and Barclay of Paisley. It powered the docks' hydraulic system of cranes, bridges and locks until 2010.\n\nWater is pumped from the harbour to a header tank and then fed by gravity to the high pressure pumps, where it is pressurised thence raising the external hydraulic accumulator. This stores the hydraulic energy ensuring a smooth delivery of pressure and meaning that the pumps do not need to be running the whole time nor be capable of supplying the instantaneous peak demands. The working pressure is 750 lbs/square inch. The external accumulator was added about 1954 when the original inside the building's tower became difficult to service (but it remains in place). The building originally contained a pair of steam powered pumps however these were replaced with three electrically driven ones in 1907. The engine house provided the power for equipment such as the lock gates and cranes until 2010.\n\nThe visitor centre in the hydraulic power house opened in time for Easter 2016.\n\n"}
{"id": "1632111", "url": "https://en.wikipedia.org/wiki?curid=1632111", "title": "Hydrobromide", "text": "Hydrobromide\n\nIn chemistry, a hydrobromide is an acid salt resulting, or regarded as resulting, from the reaction of hydrobromic acid with an organic base (e.g. an amine). The compounds are similar to hydrochlorides.\n\nSome drugs are formulated as hydrobromides, eg. eletriptan hydrobromide.\n\n"}
{"id": "515624", "url": "https://en.wikipedia.org/wiki?curid=515624", "title": "James Bay Energy", "text": "James Bay Energy\n\nThe Société d'énergie de la Baie James is the company in charge of building the hydroelectric development known as the James Bay Project in northern Quebec. It was established in December 1971 by the \"Société de développement de la Baie James\" (SDBJ), a Crown corporation of the province of Quebec and became a wholly owned subsididiary of Hydro-Québec in 1978.\n\nIn the summer of 1971, the National Assembly of Quebec passed Bill 50, an Act establishing the Société de développement de la Baie James (SDBJ), a Crown corporation tasked with the development of natural resources as well as the general administration of the James Bay territory. The development of the hydroelectric potential was the focus of a newly created subsidiary (the SEBJ) with Hydro-Québec as a majority shareholder along with the SDBJ. Both corporations would be chaired by Pierre A. Nadeau, an insurance executive recruited by Robert Bourassa's money man, Paul Desrochers.\n\nThe passage of Bill 50 was controversial with Hydro-Québec's top executives, who felt the government tried to bypass the powerful utility. As the party picking up the bill, Hydro wanted control over the development of the hydroelectric project. Commissioner Robert A. Boyd, who was slated as the top Hydro-Québec executive and Nadeau's right-hand man at the SEBJ, first declined the appointment, explaining that with no background in public works, Nadeau would have little credibility with the engineers running the operations. Conflict soon erupted between Hydro-Québec and Nadeau, who left less than a year after his appointment. He was replaced by Boyd, who kept informed of the latest development on the project by consulting with colleagues on the board on a daily basis.\n\n\n\n"}
{"id": "24629990", "url": "https://en.wikipedia.org/wiki?curid=24629990", "title": "James Blyth (engineer)", "text": "James Blyth (engineer)\n\nProfessor James Blyth MA, LLD, FRSE FRSSA (4 April 1839 – 15 May 1906) was a Scottish electrical engineer and academic at Anderson's College, now the University of Strathclyde, in Glasgow. He was a pioneer in the field of electricity generation through wind power and his wind turbine, which was used to light his holiday home in Marykirk, was the world's first-known structure by which electricity was generated from wind power. Blyth patented his design and later developed an improved model which served as an emergency power source at Montrose Lunatic Asylum, Infirmary & Dispensary for the next 30 years. Although Blyth received recognition for his contributions to science, electricity generation by wind power was considered uneconomical and no more wind turbines were built in the United Kingdom until 1951, some 64 years after Blyth built his first prototype.\n\nJames Blyth was born in Marykirk, Kincardineshire, on 4 April 1839 to John Blyth, an innkeeper and small farmer, and his wife Catherine. He attended the Marykirk parish school and later, the Montrose Academy before winning a scholarship to the General Assembly Normal School, Edinburgh in 1886. After graduating as a Bachelor of Arts from The University of Edinburgh in 1861, Blyth taught mathematics at Morrison's Academy in Crieff and established the technical and scientific curriculum for the newly established George Watson's College in Edinburgh.\n\nBlyth completed his Master of Arts in 1871 and in the same year married Jesse Wilhelmena Taylor at the United Presbyterian Church in Athol Place, Edinburgh. They had two sons and five daughters, two of whom died in infancy.\n\nIn 1880 Blyth was appointed Freeland Professor of Natural Philosophy at Anderson's College, Glasgow, which became the Glasgow and West of Scotland Technical College in 1886. Whilst teaching at the technical college he pursued an active research programme with a particular interest in the generation and storage of electricity from wind power. Blyth was liked by his students and colleagues who admired his hard working nature, down-to-earth attitude and willingness to roll up his sleeves. He was also well known in the local community through a series of popular lectures and demonstrations.\n\nIn July 1887 Blyth built a cloth-sailed wind turbine (or \"windmill\") in the garden of his holiday cottage in Marykirk and used the electricity it produced to charge accumulators; the stored electricity was used to power the lights in his cottage, which thus became the first house in the world to be powered by wind-generated electricity.\nIn a paper delivered to the Philosophical Society of Glasgow on 2 May 1888, Blyth described the wind turbine as being \"of a tripod design, with a 33-foot windshaft, four arms of 13 feet with canvas sails, and a Burgin dynamo driven from the flywheel using a rope\". The turbine produced enough power to light ten 25-volt bulbs in a \"moderate breeze\" and could even be used to power a small lathe.\n\nOver the next few years Blyth experimented with a number of different designs. The final design operated for the next 25 years and produced surplus electricity which Blyth offered to the people of Marykirk to light the main street of the town. But his offer was rejected, as the people thought electricity was \"the work of The Devil\". Blyth was awarded a UK patent for his \"wind engine\" in November 1891. In 1895 he licensed the Glasgow engineering company, Mavor and Coulson, to build a second, improved turbine, which was used to supply emergency power to the Lunatic Asylum, Infirmary and Dispensary of Montrose; the system operated successfully for the next 30 years.\n\nBlyth's original wind generator was the first known structure by which electricity was generated from wind power, but its lack of a braking mechanism meant it was prone to damage in strong winds. In the winter of 1887, some months after Blyth's first wind generator was built, American, Charles F. Brush built the first automatically operated wind turbine. The design of Brush's machine allowed it to be shut down manually to protect it from wind damage. The improved design of the turbine built for the Montrose Lunatic Asylum (which was based on Thomas Robinson's anemometer design) went some way towards solving this problem but it could not be guaranteed to stall in very strong winds.\n\nIn 1891 Blyth presented a paper to the Royal Society of Edinburgh espousing his belief in the benefits of renewable energy sources, particularly wind but also wave energy. Later that year he was awarded the Brisbane Gold Medal by the Royal Scottish Society of Arts for his work in producing electrical energy from wind, but his wind turbine was not considered to be economically viable.\n\nBlyth's other research interests included the relative efficiency of different forms of lighting, telephone communication, and microphones; he also contributed entries on a number of topics for the ninth edition of the Encyclopædia Britannica.\n\nBlyth's son, Vincent James (1874–1916), and his son-in-law, George Edwin Allan (1870–1955), both became demonstrators, assistants and lecturers in the Department of Natural Philosophy. Blyth himself was awarded an honorary doctorate by the University of Glasgow in 1900. He died from apoplexy at his home in Glasgow on 15 May 1906. His friend Dr. James Colville remembered him as \"a true man of science...one who by insight, patient toil, and mechanical ingenuity did much in his day to explain and illustrate many of the facts of physical science.\"\n\n\"The Professor James Blyth Memorial Committee\", composed of Blyth's former students and associates, was established in March 1907 to raise funds for a memorial to him. The memorial eventually took the form of endowing the Blyth Memorial Prizes, and erecting a wall plaque in the College. The turbine at Montrose Asylum was dismantled in 1914. Britain's first public utility wind turbine did not come into operation until 1951, when a prototype turbine built by John Brown Engineering of Glasgow was installed at Costa Head, Orkney.\n\n\n"}
{"id": "1600866", "url": "https://en.wikipedia.org/wiki?curid=1600866", "title": "Jellium", "text": "Jellium\n\nJellium, also known as the uniform electron gas (UEG) or homogeneous electron gas (HEG), is a quantum mechanical model of interacting electrons in a solid where the positive charges (i.e. atomic nuclei) are assumed to be uniformly distributed in space whence the electron density\nis a uniform quantity as well in space. This model allows one to focus on the effects in solids that\noccur due to the quantum nature of electrons and their mutual repulsive interactions (due to like charge)\nwithout explicit introduction of the atomic lattice and structure making up a real material. Jellium is often used in solid-state physics as a simple model of delocalized electrons in a metal, where it can qualitatively reproduce features of real metals such as screening, plasmons, Wigner crystallization and Friedel oscillations.\n\nAt zero temperature, the properties of jellium depend solely upon the constant electronic density. This lends it to a treatment within density functional theory; the formalism itself provides the basis for the local-density approximation to the exchange-correlation energy density functional.\n\nThe term \"jellium\" was coined by Conyers Herring, alluding to the \"positive jelly\" background, and the typical metallic behavior it displays.\n\nThe jellium model treats the electron-electron coupling rigorously. The artificial and structureless background charge interacts electrostatically with itself and the electrons. The jellium Hamiltonian for \"N\"-electrons confined within a volume of space Ω, and with electronic density \"ρ\"(r) and (constant) background charge density \"n\"(R) = \"N\"/Ω is\n\nwhere\n\n\n\"H\" is a constant and, in the limit of an infinite volume, divergent along with \"H\". The divergence is canceled by a term from the electron-electron coupling: the background interactions cancel and the system is dominated by the kinetic energy and coupling of the electrons. Such analysis is done in Fourier space; the interaction terms of the Hamiltonian which remain correspond to the Fourier expansion of the electron coupling for which q ≠ 0.\n\nThe traditional way to study the electron gas is to start with non-interacting electrons which are governed only by the kinetic energy part of the Hamiltonian, also called a Fermi gas. The kinetic energy per electron is given by\nwhere formula_6 is the Fermi energy, formula_7 is the Fermi wave vector, and the last expression shows the dependence on the Wigner-Seitz radius formula_8 where energy is measured in Rydbergs.\n\nWithout doing much work, one can guess that the electron-electron interactions will scale like the inverse of the average electron-electron separation and hence as formula_9 (since the Coulomb interaction goes like one over distance between charges) so that if we view the interactions as a small correction to the kinetic energy, we are describing the limit of small formula_8 (i.e. formula_11 being larger than formula_12) and hence high electron density. Unfortunately, real metals typically have formula_13 between 2-5 which means this picture needs serious revision.\n\nThe first correction to the free electron model for jelium is from the Fock exchange contribution to electron-electron interactions. Adding this in, one has a total energy of\nwhere the negative term is due to exchange: exchange interactions lower the total energy. Higher order\ncorrections to the total energy are due to electron correlation and if one\ndecides to work in a series for small formula_13, one finds\nThe series is quite accurate for small formula_13 but of dubious value for formula_13 values found in actual metals.\n\nFor the full range of formula_13, Chachiyo's correlation energy density can be used as the higher order correction. In this case,\n\nThe physics of the zero-temperature phase behavior of jellium is driven by competition between the kinetic energy of the electrons and the electron-electron interaction energy. The kinetic-energy operator in the Hamiltonian scales as formula_11, where formula_13 is the Wigner-Seitz radius, whereas the interaction energy operator scales as formula_12. Hence the kinetic energy dominates at high density (small formula_13), while the interaction energy dominates at low density (large formula_13).\n\nThe limit of high density is where jellium most resembles a noninteracting free electron gas. To minimize the kinetic energy, the single-electron states are delocalized plane waves, with the lowest-momentum plane-wave states being doubly occupied by spin-up and spin-down electrons, giving a paramagnetic Fermi fluid.\n\nAt lower densities, where the interaction energy is more important, it is energetically advantageous for the electron gas to spin-polarize (i.e., to have an imbalance in the number of spin-up and spin-down electrons), resulting in a ferromagnetic Fermi fluid. This phenomenon is known as \"itinerant ferromagnetism\". At sufficiently low density, the kinetic-energy penalty resulting from the need to occupy higher-momentum plane-wave states is more than offset by the reduction in the interaction energy due to the fact that exchange effects keep indistinguishable electrons away from one another.\n\nA further reduction in the interaction energy (at the expense of kinetic energy) can be achieved by localizing the electron orbitals. As a result, jellium at zero temperature at a sufficiently low density will form a so-called Wigner crystal, in which the single-particle orbitals are of approximately Gaussian form centered on crystal lattice sites. Once a Wigner crystal has formed, there may in principle be further phase transitions between different crystal structures and between different magnetic states for the Wigner crystals (e.g., antiferromagnetic to ferromagnetic spin configurations) as the density is lowered. When Wigner crystallization occurs, jellium acquires a band gap.\n\nWithin Hartree-Fock theory, the ferromagnetic fluid abruptly becomes more stable than the paramagnetic fluid at a density parameter of formula_26 in three dimensions (3D) and formula_27 in two dimensions (2D). However, according to Hartree-Fock theory, Wigner crystallization occurs at formula_28 in 3D and formula_29 in 2D, so that jellium would crystallise before itinerant ferromagnetism occurs. Furthermore, Hartree-Fock theory predicts exotic magnetic behavior, with the paramagnetic fluid being unstable to the formation of a spiral spin-density wave. Unfortunately Hartree-Fock theory does not include any description of correlation effects, which are energetically important at all but the very highest densities, and so a more accurate level of theory is required to make quantitative statements about the phase diagram of jellium.\n\nQuantum Monte Carlo (QMC) methods, which provide an explicit treatment of electron correlation effects, are generally agreed to provide the most accurate quantitative approach for determining the zero-temperature phase diagram of jellium. The first application of the diffusion Monte Carlo method was Ceperley and Alder's famous 1980 calculation of the zero-temperature phase diagram of 3D jellium. They calculated the paramagnetic-ferromagnetic fluid transition to occur at formula_30 and Wigner crystallization (to a body-centered cubic crystal) to occur at formula_31. Subsequent QMC calculations have refined their phase diagram: there is a second-order transition from a paramagnetic fluid state to a partially spin-polarized fluid from formula_32 to about formula_33; and Wigner crystallization occurs at formula_34.\n\nIn 2D, QMC calculations indicate that the paramagnetic fluid to ferromagnetic fluid transition and Wigner crystallization occur at similar density parameters, in the range formula_35. The most recent QMC calculations indicate that there is no region of stability for a ferromagnetic fluid. Instead there is a transition from a paramagnetic fluid to a hexagonal Wigner crystal at formula_36. There is possibly a small region of stability for a (frustrated) antiferromagnetic Wigner crystal, before a further transition to a ferromagnetic crystal. The crystallization transition in 2D is not first order, so there must be a continuous series of transitions from fluid to crystal, perhaps involving striped crystal/fluid phases. Experimental results for a 2D hole gas in a GaAs/AlGaAs heterostructure (which, despite being clean, may not correspond exactly to the idealized jellium model) indicate a Wigner crystallization density of formula_37.\n\nJellium is the simplest model of interacting electrons. It is employed in the calculation of properties of metals, where the core electrons and the nuclei are modeled as the uniform positive background and the valence electrons are treated with full rigor. Semi-infinite jellium slabs are used to investigate surface properties such as work function and surface effects such as adsorption; near surfaces the electronic density varies in an oscillatory manner, decaying to a constant value in the bulk.\n\nWithin density functional theory, jellium is used in the construction of the local-density approximation, which in turn is a component of more sophisticated exchange-correlation energy functionals. From quantum Monte Carlo calculations of jellium, accurate values of the correlation energy density have been obtained for several values of the electronic density, which have been used to construct semi-empirical correlation functionals.\n\nThe jellium model has been applied to superatoms, and used in nuclear physics.\n\n"}
{"id": "35642357", "url": "https://en.wikipedia.org/wiki?curid=35642357", "title": "Joseph Neng Shun Kwong", "text": "Joseph Neng Shun Kwong\n\nJoseph Neng Shun Kwong (October 28, 1916 – January 4, 1998) was a chemical engineer, most famous for his role in the development of the Redlich–Kwong equation of state.\n\nJoseph Kwong was born in Chung Won, China in 1916, and emigrated to the United States as a child with his family. Kwong earned a bachelor's degree in 1937 from Stanford University in chemistry and basic medical sciences. He then earned a Master of Science degree in Chemical and Metallurgical Engineering at the University of Michigan, and was awarded a Ph.D. in chemical engineering from the University of Minnesota in 1942.\n\nKwong worked as a chemical engineer at Minnesota Mining and Manufacturing Co. (later 3M) from 1942 to 1944, helping to develop adhesive products. In 1944, Kwong began working at the Shell Development Company in California. During his time at Shell, Kwong met Otto Redlich, a physical chemist who had fled his native Austria to the United States in 1938 as the Nazis took power. The two presented a paper in 1948 describing what is now known as the Redlich–Kwong equation of state, which related the pressure, volume, and temperature of different compounds. Kwong returned to 3M in 1951 as a senior chemical engineer in the Chemical Division, working there until retirement in 1980, at the age of 64. The development of the Redlich-Kwong equation was the last significant theoretical treatment of thermodynamics. He died of pneumonia in Saint Paul, Minnesota, on January 4, 1998, at the age of 81.\n"}
{"id": "30047379", "url": "https://en.wikipedia.org/wiki?curid=30047379", "title": "Knudsen force", "text": "Knudsen force\n\nKnudsen force is the force experienced by two surfaces at two different temperatures that are separated by a distance comparable to a mean free path of the molecules of the ambient medium.\n"}
{"id": "17390", "url": "https://en.wikipedia.org/wiki?curid=17390", "title": "Kryptonite", "text": "Kryptonite\n\nKryptonite is a material that appears primarily in Superman stories. In its most well-known form, it is a green, crystalline material that emits a peculiar radiation that weakens and sickens Superman, but is generally harmless to humans when exposed to it in short term. However, when it gets into their bloodstream it can poison them. There are other varieties of kryptonite such as red and gold kryptonite which have different but still generally negative effects on Superman. Due to Superman's popularity \"kryptonite\" has become a byword for an extraordinary weakness, synonymous with \"Achilles' heel\".\n\nAn unpublished 1940 story titled \"The K-Metal from Krypton\", written by Superman creator Jerry Siegel, featured a prototype of kryptonite. It was a mineral from the planet Krypton that drained Superman of his strength while giving superhuman powers to mortals. This story was rejected because in it Superman reveals his identity to Lois.\n\nThe mineral known as kryptonite was introduced in the radio serial \"The Adventures of Superman\", in the story \"The Meteor from Krypton\", broadcast in June 1943. An apocryphal story claims that kryptonite was introduced to give Superman's voice actor, Bud Collyer, the possibility to take a vacation at a time when the radio serial was performed live. In an episode where Collyer would not be present to perform, Superman would be incapacitated by kryptonite, and a substitute voice actor would make groaning sounds. This tale was recounted by Julius Schwartz in his memoir. However, the historian Michael J. Hayde disputes this: in \"The Meteor From Krypton\", Superman is never exposed to kryptonite. If kryptonite allowed Collyer to take vacations, that was a fringe benefit discovered later. More likely, kryptonite was introduced as a plot device for Superman to discover his origin.\n\nIn the radio serial, Krypton was located in the same solar system as Earth, in the same orbit, but on the opposite side of the Sun. This provided an easy explanation for how kryptonite found its way to Earth.\n\nKryptonite was incorporated into the comic mythos with \"Superman\" #61 (November 1949). Editor Dorothy Woolfolk stated in an interview with \"Florida\" \"Today\" in August 1993, that she \"felt Superman's invulnerability was boring.\"\n\nVarious forms of the fictional material have been created over the years in \"Superman\" publications.\n\nOther varieties of the material have appeared, but have been revealed to be hoaxes, such as \"Yellow\" (\"Action Comics\" #277 (June 1961)), \"Kryptonite Plus\" (\"Superman's Pal, Jimmy Olsen\" #126 (Jan. 1970)) and \"Blood\" (\"52\" #13 (Aug. 2006)).\n\n\n\n\n\nColumbia Pictures produced two 15-part motion picture serials that used kryptonite as a plot device: \"Superman\" (1948) and \"Atom Man vs. Superman\" (1950).\n\nSongs:\n\n"}
{"id": "39417698", "url": "https://en.wikipedia.org/wiki?curid=39417698", "title": "Kuokuang Power Plant", "text": "Kuokuang Power Plant\n\nThe Kuokuang Power Plant () is a gas-fired power plant in Guishan District, Taoyuan City, Taiwan. At a total capacity of 480 MW, it is currently Taiwan's smallest gas-fired power plant.\n\nKuo Kuang Power Corporation is a company owned by China National Petroleum Corporation [CPC] (45%), CTCI Taiwan (20%) and Meiya Power Company (35%)\n\nKuokuang Power Plant is accessible East from TRA Nanxiang Station.\n\n\nTaipei Times: \"CPC to use state-owned land for new power plant\" 2nd Dec 2000\n"}
{"id": "30741937", "url": "https://en.wikipedia.org/wiki?curid=30741937", "title": "Lists of renewable energy topics", "text": "Lists of renewable energy topics\n\nRenewable energy is generally defined as energy that comes from resources which are naturally replenished on a human timescale, such as sunlight, wind, rain, tides, waves, and geothermal heat. Renewable energy replaces conventional fuels in four distinct areas: electricity generation, air and water heating/cooling, motor fuels, and rural (off-grid) energy services. Based on REN21's 2014 report, renewables contributed 19 percent to our global energy consumption and 22 percent to our electricity generation in 2012 and 2013, respectively.\n\nThese are lists about renewable energy:\n\n\n"}
{"id": "28993946", "url": "https://en.wikipedia.org/wiki?curid=28993946", "title": "Low-level injection", "text": "Low-level injection\n\nLow-level injection conditions for a p–n junction refers to the state where the number of minority carriers generated is small compared to the majority carriers of the material. Essentially, the semiconductor's majority-carrier concentration will remain (relatively) unchanged, while the minority-carrier concentration sees a large increase. In this condition minority-carrier recombination rates are linear.\n\nThe following equation must be satisfied for a semiconductor under carrier injection conditions:\nwhere formula_2 is the number of electrons, formula_3 is the excess carriers injected into the semiconductor, and formula_4 is the equilibrium concentration of electrons in the semiconductor\n\nThe following relation must also be true, because for every electron injected a hole must also be created to keep a balance of charge:\n\nThe assumption of low-level injection can be made regarding an n-type semiconductor, which affects the equations in the following way:\nTherefore formula_7 and formula_8.\n\nIn comparison, a semiconductor in high injection means that the number of generated carriers is large compared to the background doping density of the material. In this condition minority carrier recombination rates are proportional to the number of carriers squared\n"}
{"id": "17745", "url": "https://en.wikipedia.org/wiki?curid=17745", "title": "Lutetium", "text": "Lutetium\n\nLutetium is a chemical element with symbol Lu and atomic number 71. It is a silvery white metal, which resists corrosion in dry air, but not in moist air. Lutetium is the last element in the lanthanide series, and it is traditionally counted among the rare earths. Lutetium is sometimes considered the first element of the 6th-period transition metals, although lanthanum is more often considered as such.\n\nLutetium was independently discovered in 1907 by French scientist Georges Urbain, Austrian mineralogist Baron Carl Auer von Welsbach, and American chemist Charles James. All of these researchers found lutetium as an impurity in the mineral ytterbia, which was previously thought to consist entirely of ytterbium. The dispute on the priority of the discovery occurred shortly after, with Urbain and Welsbach accusing each other of publishing results influenced by the published research of the other; the naming honor went to Urbain, as he had published his results earlier. He chose the name lutecium for the new element, but in 1949 the spelling of element 71 was changed to lutetium. In 1909, the priority was finally granted to Urbain and his names were adopted as official ones; however, the name \"cassiopeium\" (or later \"cassiopium\") for element 71 proposed by Welsbach was used by many German scientists until the 1950s.\n\nLutetium is not a particularly abundant element, although it is significantly more common than silver in the earth's crust. It has few specific uses. Lutetium-176 is a relatively abundant (2.5%) radioactive isotope with a half-life of about 38 billion years, used to determine the age of minerals and meteorites. Lutetium usually occurs in association with the element yttrium and is sometimes used in metal alloys and as a catalyst in various chemical reactions. Lu-DOTA-TATE is used for radionuclide therapy (see Nuclear medicine) on neuroendocrine tumours. Lutetium has the highest Brinell hardness of any lanthanide, at 890–1300 MPa.\nA lutetium atom has 71 electrons, arranged in the configuration [Xe] 4f5d6s. When entering a chemical reaction, the atom loses its two outermost electrons and the single 5d-electron. The lutetium atom is the smallest among the lanthanide atoms, due to the lanthanide contraction, and as a result lutetium has the highest density, melting point, and hardness of the lanthanides.\n\nLutetium's compounds always contain the element in the oxidation state +3. Aqueous solutions of most lutetium salts are colorless and form white crystalline solids upon drying, with the common exception of the iodide. The soluble salts, such as nitrate, sulfate and acetate form hydrates upon crystallization. The oxide, hydroxide, fluoride, carbonate, phosphate and oxalate are insoluble in water.\n\nLutetium metal is slightly unstable in air at standard conditions, but it burns readily at 150 °C to form lutetium oxide. The resulting compound is known to absorb water and carbon dioxide, and may be used to remove vapors of these compounds from closed atmospheres. Similar observations are made during reaction between lutetium and water (slow when cold and fast when hot); lutetium hydroxide is formed in the reaction. Lutetium metal is known to react with the four lightest halogens to form trihalides; all of them (except the fluoride) are soluble in water.\n\nLutetium dissolves readily in weak acids and dilute sulfuric acid to form solutions containing the colorless lutetium ions, which are coordinated by between seven and nine water molecules, the average being [Lu(HO)].\n\nLutetium occurs on the Earth in form of two isotopes: lutetium-175 and lutetium-176. Out of these two, only the former is stable, making the element monoisotopic. The latter one, lutetium-176, decays via beta decay with a half-life of 3.78×10 years; it makes up about 2.5% of natural lutetium.\nTo date, 32 synthetic radioisotopes of the element have been characterized, ranging in mass from 149.973 (lutetium-150) to 183.961 (lutetium-184); the most stable such isotopes are lutetium-174 with a half-life of 3.31 years, and lutetium-173 with a half-life of 1.37 years. All of the remaining radioactive isotopes have half-lives that are less than 9 days, and the majority of these have half-lives that are less than half an hour. Isotopes lighter than the stable lutetium-175 decay via electron capture (to produce isotopes of ytterbium), with some alpha and positron emission; the heavier isotopes decay primarily via beta decay, producing hafnium isotopes.\n\nThe element also has 42 nuclear isomers, with masses of 150, 151, 153–162, 166–180 (not every mass number corresponds to only one isomer). The most stable of them are lutetium-177m, with half-life of 160.4 days and lutetium-174m, with half-life of 142 days; this is longer than half-lives of the ground states of all radioactive lutetium isotopes, except only for lutetium-173, 174, and 176.\n\nLutetium, derived from the Latin \"Lutetia\" (Paris), was independently discovered in 1907 by French scientist Georges Urbain, Austrian mineralogist Baron Carl Auer von Welsbach, and American chemist Charles James. They found it as an impurity in ytterbia, which was thought by Swiss chemist Jean Charles Galissard de Marignac to consist entirely of ytterbium. The scientists proposed different names for the elements: Urbain chose \"neoytterbium\" and \"lutecium\", whereas Welsbach chose \"aldebaranium\" and \"cassiopeium\" (after Aldebaran and Cassiopeia). Both of these articles accused the other man of publishing results based on those of the author.\n\nThe International Commission on Atomic Weights, which was then responsible for the attribution of new element names, settled the dispute in 1909 by granting priority to Urbain and adopting his names as official ones, based on the fact that the separation of lutetium from Marignac's ytterbium was first described by Urbain; after Urbain's names were recognized, neoytterbium was reverted to ytterbium. Until the 1950s, some German-speaking chemists called lutetium by Welsbach's name, \"cassiopeium\"; in 1949, the spelling of element 71 was changed to lutetium. The reason for this was that Welsbach's 1907 samples of lutetium had been pure, while Urbain's 1907 samples only contained traces of lutetium. This later misled Urbain into thinking that he had discovered element 72, which he named celtium, which was actually very pure lutetium. The later discrediting of Urbain's work on element 72 led to a reappraisal of Welsbach's work on element 71, so that the element was renamed to \"cassiopeium\" in German-speaking countries for some time. Charles James, who stayed out of the priority argument, worked on a much larger scale and possessed the largest supply of lutetium at the time. Pure lutetium metal was first produced in 1953.\n\nFound with almost all other rare-earth metals but never by itself, lutetium is very difficult to separate from other elements. Its principal commercial source is as a by-product from the processing of the rare earth phosphate mineral monazite (Ce,La...)PO, which has concentrations of only 0.0001% of the element, not much higher than the abundance of lutetium in the Earth crust of about 0.5 mg/kg. No lutetium-dominant minerals are currently known. The main mining areas are China, United States, Brazil, India, Sri Lanka and Australia. The world production of lutetium (in the form of oxide) is about 10 tonnes per year. Pure lutetium metal is very difficult to prepare. It is one of the rarest and most expensive of the rare earth metals with the price about US$10,000 per kilogram, or about one-fourth that of gold.\n\nCrushed minerals are treated with hot concentrated sulfuric acid to produce water-soluble sulfates of rare earths. Thorium precipitates out of solution as hydroxide and is removed. After that the solution is treated with ammonium oxalate to convert rare earths into their insoluble oxalates. The oxalates are converted to oxides by annealing. The oxides are dissolved in nitric acid that excludes one of the main components, cerium, whose oxide is insoluble in HNO. Several rare earth metals, including lutetium, are separated as a double salt with ammonium nitrate by crystallization. Lutetium is separated by ion exchange. In this process, rare-earth ions are sorbed onto suitable ion-exchange resin by exchange with hydrogen, ammonium or cupric ions present in the resin. Lutetium salts are then selectively washed out by suitable complexing agent. Lutetium metal is then obtained by reduction of anhydrous LuCl or LuF by either an alkali metal or alkaline earth metal.\n\nBecause of production difficulty and high price, lutetium has very few commercial uses, especially since it is rarer than most of the other lanthanides but is chemically not very different. However, stable lutetium can be used as catalysts in petroleum cracking in refineries and can also be used in alkylation, hydrogenation, and polymerization applications.\n\nLutetium aluminium garnet (AlLuO) has been proposed for use as a lens material in high refractive index immersion lithography. Additionally, a tiny amount of lutetium is added as a dopant to gadolinium gallium garnet (GGG), which is used in magnetic bubble memory devices. Cerium-doped lutetium oxyorthosilicate (LSO) is currently the preferred compound for detectors in positron emission tomography (PET). Lutetium aluminium garnet (LuAG) is used as a phosphor in LED light bulbs.\n\nAside from stable lutetium, its radioactive isotopes have several specific uses. The suitable half-life and decay mode made lutetium-176 used as a pure beta emitter, using lutetium which has been exposed to neutron activation, and in lutetium–hafnium dating to date meteorites. The synthetic isotope lutetium-177 bound to octreotate (a somatostatin analogue), is used experimentally in targeted radionuclide therapy for neuroendocrine tumors. Indeed, lutetium-177 is seeing increasing use as a radionuclide, in neuroendrocine tumor therapy and bone pain palliation. Research indicates that lutetium-ion atomic clocks could provide greater accuracy than any existing atomic clock.\n\nLutetium tantalate (LuTaO) is the densest known stable white material (density 9.81 g/cm) and therefore is an ideal host for X-ray phosphors. The only denser white material is thorium dioxide, with density of 10 g/cm, but the thorium it contains is radioactive.\n\nLike other rare-earth metals, lutetium is regarded as having a low degree of toxicity, but its compounds should be handled with care nonetheless: for example, lutetium fluoride inhalation is dangerous and the compound irritates skin. Lutetium nitrate may be dangerous as it may explode and burn once heated. Lutetium oxide powder is toxic as well if inhaled or ingested.\n\nSimilarly to the other rare-earth metals, lutetium has no known biological role, but it is found even in humans, concentrating in bones, and to a lesser extent in the liver and kidneys. Lutetium salts are known to occur together with other lanthanide salts in nature; the element is the least abundant in the human body of all lanthanides. Human diets have not been monitored for lutetium content, so it is not known how much the average human takes in, but estimations show the amount is only about several micrograms per year, all coming from tiny amounts taken by plants. Soluble lutetium salts are mildly toxic, but insoluble ones are not.\n"}
{"id": "31666957", "url": "https://en.wikipedia.org/wiki?curid=31666957", "title": "Nigerian Electricity Regulatory Commission", "text": "Nigerian Electricity Regulatory Commission\n\nNigerian Electricity Regulatory Commission (NERC) is an independent regulatory body with authority for the regulation of the electric power industry in Nigeria. NERC was formed in 2005 under the Obasanjo administration’s economic reform agenda through the Electric Power Sector Reform Act, 2005 for formation and review of electricity tariffs, transparent policies regarding subsidies, promotion of policies that are efficient and environmentally friendly, and also including forming and enforcing of standards in the creation and use of electricity in Nigeria. NERC was instituted primarily to regulate the tariff of Power Generating companies owned or controlled by the government, and any other generating company which has a license for power generation and transmission of energy, and distribution of electricity.\n\nElectric power generation in Nigeria began in 1896. \nIn 1929, the Nigeria Electric Supply Company (NESCO) was established. In 1951, the Electric Corporation of Nigeria (ECN) was established to take over the assets of NESCO. In 1962, NDA (Nigeria Dams Authority) was established to develop the hydropower potentials in Nigeria. In 1972, ECN and NDA were merged to form NEPA (National Electric Power Authority), which later metamorphosized to Power Holding Company of Nigeria, as a holding company for its imminent unbundling and subsequent privatization.\nPreviously, the Federal Ministry of Power oversees the electric power sector in Nigeria. \nIt served both as the policy making body and the regulator; doing the latter mostly through the Electrical Inspectorate Services, a department in the Ministry.\nThe electric power sector in Nigeria started with the Niger Dams Authority which controlled the Dams around Shiroro and River Niger.\nDue to abysmal power crises in the whole of Nigeria, the government of President Olusegun Obasanjo made efforts through the National Council for Privatisation/Bureau for Public Service (under the leadership of Nasir Ahmad el-Rufai to reform the sector which has seen no investment or major government attention since the 1980s.\nThe NERC was formed through the EPSRAct of 2005 and it was inaugurated on 30 October 2007 with Ramsome Owan as its first Chairman/CEO. Dr. Ransome Owan, a US trained scientist who once worked for GE, was appointed for a five-year term as the executive Chairman of NERC. On his team included other Nigerians living in Diaspora who came in to work for NERC. \nNERC was given additional responsibilities for setting up and administering a fund called “Power Consumer Assistance Fund” which shall subsidize underprivileged power consumers in Nigeria. It also had the mandate to regulate the rural systems and determine the contribution rates to be sent to the Rural Electrification Fund.\n\nThe Commission's powers and duties are provided for in the EPSRAct 2005, and effectively ushered the privatization of electric power services in Nigeria, unbundling of the defunct National Electricity Power Authority (NEPA)/Power Holding Company of Nigeria (PHCN).\nNERC’s primary duty is protect the interests of consumers, issue licenses to operators/investors, set and review electricity tariffs and where possible promote competition.\nThe Commission's main objective is to protect existing and future consumers' interests in relation to electricity generated and that conveyed by distribution or transmission systems. Consumers' interests are their interests taken as a whole, including their interests in affordable tariffs and safe, reliable and available electricity supply, and the reduction of greenhouse gases to them.\n\nThe Nigerian Electricity Regulatory Commission is governed by a tenured Board of Commissioners, headed by a Chairman. The Nigerian President nominates one nominee Commissioner to represent his/her geopolitical zone in the country for a fixed tenure of 4 years, renewable once only. The Chairman/CEO, has a period of 5 years, also renewable once only. The nominees are duly screened by the Nigerian Senate.\nThe Board of Commission of NERC issues orders on electricity matters in Nigeria. It makes regulatory decisions and issues final license to investors/operators. It also settles industrial disputes through its ADR mechanism in an open hearing. \nNERC is divided into seven Divisions: Office of the Chairman/CEO, Engineering, Standards and Safety Division, Finance and Management Services Division, Government and Consumer Affairs Division, Legal Licensing and Enforcement Division, Market Competition and Rates Division, and the Renewable Energy/Research and Development Division.\n\nThe Electric Power Sector Reform Act of 2005 established NERC's authority to impose mandatory reliability standards on the transmission system and to impose penalties on companies that manipulate the electricity markets. Since Independence from the UK, Nigeria has built 12 power plants.\nNigeria produces as much electricity as North Dakota for 249 times more people, with blackouts 320 times per year per information from the World Bank.\nThe EPSRAct of 2005 gave NERC additional responsibilities as outlined in NERC's Wide Important Goals. As part of that responsibility, NERC:\n\nIn recent years, the NERC has been promoting the voluntary formation of Independent Electric Transmission Networks (IETNs) and Independent Electric Distribution Networks (IEDNs) to eliminate the potential for undue discrimination in access to the electric grid. However, since the generation capacity is low and the transmission not robust, NERC has developed regulations to push for the primary provision of supply, electric reliability and implementation of new regulations keeping in sight when the sector fully develops.\n\nNERC regulates over 40 licensees in Nigeria. It is also responsible for permitting the construction of network of transmission lines by the Transmission Company of Nigeria, the transmission monopoly in Nigeria formed as a successor company of the PHCN. NERC works closely with the Nigerian Ministry of Environment and other related bodies in reviewing the safety, security and environmental impacts of proposed power plants and transmission networks.\n\nIn November 2013, Nigeria auctioned off 6 power plants, one belonging to Tony Elumelu, chairman of Heirs Holdings. Nigeria will need 170,000MW per day for its 174 million people. Transmission lines are outdated, ineffective, and power is stolen from electric poles, since metering does not exist.\n\nA new set of commissioners were sworn into office on February 7, 2017, filling a vacuum that had existed from December 2015 when the term of the last Board led by Mr. Sam Amadi expired. Sam Amadi served as Chairman/CEO of the Commission from December 18, 2010 to December 2015. During the 14 month vacuum, Mr. Anthony Akah acted as Chairman/CEO of the Commission. Sam Amadi is the publisher of \"Privatization and public good: The rule of law challenge” and tried to reposition NERC after a two-year period of controversy, midwifed by the suspension and then removal of the first Board of Commissioners, led by Dr. Owan.\nAmadi and his team invited 'Distinguished Personalities' for Breakfast Series' lectures at the commission headquarters, including Nasir Ahmad el-Rufai and the former Corps Marshall of the Federal Road Safety Corps, Mr. Osita Chidoka.\n\nThe other Commissioners who served with Sam Amadi are:\n\nThe new set of Commissioners who were sworn in on February 7, 2017 are:\nMIT Professor, Akintunde Akinwande who was nominated to take charge as the Chairman/CEO however did not appear for the screening and was subsequently rejected . A new nominee, Prof. James Momoh, was forwarded to the Nigerian Senate for confirmation by President Muhammadu Buhari on April 20, 2017.\n\n\n"}
{"id": "2082919", "url": "https://en.wikipedia.org/wiki?curid=2082919", "title": "Nuclear Power 2010 Program", "text": "Nuclear Power 2010 Program\n\nThe \"Nuclear Power 2010 Program\" was launched in 2002 by President George W. Bush in order to restart orders for nuclear power reactors in the U.S. by providing subsidies for a handful of Generation III+ demonstration plants. The expectation was that these plants would come online by 2010, but it was not met.\n\nIn March 2017, the leading nuclear-plant maker, Westinghouse Electric Company, filed for bankruptcy due to losing over $9 billion in construction losses from working on two nuclear plants. This loss was partly caused by safety concerns due to the Fukushima disaster, Germany's Energiewende, the growth of solar and wind power, and low natural gas prices.\n\nThe \"Nuclear Power 2010 Program\" was unveiled by the U.S. Secretary of Energy Spencer Abraham on February 14, 2002 as one means towards addressing the expected need for new power plants. The program is a joint government/industry cost-shared effort to identify sites for new nuclear power plants, to develop and bring to market advanced nuclear plant technologies, evaluate the business case for building new nuclear power plants, and demonstrate untested regulatory processes leading to an industry decision in the next few years to seek Nuclear Regulatory Commission (NRC) approval to build and operate at least one new advanced nuclear power plant in the United States.\n\nThree consortia responded in 2004 to the U.S. Department of Energy's solicitation under the Nuclear Power 2010 initiative and were awarded matching funds.\n\n\nOn September 22, 2005, NuStart selected Port Gibson (the Grand Gulf site) and Scottsboro (the Bellefonte site) for new nuclear units. Port Gibson will host an ESBWR (a passively safe version of the BWR) and Scottsboro an AP1000 (a passively safe version of the PWR). Entergy announced it will prepare its own proposal for the River Bend Station in St. Francisville. Also, Constellation Energy of Baltimore had withdrawn its Lusby and Oswego sites from the NuStart finalist list after on September 15 announcing a new joint venture, UniStar Nuclear, with Areva to offer EPR (European Pressurized Reactors) in the U.S.A. Finally, in October, 2005, Progress Energy announced it was considering constructing a new nuclear plant and had begun evaluating potential sites in central Florida.\n\nSouth Carolina Electric & Gas announced on February 10, 2006 that it chose Westinghouse for a plant to be built at the V.C. Summer plant in Jenkinsville, South Carolina.\n\nNRG Energy announced in June 2006 that it would explore building two ABWRs at the South Texas Project. Four ABWRs were already operating in Japan at that time.\n\nThe original goal of bringing two new reactors online by 2010 was missed, and \"of more than two dozen projects that were considered, only two showed signs of progress and even this progress was uncertain\".\n\nAs of March 2017, the Plant Vogtle and V.C. Summer plants (a total of four reactors) in the southeastern U.S. that have been under construction since the late 2000s have been left to an unknown fate.\n\nThe Energy Policy Act of 2005, signed by President George W. Bush on August 8, 2005, has a number of articles related to nuclear power, and three specifically to the 2010 Program.\n\nFirst, the Price-Anderson Nuclear Industries Indemnity Act was extended to cover private and DOE plants and activities licensed through 2025.\n\nAlso, the government would cover cost overruns due to regulatory delays, up to $500 million each for the first two new nuclear reactors, and half of the overruns due to such delays (up to $250 million each) for the next four reactors. Delays in construction due to vastly increased regulation were a primary cause of the high cost of some earlier plants.\n\nFinally, \"A production tax credit of 1.8 cents per kilowatt-hour for the first 6,000 megawatt-hours from new nuclear power plants for the first eight years of their operation, subject to a $125 million annual limit. The production tax credit places nuclear energy on an equal footing with other sources of emission-free power, including wind and closed-loop biomass.\"\n\nThe Act also funds a Next Generation Nuclear Plant project at INEEL to produce both electricity and hydrogen. This plant will be a DOE project and does not fall under the 2010 Program.\n\nBetween 2007 and 2009, 13 companies applied to the Nuclear Regulatory Commission for construction and operating licenses to build 25 new nuclear power reactors in the United States. \nHowever, the case for widespread nuclear plant construction was eroded due to abundant natural gas supplies, slow electricity demand growth in a weak U.S. economy, lack of financing, and uncertainty following the Fukushima nuclear disaster in Japan after a tsunami. Many license applications for proposed new reactors were suspended or cancelled.\n\nOnly a few new reactors will enter service by 2020. These will not be cheaper than coal or natural gas, but they are an attractive investment for utilities because the government mandates that taxpayers pay for construction in advance. In 2013, four aging reactors were permanently closed due to the stringent requirements of the NRC and actions by local politicians.\n\n"}
{"id": "24989", "url": "https://en.wikipedia.org/wiki?curid=24989", "title": "Pendulum clock", "text": "Pendulum clock\n\nA pendulum clock is a clock that uses a pendulum, a swinging weight, as its timekeeping element. The advantage of a pendulum for timekeeping is that it approximates a harmonic oscillator: it swings back and forth in a precise time interval dependent on its length, and resists swinging at other rates. From its invention in 1656 by Christiaan Huygens until the 1930s, the pendulum clock was the world's most precise timekeeper, accounting for its widespread use. Throughout the 18th and 19th centuries pendulum clocks in homes, factories, offices and railroad stations served as primary time standards for scheduling daily life, work shifts, and public transportation, and their greater accuracy allowed the faster pace of life which was necessary for the Industrial Revolution. The home pendulum clock was replaced by cheaper synchronous electric clocks in the 1930s and '40s, and they are now kept mostly for their decorative and antique value.\n\nPendulum clocks must be stationary to operate; any motion or accelerations will affect the motion of the pendulum, causing inaccuracies, so other mechanisms must be used in portable timepieces.\n\nThe pendulum clock was invented in 1656 by Dutch scientist and inventor Christiaan Huygens, and patented the following year. Huygens contracted the construction of his clock designs to clockmaker Salomon Coster, who actually built the clock. Huygens was inspired by investigations of pendulums by Galileo Galilei beginning around 1602. Galileo discovered the key property that makes pendulums useful timekeepers: isochronism, which means that the period of swing of a pendulum is approximately the same for different sized swings. Galileo had the idea for a pendulum clock in 1637, which was partly constructed by his son in 1649, but neither lived to finish it. The introduction of the pendulum, the first harmonic oscillator used in timekeeping, increased the accuracy of clocks enormously, from about 15 minutes per day to 15 seconds per day leading to their rapid spread as existing 'verge and foliot' clocks were retrofitted with pendulums.\n\nThese early clocks, due to their verge escapements, had wide pendulum swings of 80–100°. In his 1673 analysis of pendulums, \"Horologium Oscillatorium\", Huygens showed that wide swings made the pendulum inaccurate, causing its period, and thus the rate of the clock, to vary with unavoidable variations in the driving force provided by the movement. Clockmakers' realization that only pendulums with small swings of a few degrees are isochronous motivated the invention of the anchor escapement around 1670, which reduced the pendulum's swing to 4–6°. The anchor became the standard escapement used in pendulum clocks. In addition to increased accuracy, the anchor's narrow pendulum swing allowed the clock's case to accommodate longer, slower pendulums, which needed less power and caused less wear on the movement. The seconds pendulum (also called the Royal pendulum), 0.994 m (39.1 in) long, in which each swing takes one second, became widely used in quality clocks. The long narrow clocks built around these pendulums, first made by William Clement around 1680, became known as grandfather clocks. The increased accuracy resulting from these developments caused the minute hand, previously rare, to be added to clock faces beginning around 1690.\n\nThe 18th and 19th century wave of horological innovation that followed the invention of the pendulum brought many improvements to pendulum clocks. The deadbeat escapement invented in 1675 by Richard Towneley and popularized by George Graham around 1715 in his precision \"regulator\" clocks gradually replaced the anchor escapement and is now used in most modern pendulum clocks. Observation that pendulum clocks slowed down in summer brought the realization that thermal expansion and contraction of the pendulum rod with changes in temperature was a source of error. This was solved by the invention of temperature-compensated pendulums; the mercury pendulum by George Graham in 1721 and the gridiron pendulum by John Harrison in 1726. With these improvements, by the mid-18th century precision pendulum clocks achieved accuracies of a few seconds per week.\n\nUntil the 19th century, clocks were handmade by individual craftsmen and were very expensive. The rich ornamentation of pendulum clocks of this period indicates their value as status symbols of the wealthy. The clockmakers of each country and region in Europe developed their own distinctive styles. By the 19th century, factory production of clock parts gradually made pendulum clocks affordable by middle-class families.\n\nDuring the Industrial Revolution, daily life was organized around the home pendulum clock. More accurate pendulum clocks, called \"regulators\", were installed in places of business and railroad stations and used to schedule work and set other clocks. The need for extremely accurate timekeeping in celestial navigation to determine longitude drove the development of the most accurate pendulum clocks, called \"astronomical regulators\". These precision instruments, installed in naval observatories and kept accurate within a second by observation of star transits overhead, were used to set marine chronometers on naval and commercial vessels. Beginning in the 19th century, astronomical regulators in naval observatories served as primary standards for national time distribution services that distributed time signals over telegraph wires. From 1909, US National Bureau of Standards (now NIST) based the US time standard on Riefler pendulum clocks, accurate to about 10 milliseconds per day. In 1929 it switched to the Shortt-Synchronome free pendulum clock before phasing in quartz standards in the 1930s.\n\nPendulum clocks remained the world standard for accurate timekeeping for 270 years, until the invention of the quartz clock in 1927, and were used as time standards through World War 2. The French Time Service used pendulum clocks as part of their ensemble of standard clocks until 1954. The home pendulum clock began to be replaced as domestic timekeeper during the 1930s and 1940s by the synchronous electric clock, which kept more accurate time because it was synchronized to the oscillation of the electric power grid. The most accurate experimental pendulum clock ever made may be the Littlemore Clock built by Edward T. Hall in the 1990s\n(donated in 2003 to the National Watch and Clock Museum, Columbia, Pennsylvania, USA).\n\nThe mechanism which runs a mechanical clock is called the movement. The movements of all mechanical pendulum clocks have these five parts:\n\nAdditional functions in clocks besides basic timekeeping are called complications. More elaborate pendulum clocks may include these complications:\n\n\nIn \"electromechanical pendulum clocks\" such as used in mechanical Master clocks the power source is replaced by an electrically powered solenoid that provides the impulses to the pendulum by magnetic force, and the escapement is replaced by a switch or photodetector that senses when the pendulum is in the right position to receive the impulse. These should not be confused with more recent quartz pendulum clocks in which an electronic quartz clock module swings a pendulum. These are not true pendulum clocks because the timekeeping is controlled by a quartz crystal in the module, and the swinging pendulum is merely a decorative simulation.\n\nThe pendulum swings with a period that varies with the square root of its effective length. For small swings the period \"T\", the time for one complete cycle (two swings), is\n\nwhere \"L\" is the length of the pendulum and \"g\" is the local acceleration of gravity. All pendulum clocks have a means of adjusting the rate. This is usually an adjustment nut under the pendulum bob which moves the bob up or down on its rod. Moving the bob up reduces the length of the pendulum, reducing the pendulum's period so the clock gains time. In some pendulum clocks, fine adjustment is done with an auxiliary adjustment, which may be a small weight that is moved up or down the pendulum rod. In some master clocks and tower clocks, adjustment is accomplished by a small tray mounted on the rod where small weights are placed or removed to change the effective length, so the rate can be adjusted without stopping the clock.\n\nThe period of a pendulum increases slightly with the width (amplitude) of its swing. The \"rate\" of error increases with amplitude, so when limited to small swings of a few degrees the pendulum is nearly \"isochronous\"; its period is independent of changes in amplitude. Therefore, the swing of the pendulum in clocks is limited to 2° to 4°.\n\nA major source of error in pendulum clocks is thermal expansion; the pendulum rod changes in length slightly with changes in temperature, causing changes in the rate of the clock. An increase in temperature causes the rod to expand, making the pendulum longer, so its period increases and the clock loses time. Many older quality clocks used wooden pendulum rods to reduce this error, as wood expands less than metal.\n\nThe first pendulum to correct for this error was the \"mercury pendulum\" invented by George Graham in 1721, which was used in precision regulator clocks into the 20th century. These had a bob consisting of a container of the liquid metal mercury. An increase in temperature would cause the pendulum rod to expand, but the mercury in the container would also expand and its level would rise slightly in the container, moving the center of gravity of the pendulum up toward the pivot. By using the correct amount of mercury, the centre of gravity of the pendulum remained at a constant height, and thus its period remained constant, despite changes in temperature.\n\nThe most widely used temperature-compensated pendulum was the gridiron pendulum invented by John Harrison around 1726. This consisted of a \"grid\" of parallel rods of high-thermal-expansion metal such as zinc or brass and low-thermal-expansion metal such as steel. If properly combined, the length change of the high-expansion rods compensated for the length change of the low-expansion rods, again achieving a constant period of the pendulum with temperature changes. \nThis type of pendulum became so associated with quality that decorative \"fake\" gridirons are often seen on pendulum clocks, that have no actual temperature compensation function.\n\nBeginning around 1900, some of the highest precision scientific clocks had pendulums made of ultra-low-expansion materials such as the nickel steel alloy Invar or fused silica, which required very little compensation for the effects of temperature.\n\nThe viscosity of the air through which the pendulum swings will vary with atmospheric pressure, humidity, and temperature. This drag also requires power that could otherwise be applied to extending the time between windings. Traditionally the pendulum bob is made with a narrow streamlined lens shape to reduce air drag, which is where most of the driving power goes in a quality clock. In the late 19th century and early 20th century, pendulums for precision regulator clocks in astronomical observatories were often operated in a chamber that had been pumped to a low pressure to reduce drag and make the pendulum's operation even more accurate by avoiding changes in atmospheric pressure. Fine adjustment of the rate of the clock could be made by slight changes to the internal pressure in the sealed housing.\n\nTo keep time accurately, pendulum clocks must be absolutely level. If they are not, the pendulum swings more to one side than the other, upsetting the symmetrical operation of the escapement. This condition can often be heard audibly in the ticking sound of the clock. The ticks or \"beats\" should be at precisely equally spaced intervals to give a sound of, \"tick...tock...tick...tock\"; if they are not, and have the sound \"tick-tock...tick-tock...\" the clock is \"out of beat\" and needs to be leveled. This problem can easily cause the clock to stop working, and is one of the most common reasons for service calls. A spirit level or watch timing machine can achieve a higher accuracy than relying on the sound of the beat; precision regulators often have a built in spirit level for the task. Older freestanding clocks often have feet with adjustable screws to level them, more recent ones have a leveling adjustment in the movement. Some modern pendulum clocks have 'auto-beat' or 'self-regulating beat adjustment' devices, and don't need this adjustment.\n\nSince the pendulum rate will increase with an increase in gravity, and local gravity varies with latitude and elevation on Earth, precision pendulum clocks must be readjusted to keep time after a move. For example, a pendulum clock moved from sea level to will lose 16 seconds per day. With the most accurate pendulum clocks, even moving the clock to the top of a tall building would cause it to lose measurable time due to lower gravity.\n\nAlso called torsion-spring pendulum, this is a wheel-like mass (most often four spheres on cross spokes) suspended from a vertical strip (ribbon) of spring steel, used as the regulating mechanism in torsion pendulum clocks. Rotation of the mass winds and unwinds the suspension spring, with the energy impulse applied to the top of the spring. With a period of 12—15 seconds, compared to the gravity swing pendulum's period of 0.5—2s, it is possible to make clocks that need to be wound only every 30 days, or even only once a year or more. This type is independent of the local force of gravity but is more affected by temperature changes than an uncompensated gravity-swing pendulum. \n\nA clock requiring only annual winding is sometimes called a \"400-Day clock\" or \"anniversary clock\", the latter sometimes given as a wedding memorialisation gift. German firms Schatz and Kieninger & Obergfell (known as \"Kundo\", from \"K und O\"), were the main manufacturers of this type of clock. The \"perpetual motion\" clock, called the Atmos because its mechanism was kept wound by changes in atmospheric temperature, also makes use of a torsion pendulum. In this case the oscillation cycle takes a full 60 seconds.\n\nThe escapement is a mechanical linkage that converts the force from the clock's wheel train into impulses that keep the pendulum swinging back and forth. It is the part that makes the \"ticking\" sound in a working pendulum clock. Most escapements consist of a wheel with pointed teeth called the \"escape wheel\" which is turned by the clock's wheel train, and surfaces the teeth push against, called \"pallets\". During most of the pendulum's swing the wheel is prevented from turning because a tooth is resting against one of the pallets; this is called the \"locked\" state. Each swing of the pendulum a pallet releases a tooth of the escape wheel. The wheel rotates forward a fixed amount until a tooth catches on the other pallet. These releases allow the clock's wheel train to advance a fixed amount with each swing, moving the hands forward at a constant rate, controlled by the pendulum. \n\nAlthough the escapement is necessary, its force disturbs the natural motion of the pendulum, and in precision pendulum clocks this was often the limiting factor on the accuracy of the clock. Different escapements have been used in pendulum clocks over the years to try to solve this problem. In the 18th and 19th century escapement design was at the forefront of timekeeping advances. The anchor escapement (see animation) was the standard escapement used until the 1800s when an improved version, the deadbeat escapement took over in precision clocks. It is used in almost all pendulum clocks today. The remontoire, a small spring mechanism rewound at intervals which serves to isolate the escapement from the varying force of the wheel train, was used in a few precision clocks. In tower clocks the wheel train must turn the large hands on the clock face on the outside of the building, and the weight of these hands, varying with snow and ice buildup, put a varying load on the wheel train. Gravity escapements were used in tower clocks. \n\nBy the end of the 19th century specialized escapements were used in the most accurate clocks, called \"astronomical regulators\", which were employed in naval observatories and for scientific research. The Riefler escapement, used in Clemens-Riefler regulator clocks was accurate to 10 milliseconds per day. Electromagnetic escapements, which used a switch or phototube to turn on a solenoid electromagnet to give the pendulum an impulse without requiring a mechanical linkage, were developed. The most accurate pendulum clock was the Shortt-Synchronome clock, a complicated electromechanical clock with two pendulums developed in 1923 by W.H. Shortt and Frank Hope-Jones, which was accurate to better than one second per year. A slave pendulum in a separate clock was linked by an electric circuit and electromagnets to a master pendulum in a vacuum tank. The slave pendulum performed the timekeeping functions, leaving the master pendulum to swing virtually undisturbed by outside influences. In the 1920s the Shortt-Synchronome briefly became the highest standard for timekeeping in observatories before quartz clocks superseded pendulum clocks as precision time standards.\n\nThe indicating system is almost always the traditional dial with moving hour and minute hands. Many clocks have a small third hand indicating seconds on a subsidiary dial. Pendulum clocks are usually designed to be set by opening the glass face cover and manually pushing the minute hand around the dial to the correct time. The minute hand is mounted on a slipping friction sleeve which allows it to be turned on its arbor. The hour hand is driven not from the wheel train but from the minute hand's shaft through a small set of gears, so rotating the minute hand manually also sets the hour hand.\n\nPendulum clocks were more than simply utilitarian timekeepers; they were status symbols that expressed the wealth and culture of their owners. They evolved in a number of traditional styles, specific to different countries and times as well as their intended use. Case styles somewhat reflect the furniture styles popular during the period. Experts can often pinpoint when an antique clock was made within a few decades by subtle differences in their cases and faces. These are some of the different styles of pendulum clocks:\n\n"}
{"id": "20368358", "url": "https://en.wikipedia.org/wiki?curid=20368358", "title": "Perfluorononanoic acid", "text": "Perfluorononanoic acid\n\nPerfluorononanoic acid, or PFNA, is a synthetic perfluorinated carboxylic acid and fluorosurfactant that is also an environmental contaminant found in people and wildlife along with PFOS and PFOA.\n\nIn acidic form it is a highly reactive strong acid. In its conjugate base form as a salt it is stable and commonly ion paired with ammonium. In the commercial product Surflon S-111 (CAS 72968-3-88) it is the primary compound present by weight. PFNA is used as surfactant for the production of the fluoropolymer polyvinylidene fluoride. It is produced mainly in Japan by the oxidation of a linear fluorotelomer olefin mixture containing F(CF)CH=CH. It can also be synthesized by the carboxylation of F(CF)I. PFNA can form from the biodegradation of 8:2 fluorotelomer alcohol. Additionally, it is considered a probable degradation product of many other compounds.\n\nPFNA is the largest perfluorinated carboxylic acid surfactant. Fluorocarbon derivatives with terminal carboxylates are only surfactants when they possess five to nine carbons. Fluorosurfactants reduce the surface tension of water down to half of what hydrocarbon surfactants can by concentrating at the liquid-air interface due to the lipophobicity of fluorocarbons. PFNA is very stable and is not known to degrade in the environment by oxidative processes because of the strength of the carbon–fluorine bond and the electronegativity of fluorine.\n\nLike the eight-carbon PFOA, the nine-carbon PFNA is a developmental toxicant and an immune system toxicant. However, longer chain perfluorinated carboxylic acids (PFCAs) are considered more bioaccumulative and toxic. PFNA is an agonist of the nuclear receptors PPARα and PPARγ. In the years between 1999–2000 and 2003–2004, the geometric mean of PFNA increased from 0.5 parts per billion to 1.0 parts per billion in the US population's blood serum. and has also been found in human follicular fluid PFNA has lso In a cross-sectional study of 2003–2004 US samples, a higher (13.9 milligram per deciliter) total cholesterol level was observed in when the highest quartile was compared to the lowest. Non-HDL cholesterol (or \"bad cholesterol\") levels were also higher in samples with more PFNA.\n\nIn bottlenose dolphins from Delaware Bay, PFNA was the perfluorinated carboxylic acid measured in the highest concentration in blood plasma; it was found in concentrations well over 100 parts per billion. PFNA has been detected in polar bears in concentrations over 400 parts per billion. PFNA was the perfluorinated chemical measured in the highest concentration in Russian Baikal seals. However, PFOS is the perfluorinated compound that dominates in most wildlife biomonitoring samples.\n\n\n"}
{"id": "56956044", "url": "https://en.wikipedia.org/wiki?curid=56956044", "title": "Petrolex", "text": "Petrolex\n\nPetrolex Oil & Gas Limited is a Nigerian company and part of Petrolex Group, an African integrated energy conglomerate. The company was founded in February 2007 by Segun Adebutu, a Nigerian entrepreneur. It provides services to the oil and gas industry. It is mainly involved in the refining, storage, distribution and retail of petroleum products in Nigeria and Africa. Petrolex is best known for starting in December 2017, the construction of a 3.6 billion dollar high capacity refinery and Sub-Saharan Africa’s largest tank farm as part of its Mega Oil City project in Ogun State, Nigeria. \n\nPetrolex CEO, Adebutu started an oil and fuel trading business around 2005 but showed interest in “mid-stream infrastructure” for $330 million. His experience in family business, laid the foundation for new ideas in his business career. Over the years, Adebutu was involved in bold projects including oil and gas, solid minerals, construction and maritime. This background inspired Adebutu to replicate similar practices with his new initiative Petrolex Oil & Gas Ltd.\n\nIn December 2017, Petrolex announced its plan to build a $3.6 billion refinery plant with an output capacity of 250,000 barrels a day. The company is currently working on the “front-end engineering design” and will complete construction in 2021. This initiative is part of a larger Government program to end petroleum products imports in two years.\n\nWith support from partners, Petrolex Group has invested over $330 million in the Ibefun tank farm with a 600,000 million litres monthly capacity. The farm was commissioned by the Vice President of Nigeria Yemi Osinbajo, as part of phase one of a 10-year expansion program. This phase would ease the Apapa and Ibafon tanker traffic gridlock, a source of anxiety for stakeholders.\n\nPetrolex provides services in refining, storage, distribution and retailing of petroleum products. The company intends to be listed on the Nigerian Stock Exchange in the coming decade. The company launched the planning, design and development of the Petrolex Mega Oil City in Ibefun, Ogun State in 2012. The complex spreads over 101 square kilometres, about 10 percent the size of Lagos State. It houses a residential estate for staff, an army barracks, 30 loading gantries for product disbursement, and a 4,000 truck capacity trailer park with accommodation for drivers. The Oil City project is the original idea of Segun Adebutu, CEO of Petrolex and son of the Nigerian entrepreneur Sir Kesington Adebutu. Its goal is to create the largest petrochemical industrial estate in Sub-Saharan Africa. Upon completion, this estate will include a large capacity refinery, a tank farm, a liquefied petroleum gas processing plant, a lubricant facility and raw material industries (ex. fertiliser plants). The company has also negotiated the addition of 12,000 acres to expand the Oil City.\n\nPetrolex downstream operations include the processing of petroleum products, the supply and distribution of gas oil, kerosene; and the retail marketing of specific oil products. Petrolex has built a storage-tank farm and other “mid-stream infrastructure” for $330 million. The company is connecting its infrastructure to the Nigeria System 2B pipeline at Mosimi to support supply and distribution of petroleum products around the country. This infrastructure includes a procurement of barges, tug boats and a daughter vessel.\n\n"}
{"id": "7346797", "url": "https://en.wikipedia.org/wiki?curid=7346797", "title": "Polyimide foam", "text": "Polyimide foam\n\nPolyimide foam is a foam originally designed for NASA by Inspec Foams Inc. under the brand name Solimide. Its primary purposes are as an insulator (such as for rocket fuels) and acoustic damper. NASA engineered the product to have relatively low outgassing (a problem in vacuum and aboard spacecraft), desirable thermal and acoustic performance, as well as uniformity during distribution and application. Typical uses of the foam include ducting, duct/piping insulation, structural components, and strengthening of hollow components while remaining lightweight. In addition to thermal and acoustic properties, polyimide foam is fire resistant, lightweight and non-toxic.\n\n"}
{"id": "2833859", "url": "https://en.wikipedia.org/wiki?curid=2833859", "title": "Post chaise", "text": "Post chaise\n\nA post-chaise is a fast carriage for traveling post built in the 18th and early 19th centuries. It usually had a closed body on four wheels, sat two to four persons, and was drawn by two or four horses. \n\nA postilion rode on the near-side (left, nearest the roadside) horse of a pair or of one of the pairs attached to the post-chaise leaving passengers a clear view of the road ahead.\nHired when long-distance travel at speed was very important a post chaise would be taken with its own postilion/s and horses. At the next posting station the postilions would most likely return to their base with their own horses but might continue the journey with fresh horses. \n\nPrivate posting was expensive and passengers — particularly if the only passenger was a woman — would be accompanied by one or two of their own footmen riding above and behind the body of the post chaise. The footmen would be responsible for making all travel arrangements.\n\nPrivate individuals did own their own post-chaises, some had their light chariots made with the coachman’s seat removable. Designed to withstand rapid long-distance travel the post chaise should have been utilitarian but private vehicles might be extravagantly decorated and finished.\n\nIn a 1967 article in \"The Carriage Journal\", published for the Carriage Association of America, Paul H Downing recounts that the word post is derived from the Latin \"postis\" which in turn derives from the word which means to place an upright timber (a post) as a convenient place to attach a public notice. Postal and postage follow from this. Medieval couriers were \"caballari postarus\" or riders of the posts. The riders mounted fresh horses at each post on their route and then rode on. Post came to be applied to the riders then to the mail they carried and eventually to the whole system. In England regular posts were set up in the 16th century.\n\nThe riders of the posts carried government messages and letters. The local postmasters delivered the letters as well as providing horses to the royal couriers. They also provided horses to other travellers.\nThe system of ‘’posting’’ was common in France. An artillery officer, John Trull, entered business in England in 1743 hiring out travelling carriages. At first these carriages had two wheels but they were soon replaced by four wheel carriages given the same name, Post-chaise.\n\nThe original French design was amended, a conventional pole was fitted, no driver was provided for — leaving a view through the front window for the passengers — and the horses were ridden by postilions or post-boys. The postilions went from post to post, stayed with their own horses and took them back home at the end of that stage.\n\nAt that time there was no perfected posting system in America. George Washington’s tour of the South in 1791 had a target (when there were no other commitments) of an average 35 miles a day whereas in England an average speed of 8 or 10 miles an hour might be achieved right round the clock.\n\nA true chaise is an open two wheeled carriage with a bench seat for two passengers drawn by one or two horses. Given two more wheels it would have been, if the name had been used then, a phaeton. A phaeton was for the owner to drive and generally drawn by one or two horses. A four-wheeled chaise would be drawn by at least four horses.\n\nAround the time Trull was introducing post-chaises to England Americans began to use the same name for what had been called a four-wheeled chaise.\n"}
{"id": "1838427", "url": "https://en.wikipedia.org/wiki?curid=1838427", "title": "Prague Metronome", "text": "Prague Metronome\n\nThe Metronome is a functioning metronome in Letná Park, overlooking the Vltava River and the city center of Prague. It was erected in 1991, on the plinth left vacant by the demolition in 1962 of an enormous monument to former Soviet leader Joseph Stalin. The metronome was designed by international artist Vratislav Novak.\n\nAlthough the metronome is functional, it is not always in operation. The site is now mostly a scenic vista and a meeting place for young people. \n\n"}
{"id": "41831237", "url": "https://en.wikipedia.org/wiki?curid=41831237", "title": "Railbit", "text": "Railbit\n\nRailbit is a common blend of bitumen and diluent used for rail transport. Railbit which contains approximately 17% diluents or less. compared to 30% in dilbit. Dilbit can be transported through pipelines but railbit cannot. To prevent solidifying in lower temperatures, both raw bitumen and railbit require insulated rail cars with steam-heated coils. Because it has a smaller percentage of diluents, railbit crude requires special capacity rail unload terminals capable of loading railbit and of handling larger unit trains. By the fall 2013 approximately 25% had that capacity. The U.S. State Department in their 2014 \"Final Supplemental Environmental Impact Statement (SEIS)\" regarding the proposed extension to the Keystone Pipeline, acknowledged that,\n\nAlthough dilbit has been transported by rail since at least 1998, however, the trend in Canada to transport crude oil by rail was slower than in the United States. According to Statistics Canada there were 60 percent more rail cars transporting crude oil in Canada from February 2012 to February 2013. . The destination of approximately 48 percent of the crude oil was exported to the Gulf Coast of the United States; 43 percent went to PADD I and the rest to PADD II and PADD V.\n\nSandy Fielden explained the economics behind railbit, dilbit and raw bitumen.\n\nIn their June 2013 report, the Canadian Association of Petroleum Producers (CAPP) categorized \"the various crude oil types that comprise western Canadian crude oil supply:\n\nVarious crude oil types \nOil Sands Heavy\n\n\"Oil Sands Heavy includes some volumes of upgraded heavy sour crude oil and bitumen blended with diluent or ugpraded crude oil.\" \n\nCondensate is a \"mixture of mainly pentanes and heavier hydrocarbons. It may be gaseous in its reservoir state but is liquid at the conditions under which its volumes is measured or estimated.\"\n\nThis was sent to Copyright examinations on December 19, 2016:\nDevon Canada Corporation, Devon's Canadian operations, which was established on January 1, 1982, are conducted by Devon Energy's wholly owned subsidiary\nwhich is headquartered in downtown Calgary, Alberta.\n"}
{"id": "2183606", "url": "https://en.wikipedia.org/wiki?curid=2183606", "title": "Sleep mode", "text": "Sleep mode\n\nSleep mode is a low power mode for electronic devices such as computers, televisions, and remote controlled devices. These modes save significantly on electrical consumption compared to leaving a device fully on and, upon resume, allow the user to avoid having to reissue instructions or to wait for a machine to reboot. Many devices signify this power mode with a pulsed or red colored LED power light.\n\nIn computers, entering a sleep state is roughly equivalent to \"pausing\" the state of the machine. When restored, the operation continues from the same point, having the same applications and files open.\n\nSleep mode has gone by various names, including \"Stand By\", \"Suspend\" and \"Suspend to RAM\" . Machine state is held in RAM and, when placed in sleep mode, the computer cuts power to unneeded subsystems and places the RAM into a minimum power state, just sufficient to retain its data. Because of the large power saving, most laptops automatically enter this mode when the computer is running on batteries and the lid is closed. If undesired, the behavior can be altered in the operating system settings.\n\nA computer must consume some energy while sleeping in order to power the RAM and to be able to respond to a wake-up event. A sleeping PC is a case of a machine on standby power, and this is covered by regulations in many countries, for example in the United States limiting such power under the One Watt Initiative, from 2010. In addition to a wake-up press of the power button, PCs can also respond to other wake cues, such as from keyboard, mouse, incoming telephone call on a modem, or local area network signal..\n\nHibernation, also called Suspend to Disk on Linux, saves all computer operational data on the fixed disk before turning the computer off completely. On switching the computer back on, the computer is restored to its state prior to hibernation, with all programs and files open, and unsaved data intact. In contrast with standby mode, hibernation mode saves the computer's state on the hard disk, which requires no power to maintain, whereas standby mode saves the computer's state in RAM, which requires a small amount of power to maintain.\n\nSleep mode and hibernation can be combined: the contents of RAM are first copied to non-volatile storage like for regular hibernation, but then, instead of powering down, the computer enters sleep mode. This approach combines the benefits of sleep mode and hibernation: The machine can resume instantaneously, but it can also be powered down completely (e.g. due to loss of power) without loss of data, because it is already effectively in a state of hibernation. This mode is called \"hybrid sleep\" in Microsoft Windows other than Windows XP.\n\nA hybrid mode is supported by some portable Apple Macintosh computers, compatible hardware running Microsoft Windows Vista or newer, as well as Linux distributions running kernel 3.6 or newer.\n\nACPI (Advanced Configuration and Power Interface) is the current standard for power management, superseding APM (Advanced Power Management) and providing the backbone for sleep and hibernation on modern computers. Sleep mode corresponds to ACPI mode S3. When a non-ACPI device is plugged in, Windows will sometimes disable stand-by functionality for the whole operating system. Without ACPI functionality, as seen on older hardware, sleep mode is usually restricted to turning off the monitor and spinning down the hard drive.\n\nWhen sleep mode was first introduced, not all PC hardware supported it correctly, which could cause problems with peripherals that didn't detect the transition. This is rarely a problem with newer hardware and newer versions of Windows.\n\nMicrosoft Windows 2000 and later support sleep at the operating system level (OS-controlled ACPI S4 sleep state) without special drivers from the hardware manufacturer. Windows Vista's Fast Sleep and Resume feature saves the contents of volatile memory to hard disk before entering sleep mode (aka Hybrid sleep). If power to memory is lost, it will use the hard disk to wake up. The user has the option of hibernating directly if they wish.\n\nIn older versions prior to Windows Vista, sleep mode was under-used in business environments as it was difficult to enable organization-wide without resorting to third-party PC power management software. As a result, these earlier versions of Windows were criticized for wasting energy.\n\nThere remains a market in third-party PC power management software for newer versions of Windows, offering features beyond those built into the operating system. Most products offer Active Directory integration and per-user/per-machine settings with the more advanced offering multiple power plans, scheduled power plans, anti-insomnia features and enterprise power usage reporting. Vendors include 1E NightWatchman, Data Synergy PowerMAN (Software) and Verdiem SURVEYOR.\n\nSleep on Macs running macOS consist of the traditional sleep, Safe Sleep, and Power Nap. In System Preferences, Safe Sleep is referred to as sleep. Since Safe Sleep also allowed state to be restored in an event of a power outage, unlike other operating systems, hibernate was never offered as an option.\n\nIn 2005, some Macs running Mac OS X v10.4 began to support Safe Sleep. The feature saves the contents of volatile memory to the system hard disk each time the Mac enters Sleep mode. The Mac can instantaneously wake from sleep mode if power to the RAM has not been lost. However, if the power supply was interrupted, such as when removing batteries without an AC power connection, the Mac would wake from Safe Sleep instead, restoring memory contents from the hard drive.\n\nSafe Sleep capability is found in Mac models starting with the October 2005 revision of the PowerBook G4 (Double-Layer SD). Mac OS X v10.4 or higher is also required. A hack enabled the feature as well on older Macs running Mac OS X v10.4.\n\nIn 2012, Apple introduced Power Nap with OS X Mountain Lion (10.8) and select Mac models. Power Nap allows the Mac to perform tasks silently, such as iCloud syncing and Spotlight indexing. Only low energy tasks are performed when on battery power, while higher energy tasks are performed with AC power.\n\nBecause of widespread use of this symbol, a campaign was launched to add a set of power characters to Unicode. In February 2015, the proposal was accepted by Unicode and the characters were included in Unicode 9.0. The characters are in the \"Miscellaneous Technical\" block, with code points 23FB-FE.\n\nThe symbol is ⏾ (&#x23FE;) - defined as \"Power Sleep Symbol\"\n\n"}
{"id": "20959928", "url": "https://en.wikipedia.org/wiki?curid=20959928", "title": "They Killed Sister Dorothy", "text": "They Killed Sister Dorothy\n\nThey Killed Sister Dorothy is a 2008 documentary film about American-born Brazilian nun Dorothy Stang, who was murdered on February 12, 2005, in Anapu, a city located in the Amazon Rainforest. Directed by Daniel Junge, the film is narrated by Martin Sheen in the English version and by Wagner Moura in the Portuguese version.\n\nThe film traces the reasons of the murder of Dayton, Ohio, native Dorothy Mae Stang, a 73-year-old nun of the Sisters of Notre Dame de Namur order who fought for the preservation of the Amazon Rainforest in the Brazilian state of Pará. It also follows the trial of those convicted for murdering Stang.\n\nThe film received the Audience Award and the Competition Award at the 2008 South by Southwest Festival, where it had its worldwide premiere. In Brazil, where the film title received a literal translation, it premiered at the 2007 Rio de Janeiro Film Festival. It was also screened at the 32nd São Paulo International Film Festival. It is planned to receive a nationwide release on February 13, 2009.\n\nThe film and its song \"Forever\", written and performed by Bebel Gilberto, daughter of Bossa Nova pioneer João Gilberto, were pre-selected for the Academy Awards for Best Documentary Feature and Best Original Song, respectively.\n\n"}
{"id": "2079074", "url": "https://en.wikipedia.org/wiki?curid=2079074", "title": "Whale oil", "text": "Whale oil\n\nWhale oil is oil obtained from the blubber of whales. Whale oil was sometimes known as train oil, which comes from the Dutch word \"traan\" (\"tear\" or \"drop\").\n\nSperm oil, a special kind of oil obtained from the head cavities of sperm whales, differs chemically from ordinary whale oil: it is composed mostly of liquid wax. Its properties and applications differ from those of regular whale oil, and is sold at a higher price when marketed.\n\nEarly industrial societies used whale oil widely in oil lamps and to make soap and margarine. With the commercial development of the petroleum industry and vegetable oils, the use of whale oils declined considerably from its peak in the 19th century into the 20th century. In the 21st century, with most countries having banned whaling, the sale and use of whale oil has practically ceased.\n\nWhale oil was obtained by boiling strips of blubber harvested from whales. The removal is known as \"flensing\" and the boiling process was called \"trying out\". The boiling was carried out on land in the case of whales caught close to shore or beached. On longer deep-sea whaling expeditions, the trying-out was done on the ship, so that the waste carcass could be thrown away to make room for the next catch.\n\nBaleen whales were generally the main source of whale oil. The oil of baleen whales is exclusively composed of triglycerides, whereas that of toothed whales contains wax esters. The bowhead whale and right whale were considered the ideal whaling targets. They are slow and docile, and they float when killed. They yield plenty of high-quality oil and whalebone, and as a result, they were hunted nearly to extinction.\n\nWhale oil has low viscosity (lower than olive oil), is clear, and varies in color from a bright honey yellow to a dark brown, according to the condition of the blubber from which it has been extracted and the refinement through which it went. It has a strong fishy odor. When hydrogenated, it turns solid and white and its taste and odor change.\n\nThe composition of whale oil varies with the species from which it was sourced and the method by which it was harvested and processed. Whale oil is mainly composed of triglycerides (molecules of fatty acids attached to a glycerol molecule). Oil sourced from toothed whales contains a substantial amount of wax esters (especially the oil of sperm whales). Most of the fatty acids are unsaturated. The most common fatty acids are oleic acid and its isomers (18:1 carbon chains).\n\nWhale oil is exceptionally stable.\n\nThe use of whale oil had a steady decline starting in the late 19th century due to the development of superior alternatives, and later, the passing of environmental laws. In 1986, the International Whaling Commission declared a moratorium on commercial whaling, which has all but eliminated the use of whale oil today. The Inuit of North America are granted special whaling rights (justified as being integral to their culture), and they still use whale oil as a food and as lamp oil. See Aboriginal whaling.\n\nWhale oil was used as a cheap illuminant, though it gave off a strong odor when burnt and was not very popular. It was replaced in the late 19th century by cheaper, more efficient, and longer-lasting kerosene. Burning fluid known as camphine was the dominant replacement for whale oil until the arrival of kerosene.\n\nIn the US, whale oil was used in cars as a constituent of automatic transmission fluid until it was banned by the 1973 Endangered Species Act.\n\nIn the UK, whale oil was used in toolmaking machinery as a high-quality lubricant \n\nAfter the invention of hydrogenation in the early 20th century, whale oil was used to make margarine, a practice that has since been discontinued. Whale oil in margarine has been replaced by vegetable oil.\n\nWhale oil was used to make soap. Until the invention of hydrogenation, it was used only in industrial-grade cleansers, because its foul smell and tendency to discolor made it unsuitable for cosmetic soap.\n\nWhale oil was widely used in the First World War as a preventive measure against trench foot. A British infantry battalion on the Western Front could be expected to use 10 gallons of whale oil a day. The oil was rubbed directly onto bare feet in order to protect them from the effects of immersion.\n\nThe pursuit and use of whale oil, along with many other aspects of whaling, are discussed in Herman Melville's \"Moby-Dick\". In the novel, the preciousness of the substance to contemporary American society is emphasized when the fictional narrator notes that whale oil is \"as rare as the milk of queens.\" John R. Jewitt, an Englishman who wrote a memoir about his years as a captive of the Nootka people on the Pacific Northwest Coast in 1802–1805, describes how whale oil was used as a condiment with every dish, even strawberries.\n\nFriedrich Ratzel in \"The History of Mankind\" (1896), when discussing food materials in Oceania, quoted Captain James Cook's comment in relation to \"the Maoris\" saying \"No Greenlander was ever so sharp set upon train-oil as our friends here, they greedily swallowed the stinking droppings when we were boiling down the fat of dog-fish.\"\n\nDunwall, a port city in the video game \"Dishonored\" (2012) and \"Dishonored 2\" (2016), uses supernaturally-enhanced whale oil as a basis for its industrial revolution.\n\n\"In the Heart of the Sea\" (2015) is a film based on the book \"of the same name\" by Nathaniel Philbrick.\n\n \n"}
{"id": "275860", "url": "https://en.wikipedia.org/wiki?curid=275860", "title": "Wood carving", "text": "Wood carving\n\nWood carving is a form of woodworking by means of a cutting tool (knife) in one hand or a chisel by two hands or with one hand on a chisel and one hand on a mallet, resulting in a wooden figure or figurine, or in the sculptural ornamentation of a wooden object. The phrase may also refer to the finished product, from individual sculptures to hand-worked mouldings composing part of a tracery.\n\nThe making of sculpture in wood has been extremely widely practiced, but survives much less well than the other main materials such as stone and bronze, as it is vulnerable to decay, insect damage, and fire. It therefore forms an important hidden element in the art history of many cultures. Outdoor wood sculptures do not last long in most parts of the world, so it is still unknown how the totem pole tradition developed. Many of the most important sculptures of China and Japan in particular are in wood, and so are the great majority of African sculpture and that of Oceania and other regions. Wood is light and can take very fine detail so it is highly suitable for masks and other sculpture intended to be worn or carried. It is also much easier to work on than stone.\n\nSome of the finest extant examples of early European wood carving are from the Middle Ages in Germany, Russia, Italy and France, where the typical themes of that era were Christian iconography. In England, many complete examples remain from the 16th and 17th century, where oak was the preferred medium.\n\n\nPattern,\nBlocking,\nDetailing,\nSurfacing,\nand Smoothening\n\n\nA special screw for fixing work to the workbench, and a mallet, complete the carvers kit, though other tools, both specialized and adapted, are often used, such as a router for bringing grounds to a uniform level, bent gouges and bent chisels for cutting hollows too deep for the ordinary tool.\n\nThe nature of the wood being carved limits the scope of the carver in that wood is not equally strong in all directions: it is an anisotropic material. The direction in which wood is strongest is called \"grain\" (grain may be straight, interlocked, wavy or fiddleback, \"etc\".). It is smart to arrange the more delicate parts of a design along the grain instead of across it. Often however a \"line of best fit\" is instead employed, since a design may have multiple weak points in different directions, or orientation of these along the grain would necessitate carving detail on end grain, (which is considerably more difficult). Carving blanks are also sometimes assembled, as with carousel horses, out of many smaller boards, and in this way one can orient different areas of a carving in the most logical way, both for the carving process and for durability. Less commonly, this same principle is used in solid pieces of wood, where the fork of two branches is utilized for its divergent grain, or a branch off of a larger log is carved into a beak (this was the technique employed for traditional Welsh shepherd's crooks, and some Native American adze handles). The failure to appreciate these primary rules may constantly be seen in damaged work, when it will be noticed that, whereas tendrils, tips of birds beaks, etc., arranged across the grain have been broken away, similar details designed more in harmony with the growth of the wood and not too deeply undercut remain intact.\n\nProbably the two most common woods used for carving in North America are basswood (aka tilia or lime) and tupelo; both are hardwoods that are relatively easy to work with. Chestnut, butternut, oak, American walnut, mahogany and teak are also very good woods; while for fine work Italian walnut, sycamore maple, apple, pear, box or plum, are usually chosen. Decoration that is to be painted and of not too delicate a nature is often carved in pine, which is relatively soft and inexpensive.\n\nA wood carver begins a new carving by selecting a chunk of wood the approximate size and shape of the figure he or she wishes to create or, if the carving is to be large, several pieces of wood may be laminated together to create the required size. The type of wood is important. Hardwoods are more difficult to shape but have greater luster and longevity. Softer woods may be easier to carve but are more prone to damage. Any wood can be carved but they all have different qualities and characteristics. The choice will depend on the requirements of carving being done: for example a detailed figure would need a wood with a fine grain and very little figure as strong figure can interfere with 'reading' fine detail.\n\nOnce the sculptor has selected their wood, he or she begins a general shaping process using gouges of various sizes. The gouge is a curved blade that can remove large portions of wood smoothly. For harder woods, the sculptor may use gouges sharpened with stronger bevels, about 35 degrees, and a mallet similar to a stone carver's. The terms \"gouge\" and \"chisel\" are open to confusion. Correctly, a gouge is a tool with a curved cross section and a chisel is a tool with a flat cross section. However, professional carvers tend to refer to them all as 'chisels'. Smaller sculptures may require the wood carver to use a knife, and larger pieces might require the use of a saw. No matter what wood is selected or tool used, the wood sculptor must always carve either across or with the grain of the wood, never against the grain.\n\nOnce the general shape is made, the carver may use a variety of tools for creating details. For example, a “veiner” or “fluter” can be used to make deep gouges into the surface, or a “v-tool” for making fine lines or decorative cuts. Once the finer details have been added, the wood carver finishes the surface. The method chosen depends on the required quality of surface finish. The texture left by shallow gouges gives 'life' to the carving's surface and many carvers prefer this 'tooled' finish. If a completely smooth surface is required general smoothing can be done with tools such as “rasps,” which are flat-bladed tools with a surface of pointed teeth. “Rifflers” are similar to rasps, but smaller, usually double ended, and of various shapes for working in folds or crevasses. The finer polishing is done with abrasive paper. Large grained paper with a rougher surface is used first, with the sculptor then using finer grained paper that can make the surface of the sculpture slick to the touch.\n\nAfter the carving and finishing is completed, the artist may seal & color the wood with a variety of natural oils, such as walnut or linseed oil which protects the wood from dirt and moisture. Oil also imparts a sheen to the wood which, by reflecting light, helps the observer 'read' the form. Carvers seldom use gloss varnish as it creates too shiny a surface, which reflects so much light it can confuse the form; carvers refer to this as 'the toffee apple effect'. Objects made of wood are frequently finished with a layer of wax, which protects the wood and gives a soft lustrous sheen. A wax finish (e.g. shoe polish) is comparatively fragile though and only suitable for indoor carvings.\n\nThe making of decoys and fish carving are two of the artistic traditions that use wood carvings.\n\n\n\n"}
{"id": "24250561", "url": "https://en.wikipedia.org/wiki?curid=24250561", "title": "‘Aziziya", "text": "‘Aziziya\n\n‘Aziziya ( ; ' / ' / \"\"), sometimes spelled \"El Azizia\", is a small town and it was the capital of the Jafara district in northwestern Libya, southwest of the capital Tripoli. From 1918-22 it was the capital of the Tripolitanian Republic, the first formal republic in the Arab world. Before 2001 it was in the ‘Aziziya District and its capital. ‘Aziziya is a major trade centre of the Sahel Jeffare plateau, being on a trade route from the coast to the Nafusa Mountains and the Fezzan region to the south . As of 2006, the town's population has been estimated at over 23,399.\n\nOn 13 September 1922, a high temperature of was recorded in Al-ʿAzīzīyah. This was long considered the highest temperature ever measured on Earth.\n\nHowever, that reading was controversial:\n\nThe coldest temperature recorded in Al-ʿAzīzīyah was in February. Al-ʿAzīzīyah has a mean high temperature of , a mean temperature of , and a mean low temperature of , making it one of the warmest places in the world. The coldest month, with a mean temperature of , is January with a mean high temperature of and a mean low temperature of , and the warmest month is July, with a mean temperature of , a mean high temperature of , and a mean low temperature of .\n"}
