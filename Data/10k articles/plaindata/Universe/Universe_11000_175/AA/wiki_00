{"id": "17643449", "url": "https://en.wikipedia.org/wiki?curid=17643449", "title": "5.9 kiloyear event", "text": "5.9 kiloyear event\n\nThe 5.9-kiloyear event was one of the most intense aridification events during the Holocene. It occurred around 3900 BC (5900 years Before Present), ending the Neolithic Subpluvial. \n\nIt is associated with the last round of the Sahara pump theory, and probably initiated the most recent desiccation of the Sahara, as well as a five century period of colder climate in more northerly latitudes. It triggered human migration to the Nile, which eventually led to the emergence of the first complex, highly organized, state-level societies in the 4th millennium BC. It may have contributed to the decline of Old Europe and the first Indo-European migrations into the Balkans from the Pontic–Caspian steppe.\n\nA model by Claussen \"et al.\" (1999) suggested rapid desertification, associated with vegetation-atmosphere interactions following a cooling event, Bond event 4. Bond \"et al.\" (1997) identified a North Atlantic cooling episode 5,900 years ago from ice-rafted debris as well as other such events, now called Bond events, which indicate the existence of a quasiperiodic cycle of Atlantic cooling events approximately every 1500 years ± 500 years. For some reason, all the earlier arid events (including the 8.2-kiloyear event) were followed by recovery, as is attested by the wealth of evidence of humid conditions in the Sahara between 10,000 and 6,000 BP. However, it appears that the 5.9-kiloyear event was followed by a partial recovery at best, with accelerated desiccation in the millennium that followed.\n\nFor example, Cremaschi (1998) describes evidence of rapid aridification in Tadrart Acacus of southwestern Libya, in the form of increased aeolian erosion, sand incursions and the collapse of the roofs of rock shelters. The 5.9-kiloyear event was also recorded as a cold event in the Erhai Lake (China) sediments.\n\nIn the eastern Arabian Peninsula, the 5.9-kiloyear event may have contributed to an increase in relatively greater social complexity and have corresponded to an end of the local Ubaid period and the emergence of the first state societies at the lower end of the Tigris and Euphrates Rivers in southern Mesopotamia.\n\nIn central North Africa it triggered human migration to the Nile, which eventually led to the emergence of the second complex, highly organized, state-level societies in the 4th millennium BC.\n\nBy causing a period of cooling in Europe, it may have contributed to the decline of Old Europe and the first Indo-European migrations into the Balkans from the Pontic–Caspian steppe. Around 4200–4100 BCE a climate change occurred, manifesting in colder winters in Europe. Between 4200–3900 BCE many tell settlements in the lower Danube Valley were burned and abandoned, while the Cucuteni–Trypillia culture showed an increase in fortifications, meanwhile moving eastwards towards the Dniepr. Steppe herders, archaic Proto-Indo-European speakers, spread into the lower Danube valley about 4200–4000 BCE, either causing or taking advantage of the collapse of Old Europe.\n\n"}
{"id": "40982867", "url": "https://en.wikipedia.org/wiki?curid=40982867", "title": "Avraham Hermoni", "text": "Avraham Hermoni\n\nAvraham Hermoni (May 10, 1926- June 24, 2006) was an Israeli chemist, government official, scientific counselor in the Israeli embassy in Washington, and a senior technical director at RAFAEL, Israel’s national center for weapons development.\n\nHermoni’s involvement in the Israeli nuclear program was instrumental to its ultimate success. Between 1959 and 1969 he served as technical director (equivalent to vice president) at RAFAEL. His main duties included overseeing and planning RAFAEL’s work on Israel’s nuclear weapons.\n\n\n"}
{"id": "56778", "url": "https://en.wikipedia.org/wiki?curid=56778", "title": "Azeotrope", "text": "Azeotrope\n\nAn azeotrope (UK , US ) or a constant boiling point mixture is a mixture of two or more liquids whose proportions cannot be altered or changed by simple distillation. This happens because when an azeotrope is boiled, the vapour has the same proportions of constituents as the unboiled mixture. Because their composition is unchanged by distillation, azeotropes are also called (especially in older texts) constant boiling point mixtures.\n\nMany azeotropic mixtures of pairs of compounds are known, and many azeotropes of three or more compounds are also known. In such a case it is not possible to separate the components by fractional distillation. There are two types of azeotropes: minimum boiling azeotrope and maximum boiling azeotrope. A solution that shows greater positive deviation from Raoult's law forms a minimum boiling azeotrope at a specific composition. For example, an ethanol-water mixture (obtained by fermentation of sugars) on fractional distillation yields a solution containing approximately 95% by volume of ethanol. Once this composition has been achieved, the liquid and vapour have the same composition, and no further separation occurs. \nA solution that shows large negative deviation from Raoult's law forms a maximum boiling azeotrope at a specific composition. Nitric acid and water is an example of this class of azeotrope. This azeotrope has an approximate composition of 68% nitric acid and 32% water by mass, with a boiling point of .\n\nThe term \"azeotrope\" is derived from the Greek words ζέειν (boil) and τρόπος (turning) with the prefix α- (no) to give the overall meaning, \"no change on boiling\". The term was coined in 1911 by English chemist John Wade (1864–1912) and Richard William Merriman.\n\nEach azeotrope has a characteristic boiling point. The boiling point of an azeotrope is either less than the boiling point temperatures of any of its constituents (a positive azeotrope), or greater than the boiling point of any of its constituents (a negative azeotrope).\n\nA well-known example of a positive azeotrope is 95.63% ethanol and 4.37% water (by mass) boils at 78.2 °C.\nEthanol boils at 78.4 °C, water boils at 100 °C, but the azeotrope boils at 78.2 °C, which is lower than either of its constituents. Indeed, 78.2 °C is the minimum temperature at which any ethanol/water solution can boil at atmospheric pressure. In general, a positive azeotrope boils at a lower temperature than any other ratio of its constituents. Positive azeotropes are also called \"minimum boiling mixtures\" or \"pressure maximum\" azeotropes.\n\nIn general, a negative azeotrope boils at a higher temperature than any other ratio of its constituents. Negative azeotropes are also called \"maximum boiling mixtures\" or \"pressure minimum\" azeotropes. An example of a negative azeotrope is hydrochloric acid at a concentration of 20.2% and 79.8% water (by mass). Hydrogen chloride boils at −84 °C and water at 100 °C, but the azeotrope boils at 110 °C, which is higher than either of its constituents. The maximum temperature at which any hydrochloric acid solution can boil is 110 °C. Other examples:\n\nIf the constituents of a mixture are completely miscible in all proportions with each other, the type of azeotrope is called a \"homogeneous azeotrope\". For example, any amount of ethanol can be mixed with any amount of water to form a homogeneous solution.\nIf the constituents are not completely miscible, an azeotrope can be found inside the miscibility gap. This type of azeotrope is called \"heterogeneous azeotrope\" or heteroazeotrope. A heteroazeotropic distillation will have two liquid phases. For example, acetone / methanol / chloroform form an intermediate boiling (saddle) azeotrope.\n\nFor example, if equal volumes of chloroform (water solubility 0.8 g/100 ml at 20 °C) and water are shaken together and then left to stand, the liquid will separate into two layers. Analysis of the layers shows that the top layer is mostly water with a small amount of chloroform dissolved in it, and the bottom layer is mostly chloroform with a small amount of water dissolved in it. If the two layers are heated together, the system of layers will boil at 53.3 °C, which is lower than either the boiling point of chloroform (61.2 °C) or the boiling point of water (100 °C). The vapor will consist of 97.0% chloroform and 3.0% water regardless of how much of each liquid layer is present provided both layers are indeed present. If the vapor is re-condensed, the layers will reform in the condensate, and will do so in a fixed ratio, which in this case is 4.4% of the volume in the top layer and 95.6% in the bottom layer. Such a system of solvents is known as a heteroazeotrope. Heteroazeotropes are always minimum boiling mixtures.\n\nThe diagram illustrates how the various phases of a heteroazeotrope are related.\n\nAzeotropes consisting of two constituents are called \"binary\" azeotropes such as diethyl ether (33%) / halothane (66%) a mixture once commonly used in anaesthesia. For example, benzene and hexafluorobenzene form a double binary azeotrope.\nAzeotropes consisting of three constituents are called \"ternary\" azeotropes, e.g. acetone / methanol / chloroform. Azeotropes of more than three constituents are also known.\n\nCombinations of solvents that do not form an azeotrope when mixed in any proportion are said to be zeotropic. Azeotropes are useful in separating zeotropic mixtures. An example is acetic acid and water, which do not form an azeotrope. Despite this it is very difficult to separate pure acetic acid (boiling point: 118.1 °C) from a solution of acetic acid and water by distillation alone. As progressive distillations produce solutions with less and less water, each further distillation becomes less effective at removing the remaining water. Distilling the solution to dry acetic acid is therefore economically impractical. But ethyl acetate forms an azeotrope with water that boils at 70.4 °C. By adding ethyl acetate as an entrainer, it is possible to distill away the azeotrope and leave nearly pure acetic acid as the residue.\n\nThe condition relates activity coefficients in liquid phase to total pressure and the vapour pressures of pure components.\nAzeotropes can form only when a mixture deviates from Raoult's law, the equality of compositions in liquid phase and vapor phases, in vapour-liquid equilibrium and Dalton's law the equality of pressures for total pressure being equal to the sum of the partial pressures in real mixtures.\n\nIn other words: Raoult's law predicts the vapor pressures of ideal mixtures as a function of composition ratio. More simply: per Raoult's law molecules of the constituents stick to each other to the same degree as they do to themselves. For example, if the constituents are X and Y, then X sticks to Y with roughly equal energy as X does with X and Y does with Y. A so-called \"positive deviation\" from Raoult's law results when the constituents have a disaffinity for each other – that is X sticks to X and Y to Y better than X sticks to Y. Because this results in the mixture having less total affinity of the molecules than the pure constituents, they more readily escape from the stuck-together phase, which is to say the liquid phase, and into the vapor phase. When X sticks to Y more aggressively than X does to X and Y does to Y, the result is a \"negative deviation\" from Raoult's law. In this case because the molecules in the mixture are sticking together more than in the pure constituents, they are more reluctant to escape the stuck-together liquid phase.\n\nWhen the deviation is great enough to cause a maximum or minimum in the vapor pressure versus composition function, it is a mathematical consequence that at that point, the vapor will have the same composition as the liquid, resulting in an azeotrope.\n\nThe adjacent diagram illustrates total vapor pressure of three hypothetical mixtures of constituents, X, and Y. The temperature throughout the plot is assumed to be constant.\n\nThe center trace is a straight line, which is what Raoult's law predicts for an ideal mixture. In general solely mixtures of chemically similar solvents, such as \"n\"-hexane with \"n\"-heptane, form nearly ideal mixtures that come close to obeying Raoult's law. \nThe top trace illustrates a nonideal mixture that has a positive deviation from Raoult's law, where the total combined vapor pressure of constituents, X and Y, is greater than what is predicted by Raoult's law. The top trace deviates sufficiently that there is a point on the curve where its tangent is horizontal. Whenever a mixture has a positive deviation and has a point at which the tangent is horizontal, the composition at that point is a positive azeotrope. At that point the total vapor pressure is at a maximum. Likewise the bottom trace illustrates a nonideal mixture that has a negative deviation from Raoult's law, and at the composition where tangent to the trace is horizontal there is a negative azeotrope. This is also the point where total vapor pressure is minimum.\n\nThe boiling and recondensation of a mixture of two solvents are changes of chemical state; as such, they are best illustrated with a phase diagram. If the pressure is held constant, the two variable parameters are the temperature and the composition.\n\nThe phase diagram on the right shows a \"positive\" azeotrope of hypothetical constituents, X and Y. The bottom trace illustrates the boiling temperature of various compositions. Below the bottom trace, only the liquid phase is in equilibrium. The top trace illustrates the vapor composition above the liquid at a given temperature. Above the top trace, only the vapor is in equilibrium. Between the two traces, liquid and vapor phases exist simultaneously in equilibrium: for example, heating a 25% X : 75% Y mixture to temperature AB would generate vapor of composition B over liquid of composition A. The azeotrope is the point on the diagram where the two curves touch. The horizontal and vertical steps show the path of repeated distillations. Point A is the boiling point of a nonazeotropic mixture. The vapor that separates at that temperature has composition B. The shape of the curves requires that the vapor at B be richer in constituent X than the liquid at point A. The vapor is physically separated from the VLE (vapor-liquid equilibrium) system and is cooled to point C, where it condenses. The resulting liquid (point C) is now richer in X than it was at point A. If the collected liquid is boiled again, it progresses to point D, and so on. The stepwise progression shows how repeated distillation can never produce a distillate that is richer in constituent X than the azeotrope. Note that starting to the right of the azeotrope point results in the same stepwise process closing in on the azeotrope point from the other direction.\n\nThe phase diagram on the right shows a \"negative\" azeotrope of ideal constituents, X and Y. Again the bottom trace illustrates the boiling temperature at various compositions, and again, below the bottom trace the mixture must be entirely liquid phase. The top trace again illustrates the condensation temperature of various compositions, and again, above the top trace the mixture must be entirely vapor phase. The point, A, shown here is a boiling point with a composition chosen very near to the azeotrope. The vapor is collected at the same temperature at point B. That vapor is cooled, condensed, and collected at point C. Because this example is a negative azeotrope rather than a positive one, the distillate is \"farther\" from the azeotrope than the original liquid mixture at point A was. So the distillate is poorer in constituent X and richer in constituent Y than the original mixture. Because this process has removed a greater fraction of Y from the liquid than it had originally, the residue must be poorer in Y and richer in X after distillation than before.\n\nIf the point, A had been chosen to the right of the azeotrope rather than to the left, the distillate at point C would be farther to the right than A, which is to say that the distillate would be richer in X and poorer in Y than the original mixture. So in this case too, the distillate moves away from the azeotrope and the residue moves toward it. This is characteristic of negative azeotropes. No amount of distillation, however, can make either the distillate or the residue arrive on the opposite side of the azeotrope from the original mixture. This is characteristic of \"all\" azeotropes.\n\nThe traces in the phase diagrams separate whenever the composition of the vapor differs from the composition of the liquid at the same temperature. Suppose the total composition were 50/50%. You could make this composition using 50% of 50/50% vapor and 50% of 50/50% liquid, but you could also make it from 83.33% of 45/55% vapor and 16.67% of 75%/25% liquid, as well as from many other combinations. The separation of the two traces represents the range of combinations of liquid and vapor that can make each total composition.\n\nFor both the top and bottom traces, the temperature point of the azeotrope is the constant temperature chosen for the graph. If the ambient pressure is controlled to be equal to the total vapor pressure at the azeotropic mixture, then the mixture will boil at this fixed temperature.\n\nVapor pressure of both pure liquids as well as mixtures is a sensitive function of temperature. As a rule, vapor pressure of a liquid increases nearly exponentially as a function of temperature. If the graph were replotted for a different fixed temperature, then the total vapor pressure at the azeotropic composition will certainly change, but it is also possible that the composition at which the azeotrope occurs will change. This implies that the composition of an azeotrope is affected by the pressure chosen at which to boil the mixture. Ordinarily distillation is done at atmospheric pressure, but with proper equipment it is possible to carry out distillation at a wide variety of pressures, both above and below atmospheric pressure.\n\nIf two solvents can form a negative azeotrope, then distillation of any mixture of those constituents will result in the residue being away from the composition at the azeotrope than the original mixture. \nFor example, if a hydrochloric acid solution contains less than 20.2% hydrogen chloride, boiling the mixture will leave behind a solution that is richer in hydrogen chloride than the original. If the solution initially contains more than 20.2% hydrogen chloride, then boiling will leave behind a solution that is poorer in hydrogen chloride than the original. Boiling of any hydrochloric acid solution long enough will cause the solution left behind to approach the azeotropic ratio.. On the other hand, if two solvents can form a positive azeotrope, then distillation of any mixture of those constituents will result in the residue closer to the composition at the azeotrope than the original mixture. For example, if a 50/50 mixture of ethanol and water is distilled once, the distillate will be 80% ethanol and 20% water, which is closer to the azeotropic mixture than the original, which means the solution left behind will be poorer in ethanol. Distilling the 80/20% mixture produces a distillate that is 87% ethanol and 13% water. Further repeated distillations will produce mixtures that are progressively closer to the azeotropic ratio of 95.5/4.5%. No numbers of distillations will ever result in a distillate that exceeds the azeotropic ratio. Likewise, when distilling a mixture of ethanol and water that is richer in ethanol than the azeotrope, the distillate (contrary to intuition) will be poorer in ethanol than the original but still richer than the azeotrope.\n\nDistillation is one of the primary tools that chemists and chemical engineers use to separate mixtures into their constituents. Because distillation cannot separate the constituents of an azeotrope, the separation of azeotropic mixtures (also called \"azeotrope breaking\") is a topic of considerable interest. Indeed, this difficulty led some early investigators to believe that azeotropes were actually compounds of their constituents. But there are two reasons for believing that this is not the case. One is that the molar ratio of the constituents of an azeotrope is not generally the ratio of small integers. For example, the azeotrope formed by water and acetonitrile contains 2.253 moles ( or 9/4 with a relative error of just 2% ) of acetonitrile for each mole of water. A more compelling reason for believing that azeotropes are not compounds is, as discussed in the last section, that the composition of an azeotrope can be affected by pressure. Contrast that with a true compound, carbon dioxide for example, which is two moles of oxygen for each mole of carbon no matter what pressure the gas is observed at. That azeotropic composition can be affected by pressure suggests a means by which such a mixture can be separated.\n\nA hypothetical azeotrope of constituents X and Y is shown in the adjacent diagram. Two sets of curves on a phase diagram one at an arbitrarily chosen low pressure and another at an arbitrarily chosen, but higher, pressure. The composition of the azeotrope is substantially different between the high- and low-pressure plots – higher in X for the high-pressure system. The goal is to separate X in as high a concentration as possible starting from point A. At the low pressure, it is possible by progressive distillation to reach a distillate at the point, B, which is on the same side of the azeotrope as A. Note that successive distillation steps near the azeotropic composition exhibit very little difference in boiling temperature. If this distillate is now exposed to the high pressure, it boils at point C. From C, by progressive distillation it is possible to reach a distillate at the point D, which is on the same side of the high-pressure azeotrope as C. If that distillate is then exposed again to the low pressure, it boils at point E, which is on the \"opposite\" side of the low-pressure azeotrope to A. So, by means of the pressure swing, it is possible to cross over the low-pressure azeotrope.\n\nWhen the solution is boiled at point E, the distillate is poorer in X than the residue at point E. This means that the residue is richer in X than the distillate at point E. Indeed, progressive distillation can produce a residue as rich in X as is required.\n\nIn summary:\n1. Low-pressure rectification (A to B)\n2. High-pressure rectification (C to D)\n3. Low-pressure stripping (E to target purity)\n\nNote that both azeotropes above are of the \"positive\", or \"minimum boiling\" type; care must be taken to ensure that the correct component of the separation step is retained, i.e. the binary phase-envelope diagram (boiling-point curve) must be correctly read.\n\nA mixture of 5% water with 95% tetrahydrofuran is an example of an azeotrope that can be economically separated using a pressure swing – a swing in this case between 1 atm and 8 atm. By contrast the composition of the water to ethanol azeotrope discussed earlier is not affected enough by pressure to be easily separated using pressure swings and instead, an entrainer may be added that \"either\" modifies the azeotropic composition and exhibits immiscibility with one of the components, \"or\" extractive distillation may be used.\n\nOther methods of separation involve introducing an additional agent, called an \"entrainer\", that will affect the volatility of one of the azeotrope constituents more than another. When an entrainer is added to a binary azeotrope to form a ternary azeotrope, and the resulting mixture distilled, the method is called azeotropic distillation. The best known example is adding benzene or cyclohexane to the water/ethanol azeotrope. With cyclohexane as the entrainer, the ternary azeotrope is 7% water, 17% ethanol, and 76% cyclohexane, and boils at 62.1 °C. Just enough cyclohexane is added to the water/ethanol azeotrope to engage all of the water into the ternary azeotrope. When the mixture is then boiled, the azeotrope vaporizes leaving a residue composed almost entirely of the excess ethanol.\n\nAnother type of entrainer is one that has a strong chemical affinity for one of the constituents. Using again the example of the water/ethanol azeotrope, the liquid can be shaken with calcium oxide, which reacts strongly with water to form the nonvolatile compound, calcium hydroxide. Nearly all of the calcium hydroxide can be separated by filtration and the filtrate redistilled to obtain 100% pure ethanol.\n\nA more extreme example is the azeotrope of 1.2% water with 98.8% diethyl ether. Ether holds the last bit of water so tenaciously that only a very powerful desiccant such as sodium metal added to the liquid phase can result in completely dry ether.\n\nAnhydrous calcium chloride is used as a desiccant for drying a wide variety of solvents since it is inexpensive and does not react with most nonaqueous solvents. Chloroform is an example of a solvent that can be effectively dried using calcium chloride.\n\nWhen a salt is dissolved in a solvent, it always has the effect of raising the boiling point of that solvent – that is it decreases the volatility of the solvent. When the salt is readily soluble in one constituent of a mixture but not in another, the volatility of the constituent in which it is soluble is decreased and the other constituent is unaffected. In this way, for example, it is possible to break the water/ethanol azeotrope by dissolving potassium acetate in it and distilling the result.\n\nExtractive distillation is similar to azeotropic distillation, except in this case the entrainer is less volatile than any of the azeotrope's constituents. For example, the azeotrope of 20% acetone with 80% chloroform can be broken by adding water and distilling the result. The water forms a separate layer in which the acetone preferentially dissolves. The result is that the distillate is richer in chloroform than the original azeotrope.\n\nThe pervaporation method uses a membrane that is more permeable to the one constituent than to another to separate the constituents of an azeotrope as it passes from liquid to vapor phase. The membrane is rigged to lie between the liquid and vapor phases. Another membrane method is vapor permeation, where the constituents pass through the membrane entirely in the vapor phase. In all membrane methods, the membrane separates the fluid passing through it into a permeate (that which passes through) and a retentate (that which is left behind). When the membrane is chosen so that is it more permeable to one constituent than another, then the permeate will be richer in that first constituent than the retentate.\n\nThe rules for positive and negative azeotropes apply to all the examples discussed so far. But there are some examples that don't fit into the categories of positive or negative azeotropes. The best known of these is the ternary azeotrope formed by 30% acetone, 47% chloroform, and 23% methanol, which boils at 57.5 °C. Each pair of these constituents forms a binary azeotrope, but chloroform/methanol and acetone/methanol both form positive azeotropes while chloroform/acetone forms a negative azeotrope. The resulting ternary azeotrope is neither positive nor negative. Its boiling point falls \"between\" the boiling points of acetone and chloroform, so it is neither a maximum nor a minimum boiling point. This type of system is called a saddle azeotrope. Only systems of three or more constituents can form saddle azeotropes.\n\nA rare type of complex binary azeotrope is one where the boiling point and condensation point curves touch at two points in the phase diagram. Such a system is called a double azeotrope, and will have two azeotropic compositions and boiling points. An example is water and \"N\"-methylethylenediamine.\n\n"}
{"id": "58529850", "url": "https://en.wikipedia.org/wiki?curid=58529850", "title": "Bernstein–Greene–Kruskal modes", "text": "Bernstein–Greene–Kruskal modes\n\nBernstein–Greene–Kruskal modes (a.k.a. BGK modes) are nonlinear electrostatic waves that propagate in a unmagnetized, collisionless plasma. They are nonlinear solutions to the Vlasov–Poisson equation in plasma physics, and are named after physicists Ira B. Bernstein, John M. Greene, and Martin D. Kruskal, who solved and published the exact solution for the one-dimensional case in 1957.\n\nBGK modes have been studied extensively in numerical simulations for two- and three-dimensional cases, and are believed to be produced by the two-stream instability. They have been observed as electron phase space holes (electrostatic solitary structures) and double layers in space plasmas, as well as in scattering experiments in the laboratory.\n\nIn the linear limit of BGK modes (e.g. in the small amplitude approximation), the solutions reduce to what is known as Van Kampen modes, named after Nico van Kampen who derived the solutions in 1955. The phase mixing of Van Kampen modes gives rise to Landau damping.\n\nBGK modes have been generalized to quantum mechanics, in which the solutions (called quantum BGK modes) solve the quantum equivalent of the Vlasov–Poisson system known as the Wigner–Poisson system, with periodic boundary conditions. The solutions for the QBGK modes were put forth by Lange et al. in 1996, with potential applications to quantum plasmas.\n"}
{"id": "4199006", "url": "https://en.wikipedia.org/wiki?curid=4199006", "title": "Bone wax", "text": "Bone wax\n\nBone wax is a waxy substance used to help mechanically control bleeding from bone surfaces during surgical procedures.\n\nIt is generally made of beeswax with a softening agent such as paraffin or petroleum jelly and is smeared across the bleeding edge of the bone, blocking the holes and causing immediate bone hemostasis through a tamponade effect. Bone wax is most commonly supplied in sterile sticks, and usually requires softening before it can be applied.\n\nA note by Victor Horsley published in the British Medical Journal in 1892 described a formulation of “antiseptic wax” having seven parts beeswax, one part almond oils, and 1% salicylic acid. The material was useful for controlling bleeding when pressed into the pores and channels of cut or damaged bone. The wax was sterilized by boiling and kept in stoppered bottles. This material soon became the standard of care for bleeding control in bone for general orthopedics, craniomaxillofacial surgery, and cardiothoracic surgery, where the sternum is often split longitudinally to provide access to the heart.\n\nOrdinary bone wax is effective by virtue of its tamponade action, but is considered to have no active hemostatic properties (i.e. does not activate the blood clotting cascade). In addition, bone wax is not soluble in the bodily fluids and thus remains at the site of implantation for long periods of time, if not indefinitely. The portion of traditional bone wax that departs the implant site is most likely carried away through the action of the foreign body response and is associated with a low-grade inflammatory response at and near the implant site. The residual product can also potentially serve as a nidus for post-operative infection.\n\nModern day bone wax is commercially available in substantially non-absorbable formulations similar to Horsley’s original composition, as well as in absorbable/resorbable formats. Most are available as a firm wax in stick form that must be softened by kneading prior to use. \n\nMore recent advances have led to the introduction of a bone hemostat in putty format. Hemostatic putties act via tamponade in the same way as the stick waxes, but are ready to use and eliminate the requirement to soften the product prior to use.\n\nHemasorb Resorbable Hemostatic Bone Putty is a sterile, soft, moldable, biocompatible, absorbable material of putty-like consistency. The material is a mixture of calcium stearate, Vitamin E acetate, and liquid surfactant. It is virtually odorless, off-white in color and can be spread easily with minimal adhesion to surgical gloves. The bone putty requires no kneading prior to application and does not soften appreciably at body temperature.\n\nThe FDA has recently approved a new water-soluble bone hemostasis material called Ostene, which is designed to look and feel like bone wax. This material comprises a sterile mixture of water-soluble alkylene oxide copolymers, derived from ethylene oxide and propylene oxide. These copolymers have a long history in the medical and pharmaceutical fields, and they are considered inert. These compounds are not metabolized, but eliminated from the body unchanged.\n\n"}
{"id": "9096428", "url": "https://en.wikipedia.org/wiki?curid=9096428", "title": "Caster board", "text": "Caster board\n\nA caster board or vigorboard is a two-wheeled, human-powered land vehicle. It is somewhat like a snakeboard. Other names are waveboard \"J-board\" and RipStik (sometimes written \"rip stick\"), both of which are derived from commercial brands.\n\nA caster board has two narrow platforms known as \"decks\" that are joined by a \"torsion bar\", which consists of a metal beam, usually coated by rubber, that houses a strong spring. One polyurethane wheel is mounted to each deck with a caster so that each wheel can steer independently, and each caster has a steering axis that is tilted about 30° back from the vertical.\n\nThe motion requires that the board be twisted back and forth so as to move either just the back foot or both the front and back feet side to side, essentially pushing the board forward at the outside of the movement, before the foot is brought back in the other direction. In principle, the act is similar to what is required to propel one who is riding inline skates forward, as opposed to how skateboarders push with their feet on the ground. Riding a caster board requires using a twisting motion of hips and legs.\n\nA rider or \"caster boarder\" gains speed because each wheel is mounted on a 30° slant on the bottom of each deck. When each deck is pushed to the side, it causes the board to be pushed upward by the wheels' rotation against the gradient of the mounts. This creates potential energy that is then released moves back down under the rider's weight and its own combined. The weight pushing the board back down causes the wheels to turn to face straight again. While riding on a caster board, the increase in height is barely noticeable unless the rider twists the board along the vertical axis too hard, causing stability to be momentarily reduced. \n\nCaster boarding has been introduced into many school curricula as a means of teaching the basic movement principles that govern board-sports. The success in its ability to engage with pupils not interested in sport was assessed in a 12-week, 6 secondary school case study in the UK carried out by Curriculum Ex.\n\nFoot placement is critical on a caster board because one wheel rests under each foot while in use. In order to start with proper foot placement, it is necessary to have the front foot above the center of the front caster and to allow the back foot to give a good push of speed that will allow the board to keep proper balance. Attempting to place the back foot too quickly will make it even more difficult to achieve a desirable foot placement, so it is best to give that foot a maximum of two seconds for it to properly set itself on the board. More experienced riders will be able to place their back feet more quickly. A manual is performed by putting the rear foot on the back end of the rear deck without letting it come off and gently lifting the front foot.\n\nIf the user is already riding the board and the user recognizes that the foot placement is slightly undesirable, he/she could replace both feet simultaneously without interrupting his/her ride that is already taking place. This is done by the rider's first making sure that he/she is riding at a normal speed and that the riding surface ahead is stable for riding on and then jumping with both feet at a minimal height that allows both shoes to separate their treads from the grips of the caster board. The rider may continue to \"hop around\" the board until a most desirable foot placement is achieved and for as long as a proper speed is maintained. Hopping around may even more easily produce better results than getting off the board and getting back on again. A much more difficult means of replacing the feet while riding is attempting to correct only one foot at a time, increasing the risk of shifting his/her weight too far forward or backward and falling to the ground.\n\nIn order to steer properly on a caster board, the front foot must lean into the curve while the back foot leans out of the curve. By leaning the front foot in and the back foot out, the front wheel, which will have its front facing inward, is forced to form an arc with the back wheel, which will have its front facing outward. This arcing allows for very sharp turning, but can be exercised for making wide turns as well. While attempting to turn on the board at a higher speed and/or using a tighter arc, the rider leans his/her center of gravity into the turn to keep from falling from the board. However, like all vehicles, there is a limit to the combined sharpness and speed of turning on a caster board without it becoming overturned. It is possible to propel the board while turning by making weaving motions that are typically smaller than those of a relatively straight trajectory.\n\nThe wrong way to turn on a caster board is to lean both decks in the direction of the turn, which will cause the board to move away from the leaning direction in a parallel-sliding fashion.\n\nA helmet, elbow pads, knee pads, gloves, and shin guards are recommended when using a caster board. Falling is common for inexperienced riders.\n\nA variety of tricks can be done on a caster board. They are coping and ledge tricks, manuals, Carves and flips.\nThe various flips tricks include: Kickflip, No Comply Impossible, Double Kickflip, Fakie Kickflip, Switch Kickflip, Fakie Bigspin, Nollie Kickflip, Nollie Heelflip, Nollie Frontside 180 Bigspin, Varial Kickflip, Varial Heelflip, Backside 180 Kickflip, Frontside 180 Heelflip, Frontside 180 Kickflip, 360 Kickflip and the Frontside 180 Double Kickflip.\n\nCasterboarders can ride in skateparks as with other types of skateboards. However, some skateparks have prohibited caster boards. Planet Park (Hachioji city) skatepark in Tokyo, Japan allows only skateboards with four wheels and a single deck. The Japan Skatepark Association claims that if a caster board rider falls, it can be difficult to predict which direction the board will travel, constituting an unpredictable element of danger that may interfere with other skateboarders, inline-skaters and BMX riders in the park.\n\nThe Vigorboard's ability to combine fun with a full cardio-vascular workout has prompted St Helens council to promote the boards in all of its high schools and some of its primary schools, seeing it as a method of inspiring children who wouldn't normally take part in traditional exercise.\n\nThe Vigorboard is proving a valuable weapon in the fight against childhood obesity, with some teachers at the schools currently trialing the boards also reporting drops in truancy, with pupils not wishing to miss school should they miss their Vigorboard session.\n\nAn independent study conducted by the University of Salford found that vigorboarding resulted in an average increase in heart rate of 227% and a 535% increase in energy expenditure. The study also concluded that vigorboarding results in a 233% greater energy expenditure (407kcal/hr) than walking at 3 mph (174.5kcal/hr).\n\nIn its conclusion, the authors of the university study went on to say, 'Vigorboarding may be a useful alternative to increasing physical activity levels and energy expenditure, especially for individuals that do not wish to participate in the usual sports or activities performed by adolescents.'.\n\n"}
{"id": "748281", "url": "https://en.wikipedia.org/wiki?curid=748281", "title": "Cermet", "text": "Cermet\n\nA cermet is a composite material composed of ceramic (cer) and metal (met) materials.\n\nA cermet is ideally designed to have the optimal properties of both a ceramic, such as high temperature resistance and hardness, and those of a metal, such as the ability to undergo plastic deformation. The metal is used as a binder for an oxide, boride, or carbide. Generally, the metallic elements used are nickel, molybdenum, and cobalt. Depending on the physical structure of the material, cermets can also be metal matrix composites, but cermets are usually less than 20% metal by volume.\n\nCermets are used in the manufacture of resistors (especially potentiometers), capacitors, and other electronic components which may experience high temperature.\n\nCermets are used instead of tungsten carbide in saws and other brazed tools due to their superior wear and corrosion properties. Titanium nitride (TiN), titanium carbonitride (TiCN), titanium carbide (TiC) and similar can be brazed like tungsten carbide if properly prepared however they require special handling during grinding.\n\nComposites of MAX phases, an emerging class of ternary carbides or nitrides with aluminium or titanium alloys have been studied since 2006 as high-value materials exhibiting favourable properties of ceramics in terms of hardness and compressive strength alongside ductility and fracture toughness typically associated with metals. Such cermet materials, including aluminium-MAX phase composites, have potential applications in automotive and aerospace applications.\n\nSome types of cermets are also being considered for use as spacecraft shielding as they resist the high velocity impacts of micrometeoroids and orbital debris much more effectively than more traditional spacecraft materials such as aluminum and other metals.\n\nAfter World War II, the need to develop high temperature and high stress-resistant materials became clear. During the war, German scientists developed oxide base cermets as substitutes for alloys. They saw a use for this for the high-temperature sections of new jet engines as well as high temperature turbine blades. Today ceramics are routinely implemented in the combuster part of jet engines because it provides a heat-resistant chamber. Ceramic turbine blades have also been developed. These blades are lighter than steel and allow for greater acceleration of the blade assemblies.\nThe United States Air Force saw potential in the material technology and became one of the principal sponsors for various research programs in the US. Some of the first universities to research were Ohio State University, University of Illinois, and Rutgers University.\n\nThe word cermet was actually coined by the United States Air Force, the idea being that they are a combination of two materials, a metal and a ceramic. Basic physical properties of metals include ductility, high strength, and high thermal conductivity. Ceramics possess basic physical properties such as a high melting point, chemical stability, and especially oxidation resistance.\n\nThe first ceramic metal material developed used magnesium oxide (MgO), beryllium oxide (BeO), and aluminum oxide (AlO) for the ceramic part. Emphasis on high stress rupture strengths was around 980 °C. Ohio State University was the first to develop AlO based cermets with high stress rupture strengths around 1200 °C. Kennametal, a metal-working and tool company based in Latrobe, PA, developed the first titanium carbide cermet with a 2800 psi and 100-hour stress-to-rupture strength at 980 °C. Jet engines operate at this temperature and further research was invested on using these materials for components.\n\nQuality control in manufacturing these ceramic metal composites was hard to standardize. Production had to be kept to small batches and within these batches, the properties varied greatly. Failure of the material was usually a result of undetected flaws usually nucleated during processing.\n\nThe existing technology in the 1950s reached a limit for jet engines where little more could be improved. Subsequently, engine manufactures were reluctant to develop ceramic metal engines.\n\nInterest was renewed in the 1960s when silicon nitride and silicon carbide were looked at more closely. Both materials possessed better thermal shock resistance, high strength, and moderate thermal conductivity.\n\nCermets were first used extensively in ceramic-to-metal joint applications. Construction of vacuum tubes was one of the first critical systems, with the electronics industry employing and developing such seals. German scientists recognized that vacuum tubes with improved performance and reliability could be produced by substituting ceramics for glass. Ceramic tubes can be outgassed at higher temperatures. Because of the high-temperature seal, ceramic tubes withstand higher temperatures than glass tubes. Ceramic tubes are also mechanically stronger and less sensitive to thermal shock than glass tubes. Today, cermet vacuum tube coatings have proved to be key to solar hot water systems.\n\nCeramic-to-metal mechanical seals have also been used. Traditionally they have been used in fuel cells and other devices that convert chemical, nuclear, or thermionic energy to electricity. The ceramic-to-metal seal is required to isolate the electrical sections of turbine-driven generators designed to operate in corrosive liquid-metal vapors.\n\nBioceramics play an extensive role in biomedical materials. The development of these materials and diversity of manufacturing techniques has broadened the applications that can be used in the human body. They can be in the form of thin layers on metallic implants, composites with a polymer component, or even just porous networks. These materials work well within the human body for several reasons. They are inert, and because they are resorbable and active, the materials can remain in the body unchanged. They can also dissolve and actively take part in physiological processes, for example, when hydroxylapatite, a material chemically similar to bone structure, can integrate and help bone grow into it. Common materials used for bioceramics include alumina, zirconia, calcium phosphate, glass ceramics, and pyrolytic carbons.\n\nOne important use of bioceramics is in hip replacement surgery. The materials used for the replacement hip joints were usually metals such as titanium, with the hip socket usually lined with plastic. The multiaxial ball was tough metal ball but was eventually replaced with a longer-lasting ceramic ball. This reduced the roughening associated with the metal wall against the plastic lining of the artificial hip socket. The use of ceramic implants extended the life of the hip replacement parts.\n\nCermets are also used in dentistry as a material for fillings and prostheses.\n\nCeramic parts have been used in conjunction with metal parts as friction materials for brakes and clutches.\n\nThe United States Army and British Army have had extensive research in the development of cermets. These include the development of lightweight ceramic projectile-proof armor for soldiers and also Chobham armor.\n\nCermets are also used in machining on cutting tools.\n\nCermets are also used as the ring material in high-quality line guides for fishing rods.\n\nA cermet of depleted fissiable material (e.g. uranium, plutonium) and sodalite has been researched for its benefits in the storage of nuclear waste. Similar composites have also been researched for use as a fuel source.\n\nAs nanostructured cermet, this material is used in the optical field, such as solar absorbers/selective surface. Thanks to the size of the particles (~5 nm), surface plasmons on the metallic particles are generated and enable the heat transmission.\n\nFor reasons regarding luxury, cermet is sometimes found to be case materials for some watches, including Jaeger-LeCoultre's Deep Sea Chronograph Vintage Cermet watch.\n\n\n"}
{"id": "38737", "url": "https://en.wikipedia.org/wiki?curid=38737", "title": "Cosmos", "text": "Cosmos\n\nThe cosmos (, ) is the universe. Using the word \"cosmos\" rather than the word \"universe\" implies viewing the universe as a complex and orderly system or entity; the opposite of chaos.\nThe cosmos, and our understanding of the reasons for its existence and significance, are studied in cosmology - a very broad discipline covering any scientific, religious, or philosophical contemplation of the cosmos and its nature, or reasons for existing. Religious and philosophical approaches may include in their concepts of the cosmos various spiritual entities or other matters deemed to exist outside our physical universe.\n\nThe philosopher Pythagoras first used the term \"cosmos\" () for the order of the universe. The term became part of modern language in the 19th century when geographer–polymath Alexander von Humboldt resurrected the use of the word from the ancient Greek, assigned it to his five-volume treatise, \"Kosmos\", which influenced modern and somewhat holistic perception of the universe as one interacting entity.\n\nCosmology is the study of the cosmos, and in its broadest sense covers a variety of very different approaches: scientific, religious and philosophical. All cosmologies have in common an attempt to understand the implicit order within the whole of being. In this way, most religions and philosophical systems have a cosmology.\n\nWhen \"cosmology\" is used without a qualifier, it often signifies physical cosmology, unless the context makes clear that a different meaning is intended.\n\nPhysical cosmology (often simply described as 'cosmology') is the scientific study of the universe, from the beginning of its physical existence. It includes speculative concepts such as a multiverse, when these are being discussed. In physical cosmology, the term \"cosmos\" is often used in a technical way, referring to a particular spacetime continuum within a (postulated) multiverse. Our particular cosmos, the observable universe, is generally capitalized as \"the Cosmos\". \n\nIn physical cosmology, the uncapitalized term cosmic signifies a subject with a relationship to the universe, such as 'cosmic time' (time since the Big Bang), 'cosmic rays' (high energy particles or radiation detected from space), and 'cosmic microwave background' (microwave radiation detectable from all directions in space).\n\nAccording to in Sir William Smith \"Dictionary of Greek and Roman Biography and Mythology\" (1870, see book screenshot for full quote), Pythagoreans described the universe.\n\nCosmology is a branch of metaphysics that deals with the nature of the universe, a theory or doctrine describing the natural order of the universe. The basic definition of Cosmology is the science of the origin and development of the universe. In modern astronomy the Big Bang theory is the dominant postulation.\n\nIn theology, the cosmos is the created heavenly bodies (sun, moon, planets, and fixed stars). In Christian theology, the word is also used synonymously with \"aion\" to refer to \"worldly life\" or \"this world\" or \"this age\" as opposed to the afterlife or world to come.\n\nThe 1870 book \"Dictionary of Greek and Roman Biography and Mythology\" noted\n\nThe book \"The Works of Aristotle\" (1908, p. 80 \"Fragments\") mentioned\n\nBertrand Russell (1947) noted\n\n"}
{"id": "27977618", "url": "https://en.wikipedia.org/wiki?curid=27977618", "title": "Cryoconite", "text": "Cryoconite\n\nCryoconite is powdery windblown dust made of a combination of small rock particles, soot and microbes which is deposited and builds up on snow, glaciers, or ice caps. The darkening, especially from small amounts of soot, absorbs solar radiation melting the snow or ice beneath the deposit, and sometimes creating a cryoconite hole. Cryoconite may contain dust from far away continental deserts or farmland, particles from volcanic eruptions or power plant emissions, and soot. It was first described and named by Nils A. E. Nordenskiöld when he traveled on Greenland's icecap in 1870. During summer, cryoconite holes frequently contain liquid water and thus provide a niche for cold-adapted microorganisms like bacteria, algae and animals like rotifers and tardigrades to thrive. Cryoconite typically settles and concentrates at the bottom of these holes creating a noticeable dark mass.\n\nSoot decreases the reflectivity, or albedo of ice, increasing absorption of heat. Cryoconite is constantly being added to snow and ice formations along with snow. It is buried within the snow or ice, but as the snow or ice melts increasing amounts of dark material is exposed on the surface, accelerating melting.\n\n"}
{"id": "21289989", "url": "https://en.wikipedia.org/wiki?curid=21289989", "title": "Deaereating feed tank", "text": "Deaereating feed tank\n\nA deaerating feed tank (DFT), often found in steam plants that propel ships, is located after the main condensate pump and before the main feed booster pump. It has these three purposes:\n\n\nBased on the relevant theoretical Rankine cycle diagram, there are four main processes, or steps:\n\nIn the practical implementation of a Rankine cycle, it is common to break the single pump (process 1 to 2) into three pumps: (in water flow order: condensate pump, feed booster pump and then feedwater pump). \n\n\nA surge volume allows the plant to change bells (power output level) without running the feed pump dry or flooding the turbines. Consider the plant running in a steady state condition.\n\nThe bell is increased, more power output demanded, the rate of feed is increased. This draws more water from the condenser, perhaps to the point of being dry and starving the boiler resulting in a loss of propulsion. This is until the water, converted to steam, provides its energy to the turbine and then is condensed in the condenser.\n\nThe bell is decreased, less power output demanded, the rate of feed is decreased. Since less water is drawn from the condenser the condensate level rises, covering more condenser tubes, reducing the ability of the condenser to maintain vacuum and, if the level is allowed to go high enough, vacuum could be lost and/or water could impinge (and damage) the turbine blades as the turbine normally sits directly above the condenser.\n"}
{"id": "17442209", "url": "https://en.wikipedia.org/wiki?curid=17442209", "title": "Drift current", "text": "Drift current\n\nIn condensed matter physics and electrochemistry, drift current is the electric current, or movement of charge carriers, which is due to the applied electric field, often stated as the electromotive force over a given distance. When an electric field is applied across a semiconductor material, a current is produced due to the flow of charge carriers.\n\nThe \"drift velocity\" is the average velocity of the charge carriers in the drift current. The drift velocity, and resulting current, is characterized by the \"mobility\"; for details, see electron mobility (for solids) or electrical mobility (for a more general discussion).\n\nSee drift–diffusion equation for the way that the drift current, diffusion current, and carrier generation and recombination are combined into a single equation.\n\nDrift current is the electric current caused by particles getting pulled by an electric field. The term is most commonly used in the context of electrons and holes in semiconductors, although the same concept also applies to metals, electrolytes, and so on.\n\nDrift current is caused by the electric force: Charged particles get pushed by an electric field. Electrons, being negatively charged, get pushed in the opposite direction to the electric field, while holes get pushed in the same direction as the electric field, but the resulting conventional current points in the same direction as the electric field in both cases.\n\nIf an electric field is applied to an electron in a vacuum, the electron will accelerate faster and faster, in approximately a straight line. A drift current looks very different than that up close. Typically, electrons are moving randomly in all directions (Brownian motion), frequently changing direction when they collide with grain boundaries or other disturbances. Between collisions, the electric field subtly accelerates them in one direction. So over time, they move at the drift velocity on average, but at any instant the electrons are moving at the (typically much faster) thermal velocity.\n\nThe amount of drift current depends on the concentration of charge carriers and their mobility in the material or medium.\n\nDrift current frequently occurs at the same time as diffusion current; the following table compares the two forms of current:\n\nIn a p-n junction diode, electrons and holes are the minority charge carriers in the p-region and the n-region, respectively. In an unbiased junction, due to the diffusion of charge carriers, the diffusion current, which flows from the p to n region, is exactly balanced by the equal and opposite drift current. \nIn a biased p-n junction, the drift current is independent of the biasing, as the number of minority carriers is independent of the biasing voltages. But as minority charge carriers can be thermally generated, drift current is temperature dependent. \n\nWhen an electric field is applied across the semiconductor material, the charge carriers attain a certain drift velocity . This combined effect of movement of the charge carriers constitutes a current known as \"drift current\". Drift current density due to the charge carriers such as free electrons and holes is the current passing through a square centimeter area perpendicular to the direction of flow.\n\n(i) Drift current density J, due to free electrons is given by:\n\nformula_1\n\n(ii) Drift current density J, due to holes is given by:\n\nformula_2\n\nWhere:\nn - Number of free electrons per cubic centimeter.\n\np - Number of holes per cubic centimeter\n\nformula_3 – Mobility of electrons in formula_4\n\nformula_5 – Mobility of holes in formula_4\n\nE – Applied Electric Field Intensity in V /cm\n\nq – Charge of an electron = 1.6 × 10 coulomb.\n"}
{"id": "5188309", "url": "https://en.wikipedia.org/wiki?curid=5188309", "title": "Energy efficiency in British housing", "text": "Energy efficiency in British housing\n\nDomestic housing in the United Kingdom presents a possible opportunity for achieving the 20% overall cut in UK carbon dioxide emissions targeted by the Government for 2010. However, the process of achieving that drop is proving problematic given the very wide range of age and condition of the UK housing stock.\n\nAlthough carbon emissions from housing have remained fairly stable since 1990 (due to the increase in household energy use having been compensated for by the 'dash for gas'), housing accounted for around 30% of all the UK's carbon dioxide emissions in 2004 (40 million tonnes of carbon) up from 26.42% in 1990 as a proportion of the UK's total emissions. The Select Committee on Environmental Audit noted that emissions from housing could constitute over 55% of the UK's target for carbon emissions in 2050.\n\nA 2006 report commissioned by British Gas estimated the average carbon emissions for housing in each of the local authorities in Great Britain, the first time that this had been done. This indicated that housing in Uttlesford (Essex) produced the highest emissions (8,092 kg of carbon dioxide per dwelling). This was 250% higher than housing in Camden (London) which produced the least (averaging 3,255 kg). Among the 23 towns included, Reading had the highest emissions (6,189 kg), with Hull the lowest (4,395 kg). The variations are due to a number of factors, including the age, size and type of the housing stock, together with the efficiency of heating systems, the mix of fuels used, the ownership of appliances, occupancy levels and the habits of the occupants.\n\nIn the December 2006 Pre-Budget Report, the Government announced their 'ambition' that all new homes will be 'zero-carbon' by 2016 (i.e. built to zero-carbon building standards). To encourage this, an exemption from Stamp duty land tax is to be granted, lasting until 2012, for all new zero-carbon homes up to £500,000 in value.\n\nWhilst some organisations applauded the initial announcement of the scheme, in the pre-budget statement from the then UK Chancellor, Gordon Brown, others are concerned about the government's ability to deliver on the promise.\n\nThe housing stock in the United Kingdom is amongst the least energy efficient in Europe. In 2004, housing (including space heating, hot water, lighting, cooking, and appliances) accounted for 30.23% of all energy use in the UK (up from 27.70% in 1990). The figure for London is higher at approximately 37%.\n\nIn view of the progressive tightening of the Building Regulations' requirements for energy efficiency since the 1970s (see the history section below), it might be expected that a significant cut in domestic energy use would have occurred, however this has not yet been the case.\n\nAlthough insulation standards have been increasing, so has the standard of home heating. In 1970, only 31% of homes had central heating. By 2003 it had been installed in 92% of British homes, leading in turn to a rise in the average temperature within them (from 12.1 °C to 18.20 °C). Even in homes with central heating, average temperatures rose 4.55 °C during this period.\n\nAt the same time, the increase in the number of households, increasing numbers of domestic electrical appliances, an increase in the number of light fittings, reduction in the average number of occupants per household, plus other factors, had led to an increase in total national domestic energy consumption from around 25% in 1970 to about 30% in 2001, and remained on an upward trend (BRE figures).\n\nThe figures for energy consumed by end use for 2003.\n\nThe Green Deal provided low interest loans for energy efficiency improvements to the energy bills of the properties the upgrades are performed on. These debts are passed onto new occupiers when they take over the payment of energy bills. The costs of the loan repayments should be less than the savings on the bills from the upgrades, however this will be a guideline and not legally enforceable guarantee. It is believed that tying repayment to energy bills will give investors a secure return.\nThe Green Deal for the domestic property market was launched in October 2012. The Commercial Green Deal was launched in January 2012 and released in a series of stages to help with the varying needs and requirements of commercial properties.\n\nThe 1965 Building Regulations introduced the first limits on the amount of energy that could be lost through certain elements of the fabric of new houses. This was expressed as a u-value—the amount of heat lost per square metre, for each degree Celsius of temperature difference between inside and outside.\n\nIn effect, the Target Insulation is a ratio of 1.33 W/m²·K of wall area (Document L 2006). So to keep your square metre warm, you are limited as to how much power you can use. This is slightly regressive in that richer people live in bigger houses which tend to have a lower surface area/floor area, although this is partially offset by them being detached, as opposed to, say, terraced.\n\nThese limits were tightened following the 1973 oil crisis, and on several subsequent occasions (see below). Despite this, UK insulation levels have remained low compared to the EU average.\n\nThe energy policy of the United Kingdom through the 2003 Energy White Paper articulated directions for more energy efficient building construction. Hence, the year 2006 saw a significant tightening of energy efficiency requirements within the Building Regulations (for earlier regulations, see separate section below).\n\nWith the long term aim of cutting overall emissions by 60% by 2050, and by 80% by 2100, the intention of the 2006 changes was to cut energy use in new housing by 20% compared to a similar building constructed to the 2002 standards. The changes were the first to the regulations brought about by the desire to reduce emissions, though some have raised doubts about whether they will actually achieve the 20% cut (see criticisms section).\n\nIn the 2006 regulations, the u-value was replaced as the primary measure of energy efficiency by the Dwelling Carbon Dioxide Emission Rate (DER), an estimate of carbon dioxide emissions per m² of floor area. This is calculated using the Government's Standard Assessment Procedure for Energy Rating of Dwellings (SAP 2005).\n\nIn addition to the levels of insulation provide by the structure of the building, the DER also takes into account the airtightness of the building, the efficiency of space and water heating, the efficiency of lighting, and any savings from solar power or other energy generation technologies employed, and other factors. For the first time, it also became compulsory to upgrade the energy efficiency in existing houses when extensions or certain other works are carried out.\n\nSome organisations have raised doubts over the claim that the changes will result in a 20% saving. Issues cited have included alleged problems with the calculation methods, the limitations of the modelling software, and the specification of the reference building used in the model. For example, a 2005 study sponsored by the Pilkington Energy Efficiency Trust indicated that the savings would only be in the region of 9%.\n\nThere are also concerns about enforcement, with a Building Research Establishment study in 2004 indicating that 60% of new homes do not conform to existing regulations. A 2006 survey for the Energy Saving Trust revealed that Building Control Officers considered energy efficiency 'a low priority' and that few would take any action over failure to comply with the Building Regulations because the matter 'seemed trivial'.\n\nIn December 2006, the government announced their ambition that all new housing should be built to zero-carbon standards from 2016; i.e., that the carbon emitted during a typical year should be balanced by renewable energy generation. Despite being the first country in the world to adopt such a policy the initiative was generally welcomed by the industry in principle, despite some subsequent concern over the practicalities.\n\nOn 1 April 2011 the WWF resigned from the taskforce on Zero-Carbon homes, stating that 'the zero-carbon policy is now in tatters' after the Government unilaterally decided to change the scope of the 'zero carbon' policy to exclude some emissions not currently covered by the building regulations. The UK Green Building Council estimate that the change, published at the time of the March 2011 budget, will result in only two thirds of the emissions of a new home being mitigated.\n\nIn 2004, the Government indicated that the next revision to the energy performance standards of the Building Regulations would be in 2010. In the consultation document \"Building a Greener Future: Towards Zero Carbon Development\" it is proposed that the 2010 revision should require a further 25% improvement in the energy/carbon performance, in line with the 2004 proposals. It is further envisaged that there would be a 44% improvement in 2013, compared to 2006 levels. This would then be followed by the adoption of a zero carbon requirement in 2016, applied to all home energy use including appliances. These steps in performance would align the energy efficiency requirement of the Building Regulations with those of Levels 3, 4 and 6 of the Code for Sustainable Homes in 2010, 2013 and 2016 respectively.\n\nOriginally, from June 2007, all homes (and other buildings) in the UK would have to undergo Energy Performance Certification (also commonly known as an EPC Certificate) before they are sold or let, in order to meet the requirements of the European Energy Performance of Buildings Directive (Directive 2002/91/EC). The scheme provides the owner or landlord with an 'energy label' so that they can demonstrate the energy efficiency of the property, and is also included in the new Home Information Packs. The scheme has been criticized for its methodology and superficial approach, especially for old buildings. For example, it ignores thick walls with their low heat transmission, and its recommendations for compact fluorescent lamps, which can damage sensitive textiles and paintings.\n\nIt is hoped that energy labelling will raise awareness of energy efficiency, and encourage upgrading to make properties more marketable. Incentives may be available for carrying out energy conservation measures.\n\nFor new building, SAP 2005 calculations are to form the basis for the certification, while RDSAP (Reduced Data SAP) will be used to assess existing properties. It is estimated that only 10% of the nation's housing will score above 60 on the scale, although most will score above 40.\n\nAnother rating scheme of note is the Government sponsored EcoHomes rating, mostly used in public sector housing, and only applicable to new properties or major refurbishments. This actually measures a range of sustainability issues, of which energy efficiency is only one. EcoHomes is to be replaced by the Government's Code for Sustainable Homes in 2007.\n\nThe Energy Saving Trust set requirements for 'good practice' and 'advanced practice' for achieving lower energy buildings, while the Association for Environment Conscious Building's \"CarbonLite\" programme specifies Silver and Gold standards, the latter approaching a zero energy building.\n\nIn Wales where 'zero-carbon homes' are the aspiration for 2011 (although 2012 is more likely) the requirements are for Code for Sustainable Homes or equivalent. This has opened the doors for standards like Passiv Haus and the CarbonLite programme. Another lesser known building type that does not rely on airtightness in order to get its energy rating is Bio-Solar-Haus. This is not a well known type of house, but it has a range of positive advantages like it is built out of renewable resources and it is a breathable structure thus making it much healthier to live in.\n\nThe Government's low carbon buildings programme was launched in 2006 to replace the earlier \"Clear Skies\" and \"Solar PV\" programmes. It offers grants towards the costs of solar thermal heating, small wind turbine, micro hydro, ground source heat pump, and biomass installations. As of January 2007 funding for grants is proving insufficient to meet demand.\n\nA similar scheme, the Scottish Community and Household Renewables Initiative operates in Scotland, which also offers grants towards the cost of air source heat pumps.\n\nUnder the Home Energy Conservation Act 1995, local authorities are required to consider measures to improve the energy efficiency of all residential accommodation in their areas, although they are not required to implement any measures. Most local authorities provide free advice on energy conservation and some also provide home visits, often targeting those in social housing and the fuel poor. Some also demand minimum levels of energy efficiency in newly constructed buildings. It was expected that the Act would result in a 30% cut in energy usage between 1996 and 2010. An overall cumulative improvement of 14.7% was reported to DEFRA for the year ending March 2004, but a large part of this would have happened without HECA.\n\nIn the South, most local authority housing was sold off in the 1980s-90s under RTB (Right to buy scheme), so the remaining stock is small. Much social housing has also been transferred to housing associations.\n\nOne of the most important energy efficiency demonstration projects was the 1986 Energy World exhibition in Milton Keynes, which attracted international interest. Fifty-one houses were built, designed to be at least 30% more efficient than the Building Regulations then in force. This was calculated using the Milton Keynes Energy Cost Index (MKECI), a test-bed for the subsequent SAP rating system and the National Home Energy Rating scheme. Energy World was preceded by the earlier Salford low-energy houses, built in the early 1980s, which continue to be 40% more efficient than the 2010 Building Regulations.\n\nThe Beddington Zero Energy Development (BedZED), a non-traditional housing scheme of 82 dwellings near Beddington, London included zero fossil energy usage as one of its key design features. The project was completed in 2002 and is the UK's largest eco-development. As designed, the energy used is generated from renewables on site. In use, BedZED has yielded considerable useful feedback, not least that energy efficiency and passive design features delivery more reliable reduced carbon emissions than active systems. Due to their superinsulation, the properties use 88% less energy (measured) for space heating compared to those built to the 2002 Building Regulations, while the reduction for water heating is 57%. Measured electrical use for cooking, appliances and occupant's plug loads ('unregulated energy' consumption) are some 55% lower than UK norms (bedzed-seven-years-on).\n\nThe Green Building in Manchester City Centre and has been built to high energy efficiency standards and won a 2006 Civic Trust Award for its sustainable design. The cylindrical shape of the ten storey tower provides the smallest surface area related to the volume, ensuring less energy is lost through thermal dissipation. Other technologies including solar water heating, a wind turbine and triple glazing.\n\nThe South Yorkshire Energy Centre at Heeley City Farm in Sheffield is an example of refurbishing an existing property to show the options available.\n\nThe EcoHouse in Leicester is to be renovated in 2011 to provide a demonstration of Retrofit for the Future energy efficiency standards.\n\nThe \"Old Home SuperHome\" initiative features many owner occupied, existing home retrofits which achieve a 60% carbon saving which can be visited by the public. Many of the homes have dramatically improved their energy efficiency to achieve these carbon savings, while some have also installed renewable energy technologies.\n\nInternational comparisons of particular note include:\n\nSince then many more have been built in Canada, in Japan, and in various other countries including a number in the UK. Currently energy savings of 30% to 40% are typically achieved in Canada.\n\nIn 2005, the Select Committee on Environmental Audit expressed their concern that there was a lack of significant funding for research and development of sustainable construction methods, with funding for the Building Research Establishment having been \"drastically\" cut in the previous 4 years. As a result, many of the sustainable building materials used in the UK are imported from Germany, Switzerland and Austria—some of the countries that have been prominent in research.\n\nEven if all new housing does become zero carbon by 2016, the energy efficiency of the remainder of the housing stock would need to be addressed.\n\nThe 2006 \"Review of the Sustainability of Existing Buildings\" revealed that 6.1 million homes lacked an adequate thickness of loft insulation, 8.5 million homes had uninsulated cavity walls, and that there is a potential to insulate 7.5 million homes that have solid external walls. These three measures alone have the potential to save 8.5 million tonnes of carbon emissions each year. Despite this, 95% of home owners think that the heating of their own home is currently effective.\n\nSee UK Government policy for improving home energy efficiency for further information of policies from 1945 to 2016 and their effectiveness. \n\nThe u-value limits introduced in 1965 were:\n\nFollowing the 1973 oil crisis, these were tightened in 1976 to:\n\n1985 saw the second tightening of these limits, to:\n\nThese limits were reduced again in 1990:\nLike the 2006 changes, it was predicted that the introduction of these limits would result in a 20% reduction in energy use for heating. A survey by Liverpool John Moores University predicted that the actual figure would be 6% \"(Johnson, JA “Building Regulations Research Project”)\".\n\nIn the 1995 Building Regulations, insulation standards were cut to the following U-values:\n\nThe 2002 regulations reduced the U-values, and made additional elements of the building fabric subject to control. Although there was in practice considerable flexibility and the ability to 'trade off' reductions in one area for increases in another, the 'target' limits became:\n\nSimilar limits were introduced into Scotland in 2002 & 2006, though with a lower limit of 0.3 or 0.27 for walls, and some other variations.\n\nIt was claimed by Government that these measures should cut the heating requirement by 25% compared to the 1995 Regulations. It was subsequently also claimed that they had achieved a 50% cut compared to the 1990 Regulations.\n\nWhile the u-value ceased being the sole consideration in 2006, u-value limits similar to those in the 2002 regulations still apply, but are no longer sufficient by themselves. The DER, and TER (Target Emission rate) calculated through either the UK Government's Standard Assessment Procedure for Energy Rating of Dwellings (SAP rating), 2005 edition, or the newer SBEM (*Simplified Building Energy Model) which is aimed at non-dwellings, became the only acceptable calculation methods. Several commercial energy modeling software packages have now also been verified as producing acceptable evidence by the BRE Global & UK Government. Calculations using previous versions of SAP had been an optional way of demonstrating compliance since 1991(?). They are now a statutory requirement (B. Reg.17C et al.) for all building regulations applications, involving new dwelling/buildings and large extensions to existing non-domestic buildings.\n\n\n\n\n"}
{"id": "1982315", "url": "https://en.wikipedia.org/wiki?curid=1982315", "title": "Exotic hadron", "text": "Exotic hadron\n\nExotic hadrons are a subatomic particles composed of quarks and gluons, but which - unlike \"well-known\" hadrons such as protons , neutrons and mesons - consist of more than three valence quarks. By contrast, \"ordinary\" hadrons contain just two or three quarks. Hadrons with explicit valence gluon content would also be considered exotic. In theory, there is no limit on the number of quarks in a hadron, as long as the hadron's color charge is white, or color-neutral.\n\nConsistent with ordinary hadrons, exotic hadrons are classified as being either fermions, like ordinary baryons, or bosons, like ordinary mesons. According to this classification scheme, pentaquarks, containing five valence quarks, are exotic baryons, while tetraquarks (four valence quarks) and hexaquarks (six quarks, consisting of either a dibaryon or three quark-antiquark pairs) would be considered exotic mesons. Tetraquark and pentaquark particles are believed to have been observed and are being investigated; Hexaquarks have not yet been confirmed as observed.\n\nExotic hadrons can be searched for by looking for S-matrix poles with quantum numbers forbidden to ordinary hadrons. Experimental signatures for such exotic hadrons have been seen by at least 2003 but remain a topic of controversy in particle physics.\n\nJaffe and Low suggested that the exotic hadrons manifest themselves as poles of the P matrix, and not of the S matrix. Experimental P-matrix poles are determined reliably in both the meson-meson channels and nucleon-nucleon channels.\n\nWhen the quark model was first postulated by Murray Gell-Mann and others in the 1960s, it was to organize the states known then to be in existence in a meaningful way. As quantum chromodynamics (QCD) developed over the next decade, it became apparent that there was no reason why only three-quark and quark-antiquark combinations could exist. Indeed, Gell-Mann's original 1964 paper alludes to the possibility of exotic hadrons and classifies hadrons into baryons and mesons depending upon whether they have an odd (baryon) or even (meson) number of valence quarks. In addition, it seemed that gluons, the mediator particles of the strong interaction, could also form bound states by themselves (glueballs) and with quarks (hybrid hadrons). Several decades have passed without conclusive evidence of an exotic hadron that could be associated with the S-matrix pole.\n\nIn April 2014, the LHCb collaboration confirmed the existence of the Z(4430), discovered by Belle, and demonstrated that it must have a minimal quark content of \"cd\".\n\nIn July 2015, LHCb announced the discovery of two particles, named and , which must have minimal quark content \"cuud\", making them pentaquarks.\n\nThere are several exotic hadron candidates:\n\n"}
{"id": "44708", "url": "https://en.wikipedia.org/wiki?curid=44708", "title": "Ferroelectricity", "text": "Ferroelectricity\n\nFerroelectricity is a characteristic of certain materials that have a spontaneous electric polarization that can be reversed by the application of an external electric field. All ferroelectrics are pyroelectric, with the additional property that their natural electrical polarization is reversible. The term is used in analogy to ferromagnetism, in which a material exhibits a permanent magnetic moment. Ferromagnetism was already known when ferroelectricity was discovered in 1920 in Rochelle salt by Valasek. Thus, the prefix \"ferro\", meaning iron, was used to describe the property despite the fact that most ferroelectric materials do not contain iron.\n\nWhen most materials are polarized, the polarization induced, \"P\", is almost exactly proportional to the applied external electric field \"E\"; so the polarization is a linear function. This is called dielectric polarization (see figure). Some materials, known as paraelectric materials, show a more enhanced nonlinear polarization (see figure). The electric permittivity, corresponding to the slope of the polarization curve, is not constant as in dielectrics but is a function of the external electric field.\n\nIn addition to being nonlinear, ferroelectric materials demonstrate a spontaneous nonzero polarization (after entrainment, see figure) even when the applied field \"E\" is zero. The distinguishing feature of ferroelectrics is that the spontaneous polarization can be \"reversed\" by a suitably strong applied electric field in the opposite direction; the polarization is therefore dependent not only on the current electric field but also on its history, yielding a hysteresis loop. They are called ferroelectrics by analogy to ferromagnetic materials, which have spontaneous magnetization and exhibit similar hysteresis loops.\n\nTypically, materials demonstrate ferroelectricity only below a certain phase transition temperature, called the Curie temperature (\"T\") and are paraelectric above this temperature: the spontaneous polarization vanishes, and the ferroelectric crystal transforms into the paraelectric state. Many ferroelectrics lose their piezoelectric properties above Tc completely, because their paraelectric phase has a centrosymmetric crystal structure.\n\nThe nonlinear nature of ferroelectric materials can be used to make capacitors with tunable capacitance. Typically, a ferroelectric capacitor simply consists of a pair of electrodes sandwiching a layer of ferroelectric material. The permittivity of ferroelectrics is not only tunable but commonly also very high in absolute value, especially when close to the phase transition temperature. Because of this, ferroelectric capacitors are small in physical size compared to dielectric (non-tunable) capacitors of similar capacitance.\n\nThe spontaneous polarization of ferroelectric materials implies a hysteresis effect which can be used as a memory function, and ferroelectric capacitors are indeed used to make ferroelectric RAM for computers and RFID cards. In these applications thin films of ferroelectric materials are typically used, as this allows the field required to switch the polarization to be achieved with a moderate voltage. However, when using thin films a great deal of attention needs to be paid to the interfaces, electrodes and sample quality for devices to work reliably.\n\nFerroelectric materials are required by symmetry considerations to be also piezoelectric and pyroelectric. The combined properties of memory, piezoelectricity, and pyroelectricity make ferroelectric capacitors very useful, e.g. for sensor applications. Ferroelectric capacitors are used in medical ultrasound machines (the capacitors generate and then listen for the ultrasound ping used to image the internal organs of a body), high quality infrared cameras (the infrared image is projected onto a two dimensional array of ferroelectric capacitors capable of detecting temperature differences as small as millionths of a degree Celsius), fire sensors, sonar, vibration sensors, and even fuel injectors on diesel engines.\n\nAnother idea of recent interest is the \"ferroelectric tunnel junction\" (\"FTJ\") in which a contact is made up by nanometer-thick ferroelectric film placed between metal electrodes. The thickness of the ferroelectric layer is small enough to allow tunneling of electrons. The piezoelectric and interface effects as well as the depolarization field may lead to a giant electroresistance (GER) switching effect.\n\nYet another hot topic is multiferroics, where researchers are looking for ways to couple magnetic and ferroelectric ordering within a material or heterostructure; there are several recent reviews on this topic.\n\nThe internal electric dipoles of a ferroelectric material are coupled to the material lattice so anything that changes the lattice will change the strength of the dipoles (in other words, a change in the spontaneous polarization). The change in the spontaneous polarization results in a change in the surface charge. This can cause current flow in the case of a ferroelectric capacitor even without the presence of an external voltage across the capacitor. Two stimuli that will change the lattice dimensions of a material are force and temperature. The generation of a surface charge in response to the application of an external stress to a material is called piezoelectricity. A change in the spontaneous polarization of a material in response to a change in temperature is called pyroelectricity.\n\nGenerally, there are 230 space groups among which 32 crystalline classes can be found in crystals. There are 21 non-centrosymmetric classes, within which 20 are piezoelectric. Among the piezoelectric classes, 10 have a spontaneous electric polarization, that varies with the temperature, therefore they are pyroelectric. Among pyroelectric materials, some of them are ferroelectric.\n\nFerroelectric phase transitions are often characterized as either displacive (such as BaTiO) or order-disorder (such as NaNO), though often phase transitions will demonstrate elements of both behaviors. In barium titanate, a typical ferroelectric of the displacive type, the transition can be understood in terms of a polarization catastrophe, in which, if an ion is displaced from equilibrium slightly, the force from the local electric fields due to the ions in the crystal increases faster than the elastic-restoring forces. This leads to an asymmetrical shift in the equilibrium ion positions and hence to a permanent dipole moment. The ionic displacement in barium titanate concerns the relative position of the titanium ion within the oxygen octahedral cage. In lead titanate, another key ferroelectric material, although the structure is rather similar to barium titanate the driving force for ferroelectricity is more complex with interactions between the lead and oxygen ions also playing an important role. In an order-disorder ferroelectric, there is a dipole moment in each unit cell, but at high temperatures they are pointing in random directions. Upon lowering the temperature and going through the phase transition, the dipoles order, all pointing in the same direction within a domain.\n\nAn important ferroelectric material for applications is lead zirconate titanate (PZT), which is part of the solid solution formed between ferroelectric lead titanate and anti-ferroelectric lead zirconate. Different compositions are used for different applications; for memory applications, PZT closer in composition to lead titanate is preferred, whereas piezoelectric applications make use of the diverging piezoelectric coefficients associated with the morphotropic phase boundary that is found close to the 50/50 composition.\n\nFerroelectric crystals often show several transition temperatures and domain structure hysteresis, much as do ferromagnetic crystals. The nature of the phase transition in some ferroelectric crystals is still not well understood.\n\nIn 1974 R.B. Meyer used symmetry arguments to predict ferroelectric liquid crystals, and the prediction could immediately be verified by several observations of behavior connected to ferroelectricity in smectic liquid-crystal phases that are chiral and tilted. The technology allows the building of flat-screen monitors. Mass production between 1994 and 1999 was carried out by Canon. Ferroelectric liquid crystals are used in production of reflective LCoS.\n\nIn 2010 David Field found that prosaic films of chemicals such as nitrous oxide or propane exhibited ferroelectric properties. This new class of ferroelectric materials exhibit \"spontelectric\" properties, and may have wide-ranging applications in device and nano-technology and also influence the electrical nature of dust in the interstellar medium.\n\nOther ferroelectric materials used include triglycine sulfate, polyvinylidene fluoride (PVDF) and lithium tantalate.\nIt should be possible to produce materials which combine both ferroelectric and metallic properties simultaneously, at room temperature.. According to research published in 2018 in \"Nature Communications\", scientists were able to produce a \"two-dimensional\" sheet of material which was both \"ferroelectric\" (had a polar crystal structure) and which conducted electricity.\n\nAn introduction to Landau theory can be found here.\nBased on Ginzburg–Landau theory, the free energy of a ferroelectric material, in the absence of an electric field and applied stress may be written as a Taylor expansion in terms of the order parameter, \"P\". If a sixth order expansion is used (i.e. 8th order and higher terms truncated), the free energy is given by:\n\nwhere P, P, and P are the components of the polarization vector in the x, y, and z directions respectively, and the coefficients, formula_2 must be consistent with the crystal symmetry. To investigate domain formation and other phenomena in ferroelectrics, these equations are often used in the context of a phase field model. Typically, this involves adding a gradient term, an electrostatic term and an elastic term to the free energy. The equations are then discretized onto a grid using the finite difference method and solved subject to the constraints of Gauss's law and Linear elasticity.\n\nIn all known ferroelectrics, formula_3 and formula_4. These coefficients may be obtained experimentally or from ab-initio simulations. For ferroelectrics with a first order phase transition, formula_5, whereas formula_6 for a second order phase transition.\n\nThe spontaneous polarization, \"P\" of a ferroelectric for a cubic to tetragonal phase transition may be obtained by considering the 1D expression of the free energy which is:\n\nThis free energy has the shape of a double well potential with two free energy minima at formula_8, where \"P\" is the spontaneous polarization. At these two minima, the derivative of the free energy is zero, i.e.:\n\nSince \"P = 0\" corresponds to a free energy maxima in the ferroelectric phase, the spontaneous polarization, \"P\", is obtained from the solution of the equation:\n\nwhich is:\n\nand elimination of solutions yielding a negative square root (for either the first or second order phase transitions) gives:\n\nIf formula_14, using the same approach as above, the spontaneous polarization may be obtained as:\n\nThe hysteresis loop (P versus E) may be obtained from the free energy expansion by adding another electrostatic term, E P, as follows:\n\nPlotting E as a function of P and reflecting the graph about the 45 degree line gives an 'S' shaped curve. The central part of the 'S' corresponds to a free energy local maximum (since formula_19 ). Elimination of this region, and connection of the top and bottom portions of the 'S' curve by vertical lines at the discontinuities gives the hysteresis loop.\n\n\nPhysics\n\nLists\n\n"}
{"id": "41360236", "url": "https://en.wikipedia.org/wiki?curid=41360236", "title": "Filigranology", "text": "Filigranology\n\nFiligranology is the study of watermarks. It is usually pursued in order to discover information about the date and origin for a paper based piece of written work or a piece of art. There are several catalogues of watermarks - most notably C. M. Briquet's, \"Les Filigranes\" (1907) - including illustrations of many watermarks from dated documents to aid those wishing to undertake this kind of research.\n"}
{"id": "1996536", "url": "https://en.wikipedia.org/wiki?curid=1996536", "title": "Grain boundary", "text": "Grain boundary\n\nA grain boundary is the interface between two grains, or crystallites, in a polycrystalline material. Grain boundaries are 2D defects in the crystal structure, and tend to decrease the electrical and thermal conductivity of the material. Most grain boundaries are preferred sites for the onset of corrosion and for the precipitation of new phases from the solid. They are also important to many of the mechanisms of creep. On the other hand, grain boundaries disrupt the motion of dislocations through a material, so reducing crystallite size is a common way to improve mechanical strength, as described by the Hall–Petch relationship. The study of grain boundaries and their effects on the mechanical, electronic and other properties of materials forms an important topic in materials science.\n\nIt is convenient to categorize grain boundaries according to the extent of misorientation between the two grains. \"Low-angle grain boundaries\" or \"subgrain boundaries\" are those with a misorientation less than about 15 degrees. Generally speaking they are composed of an array of dislocations and their properties and structure are a function of the misorientation. In contrast the properties of \"high-angle grain boundaries\", whose misorientation is greater than about 15 degrees (the transition angle varies from 10–15 degrees depending on the material), are normally found to be independent of the misorientation. However, there are 'special boundaries' at particular orientations whose interfacial energies are markedly lower than those of general high-angle grain boundaries.\nThe simplest boundary is that of a tilt boundary where the rotation axis is parallel to the boundary plane. This boundary can be conceived as forming from a single, contiguous crystallite or grain which is gradually bent by some external force. The energy associated with the elastic bending of the lattice can be reduced by inserting a dislocation, which is essentially a half-plane of atoms that act like a wedge, that creates a permanent misorientation between the two sides. As the grain is bent further, more and more dislocations must be introduced to accommodate the deformation resulting in a growing wall of dislocations – a low-angle boundary. The grain can now be considered to have split into two sub-grains of related crystallography but notably different orientations.\n\nAn alternative is a twist boundary where the misorientation occurs around an axis that is perpendicular to the boundary plane. This type of boundary incorporates two sets of screw dislocations. If the Burgers vectors of the dislocations are orthogonal, then the dislocations do not strongly interact and form a square network. In other cases, the dislocations may interact to form a more complex hexagonal structure.\n\nThese concepts of tilt and twist boundaries represent somewhat idealized cases. The majority of boundaries are of a mixed type, containing dislocations of different types and Burgers vectors, in order to create the best fit between the neighboring grains.\n\nIf the dislocations in the boundary remain isolated and distinct, the boundary can be considered to be low-angle. If deformation continues, the density of dislocations will increase and so reduce the spacing between neighboring dislocations. Eventually, the cores of the dislocations will begin to overlap and the ordered nature of the boundary will begin to break down. At this point the boundary can be considered to be high-angle and the original grain to have separated into two entirely separate grains.\n\nIn comparison to low-angle grain boundaries, high-angle boundaries are considerably more disordered, with large areas of poor fit and a comparatively open structure. Indeed, they were originally thought to be some form of amorphous or even liquid layer between the grains. However, this model could not explain the observed strength of grain boundaries and, after the invention of electron microscopy, direct evidence of the grain structure meant the hypothesis had to be discarded. It is now accepted that a boundary consists of structural units which depend on both the misorientation of the two grains and the plane of the interface. The types of structural unit that exist can be related to the concept of the \"coincidence site lattice\", in which repeated units are formed from points where the two misoriented lattices happen to coincide.\n\nIn coincident site lattice (CSL) theory, the degree of fit (Σ) between the structures of the two grains is described by the reciprocal of the ratio of coincidence sites to the total number of sites. In this framework, it is possible to draw the lattice for the 2 grains and count the number of atoms that are shared (coincidence sites), and the total number of atoms on the boundary (total number of site). For example, when Σ=3 there will be one atom of each three that will be shared between the two lattices. Thus a boundary with high Σ might be expected to have a higher energy than one with low Σ. Low-angle boundaries, where the distortion is entirely accommodated by dislocations, are Σ1. Some other low-Σ boundaries have special properties, especially when the boundary plane is one that contains a high density of coincident sites. Examples include coherent twin boundaries (e.g., Σ3) and high-mobility boundaries in FCC materials (e.g., Σ7). Deviations from the ideal CSL orientation may be accommodated by local atomic relaxation or the inclusion of dislocations at the boundary.\n\nA boundary can be described by the orientation of the boundary to the two grains and the 3-D rotation required to bring the grains into coincidence. Thus a boundary has 5 macroscopic degrees of freedom. However, it is common to describe a boundary only as the orientation relationship of the neighbouring grains. Generally, the convenience of ignoring the boundary plane orientation, which is very difficult to determine, outweighs the reduced information.\nThe relative orientation of the two grains is described using the rotation matrix:\n\nUsing this system the rotation angle θ is:\n\nwhile the direction [uvw] of the rotation axis is:\n\nThe nature of the crystallography involved limits the misorientation of the boundary. A completely random polycrystal, with no texture, thus has a characteristic distribution of boundary misorientations (see figure). However, such cases are rare and most materials will deviate from this ideal to a greater or lesser degree.\n\nThe energy of a low-angle boundary is dependent on the degree of misorientation between the neighbouring grains up to the transition to high-angle status. In the case of simple \"tilt boundaries\" the energy of a boundary made up of dislocations with Burgers vector \"b\" and spacing \"h\" is predicted by the Read-Shockley equation:\n\nwhere:\n\nwith formula_8 is the shear modulus, formula_9 is Poisson's ratio, and formula_10 is the radius of the dislocation core. It can be seen that as the energy of the boundary increases the energy per dislocation decreases. Thus there is a driving force to produce fewer, more misoriented boundaries (i.e., grain growth).\n\nThe situation in high-angle boundaries is more complex. Although theory predicts that the energy will be a minimum for ideal CSL configurations, with deviations requiring dislocations and other energetic features, empirical measurements suggest the relationship is more complicated. Some predicted troughs in energy are found as expected while others missing or substantially reduced. \nSurveys of the available experimental data have indicated that simple relationships such as low formula_11 are misleading:\n\nIt is concluded that no general and useful criterion for low energy can be enshrined in a simple geometric framework. Any understanding of the variations of interfacial energy must take account of the atomic structure and the details of the bonding at the interface.\n\nThe excess volume is another important property in the characterization of grain boundaries. Excess volume was first proposed by Bishop in a private communication to Aaron and Bolling in 1972 . It describes how much expansion is induced by the presence of a GB and is thought that the degree and susceptibility of segregation is directly proportional to this. Despite the name the excess volume is actually a change in length, this is because of the 2D nature of GBs the length of interest is the expansion normal to the GB plane. The excess volume (formula_12) is defined in the following way,\n\nat constant temperature formula_14, pressure formula_15 and number of atoms formula_16. Although a rough linear relationship between GB energy and excess volume exists the orientations where this relationship is violated can behave significantly differently affecting mechanical and electrical properties.\n\nExperimental techniques have been developed which directly probe the excess volume and have been used to explore the properties of nanocrystalline copper and nickel. Theoretical methods have also been developed and are in good agreement. A key observation is that there is an inverse relationship with the bulk modulus meaning that the larger the bulk modulus (the ability to compress a material) the smaller the excess volume will be, there is also direct relationship with the lattice constant, this provides methodology to find materials with a desirable excess volume for a specific application.\n\nThe movement of grain boundaries (HAGB) has implications for recrystallization and grain growth while subgrain boundary (LAGB) movement strongly influences recovery and the nucleation of recrystallization.\n\nA boundary moves due to a pressure acting on it. It is generally assumed that the velocity is directly proportional to the pressure with the constant of proportionality being the mobility of the boundary. The mobility is strongly temperature dependent and often follows an Arrhenius type relationship:\n\nThe apparent activation energy (Q) may be related to the thermally activated atomistic processes that occur during boundary movement. However, there are several proposed mechanisms where the mobility will depend on the driving pressure and the assumed proportionality may break down.\n\nIt is generally accepted that the mobility of low-angle boundaries is much lower than that of high-angle boundaries. The following observations appear to hold true over a range of conditions:\n\nSince low-angle boundaries are composed of arrays of dislocations and their movement may be related to dislocation theory. The most likely mechanism, given the experimental data, is that of dislocation climb, rate limited by the diffusion of solute in the bulk.\n\nThe movement of high-angle boundaries occurs by the transfer of atoms between the neighbouring grains. The ease with which this can occur will depend on the structure of the boundary, itself dependent on the crystallography of the grains involved, impurity atoms and the temperature. It is possible that some form of diffusionless mechanism (akin to diffusionless phase transformations such as martensite) may operate in certain conditions. Some defects in the boundary, such as steps and ledges, may also offer alternative mechanisms for atomic transfer.\n\nSince a high-angle boundary is imperfectly packed compared to the normal lattice it has some amount of \"free space\" or \"free volume\" where solute atoms may possess a lower energy. As a result, a boundary may be associated with a \"solute atmosphere\" that will retard its movement. Only at higher velocities will the boundary be able to break free of its atmosphere and resume normal motion.\n\nBoth low- and high-angle boundaries are retarded by the presence of particles via the so-called Zener pinning effect. This effect is often exploited in commercial alloys to minimise or prevent recrystallization or grain growth during heat-treatment.\n\nGrain boundaries are the preferential site for segregation of impurities, which may form a thin layer with a different composition from the bulk. For example, a thin layer of silica, which also contains impurity cations, is often present in silicon nitride. These grain boundary phases are thermodynamically stable and can be considered as quasi-two-dimensional phase, which may undergo to transition, similar to those of bulk phases. In this case structure and chemistry abrupt changes are possible at a critical value of a thermodynamic parameter like temperature or pressure. This may strongly affect the macroscopic properties of the material, for example the electrical resistance or creep rates. Grain boundaries can be analyzed using equilibrium thermodynamics but cannot be considered as phases, because they do not satisfy Gibbs'definition: they are inhomogeneous, may have a gradient of structure, composition or properties. For this reasons they are defined as complexion: an interfacial material or stata that is in thermodynamic equilibrium with its abutting phases, with a finite and stable thickness (that is typically 2–20 Å). A complexion need the abutting phase to exist and its composition and structure need to be different from the abutting phase. Contrary to bulk phases, complexions also depend on the abutting phase. For example, silica rich amorphous layer present in SiN, is about 10 Å thick, but for special boundaries this equilibrium thickness is zero. Complexion can be grouped in 6 categories, according to their thickness: monolayer, bilayer, trilayer, nanolayer (with equilibrium thickness between 1 and 2 nm) and wetting. In the first cases the thickness of the layer will be constant; if extra material is present it will segregate at multiple grain junction, while in the last case there is no equilibrium thickness and this is determined by the amount of secondary phase present in the material. One example of grain boundary complexion transition is the passage from dry boundary to biltilayer in Au-doped Si, which is produced by the increase of Au.\n\nGrain boundaries can cause failure mechanically by embrittlement through solute segregation (see Hinkley Point A nuclear power station) but they also can detrimentally affect the electronic properties. In metal oxides it has been shown theoretically that at the grain boundaries in AlO and MgO the insulating properties can be significantly diminished. Using density functional theory computer simulations of grain boundaries have shown that the band gap can be reduced by up to 45%. In the case of metals grain boundaries increase the resistivity as the size of the grains relative to the mean free path of other scatters becomes significant.\n\n\n"}
{"id": "31827448", "url": "https://en.wikipedia.org/wiki?curid=31827448", "title": "Great Bakersfield Dust Storm of 1977", "text": "Great Bakersfield Dust Storm of 1977\n\nThe Great Bakersfield Dust Storm of 1977 (also known as the Southern San Joaquin Valley Dust Storm) was a severe dust storm in the Southern San Joaquin Valley, California. It started in the late evening on December 19, 1977 and ended in the afternoon of December 21. It resulted in 3 deaths and $40 million in damages (does not include subsequent agricultural losses).\n\nDecember 19 started like most cold winter days. At 11:00 pm, temperature was with a light northwestern wind. By 11:30 pm, the weather had started to change. The temperature began to warm up and the wind had shifted direction. It also started to grow in strength and dust started to restrict visibility. In the very early morning, the next day, power was sporadic throughout the city. The wind was stronger, but people went to work and school was still in session. However, by 9:00 am, school was cancelled. Parents were requested to pick up their children because of the concern that high-profile buses could blow over.\n\nBy late morning, the wind was blowing hard, and sounded like a loud roar. Enough dust was in the air that it blocked out the sun. Roads into and out of the southern valley were closed. Only one TV station and two or three AM radio stations continued to have power and were able to broadcast. Also, since Bakersfield did not have a direct feed to national news broadcast, no one outside of the area knew the severity of the situation.\n\nWind continued to blow throughout the afternoon and evening. Swamp coolers were blown off the roofs of buildings. Windows were shattering and store signs were blowing in the wind. It was described as if a twister was unrolled and blew up the valley in a sheet. Farther north in the valley, Fresno was having a typical December rain. When the dust reached the rain, it turned into mud. It fell in sheets from the sky.\n\nPeople awoke the next morning to a dark sky. The wind had blown throughout the night and was continuing in the morning. Schools remained closed that day. By the afternoon, the wind began to subside, and the air started to clear.\n\nBy the afternoon of December 21, people began to see the damage from the dust storm. The result was devastating. Trees, fences, and swamp coolers had blown down throughout the region. Below grade freeways, canals, and creeks were buried. Dirt had piled up on the south side of the buildings. Dust had seeped into cracks and crevices of buildings, filling the interior with a layer of dust. After several days, the roads were reopened and news reports started leaving the valley. People also started digging out and cleaning up. By spring, not all of the canals and creeks were cleared. As a result, the rain caused them to flood.\n\nThe storm resulted in three deaths and $40 million in damage. Over 25 million cubic feet of topsoil from grazing land alone was moved. Wind was measured at in Arvin (southeast of Bakersfield). In the foothills, the wind was measured at . In the mountain passes, it was .\n\nThe great dust storm was caused by many different events. There had been a drought in the region for several years, which caused the ground to be dry. Cotton had recently been plowed under (end of the season) but the winter crop had not taken root yet. This caused the soil to be loose.\n\nThe high winds were also caused by a series of events. Over the Great Basin, which is located in Nevada and Idaho, was very cold and heavy air (high pressure). A very strong low pressure system was approaching the northwest coast of California. A mercury reading of 0.10 inches of pressure gradient force (difference of pressure between two points) is typically needed for a 10 to 12 miles per hour (16 to 19 km/h) northwestern wind. That day, the reading was 0.60 inches.\n"}
{"id": "26419837", "url": "https://en.wikipedia.org/wiki?curid=26419837", "title": "Green Mountain Wind Energy Center", "text": "Green Mountain Wind Energy Center\n\nThe Green Mountain Wind Energy Center was a wind power plant near Garrett, Somerset County, Pennsylvania with eight Nordex 1.3 MW turbines that began commercial operation in May 2000. This was the first commercial wind farm constructed in Pennsylvania. The wind farm had a combined total nameplate capacity of 10.4 MW and the potential to produce about 27,000 megawatt-hours of electricity annually, enough to power 3,300 homes, assuming a 30% capacity factor. The wind farm was developed by National Wind Power of the UK, now part of NPower Renewables, operated by NextEra Energy Resources, based in Florida. Energy from the wind farm was purchased and sold by Green Mountain Energy based in Texas, and was the first commercial wind farm for Green Mountain Energy. In 2016 the wind farm was dismantled and replaced with new battery technology.\n\n\n"}
{"id": "5716217", "url": "https://en.wikipedia.org/wiki?curid=5716217", "title": "Greigite", "text": "Greigite\n\nGreigite is an iron sulfide mineral with formula FeS (Iron(II,III) sulfide). It is the sulfur equivalent of the iron oxide magnetite (FeO). It was first described in 1964 for an occurrence in San Bernardino County, California, and named after the mineralogist and physical chemist Joseph W. Greig (1895–1977).\n\nIt occurs in lacustrine sediments with clays, silts and arkosic sand often in varved sulfide rich clays. It is also found in hydrothermal veins. Greigite is formed by magnetotactic bacteria and sulfate-reducing bacteria. Greigite has also been identified in the sclerites of scaly-foot gastropods. \nThe mineral typically appears as microscopic (< 0.03 mm) isometric hexoctahedral crystals and as minute sooty masses. Association minerals include montmorillonite, chlorite, calcite, colemanite, veatchite, sphalerite, pyrite, marcasite, galena and dolomite.\n\nCommon impurities include Cu, Ni, Zn, Mn, Cr, Sb and As. Ni impurities are of particular interest because the structural similarity between Ni-doped greigite and the (Fe, Ni)S clusters present in biological enzymes has led to suggestions that greigite minerals could have acted as catalysts for the origin of life. In particular, the cubic FeS unit of greigite is found in the FeS thiocubane units of proteins of relevance to the acetyl-CoA pathway.\n\nGreigite has the spinel structure. The crystallographic unit cell is cubic, with space group Fd3m. The S anions form a cubic close-packed lattice, and the Fe cations occupy both tetrahedral and octahedral sites.\n\nLike the related oxide magnetite (FeO), greigite is ferrimagnetic, with the spin magnetic moments of the Fe cations in the tetrahedral sites oriented in the opposite direction as those in the octahedral sites, and a net magnetization. It is a mixed-valence compound, featuring both Fe(II) and Fe(III) centers in a 1:2 ratio. Both metal sites have high spin quantum numbers. The electronic structure of greigite is that of a half metal.\n"}
{"id": "7060924", "url": "https://en.wikipedia.org/wiki?curid=7060924", "title": "Hankinson's equation", "text": "Hankinson's equation\n\nHankinson's equation (also called Hankinson's formula or Hankinson's criterion) is a mathematical relationship for predicting the off-axis uniaxial compressive strength of wood. The formula can also be used to compute the fiber stress or the stress wave velocity at the elastic limit as a function of grain angle in wood. For a wood that has uniaxial compressive strengths of formula_1 parallel to the grain and formula_2 perpendicular to the grain, Hankinson's equation predicts that the uniaxial compressive strength of the wood in a direction at an angle formula_3 to the grain is given by\n\nEven though the original relation was based on studies of spruce, Hankinson's equation has been found to be remarkably accurate for many other types of wood. A generalized form of the Hankinson formula has also been used for predicting the uniaxial tensile strength of wood at an angle to the grain. This formula has the form\nwhere the exponent formula_6 can take values between 1.5 and 2.\n\nThe stress wave velocity at angle formula_3 to the grain at the elastic limit can similarly be obtained from the Hankinson formula\nwhere formula_9 is the velocity parallel to the grain, formula_10 is the velocity perpendicular to the grain and formula_3 is the grain angle.\n\n\n"}
{"id": "28426846", "url": "https://en.wikipedia.org/wiki?curid=28426846", "title": "Heteronuclear molecule", "text": "Heteronuclear molecule\n\nHeteronuclear molecules, or heteronuclear species, are molecules composed of more than one type of element, for example, HCl.\nIn heteronuclear molecules e.g. HCl where bonded atoms are of different elements, the molecules may become polar due to electronegativity differences.\n\n"}
{"id": "157736", "url": "https://en.wikipedia.org/wiki?curid=157736", "title": "Hybrid vehicle", "text": "Hybrid vehicle\n\nA hybrid vehicle uses two or more distinct types of power, such as internal combustion engine to drive an electric generator that powers an electric motor, e.g. in diesel-electric trains using diesel engines to drive an electric generator that powers an electric motor, and submarines that use diesels when surfaced and batteries when submerged. Other means to store energy include pressurized fluid in hydraulic hybrids.\n\nThe basic principle with hybrid vehicles is that the different motors work better at different speeds; the electric motor is more efficient at producing torque, or turning power, and the combustion engine is better for maintaining high speed (better than typical electric motor). Switching from one to the other at the proper time while speeding up yields a win-win in terms of energy efficiency, as such that translates into greater fuel efficiency, for example.\nPower sources for hybrid vehicles include:\n\nMopeds, electric bicycles, and even electric kick scooters are a simple form of a hybrid, powered by an internal combustion engine or electric motor and the rider's muscles. Early prototype motorcycles in the late 19th century used the same principle.\n\n\nThe first published prototype of an SHB is by Augustus Kinzel (US Patent 3'884'317) in 1975. In 1994 Bernie Macdonalds conceived the Electrilite SHB with power electronics allowing regenerative braking and pedaling while stationary. In 1995 Thomas Muller designed and built a \"Fahrrad mit elektromagnetischem Antrieb\" for his 1995 diploma thesis. In 1996 Jürg Blatter and Andreas Fuchs of Berne University of Applied Sciences built an SHB and in 1998 modified a Leitra tricycle (European patent EP 1165188). Until 2005 they built several prototype SH tricycles and quadricycles. In 1999 Harald Kutzke described an \"active bicycle\": the aim is to approach the ideal bicycle weighing nothing and having no drag by electronic compensation.\n\n\nA SHEPB prototype made by David Kitson in Australia in 2014 used a lightweight brushless DC electric motor from an aerial drone and small hand-tool sized internal combustion engine, and a 3D printed drive system and lightweight housing, altogether weighing less than 4.5 kg. Active cooling keeps plastic parts from softening. The prototype uses a regular electric bicycle charge port.\n\nHybrid power trains use diesel-electric or turbo-electric to power railway locomotives, buses, heavy goods vehicles, mobile hydraulic machinery, and ships. A diesel/turbine engine drives an electric generator or hydraulic pump, which powers electric/hydraulic motor(s) - strictly an electric/hydraulic transmission (not a hybrid), unless it can accept power from outside. With large vehicles conversion losses decrease, and the advantages in distributing power through wires or pipes rather than mechanical elements become more prominent, especially when powering multiple drives — e.g. driven wheels or propellers. Until recently most heavy vehicles had little secondary energy storage, e.g. batteries/hydraulic accumulators — excepting non-nuclear submarines, one of the oldest production hybrids, running on diesels while surfaced and batteries when submerged. Both series and parallel setups were used in WW2 submarines.\n\nEurope<br>\nThe new Autorail à grande capacité (AGC or high-capacity railcar) built by the Canadian company Bombardier for service in France is diesel/electric motors, using 1500 or 25000 V on different rail systems. It was tested in Rotterdam, the Netherlands with Railfeeding, a Genesse and Wyoming company.\n\nChina<br>\nThe First Hybrid Evaluating locomotive was designed by rail research center MATRAI in 1999 and built in 2000. It was a G12 locomotive upgraded with batteries, a 200 kW diesel generator and 4 AC motors.\n\nJapan<br>\nJapan's first hybrid train with significant energy storage is the KiHa E200, with roof-mounted lithium ion batteries.\n\nIndia<br>\nIndian railway launched one of its kind CNG-Diesel hybrid trains in January 2015. The train has a 1400 hp engine which uses fumigation technology.The first of these train is set to run on the 81 km long Rewari-Rohtak route. CNG is less-polluting alternative for diesel and petrol and is popular as an alternative fuel in India. Already many transport vehicles such as auto-rickshaws and buses run on CNG fuel.\n\nNorth America<br>\nIn the US, General Electric made a locomotive with sodium - nickel chloride (Na-NiCl) battery storage. They expect ≥10% fuel economy.\n\nVariant diesel electric locomotive include the Green Goat (GG) and Green Kid (GK) switching/yard engines built by Canada's Railpower Technologies, with lead acid (Pba) batteries and 1000 to 2000 hp electric motors, and a new clean burning ~160 hp diesel generator. No fuel is wasted for idling — ~60–85% of the time for these type locomotives. It is unclear if regenerative braking is used; but in principle it is easily utilized.\n\nSince these engines typically need extra weight for traction purposes anyway the battery pack's weight is a negligible penalty. The diesel generator and batteries are normally built on an existing \"retired\" \"yard\" locomotive's frame. The existing motors and running gear are all rebuilt and reused. Fuel savings of 40–60% and up to 80% pollution reductions are claimed over a \"typical\" older switching/yard engine. The advantages hybrid cars have for frequent starts and stops and idle periods apply to typical switching yard use. \"Green Goat\" locomotives have been purchased by Canadian Pacific Railway, BNSF Railway, Kansas City Southern Railway, and Union Pacific Railroad among others.\n\nRailpower Technologies engineers working with TSI Terminal Systems are testing a hybrid diesel electric power unit with battery storage for use in Rubber Tyred Gantry (RTG) cranes. RTG cranes are typically used for loading and unloading shipping containers onto trains or trucks in ports and container storage yards. The energy used to lift the containers can be partially regained when they are lowered. Diesel fuel and emission reductions of 50–70% are predicted by Railpower engineers. First systems are expected to be operational in 2007.\n\nHybrid systems are coming into use for trucks, buses and other heavy highway vehicles. Small fleet sizes and installation costs are compensated by fuel savings. With advances such as higher capacity, lowered battery cost etc. Toyota, Ford, GM and others are introducing hybrid pickups and SUVs. Kenworth Truck Company recently introduced the Kenworth T270 Class 6 that for city usage seems to be competitive. FedEx and others are investing in hybrid delivery vehicles — particularly for city use where hybrid technology may pay off first. FedEx is trialling two delivery trucks with Wrightspeed electric motors and diesel generators; the retrofit kits are claimed to pay for themselves in a few years. The diesel engines run at a constant RPM for peak efficiency.\n\nIn 1978 students at Minneapolis, Minnesota's Hennepin Vocational Technical Center, converted a Volkswagen Beetle to a petro-hydraulic hybrid with off-the shelf components. A car rated at 32 mpg was returning 75 mpg with the 60 hp engine replaced by a 16 hp engine, and reached 70 mph.\nIn the 1990s, engineers at EPA’s National Vehicle and Fuel Emissions Laboratory developed a petro-hydraulic powertrain for a typical American sedan car. The test car achieved over 80 mpg on combined EPA city/highway driving cycles. Acceleration was 0-60 mph in 8 seconds, using a 1.9 liter diesel engine. No lightweight materials were used. The EPA estimated that produced in high volumes the hydraulic components would add only $700 to the cost. Under EPA testing, a hydraulic hybrid Ford Expedition returned 32 mpg (7.4 L/100 km) City, and 22 mpg (11 L/100 km) highway. UPS currently has two trucks in service using this technology.\n\nSince 1985, the US military has been testing serial hybrid Humvees and have found them to deliver faster acceleration, a stealth mode with low thermal signature/ near silent operation, and greater fuel economy.\n\nShips with both mast-mounted sails and steam engines were an early form of hybrid vehicle. Another example is the diesel-electric submarine. This runs on batteries when submerged and the batteries can be re-charged by the diesel engine when the craft is on the surface.\n\nNewer hybrid ship-propulsion schemes include large towing kites manufactured by companies such as SkySails. Towing kites can fly at heights several times higher than the tallest ship masts, capturing stronger and steadier winds.\n\nThe Boeing Fuel Cell Demonstrator Airplane has a Proton Exchange Membrane (PEM) fuel cell/lithium-ion battery hybrid system to power an electric motor, which is coupled to a conventional propeller. The fuel cell provides all power for the cruise phase of flight. During takeoff and climb, the flight segment that requires the most power, the system draws on lightweight lithium-ion batteries.\n\nThe demonstrator aircraft is a Dimona motor glider, built by Diamond Aircraft Industries of Austria, which also carried out structural modifications to the aircraft. With a wing span of , the airplane will be able to cruise at about on power from the fuel cell.\n\nHybrid FanWings have been designed. A FanWing is created by two engines with the capability to autorotate and landing like a helicopter.\n\nWhen the term \"hybrid vehicle\" is used, it most often refers to a Hybrid electric vehicle. These encompass such vehicles as the Saturn Vue, Toyota Prius, Toyota Yaris, Toyota Camry Hybrid, Ford Escape Hybrid, Toyota Highlander Hybrid, Honda Insight, Honda Civic Hybrid, Lexus RX 400h and 450h, Hyundai Ioniq and others. A petroleum-electric hybrid most commonly uses internal combustion engines (using a variety of fuels, generally gasoline or Diesel engines) and electric motors to power the vehicle. The energy is stored in the fuel of the internal combustion engine and an electric battery set. There are many types of petroleum-electric hybrid drivetrains, from Full hybrid to Mild hybrid, which offer varying advantages and disadvantages.\n\nWilliam H. Patton filed a patent application for a gasoline-electric hybrid rail-car propulsion system in early 1889, and for a similar hybrid boat propulsion system in mid 1889. There is no evidence that his hybrid boat met with any success, but he built a prototype hybrid tram and sold a small hybrid locomotive.\n\nIn 1899, Henri Pieper developed the world's first petro-electric hybrid automobile. In 1900, Ferdinand Porsche developed a series-hybrid using two motor-in-wheel-hub arrangements with an internal combustion generator set providing the electric power; Porsche's hybrid set two speed records.\nWhile liquid fuel/electric hybrids date back to the late 19th century, the braking regenerative hybrid was invented by David Arthurs, an electrical engineer from Springdale, Arkansas in 1978–79. His home-converted Opel GT was reported to return as much as 75 mpg with plans still sold to this original design, and the \"Mother Earth News\" modified version on their website.\n\nThe plug-in-electric-vehicle (PEV) is becoming more and more common. It has the range needed in locations where there are wide gaps with no services. The batteries can be plugged into house (mains) electricity for charging, as well being charged while the engine is running.\n\nSome battery electric vehicles (BEVs) can be recharged while the user drives. Such a vehicle establishes contact with an electrified rail, plate or overhead wires on the highway via an attached conducting wheel or other similar mechanism (see Conduit current collection). The BEV's batteries are recharged by this process—on the highway—and can then be used normally on other roads until the battery is discharged. For example, some of the battery-electric locomotives used for maintenance trains on the London Underground are capable of this mode of operation.\n\nDeveloping a BEV infrastructure would provide the advantage of virtually unrestricted highway range. Since many destinations are within 100 km of a major highway, BEV technology could reduce the need for expensive battery systems. Unfortunately, private use of the existing electrical system is almost universally prohibited. Besides, the technology for such electrical infrastructure is largely outdated and, outside some cities, not widely distributed (see Conduit current collection, trams, electric rail, trolleys, third rail). Updating the required electrical and infrastructure costs could perhaps be funded by toll revenue or by dedicated transportation taxes.\n\nIn addition to vehicles that use two or more different devices for propulsion, some also consider vehicles that use distinct energy sources or input types (\"fuels\") using the same engine to be hybrids, although to avoid confusion with hybrids as described above and to use correctly the terms, these are perhaps more correctly described as dual mode vehicles:\n\nHydraulic hybrid and pneumatic hybrid vehicles use an engine to charge a pressure accumulator to drive the wheels via hydraulic (liquid) or pneumatic (compressed air) drive units. In most cases the engine is detached from the drivetrain, serving solely to charge the energy accumulator. The transmission is seamless. Regenerative braking can be used to recover some of the supplied drive energy back into the accumulator.\n\nA French company, MDI, has designed and has running models of a petro-air hybrid engine car. The system does not use air motors to drive the vehicle, being directly driven by a hybrid engine. The engine uses a mixture of compressed air and gasoline injected into the cylinders. A key aspect of the hybrid engine is the \"active chamber\", which is a compartment heating air via fuel doubling the energy output. Tata Motors of India assessed the design phase towards full production for the Indian market and moved into \"completing detailed development of the compressed air engine into specific vehicle and stationary applications\".\n\nPetro-hydraulic configurations have been common in trains and heavy vehicles for decades. The auto industry recently focused on this hybrid configuration as it now shows promise for introduction into smaller vehicles.\n\nIn petro-hydraulic hybrids, the energy recovery rate is high and therefore the system is more efficient than electric battery charged hybrids using the current electric battery technology, demonstrating a 60% to 70% increase in energy economy in US Environmental Protection Agency (EPA) testing. The charging engine needs only to be sized for average usage with acceleration bursts using the stored energy in the hydraulic accumulator, which is charged when in low energy demanding vehicle operation. The charging engine runs at optimum speed and load for efficiency and longevity. Under tests undertaken by the US Environmental Protection Agency (EPA), a hydraulic hybrid Ford Expedition returned City, and highway. UPS currently has two trucks in service using this technology.\n\nAlthough petro-hydraulic hybrid technology has been known for decades, and used in trains and very large construction vehicles, high costs of the equipment precluded the systems from lighter trucks and cars. In the modern sense an experiment proved the viability of small petro-hydraulic hybrid road vehicles in 1978. A group of students at Minneapolis, Minnesota's Hennepin Vocational Technical Center, converted a Volkswagen Beetle car to run as a petro-hydraulic hybrid using off-the shelf components. A car rated at was returning with the 60 hp engine replaced by a 16 hp engine. The experimental car reached .\n\nIn the 1990s, a team of engineers working at EPA’s National Vehicle and Fuel Emissions Laboratory succeeded in developing a revolutionary type of petro-hydraulic hybrid powertrain that would propel a typical American sedan car. The test car achieved over 80 mpg on combined EPA city/highway driving cycles. Acceleration was 0-60 mph in 8 seconds, using a 1.9 liter diesel engine. No lightweight materials were used. The EPA estimated that produced in high volumes the hydraulic components would add only $700 to the base cost of the vehicle.\n\nThe petro-hydraulic hybrid system has faster and more efficient charge/discharge cycling than petro-electric hybrids and is also cheaper to build. The accumulator vessel size dictates total energy storage capacity and may require more space than an electric battery set. Any vehicle space consumed by a larger size of accumulator vessel may be offset by the need for a smaller sized charging engine, in HP and physical size.\n\nResearch is underway in large corporations and small companies. Focus has now switched to smaller vehicles. The system components were expensive which precluded installation in smaller trucks and cars. A drawback was that the power driving motors were not efficient enough at part load. A British company (Artemis Intelligent Power) made a breakthrough introducing an electronically controlled hydraulic motor/pump, the Digital Displacement® motor/pump. The pump is highly efficient at all speed ranges and loads, giving feasibility to small applications of petro-hydraulic hybrids. The company converted a BMW car as a test bed to prove viability. The BMW 530i, gave double the mpg in city driving compared to the standard car. This test was using the standard 3,000 cc engine, with a smaller engine the figures would have been more impressive. The design of petro-hydraulic hybrids using well sized accumulators allows downsizing an engine to average power usage, not peak power usage. Peak power is provided by the energy stored in the accumulator. A smaller more efficient constant speed engine reduces weight and liberates space for a larger accumulator.\n\nCurrent vehicle bodies are designed around the mechanicals of existing engine/transmission setups. It is restrictive and far from ideal to install petro-hydraulic mechanicals into existing bodies not designed for hydraulic setups. One research project's goal is to create a blank paper design new car, to maximize the packaging of petro-hydraulic hybrid components in the vehicle. All bulky hydraulic components are integrated into the chassis of the car. One design has claimed to return 130 mpg in tests by using a large hydraulic accumulator which is also the structural chassis of the car. The small hydraulic driving motors are incorporated within the wheel hubs driving the wheels and reversing to claw-back kinetic braking energy. The hub motors eliminates the need for friction brakes, mechanical transmissions, drive shafts and U joints, reducing costs and weight. Hydrostatic drive with no friction brakes are used in industrial vehicles. The aim is 170 mpg in average driving conditions. Energy created by shock absorbers and kinetic braking energy that normally would be wasted assists in charging the accumulator. A small fossil fuelled piston engine sized for average power use charges the accumulator. The accumulator is sized at running the car for 15 minutes when fully charged. The aim is a fully charged accumulator which will produce a 0-60 mph acceleration speed of under 5 seconds using four wheel drive.\n\nIn January 2011 industry giant Chrysler announced a partnership with the US Environmental Protection Agency (EPA) to design and develop an experimental petro-hydraulic hybrid powertrain suitable for use in large passenger cars. In 2012 an existing production minvan was adapted to the new hydraulic powertrain for assessment.\n\nPSA Peugeot Citroën exhibited an experimental \"Hybrid Air\" engine at the 2013 Geneva Motor Show. The vehicle uses nitrogen gas compressed by energy harvested from braking or deceleration to power a hydraulic drive which supplements power from its conventional gasoline engine. The hydraulic and electronic components were supplied by Robert Bosch GmbH. Mileage was estimated to be about on the Euro test cycle if installed in a Citroën C3 type of body. PSA Although the car was ready for production and was proven and feasible delivering the claimed results, Peugeot Citroën were unable to attract a major manufacturer to share the high development costs and are shelving the project until a partnership can be arranged.\n\nAnother form of hybrid vehicle are human power-electric vehicles. These include such vehicles as the Sinclair C5, Twike, electric bicycles, and electric skateboards.\n\nIn a parallel hybrid vehicle an electric motor and an internal combustion engine are coupled such that they can power the vehicle either individually or together. Most commonly the internal combustion engine, the electric motor and gear box are coupled by automatically controlled clutches. For electric driving the clutch between the internal combustion engine is open while the clutch to the gear box is engaged. While in combustion mode the engine and motor run at the same speed.\n\nThe first mass production parallel hybrid sold outside Japan was the 1st generation Honda Insight.\n\nThese types use a generally compact electric motor (usually <20 kW) to provide auto-stop/start features and to provide extra power assist during the acceleration, and to generate on the deceleration phase (aka regenerative braking).\n\nOn-road examples include Honda Civic Hybrid, Honda Insight 2nd generation, Honda CR-Z, Honda Accord Hybrid, Mercedes Benz S400 BlueHYBRID, BMW 7 Series hybrids, General Motors BAS Hybrids, Suzuki S-Cross, Suzuki Wagon R and Smart fortwo with micro hybrid drive.\n\nIn a power-split hybrid electric drive train there are two motors: a traction electric motor and an internal combustion engine. The power from these two motors can be shared to drive the wheels via a power split device, which is a simple planetary gear set. The ratio can be from 100% for the combustion engine to 100% for the traction electric motor, or anything in between, such as 40% for the electric motor and 60% for the combustion engine. The combustion engine can act as a generator charging the batteries.\n\nModern versions such as the Toyota Hybrid Synergy Drive have a second electric motor/generator connected to the planetary gear. In cooperation with the traction motor/generator and the power-split device this provides a continuously variable transmission.\n\nOn the open road, the primary power source is the internal combustion engine. When maximum power is required, for example to overtake, the traction electric motor is used to assist. This increases the available power for a short period, giving the effect of having a larger engine than actually installed. In most applications, the combustion engine is switched off when the car is slow or stationary thereby reducing curbside emissions.\n\nPassenger car installations include Toyota Prius, Ford Escape and Fusion, as well as Lexus RX400h, RX450h, GS450h, LS600h, and CT200h.\n\nA series- or serial-hybrid vehicle is driven by an electric motor, functioning as an electric vehicle while the battery pack energy supply is sufficient, with an engine tuned for running as a generator when the battery pack is insufficient. There is typically no mechanical connection between the engine and the wheels, and the primary purpose of the range extender is to charge the battery. Series-hybrids have also been referred to as extended range electric vehicle, range-extended electric vehicle, or electric vehicle-extended range (EREV/REEV/EVER).\n\nThe BMW i3 with Range Extender is a production series-hybrid. It operates as an electric vehicle until the battery charge is low, and then activates an engine-powered generator to maintain power, and is also available without the range extender. The Fisker Karma was the first series-hybrid production vehicle.\n\nWhen describing cars, the battery of a series-hybrid is usually charged by being plugged in - but a series-hybrid may also allow for a battery to only act as a buffer (and for regeneration purposes), and for the electric motor's power to be supplied constantly by a supporting engine. Series arrangements have been common in diesel-electric locomotives and ships. Ferdinand Porsche effectively invented this arrangement in speed-record-setting racing cars in the early 20th century, such as the Lohner-Porsche Mixte Hybrid. Porsche named his arrangement \"System Mixt\" and it was a wheel hub motor design, where each of the two front wheels was powered by a separate motor. This arrangement was sometimes referred to as an \"electric transmission\", as the electric generator and driving motor replaced a mechanical transmission. The vehicle could not move unless the internal combustion engine was running.\n\nIn 1997 Toyota released the first series-hybrid bus sold in Japan. GM introduced the Chevy Volt series plug-in hybrid in 2010, aiming for an all-electric range of , though this car also has a mechanical connection between the engine and drivetrain. Supercapacitors combined with a lithium ion battery bank have been used by AFS Trinity in a converted Saturn Vue SUV vehicle. Using supercapacitors they claim up to 150 mpg in a series-hybrid arrangement.\n\nAnother subtype of hybrid vehicles is the plug-in hybrid electric vehicle (PHEV). The plug-in hybrid is usually a general fuel-electric (parallel or serial) hybrid with increased energy storage capacity, usually through a lithium-ion battery, which allows the vehicle to drive on all-electric mode a distance that depends on the battery size and its mechanical layout (series or parallel). It may be connected to mains electricity supply at the end of the journey to avoid charging using the on-board internal combustion engine.\n\nThis concept is attractive to those seeking to minimize on-road emissions by avoiding – or at least minimizing – the use of ICE during daily driving. As with pure electric vehicles, the total emissions saving, for example in CO terms, is dependent upon the energy source of the electricity generating company.\n\nFor some users, this type of vehicle may also be financially attractive so long as the electrical energy being used is cheaper than the petrol/diesel that they would have otherwise used. Current tax systems in many European countries use mineral oil taxation as a major income source. This is generally not the case for electricity, which is taxed uniformly for the domestic customer, however that person uses it. Some electricity suppliers also offer price benefits for off-peak night users, which may further increase the attractiveness of the plug-in option for commuters and urban motorists.\n\nA 2009 National Highway Traffic Safety Administration report examined hybrid electric vehicle accidents that involved pedestrians and cyclists and compared them to accidents involving internal combustion engine vehicles (ICEV). The findings showed that, in certain road situations, HEVs are more dangerous for those on foot or bicycle. For accidents where a vehicle was slowing or stopping, backing up, entering or leaving a parking space (when the sound difference between HEVs and ICEVs is most pronounced), HEVs were twice as likely to be involved in a pedestrian crash than ICEVs. For crashes involving cyclists or pedestrians, there was a higher incident rate for HEVs than ICEVs when a vehicle was turning a corner. But there was no statistically significant difference between the types of vehicles when they were driving straight.\n\nSeveral automakers developed electric vehicle warning sounds designed to alert pedestrians to the presence of electric drive vehicles such as hybrid electric vehicle, plug-in hybrid electric vehicles and all-electric vehicles (EVs) travelling at low speeds. Their purpose is to make pedestrians, cyclists, the blind, and others aware of the vehicle's presence while operating in all-electric mode.\n\nVehicles in the market with such safety devices include the Nissan Leaf, Chevrolet Volt, Fisker Karma, Honda FCX Clarity, Nissan Fuga Hybrid/Infiniti M35, Hyundai ix35 FCEV, Hyundai Sonata Hybrid, 2012 Honda Fit EV, the 2012 Toyota Camry Hybrid, 2012 Lexus CT200h, and all Prius family cars recently introduced, including the standard 2012 model year Prius, the Toyota Prius v, and the Toyota Prius Plug-in Hybrid.\n\nThe hybrid vehicle typically achieves greater fuel economy and lower emissions than conventional internal combustion engine vehicles (ICEVs), resulting in fewer emissions being generated. These savings are primarily achieved by three elements of a typical hybrid design:\nOther techniques that are not necessarily 'hybrid' features, but that are frequently found on hybrid vehicles include:\n\nThese features make a hybrid vehicle particularly efficient for city traffic where there are frequent stops, coasting and idling periods. In addition noise emissions are reduced, particularly at idling and low operating speeds, in comparison to conventional engine vehicles. For continuous high speed highway use these features are much less useful in reducing emissions.\n\nHybrid vehicle emissions today are getting close to or even lower than the recommended level set by the EPA (Environmental Protection Agency). The recommended levels they suggest for a typical passenger vehicle should be equated to 5.5 metric tons of . The three most popular hybrid vehicles, Honda Civic, Honda Insight and Toyota Prius, set the standards even higher by producing 4.1, 3.5, and 3.5 tons showing a major improvement in carbon dioxide emissions.\nHybrid vehicles can reduce air emissions of smog-forming pollutants by up to 90% and cut carbon dioxide emissions in half.\n\nMore fossil fuel is needed to build hybrid vehicles than conventional cars but reduced emissions when running the vehicle more than outweigh this.\n\nThough hybrid cars consume less fuel than conventional cars, there is still an issue regarding the environmental damage of the hybrid car battery. Today most hybrid car batteries are one of two types: 1) nickel metal hydride, or 2) lithium ion; both are regarded as more environmentally friendly than lead-based batteries which constitute the bulk of petrol car starter batteries today. There are many types of batteries. Some are far more toxic than others. Lithium ion is the least toxic of the two mentioned above.\n\nThe toxicity levels and environmental impact of nickel metal hydride batteries—the type currently used in hybrids—are much lower than batteries like lead acid or nickel cadmium according to one source. Another source claims nickel metal hydride batteries are much more toxic than lead batteries, also that recycling them and disposing of them safely is difficult. In general various soluble and insoluble nickel compounds, such as nickel chloride and nickel oxide, have known carcinogenic effects in chick embryos and rats. The main nickel compound in NiMH batteries is nickel oxyhydroxide (NiOOH), which is used as the positive electrode.\n\nThe lithium-ion battery has attracted attention due to its potential for use in hybrid electric vehicles. Hitachi is a leader in its development. In addition to its smaller size and lighter weight, lithium-ion batteries deliver performance that helps to protect the environment with features such as improved charge efficiency without memory effect.\nThe lithium-ion batteries are appealing because they have the highest energy density of any rechargeable batteries and can produce a voltage more than three times that of nickel–metal hydride battery cell while simultaneously storing large quantities of electricity as well. The batteries also produce higher output (boosting vehicle power), higher efficiency (avoiding wasteful use of electricity), and provides excellent durability, compared with the life of the battery being roughly equivalent to the life of the vehicle.\nAdditionally, use of lithium-ion batteries reduces the overall weight of the vehicle and also achieves improved fuel economy of 30% better than petro-powered vehicles with a consequent reduction in CO emissions helping to prevent global warming.\nThere is two different levels of charging. Level one charging is the slower method as it uses a 120 V/15 A single-phase grounded outlet. Level two is a faster method; existing Level 2 equipment offers charging from 208 V or 240 V (at up to 80 A, 19.2 kW). It may require dedicated equipment and a connection installation for home or public units, although vehicles such as the Tesla have the power electronics on board and need only the outlet. The optimum charging window for Lithium ion batteries is 3-4.2 V. Recharging with a 120 volt household outlet takes several hours, a 240 volt charger takes 1–4 hours, and a quick charge takes approximately 30 minutes to achieve 80% charge. Three important factors—distance on charge, cost of charging, and time to charge \nIn order for the hybrid to run on electrical power, the car must perform the action of braking in order to generate some electricity. The electricity then gets discharged most effectively when the car accelerates or climbs up an incline.\nIn 2014, hybrid electric car batteries can run on solely electricity for 70–130 miles (110–210 km) on a single charge. Hybrid battery capacity currently ranges from 4.4 kWh to 85 kWh on a fully electric car. On a hybrid car, the battery packs currently range from 0.6 kWh to 2.4 kWh representing a large difference in use of electricity in hybrid cars.\n\nThere is an impending increase in the costs of many rare materials used in the manufacture of hybrid cars. For example, the rare earth element dysprosium is required to fabricate many of the advanced electric motors and battery systems in hybrid propulsion systems. Neodymium is another rare earth metal which is a crucial ingredient in high-strength magnets that are found in permanent magnet electric motors.\n\nNearly all the rare earth elements in the world come from China, and many analysts believe that an overall increase in Chinese electronics manufacturing will consume this entire supply by 2012. In addition, export quotas on Chinese rare earth elements have resulted in an unknown amount of supply.\n\nA few non-Chinese sources such as the advanced Hoidas Lake project in northern Canada as well as Mount Weld in Australia are currently under development; however, the barriers to entry are high and require years to go online.\n\nHybrids-Electric vehicles (HEVs) combine the advantage of gasoline \"engines\" and electric \"motors\". The key areas for efficiency or performance gains are regenerative braking, dual power sources, and less idling.\n\nOther types of green vehicles include other vehicles that go fully or partly on alternative energy sources than fossil fuel. Another option is to use alternative fuel composition (i.e. biofuels) in conventional fossil fuel-based vehicles, making them go partly on renewable energy sources.\n\nOther approaches include personal rapid transit, a public transportation concept that offers automated on-demand non-stop transportation, on a network of specially built guideways.\n\nPeugeot and Citroën have announced that they too are building a car that uses compressed air as an energy source. However, the car they are designing uses a hybrid system which also uses a gasoline engine (which is used for propelling the car over 70 km/h, or when the compressed air tank has been depleted.\n\nAutomakers spend around $US8 million in marketing Hybrid vehicles each year. With combined effort from many car companies, the Hybrid industry has sold millions of Hybrids. Hybrid car companies like Toyota, Honda, Ford and BMW have pulled together to create a movement of Hybrid vehicle sales pushed by Washington lobbyist to lower the worlds emissions and become less reliant on our petroleum consumption. In 2005, sales went beyond 200,000 Hybrids, but in retrospect that only reduced the global use for gasoline consumption by 200,000 gallons per day — a tiny fraction of the 360 million gallons used per day. According to Bradley Berman author of Driving Change—One Hybrid at a time, \"Cold economics shows that in real dollars, except for a brief spike in the 1970s, gas prices have remained remarkably steady and cheap. Fuel continues to represent a small part of the overall cost of owning and operating a personal vehicle\". Other marketing tactics include greenwashing which is the \"unjustified appropriation of environmental virtue.\" Temma Ehrenfeld explained in an article by Newsweek. Hybrids may be more efficient than many other gasoline motors as far as gasoline consumption is concerned but as far as being green and good for the environment is completely inaccurate. Hybrid car companies have a long time to go if they expect to really go green. According to Harvard business professor Theodore Levitt states \"managing products\" and \"meeting customers' needs\", \"you must adapt to consumer expectations and anticipation of future desires.\" This means people buy what they want, if they want a fuel efficient car they buy a Hybrid without thinking about the actual efficiency of the product. This \"Green Myopia\" as Ottman calls it, fails because marketers focus on the greenness of the product and not on the actual effectiveness. Researchers and analysts say people are drawn to the new technology, as well as the convenience of fewer fill ups. Secondly, people find it rewarding to own the better, newer, flashier, and so called greener car. In the beginning of the Hybrid movement car companies reached out to the young people, by using top celebrities, astronauts, and popular TV shows to market Hybrids. This made the new technology of Hybrids a status to obtain for many people and a must to be cool or even the practical choice for the time. With the many benefits and status of owning a Hybrid it is easy to think it's the right thing to do, but in fact may not be as green as it appears.\n\nWhile the adoption rate for hybrids in the US is small today (2.2% of new car sales in 2011), this compares with a 17.1% share of new car sales in Japan in 2011, and it has the potential to be very large over time as more models are offered and incremental costs decline due to learning and scale benefits. However, forecasts vary widely. For instance, Bob Lutz, a long-time skeptic of hybrids, indicated he expects hybrids \"will never comprise more than 10% of the US auto market.\" Other sources also expect hybrid penetration rates in the US will remain under 10% for many years.\n\nMore optimistic views as of 2006 include predictions that hybrids would dominate new car sales in the US and elsewhere over the next 10 to 20 years. Another approach, taken by Saurin Shah, examines the penetration rates (or S-curves) of four analogs (historical and current) to hybrid and electrical vehicles in an attempt to gauge how quickly the vehicle stock could be hybridized and/or electrified in the United States. The analogs are (1) the electric motors in US factories in the early 20th century, (2) diesel electric locomotives on US railways in the 1920–1945 period, (3) a range of new automotive features/technologies introduced in the US over the past fifty years, and 4) e-bike purchases in China over the past few years. These analogs collectively suggest it would take at least 30 years for hybrid and electric vehicles to capture 80% of the US passenger vehicle stock.\nThe European Parliament, Council and European Commission has reached an agreement which is aimed at reducing the average CO2 passenger car emissions to 95 g/km by 2020, according to a European Commission press release.\n\nAccording to the release, the key details of the agreement are as follows:\n\nEmissions target: The agreement will reduce average CO2 emissions from new cars to 95 g/km from 2020, as proposed by the Commission. This is a 40% reduction from the mandatory 2015 target of 130 g/km. The target is an average for each manufacturer's new car fleet; it allows OEMs to build some vehicles that emit less than the average and some that emit more.\n2025 target: The Commission is required to propose a further emissions reduction target by end-2015 to take effect in 2025. This target will be in line with the EU's long-term climate goals.\nSupercredits for low-emission vehicles: The Regulation will give manufacturers additional incentives to produce cars with CO2 emissions of 50 g/km or less (which will be electric or plug-in hybrid cars). Each of these vehicles will be counted as two vehicles in 2020, 1.67 in 2021, 1.33 in 2022 and then as one vehicle from 2023 onwards. These supercredits will help manufacturers further reduce the average emissions of their new car fleet. However, to prevent the scheme from undermining the environmental integrity of the legislation, there will be a 2.5 g/km cap per manufacturer on the contribution that supercredits can make to their target in any year.\n\n"}
{"id": "50027537", "url": "https://en.wikipedia.org/wiki?curid=50027537", "title": "Journey to the Safest Place on Earth", "text": "Journey to the Safest Place on Earth\n\nJourney to the Safest Place on Earth is a 2013 documentary film written and directed by Edgar Hagen. It discusses the huge quantity of radioactive waste and spent fuel rods being stored at various locations on the planet.\n\nCharles McCombie is a Swiss-based nuclear physicist with 35 years of experience in this field, and he accompanies Hagen on a worldwide search for the best location. One disposal site in Texas is adjacent to a location where oil drilling is in progress. A proposed disposal site in Nevada is adjacent to the young volcano Yucca Mountain. \n\nThere is an urgent need for a location where the waste can be stored without harm to living things or the environment. Meanwhile, nuclear waste continues to be produced.\n\n"}
{"id": "1641702", "url": "https://en.wikipedia.org/wiki?curid=1641702", "title": "Lamella (materials)", "text": "Lamella (materials)\n\nA lamella (plural \"lamellae\") is a small plate or flake, from the Latin, and may also be used to refer to collections of fine sheets of material held adjacent to one another, in a gill-shaped structure, often with fluid in between though sometimes simply a set of 'welded' plates. The term is used in biological and engineering contexts, such as filters and heat exchangers. The microscopic structures in bone and nacre are lamellae in the materials science sense of the word.\n\nIn surface chemistry (especially mineralogy and materials science), lamellar structures are fine layers, alternating between different materials. They can be produced by chemical effects (as in eutectic solidification), biological means, or a deliberate process of lamination, such as pattern welding. Lamellae can also describe the layers of atoms in the crystal lattice of a material such as a metal.\n\nThe term has been used to describe the construction of lamellar armour, as well as the layered structures that can be described by a lamellar vector field.\n\nIn a water-treatment context, lamellar filters may be referred to as \"plate filters\" or \"tube filters\".\n\nThis term is used to describe a certain type of ichthyosis, a congenital skin condition. Lamellar Ichthyosis often presents with a \"colloidal\" membrane at birth. It is characterized by generalized dark scaling.\n\nThe term \"lamella(e)\" is used in the flooring industry to describe the finished top-layer of an engineered wooden floor. For example, an engineered walnut floor will have several layers of wood and a top walnut lamella. \n\nIn archaeology the term is used for a variety of small flat and thin objects, such as Amulet MS 5236, a very thin gold plate with a stamped text from Ancient Greece in the 6th century BC. \n\nIn textiles, lamella is thin metallic strip used alone or wound around a core thread for goldwork embroidery and tapestry weaving.\n\nIn September 2010, the U.S. Food and Drug Administration (FDA) announced a recall of two medications which contained \"extremely thin glass flakes (lamellae) that are barely visible in most cases. The lamellae result from the interaction of the formulation with glass vials over the shelf life of the product.\"\n"}
{"id": "15157206", "url": "https://en.wikipedia.org/wiki?curid=15157206", "title": "Lewiston Lake", "text": "Lewiston Lake\n\nLewiston Lake is a reservoir impounded by Lewiston Dam on the Trinity River, in Trinity County, California.\n\nLewiston Lake is near the towns of Weaverville and Lewiston in Trinity County, California.\n\nIt is used for transbasin diversion to the Sacramento River and flood control, as well as for hydroelectric generation. It is in the canyon between the Trinity Mountains and Marble Mountains of the southern Klamath Mountains System.\n\nLewiston reservoir is within the Trinity Unit of the Whiskeytown-Shasta-Trinity National Recreation Area, in the Shasta-Trinity National Forest. It is a popular destination for fishing, kayaking, and camping.\n\nThe California Office of Environmental Health Hazard Assessment (OEHHA) has developed a safe eating advisory for Lewiston Lake based on levels of mercury or PCBs found in fish caught from this water body.\n\n\n"}
{"id": "30403278", "url": "https://en.wikipedia.org/wiki?curid=30403278", "title": "Ludila Dam", "text": "Ludila Dam\n\nThe Ludila Dam(鲁地拉水电站 in chinese) is a gravity dam on the Jinsha River near Lijiang in Yunnan province, China. The primary purpose of the dam is hydroelectric power generation and it will support a 2,160 MW power station. Construction on the dam began in 2007 and was briefly halted in June 2009 by the Ministry of Environmental Protection after it was being constructed without approval. On 13 June 2013 the dam's first generator became operational. In May 2014 reports surfaced that the dam and been damaged or was structurally unsound, forcing engineers to draw down the reservoir level and leaving the power station inoperable. An estimated 16,900 people will be relocated after its construction.\n\n"}
{"id": "28210296", "url": "https://en.wikipedia.org/wiki?curid=28210296", "title": "Luggage scale", "text": "Luggage scale\n\nA luggage scale also called suitcase scale is used to weigh luggage before going to an airport to avoid luggage being overweight.\n"}
{"id": "176354", "url": "https://en.wikipedia.org/wiki?curid=176354", "title": "Mineral wool", "text": "Mineral wool\n\nMineral wool is any fibrous material formed by spinning or drawing molten mineral or rock materials such as slag and ceramics.\n\nApplications of mineral wool include thermal insulation (as both structural insulation and pipe insulation, though it is not as fire-resistant as high-temperature insulation wool), filtration, soundproofing, and hydroponic growth medium.\n\nMineral wool is also known as \"mineral fiber\", \"mineral cotton\", \"mineral fibre\", \"man-made mineral fibre\" (MMMF), and \"man-made vitreous fiber\" (MMVF).\n\nThe nomenclature of these wool products is simply the parent/raw material name in prefix to \"wool\". Specific mineral wool products are \"stone wool\" and \"slag wool\". Europe also includes glass wool which, together with ceramic fiber, are completely man-made fibers.\n\nSlag wool was first made in 1840 in Wales by Edward Parry, \"but no effort appears to have been made to confine the wool after production; consequently it floated about the works with the slightest breeze, and became so injurious to the men that the process had to be abandoned\". A method of making mineral wool was patented in the United States in 1870 by John Player and first produced commercially in 1871 at Georgsmarienhütte in Osnabrück Germany. The process involved blowing a strong stream of air across a falling flow of liquid iron slag which was similar to the natural occurrence of fine strands of volcanic slag from Kilauea called Pele's hair created by strong winds blowing apart the slag during an eruption. \n\nAmerican chemical engineer Charles Corydon Hall in 1897 developed a technology to convert molten limestone into fibers and initiated the rock wool insulation industry in America.\n\nAccording to a mineral wool manufacturer, the first mineral wool intended for high-temperature applications was invented in the United States in 1942, but was not commercially viable until approximately 1953. More forms of mineral wool become available in the 1970s and 1980s.\n\nHigh-temperature mineral wool (HTMW) is a type of mineral wool created for use as high-temperature insulation, usually for use in industrial furnaces and foundries, generally defined as being resistant to temperatures above 1,000 °C.\n\nDue to the costly production and limited availability compared to regular mineral wool, HTMW products are used almost exclusively in high-temperature industrial applications and processes.\n\nThe classification temperature is defined as the temperature at which a linear shrinkage of a certain amount (usually 2–4%) is not exceeded after a 24‑hour heat treatment in the electrically heated laboratory oven and in a neutral atmosphere. Depending on the type of product, the value may not exceed the following limits: 2% for boards and shaped products, 4% for mats and papers.\n\nThe classification temperature is specified in 50 °C steps (starting at 850 °C and up to 1600 °C). The classification temperature does not mean that the product can be used continuously at this temperature. In the field, the continuous application temperature of amorphous HTMW (AES and ASW) is typically 100–150 °C below the classification temperature. Products made of polycrystalline wool can generally be used up to classification temperature.\n\nThere are several types of HTMWs made from different types of mineral, and with different properties and temperatures that they can withstand. Below are some common types:\n\nAES Wool consists of amorphous glass fibres, which are produced by melting a combination of CaO−, MgO−, SiO . Products made from AES are generally used in continuously operating equipment and domestic appliances. AES wool has the advantage of being biosoluble; if inhaled it will generally stay in the body only a couple of weeks, reducing the chances of causing silicosis.\n\nAlumino silicate wool also known as “refractory ceramic fibre” (RCF), are amorphous fibres produced by melting a combination of AlO and SiO, usually in a weight ratio 50:50 (see also VDI 3469 Parts 1 and 5, as well as TRGS 521). Products made of alumino silicate wool are generally used at application temperatures of greater than 900 °C and in intermittently operating equipment and critical application conditions (see Technical Rules TRGS 619).\n\nPolycrystalline wool consists of fibres containing greater than 70 wt.% AlO; they are produced by a \"sol-gel method\" from aqueous spinning solutions. The water-soluble green fibres obtained as a precursor are crystallized by means of heat treatment. Polycrystalline wool is generally used at application temperatures greater than 1300 °C and in critical chemical and physical application conditions, also at lower temperatures.\n\nKaowool is a type of HTMW made from the mineral kaolin. It was one of the first types of HTMW invented and has continued use into the 21st century. It can withstand temperatures close to 3,000° F (1649° C).\n\nStone wool is a furnace product of molten rock at a temperature of about 1600 °C, through which a stream of air or steam is blown. More advanced production techniques are based on spinning molten rock in high-speed spinning heads somewhat like the process used to produce cotton candy. The final product is a mass of fine, intertwined fibres with a typical diameter of 2 to 6 micrometers. Mineral wool may contain a binder, often a terpolymer, and an oil to reduce dusting.\n\nThough the individual fibers conduct heat very well, when pressed into rolls and sheets, their ability to partition air makes them excellent insulators and sound absorbers. Though not immune to the effects of a sufficiently hot fire, the fire resistance of fiberglass, stone wool, and ceramic fibers makes them common building materials when passive fire protection is required, being used as spray fireproofing, in stud cavities in drywall assemblies and as packing materials in firestops.\n\nOther uses are in resin bonded panels, as filler in compounds for gaskets, in brake pads, in plastics in the automotive industry, as a filtering medium, and as a growth medium in hydroponics.\n\nMineral fibers are produced in the same way, without binder. The fiber as such is used as a raw material for its reinforcing purposes in various applications, such as friction materials, gaskets, plastics, and coatings.\n\nMineral wool products can be engineered to hold large quantities of water and air that aid root growth and nutrient uptake in hydroponics; their fibrous nature also provides a good mechanical structure to hold the plant stable. The naturally high pH of mineral wool makes them initially unsuitable to plant growth and requires \"conditioning\" to produce a wool with an appropriate, stable pH.\n\nHigh-temperature mineral wool is used primarily for insulation and lining of industrial furnaces and foundries to improve efficiency and safety. It is also used to prevent the spread of fire.\n\nThe use of HTMW enables a more lightweight construction of industrial furnaces and other technical equipment as compared to other methods such as fire bricks, due to its high heat resistance capabilities per weight, but has the disadvantage of being more expensive than other methods.\n\nThe International Agency for Research on Cancer (IARC) has reviewed the carcinogenicity of man-made mineral fibres in October 2002.\nThe IARC Monograph's working group concluded only the more biopersistent materials remain classified by IARC as \"possibly carcinogenic to humans\" (Group 2B). These include refractory ceramic fibres, which are used industrially as insulation in high-temperature environments such as blast furnaces, and certain special-purpose glass wools not used as insulating materials. In contrast, the more commonly used vitreous fibre wools produced since 2000, including insulation glass wool, stone wool, and slag wool, are considered \"not classifiable as to carcinogenicity in humans\" (Group 3).\n\nHigh biosoluble fibres are produced that do not cause damage to the human cell. These newer materials have been tested for carcinogenicity and most are found to be noncarcinogenic. IARC elected not to make an overall evaluation of the newly developed fibres designed to be less biopersistent such as the alkaline earth silicate or high-alumina, low-silica wools. This decision was made in part because no human data were available, although such fibres that have been tested appear to have low carcinogenic potential in experimental animals, and because the Working Group had difficulty in categorizing these fibres into meaningful groups based on chemical composition.\"\n\nThe European Regulation (CE) n° 1272/2008 on classification, labelling and packaging of substances and mixtures updated by the Regulation (CE) n°790/2009 does not classify mineral wool fibres as a dangerous substance if they fulfil criteria defined in its note Q.\n\nThe European Certification Board for mineral wool products, EUCEB, certify mineral wool products made of fibres fulfilling Note Q ensuring that they have a low biopersistence and so that they are fast removed from the lung. The certification is based on independent experts’ advice and regular control of the chemical composition\nwww.euceb.org\n\nDue to the mechanical effect of fibres, mineral wool products may cause temporary skin itching. To diminish this and to avoid unnecessary exposure to mineral wool dust, information on good practices is available on the packaging of mineral wool products with pictograms or sentences. Safe Use Instruction Sheets similar to Safety data sheet are also available from each producer.\n\nPeople can be exposed to mineral wool fibers in the workplace by breathing them in, skin contact, and eye contact. The Occupational Safety and Health Administration (OSHA) has set the legal limit (permissible exposure limit) for mineral wool fiber exposure in the workplace as 15 mg/m total exposure and 5 mg/m respiratory exposure over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 5 mg/m total exposure and 3 fibers per cm over an 8-hour workday.\n\nRegistration, Evaluation, Authorisation and Restriction of Chemicals (REACH) is a European Union regulation of 18 December 2006. REACH addresses the production and use of chemical substances, and their potential impacts on both human health and the environment. A Substance Information Exchange Forum (SIEF) has been set up for several types of mineral wool. AES, ASW and PCW have been registered before the first deadline of 1 December 2010 and can, therefore, be used on the European market.\n\n\nOn 13 January 2010, some of the aluminosilicate refractory ceramic fibres and zirconia aluminosilicate refractory ceramic fibres have been included in the candidate list of Substances of Very High Concern. In response to concerns raised with the definition and the dossier two additional dossiers were posted on the ECHA website for consultation and resulted in two additional entries on the candidate list. This actual (having four entries for one substance/group of substances) situation is contrary to the REACH procedure intended. Aside from this situation, concerns raised during the two consultation periods remain valid.\n\nRegardless of the concerns raised, the inclusion of a substance in the candidate list triggers immediately the following legal obligations of manufacturers, importers and suppliers of articles containing that substance in a concentration above 0.1% (w/w):\n\n\nBased on the total experience with humans and the findings of scientific research (animals, cells), it can be concluded that elongated dust particles of every type have in principle the potential to cause the development of tumours providing they are sufficiently long, thin and biopersistent. According to scientific findings inorganic fibre dust particles with a length-to-diameter ratio exceeding 3:1, a length longer than 5 μm (0.005 mm) and a diameter smaller than 3 μm (WHO-Fibres) are considered health-critical.\n\nHTMW processed to products contain fibres with different diameters and lengths. During handling of HTMW products, fibrous dusts can be emitted. These can include fibres complying with the WHO definition. The amount depends on how the material is handled. High concentrations are usually found during removal of after-use HTMW and also during mechanical finishing activities and in the assembly of modules. Where fibre products are mechanically abraded by sawing, sanding, routing, or other machining the airborne fibre concentrations will be high if uncontrolled. Dust release is further modified by the intensity of energy applied to the product, the surface area to which the energy is applied, and the type, quantity and dimensions of materials being handled or processed. Dispersion or dilution of dust produced depends on the extent of confinement of the sources and the work area, as well as the presence and effectiveness of exhaust ventilation.\n\nAmorphous HTMW (AES and ASW) are produced from a molten glass stream which is aerosolised by a jet of high-pressure air or by letting the stream impinge onto spinning wheels. The droplets are drawn into fibres; the mass of both fibres and remaining droplets cool very rapidly so that no crystalline phases may form.\n\nWhen amorphous HTMW are installed and used in high-temperature applications such as industrial furnaces, at least one face may be exposed to conditions causing the fibres to partially devitrify. Depending on the chemical composition of the glassy fibre and the time and temperature to which the materials are exposed, different stable crystalline phases may form.\n\nIn after-use HTMW crystalline silica crystals are embedded in a matrix composed of other crystals and glasses. Experimental results on the biological activity of after-use HTMW have not demonstrated any hazardous activity that could be related to any form of silica they may contain.\n\n\n"}
{"id": "33545796", "url": "https://en.wikipedia.org/wiki?curid=33545796", "title": "Okhotnykovo Solar Park", "text": "Okhotnykovo Solar Park\n\nThe Okhotnykovo Solar Power Station is a Photovoltaic power station in Crimea with installed capacity of 82,65 megawatts (MW). , it is the world's 6th largest solar plant and the second largest PV station in central and eastern Europe. The power station is located at Okhotnykovo in Crimea, Ukraine and was developed by the Austrian company Activ Solar. The first and second phases of the project were commissioned in July 2011, the third and fourth in October 2011.\n\nThe park comprises 360,000 modules, generating around 100 GWh of electricity per year, enough to meet the needs of 20,000 households. The larger capacity is installed in the plants in Perovo (Ukraine, 100 MW), Sarnia Canada (97 MW) and Montalto di Castro Italy (84.2 MW).\n\nThe solar power plant is part of the Ukraine's National Natural Energy Project which is aiming to reduce Crimea's dependence on mainland Ukraine for energy. The State Agency of Ukraine for Energy Efficiency and Energy Conservation launched the project in 2010 in the hope of attracting investors to the reputed high solar radiation area, which reaches a capacity of 800-1450 W/m2.\n\n"}
{"id": "299936", "url": "https://en.wikipedia.org/wiki?curid=299936", "title": "Oklo", "text": "Oklo\n\nOklo is a region near the town of Franceville, in the Haut-Ogooué province of the Central African state of Gabon. Several natural nuclear fission reactors were discovered in the uranium mines in the region during 1972.\n\nGabon was a French colony when prospectors from the French nuclear energy commissariat (the industrial parts, which later became the COGEMA and later Areva NC) discovered uranium in the remote region in 1956. France immediately opened mines operated by Comuf (Compagnie des Mines d'Uranium de Franceville) near Mounana village in order to exploit the vast mineral resources and the State of Gabon was given a minority share in the company.\n\nFor forty years, France mined for uranium in Gabon. Once extracted, the uranium was used for electricity production in France and much of Europe. Today, however, the uranium deposits are exhausted, and the mine is no longer worked. Currently, mine reclamation work is ongoing in the region affected by the mine operations.\n\nThere is strong geochemical evidence that the Oklo uranium deposit behaved as a natural nuclear fission reactor in Precambrian times: some of the mined uranium was found to have a lower concentration of uranium-235 than expected, as if it had already been in a reactor. Geologists found that it \"had\" been in a reactor before—two billion years ago. At that time the natural uranium had a concentration of about 3% U, and could have reached criticality with natural water as neutron moderator.\n\n\n"}
{"id": "542488", "url": "https://en.wikipedia.org/wiki?curid=542488", "title": "Palmitic acid", "text": "Palmitic acid\n\nPalmitic acid, or hexadecanoic acid in IUPAC nomenclature, is the most common saturated fatty acid found in animals, plants and microorganisms. Its chemical formula is CH(CH)COOH, and its C:D is 16:0. As its name indicates, it is a major component of the oil from the fruit of oil palms (palm oil). Palmitic acid can also be found in meats, cheeses, butter, and dairy products. Palmitates are the salts and esters of palmitic acid. The palmitate anion is the observed form of palmitic acid at physiologic pH (7.4).\n\nAluminium salts of palmitic acid and naphthenic acid were combined during World War II to produce napalm. The word \"napalm\" is derived from the words naphthenic acid and palmitic acid.\n\nPalmitic acid was discovered by Edmond Frémy in 1840, in saponified palm oil. This remains the primary industrial route for its production, with the triglycerides (fats) in palm oil being hydrolysed by high temperature water (above ), and the resulting mixture fractionally distilled to give the pure product.\n\nPalmitic acid is naturally produced by a wide range of other plants and organisms, typically at low levels. It is naturally present in butter, cheese, milk, and meat, as well as cocoa butter, soybean oil, and sunflower oil. Karukas contain 44.90% palmitic acid. The cetyl ester of palmitic acid (cetyl palmitate) occurs in spermaceti.\n\nExcess carbohydrates in the body are converted to palmitic acid. Palmitic acid is the first fatty acid produced during fatty acid synthesis and is the precursor to longer fatty acids. As a consequence, palmitic acid is a major body component of animals. In humans, one analysis found it to make up 21–30% (molar) of human depot fat, and it is a major, but highly variable, lipid component of human breast milk. Palmitate negatively feeds back on acetyl-CoA carboxylase (ACC), which is responsible for converting acetyl-CoA to malonyl-CoA, which in turn is used to add to the growing acyl chain, thus preventing further palmitate generation. In biology, some proteins are modified by the addition of a palmitoyl group in a process known as palmitoylation. Palmitoylation is important for membrane localisation of many proteins.\n\nPalmitic acid is used to produce a soaps, cosmetics, and industrial mold release agents. These applications use sodium palmitate, which is commonly obtained by saponification of palm oil. To this end, palm oil, rendered from palm tree (species \"Elaeis guineensis\"), is treated with sodium hydroxide (in the form of caustic soda or lye), which causes hydrolysis of the ester groups, yielding glycerol and sodium palmitate.\n\nBecause it is inexpensive and adds texture and \"mouth feel\" to processed foods (convenience food), palmitic acid and its sodium salt find wide use in foodstuffs. Sodium palmitate is permitted as a natural additive in organic products. The aluminium salt is used as a thickening agent of napalm used in military actions.\n\nHydrogenation of palmitic acid yields cetyl alcohol, which is used to produce detergents and cosmetics.\n\nRecently, a long-acting antipsychotic medication, paliperidone palmitate (marketed as INVEGA Sustenna), used in the treatment of schizophrenia, has been synthesized using the oily palmitate ester as a long-acting release carrier medium when injected intramuscularly. The underlying method of drug delivery is similar to that used with decanoic acid to deliver long-acting depot medication, in particular, neuroleptics such as haloperidol decanoate.\n\nAccording to the World Health Organization, evidence is \"convincing\" that consumption of palmitic acid increases the risk of developing cardiovascular disease, based on studies indicating that it may increase LDL levels in the blood. Retinyl palmitate is a source of vitamin A added to low fat milk to replace the vitamin content lost through the removal of milk fat. Palmitate is attached to the alcohol form of vitamin A, retinol, to make vitamin A stable in milk.\n\n"}
{"id": "32281625", "url": "https://en.wikipedia.org/wiki?curid=32281625", "title": "Pedal bin", "text": "Pedal bin\n\nA pedal bin is a container with a lid operated by a foot pedal. Lillian Moller Gilbreth (an industrial engineer and efficiency expert as well as mother of twelve) invented the pedal bin in the 1920s for the disposal of kitchen waste. The footpedal enables the user to open the lid without touching it with their hands.\n"}
{"id": "17355301", "url": "https://en.wikipedia.org/wiki?curid=17355301", "title": "Plasma containment", "text": "Plasma containment\n\nIn plasma physics, plasma containment refers to the act of maintaining a plasma in a discrete volume. For example, a toroidal fusion reactor is a plasma containment device. Electromagnetic interaction must often be used since plasma is ordinarily hotter than the degradation temperature of any known material.\n\n"}
{"id": "1539804", "url": "https://en.wikipedia.org/wiki?curid=1539804", "title": "Sheet resistance", "text": "Sheet resistance\n\nSheet resistance is a measure of resistance of thin films that are nominally uniform in thickness. It is commonly used to characterize materials made by semiconductor doping, metal deposition, resistive paste printing, and glass coating. Examples of these processes are: doped semiconductor regions (e.g., silicon or polysilicon), and the resistors that are screen printed onto the substrates of thick-film hybrid microcircuits.\n\nThe utility of sheet resistance as opposed to resistance or resistivity is that it is directly measured using a four-terminal sensing measurement (also known as a four-point probe measurement) or indirectly by using a non-contact eddy current based testing device. Sheet resistance is invariable under scaling of the film contact and therefore can be used to compare the electrical properties of devices that are significantly different in size.\n\nSheet resistance is applicable to two-dimensional systems in which thin films are considered two-dimensional entities. When the term sheet resistance is used, it is implied that the current is along the plane of the sheet, not perpendicular to it.\n\nIn a regular three-dimensional conductor, the resistance can be written as\nwhere formula_2 is the resistivity, formula_3 is the cross-sectional area, and formula_4 is the length. The cross-sectional area can be split into the width formula_5 and the sheet thickness formula_6.\n\nUpon combining the resistivity with the thickness, the resistance can then be written as\nwhere formula_8 is the sheet resistance. If the film thickness is known, the bulk resistivity formula_2 (in Ω·cm) can be calculated by multiplying the sheet resistance by the film thickness in cm:\n\nSheet resistance is a special case of resistivity for a uniform sheet thickness. Commonly, resistivity (also known as bulk resistance, specific electrical resistance, or volume resistivity) is in units of Ω·m, which is more completely stated in units of Ω·m/m (Ω·area/length). When divided by the sheet thickness (m), the units are Ω·m·(m/m)/m = Ω. The term \"(m/m)\" cancels, but represents a special \"square\" situation yielding an answer in ohms. An alternative, common unit is \"ohms square\" (denoted \"formula_11\") or \"ohms per square\" (denoted \"Ω/sq\" or \"formula_12\"), which is dimensionally equal to an ohm, but is exclusively used for sheet resistance. This is an advantage, because sheet resistance of 1 Ω could be taken out of context and misinterpreted as bulk resistance of 1 ohm, whereas sheet resistance of 1 Ω/sq cannot thus be misinterpreted.\n\nThe reason for the name \"ohms per square\" is that a square sheet with sheet resistance 10 ohm/square has an actual resistance of 10 ohm, regardless of the size of the square. (For a square, formula_13, so formula_14.) The unit can be thought of as, loosely, \"ohms · aspect ratio\". Example: A 3-unit long by 1-unit wide (aspect ratio = 3) sheet made of material having a sheet resistance of 21 Ω/sq would measure 63 Ω (since it is composed of three 1-unit by 1-unit squares), if the 1-unit edges were attached to an ohmmeter that made contact entirely over each edge.\n\nFor semiconductors doped through diffusion or surface peaked ion implantation we define the sheet resistance using the average resistivity formula_15 of the material:\n\nwhich in materials with majority-carrier properties can be approximated by (neglecting intrinsic charge carriers):\n\nwhere formula_18 is the junction depth, formula_19 is the majority-carrier mobility, formula_20 is the carrier charge, and formula_21 is the net impurity concentration in terms of depth. Knowing the background carrier concentration formula_22 and the surface impurity concentration, the \"sheet resistance-junction depth\" product formula_23 can be found using Irvin's curves, which are numerical solutions to the above equation.\n\nA four-point probe is used to avoid contact resistance, which can often have the same magnitude as the sheet resistance. Typically a constant current is applied to two probes, and the potential on the other two probes is measured with a high-impedance voltmeter. A geometry factor needs to be applied according to the shape of the four-point array. Two common arrays are square and in-line. For more details see Van der Pauw method.\n\nMeasurement may also be made by applying high-conductivity bus bars to opposite edges of a square (or rectangular) sample. Resistance across a square area will be measured in Ω/sq. For a rectangle an appropriate geometric factor is added. Bus bars must make ohmic contact.\n\nInductive measurement is used as well. This method measures the shielding effect created by eddy currents. In one version of this technique a conductive sheet under test is placed between two coils. This non-contact sheet resistance measurement method also allows to characterize encapsulated thin-films or films with rough surfaces. \nA very crude two-point probe method is to measure resistance with the probes close together and the resistance with the probes far apart. The difference between these two resistances will be of the order of magnitude of the sheet resistance.\n\nSheet resistance measurements are very common to characterize the uniformity of conductive or semiconductive coatings and materials, e.g. for quality assurance. Typical applications include the inline process control of metal, TCO, conductive nanomaterials or other coatings on architectural glass, wafers, flat panel displays, polymer foils, OLED, ceramics, etc.\nThe contacting four-point probe is often applied for single-point measurements of hard or coarse materials. Non-contact eddy current systems are applied for sensitive or encapsulated coatings, for inline measurements and for high-resolution mapping.\n\n\n"}
{"id": "45495882", "url": "https://en.wikipedia.org/wiki?curid=45495882", "title": "SolaRoad", "text": "SolaRoad\n\nThe SolaRoad is the world's first bike path made from solar panels, and is a prototype project testing the feasibility of various proposals for smart highways. The path opened in the week of 21 October 2014, and was designed by a consortium of organizations, which built the pathway in Krommenie, Netherlands.\n\nThe path was formally opened in November 2014 by the Dutch Minister of Energy Henk Kamp.\n\nThe technology was developed by a consortium consisting of Netherlands Organisation for Applied Scientific Research (TNO), Imtech (Dynniq) and Ooms Civiel, with a grant of €1.5million from the province (county) North Holland as owner of the path. The total cost of the pilot project was €3.5million. In addition to the €1.5million from the province North Holland were that contributions from TNO, Ooms Civil, Imtech (Dynniq) and the European PV-Sin project (partly subsidized by the Dutch government).\n\nThe road surface consists of prefabricated panels with a surface of thick hardened glass. Beneath the glass solar cells are installed. TNO states that this energy can be used for lighting of the road, traffic lights and road signs. The energy is also delivered to local dwellings. TNO thinks in future electrical vehicles might be driven by the road itself. This prototype will be studied over the next three years.\n\nIn the first month, the path delivered enough energy to sustain one family.\n\nOn 26 December 2014, a section of the top-layer coating detached from the glass layer, and that portion of the bike path had to be repaired.\n\nIn October 2015 the top-layer coating was in such poor condition that it was replaced.\n\nCritics of the technology see several problems:\n\nAfter a six-month test engineers report results are \"better than expected\". \"If we translate this to an annual yield, we expect more than the 70kWh per square metre per year,\" Sten de Wit, spokesman for SolaRoad, the company that put it in.\n\nThe EEVblog compared the 6 and 12 months trial results from SolaRoad with data from 3 rooftop solar systems within a few kilometers of the prototype road. The data showed that rooftop solar systems produced twice the output of the SolaRoad per square meter over the same period.\n\nIn November 2015 it was announced that the path had produced 9800 kWh of electricity in one year.\n\nIn October 2016, the path was expanded with 7 new improved elements. Two elements of the first generation were removed. In total the expanded path consist of 32 elements (83 meters).\n\nIn February 2017, a crack appeared in the top coating of one of the improved elements.\n\n\nAn innovative cycle lane in South Korea has a solar powered roof, providing shelter from sun and rain for cyclists while generating electricity. In this concept the solar panels are directed in the most profitable position for optimal efficiency. \nThe 32 km (20 mile) path between Daejeon and Sejong runs down the middle of a six-lane motorway.\n\nParking under a solar panel roof is an efficient way to produce electricity in combination with infrastructure.\n\nAnother economically viable solution for harvesting energy from roads is Road Energy Systems (RES). This system is based on solar water heating and could easily be placed in a road without changing its appearance.\n"}
{"id": "3654631", "url": "https://en.wikipedia.org/wiki?curid=3654631", "title": "Stevenson Plan", "text": "Stevenson Plan\n\nThe Stevenson Plan, also known as the Stevenson Restriction Scheme, was an effort by the British government to stabilize low rubber prices resulting from a glut of rubber following World War I.\n\nIn the early 1900s, increased reliance on the automobile and the use of rubber in common products such as boots were driving demand for rubber. At that time rubber was made from naturally occurring latex extracted from certain plants. The most important of the plants for latex production is the rubber tree, \"Hevea brasiliensis\" whose cultivation is restricted to tropical climates. At this time about 75% of rubber was controlled by British corporations, spurring efforts in Russia, Germany and the United States to reduce dependence on British rubber. All three countries were trying to develop methods of manufacturing synthetic rubber, and the United States Rubber Company began producing natural rubber in Sumatra in 1910. However, synthetic rubber was not yet practical, and natural rubber sources develop rather slowly (rubber trees must grow for six or seven years before they are productive).\n\nBetween 1914 and 1922, natural rubber prices fluctuated between $0.115 and $1.02 per pound for several reasons. One reason is a blight that affected rubber trees in Brazil that reduced productivity and caused British and Dutch rubber producers to start new plantations in Malaya and in the Dutch East Indies. \n\nA second reason was that after the 1917 October Revolution, Russia renewed its effort to make synthetic rubber as part of two projects: 1) Project Bogatyr in which rubber is made from ethyl alcohol, and 2) Project Treugolnik in which the feedstock is petroleum. These projects succeeded in reducing Russian demand for British rubber.\n\nA third reason is that during World War I (1914-1918), demand for rubber was high resulting in new sources of rubber being developed. Following the War, demand for rubber diminished, creating a glut of rubber on the market and very low prices. The world became keenly aware of the importance of a stable supply of rubber for containing and initiating a modern war.\n\nAround 1920 the British Rubber Growers Association turned to then Secretary of State for the Colonies, Winston Churchill, for help. Churchill initiated a committee of inquiry, the Rubber Investigation Committee, consisting primarily of Association members and chaired by Sir James Stevenson, to come up a plan to stabilize rubber prices. The committee came up with the Stevenson Plan which would stabilize prices by limiting the tonnage of rubber exported. The plan was enacted by the governments of Ceylon and British Malaya. The Federal Legislative Council of the Federated Malay States passed the \"Export of Rubber (Restriction) Enactment\" in October 1922, to take effect on November 1.\n\nIn 1922, British interests controlled about 75% of rubber production and the United States consumed about 75% of the rubber produced. The British were still paying war debt to the United States following World War I, and needed to have a profitable rubber industry. The Dutch refused to go along with the plan out of a philosophical reluctance to regulate their industry and because they would profit from a unilateral action by the British. In the United States tiremaker Harvey Firestone reacted angrily to the act as did Secretary of Commerce Herbert Hoover.\n\nBy 1925 high prices resulting from the Stevenson Act were beginning to threaten the \"American way of life\", so Hoover informed the British that if the Stevenson Plan stayed in effect, the United States would try to protect itself in any way it could. DuPont, under the direction of Elmer Keiser Bolton had been working on synthetic rubber since 1920. Thomas Edison, along with several tire companies, was trying to create American-based rubber production, but with little success. By 1928, the Stevenson Act was repealed, but not after expanding Dutch rubber plantations had successfully captured most of the rubber market in the United States.\n\nTired of regulation, rubber producers returned control of rubber prices to the free market. That worked well until the Great Depression in the 1930s lowered demand for rubber, and again rubber prices plunged. Rubber producers once again turned to regulation to maintain prices. This time it was done under the auspices of the International Rubber Regulation Agreement which was signed by all major rubber producing countries. This law succeeded in governing the price of rubber to the satisfaction of producers and most consumers. However, Japan was a consumer of rubber in the 1930s, using rubber to support its war effort in Manchuria and China, and its leaders were not happy with the price of rubber. This was one of the provocations said to motivate the Japanese to the attack on Pearl Harbor and the United States entry into World War II.\n\n"}
{"id": "55666445", "url": "https://en.wikipedia.org/wiki?curid=55666445", "title": "Ternary phase", "text": "Ternary phase\n\nIn materials chemistry, a ternary phase is chemical compound containing three different elements. Some ternary phases compounds are molecular, e.g. chloroform (HCCl). More typically ternary phases refer to extended solids. Famous example are the perovskites.\n\nBinary phases with only two elements, have lower degrees of complexity than ternary phases. With four elements quaternary phases are more complex.\n"}
{"id": "50413563", "url": "https://en.wikipedia.org/wiki?curid=50413563", "title": "The Hurricane Rainband and Intensity Change Experiment", "text": "The Hurricane Rainband and Intensity Change Experiment\n\nThe Hurricane Rainband and Intensity Change Experiment (RAINEX) is a project to improve hurricane intensity forecasting via measuring interactions between rainbands and the eyewalls of tropical cyclones. The experiment was planned for the 2005 Atlantic hurricane season. This coincidence of RAINEX with the 2005 Atlantic hurricane season led to the study and exploration of infamous hurricanes Katrina, Ophelia, and Rita. Where Hurricane Katrina and Hurricane Rita. would go on to cause major damage to the US Gulf coast. Hurricane Ophelia provided an interesting contrast to these powerful cyclones as it never developed greater than a category 1. \nThe RAINEX project was a collaboration between the University of Miami (UM), Rosenstiel School of Marine and Atmospheric Science (RSMAS), The University of Washington, Department of Atmospheric Sciences, The National Oceanic and Atmospheric Administration (NOAA) and the US Navy, Office of Naval Research. \nThe objective of the research was to study the mechanism by which hurricane eyewall replacement cycle occurs. Luckily for the sake of the research, one such case of eyewall replacement occurred during the study of Hurricane Rita. In tropical cyclones maximum wind speed of the storm, which occurs at the eyewall, is a primary indicator of its overall strength which is important in predicting overall intensity. Just beyond this eyewall is a moat which separates the inner rainbands (eventually the outer eyewall) from the (inner) eyewall. Better understanding the dynamics of this region before, and during eyewall replacement could aid in better intensity predictions.\n\nRAINEX’ main purpose was to accomplish this task via studying the fluctuations of storm intensity as they are influenced by interactions between the eye, eyewalls, and rainbands of a tropical cyclone. Previously, tropical cyclone intensity forecasting was heavily based on sea surface temperature and upper-atmosphere dynamics. These factors are useful in predicting the maximum potential of a tropical cyclone. However, since the intensity of a storm undergoes large daily fluctuations, the maximum possible intensity of a cyclone is usually not reached.\n\nMost hurricanes exhibit a definitive eyewall and spiral rainbands outside of the eye. These spiral rainbands were known to be complex structures that possess deep convective cores enmeshed in low altitude precipitative clouds.\n\nThe eye or core of a tropical cyclone is characterized by low pressure which causes warm air to spiral upward and rise into the atmosphere. A tropical cyclone usually develops a distinct eye when the maximum sustained winds of the storm reach and exceed 74 mph. A well-formed eye is a good indicator of overall intensity due to an increase in rotational velocity when the distance between the moving particles and the center of the vortex is decreased. The angular momentum associated with the tropical cyclone can explain this phenomenon.\n\nAngular momentum of a particle with mass, m with respect to the origin, \"r\", can be given by\n\n\"L = mvr(sin(θ))\"\n\nWhen \"r\" is decreased (the distance between the moving particle and the center of the vortex), the mass of this particle, \"m\" remains the same and the angular momentum, \"L\" is conserved. Therefore, the rotational velocity of the particle must increase. In tropical cyclones, when the eye contracts, wind speed increases. \nAnother example of this intensification can be seen in figure skating. When a spinning figure skater pulls his/her arms in to their chest while spinning the distance between the skaters hands and his/her angular momentum is conserved but his/her rotational velocity, \"v\" increases.\n\nThree P-3 Orion aircraft were deployed during 13 flights into Hurricanes Katrina, Rita, and Ophelia. Two of the WP-3D aircraft were owned and operated by NOAA and were named N42 and N43. The P-3 N42 was equipped with a fore and aft fixed flat-plate antenna which served as a dual-beam Doppler weather radar. The P-3, N43 was equipped with one single-parabolic antenna which was able to operate as a dual-Doppler radar by alternating scanning direction (once again between fore and aft). These NOAA aircraft were able to attain 1.5 km horizontal resolution. The third P-3, NRL, was equipped with an ELDORA (Electra Doppler radar) and was the first ELDORA used in the imaging of tropical cyclones. In addition to the radars, each aircraft was equipped with a large quantity of dropsondes to be deployed every 5–10 minutes (about 30–65 km on flight path). During Hurricane Katrina, 302 dropsondes were deployed, during Ophelia 462, and Rita 503. A detailed description of dropsonde specifications can be found in Hock and Franklin 1999. The aircraft transmitted all of the information collected by these instruments to the RAINEX operations center (ROC) at RSMAS during flight in order for the ground team to forecast the development of the tropical cyclone while flight crews were in the air and afterward.\n\nThe experiment entailed a high-resolution numerical model of the internal structure of the vortex and collection of data by three P3 Orion aircraft equipped with dual beam Electra Doppler weather radar and intensive dropsonde coverage. These aircraft were based at the National Oceanic and Atmospheric Administration (NOAA) Aircraft Operations Center (AOC) at MacDill Air Force Base in Tampa, Florida. All flights were controlled from the RAINEX operations center (ROC) at the Rosenstiel School of Marine and Atmospheric Science (RSMAS) at the University of Miami (UM). Postanalysis was to include high-resolution model simulations of the data collected in flight at the RSMAS atmosphere-wave-ocean modeling system.\n\nAs data was collected in the field, satellite communications relayed the information from aircraft to the RAINEX Operations Center at RSMAS. In order to determine which days were suitable for flight, principal investigators, forecasters, pilots, and facility engineering staff held a daily conference call originating from the RSMAS center in Miami, Florida. Based on the forecast of evolution of the tropical cyclone throughout the proposed time of flight, principal investigators would develop a plan of flight for the day. Flight patterns typically followed one of two plans accepting special cases. Plan A was usually selected when aircraft were to arrive during a time without eyewall replacement. Plan B was employed when eyewall replacement was expected to occur during flight. For instance, during flight into Hurricane Rita a second eyewall was forming and Plan B was executed.\n\nBecause RAINEX was planned in advance of the 2005 Atlantic Hurricane Season, it did fly in to Hurricane Katrina among other storms. Hurricane Katrina followed a very similar track to a later storm in this season (Hurricane Rita); however, Katrina did not undergo eyewall replacement during its time in the Gulf of Mexico. RAINEX flights into Hurricane Katrina occurred on August 25, 26, 27, 28, and 29, 2005. These flights followed the storm through its development from a tropical cyclone into a Category 5 hurricane.\n\nHurricane Ophelia was an interesting storm to document due to its long duration and considerable fluctuations in strength throughout its existence. RAINEX flights into Hurricane Ophelia occurred on September 6, 9, and 11, 2005.\n\nHurricane Rita was not a welcome sight in the Gulf of Mexico following the devastating Katrina. Hurricane Rita underwent eyewall replacement while in the Gulf of Mexico where the storm went from a category 5 on the Saffir-Simpson Hurricane Wind Scale to a category 3 storm by landfall. RAINEX flights into Hurricane Rita occurred on September 20, 21, 22, and 23, 2005. These flights observed the rapid development of Hurricane Rita from a Category 1 hurricane into a Category 5 and eventually through its eyewall replacement cycle and weakening.\n\nThe RAINEX database can be found at RAINEX database.\n\n"}
{"id": "39459496", "url": "https://en.wikipedia.org/wiki?curid=39459496", "title": "The New Noah", "text": "The New Noah\n\nThe New Noah is a book written by British naturalist and writer Gerald Durrell. It was first published by Collins in 1955.\n\nThe book is an account for older children of his various expeditions to collect animals for zoos, to some extent an anthology of the best bits from various previous accounts.\n\n"}
{"id": "12294095", "url": "https://en.wikipedia.org/wiki?curid=12294095", "title": "The Solar Car Challenge", "text": "The Solar Car Challenge\n\nThe Solar Car Challenge is an annual solar-powered car race for high school students.The event attracts teams from around the world, but mostly from American high schools. The race was first held in 1995. Each event is the end product of a two-year education cycle launched by the Winston Solar Car Team. On odd-numbered years, the race is a road course that starts at the Texas Motor Speedway, Fort Worth, Texas; the end of the course varies from year to year. On even-numbered years, the race is a track race around the Texas Motor Speedway. Dell sponsored the event from 2002-2008. Hunt Oil Company sponsored the 2010 race. \nIn 1993, the Solar Car Team launched an education program to teach high school students how to build and safely race roadworthy solar cars. The Solar Education Program met this objective, and worked to provide curriculum materials, on-site visits, and workshop opportunities for high schools across the country. This program was designed to motivate students in the sciences, engineering, and technology. The end product of each two-year education cycle is the Solar Car Challenge: a closed-track event at the world famous Texas Motor Speedway, or a cross country race designed to give students an opportunity to display their work.\n\nThe Solar Car Challenge is designed to motivate students into science, engineering, technology, and green energy engineering. The participants not only learn about importance of science to effectively have their car go a farther distance, but also learn the importance of teamwork, where teams work harmoniously with one another to do well in this challenge.\n\nThe participants in this race like to think of this as a challenge, rather than a competition, because this race focuses more detail in partnership and unity. This is how the idea of \"The Spirit of Solar Car Racing\" derived from, establishing an award known as the Marx Award (a prestigious award given to a single team member every year).\n\nTeams experience the fun of the Solar Car Challenge at the world famous Texas Motor Speedway. Car breakdowns, weather, and team experience limit the number of laps a team can drive each day. The team driving the most laps accumulated over the four days of racing will be declared the winner.\n\nThe purpose of the Solar Car Challenge is to provide a level playing-field for high school solar car teams. Newer teams generally enter the Classic Division which requires participants to use less expensive conventional motors, lead acid batteries, and less efficient solar cells. Older teams enter the Open Division based on their use of more expensive technology. The new Advanced Division allows teams to use university body molds and more exotic batteries.\n\nTeams seeking admission to the event must register their vehicle and demonstrate that their solar car complies with all the rules during a qualifying process known as “Scrutineering.” In cross-country races, teams are licensed in Texas as experimental vehicles, and carry liability insurance.\n\nEach car must have a roll cage, “crush zones,” safety harness, horn, communications, turn signals, and a fire extinguisher. Chase vehicles and trailers are available for support in the event of a breakdown on the track. All aspects of the Solar Car Challenge Rules are closely monitored. A wireless computer network helps race officials closely monitor the individual cars.\n\n\nPast races have traveled from Round Rock, which is Dell Computers Headquarters to California, Florida, New York, and Indiana. These races usually take place on odd-numbered years, and takes place at the Texas Motor Speedway on even-numbered years.\n\nCloses Track are which take place at the Texas Motor Speedway usually take place on the odd-numbered years. These races normally go on for a week, while the Cross-Country take two to three weeks.\n\nFormer solar car team members are invited to help run the event in the Intern Program. Interns have come from teams from all over the USA and Mexico. Interns work alongside judges to help with scrutineering, judge teams during the event, and act as spotters during closed.\n\nThe Solar Car Challenge is the product of the Solar Education Program. The Solar Car Challenge Foundation provides an international high school solar education program. Workshops, curriculum materials, DVD’s, and on site visits have introduced this challenge to more than 1100 schools in 20 countries. The Solar Car Challenge Foundation is recognized by the IRS as a 501(c)(3) non-profit educational organization.\n\nCross-country events have traditionally started in Texas (generally either Dallas or Round Rock) and ended in a number of different locations across the USA.\nThe 1995 race was a 70-mile circuit of Dallas County repeated over 3 days. The team completing the race in the shortest period of time was declared the winner. The winner, Los Altos (Hacienda Heights, California)\n1997 Race Rules calculate the winner by the highest average speed. Winner, Solar Shadow, Los Altos School, Hacienda Heights, California\n3 Day Track Race, Winner: Project RayC4 Knowledge (Solar Stealth), C4 Technology Program, Columbus, IN with a total of 390.3 miles\n1999: Dallas, TX to Los Angeles, CA\n7 Day Cross-Country Race: Winner; Solar Stealth, C4 Technology Program, Columbus, IN with a total of 595 miles\n3 Day Track Race, Winner: NFA Solar Racing Team, Newburgh High School, Newburgh, NY with a total of 381 miles\n8 Day Cross-Country Race: Winner; Sundancer II, Houston High School, Houston, MS with 795.1 miles (average speed was 29.6 mph)\n3 Day Track Race, Winner of the Classic Division: The Winston Solar Car Team with 252 miles, Winner of the Open Division: Sundancer II, Houston High School, Houston, MS with 361.5 miles. \n9 Day Cross-Country Race: Overall Winner; Sundancer II, Houston High School, Houston, MS with 689.2 miles\n4 Day Track Race, Winner of the Classic Division: The Winston Solar Car Team with 417 miles, WInner of the Open Division: Sundancer II, Houston High School, Houston, MS with 576 miles.\n8 Day Cross-Country Race: Winner of the Classic Division: Saint Thomas Academy Experimental Vehicle Team, Saint Thomas Academy, Mendota Heights, MN with 959.8 miles (average speed was 26.09 mph), Winner of the Open Division wasSundancer II, Houston High School, Houston, MS with a total of 853.4 miles (average speed was 28.63 mph)\n4 Day Track Race, Winner of the Classic Division was Saint Thomas Academy Experimental Vehicle Team, Saint Thomas Academy, Mendota Heights, MN with 598.5 miles, Winner of the Open Division was Sundancer II, Houston High School, Houston, MS with 624 miles.\nWinner of the Classic Division was Newton County Solar Car Team, Newton County Career & Technical Center \nDecatur, MS (average speed was 17.05 mph), Winner of the Open Division ended with a tie, Sundancer II, Houston High School, Houston, MS (average speed was 38.76 mph) and NFA Solar Racing Team, Newburgh High School, Newburgh, NY (average speed was 38.42 mph) \nFor the 2007 race a tie was declared because both teams finished completed the course (689.1 mi) and had total running time was within 1%\n4 Day Track Race, Winner of the Classic Division was Sundacer I, Houston High School, Houston MS with 495 miles, Winner of the Open Division was Sundancer II, Houston High School, Houston, MS with 657 miles.\n4 Day Track Race, Winner of the Classic Division was Solar Knight II, South Plantation High School, Plantation, FL with 592 miles. Winner of the Open Division was Sundancer II, Houston High School, Houston, MS with 831 miles.\n7 Day Cross-Country Race: Winner of the Classic Division was Sundancer II, Houston High School, Houston, MS with 685 miles. Winner of the Open Division was Sundancer I, Houston High School, Winner of the Advanced division/overall winner Mississippi Choctaw High School Solar Car Project, Choctaw Mississippi\nClosed Track Race that will take place July 18–21\n\n\n"}
{"id": "45275296", "url": "https://en.wikipedia.org/wiki?curid=45275296", "title": "Thorcon", "text": "Thorcon\n\nThorcon is a company that is designing the ThorCon Reactor, a small modular reactor (SMR) that employs molten salt technology, based on the DMSR design from Oak Ridge National Laboratory. It relies on large modules as are used in modern ship building. The ThorCon reactor is a \"burner\" reactor that employs liquid fuel, rather than a conventional solid fuel; this liquid contains the nuclear fuel and also serves as primary coolant.\n\nIn December 2015, Thorcon signed a memorandum of understanding with three Indonesian companies to develop its molten salt reactor technology in Indonesia.\n\nA 2017 study included ThorCon and seven other designs. Conclusion: \"If power plants featuring these technologies are able to produce electricity at the average LCOE price projected here (much less the low-end estimate), it would have a significant impact on electricity markets.\"\n\nIn April 2018, the United States Department of Energy awarded Thorcon $400,000 as a GAIN research project to be conducted jointly by ThorCon USA Inc and Argonne National Laboratory.\n\nThorCon uses modular shipbuilding production processes. except the blocks are barged to the site and dropped into place. Thorcon plans to build its reactors in shipyards, It reauires as much steel as a medium size, 125,000 dwt Suezmax tanker. The reactor consists of two main components, steam/electrical and nuclear. The steam/electrical component features the same design and cost ($700/kw) of a 500 MWe coal plant. A 1 GWe nuclear component requires less than 400 tons of supercritical alloys and other exotic materials.\n\nThe reactor operates at near-ambient pressure, reducing steel requirements by 50% and concrete requirements by 80% versus a conventional reactor. Little of the concrete must be reinforced.\n\nPassive cooling is needed only in the event of overheating, which first stops the reaction, and then triggers freeze valves to drain the reactor. Fluoride salt reacts with hazardous fission products iodine-131, cesium-137 and strontium-90, preventing their release. Each reactor unit operates for four years, cools for four years, and then is replaced. All recycling occurs offsite. Each power module has two siloed reactor units generating 557 MW (thermal) yielding 250 MW (electric).\n\nIn addition to (low cost) thorium, a 1 GWe reactor initially requires 3,156 kg of 20% low enriched uranium along with 11 kg per day of operation. Every 8 years the fuel must be changed out. At a yellowcake cost of $66/kg, a $7.50 conversion cost and $90 per separative work unit, the levelized fuel cost is 0.53 cents per kilowatt-hour.\n\nEvery 8 years 160 tons of spent fuel travel to the recycling facility, consisting of about 75% thorium, with 95% of the balance uranium. Without separation (other than removing the salt), the total fuel waste stream averages about 2 m.\n\nNo greenhouse gases are produced by ThorCon reactors because heat is produced by nuclear fission rather than combustion.\n\n\n"}
{"id": "25397638", "url": "https://en.wikipedia.org/wiki?curid=25397638", "title": "Tier 1 – UK Nuclear Site Management &amp; Licensing", "text": "Tier 1 – UK Nuclear Site Management &amp; Licensing\n\nUnited Kingdom Nuclear Decommissioning Authority (NDA) term for Nuclear Site management licensees, known as Tier 1 contractors, who receive funding from the NDA.\n"}
{"id": "11132202", "url": "https://en.wikipedia.org/wiki?curid=11132202", "title": "Tornado emergency", "text": "Tornado emergency\n\nA tornado emergency is an enhanced version of a tornado warning, which is used by the National Weather Service (NWS) in the United States during significant tornado occurrences in highly populated areas. Although it is not a new warning type from the NWS, issued instead within a severe weather statement (or in rare cases, in the initial tornado warning), a tornado emergency generally means that significant, widespread damage is expected to occur and a high likelihood of numerous fatalities is expected with a large, strong to violent tornado. \n\nThese enhanced warnings are intended to convey the urgency of the weather situation to the general public, who are advised to take safety precautions immediately if they are in or near the projected path of a large tornado or its accompanying thunderstorm; tornado emergencies are usually identified following the preceding storm summary in the tornado warning product, which itself will denote visual or radar confirmation of \"a large and extremely dangerous [or destructive] tornado\" that is ongoing; precautionary action statements in the product also recommend that people in the storm's path find shelter in an underground shelter or safe room to protect themselves from the storm, if available. \n\nWhile many tornadoes observed to be at or larger than ¼-mile in width have been documented to have produced catastrophic damage falling under the \"strong\" or \"violent\" categories (EF2-EF5) of the Enhanced Fujita Scale, there have been instances in which tornadoes of this have resulted in very few to no fatalities and, occasionally, have produced damage corresponding to the Fujita Scale's \"weak\" category (EF0-EF1).\n\nThe term was first used during the May 3, 1999 tornado outbreak that spawned an F5 tornado which struck the municipalities of Bridge Creek and Moore, located just south of Oklahoma City, followed by southern and eastern parts of the city itself, Del City, and Midwest City. On that day, between 5:30 and 6:30 p.m., David Andra, the Science and Operations Officer at the National Weather Service Weather Forecast Office in Norman watched as the large, destructive tornado approached Oklahoma City. This led to the issuance of the first tornado emergency, which in this instance was released as a standalone weather statement issued separately from the original tornado warning. \"As the large tornado approached western sections of the OKC metro area, we asked ourselves more than once, 'Are we doing all we can do to provide the best warnings and information?' It became apparent that unique and eye-catching phrases needed to be included in the products. At one point we used the phrase 'Tornado Emergency' to paint the picture that a rare and deadly tornado was imminent in the metro area. We hoped that such dire phrases would prompt action from anyone that still had any questions about what was about to happen.\"\n\nSource:\n\nAt 3:01 p.m. CDT on May 20, 2013, this bulletin was issued by the National Weather Service Norman forecast office confirming that a destructive tornado was on the ground and headed for Moore and southern portions of Oklahoma City, Oklahoma. Note the differences between this bulletin and the first-ever bulletin from 1999. It should also be noted that this was issued during an updated tornado warning and not in a follow-up severe weather statement. The Moore disaster did claim many lives, but this emergency prevented many more fatalities. The type of tornado, EF5, is a very deadly type, thus this bulletin was issued, with the particularly dangerous situation phrase intended.\n\nSource:\n\nThis tornado, rated EF3, tornado caused severe damage on the south side of Kokomo, Indiana, and was the only tornado emergency in Howard County history, and the second ever tornado emergency issued by the National Weather Service in Indianapolis. The warning again saved Kokomo from many deaths. However, this was in a severe weather statement, not a tornado warning.\n\nAfter the original usage for the May 3, 1999 F5 tornado, the term Tornado Emergency was used by other National Weather Service Weather Forecast Offices (WFOs), although no uniform criteria existed and the issuance was entirely at the discretion of the forecaster issuing the warnings. Usage of the term varied from simply confirmed tornadoes in populated areas to significant, rare tornadoes causing severe damage and injuries. Some NWS forecast offices, such as the one serving the Des Moines, Iowa metropolitan area, have created standardized criteria and purpose for the usages of the heightened wording. Because data about the tornado and its exact path are often ascertained after the initial tornado warning is issued, this designation is usually added to the Severe Weather Statement (SAME code: SVS) that is used to follow up a tornado warning.\n\nOn April 2, 2012, the National Weather Service began an experimental program within its Wichita, Topeka, Springfield, St. Louis and Kansas City/Pleasant Hill offices in Kansas and Missouri called Impact Based Warning, which allows the respective offices to enhance warning information, such as adding tags to the warning messages which signify the potential damage severity. In regards to tornadoes, the creation of this multi-tiered system resulted in the implementation of an intermediate tornado warning product, a Particularly Dangerous Situation Tornado Warning.\n\nOn April 1, 2013, the IBW experiment expanded to include all National Weather Service WFOs within the Central Region; the IBW experiment was expanded again to include eight additional offices within the Eastern, Southern and Western Regions in the spring of 2014. Within the span of eleven days, the National Weather Service WFO in Norman issued tornado emergencies for parts of the Oklahoma City metropolitan area and central Oklahoma: first on May 20, 2013 for the EF5 tornado that struck Moore and portions of southern Oklahoma City, and again on May 31, for portions of eastern Canadian County and western sections of the immediate Oklahoma City area for another tornado.\n\nThe usage of tornado emergencies to alert major population centers to the imminent threat of a catastrophic tornado impact has also led to the development of the flash flood emergency which is similarly employed when severe flash floods threaten populated areas.\n\nThe National Weather Service Weather Forecast Office in Des Moines is one of the forecast offices to have created a set purpose and criteria for the usage of \"tornado emergencies\" in tornado warning products, which were made effective on March 12, 2010. According to the Des Moines office, the purpose of the tornado emergency wording is as follows:\nAnd before usage, the following criteria must be met:\n\nThe National Weather Service office in Nashville, Tennessee also created criteria to declare a tornado emergency within a tornado warning statement effective January 1, 2011. It states, \"Tornado Emergency can be inserted in the third bulletin of the initial tornado warning (TOR) or in a severe weather statement (SVS).\" Before the phrase can be used:\nThe Washington, Illinois tornado on November 17, 2013 did not prompt a Tornado Emergency; however, it affected many people. The town of Washington is not populated enough; however, it has a population of approximately 15,000 residents. The National Weather Service in Lincoln may have similar criteria, however.\n\nIt is recommended that people in the path of a large and violent tornado, whether referenced in a tornado warning or a tornado emergency, seek shelter in a basement, cellar or safe room, as stronger tornadoes (particularly those significant enough to warrant the inclusion of a tornado emergency declaration within a tornado warning) pose a significant risk of major injury or death for people above ground level. Those who do not have below-ground shelter are still advised to take cover in a room in the center of the home on the lowest floor, and cover themselves with some type of thick padding (such as mattresses or blankets), to protect against falling debris in the event that the roof and ceiling collapse.\n\nThe audible alert to the right was issued in Sumiton, Alabama, in 2015.\n"}
{"id": "1552050", "url": "https://en.wikipedia.org/wiki?curid=1552050", "title": "Training (meteorology)", "text": "Training (meteorology)\n\nIn meteorology, training denotes repeated areas of rain, typically associated with thunderstorms, that move over the same region in a relatively short period of time. Training thunderstorms are capable of producing excessive rainfall totals, often causing flash flooding. The name \"training\" is derived from how a train and its cars travel along a track (moving along a single path), without the track moving.\n\nShowers and thunderstorms along thunderstorm trains usually develop in one area of stationary instability, and are advanced along a single path by prevailing winds. Additional showers and storms can also develop when the gust front from a storm collides with warmer air outside of the storm. The same process repeats in the new storms, until overall conditions in the surrounding atmosphere become too stable for support of thunderstorm activity. Showers and storms can also develop along stationary fronts, and winds move them down the front. The showers that often accompany thunderstorms are usually thunderstorms that are not completely developed.\n\nA series of storms continually moving over the same area, dumping heavy rains, can cause flash flooding. Each storm usually produces heavy rain, and after a significant amount of rain falls from the storms which have moved over the same area, flooding occurs.\n\nThunderstorm training is used to refer specifically to training occurring with thunderstorms. It forms when storms tend to backbuild. This type of training can quickly cause flash flooding, especially if the thunderstorms are strong.\n\n"}
{"id": "2433190", "url": "https://en.wikipedia.org/wiki?curid=2433190", "title": "Variable-frequency transformer", "text": "Variable-frequency transformer\n\nA variable-frequency transformer (VFT) is used to transmit electricity between two (asynchronous or synchronous) alternating current frequency domains. The VFT is a relatively recent development. Most asynchronous grid inter-ties use high-voltage direct current converters, while synchronous grid inter-ties are connected by lines and \"ordinary\" transformers, but without the ability to control power flow between the systems.\n\nIt can be thought of as a very high power synchro, or a rotary converter acting as a frequency changer, which is more efficient than a motor–generator of the same rating.\n\nA variable-frequency transformer is a doubly fed electric machine resembling a vertical shaft hydroelectric generator with a three-phase wound rotor, connected by slip rings to one external power circuit. The stator is connected to the other. With no applied torque, the shaft rotates due to the difference in frequency between the networks connected to the rotor and stator. A direct-current torque motor is mounted on the same shaft; changing the direction of torque applied to the shaft changes the direction of power flow.\n\nThe variable-frequency transformer behaves as a continuously adjustable phase-shifting transformer. It allows control of the power flow between two networks. Unlike power electronics solutions such as back-to-back HVDC, the variable frequency transformer does not demand harmonic filters and reactive power compensation. Limitations of the concept are the current-carrying capacity of the slip rings for the rotor winding.\n\nFive small variable-frequency transformer with a total power rate of 25 MVA were in use at Neuhof Substation, Bad Sachsa, Germany for coupling power grids of former East and West Germany between 1985 and 1990.\n\nLanglois Substation in Québec, Canada () installed a 100 MW variable-frequency transformer in 2004 to connect the asynchronous grids in Québec and the northeastern United States. This was the first large scale, commercial variable frequency transformer, and was installed at Hydro-Québec Langlois substation and is located electrically near sixteen hydro generators at Les Cèdres, Quebec and thirty-six more hydro generators at Beauharnois, Quebec. The operating experience since April 2004 has demonstrated the VFT’s inherent compatibility with the nearby generators\n\nAEP Texas installed a 100 MW VFT substation in Laredo, Texas, United States () in early 2007. It connects the power systems of ERCOT (in the United States) to CFE (in Mexico). (See The Laredo VFT Project.)\n\nSmaller VFTs are used in large land-based wind turbines, so that the turbine rotation speed can vary while connected to an electric power distribution grid.\n\nGeneral Electric installed a 3 × 100 MW VFT substation in Linden, New Jersey, in the United States in 2009. It connects the power systems of PJM & New York Independent System Operator (NYISO). This installation is in parallel with three existing phase-shifting transformers to regulate synchronous power flow.\n\nVFTs provide the technical feasibility to flow power in both directions between two grids, permitting power exchanges that were previously impossible. Energy in a grid with lower costs can be transmitted to a grid with higher costs (higher demand), with energy trading. Power capacity is sold by providers. Transmission scheduling rights (TSRs) are auctioned by the transmission line owners.\n\nFinancial Transmission Rights (FTRs) are a financial instrument used to balance energy congestion and demand costs.\n\n\n"}
{"id": "104524", "url": "https://en.wikipedia.org/wiki?curid=104524", "title": "Yo-yo", "text": "Yo-yo\n\nA yo-yo (also spelled yoyo) is a toy consisting of an axle connected to two disks, and a string looped around the axle. It has some similarities to a slender spool. \n\nA yo-yo is played by holding the free end of the string known as the handle (by inserting one fingerusually the middle or index fingerinto a slip knot) allowing gravity (or the force of a throw and gravity) to spin the yo-yo and unwind the string (similar to how a pullstring works). The player then allows the yo-yo to wind itself back to the player's hand, exploiting its spin (and the associated rotational energy). This is often called \"yo-yoing\". \n\nIn the simplest play, the string is intended to be wound on the spool by hand; The yo-yo is thrown downwards, hits the end of the string, then winds up the string toward the hand, and finally the yo-yo is grabbed, ready to be thrown again. One of the most basic tricks is called the sleeper, where the yo-yo spins at the end of the string for a noticeable amount of time before returning to the hand.\n\nThe word probably comes from the Ilocano word yóyo, which is a cognate word from the Philippines.\n\nA Greek vase painting from 440 BC shows a boy playing with a yo-yo (see right). Greek records from the period describe toys made out of wood, metal, or painted terra cotta (fired clay). The terra cotta disks were used to ceremonially offer the toys of youth to certain gods when a child came of age—discs of other materials were used for actual play.\n\nIn 1928, Pedro Flores, a Filipino immigrant to the United States, opened the Yo-yo Manufacturing Company in Santa Barbara, California. The business started with a dozen handmade toys; by November 1929, Flores was operating two additional factories in Los Angeles and Hollywood, which altogether employed 600 workers and produced 300,000 units daily.\n\nThe principal distinction between the Filipino design popularized by Flores and more primitive yo-yos is in the way the yo-yo is strung. In older (and some remaining inexpensive) yo-yo designs, the string is tied to the axle using a knot. With this technique, the yo-yo just goes back-and-forth; it returns easily, but it is impossible to make it sleep. In Flores's design, one continuous piece of string, double the desired length, is twisted around something to produce a loop at one end which is fitted around the axle. Also termed a \"looped slip-string\", this seemingly minor modification allows for a far greater variety and sophistication of motion, thanks to increased stability and suspension of movement during free spin.\n\nShortly thereafter (c. 1929), an entrepreneur named Donald F. Duncan recognized the potential of this new fad and purchased the Flores yo-yo Corporation and all its assets, including the Flores name, which was transferred to the new company in 1932. \n\nThe name \"Yo-yo\" was registered in 1932 as a trademark by in Vancouver, Canada, and Harvey Lowe won the first World Yo-Yo Contest in London, England. In 1932, Swedish Kalmartrissan yo-yos started to be manufactured as well.\n\nIn 1933 yo-yos were banned in Syria, because many locals superstitiously blamed the use of them for a severe drought. \n\nIn 1946, the Duncan Toys Company opened a yo-yo factory in Luck, Wisconsin. The Duncan yo-yo was inducted into the National Toy Hall of Fame at The Strong in Rochester, New York, in 1999.\n\nDeclining sales after the Second World War prompted Duncan to launch a comeback campaign for his trademarked \"Yo-Yo\" in 1962 with a series of television advertisements.\n\nIn a trademark case in 1965, a federal court's appeals ruled in favor of the Royal Tops Company, determining that \"yo-yo\" had become a part of common speech and that Duncan no longer had exclusive rights to the term. As a result of the expenses incurred by this legal battle as well as other financial pressures, the Duncan family sold the company name and associated trademarks in 1968 to Flambeau, Inc, who had manufactured Duncan's plastic models since 1955. , Flambeau Plastics continued to run the company.\n\nAs popularity spread through the 1970s and 1980s, there were a number of innovations in yo-yo technology, primarily regarding the connection between the string and the axle. In 1979, dentist and yo-yo celebrity Tom Kuhn patented the “No Jive 3-in-1” yo-yo, creating the world's first \"take-apart\" yo-yo, which enabled yo-yo players to change the axle.\n\nSwedish bearing company SKF briefly manufactured novelty yo-yos with ball bearings in 1984. In 1990, Kuhn introduced the SB-2 yo-yo that had an aluminum transaxle, making it the first successful ball-bearing yo-yo.\n\nIn all transaxle yo-yos, ball bearings significantly reduce friction when the yo-yo is spinning, enabling longer and more complex tricks. Subsequent yo-yoers used this ability to their advantage, creating new tricks that had not been possible with fixed-axle designs.\n\nThere are many new types of ball bearings in the market which deviate from the original design and/or material of the standard stainless steel ball bearing. For example, a certain type of bearing has an inward facing curved surface, to prevent the string from rubbing on the sides of the yo-yo, which would cause unwanted friction when performing intricate string tricks. Other manufacturers replicate this with a similar inwardly curved surface, but use minor modifications. Some high-end bearings use ceramic composites in the balls of the bearing, to reduce internal friction, again making for a smoother spinning yo-yo.\n\nThe sleeper is one of the most common yo-yo throws and is the basis for nearly all yo-yo throws other than looping. Keeping a yo-yo spinning while remaining at the end of its uncoiled string is known as sleeping. While the yo-yo is in the \"sleeping\" state at the end of the string, one can then execute tricks like \"walk the dog\", \"around the world\", or the more complex \"rock the baby\".\n\nThe essence of the throw is that one throws the yo-yo with a very pronounced wrist action so that when the yo-yo reaches the end of the string it spins in place rather than rolling back up the string to the thrower's hand. Most modern yo-yos have a transaxle or ball bearing to assist this, but if it is a fixed axle yo-yo, the tension must be loose enough to allow this. The two main ways to do this are (1), allow the yo-yo to sit at the bottom of the string to unwind, or (2) perform lariat or UFO to loosen the tension. When one decides to end the \"sleeping\" state, one merely jerks the wrist and the yo-yo \"catches\" the string and rolls back up to the hand. Ball-bearing yo-yos with a \"butterfly\" shape, primarily used for string tricks, frequently (but not always) have low response (or are, in fact, completely unresponsive), requiring a \"bind\" for the yo-yo to return.\n\nIn competition, mastery of sleeping is the basis for the 1A division. Inexpensive fixed-axle yo-yos usually spin between 10–20 seconds, while expensive ball bearing yo-yos can spin about 1–4 minutes depending on the throw , the world record sleep times were 3:51.54 minutes for fixed-axle and 21:15.17 minutes for transaxle yo-yos. In 2012, the transaxle yo-yo sleep time record was broken by the C3YoyoDesign BTH, with a time of 30:28.30 minutes.\n\nLooping is a yo-yo technique which emphasizes keeping the body of the yo-yo in constant motion, without sleeping.\n\nYo-yos optimized for looping have weight concentrated in their centers so they may easily rotate about the string's axis without their mass contributing to a resistance due to a gyroscopic effect.\n\nIn yo-yo competitions, looping both to the inside and outside of the hand with the yo-yo plays a strong role in the 2A division.\n\nIn the \"off-string\" technique, the yo-yo's string is not tied directly to the yo-yo's axle, and the yo-yo is usually launched into the air by performing a \"forward pass\" to be caught again on the string.\nHowever, some players can 'throw down' off-string yo-yos and catch it on the string just as it leaves the end of the string by pivoting the string around a finger as it unwinds, so that the yo-yo is caught on the string. This is exactly the opposite of a \"forward pass\", but with the same result.\n\nYo-yos optimized for off-string tricks have flared designs, like the butterfly shape, which makes it easier to land on the string, and often have soft rubber rings on the edges, so minimum damage is inflicted on the yo-yo, the player, or anyone who happens to be standing nearby, should a trick go wrong.\n\nYo-yo competitions have the 4A division for off-string tricks.\n\nIn freehand (5A) tricks, the yo-yo's string is not tied to the player's hand, instead ending in a counterweight. The counterweight is then thrown from hand to hand and used as an additional element in the trick.\n\nDeveloped in 1999 by Steve Brown, as of 2008 freehand is considered to be the fastest-growing style of yo-yo play. Steve Brown was awarded a patent on his freehand yo-yo system, which was assigned to Flambeau Products (Duncan's parent company).\n\nIn yo-yo competitions, counterweight yo-yos are emphasized in the 5A division.\n\nWhen the yo-yo is first released, the gravity (and the throw) give it translational kinetic energy and necessarily, since the string must unwind, much of this energy is converted into rotational kinetic energy establishing the free move of the yo-yo, and causing it to spin rapidly. As the yo-yo unwinds downwards, it also converts potential energy from gravity to both translational and rotational kinetic energy. Because the yo-yo has significant rotational inertia, it can store enough energy in its rotation to overcome gravity all the way back up to the hand.\n\nBecause the sense of spinning does not change during the whole move, the string winds up in the opposite direction upon the return of the yo-yo. If the shaft of the yo-yo is connected to the string with a loop, there may not be enough frictional force to overcome the weight of the yo-yo, which is necessary to begin winding up the string. In this case, the yo-yo will continue to spin in the loop at the end of the string (= \"sleep\"), just being slightly braked by the small dynamic friction, instead of returning. However, if the string is jerked slightly up, this will increase the force from the string on the shaft, thereby rising the boundary for static friction, proportional to the \"weight\" of the yo-yo, above the current frictional force, making it static friction, thereby forcing the rotational energy of the yo-yo to wind up the string and finish the rest of the return.\n\nPatents have been issued to create more complicated mechanisms to allow tension control and an adjustable mechanism.\n\n\n"}
