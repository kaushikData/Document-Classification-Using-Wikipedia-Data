{"id": "25795844", "url": "https://en.wikipedia.org/wiki?curid=25795844", "title": "Alexander Carnegie Kirk", "text": "Alexander Carnegie Kirk\n\nAlexander Carnegie Kirk (16 July 1830 – 5 October 1892) was a Scottish engineer responsible for several major innovations in the shipbuilding, refrigeration, and oil shale industries of the 19th century. Kirk, born in Barry, Angus, received his formal education at the University of Edinburgh and a technical education at plants operated by Robert Napier and Sons.\n\nAlexander Carnegie Kirk was the eldest son of John Kirk (died 1858) and Christian Guthrie, née Carnegie, (died 1865). The naturalist John Kirk was his younger brother. A.C. Kirk married Ada Waller at Croydon in 1869 and they had six children.\n\nIn 1850, Kirk began a five-year apprenticeship with Robert Napier and Sons. In 1861, he became chief draughtsman at Maudslay, Sons and Field in London but this seems to have lasted less than a year. Later in 1861 he became an engineering manager in the shale-oil industry, working for James Young. During this employment he developed an oil shale retort and a refrigeration technology, involving the delivery of ether. The latter was to address production problems stemming from summer heat. In 1865 he joined the management of James Aitken and Company, an engine works in Glasgow. In 1870 he was appointed manager of the John C. Elder engineering works. After returning to the Napier firm as a senior partner in 1877, his work was thereafter focused on marine engineering. His triple-expansion engines as designed for the steamship \"Propontis\" were unsuccessful, but his subsequent versions of the engine design, particularly those designed for the steamship \"Aberdeen\", are credited as technological breakthroughs.\n\nHe served as the President of The Institution of Engineers and Shipbuilders in Scotland from 1887 to 1889.\n\n"}
{"id": "17903747", "url": "https://en.wikipedia.org/wiki?curid=17903747", "title": "Burn Up (miniseries)", "text": "Burn Up (miniseries)\n\nBurn Up is a 2008 BBC/Global Television Drama dealing with the issues of climate change and peak oil.\n\nOil surveyor Masud Khamil narrowly escapes an attack on his camp in the deserts of Saudi Arabia and flees with some valuable data. Tom McConnell replaces his father-in-law Sir Mark Foxbay as Chairman of Arrow Oil. At the celebration party, Inuit activist Mika Namuvai enters uninvited and serves Tom a writ from her people. The writ decries global warming and its causes, such as Arrow's production facility on the Athabasca tar sands. The shock of Mika's subsequent ejection causes Tom's daughter to have an asthma attack, but she is saved by Holly Dernay, head of renewable energy at Arrow.\n\nIn testimony before the US Senate in Washington, Sir Richard Langham, Tom's former geology professor, testifies on the dangers of global warming, but he is discredited by a pro-oil senator using muck racked up by devious oil lobbyist James \"Mack\" Mackintosh. Tom confronts Mack, his longtime friend, about the tactics. Mack denies that he spread the story, but he is clearly committed to serving his employers. Back in London, Tom wins his court case but then sees Mika set fire to herself on the court steps.\n\nTom and Holly attend Mika’s funeral in Alberta, where Sir Richard sends them into the wilderness to see the truth for themselves. Holly pokes a hole in the ice covering a swamp and is able to ignite the methane gas waiting to be released. Later, while overnighting in an isolated shack on the tundra, Tom and Holly become intimate.\n\nMeanwhile, British Prime-Ministerial Aide Philip Crowley offers unofficial government assistance to the Green Congress as they prepare for the World Forum on Climate Change (WFCC), a United Nations conference about global warming in Calgary, Alberta, Canada.\n\nMasud returns to London and makes contact with Sir Mark, who has spoken with Masud before and agrees to meet to accept some confidential geology data. Sir Mark is clearly conflicted over what to do with the data.\n\nTom and Holly discuss pulling out of Athabasca, which has the most polluting oil of Arrow's many interests, and investing that money in renewables. Phillip confronts Holly with his knowledge that the desert massacre was done by mercenaries for the oil industry and not terrorists. But before she can talk to Tom, Tom finds Mack at her overturned apartment, where Mack reveals that Holly orchestrated Mika's protest at the party. Phillip stops Holly from returning to her apartment, instructing her to lie low. Tom wanders off to visit Sir Mark. Sir Mark hears a noise and goes outside, only to be struck dead in a hit-and-run. Tom and Masud, approaching from different directions, are witnesses. Tom runs to help, and Masud flees.\n\nCrowley sends Dernay to the World Forum on Climate Change in Calgary to look for Masud. Mack leads the Open Business Coalition and their friendly countries in moves to scupper Kyoto II. Dernay contacts Masud and tries to go to McConnell but he dismisses her. Crowley gets support within the Open Business Coalition from McConnell and the insurance companies and they win over the United States Treasury Secretary Brent Schlaes. Under pressure from Schlaes the US delegate softens his stance at the conference. Mack feeling under threat from his boss tries to scare off Dernay who has finally convinced McConnell to meet with Masud. At the meeting Masud hands over the data from the survey Foxbay commissioned that proves that the Saudi oil reserves have run out.\n\nAt a meeting in the Calgary Tower Mack warns McConnell of the chaos that will be unleashed if Masud’s data is published. The hard-line Robert Cooper is appointed as the new US delegate and goes back on the agreements of his predecessor. The Chinese delegate makes approaches to McConnell and the Green Congress so Crowley brokers a deal with them to sign Kyoto II. Dernay tries to bluff Mack and he warns her off but he is too late and she is killed. Mack confronts his boss who threatens to go after McConnell next to get the data. Cooper agrees a sideline treaty with the Chinese and walks out without signing Kyoto II. Mack confronts McConnell on the roof and after getting the data from him, he smuggles it past his boss’s men and hands it over to Crowley for publication.\n\nBBC Controller of Fiction Jane Tranter commissioned the film for transmission on BBC Two.\n\nThe film was produced by Kudos Film and Television in co-production with Canadian production company SEVEN24 Films.\n\nShot in Calgary, Alberta and London, England, the film is unusual for actually being set in Calgary, prompting Eric Volmers to write in the \"Calgary Herald\" that, \"while filming in the city is hardly unique, it's rare for a high-profile project to feature Calgary as Calgary.\"\n\nThe screenplay was written by Simon Beaufoy. See his works below.\n\nEric Volmers writing in the \"Calgary Herald\" states that, \"the opening scene is a suitably violent and tense intro that manages to conjure up both a sinister and international tone to the four-hour Canadian-U.K. co-production,\" and goes on to say that, \"Burn Up asks decidedly unsettling questions about big oil and political expediency, the environmental impact of the Athabasca oil sands and even American foreign policy,\" which he claims, \"was perhaps a surprising project to find life amid the \"oil-is-good\" sentiment of Alberta.\"\n\nSam Wollaston writing in \"The Guardian\" compliments the character development and interaction that is, \"going on all over the place,\" and credits Whitford and Warren as \"the stars of Burn Up,\" before going on to say Penry-Jones is, \"lively in the lead.\" He claims the film is an improvement on recent docudramas in that, \"it has a proper script\" and that while, \"the across-the-green-line romance is a little embarrassing,\" \"the whole thing skips along,\" and, \"at times it thrills, which is no bad thing in a thriller,\" but it, \"is issue-led, rather than story-led,\" and overall it feels, \"a bit crude,\" \"as opposed to refined.\" Gareth McLean writing in the same publication said that, \"despite the occasional flashy moment, high-octane drama this is not,\" criticising the film for, \"copious amounts of tiresome expositional dialogue and a leaden plot,\" before concluding, \"why not treat yourself and rent Syriana instead?\"\n\nTim Teeman writing in \"The Times\" calls the film, \"a consciousness-raising thriller,\" that, \"is trying desperately to do that BBC thing of being sexy and responsible.\" He states that, \"the arguments about oil production and the environment felt clunky and little more than speechifying,\" that the film was, \"in danger of forgetting that drama should be drama, not a lecture, and that characters should be characters, not ciphers,\" but, \"despite the didactic politicking, the pace is unrelenting and the baddies at least are colourful and engaging.\" He complimented Whitford who, \"villainously devoured every scene,\" for his, \"needling chemistry\" with the, \"brilliant,\" Warren, and concluded that, \"Whitford and Campbell together, him snarling, her looking all impish and button-nosed, had me almost pathetically excited: The West Wing and Party of Five united on screen is pretty near Teeman TV Heaven.\" David Chater writing in the same publication describes the film as, \"highly watchable,\" thanks to, \"a fast-paced, intelligent script and some fine acting,\" although the, \"characters tend to come with labels, and the elements of thriller are there for excitement rather than credibility\", he concludes, \"it is still an exciting ride in an eco-friendly racer,\" and, \"it’s like Doctor Who for grown-ups.\"\n\n"}
{"id": "261297", "url": "https://en.wikipedia.org/wiki?curid=261297", "title": "California electricity crisis", "text": "California electricity crisis\n\nThe California electricity crisis, also known as the Western U.S. Energy Crisis of 2000 and 2001, was a situation in which the U.S. state of California had a shortage of electricity supply caused by market manipulations, and capped retail electricity prices. The state suffered from multiple large-scale blackouts, one of the state's largest energy companies collapsed, and the economic fall-out greatly harmed Governor Gray Davis' standing.\n\nDrought, delays in approval of new power plants, and market manipulation decreased supply. This caused an 800% increase in wholesale prices from April 2000 to December 2000. In addition, rolling blackouts adversely affected many businesses dependent upon a reliable supply of electricity, and inconvenienced a large number of retail consumers.\n\nCalifornia had an installed generating capacity of 45 GW. At the time of the blackouts, demand was 28 GW. A demand supply gap was created by energy companies, mainly Enron, to create an artificial shortage. Energy traders took power plants offline for maintenance in days of peak demand to increase the price. Traders were thus able to sell power at premium prices, sometimes up to a factor of 20 times its normal value. Because the state government had a cap on retail electricity charges, this market manipulation squeezed the industry's revenue margins, causing the bankruptcy of Pacific Gas and Electric Company (PG&E) and near bankruptcy of Southern California Edison in early 2001.\n\nThe financial crisis was possible because of partial deregulation legislation instituted in 1996 by the California Legislature (AB 1890) and Governor Pete Wilson. Enron took advantage of this deregulation and was involved in economic withholding and inflated price bidding in California's spot markets.\n\nThe crisis cost between US$40 to $45 billion.\n\nAs the FERC report concluded, market manipulation was only possible as a result of the complex market design produced by the process of partial deregulation. Manipulation strategies were known to energy traders under names such as \"Fat Boy\", \"Death Star\", \"Forney Perpetual Loop\", \"Ricochet\", \"Ping Pong\", \"Black Widow\", \"Big Foot\", \"Red Congo\", \"Cong Catcher\" and \"Get Shorty\". Some of these have been extensively investigated and described in reports.\n\nMegawatt laundering is the term, analogous to money laundering, coined to describe the process of obscuring the true origins of specific quantities of electricity being sold on the energy market. The California energy market allowed for energy companies to charge higher prices for electricity produced out-of-state. It was therefore advantageous to make it appear that electricity was being generated somewhere other than California.\n\nOverscheduling is a term used in describing the manipulation of capacity available for the transportation of electricity along power lines. Power lines have a defined maximum load. Lines must be booked (or \"scheduled\") in advance for transporting bought-and-sold quantities of electricity. \"Overscheduling\" means a deliberate reservation of more line usage than is actually required and can create the appearance that the power lines are congested. Overscheduling was one of the building blocks of a number of scams. For example, the \"Death Star\" group of scams played on the market rules which required the state to pay \"congestion fees\" to alleviate congestion on major power lines. \"Congestion fees\" were a variety of financial incentives aimed at ensuring power providers solved the congestion problem. But in the Death Star scenario, the congestion was entirely illusory and the congestion fees would therefore simply increase profits.\n\nIn a letter sent from David Fabian to Senator Boxer in 2002, it was alleged that:\n\nOn a federal level, the Energy Policy Act of 1992, for which Enron had lobbied, opened electrical transmission grids to competition, unbundling generation and transmission of electricity.\n\nOn the state level, part of California's deregulation process, which was promoted as a means of increasing competition, was also influenced by lobbying from Enron, and began in 1996 when California became the first state to deregulate its electricity market. Eventually a total of 40% of installed capacity – 20 gigawatts – was sold to what were called \"independent power producers.\" These included Mirant, Reliant, Williams, Dynegy, and AES. The utilities were then required to buy their electricity from the newly created day-ahead only market, the California Power Exchange (PX). Utilities were precluded from entering into longer-term agreements that would have allowed them to hedge their energy purchases and mitigate day-to-day swings in prices due to transient supply disruptions and demand spikes from hot weather.\nThen, in 2000, wholesale prices were deregulated, but retail prices were regulated for the incumbents as part of a deal with the regulator, allowing the incumbent utilities to recover the cost of assets that would be stranded as a result of greater competition, based on the expectation that \"frozen\" rates would remain higher than wholesale prices. This assumption remained true from April 1998 through May 2000.\n\nEnergy deregulation put the three companies that distribute electricity into a tough situation. Energy deregulation policy froze or capped the existing price of energy that the three energy distributors could charge. Deregulating the producers of energy did not lower the cost of energy. Deregulation did not encourage new producers to create more power and drive down prices. Instead, with increasing demand for electricity, the producers of energy charged more for electricity. The producers used moments of spike energy production to inflate the price of energy. In January 2001, energy producers began shutting down plants to increase prices.\n\nWhen electricity wholesale prices exceeded retail prices, end user demand was unaffected, but the incumbent utility companies still had to purchase power, albeit at a loss. This allowed independent producers to manipulate prices in the electricity market by withholding electricity generation, arbitraging the price between internal generation and imported (interstate) power, and causing artificial transmission constraints. This was a procedure referred to as \"gaming the market.\" In economic terms, the incumbents who were still subject to retail price caps were faced with inelastic demand (see also: Demand response). They were unable to pass the higher prices on to consumers without approval from the public utilities commission. The affected incumbents were Southern California Edison (SCE) and Pacific Gas & Electric (PG&E). Pro-privatization advocates insist the cause of the problem was that the regulator still held too much control over the market, and true market processes were stymied, whereas opponents of deregulation assert that the fully regulated system had worked for 40 years without blackouts.\n\nBy keeping the consumer price of electricity artificially low, the California government discouraged citizens from practicing conservation. In February 2001, California governor Gray Davis stated, \"Believe me, if I wanted to raise rates I could have solved this problem in 20 minutes.\"\n\nEnergy price regulation incentivized suppliers to ration their electricity supply rather than expand production. The resulting scarcity created opportunities for market manipulation by energy speculators.\n\nState lawmakers expected the price of electricity to decrease due to the resulting competition; hence they capped the price of electricity at the pre-deregulation level. Since they also saw it as imperative that the supply of electricity remain uninterrupted, utility companies were required by law to buy electricity from spot markets at uncapped prices when faced with imminent power shortages.\n\nWhen the electricity demand in California rose, utilities had no financial incentive to expand production, as long term prices were capped. Instead, wholesalers such as Enron manipulated the market to force utility companies into daily spot markets for short term gain. For example, in a market technique known as megawatt laundering, wholesalers bought up electricity in California at below cap price to sell out of state, creating shortages. In some instances, wholesalers scheduled power transmission to create congestion and drive up prices.\n\nAfter extensive investigation, the Federal Energy Regulatory Commission (FERC) substantially agreed in 2003:\n\nThe major flaw of the deregulation scheme was that it was an incomplete deregulation – that is, \"middleman\" utility distributors continued to be regulated and forced to charge fixed prices, and continued to have limited choice in terms of electricity providers. Other, less catastrophic energy deregulation schemes, such as Pennsylvania's, have generally deregulated utilities but kept the providers regulated, or deregulated both.\n\nIn the mid-90's, under Republican Governor Pete Wilson, California began changing the electricity industry. Democratic State Senator Steve Peace was the Chairman of the Senate Committee on Energy at the time and is often credited as \"the father of deregulation\". The author of the bill was Senator Jim Brulte, a Republican from Rancho Cucamonga. Wilson admitted publicly that defects in the deregulation system would need fixing by \"the next governor\".\nThe new rules called for the Investor Owned Utilities, or IOUs, (primarily Pacific Gas and Electric, Southern California Edison, and San Diego Gas and Electric) to sell off a significant part of their electricity generation to wholly private, unregulated companies such as AES, Reliant, and Enron. The buyers of those power plants then became the wholesalers from which the IOUs needed to buy the electricity that they used to own themselves.\n\nWhile the selling of power plants to private companies was labeled \"deregulation\", in fact Steve Peace and the California legislature expected that there would be regulation by FERC which would prevent manipulation. FERC's job, in theory, is to regulate and enforce federal law, preventing market manipulation and price manipulation of energy markets. When called upon to regulate the out-of-state privateers which were clearly manipulating the California energy market, FERC hardly reacted at all and did not take serious action against Enron, Reliant, or any other privateers. FERC's resources are in fact quite sparse in comparison to their entrusted task of policing the energy market. Lobbying by private companies may also have slowed down regulation and enforcement.\n\nCalifornia's population increased by 13% during the 1990s. The state did not build any new major power plants during that time, although existing in-state power plants were expanded and power output was increased nearly 30% from 1990 to 2001.\n\nCalifornia's utilities came to depend in part on the import of excess hydroelectricity from the Pacific Northwest states of Oregon and Washington. California's clean air standards favored in-state electricity generation which burned natural gas because of its lower emissions, as opposed to coal whose emissions are more toxic and contain more pollutants.\n\nIn the summer of 2001 a drought in the northwest states reduced the amount of hydroelectric power available to California. Though at no point during the crisis was California's sum of actual electric-generating capacity plus out-of-state supply less than demand, California's energy reserves were low enough that during peak hours the private industry which owned power-generating plants could effectively hold the State hostage by shutting down their plants for \"maintenance\" in order to manipulate supply and demand. These critical shutdowns often occurred for no other reason than to force California's electricity grid managers into a position where they would be forced to purchase electricity on the \"spot market\", where private generators could charge astronomical rates. Even though these rates were semi-regulated and tied to the price of natural gas, the companies (which included Enron and Reliant Energy) controlled the supply of natural gas as well. Manipulation by the industry of natural gas prices resulted in higher electricity rates that could be charged under the semi-regulations.\n\nIn addition, the energy companies took advantage of California's electrical infrastructure weakness. The main line which allowed electricity to travel from the north to the south, Path 15, had not been improved for many years and became a major bottleneck point which limited the amount of power that could be sent south to 3,900 MW. Without the manipulation by energy companies, this bottleneck was not problematic, but the effects of the bottleneck compounded the price manipulation by hamstringing energy grid managers in their ability to transport electricity from one area to another. With a smaller pool of generators available to draw from in each area, managers were forced to work in two markets to buy energy, both of which were being manipulated by the energy companies.\n\nThe International Energy Agency estimates that a 5% lowering of demand would result in a 50% price reduction during the peak hours of the California electricity crisis in 2000/2001. With better demand response the market also becomes more resilient to intentional withdrawal of offers from the supply side.\n\nRolling blackouts affecting 97,000 customers hit the San Francisco Bay area on June 14, 2000, and San Diego Gas & Electric Company filed a complaint alleging market manipulation by some energy producers in August 2000. On December 7, 2000, suffering from low supply and idled power plants, the California Independent System Operator (ISO), which manages the California power grid, declared the first statewide Stage 3 power alert, meaning power reserves were below 3 percent. Rolling blackouts were avoided when the state halted two large state and federal water pumps to conserve electricity.\n\nMost notably, the city of Los Angeles was unaffected by the crisis because government-owned public utilities in California (including the Los Angeles Department of Water & Power) were exempt from the deregulation legislation and sold their excess power to private utilities in the state (mostly to Southern California Edison) during the crises. That enabled much of the greater Los Angeles area to suffer only rolling brown-outs rather than long term black outs suffered in other parts of the state.\n\nOn December 15, 2000, the Federal Energy Regulatory Commission (FERC) rejected California's request for a wholesale rate cap for California, instead approving a \"flexible cap\" plan of $150 per megawatt-hour. That day, California was paying wholesale prices of over $1400 per megawatt-hour, compared to $45 per megawatt-hour average one year earlier.\n\nOn January 17, 2001, the electricity crisis caused Governor Gray Davis to declare a state of emergency. Speculators, led by Enron Corporation, were collectively making large profits while the state teetered on the edge for weeks, and finally suffered rolling blackouts on January 17 & 18. Davis was forced to step in to buy power at highly unfavorable terms on the open market, since the California power companies were technically bankrupt and had no buying power. The resulting massive long term debt obligations added to the state budget crisis and led to widespread grumbling about Davis' administration.\n\nAs a result of the actions of electricity wholesalers, Southern California Edison (SCE) and Pacific Gas & Electric (PG&E) were buying from a spot market at very high prices but were unable to raise retail rates. A product that the IOU's used to produce for about three cents per kilowatt hour of electricity, they were paying eleven cents, twenty cents, fifty cents or more; and, yet, they were capped at 6.7 cents per kilowatt hour when charging their retail customers. As a result, PG&E filed bankruptcy, and Southern California Edison worked diligently on a workout plan with the State of California to save their company from the same fate. \n\nPG&E and SCE had racked up $20 billion in debt by Spring of 2001 and their credit ratings were reduced to junk status. The financial crisis meant that PG&E and SCE were unable to purchase power on behalf of their customers. The state stepped in on January 17, 2001, having the California Department of Water Resources buy power. By February 1, 2001 this stop-gap measure had been extended and would also include SDG&E. It would not be until January 1, 2003 that the utilities would resume procuring power for their customers.\n\nBetween 2000 and 2001, the combined California utilities laid off 1,300 workers, from 56,000 to 54,700, in an effort to remain solvent. SDG&E had worked through the stranded asset provision and was in a position to increase prices to reflect the spot market. Small businesses were badly affected.\n\nAccording to a 2007 study of Department of Energy data by Power in the Public Interest, retail electricity prices rose much more from 1999 to 2007 in states that adopted deregulation than in those that did not.\n\nOne of the energy wholesalers that became notorious for \"gaming the market\" and reaping huge speculative profits was Enron Corporation. Enron CEO Kenneth Lay mocked the efforts by the California state government to thwart the practices of the energy wholesalers, saying, \"In the final analysis, it doesn't matter what you crazy people in California do, because I got smart guys who can always figure out how to make money.\" The original statement was made in a phone conversation between S. David Freeman (Chairman of the California Power Authority) and Kenneth Lay in 2000, according to the statements made by Freeman to the Senate Subcommittee on Consumer Affairs, Foreign Commerce and Tourism in April and May 2002.\n\nS. David Freeman, who was appointed Chair of the California Power Authority in the midst of the crisis, made the following statements about Enron's involvement in testimony submitted before the Subcommittee on Consumer Affairs, Foreign Commerce and Tourism of the Senate Committee on Commerce, Science and Transportation on May 15, 2002:\n\nEnron eventually went bankrupt, and signed a US$1.52 billion settlement with a group of California agencies and private utilities on July 16, 2005. However, due to its other bankruptcy obligations, only US$202 million of this was expected to be paid. Ken Lay was convicted of multiple criminal charges unrelated to the California energy crisis on May 25, 2006, but he died on July 5 of that year before he could be sentenced. Because Lay died while his case was on federal appeal, his conviction was vacated and his family was allowed to retain all his property.\n\nEnron traded in energy derivatives specifically exempted from regulation by the Commodity Futures Trading Commission. At a Senate hearing in January 2002, Vincent Viola, chairman of the New York Mercantile Exchange – the largest forum for energy contract trading and clearing – urged that Enron-like companies, which don't operate in trading \"pits\" and don't have the same government regulations, be given the same requirements for \"compliance, disclosure, and oversight.\" He asked the committee to enforce \"greater transparency\" for the records of companies like Enron. In any case, the U.S. Supreme Court had ruled \"that FERC has had the authority to negate bilateral contracts if it finds that the prices, terms or conditions of those contracts are unjust or unreasonable.\" Nevada was the first state to attempt recovery of such contract losses.\n\nPerhaps the heaviest point of controversy is the question of blame for the California electricity crisis. Former Governor Gray Davis's critics often charge that he did not respond properly to the crisis, while his defenders attribute the crisis to the power trading fraud and corporate accounting scandals and say that Davis did all he could considering the fact that the federal government, not states, regulate interstate power commerce.\n\nIn a speech at UCLA on August 19, 2003, Davis apologized for being slow to act during the energy crisis, but then forcefully attacked the Houston-based energy suppliers: \"I inherited the energy deregulation scheme which put us all at the mercy of the big energy producers. We got no help from the Federal government. In fact, when I was fighting Enron and the other energy companies, these same companies were sitting down with Vice President Cheney to draft a national energy strategy.\"\n\nSigns of trouble first cropped up in the spring of 2000 when electricity bills skyrocketed for customers in San Diego, the first area of the state to deregulate. Experts warned of an impending energy crisis, but Governor Davis did little to respond until the crisis became statewide that summer. Davis began asking the federal regulator FERC to probe possible price manipulation by power suppliers as early as August 2000. Davis would issue a state of emergency on January 17, 2001, when wholesale electricity prices hit new highs and the state began issuing rolling blackouts.\n\nSome critics, such as Arianna Huffington, alleged that Davis was lulled to inaction by campaign contributions from energy producers. In addition, the California State Legislature would sometimes push Davis to act decisively by taking over power plants which were known to have been gamed and place them back under control of the utilities, ensuring a more steady supply and slapping the nose of the worst manipulators . Meanwhile, conservatives argued that Davis signed overpriced energy contracts, employed incompetent negotiators, and refused to allow prices to rise for residences statewide much like they did in San Diego, which they argue could have given Davis more leverage against the energy traders and encouraged more conservation. More criticism is given in the book \"Conspiracy of Fools\", which gives the details of a meeting between the governor and his officials; Clinton Administration Treasury officials; and energy executives, including market manipulators such as Enron, where Gray Davis disagreed with the treasury officials and energy executives. They advised suspending environmental studies to build power plants and a small rate hike to prepare for long-term power contracts (Davis eventually signed overpriced ones, as noted above), while Davis supported price caps, denounced the other solutions as too politically risky, and allegedly acted rudely. The contracts Davis signed locked Californians into high electric costs for the next decade. As of October 2011 electric rates in California had yet to return to pre-contract levels.\n\nThe crisis, and the subsequent government intervention, have had political ramifications, and is regarded as one of the major contributing factors to the 2003 recall election of Governor Davis.\n\nOn November 13, 2003, shortly before leaving office, Davis officially brought the energy crisis to an end by issuing a proclamation ending the state of emergency he declared on January 17, 2001. The state of emergency allowed the state to buy electricity for the financially strapped utility companies. The emergency authority allowed Davis to order the California Energy Commission to streamline the application process for new power plants. During that time, California issued licenses to 38 new power plants, amounting to 14,365 megawatts of electricity production when completed.\n\nOn May 17, 2001, future Republican governor Arnold Schwarzenegger and former Los Angeles Mayor Republican Richard Riordan met with Enron CEO Kenneth Lay at the Peninsula Beverly Hills Hotel in Beverly Hills. The meeting was convened for Enron to present its \"Comprehensive Solution for California,\" which called for an end to federal and state investigations into Enron's role in the California energy crisis.\n\nOn October 7, 2003, Schwarzenegger was elected Governor of California to replace Davis.\n\nOver a year later, he attended the commissioning ceremony of a new Western Area Power Administration (WAPA) 500 kV line remedying the aforementioned power bottleneck on Path 15.\n\nVice President Dick Cheney was appointed in January 2001 to head the National Energy Development Task Force. In the spring of that year, officials of the Los Angeles Department of Water and Power met with the Task Force, asking for price controls to protect consumers. The Task Force refused, and insisted that deregulation must remain in place.\n\nThe Federal Energy Regulatory Commission (FERC) was intimately involved with the handling of the crisis from the summer of 2000. There were in fact at least four separate FERC investigations.\n\nOn August 17, 2013, the British Columbia company Powerex agreed to a $750 million refund as a settlement over charges of manipulating electricity prices during 2000.\n\n\n"}
{"id": "241028", "url": "https://en.wikipedia.org/wiki?curid=241028", "title": "Charm quark", "text": "Charm quark\n\nThe charm quark, charmed quark or c quark (from its symbol, c) is the third most massive of all quarks, a type of elementary particle. Charm quarks are found in hadrons, which are subatomic particles made of quarks. Examples of hadrons containing charm quarks include the J/ψ meson (), D mesons (), charmed Sigma baryons (), and other charmed particles.\n\nIt, along with the strange quark is part of the second generation of matter, and has an electric charge of + \"e\" and a bare mass of . Like all quarks, the charm quark is an elementary fermion with spin , and experiences all four fundamental interactions: gravitation, electromagnetism, weak interactions, and strong interactions. The antiparticle of the charm quark is the charm antiquark (sometimes called \"anticharm quark\" or simply \"anticharm\"), which differs from it only in that some of its properties have equal magnitude but opposite sign.\n\nThe existence of a fourth quark had been speculated by a number of authors around 1964 (for instance by James Bjorken and Sheldon Glashow), but its prediction is usually credited to Sheldon Glashow, John Iliopoulos and Luciano Maiani in 1970 (see GIM mechanism). The first charmed particle (a particle containing a charm quark) to be discovered was the J/ψ meson. It was discovered by a team at the Stanford Linear Accelerator Center (SLAC), led by Burton Richter, and one at the Brookhaven National Laboratory (BNL), led by Samuel Ting.\n\nThe 1974 discovery of the (and thus the charm quark) ushered in a series of breakthroughs which are collectively known as the \"November Revolution\".\n\nSome of the hadrons containing charm quarks include:\n\n\n"}
{"id": "23542186", "url": "https://en.wikipedia.org/wiki?curid=23542186", "title": "Cryogenic nitrogen plant", "text": "Cryogenic nitrogen plant\n\nNitrogen, as an element of great technical importance, can be produced in a cryogenic nitrogen plant with a purity of more than 99.9999%.\nAir inside a distillation column is separated at cryogenic temperatures (about 100K/-173°C) to produce high purity nitrogen with 1ppm of impurities.\nThe process is based on the air separation, which was invented by Dr. Carl von Linde in 1895.\n\nThe main purpose of a cryogenic nitrogen plant is to provide a customer with high purity gaseous nitrogen (GAN). In addition liquid nitrogen (LIN) is produced simultaneously and is typically 10% of the gas production. High purity liquid nitrogen produced by cryogenic plants is stored in a local tank and used as a strategic reserve. This liquid can be vaporised to cover peaks in demand or for use when the nitrogen plant is offline.\nTypical cryogenic nitrogen plants range from 250 Nm³/hour to very large range plants with a daily capacity of 63.000 tonnes of nitrogen a day (as the Cantarell Field plant in Mexico).\n\nA cryogenic nitrogen plant comprises:\n\n\n\n\nAtmospheric air is roughly filtered and pressurised by a compressor, which provides the product pressure to deliver to the customer. The amount of air sucked in depends on the customer’s nitrogen demand.\n\nThe Air Receiver collects condensate and minimises pressure drop. The dry and compressed air leaves the air to refrigerant heat exchanger at about 10°C.\n\nTo clean the process air further, there are different stages of filtration. First of all, more condensate is removed, this removes some hydrocarbons.\n\nThe last unit process in the warm end container is the thermal swing adsorber (TSA). The Air purification unit cleans the compressed process air by removing any residual water vapour, carbon dioxide and hydrocarbons. It comprises two vessels, valves and exhaust to allow the changeover of vessels. While one of the TSA beds is on stream the second one is regenerated by the oxygen rich waste flow, which is vented through a silencer into the ambient environment.\n\nAfter leaving the air purification unit, the process air enters the main heat exchanger, where it is rapidly cooled down to -165°C. All residual impurities (e.g. CO) freeze out, and the process air enters at the bottom of the distillation column partially liquefied.\n\nLiquid Nitrogen produced from the cold box transfers into the liquid storage tank. An ambient air vaporiser is used to vaporise stored liquid during peak demand. A pressure control panel senses the demand for gaseous nitrogen and regulates the gas flow into the end-users pipeline to maintain line pressure.\n\n\n"}
{"id": "19181442", "url": "https://en.wikipedia.org/wiki?curid=19181442", "title": "Dorney Road Landfill", "text": "Dorney Road Landfill\n\nDorney Road Landfill is a Superfund site in Mertztown, Pennsylvania.\n\nFormerly an open-pit mine, this site housed a landfill from 1952 to 1978. During that time, the disposal of toxic materials caused contamination of local groundwater. Dorney Road Landfill was added to the National Priorities List in 1984.\n"}
{"id": "29649400", "url": "https://en.wikipedia.org/wiki?curid=29649400", "title": "Electricity Supply Commission of Malawi", "text": "Electricity Supply Commission of Malawi\n\nElectricity Supply Corporation of Malawi Ltd. (ESCOM) is the state-owned power producing company in Malawi. It is entirely in control of transmission and distribution of electric power in the country. ESCOM represents Malawi in the Southern African Power Pool. It is not to be confused with its South African equivalent Eskom.\n\nNearly 95% of Malawi’s electricity supply is provided by hydropower from a cascaded group of interconnected hydroelectric power plants located on the middle part of Shire River and a mini hydro on the Wovwe River, which constitute the interconnected system. Total installed capacity of these hydropower plants is 285.82MW (thus 281.5MW for main hydro, and 4.32MW of mini-hydro Wovwe). The hydroelectric schemes on the Shire River are all of run-of the river type. Some thermal power plants serve as stand-by for the interconnected system: a 15 MW Gas Turbine Plant in Blantyre and a 1.1 MW Diesel Power Plant in Mzuzu. Likoma District has two separate isolated systems with a total installed thermal power plants capacity of 1.050MW.\n\nPower generation is however not meeting demand and rolling blackouts are scheduled routinely.\n\n\nNKula B = 100MW; \nNkula A = 24MW;\nTedzani I&II =40MW;\nTedzani III = 52.7MW;\nKapichira = 64.8MW;\nWovwe = 4.32MW\n\n"}
{"id": "31691263", "url": "https://en.wikipedia.org/wiki?curid=31691263", "title": "Euler's pump and turbine equation", "text": "Euler's pump and turbine equation\n\nThe Euler pump and turbine equations are the most fundamental equations in the field of turbomachinery. These equations govern the power, efficiencies and other factors that contribute to the design of turbomachines. With the help of these equations the head developed by a pump and the head utilised by a turbine can be easily determined. As the name suggests these equations were formulated by Leonhard Euler in the eighteenth century. These equations can be derived from the moment of momentum equation when applied for a pump or a turbine.\n\nA consequence of Newton's second law of mechanics is the conservation of the angular momentum (or the “moment of momentum”) which is fundamental to all turbomachines. Accordingly, the change of the angular momentum is equal to the sum of the external moments. Angular momentums ρ×Q×r×cu at inlet and outlet, an external torque M and friction moments due to shear stresses Mτ act on an impeller or a diffuser.\n\nSince no pressure forces are created on cylindrical surfaces in the circumferential direction, it is possible to write:\n\nThe color triangles formed by velocity vectors u,c and w are called velocity triangles and are helpful in explaining how pumps work.\n\n'a' and 'b' show impellers with backward and forward-curved vanes respectively.\n\nBased on Eq.(1.13), Euler developed the equation for the pressure head created by an impeller:\n\nY : theoretical specific supply ; H : theoretical head pressure ; g : gravitational acceleration\n\nFor the case of a pelton turbine the static component of the head is \nzero, hence the equation reduces to:\n\nEuler’s pump and turbine equations can be used to predict the effect that\nchanging the impeller geometry has on the head. Qualitative estimations can \nbe made from the impeller geometry about the performance of the \nturbine/pump.\n\n"}
{"id": "1877810", "url": "https://en.wikipedia.org/wiki?curid=1877810", "title": "Gasometer, Vienna", "text": "Gasometer, Vienna\n\nThe Gasometers in Vienna are four former gas tanks, each of 90,000 m³ storage capacity, built as part of the Vienna municipal gas works \"Gaswerk Simmering\" in 1896–1899. They are located in the 11th district, Simmering. They were used from 1899 to 1984 as gas storage tanks. After the changeover from town gas to natural gas between 1969 and 1978, they were no longer used and were shut down. Only the brick exterior front walls were preserved. The structures have found new residential and commercial use in modern times.\n\nThe Gasometers were built from 1896 to 1899 in the Simmering district of Vienna near the \"Gaswerk Simmering\" gas works of the district. The containers were used to help supply Vienna with town gas, facilities which had previously been provided by the English firm Inter Continental Gas Association (ICGA). Once the contracts with the ICGA expired, the city decided to construct facilities to handle its own gas needs. At the time, the design was the largest in all of Europe.\n\nThe Gasometers were retired in 1984 due to new technologies in gasometer construction, as well as the city's conversion from town gas and coal gas to natural gas. Gas can be stored underground or in modern high-pressure gas storage spheres under much higher pressures and in smaller volumes than the relatively large gasometers. In 1978, they were designated as protected historic landmarks.\n\nVienna undertook a remodelling and revitalization of the protected monuments and in 1995 called for ideas for the new use of the structures. The chosen designs by the architects Jean Nouvel (Gasometer A), Coop Himmelblau (Gasometer B), Manfred Wehdorn (Gasometer C) and Wilhelm Holzbauer (Gasometer D) were completed between 1999 and 2001. Each gasometer was divided into several zones for living (apartments in the top), working (offices in the middle floors) and entertainment and shopping (shopping malls in the ground floors). The shopping mall levels in each gasometer are connected to the others by skybridges. The historic exterior wall was conserved. One of the ideas rejected for the project was the plan by architect Manfred Wehdorn to use the Gasometers for hotels and facilities for the planned World Expo in Vienna and Budapest.\n\nOn 30 October 2001, the mayor attended the official grand opening of the Gasometers, but people had begun moving in as early as May 2001.\n\nThe Gasometers are four cylindrical telescopic gas containers, each with a volume of about 90,000 m³ seated in a water basin; each is enclosed by a red-brick facade. They are each 70 meters tall and 60 meters in diameter. The Gasometers were gutted during the remodelling and only the brick exterior and parts of the roof were left standing.\n\nCoal gas was dry-distilled from coal and was stored in these containers before it was distributed into the city gas network. The \"town gas\" was originally used only by the street lamps, but in 1910, its use for cooking and heating in private homes was introduced.\n\nIndoor facilities include a music hall (capacity 2000–3000 people), movie theatre, student dormitory, municipal archive, and so on. There are about 800 apartments (two thirds within the historic brick walls) with 1600 regular tenants, as well as about 70 student apartments with 250 students in residence.\n\n"}
{"id": "47665003", "url": "https://en.wikipedia.org/wiki?curid=47665003", "title": "Gravitic density meter", "text": "Gravitic density meter\n\nA Gravitic density meter is a type of density meter used in multiple industries to measure the density of a slurry flowing through a pipe line. It consists of a flexible rubber hose that deflects when weight is flowing through the hose. A displacement measurement device, usually a high precision laser or load cell, is used to measure how much change has occurred in the system. \nBy calculating the change of deflection of the hose, a gravitic density meter can be used to \nfind the real time, continuous density.\n\nGravitic density meters do not measure the specific gravity directly. The specific gravity is \ncalculated after the density has been measured.\n\nA gravitic density meter has multiple components.\n\nAn resistance temperature detector (RTD) is used to acquire \ntemperature readings. These readings are used to compensate for temperature. A pressure transducer is used to account for changes in pressure. Pressure affects the \nrigidity of the flexible hose. Changes in rigidity affect the way that the flexible hose \ndeflects. The flexible hose is used to measure deflection. Larger weights cause more \ndeflection in the flexible rubber hose. The flexible hose must be repeatable, accurate, and \nprecise. Without these three attributes, an accurate measurement cannot be obtained.\n\nInsulation is used to protect the flexible cartridge as well as isolate the flexible \ncartridge from the effects of ambient temperature. Large differentials between the ambient \ntemperature and media temperature cause the flexible cartridge to behave differently than \nwhat is normal. The displacement measurement device needs to have a fast response time. A \nfast response results in more data which can be used to compensate for vibration. \nGravitic density meters measure the entire volume inside the flexible cartridge. This makes \nthe sample size very large. This large sample size leads to measurements that reflect the \nentire sample, because the entire sample is going to go through the hose.\n\nGravitic density meters have many uses in many different industries. Mining, dredging, \nwastewater, paper, and oil and gas all make use of gravitic density meters. All these \nindustries need to measure the amount of solids moving through its respective process.\n\nMining industries use gravitic density meters to measure the amount of ore being transported.\n\nDredging industries use gravitic density meters to measure the amount of debris being moved \nand carried by the dredge.\n\nWastewater industries use gravitic density meters to measure the amount of sewage that needs \nto be treated.\n\nPaper production uses gravitic density meters to measure the amount of pulp currently being \nused in the process.\n\nOil and gas use gravitic density meters to measure the amount of oil and gas in the system. \nThis case exemplifies the versatility of gravitic density meters because oil and gas are \nlighter than water.\n\n"}
{"id": "1661698", "url": "https://en.wikipedia.org/wiki?curid=1661698", "title": "HVDC Gezhouba–Shanghai", "text": "HVDC Gezhouba–Shanghai\n\nThe HVDC Gezhouba–Shanghai is a high voltage direct current electric power transmission system between Gezhouba and Nanqiao near Shanghai, China put in service in 1989. The bipolar line is rated at 500 kV and a maximum power of 1,200 MW.\n\nBetween 2008 and 2011 the towers of the most part of the line, which were almost guyed towers capable of carrying two conductors were replaced by new free-standing line towers for 4 conductors in order to install the conductors for the HVDC Hubei-Shanghai on them. This 970 km long line, which is also a bipolar HVDC with an operating voltage of 500 kV runs from Jingmen to Fenjing and has a transmission capacity of 3000 MW.\n\nHVDC Hubei-Shanghei shares also the grounding electrode of HVDC Gezhouba–Shanghai for the Nanqiao terminal. In most parts of its length, the electrode line of HVDC Gezhouba–Shanghai carries also the electrode line of HVDC Hubei-Shanghei. HVDC Gezhouba–Shanghai carries between Fenjing and Nanqiao also the electrode line of HVDC Hubei-Shanghei in most part of its length on its towers.\n\nThe grounding electrode of HVDC Hubei-Shanghai at Chujiahu is also used by HVDC Three Gorges-Changzhou.\n"}
{"id": "13821", "url": "https://en.wikipedia.org/wiki?curid=13821", "title": "Hadron", "text": "Hadron\n\nIn particle physics, a hadron (, \"hadrós;\" \"stout, thick\") is a composite particle made of two or more quarks held together by the strong force in a similar way as molecules are held together by the electromagnetic force. Most of the mass of ordinary matter comes from two hadrons, the proton and the neutron.\n\nHadrons are categorized into two families: baryons, made of an odd number of quarks – usually three quarks – and mesons, made of an even number of quarks—usually one quark and one antiquark. Protons and neutrons are examples of baryons; pions are an example of a meson. \"Exotic\" hadrons, containing more than three valence quarks, have been discovered in recent years. A tetraquark state (an exotic meson), named the Z(4430), was discovered in 2007 by the Belle Collaboration and confirmed as a resonance in 2014 by the LHCb collaboration. Two pentaquark states (exotic baryons), named and , were discovered in 2015 by the LHCb collaboration. There are several more exotic hadron candidates, and other colour-singlet quark combinations that may also exist.\n\nAlmost all \"free\" hadrons and antihadrons (meaning, in isolation and not bound within an atomic nucleus) are believed to be unstable and eventually decay (break down) into other particles. The only known exception relates to free protons, which are \"possibly\" stable, or at least, take immense amounts of time to decay (order of 10 years). Free neutrons are unstable and decay with a half-life of about 611 seconds. Their respective antiparticles are expected to follow the same pattern, but they are difficult to capture and study, because they immediately annihilate on contact with ordinary matter. \"Bound\" protons and neutrons, contained within an atomic nucleus, are generally considered stable. Experimentally, hadron physics is studied by colliding protons or nuclei of heavy elements such as lead or gold, and detecting the debris in the produced particle showers. In the environment, mesons such as pions are produced by the collisions of cosmic rays with the atmosphere.\n\nThe term \"hadron\" was introduced by Lev B. Okun in a plenary talk at the 1962 International Conference on High Energy Physics. In this talk he said:\n\nAccording to the quark model, the properties of hadrons are primarily determined by their so-called \"valence quarks\". For example, a proton is composed of two up quarks (each with electric charge +, for a total of + together) and one down quark (with electric charge −). Adding these together yields the proton charge of +1. Although quarks also carry color charge, hadrons must have zero total color charge because of a phenomenon called color confinement. That is, hadrons must be \"colorless\" or \"white\". The simplest ways for this to occur are with a quark of one color and an antiquark of the corresponding anticolor, or three quarks of different colors. Hadrons with the first arrangement are a type of meson, and those with the second arrangement are a type of baryon.\n\nMassless virtual gluons compose the numerical majority of particles inside hadrons. The strength of the strong force gluons which bind the quarks together has sufficient energy (\"E\") to have resonances composed of massive (\"m\") quarks (\"E > mc\") . One outcome is that short-lived pairs of virtual quarks and antiquarks are continually forming and vanishing again inside a hadron. Because the virtual quarks are not stable wave packets (quanta), but an irregular and transient phenomenon, it is not meaningful to ask which quark is real and which virtual; only the small excess is apparent from the outside in the form of a hadron. Therefore when a hadron or anti-hadron is stated to consist of (typically) 2 or 3 quarks, this technically refers to the constant excess of quarks vs. antiquarks.\n\nLike all subatomic particles, hadrons are assigned quantum numbers corresponding to the representations of the Poincaré group: \"J\"(\"m\"), where \"J\" is the spin quantum number, \"P\" the intrinsic parity (or P-parity), \"C\" the charge conjugation (or C-parity), and \"m\" the particle's mass. Note that the mass of a hadron has very little to do with the mass of its valence quarks; rather, due to mass–energy equivalence, most of the mass comes from the large amount of energy associated with the strong interaction. Hadrons may also carry flavor quantum numbers such as isospin (G parity), and strangeness. All quarks carry an additive, conserved quantum number called a baryon number (\"B\"), which is + for quarks and − for antiquarks. This means that baryons (composite particles made of three, five or a larger odd number of quarks) have \"B\" = 1 whereas mesons have \"B\" = 0.\n\nHadrons have excited states known as resonances. Each ground state hadron may have several excited states; several hundreds of resonances have been observed in experiments. Resonances decay extremely quickly (within about 10 seconds) via the strong nuclear force.\n\nIn other phases of matter the hadrons may disappear. For example, at very high temperature and high pressure, unless there are sufficiently many flavors of quarks, the theory of quantum chromodynamics (QCD) predicts that quarks and gluons will no longer be confined within hadrons, \"because the strength of the strong interaction diminishes with energy\". This property, which is known as asymptotic freedom, has been experimentally confirmed in the energy range between 1 GeV (gigaelectronvolt) and 1 TeV (teraelectronvolt).\n\nAll free hadrons except (possibly) the proton and antiproton are unstable.\n\nBaryons are hadrons containing an odd number of valence quarks (at least 3). Most well known baryons such as the proton and neutron have three valence quarks, but pentaquarks with five quarks – three quarks of different colors, and also one extra quark-antiquark pair – have also been proven to exist. Because baryons have an odd number of quarks, they are also all fermions, \"i.e.\", they have half-integer spin. As quarks possess baryon number \"B\" = , baryons have baryon number \"B\" = 1. \n\nEach type of baryon has a corresponding antiparticle (antibaryon) in which quarks are replaced by their corresponding antiquarks. For example, just as a proton is made of two up-quarks and one down-quark, its corresponding antiparticle, the antiproton, is made of two up-antiquarks and one down-antiquark.\n\nAs of August 2015, there are two known pentaquarks, and , both discovered in 2015 by the LHCb collaboration.\n\nMesons are hadrons containing an even number of valence quarks (at least 2). Most well known mesons are composed of a quark-antiquark pair, but possible tetraquarks (4 quarks) and hexaquarks (6 quarks, comprising either a dibaryon or three quark-antiquark pairs) may have been discovered and are being investigated to confirm their nature. Several other hypothetical types of exotic meson may exist which do not fall within the quark model of classification. These include glueballs and hybrid mesons (mesons bound by excited gluons).\n\nBecause mesons have an even number of quarks, they are also all bosons, with integer spin, \"i.e.\", 0, 1, or −1. They have baryon number \"B\" =  −  = 0. Examples of mesons commonly produced in particle physics experiments include pions and kaons. Pions also play a role in holding atomic nuclei together via the residual strong force.\n\n"}
{"id": "12487023", "url": "https://en.wikipedia.org/wiki?curid=12487023", "title": "Hadron spectroscopy", "text": "Hadron spectroscopy\n\nHadron spectroscopy is the subfield of particle physics that studies the masses and decays of hadrons. Hadron spectroscopy is also an important part of the new nuclear physics. The properties of hadrons are a consequence of a theory called quantum chromodynamics (QCD).\n\nQCD predicts that quarks and antiquarks bind into particles called mesons. Another type of hadron is called a baryon, that is made of three quarks. There is good experimental evidence for both mesons and baryons. Potentially QCD also has bound states of just gluons called glueballs. One of the goals of the field of hadronic spectroscopy is to find experimental evidence for exotic mesons, tetraquarks, molecules of hadrons, and glueballs.\n\nAn important part of the field of hadronic spectroscopy are the attempts to solve QCD. The properties of hadrons require the solution of QCD in the strong coupling regime, where perturbative techniques based on Feynman diagrams do not work. There are several approaches to trying to solve QCD to compute the masses of hadrons:\n\n\n"}
{"id": "429296", "url": "https://en.wikipedia.org/wiki?curid=429296", "title": "Hausdorff distance", "text": "Hausdorff distance\n\nIn mathematics, the Hausdorff distance, or Hausdorff metric, also called Pompeiu–Hausdorff distance, measures how far two subsets of a metric space are from each other. It turns the set of non-empty compact subsets of a metric space into a metric space in its own right. It is named after Felix Hausdorff.\n\nInformally, two sets are close in the Hausdorff distance if every point of either set is close to some point of the other set. The Hausdorff distance is the longest distance you can be forced to travel by an adversary who chooses a point in one of the two sets, from where you then must travel to the other set. In other words, it is the greatest of all the distances from a point in one set to the closest point in the other set.\n\nIt seems that this distance was first introduced by Hausdorff in his book \"Grundzüge der Mengenlehre\", first published in 1914, although a very close relative appeared in the doctoral thesis of Maurice Fréchet in 1906, in his study of the space of all continuous curves from formula_1.\n\nLet \"X\" and \"Y\" be two non-empty subsets of a metric space (\"M\", \"d\"). We define their Hausdorff distance by\n\nwhere \"sup\" represents the supremum and \"inf\" the infimum.\n\nEquivalently\n\nwhere\n\nthat is, the set of all points within formula_5 of the set formula_6 (sometimes called the formula_5-fattening of formula_6 or a generalized ball of radius formula_5 around formula_6).\n\nIt is not true in general that if formula_11, then\n\nFor instance, consider the metric space of the real numbers formula_13 with the usual metric formula_14 induced by the absolute value,\n\nTake\n\nThen formula_17. However formula_18 because formula_19, but formula_20.\n\n\nThe definition of the Hausdorff distance can be derived by a series of natural extensions of the distance function \"d\"(\"x\", \"y\") in the underlying metric space \"M\", as follows:\n\n\n\nIn computer vision, the Hausdorff distance can be used to find a given template in an arbitrary target image. The template and image are often pre-processed via an edge detector giving a binary image. Next, each 1 (activated) point in the binary image of the template is treated as a point in a set, the \"shape\" of the template. Similarly, an area of the binary target image is treated as a set of points. The algorithm then tries to minimize the Hausdorff distance between the template and some area of the target image. The area in the target image with the minimal Hausdorff distance to the template, can be considered the best candidate for locating the template in the target.\nIn computer graphics the Hausdorff distance is used to measure the difference between two different representations of the same 3D object particularly when generating level of detail for efficient display of complex 3D models.\n\nA measure for the dissimilarity of two shapes is given by \"Hausdorff distance up to isometry\", denoted \"D\". Namely, let \"X\" and \"Y\" be two compact figures in a metric space \"M\" (usually a Euclidean space); then \"D\"(\"X\",\"Y\") is the infimum of \"d\"(\"I\"(\"X\"),\"Y\") along all isometries \"I\" of the metric space \"M\" to itself. This distance measures how far the shapes \"X\" and \"Y\" are from being isometric.\n\nThe Gromov–Hausdorff convergence is a related idea: we measure the distance of two metric spaces \"M\" and \"N\" by taking the infimum of \"d\"(\"I\"(\"M\"),\"J\"(\"N\")) along all isometric embeddings \"I\":\"M\"→\"L\" and \"J\":\"N\"→\"L\" into some common metric space \"L\".\n\n\n"}
{"id": "16775440", "url": "https://en.wikipedia.org/wiki?curid=16775440", "title": "Havsnäs Wind Farm", "text": "Havsnäs Wind Farm\n\nThe Havsnäs Wind Farm is the largest onshore wind farm in Sweden. The wind farm consists of 48 Vestas V90 wind turbines. The 95.4 megawatt wind farm was officially opened on September 2 2010. It is owned by HgCapital (75%) and Nordisk Vindkraft (25%), a subsidiary of the RES Group.\n"}
{"id": "188676", "url": "https://en.wikipedia.org/wiki?curid=188676", "title": "Hitachi", "text": "Hitachi\n\nHitachi is listed on the Tokyo Stock Exchange and is a constituent of the Nikkei 225 and TOPIX indices. It is ranked 38th in the 2012 Fortune Global 500 and 129th in the 2012 Forbes Global 2000.\n\nHitachi was founded in 1910 by electrical engineer Namihei Odaira in Ibaraki Prefecture. The company's first product was Japan's first induction motor, initially developed for use in copper mining. Odaira's company soon became the domestic leader in electric motors and electric power industry infrastructure.\n\nThe company began as an in-house venture of Fusanosuke Kuhara's mining company in Hitachi, Ibaraki. Odaira moved headquarters to Tokyo in 1918. Long before that, he coined the company’s toponymic name by superimposing two \"kanji\" characters: hi meaning “sun” and tachi meaning “rise”. The young company's national aspirations were conveyed by its original brand mark, which evoked Japan's imperial rising sun flag.\n\nWorld War II and its aftermath devastated the company. Many of its factories were destroyed by Allied bombing raids, and after the war, American occupational forces tried to disband Hitachi altogether. Founder Odaira was removed from the company. Nevertheless, as a result of three years of negotiations, Hitachi was permitted to maintain all but 19 of its manufacturing plants. The cost of such a production shutdown, though, compounded by a three-month labor strike in 1950, severely hindered Hitachi's reconstruction efforts. Only the Korean War saved the company from complete collapse. Hitachi and many other struggling Japanese industrial firms benefited from defense contracts offered by the American military. Meanwhile, Hitachi went public in 1949.\n\nHitachi America, Ltd. was established in 1959. Hitachi Europe, Ltd. was established in 1982.\nIn March 2011, Hitachi agreed to sell its hard disk drive subsidiary, HGST, to Western Digital (WD) for a combination of cash and shares worth US$4.3 billion. Due to concerns of a duopoly of WD and Seagate Technology by the EU Commission and the Federal Trade Commission, Hitachi's 3.5\" HDD division was sold to Toshiba. The transaction was completed in March 2012.\n\nHitachi entered talks with Mitsubishi Heavy Industries in August 2011 about a potential merger of the two companies, in what would have been the largest merger between two Japanese companies in history. The talks subsequently broke down and were suspended.\n\nIn October 2012, Hitachi agreed to acquire the United Kingdom-based nuclear energy company Horizon Nuclear Power, which plans to construct up to six nuclear power plants in the UK, from E.ON and RWE for £700 million.\n\nIn November 2012, Hitachi and Mitsubishi Heavy Industries agreed to merge their thermal power generation businesses into a joint venture to be owned 65% by Mitsubishi Heavy Industries and 35% by Hitachi. The joint venture began operations in February 2014.\n\nOn March 14, 2018, Zoomdata announced its partnership with Hitachi INS Software to help develop big data analytics market in Japan.\n\n\n\n\n\n\nNote: A new product from Hitachi called \"Memory glass\" was to be introduced in 2015. It is a high density information storage medium utilizing laser etched/readable Fused quartz.\n\n\n\n\n\n\nFollowing the Fukushima Daiichi nuclear disaster in 2011 and the extended temporary closure of most Japanese nuclear plants, Hitachi's nuclear business became unprofitable and in 2016 Hitachi CEO Toshiaki Higashihara argued Japan should consider a merger of the various competing nuclear businesses. Hitachi is taking for 2016 an estimated ¥65 billion write-off in value of a SILEX technology laser uranium enrichment joint venture with General Electric.\n\n\n\nHitachi Communication Technologies America provides communications products and services for the telecommunications, cable TV, utility, enterprise, industrial and other markets.\n\nHitachi Consulting is an international management and technology consulting firm with headquarters in Dallas, Texas. It was founded in 2000 and currently employs approximately 6,500 people across the United States, Japan, the United Kingdom, the Netherlands, India, Spain, Portugal, Germany, China, Brazil and Vietnam.\n\nHitachi Data Systems (HDS) is a wholly owned subsidiary of Hitachi which provides hardware, software and services to help companies manage their digital data. Its flagship products are the Virtual Storage Platform (for enterprise storage), Hitachi Unified Storage VM for large sized companies, Hitachi Unified Storage for small and mid-sized companies, Hitachi Content Platform (archiving and cloud architecture), Hitachi Command Suite (for storage management), Hitachi TrueCopy and Hitachi Universal Replicator (for remote replication), and the Hitachi NAS Platform.\n\nSince September 19, 2017, Hitachi Data Systems (HDS) has become part of Hitachi Vantara, a new company that unifies the operations of Pentaho, Hitachi Data Systems and Hitachi Insight Group. The company name \"Hitachi Data Systems\" (HDS) and its logo is no longer used in the market.\n\nHitachi manufactures many types of electronic products including TVs, Camcorders, Projectors and Recording Media under its own brand name.\n\nHitachi provides various defense related/derived products & services.\n\nAmong other things, Hitachi Metals supplies materials for aircraft engines and fuselage components (e.g. landing gear), along with finished components for same and other aerospace applications. It also provides materials, components and tools for the automotive and electronics industries.\n\nHitachi Koki manufactures many types of tools including chainsaws, drills, woodworking power tools. Some are branded Koki Tanaka. March 1, 2016 Hitachi Koki acquired German power tools manufacturer Metabo from Chequers Capital.\n\nHitachi Plant Technologies, Ltd., along with its subsidiaries, engages in the design, development, manufacture, sale, servicing, and execution of social and industrial infrastructure machinery, mechatronics, air-conditioning systems, industrial plants, and energy plant equipment in Asia and internationally.\n\nHitachi Rail is the rolling stock manufacturing division of Hitachi. It and Mitsubishi Heavy Industries agreed to cooperate in the field of international intra-city railway systems in 2010.\n\nHitachi markets a general-purpose train known as the \"A-train\", which uses double-skin, friction-stir-welded aluminium body construction. The A-train concept can be customised to form different types of trains, ranging from high-capacity commuter and metro trains (as in the automated 3000 series train for the Nanakuma Line of the Fukuoka City Subway) to limited express (as in the E257 series jointly produced with Tokyu Corporation) and high-speed trains (as in the Class 395 trains for Southeastern in the UK). They have made such trains for domestic and international operators alike. Among its most significant orders was the winning tender for the UK Department for Transport's Intercity Express Programme in June 2008.\n\nHitachi's many products include the designing and manufacturing of many Shinkansen models, including the N700 Series Shinkansen, which has been exported as the THSR 700T for Taiwan High Speed Rail. \n\nThe company also markets a driverless metro system developed by Hitachi Rail Italy, pioneered on the Copenhagen Metro, and straddle beam monorail technology, known as the Hitachi Monorail, which form the basis of the trains operating on the world's longest monorail system, currently part of the Chongqing Rail Transit network.\n\nOn February 24, 2015, Hitachi agreed to purchase the Italian rolling stock manufacturer AnsaldoBreda and acquire Finmeccanica's stake in Ansaldo STS, the railway signaling division of Finmeccanica The purchase was completed later that year, at which point the company was renamed as Hitachi Rail Italy. Since then, Hitachi has obtained a majority stake in Ansaldo STS.\n\nHitachi Solutions America is a consulting firm and systems integrator focusing primarily on Microsoft Dynamics. The firm utilizes AX and CRM from the Dynamics family to provide customers with a broad base of solutions. The company is international, with subsidiaries residing in the United Kingdom, Canada, Philippines, Thailand, Japan and India. Hitachi Solutions America acquired Ignify – another leading Microsoft Dynamics Solution providers in December 2015. Hitachi Solutions has about 2000 Microsoft Dynamics consultants worldwide after the acquisition of Ignify.\n\nHitachi Works is the oldest member of the Hitachi Group and consists of three factories: Kaigan Works, Yamate Works, and Rinkai Works. Yamate Works, the oldest of the three factories, was founded in 1910 by Namihei Odaira as an electrical equipment repair and manufacturing facility. This facility was named Hitachi, after the Hitachi Mine near Hitachi, Ibaraki, and is regarded as the ancestral home of Hitachi, Ltd.\n\nMany management trainees intern at Hitachi Works before being permanently assigned to other Hitachi divisions. Senior management personnel are often participants in rotations at Hitachi Works for a few years as their career develops towards eventual head office stature. As a result, many of the senior managers of Hitachi Ltd have passed through Hitachi Works.\n\nSpin-off entities from Hitachi Works include Hitachi Cable (1956) and Hitachi Canadian Industries (1988).\n\n\nHitachi Global Storage Technologies (Hitachi GST) manufactures computer hard drives. There are 3 main ranges: Hitachi Travelstar, Hitachi Deskstar, and Hitachi Ultrastar.\n\nOn March 7, 2011 Hitachi Global Storage Technologies was purchased by Western Digital Corporation for $3.5 billion in cash and $750 million in Western Digital common stock.\n\nHitachi Printing Systems was established in 1980 and was acquired by Ricoh in 2004, becoming Ricoh Printing Systems, Ltd.\n\nHitachi had a joint venture with Kerala public sector company TELK from 1963 to 1989 for the production of electrical equipment. In collaboration with Hitachi, TELK was the first company to manufacture 400kV transformers in India. TELK transformers are well known for their quality and are in great demand for the Indian power system.\n\nIn August 2011, it was announced that Hitachi would donate an electron microscope to each of five universities in Indonesia (the University of North Sumatra in Medan, the Indonesian Christian University in Jakarta, Padjadjaran University in Bandung, General Soedirman University in Purwokerto and Muhammadiyah University in Malang).\n\n\nHitachi Group()\n"}
{"id": "23443162", "url": "https://en.wikipedia.org/wiki?curid=23443162", "title": "Hélène Pelosse", "text": "Hélène Pelosse\n\nHélène Pelosse (born 5 March 1970) was the first elected Interim Director-General of the International Renewable Energy Agency (IRENA). She served in this position for 15 months, from 30 June 2009 until 19 October 2010.\n\nPelosse is a citizen of France and was born in Montreal, Canada. She graduated from the École nationale d'administration and the École supérieure des sciences économiques et commerciales. After graduation, she worked in the Inspectorate General of Finances in the Ministry of Finance of France. In 1999-2000, Pelosse worked for Saint-Gobain. In this period, she was posted in Worcester, Massachusetts, as the Director of Strategy. From 2001-2005, Pelosse was the financial and later trade adviser in the French Prime Minister's Office. In 2007, during the German EU Presidency, she served as an adviser in Angela Merkel's private office. \nAfter the German EU Presidency Hélène Pelosse worked as the Deputy Head of Staff in charge of international affairs in the Private Office of the French Minister of State in charge of Ecology, Energy, Sustainable Development, and Town and Country Planning, Jean-Louis Borloo (cabinet Fillon II).\n\nOn 30 June 2009, Pelousse was elected as the Interim Director-General of IRENA. She resigned on 19 October 2010. According to Pelosse, the United Arab Emirates, the host country of the IRENA, asked France to make her resign because she made a gender parity a priority of the IRENA. She also accused UAE authorities of intimidation by intruding into her home, bugging her phone and office, and searching her baggage. According to her, \"if we had wanted a IRENA that works, it would not have taken the installation in Abu Dhabi\", notwithstanding the fact that electing Abu Dhabi as a location of the headquarters was supported by France. Her successor in the post, Adnan Amin, referred to management problems instead. Financial mismanagement and lack of good accounting standards were also cited, in addition to Pelosse's opposition to the carbon capture and nuclear energy. After IRENA, Pelousse returned to the General Inspectorate for Finances of France.\n\nPelosse is married and has 3 children.\n"}
{"id": "34610436", "url": "https://en.wikipedia.org/wiki?curid=34610436", "title": "Linea Aeropostal Venezolana Flight 253 (November 1956)", "text": "Linea Aeropostal Venezolana Flight 253 (November 1956)\n\nLinea Aeropostal Flight 253 was being operated by a Lockheed L-749 Constellation, registration YV-C-AMA, on an international scheduled passenger service that took off from Idlewild International Airport bound for Caracas International Airport on November 27, 1956. Flight 253, operated by a Constellation, named \"Jose Marti\" being piloted by a French Captain named Marcel Combalbert, crashed into a mountain near Caracas Venezuela. All 25 passengers and crew on board were killed.\n\nFlight 253 was flying through a rainstorm as it approached Caracas Airport. It was approximately 18 kilometers from the runway, when the aircraft struck the southern ridge of Cerro El Ávila at 6700 feet altitude.\n\nTen Americans were among those killed in the crash. St. Louis Cardinals outfielder Charlie Peete, his wife Nettie, and their three small children were among the victims. Peete was traveling to Venezuela in order to play winter ball there.\n\nCable cars were used in the recovery of bodies.\n\nThe probable cause of the crash was determined to be: \"The instrument flight training manuals show that the Linea Aeropostal Venezolana has approved a procedure for entering Maiquetia in semi-IFR conditions. This procedure consists in maintaining a minimum flight level of 10000ft as far as the station (Miq 292.5), then turning north over this pount and continuing on a 360deg heading for 4 minutes followed by a standard let-down to 1200ft above sea level until contact is established, and a return to the aerodrome under VFR. It is obvious that the pilot-in-command did not fully comply with this procedure, and, after accumulating errors in estimating his speed, endeavoured to make a direct approach which proved fatal because his altitude at the time of his last report was insufficient to cross the Avila mountain range against which the impact occurred.\"\n\n"}
{"id": "8604844", "url": "https://en.wikipedia.org/wiki?curid=8604844", "title": "Lubiprostone", "text": "Lubiprostone\n\nLubiprostone (rINN, marketed under the trade name Amitiza among others) is a medication used in the management of chronic idiopathic constipation, predominantly irritable bowel syndrome-associated constipation in women and opioid-induced constipation.\n\nIt was initially approved by the U.S. Food and Drug Administration (FDA) in 2006 and recommended for use in the UK by the National Institute for Health and Care Excellence (NICE) in 2014. It is expensive as of 2017, with the cost to the NHS being £29.68 per 24mg 28-cap pack as of April 2017.\n\nLubiprostone is used for the treatment of chronic constipation of unknown cause in adults, as well as irritable bowel syndrome associated with constipation in women.\n\nLubiprostone is approved to treat chronic idiopathic constipation (CIC) in adults.\n\nLubiprostone is also approved to treat opioid-induced constipation, in adults with chronic non-cancer pain. The effectiveness of lubiprostone has not been established in patients who are taking a diphenylheptane opioid (e.g., methadone).\n\nLubiprostone is approved to treat irritable bowel syndrome with constipation (IBS-C) in women 18 years of age and older.\n\nAs of 12 November 2014, lubiprostone has not been studied in children. There is current research underway to determine the safety and efficacy in postoperative bowel dysfunction.\n\nIn clinical trials, the most common adverse event was nausea (31%). Other adverse events (≥5% of patients) included diarrhea (13%), headache (13%), abdominal distention (5%), abdominal pain (5%), flatulence (6%), sinusitis (5%) vomiting (5%) and fecal incontinence (1%).\n\nThere are no current data on use in people with liver or kidney complications. The effects on pregnancy have not been studied in humans but testing in Guinea pigs resulted in fetal loss. \nAmitiza is not approved for use in children. Lubiprostone is contraindicated in patients exhibiting chronic diarrhea, bowel obstruction, or diarrhea-predominant irritable bowel syndrome.\n\nLubiprostone is a bicyclic fatty acid derived from prostaglandin E1 that acts by specifically activating ClC-2 chloride channels on the apical aspect of gastrointestinal epithelial cells, producing a chloride-rich fluid secretion. These secretions soften the stool, increase motility, and promote spontaneous bowel movements (SBM).\n\nSymptoms of constipation such as pain and bloating are usually improved within one week, and SBM may occur within one day.\n\nUnlike many laxative products, lubiprostone does not show signs of tolerance, dependency, or altered serum electrolyte concentration. There was no rebound effect following withdrawal of treatment, but a gradual return to pre-treatment bowel movement frequency should be expected.\n\nMinimal distribution of the drug occurs beyond the immediate gastrointestinal tissues. Lubiprostone is rapidly metabolized by reduction/oxidation, mediated by carbonyl reductase. There is no metabolic involvement of the hepatic cytochrome P450 system. The measurable metabolite, M3, exists in very low levels in plasma and makes up less than 10% of the total administered dose.\n\nData indicate that metabolism occurs locally in the stomach and jejunum.\n\nLubiprostone received approval from the Food and Drug Administration in 2008 to treat irritable bowel syndrome with constipation (IBS-C) and is available through prescription only. , the drug is available in the United States, Japan, Switzerland,India and the United Kingdom; review by Health Canada began in late 2014.\n\nIn Bangladesh and India, lubiprostone is marketed under the trade name Lubilax by Beacon Pharmaceuticals Limited, and under the trade name Lubowel by SunPharma.\n\n"}
{"id": "12993112", "url": "https://en.wikipedia.org/wiki?curid=12993112", "title": "Magic angle (EELS)", "text": "Magic angle (EELS)\n\nThe magic angle is a particular value of the collection angle of an electron microscope at which the measured energy-loss spectrum \"magically\" becomes independent of the tilt angle of the sample with respect to the beam direction. The magic angle is not uniquely defined for isotropic samples, but the definition is unique in the (typical) case of small angle scattering on materials with a \"c-axis\", such as graphite.\n\nThe \"magic\" angle depends on both the incoming electron energy (which is typically fixed) and the energy loss suffered by the electron. The ratio of the magic angle formula_1 to the characteristic angle formula_2 is roughly independent of the energy loss and roughly independent of the particular type of sample considered.\n\nFor the case of a relativistic incident electron, the \"magic\" angle is defined by the equality of two different functions\n(denoted below by formula_3 and formula_4) of the collection angle formula_5:\n\nformula_6\n\nand\n\n<math>\n"}
{"id": "1102240", "url": "https://en.wikipedia.org/wiki?curid=1102240", "title": "Marx generator", "text": "Marx generator\n\nA Marx generator is an electrical circuit first described by Erwin Otto Marx in 1924. Its purpose is to generate a high-voltage pulse from a low-voltage DC supply. Marx generators are used in high-energy physics experiments, as well as to simulate the effects of lightning on power-line gear and aviation equipment. A bank of 36 Marx generators is used by Sandia National Laboratories to generate X-rays in their Z Machine.\n\nThe circuit generates a high-voltage pulse by charging a number of capacitors in parallel, then suddenly connecting them in series. See the circuit above. At first, \"n\" capacitors (\"C\") are charged in parallel to a voltage \"V\" by a high-voltage DC power supply through the resistors (\"R\"). The spark gaps used as switches have the voltage \"V\" across them, but the gaps have a breakdown voltage greater than \"V\", so they all behave as open circuits while the capacitors charge. The last gap isolates the output of the generator from the load; without that gap, the load would prevent the capacitors from charging. To create the output pulse, the first spark gap is caused to break down (triggered); the breakdown effectively shorts the gap, placing the first two capacitors in series, applying a voltage of about 2\"V\" across the second spark gap. Consequently, the second gap breaks down to add the third capacitor to the \"stack\", and the process continues to sequentially break down all of the gaps. This process of the spark gaps connecting the capacitors in series to create the high voltage is called \"erection\". The last gap connects the output of the series \"stack\" of capacitors to the load. Ideally, the output voltage will be \"nV\", the number of capacitors times the charging voltage, but in practice the value is less. Note that none of the charging resistors \"R\" are subjected to more than the charging voltage even when the capacitors have been erected. The charge available is limited to the charge on the capacitors, so the output is a brief pulse as the capacitors discharge through the load. At some point, the spark gaps stop conducting, and the high-voltage supply begins charging the capacitors again.\n\nThe principle of multiplying voltage by charging capacitors in parallel and discharging them in series is also used in the voltage multiplier circuit, used to produce high voltages for laser printers and cathode ray tube television sets, which has similarities to this circuit. The difference is that the voltage multiplier is powered with alternating current and produces a steady DC output voltage, whereas the Marx generator produces a pulse.\n\nProper performance depends upon selection of capacitor and the timing of the discharge. Switching times can be improved by doping of the electrodes with radioactive isotopes caesium 137 or nickel 63, and by orienting the spark gaps so that ultraviolet light from a firing spark gap switch illuminates the remaining open spark gaps. Insulation of the high voltages produced is often accomplished by immersing the Marx generator in transformer oil or a high pressure dielectric gas such as sulfur hexafluoride (SF).\n\nNote that the less resistance there is between the capacitor and the charging power supply, the faster it will charge. Thus, in this design, those closer to the power supply will charge quicker than those farther away. If the generator is allowed to charge long enough, all capacitors will attain the same voltage.\n\nIn the ideal case, the closing of the switch closest to the charging power supply applies a voltage 2\"V\" to the second switch. This switch will then close, applying a voltage 3\"V\" to the third switch. This switch will then close, resulting in a cascade down the generator that produces \"nV\" at the generator output (again, only in the ideal case).\n\nThe first switch may be allowed to spontaneously break down (sometimes called a \"self break\") during charging if the absolute timing of the output pulse is unimportant. However, it is usually intentionally triggered once all the capacitors in the Marx bank have reached full charge, either by reducing the gap distance, by pulsing an additional trigger electrode (such as a Trigatron), by ionising the air in the gap using a pulsed laser, or by reducing the air pressure within the gap.\n\nThe charging resistors, Rc, need to be properly sized for both charging and discharging. They are sometimes replaced with inductors for improved efficiency and faster charging. In many generators the resistors are made from plastic or glass tubing filled with dilute copper sulfate solution. These liquid resistors overcome many of the problems experienced by more-conventional solid resistive materials, which have a tendency to lower their resistance over time under high voltage conditions.\n\nThe Marx generator is also used to generate short high-power pulses for Pockels cells, driving a TEA laser, ignition of the conventional explosive of a nuclear weapon, and radar pulses.\n\nShortness is relative, as the switching time of even high-speed versions is not less than 1 ns, and thus many low-power electronic devices are faster. In the design of high-speed circuits, electrodynamics is important, and the Marx generator supports this insofar as it uses short thick leads between its components, but the design is nevertheless essentially an electrostatic one. (In electrodynamic terms, when the first stage breaks down it creates a spherical electromagnetic wave whose electric field vector is opposed to the static high voltage. This moving electromagnetic field has the wrong orientation to trigger the next stage, and may even reach the load; such noise in front of the edge is undesirable in many switching applications. If the generator is inside a tube of (say) 1 m diameter, it requires around 10 wave reflections for the field to settle to static conditions, which restricts pulse leading edge width to 30 ns or more. Smaller devices are of course faster.) When the first gap breaks down, pure electrostatic theory predicts that the voltage across all stages rises. However, stages are coupled capacitively to ground and serially to each other, and thus each stage encounters a voltage rise that is increasingly weaker the further the stage is from the switching one; the adjacent stage to the switching one therefore encounters the largest voltage rise, and thus switches in turn. As more stages switch, the voltage rise to the remainder increases, which speeds up their operation. Thus a voltage rise fed into the first stage becomes amplified and steepened at the same time.\n\nThe speed of a switch is determined by the speed of the charge carriers, which gets higher with higher voltage, and by the current available to charge the inevitable parasitic capacity. In solid-state avalanche devices, a high voltage automatically leads to high current. Because the high voltage is applied only for a short time, solid-state switches will not heat up excessively. As compensation for the higher voltages encountered, the later stages have to carry lower charge too. Stage cooling and capacitor recharging also go well together.\n\nAvalanche diodes can replace a spark gap for stage voltages less than 500 volts. The charge carriers easily leave the electrodes, so no extra ionisation is needed and jitter is low. The diodes also have a longer lifetime than spark gaps.\n\nA speedy switching device is an NPN avalanche transistor fitted with a coil between base and emitter. The transistor is initially switched off and about 300 volts exists across its collector-base junction. This voltage is high enough that a charge carrier in this region can create more carriers by impact ionisation, but the probability is too low to form a proper avalanche; instead a somewhat noisy leakage current flows. When the preceding stage switches, the emitter-base junction is pushed into forward bias and the collector-base junction enters full avalanche mode, so charge carriers injected into the collector-base region multiply in a chain reaction. Once the Marx generator has completely fired, voltages everywhere drop, each switch avalanche stops, its matched coil puts its base-emitter junction into reverse bias, and the low static field allows remaining charge carriers to drain out of its collector-base junction.\n\nOne application is so-called boxcar switching of a Pockels cell. Four Marx generators are used, each of the two electrodes of the Pockels cell being connected to a positive pulse generator and a negative pulse generator. Two generators of opposite polarity, one on each electrode, are first fired to charge the Pockels cell into one polarity. This will also partly charge the other two generators but not trigger them, because they have been only partly charged beforehand. Leakage through the Marx resistors needs to be compensated by a small bias current through the generator. At the trailing edge of the boxcar, the two other generators are fired to \"reverse\" the cell.\n\nMarx generators are used to provide high-voltage pulses for the testing of insulation of electrical apparatus such as large power transformers, or insulators used for supporting power transmission lines. Voltages applied may exceed 2 million volts for high-voltage apparatus.\n\n\n\n"}
{"id": "1566768", "url": "https://en.wikipedia.org/wiki?curid=1566768", "title": "Material properties of diamond", "text": "Material properties of diamond\n\nDiamond is the allotrope of carbon in which the carbon atoms are arranged in the specific type of cubic lattice called diamond cubic. Diamond is an optically isotropic crystal that is transparent to opaque. Diamond is the hardest naturally occurring material known. Yet, due to important structural weaknesses, diamond's toughness is only fair to good. The precise tensile strength of diamond is unknown, however strength up to 60 GPa has been observed, and it could be as high as 90–225 GPa depending on the crystal orientation. The anisotropy of diamond hardness is carefully considered during diamond cutting. Diamond has a high refractive index (2.417) and moderate dispersion (0.044) properties which give cut diamonds their brilliance. Scientists classify diamonds into four main types according to the nature of crystallographic defects present. Trace impurities substitutionally replacing carbon atoms in a diamond's crystal structure, and in some cases structural defects, are responsible for the wide range of colors seen in diamond. Most diamonds are electrical insulators but extremely efficient thermal conductors. Unlike many other minerals, the specific gravity of diamond crystals (3.52) has rather small variation from diamond to diamond.\n\nKnown to the ancient Greeks as ἀδάμας – \"adámas\" (\"proper\", \"unalterable\", \"unbreakable\") and sometimes called adamant, diamond is the hardest known naturally occurring material, scoring 10 on the Mohs scale of mineral hardness. Diamond is extremely strong owing to the structure of its carbon atoms, where each carbon atom has four neighbors joined to it with covalent bonds. The material boron nitride, when in a form structurally identical to diamond (zincblende structure), is nearly as hard as diamond; a currently hypothetical material, beta carbon nitride, may also be as hard or harder in one form. It has been shown that some diamond aggregates having nanometer grain size are harder and tougher than conventional large diamond crystals, thus they perform better as abrasive material. Owing to the use of those new ultra-hard materials for diamond testing, more accurate values are now known for diamond hardness. A surface perpendicular to the [111] crystallographic direction (that is the longest diagonal of a cube) of a pure (i.e., type IIa) diamond has a hardness value of 167 GPa when scratched with an nanodiamond tip, while the nanodiamond sample itself has a value of 310 GPa when tested with another nanodiamond tip. Because the test only works properly with a tip made of harder material than the sample being tested, the true value for nanodiamond is likely somewhat lower than 310 GPa.\n\nThe precise tensile strength of diamond is unknown, however strength up to 60 GPa has been observed, and it could be as high as 90–225 GPa depending on the perfection of diamond lattice and on its orientation: Tensile strength is the highest for the [100] crystal direction (normal to the cubic face), smaller for the [110] and the smallest for the [111] axis (along the longest cube diagonal). Diamond also has one of the smallest compressibilities of any material.\n\nCubic diamonds have a perfect and easy octahedral cleavage, which means that they only have four planes—weak directions following the faces of the octahedron where there are fewer bonds—along which diamond can easily split upon blunt impact to leave a smooth surface. Similarly, diamond's hardness is markedly \"directional\": the hardest direction is the diagonal on the cube face, 100 times harder than the softest direction, which is the dodecahedral plane. The octahedral plane is intermediate between the two extremes. The diamond cutting process relies heavily on this directional hardness, as without it a diamond would be nearly impossible to fashion. Cleavage also plays a helpful role, especially in large stones where the cutter wishes to remove flawed material or to produce more than one stone from the same piece of rough (e.g. Cullinan Diamond).\n\nDiamonds crystallize in the diamond cubic crystal system (space group Fdm) and consist of tetrahedrally, covalently bonded carbon atoms. A second form called lonsdaleite, with hexagonal symmetry, has also been found, but it is extremely rare and forms only in meteorites or in laboratory synthesis. The local environment of each atom is identical in the two structures. From theoretical considerations, lonsdaleite is expected to be harder than diamond, but the size and quality of the available stones are insufficient to test this hypothesis. In terms of crystal habit, diamonds occur most often as euhedral (well-formed) or rounded octahedra and twinned, flattened octahedra with a triangular outline. Other forms include dodecahedra and (rarely) cubes. There is evidence that nitrogen impurities play an important role in the formation of well-shaped euhedral crystals. The largest diamonds found, such as the Cullinan Diamond, were shapeless. These diamonds are pure (i.e. type II) and therefore contain little if any nitrogen.\n\nThe faces of diamond octahedrons are highly lustrous owing to their hardness; triangular shaped growth defects (\"trigons\") or \"etch pits\" are often present on the faces. A diamond's fracture may be step-like, conchoidal (shell-like, similar to glass) or irregular. Diamonds which are nearly round, due to the formation of multiple steps on octahedral faces, are commonly coated in a gum-like skin (\"nyf\"). The combination of stepped faces, growth defects, and nyf produces a \"scaly\" or corrugated appearance. Many diamonds are so distorted that few crystal faces are discernible. Some diamonds found in Brazil and the Democratic Republic of the Congo are polycrystalline and occur as opaque, darkly colored, spherical, radial masses of tiny crystals; these are known as ballas and are important to industry as they lack the cleavage planes of single-crystal diamond. Carbonado is a similar opaque microcrystalline form which occurs in shapeless masses. Like ballas diamond, carbonado lacks cleavage planes and its specific gravity varies widely from 2.9 to 3.5. Bort diamonds, found in Brazil, Venezuela, and Guyana, are the most common type of industrial-grade diamond. They are also polycrystalline and often poorly crystallized; they are translucent and cleave easily.\n\nBecause of its great hardness and strong molecular bonding, a cut diamond's facets and facet edges appear the flattest and sharpest. A curious side effect of diamond's surface perfection is \"hydrophobia\" combined with \"lipophilia\". The former property means a drop of water placed on a diamond will form a coherent droplet, whereas in most other minerals the water would spread out to cover the surface. Similarly, diamond is unusually lipophilic, meaning grease and oil readily collect on a diamond's surface. Whereas on other minerals oil would form coherent drops, on a diamond the oil would spread. This property is exploited in the use of so-called \"grease pens,\" which apply a line of grease to the surface of a suspect diamond simulant. Diamond surfaces are hydrophobic when the surface carbon atoms terminate with a hydrogen atom and hydrophilic when the surface atoms terminate with an oxygen atom or hydroxyl radical. Treatment with gases or plasmas containing the appropriate gas, at temperatures of 450 °C or higher, can change the surface property completely. Naturally occurring diamonds have a surface with less than a half monolayer coverage of oxygen, the balance being hydrogen and the behavior is moderately hydrophobic. This allows for separation from other minerals at the mine using the so-called \"grease-belt\".\n\nUnlike hardness, which denotes only resistance to scratching, diamond's toughness or tenacity is only fair to good. Toughness relates to the ability to resist breakage from falls or impacts. Because of diamond's perfect and easy cleavage, it is vulnerable to breakage. A diamond will shatter if hit with an ordinary hammer. The toughness of natural diamond has been measured as 2.0 MPa m, which is good compared to other gemstones, but poor compared to most engineering materials. As with any material, the macroscopic geometry of a diamond contributes to its resistance to breakage. Diamond has a cleavage plane and is therefore more fragile in some orientations than others. Diamond cutters use this attribute to cleave some stones, prior to faceting.\n\nBallas and carbonado diamond are exceptional, as they are polycrystalline and therefore much tougher than single-crystal diamond; they are used for deep-drilling bits and other demanding industrial applications. Particular faceting shapes of diamonds are more prone to breakage and thus may be uninsurable by reputable insurance companies. The brilliant cut of gemstones is designed specifically to reduce the likelihood of breakage or splintering.\n\nSolid foreign crystals are commonly present in diamond. They are mostly minerals, such as olivine, garnets, ruby, and many others. These and other inclusions, such as internal fractures or \"feathers\", can compromise the structural integrity of a diamond. Cut diamonds that have been enhanced to improve their clarity via glass infilling of fractures or cavities are especially fragile, as the glass will not stand up to ultrasonic cleaning or the rigors of the jeweler's torch. Fracture-filled diamonds may shatter if treated improperly.\n\nUsed in so-called diamond anvil experiments to create high-pressure environments, diamonds are able to withstand crushing pressures in excess of 600 gigapascals (6 million atmospheres).\n\nDiamonds occur in various colors: black, brown, yellow, gray, white, blue, orange, purple to pink and red. Colored diamonds contain crystallographic defects, including substitutional impurities and structural defects, that cause the coloration. Theoretically, pure diamonds would be transparent and colorless. Diamonds are scientifically classed into two main \"types\" and several subtypes, according to the nature of defects present and how they affect light absorption:\n\nType I diamond has nitrogen (N) atoms as the main impurity, at a concentration of up to 1%. If the N atoms are in pairs or larger aggregates, they do not affect the diamond's color; these are Type Ia. About 98% of gem diamonds are type Ia: these diamonds belong to the \"Cape series\", named after the diamond-rich region formerly known as Cape Province in South Africa, whose deposits are largely Type Ia. If the nitrogen atoms are dispersed throughout the crystal in isolated sites (not paired or grouped), they give the stone an intense yellow or occasionally brown tint (type Ib); the rare canary diamonds belong to this type, which represents only ~0.1% of known natural diamonds. Synthetic diamond containing nitrogen is usually of type Ib. Type Ia and Ib diamonds absorb in both the infrared and ultraviolet region of the electromagnetic spectrum, from 320 nm. They also have a characteristic fluorescence and visible absorption spectrum (see Optical properties).\n\nType II diamonds have very few if any nitrogen impurities. Pure (type IIa) diamond can be colored pink, red, or brown owing to structural anomalies arising through \"plastic deformation\" during crystal growth; these diamonds are rare (1.8% of gem diamonds), but constitute a large percentage of Australian diamonds. Type IIb diamonds, which account for ~0.1% of gem diamonds, are usually a steely blue or gray due to boron atoms scattered within the crystal matrix. These diamonds are also semiconductors, unlike other diamond types (see Electrical properties). Most blue-gray diamonds coming from the Argyle mine of Australia are not of type IIb, but of Ia type. Those diamonds contain large concentrations of defects and impurities (especially hydrogen and nitrogen) and the origin of their color is yet uncertain. Type II diamonds weakly absorb in a different region of the infrared (the absorption is due to the diamond lattice rather than impurities), and transmit in the ultraviolet below 225 nm, unlike type I diamonds. They also have differing fluorescence characteristics, but no discernible visible absorption spectrum.\n\nCertain diamond enhancement techniques are commonly used to artificially produce an array of colors, including blue, green, yellow, red, and black. Color enhancement techniques usually involve irradiation, including proton bombardment via cyclotrons; neutron bombardment in the piles of nuclear reactors; and electron bombardment by Van de Graaff generators. These high-energy particles physically alter the diamond's crystal lattice, knocking carbon atoms out of place and producing color centers. The depth of color penetration depends on the technique and its duration, and in some cases the diamond may be left radioactive to some degree.\n\nSome irradiated diamonds are completely natural; one famous example is the Dresden Green Diamond. In these natural stones the color is imparted by \"radiation burns\" (natural irradiation by alpha particles originating from uranium ore) in the form of small patches, usually only micrometers deep. Additionally, Type IIa diamonds can have their structural deformations \"repaired\" via a high-pressure high-temperature (HPHT) process, removing much or all of the diamond's color.\n\nThe luster of a diamond is described as 'adamantine', which simply means diamond-like. Reflections on a properly cut diamond's facets are undistorted, due to their flatness. The refractive index of diamond (as measured via sodium light, 589.3 nm) is 2.417. Because it is cubic in structure, diamond is also isotropic. Its high dispersion of 0.044 (variation of refractive index across the visible spectrum) manifests in the perceptible \"fire\" of cut diamonds. This fire—flashes of prismatic colors seen in transparent stones—is perhaps diamond's most important optical property from a jewelry perspective. The prominence or amount of fire seen in a stone is heavily influenced by the choice of diamond cut and its associated proportions (particularly crown height), although the body color of fancy (i.e., unusual) diamonds may hide their fire to some degree.\n\nMore than 20 other minerals have higher dispersion (that is difference in refractive index for blue and red light) than diamond, such as titanite 0.051, andradite 0.057, cassiterite 0.071, strontium titanate 0.109, sphalerite 0.156, synthetic rutile 0.330, cinnabar 0.4, etc. (see dispersion). However, the combination of dispersion with extreme hardness, wear and chemical resistivity, as well as clever marketing, determines the exceptional value of diamond as a gemstone.\nDiamonds exhibit fluorescence, that is, they emit light of various colors and intensities under long-wave ultraviolet light (365 nm): Cape series stones (type Ia) usually fluoresce blue, and these stones may also phosphoresce yellow, a unique property among gemstones. Other possible long-wave fluorescence colors are green (usually in brown stones), yellow, mauve, or red (in type IIb diamonds). In natural diamonds, there is typically little if any response to short-wave ultraviolet, but the reverse is true of synthetic diamonds. Some natural type IIb diamonds phosphoresce blue after exposure to short-wave ultraviolet. In natural diamonds, fluorescence under X-rays is generally bluish-white, yellowish or greenish. Some diamonds, particularly Canadian diamonds, show no fluorescence.\n\nThe origin of the luminescence colors is often unclear and not unique. Blue emission from type IIa and IIb diamonds is reliably identified with dislocations by directly correlating the emission with dislocations in an electron microscope. However, blue emission in type Ia diamond could be either due to dislocations or the N3 defects (three nitrogen atoms bordering a vacancy). Green emission in natural diamond is usually due to the H3 center (two substitutional nitrogen atoms separated by a vacancy), whereas in synthetic diamond it usually originates from nickel used as a catalyst (see figure). Orange or red emission could be due to various reasons, one being the nitrogen-vacancy center which is present in sufficient quantities in all types of diamond, even type IIb.\n\nCape series (Ia) diamonds have a visible absorption spectrum (as seen through a direct-vision spectroscope) consisting of a fine line in the violet at 415.5 nm; however, this line is often invisible until the diamond has been cooled to very low temperatures. Associated with this are weaker lines at 478 nm, 465 nm, 452 nm, 435 nm, and 423 nm.\nAll those lines are labeled as N3 and N2 optical centers and associated with a defect consisting of three nitrogen atoms bordering a vacancy. Other stones show additional bands: brown, green, or yellow diamonds show a band in the green at 504 nm (H3 center, see above), sometimes accompanied by two additional weak bands at 537 nm and 495 nm (H4 center, a large complex presumably involving 4 substitutional nitrogen atoms and 2 lattice vacancies). Type IIb diamonds may absorb in the far red due to the substitutional boron, but otherwise show no observable visible absorption spectrum.\n\nGemological laboratories make use of spectrophotometer machines that can distinguish natural, artificial, and color-enhanced diamonds. The spectrophotometers analyze the infrared, visible, and ultraviolet absorption and luminescence spectra of diamonds cooled with liquid nitrogen to detect tell-tale absorption lines that are not normally discernible.\n\nDiamond is a good electrical insulator, having a resistivity of 100 GΩ·m to 1 EΩ·m (10 to 10 Ω·m). Most natural blue diamonds are an exception and are semiconductors due to substitutional boron impurities replacing carbon atoms. Natural blue or blue-gray diamonds, common for the Argyle diamond mine in Australia, are rich in hydrogen; these diamonds are not semiconductors and it is unclear whether hydrogen is actually responsible for their blue-gray color. Natural blue diamonds containing boron and synthetic diamonds doped with boron are p-type semiconductors. N-type diamond films are reproducibly synthesized by phosphorus doping during chemical vapor deposition. Diode p-n junctions and UV light emitting diodes (LEDs, at 235 nm) have been produced by sequential deposition of p-type (boron-doped) and n-type (phosphorus-doped) layers.\n\nDiamond transistors have been produced (for research purposes). FETs with SiN dielectric layers, and SC-FETs have been made.\n\nIn April 2004, the journal Nature reported that below the superconducting transition temperature 4 K, boron-doped diamond synthesized at high temperature and high pressure is a bulk superconductor. Superconductivity was later observed in heavily boron-doped films grown by various chemical vapor deposition techniques, and the highest reported transition temperature (by 2009) is 11.4 K. (See also Covalent superconductor#Diamond)\n\nUncommon magnetic properties (spin glass state) were observed in diamond nanocrystals intercalated with potassium. Unlike paramagnetic host material, magnetic susceptibility measurements of intercalated nanodiamond revealed distinct ferromagnetic behavior at 5 K. This is essentially different from results of potassium intercalation in graphite or C60 fullerene, and shows that sp3 bonding promotes magnetic ordering in carbon. The measurements presented first experimental evidence of intercalation-induced spin-glass state in a nanocrystalline diamond system.\n\nUnlike most electrical insulators, diamond is a good conductor of heat because of the strong covalent bonding and low phonon scattering. Thermal conductivity of natural diamond was measured to be about 2200W/(m·K), which is five times more than copper. Monocrystalline synthetic diamond enriched to 99.9% the isotope C has the highest thermal conductivity of any known solid at room temperature: 3320 W/(m·K). Because diamond has such high thermal conductance it is already used in semiconductor manufacture to prevent silicon and other semiconducting materials from overheating. At lower temperatures conductivity becomes even better, and reaches 41000 W/(m·K) at 104 K (C-enriched diamond).\n\nDiamond's high thermal conductivity is used by jewelers and gemologists who may employ an electronic \"thermal probe\" to distinguish diamonds from their imitations. These probes consist of a pair of battery-powered thermistors mounted in a fine copper tip. One thermistor functions as a heating device while the other measures the temperature of the copper tip: if the stone being tested is a diamond, it will conduct the tip's thermal energy rapidly enough to produce a measurable temperature drop. This test takes about 2–3 seconds. However, older probes will be fooled by moissanite, a crystalline mineral form of silicon carbide introduced in 1998 as an alternative to diamonds, which has a similar thermal conductivity.\n\nBeing a form of carbon, diamond oxidizes in air if heated over 700 °C. In absence of oxygen, e.g. in a flow of high-purity argon gas, diamond can be heated up to about 1700 °C. Its surface blackens, but can be recovered by re-polishing. At high pressure (~20 GPa) diamond can be heated up to 2500 °C, and a report published in 2009 suggests that diamond can withstand temperatures of 3000 °C and above.\n\nDiamonds are carbon crystals that form deep within the Earth under high temperatures and extreme pressures. At surface air pressure (one atmosphere), diamonds are not as stable as graphite, and so the decay of diamond is thermodynamically favorable (δ\"H\" = −2 kJ / mol). So, contrary to De Beers' ad campaign extending from 1948 to at least 2013 under the slogan \"A diamond is forever\", diamonds are definitely not forever. However, owing to a very large kinetic energy barrier, diamonds are metastable; they will not decay into graphite under normal conditions.\n\n\n"}
{"id": "19768186", "url": "https://en.wikipedia.org/wiki?curid=19768186", "title": "Mechanics of planar particle motion", "text": "Mechanics of planar particle motion\n\nThis article describes a particle in planar motion when observed from non-inertial reference frames. The most famous examples of planar motion are related to the motion of two spheres that are gravitationally attracted to one another, and the generalization of this problem to planetary motion. See centrifugal force, two-body problem, orbit and Kepler's laws of planetary motion. Those problems fall in the general field of analytical dynamics, the determination of orbits from given laws of force. This article is focused more on the kinematical issues surrounding planar motion, that is, determination of the forces necessary to result in a certain trajectory \"given\" the particle trajectory. \nGeneral results presented in fictitious forces here are applied to observations of a moving particle as seen from several specific non-inertial frames, for example, a \"local\" frame (one tied to the moving particle so it appears stationary), and a \"co-rotating\" frame (one with an arbitrarily located but fixed axis and a rate of rotation that makes the particle appear to have only radial motion and zero azimuthal motion). The Lagrangian approach to fictitious forces is introduced.\n\nUnlike real forces such as electromagnetic forces, fictitious forces do not originate from physical interactions between objects.\n\nThe appearance of fictitious forces normally is associated with use of a non-inertial frame of reference, and their absence with use of an inertial frame of reference. The connection between inertial frames and fictitious forces (also called \"inertial forces\" or \"pseudo-forces\"), is expressed, for example, by Arnol'd:\nA slightly different tack on the subject is provided by Iro:\nFictitious forces do not appear in the equations of motion in an inertial frame of reference: in an inertial frame, the motion of an object is explained by the real impressed forces. In a non-inertial frame such as a rotating frame, however, Newton's first and second laws still can be used to make accurate physical predictions provided fictitious forces are included along with the real forces. For solving problems of mechanics in non-inertial reference frames, the advice given in textbooks is to treat the fictitious forces like real forces and to pretend you are in an inertial frame.\n\nIt should be mentioned that \"treating the fictitious forces like real forces\" means, in particular, that fictitious forces as seen in a particular non-inertial frame transform as \"vectors\" under coordinate transformations made within that frame, that is, like real forces.\n\nNext, it is observed that time varying coordinates are used in both inertial and non-inertial frames of reference, so the use of time varying coordinates should not be confounded with a change of observer, but is only a change of the observer's choice of description. Elaboration of this point and some citations on the subject follow.\n\nThe term frame of reference is used often in a very broad sense, but for the present discussion its meaning is restricted to refer to an observer's \"state of motion\", that is, to either an inertial frame of reference or a non-inertial frame of reference.\n\nThe term coordinate system is used to differentiate between different possible choices for a set of variables to describe motion, choices available to any observer, regardless of their state of motion. Examples are Cartesian coordinates, polar coordinates and (more generally) curvilinear coordinates.\nHere are two quotes relating \"state of motion\" and \"coordinate system\":\n\nIn a general coordinate system, the basis vectors for the coordinates may vary in time at fixed positions, or they may vary with position at fixed times, or both. It may be noted that coordinate systems attached to both inertial frames and non-inertial frames can have basis vectors that vary in time, space or both, for example the description of a trajectory in polar coordinates as seen from an inertial frame. or as seen from a rotating frame. A time-dependent \"description\" of observations does not change the frame of reference in which the observations are made and recorded.\n\nIn discussion of a particle moving in a circular orbit, in an inertial frame of reference one can identify the centripetal and tangential forces. It then seems to be no problem to switch hats, change perspective, and talk about the fictitious forces commonly called the centrifugal and Euler force. But what underlies this switch in vocabulary is a change of observational frame of reference from the inertial frame where we started, where centripetal and tangential forces make sense, to a rotating frame of reference where the particle appears motionless and fictitious centrifugal and Euler forces have to be brought into play. That switch is unconscious, but real.\n\nSuppose we sit on a particle in general planar motion (not just a circular orbit). What analysis underlies a switch of hats to introduce fictitious centrifugal and Euler forces?\n\nTo explore that question, begin in an inertial frame of reference. By using a coordinate system commonly used in planar motion, the so-called \"local\" coordinate system, as shown in Figure 1, it becomes easy to identify formulas for the centripetal inward force normal to the trajectory (in direction opposite to u in Figure 1), and the tangential force parallel to the trajectory (in direction u), as shown next.\n\nTo introduce the unit vectors of the local coordinate system shown in Figure 1, one approach is to begin in Cartesian coordinates in an inertial framework and describe the local coordinates in terms of these Cartesian coordinates. In Figure 1, the arc length \"s\" is the distance the particle has traveled along its path in time \"t\". The path r (\"t\") with components \"x\"(\"t\"), \"y\"(\"t\") in Cartesian coordinates is described using arc length \"s\"(\"t\") as:\nOne way to look at the use of \"s\" is to think of the path of the particle as sitting in space, like the trail left by a skywriter, independent of time. Any position on this path is described by stating its distance \"s\" from some starting point on the path. Then an incremental displacement along the path \"ds\" is described by:\nwhere primes are introduced to denote derivatives with respect to \"s\". The magnitude of this displacement is \"ds\", showing that:\nThis displacement is necessarily tangent to the curve at \"s\", showing that the unit vector tangent to the curve is:\nwhile the outward unit vector normal to the curve is \nOrthogonality can be verified by showing the vector dot product is zero. The unit magnitude of these vectors is a consequence of Eq. 1.\n\nAs an aside, notice that the use of unit vectors that are not aligned along the Cartesian \"xy\"-axes does not mean we are no longer in an inertial frame. All it means is that we are using unit vectors that vary with \"s\" to describe the path, but still observe the motion from the inertial frame.\n\nUsing the tangent vector, the angle of the tangent to the curve, say θ, is given by:\nThe radius of curvature is introduced completely formally (without need for geometric interpretation) as:\nThe derivative of θ can be found from that for sin θ:\nNow:\nin which the denominator is unity according to Eq. 1. With this formula for the derivative of the sine, the radius of curvature becomes:\nwhere the equivalence of the forms stems from differentiation of Eq. 1:\nHaving set up the description of any position on the path in terms of its associated value for \"s\", and having found the properties of the path in terms of this description, motion of the particle is introduced by stating the particle position at any time \"t\" as the corresponding value \"s (t)\".\n\nUsing the above results for the path properties in terms of \"s\", the acceleration in the inertial reference frame as described in terms of the components normal and tangential to the path of the particle can be found in terms of the function \"s\"(\"t\") and its various time derivatives (as before, \"primes\" indicate differentiation with respect to \"s\"):\nas can be verified by taking the dot product with the unit vectors u(\"s\") and u(\"s\"). This result for acceleration is the same as that for circular motion based on the radius ρ. Using this coordinate system in the inertial frame, it is easy to identify the force normal to the trajectory as the centripetal force and that parallel to the trajectory as the tangential force.\n\nNext, we change observational frames. Sitting on the particle, we adopt a non-inertial frame where the particle is at rest (zero velocity). This frame has a continuously changing origin, which at time \"t\" is the center of curvature (the center of the osculating circle in Figure 1) of the path at time \"t\", and whose rate of rotation is the angular rate of motion of the particle about that origin at time \"t\". This non-inertial frame also employs unit vectors normal to the trajectory and parallel to it.\n\nThe angular velocity of this frame is the angular velocity of the particle about the center of curvature at time \"t\". The centripetal force of the inertial frame is interpreted in the non-inertial frame where the body is at rest as a force necessary to overcome the centrifugal force. Likewise, the force causing any acceleration of speed along the path seen in the inertial frame becomes the force necessary to overcome the Euler force in the non-inertial frame where the particle is at rest. There is zero Coriolis force in the frame, because the particle has zero velocity in this frame. For a pilot in an airplane, for example, these fictitious forces are a matter of direct experience. However, these fictitious forces cannot be related to a simple observational frame of reference other than the particle itself, unless it is in a particularly simple path, like a circle.\n\nThat said, from a qualitative standpoint, the path of an airplane can be approximated by an arc of a circle for a limited time, and for the limited time a particular radius of curvature applies, the centrifugal and Euler forces can be analyzed on the basis of circular motion with that radius. See article discussing turning an airplane.\n\nNext, reference frames rotating about a fixed axis are discussed in more detail.\n\nDescription of particle motion often is simpler in non-Cartesian coordinate systems, for example, polar coordinates. When equations of motion are expressed in terms of any curvilinear coordinate system, extra terms appear that represent how the basis vectors change as the coordinates change. These terms arise automatically on transformation to polar (or cylindrical) coordinates and are thus not fictitious \"forces\", but rather are simply added \"terms\" in the acceleration in polar coordinates.\n\nIn a purely mathematical treatment, regardless of the frame that the coordinate system is associated with (inertial or non-inertial), extra terms appear in the acceleration of an observed particle when using curvilinear coordinates. For example, in polar coordinates the acceleration is given by (see below for details):\nwhich contains not just double time derivatives of the coordinates but added terms. This example employs polar coordinates, but more generally the added terms depend upon which coordinate system is chosen (that is, polar, elliptic, or whatever).\nSometimes these coordinate-system dependent \"terms\" also are referred to as \"fictitious forces\", introducing a second meaning for \"fictitious forces\", despite the fact that these terms do not have the vector transformation properties expected of forces. For example, see Shankar and Hildebrand. According to this terminology, fictitious forces are determined in part by the coordinate system itself, regardless of the frame it is attached to, that is, regardless of whether the coordinate system is attached to an inertial or a non-inertial frame of reference. In contrast, the fictitious forces defined in terms of the \"state of motion of the observer\" vanish in inertial frames of reference. To distinguish these two terminologies, the fictitious forces that vanish in an inertial frame of reference, the inertial forces of Newtonian mechanics, are called in this article the \"state-of-motion\" fictitious forces and those that originate in the interpretation of time derivatives in particular coordinate systems are called \"coordinate\" fictitious forces.\n\nAssuming it is clear that \"state of motion\" and \"coordinate system\" are \"different\", it follows that the dependence of centrifugal force (as in this article) upon \"state of motion\" and its independence from \"coordinate system\", which contrasts with the \"coordinate\" version with exactly the opposite dependencies, indicates that two different ideas are referred to by the terminology \"fictitious force\". The present article emphasizes one of these two ideas (\"state-of-motion\"), although the other also is described.\n\nBelow, polar coordinates are introduced for use in (first) an inertial frame of reference and then (second) in a rotating frame of reference. The two different uses of the term \"fictitious force\" are pointed out. First, however, follows a brief digression to explain further how the \"coordinate\" terminology for fictitious force has arisen.\n\nTo motivate the introduction of \"coordinate\" inertial forces by more than a reference to \"mathematical convenience\", what follows is a digression to show these forces correspond to what are called by some authors \"generalized\" fictitious forces or \"generalized inertia forces\". These forces are introduced via the Lagrangian mechanics approach to mechanics based upon describing a system by \"generalized coordinates\" usually denoted as {\"q\"}. The only requirement on these coordinates is that they are necessary and sufficient to uniquely characterize the state of the system: they need not be (although they could be) the coordinates of the particles in the system. Instead, they could be the angles and extensions of links in a robot arm, for instance. If a mechanical system consists of \"N\" particles and there are \"m\" independent kinematical conditions imposed, it is possible to characterize the system uniquely by \"n\" = 3\"N - m\" independent generalized coordinates {\"q\"}.\n\nIn classical mechanics, the Lagrangian is defined as the kinetic energy, formula_21, of the system minus its potential energy, formula_22. In symbols,\n\nUnder conditions that are given in Lagrangian mechanics, if the Lagrangian of a system is known, then the equations of motion of the system may be obtained by a direct substitution of the expression for the Lagrangian into the Euler–Lagrange equation, a particular family of partial differential equations.\n\nHere are some definitions:\n\nIt is not the purpose here to outline how Lagrangian mechanics works. The interested reader can look at other articles explaining this approach. For the moment, the goal is simply to show that the Lagrangian approach can lead to \"generalized fictitious forces\" that \"do not vanish in inertial frames\". What is pertinent here is that in the case of a single particle, the Lagrangian approach can be arranged to capture exactly the \"coordinate\" fictitious forces just introduced.\n\nTo proceed, consider a single particle, and introduce the generalized coordinates as {\"q\"} = (\"r, θ\"). Then Hildebrand shows in polar coordinates with the \"q\" = \"(r, θ)\" the \"generalized momenta\" are:\nleading, for example, to the generalized force:\nwith \"Q\" the impressed radial force. The connection between \"generalized forces\" and Newtonian forces varies with the choice of coordinates. This Lagrangian formulation introduces exactly the \"coordinate\" form of fictitious forces mentioned above that allows \"fictitious\" (generalized) forces in inertial frames, for example, the term formula_31 Careful reading of Hildebrand shows he doesn't discuss the role of \"inertial frames of reference\", and in fact, says \"[The] presence or absence [of inertia forces] depends, not upon the particular problem at hand but \"upon the coordinate system chosen\".\" By coordinate system presumably is meant the choice of {\"q\"}. Later he says \"If \"accelerations\" associated with generalized coordinates are to be of prime interest (as is usually the case), the [nonaccelerational] terms may be conveniently transferred to the right … and considered as additional (generalized) inertia forces. Such inertia forces are often said to be of the \"Coriolis\" type.\"\n\nIn short, the emphasis of some authors upon coordinates and their derivatives and their introduction of (generalized) fictitious forces that do not vanish in inertial frames of reference is an outgrowth of the use of generalized coordinates in Lagrangian mechanics. For example, see McQuarrie Hildebrand, and von Schwerin. Below is an example of this usage as employed in the design of robotic manipulators:\nFor a robot manipulator, the equations may be written in a form using Christoffel symbols \"Γ\" (discussed further below) as:\n\nwhere \"M\" is the \"manipulator inertia matrix\" and \"V\" is the potential energy due to gravity (for example), and formula_33 are the generalized forces on joint \"i\". The terms involving Christoffel symbols therefore determine the \"generalized centrifugal\" and \"generalized Coriolis\" terms.\n\nThe introduction of \"generalized\" fictitious forces often is done without notification and without specifying the word \"generalized\". This sloppy use of terminology leads to endless confusion because these \"generalized\" fictitious forces, unlike the standard \"state-of-motion\" fictitious forces, do not vanish in inertial frames of reference.\n\nBelow, the acceleration of a particle is derived as seen in an inertial frame using polar coordinates. There are no \"state-of-motion\" fictitious forces in an inertial frame, by definition. Following that presentation, the contrasting terminology of \"coordinate\" fictitious forces is presented and critiqued on the basis of the non-vectorial transformation behavior of these \"forces\".\n\nIn an inertial frame, let formula_34 be the position vector of a moving particle. Its Cartesian components (\"x\", \"y\") are:\n\nwith polar coordinates \"r\" and θ depending on time \"t\".\n\nUnit vectors are defined in the radially outward direction formula_34: \n\nand in the direction at right angles to formula_34:\n\nThese unit vectors vary in direction with time:\nand:\n\nUsing these derivatives, the first and second derivatives of position are:\n\nwhere dot-overmarkings indicate time differentiation. With this form for the acceleration formula_44, in an inertial frame of reference Newton's second law expressed in polar coordinates is:\nwhere F is the net real force on the particle. No fictitious forces appear because all fictitious forces are zero by definition in an inertial frame.\n\nFrom a mathematical standpoint, however, it sometimes is handy to put only the second-order derivatives on the right side of this equation; that is we write the above equation by rearrangement of terms as:\nwhere a \"coordinate\" version of the \"acceleration\" is introduced:\nconsisting of only second-order time derivatives of the coordinates \"r\" and θ. The terms moved to the force-side of the equation are now treated as \"extra\" \"fictitious forces\" and, confusingly, the resulting forces also are called the \"centrifugal\" and \"Coriolis\" force.\n\nThese newly defined \"forces\" are non-zero in an \"inertial frame\", and so certainly are not the same as the previously identified fictitious forces that are zero in an inertial frame and non-zero only in a non-inertial frame. In this article, these newly defined forces are called the \"coordinate\" centrifugal force and the \"coordinate\" Coriolis force to separate them from the \"state-of-motion\" forces.\n\nHere is an illustration showing the so-called \"centrifugal term\" formula_48 does not transform as a true force, putting any reference to this term not just as a \"term\", but as a centrifugal \"force\", in a dubious light. Suppose in frame \"S\" a particle moves radially away from the origin at a constant velocity. See Figure 2. The force on the particle is zero by Newton's first law. Now we look at the same thing from frame \"S' \", which is the same, but displaced in origin. In \"S' \" the particle still is in straight line motion at constant speed, so again the force is zero.\n\nWhat if we use polar coordinates in the two frames? In frame \"S\" the radial motion is constant and there is no angular motion. Hence, the acceleration is:\nand each term individually is zero because formula_50 and formula_51. There is no force, including no formula_48 \"force\" in frame \"S\".\nIn frame \"S' \", however, we have:\nIn this case the azimuthal term is zero, being the rate of change of angular momentum. To obtain zero acceleration in the radial direction, however, we require:\nThe right-hand side is non-zero, inasmuch as neither formula_55 nor formula_56 is zero. That is, we cannot obtain zero force (zero formula_57) if we retain only formula_58 as the acceleration; we need both terms.\n\nDespite the above facts, suppose we adopt polar coordinates, and wish to say that formula_48 is \"centrifugal force\", and reinterpret formula_60 as \"acceleration\" (without dwelling upon any possible justification). How does this decision fare when we consider that a proper formulation of physics is geometry and coordinate-independent? See the article on general covariance. To attempt to form a covariant expression, this so-called centrifugal \"force\" can be put into vector notation as:\n\nwith:\n\nand formula_63 a unit vector normal to the plane of motion. Unfortunately, although this expression formally looks like a vector, when an observer changes origin the value of formula_64 changes (see Figure 2), so observers in the same frame of reference standing on different street corners see different \"forces\" even though the actual events they witness are identical.\nHow can a physical force (be it fictitious or real) be zero in one frame \"S\", but non-zero in another frame \"S' \" identical, but a few feet away? Even for exactly the same particle behavior the expression formula_48 is different in every frame of reference, even for very trivial distinctions between frames. In short, if we take formula_48 as \"centrifugal force\", it does not have a universal significance: it is \"unphysical\".\n\nBeyond this problem, the real impressed net force is zero. (There is no real impressed force in straight-line motion at constant speed). If we adopt polar coordinates, and wish to say that formula_48 is \"centrifugal force\", and reinterpret formula_60 as \"acceleration\", the oddity results in frame \"S' \" that straight-line motion at constant speed requires a net force in polar coordinates, but not in Cartesian coordinates. Moreover, this perplexity applies in frame \"S\", but not in frame \"S\".\n\nThe absurdity of the behavior of formula_48 indicates that one must say that formula_48 is \"not\" centrifugal \"force\", but simply one of two \"terms\" in the acceleration. This view, that the acceleration is composed of two terms, is frame-independent: there is zero centrifugal force in any and every inertial frame. It also is coordinate-system independent: we can use Cartesian, polar, or any other curvilinear system: they all produce zero.\n\nApart from the above physical arguments, of course, the derivation above, based upon application of the mathematical rules of differentiation, shows the radial acceleration does indeed consist of the two terms formula_71.\n\nThat said, the next subsection shows there is a connection between these centrifugal and Coriolis \"terms\" and the fictitious \"forces\" that pertain to a particular \"rotating\" frame of reference (as distinct from an inertial frame).\n\nIn the case of planar motion of a particle, the \"coordinate\" centrifugal and Coriolis acceleration terms found above to be non-zero in an inertial frame can be shown to be the negatives of the \"state-of-motion\" centrifugal and Coriolis terms that appear in a very particular non-inertial \"co-rotating\" frame (see next subsection). See Figure 3. To define a co-rotating frame, first an origin is selected from which the distance \"r(t)\" to the particle is defined. An axis of rotation is set up that is perpendicular to the plane of motion of the particle, and passing through this origin. Then, at the selected moment \"t\", the rate of rotation of the co-rotating frame Ω is made to match the rate of rotation of the particle about this axis, \"dθ/dt\". The co-rotating frame applies only for a moment, and must be continuously re-selected as the particle moves. For more detail, see Polar coordinates, centrifugal and Coriolis terms.\n\nNext, the same approach is used to find the fictitious forces of a (non-inertial) rotating frame. For example, if a rotating polar coordinate system is adopted for use in a rotating frame of observation, both rotating at the same constant counterclockwise rate Ω, we find the equations of motion in this frame as follows: the radial coordinate in the rotating frame is taken as \"r\", but the angle θ' in the rotating frame changes with time:\nConsequently,\nPlugging this result into the acceleration using the unit vectors of the previous section:\nThe leading two terms are the same form as those in the inertial frame, and they are the only terms if the frame is \"not\" rotating, that is, if Ω=0. However, in this rotating frame we have the extra terms:\n\nThe radial term Ω \"r\" is the centrifugal force per unit mass due to the system's rotation at rate Ω and the radial term formula_77 is the radial component of the Coriolis force per unit mass, where formula_78 is the tangential component of the particle velocity as seen in the rotating frame. The term formula_79 is the so-called \"azimuthal\" component of the Coriolis force per unit mass. In fact, these extra terms can be used to \"measure\" Ω and provide a test to see whether or not the frame is rotating, just as explained in the example of rotating identical spheres. If the particle's motion can be described by the observer using Newton's laws of motion \"without\" these Ω-dependent terms, the observer is in an inertial frame of reference where Ω=0.\n\nThese \"extra terms\" in the acceleration of the particle are the \"state of motion\" fictitious forces for this rotating frame, the forces introduced by rotation of the frame at angular rate Ω.\n\nIn this rotating frame, what are the \"coordinate\" fictitious forces? As before, suppose we choose to put only the second-order time derivatives on the right side of Newton's law:\n\nIf we choose for convenience to treat formula_83 as some so-called \"acceleration\", then the terms formula_84 are added to the so-called \"fictitious force\", which are not \"state-of-motion\" fictitious forces, but are actually components of force that persist even when Ω=0, that is, they persist even in an inertial frame of reference. Because these extra terms are added, the \"coordinate\" fictitious force is not the same as the \"state-of-motion\" fictitious force. Because of these extra terms, the \"coordinate\" fictitious force is not zero even in an inertial frame of reference.\n\nNotice however, the case of a rotating frame that happens to have the same angular rate as the particle, so that Ω = \"dθ/dt\" at some particular moment (that is, the polar coordinates are set up in the instantaneous, non-inertial co-rotating frame of Figure 3). In this case, at this moment, \"dθ'/dt = 0\". In this co-rotating non-inertial frame at this moment the \"coordinate\" fictitious forces are only those due to the motion of the frame, that is, they are the same as the \"state-of-motion\" fictitious forces, as discussed in the remarks about the co-rotating frame of Figure 3 in the previous section.\n\nTo quote Bullo and Lewis: \"Only in exceptional circumstances can the configuration of Lagrangian system be described by a vector in a vector space. In the natural mathematical setting, the system's configuration space is described loosely as a curved space, or more accurately as a differentiable manifold.\"\n\nInstead of Cartesian coordinates, when equations of motion are expressed in a curvilinear coordinate system, Christoffel symbols appear in the acceleration of a particle expressed in this coordinate system, as described below in more detail. Consider description of a particle motion from the viewpoint of an \"inertial frame of reference\" in curvilinear coordinates. Suppose the position of a point \"P\" in Cartesian coordinates is (\"x\", \"y\", \"z\") and in curvilinear coordinates is (\"q\", \"q\". \"q\"). Then functions exist that relate these descriptions:\nand so forth. (The number of dimensions may be larger than three.) An important aspect of such coordinate systems is the element of arc length that allows distances to be determined. If the curvilinear coordinates form an orthogonal coordinate system, the element of arc length \"ds\" is expressed as:\nwhere the quantities \"h\" are called \"scale factors\". A change \"dq\" in \"q\" causes a displacement \"h dq\" along the coordinate line for \"q\". At a point \"P\", we place unit vectors e each tangent to a coordinate line of a variable \"q\". Then any vector can be expressed in terms of these basis vectors, for example, from an inertial frame of reference, the position vector of a moving particle r located at time \"t\" at position \"P\" becomes:\nwhere \"q\" is the vector dot product of r and e.\nThe velocity v of a particle at \"P\", can be expressed at \"P\" as:\nwhere \"v\" is the vector dot product of v and e, and over dots indicate time differentiation.\nThe time derivatives of the basis vectors can be expressed in terms of the scale factors introduced above. for example:\nin which the coefficients of the unit vectors are the Christoffel symbols for the coordinate system. The general notation and formulas for the Christoffel symbols are:\nwhich allows all the time derivatives to be evaluated. For example, for the velocity:\nwith the Γ-notation for the Christoffel symbols replacing the curly bracket notation.\nUsing the same approach, the acceleration is then\nLooking at the relation for acceleration, the first summation contains the time derivatives of velocity, which would be associated with acceleration if these were Cartesian coordinates, and the second summation (the one with Christoffel symbols) contains terms related to the way the unit vectors change with time.\n\nEarlier in this article a distinction was introduced between two terminologies, the fictitious forces that vanish in an inertial frame of reference are called in this article the \"state-of-motion\" fictitious forces and those that originate from differentiation in a particular coordinate system are called \"coordinate\" fictitious forces. Using the expression for the acceleration above, Newton's law of motion in the inertial frame of reference becomes:\nwhere F is the net real force on the particle. No \"state-of-motion\" fictitious forces are present because the frame is inertial, and \"state-of-motion\" fictitious forces are zero in an inertial frame, by definition.\n\nThe \"coordinate\" approach to Newton's law above is to retain the second-order time derivatives of the coordinates {\"q\"} as the only terms on the right side of this equation, motivated more by mathematical convenience than by physics. To that end, the force law can be rewritten, taking the second summation to the force-side of the equation as:\nwith the convention that the \"acceleration\" formula_105 is now:\nIn the expression above, the summation added to the force-side of the equation now is treated as if added \"forces\" were present. These summation terms are customarily called fictitious forces within this \"coordinate\" approach, although in this inertial frame of reference all \"state-of-motion\" fictitious forces are identically zero. Moreover, these \"forces\" do not transform under coordinate transformations as \"vectors\". Thus, the designation of the terms of the summation as \"fictitious forces\" uses this terminology for contributions that are completely different from any real force, and from the \"state-of-motion\" fictitious forces. What adds to this confusion is that these \"coordinate\" fictitious forces are divided into two groups and given the \"same names\" as the \"state-of-motion\" fictitious forces, that is, they are divided into \"centrifugal\" and \"Coriolis\" terms, despite their inclusion of terms that are not the \"state-of-motion\" centrifugal and Coriolis terms. For example, these \"coordinate\" centrifugal and Coriolis terms can be nonzero \"even in an inertial frame of reference\" where the \"state-of-motion\" centrifugal force (the subject of this article) and Coriolis force always are zero.\n\nIf the frame is not inertial, for example, in a rotating frame of reference, the \"state-of-motion\" fictitious forces are included in the above \"coordinate\" fictitious force expression. Also, if the \"acceleration\" expressed in terms of first-order time derivatives of the velocity happens to result in terms that are \"not\" simply second-order derivatives of the coordinates {\"q\"} in time, then these terms that are not second-order also are brought to the force-side of the equation and included with the fictitious forces. From the standpoint of a Lagrangian formulation, they can be called \"generalized\" fictitious forces. See Hildebrand, for example.\n\nFormulation of dynamics in terms of Christoffel symbols and the \"coordinate\" version of fictitious forces is used often in the design of robots in connection with a Lagrangian formulation of the equations of motion.\n\n\n\n\n\n\n<br>\n"}
{"id": "6005421", "url": "https://en.wikipedia.org/wiki?curid=6005421", "title": "Mictomagnetism", "text": "Mictomagnetism\n\nMictomagnetism is a spin system in which various exchange interactions are mixed. It is observed in several kinds of alloys, including Cu-Mn, Fe-Al and Ni-Mn alloys. Cooled in zero magnetic field, these materials have low remanence and coercivity. Cooled in a magnetic field, they have much larger remanence and the hysteresis loop is shifted in the direction opposite to the field (an effect similar to exchange bias).\n\n"}
{"id": "11270473", "url": "https://en.wikipedia.org/wiki?curid=11270473", "title": "Monte Alto photovoltaic power plant", "text": "Monte Alto photovoltaic power plant\n\nThe Monte Alto photovoltaic power plant in Spain has a generating capacity of 9.55 megawatts peak (MWp) and will generate 14 million kilowatt-hours of electricity per annum. It cost 65 million euros [US$87 million].\n\nThe installation covers an area of 51 hectares on agricultural land near the locality of Milagro (Navarre) and contains 889 solar structures, of which 864 are equipped with automated solar tracking. The rest are fixed structures adapted to the relief of the terrain.\n\nIn five years Acciona Energy has developed seven \"solar gardens\" in Navarre with a total capacity of 20 MWp, and another two are under construction in Castilla-La Mancha. Overall, the company's installed capacity is 23 megawatts (MW), through the approximately 3,000 automated solar monitoring structures, and represents a total investment of 177 million euros [US$236 million] shared among more than 2,000 owners. The yield from these investments is somewhere between 8 and 10% and the payback of the investment is estimated at around 10 years.\n\n"}
{"id": "4795774", "url": "https://en.wikipedia.org/wiki?curid=4795774", "title": "National Electric Power Regulatory Authority", "text": "National Electric Power Regulatory Authority\n\nThe National Electric Power Regulatory Authority (, abbreviated as NEPRA) is responsible for regulating the electricity supply in Pakistan. \n\nNEPRA was created when the Parliament of Pakistan passed the NEPRA Act, 1997.\n\n\n"}
{"id": "3947316", "url": "https://en.wikipedia.org/wiki?curid=3947316", "title": "Non-stoichiometric compound", "text": "Non-stoichiometric compound\n\nNon-stoichiometric compounds are chemical compounds, almost always solid inorganic compounds, having elemental composition whose proportions cannot be represented by integers; most often, in such materials, some small percentage of atoms are missing or too many atoms are packed into an otherwise perfect lattice work. Contrary to earlier definitions, modern understanding of non-stoichiometric compounds view them as homogenous, and not mixtures of stoichiometric chemical compounds. Since the solids are overall electrically neutral, the defect is compensated by a change in the charge of other atoms in the solid, either by changing their oxidation state, or by replacing them with atoms of different elements with a different charge. Many metal oxides and sulfides have non-stoichiometric examples; for example, stoichiometric iron(II) oxide, which is rare, has the formula FeO, whereas the more common material is nonstoichiometric, with the formula FeO. The type of equilibrium defects in non-stoichiometric compounds can vary with attendant variation in bulk properties of the material. Non-stoichiometric compounds also exhibit special electrical or chemical properties because of the defects; for example, when atoms are missing, electrons can move through the solid more rapidly. Non-stoichiometric compounds have applications in ceramic and superconductive material and in electrochemical (i.e., battery) system designs.\n\nNonstoichiometry is pervasive for metal oxides, especially when the metal is not in its highest oxidation state. For example, although wüstite (ferrous oxide) has an ideal (stoichiometric) formula FeO, the actual stoichiometry is closer to FeO. The non-stoichiometry reflect the ease of oxidation of Fe to Fe effectively replacing a small portion of Fe with two thirds their number of Fe. Thus for every three \"missing\" Fe ions, the crystal contains two Fe ions to balance the charge. The composition of a non-stoichiometric compound usually varies in a continuous manner over a narrow range. Thus, the formula for wüstite is written as FeO, where \"x\" is a small number (0.05 in the previous example) representing the deviation from the \"ideal\" formula. Nonstoichiometry is especially important in solid, three-dimensional polymers that can tolerate mistakes. To some extent, entropy drives all solids to be non-stoichiometric. But for practical purposes, the term describes materials where the non-stoichiometry is measurable, usually at least 1% of the ideal composition.\n\nThe monosulfides of the transition metals are often nonstoichiometric. Best known perhaps is nominally iron(II) sulfide (the mineral pyrrhotite) with a composition FeS (\"x\" = 0 to 0.2). The rare stoichiometric FeS endmember is known as the mineral \"troilite\". Pyrrhotite is remarkable in that it has numerous polytypes, i.e. crystalline forms differing in symmetry (monoclinic or hexagonal) and composition (FeS, FeS, FeS and others). These materials are always iron-deficient owing to the presence of lattice defects, namely iron vacancies. Despite those defects, the composition is usually expressed as a ratio of large numbers and the crystals symmetry is relatively high. This means the iron vacancies are not randomly scattered over the crystal, but form certain regular configurations. Those vacancies strongly affect the magnetic properties of pyrrhotite: the magnetism increases with the concentration of vacancies and is absent for the stoichiometric FeS.\n\nPalladium hydride is a nonstoichiometric material of the approximate composition PdH (0.02 < \"x\" < 0.58). This solid conducts hydrogen by virtue of the mobility of the hydrogen atoms within the solid.\n\nIt is sometimes difficult to determine if a material is non-stoichiometric or if the formula is best represented by large numbers. The oxides of tungsten illustrate this situation. Starting from the idealized material tungsten trioxide, one can generate a series of related materials that are slightly deficient in oxygen. These oxygen-deficient species can be described as WO, but in fact they are stoichiometric species with large unit cells with the formulas WO, where \"n\" = 20, 24, 25, 40. Thus, the last species can be described with the stoichiometric formula WO, whereas the non-stoichiometric description WO implies a more random distribution of oxide vacancies.\n\nAt high temperatures (1000 °C), titanium sulfides present a series of non-stoichiometric compounds.\n\nThe coordination polymer Prussian blue, nominally Fe(CN) and their analogs are well known to form in non-stoichiometric proportions. The non-stoichiometric phases exhibit useful properties vis-à-vis their ability to bind caesium and thallium ions.\n\nMany useful compounds are produced by the reactions of hydrocarbons with oxygen, a conversion that is catalyzed by metal oxides. The process operates via the transfer of \"lattice\" oxygen to the hydrocarbon substrate, a step that temporarily generates a vacancy (or defect). In a subsequent step, the missing oxygen is replenished by O. Such catalysts rely on the ability of the metal oxide to form phases that are not stoichiometric. An analogous sequence of events describes other kinds of atom-transfer reactions including hydrogenation and hydrodesulfurization catalysed by solid catalysts. These considerations also highlight the fact that stoichiometry is determined by the interior of crystals: the surfaces of crystals often do not follow the stoichiometry of the bulk. The complex structures on surfaces are described by the term \"surface reconstruction\".\n\nThe migration of atoms within a solid is strongly influenced by the defects associated with non-stoichiometry. These defect sites provide pathways for atoms and ions to migrate through the otherwise dense ensemble of atoms that form the crystals. Oxygen sensors and solid state batteries are two applications that rely on oxide vacancies. One example is the CeO-based sensor in automotive exhaust systems. At low partial pressures of O, the sensor allows the introduction of increased air to effect more thorough combustion.\n\nMany superconductors are non-stoichiometric. For example, yttrium barium copper oxide, arguably the most notable high-temperature superconductor, is a non-stoichiometric solid with the formula YBaCuO. The critical temperature of the superconductor depends on the exact value of \"x\". The stoichiometric species has \"x\" = 0, but this value can be as great as 1.\n\nIt was mainly through the work of Nikolai Semenovich Kurnakov and his students that Berthollet's opposition to Proust's law was shown to have merit for many solid compounds. Kurnakov divided non-stoichiometric compounds into \"berthollides\" and \"daltonides\" depending on whether their properties showed monotonic behavior with respect to composition or not. The term berthollide was accepted by IUPAC in 1960. The names come from Claude Louis Berthollet and John Dalton, respectively, who in the 19th century advocated rival theories of the composition of substances. Although Dalton \"won\" for the most part, it was later recognized that the law of definite proportions had important exceptions.\n\n\n"}
{"id": "23196416", "url": "https://en.wikipedia.org/wiki?curid=23196416", "title": "Northern Power Systems", "text": "Northern Power Systems\n\nNorthern Power Systems designs, manufactures, and sells wind turbines, and provides engineering development services and technology licenses for energy applications, into the global marketplace. The company was founded in 1974, and has since grown into a multinational corporation headquartered in Barre, Vermont, with European headquarters in Zurich, Switzerland and a significant presence in the United Kingdom and Italy.\n\nToday's Northern Power Systems was started in 1974 in Warren, Vermont, as Northwind Power Company. Its founders hitchhiked to Colorado, Minnesota, and North Dakota to buy secondhand Jacobs wind plants. They trucked the wind turbines to Vermont where they reconditioned them for resale as the North Wind “Eagle.”\n\nIn 1978, North Wind was awarded a U.S. Department of Energy contract to develop a high-reliability 2 kW wind turbine for the growing telecommunications market. As a result of that work, North Wind developed its HR2 wind turbine, a three-bladed, horizontal axis up wind rotor configuration utilizing a slow-speed, direct-drive 2.2 kW alternator. This small wind turbine soon gained international market acceptance as one of the most rugged, high-reliability wind turbines available. Over 600 HR2 and its successor HR3 wind turbines were sold over approximately 20 years of production. Northern Power wind turbines have been installed in over 40 countries on all seven continents, with many still operational today.\n\nIn 1986, the team continued its DOE-sponsored research and development work, but also began to design, fabricate, and install high-reliability hybrid power systems for remote applications using gas- and diesel-fired reciprocating engine generators, photovoltaic panels, wind turbines, and battery banks for energy storage.\n\nIn the summer of 2008, the Vermont-based company was acquired by its current owners who have invested over $100 million into Northern Power Systems over the past several years (2008 – to date). Major shareholders include Rockport Capital, Allen & Company and BeCapital.\nToday Northern Power Systems continues plans for aggressive growth with support from some of the most trusted and well-known financial partners in the industry.\n\nNorthern Power Systems has almost 40 years’ experience in technologies and products generating renewable energy. Northern Power Systems currently manufactures the NPS 60 and NPS 100 wind turbines for community applications. The NPS 100 is rated as having a 100 kilowatt capacity, and the NPS 60 is rated as having a 60 kilowatt capacity. Northern Power Systems also manufactures the larger 2.3 megawatt turbine designed for industrial applications such as wind farms, called the Northern Power 2X platform. With millions of run time hours across its global fleet, Northern Power wind turbines provide customers with clean, cost effective, reliable renewable energy. Patented next generation permanent magnet/direct drive (PM/DD) technology uses fewer moving parts, delivers higher energy capture, and provides increased reliability due to reduced maintenance and downtime.\n\nNorthern Power Systems offers comprehensive in‐house development services, including systems level engineering, advanced drive trains, power electronics, PM machine design, and remote monitoring systems to the energy industry.\n\nSome of the world’s largest manufacturers license NPS next generation technology and IP for their utility and distributed wind products and markets.\n\n\n"}
{"id": "41913525", "url": "https://en.wikipedia.org/wiki?curid=41913525", "title": "Odd-chain fatty acid", "text": "Odd-chain fatty acid\n\nOdd chain fatty acids are those that contain an odd number of carbon atoms in the structure. Almost all animal fatty acid synthesis is done by assembling two 2C Acetyl-CoA molecules together. Because the segments are each two carbons in length the resulting fatty acid has an even number of carbon atoms in it. However, propionyl-CoA instead of acetyl-CoA is used as the primer for the synthesis of long-chain fatty acids with an odd number of carbon atoms, which are found particularly in ruminant fat and milk Some plant-based fatty acids, also have an odd number of carbon atoms, and Phytanic fatty acid absorbed from plant chlorophyll has multiple methyl branch points. As a result, it breaks down into three odd numbered 3C Propionyl segments as well as three even numbered 2C Acetyl segments and one even numbered 4C Isobutynoyl segment. In humans, in sharp contrast to butyrate and octanoate, the odd-chain SCFA, propionate, has no inhibitory effect on glycolysis and does not stimulate ketogenesis. . Odd-chain and branched-chain fatty acids, which form propionyl-CoA, can serve as minor precursors for gluconeogenesis .\n"}
{"id": "14574252", "url": "https://en.wikipedia.org/wiki?curid=14574252", "title": "Oil &amp; Gas University of Ploiești", "text": "Oil &amp; Gas University of Ploiești\n\nPetroleum-Gas University of Ploieşti (\"Universitatea Petrol-Gaze\", \"UPG\") is a public university in Ploiești, Romania. Founded in 1948 under the name of Institute of Petroleum and Gas, in response to the increasing industrialization in Romania and the lack of high level education in the petroleum and gas fields, it gained fast the status of university, hence changing its name to the actual one in 1993 and extending with new faculties and departments in the field of economic sciences and humanities.\n\nThe UPG's academic structure includes 5 faculties: Faculty of Petroleum and Gas Engineering, Faculty of Mechanical and Electrical Engineering, Faculty of Petroleum Technology and Petrochemistry, Faculty of Economic Sciences and Faculty of Letters and Sciences.\n\n"}
{"id": "26274453", "url": "https://en.wikipedia.org/wiki?curid=26274453", "title": "Oklahoma Crude (film)", "text": "Oklahoma Crude (film)\n\nOklahoma Crude is a 1973 American drama Metrocolor film directed by Stanley Kramer in Panavision. It stars George C. Scott and Faye Dunaway. It was entered into the 8th Moscow International Film Festival where Kramer won the Golden Prize for Direction. The song \"Send a Little Love My Way\", sung by Anne Murray, was featured in the film and was nominated for a Golden Globe Award for Best Original Song in 1973.\n\nSet in the early 20th century, the film is about a lone woman, Lena Doyle (Faye Dunaway) who finds herself threatened by tough businessmen who want to take her land which possesses crude oil. Rather than settle and sell the land she rightfully owns, Lena decides to fight and to do this, she accepts the help of her father (John Mills) and a hired gun named Mason (George C. Scott).\n\n\n\n"}
{"id": "486384", "url": "https://en.wikipedia.org/wiki?curid=486384", "title": "Plasma afterglow", "text": "Plasma afterglow\n\nA plasma afterglow (also afterglow) is the radiation emitted from a plasma after the source of ionization is removed.\nThe external electromagnetic fields that sustained the plasma glow are absent or insufficient to maintain the discharge in the afterglow. A plasma afterglow can either be a temporal, due to an interrupted (pulsed) plasma source, or spatial, due to a distant plasma source. In the afterglow, plasma-generated species de-excite and participate in secondary chemical reactions that tend to form stable species. Depending on the gas composition, super-elastic collisions may continue to sustain the plasma in the afterglow for a while by releasing the energy stored in rovibronic degrees of freedom of the atoms and molecules of the plasma. Especially in molecular gases, the plasma chemistry in the afterglow is significantly different from the plasma glow. The afterglow of a plasma is still a plasma and as thus retains most of the properties of a plasma.\n\nThe first published pictures of plasma afterglow were taken in 1953.\n\nHelium afterglow, one of the most commonly used forms of afterglow, was first described in 1963 by Arthur L. Schmeltekopf Jr. and H. P. Broida.\n\nThe first flowing afterglow ionization studies began in the early 1960s in an effort to understand atmospheric ion chemistry. At the time stationary afterglow studies had already been done however this approach was limited by lack of versatility and lacked consistency as studies done prior to 1964 showed common atmospheric reactions to have drastically differing reaction rates between studies. Flowing-afterglow was then used to more precisely describe the rate constants of common atmospheric reactions\n\nA remote plasma refers to a plasma that is spatially separated from the external electromagnetic fields that initiate the discharge. An afterglow is a remote plasma if the plasma is channeled away from the original plasma source.\n\nAn advantage that remote plasma has over temporal plasma is that remote plasma can be used as a continuous plasma source and thus has more applications in supplying reagent ions for most systems.\n\nRemote plasmas are often used in the field of analytical chemistry when a constant stream of ions is required. They are also very commonly used a method of cleaning complex vacuum systems without having to take them apart.\n\nA temporal plasma refers to an afterglow from a plasma source that is time delineated. Removing the source of excitation allows for an afterglow to be present in the same space that the initial plasma was excited for a short time.\n\nAn advantage that temporal plasma has over remote plasma is that it can be contained in a closed system and thus makes controlling the temperature and pressure is easier.\n\nTemporal plasma is often used to replicate ionic reactions in atmospheric conditions in a controlled environment.\n\nA flowing afterglow is an ion source that is used to create ions in a flow of inert gas, typically helium or argon. Flowing afterglow ion sources usually consist of a dielectric discharge that gases are channeled through to be excited and thus made into plasma. Flowing afterglow ion sources can be coupled with a selected-ion flow-tube for selection of reactant ions. When this ion source is coupled with mass spectrometry it is referred to as flowing afterglow mass spectrometry.\n\nFlowing-afterglow mass spectrometry uses a flowing afterglow to create protonated water cluster ions in a helium or argon carrier gas in a flow tube that react with sample molecules that are measured by a mass spectrometer downstream. These systems can be used for trace gas analysis. This works by keeping the initial ionization source spatially separated from the target analyte and channeling the afterglow of the initial ionization towards the analyte. Analytes are added downstream to create ion products. Ions Detection of ions is usually accomplished using a mass spectrometer or by optical spectroscopy.\n\nStationary afterglow (SA) is a technique for studying remote plasma that consist of a gaseous mixture inside a bulb that is subjected to an ionizing pulse. After said ionizing pulse the ion composition of the mixture is measured as a function of time at the wall of the containing bulb. Stationary afterglow methods are often used to study atmospheric reactions as they mimic atmospheric conditions in a controlled environment.\n\nPlasma afterglow has shown to be an effective means of cleaning and sterilizing difficult to take apart machinery and glassware. Plasma cleaning uses remote plasma sources to generate an afterglow that is ventilated into the system to be cleaned and then the afterglow ions react with the contaminants. When oxygen is used as the carrier gas, ionized oxygen species react with heavier organic compounds to form HO, CO, and CO. These products are then easily vented from the system effectively removing organic contaminants from the system. This provides the advantage of not having to take systems apart and thus saves time on disassembly and on vacuum systems it saves time changing the pressure of the system.\n\nThis plasma cleaning method is especially effective for chemical vapor deposition methods where cleanliness is a key part of productivity.\n\n"}
{"id": "40103334", "url": "https://en.wikipedia.org/wiki?curid=40103334", "title": "Powerful: Energy for Everyone", "text": "Powerful: Energy for Everyone\n\nPowerful: Energy for Everyone is a 2010 Canadian documentary that explores different sources of renewable energy.\n\nDavid Chernushenko takes audiences on a global journey to discover different ways of achieving a more sustainable lifestyle.The film introduces audiences to communities, both small and large, that have managed to adapt their way of life and embrace renewable energy.\n\n\n"}
{"id": "25908160", "url": "https://en.wikipedia.org/wiki?curid=25908160", "title": "Program 477L", "text": "Program 477L\n\nProgram 477L (477L), also known as NUDETS (NUclear DETonation System) was a Cold War-era system used to monitor nuclear detonations over the Continental United States.\n\n"}
{"id": "44776375", "url": "https://en.wikipedia.org/wiki?curid=44776375", "title": "Pulsed laser", "text": "Pulsed laser\n\nPulsed operation of lasers refers to any laser not classified as continuous wave, so that the optical power appears in pulses of some duration at some repetition rate. This encompasses a wide range of technologies addressing a number of different motivations. Some lasers are pulsed simply because they cannot be run in continuous mode.\n\nIn other cases the application requires the production of pulses having as large an energy as possible. Since the pulse energy is equal to the average power divided by the repetition rate, this goal can sometimes be satisfied by lowering the rate of pulses so that more energy can be built up in between pulses. In laser ablation for example, a small volume of material at the surface of a work piece can be evaporated if it is heated in a very short time, whereas supplying the energy gradually would allow for the heat to be absorbed into the bulk of the piece, never attaining a sufficiently high temperature at a particular point.\n\nOther applications rely on the peak pulse power (rather than the energy in the pulse), especially in order to obtain nonlinear optical effects. For a given pulse energy, this requires creating pulses of the shortest possible duration utilizing techniques such as Q-switching.\n\nThe optical bandwidth of a pulse cannot be narrower than the reciprocal of the pulse width. In the case of extremely short pulses, that implies lasing over a considerable bandwidth, quite contrary to the very narrow bandwidths typical of CW lasers. The lasing medium in some \"dye lasers\" and \"vibronic solid-state lasers\" produces optical gain over a wide bandwidth, making a laser possible which can thus generate pulses of light as short as a few femtoseconds.\n\nIn a Q-switched laser, the population inversion is allowed to build up by introducing loss inside the resonator which exceeds the gain of the medium; this can also be described as a reduction of the quality factor or 'Q' of the cavity. Then, after the pump energy stored in the laser medium has approached the maximum possible level, the introduced loss mechanism (often an electro- or acousto-optical element) is rapidly removed (or that occurs by itself in a passive device), allowing lasing to begin which rapidly obtains the stored energy in the gain medium. This results in a short pulse incorporating that energy, and thus a high peak power.\n\nA mode-locked laser is capable of emitting extremely short pulses on the order of tens of picoseconds down to less than 10 femtoseconds. These pulses will repeat at the round trip time, that is, the time that it takes light to complete one round trip between the mirrors comprising the resonator. Due to the Fourier limit (also known as energy-time uncertainty), a pulse of such short temporal length has a spectrum spread over a considerable bandwidth. Thus such a gain medium must have a gain bandwidth sufficiently broad to amplify those frequencies. An example of a suitable material is titanium-doped, artificially grown sapphire (Ti:sapphire) which has a very wide gain bandwidth and can thus produce pulses of only a few femtoseconds duration.\n\nSuch mode-locked lasers are a most versatile tool for researching processes occurring on extremely short time scales (known as femtosecond physics, femtosecond chemistry and ultrafast science), for maximizing the effect of nonlinearity in optical materials (e.g. in second-harmonic generation, parametric down-conversion, optical parametric oscillators and the like) due to the large peak power, and in ablation applications. Again, because of the extremely short pulse duration, such a laser will produce pulses which achieve an extremely high peak power.\n\nAnother method of achieving pulsed laser operation is to pump the laser material with a source that is itself pulsed, either through electronic charging in the case of flash lamps, or another laser which is already pulsed. Pulsed pumping was historically used with dye lasers where the inverted population lifetime of a dye molecule was so short that a high energy, fast pump was needed. The way to overcome this problem was to charge up large capacitors which are then switched to discharge through flashlamps, producing an intense flash. Pulsed pumping is also required for three-level lasers in which the lower energy level rapidly becomes highly populated preventing further lasing until those atoms relax to the ground state. These lasers, such as the excimer laser and the copper vapor laser, can never be operated in CW mode.\n\nWhen a laser beam comes into contact with soft-tissue, one important factor is to not overheat surrounding tissue, so necrosis can be prevented. Laser pulses must be spaced out to allow for efficient tissue cooling (thermal relaxation time) between pulses.\n\n\n"}
{"id": "1806952", "url": "https://en.wikipedia.org/wiki?curid=1806952", "title": "Reverse vending machine", "text": "Reverse vending machine\n\nA reverse vending machine is a device that accepts used (empty) beverage containers and returns money to the user. The machines are popular in places that have mandatory recycling laws or container deposit legislation. In some places, bottlers paid funds into a centralized pool to be disbursed to people who recycled the containers. Any excess funds were to be used for general environmental cleanup. In other places, such as Norway, the state mandated that a vendor pay for recycled bottles, but left the system in the hands of private industry.\n\nThe recycler places the empty bottle or can into the receiving aperture; the horizontal in-feed system allows the user to insert containers one at a time. (An alternative system, found in many older machines, is one in which the user opens a door by hand and places the empty container in a pan. When the door is released and closed, the process continues.) The bottle or can is then automatically rotated; the bottle/can is then scanned by an omnidirectional UPC scanner, which scans the beverage container's UPC. \nOnce a container is scanned, identified (matched to database) and determined to be a participating container, it is processed and typically crushed (for one-time-use containers) to reduce its size, to avoid spillages of liquid and to increase storage capacity. Refillable containers are collected and sorted by hand to be brought back to the bottling company. The machines can use material recognition instead of/as well as a bar code scanner when needed.\n\nThe first patent for a \"Bottle Return and Handling Machine\" was filed in the United States in 1920. The first working bottle return machine was invented and manufactured by Wicanders from Sweden used in the late 1950s. In 1962 an advanced automatic bottle return machine was designed by Aage Tveitan and manufactured in Norway by his company Arthur Tveitan AS. The first 3 in 1 machine was invented in 1994 by Kansmacker and is still in operation today in Detroit, Michigan.\n"}
{"id": "1496902", "url": "https://en.wikipedia.org/wiki?curid=1496902", "title": "Solar vehicle", "text": "Solar vehicle\n\nA solar vehicle is an electric vehicle powered completely or significantly by direct solar energy. Usually, photovoltaic (PV) cells contained in solar panels convert the sun's energy directly into electric energy. The term \"solar vehicle\" usually implies that solar energy is used to power all or part of a vehicle's propulsion. Solar power may be also used to provide power for communications or controls or other auxiliary functions.\n\nSolar vehicles are not sold as practical day-to-day transportation devices at present, but are primarily demonstration vehicles and engineering exercises, often sponsored by government agencies. However, indirectly solar-charged vehicles are widespread and solar boats are available commercially.\n\nSolar cars depend on PV cells to convert sunlight into electricity to drive electric motors. Unlike solar thermal energy which converts solar energy to heat, PV cells directly convert sunlight into electricity.\n\nThe design of a solar car is severely limited by the amount of energy input into the car. Solar cars are built for solar car races and also for public use List of prototype solar-powered cars. Even the best solar cells can only collect limited power and energy over the area of a car's surface. This limits solar cars to ultralight composite bodies to save weight. Solar cars lack the safety and convenience features of conventional vehicles. The first solar family car was built in 2013 by students in the Netherlands. This vehicle is capable of 550 miles on one charge during sunlight. It weighs 850 pounds and has a 1.5kw solar array. Solar vehicles must be light and efficient. 3,000 pound or even 2,000 pound vehicles are less practical. Stella Lux, the predecessor to Stella, broke a record with a 932 mile single charge range. The Dutch are trying to commercialize this technology. During racing Stella Lux is capable of 700 miles during daylight. At 45 mph Stella Lux has infinite range. This is again due to high efficiency including a Coefficient of drag of .16. The average family who never drive more than 200 miles a day would never need to charge from the mains. They would only plug in if they wanted to return energy to the grid.\nSolar cars are often fitted with gauges and/or wireless telemetry, to carefully monitor the car's energy consumption, solar energy capture and other parameters. Wireless telemetry is typically preferred as it frees the driver to concentrate on driving, which can be dangerous in such a small, lightweight car. The Solar Electric Vehicle system was designed and engineered as an easy to install (2 to 3 hours) integrated accessory system with a custom molded low profile solar module, supplemental battery pack and a proven charge controlling system.\n\nAs an alternative, a battery-powered electric vehicle may use a solar array to recharge; the array may be connected to the general electrical distribution grid.\n\nSolar buses are propulsed by solar energy, all or part of which is collected from stationary solar panel installations. The Tindo bus is a 100% solar bus that operates as free public transport service in Adelaide City as an initiative of the City Council. Bus services which use electric buses that are partially powered by solar panels installed on the bus roof, intended to reduce energy consumption and to prolong the life cycle of the rechargable battery of the electric bus, have been put in place in China.\n\nSolar buses are to be distinguished from conventional buses in which electric functions of the bus such as lighting, heating or air-conditioning, but not the propulsion itself, are fed by solar energy. Such systems are more widespread as they allow bus companies to meet specific regulations, for example the anti-idling laws that are in force in several of the US states, and can be retrofitted to existing vehicle batteries without changing the conventional engine.\n\nThe first solar \"cars\" were actually tricycles or Quadracycles built with bicycle technology. These were called solarmobiles at the first solar race, the Tour de Sol in Switzerland in 1985. With 72 participants, half used solar power exclusively while the other half used solar-human-powered hybrids. A few true solar bicycles were built, either with a large solar roof, a small rear panel, or a trailer with a solar panel. Later more practical solar bicycles were built with foldable panels to be set up only during parking. Even later the panels were left at home, feeding into the electric mains, and the bicycles charged from the mains. Today highly developed electric bicycles are available and these use so little power that it costs little to buy the equivalent amount of solar electricity. The \"solar\" has evolved from actual hardware to an indirect accounting system. The same system also works for electric motorcycles, which were also first developed for the Tour de Sol.\n\nThe Venturi Astrolab in 2006 was the world's first commercial electro-solar hybrid car, and was originally due to be released in January 2008.\n\nIn May 2007 a partnership of Canadian companies led by Hymotion altered a Toyota Prius to use solar cells to generate up to 240 watts of electrical power in full sunshine. This is reported as permitting up to 15 km extra range on a sunny summer day while using only the electric motors.\n\nAn inventor from Michigan, USA built a street legal, licensed, insured, solar charged electric scooter in 2005. It had a top speed controlled at a bit over 30 mph, and used fold-out solar panels to charge the batteries while parked.\n\nPhotovoltaic modules are used commercially as auxiliary power units on passenger cars in order to ventilate the car, reducing the temperature of the passenger compartment while it is parked in the sun. Vehicles such as the 2010 Prius, Aptera 2, Audi A8, and Mazda 929 have had solar sunroof options for ventilation purposes.\n\nThe area of photovoltaic modules required to power a car with conventional design is too large to be carried on board. A prototype car and trailer has been built Solar Taxi. According to the website, it is capable of 100 km/day using 6m of standard crystalline silicon cells. Electricity is stored using a nickel/salt battery. A stationary system such as a rooftop solar panel, however, can be used to charge conventional electric vehicles.\n\nIt is also possible to use solar panels to extend the range of a hybrid or electric car, as incorporated in the Fisker Karma, available as an option on the Chevy Volt, on the hood and roof of \"Destiny 2000\" modifications of Pontiac Fieros, Italdesign Quaranta, Free Drive EV Solar Bug, and numerous other electric vehicles, both concept and production. In May 2007 a partnership of Canadian companies led by Hymotion added PV cells to a Toyota Prius to extend the range. SEV claims 20 miles per day from their combined 215W module mounted on the car roof and an additional 3kWh battery.\n\nOn 9 June 2008, the German and French Presidents announced a plan to offer a credit of 6-8g/km of CO emissions for cars fitted with technologies \"not yet taken into consideration during the standard measuring cycle of the emissions of a car\". This has given rise to speculation that photovoltaic panels might be widely adopted on autos in the near future\n\nIt is also technically possible to use photovoltaic technology, (specifically thermophotovoltaic (TPV) technology) to provide motive power for a car. Fuel is used to heat an emitter. The infrared radiation generated is converted to electricity by a low band gap PV cell (e.g. GaSb). A prototype TPV hybrid car was even built. The \"Viking 29\" was the World’s first thermophotovoltaic (TPV) powered automobile, designed and built by the Vehicle Research Institute (VRI) at Western Washington University. Efficiency would need to be increased and cost decreased to make TPV competitive with fuel cells or internal combustion engines.\n\nSeveral personal rapid transit (PRT) concepts incorporate photovoltaic panels.\n\nRailway presents a low rolling resistance option that would be beneficial of planned journeys and stops. PV panels were tested as APUs on Italian rolling stock under EU project.PVTRAIN. Direct feed to a DC grids avoids losses through DC to AC conversion. DC grids are only to be found in electric powered transport: railways, trams and trolleybuses. Conversion of DC from PV panels to grid alternating current (AC) was estimated to cause around 3% of the electricity being wasted . \n\nPVTrain concluded that the most interest for PV in rail transport was on freight cars where on board electrical power would allow new functionality:\n\n\nThe Kismaros – Királyrét narrow-gauge line near Budapest has built a solar powered railcar called 'Vili'. With a maximum speed of 25 km/h, 'Vili' is driven by two 7 kW motors capable of regenerative braking and powered by 9.9m2 of PV panels. Electricity is stored in on-board batteries. In addition to on-board solar panels, there is the possibility to use stationary (off-board) panels to generate electricity specifically for use in transport.\n\nA few pilot projects have also been built in the framework of the \"Heliotram\" project, such as the tram depots in Hannover Leinhausen and Geneva (Bachet de Pesay). The 150 kW Geneva site injected 600V DC directly into the tram/trolleybus electricity network provided about 1% of the electricity used by the Geneva transport network at its opening in 1999. On December 16, 2017 a fully solar-powered train was launched in New South Wales, Australia. The train is powered using onboard solar panels and onboard rechargeable batteries. It holds a capacity for 100 seated passenger for a 3 km journey.\n\nRecently Imperial College London and the environmental charity have announced the Renewable Traction Power project to investigate using track-side solar panels to power trains . Meanwhile, Indian railways announced their intention to use on board PV to run air conditioning systems in railway coaches.. Also, Indian Railways announced it is to conduct a trial run by the end of May 2016. It hopes that an average of 90,800 liters of diesel per train will be saved on an annual basis, which in turn results in reduction of 239 tones of CO.\n\nSolar powered boats have mainly been limited to rivers and canals, but in 2007 an experimental 14m catamaran, the Sun21 sailed the Atlantic from Seville to Miami, and from there to New York. It was the first crossing of the Atlantic powered only by solar.\nJapan's biggest shipping line Nippon Yusen KK and Nippon Oil Corporation said solar panels capable of generating 40 kilowatts of electricity would be placed on top of a 60,213 ton car carrier ship to be used by Toyota Motor Corporation.\n\nIn 2010, the Tûranor PlanetSolar, a 30 metre long, 15.2 metre wide catamaran yacht powered by 470 square metres of solar panels, was unveiled. It is, so far, the largest solar-powered boat ever built. In 2012, PlanetSolar became the first ever solar electric vehicle to circumnavigate the globe.\n\nVarious demonstration systems have been made. Curiously, none yet takes advantage of the huge power gain that water cooling would bring.\n\nThe low power density of current solar panels limits the use of solar propelled vessels, however boats that use sails (which do not generate electricity unlike combustion engines) rely on battery power for electrical appliances (such as refrigeration, lighting and communications). Here solar panels have become popular for recharging batteries as they do not create noise, require fuel and often can be seamlessly added to existing deck space.\n\nSolar ships can refer to solar powered airships or hybrid airships.\n\nThere is considerable military interest in unmanned aerial vehicles (UAVs); solar power would enable these to stay aloft for months, becoming a much cheaper means of doing some tasks done today by satellites. In September 2007, the first successful flight for 48h under constant power of a UAV was reported.\nThis is likely to be the first commercial use for photovoltaics in flight.\n\nMany demonstration solar aircraft have been built, some of the best known by AeroVironment.\n\n\n\nSolar energy is often used to supply power for satellites and spacecraft operating in the inner solar system since it can supply energy for a long time without excess fuel mass. A Communications satellite contains multiple radio transmitters which operate continually during its life. It would be uneconomic to operate such a vehicle (which may be on-orbit for years) from primary batteries or fuel cells, and refuelling in orbit is not practical. Solar power is not generally used to adjust the satellite's position, however, and the useful life of a communications satellite will be limited by the on-board station-keeping fuel supply.\n\nA few spacecraft operating within the orbit of Mars have used solar power as an energy source for their propulsion system.\n\nAll current solar powered spacecraft use solar panels in conjunction with electric propulsion, typically ion drives as this gives a very high exhaust velocity, and reduces the propellant over that of a rocket by more than a factor of ten. Since propellant is usually the biggest mass on many spacecraft, this reduces launch costs.\n\nOther proposals for solar spacecraft include solar thermal heating of propellant, typically hydrogen or sometimes water is proposed. An electrodynamic tether can be used to change a satellite's orientation or adjust its orbit.\n\nAnother concept for solar propulsion in space is the light sail; this doesn't require conversion of light to electrical energy, instead relying directly on the tiny but persistent radiation pressure of light.\n\nPerhaps the most successful solar-propelled vehicles have been the \"rovers\" used to explore surfaces of the Moon and Mars. The 1977 Lunokhod programme and the 1997 Mars Pathfinder used solar power to propel remote controlled vehicles. The operating life of these rovers far exceeded the limits of endurance that would have been imposed, had they been operated on conventional fuels.\n\nA Swiss project, called \"Solartaxi\", has circumnavigated the world. This is the first time in history an electric vehicle (not self sufficient solar vehicle) has gone around the world, covering 50000 km in 18 months and crossing 40 countries. It is a road-worthy electric vehicle hauling a trailer with solar panels, carrying a 6 m² sized solar array. The Solartaxi has Zebra batteries, which permit a range of 400 km without recharging. The car can also run for 200 km without the trailer. Its maximum speed is 90 km/h. The car weighs 500 kg and the trailer weighs 200 kg. According to initiator and tour director Louis Palmer, the car in mass production could be produced for 16000 Euro. Solartaxi has toured the World from July 2007 till December 2008 to show that solutions to stop global warming are available and to encourage people in pursuing alternatives to fossil fuel. Palmer suggests the most economical location for solar panels for an electric car is on building rooftops though, likening it to putting money into a bank in one location and withdrawing it in another.\nSolar Electrical Vehicles is adding convex solar cells to the roof of hybrid electric vehicles.\n\nAn interesting variant of the electric vehicle is the triple hybrid vehicle—the PHEV that has solar panels as well to assist.\n\nThe 2010 Toyota Prius model has an option to mount solar panels on the roof. They power a ventilation system while parked to help provide cooling. There are many applications of photovoltaics in transport either for motive power or as auxiliary power units, particularly where fuel, maintenance, emissions or noise requirements preclude internal combustion engines or fuel cells. Due to the limited area available on each vehicle either speed or range or both are limited when used for motive power.\n\nThere are limits to using photovoltaic (PV) cells for vehicles:\n\n"}
{"id": "3200600", "url": "https://en.wikipedia.org/wiki?curid=3200600", "title": "Southeastern Cave Conservancy Inc.", "text": "Southeastern Cave Conservancy Inc.\n\nThe Southeastern Cave Conservancy (SCCi) is a United States not-for-profit corporation dedicated to cave conservation, caver education, and cave management. It was formed in 1991 by a group of southeastern United States cavers. The SCCi is an institutional member of the National Speleological Society.\n\nAccording to its Articles of Incorporation, the organization's purpose is \"to acquire and manage caves for scientific study, education of those persons interested in speleology, and conservation of these resources\".\n\nThe organization owns or leases of land in six states, 63 caves, 27 cave preserves, and over $1.5 million in land assets. The SCCi is particularly interested in caves that are threatened with closure or destruction or those that provide a habitat for endangered species such as the gray bat, Tennessee cave salamander, and Hart's-tongue fern. \n\nCaves and preserves owned or leased by the organization are listed below.\n\n\n\n\n\n\n\n"}
{"id": "30650835", "url": "https://en.wikipedia.org/wiki?curid=30650835", "title": "Sun Come Up (film)", "text": "Sun Come Up (film)\n\nSun Come Up is a 2010 documentary film on the effect of global warming on the Carteret Islands. The film showed at the 2010 Full Frame Documentary Film Festival on April 8. It was named as a nominee for the Academy Award for Best Documentary (Short Subject) at the 83rd Academy Awards on January 25, 2011 but lost to Strangers No More.\n\n"}
{"id": "39274704", "url": "https://en.wikipedia.org/wiki?curid=39274704", "title": "Syngas to gasoline plus", "text": "Syngas to gasoline plus\n\nSyngas to gasoline plus (STG+) is a thermochemical process to convert natural gas, other gaseous hydrocarbons or gasified biomass into drop-in fuels, such as gasoline, diesel fuel or jet fuel, and organic solvents.\n\nThis process follows four principal steps in one continuous integrated loop, comprising four fixed bed reactors in a series in which a syngas is converted to synthetic fuels. The steps for producing high-octane synthetic gasoline are as follows:\n\n\nThe STG+ process uses standard catalysts similar to those used in other gas to liquids technologies, specifically in methanol to gasoline processes. Methanol to gasoline processes favor molecular size- and shape-selective zeolite catalysts, and the STG+ process also utilizes commercially available shape-selective catalysts, such as ZSM-5.\n\nAccording to Primus Green Energy, the STG+ process converts approximately one MMBtu of natural gas into more than five gallons of 90+-octane gasoline. One gallon of gasoline contains 120000 to 125000 BTU, making this process about 60% efficient (40% loss of energy).\n\nAs is the case with other gas to liquids processes, STG+ utilizes syngas produced via other technologies as a feedstock. This syngas can be produced through several commercially available technologies and from a wide variety of feedstocks, including natural gas, biomass and municipal solid waste.\n\nNatural gas and other methane-rich gases, including those produced from municipal waste, are converted into syngas through methane reforming technologies such as steam methane reforming and auto-thermal reforming.\n\nBiomass gasification technologies are less established, though several systems being developed utilize fixed bed or fluidized bed reactors.\n\nOther technologies for syngas to liquid fuels synthesis include the Fischer-Tropsch process and the methanol to gasoline processes.\n\nResearch conducted at Princeton University indicates that methanol to gasoline processes are consistently more cost-effective, both in capital cost and overall cost, than the Fischer-Tropsch process at small, medium and large scales. Preliminary studies suggest that the STG+ process is more energetically efficient and the highest yielding methanol to gasoline process.\n\nThe primary difference between the Fischer-Tropsch process and methanol to gasoline processes such as STG+ are the catalysts used, product types and economics.\n\nGenerally, the Fischer-Tropsch process favors unselective cobalt and iron catalysts, while methanol to gasoline technologies favor molecular size- and shape-selective zeolites. In terms of product types, Fischer-Tropsch production has been limited to linear paraffins, such as synthetic crude oil, whereas methanol to gasoline processes can produce aromatics, such as xylene and toluene, and naphthenes and iso-paraffins, such as drop-in gasoline and jet fuel.\n\nThe main product of the Fischer-Tropsch process, synthetic crude oil, requires additional refining to produce fuel products such as diesel fuel or gasoline. This refining typically adds additional costs, causing some industry leaders to label the economics of commercial-scale Fischer-Tropsch processes as challenging.\n\nThe STG+ technology offers several differentiators that distinguish it from other methanol to gasoline processes. These differences include product flexibility, durene reduction, environmental footprint and capital cost.\n\nTraditional methanol to gasoline technologies produce diesel, gasoline or liquefied petroleum gas. STG+ produces gasoline, diesel, jet fuel and aromatics, depending on the catalysts used. The STG+ technology also incorporates durene reduction into its core process, meaning that the entire fuel production process requires only two steps: syngas production and gas to liquids synthesis. Other methanol to gasoline processes do not incorporate durene reduction into the core process, and they require the implementation of an additional refining step.\n\nDue to the additional number of reactors, traditional methanol to gasoline processes include inefficiencies such as the additional cost and energy loss of condensing and evaporating the methanol prior to feeding it to the durene reduction unit. These inefficiencies can lead to a greater capital cost and environmental footprint than methanol to gasoline processes that use fewer reactors, such as STG+. The STG+ process eliminates multiple condensation and evaporation, and the process converts syngas to liquid transportation fuels directly without producing intermediate liquids. This eliminates the need for storage of two products, including pressure storage for liquefied petroleum gas and storage of liquid methanol.\n\nSimplifying a gas to liquids process by combining multiple steps into fewer reactors leads to increased yield and efficiency, enabling less expensive facilities that are more easily scaled.\n\nThe STG+ technology is currently operating at pre-commercial scale in Hillsborough, New Jersey at a plant owned by alternative fuels company Primus Green Energy. The plant produces approximately 100,000 gallons of high-quality, drop-in gasoline per year directly from natural gas. Further, the company announced the findings of an independent engineer’s report prepared by E3 Consulting, which found that STG+ system and catalyst performance exceeded expectations during plant operation. The pre-commercial demonstration plant has also achieved 720 hours of continuous operation.\n\nPrimus Green Energy has announced plans to break ground on its first commercial STG+ plant in the second half of 2014, and the company has announced that this plant is expected to produce approximately 27.8 million gallons of fuel annually.\n\nIn early 2014, the U.S. Patent and Trademark Office (USPTO) allowed Primus Green Energy’s patent covering its single-loop STG+ technology.\n\n"}
{"id": "4406486", "url": "https://en.wikipedia.org/wiki?curid=4406486", "title": "Techron", "text": "Techron\n\nTechron is a patented fuel additive developed by the Chevron Corporation, usually consisting of gasoline mixed with 400 ppm of polyetheramine (PEA). With the introduction of Techron, Chevron gasolines became designated as meeting Top Tier standards for fuel cleanliness. Chevron gasolines with Techron were some of the first gasolines to be named as a \"Top Tier Detergent Gasoline\". \"Top Tier Detergent Gasoline(s)\" are agreed to lead to better performance in engines by BMW, General Motors, Honda, Toyota, Volkswagen, and Audi.\n\nTechron had been first patented and introduced as a concentrate available at Chevron stations in 1981. Techron was then marketed in all Chevron gasolines in May 1995, and the Chevron Cars made their debut to promote this introduction in advertising.\n\nTechron is available at Chevron, Texaco, and Caltex stations in all three grades of gasoline. Techron is also available as a packaged concentrate, for use with gasoline that does not include Techron.\n\nTechron consists of five components:\n\nTechroline was the predecessor to Techron. The company claimed it could control combustion-chamber deposits in cars, as well as keep their fuel-intake systems clean.\n\n\n"}
{"id": "9704012", "url": "https://en.wikipedia.org/wiki?curid=9704012", "title": "Tropical cyclone basins", "text": "Tropical cyclone basins\n\nTraditionally, areas of tropical cyclone formation are divided into seven basins. These include the north Atlantic Ocean, the eastern and western parts of the northern Pacific Ocean, the southwestern Pacific, the southwestern and southeastern Indian Oceans, and the northern Indian Ocean (Arabian Sea and Bay of Bengal). The western Pacific is the most active and the north Indian the least active. An average of 86 tropical cyclones of tropical storm intensity form annually worldwide, with 47 reaching hurricane/typhoon strength, and 20 becoming intense tropical cyclones, super typhoons, or major hurricanes (at least of Category 3 intensity).\n\nThis region includes the North Atlantic Ocean, the Caribbean Sea, and the Gulf of Mexico. Tropical cyclone formation here varies widely from year to year, ranging from one to over twenty-five per year. Most Atlantic tropical storms and hurricanes form between June 1 and November 30. The United States National Hurricane Center (NHC) monitors the basin and issues reports, watches and warnings about tropical weather systems for the Atlantic Basin as one of the Regional Specialized Meteorological Centres for tropical cyclones as defined by the World Meteorological Organization. On average, 11 named storms (of tropical storm or higher strength) occur each season, with an average of 6 becoming hurricanes and 2 becoming major hurricanes. The climatological peak of activity is around September 10 each season.\n\nThe United States Atlantic coast and Gulf Coast, Mexico, Central America, the Caribbean Islands, and Bermuda are frequently affected by storms in this basin. Venezuela, the 4 provinces of Atlantic Canada, and Atlantic Macaronesian islands also are occasionally affected. Many of the more intense Atlantic storms are Cape Verde-type hurricanes, which form off the west coast of Africa near the Cape Verde islands. Occasionally, a hurricane that evolves into an extratropical cyclone can reach western Europe, including Hurricane Gordon, which spread high winds across Spain and the British Isles in September 2006. Hurricane Vince, which made landfall on the southwestern coast of Spain as a tropical depression in October 2005, is the only known system to impact mainland Europe as a tropical cyclone in the NHC study period commencing in 1851 (it is believed a hurricane made landfall in Spain in 1842).\n\nThe Northeastern Pacific is the second most active basin and has the highest number of storms per unit area. The hurricane season runs between May 15 and November 30 each year, and encompasses the vast majority of tropical cyclone activity in the region. In the 1971–2005 period, there were an average of 15–16 tropical storms, 9 hurricanes, and 4–5 major hurricanes (storms of Category 3 intensity or greater) annually in the basin.\n\nStorms that form here often affect western Mexico, and less commonly the Continental United States (in particular California), or northern Central America. No hurricane included in the modern database has made landfall in California; however, historical records from 1858 speak of a storm that brought San Diego winds over (marginal hurricane force), though it is not known if the storm actually made landfall. Tropical storms in 1939, 1976 and 1997 brought gale-force winds to California.\n\nThe Central Pacific Hurricane Center's area of responsibility (AOR) begins at the boundary with the National Hurricane Center' AOR (at 140 °W), and ends at the International Date Line, where the Northwestern Pacific begins. The hurricane season in the North Central Pacific runs annually from June 1 to November 30; The Central Pacific Hurricane Center monitors the storms that develop or move into the defined area of responsibility. The CPHC previously tasked with monitoring tropical activity in the basin was originally known as the Joint Hurricane Warning Center; today it is called the Joint Typhoon Warning Center.\n\nCentral Pacific hurricanes are rare and on average 4 to 5 storms form or move in this area annually. As there are no large contiguous landmasses in the basin, direct hits and landfalls are rare; however, they occur occasionally, as with Hurricane Iniki in 1992, which made landfall on Hawaii, and Hurricane Ioke in 2006, which made a direct hit on Johnston Atoll.\n\nThe Northwest Pacific Ocean is the most active basin on the planet, accounting for one-third of all tropical cyclone activity. Annually, an average of 25.7 tropical cyclones in the basin acquire tropical storm strength or greater; also, an average of 16 typhoons occurred each year during the 1968–1989 period. The basin occupies all the territory north of the equator and west of the International Date Line, including the South China Sea. The basin sees activity year-round; however, tropical activity is at its minimum in February and March.\n\nTropical storms in this region often affect China, Hong Kong, Macau, Japan, Korea, Philippines, Taiwan and Vietnam, plus numerous Oceanian islands such as Guam, the Northern Marianas and Palau. Sometimes, tropical storms in this region affect Laos, Thailand, Cambodia and even Singapore, Malaysia and Indonesia. The coast of China sees the most landfalling tropical cyclones worldwide. The Philippines receives an average of 6–7 tropical cyclone landfalls per year, with Super Typhoon Haiyan being the strongest and most powerful to date since its landfall in November 8, 2013.\n\nThis basin is divided into two areas: the Bay of Bengal and the Arabian Sea, with the Bay of Bengal dominating (5 to 6 times more activity). Still, this basin is the least active worldwide, with only 4 to 6 storms per year.\n\nThis basin’s season has a double peak: one in April and May, before the onset of the monsoon, and another in October and November, just after. This double peak occurs because powerful vertical wind shear in between the surface monsoonal low and upper tropospheric high during the monsoon season tears apart incipient cyclones. High shear explains why no cyclones can form in the Red Sea, which possesses the necessary depth, vorticity and surface temperatures year-round. Rarely do tropical cyclones that form elsewhere in this basin affect the Arabian Peninsula or Somalia; however, Cyclone Gonu caused heavy damage in Oman on the peninsula in 2007.\n\nAlthough the North Indian Ocean is a relatively inactive basin, extremely high population densities in the Ganges and Ayeyarwady Deltas mean that the deadliest tropical cyclones in the world have formed here, including the 1970 Bhola cyclone, which killed 500,000 people. Nations affected include India, Bangladesh, Sri Lanka, Thailand, Myanmar, and Pakistan.\n\nOn rare occasions, tropical-like systems that can reach the intensity of hurricanes, occur over the Mediterranean Sea. Such a phenomenon is called a Medicane (Mediterranean-hurricane). Although the geographical dimensions of tropical oceans and the Mediterranean Sea are clearly different, the precursor mechanisms of these perturbations, based on the air-sea thermodynamic imbalance, are similar. Their origins are typically non-tropical, and develop over open waters under strong, initially cold-core cyclones, similar to subtropical cyclones or anomalous tropical cyclones in the Atlantic Basin, like Karl (1980), Vince (2005), Grace (2009), Chris (2012), or Ophelia (2017). Sea surface temperatures in late-August and early-September are quite high over the basin (24/28 °C or 75/82 °F), though research indicates water temperatures of 20 °C (68 °F) are normally required for development.\n\nMeteorological literature document that such systems occurred in September 1947, September 1969, January 1982, September 1983, January 1995, October 1996, September 2006, November 2011, November 2014, and November 2017. The 1995 system developed a well-defined eye, and a ship recorded 85 mph (140 km/h) winds, along with an atmospheric pressure of 975 mbar. Although it had the structure of a tropical cyclone, it occurred over water temperatures, suggesting it could have been a polar low.\n\nWithin the Southern Hemisphere tropical cyclones generally form on a regular basis between the African coast and the middle of the South Pacific. Tropical and Subtropical Cyclones have also been noted occurring in the Southern Atlantic Ocean at times. For various reasons including where tropical cyclones form, there are several different ways to split the area between the American and African coasts. For instance the World Meteorological Organization define three different basins for the tracking and warning of tropical cyclones. These are the South-West Indian Ocean between the African Coast and 90°E, the Australian region between 90°E and 160°E and the South Pacific between 160°E and 120°W. The United States Joint Typhoon Warning Center also monitors the whole region, but splits it at 135°E into the South Pacific and the Southern Indian Ocean.\n\nThe South-West Indian Ocean is located within the Southern Hemisphere between the Africa's east coast and 90°E and is primarily monitored by the Meteo France's La Reunion RSMC, while the Mauritian, Australian Indonesian, and Malagasy weather services also monitor parts of it. Until the start of the 1985–86 tropical cyclone season the basin only extended to 80°E, with the 10 degrees between 80 and 90E considered to be a part of the Australian region. On average about 9 cyclones per year develop into tropical storms, while 5 of those go on to become tropical cyclones that are equivalent to a hurricane or a typhoon. The tropical cyclones that form in this area can affect some of the various Indian Ocean island nations and or various countries along Africa's east coast.\n\nThrough the middle of 1985, this basin extended westward to 80E. Since then, its western boundary has been 90E. Tropical activity in this region affects Australia and Indonesia. According to the Australian Bureau of Meteorology, the most frequently hit portion of Australia is between Exmouth and Broome in Western Australia. The basin sees an average of about seven cyclones each year, although more can form or come in from other basins, such as the South Pacific. The tropical cyclone Cyclone Vance in 1999 produced the highest recorded speed winds in an Australian town or city at around .\n\nThe South Pacific Ocean basin runs between 160°E and 120°W, with tropical cyclones developing in it officially monitored by the Fiji Meteorological Service and New Zealand's MetService. Tropical Cyclones that develop within this basin generally affect countries to the west of the dateline, though during El Niños cyclones have been known to develop to the east of the dateline near French Polynesia. On average the basin sees nine tropical cyclones annually with about half of them becoming severe tropical cyclones.\n\nCyclones rarely form in other tropical ocean areas, which are not formally considered tropical cyclone basins. Tropical depressions and tropical storms occur occasionally in the South Atlantic, and the only full-blown tropical cyclones on record were 2004's Hurricane Catarina, which made landfall in Brazil and 2010's Tropical Storm Anita, which formed off the coast of Rio Grande do Sul. The South Atlantic Ocean is not officially classified as a tropical cyclone basin by the World Meteorological Organization and does not have a designated regional specialized meteorological center (RSMC). However, the Brazilian Navy Hydrographic Center has started in 2011 to assign names to tropical and subtropical systems in this basin, when they have sustained wind speeds of at least .\n\n"}
{"id": "952340", "url": "https://en.wikipedia.org/wiki?curid=952340", "title": "Tumbrel", "text": "Tumbrel\n\nA tumbrel (alternatively tumbril) is a two-wheeled cart or wagon typically designed to be hauled by a single horse or ox. Their original use was for agricultural work; in particular they were associated with carrying manure. Their most notable use was taking prisoners to the guillotine during the French Revolution. They were also used by the military for hauling supplies. In this use the carts were sometimes covered. The two wheels allowed the cart to be tilted to discharge its load more easily. The word is also used as a name for the cucking-stool and for a type of balancing scale used in medieval times to check the weight of coins.\n"}
{"id": "33791207", "url": "https://en.wikipedia.org/wiki?curid=33791207", "title": "Ultralight material", "text": "Ultralight material\n\nUltralight materials are solids with a density of less than 10 mg/cm, including silica aerogels, carbon nanotube aerogels, aerographite, metallic foams, polymeric foams, and metallic microlattices. The density of air is about 1.275 mg/cm, which means that the air in the pores contributes significantly to the density of these materials in atmospheric conditions. They can be classified by production method as aerogels, stochastic foams, and structured cellular materials.\n\nThe first ultralight materials, aerogel was first created by Samuel Stephens Kistler in 1931.\n\nGraphene foams and graphite foams are examples of stochastic foams.\n\nStructured cellular materials can be remarkably strong despite very low density. \n\nReversibly assembled cellular composite materials enable tailorable composite materials properties, to the ideal linear specific stiffness scaling regime. Using projection microstereolithography, octet microlattices have also been fabricated from polymers, metals, and ceramics. \n\nThe design of the high performing lattices mean that the individual struts making up the materials do not bend. The materials are therefore exceptionally stiff and strong for their weight.\n"}
{"id": "325009", "url": "https://en.wikipedia.org/wiki?curid=325009", "title": "Valve amplifier", "text": "Valve amplifier\n\nA valve amplifier or tube amplifier is a type of electronic amplifier that uses vacuum tubes to increase the amplitude or power of a signal. Low to medium power valve amplifiers for frequencies below the microwaves were largely replaced by solid state amplifiers during the 1960s and 1970s. Valve amplifiers are used for applications such as guitar amplifiers, satellite transponders such as DirecTV and GPS, audiophile stereo amplifiers, military applications (such as radar) and very high power radio and UHF television transmitters.\n\nUntil the invention of the transistor in 1947, most practical high-frequency electronic amplifiers were made using thermionic valves. The simplest valve was invented by John Ambrose Fleming while working for the Marconi Company in London in 1904 and named the diode, as it had two electrodes. The diode conducted electricity in one direction only and was used as a radio detector and a rectifier.\n\nIn 1906 Lee De Forest added a third electrode and invented the first electronic amplifying device, the triode, which he named the \"Audion\". This additional \"control grid\" modulates the current that flows between cathode and anode. The relationship between current flow and plate and grid voltage is often represented as a series of \"characteristic curves\" on a diagram. Depending on the other components in the circuit this modulated current flow can be used to provide current or voltage gain.\n\nThe first application of valve amplification was in the regeneration of long distance telephony signals. Later, valve amplification was applied to the 'wireless' market that began in the early thirties. In due course amplifiers for music and later television were also built using valves.\nThe overwhelmingly dominant circuit topology during this period was the single-ended triode gain stage, operating in class A, which gave very good sound (and reasonable measured distortion performance) despite extremely simple circuitry with very few components: important at a time when components were handmade and extremely expensive. Before World War II, almost all valve amplifiers were of low gain and with linearity dependent entirely on the inherent linearity of the valve itself, typically 5% distortion at full power.\n\nNegative feedback (NFB) was invented by Harold Stephen Black in 1927, but initially little used since at that time gain was at a premium. This technique allows amplifiers to trade gain for reduced distortion levels (and also gave other benefits such as reduced output impedance). The introduction of the Williamson amplifier in 1947, which was extremely advanced in many respects including very successful use of NFB, was a turning point in audio power amplifier design, operating a push-pull output circuit in class AB1 to give performance surpassing its contemporaries.\n\nWorld War II stimulated dramatic technical progress and industrial scale production economies. Increasing affluence after the war led to a substantial and expanding consumer market. This enabled electronics manufacturers to build and market more advanced valve (tube) designs at affordable prices, with the result that the 1960s saw the increasing spread of electronic gramophone players, and ultimately the beginnings of \"high fidelity\". Hifi was able to drive full frequency range loudspeakers (for the first time, often with multiple drivers for different frequency bands) to significant volume levels. This, combined with the spread of TV, produced a 'golden age' in valve (tube) development and also in the development of the design of valve amplifier circuits.\n\nA range of topologies with only minor variations (notably different phase splitter arrangements and the \"Ultra-Linear\" transformer connection for tetrodes) rapidly became widespread. This family of designs remains the dominant high power amplifier topology to this day for music application. This period also saw continued growth in civilian radio, with valves being used for both transmitters and receivers.\n\nFrom the 1970s the silicon transistor became increasingly pervasive. Valve production was sharply decreased, with the notable exception of cathode ray tubes (CRTs), and a reduced range of valves for amplifier applications. Popular low power tubes were dual triodes (ECCnn, 12Ax7 series) plus the EF86 pentode, and power valves were mostly being beam tetrode and pentodes (EL84, EL34, KT88 / 6550, 6L6), in both cases with indirect heating. This reduced set of types remains the core of valve production today.\n\nThe Soviets retained valves to a much greater extent than the West during the Cold War, for the majority of their communications and military amplification requirements, in part due to valves' ability to withstand instantaneous overloads (notably due to a nuclear detonation) that would destroy a transistor.\n\nThe dramatic reduction in size, power consumption, reduced distortion levels and above all cost of electronics products based on transistors has made valves obsolete for mainstream products since the 1970s. Valves remained in certain applications such as high power RF transmitters and the microwave oven, and audio amplification equipment, particularly for the electric guitar, recording studios, and high-end home stereos.\n\nIn audio applications, valves continue to be highly desired by most professional users, particularly in recording studios' equipment and guitar amplifiers. Among stereo enthusiasts, there is a subgroup of audio buffs who advocate the use of tube amplifiers for home listening; they argue that tube amplifiers produce a \"warmer\" or more \"natural\" valve sound. Companies in Asia and Eastern Europe continue to produce valves to cater to this market.\n\nMany professional guitar players use 'tube amps' because of their renowned 'tone' . 'Tone' in this usage is referring to timbre, or pitch color, and can be a very subjective quality to quantify. Most audio technicians and scientists theorize that the 'even harmonic distortion' produced by valve tubes sounds more pleasing to the ear than transistors, regardless of style. Many of the musicians who use solid state amplification technology do so for its portability, low cost and high reliability, not its 'tone'. It is the tonal characteristics of valve tubes that have sustained them as the industry standard for guitars and studio microphone pre-amplification.\n\nTube amplifiers respond differently from transistor amplifiers when signal levels approach and reach the point of clipping. In a tube amplifier, the transition from linear amplification to limiting is less abrupt than in a solid state unit, resulting in a less grating form of distortion at the onset of clipping. For this reason, some guitarists prefer the sound of an all-tube amplifier; the aesthetic properties of tube versus solid state amps, though, are a topic of debate in the guitarist community.\n\nPower valves typically operate at higher voltages and lower currents than transistors - although solid state operating voltages have steadily increased with modern device technologies. High RF power radio transmitters in use today operate in the kilovolt range, where there is still no other comparable technology available. ([power = volts * amps], so high power requires: high volts, high amps, or both)\n\nMany power valves have good linearity but modest gain or transconductance. Signal amplifiers using tubes are capable of very high frequency response ranges – up to radio frequency. Indeed, many of the directly heated single-ended triode (DH-SET) audio amplifiers are in fact radio transmitting tubes designed to operate in the megahertz range. In practice, however, tube amplifier designs typically \"couple\" stages either capacitively, limiting bandwidth at the low end, or inductively with transformers, limiting the bandwidth at both ends.\n\n\n\nAll amplifier circuits are classified by \"class of operation\" as A, B, AB and C etc. See power amplifier classes. Some significantly different circuit topologies exist compared to transistor designs.\n\n\n\nThe high output impedance of tube plate circuits is not well matched to low-impedance loads such as loudspeakers or antennas. A matching network is required for efficient power transfer; this may be a transformer at audio frequencies, or various tuned networks at radio frequencies.\n\nIn a \"cathode follower\" or \"common-plate\" configuration, the output is taken from the cathode resistance. Because of negative feedback (the cathode-ground voltage cancels the grid-ground voltage) the voltage gain is close to unity and the output voltage \"follows\" the grid voltage. Although the cathode resistor can be many kilohms (depending on biasing requirements), the small-signal output impedance is very low (see operational amplifier).\n\nValves remain in widespread use in guitar and high-end audio amplifiers due to the perceived sound quality they produce. They are largely obsolete elsewhere because of higher power consumption, distortion, costs, reliability, and weight in comparison to transistors.\n\nTelephony was the original, and for many years was a driving application for audio amplification. A specific issue for the telecommunication industry was the technique of multiplexing many (up to a thousand) voice lines onto a single cable, at different frequencies.\n\nThe advantage of this is that a single valve \"repeater\" amplifier can amplify many calls at once, this being very cost effective. The problem is that the amplifiers need to be extremely linear, otherwise \"intermodulation distortion\" (IMD) will result in \"crosstalk\" between the multiplexed channels. This stimulated development emphasis towards low distortion far beyond the nominal needs of a single voice channel.\n\nToday, the main application for valves is audio amplifiers for high-end hi-fi and musical performance use with electric guitars, electric basses, and Hammond organs, although these applications have different requirements regarding distortion which result in different design compromises, although the same basic design techniques are generic and widely applicable to all broadband amplification applications, not only audio.\n\nPost World War II, the majority of valve power amplifiers are of the Class AB-1 \"push pull\" ultralinear topology, or lower cost single ended i.e. 6BQ5/EL84 power tubes, but niche products using the DH-SET and even OTL topologies still exist in small numbers.\n\nThe basic moving coil voltmeter and ammeter itself takes a small current and thus loads the circuit to which it is attached. This can significantly alter the operating conditions in the circuit being measured. The vacuum tube voltmeter (VTVM) uses the high input impedance of a valve to buffer the circuit being measured from the load of the ammeter.\n\nValve oscilloscopes share this very high input impedance and thus can be used to measure voltages even in very high impedance circuits. There may typically be 3 or 4 stages of amplification per display channel. In later oscilloscopes, a type of amplifier using a series of tubes connected at equal distances along transmission lines, known as a distributed amplifier was employed to amplify very high frequency vertical signals before application to the display tube. Valve oscilloscopes are now obsolete.\n\nIn the closing years of the valve era, valves were even used to make \"operational amplifiers\" – the building blocks of much modern linear electronics. An op-amp typically has a differential input stage and a totem pole output, the circuit usually having a minimum of five active devices. A number of \"packages\" were produced that integrated such circuits (typically using two or more glass envelopes) into a single module that could be plugged into a larger circuit (such as an analog computer). Such valve op-amps were very far from ideal and quickly became obsolete, being replaced with solid-state types.\n\nHistorically, pre-WWII \"transmitting tubes\" were among the most powerful tubes available. These usually had directly heated thoriated filament cathodes that glowed like light bulbs. Some tubes were capable of being driven so hard that the anode itself would glow cherry red; the anodes were machined from solid material (rather than fabricated from thin sheet) to withstand heat without distorting. Notable tubes of this type are the 845 and 211. Later tetrodes and pentodes such as 817 and (direct heated) 813 were also used in large numbers in (especially military) radio transmitters\n\nRF circuits are significantly different from broadband amplifier circuits. The antenna or following circuit stage typically contains one or more adjustable capacitive or inductive component allowing the resonance of the stage to be accurately matched with carrier frequency in use, to optimize power transfer from and loading on the valve, a so-called \"tuned circuit\".\n\nBroadband circuits require flat response over a wide range of frequencies. RF circuits by contrast are typically required to operate at high frequencies but often over a very narrow frequency range. For example, an RF device might be required to operate over the range 144 to 146 MHz (just 1.4%)\n\nToday, radio transmitters are overwhelmingly silicon based, even at microwave frequencies. However, an ever-decreasing minority of high power radio frequency amplifiers continue to have valve construction.\n\n\n\n"}
{"id": "1864331", "url": "https://en.wikipedia.org/wiki?curid=1864331", "title": "Water content", "text": "Water content\n\nWater content or moisture content is the quantity of water contained in a material, such as soil (called soil moisture), rock, ceramics, crops, or wood. Water content is used in a wide range of scientific and technical areas, and is expressed as a ratio, which can range from 0 (completely dry) to the value of the materials' porosity at saturation. It can be given on a volumetric or mass (gravimetric) basis.\n\nVolumetric water content, θ, is defined mathematically as:\nwhere formula_2 is the volume of water and formula_3 is equal to the total volume of the wet material, i.e. of the sum of the volume of solid host material (e.g., soil particles, vegetation tissue) formula_4, of water formula_2, and of air formula_6.\n\nGravimetric water content is expressed by mass (weight) as follows:\nwhere formula_8 is the mass of water and formula_9 is the mass of the substance. Normally the latter is taken before drying: \nexcept for woodworking, geotechnical and soil science applications where oven-dried material is used instead:\n\nTo convert gravimetric water content to volumetric water content, multiply the gravimetric water content by the bulk specific gravity formula_12 of the material:\n\nIn soil mechanics and petroleum engineering the water saturation or degree of saturation, formula_14, is defined as\nwhere formula_16 is the porosity, in terms of the volume of void or pore space formula_17 and the total volume of the substance formula_18. Values of \"S\" can range from 0 (dry) to 1 (saturated). In reality, \"S\" never reaches 0 or 1 - these are idealizations for engineering use.\n\nThe normalized water content, formula_19, (also called effective saturation or formula_20) is a dimensionless value defined by van Genuchten as:\nwhere formula_22 is the volumetric water content; formula_23 is the residual water content, defined as the water content for which the gradient formula_24 becomes zero; and, formula_25 is the saturated water content, which is equivalent to porosity, formula_26.\n\nWater content can be directly measured using a known volume of the material, and a drying oven. Volumetric water content, θ, is calculated via the volume of water formula_2 and the mass of water formula_8:\nwhere\n\nFor materials that change in volume with water content, such as coal, the water content, \"u\", is expressed in terms of the mass of water per unit mass of the moist specimen: \n\nHowever, geotechnics requires the moisture content to be expressed with respect to the sample's dry weight (often as a percentage, i.e. % moisture content = \"u\"×100%)\n\nFor wood, the convention is to report moisture content on oven-dry basis (i.e. generally drying sample in an oven set at 105 deg Celsius for 24 hours). In wood drying, this is an important concept.\n\nOther methods that determine water content of a sample include chemical titrations (for example the Karl Fischer titration), determining mass loss on heating (perhaps in the presence of an inert gas), or after freeze drying. In the food industry the Dean-Stark method is also commonly used.\n\nFrom the Annual Book of ASTM (American Society for Testing and Materials) Standards, the total evaporable moisture content in Aggregate (C 566) can be calculated with the formula: \nwhere formula_36 is the fraction of total evaporable moisture content of sample, formula_37 is the mass of the original sample, and formula_38 is mass of dried sample.\n\nThere are several geophysical methods available that can approximate \"in situ\" soil water content. These methods include: time-domain reflectometry (TDR), neutron probe, frequency domain sensor, capacitance probe, amplitude domain reflectometry, electrical resistivity tomography, ground penetrating radar (GPR), and others that are sensitive to the physical properties of water . Geophysical sensors are often used to monitor soil moisture continuously in agricultural and scientific applications.\n\nSatellite microwave remote sensing is used to estimate soil moisture based on the large contrast between the dielectric properties of wet and dry soil. The microwave radiation is not sensitive to atmospheric variables, and can penetrate through clouds. Also, microwave signal can penetrate, to a certain extent, the vegetation canopy and retrieve information from ground surface. The data from microwave remote sensing satellites such as WindSat, AMSR-E, RADARSAT, ERS-1-2, Metop/ASCAT, and SMAP are used to estimate surface soil moisture.\n\nMoisture may be present as adsorbed moisture at internal surfaces and as capillary condensed water in small pores. At low relative humidities, moisture consists mainly of adsorbed water. At higher relative humidities, liquid water becomes more and more important, depending or not depending on the pore size can also be an influence of volume. In wood-based materials, however, almost all water is adsorbed at humidities below 98% RH.\n\nIn biological applications there can also be a distinction between physisorbed water and \"free\" water — the physisorbed water being that closely associated with and relatively difficult to remove from a biological material. The method used to determine water content may affect whether water present in this form is accounted for. For a better indication of \"free\" and \"bound\" water, the water activity of a material should be considered.\n\nWater molecules may also be present in materials closely associated with individual molecules, as \"water of crystallization\", or as water molecules which are static components of protein structure.\n\nIn soil science, hydrology and agricultural sciences, water content has an important role for groundwater recharge, agriculture, and soil chemistry. Many recent scientific research efforts have aimed toward a predictive-understanding of water content over space and time. Observations have revealed generally that spatial variance in water content tends to increase as overall wetness increases in semiarid regions, to decrease as overall wetness increases in humid regions, and to peak under intermediate wetness conditions in temperate regions .\n\nThere are four standard water contents that are routinely measured and used, which are described in the following table:\n\nAnd lastly the available water content, θ, which is equivalent to:\nwhich can range between 0.1 in gravel and 0.3 in peat.\n\nWhen a soil becomes too dry, plant transpiration drops because the water is increasingly bound to the soil particles by suction. Below the wilting point plants are no longer able to extract water. At this point they wilt and cease transpiring altogether. Conditions where soil is too dry to maintain reliable plant growth is referred to as agricultural drought, and is a particular focus of irrigation management. Such conditions are common in arid and semi-arid environments.\n\nSome agriculture professionals are beginning to use environmental measurements such as soil moisture to schedule irrigation. This method is referred to as \"smart irrigation\" or \"soil cultivation\".\n\nIn saturated groundwater aquifers, all available pore spaces are filled with water (volumetric water content = porosity). Above a capillary fringe, pore spaces have air in them too.\n\nMost soils have a water content less than porosity, which is the definition of unsaturated conditions, and they make up the subject of vadose zone hydrogeology. The capillary fringe of the water table is the dividing line between saturated and unsaturated conditions. Water content in the capillary fringe decreases with increasing distance above the phreatic surface.\n\nOne of the main complications which arises in studying the vadose zone, is the fact that the unsaturated hydraulic conductivity is a function of the water content of the material. As a material dries out, the connected wet pathways through the media become smaller, the hydraulic conductivity decreasing with lower water content in a very non-linear fashion.\n\nA water retention curve is the relationship between volumetric water content and the water potential of the porous medium. It is characteristic for different types of porous medium. Due to hysteresis, different wetting and drying curves may be distinguished.\n\n"}
{"id": "2656684", "url": "https://en.wikipedia.org/wiki?curid=2656684", "title": "William Cameron Edwards", "text": "William Cameron Edwards\n\nWilliam Cameron Edwards (7 May 1844 – 17 September 1921) was a Canadian businessman and parliamentarian.\n\nHe was born in Clarence Township in Russell County, Ontario, the son of William Edwards and Ann Cameron, received basic schooling in Ottawa at the District Grammar School \n\nIn 1885, he married Catherine Wilson.\n\nA Liberal, he was five times elected as a Member of Parliament representing the Ontario electoral district of Russell. He was first elected in the Canadian federal election of 1887, and was re-elected in 1888, 1891, 1896 and 1900. On 17 March 1903 he was appointed to the Senate of Canada upon the recommendation of Sir Wilfrid Laurier. He represented the senatorial division of Russell County, Ontario until his death in Ottawa.\n\nAt one time, Edwards owned the residence at 24 Sussex Drive, having purchased it from Joseph Merrill Currier in 1902. Ironically his nephew Cameron Macpherson Edwards's home, Harrington Lake, is now the country retreat of the Prime Minister of Canada.\n\nEdwards was also president of Canada Cement Company, was a noted livestock breeder and served as president of the Russell Agricultural Society.\n\nHis nephew Gordon Cameron Edwards also served in the House of Commons of Canada.\n"}
{"id": "262149", "url": "https://en.wikipedia.org/wiki?curid=262149", "title": "William John Macquorn Rankine", "text": "William John Macquorn Rankine\n\nProf William John Macquorn Rankine () LLD (5 July 1820 – 24 December 1872) was a Scottish mechanical engineer who also contributed to civil engineering, physics and mathematics. He was a founding contributor, with Rudolf Clausius and William Thomson (Lord Kelvin), to the science of thermodynamics, particularly focusing on the first of the three thermodynamic laws. He developed the Rankine scale, an equivalent to the Kelvin scale of temperature, but in degrees Fahrenheit rather than centigrade.\n\nRankine developed a complete theory of the steam engine and indeed of all heat engines. His manuals of engineering science and practice were used for many decades after their publication in the 1850s and 1860s. He published several hundred papers and notes on science and engineering topics, from 1840 onwards, and his interests were extremely varied, including, in his youth, botany, music theory and number theory, and, in his mature years, most major branches of science, mathematics and engineering. \n\nHe was an enthusiastic amateur singer, pianist and cellist who composed his own humorous songs.\n\nHe was born in Edinburgh to Lt David Rankin (sic), a civil engineer from a military background, who later worked on the Edinburgh to Dalkeith Railway (locally known as the Innocent Railway). His mother was Barbara Grahame, of a prominent legal and banking family. \n\nHis father moved around Scotland on various projects and the family moved with him. William was initially educated at home but he later attended Ayr Academy (1828–29) and then the High School of Glasgow (1830). Around 1830 the family moved to Edinburgh when the father got a post as Manager of the Edinburgh to Dalkeith Railway. The family then lived at 2 Arniston Place. \n\nIn 1834 he was sent to the Scottish Naval and Military Academy on Lothian Road in Edinburgh with the mathematician George Lee. By that year William was already highly proficient in mathematics and received, as a gift from his uncle, Isaac Newton's \"Principia\" (1687) in the original Latin.\n\nIn 1836, Rankine began to study a spectrum of scientific topics at the University of Edinburgh, including natural history under Robert Jameson and natural philosophy under James David Forbes. Under Forbes he was awarded prizes for essays on methods of physical inquiry and on the undulatory (or wave) theory of light. During vacations, he assisted his father who, from 1830, was manager and, later, effective treasurer and engineer of the Edinburgh and Dalkeith Railway which brought coal into the growing city. He left the University of Edinburgh in 1838 without a degree (which was not then unusual) and, perhaps because of straitened family finances, became an apprentice to Sir John Benjamin Macneill, who was at the time surveyor to the Irish Railway Commission. During his pupilage he developed a technique, later known as Rankine's method, for laying out railway curves, fully exploiting the theodolite and making a substantial improvement in accuracy and productivity over existing methods. In fact, the technique was simultaneously in use by other engineers – and in the 1860s there was a minor dispute about Rankine's priority.\n\nThe year 1842 also marked Rankine's first attempt to reduce the phenomena of heat to a mathematical form but he was frustrated by his lack of experimental data. At the time of Queen Victoria's visit to Scotland, later that year, he organised a large bonfire situated on Arthur's Seat, constructed with radiating air passages under the fuel. The bonfire served as a beacon to initiate a chain of other bonfires across Scotland.\n\nIn 1850 he was elected a Fellow of the Royal Society of Edinburgh his proposer being Prof James David Forbes. He won the Society's Keith Prize for the period 1851–53. He served as the Society's Vice President from 1871 to 1872.\n\nFrom 1855 he was Professor of Civil Engineering and Mechanics at Glasgow University.\n\nHe died at 59 St Vincent Street in central Glasgow on Christmas Eve, 24 December 1872, aged only 52. He was unmarried and had no children.\n\nUndaunted, he returned to his youthful fascination with the mechanics of the heat engine. Though his theory of circulating streams of elastic vortices whose volumes spontaneously adapted to their environment sounds fanciful to scientists formed on a modern account, by 1849, he had succeeded in finding the relationship between saturated vapour pressure and temperature. The following year, he used his theory to establish relationships between the temperature, pressure and density of gases, and expressions for the latent heat of evaporation of a liquid. He accurately predicted the surprising fact that the apparent specific heat of saturated steam would be negative.\n\nEmboldened by his success, in 1851 he set out to calculate the efficiency of heat engines and used his theory as a basis to deduce the principle, that the maximum efficiency possible for any heat engine is a function only of the two temperatures between which it operates. Though a similar result had already been derived by Rudolf Clausius and William Thomson, Rankine claimed that his result rested upon his hypothesis of molecular vortices alone, rather than upon Carnot's theory or some other additional assumption. The work marked the first step on Rankine's journey to develop a more complete theory of heat.\n\nRankine later recast the results of his molecular theories in terms of a macroscopic account of energy and its transformations. He defined and distinguished between \"actual energy\" which was lost in dynamic processes and \"potential energy\" by which it was replaced. He assumed the sum of the two energies to be constant, an idea already, although surely not for very long, familiar in the law of conservation of energy. From 1854, he made wide use of his \"thermodynamic function\" which he later realised was identical to the entropy of Clausius. By 1855, Rankine had formulated a \"science of energetics\" which gave an account of dynamics in terms of energy and its transformations rather than force and motion. The theory was very influential in the 1890s. In 1859 he proposed the Rankine scale of temperature, an absolute or thermodynamic scale whose degree is equal to a Fahrenheit degree.\n\nEnergetics offered Rankine an alternative, and rather more mainstream, approach, to his science and, from the mid-1850s, he made rather less use of his molecular vortices. Yet he still claimed that Maxwell's work on electromagnetics was effectively an extension of his model. And, in 1864, he contended that the microscopic theories of heat proposed by Clausius and James Clerk Maxwell, based on linear atomic motion, were inadequate. It was only in 1869 that Rankine admitted the success of these rival theories. By that time, his own model of the atom had become almost identical with that of Thomson.\n\nAs was his constant aim, especially as a teacher of engineering, he used his own theories to develop a number of practical results and to elucidate their physical principles including:\n\n\nThe history of rotordynamics is replete with the interplay of theory and practice. Rankine first performed an analysis of a spinning shaft in 1869, but his model was not adequate and he predicted that supercritical speeds could not be attained.\n\nRankine was one of the first engineers to recognise that fatigue failures of railway axles was caused by the initiation and growth of brittle cracks. In the early 1840s he examined many broken axles, especially after the Versailles train crash of 1842 when a locomotive axle suddenly fractured and led to the death of over 50 passengers. He showed that the axles had failed by progressive growth of a brittle crack from a shoulder or other stress concentration source on the shaft, such as a keyway. He was supported by similar direct analysis of failed axles by Joseph Glynn, where the axles failed by slow growth of a brittle crack in a process now known as metal fatigue. It was likely that the front axle of one of the locomotives involved in the Versailles train crash failed in a similar way. Rankine presented his conclusions in a paper delivered to the Institution of Civil Engineers. His work was ignored however, by many engineers who persisted in believing that stress could cause \"re-crystallisation\" of the metal, a myth which has persisted even to recent times. The theory of recrystallisation was quite wrong, and inhibited worthwhile research until the work of William Fairbairn a few years later, which showed the weakening effect of repeated flexure on large beams. Nevertheless, fatigue remained a serious and poorly understood phenomenon, and was the root cause of many accidents on the railways and elsewhere. It is still a serious problem, but at least is much better understood today, and so can be prevented by careful design.\n\nHe served as Regius Professor of Civil Engineering and Mechanics at the University of Glasgow from November 1855 until his death in December 1872, pursuing engineering research along a number of lines in civil and mechanical engineering.\n\nRankine was instrumental in the formation of the forerunner of Glasgow University Officer Training Corps, the 2nd Lanarkshire Rifle Volunteer Corps at Glasgow University in July 1859, becoming Major in 1860 after it was formed into the first company of the 2nd Battalion, 1st Lanarkshire Rifle Volunteer Corps; he served until 1864, when he resigned due to pressure of work – much of it associated with Naval Architecture.\n\nThe Rankine Lectures, organised by the British Geotechnical Association, are named in recognition of the significant contributions he made to:\n\nRankine worked closely with Clyde shipbuilders, especially his friend and lifelong collaborator James Robert Napier, to make naval architecture into an engineering science. He was a founding member, and first President of the Institution of Engineers & Shipbuilders in Scotland in 1857. He was an early member of the Royal Institution of Naval Architects (founded 1860) and attended many of its annual meetings. With William Thomson and others, Rankine was a member of the board of enquiry into the controversial sinking of HMS \"Captain\".\n\n\n\n\n\n\n"}
