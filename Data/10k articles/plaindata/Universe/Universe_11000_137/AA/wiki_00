{"id": "439245", "url": "https://en.wikipedia.org/wiki?curid=439245", "title": "A3W reactor", "text": "A3W reactor\n\nThe A3W reactor is a naval reactor used by the United States Navy to provide electricity generation and propulsion on warships. Like all operational U.S. naval reactors it was a pressurized water reactor (PWR) design. The A3W designation stands for:\n\n\nThe reactor was intended for use aboard USS \"John F. Kennedy\". This four-reactor design was intended to reduce the cost involved in the construction and operation, as compared with the \"Enterprise\" and its eight nuclear reactors.\n\nEarly in the construction, the United States Secretary of the Navy had the plans changed to save money, and fossil fuel boilers were installed on the Kennedy. Because the plans for the ship did not include a funnel, the funnel on the Kennedy protrudes out from the ship at an angle.\n\nThe return to nuclear power for carriers came with the \"Nimitz\" class's A4W reactor's two reactor per ship design. While the two reactor configuration, with each core providing a much greater thermal output, is relatively less expensive than earlier designs, the power plant still represents about half the total cost of the ship.\n"}
{"id": "1241750", "url": "https://en.wikipedia.org/wiki?curid=1241750", "title": "Actinism", "text": "Actinism\n\nActinism () is the property of solar radiation that leads to the production of photochemical and photobiological effects. \"Actinism\" is derived from the Greek ακτίς, ακτῖνος (a ray or beam). The word \"actinism\" is found, for example, in the terminology of imaging technology (esp. photography), medicine (concerning sunburn), and chemistry (concerning containers that protect from photo-degradation), and the concept of actinism is applied, for example, in chemical photography and X-ray imaging.\n\nActinic () chemicals include silver salts used in photography and other light sensitive chemicals.\n\nIn chemical terms, actinism is the property of radiation that lets it be absorbed by a molecule and cause a photochemical reaction as a result. Einstein was the first to correctly theorize that each photon would be able to cause only one molecular reaction. This distinction separates photochemical reactions from exothermic reduction reactions triggered by radiation.\n\nFor general purposes, \"photochemistry\" is the commonly used vernacular rather than \"actinic\" or \"actino-chemistry\", which are again more commonly seen used for photography or imaging.\n\nIn medicine, actinic effects are generally described in terms of the dermis or outer layers of the body, such as eyes (see: Actinic conjunctivitis) and upper tissues that the sun would normally affect, rather than deeper tissues that higher-energy shorter-wavelength radiation such as x-ray and gamma might affect. Actinic is also used to describe medical conditions that are triggered by exposure to light, especially UV light (see actinic keratosis). \n\nThe term \"actinic rays\" is used to refer to this phenomenon.\n\nIn biology, \"actinic light\" denotes light from solar or other sources that can cause photochemical reactions such as photosynthesis in a species.\n\n\"Actinic light\" was first commonly used in early photography to distinguish light that would expose the monochrome films from light that would not. A non-actinic safe-light (e.g., red or amber) could be used in a darkroom without risk of exposing (fogging) light-sensitive films, plates or papers.\n\nEarly \"non colour-sensitive\" (NCS) films, plates and papers were only sensitive to the high-energy end of the visible spectrum from green to UV (shorter-wavelength light). This would render a print of the red areas as a very dark tone because the red light was not actinic. Typically, light from xenon flash lamps is highly actinic, as is daylight as both contain significant green-to-UV light.\n\nIn the first half of the 20th century, developments in film technology produced films sensitive to red and yellow light, known as orthochromatic and panchromatic, and extended that through to near infra-red light. These gave a truer reproduction of human perception of lightness across the color spectrum. In photography, therefore, actinic light must now be referenced to the photographic material in question.\n\nActinic inspection of masks in computer chip manufacture refers to inspecting the mask with the same wavelength of light that the lithography system will use.\n\nActinic lights are also common in the reef aquarium industry. They are used to promote coral and invertebrate growth.\nThey are also used to accentuate the fluorescence of fluorescent fish.\n\nActinic lighting is also used to limit algae growth in the aquarium. Since algae (like many other plants), flourish in shallower warm water, algae cannot effectively photosynthesize from blue and violet light, thus actinic light minimizes its photosynthetic benefit.\n\nActinic lighting is also a great alternative to black lights as it provides a \"night environment\" for the fish, while still allowing enough light for coral and other marine life to grow. Aesthetically, they make fluorescent coral \"pop\" to the eye, but in some cases also to promote the growth of deeper-water coral that is specialized in photosynthesis using blue light.\n\n\"Actinic\" lights are a high-color-temperature blue light. They are also used in electric fly killers to attract flies.\n"}
{"id": "277262", "url": "https://en.wikipedia.org/wiki?curid=277262", "title": "Alcoholate", "text": "Alcoholate\n\nOriginally, an alcoholate was the crystalline form of a salt in which alcohol took the place of water of crystallization.\n\nThe second meaning of the word is that of a tincture, or alcoholic extract of plant material.\n\nThe third, and more usual meaning of the word is as a synonym for alkoxide—a compound formed by the substitution of the hydrogen atom of the hydroxyl group of an alcohol by a metal atom.\n"}
{"id": "1049639", "url": "https://en.wikipedia.org/wiki?curid=1049639", "title": "Alkaline fuel cell", "text": "Alkaline fuel cell\n\nThe alkaline fuel cell (AFC), also known as the \"Bacon\" fuel cell after its British inventor, Francis Thomas Bacon, is one of the most developed fuel cell technologies. NASA has used alkaline fuel cells since the mid-1960s, in Apollo-series missions and on the Space Shuttle. Alkaline fuel cells consume hydrogen and pure oxygen producing potable water, heat, and electricity. They are among the most efficient fuel cells, having the potential to reach 70%.\n\nThe fuel cell produces power through a redox reaction between hydrogen and oxygen. At the anode, hydrogen is oxidized according to the reaction:\n\nformula_1\n\nproducing water and releasing electrons. The electrons flow through an external circuit and return to the cathode, reducing oxygen in the reaction:\n\nformula_2\n\nproducing hydroxide ions. The net reaction consumes one oxygen molecule and two hydrogen molecules in the production of two water molecules. Electricity and heat are formed as by-products of this reaction.\n\nThe two electrodes are separated by a porous matrix saturated with an aqueous alkaline solution, such as potassium hydroxide (KOH). Aqueous alkaline solutions do not reject carbon dioxide (CO) so the fuel cell can become \"poisoned\" through the conversion of KOH to potassium carbonate (KCO). Because of this, alkaline fuel cells typically operate on pure oxygen, or at least purified air and would incorporate a 'scrubber' into the design to clean out as much of the carbon dioxide as is possible. Because the generation and storage requirements of oxygen make pure-oxygen AFCs expensive, there are few companies engaged in active development of the technology. There is, however, some debate in the research community over whether the poisoning is permanent or reversible. The main mechanisms of poisoning are blocking of the pores in the cathode with KCO, which is not reversible, and reduction in the ionic conductivity of the electrolyte, which may be reversible by returning the KOH to its original concentration. An alternate method involves simply replacing the KOH which returns the cell back to its original output.\n\nWhen carbon dioxide reacts with the electrolyte carbonates are formed. The carbonates could precipitate on the pores of electrodes that eventually block them. It has been found that AFCs operating at higher temperature do not show a reduction in performance, whereas at around room temperature, a significant drop in performance has been shown. The carbonate poisoning at ambient temperature is thought to be a result of the low solubility of KCO around room temperature, which leads to precipitation of KCO that blocks the electrode pores. Also, these precipitants gradually decrease the hydrophobicity of the electrode backing layer leading to structural degradation and electrode flooding.\nformula_3\n\nOn the other hand, the charge-carrying hydroxyl ions in the electrolyte can react with carbon dioxide from organic fuel oxidation (i.e. methanol, formic acid) or air to form carbonate species. \nformula_4\n\nCarbonate formation depletes hydroxyl ions from the electrolyte, which reduces electrolyte conductivity and consequently cell performance.\nAs well as these bulk effects, the effect on water management due to a change in vapor pressure and/or a change in electrolyte volume can be detrimental as well .\n\nBecause of this poisoning effect, two main variants of AFCs exist: static electrolyte and flowing electrolyte. Static, or immobilized, electrolyte cells of the type used in the Apollo space craft and the Space Shuttle typically use an asbestos separator saturated in potassium hydroxide. Water production is controlled by evaporation from the anode, as pictured above, which produces pure water that may be reclaimed for other uses. These fuel cells typically use platinum catalysts to achieve maximum volumetric and specific efficiencies.\n\nFlowing electrolyte designs use a more open matrix that allows the electrolyte to flow either between the electrodes (parallel to the electrodes) or through the electrodes in a transverse direction (the ASK-type or EloFlux fuel cell). In parallel-flow electrolyte designs, the water produced is retained in the electrolyte, and old electrolyte may be exchanged for fresh, in a manner analogous to an oil change in a car. More space is required between electrodes to enable this flow, and this translates into an increase in cell resistance, decreasing power output compared to immobilized electrolyte designs. A further challenge for the technology is how severe the problem of permanent blocking of the cathode is by KCO; some published reports have indicated thousands of hours of operation on air. These designs have used both platinum and non-noble metal catalysts, resulting in increased efficiencies and increased cost. \n\nThe EloFlux design, with its transverse flow of electrolyte, has the advantage of low-cost construction and replaceable electrolyte but so far has only been demonstrated using oxygen. \n\nThe electrodes consist of a double layer structure: an active electrocatalyst layer and a hydrophobic layer. The active layer consists of an organic mixture which is ground and then rolled at room temperature to form a crosslinked self-supporting sheet. The hydrophobic structure prevents the electrolyte from leaking into the reactant gas flow channels and ensures diffusion of the gases to the reaction site. The two layers are then pressed onto a conducting metal mesh, and sintering completes the process.\n\nFurther variations on the alkaline fuel cell include the metal hydride fuel cell and the direct borohydride fuel cell.\n\nAlkaline fuel cells operate between ambient temperature and 90 °C with an electrical efficiency higher than fuel cells with acidic electrolyte, such as proton exchange membrane fuel cells (PEMFC), solid oxide fuel cells, and phosphoric acid fuel cells.\nBecause of the alkaline chemistry, oxygen reduction reaction (ORR) kinetics at the cathode are much more facile than in acidic cells, allowing use of non-noble metals, such as iron, cobalt, or nickel, at the anode (where fuel is oxidized); and cheaper catalysts such as silver or iron phtalocyanines at the cathode. due to the low overpotentials associated with electrochemical reactions at high pH.\n\nAn alkaline medium also accelerates oxidation of fuels like methanol, making them more attractive. \n\nAFCs are the cheapest of fuel cells to manufacture. The catalyst required for the electrodes can be any of a number of different chemicals that are inexpensive compared to those required for other types of fuel cells.\n\nThe commercial prospects for AFCs lie largely with the recently developed bi-polar plate version of this technology, considerably superior in performance to earlier mono-plate versions.\n\nThe world's first Fuel Cell Ship HYDRA used an AFC system with 5 kW net output.\n\nAnother recent development is the solid-state alkaline fuel cell, utilizing a solid anion exchange membrane instead of a liquid electrolyte. This resolves the problem of poisoning and allows the development of alkaline fuel cells capable of running on safer hydrogen-rich carriers such as liquid urea solutions or metal amine complexes.\n\n\nDevelopers\n"}
{"id": "37089288", "url": "https://en.wikipedia.org/wiki?curid=37089288", "title": "Artificial butter flavoring", "text": "Artificial butter flavoring\n\nArtificial butter flavoring may contain diacetyl, acetylpropionyl, or acetoin, three natural compounds in butter that contribute to its characteristic flavor. Because of this, manufacturers of margarines or similar oil-based products typically add diacetyl, acetylpropionyl and acetoin (along with beta carotene for the yellow color) to make the final product butter-flavored, because it would otherwise be relatively tasteless.\n\nChronic industrial exposure to diacetyl fumes, such as in the microwave popcorn production industry, has been associated with bronchiolitis obliterans, a rare and life-threatening form of non-reversible obstructive lung disease in which the bronchioles (small airway branches) are compressed and narrowed by fibrosis (scar tissue) and/or inflammation.\n"}
{"id": "2177071", "url": "https://en.wikipedia.org/wiki?curid=2177071", "title": "B-tagging", "text": "B-tagging\n\nb-tagging is a method of jet flavor tagging used in modern particle physics experiments. It is the identification (or \"tagging\") of jets originating from bottom quarks (or b quarks, hence the name).\n\nb-tagging is important because:\n\n\nThe methods for b-tagging are based on the unique features of b-jets. These include:\n\n\nNone of the methods of identifying b-jets are foolproof, and modern particle physics experiments must devote significant time to studying how often they successfully identify b-jets and how often they misidentify other jets. Monte Carlo simulations are used to develop and evaluate the performance of tagging algorithms.\n\nExperiments making precise measurements of B mesons (mesons containing b-quarks) also try to identify the particular initial B meson within the jet. This is done in order to observe the oscillation of one meson into another (– oscillation), which allows the measurement of CP violation.\n\n"}
{"id": "146847", "url": "https://en.wikipedia.org/wiki?curid=146847", "title": "Beta carbon nitride", "text": "Beta carbon nitride\n\nBeta carbon nitride (β-CN) is a superhard material predicted to be harder than diamond.\n\nThe material was first proposed in 1985 by Marvin Cohen and Amy Liu. Examining the nature of crystalline bonds they theorised that carbon and nitrogen atoms could form a particularly short and strong bond in a stable crystal lattice in a ratio of 1:1.3. That this material would be harder than diamond on the Mohs scale was first proposed in 1989.\n\nThe material has been considered difficult to produce and could not be synthesized for many years. Recently, the production of beta carbon nitride was achieved. For example, nanosized beta carbon nitride crystals and nanorods of this material were prepared by means of an approach involving mechanochemical processing.\n\nThrough a mechanochemical reaction process, β-CN can be synthesized. This method is achieved by ball milling high purity graphite powders down to an amorphous nanoscale size while under an argon atmosphere, then the argon is purged and the graphite powders are introduced to an NH gas atmosphere, which after high energy ball milling, has been found to form a nanosized flake-like structure of β-CN. During milling, fracture and welding of the reactants and graphite powder particles occur repeatedly from ball/powder collisions. Plastic deformation of the graphite powder particles occur due to the shear bands decomposing into sub-grains that are separated by low-angle grain boundaries, further milling decreases the sub-grain size until nanosize sub-grains form. The high pressure and intense motion promotes catalytic dissociation of NH molecules into monatomic nitrogen on the fractured surface of the carbon. Nanosized carbon powders act substantially different from its bulk material as a result of particle dimension and surface area, causing the nanosized carbon to easily react with the free nitrogen atoms, forming β-CN powder.\n\nSingle crystal β-CN nanorods can be formed after the powder-like or flake-like compound is thermally annealed with an NH gas flow. The size of the nanorods is determined by the temperature and time of thermal annealing. These nanorods grow faster in their axis direction that the diameter direction and have hemispherical-like ends. A cross section of the nanorods indicates that their section morphology is prismatic. It was discovered that they contain amorphous phases, however when annealed to 450 degrees Celsius for three hours under an NH atmosphere, the amount of the amorphous phase diminished to almost none. These nanorods are dense and twinned rather than nanotubes. Synthesizing these nanorods through thermal annealing provides an effective, low cost, and high yield method for the synthesis of single crystal nanorods.\n\nRather than forming a powder or nanorod, the carbon nitride compound can alternatively be formed in thin amorphous films by either shock-wave compression technology, pyrolysis of high nitrogen content precursors, diode sputtering, solvothermal preparation, pulsed laser ablation, or ion implantation.\n\nAlthough extensive studies on the process and synthesis of the formed carbon nitride have been reported, the nitrogen concentration of the compound tends to be below the ideal composition for CN. This is due to the low thermodynamic stability with respect to the elements C and N, indicated by a positive value of the enthalpies of formation. The commercial exploitation of nanopowders is very limited by the high synthesis cost along with difficult methods of production that causes a low yield.\n\nThe structure was determined by Fourier transformation infrared spectroscopy, transmission electron microscopy, and X-ray diffraction. By using an SAED, a polycrystalline β-CN with a lattice constant of a = 6.36 Å, c = 4.648 Å can be determined. Thermal annealing can be used to change the flake-like structure into sphere- or rod-like structures.\n\nIt has the same crystal structure as β-SiN with a hexagonal network of tetrahedrally (sp) bonded carbon and trigonal planar nitrogen (sp).\n\nThe nanorods are generally straight and contain no other defects.\n\nProperties show a hardness equal or above diamond, the hardest known material.\n\nThe bulk modulus of diamond is 4.43 MBar while β-CN only has a bulk modulus of 4.27 MBar(± .15). This is the closest conceived bulk modulus to diamond.\n\nPromising in the field of tribology, wear resistant coating, optical engineering, and electronic engineering.\n\nComposite opportunities also exist using TiN as seeding layers for carbon nitride, which produces actual crystalline composites with hardness at levels of 45-55 (GPa) which is on the lower end of diamond.\n\nThe predicted hardness for pure beta carbon nitride(4.27 ± .15 Mbar) is similar to that of diamond (4.43 Mbar), giving it the potential to be useful in the same fields as diamond.\n\n"}
{"id": "3568885", "url": "https://en.wikipedia.org/wiki?curid=3568885", "title": "BirdLife Malta", "text": "BirdLife Malta\n\nBirdLife Malta is Malta's biggest environmental movement. The organization's stated aim is to \"achieve protection of wild birds, natural habitat and biodiversity\".\n\nBirdLife Malta is also Malta's first environment NGO. It was founded in January 1962, and was then known as MOS (Malta Ornithological Society). It started off more as a study group, but quickly came to realize that wild birds were in dire need of protection. Today Birdlife Malta works to protect bird populations.\n\nMalta is a very densely populated small Mediterranean island state, with no truly wild areas left. The biggest threats to birds in Malta are habitat destruction (e.g. new roads, building development, hotels, golf courses), disturbance and direct persecution (hunting, trapping). Malta has been called \"the most savagely bird-hostile place in Europe\".\n\nBirdLife Malta uses various methods to protect birds. Foremost among its methods are education, especially through schools campaigns, publications, the media and an active junior member section; the management of two wetland nature reserves, namely Għadira and Is-Simar; lobbying the authorities for better bird-protection legislation; assisting the police for better law enforcement; reclaiming and improving degraded habitat; and doing research about birds and their habitat to identify more areas for protection.\n\nBirdLife Malta issues various newsletters and magazines for its membership, and publishes books about natural history. Its latest book is \"Nature in Gozo\" (2007).\n\nBirdLife Malta is an entirely voluntary organisation and dependent on subscription and donations. Membership currently stands at about 3000.\n\nBirdLife Malta is a Partner of BirdLife International, a global network of environmental organizations in 100 countries and territories.\n\nCauses of Population Decline:\n\nHunting and illegal poaching of birds, especially larger predatory birds can have a significant impact on their population size and has resulted in some population decline.[3] Recently the Maltese government agreed to allow the shooting of 5,000 Turtle Doves. This created controversy due to the endangered status of these birds. BirdLife Malta works to prevent the illegal hunting of birds and works to maintain Malta as a safe stop for the many migratory bird species that pass through on an annual bases. \n\n\n3. Hirschfeld, A. X. E. L., & Heyd, A. L. E. X. A. N. D. E. R. (2005). Mortality of migratory birds caused by hunting in Europe: bag statistics and proposals for the conservation of birds and animal welfare. \"Berichte zum Vogelschutz\", \"42\", 47-74.\n\nadd\nSultana, J. & Borg, J.J. 1915. History of Ornithology in Malta. BirdLife Malta, Malta - see Chapter 13.\n\n"}
{"id": "18315820", "url": "https://en.wikipedia.org/wiki?curid=18315820", "title": "Bosseopentaenoic acid", "text": "Bosseopentaenoic acid\n\nBosseopentaenoic acid (BPA) is a conjugated polyunsaturated fatty acid. Bosseopentaenoic acid can be extracted from the red coralline algae, \"Bossiella orbigniana\". The first total synthesis of methyl bosseopentaenoate by consecutive palladium-catalyzed reactions was reported in 2011. In 2017, bosseopentaenoic acid was obtained from the ester hydrolysis of methyl bosseopentaenoate in good yield using mild condition and the synthesis of its sulfur-bridged analogue of BPA; thiophene analogue was achieved by Mohamed and Solum In this study, a comparison between the thiophene analogue and BPA with respect to their antioxidant activity was accomplished. It was shown that the rigidified analogue; the thiophene analogue exhibited higher free radical scavenging potential than the bosseopentaenoic acid. The results showed that by lowering the flexibility of the BPA as lead compound by incorporation thiophene ring in its structure, an increased antioxidant activity was observed. This study opens the door to investigate the relationship between the flexibility of other polyunsaturated fatty acids (PUFAs) and enhancement in the biological activity.\n"}
{"id": "25010364", "url": "https://en.wikipedia.org/wiki?curid=25010364", "title": "Campos del Tuyú National Park", "text": "Campos del Tuyú National Park\n\nCampos del Tuyú National Park () is a national park in Buenos Aires Province, Argentina. Situated on the southern shore of Samborombón Bay, the park was established on May 13, 2009. The main attraction of Campos del Tuyú is the rare pampas deer; in fact, it is one of the few places in the Pampas where this species survive. Other inhabitants of the park include over a hundred bird species, capybara, and the likewise endangered Leopardus geoffroyi.\n"}
{"id": "310378", "url": "https://en.wikipedia.org/wiki?curid=310378", "title": "Clark Y", "text": "Clark Y\n\nClark Y is the name of a particular aerofoil profile, widely used in general purpose aircraft designs, and much studied in aerodynamics over the years. The profile was designed in 1922 by Virginius E. Clark. The airfoil has a thickness of 11.7 percent and is flat on the lower surface from 30 percent of chord back. The flat bottom simplifies angle measurements on propellers, and makes for easy construction of wings on a flat surface.\n\nFor many applications the Clark Y has been an adequate airfoil section; it gives reasonable overall performance in respect of its lift-to-drag ratio, and has gentle and relatively benign stall characteristics. But the flat lower surface is not optimal from an aerodynamic perspective, and it is rarely used in modern designs.\nThe Clark YH airfoil is similar but with a reflexed (turned up) trailing edge producing a more positive pitching moment reducing the horizontal tail load required to trim an aircraft.\n\nThe Lockheed Vega and Spirit of St. Louis are two of the better known aircraft using the Clark Y profile, whilst the Ilyushin Il-2 and Hawker Hurricane are examples of mass-produced users of the Clark YH.\n\nThe Northrop Tacit Blue stealth technology demonstrator aircraft also used a Clark Y. The Clark Y was chosen as its flat bottom worked well with the design goal of a low radar cross-section.\n\nThe Clark Y has found favour for the construction of model aircraft, thanks to the flight performance that the section offers at medium Reynolds number airflows. Applications on model aircraft is very wide, ranging from free-flight gliders through to multi-engined radio control scale models.\n\nThe Clark Y is appealing for its near-flat lower surface, which aids in the construction of wings on plans mounted on a flat construction board. Inexperienced modellers are more readily able to build model aircraft which provide a good flight performance with benign stalling characteristics.\n\nAn inverted Clark Y airfoil was used on the spoilers of the Dodge Charger Daytona and Plymouth Superbird. \n\nSome of the better known aircraft that use the Clark Y and YH include:\n"}
{"id": "54979728", "url": "https://en.wikipedia.org/wiki?curid=54979728", "title": "Cloghanecarhan", "text": "Cloghanecarhan\n\nCloghanecarhan is a ringfort and ogham stone (CIIC 230) forming a National Monument located in County Kerry, Ireland.\n\nCloghanecarhan is located on the western end of the Iveragh Peninsula, south-southeast of Cahersiveen.\n\nThe ogham stone was erected some time in the Middle Ages; based on the grammar, it is a late inscription, c. AD 600. Next to it is a stone cashel used for later Christian burials.\n\nThe ogham stone originally stood at the east entrance of the ringfort but now lies to the north. It is slate, 208 × 38 × 18 cm. The inscription reads (\"'of Ec...án? son of Mac-Cáirthinn\"); this is overwritten on an earlier inscription, . The same name, in the form MAQI-CAIRATINI, appears on an ogham stone in Painestown (CIIC 40), and it means \"devotee of the rowan.\" The first element of the townland name could mean either \"ford of stepping-stones\" (there is a small stream, the Direen, to the east) or to a stone beehive hut, such as is found in the cashel.\n\nThe ringfort was known locally as 'Keeldarragh'; it is circular and enclosed by a bank with entrance at east and \"pillars\" at the west end. Inside is a circular hut, three \"leachta\", a souterrain and a cross slab.\n"}
{"id": "1110671", "url": "https://en.wikipedia.org/wiki?curid=1110671", "title": "Copper-clad steel", "text": "Copper-clad steel\n\nCopper-clad steel (CCS), also known as copper-covered steel or the trademarked name Copperweld is a bi-metallic product, mainly used in the wire industry that combines the high mechanical resistance of steel with the conductivity and resistance to corrosion of copper.\n\nIt is mainly used for grounding purposes, line tracing to locate underground utilities, drop wire of telephone cables, and inner conductor of coaxial cables, including thin hookup cables like RG174, and CATV cable.\n\nThe first recorded attempt to make copper clad steel wire took place in the early 1860s, according to the \"Copper Clad Handbook\", issued by the Duplex Metals Co., Chester, PA at the turn of the 20th century. Although for over 100 years people had been suggesting various ways of uniting copper and steel, it was not until the period mentioned that Farmer and Milliken tried wrapping a strip of copper about a steel wire. American engineers in 1883 and again in the 1890s made attempts to produce a copper-steel wire, in one instance at least, by electro-plating copper on steel.\n\nThe Duplex Metals Co. traces its beginning to John Ferreol Monnot between 1900 and 1905. He had been very interested in the work of Mr. Martin in Paris, and, as the \"Handbook says\": \"After several years devoted to experimenting, organized the Duplex Metals Company. Prior to his discovery of the process under which this company operates in producing its copper clad, probably almost every other possible way of welding copper and steel together had been tried by Mr. Monnot, but found useless for the purpose.\"\n\nCopper-clad steel wire find applications in grounding, connection of ground rods to metallic structures, ground grid meshes, substations, power installations and lightning arresters. \n\nCopper coated welding wire has become common since wire welding equipment has become popular.\n\nThis wire is also sometimes used for power transmission,and by a few for making antennas, especially long wire antennas that need extra strength to withstand the tension without stretching excessively. \n\nThe main properties of these conductors include:\n\n\nSince the outer conductor layer is low-impedance copper, and the center is higher impedance steel, the skin effect gives copper-clad RF transmission lines impedance at high AC frequencies similar to that of a solid copper conductor.\n\nTensile strength of copper-clad steel conductors is greater than that of ordinary copper conductors permitting greater span lengths than with copper.\n\nAnother advantage is that smaller diameter copper-clad steel conductors may be used in coaxial cables, permitting higher impedance and smaller cable diameter than with copper conductors of similar strength.\nDue to the inseparable union of the two metals, it deters theft since copper recovery is impractical and thus has very little scrap value.\n\nInstallations with copper-clad steel conductors are generally recognized as fulfilling the required specifications for a good ground. For this reason it is used with preference by utilities and oil companies when cost is a concern.\n\n\n"}
{"id": "30941457", "url": "https://en.wikipedia.org/wiki?curid=30941457", "title": "DeGolyer and MacNaughton", "text": "DeGolyer and MacNaughton\n\nDeGolyer and MacNaughton is a petroleum consulting company based in Dallas, Texas, with offices in Houston, Moscow, Astana, Buenos Aires, and Algiers.\n\nDeGolyer and MacNaughton was founded in 1936 by Everette Lee DeGolyer and Lewis MacNaughton. In 2004, it acquired Calgary-based Outtrim Szabo Associates forming its office in Canada as a subsidiary company.\n\nDeGolyer and MacNaughton Corp. provides petroleum consulting services in the United States and internationally. It offers appraisals, economic forecasts, geological studies, geophysical studies, petrophysical studies, engineering analysis, reservoir simulation, reserves assessment, and management services. It serves the petroleum industry and financial community.\n\n\n"}
{"id": "30980881", "url": "https://en.wikipedia.org/wiki?curid=30980881", "title": "Deodar forests", "text": "Deodar forests\n\nDeodar forests are forests dominated Cedrus deodara and are found in Western Himalayas from Gandak river in central Nepal to Hindukush in Afghanistan.\n\nDeodar cedar\n\"Timber of the Gods\"\nThe deodar cedar is native to the Himalayan Mountains where its local name is deodar, which translates from the original Sanskrit as \"timber of the gods\". They were officially introduced into cultivation about 1831 although they have been grown in Chinese parks and gardens for centuries.\n\nIt is widely grown as an ornamental tree, and is often found in urban forests and parks and along highway mediums.\n\nWith its pyramidal shape, soft grayish-green needles and drooping branches, this cedar makes a graceful specimen. Growing rapidly to 40 to 50 feet tall and 20 to 30 feet wide, it also works well as a soft screen. Needles are borne in dense clusters on large, woody pegs and are 1 to 2 inches long.\n\nAt use in landscaping and urban settings, the understory is often determined by design. However, native species of shrubs and trees should be removed to reduce competition and improve growing conditions.\n\nGeneral cultivation is limited to areas with mild winters as these trees are frequently killed by temperatures below −13 °F. Prefers sunny and well-drained locations.\n"}
{"id": "1977933", "url": "https://en.wikipedia.org/wiki?curid=1977933", "title": "Downstream (petroleum industry)", "text": "Downstream (petroleum industry)\n\nThe oil and gas industry is usually divided into three major sectors: upstream, midstream, and downstream. The downstream sector is the refining of petroleum crude oil and the processing and purifying of raw natural gas, as well as the marketing and distribution of products derived from crude oil and natural gas. The downstream sector reaches consumers through products such as gasoline or petrol, kerosene, jet fuel, diesel oil, heating oil, fuel oils, lubricants, waxes, asphalt, natural gas, and liquefied petroleum gas (LPG) as well as hundreds of petrochemicals.\n\nMidstream operations are often included in the downstream category and are considered to be a part of the downstream sector.\n\nCrude oil is a mixture of many varieties of hydrocarbons and most usually have many sulfur-containing compounds. The oil refining process commonly includes hydrodesulfurization which converts most of that sulfur into gaseous hydrogen sulfide. Raw natural gas also may contain gaseous hydrogen sulfide and sulfur-containing mercaptans, which are removed in natural-gas processing plants before the gas is distributed to consumers.\n\nThe hydrogen sulfide removed in the refining and processing of crude oil and natural gas is subsequently converted into byproduct elemental sulfur. In fact, the vast majority of the 64,000,000 metric tons of sulfur produced worldwide in 2005 was byproduct sulfur from refineries and natural-gas processing plants.\n\nISO 20815 defines \"downstream\" in its definition section as:<br>\n3.1.8 downstream<br>\nbusiness process, most commonly in petroleum industry, associated with post-production activities.\n\n"}
{"id": "38553077", "url": "https://en.wikipedia.org/wiki?curid=38553077", "title": "Duyên Hải Power Station", "text": "Duyên Hải Power Station\n\nThe Duyên Hải Power Station is a complex of under-construction coal-fired power plants in Vietnam. It is located in Mu U Hamlet, Dan Thanh Commune, Duyên Hải District, Trà Vinh Province. The complex will have a total capacity of 3,689 MW. It includes also a seaport coal terminal, to be built by China Communications Construction Company, with a capacity of 12 million tonnes of coal and oil per year.\n\nDuyen Hai 1 will have an installed capacity of 1,245 MW (2 X 622.5MW) and its annual output will be 7.5–8 GWh. The plant will cost US$1.5 billion. It is owned by Vietnam Electricity.\n\nEngineering, procurement and construction contract was signed on 30 March 2010 and construction started on 19 September 2010. The main contractor is Chinese Oriental Power Group. According to the contract, unit 1 would be operational by 25 July 2015 and unit 2 by 25 September 2015. It is expected that the first boiler at unit 1 would be fired on 25 October 2014.\n\nThe 1,200-MW Duyen Hai 2 will be developed by Malaysian company Janakuasa under build–operate–transfer agreement. The engineering, procurement and construction contract is awarded to Alstom.\n\nDuyen Hai 3 has a planned capacity of 1,244 MW. It consists of two condensing units, 622 MW each. When built, it is expected to use 3.6 million metric tons of coal a year for annual production of 7.8 GWh of electricity. The plant will cost US$1.37 billion. It is owned by Vietnam Electricity.\n\nThe plant will be built by Chinese Chengda-Dec-Swepdi-Zepc consortium as the main contractor. Construction started on 8 December 2012 and it is expected to become operational in 2015-2016. The plant covers area of .\n"}
{"id": "10731380", "url": "https://en.wikipedia.org/wiki?curid=10731380", "title": "Eccentric-hub scooter", "text": "Eccentric-hub scooter\n\nAn Eccentric-hub scooter is a two-wheeled human powered vehicle with an off-centered hub on the large rear wheel and is powered by the rider making a bouncing motion on the platform.\n\nAn Ingo-bike is a vehicle similar to a kick scooter, with a large rear wheel, mounted off-centre. The vehicle is propelled by the user bouncing up and down or rocking backwards and forwards on the platform to drive the rear wheel around the eccentric hub. One early inventor described the vehicle as a 'galloping scooter' and the rider's motion does resemble a horse-rider's motion. Although the motion produced is impact-free, it is reportedly less efficient than a conventional bicycle and will not propel the vehicle uphill.\n\nAlthough several patents for eccentric-hub cycles exist from 1928\n, the most popular incarnation was made from 1934 by brothers Phillip and Prescott Huyssen and called the \"Exercycle\"\n\n. It was produced by the Ingersoll Steel & Disc Co. (a division of Borg-Warner Corp.), from 1934–1937, under the name \"Ingo-bike\". A large number of publicity events and promotions popularised the bike and a group of Ingo-bikers in the late 1930s traveled from Chicago to Miami in 30 days. Production ceased when the factory re-tooled to begin producing armaments prior to the Second World War. Phillip Huyssen continued designing variations until at least the 1970s.\n\nIn the Three Stooges movie Yes, We Have No Bonanza\n\n, Curly rides up to a saloon bar on an Ingo-Bike.\nApparently re-introduced as a children's scooter in the 1960s with the name \"kangaroo scooter\", kangaroo scooters were built in Taiwan for a company based in Concord, California. They had the eccentric rear wheel and a front caliper brake,chrome fenders and a kickstand.\n\nAnother device also called the Kangaroo Scooter has been designed and sold by Ron and Johnny Knox's company, Knoxcooter Inc., of Weyburn, Saskatchewan, Canada since around 2004. This variant includes a one-sided rear wheel support, disk brakes and uniquely a clutched rear hub; which allows the user to coast for periods without bouncing up and down. Contrary to popular belief, the Eco-Fav (Frequency Accelerated Velocipede) scooter is efficient enough to ride uphill. Many users prefer to ride it on an incline to increase their workout resistance.\n\nA similar concept is the 'Bounce-Bike' which has an eccentric front wheel and a small rear wheel.\n\nVariants have been produced since the 1990s and called \"whymcycle\", by inventor/substitute teacher Peter Wagner. The term 'Whymcycle' Wagner coined by taking the common word 'whimsical',\nwhich means imaginative, fanciful or eccentric; socially eccentric, that is. Also, the Ingo-style bike he makes has an eccentric rear axle for propulsion, as well as being a rather fanciful mode of transport. To complete the trifecta of this bike's nomenclatural origins, Wagner's middle name is William, which he abbreviates as 'Wm.'. Hence the bikes he makes are:\n\n\nTwenty five years of production has resulted in 125 'Whymsies', thus far. The term has come to apply to all of the styles of creative cycles he makes, from handcycles for riders with leg disabilities, to assorted cycles: front wheel drive, recumbent\nbikes and trikes, quadracycles, tall bikes, high wheelers- big and small,\nas well as tandems and sociable trikes of various stripes. Kinetic Sculptures, that is, all-terrain amphibious human -powered machines, which have been built and raced on the West Coast since 1969, are also some of the machines Wagner builds. See 'That's A Mower, Eh?', 'Kinetic Choo Choo', and his race car tired amphibious Whymcycle, #115 in fact,\n'Bounce For Glory'.\n"}
{"id": "212253", "url": "https://en.wikipedia.org/wiki?curid=212253", "title": "Electric power distribution", "text": "Electric power distribution\n\nElectric power distribution is the final stage in the delivery of electric power; it carries electricity from the transmission system to individual consumers. Distribution substations connect to the transmission system and lower the transmission voltage to medium voltage ranging between 2 kV and 35 kV with the use of transformers. \"Primary\" distribution lines carry this medium voltage power to distribution transformers located near the customer's premises. Distribution transformers again lower the voltage to the utilization voltage used by lighting, industrial equipment or household appliances. Often several customers are supplied from one transformer through \"secondary\" distribution lines. Commercial and residential customers are connected to the secondary distribution lines through service drops. Customers demanding a much larger amount of power may be connected directly to the primary distribution level or the subtransmission level.\n\nElectric power distribution only became necessary in the 1880s when electricity started being generated at power stations. Before that electricity was usually generated where it was used. The first power distribution systems installed in European and US cities were used to supply lighting: arc lighting running on very high voltage (around 3000 volts) alternating current (AC) or direct current (DC), and incandescent lighting running on low voltage (100 volt) direct current. Both were supplanting gas lighting systems, with arc lighting taking over large area and street lighting, and incandescent lighting replacing gas for business and residential lighting.\n\nDue to the high voltages used in arc lighting, a single generating station could supply a long string of lights, up to long circuits. Each doubling of the voltage would allow the same size cable to transmit the same amount of power four times the distance for a given power loss. Direct current indoor incandescent lighting systems, for example the first Edison Pearl Street Station installed in 1882, had difficulty supplying customers more than a mile away. This was due to the low 110 volt system being used throughout the system, from the generators to the final use. The Edison DC system needed thick copper conductor cables, and the generating plants needed to be within about of the farthest customer to avoid excessively large and expensive conductors.\n\nTransmitting electricity a long distance at high voltage and then reducing it to a lower voltage for lighting became a recognized engineering roadblock to electric power distribution with many, not very satisfactory, solutions tested by lighting companies. The mid-1880s saw a breakthrough with the development of functional transformers that allowed the AC voltage to be \"stepped up\" to much higher transmission voltages and then dropped down to a lower end user voltage. With much cheaper transmission costs and the greater economies of scale of having large generating plants supply whole cities and regions, the use of AC spread rapidly.\n\nIn the US the competition between direct current and alternating current took a personal turn in the late 1880s in the form of a \"War of Currents\" when Thomas Edison started attacking George Westinghouse and his development of the first US AC transformer systems, pointing out all the deaths caused by high voltage AC systems over the years and claiming any AC system was inherently dangerous. Edison's propaganda campaign was short lived with his company switching over to AC in 1892.\n\nAC became the dominant form of transmission of power with innovations in Europe and the US in electric motor designs and the development of engineered \"universal systems\" allowing the large number of legacy systems to be connected to large AC grids.\n\nIn the first half of the 20th century, in many places the electric power industry was vertically integrated, meaning that one company did generation, transmission, distribution, metering and billing. Starting in the 1970s and 1980s, nations began the process of deregulation and privatisation, leading to electricity markets. The distribution system would remain regulated, but generation, retail, and sometimes transmission systems were transformed into competitive markets.\n\nElectric power begins at a generating station, where the potential difference can be as high as 33,000 volts. AC is usually used. Users of large amounts of DC power such as some railway electrification systems, telephone exchanges and industrial processes such as aluminium smelting use rectifiers to derive DC from the public AC supply, or may have their own generation systems. High-voltage DC can be advantageous for isolating alternating-current systems or controlling the quantity of electricity transmitted. For example, Hydro-Québec has a direct-current line which goes from the James Bay region to Boston.\n\nFrom the generating station it goes to the generating station’s switchyard where a step-up transformer increases the voltage to a level suitable for transmission, from 44kV to 765kV. Once in the transmission system, electricity from each generating station is combined with electricity produced elsewhere. Electricity is consumed as soon as it is produced. It is transmitted at a very high speed, close to the speed of light.\n\nThe transition from transmission to distribution happens in a power substation, which has the following functions:\nUrban distribution is mainly underground, sometimes in common utility ducts. Rural distribution is mostly above ground with utility poles, and suburban distribution is a mix.\nCloser to the customer, a distribution transformer steps the primary distribution power down to a low-voltage secondary circuit, usually 120/240 volts in the US for residential customers. The power comes to the customer via a service drop and an electricity meter. The final circuit in an urban system may be less than 50 feet, but may be over 300 feet for a rural customer.\n\nPrimary distribution voltages range from 4 kV to 35 kV phase-to-phase (2.4 kV to 20 kV phase-to-neutral) Only large consumers are fed directly from distribution voltages; most utility customers are connected to a transformer, which reduces the distribution voltage to the low voltage \"utilization voltage\", \"supply voltage\" or \"mains voltage\" used by lighting and interior wiring systems.\n\nDistribution networks are divided into two types, radial or network. A radial system is arranged like a tree where each customer has one source of supply. A network system has multiple sources of supply operating in parallel. Spot networks are used for concentrated loads. Radial systems are commonly used in rural or suburban areas.\n\nRadial systems usually include emergency connections where the system can be reconfigured in case of problems, such as a fault or planned maintenance. This can be done by opening and closing switches to isolate a certain section from the grid.\n\nLong feeders experience voltage drop (power factor distortion) requiring capacitors or voltage regulators to be installed.\n\nReconfiguration, by exchanging the functional links between the elements of the system, represents one of the most important measures which can improve the operational performance of a distribution system. The problem of optimization through the reconfiguration of a power distribution system, in terms of its definition, is a historical single objective problem with constraints. Since 1975, when Merlin and Back introduced the idea of distribution system reconfiguration for active power loss reduction, until nowadays, a lot of researchers have proposed diverse methods and algorithms to solve the reconfiguration problem as a single objective problem. Some authors have proposed Pareto optimality based approaches (including active power losses and reliability indices as objectives). For this purpose, different artificial intelligence based methods have been used: microgenetic, branch exchange, particle swarm optimization and non-dominated sorting genetic algorithm.\n\nRural electrification systems tend to use higher distribution voltages because of the longer distances covered by distribution lines (see Rural Electrification Administration). 7.2, 12.47, 25, and 34.5 kV distribution is common in the United States; 11 kV and 33 kV are common in the UK, Australia and New Zealand; 11 kV and 22 kV are common in South Africa and China. Other voltages are occasionally used.\n\nRural services normally try to minimize the number of poles and wires. It uses higher voltages (than urban distribution), which in turn permits use of galvanized steel wire. The strong steel wire allows for less expensive wide pole spacing. In rural areas a pole-mount transformer may serve only one customer. In New Zealand, Australia, Saskatchewan, Canada, and South Africa, Single-wire earth return systems (SWER) are used to electrify remote rural areas.\n\nThree phase service provides power for large agricultural facilities, petroleum pumping facilities, water plants, or other customers that have large loads (Three phase equipment).\n\nUsually in the United States, a \"4-wire wye system\" is used, which includes 3 primary wires and 1 neutral wire. \nIn other countries or in extreme rural areas the neutral wire is connected to the ground to use that as a return (Single-wire earth return). This is called an unground wye system.\n\nElectricity is delivered at a frequency of either 50 or 60 Hz, depending on the region. It is delivered to domestic customers as single-phase electric power. In some countries as in Europe a three phase supply may be made available for larger properties. Seen with an oscilloscope, the domestic power supply in North America would look like a sine wave, oscillating between −170 volts and 170 volts, giving an effective voltage of 120 volts RMS. Three-phase power is more efficient in terms of power delivered per cable used, and is more suited to running large electric motors. Some large European appliances may be powered by three-phase power, such as electric stoves and clothes dryers.\n\nA ground connection is normally provided for the customer's system as well as for the equipment owned by the utility. The purpose of connecting the customer's system to ground is to limit the voltage that may develop if high voltage conductors fall down onto lower-voltage conductors which are usually mounted lower to the ground, or if a failure occurs within a distribution transformer. Earthing systems can be TT, TN-S, TN-C-S or TN-C.\n\nMost of the world uses 50 Hz 220 or 230 V single phase, or 400V 3 phase for residential and light industrial services. In this system, the primary distribution network supplies a few substations per area, and the 230V / 400V power from each substation is directly distributed to end users over a region of normally less than 1 km radius. Three live (hot) wires and the neutral are connected to the building for a three phase service. Single-phase distribution, with one live wire and the neutral is used domestically where total loads are light. In Europe, electricity is normally distributed for industry and domestic use by the three-phase, four wire system. This gives a phase-to-phase voltage of wye service and a single-phase voltage of between any one phase and neutral. In the UK a typical urban or suburban low-voltage substation would normally be rated between 150 kVA and 1 MVA and supply a whole neighbourhood of a few hundred houses. Transformers are typically sized on an average load of 1 to 2 kW per household, and the service fuses and cable is sized to allow any one property to draw a peak load of perhaps ten times this.\nFor industrial customers, 3-phase is also available, or may be generated locally. \nLarge industrial customers have their own transformer(s) with an input from 11 kV to 220 kV.\n\nMost of the Americas use 60 Hz AC, the 120/240 volt split phase system domestically and three phase for larger installations. North American transformers usually power homes at 240 volts, similar to Europe's 230 volts. It is the split-phase that allows use of 120 volts in the home.\n\nIn the electricity sector in Japan, the standard voltage is 100V, with both 50 and 60 Hz AC frequencies being used. Parts of the country use 50 Hz, while other parts use 60 Hz. This is a relic of the 1800s. Some local providers in Tokyo imported 50 Hz German equipment, while the local power providers in Osaka brought in 60 Hz generators from the United States. The grids grew until eventually the entire country was wired. Today the frequency is 50 Hz in Eastern Japan (including Tokyo, Yokohama, Tohoku, and Hokkaido) and 60 Hertz in Western Japan (including Nagoya, Osaka, Kyoto, Hiroshima, Shikoku, and Kyushu).\n\nMost household appliances are made to work on either frequency. The problem of incompatibility came into the public eye when the 2011 Tōhoku earthquake and tsunami knocked out about a third of the east’s capacity, and power in the west couldn’t be fully shared with the east, since the country does not have a common frequency.\n\nThere are four high-voltage direct current (HVDC) converter stations that move power across Japan’s AC frequency border. Shin Shinano is a back-to-back HVDC facility in Japan which forms one of four frequency changer stations that link Japan's western and eastern power grids. The other three are at Higashi-Shimizu, Minami-Fukumitsu and Sakuma Dam. Together they can move up to 1.2 GW of power east or west.\n\nMost modern North American homes are wired to receive 240 volts from the transformer, and through the use of split-phase electrical power, can have both 120 volt receptacles and 240 volt receptacles. The 120 volts is typically used for lighting and most wall outlets. The 240 volt outlets are usually located to service the oven and stovetop, water heater, and clothes dryer (if they are electric, rather than using natural gas). Sometimes a 240 volt outlet is mounted in the garage for machinery or for charging an electric car.\n\n"}
{"id": "8296169", "url": "https://en.wikipedia.org/wiki?curid=8296169", "title": "Fibre saturation point", "text": "Fibre saturation point\n\nFibre saturation point is a term used in wood mechanics and especially wood drying, to denote the point in the drying process at which only water bound in the cell walls remains - all other water, called free water, having been removed from the cell cavities. Further drying of the wood results in strengthening of the wood fibres, and is usually accompanied by shrinkage. Wood is normally dried to a point where it is in equilibrium with the atmospheric moisture content or relative humidity, and since this varies so does the equilibrium moisture content.\nLaboratory testing has found the average FSP in many types of wood to be approximately 26%. Individual species may differ from the average.\n"}
{"id": "537264", "url": "https://en.wikipedia.org/wiki?curid=537264", "title": "Fractional freezing", "text": "Fractional freezing\n\nFractional freezing is a process used in process engineering and chemistry to separate substances with different melting points. It can be done by partial melting of a solid, for example in zone refining of silicon or metals, or by partial crystallization of a liquid, as in freeze distillation, also called normal freezing or progressive freezing. The initial sample is thus fractionated (separated into fractions).\n\nPartial crystallization can also be achieved by adding a dilute solvent to the mixture, and cooling and concentrating the mixture by evaporating the solvent, a process called solution crystallization. Fractional freezing is generally used to produce ultra-pure solids, or to concentrate heat-sensitive liquids.\n\nFreeze distillation is a misnomer, because it is not distillation but rather a process of enriching a solution by partially freezing it and removing frozen material that is poorer in the dissolved material than is the liquid portion left behind. Such enrichment parallels enrichment by true distillation, where the evaporated and re-condensed portion is richer than the liquid portion left behind.\n\nThe detailed situation is the subject of thermodynamics, a subdivision of physics of importance to chemistry. Without resorting to mathematics, the following can be said for a mixture of water and alcohol:\nThe best-known freeze-distilled beverages are applejack and ice beer. Ice wine is the result of a similar process, but in this case, the freezing happens \"before\" the fermentation, and thus it is sugar, not alcohol, that gets concentrated. For an in-depth discussion of the physics and chemistry, see eutectic point.\n\nWhen a pure solid is desired, two possible situations can occur. If the contaminant is soluble in the desired solid, a multiple stage fractional freezing is required, analogous to multistage distillation. If, however, a eutectic system forms (analogous to an azeotrope in distillation), a very pure solid can be recovered, as long as the liquid is not at its eutectic composition (in which case a mixed solid forms, which can be hard to separate) or above its eutectic composition (in which case the undesired solid forms).\n\nWhen the requirement is to concentrate a liquid phase, fractional freezing can be useful due to its simplicity. Fractional freezing is also used in the production of fruit juice concentrates and other heat-sensitive liquids, as it does not involve heating the liquid (as happens during evaporation).\n\nFractional freezing can be used to desalinate sea water. In a process that naturally occurs with sea ice, frozen salt water, when partially melted, leaves behind ice that is of a much lower salt content. Because sodium chloride lowers the melting point of water, the salt in sea water tends to be forced out of pure water while freezing, called brine rejection. Large lakes of higher salinity water, called polynas, form in the middle of floes, and the water eventually sinks to the bottom. Likewise, the frozen water with the highest concentration of salt melts first. Either method decreases the salinity of the remaining frozen water, and after multiple runs the results can be drinkable.\n\nFractional freezing can be used as a simple method to increase the alcohol concentration in fermented alcoholic beverages, a process sometimes called freeze distillation. Examples are applejack, made from hard cider, and ice beer. In practice, while not able to produce an alcohol concentration comparable to distillation, this technique can achieve some concentration with far less effort than any practical distillation apparatus would require. The danger of freeze distillation of alcoholic beverages, is that unlike heat distillation, where the methanol and other impurities can be separated from the finished product, freeze distillation does not remove them. Thus the ratio of impurities may be increased compared to the total volume of the beverage. This concentration may cause side effects to the drinker, leading to intense hangovers and a condition known as \"apple palsy\" (although this term has also simply been used to refer to intoxication, especially from applejack.)\n\nFractional freezing is commonly used as a simple method to reduce the gel point of biodiesel and other alternative diesel fuels, whereby esters of higher gel point are removed from esters of lower gel point through cold filtering, or other methods to reduce the subsequent alternative fuel gel point of the fuel blend. This process employs fuel stratification whereby components in the fuel blend develop a higher specific gravity as they approach their respective gel points and thus sink to the bottom of the container, where they can be removed.\n\n\n"}
{"id": "226403", "url": "https://en.wikipedia.org/wiki?curid=226403", "title": "Gelignite", "text": "Gelignite\n\nGelignite (), also known as blasting gelatin or simply jelly, is an explosive material consisting of collodion-cotton (a type of nitrocellulose or gun cotton) dissolved in either nitroglycerine or nitroglycol and mixed with wood pulp and saltpetre (sodium nitrate or potassium nitrate).\n\nIt was invented in 1875 by Swedish chemist Alfred Nobel, who also invented dynamite. Unlike dynamite, gelignite does not suffer from the dangerous problem of \"sweating\", the leaking of unstable nitroglycerine from the solid matrix. Its composition makes it easily moldable and safe to handle without protection, as long as it is not near anything capable of detonating it. One of the cheapest explosives, it burns slowly and cannot explode without a detonator, so it can be stored safely.\n\nIn the United Kingdom, an explosives certificate, issued by the local Chief Officer of Police, is required for possession of gelignite. Due to its widespread civilian use in quarries and mining, it has historically been often used by irregular or paramilitary groups such as the Irish Republican Army and, less frequently, by British loyalists.\n\nThe 1970s saw Irish Industrial Explosives Limited producing annually 6000 tonnes of Frangex, a commercial gelignite intended for use in mines and quarries. It was produced at Ireland's largest explosives factory in Enfield, County Meath. The Gardaí and the Irish Army patrolled the area, preventing the IRA from gaining direct access. However, indirectly the Provisional Irish Republican Army (PIRA) acquired amounts of the material. was found in the possession of Patrick Magee at the time of his arrest and discovered in a hijacked road tanker in January 1976. \n\nPIRA volunteer, later informer, Sean O'Callaghan estimated that planting of Frangex would kill everyone within a radius. The Real IRA (RIRA) also acquired Frangex, and, in December 2000, 80 sticks were discovered on a farm in Kilmacow, County Kilkenny, near Waterford.\n\nOn September 12, 2015, two powerful explosions took place in a restaurant during breakfast hours in Jhabua district of Madhya Pradesh, India. The authorities estimated that at least 89 people died in the explosions, more than 100 were injured, including several at a nearby bus stop. Several adjacent buildings sustained severe damage.\n"}
{"id": "41186031", "url": "https://en.wikipedia.org/wiki?curid=41186031", "title": "Gypsum recycling", "text": "Gypsum recycling\n\nGypsum recycling is the process of turning gypsum waste into recycled gypsum, thereby generating a raw material that can replace virgin gypsum raw materials in the manufacturing of new products.\n\nGypsum waste primarily consists of waste from gypsum boards, which are wall or ceiling panels made of a gypsum core between paper lining. Such boards are also referred to as plasterboards, drywall, wallboards and gyprock. Gypsum waste in some countries also consists of gypsum blocks and plaster, among others.\n\nThree main types of gypsum waste based on their origin can be distinguished:\n\n\n\nThe recycled gypsum obtained from this is known as post-consumer recycled gypsum.\n\nGypsum waste can be turned into recycled gypsum by processing the gypsum waste in such a way that the contaminants are removed and the paper facing of the plasterboard is separated from the gypsum core through mechanical processes including grinding and sieving in specialised equipment. Gypsum waste such as gypsum blocks and plaster do not require the removal of paper, as they are not made with paper from the beginning.\nIt is typical for the gypsum recyclers to accept up to 3 per cent of contamination from other materials. The professional recyclers are capable of handling gypsum waste with nails and screws, wall coverings etc.\n\nGypsum materials consist of calcium sulfate dihydrate (CaSO4·2H2O). Sulfate-reducing bacteria convert sulfates to toxic hydrogen sulphide gas; they are killed by exposure to air, but the moist, airless, carbon-containing environment in a landfill is a good habitat for them. So gypsum put into landfill will decompose, releasing up to a quarter its weight in hydrogen sulfide. Moreover, methanogenic bacteria also thrive in such an environment, and convert the paper in the plasterboard to methane gas which is a potent greenhouse gas.\n\nRecycling gypsum waste also reduces the need for the quarrying and production of virgin gypsum raw materials.\n\nRecycling one ton of the ordinary gypsum will save 1,000 pounds of black alkali, 1 ton of lactic acid and 500 kwh of energy.\n\nRecycling one metric ton of gypsum will save 28 kwh of energy and 4 pounds of aluminium.\n\nGypsum is fully and eternally recyclable and, as a consequence, gypsum waste is one of the few construction materials for which closed loop recycling is possible.\n\nClosed loop recycling of gypsum products involves the collection and processing of the gypsum waste, and the delivery of the obtained recycled gypsum to the manufacturer of gypsum products. It is therefore essential that the recycled gypsum achieves a pre-determined quality suitable for the manufacturing of new gypsum products. Presently there is no European or American standard pre-determining the recycled gypsum's quality and the criteria vary from plant to plant.\n\nBy choosing closed loop recycling the need for manufacturers to acquire virgin gypsum is reduced, contributing therefore to promote a sustainable manufacturing process.\n\nThe most advanced plants, and most of these are found in the Nordic countries in Europe, have substituted up to 30 per cent of virgin gypsum raw materials with recycled gypsum.\n\nGypsum recycling in Europe was started by the Danish company Gypsum Recycling International A/S in Denmark, in 2001. After a few years the recycling system received waste from approximately 85 per cent of all public civic amenity/recycling centres and a recycling rate of 60 per cent of all gypsum waste was achieved. The system has been exported to cover other European countries.\nToday also new recyclers have emerged and gypsum recycling systems have been introduced in more countries, like the UK, France and in the Benelux, but the highest recycling rates for gypsum waste are still found in Denmark, Norway and Sweden. \nJanuary 1, 2013 the European Life + project “Gypsum to Gypsum” started, with the overall aim of transforming the gypsum demolition waste market to achieve higher recycling rates of gypsum waste, thereby helping to achieve a resource efficient economy. One of the drivers for the project is the target set by the European Union to achieve that 70 per cent of construction and demolition waste is recycled by 2020.\n\nNew West Gypsum Recycling began recycling of wallboard waste in Canada in 1985. The recycled material is a blend of pre- and post-consumer, wet and dry gypsum waste that is a source of raw material for use in the manufacture of new drywall products. Gypsum Agri-cycle is one of the first companies to recycle drywall in the USA. Gypsum Agri-cycle is another North American recycler of new construction drywall located in Pennsylvania. Pennsylvania does not allow Gypsum Agri-cycle to recycle demolition drywall.\n\nZanker Recycling began recycling gypsum in the form of sheetrock in 1999. In the recycling process, materials such as wood, metals, and trash are removed on-site where a dozer is used to crush the materials. \n\nAmerican Gypsum Recycling\nAmerican Gypsum Recycling was founded in 2018 by Chris Stapleton. His vision for the company is to transform the Northwest drywall waste stream into a valuable product for agriculture and industry.\n\nUSA Gypsum located in Denver, PA provides both closed loop recycling and up-cycling reclaimed gypsum to higher value gypsum products such as agricultural gypsum.\n\n"}
{"id": "8749261", "url": "https://en.wikipedia.org/wiki?curid=8749261", "title": "Hepoxilin", "text": "Hepoxilin\n\nHepoxilins (Hx) are a set of epoxyalcohol metabolites of polyunsaturated fatty acids (PUFA), i.e. they possess both an epoxide and an alcohol (i.e. hydroxyl) residue. HxA3, HxB3, and their non-enzymatically formed isomers are nonclassic eicosanoid derived from acid the (PUFA), arachidonic acid. A second group of less well studied hepoxilins, HxA4, HxB4, and their non-enzymatically formed isomers are nonclassical eicosanoids derived from the PUFA, eicosapentaenoic acid. Recently, 14,15-HxA3 and 14,15-HxB3 have been defined as arachidonic acid derivatives that are produced by a different metabolic pathway than HxA3, HxB3, HxA4, or HxB4 and differ from the aforementioned hepoxilins in the positions of their hydroxyl and epoxide residues. Finally, hepoxilin-like products of two other PUFAs, docosahexaenoic acid and linoleic acid, have been described. All of these epoxyalcohol metabolites are at least somewhat unstable and are readily enzymatically or non-enzymatically to their corresponding trihydroxy counterparts, the trioxilins (TrX). HxA3 and HxB3, in particular, are being rapidly metabolized to TrXA3, TrXB3, and TrXC3. Hepoxilins have various biological activities in animal models and/or cultured mammalian (including human) tissues and cells. The TrX metabolites of HxA3 and HxB3 have less or no activity in most of the systems studied but in some systems retain the activity of their precursor hepoxilins. Based on these studies, it has been proposed that the hepoxilins and trioxilins function in human physiology and pathology by, for example, promoting inflammation responses and dilating arteries to regulate regional blood flow and blood pressure.\n\nHxA3 and HxB3 were first identified, named, shown to have biological activity in stimulating insulin secretion in cultured rat pancreatic islets of Langerhans in Canada in 1984 by CR Pace-Asciak and JM Martin. Shortly thereafter, Pace-Asciak identified, named, and showed to have insulin secretagogue activity HxA4 and HxB4.\n\nHxA3, HxB3, and their isomers are distinguished from most other eicosanoids (i.e. signaling molecules made by oxidation of 20-carbon fatty acids) in that they contain both epoxide and hydroxyl residues; they are structurally differentiated in particular from two other classes of arachidonic acid-derived eicosanoids, the leukotrienes and lipoxins, in that they lack conjugated double bonds. HxA4 and HxB4 are distinguished from HxA3 and HxB3 by possessing four rather than three double bonds. The 14,15-HxA3 and 14,15-HxB3 non-classical eicosanoids are distinguished from the aforementioned hepoxilins in that they are formed by a different metabolic pathway and differ in the positioning of their epoxide and hydroxyl residues. Two other classes of epoxyalcohol fatty acids, those derived from the 22-carbon polyunsaturated fatty acid, docosahexaenoic acid, and the 18-carbon fatty acid, linoleic acid, are distinguished from the aforementioned hepoxilins by their carbon chain length; they are termed hepoxilin-like rather than hepoxilins. A hepoxilin-like derivative of linoleic acid is formed on linoleic acid that is esterified to a sphingosine in a complex lipid termed esterified omega-hydroxylacyl-sphingosin (EOS).\n\nThe full structural identities of the hepoxilins and hepoxilin-like compounds in most studies are unclear in two important respects. First, the \"R\" versus \"S\" chirality of their hydroxy residue in the initial and most studies thereafter is undefined and therefore given with, for example, HxB3 as 10\"R/S\"-hydroxy or just 10-hydroxy. Second, the \"R\",\"S\" versus \"S\",\"R\" chirality of the epoxide residue in these earlier studies likewise goes undefined and given with, for example, HxB3 as 11,12-epoxide. While some later studies have defined the chirality of these residues for the products they isolated, it is often not clear that the earlier studies dealt with products that had exactly the same or a different chirality at these residues.\n\nHuman HxA3 and HxB3 are formed in a two-step reaction. First, molecular oxygen (O) is added to carbon 12 of arachidonic acid (i.e. 5Z,8Z,11Z,14Z-eicosatetraenoic acid) and concurrently the 8\"Z\" double bond in this arachidonate moves to the 9\"E\" position to form the intermediate product, 12\"S\"-hydroperoxy-5Z,8Z,10E,14Z-eicosatetraenoic acid (i.e. 12\"S\"-hydroperoxyeicosatetraenoic acid or 12\"S\"-HpETE). Second, 12\"S\"-HpETE is converted to the hepoxilin products, HxA3 (i.e. 8\"R/S\"-hydroxy-11,12-oxido-5\"Z\",9\"E\",14\"Z\"-eicosatrienoic acid) and HxB3 (i.e. 10\"R/S\"-hydroxy-11,12-oxido-5\"Z\",8\"Z\",14\"Z\"-eicosatrienoic acid). This two-step metabolic reaction is illustrated below:\n5Z,8Z,11Z,14Z-eicosatetraenoic acid + O → 12\"S\"-hydroperoxy-5Z,8Z,10E,14Z-eicosatetraenoic acid → 8\"R/S\"-hydroxy-11,12-oxido-5\"Z\",9\"E\",14\"Z\"-eicosatrienoic acid + 10\"R/S\"-hydroxy-11,12-oxido-5\"Z\",8\"Z\",14\"Z\"-eicosatrienoic acid\n\nThe second step in this reaction, the conversion of 12(\"S\")-HpETE to HxA3 and HxB3, may be catalyzed by ALOX12 as an intrinsic property of the enzyme. Based on gene knockout studies, however, the epidermal lipoxygenase, ALOXE3, or more correctly, its mouse ortholog Aloxe3, appears responsible for converting 12(\"S\")-HpETE to HxB3 in mouse skin and spinal tissue. It is suggested that ALOXE3 contributes in part or whole to the production of HxB3 and perhaps other hepoxilins by tissues where it is expressed such as the skin. Furthermore, hydroperoxide-containing unsaturated fatty acids can rearrange non-enzymatically to form a variety of epoxyalcohol isomers. The 12(\"S\")-HpETE formed in tissues, it is suggested, may similar rearrange non-enzymatically to form HxA3 and HXB3. Unlike the products made by ALOX12 and ALOXE3, which are stereospecific in forming only HxA3 and HxB3, however, this non-enzymatic production of hepoxilins may form a variety of hepoxilin isomers and occur as an artifact of tissue processing. Finally, cellular peroxidases readily and rapidly reduce 12(\"S\")-HpETE to its hydroxyl analog, 12\"S\"-hydroxy-5Z,8Z,10E,14Z-eicosatetraenoic acid (12\"S\"-HETE; see 12-hydroxyeicosatetraenoic acid; this reaction competes with the hepoxilin-forming reaction and in cells expressing very high peroxidase activity may be responsible for blocking the formation of the hepoxilins.\n\nALOX15 is responsible for metabolizing arachidonic acid to 14,15-HxA3 and 14,15-HxB3 as indicated in the following two-step reaction which first forms 15(\"S\")-hydroperoxy-5\"Z\",8\"Z\",11\"Z\",13\"E\"-eicosatetraenoic acid (15\"S\"-HpETE) and then two specific isomers of 11\"S/R\"-hydroxy-14\"S\",15\"S\"-epoxy-5\"Z\",8\"Z\",12\"E\"-eicosatrienoic acid (i.e. 14,15-HxA3) and 13\"S/\"R)-hydroxy-14\"S\",15\"S\"-epoxy-5\"Z\",8\"Z\",11\"Z\"-eicosatrienoic acid (i.e. 14,15-HxB3):\n\n5Z,8Z,11Z,14Z-eicosatetraenoic acid + O → 15(\"S\")-hydroperoxy-5\"Z\",8\"Z\",11\"Z\",13\"E\"-eicosatetraenoic acid → 11\"R\"-hydroxy-14\"S\",15 \"S\"-epoxy-5\"Z\",8\"Z\",12\"E\"-eicosatrienoic acid and 13\"R\"-hydroxy-14\"S\",15\"S\"-epoxy-5\"Z\",8\"Z\",11\"Z\"-eicosatrienoic acid\nALOX15 appears capable of conducting both steps in this reaction although further studies may show that ALOXE3, non-enzymatic rearrangements, and the reduction of 15\"S\"-HpETE to 15(\"S\")-hydroxy-5\"Z\",8\"Z\",11\"Z\",13\"E\"-eicosatetraenoic acid (i.e. 15\"S\"-HETE; see 15-hydroxyicosatetraenoic acid) may be involved in the production of 14,15-HxA3 and 14,15-HxB3 as they are in that of HxA3 and HxB3.\n\nProduction of the hepoxilin-like metabolites of docosahexaenoic acid, 7\"R/S\"-hydroxy-10,11-epoxy-4\"Z\",7\"E\",13\"Z\",16\"Z\",19\"Z\"-docosapentaenoic acid (i.e. 7-hydroxy-bis-α-dihomo-HxA5) and 10-hydroxy-13,14-epoxy-4\"Z\",7\"EZ\",11\"E\",16\"Z\",19\"Z\"-docosapentaenoic acid (i.e. 10-hydroxy-bis-α-dihomo-HxA5) was formed (or inferred to be formed based on the formation of their tihydroxy metabolites (see trioxilins, below) as a result of adding docosahexaenoic acid to the pineal gland or hippocampus isolated from rats; the pathway(s) making these products has not been described.\n\nA hepoxilin-like metabolite of linoleic acid forms in the skin of humans and rodents. This hepoxilin is esterified to sphinganine in a lipid complex termed EOS (i.e. esterified omega-hydroxyacyl-sphingosine, see Lipoxygenase#Biological function and classification#Human lipoxygenases) that also contains a very long chain fatty acid. In this pathway, ALOX12B metabolizes the esterified linoleic acid to its 9\"R\"-hydroperoxy derivative and then ALOXE3 metabolizes this intermediate to its 13\"R\"-hydroxy-9\"R\",10\"R\"-epoxy product. The pathway functions to deliver very long chain fatty acids to the cornified lipid envelope of the skin surface.\n\nHxA3 is extremely unstable and HxB3 is moderately unstable, rapidly decomposing to their tri-hydroxy products, for example, during isolation procedures that use an even mildly acidic methods; they are also rapidly metabolized enzymatically in cells to these same tri-hydroxy products, termed trioxilins (TrX's) or trihydroxyeicoxatrienoic acids (THETA's); HxA3 is converted to 8,11,12-trihydroxy-5\"Z\",9\"E\",14\"Z\"-eicosatrienoic acid (trioxilin A3 or TrXA3) while TxB3 is converted to 10,11,12-trihydroxy-5\"Z\",8\"Z\",14\"Z\"-eicosatrienoic acid (trioxilin B3 or TrXB3). A third trihydroxy acid, 8,9,12-trihydroxy-5\"Z\",10\"E\",14\"Z\" eicosatrienoic acid (trioxilin C3 or TrXC3), has been detected in rabbit and mouse aorta tissue incubated with arachidonic acid. The metabolism of HxA3 to TrXA3 and HXB3 to TrX is accomplished by soluble epoxide hydrolase in mouse liver; since it is widely distributed in various tissues of various mammalian species, including humans, soluble epoxide hydrolase may be the principal enzyme responsible for metabolizing these and perhaps other hepoxilin compounds. It seems possible, however, that other similarly acting epoxide hydrolases such as microsomal epoxide hydrolase or epoxide hydrolase 2 may prove to hepoxilin hydrolase activity. While the trihydroxy products of hepoxilin synthesis are generally considered to be inactive and the sEH pathway therefore considered as functioning to limiting the actions of the hepoxilins, some studies found that TrXA3, TrXB3, and TrXC3 were more powerful than HxA3 in relaxing pre-contracted mouse arteries and that TrXC3 was a relatively potent relaxer of rabbit pre-contracted aorta.\n\nHxA3 was converted through a Michael addition catalyzed by glutathione transferase to its glutathione conjugate, HxA3-C, i.e., 11-glutathionyl-HxA3, in a cell-free system or in homogenates of rat brain hippocampus tissue; HxA3-C proved to be a potent stimulator of membrane hyperpolarization in rat hippocampal CA1 neurons. This formation of hepoxilin A3-C appears analogous to the formation of leukotriene C4 by the conjugation of glutathione to leukotriene A4. Glutathione conjugates of 14,15-HxA3 and 14,15-HxB3 have also been detected the human Hodgkin disease Reed–Sternberg cell line, L1236.\n\nHxB3 and TrX3 are found esterified into the \"sn\"-2 position of phospholipid in human psoriasis lesions and samples of human psoriatic skin acylate HxBw and TrX2 into these phospholipids in vitro.\n\nVirtually all of the biological studies on hepoxilins have been conducted in animals or in vitro on animal and human tissues, However, these studies give species-specific different results which complicate their relevancy to humans. The useful translation of these studies to human physiology, pathology, and clinical medicine and therapies requires much further study.\n\nHxA3 and HxB3 possess pro-inflammatory actions in, for example, stimulating human neutrophil chemotaxis and increasing the permeability of skin capillaries. Studies in humans have found that the amount of HxB3 is >16-fold higher in psoriatic lesions than normal epidermis. It is present in psoriatic scales at ~10 micromolar, a concentration which is able to exert biologic effects; HxB3 was not detected in these tissues although its present was strongly indicated by the presence of its metabolite, TrXB3, at relatively high levels in psoriatic scales but not normal epidermal tissue. These results suggest that the pro-inflammatory effects of HxA3 and HxB3 may contribute to the inflammatory response that accompanies psoriasis and perhaps other inflammatory skin conditions. HxA3 has also been implicating in promoting the neutrophil-based inflammatory response to various bacteria in the intestines and lungs of rodents.; this allows that this hepoxilin may also promote the inflammatory response of humans in other tissues, particularly those with a mucosa surface, besides the skin. In addition, HxA3 and a synthetic analog of HxB3, PBT-3, induce human neutrophils to produce neutrophil extracellular traps, i.e. DNA-rich extracellular fibril matrixes able to kill extracellular pathogens while minimizing tissue; hence these hepoxilins may contribute to innate immunity by being responsible of the direct killing of pathogens.\n\nIn addition to 12\"S\"-HETE and 12\"R\"-HETE (see 12-HETE#Blood pressure), HxA3, TrXA3, and TrXC3 but neither HxB3 nor TrXB3 relax mouse mesentery arteries pre-contracted by thromboxane A2)(TXA2). Mechanistically, these metabolites form in the vascular endothelium, move to the underlining smooth muscle, and reverse the smooth muscle contraction caused by TXA2 by functioning as a Receptor antagonist, i.e. they competitively inhibit the binding of TXA2 to its thromboxane receptor, α isoform. Contrastingly, 15-lipoxgenase-derived epoxyalcohol and trihydroxy metabolites of arachidonic acid viz., 15-hydroxy-11,12-epoxyeicosatrienoic acid, 13-hydroxy-14,15-epoxy-eicosatrienoic acid (a 14,15-HxA4 isomer), and 11,12,15-trihydroxyeicosatrienoic acid dilate rabbit aorta by an Endothelium-derived hyperpolarizing factor (EDHF) mechanism, i.e. they form in the vessels endothelium, move to underlying smooth muscles, and trigger a response of Hyperpolarization (biology)-induced relaxation by binding to and thereby opening their apamin-sensitive small conductance (SK) Calcium-activated potassium channel#SK channels. The cited metabolites may use one or the other of these two mechanisms in different vascular beds and in different animal species to contribute in regulating regional blood flow and blood pressure. While the role of these metabolites in the human vasculature has not been studied, 12\"S\"-HETE, 12\"R\"-HETE, HxA3, TrXA3, and TrXC3 do inhibit the binding of TXA2 to the human thromboxane receptor.\n\nHXA3 and HXB3 appear responsible for hyperalgesia and tactile allodynia (pain caused by a normally non-painful stimulus) response of mice to skin inflammation. In this model, the hepoxilins are released in spinal cord and directly activate TRPV1 and TRPA1 receptors to augment the perception of pain. TRPV1 (the transient receptor potential cation channel subfamily V member 1 (TrpV1), also termed the capsaicin receptor or vanilloid receptor) and TRPA1 (Transient receptor potential cation channel, member A1) are plasma membrane ion channels on cells; these channels are known to be involved in the perception of pain caused by exogenous and endogenous physical and chemical stimuli in a wide range of animal species including humans.\n\nCultured rat RINm5F pancreatic islet cells undergoing oxidative stress secrete HxB3; HxB3 (and HxA3) in turn upregulates peroxidase enzymes which act to decrease this stress; it is proposed that this HxB3-triggered induction of oxidases constitutes a general compensatory defense response used by a variety of cells to protect their vitality and functionality.\n\nThe insulin-secreting actions of HxA3 and HxB3 on isolate rat pancreatic islet cells involves their ability to increase or potentiate the insulin-secreting activity of glucose, requires very high concentrations (e.g. 2 micromolar) of the hepoxilins, and has not been extended to intact animals or humans.\n\nHepoxilins are also produced in the brain.\n"}
{"id": "2655717", "url": "https://en.wikipedia.org/wiki?curid=2655717", "title": "Heusler compound", "text": "Heusler compound\n\nHeusler compounds are magnetic intermetallics with face-centered cubic crystal structure and a composition of XYZ (half-Heuslers) or XYZ (full-Heuslers), where X and Y are transition metals and Z is in the p-block. Many of these compounds exhibit properties relevant to spintronics, such as magnetoresistance, variations of the Hall effect, ferro-, antiferro-, and ferrimagnetism, half- and semimetallicity, semiconductivity with spin filter ability, superconductivity, and topological band structure. Their magnetism results from a double-exchange mechanism between neighboring magnetic ions. Manganese, which sits at the body centers of the cubic structure, was the magnetic ion in the first Heusler compound discovered. (See the Bethe-Slater curve for details of why this happens.)\n\nThe term derives from the name of German mining engineer and chemist Friedrich Heusler, who studied such a compound in 1903. It contained two parts copper, one part manganese, and one part tin, that is CuMnSn, and has the following properties. Its magnetism varies considerably with heat treatment and composition. It has a room-temperature saturation induction of around 8,000 gauss, which exceeds that of the element nickel (around 6100 gauss) but is smaller than that of iron (around 21500 gauss). For early studies see. In 1934, Bradley and Rogers showed that the room-temperature ferromagnetic phase was a fully ordered structure of the L2 type. This has a primitive cubic lattice of copper atoms with alternate cells body-centered by manganese and aluminium. The lattice parameter is 5.95 Å. The molten alloy has a solidus temperature of about 910 °C. As it is cooled below this temperature, it transforms into disordered, solid, body-centered cubic beta-phase. Below 750 °C, a B2 ordered lattice forms with a primitive cubic copper lattice, which is body-centered by a disordered manganese-aluminium sublattice. Cooling below 610 °C causes further ordering of the manganese and aluminium sub-lattice to the L2 form. In non-stoichiometric alloys, the temperatures of ordering decrease, and the range of anealing temperatures, where the alloy does not form microprecipitates, becomes smaller than for the stoichiometric material.\n\nOxley found a value of 357 °C for the Curie temperature, below which the compound becomes ferromagnetic. Neutron diffraction and other techniques have shown that a magnetic moment of around 3.7 Bohr magnetons resides almost solely on the manganese atoms. As these atoms are 4.2 Å apart, the exchange interaction, which aligns the spins, is likely indirect and is mediated through conduction electrons or the aluminium and copper atoms.\n\nElectron microscopy studies demonstrated that thermal antiphase boundaries (APBs) form during cooling through the ordering temperatures, as ordered domains nucleate at different centers within the crystal lattice and are often out of step with each other where they meet. The anti-phase domains grow as the alloy is annealed. There are two types of APBs corresponding to the B2 and L2 types of ordering. APBs also form between dislocations if the alloy is deformed. At the APB the manganese atoms will be closer than in the bulk of the alloy and, for non-stoichiometric alloys with an excess of copper (e.g. CuMnAl), an antiferromagnetic layer forms on every thermal APB. These antiferromagnetic layers completely supersede the normal magnetic domain structure and stay with the APBs if they are grown by annealing the alloy. This significantly modifies the magnetic properties of the non-stoichiometric alloy relative to the stoichiometric alloy which has a normal domain structure. Presumably this phenomenon is related to the fact that pure manganese is an antiferromagnet although it is not clear why the effect is not observed in the stoichiometric alloy. Similar effects occur at APBs in the ferromagnetic alloy MnAl at its stoichiometric composition.\n\nSome Heusler compounds also exhibit properties of materials known as ferromagnetic shape-memory alloys. These are generally composed of nickel, manganese and gallium and can change their length by up to 10% in a magnetic field.\n\n\n\n"}
{"id": "951197", "url": "https://en.wikipedia.org/wiki?curid=951197", "title": "History of the electric vehicle", "text": "History of the electric vehicle\n\nElectric vehicles first appeared in the mid-19th century. An electric vehicle held the vehicular land speed record until around 1900. The high cost, low top speed, and short range of battery electric vehicles, compared to later internal combustion engine vehicles, led to a worldwide decline in their use; although electric vehicles have continued to be used in the form of electric trains and other niche uses.\n\nAt the beginning of the 21st century, interest in electric and other alternative fuel vehicles has increased due to growing concern over the problems associated with hydrocarbon-fueled vehicles, including damage to the environment caused by their emissions, and the sustainability of the current hydrocarbon-based transportation infrastructure as well as improvements in electric vehicle technology. Since 2010, combined sales of all-electric cars and utility vans achieved 1 million units delivered globally in September 2016, and combined global sales of light-duty all-electrics and plug-in hybrids reached 4 million in September 2018.\n\nThe invention of the first model electric vehicle is attributed to various people. In 1828, Ányos Jedlik, a Hungarian who invented an early type of electric motor, created a small model car powered by his new motor. In 1834, Vermont blacksmith Thomas Davenport built a similar contraption which operated on a short, circular, electrified track. In 1834, Professor Sibrandus Stratingh of Groningen, the Netherlands and his assistant Christopher Becker created a small-scale electrical car, powered by non-rechargeable primary cells.\n\nThe first known electric locomotive was built in 1837, in Scotland by chemist Robert Davidson of Aberdeen. It was powered by galvanic cells (batteries). Davidson later built a larger locomotive named \"Galvani\", exhibited at the Royal Scottish Society of Arts Exhibition in 1841. The vehicle had two direct-drive reluctance motors, with fixed electromagnets acting on iron bars attached to a wooden cylinder on each axle, and simple commutators. It hauled a load of at for a distance of . It was tested on the Edinburgh and Glasgow Railway in September of the following year, but the limited power from batteries prevented its general use. It was destroyed by railway workers, who saw it as a threat to their security of employment.\nBetween 1832 and 1839, Scottish inventor Robert Anderson also invented a crude electrical carriage. A patent for the use of rails as conductors of electric current was granted in England in 1840, and similar patents were issued to Lilley and Colten in the United States in 1847.\n\nRechargeable batteries that provided a viable means for storing electricity on board a vehicle did not come into being until 1859, with the invention of the lead–acid battery by French physicist Gaston Planté. Camille Alphonse Faure, another French scientist, significantly improved the design of the battery in 1881; his improvements greatly increased the capacity of such batteries and led directly to their manufacture on an industrial scale.\n\nAn early electric-powered two-wheel cycle was put on display at the 1867 World Exposition in Paris by the Austrian inventor Franz Kravogl, but it was regarded as a curiosity and could not drive reliably in the street. Another cycle, this time with three wheels, was tested along a Paris street in April 1881 by French inventor Gustave Trouvé \n\nEnglish inventor Thomas Parker, who was responsible for innovations such as electrifying the London Underground, overhead tramways in Liverpool and Birmingham, and the smokeless fuel coalite, built the first production electric car in London in 1884, using his own specially designed high-capacity rechargeable batteries. Parker's long-held interest in the construction of more fuel-efficient vehicles led him to experiment with electric vehicles. He also may have been concerned about the malign effects smoke and pollution were having in London.\n\nProduction of the car was in the hands of the Elwell-Parker Company, established in 1882 for the construction and sale of electric trams. The company merged with other rivals in 1888 to form the Electric Construction Corporation; this company had a virtual monopoly on the British electric car market in the 1890s. The company manufactured the first electric 'dog cart' in 1896.\n\nFrance and the United Kingdom were the first nations to support the widespread development of electric vehicles. The first electric car in Germany was built by the engineer Andreas Flocken in 1888.\n\nElectric trains were also used to transport coal out of mines, as their motors did not use up precious oxygen. Before the pre-eminence of internal combustion engines, electric automobiles also held many speed and distance records. Among the most notable of these records was the breaking of the speed barrier, by Camille Jenatzy on 29 April 1899 in his 'rocket-shaped' vehicle Jamais Contente, which reached a top speed of . Also notable was Ferdinand Porsche's design and construction of an all-wheel drive electric car, powered by a motor in each hub, which also set several records in the hands of its owner E.W. Hart.\n\nThe first electric car in the United States was developed in 1890-91 by William Morrison of Des Moines, Iowa; the vehicle was a six-passenger wagon capable of reaching a speed of . It was not until 1895 that consumers began to devote attention to electric vehicles, after A.L. Ryker introduced the first electric tricycles to the U.S., by which point Europeans had been making use of electric tricycles, bicycles, and cars for almost 15 years.\n\nInterest in motor vehicles increased greatly in the late 1890s and early 1900s.\nElectric battery-powered taxis became available at the end of the 19th century. In London, Walter C. Bersey designed a fleet of such cabs and introduced them to the streets of London in 1897. They were soon nicknamed \"Hummingbirds\" due to the idiosyncratic humming noise they made. In the same year in New York City, the Samuel's Electric Carriage and Wagon Company began running 12 electric hansom cabs. The company ran until 1898 with up to 62 cabs operating until it was reformed by its financiers to form the Electric Vehicle Company.\n\nElectric vehicles had a number of advantages over their early-1900s competitors. They did not have the vibration, smell, and noise associated with gasoline cars. They also did not require gear changes. (While steam-powered cars also had no gear shifting, they suffered from long start-up times of up to 45 minutes on cold mornings.) The cars were also preferred because they did not require a manual effort to start, as did gasoline cars which featured a hand crank to start the engine.\n\nElectric cars found popularity among well-heeled customers who used them as city cars, where their limited range proved to be even less of a disadvantage. Electric cars were often marketed as suitable vehicles for women drivers due to their ease of operation; in fact, early electric cars were stigmatized by the perception that they were \"women's cars\", leading some companies to affix radiators to the front to disguise the car's propulsion system.\n\nAcceptance of electric cars was initially hampered by a lack of power infrastructure, but by 1912, many homes were wired for electricity, enabling a surge in the popularity of the cars. In the United States by the turn of the century, 40 percent of automobiles were powered by steam, 38 percent by electricity, and 22 percent by gasoline. A total of 33,842 electric cars were registered in the United States, and the U.S. became the country where electric cars had gained the most acceptance. Most early electric vehicles were massive, ornate carriages designed for the upper-class customers that made them popular. They featured luxurious interiors and were replete with expensive materials. Sales of electric cars peaked in the early 1910s.\n\nIn order to overcome the limited operating range of electric vehicles, and the lack of recharging infrastructure, an exchangeable battery service was first proposed as early as 1896. The concept was first put into practice by Hartford Electric Light Company through the GeVeCo battery service and initially available for electric trucks. The vehicle owner purchased the vehicle from General Vehicle Company (GVC, a subsidiary of the General Electric Company) without a battery and the electricity was purchased from Hartford Electric through an exchangeable battery. The owner paid a variable per-mile charge and a monthly service fee to cover maintenance and storage of the truck. Both vehicles and batteries were modified to facilitate a fast battery exchange. The service was provided between 1910 and 1924 and during that period covered more than 6 million miles. Beginning in 1917 a similar successful service was operated in Chicago for owners of Milburn Wagon Company cars who also could buy the vehicle without the batteries.\n\nAfter enjoying success at the beginning of the 20th century, the electric car began to lose its position in the automobile market. A number of developments contributed to this situation. By the 1920s an improved road infrastructure required vehicles with a greater range than that offered by electric cars. Worldwide discoveries of large petroleum reserves led to the wide availability of affordable gasoline, making gas-powered cars cheaper to operate over long distances. Electric cars were limited to urban use by their slow speed (no more than 24–32 km/h or 15–20 mph) and low range (50–65 km or 30–40 miles), and gasoline cars were now able to travel farther and faster than equivalent electrics.\n\nGasoline cars became even easier to operate thanks to the invention of the electric starter by Charles Kettering in 1912, which eliminated the need of a hand crank for starting a gasoline engine, and the noise emitted by ICE cars became more bearable thanks to the use of the muffler, which Hiram Percy Maxim had invented in 1897. Finally, the initiation of mass production of gas-powered vehicles by Henry Ford brought their price down. By contrast, the price of similar electric vehicles continued to rise; by 1912, an electric car sold for almost double the price of a gasoline car.\n\nMost electric car makers stopped production at some point in the 1910s. Electric vehicles became popular for certain applications where their limited range did not pose major problems. Forklift trucks were electrically powered when they were introduced by Yale in 1923. In Europe, especially the United Kingdom, milk floats were powered by electricity, and for most of the 20th century the majority of the world's battery electric road vehicles were British milk floats. Electric golf carts were produced by Lektro as early as 1954. By the 1920s, the early heyday of electric cars had passed, and a decade later, the electric automobile industry had effectively disappeared. Michael Brian examines the social and technological reasons for the failure of electric cars in his book \"Taking Charge: The Electric Automobile in America\".\n\nYears passed without a major revival in the use of electric cars. Fuel-starved European countries fighting in World War II experimented with electric cars such as the British milk floats and the French Breguet Aviation car, but overall, while ICE development progressed at a brisk pace, electric vehicle technology stagnated. In the late 1950s, Henney Coachworks and the National Union Electric Company, makers of Exide batteries, formed a joint venture to produce a new electric car, the Henney Kilowatt, based on the French Renault Dauphine. The car was produced in 36-volt and 72-volt configurations; the 72-volt models had a top speed approaching and could travel for nearly an hour on a single charge. Despite the Kilowatt's improved performance with respect to previous electric cars, consumers found it too expensive compared to equivalent gasoline cars of the time, and production ended in 1961.\n\nIn 1959, American Motors Corporation (AMC) and Sonotone Corporation announced a joint research effort to consider producing an electric car powered by a \"self-charging\" battery. AMC had a reputation for innovation in economical cars while Sonotone had technology for making sintered plate nickel-cadmium batteries that could be recharged rapidly and weighed less than traditional lead-acid versions. That same year, Nu-Way Industries showed an experimental electric car with a one-piece plastic body that was to begin production in early 1960.\n\nIn the mid 1960s a few battery-electric concept cars appeared, such as the Scottish Aviation Scamp (1965), and an electric version of General Motors gasoline car, the Electrovair (1966). None of them entered production. The 1966 Enfield 8000 did make it into small-scale production, 112 were eventually produced.In 1967, AMC partnered with Gulton Industries to develop a new battery based on lithium and a speed controller designed by Victor Wouk. A nickel-cadmium battery supplied power to an all-electric 1969 Rambler American station wagon. Other \"plug-in\" experimental AMC vehicles developed with Gulton included the Amitron (1967) and the similar Electron (1977).\n\nOn 31 July 1971, an electric car received the unique distinction of becoming the first manned vehicle to drive on the Moon; that car was the Lunar rover, which was first deployed during the \"Apollo 15\" mission. The \"moon buggy\" was developed by Boeing and GM subsidiary Delco Electronics (co-founded by Kettering) featured a DC drive motor in each wheel, and a pair of 36-volt silver-zinc potassium hydroxide non-rechargeable batteries.\n\nAfter years outside the limelight, the energy crises of the 1970s and 1980s brought about renewed interest in the perceived independence electric cars had from the fluctuations of the hydrocarbon energy market. General Motors created a concept car of another of their gasoline cars, the Electrovette (1976). At the 1990 Los Angeles Auto Show, General Motors President Roger Smith unveiled the GM Impact electric concept car, along with the announcement that GM would build electric cars for sale to the public.\n\nIn the early 1990s, the California Air Resources Board (CARB), the government of California's \"clean air agency\", began a push for more fuel-efficient, lower-emissions vehicles, with the ultimate goal being a move to zero-emissions vehicles such as electric vehicles. In response, automakers developed electric models, including the Chrysler TEVan, Ford Ranger EV pickup truck, GM EV1 and S10 EV pickup, Honda EV Plus hatchback, Nissan lithium-battery Altra EV miniwagon and Toyota RAV4 EV.\nThe automakers were accused of pandering to the wishes of CARB in order to continue to be allowed to sell cars in the lucrative Californian market, while failing to adequately promote their electric vehicles in order to create the impression that the consumers were not interested in the cars, all the while joining oil industry lobbyists in vigorously protesting CARB's mandate. GM's program came under particular scrutiny; in an unusual move, consumers were not allowed to purchase EV1s, but were instead asked to sign closed-end leases, meaning that the cars had to be returned to GM at the end of the lease period, with no option to purchase, despite leasee interest in continuing to own the cars. Chrysler, Toyota, and a group of GM dealers sued CARB in Federal court, leading to the eventual neutering of CARB's ZEV Mandate.\n\nAfter public protests by EV drivers' groups upset by the repossession of their cars, Toyota offered the last 328 RAV4-EVs for sale to the general public during six months, up until 22 November 2002. Almost all other production electric cars were withdrawn from the market and were in some cases seen to have been destroyed by their manufacturers. Toyota continues to support the several hundred Toyota RAV4-EV in the hands of the general public and in fleet usage. GM famously de-activated the few EV1s that were donated to engineering schools and museums.\n\nThroughout the 1990s, interest in fuel-efficient or environmentally friendly cars declined among consumers in the United States, who instead favored sport utility vehicles, which were affordable to operate despite their poor fuel efficiency thanks to lower gasoline prices. Domestic U.S. automakers chose to focus their product lines around the truck-based vehicles, which enjoyed larger profit margins than the smaller cars which were preferred in places like Europe or Japan.\n\nMost electric vehicles on the world roads are low-speed, low-range neighborhood electric vehicles (NEVs). Pike Research estimated there were almost 479,000 NEVs on the world roads in 2011. , there were between 60,000 and 76,000 low-speed battery-powered vehicles in use in the United States, up from about 56,000 in 2004. North America's top selling NEV is the Global Electric Motorcars (GEM) vehicles, with more than 50,000 units sold worldwide by mid 2014. The world's two largest NEV markets in 2011 were the United States, with 14,737 units sold, and France, with 2,231 units. Other micro electric cars sold in Europe was the Kewet, since 1991, and replaced by the Buddy, launched in 2008. Also the Th!nk City was launched in 2008 but production was halted due to financial difficulties. Production restarted in Finland in December 2009. The Th!nk was sold in several European countries and the U.S. In June 2011 Think Global filed for bankruptcy and production was halted. Worldwide sales reached 1,045 units by March 2011. A total of 200,000 low-speed small electric cars were sold in China in 2013, most of which are powered by lead-acid batteries. These electric vehicles are not considered by the government as new energy vehicles due to safety and environmental concerns, and consequently, do not enjoy the same benefits as highway legal plug-in electric cars.\n\nCalifornia electric car maker Tesla Motors began development in 2004 on the Tesla Roadster, which was first delivered to customers in 2008. The Roadster was the first highway legal serial production all-electric car to use lithium-ion battery cells, and the first production all-electric car to travel more than per charge. Since 2008, Tesla sold approximately 2,450 Roadsters in over 30 countries through December 2012. Tesla sold the Roadster until early 2012, when its supply of Lotus Elise gliders ran out, as its contract with Lotus Cars for 2,500 gliders expired at the end of 2011. Tesla stopped taking orders for the Roadster in the U.S. market in August 2011, and the 2012 Tesla Roadster was sold in limited numbers only in Europe, Asia and Australia. The next Tesla vehicle, the Model S, was released in the U.S. on 22 June 2012 and the first delivery of a Model S to a retail customer in Europe took place on 7 August 2013. Deliveries in China began on 22 April 2014. The next model was the Tesla Model X. In November 2014 Tesla delayed one more time the start of deliveries to retail customers, and announced the company expects Model X deliveries to begin in the third quarter of 2015.\n\nThe Mitsubishi i-MiEV was launched in Japan for fleet customers in July 2009, and for individual customers in April 2010, followed by sales to the public in Hong Kong in May 2010, and Australia in July 2010 via leasing. The i-MiEV was launched in Europe in December 2010, including a rebadged version sold in Europe as Peugeot iOn and Citroën C-Zero. The market launch in the Americas began in Costa Rica in February 2011, followed by Chile in May 2011. Fleet and retail customer deliveries in the U.S. and Canada began in December 2011. Accounting for all vehicles of the iMiEV brand, Mitsubishi reports around 27,200 units sold or exported since 2009 through December 2012, including the minicab MiEVs sold in Japan, and the units rebadged and sold as Peugeot iOn and Citroën C-Zero in the European market.\n\nSenior leaders at several large automakers, including Nissan and General Motors, have stated that the Roadster was a catalyst which demonstrated that there is pent-up consumer demand for more efficient vehicles. In an August 2009 edition of The New Yorker, GM vice-chairman Bob Lutz was quoted as saying, \"All the geniuses here at General Motors kept saying lithium-ion technology is 10 years away, and Toyota agreed with us – and boom, along comes Tesla. So I said, 'How come some tiny little California startup, run by guys who know nothing about the car business, can do this, and we can't?' That was the crowbar that helped break up the log jam.\"\n\nThe Nissan Leaf, introduced in Japan and the United States in December 2010, became the first modern all-electric, zero tailpipe emission five door family hatchback to be produced for the mass market from a major manufacturer. , the Leaf is also available in Australia, Canada and 17 European countries.\n\nThe Better Place network was the first modern commercial deployment of the battery swapping model. The Renault Fluence Z.E. was the first mass production electric car enable with switchable battery technology and sold for the Better Place network in Israel and Denmark. Better Place launched its first battery-swapping station in Israel, in Kiryat Ekron, near Rehovot in March 2011. The battery exchange process took five minutes. , there were 17 battery switch stations fully operational in Denmark enabling customers to drive anywhere across the country in an electric car. By late 2012 the company began to suffer financial difficulties, and decided to put on hold the roll out in Australia and reduce its non-core activities in North America, as the company decided to concentrate its resources on its two existing markets. On 26 May 2013, Better Place filed for bankruptcy in Israel. The company's financial difficulties were caused by the high investment required to develop the charging and swapping infrastructure, about million in private capital, and a market penetration significantly lower than originally predicted by Shai Agassi. Less than 1,000 Fluence Z.E. cars were deployed in Israel and around 400 units in Denmark.\n\nThe Smart electric drive, Wheego Whip LiFe, Mia electric, Volvo C30 Electric, and the Ford Focus Electric were launched for retail customers during 2011. The BYD e6, released initially for fleet customers in 2010, began reatail sales in Shenzhen, China in October 2011. The Bolloré Bluecar was released in December 2011 and deployed for use in the Autolib' carsharing service in Paris. Leasing to individual and corporate customers began in October 2012 and is limited to the Île-de-France area. In February 2011, the Mitsubishi i MiEV became the first electric car to sell more than of more than 10,000 units, including the models badged in Europe as Citroën C-Zero and Peugeot. The record was officially registered by Guinness World Records. Several months later, the Nissan Leaf overtook the i MiEV as the best selling all-electric car ever, and by February 2013 global sales of the Leaf reached the 50,000 unit mark.\n\nModels released to the market in 2012 and 2013 include the BMW ActiveE, Coda, Renault Fluence Z.E., Tesla Model S, Honda Fit EV, Toyota RAV4 EV, Renault Zoe, Roewe E50, Mahindra e2o, Chevrolet Spark EV, Mercedes-Benz SLS AMG Electric Drive, Fiat 500e, Volkswagen e-Up!, BMW i3, and Kandi EV. Toyota released the Scion iQ EV in the U.S. (Toyota eQ in Japan) in 2013. The car production is limited to 100 units. The first 30 units were delivered to the University of California, Irvine in March 2013 for use in its Zero Emission Vehicle-Network Enabled Transport (ZEV-NET) carsharing fleet. Toyota announced that 90 out of the 100 vehicles produced globally will be placed in carsharing demonstration projects in the United States and the rest in Japan.\n\nThe Coda sedan went out of production in 2013, after selling only about 100 units in California. Its manufacturer, Coda Automotive, filed for Chapter 11 bankruptcy protection on 1 May 2013. The company stated that it expects to emerge from the bankruptcy process to focus on energy storage solutions as it has decided to abandon car manufacturing.\n\nThe Tesla Model S ranked as the top selling plug-in electric car in North America during the first quarter of 2013 with 4,900 cars sold, ahead of the Nissan Leaf (3,695). European retail deliveries of the Tesla Model S began in Oslo in August 2013, and during its first full month in the market, the Model S ranked as the top selling car in Norway with 616 units delivered, representing a market share of 5.1% of all the new cars sold in the country in September 2013, becoming the first electric car to top the new car sales ranking in any country, and contributing to a record all-electric car market share of 8.6% of new car sales during that month. In October 2013, an electric car was the best selling car in the country for a second month in a row. This time was the Nissan Leaf with 716 units sold, representing a 5.6% of new car sales that month.\n\nThe Renault–Nissan Alliance reached global sales of 100,000 all-electric vehicles in July 2013. The 100,000th customer was a U.S. student who bought a Nissan Leaf. In mid January 2014, global sales of the Nissan Leaf reached the 100,000 unit milestone, representing a 45% market share of worldwide pure electric vehicles sold since 2010. The 100,000th car was delivered to a British customer.\n\n, there were over 500,000 plug-in electric passenger cars and utility vans in the world, with the U.S. leading plug-in electric car sales with a 45% share of global sales. In September 2014, sales of plug-in electric cars in the United States reached the 250,000 unit milestone. Global cumulative sales of the Tesla Model S passed the 50,000 unit milestone in October 2014. In November 2014 the Renault–Nissan Alliance reached 200,000 all-electric vehicles delivered globally, representing a 58% share of the global light-duty all-electric market segment.\n\nThe world's top selling all-electric cars in 2014 were the Nissan Leaf (61,507), Tesla Model S (31,655), BMW i3 (16,052), and the Renault Zoe (11,323). Accounting for plug-in hybrids, the Leaf and the Model S also ranked first and second correspondinly among the world's top 10 selling plug-in electric cars. All-electric models released to the retail customers in 2014 include the BMW Brilliance Zinoro 1E, Chery eQ, Geely-Kandi Panda EV, Zotye Zhidou E20, Kia Soul EV, Volkswagen e-Golf, Mercedes-Benz B-Class Electric Drive, and Venucia e30.\n\nGeneral Motors unveiled the Chevrolet Bolt EV concept car at the 2015 North American International Auto Show. The Bolt is scheduled for availability in late 2016 as a model year 2017. GM anticipates the Bolt will deliver an all-electric range more than , with pricing starting at before any applicable government incentives. The European version, marketed as the Opel Ampera-e, will go into production in 2017.\nIn May 2015, global sales of highway legal all-electric passenger cars and light utility vehicles passed the 500,000 unit milestone, accounting for sales since 2008. Out these, Nissan accounts for about 35%, Tesla Motors about 15%, and Mitsubishi about 10%. Also in May 2015, the Renault Zoe and the BMW i3 passed the 25,000 unit global sales milestone. In June 2015, worldwide sales of the Model S passed the 75,000 unit milestone in June 2015.\n\nBy early June 2015, the Renault–Nissan Alliance continued as the leading all-electric vehicle manufacturer with global sales of over 250,000 pure electric vehicles representing about half of the global light-duty all-electric market segment. Nissan sales totaled 185,000 units, which includes the Nissan Leaf and the e-NV200 van. Renault has sold 65,000 electric vehicles, and its line-up includes the ZOE passenger car, the Kangoo Z.E. van, the SM3 Z.E. (previously Fluence Z.E.) sedan and the Twizy heavy quadricycle.\n\nBy mid-September 2015, the global stock of highway legal plug-in electric passenger cars and utility vans passed the one million sales milestone, with the pure electrics capturing about 62% of global sales. The United States is the plug-in segment market leader with a stock of over 363,000 plug-in electric cars delivered since 2008 through August 2015, representing 36.3% of global sales. The state of California is the largest plug-in car regional market, with more than 158,000 units sold between December 2010 and June 2015, representing 46.5% of all plug-in cars sold in the U.S. Until December 2014, California not only had more plug-in electric vehicles than any other state in the nation, but also more than any other country.\n\n, China ranked as the world's second top selling country plug-in market, with over 157,000 units sold since 2011 (15.7%), followed by Japan with more than 120,000 plug-in units sold since 2009 (12.1%). , over 310,000 light-duty plug-in electric vehicles have been registered in the European market since 2010. European sales are led by Norway, followed by the Netherlands, and France. In the heavy-duty segment, China is the world's leader, with over 65,000 buses and other commercial vehicles sold through August 2015.\n\n, global sales of electric cars were led by the Nissan Leaf with over 200,000 units sold making the Leaf the world's top selling highway-capable electric car in history. The Tesla Model S, with global deliveries of more than 100,000 units, is the world's second best selling all-electric car of all-time. The Model S ranked as the world's best selling plug-in electric vehicle in 2015, up from second best in 2014. The Model S was also the top selling plug-in car in the U.S. in 2015. Most models released in the world's markets to retail customers during 2015 were plug-in hybrids. The only new series production all-electric cars launched up to October 2015 were the BYD e5 and the Tesla Model X, together with several variants of the Tesla Model S line-up.\n\nThe Tesla Model 3 was unveiled on 31 March 2016. With pricing starting at and an all-electric range of , the Model 3 is Tesla Motors first vehicle aimed for the mass market. Before the unveiling event, over 115,000 people had reserved the Model 3. , one week after the event, Tesla Motors reported over 325,000 reservations, more than triple the 107,000 Model S cars Tesla had sold by the end of 2015. These reservations represent potential sales of over . , Tesla Motors has sold almost 125,000 electric cars worldwide since delivery of its first Tesla Roadster in 2008. Tesla reported the number of net reservations totaled about 373,000 , after about 8,000 customer cancellations and about 4,200 reservations canceled by the automaker because these appeared to be duplicates from speculators.\n\nThe Hyundai Ioniq Electric was released in South Korea in July 2016, and sold over 1,000 units during its first two months in the market. The Renault-Nissan Alliance achieved the milestone of 350,000 electric vehicles sold globally in August 2016, and also set an industry record of 100,000 electric vehicle sold in a single year. Nissan global electric vehicle sales passed the 250,000 unit milestone also in August 2016. Renault global electric vehicle sales passed the 100,000 unit milestone in early September 2016. Global sales of the Tesla Model X passed the 10,000 unit mark in August 2016, with most cars delivered in the United States.\n\nCumulative global sales of pure electric passenger cars and utility vans passed the 1 million unit milestone in September 2016. Global sales of the Tesla Model S achieved the 150,000 unit milestone in November 2016, four years and five months after its introduction, and just five more months than it took the Nissan Leaf to achieve the same milestone. Norway achieved the milestone of 100,000 all-electric vehicles registered in December 2016. Retail deliveries of the Chevrolet Bolt EV began in the San Francisco Bay Area on 13 December 2016. In December 2016, Nissan reported that Leaf owners worldwide achieved the milestone of driven collectively through November 2016, saving the equivalent of nearly of emissions. Global Nissan Leaf sales passed 250,000 units delivered in December 2016. The Tesla Model S was the world's best-selling plug-in electric car in 2016 for the second year running, with 50,931 units delivered globally.\n\nIn December 2016, Norway became the first country where 5% of all registered passenger cars was a plug-in electric. When new car sales in Norway are breakdown by powertrain or fuel, nine of the top ten best-selling models in 2016 were electric-drive models. The Norwegian electric-drive segment achieved a combined market share of 40.2% of new passenger car sales in 2016, consisting of 15.7% for all-electric cars, 13.4% for plug-in hybrids, and 11.2% for conventional hybris. A record monthly market share for the plug-in electric passenger segment in any country was achieved in Norway in January 2017 with 37.5% of new car sales; the plug-in hybrid segment reached a 20.0% market share of new passenger cars, and the all-electric car segment had a 17.5% market share. Also in January 2017, the electrified passenger car segment, consisting of plug-in hybrids, all-electric cars and conventional hybrids, for the first time ever surpassed combined sales of cars with a conventional diesel or gasoline engine, with a market share of 51.4% of new car sales that month. For many years Norwegian electric vehicles have been subsidised by approximately 50%, and have several other benefits, such as use of bus lanes and free parking. Many of these perks have been extended to 2020. \n\nIn February 2017 Consumer Reports named Tesla as the top car brand in the United States and ranked it 8th among global carmakers. Deliveries of the Tesla Model S passed the 200,000 unit milestone during the fourth quarter of 2017. Global sales of the Nissan Leaf achieved the 300,000 unit milestone in January 2018. The global stock of highway legal plug-in electric passenger cars and utility vans reached 4 million in September 2018, but despite the rapid growth experienced, the plug-in electric car segment represented just about 1 out of every 300 vehicles on the world's roads. Tesla delivered its 100,000th Model 3 in October 2018.\n\nIn September 2018, the Norwegian market share of all-electric cars reached 45.3% and plug-in hybrids 14.9%, for a combined market share of the plug-in car segment of 60.2% of new car registrations that month, becoming the world's highest-ever monthly market share for the plug-in electric passenger segment in Norway and in any country. Accounting for conventional hybrids, the electrified segment achieved an all-time record 71.5% market share in September 2018. In October 2018, Norway became the first country where 1 in every 10 passenger cars registered is a plug-in electric vehicle.\n\nThe principal manufacturer of e-bikes globally is China, with 2009 seeing the manufacturer of 22.2 million units. In the world Geoby is the leading manufacturers of E-bikes. Pedego is the best selling in the U.S.\nChina accounts for nearly 92% of the market worldwide. In China the number of electric bicycles on the road was 120 million in 2010. Jiangsu Yadea, an electric bicycle producer of renown in China, leads the ranking of China National Light Industry Council (CNLIC) electric bicycle industry for three years. It retains capacity of nearly 6 million electric bicycles a year.\n\nIn 1997, Charger Electric Bicycle was the first U.S. company to come out with a pedelec.\n\nSelected list of battery electric vehicles include (in chronological order):\n\n\n\n"}
{"id": "34346707", "url": "https://en.wikipedia.org/wiki?curid=34346707", "title": "Holomorphic embedding load flow method", "text": "Holomorphic embedding load flow method\n\nThe Holomorphic Embedding Load-flow Method (HELM) is a solution method for the power flow equations of electrical power systems. Its main features are that it is direct (that is, non-iterative) and that it mathematically guarantees a consistent selection of the correct operative branch of the multivalued problem, also signalling the condition of voltage collapse when there is no solution. These properties are relevant not only for the reliability of existing off-line and real-time applications, but also because they enable new types of analytical tools that would be impossible to build with existing iterative load flow methods (due to their convergence problems). An example of this would be decision-support tools providing validated action plans in real time.\n\nThe HELM load flow algorithm was invented by Antonio Trias and has been granted two US Patents.\nA detailed description was presented at the 2012 IEEE PES General Meeting and subsequently published.\nThe method is founded on advanced concepts and results from complex analysis, such as holomorphicity, the theory of algebraic curves, and analytic continuation. However, the numerical implementation is rather straightforward as it uses standard linear algebra and the Padé approximation. Additionally, since the limiting part of the computation is the factorization of the admittance matrix and this is done only once, its performance is competitive with established fast-decoupled loadflows. The method is currently implemented into industrial-strength real-time and off-line packaged EMS applications.\n\nThe load-flow calculation is one of the most fundamental components in the analysis of power systems and is the\ncornerstone for almost all other tools used in power system simulation and management. The load-flow equations can be written in the following general form:\n\nwhere the given (complex) parameters are the admittance matrix\n, the bus shunt admittances\n, and the bus power\ninjections representing\nconstant-power loads and generators.\n\nTo solve this non-linear system of algebraic equations, traditional\nload-flow algorithms were developed based on three iterative\ntechniques: the Gauss-Seidel method\n\n, which has poor convergence properties but very little memory requirements and is\nstraightforward to implement; the full Newton-Raphson method\n\n, which has fast (quadratic) iterative convergence\nproperties, but it is computationally costly; and the Fast Decoupled\nLoad-Flow (FDLF) method\n\n, which is based on Newton-Raphson, but greatly reduces its computational cost by means of a decoupling approximation that is valid in most transmission networks. Many other incremental improvements exist; however, the underlying technique in all of them is still an iterative solver, either of Gauss-Seidel or of Newton type. There are two fundamental problems with all iterative schemes of this type. On the one hand, there is no guarantee that the iteration will always converge to a solution; on the other, since the system has multiple solutions, it is not possible to control which solution will be selected. As the power system approaches the point of voltage collapse, spurious solutions get closer to the correct one, and the iterative scheme may be easily attracted to one of them because of the phenomenon of Newton fractals: when the Newton method is applied to complex functions, the basins of attraction for the various solutions show fractal behavior. As a result, no matter how close the chosen initial point of the iterations (seed) is to the correct solution, there is always some non-zero chance of straying off to a different solution. These fundamental problems of iterative loadflows have been extensively documented. A simple\nillustration for the two-bus model is provided in Although there exist homotopic continuation techniques that alleviate the problem to some degree, the fractal nature of the basins of attraction precludes a 100% reliable method for all electrical scenarios.\n\nThe key differential advantage of the HELM is that it is fully deterministic and unambiguous: it guarantees that the solution always\ncorresponds to the correct operative solution, when it exists; and it signals the non-existence of the solution when the conditions are such that there is no solution (voltage collapse). Additionally, the method is competitive with the FDNR method in terms of computational cost. It brings a solid mathematical treatment of the load-flow problem that provides new insights not previously available with the iterative numerical methods.\n\nHELM is grounded on a rigorous mathematical theory, and in practical terms it could be summarized as follows:\n\nHELM provides a solution to a long-standing problem of all iterative load-flow methods, namely the unreliability of the iterations in finding the correct solution (or any solution at all).\n\nThis makes HELM particularly suited for real-time applications, and mandatory for any EMS software based on exploratory algorithms, such as contingency analysis, and under alert and emergency conditions solving operational limits violations and restoration providing guidance through action plans.\n\nFor the purposes of the discussion, we will omit the treatment of controls, but the method can accommodate all types of controls. For the constraint equations imposed by these controls, an appropriate holomorphic embedding must be also defined.\n\nThe method uses an embedding technique by means of a complex parameter .\nThe first key ingredient in the method lies in requiring the embedding to be holomorphic, that is, that the system of equations for voltages is turned into a system of equations for functions in such a way that the new system defines as holomorphic functions (i.e. complex analytic) of the new complex variable . The aim is to be able to use the process of analytic continuation which will allow the calculation of at . Looking at equations (), a necessary condition for the embedding to be holomorphic is that is replaced under the embedding with , not . This is because complex conjugation itself is not a holomorphic function. On the other hand, it is easy to see that the replacement does allow the equations to define a holomorphic function . However, for a given arbitrary embedding, it remains to be proven that is indeed holomorphic. Taking into account all these considerations, an embedding of this type is proposed:\n\nWith this choice, at the right hand side terms become zero, (provided that the denominator is not zero), this corresponds to the case where all\nthe injections are zero and this case has a well known and simple operational solution: all voltages are equal and all flow intensities are zero. Therefore, this choice for the embedding provides at s=0 a well known operational solution.\n\nNow using classical techniques for variable elimination in polynomial systems (results from the theory of Resultants and Gröbner basis it can be proven that equations () do in fact define as holomorphic functions. More significantly, they define as algebraic curves. It is this specific fact, which becomes true because the embedding is holomorphic that guarantees the uniqueness of the result. The solution at determines uniquely the solution everywhere (except on a finite number of branch cuts), thus getting rid of the multi-valuedness of the load-flow problem.\n\nThe technique to obtain the coefficients for the power series expansion (on ) of voltages is quite straightforward, once one realizes that equations () can be used to obtain them order after order. Consider the power series expansion for formula_1 and formula_2. By substitution into equations () and identifying terms at each order in , one obtains:\n\nIt is then straightforward to solve the sequence of linear systems () successively order after order, starting from . Note that the coefficients of the expansions for and are related by the simple convolution formulas derived from the following identity:\n\nso that the right-hand side in () can always be calculated from the solution of the system at the previous order. Note also how the procedure works by solving just linear systems, in which the matrix remains constant.\n\nA more detailed discussion about this procedure is offered in Ref.\n\nOnce the power series at are calculated to the desired order, the problem of calculating them at becomes one of analytic continuation. It should be strongly remarked that this does not have anything in common with the techniques of homotopic continuation. Homotopy is powerful since it only makes use of the concept of continuity and thus it is applicable to general smooth nonlinear systems, but on the other hand it does not always provide a reliable method to approximate the functions (as it relies on iterative schemes such as Newton-Raphson).\n\nIt can be proven that algebraic curves are complete global analytic functions, that is, knowledge of the power series expansion at one point (the so-called germ of the function) uniquely determines the function everywhere on the complex plane, except on a finite number of branch cuts. Stahl’s extremal domain theorem further asserts that there exists a maximal domain for the analytic continuation of the function, which corresponds to the choice of branch cuts with minimal measure. In the case of algebraic curves the number of cuts is finite, therefore it would be feasible to find maximal continuations by finding the combination of cuts with minimal capacity. For further improvements, Stahl’s theorem on the convergence of Padé Approximants states that the diagonal and supra-diagonal Padé (or equivalently, the continued fraction approximants to the power series) converge to the maximal analytic continuation. The zeros and poles of the approximants remarkably accumulate on the set of branch cuts having minimal capacity.\n\nThese properties confer the load-flow method with the ability to unequivocally detect the condition of voltage collapse: the algebraic approximations are guaranteed to either converge to the solution if it exists, or not converge if the solution does not exist.\n\n"}
{"id": "16597340", "url": "https://en.wikipedia.org/wiki?curid=16597340", "title": "Large Underground Xenon experiment", "text": "Large Underground Xenon experiment\n\nThe Large Underground Xenon experiment (LUX) aims to directly detect weakly interacting massive particle (WIMP) dark matter interactions with ordinary matter on Earth. Despite the wealth of (gravitational) evidence supporting the existence of non-baryonic dark matter in the Universe, dark matter particles in our galaxy have never been directly detected in an experiment. LUX utilizes a 370 kg liquid xenon detection mass in a time-projection chamber (TPC) to identify individual particle interactions, searching for faint dark matter interactions with unprecedented sensitivity.\n\nThe LUX experiment, which cost approximately $10 million to build, is located underground at the Sanford Underground Laboratory (SURF, formerly the Deep Underground Science and Engineering Laboratory, or DUSEL) in the Homestake Mine (South Dakota) in Lead, South Dakota. The detector is located in the Davis campus, former site of the Nobel Prize-winning Homestake neutrino experiment led by Raymond Davis. It is operated underground to reduce the background noise signal caused by high-energy cosmic rays at the Earth's surface.\n\nThe detector is isolated from background particles by a surrounding water tank and the earth above. This shielding reduces cosmic rays and radiation interacting with the xenon.\n\nInteractions in liquid xenon generate 175 nm ultraviolet photons and electrons. The photons are immediately detected by two arrays of 61 photomultiplier tubes at the top and bottom of the detector. These prompt photons are the S1 signal. Electrons generated by the particle interactions drift upwards towards the xenon gas by an electric field. The electrons are pulled in the gas at the surface by a stronger electric field, and produce electroluminescence photons detected as the S2 signal. The S1 and subsequent S2 signal constitute a particle interaction in the liquid xenon.\n\nThe detector is a time-projection chamber (TPC), using the time between S1 and S2 signals to find the interaction depth since electrons move at constant velocity in the liquid (around 1–2 km/s, depending on the electric field). The x-y coordinate of the event is inferred from electroluminescence photons at the top array by statistical methods (Monte Carlo and maximum likelihood estimation) to a resolution under 1 cm.\n\nWIMPs would be expected to interact exclusively with the liquid xenon nuclei, resulting in nuclear recoils that would appear very similar to neutron collisions. In order to single out WIMP interactions, neutron events must be minimized, through shielding and ultra-quiet building materials.\n\nIn order to discern WIMPs from neutrons, the number of single interactions must be compared to multiple events. Since WIMPs are expected to be so weakly interacting, most would pass through the detector unnoticed. Any WIMPs that interact will have negligible chance of repeated interaction. Neutrons, on the other hand, have a reasonably large chance of multiple collisions within the target volume, the frequency of which can be accurately predicted. Using this knowledge, if the ratio of single interactions to multiple interactions exceeds a certain value, the detection of dark matter may be reliably inferred.\n\nThe \"XENON Collaboration\" is composed of over 100 scientists and engineers across 18 institutions in the US, Europe, and Middle East. LUX is composed of the majority of the US groups that collaborated in the XENON10 experiment, most of the groups in the ZEPLIN III experiment, the majority of the US component of the ZEPLIN II experiment, and groups involved in low-background rare event searches such as Super Kamiokande, SNO, IceCube, Kamland, EXO and Double Chooz.\n\nThe LUX experiment's co-spokespersons are Richard Gaitskell from Brown University (acting as co-spokesperson since 2007) and Daniel McKinsey from Yale University (acting as co-spokesperson since 2012). Tom Shutt from Case Western Reserve University was LUX co-spokesperson between 2007-2012.\n\nDetector assembly began in late 2009. The LUX detector was commissioned overground at SURF for a six-month run. The assembled detector was transported underground from the surface laboratory in a two-day operation in the summer of 2012 and began data taking April 2013, presenting initial results Fall 2013. LUX plans to operate into 2015 and perform a blinded analysis of 300 live days. The next-generation 7-ton LUX-ZEPLIN has been approved, expected to begin in 2020.\n\nInitial unblinded data taken April to August 2013 were announced on October 30, 2013. In an 85 live-day run with 118 kg fiducial volume, LUX obtained 160 events passing the data analysis selection criteria, all consistent with electron recoil backgrounds. A profile likelihood statistical approach shows this result is consistent with the background-only hypothesis (no WIMP interactions) with a p-value of 0.35. This is the most sensitive dark matter direct detection result in the world, and rules out low-mass WIMP signal hints such as from CoGeNT and CDMS-II. These results have struck out some of the theories about WIMPs, which allows researchers to focus on fewer leads.\n\nIn the final run from October 2014 to May 2016, at four times its original design sensitivity with 368 kg of liquid xenon, LUX has seen no signs of dark matter candidate—WIMPs.\n\n"}
{"id": "217946", "url": "https://en.wikipedia.org/wiki?curid=217946", "title": "Lemon battery", "text": "Lemon battery\n\nA lemon battery is a simple battery often made for the purpose of education. Typically, a piece of zinc metal (such as a galvanized nail) and a piece of copper (such as a penny) are inserted into a lemon and connected by wires. Power generated by reaction of the metals is used to power a small device such as a light emitting diode (LED).\n\nThe lemon battery is similar to the first electrical battery invented in 1800 by Alessandro Volta, who used brine (salt water) instead of lemon juice. The lemon battery illustrates the type of chemical reaction (oxidation-reduction) that occurs in batteries. The zinc and copper are called the electrodes, and the juice inside the lemon is called the electrolyte. There are many variations of the lemon cell that use different fruits (or liquids) as electrolytes and metals other than zinc and copper as electrodes.\n\nThere are numerous sets of instructions for making lemon batteries and for obtaining components such as light-emitting diodes (LEDs), electrical meters (multimeters), and zinc-coated (galvanized) nails and screws. Commercial \"potato clock\" science kits include electrodes and a low-voltage digital clock. After one cell is assembled, a multimeter can be used to measure the voltage or the electric current from the voltaic cell; a typical voltage is 0.9 V with lemons. Currents are more variable, but range up to about 1 mA (the larger the electrode surfaces, the bigger the current). For a more visible effect, lemon cells can be connected in series to power an LED (see illustration) or other devices. The series connection increases the voltage available to devices. Swartling and Morgan have published a list of low-voltage devices along with the corresponding number of lemon cells that were needed to power them; they included LEDs, piezeoelectric buzzers, and small digital clocks. With the zinc/copper electrodes, at least two lemon cells were needed for any of these devices. Substituting a magnesium electrode for the zinc electrode makes a cell with a larger voltage (1.5−1.6 V), and a single magnesium/copper cell will power some devices. Note that incandescent light bulbs from flashlights are not used because the lemon battery is not designed to produce enough electric current to light them. By multiplying the average current of a lemon (0.001A/ 1mA) by the average (lowest) voltage (potential difference) of a lemon (0.7V) we can conclude that it would take more than 6 million lemons to give us the power of an average 4320W car battery.\n\nMany fruits and liquids can be used for the acidic electrolyte. Fruit is convenient, because it provides both the electrolyte and a simple way to support the electrodes. The acid involved in citrus fruits (lemons, oranges, grapefruits, etc.) is citric acid. The acidity, which is indicated by the measured pH, varies substantially.\n\nPotatoes have phosphoric acid and work well; they are the basis for commercial \"potato clock\" kits. Potato batteries with LED lighting have been proposed for use in poor countries or by off-grid populations. International research begun in 2010 showed that boiling potatoes for eight minutes improves their electrical output, as does placing slices of potatoes between multiple copper and zinc plates. Boiled and chopped plantain pith (stem) is also suitable, according to Sri Lankan researchers.\n\nInstead of fruit, liquids in various containers can be used. Household vinegar (acetic acid) works well. Sauerkraut (lactic acid) was featured in one episode of the US television program \"Head Rush\" (an offshoot of the \"MythBusters\" program). The sauerkraut had been canned, and became the electrolyte while the can itself was one of the electrodes.\n\nZinc and copper electrodes are reasonably safe and easy to obtain. Other metals such as lead, iron, magnesium, etc., can be studied as well; they yield different voltages than the zinc/copper pair. In particular, magnesium/copper cells can generate voltages as large as 1.6 V in lemon cells. This voltage is larger than obtainable using zinc/copper cells. It is comparable to that of standard household batteries (1.5 V), which is useful in powering devices with a single cell instead of using cells in series.\n\nFor the youngest pupils, about ages 5–9, the educational goal is utilitarian: batteries are devices that can power other devices, so long as they are connected by a conductive material. Batteries are components in electrical circuits; hooking a single wire between a battery and a light bulb will not power the bulb.\n\nFor children in the age range 10−13, batteries are used to illustrate the connection between chemistry and electricity as well as to deepen the circuit concept for electricity. The fact that different chemical elements such as copper and zinc are used can be placed in the larger context that the elements do not disappear or break down when they undergo chemical reactions.\n\nFor older pupils and for college students, batteries serve to illustrate the principles of oxidation-reduction reactions. Students can discover that two identical electrodes yield no voltage, and that different pairs of metals (beyond copper and zinc) yield different voltages. The voltages and currents from series and parallel combinations of the batteries can be examined.\n\nThe current that is output by the battery through a meter will depend on the size of the electrodes, how far the electrodes are inserted into the fruit, and how close to each other the electrodes are placed; the voltage is fairly independent of these details of the electrodes.\n\nMost textbooks present the following model for the chemical reactions of a lemon battery. When the cell is providing an electric current through an external circuit, the metallic zinc at the surface of the zinc electrode is dissolving into the solution. Zinc atoms dissolve into the liquid electrolyte as electrically charged ions (Zn), leaving 2 negatively charged electrons (e) behind in the metal:\nThis reaction is called oxidation. While zinc is entering the electrolyte, two positively charged hydrogen ions (H) from the electrolyte combine with two electrons at the copper electrode's surface and form an uncharged hydrogen molecule (H):\n\nThis reaction is called reduction. The electrons used in the copper to form the molecules of hydrogen are transferred from the zinc through an external wire connecting between the copper and the zinc. The hydrogen molecules formed on the surface of the copper by the reduction reaction ultimately bubble away as hydrogen gas.\n\nThis model of the chemical reactions makes several predictions that were examined in experiments published by Jerry Goodisman in 2001. Goodisman notes that numerous recent authors propose chemical reactions for the lemon battery that involve dissolution of the copper electrode into the electrolyte. Goodisman excludes this reaction as being inconsistent with the experiments, and notes that the correct chemistry, which involves the evolution of hydrogen at the copper electrode but also can use silver instead of copper, has been known for many years. Most of the detailed predictions of the model apply to the battery's voltage that is measured directly by a meter; nothing else is connected to the battery. When the electrolyte was modified by adding zinc sulfate (ZnSO), the voltage from the cell was reduced as predicted using the Nernst equation for the model. The Nernst equation essentially says how much the voltage drops as more zinc sulfate is added. The addition of copper sulfate (CuSO) did not affect the voltage. This result is consistent with the fact that copper atoms from the electrode are not involved in the chemical reaction model for the cell.\n\nWhen the battery is hooked up to an external circuit and a significant electric current is flowing, the zinc electrode loses mass, as predicted by the zinc oxidation reaction above. Similarly, hydrogen gas evolves as bubbles from the copper electrode. Finally, the voltage from the cell depended upon the acidity of the electrolyte, as measured by its pH; decreasing acidity (and increasing pH) causes the voltage to fall. This effect is also predicted by the Nernst equation; the particular acid that was used (citric, hydrochloric, sulfuric, etc.) doesn't affect the voltage except through the pH value.\n\nThe Nernst equation prediction failed for strongly acid electrolytes (pH < 3.4), when the zinc electrode dissolves into the electrolyte even when the battery is not providing any current to a circuit. The two oxidation-reduction reactions listed above only occur when electrical charge can be transported through the external circuit. The additional, open-circuit reaction can be observed by the formation of bubbles at the zinc electrode under open-circuit. This effect ultimately limited the voltage of the cells to 1.0 V near room temperature at the highest levels of acidity.\n\nThe energy comes from the chemical change in the zinc when it dissolves into the acid. The energy does not come from the lemon or potato. The zinc is oxidized inside the lemon, exchanging some of its electrons with the acid in order to reach a lower energy state, and the energy released provides the power.\n\nIn current practice, zinc is produced by electrowinning of zinc sulfate or pyrometallurgical reduction of zinc with carbon, which requires an energy input. The energy produced in the lemon battery comes from reversing this reaction, recovering some of the energy input during the zinc production.\n\nFrom 1840 to the late 19th century, large, voltaic cells using a zinc electrode and a sulfuric acid electrolyte were widely used in the printing industry. While copper electrodes like those in lemon batteries were sometimes used, in 1840 invented a refined version of this cell that used silver with a rough platinum coating instead of a copper electrode. Hydrogen gas clinging to the surface of a silver or copper electrode reduces the electric current that can be drawn from a cell; the phenomenon is called \"polarization\". The roughened, \"platinized\" surface speeds up the bubbling of the hydrogen gas, and increases the current from the cell. Unlike the zinc electrode, the copper or platinized silver electrodes are not consumed by using the battery, and the details of this electrode do not affect the cell's voltage. The Smee cell was convenient for electrotyping, which produced copper plates for letterpress printing of newspapers and books, and also statues and other metallic objects.\n\nThe Smee cell used amalgamated zinc instead of pure zinc; the surface of amalgamated zinc has been treated with mercury. Apparently amalgamated zinc was less prone to degradation by an acidic solution than is pure zinc. Amalgamated zinc and plain zinc electrodes give essentially the same voltage when the zinc is pure. With the imperfectly refined zinc in 19th century laboratories they typically gave different voltages.\n\n\n\n"}
{"id": "52947499", "url": "https://en.wikipedia.org/wiki?curid=52947499", "title": "Lomi Hydroelectric Power Station", "text": "Lomi Hydroelectric Power Station\n\nThe Lomi Hydroelectric Power Station ( or \"Lomi kraftstasjon\") is a hydroelectric power station in the municipality of Fauske in Nordland county, Norway. It utilizes a drop of between its intake reservoir on \"Låmivatnet\" (Lake Låmi; also ), which can be regulated at a level between and . The reservoir is supplied by water from \"Storelvvatnan\" (), a lake regulated at a level between and , and also by some stream intakes. Part of the water supplying the plant is runoff from the Sulitjelma Glacier. The plant has two Francis turbines and operates at an installed capacity of , with an average annual production of about 362 GWh. Its total catchment area is . The plant is owned by Salten Kraftsamband and came into operation in 1979. The water is reused by the Sjønstå Hydroelectric Power Station further downstream in the Sulitjelma drainage system.\n"}
{"id": "22132209", "url": "https://en.wikipedia.org/wiki?curid=22132209", "title": "Luttinger parameter", "text": "Luttinger parameter\n\nIn semiconductors, valence bands are well characterized by 3 Luttinger parameters. At the \"Г\"-point in the band structure, formula_1 and formula_2 orbitals form valence bands. But spin-orbit coupling splits sixfold degeneracy into high energy 4-fold and lower energy 2-fold bands. Again 4-fold degeneracy is lifted into heavy- and light hole bands by phenomenological Hamiltonian by J. M. Luttinger.\n\nIn the presence of spin-orbit interaction, total angular momentum should take part in. From the three valence band, l=1 and s=1/2 state generate six state of |j,m> as formula_3\n\nThe spin-orbit interaction from the relativistic quantum mechanics, lowers the energy of \"j\"=1/2 states down.\n\nPhenomenological Hamiltonian in spherical approximation is written as\n\nformula_4\n\nPhenomenological Luttinger parameters formula_5 are defined as\n\nformula_6\n\nand\n\nformula_7\n\nIf we take formula_8 as formula_9, the Hamiltonian is diagonalized for formula_10 states.\n\nformula_11\n\nTwo degenerated resulting eigenenergies are\n\nformula_12 for formula_13\n\nformula_14 for formula_15\n\nformula_16 (formula_17) indicates heav-(light-) hole band energy. If we regard the electrons as nearly free electrons, the Luttinger parameters describe effective mass of electron in each bands.\n\nLuttinger parameter can be measured by Hot-electron luminescence experiment.\n\nformula_18\n\n"}
{"id": "31860373", "url": "https://en.wikipedia.org/wiki?curid=31860373", "title": "Moss–Burstein effect", "text": "Moss–Burstein effect\n\nThe Moss–Burstein effect, also known as the Burstein-Moss shift, is the phenomenon of which the apparent band gap of a semiconductor is increased as the absorption edge is pushed to higher energies as a result of some states close to the conduction band being populated. This is observed for a degenerate electron distribution such as that found in some Degenerate semiconductors and is known as a Moss–Burstein shift.\nThe effect occurs when the electron carrier concentration exceeds the conduction band edge density of states, which corresponds to degenerate doping in semiconductors. In nominally doped semiconductors, the Fermi level lies between the conduction and valence bands. For example, in n-doped semiconductor, as the doping concentration is increased, electrons populate states within the conduction band which pushes the Fermi level to higher energy. In the case of degenerate level of doping, the Fermi level lies inside the conduction band. The \"apparent\" band gap of a semiconductor can be measured using transmission/reflection spectroscopy. In the case of a degenerate semiconductor, an electron from the top of the valence band can only be excited into conduction band above the Fermi level (which now lies in conduction band) since all the states below the Fermi level are occupied states. Pauli's exclusion principle forbids excitation into these occupied states. Thus we observe an increase in the apparent band gap. Apparent band gap = Actual band gap + Moss-Burstein shift (as shown in the figure).\n\nNegative Burstein shifts can also occur. These are due to band structure changes due to doping.\n"}
{"id": "9065600", "url": "https://en.wikipedia.org/wiki?curid=9065600", "title": "Neodymium-doped yttrium lithium fluoride", "text": "Neodymium-doped yttrium lithium fluoride\n\nNeodymium-doped yttrium lithium fluoride (Nd:YLF) is a lasing medium for arc lamp-pumped and diode-pumped solid-state lasers. The YLF crystal (LiYF) is naturally birefringent, and commonly used laser transitions occur at 1047 nm and 1053 nm.\n\nIt is used in Q-switched systems in part due \nto its relatively long fluorescence lifetime.\nAs with s, harmonic generation is frequently \nemployed with Q-switched Nd:YLF \nto produce shorter wavelengths. A common application\nof frequency-doubled Nd:YLF pulses is to pump ultrafast \nTi:Sapphire chirped-pulse amplifiers.\n\nNeodymium-doped YLF can provide higher pulse energies than Nd:YAG for repetition rates of a few kHz or less. Compared to Nd:YAG, the Nd:YLF crystal is very brittle \nand fractures easily. It is also slightly water-soluble — a YLF laser rod may very \nslowly dissolve in cooling water which surrounds it.\n\n"}
{"id": "4371612", "url": "https://en.wikipedia.org/wiki?curid=4371612", "title": "Patterned vegetation", "text": "Patterned vegetation\n\nPatterned vegetation is a vegetation community that exhibits distinctive and repetitive patterns. Examples of patterned vegetation include fir waves, tiger bush, and string bog. The patterns typically arise from an interplay of phenomena that differentially encourage plant growth or mortality. A coherent pattern arises because there is a strong directional component to these phenomena, such as wind in the case of fir waves, or surface runoff in the case of tiger bush.\nThe regular patterning of some types of vegetation is a striking feature of some landscapes. Patterns can include relatively evenly spaced patches, parallel bands or some intermediate between those two. These patterns in the vegetation can appear without any underlying pattern in soil types, and are thus said to “self-organize” rather than be determined by the environment.\nSeveral of the mechanisms underlying patterning of vegetation have been known and studied since at least the middle of the 20th century, however, mathematical models replicating them have only been produced much more recently. The self-organization of complex spatial patterns can be generated from relatively simple mathematical expressions, often called a Turing pattern. These occur at many scales of life, from cellular development (where they were first proposed) to pattern formation on animal pelts to sand dunes and patterned landscapes (see also pattern formation). In their simplest form these models require two interactions at differing scales: local facilitation and more distant antagonism. For example, when Sato and Iwasa produced a simple model of fir waves in the Japanese Alps, they assumed that trees exposed to cold winds would suffer mortality from frost damage, but upwind trees would protect nearby downwind trees from wind. Banding appears because the protective boundary layer created by the wind-most trees is eventually disrupted by turbulence, exposing more distant downwind trees to freezing damage once again.\nWhen there is no directional resource flow across the landscape, regularly spaced patches or spots appear. In contrast, if there is a clear directionality to some important factor (such as a freezing wind or surface flow down a slope), then regularly spaced bands develop perpendicular to the flow.\nSince then theoretical models have been published documenting a wide variety of patterned landscapes, including: \nAlthough not strictly vegetation, sessile marine invertebrates such as mussels and oysters, have also been shown to form banding patterns.\n\n"}
{"id": "3175932", "url": "https://en.wikipedia.org/wiki?curid=3175932", "title": "Phosphorene", "text": "Phosphorene\n\nPhosphorene is a two-dimensional material and allotrope of phosphorus. Phosphorene can be viewed as a single layer of black phosphorus, much in the same way that graphene is a single layer of graphite. Phosphorene is predicted to be a strong competitor to graphene because, in contrast to graphene, phosphorene has a band gap. Phosphorene was first isolated in 2014 by mechanical exfoliation.\n\nIn the 1960s black phosphorus, a layered semiconducting allotrope of phosphorus, was synthesized, which exhibited high carrier mobility. In 2014, several groups introduced single-layer phosphorene, a monolayer of black phosphorus. It attracted renewed attention because of its potential in optoelectronics and electronics due to its band gap, which can be tuned via modifying its thickness, anisotropic photoelectronic properties and high carrier mobility. Phosphorene was initially prepared using mechanical cleavage, a commonly used technique in graphene production that is difficult to scale up. Liquid exfoliation is a promising method for scalable phosphorene production.\n\nSynthesis of phosphorene is a significant challenge. Currently, there are two main ways of phosphorene production: scotch-tape-based microcleavage and liquid exfoliation, while several other methods are being developed as well. Phosphorene production from plasma etching is also reported.\n\nIn scotch-tape-based microcleavage, phosphorene is mechanically exfoliated from a bulk of black phosphorus crystal using scotch-tape. Phosphorene is then transferred on a Si/SiO substrate, where it is then cleaned with acetone, isopropyl alcohol and methanol to remove any scotch tape residue. The sample is then heated to 180 °C to remove solvent residue.\n\nIn the liquid exfoliation method first reported by Brent et al. in 2014 and modified by others, bulk black phosphorus is first ground in a mortar and pestle and then sonicated in deoxygenated, anhydrous organic liquids such as NMP under inert atmosphere using low-power bath sonication. Suspensions are then centrifuged for 30 minutes to filter out the unexfoliated black phosphorus. Resulting 2D monolayer and few-layer phosphorene unoxidized and crystalline structure, while exposure to air oxidizes the phosphorene and produces acid.\n\nAnother variation of liquid exfoliation is “basic N-methyl-2-pyrrolidone (NMP) liquid exfoliation”. Bulk black phosphorene is added to a saturated NaOH/NMP solution, which is further sonicated for 4 hours to conduct liquid exfoliation. The solution is then centrifuged twice, first for 10 minutes to remove any unexfoliated black phosphorus and then for 20 minutes at higher speed to separate thick layers of phosphorene (5–12 layers) from NMP. The supernatant then is centrifuged again at higher speed for another 20 minutes to separate thinner layers of phosphorene (1–7 layers). The precipitate from centrifugation is then redispersed in water and washed several times by deionized water. Phosphorene/water solution is dropped onto silicon with a 280-nm SiO surface, where it is further dried under vacuum. NMP liquid exfoliation method was shown to yield phosphorene with controllable size and layer number, excellent water stability and in high yield.\n\nThe high yield production of phosphorene has been demonstrated by many groups in solvents, but to realize the potential applications of this material, it is crucial to deposit these free-standing nanosheets in solvents systematically on substrates. H. Kaur et al. demonstrated the synthesis, interface driven alignment and subsequent functional properties of few layer semiconducting phosphorene using Langmuir-Blodgett assembly. This is the first study which provides a straightforward and versatile solution towards the challenge of assembling nanosheets of phosphorene onto various supports and subsequently use these sheets in an electronic device. Therefore, wet assemblies techniques like Langmuir-Blodgett serves as a very valuable new entry point for the exploration of electronic as well as opto-electronic properties of phosphorene as well as other 2D layered inorganic materials.\n\nIt is still a challenge to directly epitaxially grow 2D phosphorene because the stability of black phosphorene is highly sensitive to substrate, which is understanding by theoretical simulations.\n\nPhosphorene 2D materials are composed of individual layers held together by van der Waals forces in lieu of covalent or ionic bonds that are found in most materials. There are five electrons on 3p orbitals of phosphorus atom, thus, giving rise to sp hybridization of phosphorus atom within phosphorene structure. Monolayered phosphorene exhibits the structure of a quadrangular pyramid because three electrons of P atom bond with three other P atoms covalently at 2.18 Å leaving one lone pair. Two of the phosphorus atoms are in the plane of the layer at 99° from one another, and the third phosphorus is between the layers at 103°, yielding an average angle of 102°.\n\nAccording to density functional theory (DFT) calculations, phosphorene forms in a honeycomb lattice structure with notable nonplanarity in the shape of structural ridges. It is predicted that crystal structure of black phosphorus can be discriminated under high pressure. This is mostly due to the anisotropic compressibility of black phosphorus because of the asymmetrical crystal structures. Subsequently, the van der Waals bond can be greatly compressed in the z-direction. However, there is a great variation in compressibility across the orthogonal x-y plane.\n\nIt is reported that controlling the centrifugal speed of production may aid in regulating the thickness of a material. For example, centrifuging at 18000 rpm during synthesis produced phosphorene with an average diameter of 210 nm and a thickness of 2.8 ± 1.5 nm (2–7 layers).\n\nPhosphorene has a thickness dependent direct band gap that changes to 1.88 eV in a monolayer from 0.3 eV in the bulk. Increase in band gap value in single-layer phosphorene is predicted to be caused by the absence of interlayer hybridization near the top of the valence and bottom of the conduction band. A pronounced peak centered at around 1.45 eV suggests the band gap structure in few- or single-layer phosphorene difference from bulk crystals.\n\nIn vacuum or on weak substrate, an interesting reconstruction with nanotubed termination of phosphorene edge is very easy to happen, transforming phosphorene edge from metallic to semiconducting.\n\nOne major disadvantage of phosphorene is its air-stability. Composed of hygroscopic phosphorus and with extremely high surface-to-volume ratio, phosphorene reacts with water vapor and oxygen assisted by visible light to degrade within the scope of hours. Through the degradation process, phosphorene (solid) reacts with oxygen/water to develop liquid phase acid 'bubbles' on the surface, and finally evaporate (vapor) to fully vanish (S-B-V degradation) and severely reducing overall quality.\n\nResearchers have fabricated transistors of phosphorene to examine its performance in actual devices. Phosphorene-based transistor consists of a channel of 1.0 μm and uses few layered phosphorene with a thickness varying from 2.1 to over 20 nm. Reduction of the total resistance with decreasing gate voltage is observed, indicating the p-type characteristic of phosphorene. Linear I-V relationship of transistor at low drain bias suggests good contact properties at the phosphorene/metal interface. Good current saturation at high drain bias values was observed. However, it was seen that the mobility is reduced in few-layer phosphorene when compared to bulk black phosphorus. Field-effect mobility of phosphorene-based transistor shows a strong thickness dependence, peaking at around 5 nm and decrease steadily with further increase of crystal thickness.\n\nAtomic layer deposition (ALD) dielectric layer and/or hydrophobic polymer is used as encapsulation layers in order to prevent device degradation and failure. Phosphorene devices are reported to maintain their function for weeks with encapsulation layer, whereas experience device failure within a week when exposed to ambient condition.\n\nResearchers have also constructed the CMOS inverter (logic circuit) by combining a phosphorene PMOS transistor with a MoS NMOS transistor, achieving high heterogeneous integration of semiconducting phosphorene crystals as a new channel material for potential electronic applications. In the inverter, the power supply voltage is set to be 1 V. The output voltage shows a clear transition from VDD to 0 within the input voltage range from −10 to −2 V. A maximum gain of ~1.4 is attained.\n\nThe potential applications of mixed bilayer phosphorene in solar-cell material was examined as well. The predicted power conversion efficiency for a monolayer MoS/AA-stacked bilayer phosphorene and MoS/AB-stacked bilayer phosphorene can get as high as ~18% and 16%, respectively. Results suggest that trilayer MoS phosphorene is a promising candidate in flexible optoelectronic devices.\n\nPhosphorene is a promising candidate for flexible nano systems due to its ultra-thin nature with ideal electrostatic control and superior mechanical flexibility. Researchers have demonstrated the flexible transistors, circuits and AM demodulator based on few-layer phosphorus, showing enhanced am bipolar transport with high room temperature carrier mobility as high as ~310 cm/Vs and strong current saturation. Fundamental circuit units including digital inverter, voltage amplifier and frequency doubler have been realized. Radio frequency (RF) transistors with highest intrinsic cutoff frequency of 20 GHz has been realized for potential applications in high frequency flexible smart nano systems.\n\n"}
{"id": "1405072", "url": "https://en.wikipedia.org/wiki?curid=1405072", "title": "Polyglycolide", "text": "Polyglycolide\n\nPolyglycolide or poly(glycolic acid) (PGA), also spelled as polyglycolic acid, is a biodegradable, thermoplastic polymer and the simplest linear, aliphatic polyester. It can be prepared starting from glycolic acid by means of polycondensation or ring-opening polymerization. PGA has been known since 1954 as a tough fiber-forming polymer. Owing to its hydrolytic instability, however, its use has initially been limited. Currently polyglycolide and its copolymers (poly(lactic-\"co\"-glycolic acid) with lactic acid, poly(glycolide-\"co\"-caprolactone) with ε-caprolactone and poly (glycolide-\"co\"-trimethylene carbonate) with trimethylene carbonate) are widely used as a material for the synthesis of absorbable sutures and are being evaluated in the biomedical field.\n\nPolyglycolide has a glass transition temperature between 35 and 40 °C and its melting point is reported to be in the range of 225-230 °C. PGA also exhibits an elevated degree of crystallinity, around 45–55%, thus resulting in insolubility in water. The solubility of this polyester is somewhat unusual, in that its high molecular weight form is insoluble in almost all common organic solvents (acetone, dichloromethane, chloroform, ethyl acetate, tetrahydrofuran), while low molecular weight oligomers sufficiently differ in their physical properties to be more soluble. However, polyglycolide is soluble in highly fluorinated solvents like hexafluoroisopropanol (HFIP) and hexafluoroacetone sesquihydrate, that can be used to prepare solutions of the high MW polymer for melt spinning and film preparation. Fibers of PGA exhibit high strength and modulus (7 GPa) and are particularly stiff.\n\nPolyglycolide can be obtained through several different processes starting with different materials:\n\n\nPolycondensation of glycolic acid is the simplest process available to prepare PGA, but it is not the most efficient because it yields a low molecular weight product. Briefly, the procedure is as follows: glycolic acid is heated at atmospheric pressure and a temperature of about 175-185 °C is maintained until water ceases to distill. Subsequently, pressure is reduced to 150 mm Hg, still keeping the temperature unaltered for about two hours and the low MW polyglycolide is obtained.\n\nThe most common synthesis used to produce a high molecular weight form of the polymer is ring-opening polymerization of \"glycolide\", the cyclic diester of glycolic acid. Glycolide can be prepared by heating under reduced pressure low MW PGA, collecting the diester by means of distillation. Ring-opening polymerization of glycolide can be catalyzed using different catalysts, including antimony compounds, such as antimony trioxide or antimony trihalides, zinc compounds (zinc lactate) and tin compounds like stannous octoate (tin(II) 2-ethylhexanoate) or tin alkoxides. Stannous octoate is the most commonly used initiator, since it is approved by the FDA as a food stabilizer. Usage of other catalysts has been disclosed as well, among these are aluminium isopropoxide, calcium acetylacetonate, and several lanthanide alkoxides (e.g. yttrium isopropoxide). The procedure followed for ring-opening polymerization is briefly outlined: a catalytic amount of initiator is added to glycolide under a nitrogen atmosphere at a temperature of 195 °C. The reaction is allowed to proceed for about two hours, then temperature is raised to 230 °C for about half an hour. After solidification the resulting high MW polymer is collected.\n\nAnother procedure consists in the thermally induced solid-state polycondensation of halogenoacetates with general formula\nX-—CHCOOM (where M is a monovalent metal like sodium and X is a halogen like chlorine), resulting in the production of polyglycolide and small crystals of a salt. Polycondensation is carried out by heating an halogenoacetate, like sodium chloroacetate, at a temperature between 160-180 °C, continuously passing nitrogen through the reaction vessel. During the reaction polyglycolide is formed along with sodium chloride which precipitates within the polymeric matrix; the salt can be conveniently removed by washing the product of the reaction with water.\n\nPGA can also be obtained by reacting carbon monoxide, formaldehyde or one of its related compounds like paraformaldehyde or trioxane, in presence of an acidic catalyst. In a carbon monoxide atmosphere an autoclave is loaded with the catalyst (chlorosulfonic acid), dichloromethane and trioxane, then it is charged with carbon monoxide until a specific pressure is reached; the reaction is stirred and allowed to proceed at a temperature of about 180 °C for two hours. Upon completion the unreacted carbon monoxide is discharged and a mixture of low and high MW polyglycolide is collected.\n\nPolyglycolide is characterized by hydrolytic instability owing to the presence of the ester linkage in its backbone. The degradation process is erosive and appears to take place in two steps during which the polymer is converted back to its monomer glycolic acid: first water diffuses into the amorphous (non-crystalline) regions of the polymer matrix, cleaving the ester bonds; the second step starts after the amorphous regions have been eroded, leaving the crystalline portion of the polymer susceptible to hydrolytic attack. Upon collapse of the crystalline regions the polymer chain dissolves.\n\nWhen exposed to physiological conditions, polyglycolide is degraded by random hydrolysis, and apparently it is also broken down by certain enzymes, especially those with esterase activity. The degradation product, glycolic acid, is nontoxic, and it can enter the tricarboxylic acid cycle, after which it is excreted as water and carbon dioxide. A part of the glycolic acid is also excreted by urine.\n\nStudies undergone using polyglycolide-made sutures have shown that the material loses half of its strength after two weeks and 100% after four weeks. The polymer is completely resorbed by the organism in a time frame of four to six months. Degradation is faster in vivo than in vitro, this phenomenon thought to be due to cellular enzymatic activity.\n\nWhile known since 1954, PGA had found little use because of its sensitivity to hydrolysis when compared with other synthetic polymers. However, in 1962 this polymer was used to develop the first synthetic absorbable suture which was marketed under the tradename of Dexon by the Davis & Geck subsidiary of the American Cyanamid Corporation. After its coating with Polycaprolactone and Calcium Stearate is being sold under the brand name of Assucryl.\n\nPGA suture is classified as a synthetic, absorbable, braided multifilament. It is coated with N-laurin and L-lysine, which render the thread extremely smooth, soft and safe for knotting. It is also coated with magnesium stearate and finally sterilized with ethylene oxide gas. It is naturally degraded in the body by hydrolysis and is absorbed as water-soluble monomers, completed between 60 and 90 days. Elderly, anemic and malnourished patients may absorb the suture more quickly. Its color is either violet or undyed and it is sold in sizes USP 6-0 (1 metric) to USP 2 (5 metric). It has the advantages of high initial tensile strength, smooth passage through tissue, easy handling, excellent knotting ability, and secure knot tying. It is commonly used for subcutaneous sutures, intracutaneous closures, abdominal and thoracic surgeries.\n\nThe traditional role of PGA as a biodegradable suture material has led to its evaluation in other biomedical fields. Implantable medical devices have been produced with PGA, including anastomosis rings, pins, rods, plates and screws. It has also been explored for tissue engineering or controlled drug delivery. Tissue engineering scaffolds made with polyglycolide have been produced following different approaches, but generally most of these are obtained through textile technologies in the form of non-woven meshes.\n\nThe Kureha Chemical Industries has commercialized high molecular weight polyglycolide for food packaging applications under the tradename of Kuredux. Production is at Belle, West Virginia, with an intended capacity of 4000 annual metric tons. Its attributes as a barrier material result from its high degree of crystallization, the basis for a tortuous path mechanism for low permeability. It is anticipated that the high molecular weight version will have use as an interlayer between layers of polyethylene terephthalate to provide improved barrier protection for perishable foods, including carbonated beverages and foods that lose freshness on prolonged exposure to air. Thinner plastic bottles which still retain desirable barrier properties may also be enabled by this polyglycolide interlayer technology. A low molecular weight version (approximately 600 amu) is available from The Chemours Company. and is purported to be useful in oil and gas applications.\n"}
{"id": "323176", "url": "https://en.wikipedia.org/wiki?curid=323176", "title": "Primacord", "text": "Primacord\n\nPrimacord is a brand of detonating cord used in blasting. The registered trademark Primacord was originally owned by the Ensign-Bickford Company; Ensign-Bickford sold the trademark to Dyno Nobel in 2003, who manufacture it in their Graham, Kentucky factory. The name is also used as a genericized trademark for any detonating cord.\n\nPrimacord consists of a continuous core of PETN, RDX or other high explosive, bound by textile yarns and finished with plastic and wax as waterproofing agents. It is produced in eight strengths:\n\nPrimaline is a related product differing principally in having a plastic jacket instead of textile. Primaline is also available in higher loadings, up to 85 g/m (400 gr/ft):\n\nPrimacord is initiated with a blasting cap or by a \"donor line\" of detonating cord or other high explosive. It detonates along its entire length at a velocity of approximately 23,000 feet (7,000 meters) per second. It is used to create explosive effects and to build reliable explosive charges. It is used in conjunction with other high explosive materials to form charges, including linear charges, capable of near instantaneous results.\n\n"}
{"id": "25016560", "url": "https://en.wikipedia.org/wiki?curid=25016560", "title": "Sensitivity (explosives)", "text": "Sensitivity (explosives)\n\nSensitivity of explosives is the degree to which an explosive can be initiated by impact, heat, or friction.\n\nSensitivity, along with stability and brisance are three of the most significant properties of explosives that affect their use and application. All explosive compounds have a certain amount of energy required to initiate. If an explosive is too sensitive, it may go off accidentally. A safer explosive is less sensitive and will not explode if accidentally dropped or mishandled. However, such explosives are more difficult to initiate intentionally.\n\nLess sensitive explosives can be initiated by smaller quantities of more sensitive explosives, called primers or detonators, such as blasting caps. The use of increasingly less sensitive explosive materials to create an escalating chain reaction is known as an explosive train, initiation sequence, or firing train.\n\nHigh explosives are conventionally subdivided into two explosives classes, differentiated by sensitivity:\n"}
{"id": "45126246", "url": "https://en.wikipedia.org/wiki?curid=45126246", "title": "Silveh Dam", "text": "Silveh Dam\n\nThe Silveh Dam is an earth-fill embankment dam being constructed on the Lavin River just downstream of the village of Silveh in Piranshahr County, West Azerbaijan Province, Iran. The primary purpose of the dam is interbasin transfer for irrigation. When complete, a tunnel and canals will shift water from the reservoir north to the Chaparabad area. The project will essentially transfer water from the Little Zab River basin to the Lake Urmia basin in an effort to help replenish the lake and irrigate about of farmland. Construction on the dam began in 2004 and it is expected to be complete by the end of 2015. The village of Silveh will be flooded when the reservoir is impounded.\n\nThe dam will be above its foundation with a length of . It will have an uncontrolled spillway with a maximum discharge capacity of . The reservoir created by the dam will store of water. Near the northeastern edge of the reservoir water will be able to enter a long tunnel which will discharge it into the opposing valley, within the Lake Urmia basin. About of water will be transferred through the tunnel annually while an estimated will be sent downstream to Piranshahr during the same period.\n\n"}
{"id": "36519158", "url": "https://en.wikipedia.org/wiki?curid=36519158", "title": "Solar power forecasting", "text": "Solar power forecasting\n\nSolar power forecasting involves knowledge of the Sun´s path, the atmosphere's condition, the scattering processes and the characteristics of a solar energy plant which utilizes the Sun's energy to create solar power. Solar photovoltaic systems transform solar energy into electric power. The power output depends on the incoming radiation and on the solar panel characteristics. Photovoltaic power production is increasing nowadays. Forecast information is essential for an efficient use, the management of the electricity grid and for solar energy trading. Common solar forecasting method include stochastic learning method, local and remote sensing method, and hybrid method (Chu et al. 2016).\n\nThe energy generation forecasting problem is closely linked to the problem of \"weather variables forecasting\". Indeed, this problem is usually split into two parts, on one hand focusing on the forecasting of solar PV or any other meteorological variable and on the other hand estimating the amount of energy that a concrete power plant will produce with the estimated meteorological resource.\nIn general, the way to deal with this difficult problem is usually related to the spatial and temporal scales we are interested in, which yields to different approaches that can be found in the literature. In this sense, it is useful to classify these techniques depending on the forecasting horizon, so it is possible to distinguish between \"now-casting\" (forecasting 3–4 hours ahead), \"short-term forecasting\" (up to 7 days ahead) and \"long-term forecasting\" (months, years…)\nSolar radiation closely follows the physical and biological development of the earth. Its spatial and sequential heterogeneity powerfully influence the forcing of environmental and hydrological organisms by manipulating air temperature, soil moisture and vapor transpiration, snow cover and lots of photochemical procedures. Therefore, solar radiation drives place efficiency and plant life allotment, organism a key feature in undeveloped and forestry sciences that be obliged to be known precisely. The quantity of solar radiation obtainable at the earth’ surface is at the outset controlled at worldwide balance, organism above all precious by the Sun Earth geometry and the atmosphere. On the other hand, a complete explanation of its freedom time unpredictability require deliberation of limited procedure which frequently turn out to be also applicable, as is the casing in mountainous region. Predominantly, limited territory adjust the inward bound solar radiation by shadow casts, slope of elevation, surface gradient and compass reading, as a result, precise spatial model of inward bound solar radiation be supposed to regard as the pressure of the terrain surface. In the final time, more than a few events to consist of the confined terrain special effects in the solar radiation countryside have been projected, such as the use of Geographical Information Systems (GIS), artificial intelligence or post dispensation of satellite stand technique. Solar radiation can be also evaluated using numerical weather forecast (NWP) models. Nevertheless, the space and time balance determined with them and the incomplete computational ability frequently avoid the deliberation of terrain connected property.\n\nOtherwise, exclamation technique agree to us to acquire spatially persistent database from data evidence at inaccessible station greater than wide region. Even though their dependability is powerfully needy on the opening coldness between position, they eventually rely on experiential statistics, which have a superior precision than extra method. Therefore, while an adequate footage spatial thickness is accessible, disturbance method are preferred. Conventionally, solar radiation has not been as densely example as additional variables as temperature or rainfall, therefore the ease of use of capacity is frequently in short supply. Though, the number of experimental system which record solar radiation has developed and interruption has been converted into an appropriate technique for solar radiation evaluation. Nevertheless, radiometric stations are frequently come together approximately farmland or occupied region, typically during basin and plane area, while mountains at rest require enough footage thickness. This truth is particularly applicable afford the tall spatial unpredictability of solar radiation in these province. As an outcome, particular interruption method that tolerate include outdoor foundation should be used to make clear this extra spatial unpredictability. Several diverse spatial interruption techniques can be established. On the other hand, data ease of use in mountainous region is often extremely restricted. As a result, it is hard to construct a precise solar radiation climatology in hilly area to be used in environmental science, climate change.\n\nSolar radiation is a hardly illustration changeable with reverence to supplementary ecological variables such as temperature or precipitation, in fraction payable to the high maintenance price of the necessary radiometric sensors. It is extremely sensitive to ecological feature on or after local to limited balance. Predominantly, terrain surface confronts the conventional interruption method while forecast through far above the ground spatial decision are required, particularly due to the lack of measurement stations in mountainous areas. Geo-statistics front a stochastic move toward to resolve the spatial forecast difficulty that stop dependence on before imagine deterministic models and permit us to consist of the consequence of outside in sequence foundation stand on investigation data-sets.\n\nNowcasting comprises the detailed description of the current weather along with forecasts for up to 3–4 hours. This very short-term forecasting service is very important for grid operators in order to guarantee the grid stability and for those power plants that can be considered manageable, at least in a certain degree, such as solar thermal power plants.\nNowcasting services are usually related to very high temporal resolution (a forecast every 10 or 15 minutes), so automatic weather data acquisition and processing is a major requirement in order to develop these techniques. Several approaches can be found in the literature, which mainly depend on the type of data that is treated to estimate future values of meteorological variables:\n\n\n\"Short-term\" forecasting provides predictions up to 7 days ahead. This kind of forecast is also valuable for grid operators in order to make decisions of grid operation, as well as, for electric market operators.\nUnder this perspective, the meteorological resources are estimated at a different temporal and spatial resolution. This implies that meteorological variables and phenomena are looked from a more general perspective, not as local as nowcasting services do. In this sense, most of the approaches make use of different numerical weather prediction models (NWP) that provide an initial estimation of weather variables. Currently, several models are available for this purpose, such as Global Forecast System (GFS) or data provided by the European Center for Medium Range Weather Forecasting (ECMWF). These two models are considered the state of the art of global forecast models, which provide meteorological forecasts all over the world.\nIn order to increase spatial and temporal resolution of these models, other models have been developed which are generally called mesoscale models. Among others, HIRLAM, WRF or MM5 are the most representative of these models since they are widely used by different communities.\nTo run these models a wide expertise is needed in order to obtain accurate results, due to the wide variety of parameters that can be configured in the models. In addition, sophisticated techniques such as data assimilation might be used in order to produce more realistic simulations.\nFinally, some communities argue for the use of post-processing techniques, once the models’ output is obtained, in order to obtain a probabilistic point of view of the accuracy of the output. This is usually done with ensemble techniques that mix different outputs of different models perturbed in strategic meteorological values and finally provide a better estimate of those variables and a degree of uncertainty, like in the model proposed by Bacher et al. (2009)\n\n\"Long-term\" forecasting usually refers to forecasting of the annual or monthly available resource. This is useful for energy producers and to negotiate contracts with financial entities or utilities that distribute the generated energy.\nIn general, these long-term forecasting is usually done at a lower scale than any of the two previous approaches. Hence, most of these models are run with mesoscale models fed with reanalysis data as input and whose output is postprocessed with statistical approaches based on measured data.\n\nAny output from any model described above must then be converted to the electric energy that a particular solar PV plant will produce. This step is usually done with statistical approaches that try to correlate the amount of available resource with the metered power output. The main advantage of these methods is that the meteorological prediction error, which is the main component of the global error, might be reduced taking into account the uncertainty of the prediction. As it was mentioned before and detailed in \"Heinemann et al.\", these statistical approaches comprises from ARMA models, neural networks, support vector machines, etc.\nOn the other hand, there also exist theoretical models that describe how a power plant converts the meteorological resource into electric energy, as described in Alonso et al. The main advantage of this type of models is that when they are fitted, they are really accurate, although they are too sensitive to the meteorological prediction error, which is usually amplified by these models.\nHybrid models, finally, are a combination of these two models and they seem to be a promising approach that can outperform each of them individually.\n\n\n\n\n\n\n\n"}
{"id": "12870923", "url": "https://en.wikipedia.org/wiki?curid=12870923", "title": "Starting vortex", "text": "Starting vortex\n\nThe starting vortex is a vortex which forms in the air adjacent to the trailing edge of an airfoil as it is accelerated from rest in a fluid. It leaves the airfoil (which now has an equal but opposite \"bound vortex\" around it), and remains (nearly) stationary in the flow. It rapidly decays through the action of viscosity.\n\nThe initial (and quite brief) presence of a starting vortex as an airfoil begins to move was predicted by early aerodynamicists, and eventually photographed.\n\nWhenever the speed or angle of attack of an airfoil changes there is a corresponding amount of vorticity deposited in the wake behind the airfoil, joining the two trailing vortices. This vorticity is a continuum of mini-starting-vortexes. The wake behind an aircraft is a continuous sheet of weak vorticity, between the two trailing vortices, and this accounts for the changes in strength of the trailing vortices as the airspeed of the aircraft and angle of attack on the wing change during flight. (The strength of a vortex cannot change within the fluid except by the dissipative action of viscosity. Vortices either form continuous loops of constant strength, or they terminate at the boundary of the fluid - usually a solid surface such as the ground.)\n\nThe starting vortex is significant to an understanding of the Kutta condition and its role in the circulation around any airfoil generating lift.\n\nThe starting vortex has certain similarities with the \"starting plume\" which forms at the leading edge of a slug of fluid, when one fluid is injected into another at rest. See plume (hydrodynamics).\n\n\n"}
{"id": "28309128", "url": "https://en.wikipedia.org/wiki?curid=28309128", "title": "State Oil Fund of Azerbaijan", "text": "State Oil Fund of Azerbaijan\n\nThe State Oil Fund of the Republic of Azerbaijan (SOFAZ) is Azerbaijan’s sovereign wealth fund, whereby energy-related earnings are accumulated and efficiently managed for future generations.\n\nSOFAZ was established in accordance with the Decree of National Leader Heydar Aliyev No. 240 on December 29, 1999. Statute of SOFAZ was approved by the Decree of the President of the Republic of Azerbaijan № 434 dated December 29, 2000. SOFAZ began operating after the approval of the Decree on \"Rules on management of foreign currency assets of the State Oil Fund of the Republic of Azerbaijan\" by the President of the Republic of Azerbaijan dated June 19, 2001. The establishment of SOFAZ plays a key role in the oil strategy founded by Azerbaijan’s National Leader Heydar Aliyev.\n\nSOFAZ's mission is to transform depletable hydrocarbon reserves into financial assets generating perpetual income for current and future generations.\n\nThe cornerstone of Sofaz’s is to ensure intergenerational equality with regard to the country's oil wealth. SOFAZ was founded in order to manage the oil and gas incomes efficiently as well as to invest them in the development of socio-economically important projects and advance spheres.\n\nSOFAZ’s activity is directed to the achievement of the following objectives:\n\n1.Supporting macroeconomic stability, participating in ensuring fiscal-tax discipline and decreasing dependence on oil revenues while stimulating development of the non-oil sector;\n\n2.Funding major national scale projects to support socio-economic progress;\n\n3.Ensuring intergenerational equality with regard to the country’s oil wealth, accumulating and preserving oil revenues for future generations.\n\nThe Supervisory Board, consisting of representatives of the state authorities and public organizations, carries out general oversight of SOFAZ’s operations. The Board reviews and evaluates SOFAZ’s draft annual budget, annual report and financial statements, along with audit report. Members of the Supervisory Board are approved by the President of the Republic of Azerbaijan. The board members act entirely on a voluntary basis.\n\nSOFAZ’s day-to-day activities are managed by the Executive Director appointed by the President of the Republic of Azerbaijan. \nThe Executive Director represents the Fund, appoints and dismisses employees of SOFAZ in a manner determined by the legislation, carries out operational management of SOFAZ’s activities, ensures the management and investment of SOFAZ’s assets in accordance with the Guidelines approved by the President of the Republic of Azerbaijan. \nExecution of SOFAZ’s budget is implemented on the basis of comments provided by the Supervisory Board in accordance with the SOFAZ budget approved by the proper Ordinances of the President of the Republic of Azerbaijan. SOFAZ assets are managed in accordance with the \"Rules on management of foreign currency assets of the State Oil Fund of the Republic of Azerbaijan\" and Investment Policy.\n\nThe sources of income for SOFAZ are:\n\n\nIn compliance with regulations relating to SOFAZ, the Fund's assets may be used for solving major problems affecting the nation, and for construction and reconstruction of strategically important infrastructure to support socio-economic progress.\n\nTransfers to the state budget also constitute a significant component of the Fund's outflow. A very small portion of the Fund's assets is utilized to cover operational costs.\n\nAs per the Law \"About Budget System\" of the Republic of Azerbaijan, all SOFAZ expenditures, except operating expenditures, are incorporated as part of an annual consolidated government budget presented to the Parliament for approval. In compliance with this Law, SOFAZ can only execute expenditures envisaged by its budget.\n\nSOFAZ is funding projects of national scale. They are:\n\n1.Azerbaijan's equity share in the Baku-Tbilisi-Ceyhan Baku-Tbilisi-Ceyhan (BTC) oil pipeline project (completed);\n\n2.Building of housing and improvement of socio-economic conditions of refugees and internally displaced persons who were forced to flee their native lands as a result of Armenian-Azerbaijan conflict;\n\n3.Oguz-Qabala-Baku water supply system (completed);\n\n4.Reconstruction of the Samur-Absheron irrigation system;\n\n5.Formation of the statutory capital of the State Investment Company (completed);\n\n6.Financing Baku-Tibilisi-Kars railway;\n\n7.Financing “The state program on the education of Azerbaijan youth abroad in the years 2007-2015”;\n\n8.Financing of the share of Azerbaijan Republic in the construction of the STAR oil refinery project in Turkey;\n\n9.Financing the share of the Republic of Azerbaijan in “South Gas Corridor” CJSC statutory capital;\n\n10.Financing the share of Azerbaijan Republic in the “Construction of New Semi-submersible Drilling rig Project” (completed).\n\n11.Financing of the share of the Republic of Azerbaijan in the “Construction of Oil, Gas Processing and Petrochemical Complex Project”\n\nSOFAZ's assets are managed in accordance with the \"Rules on management of foreign currency assets of the State Oil Fund of the Republic of Azerbaijan\" approved by Presidential Decree No. 511 dated June 19.2001 and “Investment Policy of the State Oil Fund of the Republic of Azerbaijan” approved by Presidential decrees on an annual basis.\nAccording to Investment Policy currency composition of the investment portfolio is as follows, subject to the requirement of at least 90% allocation to currencies of countries with credit ratings of no less than “A” (by Standard and Poor’s, Fitch) and “A2” (by Moody’s):\n\n1. 50% invested in assets denominated in USD;\n\n2. 35% invested in assets denominated in EUR;\n\n3. 5% invested in assets denominated in GBP.\n\n4. 10% of the investment portfolio can be invested in assets denominated in other currencies.\nInvestment portfolio of SOFAZ consists of the following sub-portfolios:\n\n1. Debt obligations and money market instruments Portfolio – 60% of the investment portfolio along with maximum lower deviation of 5% (up to 5% can be invested in non-rated fixed income debt funds);\n\n2. Equity Portfolio – up to 25% of the investment portfolio (up to 5% can be invested in private equity funds);\n\n3. Real estate Portfolio – up to 10% of the investment portfolio along with maximum upper deviation of 2%;\n\n4. Gold Portfolio – up to 5% of the investment portfolio along with maximum upper deviation of 3%.\n\nThe assets of SOFAZ as of October 1, 2018 have increased by 8.88% compared to the beginning of 2018 (USD 35 806.5 mln.) and stood at USD 38 987.7 mln. \n\nSOFAZ’s budget revenues for the period of January-September, 2018 reached 14 437.2 million manats, while budget expenditures constituted 8 005.9 million manats.\n\nSOFAZ always pays attention to transparency in its activities. A key measure to promote transparency within SOFAZ’s reporting system and operation is a regular audit of the Fund's financial activities by a reputable firm of international auditors.\n\nInformation and press releases, and both quarterly and annual statements, in Azerbaijani and English, about the Fund's revenues and expenditures are regularly published in the local press, posted on SOFAZ's official web-site (www.oilfund.az) and on social media pages (Facebook, Twitter) to reflect developments in varying areas of SOFAZ’s activities, and also to promote transparency in the utilization of revenues from oil.\n\nSOFAZ won the 2007 UN Public Service Award for Improving Transparency, Accountability and Responsiveness in Public Service. The United Nations Public Service Awards is the most prestigious international recognition of excellence in public service. Established in 2003, it rewards creative achievements and contributions of public service institutions to more effective and responsive public administration in countries worldwide. SOFAZ is the first governmental agency that is awarded the UN Public Service Award among the governmental institutions of the Eastern Europe and CIS countries.\n\nTaking into account that Azerbaijan is the first country to complete validation- the EITI's quality assurance process, which verifies compliance with EITI principles and criteria,“EITI Award 2009” was awarded to Azerbaijan, at the 4th EITI International Conference held on February 2009 in Doha, Qatar.\n\nA Sovereign Wealth Fund (SWF) is a state-owned investment fund composed of financial assets such as stocks, bonds, property, precious metals or other financial instruments that is formed from revenues from natural resources. The term sovereign wealth fund was first used in 2005. The International Working Group of Sovereign Wealth Funds (IWG) announced its establishment in April 2009 in the city-capital of Kuwait.\nSOFAZ cooperates with other SWF's within the framework of International Forum of Sovereign Wealth Funds (IFSWF). IFSWF is a voluntary group of SWFs, which meets, exchanges views on issues of common interest, and facilitates an understanding of the Santiago Principles and SWF activities. The inaugural meeting of IFSWF was held in Baku on October 8–9, 2009 and it was hosted by SOFAZ and the Government of the Republic of Azerbaijan. According to the Principle 24 of SP , SOFAZ has prepared its first self-assessment report on the implementation of SP and intends to review it on an annual basis.\n\nThe general appearance of SOFAZ’s logo portrays a man holding something dear and valuable in his hands, symbolizing SOFAZ’s main activity to preserve Azerbaijan's wealth for future generations.\n\nHands of the figure are described as two butas giving the logo an inimitable national flavour.\n\nArea between the hands is described as the eye of the Omniscience directed to the future of the country and the nation in which SOFAZ plays a crucial role.\nThe sphere between the hands can also be described as the heart of SOFAZ, symbolizing the Fund's transparency.\n\nHeydar Aliyev avenue 165, AZ 1029, Baku, the Republic of Azerbaijan\n\nWeb site: SOFAZ website\n\n"}
{"id": "38718486", "url": "https://en.wikipedia.org/wiki?curid=38718486", "title": "Sugarcane wax", "text": "Sugarcane wax\n\nSugarcane wax is a wax extracted from sugarcane.\n\nThe production of sugarcane wax is difficult and economically intensive. Sugarcane is used almost exclusively to produce sugar. More importantly, there is just about 0.1 % of sugarcane wax in sugarcane. Therefore, economic productions can only be found in the major cultivation countries Brazil, India, China, Thailand, Pakistan and Mexico. During the production of sugar remains a filter residue, the so-called bagasse. The sugar cane wax is obtained from this filter residue. In that process, plant residues and chlorophyll are separated from the sugarcane.\nAround 1840, for the first time the American pharmacist Avequin was able to produce sugarcane wax out of the bagasse in a relatively pure form. In his quantitative analysis he found that there is just 0.1 % of the whitish to dark yellow wax in sugarcane. In 1909, more than 60 years later, the Frenchman A. Wynberg was granted a patent for the production of sugarcane wax by extracting it out of the bagasse. During World War I, one of the first companies that produced large amounts of sugarcane waxes established in the South African province Natal. Already in 1924, 6000 tons of dark sugarcane wax were produced there. This amount was mainly used for the production of candles for the Russian Orthodox Church. Because of the Russian Revolution and the associated war against the church the consumption of sugarcane wax decreased so much that the company had to be closed. In the following period, sugarcane wax was produced in the USA, mainly in Louisiana where there were up to 22 sugarcane wax producers. Already in 1922, M. Rindl described applications for sugarcane wax in detail, for example as substitute for Carnauba, Bee and Montan waxes.\n\nSugarcane wax consists of the following ingredients: about 70 % of alcohols of long-chain hydrocarbons having chain lengths of C 18 to C 32, wax acids having chain lengths of C 18 to C 32, ω-hydroxycarboxylic acids and aromatic carboxylic acids. Also fatty alcohols (wax alcohols) and diols are alcohol components. Besides that, about 5 to 10 % consist of unesterified diols, long-chain wax acids such as behenic, cerotic, lignoceric or melissic acid and saturated hydrocarbons. Untreated sugarcane wax contains up to 25 % of resin and moreover, up to 60 % polycosanol (octacosanol) which can be extracted from sugarcane wax in the pure form.\nSugarcane wax is indigestible and harmless to health. In case of accidental consumption it is excreted. In its refined form it has a light yellowish colour. Due to the high melting point of 75 to 80 ° C it remains stable even if exposed to direct sunlight. Sugarcane wax offers a good oil and solvent retention for anionic bright emulsions.\nUntil the 1960s, sugar cane wax was added to the production of chewing gum as an edible wax. In this process the sugar cane wax acted as elastomer or as plasticizer and consistency regulator. In 1943, J. W. Schlegel and L. Lang were granted a patent to flour donuts with sugar. The ground sugar was mixed with 0.4 % of sugar cane wax. Thus, the donuts became fat- and water-repellent and kept their fresh appearance longer. Also chocolate was thinly coated with sugarcane wax. The gloss durability improved, the melting reduced and the packing was relieved as well. In order to keep vegetables and fruits fresh or to make it look fresh emulsions from sugarcane wax mixed with other natural waxes were prepared. The vegetables or fruits were immersed in the emulsions or sprayed with wax emulsions. \nIn the 1970s, a further field of application for polycosanols made from sugarcane wax was found in the medical industry, the lowering of the cholesterol level. This had been pursued with various investigations by the Cuban laboratory Dalmer S.A. in Havana, especially against the background of the fall of the Iron Curtain. During this time, there were many - also controversial - patent applications. The last known patent was applied in 1998 by S.A. Dalmer. Those results were analysed, summarised and developed further by I. Gouni-Berthold and H. K. Berthold. From 2004 to 2007 the cholesterol-lowering effect of polycosanols made from sugarcane wax was transferred to montan waxes because of their very similar chemical structure by E. Krendlinger and M. Neumaier. Currently, nutritional supplements containing polycosanols for lowering the cholesterol levels are offered in a wide range.\n\nSugarcane wax is not only suitable for technical applications but also for applications in the food industry. So, sugarcane wax can be used as care product (shoe, floor and car care), in the leather and plastics industry as well as for applications in the additive and cosmetics industry. Furthermore, it is applicable in the paints and printing inks industry and for the production of candles.\n"}
{"id": "3552981", "url": "https://en.wikipedia.org/wiki?curid=3552981", "title": "Surface freezing", "text": "Surface freezing\n\nSurface freezing is the appearance of long-range crystalline order in a near-surface layer of a liquid. The surface freezing effect is opposite to a far more common surface melting, or premelting. Surface Freezing was experimentally discovered in melts of alkanes and related chain molecules in the early 1990s independently by two groups. John Earnshaw and his group (Queen's University of Belfast) used light scattering, which did not allow a determination of the frozen layer's thickness, and whether or not it is laterally ordered. A group led by Ben Ocko (Brookhaven National Laboratory), Eric Sirota (Exxon) and Moshe Deutsch (Bar-Ilan University, Israel) discovered independently the same effect, using x-ray surface diffraction which allowed them to show that the frozen layer is a crystalline monolayer, with molecules oriented roughly along the surface normal, and ordered in an hexagonal lattice. A related effect, the existence of a smectic phase at the surface of a nematic liquid bulk was observed in liquid crystals by Jens Als-Nielsen (Risø National Laboratory, Denmark) and Peter Pershan (Harvard University) in the early 1980s. However, the surface layer there was neither ordered, nor confined to a single layer. Surface freezing has since been found in a wide range of chain molecules and at various interfaces: liquid-air, liquid-solid and liquid-liquid.\n"}
{"id": "6977160", "url": "https://en.wikipedia.org/wiki?curid=6977160", "title": "Tetraxenonogold(II)", "text": "Tetraxenonogold(II)\n\nTetraxenonogold(II), gold tetraxenide(II) or AuXe is a cationic complex with a square planar configuration of atoms. It is found in the compound AuXe(SbF), which exists in triclinic and tetragonal crystal modifications. The AuXe ion is stabilised by interactions with the fluoride atoms of the counterion. The Au-Xe bond length is 274 pm = 2.74 angstroms.\n\nTetraxenonogold(II) is unusual in that it is a compound of the notoriously inert atoms xenon and gold. It is also unusual in that it uses xenon as a transition metal ligand, and in that it contains gold in the +2 oxidation state. It can be produced by reduction of AuF in the presence of fluoroantimonic acid and xenon, and crystallised at low temperature. The xenon bonds with the gold(II) ion to make this complex.\n\nIt was the first description of a compound between a noble gas and a noble metal. It was first described in the year 2000 by Konrad Seppelt and Stefan Seidel.\n"}
{"id": "7633546", "url": "https://en.wikipedia.org/wiki?curid=7633546", "title": "Tornadogenesis", "text": "Tornadogenesis\n\nTornadogenesis is the process by which a tornado forms. There are many types of tornadoes, and each type of tornado can have several different methods of formation. Despite ongoing scientific study and high-profile research projects such as VORTEX, many of the mechanisms of tornado formation are still poorly understood.\n\nClassical tornadoes are supercellular tornadoes, which have a recognizable pattern of formation. The cycle begins when a strong thunderstorm develops a rotating mesocyclone a few miles up in the atmosphere. As rainfall in the storm increases, it drags with it an area of quickly descending air known as the rear flank downdraft (RFD). This downdraft accelerates as it approaches the ground, and drags the rotating mesocyclone towards the ground with it. Storm relative helicity (SRH) has been shown to play a role in tornado development and strength. SRH is horizontal vorticity that is parallel to the Inflow of the storm and is tilted upwards when it is taken up by the updraft, thus creating vertical vorticity.\n\nAs the mesocyclone lowers below the cloud base, it begins to take in cool, moist air from the downdraft region of the storm. This convergence of warm air in the updraft, and this cool air, causes a rotating wall cloud to form. The RFD also focuses the mesocyclone's base, causing it to siphon air from a smaller and smaller area on the ground. As the updraft intensifies, it creates an area of low pressure at the surface. This pulls the focused mesocyclone down, in the form of a visible condensation funnel. As the funnel descends, the RFD also reaches the ground, creating a gust front that can cause severe damage a good distance from the tornado. Usually, the funnel cloud begins causing damage on the ground (becoming a tornado) within a few minutes of the RFD reaching the ground.\n\nField studies have shown that in order for a supercell to produce a tornado the RFD needs to be no more than a few Kelvin cooler than the updraft. Also the FFD (Forward Flank Downdraft) seems to be warmer within tornadic supercells than it is the case in non-tornadic supercells.\n\nWaterspouts are defined as tornadoes over water. However, while some waterspouts are supercellular (also known as \"tornadic waterspouts\"), forming in a process similar to that of their land-based counterparts, most are much weaker and caused by different processes of atmospheric dynamics. They normally develop in moisture-laden environments with little vertical wind shear in areas where wind comes together (convergence), such as land breezes, lake effect bands, lines of frictional convergence from nearby landmasses, or surface troughs. Waterspouts normally develop as their parent clouds are in the process of development. It is theorized that they spin upward as they move up the surface boundary from the horizontal shear near the surface, and then stretch upward to the cloud once the low level shear vortex aligns with a developing cumulus or thunderstorm. Their parent cloud can be as innocuous as a moderate cumulus, or as significant as a supercell.\n\nLandspouts are tornadoes that do not form from supercells and are similar in appearance and structure to fair-weather waterspouts with the exception that they form over land instead of water. They are thought to form in a manner similar to that of weaker waterspouts in that they form during the growth stage of convective clouds by the ingestion and tightening of boundary layer vorticity by the cumuliform tower's updraft.\n\nThough these are widely accepted hypotheses for how most tornadoes form, they do not explain the formation of long-lived tornadoes, or tornadoes with multiple vortices. These each have different mechanisms which influence their development—however, most tornadoes follow a pattern similar to these ones. There are still many aspects about the formation of tornadoes which remain a mystery. Research programs, including VORTEX, deployment of TOTO (the TOtable Tornado Observatory), and dozens of other programs, hope to solve many questions that still plague meteorologists about this topic.\n\n\n"}
{"id": "4782123", "url": "https://en.wikipedia.org/wiki?curid=4782123", "title": "Units of energy", "text": "Units of energy\n\nBecause energy is defined via work, the SI unit for energy is the same as the unit of work – the joule (J), named in honor of James Prescott Joule and his experiments on the mechanical equivalent of heat. In slightly more fundamental terms, 1 joule is equal to 1 newton metre and, in terms of SI base units\n\nAn energy unit that is used in atomic physics, particle physics and high energy physics is the electronvolt (eV). One eV is equivalent to 1.60217653×10 J.\n\nIn spectroscopy the unit cm = 0.000123986 eV is used to represent energy since energy is inversely proportional to wavelength from the equation formula_2.\n\nIn discussions of energy production and consumption, the units barrel of oil equivalent and ton of oil equivalent are often used.\n\nWhen discussing amounts of energy released in explosions or bolide impact events, the TNT equivalent unit is often used.\n\nThe British Imperial units and U.S. Customary units for both energy and work include the foot-pound force (1.3558 J), the British thermal unit (BTU) which has various values in the region of 1055 J, the horsepower-hour (2.6845 MJ), and the gasoline gallon equivalent (about 120 MJ).\n\nThe energy unit used for everyday electricity, particularly for utility bills, is the kilowatt-hour (kWh); one kWh is equivalent to 3.6×10 J  (3600 kJ or 3.6 MJ). Electricity usage is often given in units of kilowatt-hours per year (kWh/yr). This is actually a measurement of average power consumption, i.e., the average rate at which energy is transferred. One kWh/yr is about 0.11 watts.\n\nNatural gas in the US is sold in Therms or 100 cubic feet (100 ft = 1 Ccf). One Therm is equal to about 105.5 megajoules. In Australia, natural gas is sold in megajoules. In the most of the world, natural gas is sold in gigajoules.\n\nThe calorie equals the amount of thermal energy necessary to raise the temperature of one gram of water by 1 Celsius degree, from a temperature of 14.5 degrees Celsius, at a pressure of 1 atm. For thermochemistry a calorie of 4.184 J is used, but other calories have also been defined, such as the International Steam Table calorie of 4.1868 J. Food energy is measured in large calories or kilocalories, often simply written capitalized as \"Calories\" (= 10 calories).\n\nIn physics and chemistry, it is still common to measure energy on the atomic scale in the non-SI, but convenient, units electronvolts (eV). The Hartree (the atomic unit of energy) is commonly used in calculations. Historically Rydberg units have been used.\n\nIn spectroscopy and related fields it is common to measure energy levels in units of reciprocal centimetres. These units (cm) are strictly speaking not energy units but units proportional to energies, with formula_3 being the proportionality constant.\n\nA gram of TNT releases 980–1100 calories upon explosion. To define the tonne of TNT, this was arbitrarily standardized by letting 1000 thermochemical calories = 1 gram TNT = 4184 J (exactly).\n\n"}
{"id": "29156200", "url": "https://en.wikipedia.org/wiki?curid=29156200", "title": "Waymo", "text": "Waymo\n\nWaymo is a self-driving technology development company. It is a subsidiary of Alphabet Inc. Waymo originated as a project of Google before it became a stand alone subsidiary in December 2016. \n\nWaymo is currently running a trial of an autonomous ride-hailing business in Phoenix, Arizona. The company has announced that the service will be available for public use by the end of 2018.\n\nGoogle's development of self-driving technology began in 2009 at the company's secretive X lab run by co-founder Sergey Brin. The project was originally led by Sebastian Thrun, former director of the Stanford Artificial Intelligence Laboratory and co-inventor of Google Street View. Thrun's team at Stanford created the robotic vehicle Stanley, which won the 2005 DARPA Grand Challenge and its prize from the United States Department of Defense. The team developing the system consisted of 15 engineers working for Google, including Chris Urmson, Dmitri Dolgov, Mike Montemerlo, and Anthony Levandowski who had worked on the DARPA Grand and Urban Challenges.\nStarting in 2010, lawmakers in various states expressed concerns over how to regulate the emerging technology. Nevada passed a law in June 2011 concerning the operation of autonomous cars in Nevada, which went into effect on March 1, 2012. A Toyota Prius modified with Google's experimental driverless technology was licensed by the Nevada Department of Motor Vehicles (DMV) in May 2012. This was the first license issue in the United States for a self-driven car.\n\nIn late May 2014, Google revealed a new prototype of its driverless car, which had no steering wheel, gas pedal, or brake pedal, being 100% autonomous, and unveiled a fully functioning prototype in December of that year that they planned to test on San Francisco Bay Area roads beginning in 2015. Called the Firefly, the car was intended to serve as a platform for experimentation and learning, not mass production.\n\nIn 2015, Google provided \"the world's first fully driverless ride on public roads\" to a legally blind friend of principal engineer Nathaniel Fairfield. The ride was taken by Steve Mahan, former CEO of the Santa Clara Valley Blind Center, in Austin, Texas. It was the first driverless ride that was on a public road and was not accompanied by a test driver or police escort. The car had no steering wheel or floor pedals.\n\nIn December 2016, the unit was renamed Waymo, and made into its own separate division in Alphabet. The name Waymo is derived from its mission, \"a new way forward in mobility\". Waymo moved to further test its cars on public roads after becoming its own subsidiary.\n\nIn 2017, Waymo sued Uber for allegedly stealing trade secrets. A court filing in lawsuit revealed Google has spent over $1.1 billion on the project between 2009 and 2015, to be compared with the $1 billion acquisition of Cruise Automation by General Motors in March 2016, a similar investment by Ford in a joint venture with Argo AI in February 2017, or the $680 million for Otto's acquisition by Uber in August 2016. Waymo and Uber settled in February 2018, with Uber granting Waymo $245 million worth of Uber stock.\n\nWaymo began testing autonomous minivans without a safety driver on public roads in Chandler, Arizona, in October 2017. The company announced in January 2018 that it would begin its ride-hailing services in the Phoenix, Arizona, area later in the year. \n\nIn 2017, Waymo unveiled new sensors and chips that are less expensive to manufacture, cameras that improve visibility, and wipers to clear the lidar system. Waymo manufactures a suite of self-driving hardware developed in-house. These sensors and hardware—enhanced vision system, improved radar, and laser-based lidar—reduce Waymo's dependence on suppliers. The in-house production system allows Waymo to efficiently integrate its technology to the hardware. In the beginning of the self-driving car program, the company spent $75,000 for each lidar system from Velodyne. As of 2017, that cost was down approximately 90 percent, due to Waymo designing its own version of lidar.\n\nWaymo officials said the cars the company uses are built for full autonomy with sensors that give 360 degree views and lasers that detect objects up to 300 meters away. Short-range lasers detect and focus on objects near the vehicle, while radar is used to see around vehicles and track objects in motion. The interior of these cars include buttons for riders to control certain functions: \"Help\", \"Lock\", \"Pull over\", and \"Start ride\".\n\nWaymo engineers have also created a program called Carcraft, a virtual world where Waymo can simulate driving conditions. The simulator is named after the video game \"World of Warcraft\". With Carcraft, 25,000 virtual self-driving cars navigate through models of Austin, Texas, Mountain View, California, Phoenix, Arizona, and other cities. , Waymo has driven more than 5 billion miles in the virtual world.\n\nWaymo has created partnerships with Fiat Chrysler Automobiles, Lyft, AutoNation, Avis, Intel, and Jaguar Land Rover. \n\nThe Waymo project team has equipped various types of cars with the self-driving equipment, including the Toyota Prius, Audi TT, Fiat Chrysler Pacifica and Lexus RX450h. Google also developed their own custom vehicle, about 100 of which were assembled by Roush Enterprises with equipment from Bosch, ZF Lenksysteme, LG, and Continental.\n\nIn May 2016, Google and Fiat Chrysler Automobiles announced an order of 100 Chrysler Pacifica hybrid minivans to test the self-driving technology. Waymo ordered an additional 500 Pacifica hybrids in 2017 and in late May 2018, Alphabet announced plans to add up to 62,000 Pacifica Hybrid minivans to the fleet. In March 2018, Jaguar Land Rover announced that Waymo had ordered up to 20,000 of its planned electric I-Pace cars, at an estimated cost more than $1 billion. Jaguar is to deliver the first I-Pace prototype later in the year, and the cars are to become part of Waymo's ride-hailing service in 2020.\n\nWaymo partners with Intel to use Intel technologies, such as processors, inside Waymo vehicles. Its deals with Avis and AutoNation are for vehicle maintenance. With Lyft, Waymo is partnering on pilot projects and product development.\n\n, Waymo had tested its system in six states and 25 cities across the U.S over a span of more than 9 years. Among the first places Google began testing its self-driving cars in 2009 was San Francisco Bay Area. Google's vehicles have traversed San Francisco's Lombard Street, famed for its steep hairpin turns, and through city traffic. The vehicles have driven over the Golden Gate Bridge and around Lake Tahoe. The system drives at the speed limit it has stored on its maps and maintains its distance from other vehicles using its system of sensors. It has since expanded its areas of testing.\n\nIn August 2012, the team announced that they had completed over 300,000 autonomous-driving miles (500,000 km) accident-free, typically having about a dozen cars on the road at any given time. Four U.S. states had passed laws permitting autonomous cars as of December 2013: Nevada, Florida, California, and Michigan. A law proposed in Texas would establish criteria for allowing \"autonomous motor vehicles\".\n\nIn April 2014, the team announced that their vehicles had logged nearly 700,000 autonomous miles (1.1 million km). In June 2015, the team announced that their vehicles had driven over , stating that this was \"the equivalent of 75 years of typical U.S. adult driving\", and that in the process they had encountered 200,000 stop signs, 600,000 traffic lights, and 180 million other vehicles. Google also announced its prototype vehicles were being road tested in Mountain View, California. During testing, the prototypes' speed did not exceed and had safety drivers aboard the entire time. As a consequence, one of the vehicles was stopped by police for impeding traffic flow.\n\nIn 2015, Google expanded its road-testing to Texas, where regulations did not prohibit cars without pedals and a steering wheel. Bills were introduced by interested parties to similarly change the legislation in California.\n\nGoogle took its first driverless ride on public roads in October 2015, when Mahan took a 10-minute solo ride around Austin in a Google \"pod car\" with no steering wheel or pedals. In 2016, the company expanded its road testing to the dry Phoenix, Arizona, area and Kirkland, Washington, which has a wet climate. In May 2016, the company opened a 53,000 square foot self-driving technology development center in Novi, Michigan. , Google had test driven their fleet of vehicles, in autonomous mode, a total of . In August 2016 alone, their cars traveled a \"total of 170,000 miles; of those, 126,000 miles were driven autonomously (i.e., the car was fully in control)\". Beginning of 2017, Waymo reported to California DMV a total of 636,868 miles covered by the fleet in autonomous mode, and the associated 124 disengagements, for the period from December 1, 2015 through November 30, 2016.\n\nIn November 2017, Waymo altered its Arizona testing by removing safety drivers in the driver position from their autonomous Chrysler Pacificas. The cars were geofenced within a 100 square miles surrounding Chandler, Arizona. Waymo's early rider program members were the first to take rides using the new technology.\n\nWaymo began testing its level 4 autonomous cars in Arizona for several reasons: good weather, simple roads, and the state not requiring that self-driving cars have any special permissions. Users hail vehicles through a Waymo app and an onboard support system can connect them to a Waymo agent at any time. In 2017, Waymo began weather testing in Michigan. Also in 2017, Waymo unveiled its test facility, Castle, on 91 acres in Central Valley, California. Castle, a former air base, has served as the project's training course since 2012.\n\nAccording to a Waymo report, as of March 2018 Waymo's self-driving technology had driven more than 5 million miles on public roads and more than 5 billion miles via simulation. Waymo's 25,000 virtual self-driving cars travel 8 million miles per day. By October 2018, Waymo had completed 10 million miles of driving on public roads and over 7 billion simulation miles.\n\nIn March 2018, Waymo announced its plans to build additional real-world self-driving experiments with the company's self-driving trucks delivering for sister company Google's data centers located in Atlanta, Georgia.\n\n, Waymo was waiting on permits to test the cars in California, hoping to test in Los Altos, Mountain View, Palo Alto, and Sunnyvale. On 30 October 2018, the California Department of Motor Vehicles issued a permit for Waymo to operate fully driverless cars (i.e., cars without a human safety drivers). Waymo was the first company to receive such a permit, which allows day and night testing on public roads and highways, in California. In a blog post, Waymo announced that its fully driverless cars would be restricted to Mountain View, Sunnyvale, Los Altos, and Palo Alto — all communities close to parent company Alphabet's headquarters.\n\nIn June 2015, Google confirmed that there had been 12 collisions as of that time, eight of which involved being rear-ended by another driver at a stop sign or traffic light, two in which the vehicle was side-swiped by another driver, one of which involved another driver rolling through a stop sign, and one where a Google employee was manually driving the car. , Google's 23 self-driving cars have been involved in 14 minor collisions on public roads, but Google maintains that, in all cases other than the February 2016 incident, the vehicle itself was not at fault because the cars were either being manually driven or the driver of another vehicle was at fault. On February 14, 2016 while creeping forward to a stoplight, a Google self-driving car attempted to avoid sandbags blocking its path. During the maneuver it struck the side of a bus. Google addressed the crash, saying \"In this case, we clearly bear some responsibility, because if our car hadn't moved there wouldn't have been a collision\". Some incomplete video footage of the crash is available. Google characterized the crash as a misunderstanding and a learning experience. The company also stated \"This type of misunderstanding happens between human drivers on the road every day\".\n\nGoogle initially maintained monthly reports that include any traffic incidents that their self-driving cars have been involved in. Waymo now publishes its own safety reports.\n\nWaymo and other companies are required by the California DMV to report the number of incidents during testing where the human driver took control for safety reasons. Some of these incidents were not reported by Google when simulations indicate the car would have coped on its own. There is some controversy concerning this distinction between driver-initiated disengagements that Google reports and those that it does not report.\n\nWaymo operates in some of its testing markets, such as Chandler, Arizona, at level 4 autonomy with no one sitting behind the steering wheel, sharing roadways with other drivers and pedestrians. However, more testing is needed. Waymo's earlier testing has focused on areas without harsh weather, extreme density or complicated road systems, but it has moved on to test under new conditions. As a result, Waymo has begun testing in areas with harsher conditions, such as its winter testing in Michigan.\n\nIn 2014, a critic wrote in the \"MIT Technology Review\" that unmapped stopped lights would cause problems with Waymo's technology and the self-driving technology could not detect potholes. Additionally, the lidar technology cannot spot some potholes or discern when humans, such as a police officer, are signaling the car to stop, the critic wrote. Waymo has worked to improve how its technology responds in construction zones.\n\nIn 2012, Brin stated that Google Self-Driving cars would be available for the general public in 2017, and in 2014 this schedule was updated by project director Chris Urmson to indicate a possible release from 2017 to 2020.\n\nIn August 2013, news reports surfaced about Robo-Taxi, a proposed driverless vehicle taxicab service from Google. These reports re-appeared again in early 2014, following the granting of a patent to Google for an advertising fee funded transportation service which included autonomous vehicles as a method of transport. Google consultant Larry Burns says self-driving, taxi-like vehicles \"should be viewed as a new form of public transportation\".\n\nIn a December 2016 blog post, Waymo CEO John Krafcik stated: \"We can see our technology being useful in personal vehicles, ridesharing, logistics, or solving last mile problems for public transport\" but also that \"Our next step as Waymo will be to let people use our vehicles to do everyday things like run errands, commute to work, or get safely home after a night on the town\". Temporary use of vehicles is known as Transportation as a Service (TaaS).\n\nIn April 2017, Waymo launched an early rider program in Phoenix, Arizona, which signed up 400 users to try out a test edition of Waymo's transportation service. Over the next year, 400 riders used the Waymo service, providing feedback. In May 2018, Waymo announced that it plans to allow everyone in Phoenix to request a driverless ride before the end of year. \n\nWaymo highlighted four specific business uses for its autonomous tech in 2017: Ridesharing, users can hail cars equipped with Waymo technology via transportation network company apps; trucking and logistics; urban last-mile solutions for public transportation; and passenger cars. Waymo is also considering licensing autonomous technology to vehicle manufacturers.\n\nIn 2018, Waymo launched a pilot program with Google to use autonomous trucks to move freight to its sister company's Atlanta-area data centers. Using the same sensors and software as Waymo's other autonomous fleet, Class 8 tractor trailers began testing Waymo's self-driving technology in California and Arizona in 2017.\n\nIn May 2018, Waymo established a subsidiary, Huimo Business Consulting, in Shanghai.\n\nIn February 2017, Waymo sued Uber and its subsidiary self-driving trucking company, Otto, for allegedly stealing Waymo's trade secrets and infringing upon its patents. The company claimed that three ex-Google employees including Anthony Levandowski stole trade secrets and joined Uber. The infringement is related to Waymo's proprietary lidar technology, which could measure the distances between objects using laser and create their three dimensional representations. Google accused Uber of colluding with Levandowski to obtain information about it and other technologies in its driverless car project. The former Google engineer downloaded 9 gigabytes of data that included over a hundred trade secrets; eight of those were at stake during the trial.\n\nThe trial began on February 5, 2018, and was dismissed on February 9, as a settlement was announced with Uber giving Waymo the equivalent of $244 million in Uber equity and agreeing to ensure Uber does not infringe Waymo's intellectual property. Part of the agreement included a guarantee that \"Waymo confidential information is not being incorporated in Uber Advanced Technologies Group hardware and software.\" Uber maintained that no trade secrets made their way to the ride-hailing company, in released statements after the settlement. \n\n"}
{"id": "56400875", "url": "https://en.wikipedia.org/wiki?curid=56400875", "title": "Wooden dolls, Natungram", "text": "Wooden dolls, Natungram\n\nWooden Dolls of Natungram is very popular in West Bengal and also in India. The art-making wooden dolls have been an age-old practice in India and Natungram is one of them. It situated in Burdwan district of West Bengal. There are many kinds of dolls like Gour-Nitai, Wooden owl, Krishna, Royal couple etc. The wooden owl is most popular among them.\n\nThe artist of Natungram used to work in stone carving first. with the king's patronage, they were working well. But after the fall of the king of Bardhaman, the craftsmen face difficulties. Then they left the stone carving industry and started doing fine arts and wooden works. One of the common family names here is Sutradhar because inspirations for most dolls are drawn from mythologies and folklore.\n\nDolls are mainly made out of Gamar wood, Mango wood, Shimul wood, Ata wood, Chatim wood. After making a shape of dolls, men carved the wood as per the requirement. Then women have painted the products with various colour. Then they used a glue, made from tamarind seed. Colouring has to be done by the female member of a family. They have used lots of colours in the dolls like white, green, yellow, black borders and red.\n"}
