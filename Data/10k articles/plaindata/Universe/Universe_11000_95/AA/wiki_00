{"id": "11516599", "url": "https://en.wikipedia.org/wiki?curid=11516599", "title": "Activation product", "text": "Activation product\n\nActivation products are materials made radioactive by neutron activation.\n\nFission products and actinides produced by neutron absorption of nuclear fuel itself are normally referred to by those specific names, and \"activation product\" reserved for products of neutron capture by other materials, such as structural components of the nuclear reactor or nuclear bomb, the reactor coolant, control rods or other neutron poisons, or materials in the environment. All of these, however, need to be handled as radioactive waste. Some nuclides originate in more than one way, as activation products or fission products.\n\nActivation products in a reactor's primary coolant loop are a main reason reactors use a chain of two or even three coolant loops linked by heat exchangers.\n\nFusion reactors will not produce radioactive waste from the fusion product nuclei themselves, which are normally just helium-4, but generate high neutron fluxes, so activation products are a particular concern.\n\nActivation product radionuclides include:\n\n[1] Branching fractions from LNHB database.\n\n[2] Branching fractions renormalised to sum to 1.0..\n\n"}
{"id": "3233599", "url": "https://en.wikipedia.org/wiki?curid=3233599", "title": "Atmospheric pressure discharge", "text": "Atmospheric pressure discharge\n\nAn atmospheric pressure discharge is an electrical discharge in air at atmospheric pressure.\n\nAn electrical discharge is a plasma, which is an ionized gas. Plasmas are sustained if there is a continuous source of energy to maintain the required degree of ionization and overcome the recombination events that lead to extinction of the discharge. Recombination events are proportional to collisions between molecules and thus to the pressure of the gas. Atmospheric discharges are thus difficult to maintain as they require a large amount of energy.\n\nTypical atmospheric discharges are:\n\n"}
{"id": "28959418", "url": "https://en.wikipedia.org/wiki?curid=28959418", "title": "Bashkirenergo", "text": "Bashkirenergo\n\nOJSC “Bashkirenergo” (Russian: ОАО «Башкирэнерго») is a power and heat company operating in Bashkortostan, Russia. The major shareholder of the company is a holding company Sistema. 21.27% of shares is owned by the Russian grid company FGC UES.\n\nThe company was dissolved in 2012, and the assets were divided between Bashenergoaktiv and Bashkir Grid Company; Bashenergoaktiv became part of Inter RAO.\n"}
{"id": "21625558", "url": "https://en.wikipedia.org/wiki?curid=21625558", "title": "Becklin–Neugebauer Object", "text": "Becklin–Neugebauer Object\n\nThe Becklin–Neugebauer Object (BN) is an object visible only in the infrared in the Orion Molecular Cloud. It was discovered in 1967 by Eric Becklin and Gerry Neugebauer during their near-infrared survey of the Orion Nebula.\n\nThe BN Object is thought to be an intermediate-mass protostar. It was the first star detected using infrared methods and is deeply embedded within the Orion star-forming nebula, where it is invisible at optical wavelengths because the light is completely scattered or absorbed due to the high density of dusty material.\n\n"}
{"id": "55918466", "url": "https://en.wikipedia.org/wiki?curid=55918466", "title": "Bismuth polycations", "text": "Bismuth polycations\n\nElectron-deficient bismuth polycations are classical examples of homopolyatomic ions (a polyatomic ion composed entirely of a single element) composed of electron-deficient bismuth atoms. They were originally observed in dilute solutions of bismuth metal in molten bismuth chloride. It has since been found that these clusters are present in the solid state, particularly in salts where germanium tetrachloride or tetrachloroaluminate serve as the counteranions, but also in amorphous phases such as glasses and gels.. Bismuth endows materials with a variety of interesting optical properties that can be tuned by changing the supporting material. Commonly-reported structures include the trigonal bipyramidal cluster, the octahedral cluster, the square antiprismatic cluster, and the tricapped trigonal prismatic cluster.\n\n\n\nBismuth polycations form despite the fact that they possess fewer total valence electrons than would seem necessary for the number of sigma bonds. The shapes of these clusters are generally dictated by Wade's rules, which are based on the treatment of the electronic structure as delocalized molecular orbitals. The bonding can also be described with three-center two-electron bonds in some cases, such as the cluster.\nBismuth clusters have been observed to act as ligands for copper and ruthenium ions. This behavior is possible due to the otherwise fairly inert lone pairs on each of the bismuth that arise primarily from the s-orbitals left out of Bi–Bi bonding.\nThe variety of electron-deficient sigma aromatic clusters formed by bismuth gives rise to a wide range of spectroscopic behaviors. Of particular interest are the systems capable of low-energy electronic transitions, as these have demonstrated potential as near-infrared light emitters. It is the tendency of electron-deficient bismuth to form sigma-delocalized clusters with small HOMO/LUMO gaps that gives rise to the near-infrared emissions. This property makes these species potentially valuable to the field of near-infrared optical tomography, which exploits the near-infrared window in biological tissue.\n"}
{"id": "57372087", "url": "https://en.wikipedia.org/wiki?curid=57372087", "title": "Bombali ebolavirus", "text": "Bombali ebolavirus\n\nBombali ebolavirus is a newly discovered strain of \"Ebolavirus,\" first reported on 27 July 2018. It was discovered by a research team from the U.S. in the western Africa country of Sierra Leone. The virus was found in the Angolan free-tailed bat and the Little free-tailed bat. \"Bombali ebolavirus\" has the capacity to infect human cells, although it had not yet been shown to be pathogenic.\n\n\"Ebolavirus\" and \"Marburgvirus\" are genera in the family Filoviridae.\n\nIn 1976 in Sudan, \"Sudan ebolavirus\" became the first species of \"Ebolavirus\" to be discovered. Later that year, the \"Zaire ebolavirus\" species was discovered.\n\nIn 1989, the \"Reston ebolavirus\" species was discovered in the Philippines (the U.S. would later be introduced to the strain in quarantine) however no fatalities occurred though 4 people developed antibodies to the virus.\n\nThe \"Taï Forest ebolavirus\" species was discovered in 1995 in Ivory Coast.\n\nThe \"Bundibugyo ebolavirus\" species was discovered during an outbreak in Uganda in 2007 that had a mortality rate of 40 percent.\n\n"}
{"id": "57940823", "url": "https://en.wikipedia.org/wiki?curid=57940823", "title": "Burevestnik (missile)", "text": "Burevestnik (missile)\n\nThe 9M730 Burevestnik (; \"Petrel\", NATO reporting name SSC-X-9 Skyfall) is a Russian nuclear-powered, nuclear-tipped cruise missile. \n\nThe missile was revealed by Russian President Vladimir Putin in his annual State of the Nation address on 1 March, 2018. The Russian defense industry began developing an intercontinental-range nuclear-powered cruise missile capable of penetrating any interceptor-based missile defense system. Its capabilites are said to be a supersonic missile, with unlimited range and ability to dodge missile defenses. The name of the weapon was chosen by the unusual route of a public vote.\n\nThe \"sister\" project of 'Burevestnik', the Poseidon / Kanjon nuclear torpedo / drone submarine, is built around a miniature nuclear propulsion (probably involving a steam turbine, if so can be charged to a turbogenerator to provide electricity, nozzle or pumpjet can also be added and collegated) system as well. A video shows a static test of the sub, which is basically a nuclear-capable torpedo.\n\n\n"}
{"id": "51847293", "url": "https://en.wikipedia.org/wiki?curid=51847293", "title": "CO2 fertilization effect", "text": "CO2 fertilization effect\n\nThe fertilization effect or carbon fertilization effect is the increased the rate of photosynthesis in plants that results from increased levels of carbon dioxide in the atmosphere. The effect varies depending on the plant species, the temperature, and the availability of water and nutrients.\n\nFrom a quarter to half of Earth's vegetated lands has shown significant greening over the last 35 years largely due to rising levels of atmospheric carbon dioxide.\n\nOne related trend may be what has been termed \"Arctic greening\". Scientists have been finding, that as northern portions of the planet warm up even as total atmospheric carbon dioxide increases, there’s been an increase in plant growth in these regions.\n\nStudies led by Trevor Keenan from the Department of Energy's Lawrence Berkeley National Laboratory (Berkeley Lab) show that, from 2002 to 2014, plants appear to have gone into overdrive, starting to pull more carbon dioxide out of the air than they have done before. The result was that the rate at which carbon dioxide accumulates in the atmosphere did not increase during this time period, although previously, it had grown considerably in concert with growing greenhouse gas emissions.\n\nResearchers report that the levels expected in the second half of this century will likely reduce the levels of zinc, iron, and protein in wheat, rice, peas, and soybeans. Some two billion people live in countries where citizens receive more than 60 percent of their zinc or iron from these types of crops. Deficiencies of these nutrients already cause an estimated loss of 63 million life-years annually.\n\n"}
{"id": "44323143", "url": "https://en.wikipedia.org/wiki?curid=44323143", "title": "Cameron Hepburn", "text": "Cameron Hepburn\n\nCameron Hepburn is an Australian Professor of Environmental Economics at the University of Oxford and the London School of Economics and Political Science, both in the United Kingdom. He is Director of the Economics of Sustainability Programme at the Institute for New Economic Thinking at the Oxford Martin School.\n\nHepburn attended Camberwell Grammar School and received his undergraduate education at the University of Melbourne in Australia and his master's degree and doctorate from the University of Oxford.\n\nHepburn is an advisor to the UK Secretary of State for Energy and Climate Change. He used to be part of the Academic Panel within the UK Department for Environment, Food and Rural Affairs and the UK Department of Energy and Climate Change. Hepburn advised the UN and the OECD on environmental policy, energy and resources. He has also worked at Shell, Mallesons, and McKinsey & Company.\n\nHepburn is a research fellow at the Grantham Research Institute on Climate Change and the Environment at the London School of Economics and Political Science and his research interests are \"Environmental economics; Climate change economics; Environmental policy; Carbon markets and emissions trading; Sustainability; Behavioural economics.\" Hepburn has \"over 30 peer-reviewed publications in a range of disciplines.\"\n\n\n"}
{"id": "8127717", "url": "https://en.wikipedia.org/wiki?curid=8127717", "title": "Central Electricity Authority (India)", "text": "Central Electricity Authority (India)\n\nThe Central Electricity Authority of India (CEA) is a statutory organisation constituted under section 3(1) of Electricity Supply Act 1948, which has been superseded by section 70(1) of the Electricity Act 2003. The CEA advises the government on policy matters and formulates plans for the development of electricity systems.\n\nUnder the Electricity Act 2003, CEA prescribes the standards on matters such as construction of electrical plants, electric lines and connectivity to the grid, installation and operation of meters and safety and grid standards. The CEA is also responsible for concurrence of hydro power development schemes of central, state and private sectors taking into consideration the factors which will result in efficient development of the river and its tributaries for power generation, consistent with the requirement of drinking water, irrigation, navigation and flood control.\n\nPreparation of technical standards for construction of electrical plants, electric lines and connectivity to the grid is the responsibility of CEA as per section 73 (b) of the Electricity Act, 2003. However as per section 7 of this Act, a generating company may establish, operate and maintain a generating station if it complies with the technical standards only relating to connectivity to the grid as given in clause (b) of section 73. This implies that generating stations need not follow compulsory the CEA technical standards specified for construction of electrical plants and electric lines. Similarly, transmission / distribution licensees need not implement compulsory the standards for construction of electric lines except the Grid Code/ Grid Standards for the operation and maintenance of transmission lines specified by CEA under clause 73 (d) of this Act. Many times, these CEA standards are conservative compromising optimum design features /cost/ utility and also do not give full clarity in selection of the system / sub system capabilities of electrical plants and electric line.\n\nThe CEA plays a lead role in promoting the integrated operations of the regional power grids and the evolution of a national grid. The eastern, western and north-eastern grids have been integrated and are operating in a synchronous mode. The eastern grid is connected to the northern as well as southern grid through HVDC back to back links. The western grid is also connected to the northern and southern grid through similar arrangements. The CEA facilitates exchange of power within the country from surplus to deficit regions and with neighbouring countries for mutual benefits.\nThe CEA advises central government, state governments and regulatory commissions on all technical matters relating to generation, transmission and distribution of electricity. It also advises state governments, licensees or generating companies on matters which enable them to operate and maintain the electricity system under their ownership or control in an improved manner.\n\nCEA responsibility also includes reliable data collection/ management/dissemination of the power sector. However, there is major mismatches (CEA data is nearly 5% more than NLDC data) in the basic electricity data given by CEA and NLDC as shown below.\n\nThere is also wide mismatch in sourcewise generation and total generation data presented in daily reports of CEA and NLDC. The definitions of terminology used is not available in these web sites.\n\n"}
{"id": "99803", "url": "https://en.wikipedia.org/wiki?curid=99803", "title": "Cinchona", "text": "Cinchona\n\nCinchona is a genus of flowering plants in the family Rubiaceae containing at least 23 species of trees and shrubs. They are native to the tropical Andean forests of western South America. A few species are reportedly naturalized in Central America, Jamaica, French Polynesia, Sulawesi, Saint Helena in the South Atlantic, and São Tomé and Príncipe off the coast of tropical Africa. Several species were sought after for their medicinal value and cultivated in India and Java where they also formed hybrids. The barks of several species yield quinine and other alkaloids that were the only effective treatments against malaria during the height of colonialism which made them of great economic and political importance. The synthesis of quinine in 1944, an increase in resistant forms of malaria, and alternate therapies ended the large-scale economic interest in their cultivation. Academic interest continues as cinchona alkaloids show promise in treating \"falciparum\" malaria which has evolved resistance to synthetic drugs.\n\nCarl Linnaeus named the genus in 1742 based on a claim, first recorded by the Italian physician Sebastiano Bado in 1663, that the plant had cured the wife of the Luis Jerónimo de Cabrera, 4th Count of Chinchón, Count of Chinchón, a viceroy in Lima. While the veracity of the claims and the details are highly debated leaving it best treated as a legend, the curative properties were known even earlier. The history of the plant, their extracts, and the cures are however highly confused and controversial. Suggestions that the plant went by the native name of Quina Quina which yielded Quina bark have been questioned. Other fever cures from South America were known as Jesuit's Bark and Jesuit's Powder in Europe earlier but although they have been traced to \"Cinchona\", there is evidence of materials being derived from other species such as \"Myroxylon\". The species that Linnaeus used to describe the genus was \"Cinchona officinalis\" which is found only in a small region in Ecuador and specimens of which were obtained by Charles Marie de La Condamine around 1735. This species is however of little medicinal significance. In the course of the quest for species yielding effective remedies, numerous species were described, some now considered invalid or synonyms of others. Linnaeus used the Italian spelling used by Bado but the name Chinchón (pronounced in Spanish) led to Clements Markham and others proposing a correction of the spelling to Chinchona and some prefer the pronunciation for the common name of the plant.\n\nThe national tree of Peru is in the genus \"Cinchona\".\n\n\"Cinchona\" plants belong to the family Rubiaceae and are large shrubs or small trees with evergreen foliage, growing in height. The leaves are opposite, rounded to lanceolate and 10–40 cm long. The flowers are white, pink or red, produced in terminal panicles. The fruit is a small capsule containing numerous seeds. A key character of the genus is that the flowers have marginally hairy corolla lobes. The tribe Cinchoneae includes other genera \"Cinchonopsis\", \"Jossia\", \"Ladenbergia\", \"Remijia\", \"Stilpnophyllum\", and \"Ciliosemina\". In South America, the species had geographically distinct distributions. The introduction of several species into cultivation in the same areas in India and Java, respectively, by the English and Dutch East India Companies led to the formation of hybrids.\n\nLinnaeus described the genus based on the species \"Cinchona officinalis.\" Nearly 300 species have been described and named in the genus but a revision of the genus in 1998 identified 23 distinct species.\n\nThe febrifugal properties of bark from trees now known to be in the genus \"Cinchona\" were used by many South American cultures but malaria was an Old World disease that was introduced into the Americas by Europeans only after 1492. The origins and claims to the use of febrifugal barks and powders in Europe, especially those used against malaria, were disputed even in the 17th century. Jesuits played a key role in the transfer of remedies from the New World. The traditional story, first recorded by Sebastiano Bado in 1663, is that the wife of the fourth Count of Chinchon fell ill in Lima with a tertian fever. A Spanish governor advised a traditional remedy which was tried, resulting in a miraculous and rapid cure. The Countess then ordered a large quantity of the bark and took it back to Europe. Bado claimed to have received this information from an Italian named Antonius Bollus who was a merchant in Peru. Clements Markham identified the Countess as Ana de Osorio but this was shown to be incorrect by Haggis. Ana de Osorio married the Count in August 1621 and died in 1625, even before the Count was appointed Viceroy of Peru in 1628. It was his second wife, Francisca Henriques de Ribera, who accompanied him to Peru. Haggis further examined the diaries of the Count of Chinchon and found no mention of the Countess suffering from fever although the Count himself had many malarial attacks. On account of numerous other discrepancies this is best treated as a legend. Quina bark was mentioned by Fray Antonio de La Calancha in 1638 as coming from a tree in Loja (Loxa). He noted that bark powder weighing about two coins was cast into water and drunk to cure fevers and \"tertians\". Jesuit Father Bernabé Cobo (1582–1657) also wrote on the \"fever tree\" in 1653. The legend was popularized in English literature by Markham in his writings and in 1874 he also published a \"plea for the correct spelling of the genus \"Chinchona\"\". A Spanish physician, Juan Fragoso wrote of bark powder from an unknown tree in 1600 that was used for treating various ills. Nicolas Monardes also wrote of a New World bark powder used in Spain in 1574. Both identify the sources as trees that do not bear fruit and having heart-shaped leaves and it has been suggested that these references could be to \"Cinchona\" species. The name Quina-Quina or Quinquina was suggested as an old name for Cinchona used in Europe and based on the native name used by the Quechua people. Italian sources spelt Quina as Cina which was a source of confusion with \"Smilax\" from China. Haggis argued that Qina and Jesuit's bark actually referred to \"Myroxylon peruiferum\" or Peruvian balsam and that this was an item of importance in Spanish trade in the 1500s. Over time, the bark of the \"Myroxylon\" was adulterated with the similar looking bark of what we now know as \"Cinchona\". Gradually the adulterant became the main product that was the key therapeutic ingredient used in malarial therapy. The bark was included as \"Cortex Peruanus\" in the London Pharmacopoeia in 1677. The \"fever tree\" was finally described carefully by the astronomer Charles Marie de la Condamine who visited Quito in 1735 on a quest to measure an arc of the meridian. The species he described, \"Cinchona officinalis,\" was however found to be of little therapeutic value. The first living plants seen in Europe were \"C. calisaya\" plants grown at the \"Jardin des Plantes\" from seeds collected by Hugh Algernon Weddell from Bolivia in 1846. José Celestino Mutis, physician to the Viceroy of Nueva Granada, Pedro Messia de la Cerda gathered information on cinchona in Colombia from 1760 and wrote a manuscript \"El Arcano de la Quina\" (1793) with illustrations. He proposed a Spanish expedition to search for plants of commercial value which was approved in 1783 and was continued after his death in 1808 by his nephew Sinforoso Mutis. As demand for the bark increased the trees in the forests began to be destroyed. To maintain their monopoly on cinchona bark, Peru and surrounding countries began outlawing the export of cinchona seeds and saplings beginning in the early 19th century.\n\nThe Colonial European powers considered growing the plant in other tropical parts. The French mission of 1743, of which de la Condamine was member, lost their plants when a wave took them off their ship. The Dutch sent Justus Hasskarl who brought plants that were then cultivated in Java from 1854. The English explorer Clements Markham went to collect plants that were introduced in Sri Lanka and the Nilgiris of southern India in 1860. The main species introduced were \"Cinchona succirubra\" or red bark, as its sap turned red on contact with air, and \"Cinchona calisaya\". The alkaloids quinine and cinchonine were extracted by Pelletier and Caventou in 1820. Later two more key alkaloids, quinidine and cinchonidine were identified and it became a routine in quinology to examine the contents of these components in assays. The yields of quinine in the cultivated trees were low and it took a while to develop sustainable methods to extract bark. In the meantime Charles Ledger and his native assistant Manuel collected another species from Bolivia. Manuel was caught and beaten by Bolivian officials leading to his death but Ledger obtained seeds of high quality which were offered to the British who were uninterested, leading to the rest being sold to the Dutch. The Dutch saw its value and multiplied the stock. The species later named as \"Cinchona ledgeriana\" had a yield of 8 to 13 percent quinine in bark grown in Dutch Indonesia which effectively out-competed the British Indian production. It was only later that the English saw the value and sought to obtain the seeds of \"C. ledgeriana\" from the Dutch.\n\nDuring World War II, the Japanese conquered Java and the United States lost access to the cinchona plantations that supplied war-critical quinine medication. Botanical expeditions – called Cinchona Missions – were launched in 1942-1944 to explore promising areas of South America in an effort to locate cinchona species that contained quinine and could be harvested for quinine production. While ultimately successful in their primary aim, these expeditions also identified new species of plants and created a new chapter in international relations between the United States and other nations in the Americas.\n\nFrancesco Torti used the response of fevers to treatment with cinchona as a system of classification of fevers or a means for diagnosis. The use of cinchona in the effective treatment of malaria brought an end to treatment by bloodletting and long-held ideas of humorism from Galen.\n\nFor his part in obtaining and helping the establishment of cinchona in British India Clements Markham was knighted. For the role in establishing cinchona in Indonesia, Hasskarl was knighted with the Dutch order of the Lion.\n\n\"Cinchona\" species are used as food plants by the larvae of some Lepidoptera species, including the engrailed, the commander, and members of the genus \"Endoclita\", including \"E. damor\", \"E. purpurescens\" and \"E. sericeus\".\n\n\"Cinchona pubescens\" has grown uncontrolled on some islands such as the Galapagos where it has posed the risk of outcompeting native plant species.\n\nIn herbalism, cinchona bark was used as an adulterant in Jesuit's bark or Peruvian bark which originally is thought to have referred to \"Myroxylon peruiferum\", another fever remedy. The bark of cinchona can be harvested in a number of ways. One approach was to cut the tree but this and girdling are equally destructive and unsustainable so small strips were cut and various techniques such as \"mossing\", the application of moss to the cut areas, were used to allow the tree to heal. Other approaches involved coppicing and chopping of side branches which were then stripped of bark. The bark was dried into what were called quills and then powdered for medicinal uses. The bark contains alkaloids, including quinine and quinidine. Cinchona is the only economically practical source of quinine, a drug that is still recommended for the treatment of \"falciparum\" malaria.\n\nThe Italian botanist Pietro Castelli wrote a pamphlet noteworthy as being the first Italian publication to mention the cinchona. By the 1630s (or 1640s, depending on the reference), the bark was being exported to Europe. In the late 1640s, the method of use of the bark was noted in the \"Schedula Romana.\"\n\nEnglish King Charles II called upon Robert Talbor, who had become famous for his miraculous malaria cure. Because at that time the bark was in religious controversy, Talbor gave the king the bitter bark decoction in great secrecy. The treatment gave the king complete relief from the malaria fever. In return, Talbor was offered membership of the prestigious Royal College of Physicians.\n\nIn 1679, Talbor was called by the King of France, Louis XIV, whose son was suffering from malaria fever. After a successful treatment, Talbor was rewarded by the king with 3,000 gold crowns and a lifetime pension for this prescription. Talbor was asked to keep the entire episode secret. After Talbor's death, the French king published this formula: seven grams of rose leaves, two ounces of lemon juice and a strong decoction of the cinchona bark served with wine. Wine was used because some alkaloids of the cinchona bark are not soluble in water, but are soluble in the ethanol in wine. In 1681 \"Água de Inglaterra\" was introduced into Portugal from England by Dr. Fernando Mendes who, similarly, “received a handsome gift from (King Pedro) on condition that he should reveal to him the secret of its composition and withhold it from the public”.\n\nIn 1738, \"Sur l'arbre du quinquina\", a paper written by Charles Marie de La Condamine, lead member of the expedition, along with Pierre Godin and Louis Bouger that was sent to Ecuador to determine the length of a degree of the 1/4 of meridian arc in the neighbourhood of the equator, was published by the French Academy of Sciences. In it he identified three separate species.\n\nThe birth of homeopathy was based on cinchona bark testing. The founder of homeopathy, Samuel Hahnemann, when translating William Cullen's \"Materia medica\", noticed Cullen had written that Peruvian bark was known to cure intermittent fevers. Hahnemann took daily a large, rather than homeopathic, dose of Peruvian bark. After two weeks, he said he felt malaria-like symptoms. This idea of \"like cures like\" was the starting point of his writings on homeopathy. Hahnemann's symptoms have been suggested by researchers, both homeopaths and skeptics, as being an indicator of his hypersensitivity to quinine.\n\nThe bark was very valuable to Europeans in expanding their access to and exploitation of resources in distant colonies and at home. Bark gathering was often environmentally destructive, destroying huge expanses of trees for their bark, with difficult conditions for low wages that did not allow the indigenous bark gatherers to settle debts even upon death.\n\nFurther exploration of the Amazon Basin and the economy of trade in various species of the bark in the 18th century is captured by Lardner Gibbon:\n\"...this bark was first gathered in quantities in 1849, though known for many years. The best quality is not quite equal to that of Yungas, but only second to it. There are four other classes of inferior bark, for some of which the bank pays fifteen dollars per quintal. The best, by law, is worth fifty-four dollars. The freight to Arica is seventeen dollars the mule load of three quintals. Six thousand quintals of bark have already been gathered from Yuracares. The bank was established in the year 1851. Mr. [Thaddäus] Haenke mentioned the existence of cinchona bark on his visit to Yuracares in 1796\". (Source: Exploration of the Valley of the Amazon, by Lieut. Lardner Gibbon, USN. Vol.II, Ch.6, pp. 146-47.)\n\nThe cultivation of cinchona led from the 1890s to a decline in the price of quinine but the quality and production of raw bark by the Dutch in Indonesia led them to dominate world markets. The producers of processed drugs in Europe (especially Germany) however bargained and caused fluctuations in prices which led to a Dutch-led Cinchona Agreement in 1913 that ensured a fixed price for producers. A \"Kina Bureau\" in Amsterdam regulated this trade.\n\nIt was estimated that the British Empire incurred direct losses of 52 to 62 million pounds a year due to malaria sickness each year. It was therefore of great importance to secure the supply of the cure. In 1860, a British expedition to South America led by Clements Markham smuggled back cinchona seeds and plants, which were introduced in several areas of British India and Sri Lanka. In India, it was planted in Ootacamund by William Graham McIvor. In Sri Lanka, it was planted in the Hakgala Botanical Garden in January 1861. James Taylor, the pioneer of tea planting in Sri Lanka, was one of the pioneers of cinchona cultivation. By 1883, about were in cultivation in Sri Lanka, with exports reaching a peak of 15 million pounds in 1886. The cultivation (initially of \"Cinchona succirubra\" and later of \"C. calisaya\") was extended through the work of George King and others into the hilly terrain of Darjeeling District of Bengal. Cinchona factories were established at Naduvattam in the Nilgiris and at Mungpoo, Darjeeling, West Bengal. Quinologists were appointed to oversee the extraction of alkaloids with John Broughton in the Nilgiris and C.H. Wood at Darjeeling. Others in the position included David Hooper and John Eliot Howard.\n\nIn 1865, \"New Virginia\" and \"Carlota Colony\" were established in Mexico by Matthew Fontaine Maury, a former confederate in the American Civil War. Postwar confederates were enticed there by Maury, now the \"Imperial Commissioner of Immigration\" for Emperor Maximillian of Mexico, and Archduke of Habsburg. All that survives of those two colonies are the flourishing groves of \"cinchonas\" established by Maury using seeds purchased from England. These seeds were the first to be introduced into Mexico.\n\nThe bark of trees in this genus is the source of a variety of alkaloids, the most familiar of which is quinine, an antipyretic (antifever) agent especially useful in treating malaria.\n\nCinchona alkaloids include:\nThey find use in organic chemistry as organocatalysts in asymmetric synthesis.\n\nAlongside the alkaloids, many cinchona barks contain cinchotannic acid, a particular tannin, which by oxidation rapidly yields a dark-coloured phlobaphene called red cinchonic, cinchono-fulvic acid or cinchona red.\n\nIn 1934, efforts to make malaria drugs cheap and effective for use across countries led to the development of a standard called \"totaquina\" proposed by the Malaria Commission of the League of Nations. Totaquina required a minimum of 70% crystallizable alkaloids of which at least 15% was to be quinine with not more than 20% amorphous alkaloids.\n\nThere are at least 24 species recognized by botanists. There are likely several unnamed species and many intermediate forms that have arisen due to the plants' tendency to hybridize.\n\n\n\n"}
{"id": "665333", "url": "https://en.wikipedia.org/wiki?curid=665333", "title": "Cymdeithas Edward Llwyd", "text": "Cymdeithas Edward Llwyd\n\nCymdeithas Edward Llwyd (English: Edward Llwyd Society) is a Welsh natural history organization whose name commemorates the great Welsh natural historian, geographer and linguist Edward Llwyd.\n\nThe Cymdeithas Edward Llwyd organizes regular country walks throughout Wales in sites of interest of the Welsh environment, including SSI's & post-industrial landscapes. These are Welsh-language walking groups, although learners are just as welcome.\n\nThey also organize a variety of Nature & Environmental activities, including lectures, publications on Welsh Nature & Environment & conservation work.\n\n"}
{"id": "900867", "url": "https://en.wikipedia.org/wiki?curid=900867", "title": "Drilling", "text": "Drilling\n\nDrilling is a cutting process that uses a drill bit to cut a hole of circular cross-section in solid materials. The drill bit is usually a rotary cutting tool, often multi-point. The bit is pressed against the work-piece and rotated at rates from hundreds to thousands of revolutions per minute. This forces the cutting edge against the work-piece, cutting off chips (swarf) from the hole as it is drilled.\n\nIn rock drilling, the hole is usually not made through a circular cutting motion, though the bit is usually rotated. Instead, the hole is usually made by hammering a drill bit into the hole with quickly repeated short movements. The hammering action can be performed from outside the hole (top-hammer drill) or within the hole (down-the-hole drill, DTH). Drills used for horizontal drilling are called drifter drills.\n\nIn rare cases, specially-shaped bits are used to cut holes of non-circular cross-section; a square cross-section is possible.\n\nDrilled holes are characterized by their sharp edge on the entrance side and the presence of burrs on the exit side (unless they have been removed). Also, the inside of the hole usually has helical feed marks.\n\nDrilling may affect the mechanical properties of the workpiece by creating low residual stresses around the hole opening and a very thin layer of highly stressed and disturbed material on the newly formed surface. This causes the workpiece to become more susceptible to corrosion and crack propagation at the stressed surface.\nA finish operation may be done to avoid these detrimental conditions.\n\nFor fluted drill bits, any chips are removed via the flutes. Chips may form long spirals or small flakes, depending on the material, and process parameters. The type of chips formed can be an indicator of the machinability of the material, with long chips suggesting good material machinability.\n\nWhen possible drilled holes should be located perpendicular to the workpiece surface. This minimizes the drill bit's tendency to \"walk\", that is, to be deflected from the intended center-line of the bore, causing the hole to be misplaced. The higher the length-to-diameter ratio of the drill bit, the greater the tendency to walk. The tendency to walk is also preempted in various other ways, which include:\n\nSurface finish produced by drilling may range from 32 to 500 microinches. Finish cuts will generate surfaces near 32 microinches, and roughing will be near 500 microinches.\n\nCutting fluid is commonly used to cool the drill bit, increase tool life, increase speeds and feeds, increase the surface finish, and aid in ejecting chips. Application of these fluids is usually done by flooding the workpiece with coolant and lubricant or by applying a spray mist.\n\nIn deciding which drill(s) to use it is important to consider the task at hand and evaluate which drill would best accomplish the task. There are a variety of drill styles that each serve a different purpose. The subland drill is capable of drilling more than one diameter. The spade drill is used to drill larger hole sizes. The indexable drill is useful in managing chips.\n\nThe purpose of spot drilling is to drill a hole that will act as a guide for drilling the final hole. The hole is only drilled part way into the workpiece because it is only used to guide the beginning of the next drilling process.\n\nCentre drill is A two-fluted tool consisting of a twist drill with a 60° countersink; used to drill countersink center holes in a work piece to be mounted between centers for turning or grinding.\n\nDeep hole drilling is defined as a hole depth greater than ten times the diameter of the hole. These types of holes require special equipment to maintain the straightness and tolerances. Other considerations are roundness and surface finish.\n\nDeep hole drilling is generally achievable with a few tooling methods, usually gun drilling or BTA drilling. These are differentiated due to the coolant entry method (internal or external) and chip removal method (internal or external). Using methods such as a rotating tool and counter-rotating workpiece are common techniques to achieve required straightness tolerances. Secondary tooling methods include trepanning, skiving and burnishing, pull boring, or bottle boring. Finally a new kind of drilling technology is available to face this issue: vibration drilling. This technology breaks up the chips by a small controlled axial vibration of the drill. The small chips are easily removed by the flutes of the drill.\n\nA high tech monitoring system is used to control force, torque, vibrations, and acoustic emission. Vibration is considered a major defect in deep hole drilling which can often cause the drill to break. A special coolant is usually used to aid in this type of drilling.\n\nGun drilling was originally developed to drill out gun barrels and is used commonly for drilling smaller diameter deep holes. The depth-to-diameter ratio can be even greater than 300:1. The key feature of gun drilling is that the bits are self-centering; this is what allows for such deep accurate holes. The bits use a rotary motion similar to a twist drill; however, the bits are designed with bearing pads that slide along the surface of the hole keeping the drill bit on center. Gun drilling is usually done at high speeds and low feed rates.\n\nTrepanning is commonly used for creating larger diameter holes (up to ) where a standard drill bit is not feasible or economical. Trepanning removes the desired diameter by cutting out a solid disk similar to the workings of a drafting compass. Trepanning is performed on flat products such as sheet metal, granite (curling stone), plates, or structural members like I-beams. Trepanning can also be useful to make grooves for inserting seals, such as O-rings.\n\nMicrodrilling refers to the drilling of holes less than . Drilling of holes at this small diameter presents greater problems since coolant fed drills cannot be used and high spindle speeds are required. High spindle speeds that exceed 10,000 RPM also require the use of balanced tool holders.\n\nThe first studies into vibration drilling began in the 1950s (Pr. V.N. Poduraev, Moscow Bauman University). The main principle consists in generating axial vibrations or oscillations in addition to the feed movement of the drill so that the chips break up and are then easily removed from the cutting zone.\n\nThere are two main technologies of vibration drilling: self-maintained vibration systems and forced vibration systems. Most vibration drilling technologies are still at a research stage. In the case of self-maintained vibration drilling, the eigenfrequency of the tool is used in order to make it naturally vibrate while cutting; vibrations are self-maintained by a mass-spring system included in the tool holder. Other works use a piezoelectric system to generate and control the vibrations. These systems allow high vibration frequencies (up to 2 kHz) for small magnitude (about a few micrometers); they are particularly suitable for drilling small holes. Finally, vibrations can be generated by mechanical systems: the frequency is given by the combination of the rotation speed and the number of oscillation per rotation (a few oscillations per rotation), with magnitude about 0.1 mm.\n\nThis last technology is a fully industrial one (example: SineHoling® technology of MITIS). Vibration drilling is a preferred solution in situations like deep hole drilling, multi-material stack drilling (aeronautics) and dry drilling (without lubrication). Generally it provides improved reliability and greater control of the drilling operation.\n\n\"Circle interpolating\", also known as \"orbital drilling\", is a process for creating holes using machine cutters.\n\nOrbital drilling is based on rotating a cutting tool around its own axis and simultaneously about a centre axis which is off-set from the axis of the cutting tool. The cutting tool can then be moved simultaneously in an axial direction to drill or machine a hole – and/or combined with an arbitrary sidewards motion to machine an opening or cavity.\n\nBy adjusting the offset, a cutting tool of a specific diameter can be used to drill holes of different diameters as illustrated. This implies that the cutting tool inventory can be substantially reduced.\n\nThe term orbital drilling comes from that the cutting tool “orbits” around the hole center. The mechanically forced, dynamic offset in orbital drilling has several advantages compared to conventional drilling that drastically increases the hole precision. The lower thrust force results in a burr-less hole when drilling in metals. When drilling in composite materials the problem with delamination is eliminated.\n\nUnder normal usage, swarf is carried up and away from the tip of the drill bit by the fluting of the drill bit. The cutting edges produce more chips which continue the movement of the chips outwards from the hole. This is successful until the chips pack too tightly, either because of deeper than normal holes or insufficient \"backing off\" (removing the drill slightly or totally from the hole while drilling). Cutting fluid is sometimes used to ease this problem and to prolong the tool's life by cooling and lubricating the tip and chip flow. Coolant may be introduced via holes through the drill shank, which is common when using a gun drill. When cutting aluminum in particular, cutting fluid helps ensure a smooth and accurate hole while preventing the metal from grabbing the drill bit in the process of drilling the hole. When cutting brass, and other soft metals that can grab the drill bit and causes \"chatter\", a face of approx. 1-2 millimeters can be ground on the cutting edge to create an obtuse angle of 91 to 93 degrees. This prevents \"chatter\" during which the drill tears rather than cuts the metal. However, with that shape of bit cutting edge, the drill is pushing the metal away, rather than grabbing the metal. This creates high friction and very hot swarf.\n\nFor heavy feeds and comparatively deep holes oil-hole drills are used in the drill bit, with a lubricant pumped to the drill head through a small hole in the bit and flowing out along the fluting. A conventional drill press arrangement can be used in oil-hole drilling, but it is more commonly seen in automatic drilling machinery in which it is the workpiece that rotates rather than the drill bit.\n\nIn computer numerical control (CNC) machine tools a process called \"\", or \"interrupted cut drilling\", is used to keep swarf from detrimentally building up when drilling deep holes (approximately when the depth of the hole is three times greater than the drill diameter). Peck drilling involves plunging the drill part way through the workpiece, no more than five times the diameter of the drill, and then retracting it to the surface. This is repeated until the hole is finished. A modified form of this process, called \"high speed peck drilling\" or \"chip breaking\", only retracts the drill slightly. This process is faster, but is only used in moderately long holes, otherwise it will overheat the drill bit. It is also used when drilling stringy material to break the chips.\n\nWhen it is not possible to bring material to the СNС machine, a Magnetic Base Drilling Machine may be used. The base allows drilling in a horizontal position and even on a ceiling. Usually for these machines it is better to use cutters because they can drill much faster with less speed. Cutter sizes vary from 12mm to 200mm DIA and from 30mm to 200mm DOC(depth of cut). These machines are widely used in construction, fabrication, marine, and oil & gas industries. In the oil and gas industry, pneumatic magnetic drilling machines are used to avoid sparks, as well as special tube magnetic drilling machines that can be fixed on pipes of different sizes, even inside.\n\nWood being softer than most metals, drilling in wood is considerably easier and faster than drilling in metal. Cutting fluids are not used or needed. The main issue in drilling wood is ensuring clean entry and exit holes and preventing burning. Avoiding burning is a question of using sharp bits and the appropriate cutting speed. Drill bits can tear out chips of wood around the top and bottom of the hole and this is undesirable in fine woodworking applications.\n\nThe ubiquitous twist drill bits used in metalworking also work well in wood, but they tend to chip wood out at the entry and exit of the hole. In some cases, as in rough holes for carpentry, the quality of the hole does not matter, and a number of bits for fast cutting in wood exist, including spade bits and self-feeding auger bits. Many types of specialised drill bits for boring clean holes in wood have been developed, including brad-point bits, Forstner bits and hole saws. Chipping on exit can be minimized by using a piece of wood as backing behind the work piece, and the same technique is sometimes used to keep the hole entry neat.\n\nHoles are easier to start in wood as the drill bit can be accurately positioned by pushing it into the wood and creating a dimple. The bit will thus have little tendency to wander.\n\nSome materials like plastics as well as other non-metals and some metals have a tendency to heat up enough to expand making the hole smaller than desired.\n\nThe following are some related processes that often accompany drilling:\n\n\n\n"}
{"id": "27799244", "url": "https://en.wikipedia.org/wiki?curid=27799244", "title": "E-Mentor Corps", "text": "E-Mentor Corps\n\nE-Mentor Corps is a project of the US State Department that will allow entrepreneurs seeking advice to access mentors on-line. The E-Mentor Corps will call on business leaders and proven entrepreneurs in the United States and overseas to serve as E-Mentors to aspiring and emerging entrepreneurs around the world.\n\nE-Mentor Corps is sponsored by leading organisations such as Intel and Ernst & Young.\n\nE-Mentors will commit to provide guidance, insights and support to mentees for at least several hours per month for at least 3 months - via e-mail, telephone, video conference and other means of communication. It is hoped that many of these connections will spawn longer-term collaborations.\nThe Department of State will encourage mentors and mentees to connect through any number of channels and mechanisms. To facilitate connection, the Department of State has partnered with three platforms which will serve as options for mentor and mentee connection: ImagineNations Group (imagine-network.org), Ning and Linkedin.\n\n\n"}
{"id": "57000496", "url": "https://en.wikipedia.org/wiki?curid=57000496", "title": "EIDD-1723", "text": "EIDD-1723\n\nEIDD-1723, also known as EPRX-01723 or as progesterone 20\"E\"-[\"O\"-[(phosphonooxy)methyl]oxime] sodium salt, is a synthetic, water-soluble analogue of progesterone and a neurosteroid which was developed for the potential treatment of traumatic brain injury. It is a rapidly converted prodrug of EIDD-036 (EPRX-036; progesterone 20-oxime), which is considered to be the active form of the agent. Previous C3 and C20 oxime derivatives of progesterone, such as P1-185 (progesterone 3-\"O\"-(-valine)-\"E\"-oxime), were also developed and studied prior to EIDD-1723.\n\n"}
{"id": "36825421", "url": "https://en.wikipedia.org/wiki?curid=36825421", "title": "Ekka (carriage)", "text": "Ekka (carriage)\n\nAn ekka (sometimes spelt hecca, ecka or ekkha) is a one-horse carriage used in northern India. \"Ekkas\" (the word is derived from Hindi \"ek\" for \"one\") were something like 'traps' (of 'a pony and trap'), and were commonly used as cabs, or private hire vehicles in 19th-century India. They find frequent mention in colonial literature of the period (for example, Kipling's \"The Three Musketeers\"). It is also said that some kind of ekkas were used by people of Indus Valley Civilisation (without the spoked wheel). \nEkkas were typically drawn by a single horse, pony, mule (or sometimes bullock) and had a pair of large wooden wheels (and traditionally, a wooden axle) and the carriage had a flat floor with a canopy providing shade to the passenger(s) and the driver. Traditionally, they lacked springs and seats, with the passengers having to sit on their haunches and withstand the jolts transmitted by the wheels. John Lockwood Kipling, artist and father of Rudyard Kipling, described the ekka as a \"tea-tray on wheels\" with the passengers sitting like \"compressed capital N's\". Bells were attached to the cart so as to warn people to stay out of the way of the cart. The space below the carriage and between the wheels was available for baggage. Wider versions with two bullocks have also been referred to as ekkas although larger two horse carriages with better seating are known as tongas.\n"}
{"id": "18462358", "url": "https://en.wikipedia.org/wiki?curid=18462358", "title": "Entlebuch Biosphere", "text": "Entlebuch Biosphere\n\nThe Entlebuch Biosphere is a natural reserve at the foot of the Alps, and includes the 395 km² valley of the Little Emme River between Bern and Lucerne in the Swiss Canton of Lucerne.\n\nIn September 2001, the region became the second UNESCO Biosphere Reserve in Switzerland, after the Swiss National Park.\n\n\n"}
{"id": "2132454", "url": "https://en.wikipedia.org/wiki?curid=2132454", "title": "Evolutionary graph theory", "text": "Evolutionary graph theory\n\nEvolutionary graph theory is an area of research lying at the intersection of graph theory, probability theory, and mathematical biology. Evolutionary graph theory is an approach to studying how topology affects evolution of a population. That the underlying topology can substantially affect the results of the evolutionary process is seen most clearly in a paper by Erez Lieberman, Christoph Hauert and Martin Nowak.\n\nIn evolutionary graph theory, individuals occupy vertices of a weighted directed graph and the weight w of an edge from vertex \"i\" to vertex \"j\" denotes the probability of \"i\" replacing \"j\". The weight corresponds to the biological notion of fitness where fitter types propagate more readily. \nOne property studied on graphs with two types of individuals is the \"fixation probability\", which is defined as the probability that a single, randomly placed mutant of type A will replace a population of type B. According to the \"isothermal theorem\", a graph has the same fixation probability as the corresponding Moran process if and only if it is isothermal, thus the sum of all weights that lead into a vertex is the same for all vertices. Thus, for example, a complete graph with equal weights describes a Moran process. The fixation probability is\nwhere \"r\" is the relative fitness of the invading type.\n\nGraphs can be classified into amplifiers of selection and suppressors of selection. If the fixation probability of a single advantageous mutation formula_2 is higher than the fixation probability of the corresponding Moran process formula_3 then the graph is an amplifier, otherwise a suppressor of selection. One example of the suppressor of selection is a linear process where only vertex \"i-1\" can replace vertex \"i\" (but not the other way around). In this case the fixation probability is formula_4 (where \"N\" is the number of vertices) since this is the probability that the mutation arises in the first vertex which will eventually replace all the other ones. Since formula_5 for all \"r\" greater than 1, this graph is by definition a suppressor of selection.\n\nEvolutionary graph theory may also be studied in a dual formulation, as a coalescing random walk, or as a stochastic process. We may consider the mutant population on a graph as a random walk between absorbing barriers representing mutant extinction and mutant fixation. For highly symmetric graphs, we can then use martingales to find the \"fixation probability\" as illustrated by Monk (2018).\n\nAlso evolutionary games can be studied on graphs where again an edge between \"i\" and \"j\" means that these two individuals will play a game against each other.\n\nClosely related stochastic processes include the voter model, which was introduced by Clifford and Sudbury (1973) and independently by Holley and Liggett (1975), and which has been studied extensively.\n\n\nA virtual laboratory for studying evolution on graphs:\n"}
{"id": "20625996", "url": "https://en.wikipedia.org/wiki?curid=20625996", "title": "External floating roof tank", "text": "External floating roof tank\n\nAn external floating roof tank is a storage tank commonly used to store large quantities of petroleum products such as crude oil or condensate. It consists of an open- topped cylindrical steel shell equipped with a roof that floats on the surface of the stored liquid. The roof rises and falls with the liquid level in the tank. As opposed to a fixed roof tank there is no vapor space (ullage) in the floating roof tank (except for very low liquid level situations). In principle, this eliminates breathing losses and greatly reduces the evaporative loss of the stored liquid. There is a rim seal system between the tank shell and roof to reduce rim evaporation.\n\nThe roof has support legs hanging down into the liquid. At low liquid levels the roof eventually lands and a vapor space forms between the liquid surface and the roof, similar to a fixed roof tank. The support legs are usually retractable to increase the working volume of the tank.\n\nExternal roof tanks are usually installed for environmental or economical reasons to limit product loss and reduce the emission of volatile organic compounds (VOC), an air pollutant.\n\nNormally (roof not landed), there is little vapor space, and consequently a much smaller risk of rim space fire.\n\nSnow can accumulate on the roof; the roofs are designed to hold up to 10\" / 255mm of water.\n\nWater on the roof is usually drained by a special flexible hose or other special drain line system that runs from drain-sumps on the roof, through the stored liquid to a drain valve on the shell at the base of the tank. A hose is the shortest quickest route, Other drain systems are available both rigid and semi-rigid. These are named 'Articulated' as they use straight lengths of steel pipes with mechanical swivel joints or consist of steel pipes with flexible sections. The main objective is to evacuate rainwater from the tank roof as it is heavier than the tank contents and will cause problems if above a given amount is accumulated (10\" / 255mm). \nAlso pumps are used to drain the roof water through outside the tank, not through the product. \n\nTanks and vessels come in many different shapes and sizes and many factors affect their design and manufacture. For example, pressure, temperature, and chemical properties are key factors that affect wall thickness, materials of construction, and shape. Tanks are container in which atmospheric pressure is maintained.\n\n"}
{"id": "35171726", "url": "https://en.wikipedia.org/wiki?curid=35171726", "title": "Field effect (semiconductor)", "text": "Field effect (semiconductor)\n\nIn physics, the field effect refers to the modulation of the electrical conductivity of a material by the application of an external electric field.\n\nIn a metal, the electron density that responds to applied fields is so large that an external electric field can penetrate only a very short distance into the material. However, in a semiconductor the lower density of electrons (and possibly holes) that can respond to an applied field is sufficiently small that the field can penetrate quite far into the material. This field penetration alters the conductivity of the semiconductor near its surface, and is called the \"field effect\". The field effect underlies the operation of the Schottky diode and of field-effect transistors, notably the MOSFET, the JFET and the MESFET. \n\nThe change in surface conductance occurs because the applied field alters the energy levels available to electrons to considerable depths from the surface, and that in turn changes the occupancy of the energy levels in the surface region. A typical treatment of such effects is based upon a \"band-bending diagram\" showing the positions in energy of the \"band edges\" as a function of depth into the material. \n\nAn example band-bending diagram is shown in the figure. For convenience, energy is expressed in eV and voltage is expressed in volts, avoiding the need for a factor \"q\" for the elementary charge. In the figure, a two-layer structure is shown, consisting of an insulator as left-hand layer and a semiconductor as right-hand layer. An example of such a structure is the \"MOS capacitor\", a two-terminal structure made up of a metal \"gate\" contact, a semiconductor \"body\" (such as silicon) with a body contact, and an intervening insulating layer (such as silicon dioxide, hence the designation \"O\"). The left panels show the lowest energy level of the conduction band and the highest energy level of the valence band. These levels are \"bent\" by the application of a positive voltage \"V\". By convention, the energy of electrons is shown, so a positive voltage penetrating the surface \"lowers\" the conduction edge. A dashed line depicts the occupancy situation: below this Fermi level the states are more likely to be occupied, the conduction band moves closer to the Fermi level, indicating more electrons are in the conducting band near the insulator.\n\nThe example in the figure shows the Fermi level in the bulk material beyond the range of the applied field as lying close to the valence band edge. This position for the occupancy level is arranged by introducing impurities into the semiconductor. In this case the impurities are so-called \"acceptors\" which soak up electrons from the valence band becoming negatively charged, immobile ions embedded in the semiconductor material. The removed electrons are drawn from the valence band levels, leaving vacancies or \"holes\" in the valence band. Charge neutrality prevails in the field-free region because a negative acceptor ion creates a positive deficiency in the host material: a hole is the absence of an electron, it behaves like a positive charge. Where no field is present, neutrality is achieved because the negative acceptor ions exactly balance the positive holes.\n\nNext the band bending is described. A positive charge is placed on the left face of the insulator (for example using a metal \"gate\" electrode). In the insulator there are no charges so the electric field is constant, leading to a linear change of voltage in this material. As a result, the insulator conduction and valence bands are therefore straight lines in the figure, separated by the large insulator energy gap. \n\nIn the semiconductor at the smaller voltage shown in the top panel, the positive charge placed on the left face of the insulator lowers the energy of the valence band edge. Consequently, these states are fully occupied out to a so-called \"depletion depth\" where the bulk occupancy reestablishes itself because the field cannot penetrate further. Because the valence band levels near the surface are fully occupied due to the lowering of these levels, only the immobile negative acceptor-ion charges are present near the surface, which becomes an electrically insulating region without holes (the \"depletion layer\"). Thus, field penetration is arrested when the exposed negative acceptor ion charge balances the positive charge placed on the insulator surface: the depletion layer adjusts its depth enough to make the net negative acceptor ion charge balance the positive charge on the gate. \n\nThe conduction band edge also is lowered, increasing electron occupancy of these states, but at low voltages this increase is not significant. At larger applied voltages, however, as in the bottom panel, the conduction band edge is lowered sufficiently to cause significant population of these levels in a narrow surface layer, called an \"inversion\" layer because the electrons are opposite in polarity to the holes originally populating the semiconductor. This onset of electron charge in the inversion layer becomes very significant at an applied \"threshold\" voltage, and once the applied voltage exceeds this value charge neutrality is achieved almost entirely by addition of electrons to the inversion layer rather than by an increase in acceptor ion charge by expansion of the depletion layer. Further field penetration into the semiconductor is arrested at this point, as the electron density increases exponentially with band-bending beyond the threshold voltage, effectively \"pinning\" the depletion layer depth at its value at threshold voltages.\n"}
{"id": "4781826", "url": "https://en.wikipedia.org/wiki?curid=4781826", "title": "Haynes–Shockley experiment", "text": "Haynes–Shockley experiment\n\nIn semiconductor physics, the Haynes–Shockley experiment was an experiment that demonstrated that diffusion of minority carriers in a semiconductor could result in a current. The experiment was reported in a short paper by Haynes and Shockley in 1948, with a more detailed version published by Shockley, Pearson, and Haynes in 1949. \nThe experiment can be used to measure carrier mobility, carrier lifetime, and diffusion coefficient.\n\nIn the experiment, a piece of semiconductor gets a pulse of holes, for example, as induced by voltage or a short laser pulse.\n\nTo see the effect, we consider a n-type semiconductor with the length \"d\". We are interested in determining the mobility of the carriers, diffusion constant and relaxation time. In the following, we reduce the problem to one dimension.\n\nThe equations for electron and hole currents are:\n\nwhere the \"j\"s are the current densities of electrons (\"e\") and holes (\"p\"), the \"μ\"s the charge carrier mobilities, \"E\" is the electric field, \"n\" and \"p\" the number densities of charge carriers, the \"D\"s are diffusion coefficients, and \"x\" is position. The first term of the equations is the drift current, and the second term is the diffusion current.\n\nWe consider the continuity equation:\n\nSubscript 0s indicate equilibrium concentrations. The electrons and the holes recombine with the carrier lifetime τ.\n\nWe define \n\nso the upper equations can be rewritten as:\n\nIn a simple approximation, we can consider the electric field to be constant between the left and right electrodes and neglect ∂\"E\"/∂\"x\". However, as electrons and holes diffuse at different speeds, the material has a local electric charge, inducing an inhomogeneous electric field which can be calculated with Gauss's law:\n\nwhere ε is permittivity, ε the permittivity of free space, ρ is charge density, and \"e\" elementary charge.\n\nNext, change variables by the substitutions: \nand suppose δ to be much smaller than formula_10. The two initial equations write:\n\nUsing the Einstein relation formula_13, where β is the inverse of the product of temperature and the Boltzmann constant, these two equations can be combined:\n\nwhere for \"D\"*, μ* and τ* holds:\n\nConsidering \"n\" » \"p\" or \"p\" → 0 (that is a fair approximation for a semiconductor with only few holes injected), we see that \"D\"* → \"D\", μ* → μ and 1/τ* → 1/τ. The semiconductor behaves as if there were only holes traveling in it.\n\nThe final equation for the carriers is:\n\nThis can be interpreted as a Dirac delta function that is created immediately after the pulse. Holes then start to travel towards the electrode where we detect them. The signal then is Gaussian curve shaped.\n\nParameters μ, \"D\" and τ can be obtained from the shape of the signal.\n\nwhere \"d\" is the distance drifted in time \"t\", and \"δt\" the pulse width.\n\n\n"}
{"id": "11688207", "url": "https://en.wikipedia.org/wiki?curid=11688207", "title": "Kia Mohave", "text": "Kia Mohave\n\nThe Kia Mohave, marketed in North America and China as the Kia Borrego, is a sport-utility vehicle (SUV) manufactured by the South Korean-based Kia Motors. The vehicle debuted in 2008 in the Korean and US markets. The Kia Borrego is named after the Anza-Borrego Desert State Park in California; Borrego means \"bighorned sheep\" which can be found in the state park.\n\nThe Kia Borrego was introduced as the largest SUV (Sport Utility Vehicle) in Kia's lineup of vehicles in the U.S. in 2009. The lineup of the Borrego in the U.S. was as follows:\n\nThe LX was the base Kia Borrego, though was very well-equipped for its $26,245.00 MSRP base price. It included such features as: cloth upholstery, keyless entry, AM/FM stereo with single-disc CD/MP3 player and USB/iPod and auxiliary audio input jacks and SIRIUS Satellite Radio, six speakers, air conditioning, alloy wheels, and a 3.8L V6 engine with automatic transmission. Features such as a 4.6L V8 engine were optional.\n\nThe EX was the uplevel version of the Kia Borrego, had a $27,995.00 MSRP base price, and added features such as: an AM/FM stereo with six-disc in-dash CD/MP3 changer and USB/iPod and auxiliary audio input jacks and SIRIUS Satellite Radio, an Infinity premium sound system with external amplifier and rear-mounted subwoofer, a power sunroof, and dual-zone climate controls. Features such as heated dual front bucket seats and a 4.6L V8 engine were optional. \n\nThe Limited was the top-of-the-line version of the Kia Borrego, had a $37,995.00 MSRP base price, and added features such as: a standard 4.6L V8 engine, leather seating surfaces, power dual front bucket seats, upgraded alloy wheels, heated dual front bucket seats, optional touch-screen GPS navigation with voice recognition, and a Homelink transmitter. \n\nBluetooth hands-free telephone and wireless stereo audio streaming were optional for all models. \n\nAfter unsuccessful sales in the U.S. for 2009, the Kia Borrego was discontinued, and its replacement, the all-new, second-generation 2011 Kia Sorento, began production in West Point, Georgia in 2010. Although it did not offer a V8 engine (instead offering all-new Inline Four-Cylinder (I4) and V6 engines), it offered all the features the Kia Borrego offered, including a new third-row seating option for all models except the Base model.\n\nThe production model, designed by automotive designer Peter Schreyer, former chief designer for Audi, was introduced at the 2008 North American International Auto Show. The vehicle was originally shown as a concept car under the Kia Mesa name at the 2005 North American International Auto Show and went on sale in Korea as the Mohave prior to its release in the United States. In the US, the Borrego went on hiatus for the 2010 model year, with no word on its return or cancellation, after lower than expected sales in 2009.\n\nAs of December 2, 2010, the Kia website no longer listed the Borrego as a production model, and the previous address for it advertised the new Kia Sorento as a replacement vehicle. Kia, however, still continued selling the Borrego in Canada, meaning the Borrego was a Canada-only nameplate from 2010-11.\n\nAs of October 28, 2011, the car was discontinued with the Sorento as a successor, except the Middle East, China, Central Asia, Brazil, Chile and Russia.\n\nIn 2016 Facelifted Mohave launched in South Korean Market.\n\nThe Borrego utilized body-on-frame construction, with available adjustable air-suspension, hill-descent control and a high- and low-range automatic transmission. The Borrego has three standard rows of seats in the US. The Borrego is fitted with either the 3.0 L VGT diesel V6 (in 2010), second-generation Lambda II 3.8 L V6 producing or the 4.6 L V8 Hyundai Tau engine. The Tau V8 is tuned to give less power but more torque than in the Hyundai Genesis sedan, and creates . The V8 has a towing capacity of , and the V6 is able to tow . A navigation system was available as an option.\n\nThe Kia Borrego FCEV is a concept car produced by Hyundai-Kia and first shown at the 2008 Los Angeles Auto Show. The green SUV concept is based on a model Borrego and has a fuel cell developed by the 4th generation Hyundai-Kia (154 hp, 115 kW) and a new battery Lithium-Ion Polymer. The driver then enjoys an autonomy of and can start in temperatures of . As part of its testing program, Kia will put on the roads in 2010 a fleet of Kia Borrego FCEVs.\n\n"}
{"id": "29055314", "url": "https://en.wikipedia.org/wiki?curid=29055314", "title": "Light Steam Power", "text": "Light Steam Power\n\nLight Steam Power was a magazine dedicated to amateur and small-scale interest in steam power. Its masthead for some years described itself as, \"Authentic World News on Steam Power for Cars, Launches and Small Stationary Units\".\n\nThe magazine was self-published by its editor John Ness Walton of Kirk Michael, Isle of Man. It began when he took over an earlier magazine, Steam Car Developments and Steam Marine Motors in 1945, renaming this in 1949. For most of its existence it was published bi-monthly, although the first and last volumes were published at three- and four-month intervals.\nIn its last years, from 1977 it changed its name to Steam Power and re-located to Loughborough. The magazine continued under this name until 1981.\n\nIn more recent years, there was a website maintaining the magazine's archives, but this has now gone. In 2009 an agreement was made with the National Steam Car Association for them to hold the rights to \"Light Steam Power\" and other Walton publications. Mr. Walton died March 19, 2013, aged 91.\n\nThe two main applications of steam power covered were steam cars and small steam launches. Much of the magazine's content was influenced, if not indeed written, by skilled and enthusiastic amateurs describing their own projects and so their choice of these projects influenced the magazine's own direction. Steam turbines and large steamships were less frequent, being beyond the constructional capacity of most readers, but they were described as news items when particularly interesting events took place. One aspect that oddly was only lightly covered was the use of steam on railways, unusually so as the magazine's heyday coincided with the demise of mainline steam but with increasing interest in the railway and steam preservation movements.\n\nThe magazine covered a broad range of topics within 'steam power', often at a far more advanced level than contemporary railway and steam locomotive practice. Low-water-content water-tube boilers were commonplace and even monotube steam generators and uniflow engines were regular features. Although the terms were not yet in use, these technical features coincided with increased recognition of the Advanced Steam Technology and Modern Steam movement.\n\nCoverage was always worldwide and international, even though this was unusual for any magazine at this time, let alone a small independent. Many articles were provided by Australian and Canadian builders.\n\nAs well as the magazine content, it also included advertisements for other services from J.N. Walton such as engineering drawings for a range of related machinery, also castings for its construction. Walton also supplied the 'KleenHeet' waste oil burner.\n\nFrom 1965 a US publication, \"Steam Calliope: A Voice for the Steam Automobile\" was published in Panorama City, California by mechanical engineer Thomas P. Hall. It originated news and published clippings for western members of the Steam Automobile Club of America.\n\n\"The Steam Automobile\" was the quarterly publication of the Steam Automobile Club of America and was published from 1959 to 1986.\nSteam Power was the quarterly of the Steam Power Club, a spinoff of the SACA in California. Its last issues were named \"Ssssteam\". \n\nThe \"Steam Automobile Bulletin\" began in 1986 following a shift of editorial material to a commercial magazine. It continues as a bimonthly with anything technical and historic on steam between toys and locomotives, including steam cars.\n\n"}
{"id": "176610", "url": "https://en.wikipedia.org/wiki?curid=176610", "title": "Log flume", "text": "Log flume\n\nA log flume is a flume specifically constructed to transport lumber and logs down mountainous terrain to a sawmill by using flowing water. These watertight trough-like channels could be built to span a long distance across chasms and down steep mountain slopes. The use of log flumes facilitated the quick and cheap transportation of logs and thereby eliminated the need for horse- or oxen-drawn carriages on dangerous mountain trails.\n\nEarly flumes were square chutes that were prone to jams that could cause damage and required constant maintenance. In 1868, James W. Haines first built the \"V\" shaped log flumes that allowed a jammed log to free itself as the rising water level in the flume pushed it up. These efficient flumes consisted of 2 boards, wide, joined perpendicularly, and came in common use in the western United States during the late 19th century.\n\nThe longest log flume was reputedly the Kings River Flume in Sanger, California. Built in 1890 by the Kings River Lumber Company, it spanned over from the Sierra Nevada Mountains to the lumber yard and railroad depot in Sanger. Together with a constant water supply from a nearby reservoir, the flume enabled the efficient transportation of boards of lumber over deep gorges and cliffs and thereby opened up the area now known as Sequoia National Forest for clearcutting of the giant Redwood forests. Proper operation was ensured by \"flume herders\" who at various locations along the flume checked the flow of lumber and water.\n\nOn occasion, despite being exceedingly dangerous, flume herders and others would ride down the flume in small crafts or boats, either for inspection or for thrills. Such rides were the precursor of the modern log-ride amusement park attractions.\n\n"}
{"id": "42822062", "url": "https://en.wikipedia.org/wiki?curid=42822062", "title": "MPNR", "text": "MPNR\n\nMoPNR is a Pakistan Government's level ministry responsible to ensure availability and security of sustainable supply of oil and gas for economic development and strategic requirements of Pakistan and to coordinate development of natural resources of energy and minerals. \n\nThe Ministry was converted into Petroleum Division in August 2017 and the division was merged into Ministry of Energy.\n\nPetroleum & Natural Resources Division was created in April 1977. Prior to that Petroleum and Natural Resources was part of the Ministry of Fuel, Power and Natural Resources.\n\n\nThe Geological Survey of Pakistan is an autonomous and independent institution under Ministry of Petroleum and Natural Resources which is tasked and mandate with advancing the geoscience knowledge and carrying out systematic studies on official mapping and area surveying.\n\nGovernment Holdings Private Limited (GHPL) was registered as private limited company in 2000 under the Companies Ordinance 1984. It is 100% owned by the Government of Pakistan and operates under the Ministry of Petroleum and Natural Resources.\nThe purpose of creating this company was to separate the regulatory and commercial functions to efficiently manage the Government's interest in petroleum exploration and production joint ventures which was previously managed by Directorate General of Petroleum Concession.\nGHPL had a mandatory 5% interest in all Exploration Licenses granted by the Government during the exploration phase which was to be carried by other joint venture partners. This working interest was to increase in case of commercial discovery from 15% to 25% depending on the discovery zone.\n\nOGDCL is the national oil & gas company of Pakistan and the flagship of the country's exploration and production sector.\n\nPMDC is an autonomous corporation under the administrative control of Ministry of Petroleum and Natural Resources, Government of Pakistan. It was established in 1974 to expand and help mineral development activities in the country.\nIt is involved in exploration and evaluation of economic mineral deposits, preparation of techno-economic feasibility reports, mining and marketing.\n\nPakistan Petroleum Limited (Reporting name: PPL or PP) was incorporated on June 5, 1950, when it inherited the assets and liabilities of the Burmah Oil Company Ltd.\n\nThe company is headquartered in Karachi. It operates major oil and gas fields, including the Sui gas field, has non-operating interests in other fields, and has an interest in an exploration portfolio onshore and offshore.\n\nInter State Gas Systems (Private) Limited (ISGS) was established in 1996 as a private limited company.\n\nIn order to meet the growing energy deficit in the country, the Government of Pakistan (GOP), besides encouraging local exploration and production, plans to import natural gas from across its borders from Iran and Turkmenistan. \n\nISGS has been mandated by the Government to develop natural gas import projects, and to serve as an interface between the GOP and other national and international agencies for the import and storage of natural gas in Pakistan.\n\nSaindak Copper-Gold Project of Saindak Metals Limited (SML) is an organization of Ministry of Petroleum and Natural Resources, Government of Pakistan engaged in exploration processing of copper-gold, silver and allied minerals.\n\nLakhra Coal Development Company Limited (LCDC), was incorporated as a Public Limited Company in 1990 under Companies Ordinance 1984. The company is a joint venture of Pakistan Mineral Development Corporation (PMDC), Government of Sindh (GOS) and Water & Power Development Authority (WAPDA).\n\nPrime objective of the company is to develop Lakhra Coal Mines to supply coal to 150 MW Thermal Plant of LPGCL (WAPDA) at Khanote which was originally planned for 6x50 MW with annual coal consumption of 15,00,000 M.Tonnes.\n\n"}
{"id": "45655292", "url": "https://en.wikipedia.org/wiki?curid=45655292", "title": "MRC Global", "text": "MRC Global\n\nMRC Global is a global distributor of pipes, valves, fittings, automation and measurement products headquartered in Houston, Texas.\n"}
{"id": "43213872", "url": "https://en.wikipedia.org/wiki?curid=43213872", "title": "Magnetic resonance velocimetry", "text": "Magnetic resonance velocimetry\n\nMagnetic resonance velocimetry (MRV) is an experimental method to obtain velocity fields in fluid mechanics. MRV is based on the phenomenon of nuclear magnetic resonance and adapts a medical magnetic resonance imaging system for the analysis of technical flows. The velocities are usually obtained by phase contrast magnetic resonance imaging techniques. This means velocities are calculated from phase differences in the image data that has been produced using special gradient techniques. MRV can be applied using common medical MRI scanners. The term \"magnetic resonance velocimetry\" became current due to the increasing use of MR technology for the measurement of technical flows in engineering.\n\nIn engineering MRV can be applied to the following areas:\n\nIn contrast to other non-invasive velocimetry methods such as PIV or LDA, no optical access is required. Besides, no particles have to be added to the fluid. Thus, MRV enables to analyze the complete flow field in complex geometries and components.\nBased on the fact that common MR scanners are designed to detect the nuclear magnetic resonance of hydrogen protons, the tested applications are limited to water flows. Common fluid mechanical scaling concepts compensate this limitation. To achieve the spatial resolution, single data acquisition steps have to be repeated a great number of times with slight variations. Thus, MRV technology is limited to steady or periodical flows.\n\n\n\n"}
{"id": "7872461", "url": "https://en.wikipedia.org/wiki?curid=7872461", "title": "Mechanoluminescence", "text": "Mechanoluminescence\n\nMechanoluminescence is light emission resulting from any mechanical action on a solid. It can be produced through ultrasound, or through other means.\n\n\n\n"}
{"id": "18446530", "url": "https://en.wikipedia.org/wiki?curid=18446530", "title": "Microplasma", "text": "Microplasma\n\nA microplasma is a plasma of small dimensions, ranging from tens to thousands of micrometers. Microplasmas can be generated at a variety of temperatures and pressures, existing as either thermal or non-thermal plasmas. Non-thermal microplasmas that can maintain their state at standard temperatures and pressures are readily available and accessible to scientists as they can be easily sustained and manipulated under standard conditions. Therefore, they can be employed for commercial, industrial, and medical applications, giving rise to the evolving field of microplasmas.\n\nThere are 4 states of matter: solid, liquid, gas, and plasma. Plasmas make up more than 99% of the visible universe. In general, when energy is applied to a gas, internal electrons of gas molecules (atoms) are excited and move up to higher energy levels. If the energy applied is high enough, outermost electron(s) can even be stripped off the molecules (atoms), forming ions. Electrons, molecules (atoms), excited species and ions form a soup of species which involves many interactions between species and demonstrate collective behavior under the influence of external electric and magnetic fields. Light always accompanies plasmas: as the excited species relax and move to lower energy levels, energy is released in the form of light. Microplasma is a subdivision of plasma in which the dimensions of the plasma can range between tens, hundreds, or even thousands of micrometers in size. The majority of microplasmas that are employed in commercial applications are cold plasmas. In a cold plasma, electrons have much higher energy than the accompanying ions and neutrals. Microplasmas are typically generated at elevated pressure to atmospheric pressure or higher.\n\nSuccessful ignition of microplasmas is governed by Paschen's Law, which describes the breakdown voltage (the voltage at which the plasma begins to arc) as a function of the product of electrode distance\nand pressure,\nwhere pd is the product of pressure and distance, and formula_2 and formula_3 are the gas constants for calculating Townsend's first ionization coefficient and formula_4 is the secondary emission coefficient of the material.\nAs the pressure increases, the distance between the electrodes must decrease to achieve the same breakdown voltage. This law is proven to be valid at inter-electrode distances as small as tens of micrometers and pressures higher than atmospheric. However, its validity at even smaller scales (approaching debye length) is still currently under investigation.\n\nWhile microplasma devices have been studied experimentally for more than a decade, understanding has been spurred in the past few years as the result of modelling and computational investigations of microplasmas.\n\nWhen the pressure of the gas medium in which the microplasma is generated increases, the distance between the electrodes must decrease to maintain the same breakdown voltage. In such microhollow cathode discharges, the product of pressure and distance ranges from fractions of Torr cm to about 10 Torr cm. At values below 5 Torr cm, the discharges are called \"pre-discharges\" and are low intensity glow discharges. Above 10 Torr cm the discharge can become uncontrollable and extend from the anode to random locations within the cavity. Further research by David Staack provided a graph of ideal electrode distances, voltages, and carrier gases tested for microplasma generation.\n\nDielectrics are poor electrical conductors, but support electrostatic fields and electric\npolarization. Dielectric barrier discharge microplasmas are typically created between metal plates, which are covered by a thin layer of dielectric or highly resistive material. The dielectric layer plays an important role in suppressing the current: the cathode/anode layer is charged by incoming positive ions/electrons during a positive cycle of AC is applied which reduces the electric field and hinders charge transport towards the electrode. DBD also has a large surface-to-volume ratio, which promotes diffusion losses and maintains a low gas temperature. When a negative cycle of AC is applied, the electrons are repelled off of the anode, and are ready to collide with other particles. Frequencies of 1000 Hz or more are required to move the electrons fast enough to create a microplasma, but excessive frequencies can damage the electrode (~50 kHz). Although dielectric barrier discharge comes in various shapes and dimensions, each individual discharge is in micrometer scale.\n\nAC and high frequency power are often used to excite dielectrics, in place of DC. Take AC as an example, there are positive and negative cycles in each period. When the positive cycle occurs, electrons accumulate on the dielectric surface. On the other hand, the negative cycle would repel the accumulated electrons, causing collisions in the gas and creating plasma. During the switch from the negative to positive cycles, the above-mentioned frequency range of 1000 Hz-50,000 Hz is needed in order for a microplasma to be generated. Because of the small mass of the electrons, they are able to absorb the sudden switch in energy and become excited; the larger particles (atoms, molecules, and ions), however aren't able to follow the fast switching, therefore keeping the gas temperature low.\n\nBased on transistor amplifiers low power RF (radio frequency) and microwave sources are used to generate a microplasma. Most of the solutions work at 2.45 GHz. Meanwhile, is a technology developed which provide the ignition on the one hand and the high efficient operation on the other hand with the same electronic and couple network.\n\nWith the use of lasers, solid substrates can be converted directly into microplasmas. Solid targets are struck by high energy lasers, usually gas lasers, which are pulsed at time periods from picoseconds to femtoseconds (mode-locking). Successful experiments have used Ti:Sm, KrF, and YAG lasers, which can be applied to a variety of substrates such as lithium, germanium, plastics, and glass.\n\nIn 1857, Werner von Siemens, a German scientist, originated\nozone generation using a dielectric barrier discharge apparatus for biological decontamination. His observations were explained without the knowledge of “microplasmas”, but were later recognized as the first use of microplasmas to date. The early electrical engineers, such as Edison and Tesla, were actually trying to prevent the generation of such \"micro-discharges\", and used dielectrics to insulate the first electrical infrastructures. Subsequent studies have observed the Paschen breakdown curve as being the prime cause of microplasma generation in an article published in 1916.\n\nSubsequent articles during the course of the 20th century have described the various conditions and specifications that lead to the generation of microplasmas. After Siemens' interactions with microplasma, Ulrich Kogelschatz was the first to identify these \"micro-discharges\" and define their fundamental properties. Kogelschatz also realized that microplasmas could be used for excimer formation. His experiments spurred the rapid development of the microplasma field.\n\nIn February 2003, Kunihide Tachibana, a professor of Kyoto University held the first international workshop on microplasmas (IWM) in Hyogo, Japan. The workshop, titled “The New World of Microplasmas”, opened a new era of microplasma research. Tachibana is recognized as one of the founding fathers as he coined the term “microplasma”.\nThe Second IWM was organized in October 2004 by Professors K.H. Becker, J.G. Eden, and K.H. Schoenbach at Stevens Institute of Technology in Hoboken, New Jersey.\nThe third international workshop was coordinated by the Institute of Low Temperature Plasma Physics alongside the Institute of Physics of Ernst-Moitz-Arndt-University in Greifswald, Germany, May 2006. Topics discussed were inspiring scientific and arising technological opportunities of microplasmas. The fourth IWM was held in Taiwan in October 2007, the fifth in San Diego, California in March 2009, and the sixth in Paris, France in April 2011. The next (seventh) workshop was held in China in approximately May 2013.\n\nThe rapid growth of applications of microplasmas renders it impossible to name all of them within a short space, but some selected applications are listed here.\n\nArtificially generated microplasmas are found on the flat panel screen of a plasma display. The technology utilizes small cells and contains electrically charged ionized gases. Across this plasma display panel, there are a millions of tiny cells called pixels that are confined to form a visual image. In the plasma display panels, X and Y grid of electrodes, separated by a MgO dielectric layer and surrounded by a mixture of inert gases - such as argon, neon or xenon, the individual picture elements are addressed. They work on the principle that passing a high voltage through a low-pressure gas generates light. Essentially, a PDP can be viewed as a matrix of tiny fluorescent tubes which are controlled in a sophisticated fashion. Each pixel comprises a small capacitor with three electrodes, one for each primary color (some newer displays include an electrode for yellow). An electrical discharge across the electrodes causes the rare gases sealed in the cell to be converted to plasma form as it ionizes. Being electrically neutral, it contains equal quantities of electrons and ions and is, by definition, a good conductor. Once energized, the plasma cells release ultraviolet (UV) light which then strikes and excites red, green and blue phosphors along the face of each pixel, causing them to glow.\n\nThe team of Gary Eden and Sung-Jin Park are pioneering the use of microplasmas for general illumination. Their apparatus uses many microplasma generators in a large array, which emit light through a clear, transparent window. Unlike fluorescent lamps, which require the electrodes to be far apart in a cylindrical cavity and vacuum conditions, microplasma light sources can be put into many different shapes and configurations, and generate heat. This is opposed to the more commonly used fluorescent lamps which require a noble gas atmosphere (usually argon), where eximer formation and resulting radiative decomposition strikes a phosphor coating to create light.\nExcimer light sources are also being produced and researched. The stable, non-equilibrium condition of microplasmas favors three-body collisions which can lead to excimer formation. The excimer, an unstable molecule produced by collisions of excited atoms, is very short lived due to its rapid dissociation. Upon their decomposition, excimers release different kinds of radiation when electrons fall to lower energy levels. One application, which has been pursued by the Hyundai Display Advanced Technology R&D Research Center and the University of Illinois, is to use excimer light sources in flat panel displays.\n\nMicroplasma are used to destroy volatile organic compounds. For example, capillary plasma electrode (CPE) discharge was used to effectively destroy volatile organic compounds such as benzene, toluene, ethylbenzene, xylene, ethylene, heptane, octane, and ammonia in the surrounding air for use in advanced life support systems designed for enclosed environments. Destruction efficiencies were determined as a function of plasma energy density, initial contaminant concentration, residence time in plasma volume, reactor volume, and the number of contaminants in the gas flow stream. Complete destruction of VOC’s can be achieved in the annular reactor for specific energies of 3 J cm−3 and above. Furthermore, specific energies approaching 10 J cm−3 are required to achieve a comparable destruction efficiency in the cross-flow reactor. This indicates that optimization of the reactor geometry is a critical aspect of achieving maximum destruction efficiencies. Koutsospyros \"et al.\" (2004, 2005) and Yin \"et al.\" (2003) reported results regarding studies of VOC destruction using CPE plasma reactors. All compounds studied reached maximum VOC destruction efficiencies between 95% and 100%. The VOC destruction efficiency increased initially with the specific energy, but remained at values of the specific energy that are compound-dependent. A similar observation was made for the dependence of the VOC destruction efficiency on the residence time. The destruction efficiency increased with rising initial contaminant concentration. For chemically similar compounds, the maximum destruction efficiency was found to be inversely related to the ionization energy of the compound and directly related to the degree of chemical substitution. This may suggest that chemical substitution sites offer the highest plasma-induced chemical activity.\n\nThe small size and modest power required for microplasma devices employ a variety of environmental sensing applications and detect trace concentrations of hazardous species. Microplasmas are sensitive enough to act as detectors, which can distinguish between excessive quantities of complex molecules. C.M. Herring and his colleagues at Caviton Inc. have simulated this system by coupling a microplasma device with a commercial gas chromatography column (GC). The microplasma device is situated at the exit of the GC column, which records the relative fluorescence intensity of specific atomic and molecular dissociation fragments. This apparatus possesses the ability to detect minute concentrations of toxic and environmentally hazardous molecules. It can also detect a wide range of wavelengths and the temporal signature of chromatograms, which identifies the species of interest. For the detection of less complex species, the temporal sorting done by the GC column is not necessary since the direct observation of fluorescence produced in the microplasma is sufficient.\n\nMicroplasmas are being used for the formation of ozone from atmospheric oxygen. Ozone (O) has been shown to be a good disinfectant and water treatment that can cause breakdown of organic and inorganic materials. Ozone is not potable\nand reverts to diatomic oxygen, with a half-life of about 3 days in air room temperature (about 20 C). In water, however, ozone has a half-life of only 20 minutes at the same temperature of 20 (C) . Degremont Technologies (Switzerland) produces microplasma arrays for commercial and industrial production of ozone for water treatment. By passing molecular oxygen through a series of dielectric barriers, using what Degremont calls the Intelligent Gap System (IGS), an increasing concentration of ozone is produced by altering the gap size and coatings used on the electrodes farther down the\nsystem. The ozone is then directly bubbled into the water to be made potable (suitable for\ndrinking). Unlike chlorine, which is still used in many water purification systems to treat\nwater, ozone does not remain in the water for extended periods. Because ozone decomposes with a half-life of 20 minutes in water at room temperature, there are no lasting effects that may cause harm.\n\nMicroplasmas serve as energetic sources of ions and radicals, which are desirable for activating chemical reactions. Microplasmas are used as flow reactors that allow molecular gases to flow through the microplasma inducing chemical modifications by molecular decomposition. The high energy electrons of microplasmas accommodate chemical modification and reformation of liquid hydrocarbon fuels to produce fuel for fuel cells. Becker and his co-workers used a single flow-through dc-excited microplasma reactor to generate hydrogen from an atmospheric pressure mixture of ammonia and argon for use in small, portable fuel cells. Lindner and Besser experimented with reforming model hydrocarbons such as methane, methanol, and butane into hydrogen for fuel cell feed. Their novel microplasma reactor was a microhollow cathode discharge with a microfluidic channel. Mass and energy balances on these experiments revealed conversions up to nearly 50%, but the conversion of electrical power input to chemical reaction enthalpy was only on the order of 1%. Although through modeling the reforming reaction it was found that the amount of input electrical power to chemical conversion could increase by improving the device as well as the system parameters.\n\nThe use of microplasmas is being looked into for the synthesis of complex macromolecules, as well as the addition of functional groups to the surfaces of other substrates. An article by Klages \"et al.\" describes the addition of amino groups to the surfaces of polymers after treatment with a pulsed DC discharge apparatus using nitrogen containing gases. It was found that ammonia gas microplasmas add on an average of 2.4 amino groups per square nanometer of a nitrocellulose membrane, and increase the strength at which the layers of the substrate can bind. The treatment can also provide a reactive surface for biomedicine, as amino groups are extremely electron-rich and energetic. Mohan Sankaran has done work on the synthesis of nanoparticles using a pulsed DC discharge. His research team has found that by applying a microplasma jet to an electrolytic solution which has either a gold or silver anode is submerged produces the relevant cations. These cations can then capture electrons supplied by the microplasma jet and results in the formation of nanoparticles. The research shows that more nanoparticles of gold and silver are shown in the solution than there are of the resulting salts that form from the acid conducting solution.\n\nMicroplasma uses in research are being considered. The plasma skin regeneration (PSR) device consists of an ultra–high-radiofrequency generator that excites a tuned resonator and imparts energy to a flow of inert nitrogen gas within the handpiece. The plasma generated has an optical emission spectrum with peaks in the visible range (mainly indigo and violet) and near-infrared range. Nitrogen is used as the gaseous source because it is able to purge oxygen from the surface of the skin, minimizing the risk of unpredictable hot spots, charring, and scar formation. As the plasma hits the skin, energy is rapidly transferred to the skin surface, causing instantaneous heating in a controlled uniform manner, without an explosive effect on tissue or epidermal removal.\nIn pretreatment samples, the zone of collagen shows a dense accumulation of elastin, but in posttreatment samples, this zone contains less dense elastin with significant, interlocking new collagen. Repeated low-energy PSR treatment is an effective modality for improving dyspigmentation, smoothness, and skin laxity associated with photoaging. Histologic analysis of posttreatment samples confirms the production of new collagen and remodeling of dermal architecture. Changes consist of erythema and superficial epidermal peeling without complete removal, generally complete by 4 to 5 days.\n\nScientists found that microplasmas are capable of inactivating bacteria that causes tooth decay and periodontal diseases. By directing low temperature microplasma beams at the calcified tissue structure beneath the tooth enamel coating called dentin, it severely reduces the amount of dental bacteria and in turn reduces infection. This aspect of microplasma could allow dentists to use microplasma technology to destroy bacteria in tooth cavities instead of using mechanical means. Developers claim that microplasma devices will enable dentists to effectively treat oral-borne diseases with little pain to their patients. \nRecent studies show that microplasmas can be a very effective method of controlling oral biofilms. Biofilms (also known as slime) are highly organized, three-dimensional bacterial communities. Dental plaque is a common example of oral biofilms. It is the main cause of both tooth decay and periodontal diseases such as Gingivitis and Periodontitis. At the University of Southern California, Parish Sedghizadeh, Director of the USC Center for Biofilms and Chunqi Jiang, assistant research professor in the Ming Hsieh Department of Electrical Engineering-Electrophysics, work with researchers from Viterbi School of Engineering searching for new ways to fight off these bacterial infections. Sedghizadeh explained that the biofilms’ slimy matrix acts as extra protection against traditional antibiotics. However, the centers’ study confirms that biofilms cultivated in the root canal of extracted human teeth can be easily destroyed by the application of microplasma. The plasma emission microscopy obtained during each experiment suggests that the atomic oxygen produced by the microplasma is responsible for the inactivation of bacteria. Sedghizadeh then suggested that the oxygen free radicals could disrupt the biofilms cellular membrane and cause them to break down. According to their ongoing research at USC, Sedghizadeh and Jiang have found that microplasma is not harmful to surrounding healthy tissues and they are confident that microplasma technology will soon become a groundbreaking tool in the medical industry.J.K. Lee along with other scientists in this field have found that microplasma can also be used for teeth bleaching. This reactive species can effectively bleach teeth along with saline or whitening gels that consist of hydrogen peroxide. Lee and his colleagues experimented with this method, examining how microplasma along with hydrogen peroxide effects blood stained human teeth. These scientists took forty extracted single-root, blood stained human teeth and randomly divided them into two groups of twenty. Group one received 30% hydrogen peroxide activated by microplasma for thirty minutes in a pulp chamber, while group two received 30% hydrogen peroxide alone for thirty minutes in the pulp chamber and the temperature was maintained at thirty seven degrees Celsius for both groups. After the tests had been performed, they found that microplasma treatment with 30% hydrogen peroxide had a significant effect on the whiteness of the teeth in group one. Lee and his associates concluded that the application of microplasma along with hydrogen peroxide is an efficient method in the bleaching of stained teeth due to its ability to remove proteins on the surface of teeth and the increased production of hydroxide.\n\nMicroplasma that is sustained near room temperature can destroy bacteria, viruses, and fungi deposited on the surfaces of surgical instruments and medical devices. Researchers discovered that bacteria cannot survive in the harsh environment created by microplasmas. They consist of chemically reactive species such as hydroxyl (OH) and atomic oxygen (O) that can kill harmful bacteria through oxidation. Oxidation of the lipids and proteins that compose a cell’s membrane can lead to the breakdown of the membrane and deactivate the bacteria.\nMicroplasma can contact skin without harming it, making it ideal for disinfecting wounds. “Medical plasmas are said to be in the ‘Goldilocks’ range—hot enough to produce and be an effective treatment, but cold enough to leave tissues unharmed” (Larousi, Kong 1). Researchers have found that microplasmas can be applied directly to living tissues to deactivate pathogens. Scientists have also discovered that microplasmas stop bleeding without damaging healthy tissue, disinfect wounds, accelerate wound healing, and selectively kill some types of cancer cells.\nAt moderate doses, microplasmas can destroy pathogens. At low doses, they can accelerate the replication of cells—an important step in the wound healing process. The ability of microplasma to kill bacteria cells and accelerate the replication of healthy tissue cells is known as the “plasma kill/plasma heal” process, this led scientists to further experiment with the use of microplasmas for wound care. Preliminary tests have also demonstrated successful treatments of some types of chronic wounds.\n\nSince microplasmas deactivate bacteria they may have the ability to destroy cancer cells. Jean Michel Pouvesle has been working at the University of Orléans in France, in the Group for Research and Studies on Mediators of Inflammation (GREMI), experimenting with the effects of microplasma on cancer cells. Pouvesle along with other scientists has created a dielectric barrier discharge and plasma gun for cancer treatment, in which microplasma will be applied to both in vitro and in vivo experiments. This application will reveal the role of ROS (Reactive Oxygen Species), DNA damage, cell cycle modification, and apoptosis induction. Studies show that microplasma treatments are able to induce programmed death (apoptosis) among cancer cells—stopping the rapid reproduction of cancerous cells, with little damage to living human tissues.\nGREMI performs many experiments with microplasmas in cancerology, their first experiment applies microplasma to mice tumors growing beneath the skin's surface. During this experiment, scientists found no changes or burns on the surface of the skin. After a five-day microplasma treatment, the results displayed a significant decrease in the growth of U87 glioma cancer (brain tumor), compared to the control group where microplasma was not applied. GREMI performed further in vitro studies regarding U87 gliomal cancer (brain tumors) and HCT116 (colon tumor) cell lines where microplasma was applied. This microplasma treatment was proven to be an efficient method in destroying cancer cells after being applied over periods of a few tens of seconds. Further studies are being conducted on the effects of microplasma treatment in oncology; this application of microplasma will impact the medical field significantly.\n\n\n\n"}
{"id": "15566308", "url": "https://en.wikipedia.org/wiki?curid=15566308", "title": "Milli mass unit", "text": "Milli mass unit\n\nThe milli mass unit or (mmu) is used as a unit of mass by some scientific authors even though this unit is not defined by the IUPAP red book nor by the IUPAC green book. It is a short form of the tongue-breaking but formally more correct \"milli unified atomic mass unit\" (mu) and equivalent to 1/1000 of the unified atomic mass unit (u). A more modern name is the millidalton (mDa)\nsince the \"unified atomic mass unit\" is more and more displaced by the unit dalton. (1 Da = 1 u)\n\nSince 1961 the unified atomic mass unit \"u\" has been defined as 1/12 the mass of C. Before that the atomic mass unit \"amu\" was defined as 1/16 the mass of O (physics) and as 1/16 the mass of O (chemistry). Thus the publication date in literature ought to be heeded when reading about the milli mass unit as its name does not reveal whether it refers to the old amu or the newer u.\n\nThe mass excess is usually indicated in mu or mmu.\n\nIn mass spectrometry the mass accuracy of a mass analyzer is often indicated in mu, even though a more correct unit would be mTh since mass spectrometers measure the mass-to-charge ratio, not the mass. The relative mass accuracy is often indicated in ppm, even though this is no longer supported by the IUPAC green book which suggests using units like μTh/Th instead of ppm.\n"}
{"id": "21274", "url": "https://en.wikipedia.org/wiki?curid=21274", "title": "Nickel", "text": "Nickel\n\nNickel is a chemical element with symbol Ni and atomic number 28. It is a silvery-white lustrous metal with a slight golden tinge. Nickel belongs to the transition metals and is hard and ductile. Pure nickel, powdered to maximize the reactive surface area, shows a significant chemical activity, but larger pieces are slow to react with air under standard conditions because an oxide layer forms on the surface and prevents further corrosion (passivation). Even so, pure native nickel is found in Earth's crust only in tiny amounts, usually in ultramafic rocks, and in the interiors of larger nickel–iron meteorites that were not exposed to oxygen when outside Earth's atmosphere.\n\nMeteoric nickel is found in combination with iron, a reflection of the origin of those elements as major end products of supernova nucleosynthesis. An iron–nickel mixture is thought to compose Earth's inner core.\n\nUse of nickel (as a natural meteoric nickel–iron alloy) has been traced as far back as 3500 BCE. Nickel was first isolated and classified as a chemical element in 1751 by Axel Fredrik Cronstedt, who initially mistook the ore for a copper mineral, in the cobalt mines of Los, Hälsingland, Sweden. The element's name comes from a mischievous sprite of German miner mythology, Nickel (similar to Old Nick), who personified the fact that copper-nickel ores resisted refinement into copper. An economically important source of nickel is the iron ore limonite, which often contains 1–2% nickel. Nickel's other important ore minerals include pentlandite and a mixture of Ni-rich natural silicates known as garnierite. Major production sites include the Sudbury region in Canada (which is thought to be of meteoric origin), New Caledonia in the Pacific, and Norilsk in Russia.\n\nNickel is slowly oxidized by air at room temperature and is considered corrosion-resistant. Historically, it has been used for plating iron and brass, coating chemistry equipment, and manufacturing certain alloys that retain a high silvery polish, such as German silver. About 9% of world nickel production is still used for corrosion-resistant nickel plating. Nickel-plated objects sometimes provoke nickel allergy. Nickel has been widely used in coins, though its rising price has led to some replacement with cheaper metals in recent years.\n\nNickel is one of four elements (the others are iron, cobalt, and gadolinium) that are ferromagnetic at approximately room temperature. Alnico permanent magnets based partly on nickel are of intermediate strength between iron-based permanent magnets and rare-earth magnets. The metal is valuable in modern times chiefly in alloys; about 68% of world production is used in stainless steel. A further 10% is used for nickel-based and copper-based alloys, 7% for alloy steels, 3% in foundries, 9% in plating and 4% in other applications, including the fast-growing battery sector. As a compound, nickel has a number of niche chemical manufacturing uses, such as a catalyst for hydrogenation, cathodes for batteries, pigments and metal surface treatments. Nickel is an essential nutrient for some microorganisms and plants that have enzymes with nickel as an active site.\n\nNickel is a silvery-white metal with a slight golden tinge that takes a high polish. It is one of only four elements that are magnetic at or near room temperature, the others being iron, cobalt and gadolinium. Its Curie temperature is , meaning that bulk nickel is non-magnetic above this temperature. The unit cell of nickel is a face-centered cube with the lattice parameter of 0.352 nm, giving an atomic radius of 0.124 nm. This crystal structure is stable to pressures of at least 70 GPa. Nickel belongs to the transition metals and is hard and ductile.\n\nThe nickel atom has two electron configurations, [Ar] 3d 4s and [Ar] 3d 4s, which are very close in energy – the symbol [Ar] refers to the argon-like core structure. There is some disagreement on which configuration has the lowest energy. Chemistry textbooks quote the electron configuration of nickel as [Ar] 4s 3d, which can also be written [Ar] 3d 4s. This configuration agrees with the Madelung energy ordering rule, which predicts that 4s is filled before 3d. It is supported by the experimental fact that the lowest energy state of the nickel atom is a 3d 4s energy level, specifically the 3d(F) 4s F, \"J\" = 4 level.\n\nHowever, each of these two configurations splits into several energy levels due to fine structure, and the two sets of energy levels overlap. The average energy of states with configuration [Ar] 3d 4s is actually lower than the average energy of states with configuration [Ar] 3d 4s. For this reason, the research literature on atomic calculations quotes the ground state configuration of nickel as [Ar] 3d 4s.\n\nThe isotopes of nickel range in atomic weight from 48 u () to 78 u ().\n\nNaturally occurring nickel is composed of five stable isotopes; , , , and , with being the most abundant (68.077% natural abundance). Isotopes heavier than cannot be formed by nuclear fusion without losing energy.\n\nNickel-62 has the highest mean nuclear binding energy per nucleon of any nuclide, at 8.7946 MeV/nucleon. Its binding energy is greater than both and , more abundant elements often incorrectly cited as having the most tightly-bound nuclides. Although this would seem to predict nickel-62 as the most abundant heavy element in the universe, the relatively high rate of photodisintegration of nickel in stellar interiors causes iron to be by far the most abundant.\n\nStable isotope nickel-60 is the daughter product of the extinct radionuclide , which decays with a half-life of 2.6 million years. Because has such a long half-life, its persistence in materials in the solar system may generate observable variations in the isotopic composition of . Therefore, the abundance of present in extraterrestrial material may provide insight into the origin of the solar system and its early history.\n\nSome 18 nickel radioisotopes have been characterised, the most stable being with a half-life of 76,000 years, with 100 years, and with 6 days. All of the remaining radioactive isotopes have half-lives that are less than 60 hours and the majority of these have half-lives that are less than 30 seconds. This element also has one meta state.\n\nRadioactive nickel-56 is produced by the silicon burning process and later set free in large quantities during type Ia supernovae. The shape of the light curve of these supernovae at intermediate to late-times corresponds to the decay via electron capture of nickel-56 to cobalt-56 and ultimately to iron-56. Nickel-59 is a long-lived cosmogenic radionuclide with a half-life of 76,000 years. has found many applications in isotope geology. has been used to date the terrestrial age of meteorites and to determine abundances of extraterrestrial dust in ice and sediment. Nickel-78's half-life was recently measured at 110 milliseconds, and is believed an important isotope in supernova nucleosynthesis of elements heavier than iron. The nuclide Ni, discovered in 1999, is the most proton-rich heavy element isotope known. With 28 protons and 20 neutrons Ni is \"double magic\", as is with 28 protons and 50 neutrons. Both are therefore unusually stable for nuclides with so large a proton-neutron imbalance.\n\nOn Earth, nickel occurs most often in combination with sulfur and iron in pentlandite, with sulfur in millerite, with arsenic in the mineral nickeline, and with arsenic and sulfur in nickel galena. Nickel is commonly found in iron meteorites as the alloys kamacite and taenite.\n\nThe bulk of the nickel is mined from two types of ore deposits. The first is laterite, where the principal ore mineral mixtures are nickeliferous limonite, (Fe,Ni)O(OH), and garnierite (a mixture of various hydrous nickel and nickel-rich silicates). The second is magmatic sulfide deposits, where the principal ore mineral is pentlandite: .\n\nAustralia and New Caledonia have the biggest estimate reserves (45% all together).\n\nIdentified land-based resources throughout the world averaging 1% nickel or greater comprise at least 130 million tons of nickel (about the double of known reserves). About 60% is in laterites and 40% in sulfide deposits.\n\nOn geophysical evidence, most of the nickel on Earth is believed to be in the Earth's outer and inner cores. Kamacite and taenite are naturally occurring alloys of iron and nickel. For kamacite, the alloy is usually in the proportion of 90:10 to 95:5, although impurities (such as cobalt or carbon) may be present, while for taenite the nickel content is between 20% and 65%. Kamacite and taenite are also found in nickel iron meteorites.\n\nThe most common oxidation state of nickel is +2, but compounds of Ni, Ni, and Ni are well known, and the exotic oxidation states Ni, Ni, and Ni have been produced and studied.\n\nNickel tetracarbonyl ), discovered by Ludwig Mond, is a volatile, highly toxic liquid at room temperature. On heating, the complex decomposes back to nickel and carbon monoxide:\nThis behavior is exploited in the Mond process for purifying nickel, as described above. The related nickel(0) complex bis(cyclooctadiene)nickel(0) is a useful catalyst in organonickel chemistry because the cyclooctadiene (or \"cod\") ligands are easily displaced.\n\nNickel(I) complexes are uncommon, but one example is the tetrahedral complex NiBr(PPh). Many nickel(I) complexes feature Ni-Ni bonding, such as the dark red diamagnetic prepared by reduction of with sodium amalgam. This compound is oxidised in water, liberating .\n\nIt is thought that the nickel(I) oxidation state is important to nickel-containing enzymes, such as [NiFe]-hydrogenase, which catalyzes the reversible reduction of protons to .\nNickel(II) forms compounds with all common anions, including sulfide, sulfate, carbonate, hydroxide, carboxylates, and halides. Nickel(II) sulfate is produced in large quantities by dissolving nickel metal or oxides in sulfuric acid, forming both a hexa- and heptahydrates useful for electroplating nickel. Common salts of nickel, such as the chloride, nitrate, and sulfate, dissolve in water to give green solutions of the metal aquo complex .\n\nThe four halides form nickel compounds, which are solids with molecules that feature octahedral Ni centres. Nickel(II) chloride is most common, and its behavior is illustrative of the other halides. Nickel(II) chloride is produced by dissolving nickel or its oxide in hydrochloric acid. It is usually encountered as the green hexahydrate, the formula of which is usually written NiCl•6HO. When dissolved in water, this salt forms the metal aquo complex . Dehydration of NiCl•6HO gives the yellow anhydrous .\n\nSome tetracoordinate nickel(II) complexes, e.g. bis(triphenylphosphine)nickel chloride, exist both in tetrahedral and square planar geometries. The tetrahedral complexes are paramagnetic, whereas the square planar complexes are diamagnetic. In having properties of magnetic equilibrium and formation of octahedral complexes, they contrast with the divalent complexes of the heavier group 10 metals, palladium(II) and platinum(II), which form only square-planar geometry.\n\nNickelocene is known; it has an electron count of 20, making it relatively unstable.\nNumerous Ni(III) compounds are known, with the first such examples being Nickel(III) trihalophosphines (Ni(PPh)X). Further, Ni(III) forms simple salts with fluoride or oxide ions. Ni(III) can be stabilized by σ-donor ligands such as thiols and phosphines.\n\nNi(IV) is present in the mixed oxide , while Ni(III) is present in nickel oxide hydroxide, which is used as the cathode in many rechargeable batteries, including nickel-cadmium, nickel-iron, nickel hydrogen, and nickel-metal hydride, and used by certain manufacturers in Li-ion batteries. Ni(IV) remains a rare oxidation state of nickel and very few compounds are known to date.\n\nBecause the ores of nickel are easily mistaken for ores of silver, understanding of this metal and its use dates to relatively recent times. However, the unintentional use of nickel is ancient, and can be traced back as far as 3500 BCE. Bronzes from what is now Syria have been found to contain as much as 2% nickel. Some ancient Chinese manuscripts suggest that \"white copper\" (cupronickel, known as \"baitong\") was used there between 1700 and 1400 BCE. This Paktong white copper was exported to Britain as early as the 17th century, but the nickel content of this alloy was not discovered until 1822. Coins of nickel-copper alloy were minted by the Bactrian kings Agathocles, Euthydemus II and Pantaleon in the 2nd Century BCE, possibly out of the Chinese cupronickel.\n\nIn medieval Germany, a red mineral was found in the Erzgebirge (Ore Mountains) that resembled copper ore. However, when miners were unable to extract any copper from it, they blamed a mischievous sprite of German mythology, Nickel (similar to \"Old Nick\"), for besetting the copper. They called this ore \"Kupfernickel\" from the German \"Kupfer\" for copper. This ore is now known to be nickeline, a nickel arsenide. In 1751, Baron Axel Fredrik Cronstedt tried to extract copper from kupfernickel at a cobalt mine in the Swedish village of Los, and instead produced a white metal that he named after the spirit that had given its name to the mineral, nickel. In modern German, Kupfernickel or Kupfer-Nickel designates the alloy cupronickel.\n\nOriginally, the only source for nickel was the rare Kupfernickel. Beginning in 1824, nickel was obtained as a byproduct of cobalt blue production. The first large-scale smelting of nickel began in Norway in 1848 from nickel-rich pyrrhotite. The introduction of nickel in steel production in 1889 increased the demand for nickel, and the nickel deposits of New Caledonia, discovered in 1865, provided most of the world's supply between 1875 and 1915. The discovery of the large deposits in the Sudbury Basin, Canada in 1883, in Norilsk-Talnakh, Russia in 1920, and in the Merensky Reef, South Africa in 1924, made large-scale production of nickel possible.\n\nAside from the aforementioned Bactrian coins, nickel was not a component of coins until the mid-19th century.\n\n99.9% nickel five-cent coins were struck in Canada (the world's largest nickel producer at the time) during non-war years from 1922–1981; the metal content made these coins magnetic. During the wartime period 1942–45, most or all nickel was removed from Canadian and U.S. coins to save it for manufacturing armor. Canada used 99.9% nickel from 1968 in its higher-value coins until 2000.\n\nCoins of nearly pure nickel were first used in 1881 in Switzerland.\n\nBirmingham forged nickel coins in about 1833 for trading in Malaya.\n\nIn the United States, the term \"nickel\" or \"nick\" originally applied to the copper-nickel Flying Eagle cent, which replaced copper with 12% nickel 1857–58, then the Indian Head cent of the same alloy from 1859–1864. Still later, in 1865, the term designated the three-cent nickel, with nickel increased to 25%. In 1866, the five-cent shield nickel (25% nickel, 75% copper) appropriated the designation. Along with the alloy proportion, this term has been used to the present in the United States.\n\nIn the 21st century, the high price of nickel has led to some replacement of the metal in coins around the world. Coins still made with nickel alloys include one- and two-euro coins, 5¢, 10¢, 25¢ and 50¢ U.S. coins, and 20p, 50p, £1 and £2 UK coins. Nickel-alloy in 5p and 10p UK coins was replaced with nickel-plated steel began in 2012, causing allergy problems for some people and public controversy.\n\nAround 2 million tonnes of nickel are produced annually worldwide. The Philippines, Indonesia, Russia, Canada and Australia are the world's largest producers of nickel, as reported by the US Geological Survey. The largest deposits of nickel in non-Russian Europe are located in Finland and Greece. Identified land-based resources averaging 1% nickel or greater contain at least 130 million tons of nickel. About 60% is in laterites and 40% is in sulfide deposits. In addition, extensive deep-sea resources of nickel are in manganese crusts and nodules covering large areas of the ocean floor, particularly in the Pacific Ocean.\n\nThe one locality in the United States where nickel has been profitably mined is Riddle, Oregon, where several square miles of nickel-bearing garnierite surface deposits are located. The mine closed in 1987. The Eagle mine project is a new nickel mine in Michigan's upper peninsula. Construction was completed in 2013, and operations began in the third quarter of 2014. In the first full year of operation, Eagle Mine produced 18,000 tonnes.\n\nNickel is obtained through extractive metallurgy: it is extracted from the ore by conventional roasting and reduction processes that yield a metal of greater than 75% purity. In many stainless steel applications, 75% pure nickel can be used without further purification, depending on the impurities.\n\nTraditionally, most sulfide ores have been processed using pyrometallurgical techniques to produce a matte for further refining. Recent advances in hydrometallurgical techniques resulted in significantly purer metallic nickel product. Most sulfide deposits have traditionally been processed by concentration through a froth flotation process followed by pyrometallurgical extraction. In hydrometallurgical processes, nickel sulfide ores are concentrated with flotation (differential flotation if Ni/Fe ratio is too low) and then smelted. The nickel matte is further processed with the Sherritt-Gordon process. First, copper is removed by adding hydrogen sulfide, leaving a concentrate of cobalt and nickel. Then, solvent extraction is used to separate the cobalt and nickel, with the final nickel content greater than 99%.\nA second common refining process is leaching the metal matte into a nickel salt solution, followed by the electro-winning of the nickel from solution by plating it onto a cathode as electrolytic nickel.\n\nThe purest metal is obtained from nickel oxide by the Mond process, which achieves a purity of greater than 99.99%. The process was patented by Ludwig Mond and has been in industrial use since before the beginning of the 20th century. In this process, nickel is reacted with carbon monoxide in the presence of a sulfur catalyst at around 40–80 °C to form nickel carbonyl. Iron gives iron pentacarbonyl, too, but this reaction is slow. If necessary, the nickel may be separated by distillation. Dicobalt octacarbonyl is also formed in nickel distillation as a by-product, but it decomposes to tetracobalt dodecacarbonyl at the reaction temperature to give a non-volatile solid.\n\nNickel is obtained from nickel carbonyl by one of two processes. It may be passed through a large chamber at high temperatures in which tens of thousands of nickel spheres, called pellets, are constantly stirred. The carbonyl decomposes and deposits pure nickel onto the nickel spheres. In the alternate process, nickel carbonyl is decomposed in a smaller chamber at 230 °C to create a fine nickel powder. The byproduct carbon monoxide is recirculated and reused. The highly pure nickel product is known as \"carbonyl nickel\".\n\nThe market price of nickel surged throughout 2006 and the early months of 2007; as of April 5, 2007, the metal was trading at US$52,300/tonne or $1.47/oz. The price subsequently fell dramatically, and as of September 2017, the metal was trading at $11,000/tonne, or $0.31/oz.\n\nThe US nickel coin contains of nickel, which at the April 2007 price was worth 6.5 cents, along with 3.75 grams of copper worth about 3 cents, with a total metal value of more than 9 cents. Since the face value of a nickel is 5 cents, this made it an attractive target for melting by people wanting to sell the metals at a profit. However, the United States Mint, in anticipation of this practice, implemented new interim rules on December 14, 2006, subject to public comment for 30 days, which criminalized the melting and export of cents and nickels. Violators can be punished with a fine of up to $10,000 and/or imprisoned for a maximum of five years.\n\nAs of September 19, 2013, the melt value of a U.S. nickel (copper and nickel included) is $0.045, which is 90% of the face value.\n\nThe global production of nickel is presently used as follows: 68% in stainless steel; 10% in nonferrous alloys; 9% in electroplating; 7% in alloy steel; 3% in foundries; and 4% other uses (including batteries).\n\nNickel is used in many specific and recognizable industrial and consumer products, including stainless steel, alnico magnets, coinage, rechargeable batteries, electric guitar strings, microphone capsules, plating on plumbing fixtures, and special alloys such as permalloy, elinvar, and invar. It is used for plating and as a green tint in glass. Nickel is preeminently an alloy metal, and its chief use is in nickel steels and nickel cast irons, in which it typically increases the tensile strength, toughness, and elastic limit. It is widely used in many other alloys, including nickel brasses and bronzes and alloys with copper, chromium, aluminium, lead, cobalt, silver, and gold (Inconel, Incoloy, Monel, Nimonic).\n\nBecause it is resistant to corrosion, nickel was occasionally used as a substitute for decorative silver. Nickel was also occasionally used in some countries after 1859 as a cheap coinage metal (see above), but in the later years of the 20th century was replaced by cheaper stainless steel (i.e., iron) alloys, except in the United States and Canada.\n\nNickel is an excellent alloying agent for certain precious metals and is used in the fire assay as a collector of platinum group elements (PGE). As such, nickel is capable of fully collecting all six PGE elements from ores, and of partially collecting gold. High-throughput nickel mines may also engage in PGE recovery (primarily platinum and palladium); examples are Norilsk in Russia and the Sudbury Basin in Canada.\n\nNickel foam or nickel mesh is used in gas diffusion electrodes for alkaline fuel cells.\n\nNickel and its alloys are frequently used as catalysts for hydrogenation reactions. Raney nickel, a finely divided nickel-aluminium alloy, is one common form, though related catalysts are also used, including Raney-type catalysts.\n\nNickel is a naturally magnetostrictive material, meaning that, in the presence of a magnetic field, the material undergoes a small change in length. The magnetostriction of nickel is on the order of 50 ppm and is negative, indicating that it contracts.\n\nNickel is used as a binder in the cemented tungsten carbide or hardmetal industry and used in proportions of 6% to 12% by weight. Nickel makes the tungsten carbide magnetic and adds corrosion-resistance to the cemented parts, although the hardness is less than those with a cobalt binder.\n\n, with its half-life of 100.1 years, is useful in krytron devices as a beta particle (high-speed electron) emitter to make ionization by the keep-alive electrode more reliable.\n\nAround 27% of all nickel production is destined for engineering, 10% for building and construction, 14% for tubular products, 20% for metal goods, 14% for transport, 11% for electronic goods, and 5% for other uses.\n\nAlthough not recognized until the 1970s, nickel is known to play an important role in the biology of some plants, eubacteria, archaebacteria, and fungi. Nickel enzymes such as urease are considered virulence factors in some organisms. Urease catalyzes the hydrolysis of urea to form ammonia and carbamate. The NiFe hydrogenases can catalyze the oxidation of to form protons and electrons, and can also catalyze the reverse reaction, the reduction of protons to form hydrogen gas. A nickel-tetrapyrrole coenzyme, cofactor F430, is present in methyl coenzyme M reductase, which can catalyze the formation of methane, or the reverse reaction, in methanogenic archaea. One of the carbon monoxide dehydrogenase enzymes consists of an Fe-Ni-S cluster. Other nickel-bearing enzymes include a rare bacterial class of superoxide dismutase and glyoxalase I enzymes in bacteria and several parasitic eukaryotic trypanosomal parasites (in higher organisms, including yeast and mammals, this enzyme contains divalent Zn).\n\nDietary nickel may affect human health through infections by nickel-dependent bacteria, but it is also possible that nickel is an essential nutrient for bacteria residing in the large intestine, in effect functioning as a prebiotic. The U.S. Institute of Medicine has not confirmed that nickel is an essential nutrient for humans, so neither a Recommended Dietary Allowance (RDA) nor an Adequate Intake have been established. The Tolerable Upper Intake Level of dietary nickel is 1000 µg/day as soluble nickel salts. Dietary intake is estimated at 70 to 100 µg/day, with less than 10% absorbed. What is absorbed is excreted in urine. Relatively large amounts of nickel – comparable to the estimated average ingestion above – leach into food cooked in stainless steel. For example, the amount of nickel leached after 10 cooking cycles into one serving of tomato sauce averages 88 µg.\n\nNickel released from Siberian Traps volcanic eruptions is suspected of assisting the growth of \"Methanosarcina\", a genus of euryarchaeote archaea that produced methane during the Permian–Triassic extinction event, the biggest extinction event on record.\n\nThe major source of nickel exposure is oral consumption, as nickel is essential to plants. Nickel is found naturally in both food and water, and may be increased by human pollution. For example, nickel-plated faucets may contaminate water and soil; mining and smelting may dump nickel into waste-water; nickel–steel alloy cookware and nickel-pigmented dishes may release nickel into food. The atmosphere may be polluted by nickel ore refining and fossil fuel combustion. Humans may absorb nickel directly from tobacco smoke and skin contact with jewelry, shampoos, detergents, and coins. A less-common form of chronic exposure is through hemodialysis as traces of nickel ions may be absorbed into the plasma from the chelating action of albumin.\n\nThe average daily exposure does not pose a threat to human health. Most of the nickel absorbed every day by humans is removed by the kidneys and passed out of the body through urine or is eliminated through the gastrointestinal tract without being absorbed. Nickel is not a cumulative poison, but larger doses or chronic inhalation exposure may be toxic, even carcinogenic, and constitute an occupational hazard.\n\nNickel compounds are classified as human carcinogens based on increased respiratory cancer risks observed in epidemiological studies of sulfidic ore refinery workers. This is supported by the positive results of the NTP bioassays with Ni sub-sulfide and Ni oxide in rats and mice. The human and animal data consistently indicate a lack of carcinogenicity via the oral route of exposure and limit the carcinogenicity of nickel compounds to respiratory tumours after inhalation. Nickel metal is classified as a suspect carcinogen; there is consistency between the absence of increased respiratory cancer risks in workers predominantly exposed to metallic nickel and the lack of respiratory tumours in a rat lifetime inhalation carcinogenicity study with nickel metal powder. In the rodent inhalation studies with various nickel compounds and nickel metal, increased lung inflammations with and without bronchial lymph node hyperplasia or fibrosis were observed. In rat studies, oral ingestion of water-soluble nickel salts can trigger perinatal mortality effects in pregnant animals. Whether these effects are relevant to humans is unclear as epidemiological studies of highly exposed female workers have not shown adverse developmental toxicity effects.\n\nPeople can be exposed to nickel in the workplace by inhalation, ingestion, and contact with skin or eye. The Occupational Safety and Health Administration (OSHA) has set the legal limit (permissible exposure limit) for the workplace at 1 mg/m per 8-hour workday, excluding nickel carbonyl. The National Institute for Occupational Safety and Health (NIOSH) specifies the recommended exposure limit (REL) of 0.015 mg/m per 8-hour workday. At 10 mg/m, nickel is immediately dangerous to life and health. Nickel carbonyl [] is an extremely toxic gas. The toxicity of metal carbonyls is a function of both the toxicity of the metal and the off-gassing of carbon monoxide from the carbonyl functional groups; nickel carbonyl is also explosive in air.\n\nSensitized individuals may show a skin contact allergy to nickel known as a contact dermatitis. Highly sensitized individuals may also react to foods with high nickel content. Sensitivity to nickel may also be present in patients with pompholyx. Nickel is the top confirmed contact allergen worldwide, partly due to its use in jewelry for pierced ears. Nickel allergies affecting pierced ears are often marked by itchy, red skin. Many earrings are now made without nickel or low-release nickel to address this problem. The amount allowed in products that contact human skin is now regulated by the European Union. In 2002, researchers found that the nickel released by 1 and 2 Euro coins was far in excess of those standards. This is believed to be the result of a galvanic reaction. Nickel was voted Allergen of the Year in 2008 by the American Contact Dermatitis Society. In August 2015, the American Academy of Dermatology adopted a position statement on the safety of nickel: \"Estimates suggest that contact dermatitis, which includes nickel sensitization, accounts for approximately $1.918 billion and affects nearly 72.29 million people.\"\n\nReports show that both the nickel-induced activation of hypoxia-inducible factor (HIF-1) and the up-regulation of hypoxia-inducible genes are caused by depletion of intracellular ascorbate. The addition of ascorbate to the culture medium increased the intracellular ascorbate level and reversed both the metal-induced stabilization of HIF-1- and HIF-1α-dependent gene expression.\n\n"}
{"id": "5064357", "url": "https://en.wikipedia.org/wiki?curid=5064357", "title": "Nickel(II) oxide", "text": "Nickel(II) oxide\n\nNickel(II) oxide is the chemical compound with the formula . It is notable as being the only well-characterized oxide of nickel (although nickel(III) oxide, and have been claimed). The mineralogical form of , bunsenite, is very rare. It is classified as a basic metal oxide. Several million kilograms are produced in varying quality annually, mainly as an intermediate in the production of nickel alloys.\n\n can be prepared by multiple methods. Upon heating above 400 °C, nickel powder reacts with oxygen to give . In some commercial processes, green nickel oxide is made by heating a mixture of nickel powder and water at 1000 °C, the rate for this reaction can be increased by the addition of . The simplest and most successful method of preparation is through pyrolysis of a nickel(II) compounds such as the hydroxide, nitrate, and carbonate, which yield a light green powder. Synthesis from the elements by heating the metal in oxygen can yield grey to black powders which indicates nonstoichiometry.\n\n adopts the structure, with octahedral Ni and O sites. The conceptually simple structure is commonly known as the rock salt structure. Like many other binary metal oxides, is often non-stoichiometric, meaning that the Ni:O ratio deviates from 1:1. In nickel oxide this non-stoichiometry is accompanied by a color change, with the stoichiometrically correct NiO being green and the non-stoichiometric being black.\n\n has a variety of specialized applications and generally applications distinguish between \"chemical grade\", which is relatively pure material for specialty applications, and \"metallurgical grade\", which is mainly used for the production of alloys. It is used in the ceramic industry to make frits, ferrites, and porcelain glazes. The sintered oxide is used to produce nickel steel alloys. Charles Édouard Guillaume won the 1920 Nobel Prize in Physics for his work on nickel steel alloys which he called invar and elinvar.\n\nAbout 4000 tons of chemical grade are produced annually. Black is the precursor to nickel salts, which arise by treatment with mineral acids. is a versatile hydrogenation catalyst.\n\nHeating nickel oxide with either hydrogen, carbon, or carbon monoxide reduces it to metallic nickel. It combines with the oxides of sodium and potassium at high temperatures (>700 °C) to form the corresponding nickelate.\n\nNickel oxide reacts with chromium(III) oxide in a basic moist environment to form nickel chromate:\n\nLong-term inhalation of NiO is damaging to the lungs, causing lesions and in some cases cancer.\n\nThe calculated half-life of dissolution of NiO in blood is more than 90 days. NiO has a long retention half-time in the lungs; after administration to rodents, it persisted in the lungs for more than 3 months. Nickel oxide is classified as a human carcinogen based on increased respiratory cancer risks observed in epidemiological studies of sulfidic ore refinery workers.\n\nIn a 2-year National Toxicology Program green NiO inhalation study, some evidence of carcinogenicity in F344/N rats but equivocal evidence in female B6C3F1 mice were observed; there was no evidence of carcinogenicity in male B6C3F1 mice. Chronic inflammation without fibrosis was observed in the 2-year studies. \n\n"}
{"id": "33321239", "url": "https://en.wikipedia.org/wiki?curid=33321239", "title": "Olli Heinonen", "text": "Olli Heinonen\n\nOlli Heinonen (born in Finland) is Senior Advisor on Science and Nonproliferation at the Foundation for Defense of Democracies (FDD) and an associate of Harvard University's Belfer Center for Science and International Affairs. Previously, he was the Deputy Director-General for Safeguards at the International Atomic Energy Agency. As such, he helped identify A. Q. Khan.\n\nHeinonen studied radiochemistry at the University of Helsinki where he obtained his Ph.D with a dissertation on nuclear material analysis in 1981. As Senior Research Officer at the Technical Research Centre of Finland Reactor Laboratory, Heinonen was in charge of research and development related to nuclear waste solidification and disposal. Heinonen co-authored several patents on radioactive waste solidification. In 1983 he joined the IAEA. From 1999 to 2002, he was Director of Operations A and from 2002–2005, he was the Director of Operations B in the Department of Safeguards.\n\nJournalist Gareth Porter wrote an article questioning Heinonen's arguments against the US's negotiations with Iran about the latter's nuclear program.\n\n"}
{"id": "41837357", "url": "https://en.wikipedia.org/wiki?curid=41837357", "title": "Pentanitratoaluminate", "text": "Pentanitratoaluminate\n\nPentanitratoaluminate is an anion of aluminium and nitrate groups with formula [Al(NO)] that can form salts called pentanitratoaluminates. It is unusual being a complex with five nitrate groups, and being a nitrate complex of a light element with nitrate. Such a complex with five nitrate groups is called a pentanitratometallate.\n\nThere is a pentanitrato complex with cerium (PPhEt)[Ce(NO)]. Tetranitratoaluminate and hexanitratoaluminate are related anions with aluminium at their core.\n\nThere are two different arrangements for the coordination of nitrate with aluminium in the complex. One nitrate is bonded with two oxygens to the aluminium (bidentate), and the other five oxygen atoms only link via one oxygen (monodentate). The bidentate nitrate has an O–Al length of 1.98 Å. Another two Al–O bonds roughly in the same plane have a length of 1.89 Å. The other two Al–O bonds complete a distorted octahedral arrangement and pop out the top and bottom of the aluminium with a length of 1.93 Å. The bidentate connected nitrate group is distorted so that the uncoordinated terminal oxygen bond is shorter than the coordinated oxygen–nitrogen distance (1.22 versus 1.31 Å). The angles are also warped, with coordinated oxygen angle subtended on the nitrogen of 109°, and angle of these oxygen atoms with the terminal atom of 126°. The whole nitrate is still planar along with the aluminium.\n\nAn example salt is caesium pentanitratoaluminate Cs[Al(NO)]. caesium pentanitratoaluminate crystallises in the trigonal form with \"α\" = 11.16 Å, \"c\" = 10.02 Å, formula mass 602.85, with three molecules per unit cell. The unit cell volume is 1080 Å, measured density 2.69 g/cm. The space group is P321. It is a crystalline substance.\n\nCaesium pentanitratoaluminate has been formed by treating a mixture of caesium chloride and caesium tetracloroaluminate with dinitrogen tetroxide and methyl nitrate, and pumping off the NOCl gas produced. If only caesium tetracloroaluminate is used, omitting the CsCl, then the result contains solid Al(NO)·CHCN as well.\n\nTetramethylammonium pentanitratoaluminate has been made by recrystalising tetramethyl ammonium tetranitratoaluminate in acetonitrile over several weeks. It forms a waxy substance.\n"}
{"id": "4859320", "url": "https://en.wikipedia.org/wiki?curid=4859320", "title": "Pierre Goldschmidt", "text": "Pierre Goldschmidt\n\nMr. Pierre Goldschmidt, a Belgian nuclear scientist, retired June, 2005, as Deputy Director General and Head of the Department of Safeguards at the International Atomic Energy Agency, (IAEA), succeeded by Olli Heinonen. Mr. Goldschmidt is currently a researcher of the Carnegie Endowment for International Peace.\n\nIAEA is an inter-governmental organization under the auspices of the United Nations. Pierre Goldschmidt was appointed DDG in May, 1999. The Department of Safeguards is responsible for verifying that nuclear material placed under safeguards is not diverted to nuclear weapons or other nuclear explosive devices and that there is no undeclared nuclear material or activities in non-nuclear weapons States party to the NPT.\n\nBefore assuming the DDG position, Mr. Goldschmidt was, for 12 years, General Manager of SYNATOM, the company responsible for the fuel supply and spent fuel management of seven Belgian nuclear plants that provide about 60% of that country’s electricity. For six years Goldschmidt was a member of the Directoire of EURODIF, the large French uranium enrichment company.\n\nMr. Goldschmidt has headed numerous European and international committees, including as Chairman of the Uranium Institute in London and Chairman of the Advisory Committee of the EURATOM Supply Agency.\n\nMr. Goldschmidt studied Electro-Mechanical Engineering and holds a Ph.D in Applied Science from the University of Brussels and a master's degree in Nuclear Engineering from the University of California, Berkeley.\n\n"}
{"id": "30047540", "url": "https://en.wikipedia.org/wiki?curid=30047540", "title": "Plasma antenna", "text": "Plasma antenna\n\nA \"plasma antenna\" is a type of radio antenna currently in development in which plasma is used instead of the metal elements of a traditional antenna. A plasma antenna can be used for both transmission and reception. Although plasma antennas have only become practical in recent years, the idea is not new; a patent for an antenna using the concept was granted to J. Hettinger in 1919.\n\nEarly practical examples of the technology used discharge tubes to contain the plasma and are referred to as ionized gas plasma antennas. Ionized gas plasma antennas can be turned on and off and are good for stealth and resistance to electronic warfare and cyber attacks. Ionized gas plasma antennas can be nested such that the higher frequency plasma antennas are placed inside lower frequency plasma antennas. Higher frequency ionized gas plasma antenna arrays can transmit and receive through lower frequency ionized gas plasma antenna arrays. This means that the ionized gas plasma antennas can be co-located and ionized gas plasma antenna arrays can be stacked. Ionized gas plasma antennas can eliminate or reduce co-site interference. Smart ionized gas plasma antennas use plasma physics to shape and steer the antenna beams without the need of phased arrays. Satellite signals can be steered and/or focused in the reflective or refractive modes using banks of plasma tubes making unique ionized gas satellite plasma antennas. The thermal noise of ionized gas plasma antennas is less than in the corresponding metal antennas at the higher frequencies. Solid state plasma antennas (also known as plasma silicon antennas) with steerable directional functionality that can be manufactured using standard silicon chip fabrication techniques are now also in development. Plasma silicon antennas are candidates for use in WiGig (the planned enhancement to Wi-Fi), and have other potential applications, for example in reducing the cost of vehicle-mounted radar collision avoidance systems.\n\nIn an ionized gas plasma antenna, a gas is ionized to create a plasma. Unlike gases, plasmas have very high electrical conductivity so it is possible for radio frequency signals to travel through them so that they act as a driven element (such as a dipole antenna) to radiate radio waves, or to receive them. Alternatively the plasma can be used as a reflector or a lens to guide and focus radio waves from another source.\n\nSolid-state antennas differ in that the plasma is created from electrons generated by activating thousands of diodes on a silicon chip.\n\nPlasma antennas possess a number of advantages over metal antennas, including:\n\n\n"}
{"id": "31917990", "url": "https://en.wikipedia.org/wiki?curid=31917990", "title": "QBlade", "text": "QBlade\n\nQBlade is an open-source, cross-platform simulation software for wind turbine blade design and aerodynamic simulation. It comes with a user-friendly graphical user interface (GUI) based on Qt.\n\nQBlade is an open-source wind turbine calculation software, distributed under the GNU General Public License. The software is seamlessly integrated into XFOIL, an airfoil design and analysis tool. The purpose of this software is the design and aerodynamic simulation of wind turbine blades. The integration in XFOIL allows for the user to rapidly design custom airfoils and compute their performance curves, Extrapolating the performance data to a range of 360°Angle of attack, and directly integrate them into a wind turbine rotor simulation. The integration of QBlade into XFLR's sophisticated graphical user interface makes this software accessible to a large potential user community.\n\nQBlade is especially adequate for teaching, as it provides a ’hands-on’ feeling for HAWT rotor design and shows all the fundamental relationships between blade twist, blade chord, section airfoil performance, turbine control, power and load curves in an easy and intuitive way. QBlade also includes post processing of conducted rotor simulations and gives deep insight into all relevant blade and rotor variables.\n\nQBlade's development started in 2009 as a small part of the PhD work of G. Pechlivanoglou at the Hermann Föttinger Institute of TU Berlin. The initial development was done by D. Marten, at the time an undergraduate student of Physical Engineering. Prof. C.O. Paschereit, head of the Hermann Föttinger Institute was a strong proponent of the idea to release QBlade under GPL and thus the software was quickly hosted on the official site of the institute. The 1st online version was received with positive remarks which led to the continuation of the development. G. Weinzierl, undergraduate student at the time initiated the development of an integrated turbulent wind field generator, while J. Wendler under the supervision of D. Marten developed the VAWT module as well as the Viterna 360° extrapolation module. M. Lennie, performed the development of a structural Euler-Bernoulli beam module (QFEM). N. Moesus further developed and integrated the code of G. Weinzierl and M. Lennie in the QBlade code. Furthermore, he initiated the integration of a fully developed (GUI) for the aeroelastic code FAST inside QBlade. The completion of the integration of all the aforementioned modules in QBlade is handled by J.Wendler, N. Moesus and D. Marten and the updated version (v0.8) of the software was released on the 9th of May 2014.\n\nAn updated stable version was released in August 2015. This included a new aerodynamic module which replaced the BEM of QBlade with a new advanced Lifting Line Theory (LLT) module. Furthermore, a Free Wake Vortex model was implemented for the accurate representation of the near and far wake of the turbine. The entire development of this version was undertaken by D. Marten.\n\nCurrent development focuses on the expansion of both the dynamic and the aerodynamic capabilities of the software as well as its export/import interface. D. Marten is the person mainly responsible for the development and supervision of future versions while G. Pechlivanoglou, J. Saverin, N. Moesus and J. Wendler are currently active in the project.\n\nThe functionality of QBlade includes the following features:\n\n\n-Added unsteady aerodynamics model with Beddoes-Leishman type dynamic stall model\n\n-All project files have been updated to include decomposed polars for the UA model\n\n-Added multi (Reynolds number) polar blade definition in the blade design module\n\n-Added polar preprocessor for dynamic stall simulations\n\n-Added PNoise module for airfoil self-noise evaluation\n\n-Added floating platform turbine simulations through *.sim files\n\n-Binary windfield files (*.bts) can now be imported through the “Windfield” menu\n\n-Added turbine startup simulations, including adaptive time stepping\n\n-Added ParaView export functionality for velocity fields\n\n-Added vortex centered & higher order velocity integration scheme for improved stability\n\n-Added graph menu to choose number and arrangement of graphs\n\n-Added VAWT blade design: “pitch axis” for blade sections has been added as a design parameter\n\n-Added HAWT blade design: Added Z-Offset parameter to advanced design for pre-bend blades\n\n-Added 3D correction for the Himmelskamp effect to LLFVW HAWT simulations\n\n-Added position of bound vortex and evaluation point for AoA's to LLFVW simulation parameters\n\n-Added estimates for vortex induced velocities to LLFVW simulation dialog\n\n-Added .stl and .txt geometry export functionality for VAWT blades\n\n-Added dialog to change graph arrangements\n\n-Several improvements of overall stability, GUI and numerous bug fixes\n\nv0.96.3 includes a hotfix for a problem leading to crashes during polar extrapolation!\n\nQBlade is distributed under the GPL license. It is maintained and continuously developed by the Hermann Föttinger Institute of TU Berlin (Chair of Fluid Dynamics).\n\nQBlade has been successfully validated against the WT_Perf Blade Element Momentum Theory code of NWTC. Furthermore, it showed good agreement with the experimental performance data measured at the NASA Ames Research Center wind tunnel during the National Renewable Energy Laboratory 10m Wind Turbine Testing campaign\n\n\n\n"}
{"id": "643769", "url": "https://en.wikipedia.org/wiki?curid=643769", "title": "Quantum tunnelling", "text": "Quantum tunnelling\n\nQuantum tunnelling or tunneling (see spelling differences) is the quantum mechanical phenomenon where a particle passes through a potential barrier that it classically cannot surmount. This plays an essential role in several physical phenomena, such as the nuclear fusion that occurs in main sequence stars like the Sun. It has important applications to modern devices such as the tunnel diode, quantum computing, and the scanning tunnelling microscope. The effect was predicted in the early 20th century, and its acceptance as a general physical phenomenon came mid-century.\n\nFundamental quantum mechanical concepts are central to this phenomenon, which makes quantum tunnelling one of the novel implications of quantum mechanics. Quantum tunneling is projected to create physical limits to the size of transistors, due to electrons being able to \"tunnel\" past them if the transistors are too small.\n\nTunnelling is often explained in terms of the Heisenberg uncertainty principle and the premise that the quantum object has more than one fixed state (not a wave nor a particle) in general.\n\nQuantum tunnelling was developed from the study of radioactivity, which was discovered in 1896 by Henri Becquerel. Radioactivity was examined further by Marie Curie and Pierre Curie, for which they earned the Nobel Prize in Physics in 1903. Ernest Rutherford and Egon Schweidler studied its nature, which was later verified empirically by Friedrich Kohlrausch. The idea of the half-life and the possibility of predicting decay was created from their work.\n\nIn 1901, Robert Francis Earhart, while investigating the conduction of gases between closely spaced electrodes using the Michelson interferometer to measure the spacing, discovered an unexpected conduction regime. J. J. Thomson commented that the finding warranted further investigation. In 1911 and then 1914, then-graduate student Franz Rother, employing Earhart's method for controlling and measuring the electrode separation but with a sensitive platform galvanometer, directly measured steady field emission currents. In 1926, Rother, using a still newer platform galvanometer of sensitivity 26 pA, measured the field emission currents in a \"hard\" vacuum between closely spaced electrodes.\n\nQuantum tunneling was first noticed in 1927 by Friedrich Hund when he was calculating the ground state of the double-well potential and independently in the same year by Leonid Mandelstam and Mikhail Leontovich in their analysis of the implications of the then new Schrödinger wave equation for the motion of a particle in a confining potential of a limited spatial extent. Its first application was a mathematical explanation for alpha decay, which was done in 1928 by George Gamow (who was aware of the findings of Mandelstam and Leontovich) and independently by Ronald Gurney and Edward Condon. The two researchers simultaneously solved the Schrödinger equation for a model nuclear potential and derived a relationship between the half-life of the particle and the energy of emission that depended directly on the mathematical probability of tunnelling.\n\nAfter attending a seminar by Gamow, Max Born recognised the generality of tunnelling. He realised that it was not restricted to nuclear physics, but was a general result of quantum mechanics that applies to many different systems. Shortly thereafter, both groups considered the case of particles tunnelling into the nucleus. The study of semiconductors and the development of transistors and diodes led to the acceptance of electron tunnelling in solids by 1957. The work of Leo Esaki, Ivar Giaever and Brian Josephson predicted the tunnelling of superconducting Cooper pairs, for which they received the Nobel Prize in Physics in 1973. In 2016, the quantum tunneling of water was discovered.\n\nQuantum tunnelling falls under the domain of quantum mechanics: the study of what happens at the quantum scale. This process cannot be directly perceived, but much of its understanding is shaped by the microscopic world, which classical mechanics cannot adequately explain. To understand the phenomenon, particles attempting to travel between potential barriers can be compared to a ball trying to roll over a hill; quantum mechanics and classical mechanics differ in their treatment of this scenario. Classical mechanics predicts that particles that do not have enough energy to classically surmount a barrier will not be able to reach the other side. Thus, a ball without sufficient energy to surmount the hill would roll back down. Or, lacking the energy to penetrate a wall, it would bounce back (reflection) or in the extreme case, bury itself inside the wall (absorption). In quantum mechanics, these particles can, with a very small probability, \"tunnel\" to the other side, thus crossing the barrier. Here, the \"ball\" could, in a sense, \"borrow\" energy from its surroundings to tunnel through the wall or \"roll over the hill\", paying it back by making the reflected electrons more energetic than they otherwise would have been.\n\nThe reason for this difference comes from the treatment of matter in quantum mechanics as having properties of waves and particles. One interpretation of this duality involves the Heisenberg uncertainty principle, which defines a limit on how precisely the position and the momentum of a particle can be known at the same time. This implies that there are no solutions with a probability of exactly zero (or one), though a solution may approach infinity if, for example, the calculation for its position was taken as a probability of 1, the other, i.e. its speed, would have to be infinity. Hence, the probability of a given particle's existence on the opposite side of an intervening barrier is non-zero, and such particles will appear on the 'other' (a semantically difficult word in this instance) side with a relative frequency proportional to this probability.\n\nThe wave function of a particle summarises everything that can be known about a physical system. Therefore, problems in quantum mechanics center on the analysis of the wave function for a system. Using mathematical formulations of quantum mechanics, such as the Schrödinger equation, the wave function can be solved. This is directly related to the probability density of the particle's position, which describes the probability that the particle is at any given place. In the limit of large barriers, the probability of tunnelling decreases for taller and wider barriers.\n\nFor simple tunnelling-barrier models, such as the rectangular barrier, an analytic solution exists. Problems in real life often do not have one, so \"semiclassical\" or \"quasiclassical\" methods have been developed to give approximate solutions to these problems, like the WKB approximation. Probabilities may be derived with arbitrary precision, constrained by computational resources, via Feynman's path integral method; such precision is seldom required in engineering practice.\n\nThere are several phenomena that have the same behaviour as quantum tunnelling, and thus can be accurately described by tunnelling. Examples include the tunnelling of a classical wave-particle association, evanescent wave coupling (the application of Maxwell's wave-equation to light) and the application of the non-dispersive wave-equation from acoustics applied to \"waves on strings\". Evanescent wave coupling, until recently, was only called \"tunnelling\" in quantum mechanics; now it is used in other contexts.\n\nThese effects are modelled similarly to the rectangular potential barrier. In these cases, there is one transmission medium through which the wave propagates that is the same or nearly the same throughout, and a second medium through which the wave travels differently. This can be described as a thin region of medium B between two regions of medium A. The analysis of a rectangular barrier by means of the Schrödinger equation can be adapted to these other effects provided that the wave equation has travelling wave solutions in medium A but real exponential solutions in medium B.\n\nIn optics, medium A is a vacuum while medium B is glass. In acoustics, medium A may be a liquid or gas and medium B a solid. For both cases, medium A is a region of space where the particle's total energy is greater than its potential energy and medium B is the potential barrier. These have an incoming wave and resultant waves in both directions. There can be more mediums and barriers, and the barriers need not be discrete; approximations are useful in this case.\n\nTunnelling occurs with barriers of thickness around 1-3 nm and smaller, but is the cause of some important macroscopic physical phenomena. For instance, tunnelling is a source of current leakage in very-large-scale integration (VLSI) electronics and results in the substantial power drain and heating effects that plague high-speed and mobile technology; it is considered the lower limit on how small computer chips can be made. Tunnelling is a fundamental technique used to program the floating gates of flash memory, which is one of the most significant inventions that have shaped consumer electronics in the last two decades.\n\nQuantum tunnelling is essential for nuclear fusion in stars. Temperature and pressure in the core of stars are insufficient for nuclei to overcome the Coulomb barrier in order to achieve a thermonuclear fusion. However, there is some probability to penetrate the barrier due to quantum tunnelling. Though the probability is very low, the extreme number of nuclei in a star generates a steady fusion reaction over millions or even billions of years - a precondition for the evolution of life in insolation habitable zones.\n\nRadioactive decay is the process of emission of particles and energy from the unstable nucleus of an atom to form a stable product. This is done via the tunnelling of a particle out of the nucleus (an electron tunnelling into the nucleus is electron capture). This was the first application of quantum tunnelling and led to the first approximations. Radioactive decay is also a relevant issue for astrobiology as this consequence of quantum tunnelling is creating a constant source of energy over a large period of time for environments outside the circumstellar habitable zone where insolation would not be possible (subsurface oceans) or effective.\n\nBy including quantum tunnelling, the astrochemical syntheses of various molecules in interstellar clouds can be explained such as the synthesis of molecular hydrogen, water (ice) and the prebiotic important formaldehyde.\n\nQuantum tunnelling is among the central non trivial quantum effects in quantum biology. Here it is important both as electron tunnelling and proton tunnelling. Electron tunnelling is a key factor in many biochemical redox reactions (photosynthesis, cellular respiration) as well as enzymatic catalysis while proton tunnelling is a key factor in spontaneous mutation of DNA.\n\nSpontaneous mutation of DNA occurs when normal DNA replication takes place after a particularly significant proton has defied the odds in quantum tunnelling in what is called \"proton tunnelling\" (quantum biology). A hydrogen bond joins normal base pairs of DNA. There exists a double well potential along a hydrogen bond separated by a potential energy barrier. It is believed that the double well potential is asymmetric with one well deeper than the other so the proton normally rests in the deeper well. For a mutation to occur, the proton must have tunnelled into the shallower of the two potential wells. The movement of the proton from its regular position is called a tautomeric transition. If DNA replication takes place in this state, the base pairing rule for DNA may be jeopardised causing a mutation. Per-Olov Lowdin was the first to develop this theory of spontaneous mutation within the double helix (quantum bio). Other instances of quantum tunnelling-induced mutations in biology are believed to be a cause of ageing and cancer.\n\nCold emission of electrons is relevant to semiconductors and superconductor physics. It is similar to thermionic emission, where electrons randomly jump from the surface of a metal to follow a voltage bias because they statistically end up with more energy than the barrier, through random collisions with other particles. When the electric field is very large, the barrier becomes thin enough for electrons to tunnel out of the atomic state, leading to a current that varies approximately exponentially with the electric field. These materials are important for flash memory, vacuum tubes, as well as some electron microscopes.\n\nA simple barrier can be created by separating two conductors with a very thin insulator. These are tunnel junctions, the study of which requires understanding of quantum tunnelling. Josephson junctions take advantage of quantum tunnelling and the superconductivity of some semiconductors to create the Josephson effect. This has applications in precision measurements of voltages and magnetic fields, as well as the multijunction solar cell.\n\nQCA is a molecular binary logic synthesis technology that operates by the inter-island electron tunneling system. This is a very low power and fast device that can operate at a maximum frequency of 15 PHz.\n\nDiodes are electrical semiconductor devices that allow electric current flow in one direction more than the other. The device depends on a depletion layer between N-type and P-type semiconductors to serve its purpose; when these are very heavily doped the depletion layer can be thin enough for tunnelling. Then, when a small forward bias is applied, the current due to tunnelling is significant. This has a maximum at the point where the voltage bias is such that the energy level of the p and n conduction bands are the same. As the voltage bias is increased, the two conduction bands no longer line up and the diode acts typically.\n\nBecause the tunnelling current drops off rapidly, tunnel diodes can be created that have a range of voltages for which current decreases as voltage is increased. This peculiar property is used in some applications, like high speed devices where the characteristic tunnelling probability changes as rapidly as the bias voltage.\n\nThe resonant tunnelling diode makes use of quantum tunnelling in a very different manner to achieve a similar result. This diode has a resonant voltage for which there is a lot of current that favors a particular voltage, achieved by placing two very thin layers with a high energy conductance band very near each other. This creates a quantum potential well that has a discrete lowest energy level. When this energy level is higher than that of the electrons, no tunnelling will occur, and the diode is in reverse bias. Once the two voltage energies align, the electrons flow like an open wire. As the voltage is increased further tunnelling becomes improbable and the diode acts like a normal diode again before a second energy level becomes noticeable.\n\nA European research project has demonstrated field effect transistors in which the gate (channel) is controlled via quantum tunnelling rather than by thermal injection, reducing gate voltage from ~1 volt to 0.2 volts and reducing power consumption by up to 100×. If these transistors can be scaled up into VLSI chips, they will significantly improve the performance per power of integrated circuits.\n\nWhile the Drude model of electrical conductivity makes excellent predictions about the nature of electrons conducting in metals, it can be furthered by using quantum tunnelling to explain the nature of the electron's collisions. When a free electron wave packet encounters a long array of uniformly spaced barriers the reflected part of the wave packet interferes uniformly with the transmitted one between all barriers so that there are cases of 100% transmission. The theory predicts that if positively charged nuclei form a perfectly rectangular array, electrons will tunnel through the metal as free electrons, leading to an extremely high conductance, and that impurities in the metal will disrupt it significantly.\n\nThe scanning tunnelling microscope (STM), invented by Gerd Binnig and Heinrich Rohrer, may allow imaging of individual atoms on the surface of a material. It operates by taking advantage of the relationship between quantum tunnelling with distance. When the tip of the STM's needle is brought very close to a conduction surface that has a voltage bias, by measuring the current of electrons that are tunnelling between the needle and the surface, the distance between the needle and the surface can be measured. By using piezoelectric rods that change in size when voltage is applied over them the height of the tip can be adjusted to keep the tunnelling current constant. The time-varying voltages that are applied to these rods can be recorded and used to image the surface of the conductor. STMs are accurate to 0.001 nm, or about 1% of atomic diameter.\n\nIn chemical kinetics, the substitution of a light isotope of an element with a heavier one typically results in a slower reaction rate. This is generally attribute to differences in the zero-point vibrational energies for chemical bonds containing the lighter and heavier isotopes and is generally modeled using transition state theory. However, in certain cases, very large isotope effects are observed that cannot be accounted for by a semi-classical treatment of kinetic isotope effects, and quantum tunneling through the energy barrier is invoked. R. P. Bell has developed a modified treatment of Arrhenius kinetics that is commonly used to model this phenomenon.\n\nSome physicists have claimed that it is possible for spin-zero particles to travel faster than the speed of light when tunnelling. This apparently violates the principle of causality, since there will be a frame of reference in which it arrives before it has left. In 1998, Francis E. Low reviewed briefly the phenomenon of zero-time tunnelling. More recently experimental tunnelling time data of phonons, photons, and electrons have been published by Günter Nimtz.\n\nOther physicists, such as Herbert Winful , have disputed these claims. Winful argues that the wavepacket of a tunnelling particle propagates locally, so a particle can't tunnel through the barrier non-locally. Winful also argues that the experiments that are purported to show non-local propagation have been misinterpreted. In particular, the group velocity of a wavepacket does not measure its speed, but is related to the amount of time the wavepacket is stored in the barrier.\n\nThe following subsections discuss the mathematical formulations of quantum tunnelling.\n\nThe time-independent Schrödinger equation for one particle in one dimension can be written as\n\nwhere formula_3 is the reduced Planck's constant, m is the particle mass, x represents distance measured in the direction of motion of the particle, Ψ is the Schrödinger wave function, V is the potential energy of the particle (measured relative to any convenient reference level), \"E\" is the energy of the particle that is associated with motion in the x-axis (measured relative to V), and M(x) is a quantity defined by V(x) – E which has no accepted name in physics.\n\nThe solutions of the Schrödinger equation take different forms for different values of x, depending on whether M(x) is positive or negative. When M(x) is constant and negative, then the Schrödinger equation can be written in the form\n\nThe solutions of this equation represent travelling waves, with phase-constant +\"k\" or -\"k\".\nAlternatively, if M(x) is constant and positive, then the Schrödinger equation can be written in the form\n\nThe solutions of this equation are rising and falling exponentials in the form of evanescent waves.\nWhen M(x) varies with position, the same difference in behaviour occurs, depending on whether M(x) is negative or positive. It follows that the sign of M(x) determines the nature of the medium, with negative M(x) corresponding to medium A as described above and positive M(x) corresponding to medium B. It thus follows that evanescent wave coupling can occur if a region of positive M(x) is sandwiched between two regions of negative M(x), hence creating a potential barrier.\n\nThe mathematics of dealing with the situation where M(x) varies with x is difficult, except in special cases that usually do not correspond to physical reality. A discussion of the semi-classical approximate method, as found in physics textbooks, is given in the next section. A full and complicated mathematical treatment appears in the 1965 monograph by Fröman and Fröman noted below. Their ideas have not been incorporated into physics textbooks, but their corrections have little quantitative effect.\n\nThe wave function is expressed as the exponential of a function:\n\nformula_8 is then separated into real and imaginary parts:\n\nSubstituting the second equation into the first and using the fact that the imaginary part needs to be 0 results in:\n\nTo solve this equation using the semiclassical approximation, each function must be expanded as a power series in formula_11. From the equations, the power series must start with at least an order of formula_12 to satisfy the real part of the equation; for a good classical limit starting with the highest power of Planck's constant possible is preferable, which leads to\n\nand\n\nwith the following constraints on the lowest order terms,\n\nand\n\nAt this point two extreme cases can be considered.\n\nCase 1\nIf the amplitude varies slowly as compared to the phase formula_17 and\n\nCase 2\n\nIn both cases it is apparent from the denominator that both these approximate solutions are bad near the classical turning points formula_23. Away from the potential hill, the particle acts similar to a free and oscillating wave; beneath the potential hill, the particle undergoes exponential changes in amplitude. By considering the behaviour at these limits and classical turning points a global solution can be made.\n\nTo start, choose a classical turning point, formula_24 and expand formula_25 in a power series about formula_24:\n\nKeeping only the first order term ensures linearity:\n\nUsing this approximation, the equation near formula_24 becomes a differential equation:\n\nThis can be solved using Airy functions as solutions.\n\nTaking these solutions for all classical turning points, a global solution can be formed that links the limiting solutions. Given the two coefficients on one side of a classical turning point, the two coefficients on the other side of a classical turning point can be determined by using this local solution to connect them.\n\nHence, the Airy function solutions will asymptote into sine, cosine and exponential functions in the proper limits. The relationships between formula_32 and formula_33 are\n\nand\n\nWith the coefficients found, the global solution can be found. Therefore, the transmission coefficient for a particle tunnelling through a single potential barrier is\n\nwhere formula_37 are the two classical turning points for the potential barrier.\n\nFor a rectangular barrier, this expression is simplified to:\n\n\n\n"}
{"id": "6146060", "url": "https://en.wikipedia.org/wiki?curid=6146060", "title": "Radium dials", "text": "Radium dials\n\n\nRadium was discovered in the early 1900's and was soon combined with paint to make luminescent paint, which was applied to clocks, airplane instruments, and the like, to be able to read them in the dark. \n\nRadium dials were typically painted by young women, who used to 'point' their brushes by licking and shaping the bristles prior to painting the fine lines and numbers on the dials. This practice resulted in the ingestion of radium, which caused serious jaw-bone degeneration and malignancy and other dental diseases reminiscent of phossy jaw. The disease, radium-induced osteonecrosis, was recognized as an occupational disease in 1925 after a group of radium painters, known as the Radium Girls, from the United States Radium Corporation sued. By 1930, all dial painters stopped pointing their brushes by mouth. Stopping this practice drastically reduced the amount of radium ingested and therefore, the incidence of malignancy, to zero by 1950 among the workers who were studied.\n\n\"Luminous Processes employees interviewed by a journalist in 1978 had been left ignorant of radium's dangers. They were told that eliminating lippointing had ended earlier problems. They worked in unvented rooms, they wore smocks that they laundered at home. Geiger counters could pick up readings from pants returned from a dry cleaner and from clothes stored away in a cedar chest.\"\n\nAlthough old radium dials may no longer produce light, this is frequently due to the breakdown of the crystal structure of zinc sulfide rather than the radioactive decay of the radium, which has a half-life of about 1600 years, so even very old radium dials remain radioactive. Radium paint can be ingested by inhaling flaking paint particles. The alpha particles emitted by the radium, which is taken up in bone, will kill off surrounding bone tissue, resulting in a condition loosely referred to as radium jaw. Inhaled or ingested particles may deposit a high local dose with a risk of lung or gastrointestinal cancer due to the radiation dose. The risks are hard to quantify due to the variable levels of radium in the paint and the quantity ingested or inhaled. Care should be taken in handling these materials especially where the paint is damaged.\n\nThe most common isotope of Radium is Ra, which primarily emits alpha particles. Alpha particles are shielded by most thin materials including paper. However, Ra only emits alpha particles 97 percent of the time with 186 keV gamma emission 3% of the time. Also, Ra is rarely present without other transuranic elements or decay products, which may emit beta or gamma radiation. Therefore, the radiation hazards from radium paint may not be entirely removed by the varnish, case or container. Care should be taken to prevent the inhalation or ingestion of flakes or dust which may contain radioactive materials. Radium dials have been shown to have dose rates near the face of in excess of 10uSv / hr, which would deliver a dose equivalent to one days background exposure in around 20 minutes. This dose rate probably only represents the gamma emission as the alpha emission will be stopped by the lacquer or case. Hence the dose rate following ingestion or inhalation of the dust could be much higher.\n\n\n"}
{"id": "35488450", "url": "https://en.wikipedia.org/wiki?curid=35488450", "title": "Roadside conservation", "text": "Roadside conservation\n\nRoadside conservation is a conservation strategy in Australia and other countries where Road verge flora and habitats are protected or improved. The general aim is to conserve or increase the amount of native flora species; especially where that work will lead to higher conservation value, for example providing food or habitat for rare or endangered native fauna.\n\nPotential benefits of roadside conservation strategies can include: \n\nProblems with the maintaining of roadsides include:\n\nFormal recognition of the importance of roadside reserves occurred in the 1960s when then-Premier of Western Australia, the Hon. David Brand, ensured all new roads in Western Australia would have road reserves at least 40 metres wider than that needed for transport purposes.\n"}
{"id": "2182941", "url": "https://en.wikipedia.org/wiki?curid=2182941", "title": "Silicide", "text": "Silicide\n\nA silicide is a compound that has silicon with (usually) more electropositive elements.\n\nSilicon is more electropositive than carbon. Silicides are structurally closer to borides than to carbides.\n\nSimilar to borides and carbides, the composition of silicides cannot be easily specified as covalent molecules. The chemical bonds in silicides range from conductive metal-like structures to covalent or ionic. Silicides of all non-transition metals, with exception of beryllium, have been described.\n\nMercury, thallium, bismuth, and lead are immiscible with liquid silicon.\n\nSilicon atoms in silicides can have many possible organizations:\n\nA silicide prepared by a self-aligned process is called a salicide. This is a process in which silicide contacts are formed only in those areas in which deposited metal (which after annealing becomes a metal component of the silicide) is in direct contact with silicon, hence, the process is self-aligned. It is commonly implemented in MOS/CMOS processes for ohmic contacts of the source, drain, and poly-Si gate..\n\nGroup 1 and 2 silicides e.g. NaSi and CaSi react with water, yielding hydrogen and/or silanes. At Consumer Electronics Show (CES) 2012 a safe and eco-friendly 1kWh or 3kWh capacity mobile phone charger with sodium silicide that runs on water has introduced for 'people who spend time away from the electricity grid'. Any type of water can be used, including salt water and it can even run on puddle water providing it isn't thickened with mud or any other sediment.\n\nThe transition metal silicides are, in contrast, usually inert to aqueous solutions of everything with exception of hydrofluoric acid; however, they react with more aggressive agents, e.g. melted potassium hydroxide, or fluorine and chlorine when red-hot.\n\nWhen magnesium silicide is placed into hydrochloric acid, HCl(aq), the gas silane, SiH, is produced. This gas is the silicon analogue of methane, CH, but is more reactive. Silane is pyrophoric, that is, due to the presence of oxygen, it spontaneously combusts in air:\n\nThese reactions are typical of a Group 2 silicide. MgSi reacts similarly with sulfuric acid. Group 1 silicides are even more reactive. For example, sodium silicide, NaSi, reacts rapidly with water to yield sodium silicate, NaSiO, and hydrogen gas.\n\nSee for a list.\n\n"}
{"id": "5994075", "url": "https://en.wikipedia.org/wiki?curid=5994075", "title": "Skyguard (area defense system)", "text": "Skyguard (area defense system)\n\nSkyguard is a chemical laser-based area defense system proposed by Northrop Grumman, to protect airports and other areas against a variety of military threats including short-range ballistic missiles, short- and long-range rockets, artillery shells, mortars, unmanned aerial vehicles and cruise missiles. Each shot costs about $1,000 which represents the cost of consumable chemicals.\n\n\n"}
{"id": "21426970", "url": "https://en.wikipedia.org/wiki?curid=21426970", "title": "Solar-powered refrigerator", "text": "Solar-powered refrigerator\n\nA solar-powered refrigerator is a refrigerator which runs on energy directly provided by sun, and may include photovoltaic or solar thermal energy.\n\nSolar-powered refrigerators are able to keep perishable goods such as meat and dairy cool in hot climates, and are used to keep much needed vaccines at their appropriate temperature to avoid spoilage.\n\nSolar-powered refrigerators are typically used in off-the-grid locations where utility provided AC power is not available.\n\nIn 1878, at the Universal Exhibition in Paris, Augustin Mouchot displayed Mouchot's engine and won a Gold Medal in Class 54 for his works, most notably the production of ice using concentrated solar heat.\n\n\"In developed countries, plug-in refrigerators with backup generators store vaccines safely, but in developing countries, where electricity supplies can be unreliable, alternative refrigeration technologies are required\". Solar fridges were introduced in the developing world to cut down on the use of kerosene or gas-powered absorption refrigerated coolers which are the most common alternatives. They are used for both vaccine storage and household applications in areas without reliable electrical supply because they have poor or no grid electricity at all. They burn a liter of kerosene per day therefore requiring a constant supply of fuel which is costly and smelly, and are responsible for the production of large amounts of carbon dioxide. They can also be difficult to adjust which can result in the freezing of medicine. The use of Kerosene as a fuel is now widely discouraged for three reasons: Recurrent cost of fuel, difficulty of maintaining accurate temperature and risk of causing fires.\n\nSolar powered refrigerators are characterized by thick insulation and the use of a DC (not AC) compressor. Traditionally solar-powered refrigerators and vaccine coolers use a combination of solar panels and lead batteries to store energy for cloudy days and at night in the absence of sunlight to keep their contents cool. These fridges are expensive and require heavy lead-acid batteries which tend to deteriorate, especially in hot climates, or are misused for other purposes. In addition, the batteries require maintenance, must be replaced approximately every three years, and must be disposed of as hazardous wastes possibly resulting in lead pollution. These problems and the resulting higher costs have been an obstacle for the use of solar powered refrigerators in developing areas.\n\nIn the mid-1990s NASA JSC began work on a solar powered refrigerator that used phase change material rather than battery to store thermal energy rather than chemical energy. The resulting technology has been commercialized and is being used for storing food products and vaccines. Solar direct-drive refrigerators don't require batteries, instead using thermal energy to solar power. These refrigerators are increasingly being used to store vaccines in remote areas.\n\nSolar-powered refrigerators and other solar appliances are commonly used by individuals living off-the-grid. They provide a means for keeping food safe and preserved while avoiding a connection to utility-provided power. Solar refrigerators are also used in cottages and camps as an alternative to absorption refrigerators, as they can be safely left running year-round. Other uses include being used to keep medical supplies at proper temperatures in remote locations, and being used to temporarily store game at hunting camps.\n\n"}
{"id": "36452813", "url": "https://en.wikipedia.org/wiki?curid=36452813", "title": "Solar power in Albania", "text": "Solar power in Albania\n\nSolar power in Albania is widely available, but not yet developed. All of Albania's electricity comes from hydroelectricity. A program to install solar water heating intended to use $2.75 million to install 75,000 m² of solar panels. By the end of 2010 a total of 52,000 m² had been installed, with an additional 50,000 m² expected by 2015. The number of sunshine hours in Albania ranges from 2100 to 2700.\n\n\n"}
{"id": "50704695", "url": "https://en.wikipedia.org/wiki?curid=50704695", "title": "Specialized pro-resolving mediators", "text": "Specialized pro-resolving mediators\n\nSpecialized pro-resolving mediators (SPM, also termed specialized proresolving mediators) are a large and growing class of cell signaling molecules formed in cells by the metabolism of polyunsaturated fatty acids (PUFA) by one or a combination of lipoxygenase, cyclooxygenase, and cytochrome P450 monooxygenase enzymes. Pre-clinical studies, primarily in animal models and human tissues, implicate SPM in orchestrating the resolution of inflammation.\n\nSPM join the long list of other physiological agents which tend to limit inflammation (see ) including glucocorticoids, interleukin 10 (an anti-inflammatory cytokine), interleukin 1 receptor antagonist (an inhibitor of the action of pro-inflammatory cytokine, interleukin 1), annexin A1 (an inhibitor of formation of pro-inflammatory metabolites of polyunsaturated fatty acids), and the gaseous resolvins, carbon monoxide (see ), nitric oxide (see ), and hydrogen sulfide (see and ).\n\nThe absolute as well as relative roles of the SPM along with other physiological anti-inflammatory agents in resolving human inflammatory responses remain to be defined precisely. However, studies suggest that synthetic SPM that are resistant to being metabolically inactivated hold promise of being clinically useful pharmacological tools for preventing and resolving a wide range of pathological inflammatory responses along with the tissue destruction and morbidity that these responses cause. Based on animal model studies, the inflammation-based diseases which may be treated by such metabolically resistant SPM analogs include not only pathological and tissue damaging responses to invading pathogens but also a wide array of pathological conditions in which inflammation is a contributing factor such as allergic inflammatory diseases (e.g. asthma, rhinitis), autoimmune diseases ( e.g. rheumatoid arthritis, systemic lupus erythematosus), psoriasis, atherosclerosis disease leading to heart attacks and strokes, type 1 and type 2 diabetes, the metabolic syndrome, and certain dementia syndromes (e.g. Alzheimer's disease, Huntington's disease).\n\nMany of the SPM are metabolites of omega-3 fatty acids and have been proposed to be responsible for the anti-inflammatory actions that are attributed to omega-3 fatty acid-rich diets.\n\nThrough most of its early period of study, acute inflammatory responses were regarded as self-limiting innate immune system reactions to invading foreign organisms, tissue injuries, and other insults. These reactions were orchestrated by various soluble signaling agents such as a) foreign organism-derived N-formylated oligopeptide chemotactic factors (e.g. N-formylmethionine-leucyl-phenylalanine); b) complement components C5a and C3a which are chemotactic factors formed during the activation of the host's blood complement system by invading organisms or injured tissues; and c) host cell-derived pro-inflammatory cytokines (e.g. interleukin 1s), host-derived pro-inflammatory chemokines (e.g. CXCL8, CCL2, CCL3, CCL4, CCL5, CCL11, CXCL10), platelet-activating factor, and PUFA metabolites including in particular leukotrienes (e.g. LTB4), hydroxyeicosatetraenoic acids (e.g., 5-HETE, 12-HETE), the hydroxylated heptadecatreineoic acid, 12-HHT, and oxoeicosanoids (e.g. 5-oxo-ETE). These agents functioned as pro-inflammatory signals by increasing the permeability of local blood vessels; activating tissue-bound pro-inflammatory cells such as mast cells, and macrophages; and attracting to nascent inflammatory sites and activating circulating neutrophils, monocytes, eosinophils, gamma delta T cells, and Natural killer T cells. The cited cells then proceeded to neutralize invading organisms, limit tissue injury, and initiate tissue repair. Hence, the classic inflammatory response was viewed as fully regulated by the soluble signaling agents. That is, the agents formed, orchestrated an inflammatory cell response, but then dissipated to allow resolution of the response. In 1974, however, Charles N. Serhan and his renowned colleagues, Mats Hamberg and Bengt Samuelsson, discovered that human neutrophils metabolize arachidonic acid to two novel products that contain 3 hydroxyl residues and 4 double bonds viz., 5,6,15-trihydroxy-7,9,11,13-icosatetraenoic acid and 5,14,15-trihydroxy-6,8,10,12-icosatetraenoic acid. These products are now termed lipoxin A4 and B4, respectively. While initially found to have in vitro activity suggesting that they might act as pro-inflammatory agents, Serhan and colleagues and other groups found that the lipoxins as well as a large number of newly discovered metabolites of other PUFA possess primarily if not exclusively anti-inflammatory activities and therefore may be crucial for causing the resolution of inflammation. In this view, inflammatory responses are not self-limiting but rather limited by the formation of a particular group of PUFA metabolites that counteract the actions of pro-inflammatory signals. Later, these PUFA metabolites were classified together and termed specialized pro-resolving mediators (i.e. SPM).\n\nThe production and activities of the SPM suggest a new view of inflammation wherein the initial response to foreign organisms, tissue injury, or other insults involves numerous soluble cell signaling molecules that not only recruit various cell types to promote inflammation but concurrently cause these cells to produce SPM which feed back on their parent and other cells to dampen their pro-inflammatory activity and to promote repair. Resolution of an inflammatory response is thus an active rather than self-limiting process which is set into motion at least in part by the initiating pro-inflammatory mediators (e.g. prostaglandin E2 and prostaglandin D2) which instruct relevant cells to produce SPM and to assume a more anti-inflammatory phenotype. Resolution of the normal inflammatory response, then, may involve switching production of pro-inflammatory to anti-inflammatory PUFA metabolites. Excessive inflammatory responses to insult as well as many pathological inflammatory responses that contribute to diverse diseases such as atherosclerosis, diabetes, Alzheimer's disease, inflammatory bowel disease, etc. (see ) may reflect, in part, a failure in this class switching. Diseases caused or worsened by non-adaptive inflammatory responses may by amenable to treatment with SPM or synthetic SPM which, unlike natural SPM, resist in vivo metabolic inactivation. The SPM possess overlapping activities which work to resolve inflammation. SPMs (typically more than one for each listed action) have the following anti-inflammatory activities on the indicated cell types as defined in animal and human model studies:\n\nSPMs also stimulate anti-inflammatory and tissue reparative types of responses in epithelium cells, endothelium cells, fibroblasts, smooth muscle cells, osteoclasts, osteoblasts, goblet cells, and kidney podocytes as well as activate the heme oxygenase system of cells thereby increasing the production of the tissue-protective gaso-transmitter, carbon monoxide (see Carbon monoxide#Normal human physiology), in inflamed tissues.\n\nSPM are metabolites of arachidonic acid (AA), eicosapentaenoic acid (EPA), docosahexaenoic acid (DHA), or n-3 DPA (i.e. \"7\",10\"Z\",13\"Z\",19\"Z\"-docosapentaenoic acid or clupanodonic acid); these metabolites are termed lipoxins (Lx), resolvins (Rv), protectins (PD) (also termed neuroprotectins [NP]), and maresins (MaR). EPA, DHA, and n-3 DPA are n-3 fatty acids; their conversions to SPM are proposed to be one mechanism by which n-3 fatty acids may ameliorate inflammatory diseases (see Omega-3 fatty acid#Inflammation). SPM act, at least in part, by either activating or inhibiting cells through binding to and thereby activating or inhibiting the activation of specific cellular receptors.\n\nHuman cells synthesize LxA4 and LxB4 by serially metabolizing arachidonic acid (5\"Z\",8\"Z\",11\"Z\",14\"Z\"-eicosatrienoic acid) with a) ALOX15 (or possibly ALOX15B) followed by ALOX5; b) ALOX5 followed by ALOX15 (or possibly ALOX15B); or c) ALOX5 followed by ALOX12. Cells and, indeed, humans treated with aspirin form the 15\"R\"-hydroxy Epimer lipoxins of these two 15\"S\"-lipoxins viz., 15-epi-LXA4 and 15-epi-LXB4, through a pathway that involves ALOX5 followed by aspirin-treated cyclooxygenase 2 (COX2). Aspirin-treated COX-2, while inactive in metabolizing arachidonic acid to prostanoids, metabolizes this PUFA to 15\"R\"-hydroperoxy-eicosatetraenoic acid whereas the ALOX15 (or ALOX15B) pathway metabolizes arachidonic acid to 15\"S\"-hydroperoxy-eicosatetraenoic acid. The two aspirin-triggered lipoxins (AT-lipoxins) or epi-lipoxins differ structurally from LxA4 and LxB4 only in the \"S\" versus \"R\" chirality of their 15-hydroxyl residue. Numerous studies have found that these metabolites have potent anti-inflammatory activity in vitro and in animal models and in humans may stimulate cells by binding to certain Receptor (biochemistry)s in or on these cells. The following table lists the structural formulae (ETE stands for eicosatetraenoic acid), major activities, cellular receptor targets (where known), and Wikipedia pages giving further information on the activity and synthesis of the lipoxins.\n\nResolvins are metabolites of omega-3 fatty acids, EPA, DHA, and 7\"Z\",10\"Z\",13\"Z\",16\"Z\",19\"Z\"-docosapentaenoic acid (n-3 DPA). All three of these omega-3 fatty acids are abundant in salt water fish, fish oils, and other seafood. n-3 DPA (also termed clupanodonic acid) is to be distinguished from its n-6 DPA isomer, i.e. 4\"Z\",7\"Z\",10\"Z\",13\"Z\",16\"Z\"-docosapentaenoic acid, also termed osbond acid.\n\nCells metabolize EPA (5\"Z\",8\"Z\",11\"Z\",14\"Z\",17\"Z\"-eicosapentaenoic acid) by a cytochrome P450 monooxygenase(s) (in infected tissues a bacterial cytochrome P450 may supply this activity) or aspirin-treated cyclooxygenase-2 to 18\"R\"-hydroperoxy-EPA which is then reduced to 18\"R\"-hydroxy-EPA and further metabolized by ALOX5 to 5\"S\"-hydroperoxy-18\"R\"-hydroxy-EPA; the later product may be reduced to its 5,18-dihydroxy product, RvE2, or converted to its 5,6-epoxide and then acted on by an epoxide hydrolase to form a 5,12,18-trihydroxy derivative, RvE1. In vitro, ALOX5 can convert 18\"S\"-HETE to the 18\"S\" analog of RvE1 termed 18\"S\"-RvE1. 18\"R\"-HETE or 18\"S\"-HETE may also be metabolized by ALOX15 to its 17\"S\"-hydroperoxy and then reduced to its 17\"S\"-hydroxy product, Rv3. Rv3, as detected in in vitro studies, is a dihydroxy mixture of 18\"S\"-dihydroxy (i.e. 18\"S\"-RvE3) and 18\"R\"-dihydroxy (i.e. 18\"R\"-RvE3) isomers, both of which, similar to the other aforementioned metabolites possess potent SPM activity in in vitro and/or animal models. In vitro studies find that ALOX5 can convert 18\"S\"-hydroperoxy-EPA to the 18\"S\"-hydroxy analog of RvE2 termed 18\"S\"-RvE2. 18\"S\"-RvE2, however has little or no SPM activity and is therefore not considered to be a SPM here. The following table lists the structural formulae (EPA stands for eicosapentaenoic acid), major activities, cellular receptor targets (where known), and Wikipedia pages giving further information on the activity and syntheses.\n\n\nCells metabolize DHA (4\"Z\",7\"Z\",10\"Z\",13\"Z\",16\"Z\",19\"Z\"-docosahexaenoic acid) by either ALOX15 or a cytochrome P450 monooxygenase(s) (bacteria may supply the cytochrome P450 activity in infected tissues) or aspirin-treated cyclooxygenase-2 to 17\"S\"-hydroperoxy-DHA which is reduced to 17\"S\"-hydroxy-DHA. ALOX5 metabolizes this intermediate to a) 7\"S\"-hydroperoxy,17\"S\"-hydroxy-DHA which is then reduced to its 7\"S\",17\"S\"-dihydroxy analog, RvD5; b) 4\"S\"-hydroperoxy,17\"S\"-hydroxy-DHA which is reduced to its 4\"S\",17\"S\"-dihydroxy analog, RvD6; c) 7\"S\",8\"S\"-epoxy-17\"S\"-DHA which is then hydrolyzed to 7,8,17-trihydroxy and 7,16,17-trihydorxy products, RvD1 and RvD2, respectively; and d) 4\"S\",5\"S\"-epoxy-17\"S\"-DHA which is then hydrolyzed to 4,11,17-trihydroxy and 4,5,17-trihydroxy products, RvD3 and RvD4, respectively. These six RvDs possess a 17\"S\"-hydroxy residue; however, if aspirin-treated cyclooxygenase-2 is the initiating enzyme, they contain a 17\"R\"-hydroxy residue and are termed 17\"R\"-RvDs, aspirin-triggered-RvDs, or AT-RvDs 1 thru 6. In certain cases, the final structures of these AT-RvDs is assumed by analogy to the structures of their RvD counterparts. Studies have found that most (and presumably all) of these metabolites have potent anti-inflammatory activity in vitro and/or in animal models. The following table lists the structural formulae, major activities with citations, cellular receptor targets, and Wikipedia pages giving further information on the activity and synthesis of these D series resolvins.\n\n\nn-3 DPA (i.e. 7\"Z\",10\"Z\",13\"Z\",16\"Z\",19\"Z\"-docosahexaenoic acid)-derived resolvins are recently identified SPM. In the model system used to identify them, human platelets pretreated with aspirin to form acetylated COX2 or the statin, atorvastatin, to form S-ntrosylated and thereby modify this enzyme's activity metabolize n-3 DPA to form a 13\"R\"-hydroperoxy-n-3 DPA intermediate which is passed over to nearby human neutrophils; these cell then metabolize the intermediate to four poly-hydroxyl metabolites termed resolvin T1 (RvT1), RvT2, RvT3, and RvT4. (The chirality of their hydroxyl residues has not yet been determined.) These T series resolvins also form in mice undergoing experimental inflammatory responses and have potent in vitro and in vivo anti-inflammatory activity; they are particularly effective in reducing the systemic inflammation as well as increasing the survival of mice injected with lethal doses of E. coli bacteria. Another set of newly described n-3 DPA resolvins, RvD1, RvD2, and RvD5, have been named based on their presumed structural analogies to the DHS-derived resolvins RvD1, RvD2, and RvD5, respectively. These three n-3 DPA-derived resolvins have not been defined with respect to the chirality of their hydroxyl residues or the Cis–trans isomerism of their double bonds but do possess potent anti-inflammatory activity in animal models and human cells; they also have protective actions in increasing the survival of mice subjected to E. coli sepsis. The following table lists the structural formulae (DPA stands for docosapentaenoic acid), major activities, cellular receptor targets (where known), and Wikipedia pages giving further information on the activity and syntheses.\n\nCells metabolize DHA by either ALOX15, by a bacterial or mammalian cytochrome P450 monooxygenase (Cyp1a1, Cyp1a2, or Cyp1b1 in mice; see CYP450#CYP families in humans and CYP450#animals) or by aspirin-treated cyclooxygenase-2 to 17\"S\"-hydroperoxy or 17\"R\"-hydroperoxy intermediates (see previous subsection); this intermediate is then converted to a 16\"S\",17\"S\"-epoxide which is then hydrolyzed (probably by a soluble epoxide hydrolase to protectin D1 (PD1, also termed neuroprotectin D1 [NPD1] when formed in neural tissue). PDX is formed by the metabolism of DHA by two serial lipoxygenases, probably a 15-lipoxygenase and ALOX12. 22-Hydroxy-PD1 (also termed 22-hydroxy-NPD1) is formed by the Omega oxidation of PD1 probably by an unidentified cytochrome P450 enzyme. While omega-oxidation products of most bioactive PUFA metabolites are far weaker than their precursors, 22-hydroxy-PD1 is as potent as PD1 in inflammatory assays. Aspirin-triggered-PD1 (AT-PD1 or AP-NPD1) is the 17\"R\"-hydroxyl diastereomer of PD1 formed by the initial metabolism of DHA by aspirin-treated COX-2 or possibly a cytochrome P450 enzyme to 17\"R\"-hydroxy-DHA and its subsequent metabolism possibly in manner similar to that which forms PD1. 10-Epi-PD1 (ent-AT-NPD1), the 10\"S\"-hydroxy diastereomer of PD1, has been detected in small amounts in human neutrophils. While its in vivo synthetic pathway has not been defined, 10-epi-PD1 has anti-inflammatory activity. The following table lists the structural formulae (DHA stands for docosahexaenoic acid), major activities, cellular receptor targets (where known), and Wikipedia pages giving further information on the activity and syntheses.\n\n\nn-3 DPA-derived protectins with structural similarities to PD1 and PD2 have been described, determined to be formed in vitro and in animal models, and termed PD1 and PD2, respectively. These products are presumed to be formed in mammals by the metabolism of n-3 DPA by an unidentified 15-lipoxygenase activity to 16,17-epoxide intermediate and the subsequent conversion of this intermediate to the di-hydroxyl products PD1 and PD2. PD1 has anti-inflammatory activity in a mouse model of peritonitis; PD2 has anti-inflammatory activity in an in vitro model. The following table lists the structural formulae (DPA stands for docosapentaenoic acid), major activities, cellular receptor targets (where known), and Wikipedia pages giving further information on the activity and syntheses.\n\nCells metabolize DHA by ALOX12, other lipoxygenase, (12/15-lipoxygenase in mice), or an unidentified pathway to a 13\"S\",14\"S\"-epoxide-4\"Z\",7\"Z\",9\"E\",11\"E\",16\"Z\",19\"Z\"-DHA intermediate (13\"S\",14\"S\"-epoxy-marisin MaR) and then hydrolyze this intermediate by an epoxide hydrolase activity (which ALOX 12 and mouse 12/15-lipoxygenase possess) to MaR1 and MaR2. During this metabolism, cells also form 7-epi-Mar1, i.e. the 7\"S\"-12\"E\" isomer of Mar1, as well as the 14\"S\"-hydroxy and 14\"R\"-hydroxy metabolites of DHA. The latter hydroxy metabolites can be converted by an unidentified cytochrome P450 enzyme to maresin like-1 (Mar-L1) and Mar-L2 by omega oxidation; alternatively, DHA may be first metabolized to 22-hydroxy-DHA by CYP1A2, CYP2C8, CYP2C9, CYP2D6, CYP2E1, or CYP3A4 and then metabolized through the cited epoxide-forming pathways to Mar-L1 and MaR-L2. Studies have found that these metabolites have potent anti-inflammatory activity in vitro and in animal models. The following table lists the structural formulae (DHA stands for docosahexaenoic acid), major activities, cellular receptor targets (where known), and Wikipedia pages giving further information on the activity and syntheses.\n\n\nn-3 DPA-derived maresins are presumed to be formed in mammals by metabolism of n-3 DPA by an undefined 12-lipoxygenase activity to a 14-hydroperoxy-DPA intermediated and the subsequent conversion of this intermediate to di-hydroxyl products which have been termed MaR1, MaR2, and MaR3 based on their structural analogies to MaR1, MaR2, and MaR3, respectively. MaR1 and MaR have been found to possess anti-inflammatory activity in in vitro assays of human neutrophil function. These n-3 DPA-derived maresins have not been defined with respect to the chirality of their hydroxyl residues or the cis–trans isomerism of their double bonds. The following table lists the structural formulae (DPA stands for docosapentaenoic acid), major activities, cellular receptor targets (where known), and Wikipedia pages giving further information on the activity and syntheses.\n\nThe following PUFA metabolites, while not yet formally classified as SPM, have been recently described and determined to have anti-inflammatory activity.\n\n10\"R\",17\"S\"-dihydroxy-7\"Z\",11\"E\",13\"E\",15\"Z\",19\"Z\"-docosapentaenoic acid (10\"R\",17\"S\"-diHDPA) has been found in inflamed exudates of animal models and possesses in vitro and in vivo anti-inflammatory activity almost as potently as PD1.\n\nn-6 DPA (i.e. 4\"Z\",7\"Z\",10\"Z\",13\"Z\",16\"Z\"-docosapentaenoic acid or osbond acid) is an isomer of n-3 DPA (clupanodonic acid) differing form the latter fatty acid only in the location of its 5 double bonds. Cells metabolize n-6 DPA to 7-hydroxy-DPA, 10,17-dihydroxy-DPA, and 7,17-dihydroxy-DPA; the former two metabolites have been shown to possess anti-inflammatory activity in in vitro and in animal model studies.\n\nCells metabolize DHA and n-3 DPA by COX2 to 13-hydroxy-DHA and 13-hydroxy-DPA products and by aspirin-treated COX2 to 17-hydroxy-DHA and 17-hydroxy-DPA products and may then oxidize these products to there corresponding oxo (i.e. ketone) derivatives, 13-oxo-DHA (also termed electrophilic fatty acid oxo derivative or EFOX-D6), 13-oxo-DPA (EFOX-D5), 17-oxo-DHA (17-EFOX-D6), and 17-oxo-DPA (17-EFOX-D3). These oxo metabolites directly activate the nuclear receptor Peroxisome proliferator-activated receptor gamma and possess anti-inflammatory activity as assesses in in vitro systems.\n\nDHA ethanolamide ester (the DHA analog of arachindonyl ethanolamide [i.e. Anandamide]) is metabolized to 10,17-dihydroxydocosahexaenoyl ethanolamide (10,17-diHDHEA) and/or 15-hydroxy-16(17)-epoxy-docosapentaenoyl ethanolamide (15-HEDPEA) by mouse brain tissue and human neutrophils. Both compounds possess anti-inflammatory activity in vitro; 15-HEDPEA also has tissue-protective effects in mouse models of lung injury and tissue reperfusion. Like anandamide, both compounds activated the Cannabinoid receptor.\n\nPUFA derivatives containing a Cyclopentenone structure are chemically reactive and can form adducts with various tissue targets, particularly proteins. Certain of these PUFA-cyclopentenones \nbind to the sulfur residues in the KEAP1 component of the KEAP1-NFE2L2 protein complex in the cytosol of cells. This negates KEAP1's ability to bind NFE2L2; in consequence, NFE2L2 becomes free to translocate to the nuclease and stimulate the transcription of genes that encode proteins active in detoxifying reactive oxygen species; this effect tends to reduce inflammatory reactions. PUFA-cyclopentenones may likewise react with the IKK2 component of the cytosolic IKK2-NFκB protein complex thereby inhibiting NFκB from stimulating the transcription of genes that encode various pro-inflammatory proteins. One or both of these mechanisms appears to contribute to the ability of certain highly reactive PUFA-cyclopenetenones to exhibit SPM activity. The PUFA-cyclopentenones include two prostaglandins, (PG) Δ12-PGJ2 and 15-deoxy-Δ12,14-PGJ2, and two isoprostanes, 5,6-epoxyisoprostane E2 and 5,6-epoxyisoprostane A2. Both PGJ2's are arachidonic acid-derived metabolites made by cyclooxygenases, primarily COX-2, which is induced in many cell types during inflammation. Both isoprostanes form non-enzymatically as a result the attack on the arachidonic acid bond to cellular phospholipids by reactive oxygen species; they are then release from the phospholipids to become free in attacking their target proteins. All four products have been shown to form and possess SPM activity in various in vitro studies of human and animal tissue as well as in in vivo studies of animal models of inflammation; they have been termed pro-resolving mediators of inflammation\n\nMice made deficient in their 12/15-lipoxygenase gene (Alox15) exhibit a prolonged inflammatory response along with various other aspects of a pathologically enhanced inflammatory response in experimental models of cornea injury, airway inflammation, and peritonitis. These mice also show an accelerated rate of progression of atherosclerosis whereas mice made to overexpress 12/15-lipoxygenase exhibit a delayed rate of atherosclerosis development. Alox15 overexpressing rabbits exhibited reduced tissue destruction and bone loss in a model of periodontitis. Similarly, Alox5 deficient mice exhibit a worsened inflammatory component, failure to resolve, and/or decrease in survival in experimental models of respiratory syncytial virus disease, Lyme disease, Toxoplasma gondii disease, and corneal injury. These studies indicate that the suppression of inflammation is a major function of 12/15-lipoxygenase and Alox5 along with the SPMs they make in at least certain rodent experimental inflammation models; although these rodent lipoxygenases differ from human ALOX15 and ALOX5 in the profile of the PUFA metabolites that they make as well as various other parameters (e.g. tissue distribution), these genetic studies allow that human ALOX15, ALOX5, and the SPMs they make may play a similar anti-inflammatory functions in humans.\n\nConcurrent knockout of the three members of the CYP1 family of Cytochrome P450 enzymes in mice, i.e. Cyp1a1, Cyp1a2, and Cyp1b1, caused an increase in the recruitment of neutrophils to the peritoneum in mice undergoing experimental peritonitis; these triple knockout mice also exhibited an increase in the peritoneal fluid LTB4 level and decreases in the levels of peritoneal fluid NPD1 as well as the precursors to various SPMS including 5-hydroxyeicosatetraenoic acid, 15-Hydroxyeicosatetraenoic acid, 18-hydroxyeicosapentaenoic acid, 17-hydroxydocosahexaenoic acid, and 14-hydroxydocosahexaenoic. These results support the notion that Cyp1 enzymes contribute to the production of certain SPMs and inflammatory responses in mice; CYP1 enzymes may therefore play a similar role in humans.\n\nIn a randomized controlled trial, AT-LXA4 and a comparatively stable analog of LXB4, 15\"R/S\"-methyl-LXB4, reduced the severity of eczema in a study of 60 infants. A synthetic analog of ReV1 is in clinical phase III testing (see Phases of clinical research) for the treatment of the inflammation-based dry eyesyndrome; along with this study, other clinical trials (NCT01639846, NCT01675570, NCT00799552 and NCT02329743) using an RvE1 analogue to treat various ocular conditions are underway. RvE1, Mar1, and NPD1 are in clinical development studies for the treatment of neurodegenerative diseases and hearing loss. And, in a single study, inhaled LXA4 decreased LTC4-initiated bronchoprovocation in patients with asthma.\n"}
{"id": "8433455", "url": "https://en.wikipedia.org/wiki?curid=8433455", "title": "The Power of Community: How Cuba Survived Peak Oil", "text": "The Power of Community: How Cuba Survived Peak Oil\n\nThe Power of Community: How Cuba Survived Peak Oil is an American documentary film that explores the Special Period in Peacetime and its aftermath; the economic collapse and eventual recovery of Cuba following the fall of the Soviet Union in 1991. Following the dramatic steps taken by both the Cuban government and citizens, its major themes include urban agriculture, energy dependence, and sustainability. The film was directed by Faith Morgan, and was released in 2006 by The Community Solution.\n\nThe film is a reflection of the peak oil scenario argued by oil industry experts and political activists, including Matthew Simmons and James Howard Kunstler. The Cuban economy, heavily dependent on economic aid from the Soviet Union, suffered tremendously following the end of the Cold War. The nation lost half of its oil imports, and 85 percent of its international trade economy. Director Faith Morgan, together with the non-profit group The Community Solution, seeks to educate audiences about peak oil and the impact it will have on transportation, agriculture, medicine, and other industries.\n\nThe idea for a film based on the Cuban recovery first arose in August 2003 when Morgan traveled to Cuba as part of the Global Exchange program. Amazed by stories of survival during The Special Period, she learned that the Cuban economic crisis was survived with a fundamental shift in the country's economic policies, rather than with new energy sources. Morgan began securing funds for the film in 2004 with help from Community Services, Inc. and began filming in the fall of the same year.\n\n\n"}
{"id": "43431512", "url": "https://en.wikipedia.org/wiki?curid=43431512", "title": "Trussed Concrete Steel Company", "text": "Trussed Concrete Steel Company\n\nThe Trussed Concrete Steel Company was a company founded by Julius Kahn, an engineer and inventor. The company manufactured prefabricated products for reinforced concrete beams and steel forms for building reinforced concrete floors and walls. Kahn invented and patented a unique new technology reinforcement system of construction called the Kahn System that was stronger, more economical, and lighter than the existing old school technology used up to that point to construct buildings. The old method was to use plain straight smooth steel beams or loose rods or stirrups in concrete beams and floors. Kahn's new technology improved system used 45 degree tab flanges or \"wings\" permanently attached on steel beams that distributed the tension stress for overall improvement in strength of reinforced concrete.\n\nKahn founded in 1903 the Trussed Concrete Steel Company in a small Detroit building with a dozen employees for manufacturing the specially designed steel products for reinforcement in concrete beams and walls. Kahn became its first president. The company had its headquarters in Detroit, Michigan. The main manufacturing factory for the steel products was located in Youngstown, Ohio, after being in Detroit at first from 1903–1906. In 1906 a one-acre industrial site was developed in Youngstown at a cost of a million dollars. The new steel factory officially opened its doors for business in May 1907 with 100 employees. The Youngstown factory was developed because of its easy access to raw materials needed for steel production. The headquarters for Trussed Concrete Steel Company was located in Detroit at the northeast corner of Lafayette Boulevard and Wayne Street, in a new eight-story skyscraper built by Albert Kahn Associates in 1907. The building was the first office building in Detroit built from concrete and was later known as the Owen Building. The Packard automobile factory plant building number 10, originally designed in 1903–05, was the first time an automobile factory was constructed in the United States using reinforced concrete.\n< = >\nThe full company name of \"Trussed Concrete Steel Company\" was difficult for the public so it was shortened to \"Truscon\". However, it did not officially change its name to Truscon Steel Company until 1918. Truscon produced their steel building materials including high ribbed steel forms for floors, ceilings, and walls. Kahn's new technology became a standard throughout the building industry. The company products had been used in over 15,000 structures worldwide by 1914. The factory developed derivatives from the initial Kahn Bar parent that the company was based on and manufactured these new steel reinforcement products from 1907 through 1914 for concrete construction of industrial buildings. The company expanded their product lines again in 1915 to include prefabricated buildings in kit form ready for assembly on site. It made products under the brand names of Hy-Rib, Rib Lath, Kahn Bars, Rib Bars, Rib Metal, and United Steel Sash.\n\nKahn's system of reinforced concrete beams allowed for long span open floor room space, larger than could be provided using wood construction. This allowed solid-slab construction where shearing stresses could be maintained better. Large walls with few supports meant generous-sized windows could be installed on exterior walls to provide better natural sunlight and ventilation. This led to the design in factories known as the \"Kahn Daylight System.\" The company had a wide selection of steel products for reinforcement and prefabricated them to customer needs and specifications. Some products were collapsible column reinforcements, steel joists, and standardized steel forms. The company had several divisions. One division was Truscon Laboratories, based in Detroit. A 1924 maintenance book from Truscon gave instructions on how to maintain a large industrial building and illustrated their \"graduate engineers\" as white-coated scientists.\n\nKahn, with his brother Albert, capitalizing on the compatible relationship between steel and concrete, exported their prefabricated products to European and Asian markets. Their products became the dominant prefabricated reinforcements exported to foreign markets between World War I and World War II, competing against French and Chinese products. The company had agent representatives in Central America, South America, Europe and Asia. The earliest British examples of the use of the Kahn system of reinforced concrete to build factories was in 1913 for the Arrol-Johnson Motor Company and the Albion Motor Car Company. The company developed a manufacturing factory about 1915 in Japan to manufacture its products for the Asian market. It eventually sold its interest to Mitsui Company in 1933.\n\nThe company capitalized on its proprietary system for 20 years from 1903 and had much success in getting most of the market share worldwide. The Kahn Bar patent expired and others then developed non-proprietary systems of reinforced concrete. By 1930 the Kahn System of reinforced concrete became obsolete compared to more modern techniques of reinforcement that were more economical to manufacture. In 1937 the company was bought out by Republic Steel and Julius Kahn was vice-president of this new entity.\n\nIt was discovered after the 1906 San Francisco earthquake most buildings were destroyed either by the earthquake or the aftermath fire, except those built with the Kahn System by the Trussed Concrete Steel Company. The engineering services of the company was specified in many cases upon rebuilding commercial buildings in San Francisco. The company received additional unintentional beneficial publicity when earthquakes occurred in Calabria in 1905, Messina in 1907, Jamaica in 1907, and the 1911 Philippines Mount Taal volcano eruption.\n\nAccording to a 1910 specification booklet published by Trussed Concrete Steel Company, the following were some products the company manufactured and sold.\n\n\n"}
{"id": "2404676", "url": "https://en.wikipedia.org/wiki?curid=2404676", "title": "Voltage reference", "text": "Voltage reference\n\nA voltage reference is an electronic device that ideally produces a fixed (constant) voltage irrespective of the loading on the device, power supply variations, temperature changes, and the passage of time. Voltage references are used in power supplies, analog-to-digital converters, digital-to-analog converters, and other measurement and control systems. Voltage references vary widely in performance; a regulator for a computer power supply may only hold its value to within a few percent of the nominal value, whereas laboratory voltage standards have precisions and stability measured in parts per million.\n\nThe earliest voltage references or standards were wet-chemical cells such as the Clark cell and Weston cell, which are still used in some laboratory and calibration applications.\n\nLaboratory-grade Zener diode secondary solid-state voltage standards used in metrology can be constructed with a drift of about 1 part per million per year.\n\nThe value of the \"conventional\" volt is now maintained by superconductive integrated circuits using the Josephson Effect to get a voltage to an accuracy of 1 parts per billion or better, the Josephson voltage standard. The paper titled, \"Possible new effects in superconductive tunnelling\", was published by Brian David Josephson in 1962 and earned Josephson the Nobel Prize in Physics in 1973.\n\nFormerly, mercury batteries were much used as convenient voltage references especially in portable instruments such as photographic light meters; mercury batteries had a very stable discharge voltage over their useful life.\n\nAny semiconductor diode has an exponential voltage /current characteristic that gives an effective \"knee\" voltage sometimes used as a voltage reference. This voltage ranges from 0.3 V for germanium diodes up to about 3 volts for certain light emitting diodes. These devices have a strong temperature dependence, which may make them useful for temperature measurement or for compensating bias in analog circuits.\n\nZener diodes are also frequently used to provide a reference voltage of moderate stability and accuracy, useful for many electronic devices. An avalanche diode displays a similar stable voltage over a range of current. The most stable diodes of this type are made by temperature-compensating a Zener diode by placing it in series with a forward diode; such diodes are made as two-terminal devices, e.g. the 1N821 series having an overall voltage drop of 6.2 V at 7.5 mA, but are also sometimes included in integrated circuits.\n\nThe most common voltage reference circuit used in integrated circuits is the bandgap voltage reference. A bandgap-based reference (commonly just called a 'bandgap') uses analog circuits to add a multiple of the voltage difference between two bipolar junctions biased at different current densities to the voltage developed across a diode. The diode voltage has a negative temperature coefficient (i.e. it decreases with increasing temperature), and the junction voltage difference has a positive temperature coefficient. When added in the proportion required to make these coefficients cancel out, the resultant constant value is a voltage equal to the bandgap voltage of the semiconductor. In silicon, this is approximately 1.25 V. Buried-Zener references can provide even lower noise levels, but require higher operating voltages which are not available in many battery-operated devices.\n\nGas filled tubes and neon lamps have also been used as voltage references, primarily in tube-based equipment, as the voltage needed to sustain the gas discharge is comparatively constant. For example, the popular RCA 991 \"Voltage regulator tube\" is an NE-16 neon lamp which fires at 87 volts and then holds 48–67 volts across the discharge path.\n\n\n"}
{"id": "35921299", "url": "https://en.wikipedia.org/wiki?curid=35921299", "title": "World Nuclear Transport Institute", "text": "World Nuclear Transport Institute\n\nThe World Nuclear Transport Institute (WNTI) is an international organisation that represents the collective interests of the nuclear power and packaging industries and those who rely on it for the safe, efficient and reliable transport of radioactive materials. Through the WNTI, companies are working together to promote a sound international framework for the future by helping to build international consensus, co-operation and understanding. The Institute was founded in 1998. The WNTI is a private, non-profit organisation funded by membership subscriptions. Member companies are drawn from a wide range of industry sectors including major utilities, fuel producers and fabricators, transport companies, package designers, package producers and mines. Headquartered in London, the WNTI Secretariat has a small staff of qualified professionals working closely with members and other international bodies involved in the transport of radioactive materials.\n\nThe Board of Directors currently comprises six directors and meets biannually. Headquartered in London, the Institute is managed by the Secretary General. The Secretary General chairs an Advisory Committee which reports to the Board of Directors. The WNTI operates successfully as a network organisation, with regional offices in Tokyo and Washington, D.C.\n\n\nThe WNTI provides:\n\nIntergovernmental organisations such as the International Atomic Energy Agency (IAEA) and the International Maritime Organization (IMO) play a pivotal role in establishing standards and regulations that apply to radioactive materials transport and it is important that industry views are represented. Through its non-governmental status, the WNTI supports the work of the key intergovernmental organisations in promoting an efficient, harmonised international transport safety regime.\n\nExchanges within intergovernmental organisations, with competent authorities and collaboration with related industry organisations such as FORATOM, the Nuclear Energy Institute (NEI), the World Nuclear Association (WNA), the World Institute for Nuclear Security (WINS) and the International Organization for Standardization (ISO) are essential and remain a priority for the WNTI.\n\nThe WNTI produces technical and factual information to support a background for balanced policies and regulations. Scientific and other academic papers are published regularly and presented to key officials including regulators. The WNTI public website provides information on nuclear transport including the nuclear fuel cycle, non-fuel cycle transport, regulations, packages and also includes an image library.\n\n"}
