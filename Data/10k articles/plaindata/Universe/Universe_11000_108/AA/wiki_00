{"id": "28299503", "url": "https://en.wikipedia.org/wiki?curid=28299503", "title": "Assam State Electricity Board", "text": "Assam State Electricity Board\n\nThe Assam State Electricity Board (ASEB) is an electricity regulation board of the state of Assam in India. It is a Public Sector Undertaking managing the generation, transmission and distribution of electricity in the state of Assam with its successor companies Assam Power Generation Co. Ltd., Assam Electricity Grid Corporation Ltd. and Assam Power Distribution Company Ltd.\n\nAssam State Electricity Board was established in 1958 in the composite state of Assam under the Electricity Act 1948. The existing Board was reconstituted in 1975 after the state was trifurcated into Assam, Meghalaya and Mizoram in 1972. Two central government corporations, North Eastern Electric Power Corporation (NEEPCO) and the Power Grid Corporation of India Ltd (PGCIL) supplement the efforts of the state in power development in generation and transmission respectively.\n\n"}
{"id": "2165489", "url": "https://en.wikipedia.org/wiki?curid=2165489", "title": "Buchholz relay", "text": "Buchholz relay\n\nIn the field of electric power distribution and transmission, a Buchholz relay is a safety device mounted on some oil-filled power transformers and reactors, equipped with an external overhead oil reservoir called a \"conservator\". The Buchholz relay is used as a protective device sensitive to the effects of dielectric failure inside the equipment. A generic designation for this type of device is \"gas detector relay\". \n\nBuchholz relays have been applied on oil-filled power and distribution transformers at least since the 1940s. The relay is connected to the oil piping between the conservator and oil tank of a transformer. The piping between the main tank and conservator is arranged so that any gas evolved in the main tank tends to flow upward toward the conservator and gas detector relay. \nDepending on the model, the relay has multiple methods to detect a failing transformer. On a slow accumulation of gas, due perhaps to slight overload, gas produced by decomposition of insulating oil accumulates in the top of the relay and forces the oil level down. A float switch in the relay is used to initiate an alarm signal. Depending on design, a second float may also serve to detect slow oil leaks.\n\nIf an electrical arc forms, gas accumulation is rapid, and oil flows rapidly into the conservator. This flow of oil operates a switch attached to a vane located in the path of the moving oil. This switch normally will operate a circuit breaker to isolate the apparatus before the fault causes additional damage. Buchholz relays have a test port to allow the accumulated gas to be withdrawn for testing. Flammable gas found in the relay indicates some internal fault such as overheating or arcing, whereas air found in the relay may only indicate low oil level or a leak. \n\nThrough a connected gas sampling device the control can also be made from the ground. Depending on the requirements, the Buchholz relay has a flange or threaded connection. The classic Buchholz relay has to comply with the requirements of the DIN EN 50216-2 standard. Depending on the requirements, it is equipped with up to four (2 per float) switches or change-over switches, which can either send a light signal or switch off the transformer.\n\nThe relay was first developed by Max Buchholz (1875–1956) in 1921. Names like \"beechwood relay\" or \"beech relay\" are an indication of incorrectly translated German language manuals of the \"Buchholz relay \"\n\n"}
{"id": "3059400", "url": "https://en.wikipedia.org/wiki?curid=3059400", "title": "CR-39", "text": "CR-39\n\nCR-39, or allyl diglycol carbonate (ADC), is a plastic polymer commonly used in the manufacture of eyeglass lenses. The abbreviation stands for \"Columbia Resin #39\", which was the 39th formula of a thermosetting plastic developed by the Columbia Resins project in 1940.\n\nThe first commercial use of CR-39 monomer was to help create glass-reinforced plastic fuel tanks for the B-17 bomber aircraft in World War II, reducing weight and increasing range of the bomber. After the War, the Armorlite Lens Company in California is credited with manufacturing the first CR-39 eyeglass lenses in 1947. CR-39 plastic has an index of refraction of 1.498 and an Abbe number of 58. CR-39 is now a trade-marked product of PPG Industries.\n\nAn alternative use includes a purified version that is used to measure neutron radiation, a type of ionizing radiation, in neutron dosimetry.\n\nAlthough CR-39 is a type of polycarbonate, it should not be confused with the general term polycarbonate, a tough homopolymer usually made from bisphenol A.\n\nCR-39 is made by polymerization of diethyleneglycol bis allylcarbonate (ADC) in presence of diisopropyl peroxydicarbonate (IPP) initiator. The presence of the allyl groups allows the polymer to form cross-links; thus, it is a thermoset resin. The monomer structure is\n\nThe polymerization schedule of ADC monomers using IPP is generally 20 hours long with a maximum temperature of 95 °C. The elevated temperatures can be supplied using a water bath or a forced air oven.\n\nBenzoyl peroxide (BPO) is an alternative organic peroxide that may be used to polymerize ADC. Pure benzoyl peroxide is crystalline and less volatile than diisopropyl peroxydicarbonate. Using BPO results in a polymer that has a higher yellowness index, and the peroxide takes longer to dissolve into ADC at room temperature than IPP.\n\n\"CR-39 is transparent in the visible spectrum and is almost completely opaque in the ultraviolet range. It has high abrasion resistance, in fact the highest abrasion/scratch resistance of any uncoated optical plastic. CR-39 is about half the weight of glass with an index of refraction only slightly lower than that of crown glass, and its high Abbe number yields low chromatic aberration, altogether making it an advantageous material for eyeglasses and sunglasses. A wide range of colors can be achieved by dyeing of the surface or the bulk of the material. CR-39 is also resistant to most solvents and other chemicals, gamma radiation, aging, and to material fatigue. It can withstand the small hot sparks from welding, something glass cannot do. It can be used continuously in temperatures up to 100 °C and up to one hour at 130 °C.\" \n\nIn the radiation detection application, CR-39 is used as a Solid-State Nuclear Track Detector to detect the presence of ionising radiation. Energetic particles colliding with the polymer structure leave a trail of broken chemical bonds within the CR-39. When immersed in a concentrated alkali solution (typically sodium hydroxide) hydroxide ions attack and break the polymer structure, etching away the bulk of the plastic at a nominally fixed rate. However, along the paths of damage left by charged particle interaction the concentration of radiation damage allows the chemical agent to attack the polymer more rapidly than it does in the bulk, revealing the paths of the charged particle ion tracks. The resulting etched plastic therefore contains a permanent record of not only the location of the radiation on the plastic but also gives spectroscopic information about the source. Principally used for the detection of alpha radiation emitting radionuclides (especially radon gas), the radiation-sensitivity properties of CR39 are also used for proton and neutron dosimetry and historically cosmic ray investigations.\n\nThe ability of CR-39 to record the location of a radiation source, even at extremely low concentrations is exploited in autoradiography studies with alpha particles, and for (comparatively cheap) detection of alpha-emitters like uranium. Typically, a thin section of a biological material is fixed against CR-39 and kept frozen for a timescale of months to years in an environment that is shielded as much as possible from possible radiological contaminants. Before etching, photographs are taken of the biological sample with the affixed CR-39 detector, with care taken to ensure that pre-scribed location marks on the detector are noted. After the etching process, automated or manual 'scanning' of the CR-39 is used to physically locate the ionising radiation recorded, which can then be mapped to the position of the radionuclide within the biological sample. There is no other non-destructive method for accurately identifying the location of trace quantities of radionuclides in biological samples at such low emission levels.\n\nCR-39 is used in some photographic filters, such as the Cokin filter system.\n\nA direct equivalent is produced by Acomon AG with the brand name RAV,\nand another one by Danyang Yueda FineChemichal Co. Ltd in China. A highly purified CR-39, manufactured under the name of TASTRAK, is available specifically for radio-dosimetry.\n\n\n"}
{"id": "11206405", "url": "https://en.wikipedia.org/wiki?curid=11206405", "title": "Clive Mather", "text": "Clive Mather\n\nClive Mather is the Chairman of the Board of Tearfund USA, the latest member of the global Tearfund family. \nIn September 2018 he retired as Chairman of Tearfund after serving his full term of 10 years. During this time he visited Tearfund's work in 47 countries, encouraging partners and local churches. Tearfund is a Christian relief and development agency \"following Jesus where the need is greatest\". It is committed to eradicating poverty through 3 primary channels. Firstly through its disaster relief capacity, it is able to bring front line emergency help to those communities ravaged by war, earthquake or other calamities. Secondly working through it worldwide partners and a global network of local churches, it is able to transform families and communities by empowering the local church to mobilise resources that are available and inspire collective action to tackle poverty in all its forms. And thirdly through its advocacy, Tearfund highlights the impact of climate change, injustice and corruption on those least able to cope. It calls governments and global institutions to account, arguing for concerted global action and a fairer allocation of resources to meet the needs of the poor and oppressed. \n\nHe serves as a Director of Emissions Reduction Alberta (ERA) which works to develop and commercialise technologies which will reduce carbon and other GHG emissions in Canada and beyond.\n\nHe is the President, Relational Peacebuilding Initiatives (RPI) based in Geneva, working to bring relational solutions to long standing conflict situations in various parts of the world. The current main focus is the Korean Peninsula. After 70 years of bitter division, North and South Korea have now begun political talks aimed at replacing recrimination with co-operation. What is needed is a parallel process which will bring economic growth and social transformation to the whole Peninsula. RPI is already working behind the scenes to develop a partnership model that will enable North and South to agree together the values, goals and priorities in each of the main sectors of society. This is vital if economic progress is to be demonstrated in the short-term and the possibility of peaceful unification nurtured for the future.\n\nIn 2008 he was appointed Chairman of the Shell Pensions Trust Ltd, one of the biggest defined benefit pension funds in the UK. The Trust oversees the investment and management of the Shell Contributory Pension Fund, providing benefits for 40,000+ active and retired members of Royal Dutch Shell in the UK. He was also appointed Chairman of Shell's new UK defined contribution scheme, the UK Shell Pension Plan Trust Limited which started in 2013. He stepped down from both roles on 31st May 2018 after 10 years' service to the Funds.\n\nIn 2012, he was appointed Chairman of the Garden Tomb (Jerusalem) Association and stepped down in 2017 after serving his full term of 9 years as a Trustee. He is the former Chairman of Iogen Corporation, a Canadian bio-technology company specialising in enzyme technology and based in Ottawa. Iogen is the leading producer of cellulosic ethanol. He served as a Director for 10 years of which 9 were as Chairman. He retired in July 2017.\n\nMather is a former employee of Royal Dutch Shell.He joined Shell in the United Kingdom 1969 and retired in 2007. His career included assignments in Brunei, Gabon, South Africa, the Netherlands, Canada and the UK where he was Chairman of Shell UK Limited between 2002 and 2004. His last position was as President and CEO of Shell Canada Ltd based in Calgary, prior to its take over by Royal Dutch Shell Limited.\n\nMather was formerly Chairman of the UK Government/Industry CSR Academy, Deputy Chairman of the Windsor Leadership Trust and Chairman of the IMD Business Advisory Council in Switzerland. He has held a number of public appointments in the UK including Commissioner for the Equal Opportunities Commission, Chairman of the Petroleum Employer’s Council and Chairman of the Lambeth Education Action Zone. In July 2009 he was appointed as one of the 12 members of the Premier's Council for Economic Strategy in Alberta. He has also served on the Royal Anniversary Trust for some 7 years. \n\nMather was educated at Warwick School and Lincoln College, Oxford. He is a Companion of the Chartered Management Institute and Fellow of the Chartered Institute of Personnel and Development. He speaks on issues of leadership, ethics, the environment and energy internationally.\nIn November 2013 Mather gave a lecture in the Palace of Westminster to the Christian Association of Business Executives on the occasion of its 75th birthday. It was entitled \"Time travel - has big business lost its compass?\" For the full text see http://www.cabe-online.org/\n\nIn October 2004 Mather gave a speech to the Evangelical Alliance in London entitled “Trust: Whose business is it” during which he said “Meticulous compliance with all the new regulations is a must, but will not be enough. We have to try harder; we have to set the highest standards in everything we do, because trust is essential to our business.”\n\nMather has been involved with the Relationships Foundation to whom he has said \"Concern for relationships is a vital part of sustainable development – whether economic, environmental or social - and must feature in political debate and decision making\".\n"}
{"id": "219144", "url": "https://en.wikipedia.org/wiki?curid=219144", "title": "Compressible flow", "text": "Compressible flow\n\nCompressible flow (gas dynamics) is the branch of fluid mechanics that deals with flows having significant changes in fluid density. Gases, mostly, display such behaviour. While all flows are compressible, flows are usually treated as being incompressible when the Mach number (the ratio of the speed of the flow to the speed of sound) is less than 0.3 (since the density change due to velocity is about 5% in that case). The study of compressible flow is relevant to high-speed aircraft, jet engines, rocket motors, high-speed entry into a planetary atmosphere, gas pipelines, commercial applications such as abrasive blasting, and many other fields.\n\nThe study of gas dynamics is often associated with the flight of modern high-speed aircraft and atmospheric reentry of space-exploration vehicles; however, its origins lie with simpler machines. At the beginning of the 19th century, investigation into the behaviour of fired bullets led to improvement in the accuracy and capabilities of guns and artillery. As the century progressed, inventors such as Gustaf de Laval advanced the field, while researchers such as Ernst Mach sought to understand the physical phenomenon involved through experimentation.\n\nAt the beginning of the 20th century, the focus of gas dynamics research shifted to what would eventually become the aerospace industry. Ludwig Prandtl and his students proposed important concepts ranging from the boundary layer to supersonic shock waves, supersonic wind tunnels, and supersonic nozzle design. Theodore von Kármán, a student of Prandtl, continued to improve the understanding of supersonic flow. Other notable figures (Meyer, Luigi Crocco, and Shapiro) also contributed significantly to the principles considered fundamental to the study of modern gas dynamics. Many others also contributed to this field.\n\nAccompanying the improved conceptual understanding of gas dynamics in the early 20th century was a public misconception that there existed a barrier to the attainable speed of aircraft, commonly referred to as the \"sound barrier.\" In truth, the barrier to supersonic flight was merely a technological one, although it was a stubborn barrier to overcome. Amongst other factors, conventional aerofoils saw a dramatic increase in drag coefficient when the flow approached the speed of sound. Overcoming the larger drag proved difficult with contemporary designs, thus the perception of a sound barrier. However, aircraft design progressed sufficiently to produce the Bell X-1. Piloted by Chuck Yeager, the X-1 officially achieved supersonic speed in October 1947. \n\nHistorically, two parallel paths of research have been followed in order to further gas dynamics knowledge. Experimental gas dynamics undertakes wind tunnel model experiments and experiments in shock tubes and ballistic ranges with the use of optical techniques to document the findings. Theoretical gas dynamics considers the equations of motion applied to a variable-density gas, and their solutions. Much of basic gas dynamics is analytical, but in the modern era Computational fluid dynamics applies computing power to solve the otherwise-intractable nonlinear partial differential equations of compressible flow for specific geometries and flow characteristics.\n\nThere are several important assumptions involved in the underlying theory of compressible flow. All fluids are composed of molecules, but tracking a huge number of individual molecules in a flow (for example at atmospheric pressure) is unnecessary. Instead, the continuum assumption allows us to consider a flowing gas as a continuous substance except at low densities. This assumption provides a huge simplification which is accurate for most gas-dynamic problems. Only in the low-density realm of rarefied gas dynamics does the motion of individual molecules become important.\n\nA related assumption is the no-slip condition where the flow velocity at a solid surface is presumed equal to the velocity of the surface itself, which is a direct consequence of assuming continuum flow. The no-slip condition implies that the flow is viscous, and as a result a boundary layer forms on bodies traveling through the air at high speeds, much as it does in low-speed flow.\n\nMost problems in incompressible flow involve only two unknowns: pressure and velocity, which are typically found by solving the two equations that describe conservation of mass and of linear momentum, with the fluid density presumed constant. In compressible flow, however, the gas density and temperature also become variables. This requires two more equations in order to solve compressible-flow problems: an equation of state for the gas and a conservation of energy equation. For the majority of gas-dynamic problems, the simple Ideal gas law is the appropriate state equation.\n\nFluid dynamics problems have two overall types of references frames, called Lagrangian and Eulerian (see Joseph-Louis Lagrange and Leonhard Euler). The Lagrangian approach follows a fluid mass of fixed identity as it moves through a flowfield. The Eulerian reference frame, in contrast, does not move with the fluid. Rather it is a fixed frame or control volume that fluid flows through. The Eulerian frame is most useful in a majority of compressible flow problems, but requires that the equations of motion be written in a compatible format.\n\nFinally, although space is known to have 3 dimensions, an important simplification can be had in describing gas dynamics mathematically if only one spatial dimension is of primary importance, hence 1-dimensional flow is assumed. This works well in duct, nozzle, and diffuser flows where the flow properties change mainly in the flow direction rather than perpendicular to the flow. However, an important class of compressible flows, including the external flow over bodies traveling at high speed, requires at least a 2-dimensional treatment. When all 3 spatial dimensions and perhaps the time dimension as well are important, we often resort to computerized solutions of the governing equations.\n\nThe Mach number (M) is defined as the ratio of the speed of an object (or of a flow) to the speed of sound. For instance, in air at room temperature, the speed of sound is about . M can range from 0 to ∞, but this broad range falls naturally into several flow regimes. These regimes are subsonic, transonic, supersonic, hypersonic, and hypervelocity flow. The figure below illustrates the Mach number \"spectrum\" of these flow regimes.\nThese flow regimes are not chosen arbitrarily, but rather arise naturally from the strong mathematical background that underlies compressible flow (see the cited reference textbooks). At very slow flow speeds the speed of sound is so much faster that it is mathematically ignored, and the Mach number is irrelevant. Once the speed of the flow approaches the speed of sound, however, the Mach number becomes all-important, and shock waves begin to appear. Thus the transonic regime is described by a different (and much more difficult) mathematical treatment. In the supersonic regime the flow is dominated by wave motion at oblique angles similar to the Mach angle. Above about Mach 5, these wave angles grow so small that a different mathematical approach is required, defining the Hypersonic speed regime. Finally, at speeds comparable to that of planetary atmospheric entry from orbit, in the range of several km/s, the speed of sound is now comparatively so slow that it is once again mathematically ignored in the Hypervelocity regime.\n\nAs an object accelerates from subsonic toward supersonic speed in a gas, different types of wave phenomena occur. To illustrate these changes, the next figure shows a stationary point (M = 0) that emits symmetric sound waves. The speed of sound is the same in all directions in a uniform fluid, so these waves are simply concentric spheres. As the sound-generating point begins to accelerate, the sound waves \"bunch up\" in the direction of motion and \"stretch out\" in the opposite direction. When the point reaches sonic speed (M = 1), it travels at the same speed as the sound waves it creates. Therefore, an infinite number of these sound waves \"pile up\" ahead of the point, forming a Shock wave. Upon achieving supersonic flow, the particle is moving so fast that it continuously leaves its sound waves behind. When this occurs, the locus of these waves trailing behind the point creates an angle known as the Mach wave angle or Mach angle, µ:\n\nwhere formula_2 represents the speed of sound in the gas and formula_3 represents the velocity of the object. Although named for Austrian physicist Ernst Mach, these oblique waves were actually first discovered by Christian Doppler.\nOne-dimensional (1-D) flow refers to flow of gas through a duct or channel in which the flow parameters are assumed to change significantly along only one spatial dimension, namely, the duct length. In analysing the 1-D channel flow, a number of assumptions are made: \n\nAs the speed of a flow accelerates from the subsonic to the supersonic regime, the physics of nozzle and diffuser flows is altered. Using the conservation laws of fluid dynamics and thermodynamics, the following relationship for channel flow is developed (combined mass and momentum conservation):\n\nwhere dP is the differential change in pressure, M is the Mach number, ρ is the density of the gas, V is the velocity of the flow, A is the area of the duct, and dA is the change in area of the duct. This equation states that, for subsonic flow, a converging duct (dA<0) increases the velocity of the flow and a diverging duct (dA>0) decreases velocity of the flow. For supersonic flow, the opposite occurs due to the change of sign of (1-M). A converging duct (dA<0) now decreases the velocity of the flow and a diverging duct (dA>0) increases the velocity of the flow. At Mach = 1, a special case occurs in which the duct area must be either a maximum or minimum. For practical purposes, only a minimum area can accelerate flows to Mach 1 and beyond. See table of sub-supersonic diffusers and nozzles.\nTherefore, to accelerate a flow to Mach 1, a nozzle must be designed to converge to a minimum cross-sectional area and then expand. This type of nozzle – the converging-diverging nozzle – is called a de Laval nozzle after Gustaf de Laval, who invented it. As subsonic flow enters the converging duct and the area decreases, the flow accelerates. Upon reaching the minimum area of the duct, also known as the throat of the nozzle, the flow can reach Mach 1. If the speed of the flow is to continue to increase, its density must decrease in order to obey conservation of mass. To achieve this decrease in density, the flow must expand, and to do so, the flow must pass through a diverging duct. See image of de Laval Nozzle.\nUltimately, because of the energy conservation law, a gas is limited to a certain maximum velocity based on its energy content. The maximum velocity, V, that a gas can attain is:\n\nwhere c is the specific heat of the gas and T is the stagnation temperature of the flow.\n\nUsing conservations laws and thermodynamics, a number of relationships of the form\n\ncan be obtained, where M is the Mach number and γ is the ratio of specific heats (1.4 for air). See table of isentropic flow Mach number relationships. \nAs previously mentioned, in order for a flow to become supersonic, it must pass through a duct with a minimum area, or sonic throat. Additionally, an overall pressure ratio, P/P, of approximately 2 is needed to attain Mach 1. Once it has reached Mach 1, the flow at the throat is said to be \"choked.\" Because changes downstream can only move upstream at sonic speed, the mass flow through the nozzle cannot be affected by changes in downstream conditions after the flow is choked.\n\nNormal shock waves are shock waves that are perpendicular to the local flow direction. These shock waves occur when pressure waves build up and coalesce into an extremely thin shockwave that converts useful energy into heat. The waves thus overtake and reinforce one another, forming a finite shock wave from an infinite series of infinitesimal sound waves. Because a loss of energy occurs over the thin shock wave, the shock is considered non-isentropic and enthalpy increases across the shock. When analysing a normal shock wave, one-dimensional, steady, and adiabatic (stagnation temperature does not change across the shock wave) flow of a perfect gas is assumed.\n\nNormal shock waves can occur in two reference frames: the standing normal shock and the moving shock. The flow before a normal shock wave must be supersonic, and the flow after a normal shock must be subsonic. The Rankine-Hugoniot equations are used to solve for the flow conditions..\n\nAlthough one-dimensional flow can be directly analysed, it is merely a specialized case of two-dimensional flow. It follows that one of the defining phenomena of one-dimensional flow, a normal shock, is likewise only a special case of a larger class of oblique shocks. Further, the name \"normal\" is with respect to geometry rather than frequency of occurrence. Oblique shocks are much more common in applications such as: aircraft inlet design, objects in supersonic flight, and (at a more fundamental level) supersonic nozzles and diffusers. Depending on the flow conditions, an oblique shock can either be attached to the flow or detached from the flow in the form of a bow shock.\n\nOblique shock waves are similar to normal shock waves, but they occur at angles less than 90° with the direction of flow. When a disturbance is introduced to the flow at a nonzero angle (δ), the flow must respond to the changing boundary conditions. Thus an oblique shock is formed, resulting in a change in the direction of the flow.\n\nBased on the level of flow deflection (δ), oblique shocks are characterized as either strong or weak. Strong shocks are characterized by larger deflection and more entropy loss across the shock, with weak shocks as the opposite. In order to gain cursory insight into the differences in these shocks, a shock polar diagram can be used. With the static temperature after the shock, T*, known the speed of sound after the shock is defined as,\n\nwith R as the gas constant and γ as the specific heat ratio. The Mach number can be broken into Cartesian coordinates\n\nwith V and V as the x and y-components of the fluid velocity V. With the Mach number before the shock given, a locus of conditions can be specified. At some δ the flow transitions from a strong to weak oblique shock. With δ = 0°, a normal shock is produced at the limit of the strong oblique shock and the Mach wave is produced at the limit of the weak shock wave.\n\nDue to the inclination of the shock, after an oblique shock is created, it can interact with a boundary in three different manners, two which are explained below.\n\nIncoming flow is first turned by angle δ with respect to the flow. This shockwave is reflected off the solid boundary, and the flow is turned by – δ to again be parallel with the boundary. It is important to note that each progressive shock wave is weaker and the wave angle is increased.\n\nAn irregular reflection is much like the case described above, with the caveat that δ is larger than the maximum allowable turning angle. Thus a detached shock is formed and a more complicated reflection occurs.\n\nPrandtl-Meyer fans can be expressed as both compression and expansion fans. Prandtl-Meyer fans also cross a boundary layer (i.e. flowing and solid) which reacts in different changes as well. When a shock wave hits a solid surface the resulting fan returns as one from the opposite family while when one hits a free boundary the fan returns as a fan of opposite type.\n\nTo this point, the only flow phenomena that have been discussed are shock waves, which slow the flow and increase its entropy. It is possible to accelerate supersonic flow in what has been termed a Prandtl-Meyer expansion fan, after Ludwig Prandtl and Theodore Meyer. The mechanism for the expansion is shown in the figure below.\n\nAs opposed to the flow encountering an inclined obstruction and forming an oblique shock, the flow expands around a convex corner and forms an expansion fan through a series of isentropic Mach waves. The expansion \"fan\" is composed of Mach waves that span from the initial Mach angle to the final Mach angle. Flow can expand around either a sharp or rounded corner equally, as the increase in Mach number is proportional to only the convex angle of the passage (δ). The expansion corner that produces the Prandtl-Meyer fan can be sharp (as illustrated in the figure) or rounded. If the total turning angle is the same, then the P-M flow solution is also the same.\n\nThe Prandtl-Meyer expansion can be seen as the physical explanation of the operation of the Laval nozzle. The contour of the nozzle creates a smooth and continuous series of Prandtl-Meyer expansion waves.\n\nA Prandtl-Meyer compression is the opposite phenomenon to a Prandtl-Meyer expansion. If the flow is gradually turned through an angle of δ, a compression fan can be formed. This fan is a series of Mach waves that eventually coalesce into an oblique shock. Because the flow is defined by an isentropic region (flow that travels through the fan) and an anisentropic region (flow that travels through the oblique shock), a slip line results between the two flow regions.\n\nSupersonic wind tunnels are used for testing and research in supersonic flows, approximately over the Mach number range of 1.2 to 5. The operating principle behind the wind tunnel is that a large pressure difference is maintained upstream to downstream, driving the flow.\n\nWind tunnels can be divided into two categories: continuous-operating and intermittent-operating wind tunnels. Continuous operating supersonic wind tunnels require an independent electrical power source that drastically increases with the size of the test section. Intermittent supersonic wind tunnels are less expensive in that they store electrical energy over an extended period of time, then discharge the energy over a series of brief tests. The difference between these two is analogous to the comparison between a battery and a capacitor.\n\nBlowdown type supersonic wind tunnels offer high Reynolds number, a small storage tank, and readily available dry air. However, they cause a high pressure hazard, result in difficulty holding a constant stagnation pressure, and are noisy during operation.\n\nIndraft supersonic wind tunnels are not associated with a pressure hazard, allow a constant stagnation pressure, and are relatively quiet. Unfortunately, they have a limited range for the Reynolds number of the flow and require a large vacuum tank.\nThere is no dispute that knowledge is gained through research and testing in supersonic wind tunnels; however, the facilities often require vast amounts of power to maintain the large pressure ratios needed for testing conditions. For example, Arnold Engineering Development Complex has the largest supersonic wind tunnel in the world and requires the power required to light a small city for operation. For this reason, large wind tunnels are becoming less common at universities.\n\nPerhaps the most common application for oblique shocks is in high-speed aircraft inlets. The purpose of the inlet is to slow incoming supersonic flow to the subsonic regime before it enters the turbojet engine, with the caveat of minimizing losses across the shock. Knowledge of normal and oblique shocks suggests that this be accomplished with a series of weakening oblique shocks followed by a very weak normal shock, usually less than M = 1.4. \nThis may sound relatively straightforward, but there is one rather large issue to be dealt with when designing a supersonic aircraft inlet: acceleration. Between taking off, manoeuvring, and cruising, an aircraft travels at a range of Mach numbers. In order to ensure efficient flight, the aircraft intake must be capable of variable geometry. If it is not, the shock waves will not reflect properly through the inlet and negatively affect performance.\n\nAlthough variable geometry is a universally recognized approach to improve aircraft efficiency and performance over a range of Mach numbers, there is no one method to achieve variable geometry. The F-15 Eagle employs wedge inlets with adjustable flaps to control the flow. For subsonic flow, the flaps are completely closed and for supersonic flow, the flaps are open. The Concorde employed an external-compression inlet, using a series of oblique shocks followed by a normal shock to slow the flow sufficiently for the turbojet engine. Perhaps the most recognizable supersonic aircraft, the SR-71, used a hydraulically actuated cone to reduce the speed of the supersonic flow through the aircraft inlet.\n\n\n\n"}
{"id": "2011637", "url": "https://en.wikipedia.org/wiki?curid=2011637", "title": "Counterion", "text": "Counterion\n\nA counterion (pronounced as two words, i.e. \"counter\" \"ion\", and sometimes written as two words) is the ion that accompanies an ionic species in order to maintain electric neutrality. In table salt (NaCl), the sodium cation is the counterion for the chlorine anion and vice versa.\nCounterions are the mobile ions in ion exchange polymers and colloids. Ion exchange resins are polymers with a net negative or positive charge. Cation exchange resins consist of an anionic polymer with countercations, typically Na+. The resin has a higher affinity for highly charged countercations, e.g., by Ca in the case of water softening. Complementarily, anion exchange resins are typically provided in the form of chloride, which is a highly mobile couteranion.\n\nCounterions are used in phase-transfer catalysis. In a typical application lipophilic countercation such as benzalkonium solubilizes reagents in organic solvents.\n\nSolubility of salts in organic solvents is a function of both the cation and the anion. The solubility of cations in organic solvents can be enhanced when the anion is lipophilic. Similarly, the solubility of anions in organic solvents is enhanced with lipophilic cations. The most common lipophilic cations are quaternary ammonium cations, called \"quat salts\".\n\nMany cationic organometallic complexes are isolated with inert, noncoordinating counterions. Ferrocenium tetrafluoroborate is one such example.\n\nIn order to achieve high ionic conductivity, electrochemical measurements are conducted in the presence of excess electrolyte. In water the electrolyte is often a simple salt such as potassium chloride. For measurements in nonaqueous solutions, salts composed of both lipophilic cations and anions are employed, e.g., tetrabutylammonium hexafluorophosphate. Even in such cases potentials are influenced by ion-pairing, an effect that is accentuated in solvents of low dielectric constant.\n\nFor many applications, the counterion simply provides charge and lipophilicity that allows manipulation of its partner ion. The counterion is expected to be chemically inert. For counteranions, inertness is expressed in terms of low Lewis basicity. The counterions are ideally rugged and unreactive. For quaternary ammonium and phosphonium countercations, inertness is related to their resistance of degradation by strong bases and strong nucleophiles.\n"}
{"id": "60744", "url": "https://en.wikipedia.org/wiki?curid=60744", "title": "Cubic zirconia", "text": "Cubic zirconia\n\nCubic zirconia (CZ) is the cubic crystalline form of zirconium dioxide (ZrO). The synthesized material is hard, optically flawless and usually colorless, but may be made in a variety of different colors. It should not be confused with zircon, which is a zirconium silicate (ZrSiO). It is sometimes erroneously called \"cubic zirconium\".\n\nBecause of its low cost, durability, and close visual likeness to diamond, synthetic cubic zirconia has remained the most gemologically and economically important competitor for diamonds since commercial production began in 1976. Its main competitor as a synthetic gemstone is a more recently cultivated material, synthetic moissanite.\n\nCubic zirconia is crystallographically isometric, an important attribute of a would-be diamond simulant. During synthesis zirconium oxide would naturally form monoclinic crystals, its stable form under normal atmospheric conditions. A stabilizer is required for cubic crystals to form, and remain stable at ordinary temperatures; this may be typically either yttrium or calcium oxide, the amount of stabilizer used depending on the many recipes of individual manufacturers. Therefore, the physical and optical properties of synthesized CZ vary, all values being ranges.\n\nIt is a dense substance, with a specific gravity between 5.6 and 6.0 — at least 1.6 times that of diamond. Cubic zirconia is relatively hard, 8–8.5 on the Mohs scale— slightly harder than most semi-precious natural gems. Its refractive index is high at 2.15–2.18 (compared to 2.42 for diamonds) and its luster is adamantine. Its dispersion is very high at 0.058–0.066, exceeding that of diamond (0.044). Cubic zirconia has no cleavage and exhibits a conchoidal fracture. Because of its high hardness, it is generally considered brittle.\n\nUnder shortwave UV cubic zirconia typically fluoresces a yellow, greenish yellow or \"beige\". Under longwave UV the effect is greatly diminished, with a whitish glow sometimes being seen. Colored stones may show a strong, complex rare earth absorption spectrum.\n\nDiscovered in 1892, the yellowish monoclinic mineral baddeleyite is a natural form of zirconium oxide.\n\nThe high melting point of zirconia (2750 °C or 4976 °F) hinders controlled growth of single crystals. However, stabilization of cubic zirconium oxide had been realized early on, with the synthetic product \"stabilized zirconia\" introduced in 1929. Although cubic, it was in the form of a polycrystalline ceramic: it was used as a refractory material, highly resistant to chemical and thermal attack (up to 2540 °C or 4604 °F).\n\nIn 1937, German mineralogists M. V. Stackelberg and K. Chudoba discovered naturally occurring cubic zirconia in the form of microscopic grains included in metamict zircon. This was thought to be a byproduct of the metamictization process, but the two scientists did not think the mineral important enough to give it a formal name. The discovery was confirmed through X-ray diffraction, proving the existence of a natural counterpart to the synthetic product.\n\nAs with the majority of grown diamond substitutes, the idea of producing single-crystal cubic zirconia arose in the minds of scientists seeking a new and versatile material for use in lasers and other optical applications. Its production eventually exceeded that of earlier synthetics, such as synthetic strontium titanate, synthetic rutile, YAG (yttrium aluminium garnet) and GGG (gadolinium gallium garnet).\n\nSome of the earliest research into controlled single-crystal growth of cubic zirconia occurred in 1960s France, much work being done by Y. Roulin and R. Collongues. This technique involved molten zirconia being contained within a thin shell of still-solid zirconia, with crystal growth from the melt. The process was named \"cold crucible\", an allusion to the system of water cooling used. Though promising, these attempts yielded only small crystals.\n\nLater, Soviet scientists under V. V. Osiko at the Lebedev Physical Institute in Moscow perfected the technique, which was then named \"skull crucible\" (an allusion either to the shape of the water-cooled container or to the form of crystals sometimes grown). They named the jewel \"Fianit\" after the institute's name FIAN (Physical Institute of the Academy of Science), but the name was not used outside of the USSR. Their breakthrough was published in 1973, and commercial production began in 1976. In 1977 cubic zirconia begun to be mass-produced in the jewelry marketplace by the Ceres Corporation with crystals stabilized with 94% yttria. Other major producers include Taiwan Crystal Company Ltd, Swarovsi and ICT inc. By 1980 annual global production had reached 60 million carats (12 tonnes)and continued to increase with production reaching around 400 tpa in 1998.\n\nBecause the natural form of cubic zirconia is so rare, all cubic zirconia used in jewelry has been synthesized, or created by humans.\n\nCurrently the primary method of cubic zirconia synthesis employed by producers remains to be through the skull-melting method. This method was patented by Josep F. Wenckus and coworkers (1997). This is largely due to the process allowing for temperatures of over 3000 degrees to be achieved, lack of contact between crucible and material as well as the freedom to choose any gas atmosphere. Primary downsides to this method include the inability to predict the size of the crystals produced and it is impossible to control the crystallization process through temperature changes.\n\nThe apparatus used in this process consists of a cup-shaped crucible surrounded by radio-frequency (RF) activated copper coils and a water-cooling system.\n\nZirconium dioxide thoroughly mixed with a stabilizer (normally 10% yttrium oxide) is fed into a cold crucible. Metallic chips of either zirconium or the stabilizer are introduced into the powder mix in a compact pile manner. The RF generator is switched on and the metallic chips quickly start heating up and readily oxidize into more zirconia. Consequently, the surrounding powder heats up by thermal conduction and begins melting, which in turn becomes electroconductive and thus it begins to heat up via the RF generator as well. This continues until the entire product is molten. Due to the cooling system surrounding the crucible, a thin shell of sintered solid material is formed. This causes the molten zirconia to remain contained within its own powder which prevents it from contamination from the crucible and reduces heat loss. The melt is left at high temperatures for some hours to ensure homogeneity and ensure all impurities have evaporated. Finally, the entire crucible is slowly removed from the RF coils to reduce the heating and let it slowly cool down (from bottom to top). The rate at which the crucible is removed from the RF coils is chosen as a function of the stability of crystallization dictated by the phase transition diagram. This provokes the crystallization process to begin and useful crystals begin to form. Once the crucible has been completely cooled to room temperature, the resulting crystals are multiple elongated-crystalline blocks.\n\nThe reason behind this shape is dictated by a concept known as crystal degeneration according to Tiller. The size and diameter of the obtained crystals is a function of the cross-sectional area of the crucible, volume of the melt and composition of the melt. The diameter of the crystals is heavily influenced by the concentration of <chem>Y2O3</chem>stabilizer.\n\nWhen observing the phase diagram the cubic phase will crystallize first as the solution is cooled down no matter the concentration of <chem>Y2O3</chem>. If the concentration of <chem>Y2O3</chem> isn’t high enough the cubic structure will start to break down into the tetragonal state which will then break down into a monoclinic phase. If the concentration of <chem>Y2O3</chem> is between 2.5-5% the resulting product will be PSZ (partially stabilized zirconia) while monophasic cubic crystals will form from around 8-40%. It should be noted that below 14% at low growth rates tend to be opaque indicating partial phase separation in the solid solution (likely due to diffusion in the crystals remaining in the high temperature region for a longer time). Above this threshold crystals tend to remain clear at reasonable growth rates and maintains good annealing conditions.\n\nBecause of cubic zirconia's isomorphic capacity it can be doped with several elements to change the color of the crystal. A list of specific dopants and colors produced by their addition can be seen below.\n\nThe vast majority of YCZ (yttrium bearing cubic zirconia) crystals clear with high optical perfection and with gradients of the refractive index lower than formula_1. However some samples contain defects with the most characteristic and common ones listed below.\n\n\n\n\n\nDue to its optical properties YCZ (yttrium cubic zirconia) has been used for window, lenses, prisms, filters and laser elements. Particularly in the chemical industry it is used as window material for the monitoring of corrosive liquids due to its chemical stability and mechanical toughness. YCZ has also been used as a substrate for semiconductor and superconductor films in similar industries.\n\nMechanical properties of partially stabilized zirconia (high hardness and shock resistance, low friction coefficient, high chemical and thermal resistance as well as high wear and tear resistance) allow it to be used as a very particular building material. Particularly in the bio-engineering industry, it has been used to make reliable super sharp medical scalpels for doctors that are compatible with bio tissues and contain an edge much smoother than one made of steel.\n\nIn recent years manufacturers have sought ways of distinguishing their product by supposedly \"improving\" cubic zirconia. Coating finished cubic zirconia with a film of diamond-like carbon (DLC) is one such innovation, a process using chemical vapor deposition. The resulting material is purportedly harder, more lustrous and more like diamond overall. The coating is thought to quench the excess fire of cubic zirconia, while improving its refractive index, thus making it appear more like diamond. Additionally, because of the high percentage of diamond bonds in the amorphous diamond coating, the finished simulant will show a positive diamond signature in Raman spectra.\n\nAnother technique first applied to quartz and topaz has also been adapted to cubic zirconia: vacuum-sputtering an extremely thin layer of a precious metal (typically gold) or even certain metal oxides or nitrides amongst other coatings onto the finished stones creates an iridescent effect. This material is marketed as \"mystic\" by many dealers. Unlike diamond-like carbon and other hard synthetic ceramic coatings, the effect is not durable with decorative precious metal coatings due to their extremely low hardness compared to the substrate along with poor abrasion wear properties.\n\nThere are a few key features of cubic zirconia which distinguish it from diamond:\n\n\nCubic zirconia, as a diamond substituent and jewel competitor, has been seen as a potential solution against conflict diamonds and the controversy surrounding the rarity and value of diamonds.\n\nRegarding the latter, the main argument presented being that the paradigm where diamonds were seen as rare due to their visual beauty is no longer the case and instead has been replaced by an artificially rarity reflected in their price. This is attributed to confirmed evidence that there were price-fixing practices taken by the major producers of rough diamonds, in majority attributed to De Beers Company known as to holding a monopoly on the market from the 1870s to early 2000s. The company plead guilty to these charges in an Ohio court in 13 July 2004 . However, De Beers and Co do not have as much power over the market, the price of diamonds continues to increase due to the increased demand in emerging markets such as India and China. Therefore, with the emergence of artificial stones, such as cubic zirconia, that have optic properties highly similar to that of diamonds (see section above), it has been presented that these could be a better alternative for jewelry buyers given their lower price and unconvoluted history.\n\nA closely related issue to this monopoly was the emergence of conflict diamonds. The Kimberley Process was established in order to deter the illicit trade of diamonds to fund civil wars in Angola and Sierra Leone. However, it has been shown that KP is not as effective in decreasing the number of conflict diamonds reaching the European and American markets. The main problem being its definition is too narrow and does not properly include forced labor conditions or human right violations. A 2015 study from the Enough Project, showed that groups in the Central African Republic have reaped between US$3 millions and US$6 millions annually from blood diamonds . UN reports shows that more than US$24 millions in conflict diamonds have been smuggled since the establishment of the KP. Diamond substituents therefore have become an alternative to boycott altogether the funding of such unethical practices. Terms have emerged for these artificial stones such as “Eco-friendly Jewelry” that define them as conflict free origin and environmentally sustainable . However, concerns from mining countries such as the Democratic Republic of Congo are that a boycott in purchases of diamonds would only worsen their economy. According to the Ministry of Mines in Congo, 10% of its population relies in the income from diamonds. Therefore, it is argued that in the short term diamonds substituents could be an alternative to reduce conflict around the market of diamond mining but a long term solution would be establish a more rigorous system of identifying the origin of these stones.\n\n"}
{"id": "9554112", "url": "https://en.wikipedia.org/wiki?curid=9554112", "title": "Cultural significance of tornadoes", "text": "Cultural significance of tornadoes\n\nTornado damage to human-made structures is a result of the high wind velocity and windblown debris. Tornadic winds have been measured in excess of 300 mph (480 km/h). Tornadoes are a serious hazard to life and limb. As such, people in tornado-prone areas often adopt plans of action in case a tornado approaches.\n\nStorm cellars are often used as a means of shelter in case of tornadoes or tropical cyclones. Common in tornado-prone areas, they have been around for more than 100 years—even referenced in the famous 1939 film \"The Wizard of Oz\". Consisting either of a simple underground room, or an elaborate above-ground bunker, they are usually small rooms, designed to keep debris from entering and causing injury. When properly constructed, they can survive an EF5 tornado. While it is unknown how many lives have been saved by storm cellars, the number is undoubtedly high.\n\nSome individuals and hobbyists, known as storm chasers, enjoy pursuing thunderstorms and tornadoes to explore their many visual and scientific aspects. Attempts have been made by some storm chasers from educational and scientific institutions to drop probes in the path of oncoming tornadoes in an effort to analyze the interior of the storms, but only about five drops have been successful since around 1990. \n\nDue to the relative rarity and large scale of destructive power that tornadoes possess, their occurrence or the possibility that they may occur can often create what could be considered sensationalism in their reporting. This results in so-called weather wars, in which competing local media outlets, particularly TV news stations, engage in continually escalating technological one-upsmanship and drama in order to increase their market share. This is especially evident in tornado-prone markets, such as those in the Great Plains.\n\nAccording to Environment Canada, the chances of being \"killed\" by a tornado are 12 million to 1 (12,000,000:1). One may revise this yearly and/or regionally, but the probability may be factually stated to be low. Regardless, tornadoes cause millions of dollars in damage, both economic and physical, many deaths, and hundreds of injuries every year.\nThe tornado has been used by cartoonists for over 100 years as a metaphor for political upheaval. The storm cellar has also been used as a metaphor for seeking safety, as shown in the cartoon from 1894 at right.\n\nAccording to political interpretations of \"The Wonderful Wizard of Oz\", the tornado takes Dorothy to a utopia, the Land of Oz, and kills the Wicked Witch of the East, who had oppressed \"the little people\", the Munchkins.\n\nA 1960s advertising campaign for the household cleaner, Ajax, claimed the product \"Cleans like a white tornado\".\n\nNumerous athletic teams across the United States employ a tornado for their mascot. There are 124 high schools, colleges, and professional sports teams that use \"Tornadoes,\" or a variant, as their team nicknames. Another 68 teams use \"Cyclones\" (a common colloquialism for \"tornado\" in parts of the U.S.) or a form thereof.\n\n\n"}
{"id": "30189110", "url": "https://en.wikipedia.org/wiki?curid=30189110", "title": "December 2010 North American blizzard", "text": "December 2010 North American blizzard\n\nThe December 2010 North American blizzard was a major nor'easter and historic blizzard affecting the Contiguous United States and portions of Canada from December 5–29, 2010. From January 4–15, the system was known as \"Windstorm Benjamin\" in Europe. It was the first significant winter storm of the 2010–11 North American winter storm season and the fifth North American blizzard of 2010. The storm system affected the northeast megalopolis, which includes major cities such as Norfolk, Philadelphia, Newark, New York City, Hartford, Providence, and Boston. The storm brought between of snow in many of these areas.\n\nOn December 5, 2010, an extratropical disturbance developed in the western Gulf of Alaska, along a stationary front. During the next few days, the system rapidly intensified, while channeling smaller winter storms and moisture from the Pineapple Express atmospheric river in the west coast of North America, before approaching the North American west coast itself on December 8. On December 9, a new low pressure area formed to the south of Alaska in the storm system's circulation, becoming the dominant low of the storm on December 11, when the original low pressure center dissipated over the coast of British Columbia. During the next few days, the storm system slowly looped back westward while slowly strengthening, before moving back towards the West Coast on December 13. During this time, the storm system channeled more moisture into the Pacific Northwest and other neighboring regions, triggering flooding in some areas. After stalling for a day, on December 14, the central circulation split again, with the original center of circulation intensifying and moving ashore, while the new low stalled in the northern Gulf of Alaska became the dominant low of the storm.\n\nOn December 15, the Aleutian Low opened up a large atmospheric river (the Pineapple Express) again, over California, which fed the storm system moisture, allowing it to strengthen significantly. This in turn brought torrents of rain and occasional gusts of wind, while the main storm itself slowly strengthened while stalling in the Gulf of Alaska. By December 15, a powerful Kona low (which would later become Tropical Storm Omeka) had also set up near Hawaii, feeding additional moisture into the Pineapple Express, which in turn strengthened the storm system even further. The favorable weather pattern allowed the storm to retain its intensity, even while it slowly moved down towards the West Coast. The massive storm complex slowly looped westward and then southward towards the West Coast, undergoing explosive intensification once more on December 19. The system stalled off the coast of the Pacific Northwest, and weakened slightly. However, the system fully opened up the Pineapple Express over California beginning on December 19, triggering flash floods, mudslides, and landslides across much of the Western States, particularly in California. The Pineapple Express system channeled large amounts of moisture into California, ravaging much of the state from December 15 through December 22, bringing with it as much as 20 inches of rain to the San Gabriel Mountains and San bernardinos and over 13 feet of snow in the Southern California mountains and Sierra Nevada. Although the entire state was affected, the Southern California counties of San Bernardino, Orange, San Diego, and Los Angeles bore the brunt of the system of storms as coastal and hillside areas were impacted by mudslides and major flooding. During the time that the storm stalled off the West Coast, the system brought significant rain, snow, and mudslides California the Southwest region. On December 21, the storm complex's central circulation fell apart, as it began to interact with another storm in the Gulf of Alaska. On December 22, after stalling off the West Coast for 5 days, the weakened system finally approached Southern California and began moving ashore, making landfall early on December 23, which finally put a temporary end to the Pineapple Express event. Later the same day, the winter storm weakened into a small, 1011 mbar storm, after moving eastward into the Four Corners region. On December 23, the winter storm interacted with another frontal system, causing the storm to weaken even further, to the point where the storm lost its circulation. By the time the winter storm reached southern Texas later in December 2010, almost all of the system's convection had dissipated.\nLate on December 24, the winter storm began absorbing an ample amount of Gulf stream energy; as a result, the storm was able to reorganize. The core of the storm eventually moved southeast into the Gulf of Mexico then abruptly turned sharply northeastward into the Florida panhandle and state of Georgia as it merged with an upper-level low pressure system located near Virginia. As it became a stacked low pressure system, the storm system began a period of rapid intensification off the North Carolina coast while tracking along the coast of the Northeast and Newfoundland.\n\nThe storm had many similarities to North American blizzard of 2006. The storm also generated a rare meteorological phenomenon known as thundersnow in which thunder and lightning occur concurrently with the falling snow. On December 27, 2010, the storm had deepened to , equal in pressure to that found in a Category 3 hurricane. Several synoptic factors contributed to the intensity of this blizzard.\n\nThe global American and European weather models encountered exceptional difficulty in portraying the path of the storm. The main issue that the models were trying to resolve was whether or not the upper-level low pressure system from Central Canada was going to phase in time with the southern stream energy, thereby revealing if the resulting storm would come up the Eastern Seaboard or simply continue to move east out to sea in the Atlantic Ocean. The latter solution would have only caused the storm to affect the Mid-Atlantic states of Virginia and North Carolina with a modest winter storm instead of the entire Eastern coastline up to Atlantic Canada. As such, it was only 24 hours prior to the storm's arrival in the Northeastern states when all the European and American models were in agreement that the storm would turn up the entire coastline. The National Weather Service's Hydrometeorological Prediction Center and many other private forecasters were skeptical of the storm impacting the Northeastern states until about 24 hours of the storm's arrival as well; although, some models depicted the storm delivering a full-blown blizzard to the New York City metropolitan area as early as a week in advance. The Hydrometeorological Prediction Center even issued a statement on Christmas Eve, 48 hours prior to the storm, that they suspected the American models of having model initialization errors; thus, they believed these errors may have forced the storm to be erroneously modeled to come up the Northeastern coast.\n\nOn December 27, the storm developed into a powerful nor'easter that buffeted the East Coast, reaching its peak intensity. On December 29, the storm weakened and left the United States. During the next several days, the system moved eastward over the Atlantic Ocean. On January 2, 2011, the storm began to re-intensify, but the system continued to drift in the eastern North Atlantic for another two days. On January 4, the system was named \"Windstorm Benjamin\" by the Free University of Berlin, which names all Low and High-Pressure systems that affect Europe. Windstorm Benjamin continued to moved eastwards across the Atlantic, and the storm began to impact Western Europe on January 8. On January 9, Windstorm Benjamin crossed the British Isles and turned to the northeast. For the next couple of days, Windstorm Benjamin held its intensity as it continued moving northeastward, before weakening on January 11, due to land interaction with the northern coast of Norway. On January 12, Windstorm Benjamin began to move inland into Western Russia, while slowly weakening. On January 13, Benjamin accelerated eastward, while rapidly continuing to deteriortate. On January 15, Windstorm Benjamin was absorbed into the circulation of another approaching extratropical cyclone.\n\nWCAU-TV in Philadelphia forecast a snowfall of between in Philadelphia itself (which received ), while nearby Trenton, New Jersey and parts of southern New Jersey were expected to receive snowfalls of or more. Wind gusts were expected to reach between .\nAreas in eastern North Carolina and southeastern Virginia saw upwards of of snow.\n\nAreas in and around New York City, which were most impacted by the storm, reported between of snowfall by Monday morning. Measurement was made difficult due to snow drifts caused by the winds. Central Park's reported of snow from the storm is the sixth-largest snowfall on record for that location.\n\nAreas to the east of New York City received 10 to 20 inches while 20 to 30 inches of snow fell across New York City, Northeast and Central New Jersey and the Lower Hudson Valley. Areas receiving the most snow are between Kendall Park and Elizabeth and between Brick and Atlantic City in New Jersey. This was because multiple convective bands of heavy snow fed in off the ocean into the large stationary very heavy snow band (with occasional lightning) over the areas that received 20 to 30 inches. These areas received snowfall totals over . A secondary heavy snow/rain band formed across Eastern Massachusetts the night of December 26. Southern New England including Boston saw more than of snow. Even areas of coastal Virginia saw more than of snow, and North Carolina received more than .\n\nWashington, D.C. and Baltimore and their environs were largely spared this storm, receiving little accumulation despite large snowfall totals further east on the Delmarva Peninsula.\n\nWind gusts routinely over were widespread during the blizzard in coastal areas. Hurricane-force wind gusts of were recorded along coastal portions of Eastern Massachusetts. A wind gust of was recorded in Grand Etang, Nova Scotia. The Greater New York area had sustained winds of 30–45 MPH with gusts to over 60 MPH during the afternoon and night of December 26. High winds caused near zero visibilities and treacherous travel conditions in many areas. The strong easterly winds also piled up water along the eastern coast of Massachusetts, leading to major coastal flood damage.\n\nLate on December 25, WPVI-TV announced that the National Weather Service had issued blizzard warnings for northern and parts of central New Jersey, New York City, Long Island and coastal southern New England. WABC-TV also offered information on the blizzard warning that was posted throughout the New York City region.\n\nOn December 26, several closures were announced on KYW-1060 (the all-news radio station in Philadelphia), including the Please Touch Museum and Independence National Historical Park. Philadelphia mayor Michael Nutter declared a snow emergency beginning at 2:00 p.m. local time.\nThe NFL game between the Minnesota Vikings and the Philadelphia Eagles at Lincoln Financial Field in Philadelphia, scheduled for Sunday, December 26, was postponed until Tuesday, December 28. Ironically, it was scheduled to be played Sunday afternoon, but flexed to the primetime slot, displacing the San Diego Chargers-Cincinnati Bengals game to the afternoon.\n\nBoston declared a State of Emergency Sunday afternoon with an expected accumulation of over .\n\nDelaware had declared a State of Emergency on Sunday evening.\n\nAmtrak's \"Acela Express\" and regular \"Northeast Regional\" service was shut down completely between Boston and New York.\n\nNorth Carolina was one of six US states where a state of emergency was declared due to the storm. Snow totals in the state ranged from less than in the east, from in the central part of the state and along the I-95 corridor, and from in the west.\n\nBy the storm's end, weather stations in Central Park had recorded of snow. A combination of heavy snow and strong winds led to massive transportation lockdowns across the city. Nearly all roads in the city (with the exception of major arteries) were left impassable for many days because of the amount of snow. Hundreds of buses were stuck in the snow. More than 400 passengers were stranded on the A train on the IND Rockaway Line for seven hours starting shortly before 1 a.m. One train did not open its doors when it stopped at the station. Dozens of LIRR trains were frozen onto the platforms. Hundreds of plows were on the road. NYC's major airports, Kennedy, Newark and LaGuardia were shut down until 6:00 p.m. Monday evening. The MTA ran on a limited schedule, even on Tuesday morning, with many above ground subway lines not functioning. By Wednesday, only the Sea Beach Line, Franklin Avenue Line, and Greenport Branch remained suspended. The subway resumed full service on Thursday, the LIRR Friday morning.\n\nResidents of the city were complaining to Mayor Michael Bloomberg about the slowness of the snowplows attempting to reach their areas. By the late afternoon on Tuesday, snowplows had reached the minor streets in Brooklyn. Many residents of Staten Island, however, had yet to see one. Many plows became stuck in the snow that had accumulated on the streets and several could not get through as many residents had simply abandoned their cars in the middle of roads once it became too hazardous or impossible to move. These cars became stuck as the snow accumulated around them, creating roadblocks for the plows. Both Mayor Bloomberg and the chief of the sanitation department were urging residents to stay indoors and not shovel snow back onto the streets that had already been plowed. State Senator Carl Kruger called the response a \"Colossal Failure\".\n\nMultiple explanations have been proposed for the relatively slow response of the snow cleanup in the New York City metropolitan area. For example, the local media did not fully alert the general public until Christmas Day about the impending storm due to the complexity and lateness of the forecast models predicting the storm; thus, public awareness was delayed. Another possible cause was that Mayor Bloomberg had previously laid off 400 New York City Sanitation workers due to budget cuts; therefore, fewer plows were at work. In addition, some city workers who are responsible for managing the snow removal process had taken the day off from work during the storm due to the Christmas holiday. Also, a snow emergency was not declared, contributing to the number of abandoned cars on the streets.\n\nOn December 30, the New York Post reported that several sanitation workers confessed that cleanup was intentionally slowed down to protest the aforementioned budget cuts. On January 3, 2011, WCBS-TV reported that prosecutors had obtained a video of two sanitation trucks driving down 155th Street in Whitestone, Queens after the blizzard with their plows raised so as not to remove the snow. Allegations were also made that sanitation workers slept on the job, hung out at a doughnut shop for 11 straight hours while on the job, and drank beer for six or seven hours while on the job. Claims were also made that supervisors encouraged such work stoppages. However, a month later, the New York Times reported that the allegations of a slowdown remained unsubstantiated and that Councilman Daniel Halloran, who had originally reported the confessions, had not named his sources.\n\nA man who attempted suicide by jumping from a ninth-story window was saved when he landed on a huge pile of garbage that had accumulated during the blizzard.\n\nTwo casualties were reported as the result of emergency workers being unable to reach those in need: a newborn baby and an elderly woman.\n\nNew Jersey declared a State of Emergency on Sunday, December 26. They were affected the hardest because of its high winds and snow. Over of snow was estimated in some counties such as Union County. Interstate 78, Interstate 80, and Interstate 280 experienced congestion, closures, and accidents. New Jersey Transit suspended all bus service, North Jersey Coast Line service, and Hudson-Bergen Light Rail Service. River Line service had partial closures and the Newark Light Rail was delayed for 20–30 minutes. Many people in the state were stranded on roadways and only emergency vehicles were allowed on the roads.\n\nThe highest amount recorded was in Brick, New Jersey, where 36 inches of snow accumulated. The visibility was near . Central and Northeastern New Jersey were hit the hardest with snow totals of . Edison got , Lyndhurst got and Elizabeth had one of the highest totals of snow of any city affected by this storm, of heavy snow. Blizzard Warnings were prompted for eastern New Jersey, while Winter Storm Warnings were issued in the western portion. Blowing and drifting snow was a big problem on December 26 and 27.\n\nNewark Liberty International Airport in Newark had received of snowfall in about three hours, an instance of very heavy snow in a short time. Snow drifts as high as seven feet were spotted in portions of eastern New Jersey. The NJ Transit was shut down on late December 26 due to blizzard conditions occurring in the area. Cape May was one of many southern New Jersey towns that were affected by snow bands containing four-inch per hour snowfall rates. When the storm was off the coast of New Jersey, bands of heavy snow formed one after another, increasing already-hefty snowfall amounts even further. There were reports of snow plows getting stuck in the snow late at night on December 26 and the early morning of the 27th. On the 26th, visibilities in some towns were less than 0.1 miles. This caused whiteout conditions in towns along the Raritan River and the Atlantic coastline.\n\nCape May had reported nearly 15 inches of snow after the storm had moved out to sea. The town had lesser snow amounts due to a break in the southern half of the snow bands pivoting toward New Jersey. The snow totals were only expected to reach 10 to 15 inches at the highest. Instead, the highest amount was in Brick, New Jersey where 36 inches fell, nearly 3 feet of snow fell. I-95 Northbound was mostly covered in layers of snow, which was dangerous for some drivers.\n\nThe higher snowfall amounts occurred in central and northern coastal areas of New Jersey. The blustery winds in New Jersey gusted as high as 65 mph. The fact of the high winds caused the blizzard warnings to be issued by the Mount Holly, New Jersey (Philadelphia, PA area) National Weather Service officials.\n\nSnow fell so fast that traffic came to a complete halt. Motorists had to be rescued the following day as snow continued to pile up and trap them on interstates and local streets, which themselves became completely impassable. Citizens began calling into news stations asking for help and giving reports, while plows and other emergency personnel became stuck everywhere. Newark Mayor Cory Booker notably used Twitter to help stranded residents and assure citizens.\n\nHeavy snow and winds of up to caused a gas station roof to collapse in North Haven, and widespread tree damage and power outages throughout the state. Service on the Metro-North New Haven Line and its branches was suspended on Monday due to equipment and switch failures caused by the storm. The greatest snow totals were in the western part of the state, with Wilton receiving 18 inches of snow. North Canaan reported 16.1 inches, and 13.9 inches of snowfall was reported at Bradley International Airport.\n\nThe blizzard was nicknamed Winter Storm \"Adrienne\" by WFSB-TV Channel 3. The local CBS affiliate has unofficially named local winter storms since 1971.\n\nBlizzard conditions arrived in Maine on December 27. On December 28, a ski lift at the Sugarloaf ski resort derailed, causing several skiers to receive injuries after falling to the ground, sending seven to the hospital.\n\nCoastal Massachusetts, along with coastal Maine, bore the brunt of the storm with a recorded wind gust of in Scituate, Massachusetts.\n\nIn Atlantic Canada, the nor'easter was the fourth major winter storm to hit the area in four weeks. Blizzard conditions forced numerous road closures and resulted in the death of a minivan driver on December 27 near Fredericton, Prince Edward Island. Wind gusts approached , and up to of snow fell in New Brunswick, while Nova Scotia was hit by heavy rain and unseasonably warm temperatures and rain continued in eastern Newfoundland.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "20000172", "url": "https://en.wikipedia.org/wiki?curid=20000172", "title": "Diazenylium", "text": "Diazenylium\n\nDiazenylium is the chemical NH, an inorganic cation that was one of the first ions to be observed in interstellar clouds. Since then, it has been observed for in several different types of interstellar environments, observations that have several different scientific uses. It gives astronomers information about the fractional ionization of gas clouds, the chemistry that happens within those clouds, and it is often used as a tracer for molecules that are not as easily detected (such as N). Its 1–0 rotational transition occurs at 93.174 GHz, a region of the spectrum where Earth's atmosphere is transparent and it has a significant optical depth in both cold and warm clouds so it is relatively easy to observe with ground-based observatories. The results of NH observations can be used not only for determining the chemistry of interstellar clouds, but also for mapping the density and velocity profiles of these clouds.\n\nNH was first observed in 1974 by B.E. Turner. He observed a previously unidentified triplet at 93.174 GHz using the NRAO 11 m telescope. Immediately after this initial observation, Green et al. identified the triplet as the 1–0 rotational transition of NH. This was done using a combination of ab initio molecular calculations and comparison of similar molecules, such as N, CO, HCN, HNC, and HCO, which are all isoelectronic to NH. Based on these calculations, the observed rotational transition would be expected to have seven hyperfine components, but only three of these were observed, since the telescope's resolution was insufficient to distinguish the peaks caused by the hyperfine splitting of the inner Nitrogen atom. Just a year later, Thaddeus and Turner observed the same transition in the Orion Molecular Cloud 2 (OMC-2) using the same telescope, but this time they integrated for 26 hours, which resulted in a resolution that was good enough to distinguish the smaller hyperfine components.\n\nOver the past three decades, NH has been observed quite frequently, and the 1–0 rotational band is almost exclusively the one that astronomers look for. In 1995, the hyperfine structure of this septuplet was observed with an absolute precision of ~7 kHz, which was good enough to determine its molecular constants with an order of magnitude better precision than was possible in the laboratory. This observation was done toward L1512 using the 37 m NEROC Haystack Telescope. In the same year, Sage et al. observed the 1–0 transition of NH in seven out of the nine nearby galaxies that they observed with the NRAO 12 m telescope at Kitt Peak. NH was one of the first few molecular ions to be observed in other galaxies, and its observation helped to show that the chemistry in other galaxies is quite similar to that which we see in our own galaxy.\n\nNH is most often observed in dense molecular clouds, where it has proven useful as one of the last molecules to freeze out onto dust grains as the density of the cloud increases toward the center. In 2002, Bergin et al. did a spatial survey of dense cores to see just how far toward the center NH could be observed and found that its abundance drops by at least two orders of magnitude when one moves from the outer edge of the core to the center. This showed that even NH is not an ideal tracer for the chemistry of dense pre-stellar cores, and concluded that HD may be the only good molecular probe of the innermost regions of pre-stellar cores.\n\nAlthough NH is most often observed by astronomers because of its ease of detection, there have been some laboratory experiments that have observed it in a more controlled environment. The first laboratory spectrum of NH was of the 1–0 rotational band in the ground vibrational level, the same microwave transition that astronomers had recently discovered in space.\n\nTen years later, Owrutsky et al. performed vibrational spectroscopy of NH by observing the plasma created by a discharge of a mixture nitrogen, hydrogen, and argon gas using a color center laser. During the pulsed discharge, the poles were reversed on alternating pulses, so the ions were pulled back and forth through the discharge cell. This caused the absorption features of the ions, but not the neutral molecules, to be shifted back and forth in frequency space, so a lock-in amplifier could be used to observe the spectra of just the ions in the discharge. The lock-in combined with the velocity modulation gave >99.9% discrimination between ions and neutrals. The feed gas was optimized for NH production, and transitions up to \"J\" = 41 were observed for both the fundamental N–H stretching band and the bending hot band.\n\nLater, Kabbadj et al. observed even more hot bands associated with the fundamental vibrational band using a difference frequency laser to observe a discharge of a mixture of nitrogen, hydrogen, and helium gases. They used velocity modulation in the same way that Owrutsky et al. had, in order to discriminate ions from neutrals. They combined this with a counterpropogating beam technique to aid in noise subtraction, and this greatly increased their sensitivity. They had enough sensitivity to observe OH, HO, and HO that were formed from the minute O and HO impurities in their helium tank.\n\nBy fitting all observed bands, the rotational constants for NH were determined to be \"B\" = 1.561928 cm and \"D\" = , which are the only constants needed to determine the rotational spectrum of this linear molecule in the ground vibrational state, with the exception of determining hyperfine splitting. Given the selection rule Δ\"J\" = ±1, the calculated rotational energy levels, along with their percent population at 30 kelvins, can be plotted. The frequencies of the peaks predicted by this method differ from those observed in the laboratory by at most 700 kHz.\n\nNH is found mostly in dense molecular clouds, where its presence is closely related to that of many other nitrogen-containing compounds. It is particularly closely tied to the chemistry of N, which is more difficult to detect (since it lacks a dipole moment). This is why NH is commonly used to indirectly determine the abundance of N in molecular clouds.\n\nThe rates of the dominant formation and destruction reactions can be determined from known rate constants and fractional abundances (relative to H) in a typical dense molecular cloud. The calculated rates here were for early time (316,000 years) and a temperature of 20 kelvins, which are typical conditions for a relatively young molecular cloud.\n\nThere are dozens more reactions possible, but these are the only ones that are fast enough to affect the abundance of NH in dense molecular clouds. Diazenylium thus plays a critical role in the chemistry of many nitrogen-containing molecules. Although the actual electron density in so-called \"dense clouds\" is quite low, the destruction of NH is governed mostly by dissociative recombination.\n"}
{"id": "39407", "url": "https://en.wikipedia.org/wiki?curid=39407", "title": "Dirac equation", "text": "Dirac equation\n\nIn particle physics, the Dirac equation is a relativistic wave equation derived by British physicist Paul Dirac in 1928. In its free form, or including electromagnetic interactions, it describes all spin- massive particles such as electrons and quarks for which parity is a symmetry. It is consistent with both the principles of quantum mechanics and the theory of special relativity, and was the first theory to account fully for special relativity in the context of quantum mechanics. It was validated by accounting for the fine details of the hydrogen spectrum in a completely rigorous way.\n\nThe equation also implied the existence of a new form of matter, \"antimatter\", previously unsuspected and unobserved and which was experimentally confirmed several years later. It also provided a \"theoretical\" justification for the introduction of several component wave functions in Pauli's phenomenological theory of spin; the wave functions in the Dirac theory are vectors of four complex numbers (known as bispinors), two of which resemble the Pauli wavefunction in the non-relativistic limit, in contrast to the Schrödinger equation which described wave functions of only one complex value. Moreover, in the limit of zero mass, the Dirac equation reduces to the Weyl equation.\n\nAlthough Dirac did not at first fully appreciate the importance of his results, the entailed explanation of spin as a consequence of the union of quantum mechanics and relativity—and the eventual discovery of the positron—represents one of the great triumphs of theoretical physics. This accomplishment has been described as fully on a par with the works of Newton, Maxwell, and Einstein before him. In the context of quantum field theory, the Dirac equation is reinterpreted to describe quantum fields corresponding to spin- particles.\n\nThe Dirac equation in the form originally proposed by Dirac is:\n\nwhere is the wave function for the electron of rest mass with spacetime coordinates . The are the components of the momentum, understood to be the momentum operator in the Schrödinger equation. Also, is the speed of light, and is the Planck constant divided by . These fundamental physical constants reflect special relativity and quantum mechanics, respectively.\n\nDirac's purpose in casting this equation was to explain the behavior of the relativistically moving electron, and so to allow the atom to be treated in a manner consistent with relativity. His rather modest hope was that the corrections introduced this way might have a bearing on the problem of atomic spectra. Up until that time, attempts to make the old quantum theory of the atom compatible with the theory of relativity, attempts based on discretizing the angular momentum stored in the electron's possibly non-circular orbit of the atomic nucleus, had failed – and the new quantum mechanics of Heisenberg, Pauli, Jordan, Schrödinger, and Dirac himself had not developed sufficiently to treat this problem. Although Dirac's original intentions were satisfied, his equation had far deeper implications for the structure of matter and introduced new mathematical classes of objects that are now essential elements of fundamental physics.\n\nThe new elements in this equation are the 4 × 4 matrices and , and the four-component wave function . There are four components in because the evaluation of it at any given point in configuration space is a bispinor. It is interpreted as a superposition of a spin-up electron, a spin-down electron, a spin-up positron, and a spin-down positron (see below for further discussion).\n\nThe 4 × 4 matrices and are all Hermitian and have squares equal to the identity matrix:\n\nand they all mutually anticommute (if and are distinct):\n\nThe single symbolic equation thus unravels into four coupled linear first-order partial differential equations for the four quantities that make up the wave function. These matrices and the form of the wave function have a deep mathematical significance. The algebraic structure represented by the gamma matrices had been created some 50 years earlier by the English mathematician W. K. Clifford. In turn, Clifford's ideas had emerged from the mid-19th-century work of the German mathematician Hermann Grassmann in his \"Lineale Ausdehnungslehre\" (\"Theory of Linear Extensions\"). The latter had been regarded as well-nigh incomprehensible by most of his contemporaries. The appearance of something so seemingly abstract, at such a late date, and in such a direct physical manner, is one of the most remarkable chapters in the history of physics.\n\nThe Dirac equation is superficially similar to the Schrödinger equation for a massive free particle:\n\nThe left side represents the square of the momentum operator divided by twice the mass, which is the non-relativistic kinetic energy. Because relativity treats space and time as a whole, a relativistic generalization of this equation requires that space and time derivatives must enter symmetrically as they do in the Maxwell equations that govern the behavior of light — the equations must be differentially of the \"same order\" in space and time. In relativity, the momentum and the energies are the space and time parts of a spacetime vector, the four-momentum, and they are related by the relativistically invariant relation\n\nwhich says that the length of this four-vector is proportional to the rest mass . Substituting the operator equivalents of the energy and momentum from the Schrödinger theory, we get the Klein-Gordon equation describing the propagation of waves, constructed from relativistically invariant objects,\n\nwith the wave function being a relativistic scalar: a complex number which has the same numerical value in all frames of reference. Space and time derivatives both enter to second order. This has a telling consequence for the interpretation of the equation. Because the equation is second order in the time derivative, one must specify initial values both of the wave function itself and of its first time-derivative in order to solve definite problems. Since both may be specified more or less arbitrarily, the wave function cannot maintain its former role of determining the probability density of finding the electron in a given state of motion. In the Schrödinger theory, the probability density is given by the positive definite expression\n\nand this density is convected according to the probability current vector\n\nwith the conservation of probability current and density following from the continuity equation:\n\nThe fact that the density is positive definite and convected according to this continuity equation implies that we may integrate the density over a certain domain and set the total to 1, and this condition will be maintained by the conservation law. A proper relativistic theory with a probability density current must also share this feature. Now, if we wish to maintain the notion of a convected density, then we must generalize the Schrödinger expression of the density and current so that space and time derivatives again enter symmetrically in relation to the scalar wave function. We are allowed to keep the Schrödinger expression for the current, but must replace the probability density by the symmetrically formed expression\n\nwhich now becomes the 4th component of a spacetime vector, and the entire probability 4-current density has the relativistically covariant expression\n\nThe continuity equation is as before. Everything is compatible with relativity now, but we see immediately that the expression for the density is no longer positive definite – the initial values of both and may be freely chosen, and the density may thus become negative, something that is impossible for a legitimate probability density. Thus, we cannot get a simple generalization of the Schrödinger equation under the naive assumption that the wave function is a relativistic scalar, and the equation it satisfies, second order in time.\n\nAlthough it is not a successful relativistic generalization of the Schrödinger equation, this equation is resurrected in the context of quantum field theory, where it is known as the Klein–Gordon equation, and describes a spinless particle field (e.g. pi meson or Higgs boson). Historically, Schrödinger himself arrived at this equation before the one that bears his name but soon discarded it. In the context of quantum field theory, the indefinite density is understood to correspond to the \"charge\" density, which can be positive or negative, and not the probability density.\n\nDirac thus thought to try an equation that was \"first order\" in both space and time. One could, for example, formally (i.e. by abuse of notation) take the relativistic expression for the energy\nreplace by its operator equivalent, expand the square root in an infinite series of derivative operators, set up an eigenvalue problem, then solve the equation formally by iterations. Most physicists had little faith in such a process, even if it were technically possible.\n\nAs the story goes, Dirac was staring into the fireplace at Cambridge, pondering this problem, when he hit upon the idea of taking the square root of the wave operator thus:\n\nOn multiplying out the right side we see that, in order to get all the cross-terms such as to vanish, we must assume\n\nwith\n\nDirac, who had just then been intensely involved with working out the foundations of Heisenberg's matrix mechanics, immediately understood that these conditions could be met if , , and are \"matrices\", with the implication that the wave function has \"multiple components\". This immediately explained the appearance of two-component wave functions in Pauli's phenomenological theory of spin, something that up until then had been regarded as mysterious, even to Pauli himself. However, one needs at least 4 × 4 matrices to set up a system with the properties required — so the wave function had \"four\" components, not two, as in the Pauli theory, or one, as in the bare Schrödinger theory. The four-component wave function represents a new class of mathematical object in physical theories that makes its first appearance here.\n\nGiven the factorization in terms of these matrices, one can now write down immediately an equation\n\nwith to be determined. Applying again the matrix operator on both sides yields\n\nOn taking we find that all the components of the wave function \"individually\" satisfy the relativistic energy–momentum relation. Thus the sought-for equation that is first-order in both space and time is\n\nSetting\n\nand because formula_20\n\nwe get the Dirac equation as written above.\n\nTo demonstrate the relativistic invariance of the equation, it is advantageous to cast it into a form in which the space and time derivatives appear on an equal footing. New matrices are introduced as follows:\n\nand the equation takes the form (remembering the definition of the covariant components of the 4-gradient and especially that = )\n\nwhere there is an implied summation over the values of the twice-repeated index , and is the 4-gradient. In practice one often writes the gamma matrices in terms of 2 × 2 sub-matrices taken from the Pauli matrices and the 2 × 2 identity matrix. Explicitly the standard representation is\n\nThe complete system is summarized using the Minkowski metric on spacetime in the form\n\nwhere the bracket expression\n\ndenotes the anticommutator. These are the defining relations of a Clifford algebra over a pseudo-orthogonal 4-dimensional space with metric signature . The specific Clifford algebra employed in the Dirac equation is known today as the Dirac algebra. Although not recognized as such by Dirac at the time the equation was formulated, in hindsight the introduction of this \"geometric algebra\" represents an enormous stride forward in the development of quantum theory.\n\nThe Dirac equation may now be interpreted as an eigenvalue equation, where the rest mass is proportional to an eigenvalue of the 4-momentum operator, the proportionality constant being the speed of light:\n\nUsing formula_27 (formula_28 is pronounced \"d-slash\") , according to Feynman slash notation, the Dirac equation becomes:\nIn practice, physicists often use units of measure such that , known as natural units. The equation then takes the simple form\n\nA fundamental theorem states that if two distinct sets of matrices are given that both satisfy the Clifford relations, then they are connected to each other by a similarity transformation:\n\nIf in addition the matrices are all unitary, as are the Dirac set, then itself is unitary;\n\nThe transformation is unique up to a multiplicative factor of absolute value 1. Let us now imagine a Lorentz transformation to have been performed on the space and time coordinates, and on the derivative operators, which form a covariant vector. For the operator to remain invariant, the gammas must transform among themselves as a contravariant vector with respect to their spacetime index. These new gammas will themselves satisfy the Clifford relations, because of the orthogonality of the Lorentz transformation. By the fundamental theorem, we may replace the new set by the old set subject to a unitary transformation. In the new frame, remembering that the rest mass is a relativistic scalar, the Dirac equation will then take the form\n\nIf we now define the transformed spinor\n\nthen we have the transformed Dirac equation in a way that demonstrates manifest relativistic invariance:\n\nThus, once we settle on any unitary representation of the gammas, it is final provided we transform the spinor according to the unitary transformation that corresponds to the given Lorentz transformation. The various representations of the Dirac matrices employed will bring into focus particular aspects of the physical content in the Dirac wave function (see below). The representation shown here is known as the \"standard\" representation – in it, the wave function's upper two components go over into Pauli's 2-spinor wave function in the limit of low energies and small velocities in comparison to light.\n\nThe considerations above reveal the origin of the gammas in \"geometry\", hearkening back to Grassmann's original motivation – they represent a fixed basis of unit vectors in spacetime. Similarly, products of the gammas such as represent \"oriented surface elements\", and so on. With this in mind, we can find the form of the unit volume element on spacetime in terms of the gammas as follows. By definition, it is\n\nFor this to be an invariant, the epsilon symbol must be a tensor, and so must contain a factor of , where is the determinant of the metric tensor. Since this is negative, that factor is \"imaginary\". Thus\n\nThis matrix is given the special symbol , owing to its importance when one is considering improper transformations of spacetime, that is, those that change the orientation of the basis vectors. In the standard representation, it is\n\nThis matrix will also be found to anticommute with the other four Dirac matrices:\n\nIt takes a leading role when questions of \"parity\" arise because the volume element as a directed magnitude changes sign under a spacetime reflection. Taking the positive square root above thus amounts to choosing a handedness convention on spacetime .\n\nBy defining the adjoint spinor\nwhere is the conjugate transpose of , and noticing that\nwe obtain, by taking the Hermitian conjugate of the Dirac equation and multiplying from the right by , the adjoint equation:\n\nwhere is understood to act to the left. Multiplying the Dirac equation by from the left, and the adjoint equation by from the right, and subtracting, produces the law of conservation of the Dirac current:\n\nNow we see the great advantage of the first-order equation over the one Schrödinger had tried – this is the conserved current density required by relativistic invariance, only now its 4th component is \"positive definite\" and thus suitable for the role of a probability density:\n\nBecause the probability density now appears as the fourth component of a relativistic vector and not a simple scalar as in the Schrödinger equation, it will be subject to the usual effects of the Lorentz transformations such as time dilation. Thus, for example, atomic processes that are observed as rates, will necessarily be adjusted in a way consistent with relativity, while those involving the measurement of energy and momentum, which themselves form a relativistic vector, will undergo parallel adjustment which preserves the relativistic covariance of the observed values.\n\nSee Dirac spinor for details of solutions to the Dirac equation. Note that since the Dirac operator acts on 4-tuples of square-integrable functions, its solutions should be members of the same Hilbert space. The fact that the energies of the solutions do not have a lower bound is unexpected – see the hole theory section below for more details.\n\nThe necessity of introducing half-integer spin goes back experimentally to the results of the Stern–Gerlach experiment. A beam of atoms is run through a strong inhomogeneous magnetic field, which then splits into parts depending on the intrinsic angular momentum of the atoms. It was found that for silver atoms, the beam was split in two—the ground state therefore could not be integer, because even if the intrinsic angular momentum of the atoms were as small as possible, 1, the beam would be split into three parts, corresponding to atoms with . The conclusion is that silver atoms have net intrinsic angular momentum of . Pauli set up a theory which explained this splitting by introducing a two-component wave function and a corresponding correction term in the Hamiltonian, representing a semi-classical coupling of this wave function to an applied magnetic field, as so in SI units: (Note that bold faced characters imply Euclidean vectors in 3 dimensions, where as the Minkowski four-vector can be defined as .)\n\nHere and formula_46 represent the components of the electromagnetic four-potential in their standard SI units, and the three sigmas are the Pauli matrices. On squaring out the first term, a residual interaction with the magnetic field is found, along with the usual classical Hamiltonian of a charged particle interacting with an applied field in SI units:\n\nThis Hamiltonian is now a 2 × 2 matrix, so the Schrödinger equation based on it must use a two-component wave function. On introducing the external electromagnetic 4-vector potential into the Dirac equation in a similar way, known as minimal coupling, it takes the form :\n\nA second application of the Dirac operator will now reproduce the Pauli term exactly as before, because the spatial Dirac matrices multiplied by , have the same squaring and commutation properties as the Pauli matrices. What is more, the value of the gyromagnetic ratio of the electron, standing in front of Pauli's new term, is explained from first principles. This was a major achievement of the Dirac equation and gave physicists great faith in its overall correctness. There is more however. The Pauli theory may be seen as the low energy limit of the Dirac theory in the following manner. First the equation is written in the form of coupled equations for 2-spinors with the SI units restored:\n\nso\n\nAssuming the field is weak and the motion of the electron non-relativistic, we have the total energy of the electron approximately equal to its rest energy, and the momentum going over to the classical value,\n\nand so the second equation may be written\n\nwhich is of order – thus at typical energies and velocities, the bottom components of the Dirac spinor in the standard representation are much suppressed in comparison to the top components. Substituting this expression into the first equation gives after some rearrangement\n\nThe operator on the left represents the particle energy reduced by its rest energy, which is just the classical energy, so we recover Pauli's theory if we identify his 2-spinor with the top components of the Dirac spinor in the non-relativistic approximation. A further approximation gives the Schrödinger equation as the limit of the Pauli theory. Thus, the Schrödinger equation may be seen as the far non-relativistic approximation of the Dirac equation when one may neglect spin and work only at low energies and velocities. This also was a great triumph for the new equation, as it traced the mysterious that appears in it, and the necessity of a complex wave function, back to the geometry of spacetime through the Dirac algebra. It also highlights why the Schrödinger equation, although superficially in the form of a diffusion equation, actually represents the propagation of waves.\n\nIt should be strongly emphasized that this separation of the Dirac spinor into large and small components depends explicitly on a low-energy approximation. The entire Dirac spinor represents an \"irreducible\" whole, and the components we have just neglected to arrive at the Pauli theory will bring in new phenomena in the relativistic regime – antimatter and the idea of creation and annihilation of particles.\n\nIn the limit , the Dirac equation reduces to the Weyl equation, which describes relativistic massless spin- particles.\n\nBoth the Dirac equation and the Adjoint Dirac equation can be obtained from (varying) the action with a specific Lagrangian density that is given by:\n\nformula_56\n\nIf one varies this with respect to one gets the Adjoint Dirac equation. Meanwhile, if one varies this with respect to one gets the Dirac equation.\n\nThe critical physical question in a quantum theory is—what are the physically observable quantities defined by the theory? According to the postulates of quantum mechanics, such quantities are defined by Hermitian operators that act on the Hilbert space of possible states of a system. The eigenvalues of these operators are then the possible results of measuring the corresponding physical quantity. In the Schrödinger theory, the simplest such object is the overall Hamiltonian, which represents the total energy of the system. If we wish to maintain this interpretation on passing to the Dirac theory, we must take the Hamiltonian to be\n\nwhere, as always, there is an implied summation over the twice-repeated index . This looks promising, because we see by inspection the rest energy of the particle and, in case , the energy of a charge placed in an electric potential . What about the term involving the vector potential? In classical electrodynamics, the energy of a charge moving in an applied potential is\n\nThus, the Dirac Hamiltonian is fundamentally distinguished from its classical counterpart, and we must take great care to correctly identify what is observable in this theory. Much of the apparently paradoxical behavior implied by the Dirac equation amounts to a misidentification of these observables.\n\nThe negative solutions to the equation are problematic, for it was assumed that the particle has a positive energy. Mathematically speaking, however, there seems to be no reason for us to reject the negative-energy solutions. Since they exist, we cannot simply ignore them, for once we include the interaction between the electron and the electromagnetic field, any electron placed in a positive-energy eigenstate would decay into negative-energy eigenstates of successively lower energy. Real electrons obviously do not behave in this way, or they would disappear by emitting energy in the form of photons.\n\nTo cope with this problem, Dirac introduced the hypothesis, known as hole theory, that the vacuum is the many-body quantum state in which all the negative-energy electron eigenstates are occupied. This description of the vacuum as a \"sea\" of electrons is called the Dirac sea. Since the Pauli exclusion principle forbids electrons from occupying the same state, any additional electron would be forced to occupy a positive-energy eigenstate, and positive-energy electrons would be forbidden from decaying into negative-energy eigenstates.\n\nIf an electron is forbidden from simultaneously occupying positive-energy and negative-energy eigenstates, then the feature known as Zitterbewegung, which arises from the interference of positive-energy and negative-energy states, would have to be considered to be an unphysical prediction of time-dependent Dirac theory. This conclusion may be inferred from the explanation of hole theory given in the preceding paragraph. Recent results have been published in Nature [R. Gerritsma, G. Kirchmair, F. Zaehringer, E. Solano, R. Blatt, and C. Roos, Nature 463, 68-71 (2010)] in which the Zitterbewegung feature was simulated in a trapped-ion experiment. This experiment impacts the hole interpretation if one infers that the physics-laboratory experiment is not merely a check on the mathematical correctness of a Dirac-equation solution but the measurement of a real effect whose detectability in electron physics is still beyond reach.\n\nDirac further reasoned that if the negative-energy eigenstates are incompletely filled, each unoccupied eigenstate – called a hole – would behave like a positively charged particle. The hole possesses a \"positive\" energy since energy is required to create a particle–hole pair from the vacuum. As noted above, Dirac initially thought that the hole might be the proton, but Hermann Weyl pointed out that the hole should behave as if it had the same mass as an electron, whereas the proton is over 1800 times heavier. The hole was eventually identified as the positron, experimentally discovered by Carl Anderson in 1932.\n\nIt is not entirely satisfactory to describe the \"vacuum\" using an infinite sea of negative-energy electrons. The infinitely negative contributions from the sea of negative-energy electrons have to be canceled by an infinite positive \"bare\" energy and the contribution to the charge density and current coming from the sea of negative-energy electrons is exactly canceled by an infinite positive \"jellium\" background so that the net electric charge density of the vacuum is zero. In quantum field theory, a Bogoliubov transformation on the creation and annihilation operators (turning an occupied negative-energy electron state into an unoccupied positive energy positron state and an unoccupied negative-energy electron state into an occupied positive energy positron state) allows us to bypass the Dirac sea formalism even though, formally, it is equivalent to it.\n\nIn certain applications of condensed matter physics, however, the underlying concepts of \"hole theory\" are valid. The sea of conduction electrons in an electrical conductor, called a Fermi sea, contains electrons with energies up to the chemical potential of the system. An unfilled state in the Fermi sea behaves like a positively charged electron, though it is referred to as a \"hole\" rather than a \"positron\". The negative charge of the Fermi sea is balanced by the positively charged ionic lattice of the material.\n\nIn quantum field theories such as quantum electrodynamics, the Dirac field is subject to a process of second quantization, which resolves some of the paradoxical features of the equation.\n\nThe Dirac equation can be formulated in a number of other ways.\n\nGenerically (if a certain linear function of electromagnetic field does not vanish identically), three out of four components of the spinor function in the Dirac equation can be algebraically eliminated, yielding an equivalent fourth-order partial differential equation for just one component. Furthermore, this remaining component can be made real by a gauge transform.\n\nThis article has developed the Dirac equation in flat spacetime according to special relativity. It is possible to formulate the Dirac equation in curved spacetime.\n\nThis article developed the Dirac equation using four vectors and Schrödinger operators. The Dirac equation in the algebra of physical space uses a Clifford algebra over the real numbers, a type of geometric algebra.\n\nFor the Dirac spinor, it is possible to show that one can always find local Lorentz transformations for which the spinor is written in the so-called polar form, that is the form manifesting only two physical degrees of freedom, given by the scalar and pseudo-scalar bi-linear quantities: the Dirac spinor field equation can always be written in the corresponding polar form, which is the form specifying all derivatives of the scalar and pseudo-scalar bi-linear quantities themselves. In this formulation, the Dirac spinor field equation (which are four complex equations, and so eight equations in total) is converted into an equivalent system of two real vector equations (which are two 4-dimensional equations, and so again eight equations in total). This is what is known as the polar form of the Dirac equation.\n\nThe Dirac equation appears on the floor of Westminster Abbey on the plaque commemorating Paul Dirac's life, which was unveiled on 13 November 1995.\n\n\n\n\n\n\n"}
{"id": "26909778", "url": "https://en.wikipedia.org/wiki?curid=26909778", "title": "Electric car use by country", "text": "Electric car use by country\n\nElectric car use by country varies worldwide, as the adoption of plug-in electric vehicles is affected by consumer demand, market prices and government incentives. Plug-in electric vehicles (PEVs) are generally divided into all-electric or battery electric vehicles (BEVs), that run only on batteries, and plug-in hybrids (PHEVs), that combine battery power with internal combustion engines. \nThe popularity of electric vehicles has been expanding rapidly due to government subsidies, their increased range and lower battery costs, and environmental sensitivity. \nHowever, the stock of plug-in electric cars represented just about 1 out of every 300 motor vehicles on the world's roads by September 2018.\n\nGlobal cumulative sales of highway-legal light-duty plug-in vehicles reached 2 million units at the end of 2016, 3 million in November 2017, and the 4 million milestone in September 2018.\nSales of light-duty plug-ins achieved a 1.3% market share of new car sales in 2017, up from 0.86% in 2016, and 0.62% in 2015. \nThe global ratio between battery BEVs and PHEVs was 66:34 in 2017, up from 61:39 in 2016, and 59:41 in 2015.\n\n, China had the largest stock of highway legal light-duty plug-ins with almost 2 million domestically built passenger cars. \nChina also dominates in plug-in electric bus deployment, with its stock reaching 343,500 units in 2016 out of global stock of about 345,000 vehicles. \nMore than one million light-duty passenger plug-ins had been registered in Europe through June 2018, with Norway as the leading market with almost 275,000 units. \nNorway had the highest market penetration per capita in the world in 2016, and 10% of all passenger cars on Norwegian roads were plug-ins by the end of October 2018.\nNorway also has the world's largest plug-in segment market share of new car sales, 39.2% in 2017. \n, the United States had one million plug-in cars, with California accounting for approximately 48% of cumulative US plug-in sales at almost 428,000 units .\n\nThe global stock of plug-in electric vehicles (PEVs) between 2005 and 2009 consisted exclusively of all-electric cars (BEV), totaling about 1,700 units in 2005, and almost 6,000 in 2009. The plug-in stock rose to about 12,500 units in 2010, of which 350 were plug-in hybrids (PHEVs). By comparison, during the Golden Age of the electric car at the beginning of the 20th century, the EV stock peaked at approximately 30,000 vehicles. After the introduction of the Nissan Leaf and the Chevrolet Volt in late December 2010, the first mass-production plug-in electric cars by major manufacturers, plug-in sales grew to about 50,000 units in 2011, to 125,000 in 2012, and almost 213,000 cars and utility vans in 2013. Sales totaled over 315,000 units in 2014, up 48% from 2013. In March 2014, Norway became the first country where over 1 in every 100 passenger cars on the roads was a plug-in.\n\nIn five years, global sales of highway legal light-duty plug-in vehicles increased more than ten-fold, totaling more than 565,000 units in 2015 - an 80% increase from 2014, driven mainly by China and Europe. About 775,000 plug-in electric cars and vans were sold in 2016, and 1.22 million in 2017 - up 57% from 2016 - with China accounting for about half of global sales. The global market share of the new light-duty plug-in segment reached 1.3% in 2017, up from 0.86% in 2016, and 0.38% in 2014. Global light-duty plug-in vehicle sales passed the 3 million milestone in November 2017 and 4 million in September 2018. In October 2018, Norway became the first country where over 1 in every 10 passenger cars registered is a plug-in electric vehicle.\n\nAlbania is considered one of the best countries for emissions for electric cars as it generates all of its electricity from hydroelectric power. Electric cars are currently used by the Albania Police Force. The Interior Minister claimed, that the cost of fuel per 100 kilometers(62 miles) would be less than 120 Albanian Leke (less than 1 Euro). Saytaxi is the first taxi company in Albania that offers electric vehicles and operates a fast EV(electric vehicles) charging point, and have been operating in the country since 2014. Its goal is to replace 80% of all non-electric cars with electric in the taxi business.\n\nIn October 31, 2017, Tirana became one of few European countries to use electric busses when they tested a Solaris Urbino 12, with the purpose of reducing pollution. Tirana's goal is to gradually convert 10 to 20 percent of the bus fleet into electric ones.\n\nBeginning in mid-2009, a twelve-month field trial was conducted with the Mitsubishi i-MiEV with potential electric vehicle customers, such as government bodies and fleet operators. The iMiEV remained the top selling electric vehicle in Australia through 2013. The Mitsubishi Outlander P-HEV, became Australia's top selling EV in 2014 and remained the leader into 2016 with 2,015 units sold since its introduction. At the end of March 2015, Tesla Model S registrations totaled 119 in New South Wales and 54 in Victoria. Although no sales figures were reported for Tesla in other states, the combined sales of these two were enough for the Model S to rank as the top selling BEV car for the first quarter of 2015, ahead of the BMW i3 (46) and the Nissan Leaf (31). , about 1,000 Leafs had been sold since its 2012 introduction .\n\nChargepoint is the only major operator of a charging network in Australia.\n\nSales of electric cars rose from 97 units in 2009, to 116 in 2010, 425 in 2011, to 900 in 2012. Of the latter, only 350 units were sold to individuals.\n\nThe Belgian government established purchase incentives for BEVs, ending in 2012. Hybrids were not eligible. A separate subsidy supported investments in public charging stations.\n\n, 2,214 hybrid and electric vehicles were registered in the state of São Paulo In March 2013, the first two Leafs were deployed in Rio de Janeiro to operate as taxis. In September 2014 the BMW i3 became the first EV available for retail customers. , other retail plug-ins were the BMW i8 and the Mitsubishi Outlander P-HEV.\n\nPlug-ins and hybrids are subject to taxes adding up to more than 120% of the retail price.\n\nIn May 2014 São Paulo City passed a municipal law to exempt EV, hybrids and fuel cell vehicles from the city's driving restriction scheme and purchase incentives.\n\nIn April 2018, BYD receives the largest Brazilian order from the city of São José dos Campos for 29 BYD e5 BEVs and one BYD e6, which will be used by the police and government.\nThere were 560 electric motobikes and 520 electric cars officially registered in Bulgaria at the end of March 2018. The government does not provide grants for buying electric cars, but at least it does not apply road tax to them. Parking electric vehicles in central urban parking zones is free of charge as well.\n\nThe first car sharing company in Bulgaria Spark.bg uses only electric cars, mostly E-up!s and in early 2018 had at least 60 electric vehicles and expected to have 100 at the end of the year. Courier company Speedy uses 20 electric Renault Kangoo.\n\nCumulative sales of EVs in Canada passed 47,800 units in December 2017 and accounted for 1.4% of all new car sales for December 2017, and 0.9% for 2017. The Chevrolet Volt was the top selling PHEV, with cumulative sales of 13,619 units through Dec 2017, and the Tesla Model S was the top selling BEV with 6,731 units sold as of Dec 2017.\n\nAn Edmonton, Alberta local ride-sharing company, TappCar, uses a fleet of BYD e6 pure-electric vehicles to serve the Edmonton region and the Edmonton International Airport.\n\nPurchase and other incentives for new EVs are offered by the provinces of Quebec and British Columbia.\n\nIn October 2016, Quebec passed legislation that obliges major carmakers to offer an increasing number of PHEV and BEV models, beginning with 3.5% in 2018 and rising to 15.5% in 2020, using a tradable credit system.\n\nChina is by far the largest electric car market in the world. \nDomestically built new energy vehicle (NEV) sales totaled 1,728,447 units between January 2011 and December 2017. These figures include heavy-duty commercial vehicles such as buses and sanitation trucks. Sales of domestically built new energy passenger cars totaled over 1.2 million units between 2011 and 2017, of which, a total of 579,000 were sold in 2017, representing about half of global plug-in car sales in 2017. Domestically-produced vehicles accounted for about 96% of total plug-in electric vehicle sales. , the NEV stock consisted of about 1,385,088 all-electric vehicles (80.1%) and 343,359 plug-in hybrid vehicles (19.9%). Entry-level vehicles dominate the Chinese plug-in passenger market. The Chinese stock of plug-in electric buses was about 343,500 vehicles , of which, almost 300,000 were BEVs. The Chinese plug-in electric bus stock grew nearly sixfold over 2014, and almost doubled from 2015 to 2016.\n\nChinese sales of domestically-built new energy vehicles in 2017 totaled 777,000 units, up 53% from 2016 consisting of 652,000 all-electric vehicles (up 59.4%) and 125,000 plug-in hybrid vehicles (up 27.6%). Sales of domestically-produced new energy passenger vehicles totaled 579,000 units, consisting of 468,000 all-electric cars and 111,000 plug-in hybrids. Accounting for foreign brands, plug-in car sales rise to about 600,000 in 2017. The plug-in segment achieved a record market share of 2.1% of new car sales.\n\nThree BYD Auto models topped the Chinese ranking of best-selling new energy passenger cars in 2016. The BYD Tang PHEV SUV was the top selling plug-in car (31,405), followed by the BYD Qin (21,868) and the BYD e6 (20,605). Through 2016, the BYD Qin (68,655), remained as the country's all-time top selling plug-in car. BYD Auto was again the world's top selling plug-in manufacturer with over 100,000 units sold in 2016. However, in terms of sales revenue, Tesla vehicle sales of topped BYD at .\n\nThe BAIC EC-Series all-electric city car was the Chinese top selling plug-in car with 78,079 units delivered, making the city car the world's top selling plug-in car in 2017. The top selling plug-in hybrid was the BYD Song PHEV with 30,920 units. BYD Auto was the top selling Chinese car manufacturer in 2017.\n\nThe Chinese government adopted in 2009 a plan to become one of the world leaders in producing EVs. The plan has four goals: create a world-leading industry; energy security; reduce urban air pollution; and to reduce carbon emissions. In June 2012 the State Council published a plan to develop the domestic industry. The plan set a sales target of 500,000 new EVs by 2015 and 5 million by 2020. Initial sales were much lower than expected, while most output was purchased for public fleets. New incentives were issued in 2014, with a sales target of 160,000 units for 2014. This goal was also not achieved.\n\nThe Chinese government uses the term new energy vehicles (NEVs) to designate plug-ins and electrics. Only EVs are eligible for purchase incentives. The Chinese government has put forward the strategy of “Energy Saving and Electric Vehicles”. The policy measures of this strategy mainly focused on carrying out pilots to subsidize EV buyers, promoting charging facilities construction and accelerating EVs commercialization. On June 1, 2010, the government announced a trial program to provide financial incentives in five cities. A 2013 joint announcement by the National Development and Reform Commission and finance, science, and industry ministries offered a maximum of toward the purchase of a BEV passenger vehicle and up to US$81,600 for electric buses. In April 2016 the Traffic Management Bureau under the Ministry of Public Security announced special green license plates to facilitate preferential traffic policies.\n\nThe Mitsubishi i-MiEV was the first EV in Chile. The first public quick charging station was opened in April 2011.\n\nIn August 2014 Mitsubishi replaced the i-Miev with the Outlander PHEV. Later that year BMW introduced their \"i\" range with the i3; Renault launched their Zero Emission (Z.E.) lineup, including the Fluence Z.E. sedan, the Kangoo utility van and Zoe city car. The French brand sold 22 electric vehicles in their first month in the Chilean market.\n\nLatin America's first BEV taxi fleet (made up of BYD e6) was launched at the beginning of 2013 in Bogotá. These taxis were exempted from the driving restriction scheme. The program is an effort to improve air quality and set an example. In September 2013 45 taxis were delivered. The e6 fleet are part of Colombia's \"BIOTAXIS Project.\" Another three BYD e6s were sent to Colceincias, Bogota's Tech, Science and Innovation Administration.\n\nThe BMW i3 was introduced in Colombia in 2014. The BEV Renault Twizy quadracycle was introduced in the Colombian market in June 2015. Sales of the Outlander P-HEV were scheduled to begin in September 2015.\n\n, 126 EVs had been sold, mostly to corporate customers, and consisting of 43 BYD e6s (taxis), 35 Mitsubishi i-MiEVs, 25 BMW i3s, 19 Renault Twizys, and four Nissan Leafs. 203 Twizys had been sold .\n\nIn 2013 the government established promotional incentives. These include the exemption from the driving restriction scheme in place in Colombian cities such as Bogotá and Medellín. The government exempted BEV and PHEV cars from import duties for three years, with an annual quota of 750 cars of each type.\n\n, the Costa Rican stock of electric drive vehicles consisted of 477 hybrid electric vehicles and 2,229 electric vehicles. Plug-in car sales totaled 108 units in 2016. The top selling model was the Outlander P-HEV with 60 units.\n\nIn 2006 electric cars were exempted from the consumption tax, while conventional vehicles faced a 30% rate. \nIn October 2012, electric cars were exempted from San José's driving restriction. EVs were exempted from import duties and the government agreed to deploy charging stations in strategic locations in San José.\n\nThe first electric car to go on sale was the REVAi, introduced in March 2009. The REVAi, powered by lead–acid batteries, sold 10 units. The Mitsubishi i MiEV was launched in February 2011, with initial availability of 25 to 50 units.\n\nIn January 2013 BYD Auto signed an agreement with the Costa Rican Ministry of Environment and Energy to deploy 200 BYD e6 electric cars for use as \"green taxis.\" Retail sales of the BYD Qin plug-in began in Costa Rica in November. Retail sales of the Outlander P-HEV began in March 2015. The BMW i3 was released in September 2016.\n\n, 2067 electric cars had been sold in Croatia. Of these, 224 were EVs, while the rest were hybrids. , 201 free public charging stations operated in Croatia.\n\nIn 2014 and 2015, the Croatian government initiated purchase incentives. The subsidies were discontinued in 2016, due to ineffectiveness.\n\n, around 4,000 electric cars had been sold in Denmark. Denmark was the second largest European market for light-duty, plug-in commercial vehicles or utility vans, with over 2,600 plug-in vans sold that year, representing 8.5% of all vans sold . Most vans were plug-in hybrids, accounting for almost all EU plug-in van sales. After the expiration of the government incentives, sales drop to about 1,300 all-electric cars in 2016, and fell further in 2017 to almost 700 pure electric cars. , the stock of registered all-electric cars amounted to 8,746 units, slightly up from 8,643 electric cars in 2016. Sales of plug-in hybrids grew from 5 sold in 2013 to 572 in 2016, and 621 units in 2017.\n\nDenmark's sales surged before the expiration of its purchase incentives, and plunged thereafter, its plug-in market penetration plunge to 0.1% . In April 2017 the government announced a partial resumption of the credit, while adding a new fund for fuel cell vehicles.\n\n, 1,188 plug-in vehicles were registered.\n\nEstonia was the first country to deploy an EV charging network with nationwide coverage, with fast chargers available along highways at a maximum distance of . , the nationwide network consisted of 165 fast chargers.\n\nIn 2011, the government confirmed the sale to Mitsubishi of 10 million carbon dioxide credits in exchange for 507 i-MiEV electric cars. The deal included funding 250 fast charging stations and subsidies for the first 500 private buyers of any electric approved by the EU. The first 50 i-MiEVs were delivered in October 2011, for use by municipal social workers.\n\nEstonia's figures are low compared to other advanced economies, attributed to lack of government incentives after the carbon credit scheme was exhausted.\n\n, about 2,250 EVs were registered. Sales reached 854 in the first three quarters of 2016. Plug-in sales were slowed over range concerns and high plug-in prices.\n\nIn November 2016, the government set the goal of 250,000 plug-in cars and 50,000 biogas cars on the road by 2030. These goals are part of the Finnish government efforts to comply with the 2015 Paris Agreement.\n\nMany companies in Finland are involved in next-generation vehicle manufacturing, including Valmet Automotive (Fisker Karma and Garia A/S electric golf cart production) Fortum (concept cars and infrastructure), Kabus (hybrid buses; part of Koiviston Auto Oy), BRP Finland (part of Bombardier Recreational Products), Lynx (snowmobile), Patria (military vehicles), European Batteries (Li-ion battery plant in Varkaus), Finnish Electric Vehicles (battery control systems), ABB, Efore, Vacon (electric motor technology production), Ensto (production of charging units), Elcat (electric vehicle production since the 1980s), production of electric car accessories, Suomen Sähköauto Oy (produces small electric cars), Oy AMC Motors Ltd. (produces and designs small electric cars), Raceabout (specialist electric sport car with very few sales), Gemoto skooters from Cabotec, Resonate's Gemini and Janus Scooters, Moto Bella Oy, Axcomotors, Randax and Visedo.\n\nResearch related to electric cars is in progress at the VTT Technical Research Centre of Finland and Tekes.\n\nElectric car organisations in Finland include the Electric Vehicle Association of Finland and Electric Vehicles Finland. A non-commercial electric car conversion organisation is called \"Electric Cars - Now!\" that converts Toyota Corollas into Li-ion battery-powered electric cars.\n\nBasic charging infrastructure is available all over Finland, used for winter engine pre-warming. Because of its climate – cold winters and warm summers – Finland is considered a convenient \"test laboratory\" for electric cars.\n\nThe stock of light-duty plug-in electric vehicles registered in France passed the 100,000 unit milestone in October 2016, making the country the second largest plug-in market in Europe after Norway. It ranked also as the world's fifth largest plug-in market after the U.S., China, Japan and Norway.\n\n, a total of 149,797 light-duty plug-in electric vehicles have been registered in France since 2010. The plug-in electric stock consisted of 92,256 all-electric passenger cars, 25,269 all-electric utility vans, and 32,272 plug-in hybrids. The plug-in passenger car segment achieved a record market share of 1.98% of new car registrations in 2017. France was the largest European market for light-duty electric commercial vehicles or utility vans in 2016.\n\nPlug-in electric car registrations have been led by the Renault Zoe for five years running since 2013, with 5,511 in 2013, 5,970 in 2014, 10,406 in 2015, 11,402 in 2016 and with 15,245 units in 2017, totaling 48,582 units since 2012. The electric utility van segment has been led by the Renault Kangoo Z.E. with over 15,000 units sold through September 2016.\n\nIn 2008 France established a bonus-\"malus\" system offering a purchase incentive for low emission cars and a penalty fee (\"malus\"), for the purchase of high-emission vehicles. In 2015 the government introduced an additional bonus for all-electric car purchasers who scrapped a diesel-powered car in circulation before 1 January 2001. , the scrappage bonus had been granted for more than 10,000 purchases.\n\n, a total of 129,246 plug-in electric cars have been registered in Germany since 2010. The country is Europe's largest passenger car market, but ranks only fifth in plug-in car sales in 2016. About 80% of the plug-ins registered through September 2016 were registered since 2014. In 2013 Germany reclassified range-extended vehicles as series plug-in hybrids instead of all-electric vehicles. As a result, the registrations figures for 2012 and older do not account for plug-in hybrids. , the country had 4,800 public charging stations.\n\nA record of 54,492 plug-in cars were registered in 2017, up 217% the previous year, and consisting of 29,436 plug-in hybrids and 25,056 all-electric cars. Registrations achieved a market share of 1.58% in 2017. The top selling models in 2017 were the Audi A3 e-tron (4,454), Renault Zoe (4,322), and BMW i3 (4,319).\n\nUnder its National Plattform for Electric Mobility, Chancellor Angela Merkel in 2010 set the goal of putting one million electric vehicles on German roads by 2020. Initially, the government did not provide subsidies in favor of research. The Bundestag passed the Electric Mobility Act in March 2015 that authorized local government to grant non-monetary incentives. The measures privilege battery-powered cars, fuel cell vehicles and some PHEVs, by granting local governments the authority to offer additional incentives.\n\nThe introduction of the purchase bonus noticeably impacted sales only in September 2016, when registrations grew to 3,061 units.\n\nAn incentive scheme was approved in April 2016 including purchase subsidies, charging stations and another federal government fleet purchases, with a target of 400,000 electric vehicles. Premium cars, such as the Tesla Model S and BMW i8, were not eligible.\n\n, BMW, Citroën, Daimler, Ford, Hyundai, Kia, Mitsubishi, Nissan, Peugeot, Renault, Toyota, Volkswagen, and Volvo had signed up to participate in the scheme. The online application system to claim the bonus went into effect on 2 July 2016.\nAs of December 2017, 10,666 plug in vehicles were registered in Hong Kong. March 2017 saw 2,964 EV's registered in one month before first registration tax exemption was repealed. 2,939 of these cars were Tesla Model S and X. \n\n, 6,298 plug-in vehicles were on the roads in Hong Kong, up from 3,253 in October 2015. The plug-in segment market share achieved 4.8% of new car sales in Hong Kong in 2015.\n\n, more than 1,200 public electric vehicle charging points were available. More than a dozen models were available for retail customers.\n\nSales of electric cars took off in Hong Kong with the Tesla Model S in 2014. The tax waiver made the Model S competitive in the luxury car segment, at about half the price of other high-end models. According to Tesla, , Hong Kong had the world's highest density of Tesla superchargers, giving most Model S owners a supercharger within a 20-minute drive.\n\nThe Government offered purchase incentives to consumers, businesses and service providers were available from 2011 to 2017. The Government further allocated million for bus companies to purchase 36 electric buses.\n\nIn November, 2018 8,482 PEVs were registered in Hungary.\nThe Hungarian government introduced its e-mobility plan in March, 2014. The Jedlik plan supported the domestic production of electric vehicles, expanding the necessary infrastructure and promoting the purchase of EVs with public incentives, including 1,5 million HUF, initiated at the end of 2016.\n\nThe plug-in car segment in Iceland reached 5.37% of all new vehicles registered in 2016, allowing the country to rank second in Europe after Norway that year. Registrations of new plug-in electric cars totaled 2,990 units in 2017, up 157% from the previous year. The segment's market share achieved a record 14%, globally, second only to Norway. The top selling plug-ins in 2017 were the Mitsubishi Outlander PHEV with 884 units and the Nissan Leaf with 524.\n\nThe government eliminated VAT (24%) and -based fees (up to 65%) on new car purchases for EVs.\n\n, Orka Náttúrunar (ON) was working to complete a network of 50 kW CCS Combo/CHAdeMO stations along the Ring Road. Iceland remains the only major EV market without a Tesla Supercharger network.\n\n, over 6,000 plug-in cars were registered, consisting of 4,350 BEV cars and 1,660 PHEVs.\nThe Indian government has FAME schemes and Lower GST on EVs to encourage electric vehicles.\nTata also launch electric buses to Himachal Pradesh road transport corporation \nThe Mahindra Reva e2o electric car was introduced in March 2013. \nIt operates on lithium ion batteries with 100 km range for 4 hours of charging.\nIn 2016, a new car, the Mahindra e-Verito, introduced a sedan class Ev at a cost of 9 to 10 lakh Indian rupees ex showroom.\nRevised and new variants(P6, P8) of e2o, now called e2o plus offer increased ranged of upto 140 km on full charge.\n\nThe government supported some trial models made by Tucuxi. Conversion of some vehicles to electric drivetrains was introduced during the APEC Meeting in October 2013.\n\nSales of electric cars in Ireland increased more than four times in 2014 from a low base.\n\nThe government committed to making 10% of all vehicles by 2020 (a projected 230,000 vehicles). \nGovernment officials reached agreements with French car maker Renault and its partner Nissan. , purchase incentives became available.\n\n over 6,100 plug-in cars were registered, consisting of 4,580 BEV cars and 1,550 PHEVs.\nThe top EV in 2015 was the Nissan Leaf (390 units sold).\n\nThe government discontinued incentives in 2014 amid a limited public charging infrastructure and tepid reception. Further, many Italian houses were equipped with electric contracts allowing only 3 kW of peak consumption, making home charging of electric cars impractical.\n\nFrom January to June 2018, 42,273 plug-in and 2,199 EVs were sold in Italy, a 130% increase compared to 2017.\n\n, Japan had a stock of light-duty plug-in vehicles of about 207,200. Sales totaled 24,660 units in 2015, and 24,851 units in 2016. The segment market share declined from 0.68% in 2014 to 0.59% in 2016. Declining sales growth reflected the governmental and domestic carmaker decision to promote hydrogen fuel cell vehicles instead. Sales recovered in 2017, with almost 56,000 plug-in cars sold, and the segment's market share reached 1.1%.\n\nIn May 2009 the Japanese Diet passed the \"Green Vehicle Purchasing Promotion Measure\". \nThe program provided purchasing subsidies for cars, mini and keis, trucks and buses, including an extra subsidy for purchases trading in a sufficiently old used car. \nThe program ended on March 31, 2010. The Japanese electric vehicle charging infrastructure climbed from 60 public stations in 2010 to 1,381 in 2012.\n\nMitsubishi introduced multiple plug-in vehicles: the Mitsubishi i MiEV in 2009, the Mitsubishi Minicab MiEV in 2011, a truck version of the Minicab MiEV and the Mitsubishi Outlander P-HEV in 2013. , Mitsubishi had sold 36,386 light-duty plug-ins.\n\nThe Nissan Leaf launched in 2010. The Toyota Prius PHEV launched in January 2012, selling 19,100 units through September 2014. Tesla Model S deliveries began in September 2014.\n\nLeaf sales in 2016 were 14,795 units. Nissan had sold 72,494 units cumulatively through 2016, making the Leaf Japan's all-time best-selling plug-in car. Sales of the Outlander PHEV fell sharply from April 2016 as a result of Mitsubishi's fuel mileage scandal. Sales totaled 34,830 units through August 2016.\n\nThere have not been much effort in by Kosovo of using Plug-in electric vehicles. However ProCredit Bank, Kosova, became the first institution in Kosovo to use electric vehicles, by buying 10 new Mitsubishi i-MiEV vehicles. In 2017, six teens in the city Gjakova, from BONEVET makerspace, became the first European teenager group to build an electric car out of an Renault Twingo, transforming it from a petrol-fuelled car to a fully functional electric car.\n\nAs of the 1st of July 2018 806 EVs were registered. Registrations were led by Nissan (50%). Also 11198 hybrids registered in Lithuania by the 1st of July 2018. Registrations were led by Toyota (64%). Source: https://sumin.lrv.lt/lt/veiklos-sritys/kita-veikla/pletra-ir-inovacijos/elektromobiliu-skaicius-lietuvoje\n\nIn October 2009 Nissan reached an agreement with the Mexico City government, purchasing 500 Leafs for use of government and corporate fleets. In exchange, recharging infrastructure was to be deployed by the city government. The first 100 Leafs (destined for the taxi fleet) were delivered in 2011.\n\n, about 70 Leafs were deployed as taxis, 50 in Aguascalientes and 20 in Mexico City.\n\nRetail Leaf sales began in June 2014. Retail deliveries of the BMW i3 began 2014.\n\nThe second generation Volt and Tesla Model S began in 2015.\n\n, no government purchase incentives were available. \nHowever, electric cars are exempted from Mexico City's driving restriction scheme.\n\n, there were 121,542 highway legal light-duty plug-in electric vehicles registered in the Netherlands, consisting of 98,217 range-extended and plug-in hybrids, 21,115 pure electric cars, and 2,210 all-electric light utility vans. When buses, trucks, motorcycles, quadricycles and tricycles are accounted for, the Dutch plug-in electric-drive fleet climbs to 123,499 units. The country's electric vehicle stock reaches 165,886 units when fuel cell electric vehicles (43), mopeds (4,376), electric bicycles (37,652), and microcars (316) are accounted for. The market was dominated by plug-in hybrids representing 80.8% of the country's stock of passenger plug-in electric cars and vans registered at the end of December 2017.\n\nPlug-in car sales fell sharply during 2016 after changes in the tax rules. Sales during the first half of 2016 were down 64% from the same period in 2015. The plug-in market share declined from 9.9% in 2015, to 6.7% in 2016, and fell to 2.6% in 2017.\n\nA total of 42,367 plug-in cars were sold in 2015. The top 5 were all plug-in hybrids, led by the Mitsubishi Outlander PHEV. The Tesla Model S continued as the top selling electric car with 1,842 units. A total of 9,185 passenger plug-ins were registered in the first three quarters of 2016. the Outlander P-HEV was the top-selling plug-in car with 25,984 units, followed by the Volvo V60 PHEV (15,804), Volkswagen Golf GTE (10,691), Volkswagen Passat GTE (7,773), Mercedes-Benz C 350 e (6,226), and the Tesla Model S (6,049).\n\nBYD introduces the first European BYD e6 in Netherlands. BYD and Rotterdam (the second-largest city in the Netherlands) have entered into a binding agreement that calls for BYD to deliver an undisclosed number of its electric e6 crossovers as part of the Netherlands green transportation initiative dubbed \"75-EV-RO.\" \n\nFrom January 1, 2016, all-electric vehicles continue to pay a 4% registration fee, but for a plug-in hybrids the fee rises from 7% to 15% if its emissions do not exceed 50 g/km. The rate for a conventional internal combustion car is 25% of its book value.\n\nThe Dutch government set a target of 15,000 electric vehicles in 2015, 200,000 in 2020 and 1 million in 2025. The government exempted selected vehicles from registration fee and road taxes. The exemption from the registration tax ended in 2013. Battery electric vehicles have special access to parking spaces in Amsterdam, queues for which can otherwise reach up to 10 years. Free charging is offered in public parking spaces.\n\nOther factors contributing to the rapid adoption of plug-in electric vehicles are the Netherlands' small size, which reduces range anxiety; a long tradition of environmental activism; high gasoline prices ( per gallon as of January 2013); and some EV leasing programs that provide free or discounted gasoline-powered vehicles for covering long distances.\n\n, about 10,200 light-duty EVs were registered. The majority of the fleet (5,700) consists of used imports from Japan and the UK. The most popular model by far is the Nissan Leaf, with 5,200 registered.\n\nThe New Zealand Government launched an Electric Vehicle Programme in May 2016, in order to encourage EV uptake. Electric vehicles in New Zealand are exempt from road user charges until at least 31 December 2021, and pay the lower petrol rates for ACC levies on motor vehicle licencing.\n\nThe stock of light-duty plug-in electric vehicles registered in Norway totaled more than 200,000 units at the end of 2017. Norway's fleet of electric cars is one of the world's cleanest, because 99% of its power comes from hydropower. Norway has the world's largest EV ownership per capita. , 21.5 EVs were registered per 1,000 people.\n\nCombined sales of new and used plug-ins captured a market share of 29.1% in 2016, rising to 39.2% in 2017. In January 2017 the electric-drive segment surpassed combined conventional internal combustion engine sales for the first time ever, achieving a combined market share of 51.4% of new car sales. Norway was the first country in the world to have all-electric cars top the new car sales monthly ranking.\n\nThe Tesla Model S was the top-selling new car four times, and the Nissan Leaf twice.\n\n, the Nissan Leaf was the plug-in car with the most units (19,150). Adding used imports from neighboring countries, 27,115 Leafs were there.\n\nNorway offered incentives to help reach the goal of 50,000 zero emission vehicles by 2018. Electrics are exempt from all non-recurring vehicle fees, making electric cars price competitive with conventional cars. BEVs are exempt from the annual road tax, public parking fees and toll payments (including domestic ferries), as well as given access to bus lanes. These incentives were to be in effect until the end of 2017 or until the goal was achieved. PHEVs have a smaller market than BEVs because they are not eligible for the same incentives. In 2013 the government reduced taxes for to improve PHEV sales.\n\nThe 50,000 vehicle target was reached on 20 April 2015 at a cost of up to 4 billion krone (around ).\n\nThe Government decided to continue the incentives through 2017, although the Parliament phased out some of the incentives.\n\nIn 2016, the government proposed its National Transport Plan 2018-2029 (NTP) with the goal that all new cars, buses and light commercial vehicles in 2025 should be zero emission vehicles. By 2030, heavy-duty vans, 75% of new long-distance buses, and 50% of new trucks must be zero emission vehicles.\n\nPakistan already has a significant market for hybrid vehicles with Honda's Vezel, Toyota's Prius and Aqua, and other models seen on the roads. The Automotive Development Policy (2016-2021) and the launch of China-Pakistan Economic Corridor (CPEC) are encouraging foreign investments for the new automobile brands to enter Pakistani market, while the leading manufacturers in the automobile industry in Pakistan are now introducing EV models with a wide range of prices which target consumers of diverse income groups. Several members of the international automobile industry including South Korea, China, and Japan also believe that Pakistan has a high potential market for EV technology, and local businesses are collaborating with them to bring EVs in Pakistan.\nOn January 2017, Dewan Motors with BMW inaugurated Pakistan's first public charging station for electric and plug-in hybrid electric vehicles in Emporium Mall, Lahore. Dewan Motors had installed another station for plug-in hybrid and electric vehicles at Dolmen Mall in Karachi on February 2017.\n\nOn 2017, Jolta International had created the first locally manufactured electric motorcycle. The company is based just outside of Bahria Town Rawalpindi, and showcased three Jolta Chargeable Electrical Motorcycles in Gwadar.\nLeading automobile manufacturers, including Super Power Motorcycles, have started introducing EV models, while Neon, a Pakistan-based motorcycle assembler, has introduced an all electric Neon M3 motorbike in Pakistan. The macho looking sports bike comes with emission free and noiseless features. Neon also assembles Electric scooters in Pakistan.\n\nThe country's first electric was launched at Silliman University by Insular Technologies in August 2007. In some major cities such as Makati, electric Jeepneys are used as well as electric tricycles (rickshaws). The Eagle G-Car is a Philippine BEV car (at a cost as low as $3,000-$6,000). E-Jeepneys were a venture of Renewable Independent Power Producer Inc., which sprang from Greenpeace and other groups, and Solarco, which in turn is a part of GRIPP.\n\nDuring a demonstration at Nanyang Technological University on February 7, 2018, Nissan Philippines' president and managing director Ramesh Narasimhan has announced that they would like to bring the Leaf to the Filipino market.\n\nIn 2009 Poland began developing charging station infrastructure in Gdańsk, Katowice, Kraków, Mielec and Warsaw with EU funds.\nIn November 2017 an electric car sharing network opened in Wrocław. The fleet is based on 2013 model of Nissan Leaf.\n\nThe biggest organization in Poland in the area of electric vehicles is Klaster Green Stream.\nThe Polish company (\"3xE - electric cars\") offer electric vehicle conversions of small city cars such as the Smart ForTwo, Citroën C1, Fiat Panda, Peugeot 107, Audi A2. \nThe converted cars have a range of about , using lithium iron phosphate () batteries and brushless DC electric motors.\n\nIn 2015 the stock of EVs reached about 2,000, consisting of 1,280 BEV cars and 720 PHEVs. EV sales totaled 1,305 units in 2015, up 260% from 2014.\n\nThe top selling model was the Mitsubishi Outlander P-HEV (229).\n\nIn 2009 Portugal worked with Renault and Nissan to create a national charging network.\n\nIn 2010 the government offered purchase incentives for the first 5,000 EVs and a separate scrappage incentive. EVs were exempted from the vehicle registration tax. These incentives were discontinued at the end of 2011.\n\n 514 EVs were registered. Registrations were led by the BMW i3. The government offered purchase incentives.\n\n 722 EVs were registered. Registrations were led by the Mitsubishi i-MiEV.\n\n, 129 EVs were registered. BMW was the EV segment leader. Adoption was slowed by high purchase prices, lack of public charging infrastructure and unclear national policies. , 74 public charging stations were operating.\n\nThe government offered purchase incentives, although the country's taxation scheme made EVs more expensive than a conventional car. EVs face a carbon surcharge and a scrap rebate, along with the annual road tax.\n\nBYD e6 electric taxis operate in Singapore, forming the largest e-taxi fleet in South East Asia. It is operated by HDT Holdings, the only e-taxi operator in Singapore, in collaboration with the ride hailing company Grab.\n\n, about 290 plug-in cars were registered, all in 2015. The Nissan Leaf was introduced in October 2013.\n, this number has increased to 375, 0.2% of all registered vehicles.\n\nGridCars is a Pretoria-based company promoting Commuter Cars, based on the TREV from Australia. The concept is to build ultra-light EVs, lessening demand on battery requirements, and making the vehicle more affordable. The Joule, designed by Cape Town-based Optimal Energy, made its debut at the 2008 Paris Motor Show, with a maximum range of .\n\nThe country does not have government incentives or subsidies to promote EVs, although new internal combustion engine vehicles face a surcharge based on engine capacity.\n\n, about 7,200 plug-in cars had been sold. 2,896 EVs were sold during the first ten months of 2016, up 12% year-on-year.\n\n, all electric models on sale were manufactured by local firms. The top selling models during 2015 were the Kia Soul EV (657) and the Samsung SM3 Z.E. (640). The Hyundai Ioniq Electric was released in July 2016.\n\nThe government offers a purchase subsidy for electric cars. Starting in 2016, the EV purchase tax surcharge was reduced, although EV drivers see various fees.\n\nThe stock of plug-in cars reached almost 6,000 plug-in as of 2015, consisting of 4,460 BEV cars and 1,490 PHEVs. The top selling model in 2015 was the Mitsubishi Outlander P-HEV (389).\n\n3,129 EVs were sold in Spain during the first three quarters of 2016. Sales continued to grow at an accelerated pace, up 79% from the same period in 2015.\n\nIn 2011 the national government initiated EV purchase incentives. Aragón, Asturias, Baleares, Madrid, Navarra, Valencia, Castilla-La Mancha, Murcia, Castile and León offered additional incentives.\n\n, 2,072 electric cars had been registered, led by the Nissan Leaf. EV sales experienced a record month in September 2015 with 471 units registered, up from only 15 in September 2014.\n\nSales of the Nissan Leaf began in 2013.\n\nNo government incentives promote EVs. Electric vehicle tax increased from 5% to 50% through the new government's Interim Budget.\n\n, a total of 50,304 light-duty plug-in vehicles have been registered since 2011, consisting of 36,405 plug-in hybrids, 12,223 all-electric cars, and 1,676 all-electric vans. The market is dominated by plug-in hybrids, representing 74.9% of plug-in car registrations through 2017. Passenger plug-ins increased their market share to 3.5% in 2016, and achieved a record of 5.2% in 2017.\n\n, the Outlander PHEV continues to rank as the all-time top selling plug-in electric car with 9,957 units registered. , the Renault Kangoo Z.E. continued as the all-time the leader in the commercial utility EV segment with 1,024 units.\n\nEffective January 2012 Sweden offered subsidies for the purchase and operation of 5,000 electric cars and other \"super green cars\" with low/no carbon emissions. The program was belatedly renewed through 2015 and again for 2016 with the addition of subsidies for electric buses.\n\n, over 12,000 EVs had been registered since 2012. During the first quarter of 2016, 1,479 EV were registered, consisting of 773 BEV cars (up 37.5% from 1Q 2015), and 706 PHEVs (up 44.1% from 1Q 2015). Registrations of plug-in cars totaled 6,288 units in 2015, up 133.9% from 2,668 in 2014.\n\nDeliveries of the Mitsubishi i MiEV. the Nissan Leaf were launched in 2011.\n\nThe government offers no subsidies or incentives for purchasing EVs. Cantons can propose special discounts on annual taxes depending on the car's efficiency label and range from 100% rebate (e.g. Solothurn) to 0%.\n\nTaiwan has a plan to ban all non-electric vehicles in the coming decades, due to concerns over air quality. \nThe plan calls for all new government vehicles and public buses to be electric by 2030, ban sales of nonelectric motorcycles by 2035, and\nban sales of nonelectric four-wheel vehicles by 2040.\n\nThe BYD e6 has been used as taxis since the first quarter of 2014.\n\nRizen Energy Co, a subsidiary of Sharich's Holding Co, is the importer and distributor for BYD vehicles in Thailand. This EV brand used to be marketed by Loxley Plc until March 2015.\n\n, 3,161 EVs and hybrids were registered in Ukraine. Registrations were led by the Nissan Leaf with 647 units.\n\nAbout 90% of EVs in Ukraine are used imports. During the first eight months of 2016, Ukrainians imported twice as many as in 2015. Imports grew to 1,550 units during the first eight months of 2016.\n\nIn August 2016, Ukrainian officials started to refuse the registration of American EVs, citing the need for certification by European rules. To comply with the certification requirements, the cars must be converted from US to European standards, which includes the replacement of a windshield, headlights and other parts, at a substantial cost.\n\nMore than 137,000 light-duty plug-in electric vehicles have been registered in the UK up until December 2017, including about 5,100 plug-in commercial vans. , the UK had 11,903 public charging 4,215 stations, of which 696 were rapid chargers.\n\nA total of 36,907 plug-in cars were registered in 2016. Sales of plug-in hybrids more than doubled the sales of all-electric cars. The plug-in market share reached 1.37% of new car sales in 2016. While overall new car registrations increased 2.3% from the previous year, plug-in registrations increased 28.6% in 2016.\nRegistrations in 2017 totaled 47,263 plug-in electric cars with a market share of 1.86% of new car sales.\n\nThe Mitsubishi Outlander P-HEV is the all-time top selling EV with 26,600 units registered through 2016, accounting for about 50% of all PHEVs sold since 2010.\n\nThe government offered purchase incentives via the Plug-in Car Grant program beginning in 2011. The program was extended to include vans in February 2012 and in October 2016 to include large electric trucks. , the number of eligible registered plug-in electric cars that have benefited with the subsidy totaled 127,509 units since the launch of the programme in 2011.\n\nIn April 2014 and December 2015, the government extended the program with modifications. Eligible ultra-low emission vehicles (ULEVs) included hydrogen fuel cell cars.\n\nSeparately the government subsidized homeowners to install charge points at home via the \"Electric Vehicle Homecharge Scheme\". EVs are exempted from London's congestion charge.\n\nSince the market launch of the Tesla Roadster in 2008, cumulative sales of highway legal plug-in electric cars in the U.S. achieved the one million unit milestone in September 2018. The 250,000 unit mark was reached in August 2014, and the 500,000 milestone in August 2016. California is the largest plug-in passenger car regional market in the country, with 491,000 units registered through October 2018, representing almost half of national sales. The other nine states that follow California's Zero Emission Vehicle (ZEV) regulations accounted for another 10%.\n\nA total 157,181 plug-in cars were sold nationwide in 2016, up 37.6% from 2015, and 194,479 in 2017, up 23.7% from 2016. The plug-in segment passed the 1% market share for the first time in 2017, with 1.13% of the country's total new car sales, up from 0.90% in 2016.\n\n, the Chevrolet Volt is the all-time best selling plug-in car with 133,838 units, followed by the Tesla Model S with 118,817, and the Nissan Leaf with 114,827 units. Sales in 2017 were led by the Tesla Model S with about 26,500 units, the top selling plug-in car for the third year running, followed by the Chevrolet Bolt (23,297), Tesla Model X (~21,700), Toyota Prius Prime (20,936), and the Chevrolet Volt (20,349), together accounting for 58% of total sales in 2017.\n\nCalifornia established a program to reduce air pollution in the 1980s. Under pressure from manufacturers, the program was revised to offer only modest support of zero-emission vehicles to promote research and development, and greater support for partial zero-emissions vehicles (PZEVs). Many manufacturers then terminated their electric car programs.\n\nThe federal tax credit for new plug-in electric vehicles (PEVs) is worth between and depending on battery capacity. Several states have established additional incentives. The government pledged in federal grants to support the development of next-generation transport, and million for the installation of charging infrastructure.\n\n"}
{"id": "53794180", "url": "https://en.wikipedia.org/wiki?curid=53794180", "title": "Elizabeth Rummel", "text": "Elizabeth Rummel\n\nElizabeth Rummel (\"Lizzie\"; born 19 February 1897 in Munich; died 1980) was a German-Canadian environmentalist. In 1980, she was made a Member of the Order of Canada for her mountaineering and environmental contributions, and \"enrich[ing] her country by sharing her deep love of the Rocky Mountains with all who meet her.\"\n"}
{"id": "35577514", "url": "https://en.wikipedia.org/wiki?curid=35577514", "title": "Fawley Tunnel", "text": "Fawley Tunnel\n\nFawley Tunnel is a diameter, long tunnel under Southampton Water between Fawley Power Station and Chilling near Warsash. It was constructed between 1962 and 1965 to carry two 400 kV circuits as part of the National Grid.\n\n"}
{"id": "53502558", "url": "https://en.wikipedia.org/wiki?curid=53502558", "title": "February 2015 North American cold wave", "text": "February 2015 North American cold wave\n\nThe February 2015 North American cold wave was an extreme weather event that affected most of Canada and the eastern half of the United States. Following an earlier cold wave in the winter, the period of below-average temperatures contributed to an already unusually cold winter for the Eastern U.S. Several places broke their records for their coldest February on record, while some areas came very close. The cause of the cold wave was due to the polar vortex advancing southwards into the eastern parts of the U.S, and even making it as far south as the Southeast, where snow is rare. By the beginning of March, although the pattern did continue for the first week, it abated and retreated near the official end of the winter.\n\nIn addition to the extremely cold weather, multiple winter storms affected nearly the entire United States, especially in the snow-weary Northeast, which had already seen nearly of snow in the latter part of January; this was added to by roughly more snow, leading to Boston having its highest seasonal snowfall on record. \n\nLike most normal cold waves, the cold wave was caused by the southwards movement of the polar vortex into the United States due to changes in the jet stream in early February 2015. However, unlike most which last for a few days, this one remained for much of the entire month. This was partly due to the Ridiculously Resilient Ridge, which persisted over parts of Alaska for much of the month, essentially keeping the jet stream pattern \"locked\" for several weeks. This allowed for bitterly cold air masses to migrate southwards into the eastern part of the country, leading to well-below average temperatures.\n\nThe average temperature in Boston for January was below the 1981–2010 normal, and the average temperature in February was , which was below the 1981–2010 normal, making it the second-coldest month of any month all-time, behind February 1934. March was below average. By the end of a period spanning from the beginning of February to the end of December, Worcester, Massachusetts saw a record of snowfall, breaking the record set in 2004–05, which was over the average. Hartford and Providence saw similar below-average temperatures for the two months, with Hartford's February finish of besting February 1934 to become the coldest month of any month all-time in record keeping.\n\nThe average temperature in Bangor, Maine for February was , about below normal, breaking the old record of set in January 1994. Portland, Maine also saw a record coldest average monthly temperature of in February. On February 24, the temperature at Dulles International Airport in Virginia fell to dipped down to , breaking the previous record of set in 1967.\n\nRutland, Vermont, saw a record averaged cold for February of , breaking its previous record of set in 1934. Montpelier, Vermont realized its coldest February with an average temperature of , below the 1979 record of .\n\nThe average February temperature in Syracuse, New York, was degrees below normal at , breaking by 3 degrees the record set in February 1934.\n\nThe average temperature in Buffalo, New York set a record in February for its all-time coldest month with an average temperature of . breaking the prior record set in 1934 of . Before that, the previous coldest February was in 1875 with an average temperature of . It was also the second time in history that the entire month of February was below freezing. Other cities that broke cold weather records for February were Cleveland, Ohio at , while Chicago, Illinois tied its February 1875 record at . Rochester, New York also set a record for coldest month overall.\n\nOhio experienced the coldest winter since 1977–78 with an average temperature of below normal.\n\nToronto, Ontario recorded its coldest month on record in February with at Pearson Airport, tying with February 1875 (recorded in downtown) and beating the previous record of set in January 1994.\n\nIn Quebec, Montreal experienced its coldest February on record with an extended cold spell and an average temperature of .\n\nMost of the northern half of the United States and even parts of Canada saw several winter storms impact them; each of them had somewhat unique traits. At the start of the month, a major snowstorm was moving across the country, having previously brought blizzard conditions to the Midwest, especially in Chicago, Illinois, where more than a foot of snow was recorded. This storm continued to dump large amounts of snow as it progressed into the Northeast and New England, before finally exiting offshore.\n\nLess than a week later, another winter storm struck New England, with up to nearly of snow recorded in Boston, Massachusetts over a period of two days. This was the beginning of a three-week streak of winter storms that would progress into the Northeast (with the expectation of one system which went further south).\n\nAround the middle of the month, near Valentine's Day, a powerful blizzard struck the Northeast again, bringing strong winds and more heavy snowfall. Afterwards, as the storm exited, it ushered in the coldest air to impact the Northeast in decades, with temperatures dipping as far as below average. Several places broke record lows for February, and temperatures even dipped below in a good part of the Northeast.\n\nSeveral more winter storms followed afterwards, but one of the more notable ones was the one that occurred from February 25–26. This particular storm took an unusual track into the Southeastern United States as the jet stream along with the polar vortex pushed even further southward, resulting in heavy snowfall in states that rarely see it at all, this included Mississippi, Alabama, Georgia, and the Carolinas. Up to fell in the hardest hit areas, which was in North Carolina and Virginia.\n\n"}
{"id": "61260", "url": "https://en.wikipedia.org/wiki?curid=61260", "title": "Filling station", "text": "Filling station\n\nA filling station is a facility that sells fuel and engine lubricants for motor vehicles. The most common fuels sold in the 2010s are gasoline (\"gasoline\" or \"gas\" in the U.S. and Canada, generally \"petrol\" elsewhere) and diesel fuel. A filling station that sells only electric energy is also known as a charging station, while a typical filling station can also be known as a fuelling, gas station (United States and Canada), gasoline stand or SS (Japan), petrol pump or petrol bunk (India), petrol garage, petrol station (Australia, Hong Kong, New Zealand, Singapore, South Africa, United Kingdom and Ireland), service station (Australia, Japan, New Zealand and United Kingdom), a services (United Kingdom), or servo (Australia).\n\nFuel dispensers are used to pump petrol/gasoline, diesel, compressed natural gas, CGH2, HCNG, LPG, liquid hydrogen, kerosene, alcohol fuel (like methanol, ethanol, butanol, propanol), biofuels (like straight vegetable oil, biodiesel), or other types of fuel into the tanks within vehicles and calculate the financial cost of the fuel transferred to the vehicle. Fuel dispensers are also known as bowsers (in some parts of Australia), petrol pumps (in most Commonwealth countries) or gas pumps (in North America). Besides fuel dispensers, one other significant device which is also found in filling stations and can refuel certain (compressed-air) vehicles is an air compressor, although generally these are just used to inflate car tyres. Also, many filling stations incorporate a convenience store, which like most other buildings generally have electricity sockets; hence plug-in electric vehicles can be recharged.\n\nThe convenience stores found in filling stations typically sell candy, soft drinks, snacks and, in some cases, a small selection of grocery items, such as milk. Some also sell propane or butane and have added shops to their primary business. Conversely, some chain stores, such as supermarkets, discount stores, warehouse clubs, or traditional convenience stores, have provided filling stations on the premises.\n\nThe term \"gas station\" is widely used in the United States, Canada and the English-speaking Caribbean, where the fuel is known as \"gasoline\" or \"gas\" as in \"gas pump\". In some regions of Canada, the term \"gas bar\" is used. Elsewhere in the English-speaking world, mainly in the Commonwealth, the fuel is known as \"petrol\", and the term \"petrol station\" or \"petrol pump\" is used. In the United Kingdom and South Africa \"garage\" is still commonly used. Similarly, in Australia, the term \"service station\" (\"servo\") describes any petrol station. In Japanese English, a commonly used term is \"gasoline stand\", although the abbreviation \"SS\" (for \"s\"ervice \"s\"tation) is also used. In Indian English, it is called a \"petrol pump\" or a \"petrol bunk\". In some regions of America and Australia, many filling stations have a mechanic on duty, but this is uncommon in other parts of the world.\n\n\n\nThe first filling station was the city pharmacy in Wiesloch, Germany, where Bertha Benz refilled the tank of the first automobile on its maiden trip from Mannheim to Pforzheim back in 1888. Shortly thereafter other pharmacies sold gasoline as a side business. Since 2008 the Bertha Benz Memorial Route commemorates this event.\n\nThe first \"posto de gasolina\" of South America was opened in Santos, Brazil, in 1920. It was located in the Ana Costa Ave., in front of the beach, in a corner that nowadays is located the Hotel Atlantico. It was an Esso Gas Station, brought by Antonio Duarte Moreira, a taxi entrepeneur. \n\nThe increase in automobile ownership after Henry Ford started to sell automobiles that the middle class could afford resulted in an increased demand for filling stations. The world's first purpose-built gas station was constructed in St. Louis, Missouri in 1905 at 420 S. Theresa Avenue. The second gas station was constructed in 1907 by Standard Oil of California (now Chevron) in Seattle, Washington, at what is now Pier 32. Reighard's Gas Station in Altoona, Pennsylvania, claims that it dates from 1909 and is the oldest existing gas station in the United States. Early on, they were known to motorists as \"filling stations\".\n\nThe first \"drive-in\" filling station, Gulf Refining Company, opened to the motoring public in Pittsburgh on December 1, 1913, at Baum Blvd & St Clair's Street (Walter's Automotive Shop was located here on the 100th anniversary). Prior to this, automobile drivers pulled into almost any general or hardware store, or even blacksmith shops in order to fill up their tanks. On its first day, the station sold 30 gallons of gasoline at 27 cents per gallon. This was also the first architect-designed station and the first to distribute free road maps. The first alternative fuel station was opened in San Diego, California, by Pearson Fuels in 2003.\n\nIn Russia, the first filling stations appeared in 1911, when the Imperial Automobile Society signed an agreement with the partnership \"Br. Nobel\". By 1914 about 440 filling stations functioned in major cities across the country.\n\nIn the mid-1960s in Moscow there were about 250 stations. A significant boost in retail network development occurred with the mass launch of the car \"Zhiguli\" at the Volga Automobile Plant, which was built in Tolyatti in 1970. Gasoline for other than non-private cars was sold for ration cards only. This type of payment system stopped in the midst of \"Perestroika\" in the early 1990s.\n\nToday, since the saturation of automobile filling stations in Russia is insufficient and lags behind the leading countries of the world, today there is a need to accommodate new stations in the cities and along the roads of different levels.\n\nMost filling stations are built in a similar manner, with most of the fueling installation underground, pump machines in the forecourt and a point of service inside a building. Single or multiple fuel tanks are usually deployed underground. Local regulations and environmental concerns may require a different method, with some stations storing their fuel in container tanks, entrenched surface tanks or unprotected fuel tanks deployed on the surface. Fuel is usually offloaded from a tanker truck into the tanks through a separate valve, located on the filling station's perimeter. Fuel from the tanks travels to the dispenser pumps through underground pipes. For every fuel tank, direct access must be available at all times. Most tanks can be accessed through a service canal directly from the forecourt.\n\nOlder stations tend to use a separate pipe for every kind of available fuel and for every dispenser. Newer stations may employ a single pipe for every dispenser. This pipe houses a number of smaller pipes for the individual fuel types. Fuel tanks, dispenser and nozzles used to fill car tanks employ vapor recovery systems, which prevents releases of vapor into the atmosphere with a system of pipes. The exhausts are placed as high as possible. A vapor recovery system may be employed at the exhaust pipe. This system collects the vapors, liquifies them and releases them back into the lowest grade fuel tank available.\n\nThe forecourt is the part of a filling station where vehicles are refueled. Fuel dispensers are placed on concrete plinths, as a precautionary measure. Additional elements may be employed, including metal barriers. The area around the fuel dispensers must have a drainage system. Since fuel sometimes spills on the ground, as little of it as possible should penetrate the soil. Any liquids present on the forecourt will flow into a channel drain before it enters a petrol interceptor which is designed to capture any hydrocarbon pollutants and filter these from rainwater which may then proceed to a foul sewer, stormwater drain or to ground.\n\nIf a filling station allows customers to pay at the register, the data from the dispensers may be transmitted via RS232, RS485 or Ethernet to the point of sale, usually inside the filling station's building, and fed into the station's cash register operating system. The cash register system gives a limited control over the fuel dispenser, and is usually limited to allowing the clerks to turn the pumps on and off. A separate system is used to monitor the fuel tank's status and quantities of fuel. With sensors directly in the fuel tank, the data is fed to a terminal in the back room, where it can be downloaded or printed out. Sometimes this method is bypassed, with the fuel tank data transmitted directly into an external database.\n\nThe underground modular filling station is a construction model for filling stations that was developed and patented by U-Cont Oy Ltd in Finland in 1993. Afterwards the same system was used in Florida, USA. Above-ground modular filling stations were built in the 1980s in eastern Europe and especially in Soviet Union, but they were not built in other parts of Europe due to the stations' lack of safety in case of fire.\n\nThe construction model for underground modular filling station makes the installation time shorter, designing easier and manufacturing less expensive. As a proof of the model's installation speed an unofficial world record of filling station installation was made by U-Cont Oy Ltd when a modular filling station was built in Helsinki, Finland in less than three days, including groundwork. The safety of modular filling stations has been tested in a filling station simulator, in Kuopio, Finland. These tests have included for instance burning cars and explosions in the station simulator.\n\nIn the United States and Canada, there are generally two marketing types of filling stations: premium brands and discount brands.\n\nFilling stations with \"premium brands\" sell well-recognized and often international brands of gasoline, including Exxon and its Esso brand, Phillips 66/Conoco/76, Chevron, Mobil, Shell, Husky Energy, Sunoco (US), BP, Valero and Texaco. Non-international premium brands include Petrobras, Petro-Canada (owned by Suncor Energy Canada), QuikTrip, Hess, Sinclair, and Pemex. Premium brand stations accept credit cards, often issue their own company cards (a.k.a. fuel cards or fleet cards) and may charge higher prices. In some cases, fuel cards for customers with a lower fuel consumption are ordered not directly from an oil company, but from an intermediary. Many premium brands have fully automated pay-at-the-pump facilities. Premium gas stations tend to be highly visible from highway and freeway exits, utilizing tall signs to display their brand logos.\n\n\"Discount brands\" are often smaller, regional chains or independent stations, offering lower prices on gasoline. Most purchase wholesale commodity gasoline from independent suppliers or from the major petroleum companies. Lower-priced gas stations are also found at some supermarkets (Albertsons, Kroger, Ingles, Lowes Foods, Giant, Weis Markets, Safeway, Hy-Vee, Vons, Meijer, Loblaws/Real Canadian Superstore, and Giant Eagle), convenience stores (7-Eleven, Circle K, Cumberland Farms, Quick Chek, Road Ranger, Sheetz and Wawa), discount stores (Walmart, Canadian Tire) and warehouse clubs (Costco, Sam's Club, and BJ's Wholesale Club). At some stations (such as Vons, Costco, BJ's Wholesale Club, or Sam's Club), consumers are required to hold a special membership card in order to be eligible for the discounted price, or pay only with the chain's cash card, debt card or a credit card issuer exclusive to that chain. In some areas, such as New Jersey, this practice is illegal, and stations are required to sell to all at the same price. Some convenience stores, such as 7-Eleven and Circle K, have co-branded their stations with one of the premium brands. After the Gulf Oil company was sold to Chevron, northeastern retail units were sold off as a chain, with Cumberland Farms controlling the remaining Gulf Oil outlets in the United States.\n\nSome countries have only one brand of filling station. In Mexico, where the oil industry is state-owned and prices are regulated, the country's main operator of filling stations is Pemex. In Malaysia, Shell is the dominant player by number of stations, with government-owned Petronas coming in second. In Indonesia, the dominant player by number of stations is the government-owned Pertamina, although other companies such as Total and Shell are increasingly found in big cities such as the capital Jakarta or Surabaya.\n\nSome companies, such as Shell, use their brand worldwide, however, Chevron uses its inherited brand Caltex in Asia Pacific, Australia and Africa, and its Texaco brand in Europe and Latin America. ExxonMobil uses its Exxon and Mobil brands but is still known as Esso (the forerunner company name, Standard Oil or S. O.) in many places, most noticeably in Canada. In Brazil, the main operators are Petrobras Distribuidora and Ipiranga, but Esso and Shell are also present. In the United Kingdom, the two largest are BP and Shell. The \"Big Four\" supermarket chains, Morrisons, Sainsburys, Asda and Tesco, all operate filling stations, as well as some of the smaller supermarkets such as The Co-operative Group and Waitrose. Indian Oil operates approximately 15,000 filling stations in India. In Japan, the main operators are Cosmo Oil, Idemitsu,\nJXTG Nippon Oil & Energy (under the brand names ENEOS, Express and General) and Mitsubishi Group (operates self-service stations under the Lawson convenience store branding), although foreign brands such as Esso, Mobil (both operated by JXTG Nippon Oil under licence from ExxonMobil) and Shell (Showa Shell Sekiyu) are also present.\n\nIn British Columbia and Alberta, it is a legal requirement that customers either pre-pay for the fuel or pay at the pump. The law is called \"Grant's Law\" and is intended to prevent \"gas-and-dash\" crimes.\nIn other provinces payment after filling is permitted and widely available, though some stations may require either a prepayment or a payment at the pump at night hours.\n\nIn the Republic of Ireland, most filling stations allow customers to pump fuel before settling the bill. Some filling stations have pay-at-the-pump facilities.\n\nMost service stations allow the customer to pump the fuel before paying; this is particularly the case in the smaller towns and cities in New Zealand. In recent years some service stations have required customers to purchase their fuel first. It is quite common for customers to hand the cash to the attendant on the forecourt if they are paying for a set amount of fuel and have no change. Some supermarkets have their own forecourts which are unmanned and payment is pay at the pump only. Customers at the supermarket will receive a discount voucher which offers discounted fuel at their forecourt. The amount of discount varies depending on the amount spent on groceries at the supermarket, but normally starts at 4 cents a litre.\n\nA large majority of stations allow customers to pay with a chip and pin payment card or pay in the shop. Many have a pay at the pump system, where customers can enter their PIN prior to filling.\n\nPre-payment, most commonly at the pump, is the norm in the U.S. Customers may typically pay either at the pump or inside the gas station's shop/pay station. Modern gas stations have pay-at-the-pump capabilities: in most cases credit, debit, ATM cards, fuel cards and fleet cards are accepted. Occasionally a station will have a pay-at-the-pump-only period per day, when attendants are not present, often at night, and some stations are pay-at-the-pump-only 24 hours a day.\n\nFilling stations typically offer one of three types of service to their customers: full service, minimum service or self-service.\n\n\nIn Brazil, self-service fuel filling is illegal, due to a federal law enacted in 2000. The law was introduced by Federal Deputy Aldo Rebelo, who claims it saved 300,000 fuel attendant jobs across the country.\n\nBefore 1998, filling stations in Japan were entirely full-service stations. Self-service filling stations were legalised in Japan in 1998 following the abolishment of the Special Petroleum Law which led to the deregulation of the petroleum industry in Japan. Under current safety regulations, while motorists are able to self-dispense fuel at self-service stations, generally identified in Japanese as , at least one fuel attendant must be on hand to keep watch over potential safety violations and to render assistance to motorists whenever necessary. \n\nIn the past, filling stations in the United States offered a choice between \"full service\" and \"self service\". Before 1970, full service was the norm, and self-service was rare. Today, few stations advertise or provide full service. Full service stations are more common in wealthy and upscale areas. The cost of full service is usually assessed as a fixed amount per U.S. gallon.\n\nThe first self-service station in the United States was in Los Angeles, opened in 1947 by Frank Urich. In Canada, the first self-service station opened in Winnipeg, Manitoba, in 1949. It was operated by the independent company Henderson Thriftway Petroleum, owned by Bill Henderson.\n\nAll stations in New Jersey and Oregon offer only full service and mini service; attendants are required to pump gas since customers are prohibited by law in both states from pumping gas themselves. The only exceptions are at the filling stations next to Joint Base McGuire-Dix-Lakehurst in Wrightstown, New Jersey and next to the Indian reservation casinos in both Pendleton and Grand Ronde, Oregon where self-service is permitted at these three locations. New Jersey prohibited self-service gasoline in 1949 after lobbying by service station owners. Proponents of the prohibition cite safety and jobs as reasons to keep the ban.\nLikewise, the 1951 Oregon statute prohibiting self-service gasoline lists 17 different justifications, including the flammability of gas, the risk of crime from customers leaving their vehicles, the toxic fumes emitted by gasoline, and the jobs created by requiring mini service. In addition, the ban on self-service gasoline is seen as part of Oregonian culture. One commentator noted, \"The joke is when babies are born in Oregon, the doctor slaps their bottom, 'No self-serve and no sales tax'... It's as much a cultural issue as an economic issue. It's a way of life.\" However, recent years have shown that this opinion might be changing, as a 2014 Public Policy Poll showed that although self-serve was favored by a narrow margin of all Oregonians, Oregonians under 45 favored self-serve gas by 53 percent to 33 percent.\nIn 1982 Oregon voters rejected a ballot measure sponsored by the service station owners, which would have legalized self-service gas. Oregon legislators passed a bill that was later signed into law by the Governor in May 2017 to allow self-service for counties with a total population of 40,000 or less beginning in January 2018.\n\nThe town of Huntington, New York prohibits gasoline self-service to save jobs. The ban came into effect in the early 1970s during a recession.\n\nThe constitutionality of the self-service bans has been disputed. The Oregon statute was brought into court in 1989 by ARCO, and the New Jersey statute was challenged in court in 1950 by a small independent service station, Rein Motors. Both challenges failed. Former New Jersey governor Jon Corzine sought to lift the ban on self-service for New Jersey. He asserted that it would be able to lower gas prices, but some New Jerseyans argued that it could cause drawbacks, especially unemployment. However, Phil Murphy's plan on $15 minimum wage will increase expenses, may cause self service.\n\nIn New Jersey and Oregon, it is legal for customers to pump their own diesel (although not every station permits diesel customers to do so; truck stops typically do). In Oregon, \"certain nonretail\" customers may also pump their own fuel.\n\nMany stations provide toilet facilities for customer use, as well as squeegees and paper towels for customers to clean their vehicle's windows. Discount stations may not provide these amenities in some countries.\n\nStations typically have an air compressor (some with a built-in or provided handheld tire-pressure gauge) to inflate tires and a hose to add water to vehicle radiators. Some air compressor machines are free of charge, while others charge a small fee to use (typically 50 cents to a dollar in North America). In many U.S. states, state law requires that paying customers must be provided with free air compressor service. In most cases, a token provided by the attendant is used in lieu of coins.\n\nMany filling stations have integrated convenience stores which sell food, beverages, and often cigarettes, lottery tickets, motor oil, and auto parts. Prices for these items tend to be higher than they would be at a supermarket or discount store.\n\nMany stations, particularly in the United States, have a fast food outlet inside. These are usually \"express\" versions with limited seating and limited menus, though some may be regular-sized and have spacious seating. Larger restaurants are common at truck stops and toll road service plazas.\n\nIn some U.S. states, beer, wine, and liquor are sold in gas stations, though this practice varies according to state law (\"see Alcohol laws of the United States by state\"). Nevada also allows the operation of slot and video poker machines without time restrictions.\n\nVacuum cleaners, often coin-operated, are a common amenity to allow the cleaning of vehicle interiors, either by the customer or by an attendant.\n\nSome stations are equipped with car washes. Car washes are sometimes offered free of charge or at a discounted price with a certain amount of gas purchased. Conversely, some car washes operate filling stations to supplement their businesses.\n\nFrom approximately 1920 to 1980, many service stations in the US provided free road maps affiliated with their parent oil companies to customers. This practice fell out of favor due to the 1970s energy crisis.\n\nIn European Union member states, petrol prices are much higher than in North America due to higher fuel excise or taxation, although the base price is also higher than in the U.S. Occasionally, price rises trigger national protests. In the UK, a large-scale protest in August and September 2000, known as 'The Fuel Crisis', caused widescale havoc not only across the UK, but also in some other EU countries. The UK Government eventually backed down by indefinitely postponing a planned increase in fuel duty. This was partially reversed during December 2006 when then-Chancellor of the Exchequer Gordon Brown raised fuel duty by 1.25 pence per litre.\n\nSince 2007, petrol prices in the UK rose by nearly 40 pence per litre, going from 97.3 pence per litre in 2007 to 136.8 pence per litre in 2012.\n\nIn much of Europe, including the UK, France and Germany, filling stations operated by large supermarket chains usually price fuel lower than stand-alone filling stations. In most of mainland Europe, sales tax is lower on diesel fuel than on petrol, and diesel is accordingly the cheaper fuel: in the UK and Switzerland, diesel has no tax advantage and retails at a higher price by quantity than petrol (offset by its higher energy yield).\n\nIn 2014, according to Eurostat, the mean EU28 price was 1.38 €/L for euro-super 95 (gazoline), 1.26 €/L for gazole (for diesel car).\nThe less expensive gasoline is in Estonia at 1.10 €/L, and the most expensive at 1.57 €/L in Italy.\nThe less expensive gasoil (diesel) is in Estonia at 1.14 €/L, and the most expensive at 1.54 €/L in the UK.\nThe less expensive GPL is in Belgium at 0.50 €/L, and the most expensive at 0.83 €/L in France.\n\nNearly all filling stations in North America advertise their prices on large signs outside the stations. Some locations have laws requiring such signage.\n\nIn Canada and the United States, federal, state or provincial, and local sales taxes are usually included in the price, although tax details are often posted at the pump and some stations may provide details on sales receipts. Gas taxes are often ring-fenced (dedicated) to fund transportation projects such as the maintenance of existing roads and the construction of new ones.\n\nIn the United States, the states of California and Hawaii typically have the highest gasoline prices, while the lowest prices are usually found in oil-producing states like Oklahoma and Texas. In Canada, prices are typically highest in the provinces of British Columbia and Quebec, and the lowest in the oil-producing province of Alberta. The provinces of Prince Edward Island (PEI), Newfoundland and Labrador, New Brunswick, and Nova Scotia have instituted gasoline price regulation, which is intended to protect small rural gas stations from low profits due to low sales volume.\n\nIndividual gas stations in the United States have little if any control over gasoline prices. The wholesale price of gasoline is determined according to area by oil companies which supply the gasoline, and their prices are largely determined by the world markets for oil. Individual gas stations are unlikely to sell gasoline at a loss, and the profit margin—typically between 7 and 11 cents a U.S. gallon—that they make from gasoline sales is limited by competitive pressures: a gas station which charges more than others will lose customers to them. Most gas stations try to compensate by selling higher-margin food products in their convenience stores.\n\nEven with oil market fluctuations, prices for gasoline in the United States are among the lowest in the industrialized world; this is principally due to lower taxes. While the sales price of gasoline in Europe is more than twice that in the United States, the price of gas excluding taxes is nearly identical in the two areas. Some Canadians and Mexicans in communities close to the U.S. border drive into the United States to purchase cheaper gasoline.\n\nDue to heavy fluctuations of gasoline price in the United States, some gas stations offer their customers the option to buy and store gas for future uses, such as the service provided by First Fuel Bank.\n\nIn order to save money, some consumers in Canada and the United States inform each other about low and high prices through the use of gasoline price websites. Such websites allow users to share prices advertised at filling stations with each other by posting them to a central server. Consumers then may check the prices listed in their geographic area in order to select the station with the lowest price available at the time. Some television and radio stations also compile pricing information via viewer and listener reports of pricing or reporter observations and present it as a regular segment of their newscasts, usually before or after traffic reports. These price observations must usually be made by reading the pricing signs outside stations, as many companies do not give their prices by telephone due to competitive concerns. In Canada, it is against federal law to provide any gas pricing from a gas station via phone. It is a criminal offence to have written or verbal arrangements with competitors, suppliers or customers for:\n\n\nGas stations must never hold discussions with other competitors regarding pricing policies and methods, terms of sale, costs, allocation of markets or boycotts of our petroleum products.\n\nIn other energy-importing countries such as Japan, gasoline and petroleum product prices are higher than in the United States because of fuel transportation costs and taxes.\n\nOn the other hand, some of the major oil-producing countries such as the Gulf States, Iran, Iraq, and Venezuela provide subsidized fuel at well-below world market prices. This practice tends to encourage heavy consumption.\n\nHong Kong has some of the highest pump prices in the world, but most customers are given discounts as card members.\n\nIn Western Australia a programme called Fuelwatch requires most WA filling stations to notify their \"tomorrow prices\" by 2pm each day; prices are changed at 6am each morning, and must be held for 24 hours. Each afternoon, the prices for the next day are released to the public and the media, allowing consumers to decide when to fill up.\n\nA service station or \"servo\" is the terminology predominantly used in Australia and New Zealand. In Australia, a \"servo\" is commonly used to describe any facility where motorists can refuel their cars.\n\nIn New Zealand a filling station is often referred to as a service station, garage, or petrol station, even though the filling station may not offer mechanical repairs or assistance with dispensing fuel. Levels of service available include full service, for which assistance in dispensing fuel is offered, as well as offers to check tyre pressure or clean vehicle windscreens. This type of service is becoming uncommon in New Zealand, particularly Auckland. Further south of Auckland, many filling stations offer full service. There is also help service or assisted service, for which customers must request assistance before it is given, and self-service, for which no assistance is available.\n\nIn the U.S., a filling station that also offers services such as oil changes and mechanical repairs to automobiles is called a service station. Until the 1970s the vast majority of gas stations were service stations; now only a minority are. These stations typically offered free air for inflating tires, as compressed air was already on hand to operate the repair garage's pneumatic tools.\n\nThis kind of business provided the name for the U.S. comic strip \"Gasoline Alley\", where a number of the characters worked.\n\n\nIn the UK, a 'service station' refers to much larger facilities, usually attached to motorways (see rest area) or major truck routes, which provide food outlets, large parking areas, and often other services such as hotels, arcade games, and shops in addition to 24-hour fuel supplies and a higher standard of restrooms. Fuel is typically more expensive from these outlets due to their premium locations. UK service stations do not usually repair automobiles.\n\nThis arrangement occurs on many toll roads and some interstate freeways and is called an oasis or service plaza. In many cases, these centres might have a food court or restaurants. In the United Kingdom these are called Motorway service areas.\n\nOften, the state government maintains public rest areas directly connected to freeways, but does not rent out space to private businesses, as this is specifically prohibited by law via the Interstate Highway Act of 1956 which created the national Interstate Highway System, except sites on freeways built before January 1, 1960, and toll highways that are self-supporting but have Interstate designation, under a grandfather clause. As a result, such areas often provide only minimal services such as restrooms and vending machines.\n\nPrivate entrepreneurs develop additional facilities, such as truck stops or travel centers, restaurants, gas stations, and motels in clusters on private land adjacent to major interchanges. In the U.S., Pilot Flying J and TravelCenters of America are two of the most common full-service chains of truck stops. Because these facilities are not directly connected to the freeway, they usually have huge signs on poles high enough to be visible by motorists in time to exit from the freeway. Sometimes, the state also posts small official signs (normally blue) indicating what types of gas stations, restaurants, and hotels are available at an upcoming exit; businesses may add their logos to these signs for a fee.\n\nIn Canada , the province of Ontario has stops along two of its 400-series highways, the 401 and the 400, traditionally referred to as \"Service Centres\" , but recently renamed \"ONroute\" as part of a full rebuild of the sites. Owned by the provincial government, but leased to private operator Host Kilmer Service Centres, they contain food courts, convenience stores, washrooms, and co-located gas and diesel bars with attached convenience stores. Food providers include Tim Hortons (at all sites), A&W, Wendy's and Pizza Pizza. At most sites fuel is sold by Canadian Tire, with a few older Esso gas bars at earlier renovated locations.\n\nIn Australia, gasoline is unleaded, and available in 91 (normally with up to 10% ethanol), 95, 98 and 100 octane (names of various petrols differ from brand to brand). Fuel additives for use in cars designed for leaded fuel are available at most filling stations.\n\nIn Canada, the most commonly found octane grades are 87 (regular), 89 (mid grade) and 91 (premium), using the same \"(R+M)/2 Method\" used in the U.S. (see below).\n\nIn China, the most commonly found octane grade is RON 91 (regular), 93 (mid grade) and 97 (premium). Almost all of the fuel has been unleaded since 2000. In some premium gas stations in large cities, such as Petrol China and SinoPec, RON 98 gas is sold for racing cars.\n\nIn Europe, gasoline is unleaded and available in 95 RON (\"Eurosuper\") and, in nearly all countries, 98 RON (\"Super Plus\") octanes; in some countries 91 RON octane gasoline is offered as well. In addition, 100 RON petrol is offered in some countries in continental Europe (Shell markets this as \"V-Power Racing\"). Some stations offer 98 RON with lead substitute (often called \"Lead-Replacement Petrol, or LRP).\n\nIn New Zealand, gasoline is unleaded, and most commonly available in 91 RON (\"Regular\") and 95 RON (\"Premium\"). 98 RON is available at selected BP (\"Ultimate\") and Mobil (\"Synergy 8000\") service stations instead of the standard 95 RON. 96 RON was replaced by 95 RON, and subsequently abolished in 2006. Leaded fuel was abolished in 1996.\n\nIn the UK the most common gasoline grade (and lowest octane generally available) is 'Premium' 95 RON unleaded. 'Super' is widely available at 97 RON (for example \"Shell V-Power\", \"BP Ultimate\"). Leaded fuel is no longer available.\nIn the United States all motor vehicle gasoline is unleaded and is available in several grades with different octane rating; 87 (Regular), 89 (Mid-Grade), and 93 (Premium) are typical grades. The maximum octane rating in California is generally 91. At high altitudes in the Mountain States and the Black Hills of South Dakota, regular unleaded can be as low as 85 octane; this practice is now increasingly controversial, since it was instituted when most cars had carburetors instead of the fuel injection and electronic engine controls standard in recent decades.\n\nIn the U.S. gasoline is described in terms of its \"pump octane\", which is the mean of their \"RON\" (Research Octane Number) and \"MON\" (Motor Octane Number). Labels on gasoline pumps in the U.S. typically describe this as the \"(R+M)/2 Method\". Some nations describe fuels according to the traditional RON or MON ratings, so octane ratings cannot always be compared with the equivalent U.S. rating by the \"(R+M)/2 method\".\n\nIn Europe and Australia, the customer selects one of several color-coded nozzles depending on the type of fuel required. The filler pipe of unleaded fuel is smaller than the one for fuels for engines designed to take leaded fuel. The tank filler opening has a corresponding diameter; this prevents inadvertently using leaded fuel in an engine not designed for it, which can damage a catalytic converter. \nIn most stations in Canada and the USA, the pump has a single nozzle and the customer selects the desired octane grade by pushing a button. Some pumps require the customer to pick up the nozzle first, then lift a lever underneath it; others are designed so that lifting the nozzle automatically releases a switch. Some newer stations have separate nozzles for different types of fuel. Where diesel fuel is provided, it is usually dispensed from a separate nozzle even if the various grades of gasoline share the same nozzle.\n\nMotorists occasionally pump gasoline into a diesel car by accident. The converse is almost impossible because diesel pumps have a large nozzle with a diameter of which does not fit the filler, and the nozzles are protected by a lock mechanism or a liftable flap. Diesel fuel in a gasoline engine — while creating large amounts of smoke — does not normally cause permanent damage if it is drained once the mistake is realized. However even a liter of petrol added to the tank of a modern diesel car can cause irreversible damage to the injection pump and other components through a lack of lubrication. In some cases, the car has to be scrapped because the cost of repairs exceeds its residual value. The issue is not clear-cut as older diesels using completely mechanical injection can tolerate some gasoline — which has historically been used to \"thin\" diesel fuel in winter.\n\nIn most countries, petrol stations are subjected to guidelines and regulations which exist to minimise the potential of fires, and increase safety.\n\nIt is prohibited to use open flames and, in some places, mobile phones on the forecourt of a filling station because of the risk of igniting gasoline vapor. In the U.S. the fire marshal is responsible for regulations at the gas pump. Most localities ban smoking, open flames and running engines. Since the increased occurrence of static-related fires many stations have warnings about leaving the refueling point.\n\nCars can build up static charge by driving on dry road surfaces. However many tire compounds contain enough carbon black to provide an electrical ground which prevents charge build-up. Newer \"high mileage\" tires use more silica and can increase the buildup of static. A driver who does not discharge static by contacting a conductive part of the car will carry it to the insulated handle of the nozzle and the static potential will eventually be discharged when this purposely-grounded arrangement is put into contact with the metallic filler neck of the vehicle. Ordinarily, vapor concentrations in the area of this filling operation are below the lower explosive limit (LEL) of the product being dispensed, so the static discharge causes no problem. The problem with ungrounded gas cans results from a combination of vehicular static charge, the potential between the container and the vehicle, and the loose fit between the grounded nozzle and the gas can. This last condition causes a rich vapor concentration in the ullage (the unfilled volume) of the gas can, and a discharge from the can to the grounded hanging hardware (the nozzle, hose, swivels and break-a-ways) can thus occur at a most inopportune point. The Petroleum Equipment Institute has recorded incidents of static-related ignition at refueling sites since early 2000.\n\nAlthough urban legends persist that a mobile phone can cause sparks, this has not been duplicated under any controlled condition. Nevertheless, mobile phone manufacturers and gas stations ask users to switch off their phones. One suggested origin of this myth is said to have been started by gas station companies because the cell phone signal would interfere with the fuel counter on some older model fuel pumps causing it to give a lower reading. In the MythBusters episode \"Cell Phone Destruction\", investigators concluded that explosions attributed to cell phones could be caused by static discharges from clothing instead and also observed that such incidents seem to involve women more often than men.\n\nThe U.S. National Fire Protection Association does most of the research and code writing to address the potential for explosions of gasoline vapor. The customer fueling area, up to above the surface, normally does not have explosive concentrations of vapors, but may from time to time. Above this height, where most fuel filler necks are located, there is no expectation of an explosive concentration of gasoline vapor in normal operating conditions. Electrical equipment in the fueling area may be specially certified for use around gasoline vapors.\n\n\n\n"}
{"id": "9612212", "url": "https://en.wikipedia.org/wiki?curid=9612212", "title": "Flame rectification", "text": "Flame rectification\n\nFlame rectification is a phenomenon in which a flame can act as an electrical rectifier. The effect is commonly described as being caused by the greater mobility of electrons relative to that of positive ions within the flame, and the asymmetric nature of the electrodes used to detect the phenomenon.\n\nThis effect is used by rectification flame sensors to detect the presence of flame. The rectifying effect of the flame on an AC voltage allows the presence of flame to be distinguished from a resistive leakage path.\n\nOne experimental study has suggested that the effect is caused by the ionization process occurring mostly at the base of the flame, making it more difficult for the electrode further from the base of the flame to attract positive ions from the burner, yet leaving the electron current largely unchanged with distance because of the greater mobility of the electron charge carriers.\n\n\n"}
{"id": "41670700", "url": "https://en.wikipedia.org/wiki?curid=41670700", "title": "Fortum Klaipėda Combined Heat and Power Plant", "text": "Fortum Klaipėda Combined Heat and Power Plant\n\nFortum Klaipėda Combined Heat and Power Plant is power plant in Klaipėda, Lithuania and it uses biomass and waste to produce energy. It built in Klaipėda Free Economic Zone by Fortum Heat Lithuania which belongs to Finnish energy company Fortum.\n\nLithuanian president Dalia Grybauskaitė and Finnish president Sauli Niinistö both participated in the opening ceremony of the plant.\n"}
{"id": "36862406", "url": "https://en.wikipedia.org/wiki?curid=36862406", "title": "Gyppo logger", "text": "Gyppo logger\n\nA gyppo logger (sometimes spelled \"gypo logger\") is a logger who runs or works for a small scale logging operation that is independent from an established sawmill or lumber company. The gyppo system is one of two main patterns of the organization of logging labor in the Pacific Northwest, the other being the \"company logger.\" Gyppo loggers were originally condemned by the Industrial Workers of the World (IWW) as strikebreakers.\n\nAfter the founding of a government-sponsored company union, the Loyal Legion of Loggers and Lumbermen, weakened the influence of the IWW on the logging industry, gyppos were seen as a component of the timber business in a less ideologically charged context.\n\nThe term is specific to the Northwest. Its etymology is certainly unrecoverable at this date. According to E.B. Mittelman it \"may be a derivation from the Greek word, signifying vulture, or may simply be a derivation or corruption of the word gypsy.\" In either case, notes Mittelman, \"it has something of the cunning or predacious in it.\" The Greek etymology is lent some plausibility by the fact that big lumber companies tried to use Greek workers, who wouldn't cooperate, to break a strike organized by the IWW in Everett, Washington in 1917.\n\nThe term \"gyppo\" was commonly prepended to form nicknames among loggers, e.g. \"Gyppo Jake.\" The word was introduced by the Industrial Workers of the World (IWW) to disparage strikebreakers and other loggers who thwarted their organizing efforts. The IWW currently uses the term to refer to \"Any piece-work system; a job where the worker is paid by the volume they produce, rather than by their time.\" Mittelman quotes an editorial from the \"Industrial Worker\" on the subject:\n\nAt present the master class of capitalists call it 'contract labor,' 'piece work,' and other fancy names...For us, the proletarians, it is 'gyppoing' and it means all that the name connotes. The gyppo is a man who 'gyps' his fellow workers and finally himself, out of the fruits of all our organized victories in the class war.\"\nThe term lost most of its derisive connotation after the decline of the IWW's influence in the lumber industry.\n\nThe term \"gyppo logger\" is opposed to the \"company logger,\" who is employed by a lumber company or lumber mill at an hourly or daily wage and generally belongs to a union. Gyppos, on the other hand, work for themselves, run economically marginal operations, and employ a small crew on a fixed-price basis, although they occasionally work for mills on a flat rate, contract, or piecework basis.\n\nThe IWW introduced the term \"gyppo\" during the unsuccessful 1917-8 Pacific Northwest strike for the eight-hour day for loggers, although undoubtedly gyppo loggers existed before 1917. Because the strike was unsuccessful, after the loggers returned to work they called a slowdown. This tactic was so effective that in response the company owners instituted piecework or flat-rate pay scales. The pine timber in the Pacific Northwest is comparatively small and a lot of it is on government land (disallowing the use of railways to transport logs). Thus the conditions facilitate logging with small crews and portable machinery.\n\nTechnological developments after World War II made gyppo logging even more economically rewarding, especially the invention of gasoline powered chainsaws which were light enough to be used by a single person and the use of diesel engines to power \"donkeys\" that had previously been powered by steam. These later gyppos also took advantage of the increased affordability of light industrial equipment, such as trucks and Caterpillar tractors and typically employed family labor in order to keep their operations economically viable.\n\nAccording to William Robbins, writing on the postwar timber boom in the Coos Bay region of Oregon,\n\nThe immediate postwar years in southwestern Oregon were the heyday of the storied gyppo logging and sawmill operator—the hardy individual who worked on marginal capital, usually through subcontracts with a major company or broker, and whose equipment was invariably pieced together with baling wire.\nBy the mid-1950s, overextraction of timber had begun to reduce the economic value of gyppo logging. By the 1970s environmental regulation and other economic changes in the logging industry had driven many gyppo loggers out of business. By the early 21st century gyppo loggers had been described as \"an endangered species.\"\n\n\n"}
{"id": "1363288", "url": "https://en.wikipedia.org/wiki?curid=1363288", "title": "Gyrotron", "text": "Gyrotron\n\nA gyrotron is a class of high-power linear-beam vacuum tubes which generates millimeter-wave electromagnetic waves by the cyclotron resonance of electrons in a strong magnetic field. Output frequencies range from about 20 to 527 GHz, covering wavelengths from microwave to the edge of the terahertz gap. Typical output powers range from tens of kilowatts to 1–2 megawatts. Gyrotrons can be designed for pulsed or continuous operation.\n\nThe gyrotron is a type of free-electron maser which generates high-frequency electromagnetic radiation by stimulated cyclotron resonance of electrons moving through a strong magnetic field. It can produce high power at millimeter wavelengths because as a \"fast-wave\" device its dimensions can be much larger than the wavelength of the radiation. This is unlike conventional microwave vacuum tubes such as klystrons and magnetrons, in which the wavelength is determined by a single-mode resonant cavity, a \"slow-wave\" structure, and thus as operating frequencies increase, the resonant cavity structures must decrease in size, which limits their power-handling capability.\n\nIn the gyrotron a hot filament in an electron gun at one end of the tube emits an annular-shaped (hollow tubular) beam of electrons, which is accelerated by a high-voltage anode and then travels through a large tubular resonant cavity structure in a strong axial magnetic field, usually created by a superconducting magnet around the tube. The field causes the electrons to move helically in tight circles around the magnetic field lines as they travel lengthwise through the tube. At the position in the tube where the magnetic field reaches its maximum the electrons radiate electromagnetic waves in a transverse direction (perpendicular to the axis of the tube) at their cyclotron resonance frequency. The millimeter radiation forms standing waves in the tube, which acts as an open-ended resonant cavity, and is formed into a beam, which radiates through a window in the side of the tube into a waveguide. The spent electron beam is absorbed by a collector electrode at the end of the tube.\n\nAs in other linear-beam microwave tubes, the energy of the output electromagnetic waves comes from the kinetic energy of the electron beam, which is due to the accelerating anode voltage. In the region before the resonant cavity where the magnetic field strength is increasing, it compresses the electron beam, converting the longitudinal drift velocity to transverse orbital velocity, in a process similar to that occurring in a magnetic mirror used in plasma confinement. The orbital velocity of the electrons is 1.5 to 2 times their axial beam velocity. Due to the standing waves in the resonant cavity, the electrons become \"bunched\"; that is, their phase becomes coherent (synchronized) so they are all at the same point in their orbit at the same time. Therefore, they emit coherent radiation. \nThe electron speed in a gyrotron is slightly relativistic (comparable to but not close to the speed of light). This contrasts to the free-electron laser (and xaser) that work on different principles and whose electrons are highly relativistic.\n\nGyrotrons are used for many industrial and high-technology heating applications. For example, gyrotrons are used in nuclear fusion research experiments to heat plasmas and also in manufacturing industry as a rapid heating tool in processing glass, composites, and ceramics, as well as for annealing (solar and semiconductors). Military applications include the Active Denial System.\n\nThe output window of the tube from which the microwave beam emerges can be in two locations\n\nThe original gyrotron described above, developed in 1964, was an oscillator, but since that time gyrotron amplifiers have been developed. The helical gyrotron electron beam can amplify an applied microwave signal similarly to the way a straight electron beam amplifies in classical microwave tubes such as the klystron, so there are a series of gyrotrons which function analogously to these tubes. Their advantage is that they can operate at much higher frequencies: \n\nThe gyrotron was invented in the Soviet Union. Present makers include Communications & Power Industries (USA), Gycom (Russia), Thales Group (EU), Toshiba (Japan), and Bridge12 Technologies, Inc.. System developers include Gyrotron Technology, Inc.\n\n\n"}
{"id": "11551960", "url": "https://en.wikipedia.org/wiki?curid=11551960", "title": "Hayes similitude principle", "text": "Hayes similitude principle\n\nThe Hayes similitude principle enabled aerodynamicists to take the results of one series of tests or calculations and apply them to the design of an entire family of similar configurations where neither tests nor detailed calculations are available.\n\nThe similitude principle was developed by Wallace D. Hayes, a pioneer in hypersonic flow, which is considered to begin at about five times the speed of sound, or Mach 5, and is described in his classic book Hypersonic Flow Theory co-written with Ronald Probstein and first published in 1959.\n\nThe behavior of the physical processes in actual problems is affected by so many physical quantities that a complete mathematical description thereof is usually very difficult and sometimes practically impossible due to the complicated nature of the phenomena. We know from experience that if two systems are geometrically similar there usually exists some kind of similarity under certain conditions, such as kinematic similarity, dynamic similarity, thermal similarity, and similarity of concentration distribution, and that if similarity conditions are satisfied we can greatly reduce the number of independent variables required to describe the behavior of the process. In this way, we can systematically understand. describe, and even predict the behavior of physical processes in real problems in a relatively simple manner. This principle is known as principle of similitude. Dimensional analysis is a method of deducing logical groupings of the variables, through which we can describe similarity criteria of the processes.\n\nPhysical quantities such as length [L], mass [M], time [T], and temperature are dimensional quantities and the magnitude of each quantity can be described by multiples of the unit of each dimension namely m, kg, s, and K, respectively. Through experience, we can select a certain number of fundamental dimensions, such as those mentioned above, and express all other dimensional quantities in terms of products of powers of these fundamental dimensions. Furthermore, in describing the behavior of physical processes, we know that there is an implicit principle that we cannot add or subtract physical quantities of different dimensions. This means that the equations governing physical processes must be dimensionally consistent and each term of the equation must have the same dimensions. This principle is known as the principle of dimensional homogeneity.\n(courtesy: Book: Mass transfer : from fundamentals to modern industrial applications, Publisher: Weinheim : WILEY-VCH, 2006.\n\n"}
{"id": "31216191", "url": "https://en.wikipedia.org/wiki?curid=31216191", "title": "Huating Lake", "text": "Huating Lake\n\nHuating Lake (), also known as the Hualiangting Reservoir (花凉亭水库 \"Huāliángtíng Shuĭkù\"), is a large scale reservoir in Taihu County, Anqing City, Anhui Province, People's Republic of China, used for the purposes of flood control, hydro-electric power generation, agricultural irrigation, transport and tourism. The lake is a National Scenic Area, National 4A Tourist Attraction and demonstration site for agro-tourism.\n\nDamming of the lake to create a reservoir began in 1958 and continued until 1962. There was then an eight-year hiatus until 1970 when construction resumed. Fundamental infrastructure was in place by 1976 and the project completed in 2001.\n\nOn October 26, 2009, a 2.1 billion RMB, 23 month reinforcement program began at the reservoir.\n\nThe reservoir provides irrigation for Susong County, Wangjiang County, Huaining County and the eastern part of Taihu County, a total area of some 1.05 million Chinese acres.\n\n\n"}
{"id": "32130816", "url": "https://en.wikipedia.org/wiki?curid=32130816", "title": "Hybrid material", "text": "Hybrid material\n\nHybrid materials are composites consisting of two constituents at the nanometer or molecular level. Commonly one of these compounds is inorganic and the other one organic in nature. Thus, they differ from traditional composites where the constituents are at the macroscopic (micrometer to millimeter) level. Mixing at the microscopic scale leads to a more homogeneous material that either show characteristics in between the two original phases or even new properties.\n\nMany natural materials consist of inorganic and organic building blocks distributed on the nanoscale. In most cases the inorganic part provides mechanical strength and an overall structure to the natural objects while the organic part delivers bonding between the inorganic building blocks and/or the soft tissue. Typical examples of such materials are bone, or nacre.\n\nThe first hybrid materials were the paints made from inorganic and organic components that were used thousands of years ago. Rubber is an example of the use of inorganic materials as fillers for organic polymers. The sol–gel process developed in the 1930s was one of the major driving forces what has become the broad field of inorganic–organic hybrid materials.\n\nHybrid materials can be classified based on the possible interactions connecting the inorganic and organic species. \"Class I\" hybrid materials are those that show weak interactions between the two phases, such as van der Waals, hydrogen bonding or weak electrostatic interactions. \"Class II\" hybrid materials are those that show strong chemical interactions between the components such as covalent bonds.\n\nStructural properties can also be used to distinguish between various hybrid materials. An organic moiety containing a functional group that allows the attachment to an inorganic network, e.g. a trialkoxysilane group, can act as a \"network modifier\" because in the final structure the inorganic network is only modified by the organic group. Phenyltrialkoxysilanes are an example for such compounds; they modify the silica network in the sol–gel process via the reaction of the trialkoxysilane group without supplying additional functional groups intended to undergo further chemical reactions to the material formed. If a reactive functional group is incorporated the system is called a \"network functionalizer\". The situation is different if two or three of such anchor groups modify an organic segment; this leads to materials in which the inorganic group is afterwards an integral part of the hybrid network. The latter type of system is known as \"network builder\"\n\nBlends are formed if no strong chemical interactions exist between the inorganic and organic building blocks. One example for such a material is the combination of inorganic clusters or particles with organic polymers lacking a strong (e.g. covalent) interaction between the components. In this case a material is formed that consists for example of an organic polymer with entrapped discrete inorganic moieties in which, depending on the functionalities of the components, for example weak crosslinking occurs by the entrapped inorganic units through physical interactions or the inorganic components are entrapped in a crosslinked polymer matrix. If an inorganic and an organic network interpenetrate each other without strong chemical interactions, so called interpenetrating networks (IPNs) are formed, which is for example the case if a sol–gel material is formed in presence of an organic polymer or vice versa. Both materials described belong to class I hybrids. Class II hybrids are formed when the discrete inorganic building blocks, e.g. clusters, are covalently bonded to the organic polymers or inorganic and organic polymers are covalently connected with each other.\n\nThe term nanocomposite is used if the combination of organic and inorganic structural units yield a material with composite properties. That is to say that the original properties of the separate organic and inorganic components are still present in the composite and are unchanged by mixing these materials. However, if a new property emerges from the intimate mixture, then the material becomes a hybrid. A macroscopic example is the mule, which is more suited for hard work than either of its parents, the horse and the donkey. The size of the individual components and the nature of their interaction (covalent, electrostatic, etc.) do not enter into the definition of a hybrid material.\n\n\nTwo different approaches can be used for the formation of hybrid materials: Either well-defined preformed building blocks are applied that react with each other to form the final hybrid material in which the precursors still at least partially keep their original integrity or one or both structural units are formed from the precursors that are transformed into a new (network) structure. It is important that the interface between the inorganic and\norganic materials which has to be tailored to overcome serious problems in the preparation of hybrid materials. Different building blocks and approaches can be used for their preparation and these have to be adapted to bridge the differences of inorganic and organic materials.\n\nBuilding blocks at least partially keep their molecular integrity throughout the material formation, which means that structural units that are present in these sources for materials formation can also be found in the final material. At the same time typical properties of these building blocks usually survive the matrix formation, which is not the case if material precursors are transferred into novel materials. Representative examples of such well-defined building blocks are modified inorganic clusters or nanoparticles with attached reactive organic groups.\n\nCluster compounds often consist of at least one functional group that allows an interaction with an organic matrix, for example by copolymerization. Depending on the number of groups that can interact, these building blocks are able to modify an organic matrix (one functional group) or form partially or fully crosslinked materials (more than one group). For instance, two reactive groups can lead to the formation of chain structures. If the building blocks contain at least three reactive groups they can be used without additional molecules for the formation of a crosslinked material.\n\nBeside the molecular building blocks mentioned, nanosized building blocks, such as particles or nanorods, can also be used to form nanocomposites. The building block approach has one large advantage compared with the in situ formation of the inorganic or organic entities: because at least one structural unit (the building block) is well-defined and usually does not undergo significant structural changes during the matrix formation, better structure–property predictions are possible. Furthermore, the building blocks can be designed in such a way to give the best performance in the materials’ formation, for example good solubility of inorganic compounds in organic monomers by surface groups showing a similar polarity as the monomers.\n\nIn recent years many building blocks have been synthesized and used for the preparation of hybrid materials. Chemists can design these compounds on a\nmolecular scale with highly sophisticated methods and the resulting systems are used for the formation of functional hybrid materials. Many future applications, in particular in nanotechnology, focus on a bottom-up approach in which complex structures are hierarchically formed by these small building blocks. This idea is also one of the driving forces of the building block approach in hybrid materials.\n\nThe in situ formation of the hybrid materials is based on the chemical transformation of the precursors used throughout materials’ preparation. Typically this is the case if organic polymers are formed but also if the sol–gel process is applied to produce the inorganic component. In these cases well-defined discrete molecules are transformed to multidimensional structures, which often show totally different properties from the original precursors. Generally simple, commercially available molecules are applied and the internal structure of the final material is determined by the composition of these precursors but also by the reaction conditions. Therefore, control over the latter is a crucial step in this process. Changing one parameter can often lead to two very different materials. If, for example, the inorganic species is a silica derivative formed by the sol–gel process, the change from base to acid catalysis makes a large difference because base catalysis leads to a more particle-like microstructure while acid catalysis leads to a polymer-like microstructure. Hence, the final performance of the derived materials is strongly dependent on their processing and its optimization.\n\nMany of the classical inorganic solid state materials are formed using solid precursors and high temperature processes, which are often not compatible with the presence of organic groups because they are decomposed at elevated temperatures. Hence, these high temperature processes are not suitable for the in situ formation of hybrid materials. Reactions that are employed should have more the character of classical covalent bond formation in solutions. One of the most prominent processes which fulfill these demands is the sol–gel process. However, such rather low temperature processes often do not lead to the thermodynamically most stable structure but to kinetic products, which has some implications for the structures obtained. For example, low temperature derived inorganic materials are often amorphous or crystallinity is only observed on a very small length scale, i.e. the nanometer range. An example of the latter is the formation of metal nanoparticles in organic or inorganic matrices by reduction of metal salts or organometallic precursors.\n\nSome methods of in situ formation of inorganic materials are:\n\n\nIf the organic polymerization occurs in the presence of an inorganic material to form the hybrid material one has to distinguish between several possibilities to overcome the incompatibility of the two species. The inorganic material can either have no surface functionalization but the bare material surface; it can be modified with nonreactive organic groups (e.g. alkyl chains); or it can contain reactive surface groups such as polymerizable functionalities. Depending on these prerequisites the material can be pretreated, for example a pure inorganic surface can be treated with surfactants or silane coupling agents to make it compatible with the organic monomers, or functional monomers can be added that react with the surface of the inorganic material. If the inorganic component has nonreactive organic groups attached to its surface and it can be dissolved in a monomer which is subsequently polymerized, the resulting material after the organic polymerization, is a blend. In this case the inorganic component interact only weakly or not at all with the organic polymer; hence, a class I material is formed. Homogeneous materials are only obtained in this case if agglomeration of the inorganic components in the organic environment is prevented. This can be achieved if the interactions between the inorganic components and the monomers are better or at least the same as between the inorganic components. However, if no strong chemical interactions are formed, the long-term stability of a once homogeneous material is questionable because of diffusion effects in the resulting hybrid material. The stronger the respective interaction between the components, the more stable is the final material. The strongest interaction is achieved if class II materials are formed, for example with covalent interactions.\n\nSimultaneous formation of the inorganic and organic polymers can result in the most homogeneous type of interpenetrating networks. Usually the precursors for the sol–gel process are mixed with monomers for the organic polymerization and both processes are carried out at the same time with or without solvent. Applying this method, three processes are competing with each other:\n\n(a) the kinetics of the hydrolysis and condensation forming the inorganic phase,\n(b) the kinetics of the polymerization of the organic phase, and\n(c) the thermodynamics of the phase separation between the two phases.\n\nBy tailoring the kinetics of the two polymerizations in such a way that they occur simultaneously and rapidly enough, phase separation is avoided or minimized. Additional parameters such as attractive interactions between the two moieties, as described above can also be used to avoid phase separation.\n\nOne problem that also arises from the simultaneous formation of both networks is the sensitivity of many organic polymerization processes for sol–gel conditions or the composition of the materials formed. Ionic polymerizations, for example, often interact with the precursors or intermediates formed in the sol–gel process. Therefore, they are not usually applied in these reactions.\n\n\n"}
{"id": "40133154", "url": "https://en.wikipedia.org/wiki?curid=40133154", "title": "Lanthanum aluminate-strontium titanate interface", "text": "Lanthanum aluminate-strontium titanate interface\n\nThe interface between lanthanum aluminate (LaAlO) and strontium titanate (SrTiO) is a notable materials interface because it exhibits properties not found in its constituent materials. Individually, LaAlO and SrTiO are non-magnetic insulators, yet LaAlO/SrTiO interfaces can exhibit electrical conductivity, superconductivity, ferromagnetism, large negative in-plane magnetoresistance, and giant persistent photoconductivity. The study of how these properties emerge at the LaAlO/SrTiO interface is a growing area of research in condensed matter physics.\n\nUnder the right conditions, the LaAlO/SrTiO interface is electrically conductive, like a metal. The angular dependence of Shubnikov-de Haas oscillations indicates that the conductivity is two-dimensional, leading many researchers to refer to it as a two-dimensional electron gas (2DEG). Two-dimensional does not mean that the conductivity has zero thickness, but rather than the electrons are confined to only move in two directions. It is also sometimes called a two-dimensional electron liquid (2DEL) to emphasize the importance of inter-electron interactions.\n\nNot all LaAlO/SrTiO interfaces are conductive. Typically, conductivity is achieved only when:\n\nConductivity can also be achieved when the SrTiO is doped with oxygen vacancies; however, in that case, the interface is technically LaAlO/SrTiO instead of LaAlO/SrTiO.\n\nThe source of conductivity at the LaAlO/SrTiO interface has been debated for years. SrTiO is a wide-band gap semiconductor that can be doped n-type in a variety of ways. Clarifying the mechanism behind the conductivity is a major goal of current research. Four leading hypotheses are:\n\n\nPolar gating was the first mechanism used to explain the conductivity at LaAlO/SrTiO interfaces. It postulates that the LaAlO, which is polar in the 001 direction (with alternating sheets of positive and negative charge), acts as an electrostatic gate on the semiconducting SrTiO. When the LaAlO layer grows thicker than three unit cells, its valence band energy rises above the Fermi level, causing holes (or positively charged oxygen vacancies ) to form on the outer surface of the LaAlO. The positive charge on the surface of the LaAlO attracts negative charge to nearby available states. In the case of the LaAlO/SrTiO interface, this means electrons accumulate in the surface of the SrTiO, in the Ti d bands.\n\nThe strengths of the polar gating hypothesis are that it explains why conductivity requires a critical thickness of four unit cells of LaAlO and that it explains why conductivity requires the SrTiO to be TiO-terminated. The polar gating hypothesis also explains why alloying the LaAlO increases the critical thickness for conductivity.\n\nOne weakness of the hypothesis is that it predicts that the LaAlO films should exhibit a built-in electric field; so far, x-ray photoemission experiments and other experiments have shown little to no built-in field in the LaAlO films. The polar gating hypothesis also cannot explain why Ti is detected when the LaAlO films are thinner than the critical thickness for conductivity.\n\nThe polar gating hypothesis is sometimes called the polar catastrophe hypothesis, alluding to the counterfactual scenario where electrons don't accumulate at the interface and instead voltage in the LaAlO builds up forever. The hypothesis has also been called the electronic reconstruction hypothesis, highlighting the fact that electrons, not ions, move to compensate the building voltage.\n\nAnother hypothesis is that the conductivity comes from free electrons left by oxygen vacancies in the SrTiO. SrTiO is known to be easily doped by oxygen vacancies, so this was initially considered a promising hypothesis. However, electron energy loss spectroscopy measurements have bounded the density of oxygen vacancies well below the density necessary to supply the measured free electron densities.\nAnother proposed possibility is that oxygen vacancies in the surface of the LaAlO are remotely doping the SrTiO. Under generic growth conditions, multiple mechanisms can coexist. A systematic study across a wide growth parameter space demonstrated different roles played by oxygen vacancy formation and the polar gating at different interfaces. An obvious difference between oxygen vacancies and polar gating in creating the interface conductivity is that the carriers from oxygen vacancies are thermally activated as the donor level of oxygen vacancies is usually separated from the SrTiO conduction band, consequently exhibiting the carrier freeze-out effect at low temperatures; in contrast, the carriers originating from the polar gating are transferred into the SrTiO conduction band (Ti 3d orbitals) and are therefore degenerate.\n\nLanthanum is a known dopant in SrTiO, so it has been suggested that La from the LaAlO mixes into the SrTiO and dopes it n-type. Multiple studies have shown that intermixing takes place at the interface; however, it is not clear whether there is enough intermixing to provide all of the free carriers. For example, a flipped interface between a SrTiO film and a LaAlO substrate is insulating.\n\nA fourth hypothesis is that the LaAlO crystal structure undergoes octahedral rotations in response to the strain from the SrTiO. These octahedral rotations in the LaAlO induce octahedral rotations in the SrTiO, increasing the Ti d-band width enough so that electrons are no longer localized.\n\nSuperconductivity was first observed in LaAlO/SrTiO interfaces in 2007, with a critical temperature of ~200 mK. Like the conductivity, the superconductivity appears to be two-dimensional.\n\nHints of ferromagnetism in LaAlO/SrTiO were first seen in 2007, when Dutch researchers observed hysteresis in the magnetoresistance of LaAlO/SrTiO. Follow up measurements with torque magnetometry indicated that the magnetism in LaAlO/SrTiO persisted all the way to room temperature. In 2011, researchers at Stanford University used a scanning SQUID to directly image the ferromagnetism, and found that it occurred in heterogeneous patches. Like the conductivity in LaAlO/SrTiO, the magnetism only appeared when the LaAlO films were thicker than a few unit cells. However, unlike conductivity, magnetism was seen at SrO-terminated surfaces as well as TiO-terminated surfaces.\n\nThe discovery of ferromagnetism in a materials system that also superconducts spurred a flurry of research and debate, because ferromagnetism and superconductivity almost never coexist together. Ferromagnetism requires electron spins to align, while superconductivity typically requires electron spins to anti-align.\n\nMagnetoresistance measurements are a major experimental tool used to understand the electronic properties of materials. The magnetoresistance of LaAlO/SrTiO interfaces has been used to reveal the 2D nature of conduction, carrier concentrations (through the hall effect), electron mobilities, and more.\n\nAt low magnetic field, the magnetoresistance of LaAlO/SrTiO is parabolic versus field, as expected for an ordinary metal. However, at higher fields, the magnetoresistance appears to become linear versus field. Linear magnetoresistance can have many causes, but so far there is no scientific consensus on the cause of linear magnetoresistance in LaAlO/SrTiO interfaces. Linear magnetoresistance has also been measured in pure SrTiO crystals, so it may be unrelated to the emergent properties of the interface.\n\nAt low temperature (T < 30 K), the LaAlO/SrTiO interface exhibits negative in-plane magnetoresistance, sometimes as large as -90%. The large negative in-plane magnetoresistance has been ascribed to the interface's enhanced spin-orbit interaction.\n\nExperimentally, the charge density profile of the electron gas at the LaAlO/SrTiO interface has a strongly asymmetric shape with a rapid initial decay over the first 2 nm and a pronounced tail that extends to about 11 nm. A wide variety of theoretical calculations support this result. Importantly, to get electron distribution one have to take into account field-dependent dielectric constant of SrTiO.\n\nThe 2D electron gas that arises at the LaAlO/SrTiO interface is notable for two main reasons. First, it has very high carrier concentration, on the order of 10 cm. Second, if the polar gating hypothesis is true, the 2D electron gas has the potential to be totally free of disorder, unlike other 2D electron gases that require doping or gating to form. However, so far researchers have been unable to synthesize interfaces that realize the promise of low disorder.\n\nMost LaAlO/SrTiO interfaces are synthesized using pulsed laser deposition. A high-power laser ablates a LaAlO target, and the plume of ejected material is deposited onto a heated SrTiO substrate. Typical conditions used are:\n\n\nSome LaAlO/SrTiO interfaces have also been synthesized by molecular beam epitaxy, sputtering, and atomic layer deposition.\n\nTo better understand in the LaAlO/SrTiO interface, researchers have synthesized a number of analogous interfaces between other polar perovskite films and SrTiO. Some of these analogues have properties similar to LaAlO/SrTiO, but some do not.\n\n\n\nAs of 2015, there are no commercial applications of the LaAlO/SrTiO interface. However, speculative applications have been suggested, including field-effect devices, sensors, photodetectors, and thermoelectrics;\nrelated LaVO/SrTiO is a functional solar cell albeit hitherto with a low efficiency.\n\n"}
{"id": "42989614", "url": "https://en.wikipedia.org/wiki?curid=42989614", "title": "Laplace equation for irrotational flow", "text": "Laplace equation for irrotational flow\n\nIrrotational flow occurs when the cross gradient of the velocity or shear is zero.\n\nThe individual parcels of a frictionless incompressible fluid initially at rest cannot be caused to rotate. This can be visualized by considering an infinitesimal small volume of fluid in the shape of a sphere. Surface forces act normal to the surface, As the fluid is frictionless no friction occurs so they act through the center of the sphere hence, no torque can be exerted on the sphere so angular momentum remains constant and no change in angular velocity occurs. \nRotation of a fluid parcel is defined as the average angular velocity of two elements originally at right angles to each other. Points A and B have an x-velocity which differs by ∂u/∂y dy over the time interval Δt. Points A and B will have a difference in x-displacements equal to\n\nand the associated angle change of side AB is\n\nThe angular velocity of the element, about the z axis in this case, is defined as the average angular velocity of sides AB and AC.\n\nNow ∇×v=0 so,\n\nThese restrictions on the velocity must hold at every point. \nLet us consider a function ∅ which satisfies the condition \n\nThe minus sign is arbitrary it is a convention that causes the value of ∅ to decrease in the direction of the velocity.This proves the existence of a function ∅ such that its negative derivative with respect to any direction is the velocity component in that direction.\n\nThe assumption of a velocity potential is equivalent to the irrotational flow as\n\nWhenever a velocity potential function exists then it must be an irrotational flow so they are equivalent to each other.\nAs we are considering ideal fluid so it must follow continuity equation \n\nYields,\n\nIn vector form it can be written as \n\nIt is called as the laplace equation.\nAny function ∅ that satisfies the laplace equation is a possible irrotational flow case. As there are infinite number of solutions to the laplace equation each of which satisfies certain flow boundaries the main problem is the selection of the proper function for the particular flow case. As ∅ appears to the first power it is a linear equation, so the sum of two solutions is also a solution.\n\nImportance of studying Irrotational flow;\n"}
{"id": "23216443", "url": "https://en.wikipedia.org/wiki?curid=23216443", "title": "List of boiler explosions", "text": "List of boiler explosions\n\nThis article contains a list of steam boiler explosions such as railway locomotive, marine transport (military and civilian), and stationary power.\n\n-SS Helen McGregor, Boiler Explosion, Memphis, TN, February 1830.\n\n-Staten Island Ferry, Westfield II, FT boiler explosion: July 30, 1871.\n\n-Ford's River Rouge Plant, Dearborn, Michigan, Gas Boiler explosion, 6 killed 2/1/99.\n\n-St Mary's Hospital, Pennington Avenue, Passaic, New Jersey: Cleaver Brooks boiler Explosion 7/15/06.\n\n-Dana Corporation, Paris Tennessee, 400 Hp fire-tube boiler explosion 6/19/07 \n\n-Givaudan Corporation, Delawanna Avenue, Clifton, New Jersey: Portable Fire Tube Boiler Explosion (1990s)\n\n-1887 Robert Henry Thurston's book Steam Boiler Explosions in Theory, and in Practice is published\n\n\n"}
{"id": "41677515", "url": "https://en.wikipedia.org/wiki?curid=41677515", "title": "List of power stations in Afghanistan", "text": "List of power stations in Afghanistan\n\nThis page lists all power stations in Afghanistan.\n\n"}
{"id": "2040454", "url": "https://en.wikipedia.org/wiki?curid=2040454", "title": "Madelung constant", "text": "Madelung constant\n\nThe Madelung constant is used in determining the electrostatic potential of a single ion in a crystal by approximating the ions by point charges. It is named after Erwin Madelung, a German physicist.\n\nBecause the anions and cations in an ionic solid are attracting each other by virtue of their opposing charges, separating the ions requires a certain amount of energy. This energy must be given to the system in order to break the anion-cation bonds. The energy required to break these bonds for one mole of an ionic solid under standard conditions is the lattice energy.\n\nThe Madelung constant allows for the calculation of the electric potential \"V\" of all ions of the lattice felt by the ion at position \"r\"\n\nwhere \"r\" =|\"r\" - \"r\"| is the distance between the \"i\"th and the \"j\"th ion. In addition,\n\nIf the distances \"r\" are normalized to the nearest neighbor distance \"r\" the potential may be written\n\nwith formula_3 being the (dimensionless) Madelung constant of the \"i\"th ion\n\nThe electrostatic energy of the ion at site formula_5 then is the product of its charge with the potential acting at its site\n\nThere occur as many Madelung constants formula_3 in a crystal structure as ions occupy different lattice sites. For example, for the ionic crystal NaCl, there arise two Madelung constants – one for Na and another for Cl. Since both ions, however, occupy lattice sites of the same symmetry they both are of the same magnitude and differ only by sign. The electrical charge of the Na and Cl ion are assumed to be onefold positive and negative, respectively, formula_8 and formula_9. The nearest neighbour distance amounts to half the lattice parameter of the cubic unit cell formula_10 and the Madelung constants become\n\nThe prime indicates that the term formula_12 is to be left out. Since this sum is conditionally convergent it is not suitable as definition of Madelung's constant unless the order of summation is also specified. There are two \"obvious\" methods of summing this series, by expanding cubes or expanding spheres. The latter, though devoid of a meaningful physical interpretation (there are no spherical crystals) is rather popular because of its simplicity. Thus, the following expansion is often found in the literature:\n\nHowever, this is wrong as this series diverges as was shown by Emersleben in 1951. The summation over expanding cubes converges to the correct value. An unambiguous mathematical definition is given by Borwein, Borwein and Taylor by means of analytic continuation of an absolutely convergent series.\n\nThere are many practical methods for calculating Madelung's constant using either direct summation (for example, the Evjen method) or integral transforms, which are used in the Ewald method.\n\nA fast converging formula for Madelung Constant is\n\nformula_14\n\nIt is assumed for the calculation of Madelung constants that an ion’s charge density may be approximated by a point charge. This is allowed, if the electron distribution of the ion is spherically symmetric. In particular cases, however, when the ions reside on lattice site of certain crystallographic point groups, the inclusion of higher order moments, i.e. multipole moments of the charge density might be required. It is shown by electrostatics that the interaction between two point charges only accounts for the first term of a general Taylor series describing the interaction between two charge distributions of arbitrary shape. Accordingly, the Madelung constant only represents the monopole-monopole term.\n\nThe electrostatic interaction model of ions in solids has thus been extended to a point multipole concept that also includes higher multipole moments like dipoles, quadrupoles etc. These concepts require the determination of higher order Madelung constants or so-called electrostatic lattice constants. In their case, instead of the nearest neighbor distance formula_15 another standard length like the cube root of the unit cell volume formula_16 is appropriately used for purposes of normalization. For instance, the Madelung constant then reads\n\nThe proper calculation of electrostatic lattice constants has to consider the crystallographic point groups of ionic lattice sites; for instance, dipole moments may only arise on polar lattice sites, i. e. exhibiting a \"C\", \"C\", \"C\" or \"C\" site symmetry (\"n\" = 2, 3, 4 or 6). These second order Madelung constants turned out to have significant effects on the lattice energy and other physical properties of heteropolar crystals.\n\nThe Madelung Constant is also a useful quantity in describing the lattice energy of organic salts. Izgorodina and coworkers have described a generalised method (called the EUGEN method) of calculating the Madelung constant for any crystal structure.\n\n"}
{"id": "1069434", "url": "https://en.wikipedia.org/wiki?curid=1069434", "title": "Maximum takeoff weight", "text": "Maximum takeoff weight\n\nThe maximum takeoff weight (MTOW) or maximum gross takeoff weight (MGTOW) or maximum takeoff mass (MTOM) of an aircraft is the maximum weight at which the pilot is allowed to attempt to take off, due to structural or other limits. The analogous term for rockets is gross lift-off mass, or GLOW. MTOW is usually specified in units of kilograms or pounds.\n\nMTOW is the heaviest weight at which the aircraft has been shown to meet all the airworthiness requirements applicable to it. MTOW of an aircraft is fixed, and does not vary with altitude, air temperature or the length of the runway to be used for takeoff or landing. A different weight, the \"maximum permissible takeoff weight\" or \"regulated takeoff weight\", varies according to flap setting, altitude, air temperature, length of runway and other factors. It is different from one takeoff to the next, but can never be higher than the MTOW.\n\nCertification standards applicable to the airworthiness of an aircraft contain many requirements. Some of these requirements can only be met by specifying a maximum weight for the aircraft, and demonstrating that the aircraft can meet the requirement at all weights up to, and including, the specified maximum. These requirements include:\n\nAt the MTOW, all aircraft of a type and model must be capable of complying with all these certification requirements.\n\nWith several of the manufacturers of large aircraft, the same model of aircraft can have more than one MTOW. An operator can choose to have the aircraft certified for a reduced weight, often for a reduced cost with an option to later increase the MTOW for a fee and the cost of certification change. Some airlines which do not require a high MTOW choose to have a lower MTOW for that particular aircraft to reduce costs (Landing fees and air traffic control fees are MTOW based).\n\nIn other examples an increased MTOW option is achieved by reinforcement due to additional or stronger materials. For example, the Airbus A330 242 tonnes MTOW variant / A330neo uses Scandium–aluminium (scalmalloy) to avoid an empty weight increase.\n\nSmaller aircraft like the Cessna 208 Caravan may have an option for a reinforced undercarriage to permit an increase in MTOW.\n\nIn many circumstances an aircraft may not be permitted to take off at its MTOW. In these circumstances the maximum weight permitted for takeoff will be determined taking account of the following:\n\nThe maximum weight at which a takeoff may be attempted, taking into account the above factors, is called the maximum permissible takeoff weight, maximum allowed takeoff weight or regulated takeoff weight.\n\n\n"}
{"id": "27376342", "url": "https://en.wikipedia.org/wiki?curid=27376342", "title": "Melissic acid", "text": "Melissic acid\n\nMelissic acid (or triacontanoic acid) is a saturated fatty acid.\n\nMelissic acid gets its name from the Greek word melissa meaning bee, since it was found in beeswax.\n\n\"n\"-Triacontanoic acid was synthesized by Bleyberg and Ulrich (1931) and by G.M. Robinson.\n\nTriacontanoic acid and triacontanamide (CH(CH2)-CONHI) can be self-assembled.\n\n\n"}
{"id": "41067335", "url": "https://en.wikipedia.org/wiki?curid=41067335", "title": "Nanotopography", "text": "Nanotopography\n\nNanotopography refers to specific surface features which form or are generated at the nanoscopic scale. While the term can be used to describe a broad range of applications ranging from integrated circuits to microfluidics, in practice it typically applied to sub-micron textured surfaces as used in biomaterials research.\n\nSeveral functional nanotopographies have been identified in nature. Certain surfaces like that of the lotus leaf have been understood to apply nanoscale textures for abiotic processes such as self-cleaning. Bio-mimetic applications of this discovery have since arrived in consumer products. In 2012, it was recognized that nanotopographies in nature are also used for antibiotic purposes. The wing of the cicada, the surface of which is covered in nanoscale pillars, induces lysis of bacteria. While the nano-pillars were not observed to prevent cell adhesion, they acted mechanistically to stretch microbial membranes to breakage. In vitro testing of the cicada wing demonstrated its efficacy against a variety of bacterial strains.\n\nNumerous technologies are available for the production of nanotopography. High-throughput techniques include plasma functionalization, abrasive blasting, and etching. Though low cost, these processes are limited in the control and replicability of feature size and geometry. Techniques enabling greater feature precision exist, among them electron beam lithography and particle deposition, but are slower and more resource intensive by comparison. Alternatively, processes such as molecular self-assembly can be utilized which provide an enhanced level of production speed and feature control.\n\nThough the effects of nanotopography on cell behavior have only been recognized since 1964, some of the first practical applications of the technology are being realized in the field of medicine. Among the few clinical applications is the functionalization of titanium implant surfaces with nanotopography, generated with submersion etching and sand blasting. This technology has been the focal point of a diverse body of research aimed at improving post-operative integration of certain implant components. The determinant of integration varies, but as most titanium implants are orthopedics-oriented, osseointegration is the dominant aim of the field.\n\nNanotopography is readily applied to cell culture and has been shown to have a significant impact on cell behavior across different lineages. Substrate features in the nanoscale regime down to the order of 9 nm are able to retain some effect. Subjected solely to topographical cues, a wide variety of cells demonstrate responses including changes in cell growth and gene expression. Certain patterns are able to induce stem cells to differentiate down specific pathways.\nNotable results include osteogenic induction in the absence of media components as well as near-total cell alignment as seen in smooth muscle. The potential of topographical cues to fulfill roles otherwise requiring xeno-based media components offers high translatability to clinical applications, as regulation and cost related to animal-derived products constitutes a major roadblock in a number of cell-related technologies.\n"}
{"id": "1423322", "url": "https://en.wikipedia.org/wiki?curid=1423322", "title": "North American blizzard of 2005", "text": "North American blizzard of 2005\n\nThe North American blizzard of 2005 was a three-day storm that affected large areas of the northern United States, dropping more than 3 feet (0.9 m) of snow in parts of southeastern Massachusetts, as well as much of the Boston metropolitan area. While this was by far the hardest hit region, it was also a significant snowstorm for the Philadelphia and New York City areas, which both suffered occasional blizzard conditions and 12-15 inch (30–38 cm) snow accumulations. \n\nThe storm began dropping snow on the upper Midwest on Thursday, January 20, 2005. It slowly moved eastward affecting the Great Lakes region and the Mid-Atlantic states on Friday and Saturday, January 21 and January 22, 2005. On Saturday evening the storm entered the Southern New England area. The strength of the storm, coupled with the extreme Arctic temperatures, created a light, fluffy snow which increased the snowfall totals.\n\nThe storm shut down Logan International Airport in Boston, Massachusetts and T. F. Green Airport in Rhode Island, while also impairing travel throughout much of Massachusetts due to the high amount of snow covering the roads. Practically all schools in the Metrowest and South East regions of Massachusetts were closed for at least two days. Cape Cod Community College, as well as all public schools on Cape Cod, Martha's Vineyard and Nantucket were closed for up to a week.\n\nAfter traveling across the Atlantic Ocean, the storm system hit parts of Great Britain and Ireland and the Scandinavian peninsula, causing even more widespread blackouts and a small number of deaths in the region.\n\nConditions throughout much of eastern Massachusetts were near-whiteout and, in some cases, were whiteout. State Police in both Dartmouth and Middleborough suggested that residents travel as little as possible. Major highways, such as Route 24, Route 6 and Route 140, could not be properly cleared because of the heavy snowfall and high winds. Secondary highways, such as Route 79 were nearly impassable in some areas.\n\nMany Boston-area newscasters credit the New England Patriots football game on January 23 for keeping most travellers indoors, avoiding the pile-ups and endless lines of stuck cars that were the hallmark of the Blizzard of 1978. The fact that the storm fell on a weekend when many people did not have to go to work or school also helped to this effect.\n\nWith much of the snow cleared from the roads by the evening of January 24, snowpiles on street corners were in excess of ten feet high in some locations. Roads were severely narrowed in most congested areas, due to parked cars that were not towed and instead simply plowed in. Most schools in eastern Massachusetts and Rhode Island remained closed for an extended period of time to allow for clean-up of the road debris.\n\nLocation - storm total - time/date - comments -\nsnowfall measurement\n\nLitchfield- 12.2\n\nEast Granby 14.3 inches (36.3 cm)<br>\nBurlington 13.3 inches (33.7 cm)<br>\nSouthington 12.8 inches (32.5 cm), 8:54 p.m. January 24 General public<br>\nWindsor Locks 12.5 inches (31.8 cm), 1:08 p.m. January 23 airport ang (bdl)<br>\nSouth Windsor 11.5 inches (29.2 cm), 1:14 p.m. January 23<br>\nWindsor 11.0 inches (27.9 cm), 8:20 p.m. January 23<br>\nUnionville 8.8 inches (22.4 cm), 1:30 p.m. January 23\n\nStafford Springs 16.0 inches (40.6 cm), 2:23 p.m. January 23<br>\nHebron 12.0 inches (30.5 cm), 1:20 p.m. January 23<br>\nAndover 7.0 inches (17.8 cm), 3:09 p.m. January 23<br>\nStorrs 7.0 inches (17.8 cm), 1:23 p.m. January 23\n\nNorth Grosvenordale 14.0 inches (35.6 cm), 2:18 p.m. January 23<br>\nPlainfield 9.5 inches (24.1 cm), 1:22 p.m. January 23<br>\nAshford 9.0 inches (22.9 cm), 1:16 p.m. January 23<br>\nEastford 8.0 inches (20.3 cm), 1:21 p.m. January 23\n\nMashpee 30.5 inches (77.5 cm), 5:23 p.m. January 23<br>\nSagamore Beach 30.0 inches (76.2 cm), 9:44 p.m. January 23<br>\nYarmouth Port 29.0 inches (73.7 cm), 2:40 p.m. January 23<br>\nBrewster 28.5 inches (72.4 cm), 8:20 p.m. January 23<br>\nHarwich Port 28.0 inches (71.1 cm), 10:49 p.m. January 23 general public<br>\nSandwich 27.5 inches (69.9 cm), 3:15 p.m. January 23 NWS employee<br>\nNorth Eastham 26.5 inches (67.3 cm), 5:28 p.m. January 23\n\nNew Bedford 26.0 inches (66 cm), 5 Ft (1.5 m) Drifts<br>\nTaunton 26.0 inches (66 cm), 3:31 p.m. January 23, 5 Ft (1.5 m) Drifts<br>\nFairhaven 25.5 inches (64.8 cm), 8:47 p.m. January 23<br>\nRehoboth 25.0 inches (63.5 cm), 2:47 p.m. January 23<br>\nEaston 23.0 inches (58.4 cm), 1:07 p.m. January 23 W.E. 1.71 <br>\nAcushnet 21.5 inches (54.6 cm), 3:04 p.m. January 23<br>\nSomerset 20.0 inches (50.8 cm), 3:03 p.m. January 23<br>\nTaunton 18.0 inches (45.7 cm), 7:00 p.m. January 23 NWS Office kbox<br>\nSeekonk 15.0 inches (38.1 cm), 4:30 p.m. January 23, 5 Ft (1.5 m) drifts\n\nEdgartown 24.0 inches (61.0 cm), 2:40 p.m. January 23\n\nSalem 38.0 inches (96.5 cm), 2:56 p.m. January 23 Em<br>\nNorth Beverly 32.0 inches (81.3 cm), 8:25 p.m. January 23<br>\nPeabody 30.0 inches (76.2 cm), 1:55 p.m. January 23<br>\nSaugus 30.0 inches (76.2 cm), 2:34 p.m. January 23<br>\nTopsfield 30.0 inches (76.2 cm), 5:52 p.m. January 23<br>\nWest Peabody 30.0 inches (76.2 cm), 3:53 p.m. January 23<br>\nMarblehead Neck 29.0 inches (73.7 cm), 2:34 p.m. January 23<br>\nMethuen 27.0 inches (68.6 cm), 2:04 p.m. January 23<br>\nSalem 27.0 inches (68.6 cm), 10:40 p.m. January 23 Salem state<br>\nHaverhill 26.5 inches (67.3 cm), 9:59 p.m. January 23, 7 Ft (2.1 m) drifts<br>\nManchester 26.5 inches (67.3 cm), 3:09 p.m. January 23<br>\nBeverly 26.0 inches (66 cm), 8:02 p.m. January 23 CO-Op observer<br>\nIpswich 26.0 inches (66 cm), 3:10 p.m. January 23<br>\nMarblehead 26.0 inches (66 cm), 9:41 p.m. January 23<br>\nLynn 24.0 inches (61 cm), 2:50 p.m. January 23<br>\nRowley 24.0 inches (61 cm), 2:56 p.m. January 23<br>\nSwampscott 24.0 inches (61 cm), 3:53 p.m. January 23<br>\nLawrence 23.0 inches (58.4 cm), 1:30 p.m. January 23\nNorth Andover 26.0 inches (66.0 cm), 1:00 p.m. January 23, 6 Ft (1.8 m) drifts<br>\n\nAshfield 15.5 inches (39.4 cm), 3:38 p.m. January 23\n\nChicopee 14.0 inches (35.6 cm), 11:56 p.m. January 23<br>\nGranville 12.0 inches (30.5 cm), 2:15 p.m. January 23<br>\nWilbraham 11.0 inches (27.9 cm), 2:07 p.m. January 23<br>\nSouthwick 10.0 inches (25.4 cm), 2:23 p.m. January 23, 0.84 inches (2.13 cm), we\n\nSouthampton 12.5 inches (31.8 cm), 2:06 p.m. January 23<br>\nNorthampton 12.0 inches (30.5 cm), 2:32 p.m. January 23<br>\nSouth Hadley 11.0 inches (27.9 cm), 2:31 p.m. January 23<br>\nAmherst 10.0 inches (25.4 cm), 2:06 p.m. January 23<br>\nBelchertown 8.5 inches (21.6 cm), 2:05 p.m. January 23\n\nMelrose 36.0 inches (91.4 cm), 1:16 p.m. January 23<br>\nCambridge 30.0 inches (76.2 cm), 2:58 p.m. January 23<br>\nWakefield 28.0 inches (71.1 cm), 4:00 p.m. January 23<br>\nBillerica 27.5 inches (69.9 cm), 9:17 p.m. January 23<br>\nSouth Chelmsford 27.0 inches (68.6 cm), 1:14 p.m. January 23<br>\nEverett 26.5 inches (67.3 cm), 3:00 p.m. January 23<br>\nBelmont 26.0 inches (66 cm), 9:18 p.m. January 23<br>\nNorth Billerica 25.0 inches (63.5 cm), 1:00 p.m. January 23<br>\nMalden 28.5 inches (71.6 cm), 2:30 p.m. January 23<br>\nWilmington 27.0 inches (68.6 cm), 1:37 p.m. January 23<br>\nWoburn 24.0 inches (61 cm), 10:40 p.m. January 23<br>\nWayland 22.0 inches (55.9 cm), 3:40 p.m. January 23<br>\nWestford 22.0 inches (55.9 cm), 5:26 p.m. January 23<br>\nLexington 21.0 inches (53.3 cm), 2:49 p.m. January 23<br>\nStoneham 21.0 inches (53.3 cm), 1:23 p.m. January 23<br>\nAyer 20.3 inches (51.6 cm), 3:30 p.m. January 23<br>\nLittleton 19.0 inches (48.3 cm), 2:41 p.m. January 23<br>\nChelmsford 18.5 inches (47 cm), 2:19 p.m. January 23<br>\nDracut 18.2 inches (46.2 cm), 2:20 p.m. January 23<br>\nPepperell 18.0 inches (45.7 cm), 2:49 p.m. January 23<br>\nTownsend 18.0 inches (45.7 cm), 4:02 p.m. January 23<br>\nShirley 14.0 inches (35.6 cm), 1:05 p.m. January 23<br>\nHudson 13.5 inches (34.3 cm), 3:00 p.m. January 23\n\nNantucket 24.0 inches (61 cm), 4:00 p.m. January 23\n\nWeymouth 28.5 inches (72.4 cm), 2:44 p.m. January 23<br>\nBraintree 28.3 inches (71.9 cm), 4:42 p.m. January 23<br>\nNorth Attleboro 28.1 inches (71.9 cm), 7:37 p.m. January 23<br>Milton 27.0 inches (68.6 cm), 4:00 p.m. January 23<br>\nSouth Weymouth 27.0 inches (68.6 cm), 9:18 p.m. January 23<br>\nSharon 26.0 inches (66 cm), 4:30 p.m. January 23<br>\nFoxboro 25.1 inches (63.8 cm), 5:11 p.m. January 23<br>\nMillis 25.0 inches (63.5 cm), 4:00 p.m. January 23<br>\nNeedham 25.0 inches (63.5 cm), 9:21 p.m. January 23<br>\nRandolph 25.0 inches (63.5 cm), 8:19 p.m. January 23<br>\nFranklin 23.0 inches (58.4 cm), 3:09 p.m. January 23<br>\nCanton 22.0 inches (55.9 cm), 2:59 p.m. January 23<br>\nDedham 22.0 inches (55.9 cm), 4:00 p.m. January 23<br>\nWalpole 20.5 inches (52.1 cm), 2:40 p.m. January 23<br>\nPlainville 20.0 inches (50.8 cm), 2:48 p.m. January 23<br>\nWellesley 17.2 inches (43.7 cm), 2:39 p.m. January 23\n\nPlymouth 36.0 inches (91.4 cm), 7:02 p.m. January 23<br>\nPlympton 35.0 inches (88.9 cm), 10:41 p.m. January 23 general public<br>\nLakeville 30.0 inches (76.2 cm), 1:17 p.m. January 23<br>\nManomet 28.0 inches (71.1 cm), 10:40 p.m. January 23<br>\nRockland 27.0 inches (68.6 cm), 8:03 p.m. January 23<br>\nWareham 26.0 inches (66 cm), 3:09 p.m. January 23<br>\nMarshfield 25.0 inches (63.5 cm), 2:30 p.m. January 23<br>\nHanson 24.2 inches (61.5 cm), 3:11 p.m. January 23<br>\nKingston 24.0 inches (61 cm), 9:41 p.m. January 23<br>\nWest Duxbury 24.0 inches (61 cm), 2:43 p.m. January 23<br>\nWhitman 23.0 inches (58.4 cm), 3:02 p.m. January 23<br>\nScituate 21.5 inches (54.6 cm), 4:01 p.m. January 23<br>\nBrockton 21.2 inches (53.8 cm), 5:20 p.m. January 23<br>\nMarion 21.0 inches (53.3 cm), 8:35 p.m. January 23<br>\nHingham 20.5 inches (52.1 cm), 5:07 p.m. January 23\n\nWinthrop 28.6 inches (72.6 cm), 4:00 p.m. January 23<br>\nWinthrop Square 27.0 inches (68.6 cm), 10:00 p.m. January 23<br>\nBoston Common 26.0 inches (66 cm), 1:16 p.m. January 23 NWS employee<br>\nRoslindale 25.5 inches (64.8 cm), 3:05 p.m. January 23<br>\nEast Boston 22.5 inches (57.2 cm), 7:00 p.m. January 23 Logan kbos\n\nNorthborough 26.0 inches (66 cm), 3:19 p.m. January 23<br>\nUxbridge 26.0 inches (66 cm), 1:29 p.m. January 23<br>\nShrewsbury 23.0 inches (58.4 cm), 3:25 p.m. January 23<br>\nWebster 23.0 inches (58.4 cm), 2:02 p.m. January 23<br>\nGardner 22.0 inches (55.9 cm), 2:50 p.m. January 23<br>\nSouthborough 22.0 inches (55.9 cm), 3:11 p.m. January 23<br>\nNorth Grafton 21.0 inches (53.3 cm), 1:04 p.m. January 23 ret NWS orh oic<br>\nFitchburg 20.7 inches (52.6 cm), 2:10 p.m. January 23<br>\nHolden 19.0 inches (48.3 cm), 2:04 p.m. January 23<br>\nLunenburg 18.5 inches (47 cm), 2:02 p.m. January 23<br>\nBoylston 18.1 inches (46 cm), 3:04 p.m. January 23<br>\nOld Sturbridge 18.0 inches (45.7 cm), 2:03 p.m. January 23<br>\nLeicester 17.0 inches (43.2 cm), 1:33 p.m. January 23, 3-4 Ft (0.9 m-1.2 m) drifts<br>\nSpencer 18.0 inches (45.7 cm), 2:05 p.m. January 23<br>\nOxford 15.0 inches (38.1 cm), 2:07 p.m. January 23<br>\nWest Brookfield 15.0 inches (38.1 cm), 2:30 p.m. January 23<br>\nAshburnham 14.0 inches (35.6 cm), 9:17 p.m. January 23<br>\nWest Warren 12.5 inches (31.8 cm), 3:14 p.m. January 23<br>\nAthol 12.0 inches (30.5 cm), 2:33 p.m. January 23\n\nDublin 18.5 inches (47 cm), 1:21 p.m. January 23<br>\nAlstead 12.5 inches (31.8 cm), 1:03 p.m. January 23 v\n\nHollis 19.0 inches (48.3 cm), 3:45 p.m. January 23<br>\nNashua 18.5 inches (47 cm), 4:00 p.m. January 23<br>\nNew Ipswich 17.5 inches (44.5 cm), 1:35 p.m. January 23<br>\nGreenville 16.5 inches (41.9 cm), 2:20 p.m. January 23<br>\nHudson 15.6 inches (39.6 cm), 1:36 p.m. January 23<br>\nSouth Weare 12.5 inches (31.8 cm), 4:29 p.m. January 23\n\nAtkinson 26.8 inches (68.07 cm) 7:00 a.m. January 23<br>\nPlaistow 24.0 inches (60.1 cm) 12:35 p.m. January 23\n\nKingston \n\nBristol 21.0 inches (53.3 cm), 5:02 p.m. January 23\n\nWest Warwick 24.5 inches (62.2 cm), 2:51 p.m. January 23<br>\nWarwick 23.4 inches (59.4 cm), 1:02 p.m. January 23 TF Green (Pvd)<br>\nWarwick 20.9 inches (53.1 cm), 3:16 p.m. January 23<br>\nCoventry 17.5 inches (44.5 cm), 4:40 p.m. January 23\n\nNorth Cumberland 27.0 inches (50.8 cm), 7:45 p.m. January 23<br>Johnston Memorial 22.5 inches (57.2 cm), 4:03 p.m. January 23 NWS employee<br>\nGreenville 21.5 inches (54.6 cm), 1:13 p.m. January 23<br>\nCranston 21.0 inches (53.3 cm), 2:53 p.m. January 23<br>\nRumford 19.0 inches (48.3 cm), 10:46 p.m. January 23<br>\nWoonsocket Reservoir 18.9 inches (48 cm), 1:55 p.m. January 23, 1.36 inches (3.45 cm), we<br>\nPawtucket 16.0 inches (40.6 cm), 2:10 p.m. January 23\n\nHopkinton 21.0 inches (53.3 cm), 2:36 p.m. January 23<br>\nWesterly 20.0 inches (50.8 cm), 5:35 p.m. January 23, 5 Ft (1.5 m) Drifts<br>\nNorth Kingstown 17.0 inches (43.2 cm), 1:02 p.m. January 23\n\nMorgantown \n\nYardley <br>\nLanghorne <br>\nPerkasie \n\nAlbrightsville <br>\nLehighton \n\nPhoenixville \nHoney Brook \n\nRadnor <br>\nBoothwyn \n\nLehigh Valley International Airport <br>\nGermansville \n\nSaylorsburg <br>\nPocono Summit \n\nConshohocken <br>\nGreen Lane <br>\nPalm \n\nForks Township <br>\nEaston \n\nNortheast Philadelphia <br>\nPhiladelphia International Airport \n\n"}
{"id": "54305940", "url": "https://en.wikipedia.org/wiki?curid=54305940", "title": "Pentamethylarsenic", "text": "Pentamethylarsenic\n\nPentamethylarsenic (or pentamethylarsorane)is an organometalllic compound containing five methyl groups bound to an arsenic atom with formula As(CH). It is an example of a hypervalent compound. The molecular shape is trigonal bipyramid.\n\nThe first claim to make pentamethylarsenic was in 1862 in a reaction of tetramethylarsonium iodide with dimethylzinc by A. Cahours. For many years all the reproductions of this proved fruitless, so the production proved not to be genuine. It was actually discovered by Karl-Heinz Mitschke and Hubert Schmidbaur in 1973.\n\nTrimethylarsine is chlorinated to trimethylarsine dichloride, which then reacts with methyl lithium to yield pentamethylarsenic.\n\nSide products include As(CH)Cl and As(CH)=CH.\n\nPentamethylarsenic is not produced by biological organisms.\n\nPentamethylarsenic smells the same as pentamethylantimony, but is otherwise unique.\n\nThe bond lengths in the molecule are for the three equatorial As−C bonds 1.975 Å and the two axial As−C bonds 2.073 Å.\n\nThe infrared spectrum of pentamethylarsenic shows strong bands at 582 and 358 cm due to axial C-As vibration, and weaker bands at 265 and 297 cm due to equatorial C-As vibration. Raman spectrum shows a strong feature at 519, 388, and 113 cm, and weak lines at 570 and 300 cm.\n\nPentamethylarsenic reacts slowly with weak acids. With water it forms tetramethylarsonium hydroxide As(CH)OH and trimethylarsenic oxide As(CH)O. With methanol, tetramethylmethoxyarsorane As(CH)OCH is produced. Hydrogen halides react resulting in the formation of tetramethylarsonium halide salts.\n\nWhen pentamethylarsenic is heated to 100° it decomposes forming trimethylarsine, methane, and ethylene.\n\nWhen trimethylindium reacts with pentamethylarsenic in benzene solution, a salt precipitates: tetramethylarsenic(V)tetramethylindate(III).\n"}
{"id": "27376338", "url": "https://en.wikipedia.org/wiki?curid=27376338", "title": "Pollyanna Pickering", "text": "Pollyanna Pickering\n\nPollyanna Pickering (30 July 1942 – 29 March 2018) was a wildlife artist and environmentalist.\n\nPickering was born in Yorkshire and was an English wildlife artist who went on many expeditions around the world to study animals in their natural habitats. She studied at Rotherham Art school and later at the Central School of Art and Design in London.\n\nPickering died on 29 March 2018 in Sheffield aged 75.\n\n"}
{"id": "31329101", "url": "https://en.wikipedia.org/wiki?curid=31329101", "title": "Polycrystalline silicon", "text": "Polycrystalline silicon\n\nPolycrystalline silicon, also called polysilicon or poly-Si, is a high purity, polycrystalline form of silicon, used as a raw material by the solar photovoltaic and electronics industry.\n\nPolysilicon is produced from metallurgical grade silicon by a chemical purification process, called the Siemens process. This process involves distillation of volatile silicon compounds, and their decomposition into silicon at high temperatures. An emerging, alternative process of refinement uses a fluidized bed reactor. The photovoltaic industry also produces upgraded metallurgical-grade silicon (UMG-Si), using metallurgical instead of chemical purification processes. When produced for the electronics industry, polysilicon contains impurity levels of less than one part per billion (ppb), while polycrystalline solar grade silicon (SoG-Si) is generally less pure. A few companies from China, Germany, Japan, Korea and the United States, such as GCL-Poly, Wacker Chemie, OCI, and Hemlock Semiconductor, as well as the Norwegian headquartered REC, accounted for most of the worldwide production of about 230,000 tonnes in 2013.\n\nThe polysilicon feedstock – large rods, usually broken into chunks of specific sizes and packaged in clean rooms before shipment – is directly cast into multicrystalline ingots or submitted to a recrystallization process to grow single crystal boules. The products are then sliced into thin silicon wafers and used for the production of solar cells, integrated circuits and other semiconductor devices.\n\nPolysilicon consists of small crystals, also known as crystallites, giving the material its typical metal flake effect. While polysilicon and multisilicon are often used as synonyms, multicrystalline usually refers to crystals larger than 1 mm. Multicrystalline solar cells are the most common type of solar cells in the fast-growing PV market and consume most of the worldwide produced polysilicon. About 5 tons of polysilicon is required to manufacture 1 megawatt (MW) of conventional solar modules. Polysilicon is distinct from monocrystalline silicon and amorphous silicon.\n\nIn single crystal silicon, also known as monocrystalline silicon, the crystalline framework is homogenous, which can be recognized by an even external colouring. The entire sample is one single, continuous and unbroken crystal as its structure contains no grain boundaries. Large single crystals are rare in nature and can also be difficult to produce in the laboratory (see also recrystallisation). In contrast, in an amorphous structure the order in atomic positions is limited to short range.\n\nPolycrystalline and paracrystalline phases are composed of a number of smaller crystals or \"crystallites.\" Polycrystalline silicon (or semi-crystalline silicon, polysilicon, poly-Si, or simply \"poly\") is a material consisting of multiple small silicon crystals. Polycrystalline cells can be recognized by a visible grain, a \"metal flake effect\". Semiconductor grade (also solar grade) polycrystalline silicon is converted to \"single crystal\" silicon – meaning that the randomly associated crystallites of silicon in \"polycrystalline silicon\" are converted to a large \"single\" crystal. Single crystal silicon is used to manufacture most Si-based microelectronic devices. Polycrystalline silicon can be as much as 99.9999% pure. Ultra-pure poly is used in the semiconductor industry, starting from poly rods that are two to three meters in length. In microelectronic industry (semiconductor industry), poly is used both at the macro-scale and micro-scale (component) level. Single crystals are grown using the Czochralski process, float-zone and Bridgman techniques.\n\nAt the component level, polysilicon has long been used as the conducting gate material in MOSFET and CMOS processing technologies. For these technologies it is deposited using low-pressure chemical-vapour deposition (LPCVD) reactors at high temperatures and is usually heavily doped n-type or p-type.\n\nMore recently, intrinsic and doped polysilicon is being used in large-area electronics as the active and/or doped layers in thin-film transistors. Although it can be deposited by LPCVD, plasma-enhanced chemical vapour deposition (PECVD), or solid-phase crystallization of amorphous silicon in certain processing regimes, these processes still require relatively high temperatures of at least 300 °C. These temperatures make deposition of polysilicon possible for glass substrates but not for plastic substrates.\n\nThe deposition of polycrystalline silicon on plastic substrates is motivated by the desire to be able to manufacture digital displays on flexible screens. Therefore, a relatively new technique called laser crystallization has been devised to crystallize a precursor amorphous silicon (a-Si) material on a plastic substrate without melting or damaging the plastic. Short, high-intensity ultraviolet laser pulses are used to heat the deposited a-Si material to above the melting point of silicon, without melting the entire substrate.\n\nThe molten silicon will then crystallize as it cools. By precisely controlling the temperature gradients, researchers have been able to grow very large grains, of up to hundreds of micrometers in size in the extreme case, although grain sizes of 10 nanometers to 1 micrometer are also common. In order to create devices on polysilicon over large-areas however, a crystal grain size smaller than the device feature size is needed for homogeneity of the devices. Another method to produce poly-Si at low temperatures is metal-induced crystallization where an amorphous-Si thin film can be crystallized at temperatures as low as 150 °C if annealed while in contact of another metal film such as aluminium, gold, or silver.\n\nPolysilicon has many applications in VLSI manufacturing. One of its primary uses is as gate electrode material for MOS devices. A polysilicon gate's electrical conductivity may be increased by depositing a metal (such as tungsten) or a metal silicide (such as tungsten silicide) over the gate. Polysilicon may also be employed as a resistor, a conductor, or as an ohmic contact for shallow junctions, with the desired electrical conductivity attained by doping the polysilicon material.\n\nOne major difference between polysilicon and a-Si is that the mobility of the charge carriers of the polysilicon can be orders of magnitude larger and the material also shows greater stability under electric field and light-induced stress. This allows more complex, high-speed circuity to be created on the glass substrate along with the a-Si devices, which are still needed for their low-leakage characteristics. When polysilicon and a-Si devices are used in the same process this is called hybrid processing. A complete polysilicon active layer process is also used in some cases where a small pixel size is required, such as in projection displays.\n\nPolycrystalline silicon is the key feedstock in the crystalline silicon based photovoltaic industry and used for the production of conventional solar cells. For the first time, in 2006, over half of the world's supply of polysilicon was being used by PV manufacturers. The solar industry was severely hindered by a shortage in supply of polysilicon feedstock and was forced to idle about a quarter of its cell and module manufacturing capacity in 2007. Only twelve factories were known to produce solar-grade polysilicon in 2008; however, by 2013 the number increased to over 100 manufacturers. Monocrystalline silicon is higher priced and a more efficient semiconductor than polycrystalline as it went through the additional recrystallization by the Czochralski process.\n\nPolysilicon deposition, or the process of depositing a layer of polycrystalline silicon on a semiconductor wafer, is achieved by the chemical decomposition of silane (SiH) at high temperatures of 580 to 650 °C. This pyrolysis process releases hydrogen.\n\nPolysilicon layers can be deposited using 100% silane at a pressure of or with 20–30% silane (diluted in nitrogen) at the same total pressure. Both of these processes can deposit polysilicon on 10–200 wafers per run, at a rate of 10–20 nm/min and with thickness uniformities of ±5%. Critical process variables for polysilicon deposition include temperature, pressure, silane concentration, and dopant concentration. Wafer spacing and load size have been shown to have only minor effects on the deposition process. The rate of polysilicon deposition increases rapidly with temperature, since it follows Arrhenius behavior, that is deposition rate = A·exp(–qE/kT) where q is electron charge and k is the Boltzmann constant. The activation energy (E) for polysilicon deposition is about 1.7 eV. Based on this equation, the rate of polysilicon deposition increases as the deposition temperature increases. There will be a minimum temperature, however, wherein the rate of deposition becomes faster than the rate at which unreacted silane arrives at the surface. Beyond this temperature, the deposition rate can no longer increase with temperature, since it is now being hampered by lack of silane from which the polysilicon will be generated. Such a reaction is then said to be 'mass-transport-limited.' When a polysilicon deposition process becomes mass-transport-limited, the reaction rate becomes dependent primarily on reactant concentration, reactor geometry, and gas flow.\n\nWhen the rate at which polysilicon deposition occurs is slower than the rate at which unreacted silane arrives, then it is said to be surface-reaction-limited. A deposition process that is surface-reaction-limited is primarily dependent on reactant concentration and reaction temperature. Deposition processes must be surface-reaction-limited because they result in excellent thickness uniformity and step coverage. A plot of the logarithm of the deposition rate against the reciprocal of the absolute temperature in the surface-reaction-limited region results in a straight line whose slope is equal to –qE/k.\n\nAt reduced pressure levels for VLSI manufacturing, polysilicon deposition rate below 575 °C is too slow to be practical. Above 650 °C, poor deposition uniformity and excessive roughness will be encountered due to unwanted gas-phase reactions and silane depletion. Pressure can be varied inside a low-pressure reactor either by changing the pumping speed or changing the inlet gas flow into the reactor. If the inlet gas is composed of both silane and nitrogen, the inlet gas flow, and hence the reactor pressure, may be varied either by changing the nitrogen flow at constant silane flow, or changing both the nitrogen and silane flow to change the total gas flow while keeping the gas ratio constant. Recent investigations have shown that e-beam evaporation, followed by SPC (if needed) can be a cost effective and faster alternative for producing solar grade poly-Si thin films. Modules produced by such method are shown to have a photovoltaic efficiency of ~6%.\n\nPolysilicon doping, if needed, is also done during the deposition process, usually by adding phosphine, arsine, or diborane. Adding phosphine or arsine results in slower deposition, while adding diborane increases the deposition rate. The deposition thickness uniformity usually degrades when dopants are added during deposition.\n\nUpgraded metallurgical-grade (UMG) silicon (also known as UMG-Si) solar cell is being produced as a low cost alternative to polysilicon created by the Siemens process. UMG-Si greatly reduces impurities in a variety of ways that require less equipment and energy than the Siemens process. It is about 99% pure which is three or more orders of magnitude less pure and about 10 times less expensive than polysilicon ($1.70 to $3.20 per kg from 2005 to 2008 compared to $40 to $400 per kg for polysilicon). It has the potential to provide nearly-as-good solar cell efficiency at 1/5 the capital expenditure, half the energy requirements, and less than $15/kg.\n\nIn 2008 several companies were touting the potential of UMG-Si in 2010, but the credit crisis greatly lowered the cost of polysilicon and several UMG-Si producers put plans on hold. The Siemens process will remain the dominant form of production for years to come due to more efficiently implementing the Siemens process. GT Solar claims a new Siemens process can produce at $27/kg and may reach $20/kg in 5 years. GCL-Poly expects production costs to be $20/kg by end of 2011. Elkem Solar estimates their UMG costs to be $25/kg, with a capacity of 6,000 tonnes by the end of 2010. Calisolar expects UMG technology to produce at $12/kg in 5 years with boron at 0.3 ppm and phosphorus at 0.6 ppm. At $50/kg and 7.5 g/W, module manufacturers spend $0.37/W for the polysilicon. For comparison, if a CdTe manufacturer pays spot price for tellurium ($420/kg in April 2010) and has a 3 µm thickness, their cost would be 10 times less, $0.037/Watt. At 0.1 g/W and $31/ozt for silver, polysilicon solar producers spend $0.10/W on silver.\nQ-Cells, Canadian Solar, and Calisolar have used Timminco UMG. Timminco is able to produce UMG-Si with 0.5 ppm boron for $21/kg but were sued by shareholders because they had expected $10/kg. RSI and Dow Corning have also been in litigation over UMG-Si technology.\n\nCurrently, polysilicon is commonly used for the conducting gate materials in semiconductor devices such as MOSFETs; however, it has potential for large-scale photovoltaic devices. The abundance, stability, and low toxicity of silicon, combined with the low cost of polysilicon relative to single crystals makes this variety of material attractive for photovoltaic production. Grain size has been shown to have an effect on the efficiency of polycrystalline solar cells. Solar cell efficiency increases with grain size. This effect is due to reduced recombination in the solar cell. Recombination, which is a limiting factor for current in a solar cell, occurs more prevalently at grain boundaries, see figure 1.\n\nThe resistivity, mobility, and free-carrier concentration in monocrystalline silicon vary with doping concentration of the single crystal silicon. Whereas the doping of polycrystalline silicon does have an effect on the resistivity, mobility, and free-carrier concentration, these properties strongly depend on the polycrystalline grain size, which is a physical parameter that the material scientist can manipulate. Through the methods of crystallization to form polycrystalline silicon, an engineer can control the size of the polycrystalline grains which will vary the physical properties of the material.\n\nThe use of polycrystalline silicon in the production of solar cells requires less material and therefore provides higher profits and increased manufacturing throughput. Polycrystalline silicon does not need to be deposited on a silicon wafer to form a solar cell, rather it can be deposited on other-cheaper materials, thus reducing the cost. Not requiring a silicon wafer alleviates the silicon shortages occasionally faced by the microelectronics industry. An example of not using a silicon wafer is crystalline silicon on glass (CSG) materials \n\nA primary concern in the photovoltaics industry is cell efficiency. However, sufficient cost savings from cell manufacturing can be suitable to offset reduced efficiency in the field, such as the use of larger solar cell arrays compared with more compact/higher efficiency designs. Designs such as CSG are attractive because of a low cost of production even with reduced efficiency. Higher efficiency devices yield modules that occupy less space and are more compact; however, the 5–10% efficiency of typical CSG devices still makes them attractive for installation in large central-service stations, such as a power station. The issue of efficiency versus cost is a value decision of whether one requires an “energy dense” solar cell or sufficient area is available for the installation of less expensive alternatives. For instance, a solar cell used for power generation in a remote location might require a more highly efficient solar cell than one used for low-power applications, such as solar accent lighting or pocket calculators, or near established power grids.\n\nThe polysilicon manufacturing market is growing rapidly. According to Digitimes, in July 2011, the total polysilicon production in 2010 was 209,000 tons. First-tier suppliers account for 64% of the market while China-based polysilicon firms have 30% of market share. The total production is likely to increase 37.4% to 281,000 tons by end of 2011. For 2012, EETimes Asia predicts 328,000 tons production with only 196,000 tons of demand, with spot prices expected to fall 56%. While good for renewable energy prospects, the subsequent drop in price could be brutal for manufacturers. As of late 2012, SolarIndustryMag reports a capacity of 385,000 tons will be reached by yearend 2012.\n\nBut as established producers (mentioned below) expand their capacities, additional newcomers – many from Asia – are moving into the market. Even long-time players in the field have recently had difficulties expanding plant production. It is yet unclear which companies will be able to produce at costs low enough to be profitable after the steep drop in spot-prices of the last months.\nLeading producer capacities.\n\nWacker's projected its total hyperpure-polysilicon production capacity to increase to 67,000 metric tons by 2014, due to its new polysilicon-production facility in Cleveland, Tennessee (USA) with an annual capacity of 15,000 metric tons.\n\n\n\nPrices of polysilicon are often divided into two categories, contract and spot prices, and higher purity commands higher prices. While in booming installation times, price rally occurs in polysilicon. Not only spot prices surpass contract prices in the market; but it is also hard to acquire enough polysilicon. Buyers will accept down payment and long term agreements to acquire a large enough volume of polysilicon. On the contrary, spot prices will be below contract prices once the solar PV installation is in a down trend. In late 2010, booming installation brought up the spot prices of polysilicon. In the first half of 2011, prices of polysilicon kept strong owing to the FIT policies of Italy. The solar PV price survey and market research firm, PVinsights, reported that the prices of polysilicon might be dragged down by lack of installation in the second half of 2011. As recently as 2008 prices were over $400/kg spiking from levels around $200/kg, while seen falling to $15/kg in 2013.\n\nThe Chinese government accused United States and South Korean manufacturers of predatory pricing or \"dumping\". As a consequence, in 2013 it imposed import tariffs of as much as 57 percent on polysilicon shipped from these two countries in order to stop the product from being sold below cost.\n\nDue to the rapid growth in manufacturing in China and the lack of regulatory controls, there have been reports of the dumping of waste silicon tetrachloride. Normally the waste silicon tetrachloride is recycled but this adds to the cost of manufacture as it needs to be heated to .\n"}
{"id": "4248652", "url": "https://en.wikipedia.org/wiki?curid=4248652", "title": "Savandurga", "text": "Savandurga\n\nSavandurga is a hill located in Ramnagara District (Karnataka, India) 60 km west of Bengaluru off the Magadi road , in India. The hill is considered to be among the largest monolith hills in Asia. The hill rises to 1227 m above mean sea level and forms a part of the Deccan plateau. It consists of peninsular gneiss, granites, basic dykes and laterites. The Arkavathi river passes nearby through the Thippagondanahalli reservoir and on towards Manchanabele dam.\n\nSavandurga is formed by two hills known locally as Karigudda (black hill) and Biligudda (white hill). The earliest record of the name of the hill is from 1340 AD by Hoysala Ballala III from Madabalu where it is called \"Savandi\". Another view is that the name is originated from \"Samantadurga\" attributed to a \"Samantharaya\", a governor under Ahchutaraya at Magadi, although there is no inscription confirming this. This was the secondary capital of the Magadi rulers such as Kempegowda.\n\nFrom 1638 to 1728, Mysore took over this place and Dalavayi Devaraja occupied this place with the palace at Nelapattana. In 1791 Lord Cornwallis captured it from Tipu Sultan's forces during the Third Anglo-Mysore War.\n\nThe area was known for megalithic settlements, the remains of one structure was described while a grave was excavated by Col. Branwell in 1881 and was described in details. It included the skeleton of a man who may have sustained injuries to his skull and died from it. It also included iron spear heads.\n\nRobert Home in his \"Select views in Mysore\" (1794) shows distant views of the hill from Bangalore. He called it \"Savinadurga\" or the \"fort of death\". There were no steps to reach the hill top and it was covered by bamboos and other trees forming a barricade.\n\nThe hills are home to the endangered yellow-throated bulbuls and were once home to long-billed vultures and white-backed vultures. Other wildlife include sloth bear and leopard. The rocks with trees growing in the cracks and valleys serve are the favoured habitat of the yellow-throated bulbuls.\nSurrounding the area is a state forest with scrub and dry deciduous forest covering 27 km. The degraded forest, which is considered as shrub and tree savanna of the \"Anogeissus\"–\"Chloroxylon\"–\"Acacia\" series is highly diverse, recording over 59 tree and 119 shrub species.\nSome of the plant species recorded here include:\n\n"}
{"id": "49763666", "url": "https://en.wikipedia.org/wiki?curid=49763666", "title": "Soap shaker", "text": "Soap shaker\n\nA soap shaker is a box entirely made from wire metal mesh with a handle. The box may be opened so as to be able to place in this box a piece or pieces of bar soap. These may be pieces that have become too small to be used as hand soap.\n\nThe box may now be securely closed. Held by its handle the box may be vigorously shaken in a water filled bucket or other container. The shaking will move the water through the box. The result is that the water will become soapy, rich with suds to be used for all kinds of cleaning purposes. This way even small pieces of bar soap could be re-used and are not wasted.\n\nThe use of a soap shaker was common early to mid 20th century. The invention and sale of powdered or liquid soap diminished its use.\n\n"}
{"id": "183824", "url": "https://en.wikipedia.org/wiki?curid=183824", "title": "Sonic boom", "text": "Sonic boom\n\nA sonic boom is the sound associated with the shock waves created whenever an object travelling through the air travels faster than the speed of sound. Sonic booms generate enormous amounts of sound energy, sounding similar to an explosion or a thunderclap to the human ear. The crack of a supersonic bullet passing overhead or the crack of a bullwhip are examples of a sonic boom in miniature.\n\nSonic booms due to large supersonic aircraft can be particularly loud and startling, tend to awaken people, and may cause minor damage to some structures. They led to prohibition of routine supersonic flight over land. Although they cannot be completely prevented, research suggests that with careful shaping of the vehicle the nuisance due to them may be reduced to the point that overland supersonic flight may become a practical option.\n\nA sonic boom does not occur only at the moment an object crosses the speed of sound; and neither is it heard in all directions emanating from the speeding object. Rather the boom is a continuous effect that occurs while the object is travelling at supersonic speeds. But it only affects observers that are positioned at a point that intersects a region in the shape of a geometrical cone behind the object. As the object moves, this conical region also moves behind it and when the cone passes over the observer, they will briefly experience the \"boom\".\n\nWhen an aircraft passes through the air it creates a series of pressure waves in front of the aircraft and behind it, similar to the bow and stern waves created by a boat. These waves travel at the speed of sound and, as the speed of the object increases, the waves are forced together, or compressed, because they cannot get out of each other's way quickly enough. Eventually they merge into a single shock wave, which travels at the speed of sound, a critical speed known as \"Mach 1\", and is approximately at sea level and .\n\nIn smooth flight, the shock wave starts at the nose of the aircraft and ends at the tail. Because the different radial directions around the aircraft's direction of travel are equivalent (given the \"smooth flight\" condition), the shock wave forms a \"Mach cone\", similar to a vapour cone, with the aircraft at its tip. The half-angle between direction of flight and the shock wave formula_1 is given by:\n\nwhere formula_3 is the inverse formula_4 of the plane's Mach number (formula_5). Thus the faster the plane travels, the finer and more pointed the cone is.\n\nThere is a rise in pressure at the nose, decreasing steadily to a negative pressure at the tail, followed by a sudden return to normal pressure after the object passes. This \"overpressure profile\" is known as an N-wave because of its shape. The \"boom\" is experienced when there is a sudden change in pressure; therefore, an N-wave causes two booms – one when the initial pressure-rise reaches an observer, and another when the pressure returns to normal. This leads to a distinctive \"double boom\" from a supersonic aircraft. When the aircraft is maneuvering, the pressure distribution changes into different forms, with a characteristic U-wave shape.\n\nSince the boom is being generated continually as long as the aircraft is supersonic, it fills out a narrow path on the ground following the aircraft's flight path, a bit like an unrolling red carpet, and hence known as the \"boom carpet\". Its width depends on the altitude of the aircraft. The distance from the point on the ground where the boom is heard to the aircraft depends on its altitude and the angle formula_1.\n\nFor today's supersonic aircraft in normal operating conditions, the peak overpressure varies from less than 50 to 500 Pa (1 to 10 psf (pound per square foot)) for an N-wave boom. Peak overpressures for U-waves are amplified two to five times the N-wave, but this amplified overpressure impacts only a very small area when compared to the area exposed to the rest of the sonic boom. The strongest sonic boom ever recorded was 7,000 Pa (144 psf) and it did not cause injury to the researchers who were exposed to it. The boom was produced by an F-4 flying just above the speed of sound at an altitude of . In recent tests, the maximum boom measured during more realistic flight conditions was 1,010 Pa (21 psf). There is a probability that some damage — shattered glass, for example — will result from a sonic boom. Buildings in good condition should suffer no damage by pressures of 530 Pa (11 psf) or less. And, typically, community exposure to sonic boom is below 100 Pa (2 psf). Ground motion resulting from sonic boom is rare and is well below structural damage thresholds accepted by the U.S. Bureau of Mines and other agencies.\n\nThe power, or volume, of the shock wave depends on the quantity of air that is being accelerated, and thus the size and shape of the aircraft. As the aircraft increases speed the shock cone gets \"tighter\" around the craft and becomes weaker to the point that at very high speeds and altitudes no boom is heard. The \"length\" of the boom from front to back depends on the length of the aircraft to a power of 3/2. Longer aircraft therefore \"spread out\" their booms more than smaller ones, which leads to a less powerful boom.\n\nSeveral smaller shock waves can and usually do form at other points on the aircraft, primarily at any convex points, or curves, the leading wing edge, and especially the inlet to engines. These secondary shockwaves are caused by the air being forced to turn around these convex points, which generates a shock wave in supersonic flow.\n\nThe later shock waves are somewhat faster than the first one, travel faster and add to the main shockwave at some distance away from the aircraft to create a much more defined N-wave shape. This maximizes both the magnitude and the \"rise time\" of the shock which makes the boom seem louder. On most aircraft designs the characteristic distance is about , meaning that below this altitude the sonic boom will be \"softer\". However, the drag at this altitude or below makes supersonic travel particularly inefficient, which poses a serious problem.\n\nThe pressure from sonic booms caused by aircraft often are a few pounds per square foot. A vehicle flying at greater altitude will generate lower pressures on the ground, because the shock wave reduces in intensity as it spreads out away from the vehicle, but the sonic booms are less affected by vehicle speed.\n\nIn the late 1950s when supersonic transport (SST) designs were being actively pursued, it was thought that although the boom would be very large, the problems could be avoided by flying higher. This assumption was proven false when the North American B-70 \"Valkyrie\" started flying, and it was found that the boom was a problem even at 70,000 feet (21,000 m). It was during these tests that the N-wave was first characterized.\n\nRichard Seebass and his colleague Albert George at Cornell University studied the problem extensively and eventually defined a \"figure of merit\" (FM) to characterize the sonic boom levels of different aircraft. FM is a function of the aircraft weight and the aircraft length. The lower this value, the less boom the aircraft generates, with figures of about 1 or lower being considered acceptable. Using this calculation, they found FMs of about 1.4 for Concorde and 1.9 for the Boeing 2707. This eventually doomed most SST projects as public resentment mixed with politics eventually resulted in laws that made any such aircraft impractical (flying supersonically only over water for instance). Another way to express this is wing span. The fuselage of even a large supersonic aircraft is very sleek and with enough angle of attack and wing span the plane can fly so high that the boom by the fuselage is not important. The larger the wing span, the greater the downwards impulse which can be applied to the air, the greater the boom felt. A smaller wing span favors small aeroplane designs like business jets.\n\nSeebass and George also worked on the problem from a different angle, trying to spread out the N-wave laterally and temporally (longitudinally), by producing a strong and downwards-focused (SR-71 Blackbird, Boeing X-43) shock at a sharp, but wide angle nosecone, which will travel at slightly supersonic speed (bow shock), and using a swept back flying wing or an oblique flying wing to smooth out this shock along the direction of flight (the tail of the shock travels at sonic speed). To adapt this principle to existing planes, which generate a shock at their nose cone and an even stronger one at their wing leading edge, the fuselage below the wing is shaped according to the area rule. Ideally this would raise the characteristic altitude from to 60,000 feet (from 12,000 m to 18,000 m), which is where most SST aircraft were expected to fly.\nThis remained untested for decades, until DARPA started the Quiet Supersonic Platform project and funded the Shaped Sonic Boom Demonstration (SSBD) aircraft to test it. SSBD used an F-5 Freedom Fighter. The F-5E was modified with a highly refined shape which lengthened the nose to that of the F-5F model. The fairing extended from the nose all the way back to the inlets on the underside of the aircraft. The SSBD was tested over a two-year period culminating in 21 flights and was an extensive study on sonic boom characteristics. After measuring the 1,300 recordings, some taken inside the shock wave by a chase plane, the SSBD demonstrated a reduction in boom by about one-third. Although one-third is not a huge reduction, it could have reduced Concorde's boom to an acceptable level; one below the FM = 1 limit stated above, for instance.\n\nAs a follow-on to SSBD, in 2006 a NASA-Gulfstream Aerospace team tested the Quiet Spike on NASA-Dryden's F-15B aircraft 836. The Quiet Spike is a telescoping boom fitted to the nose of an aircraft specifically designed to weaken the strength of the shock waves forming on the nose of the aircraft at supersonic speeds. Over 50 test flights were performed. Several flights included probing of the shockwaves by a second F-15B, NASA's Intelligent Flight Control System testbed, aircraft 837.\n\nThere are theoretical designs that do not appear to create sonic booms at all, such as the Busemann's Biplane. However, creating a shockwave is inescapable if they generate aerodynamic lift.\n\nNASA and Lockheed Martin Aeronautics Co. are working together to build an experimental aircraft called the Low Boom Flight Demonstrator (LBFD), which will reduce the sonic boom synonymous with high-speed flight to the sound of a car door closing. The agency has awarded a $247.5 million contract to construct a working version of the sleek, single-pilot plane by summer 2021 and should begin testing over the following years to determine whether the design could eventually be adapted to commercial aircraft.\n\nThe sound of a sonic boom depends largely on the distance between the observer and the aircraft shape producing the sonic boom. A sonic boom is usually heard as a deep double \"boom\" as the aircraft is usually some distance away. However, as those who have witnessed landings of space shuttles have heard, when the aircraft is nearby the sonic boom is a sharper \"bang\" or \"crack\". The sound is much like that of mortar bombs, commonly used in firework displays. It is a common misconception that only one boom is generated during the subsonic to supersonic transition; rather, the boom is continuous along the boom carpet for the entire supersonic flight. As a former Concorde pilot puts it, \"You don't actually hear anything on board. All we see is the pressure wave moving down the aeroplane - it gives an indication on the instruments. And that's what we see around Mach 1. But we don't hear the sonic boom or anything like that. That's rather like the wake of a ship - it's behind us.\".\n\nIn 1964, NASA and the Federal Aviation Administration began the Oklahoma City sonic boom tests, which caused eight sonic booms per day over a period of six months. Valuable data was gathered from the experiment, but 15,000 complaints were generated and ultimately entangled the government in a class action lawsuit, which it lost on appeal in 1969.\n\nSonic booms were also a nuisance in North Cornwall and North Devon in the UK as these areas were underneath the flight path of Concorde. Windows would rattle and in some cases the \"torching\" (pointing underneath roof slates) would be dislodged with the vibration.\n\nThere has been recent work in this area, notably under DARPA's Quiet Supersonic Platform studies. Research by acoustics experts under this program began looking more closely at the composition of sonic booms, including the frequency content. Several characteristics of the traditional sonic boom \"N\" wave can influence how loud and irritating it can be perceived by listeners on the ground. Even strong N-waves such as those generated by Concorde or military aircraft can be far less objectionable if the rise time of the overpressure is sufficiently long. A new metric has emerged, known as \"perceived\" loudness, measured in PLdB. This takes into account the frequency content, rise time, etc. A well-known example is the snapping of one's fingers in which the \"perceived\" sound is nothing more than an annoyance.\n\nThe energy range of sonic boom is concentrated in the 0.1–100 hertz frequency range that is considerably below that of subsonic aircraft, gunfire and most industrial noise. Duration of sonic boom is brief; less than a second, 100 milliseconds (0.1 second) for most fighter-sized aircraft and 500 milliseconds for the space shuttle or Concorde jetliner. The intensity and width of a sonic boom path depends on the physical characteristics of the aircraft and how it is operated. In general, the greater an aircraft's altitude, the lower the overpressure on the ground. Greater altitude also increases the boom's lateral spread, exposing a wider area to the boom. Overpressures in the sonic boom impact area, however, will not be uniform. Boom intensity is greatest directly under the flight path, progressively weakening with greater horizontal distance away from the aircraft flight track. Ground width of the boom exposure area is approximately for each of altitude (the width is about five times the altitude); that is, an aircraft flying supersonic at will create a lateral boom spread of about . For steady supersonic flight, the boom is described as a carpet boom since it moves with the aircraft as it maintains supersonic speed and altitude. Some manoeuvers, diving, acceleration or turning, can cause focusing of the boom. Other manoeuvers, such as deceleration and climbing, can reduce the strength of the shock. In some instances weather conditions can distort sonic booms.\n\nDepending on the aircraft's altitude, sonic booms reach the ground two to 60 seconds after flyover. However, not all booms are heard at ground level. The speed of sound at any altitude is a function of air temperature. A decrease or increase in temperature results in a corresponding decrease or increase in sound speed. Under standard atmospheric conditions, air temperature decreases with increased altitude. For example, when sea-level temperature is 59 degrees Fahrenheit (15 °C), the temperature at drops to minus 49 degrees Fahrenheit (−45 °C). This temperature gradient helps bend the sound waves upward. Therefore, for a boom to reach the ground, the aircraft speed relative to the ground must be greater than the speed of sound at the ground. For example, the speed of sound at is about , but an aircraft must travel at least (Mach 1.12, where Mach 1 equals the speed of sound) for a boom to be heard on the ground.\n\nThe composition of the atmosphere is also a factor. Temperature variations, humidity, atmospheric pollution, and winds can all have an effect on how a sonic boom is perceived on the ground. Even the ground itself can influence the sound of a sonic boom. Hard surfaces such as concrete, pavement, and large buildings can cause reflections which may amplify the sound of a sonic boom. Similarly grassy fields and lots of foliage can help attenuate the strength of the overpressure of a sonic boom.\n\nCurrently there are no industry accepted standards for the acceptability of a sonic boom. Until such metrics can be established, either through further study or supersonic overflight testing, it is doubtful that legislation will be enacted to remove the current prohibition on supersonic overflight in place in several countries, including the United States.\n\nThe cracking sound a bullwhip makes when properly wielded is, in fact, a small sonic boom. The end of the whip, known as the \"cracker\", moves faster than the speed of sound, thus creating a sonic boom. The whip is probably the first human invention to break the sound barrier.\n\nA bullwhip tapers down from the handle section to the cracker. The cracker has much less mass than the handle section. When the whip is sharply swung, the energy is transferred down the length of the tapering whip. Goriely and McMillen showed that the physical explanation is complex, involving the way that a loop travels down a tapered filament under tension.\n\n\n"}
{"id": "7206727", "url": "https://en.wikipedia.org/wiki?curid=7206727", "title": "Sound amplification by stimulated emission of radiation", "text": "Sound amplification by stimulated emission of radiation\n\nSound amplification by stimulated emission of radiation (SASER) refers to a device that emits acoustic radiation. It focuses sound waves in a way that they can serve as accurate and high-speed carriers of information in many kinds of applications—similar to uses of laser light.\n\nAcoustic radiation (sound waves) can be emitted by using the process of sound amplification based on stimulated emission of phonons. Sound (or lattice vibration) can be described by a phonon just as light can be considered as photons, and therefore one can state that SASER is the acoustic analogue of the laser.\n\nIn a SASER device, a source (e.g., an electric field as a pump) produces sound waves (lattice vibrations, phonons) that travel through an active medium. In this active medium, a stimulated emission of phonons leads to amplification of the sound waves, resulting in a sound beam coming out of the device. The sound wave beams emitted from such devices are highly coherent.\n\nThe first successful SASERs were developed in 2009.\n\nInstead of a feedback-built wave of electromagnetic radiation (i.e., a laser beam), a SASER delivers a sound wave. SASER may also be referred to as \"phonon laser\", \"acoustic laser\" or \"sound laser\".\n\nSASERs could have wide applications. Apart from facilitating the investigation of terahertz-frequency ultrasound, the SASER is also likely to find uses in optoelectronics (electronic devices that detect and control light—as a method of transmitting a signal from an end to the other of, for instance, fiber optics), as a method of signal modulation and/or transmission.\n\nSuch devices could be high precision measurement instruments and they could lead to high energy focused sound.\n\nUsing SASERs to manipulate electrons inside semiconductors could theoretically result in terahertz-frequency computer processors, much faster than the current chips.\n\nThis concept can be more conceivable by imagining it in analogy to laser theory. Theodore Maiman operated the first functioning LASER on May 16, 1960 at Hughes Research Laboratories, Malibu, California, A device that operates according to the central idea of the \"sound amplification by stimulated emission of radiation\" theory is the \"thermoacoustic laser\". This is a half-open pipe with a heat differential across a special porous material inserted in the pipe. Much like a light laser, a thermoacoustic SASER has a high-Q cavity and uses a gain medium to amplify coherent waves. For further explanation see thermoacoustic heat engine.\n\nThe possibility of phonon laser action had been proposed in a wide range of physical systems such as nanomechanics, semiconductors, nanomagnets and paramagnetic ions in a lattice.\n\nFinding materials that stimulate emission was needed for the development of the SASER. The generation of coherent phonons in a double-barrier semiconductor heterostructure was first proposed around 1990. The transformation of the electric potential energy in a vibrational mode of the lattice is remarkably facilitated by the electronic confinement in a double-barrier structure. On this basis, physicists were searching for materials in which stimulated emission rather than spontaneous emission, is the dominant decay process. A device was first experimentally demonstrated in the Gigahertz range in 2009.\n\nAnnounced in 2010, two independent groups came up with two different devices that produce coherent phonons at any frequency in the range megahertz to terahertz. One group from the University of Nottingham consisted of A.J. Kent and his colleagues R.P. Beardsley, A.V. Akimov, W. Maryam and M. Henini. The other group from the California Institute of Technology (Caltech) consisted of Ivan S. Grudinin, Hansuek Lee, O. Painter and Kerry J. Vahala from Caltech implemented a study on Phonon Laser Action in a tunable two-level system. The University of Nottingham device operates at about 440 GHz, while the Caltech device operates in the megahertz range. According to a member of the Nottingham group, the two approaches are complementary and it should be possible to use one device or the other to create coherent phonons at any frequency in the megahertz to terahertz range. A significant result rises from the operating frequency of these devices. The differences between the two devices suggest that SASERs could be made to operate over a wide range of frequencies.\n\nWork on the SASER continues at the University of Nottingham, the \"Lashkarev Institute of Semiconductor Physics\" at the National Academy of Sciences of Ukraine, and Caltech.\n\nSASER's central idea is based on sound waves. The set-up needed for the implement of sound amplification by stimulated emission of radiation is similar to an oscillator. An oscillator can produce oscillations without any external feed-mechanism. An example is a common sound amplification system with a microphone, amplifier and speaker. When the microphone is in front of the speaker, we hear an annoying whistle. This whistle is generated without extra contribution from the sound source, and is self-reinforced and self-sufficient while the microphone is somewhere in front of the speaker. This phenomenon, known as the Larsen effect, is the result of a positive feedback.\n\nIn general, every oscillator consists of three main parts. These are the power source or pump, the amplifier and the positive feedback leading to the output. The corresponding parts in a SASER device are the excitation or pumping mechanism, the active (amplifying) medium, and the feedback leading to acoustic radiation. Pumping can be performed, for instance, with an alternating electric field or with some mechanical vibrations of resonators. The active medium should be a material in which sound amplification can be induced. An example of a feedback mechanism into the active medium is the existence of superlattice layers that reflect the phonons back and force them to bounce repeatedly to amplify sound.\n\nTherefore, to proceed to an understanding of a SASER design we need to imagine it in analogy with a laser device. In a laser, the active medium is placed between two mirror surfaces (reflectors)of a Fabry–Pérot interferometer. A spontaneously emitted photon inside this interferometer can force excited atoms to decay a photon of same frequency, same momentum, same polarization and same phase. Because the momentum (as a vector) of the photon is nearly parallel to the axes of the mirrors, it is possible for photons to repeat multiple reflections and force more and more photons to follow them producing an avalanche effect. The number of photons of this coherent laser beam increases and competes the number of photons perished due to losses. The basic necessary condition for the generation of a laser radiation is the population inversion, which can be achieved either by exciting atoms and inducing percussion or by external radiation absorption. \nA SASER device mimics this procedure using a source-pump to induce a sound beam of phonons. This sound beam propagates not in an optical cavity, but in a different active medium. An example of an active medium is the superlattice. A superlattice can consist of multiple ultra-thin lattices of two different semiconductors. These two semiconductor materials have different band gaps, and form quantum wells—which are potential wells that confine particles to move in two dimensions instead of three, forcing them to occupy a planar region. In the superlattice, a new set of selection rules is composed that affects the flow-conditions of charges through the structure. When this set-up is excited by a source, the phonons start to multiply while they reflect on the lattice levels, until they escape from the lattice structure in a form of an ultrahigh frequency-phonon beam. \n\nNamely, a concerted emission of phonons can lead to coherent sound and an example of concerted phonon emission is the emission coming from quantum wells. This stands in similar paths with the laser where a coherent light can build up by the concerted stimulated emission of light from a lot of atoms. A SASER device transforms the electric potential energy in a single vibrational mode of the lattice (phonon).\n\nThe medium where the amplification takes place, consists of stacks of thin layers of semiconductors that together form quantum wells. In these wells, electrons can be excited by parcels of ultrasound of millielectronvolts of energy. This amount of energy is equivalent to a frequency of 0.1 to 1 THz.\n\nJust as light is a wave motion that is considered as composed of particles called photons, we can think of the normal modes of vibration in a solid as being particle-like. The quantum of lattice vibration is called phonon. In lattice dynamics we want to find the normal modes of vibration of a crystal. In other words, we need to calculate the energies (or frequencies ) of the phonons as a function of their wave vector's \"k\" . The relationship between frequency \"ω\" and wave vector \"k\" is called phonon dispersion.\n\nLight and sound are similar in various ways. They both can be thought of in terms of waves, and they both come in quantum mechanical units. In the case of light we have photons while in sound we have phonons. Both sound and light can be produced as random collections of quanta (e.g. light emitted by a light bulb) or orderly waves that travel in a coordinated form (e.g. laser light). This parallelism implies that lasers should be as feasible with sound as they are with light. In the 21st century, it is easy to produce low frequency sound in the range that humans can hear (~20 kHz), in either a random or orderly form. However, at the terahertz frequencies in the regime of phonon laser applications, more difficulties arise. The problem stems from the fact that sound travels much slower than light. This means that the wavelength of sound is much shorter than light at a given frequency. Instead of resulting in orderly, coherent phonon laser structures that can produce terahertz sound, tend to emit phonons randomly. Researchers have overcome the problem of terahertz frequencies by following various approaches. Scientists in Caltech have overcome this problem by assembling a pair of microscopic cavities that only permit specific frequencies of phonons to be emitted. This system can be also tuned to emit phonons of different frequencies by changing the relative separation of the microcavities. On the other hand, the group from the University of Nottingham took a different approach. They have built their device out of electrons moving through a series of structures known as quantum wells. Briefly, as an electron hops from one quantum well to another neighbouring well it produces a phonon.\n\nExternal energy pumping (e.g. a light beam or voltage) can help to the excitation of an electron. Relaxation of an electron from one of the upper states may occur by emission of either a photon or a phonon. This is determined by the density of states of phonons and photons. Density of states is the number of states per volume unit in an interval of energy (E, E+dE) that are available to be occupied by electrons. Both phonons and photons are bosons and thus, they obey Bose–Einstein statistics. This means that, since bosons with the same energy can occupy the same place in space, phonons and photons are force carrier particles and they have integer spins. There are more allowed states available for occupancy in a phonon field than in a photon field. Therefore, since the density of terminal states in the phonon field exceeds that in a photon field (by up to ~10), phonon emission is by far the more likely event. We could also imagine a concept where the excitation of an electron briefly leads to vibration of the lattice and thus to phonon generation. The vibration energy of the lattice can take discrete values for every excitation. Every one of this \"excitation packages\" is called phonon. An electron does not stay in an excited state for too long. It readily releases energy to return to its stable low energy state. The electrons release energy in any random direction and at any time (after their excitation). At some particular times, some electrons get excited while others lose energy in a way that the average energy of system is the lowest possible.\nBy pumping energy into the system we can achieve a population inversion. This means that there are more excited electrons than electrons in the lowest energy state in the system. As electron releases energy (e.g. phonon) it interacts with another excited electron to release its energy too. Therefore, we have a stimulated emission, which means a lot of energy (e.g., acoustic radiation, phonons) is released at the same time. One can mention that the stimulated emission is a procedure where we have a spontaneous and an induced emission at the same time. The induced emission comes from the pumping procedure and then is added to the spontaneous emission.\n\nA SASER device should consist of a pumping mechanism and an active medium. The pumping procedure can be induced for example by an alternating electric field or with some mechanical vibrations of resonators, followed by acoustic amplification in the active medium. The fact that a SASER operates on principles remarkably similar to a laser, can lead to an easier way of understanding the relevant operation circumstances. Instead of a feedback-built potent wave of electromagnetic radiation, a SASER delivers a potent sound wave. Some methods for sound amplification of GHz-THz have been proposed so far. Some have been explored only theoretically and others have been explored in non-coherent experiments.\n\nWe note that acoustic waves of 100 GHz to 1 THz have wavelengths in nanometre range. Sound amplification according to the experiment taken in the University of Nottingham could be based on an induced cascade of electrons in semiconductor superlattices. The energy levels of electrons are confined in the superlattice layers. As the electrons hop between gallium arsenide quantum wells in the superlattice they emit phonons. Then, one phonon going in, produces two phonons coming out of the superlattice. This process can be stimulated by other phonons and then give rise to an acoustic amplification. Upon the addition of electrons, short-wavelength (in the terahertz range) phonons are produced. Since the electrons are confined to the quantum wells existing within the lattice, the transmission of their energy depends upon the phonons they generate. As these phonons strike other layers in the lattice, they excite electrons, which produce further phonons, which go on to excite more electrons, and so on. Eventually, a very narrow beam of high-frequency ultrasound exits the device. Semiconductor superlattices are used as acoustic mirrors. These superlattice structures must be in the right size obeying the theory of multilayer distributed Bragg reflector, in similarity with multilayer dielectric mirrors in optics.\n\nBasic understanding of the SASER development requires the evaluation of some proposed examples of SASER devices and SASER theoretical schemes.\n\nIn this proposed theoretical scheme, the active medium is a liquid dielectric (e.g. ordinary distilled water) in which dispersed particles are uniformly distributed. Means of electrolysis cause gas bubbles that serve as the dispersed particles. A pumped wave excited in the active medium produces a periodic variation of the volumes of the dispersed particles (gas bubbles). Since, the initial spatial distribution of the particles is uniform, the waves emitted by the particles are added with different phases and give zero on the average. Nevertheless, if the active medium is located in a resonator, then a standing mode can be excited in it. Particles then bunch under the action of the acoustic radiation forces. In this case, the oscillations of the bubbles are self-synchronized and the useful mode amplifies.\n\nThe similarity of this with the Free-electron laser is useful to understand the theoretical concepts of the scheme. In a FEL, electrons move through magnetic periodic systems producing electromagnetic radiation. The radiation of the electrons is initially incoherent but then on account of the interaction with the useful electromagnetic wave they start to bunch according to phase and they become coherent. Thus, the electromagnetic field is amplified.\n\nWe note that, in the case of the piezoelectric radiators usually used to generate ultrasound, only the working surface radiates and therefore the working system is two-dimensional. On the other hand, a sound amplification by stimulated emission of radiation device is a three-dimensional system, since the entire volume of the active medium radiates.\n\nThe active medium gas-liquid mixture fills the resonator. The bubble density in the liquid is initially distributed uniformly in space. Since the wave propagates in such a medium, the pump wave leads to the appearance of an additional quasi-periodic wave. This wave is coupled with the spatial variation of the bubble density under the action of radiation pressure forces. Hence, the wave amplitude and the bubble density vary slowly compared with the period of the oscillations.\n\nIn the theoretical scheme where the usage of resonators is essential, the SASER radiation passes through the resonator walls, which are perpendicular to the direction of propagation of the pump wave. According to an example of an electrically pumped SASER, the active medium is confined between two planes, which are defined by the solid walls of the resonator. The radiation then, propagates along an axis parallel to the plane defined by the two resonator walls. The static electric field acting on the liquid with gas bubbles results in the deformation of dielectrics and therefore leads to a change in the volumes of the particles. We note that, the electromagnetic waves in the medium propagate with a velocity much greater than the velocity of sound in the same medium. This results to the assumption that the effective pump wave acting on the bubbles does not depend on the spatial coordinates. The pressure of a wave pump in the system leads to both the appearance of a backward wave and a dynamical instability of the system.\n\nMathematical analyses have shown that two types of losses must be overcome for generation of oscillations to start. Losses of the first type are associated with the dispersion of energy inside the active medium and second type losses are due to radiation losses at the ends of the resonator. These types of losses are inversely proportional to the amount of energy stored in the resonator. \nIn general, the disparity of the radiators does not play a role in any attempt of a mathematical calculation of the starting conditions. Bubbles with resonance frequencies close to the pump frequency make the main contribution to the gain of the useful mode. In contrast, the determination of the starting pressure in ordinary lasers is independent from the number of radiators. The useful mode grows with the number of particles but sound absorption increases at the same time. Both these factors neutralize each other. Bubbles play the main role in the energy dispersion in a SASER.\n\nA relevant suggested scheme of sound amplification by stimulated emission of radiation using gas bubbles as the active medium was introduced around 1995 The pumping is created by mechanical oscillations of a cylindrical resonator and the phase bunching of bubbles is realized by acoustic radiation forces. A notable fact is that gas bubbles can only oscillate under an external action, but not spontaneously. According to other proposed schemes, the electrostriction oscillations of the dispersed particle volumes in the cylindrical resonator are realized by an alternating electromagnetic field. However, a SASER scheme with an alternating electric field as the pump has a limitation. A very large amplitude of electric field (up to tens of kV/cm) is required to realize the amplification. Such values approach the electric puncture intensity of liquid dielectrics. Hence, a study proposes a SASER scheme without this limitation. The pumping is created by radial mechanical pulsations of a cylinder. This cylinder contains an active medium—a liquid dielectric with gas bubbles. The radiation emits through the faces of the cylinder.\n\nA proposal for the development of a phonon laser on resonant phonon transitions has been introduced from a group in Institute of Spectroscopy in Moscow, Russia.\nTwo schemes for steady stimulated phonon generation were mentioned. The first scheme exploits a narrow - gap indirect semiconductor or analogous indirect gap semiconductor heterostructure where the tuning into resonance of one-phonon transition of electron - hole recombination can be carried out by external pressure, magnetic or electric fields. The second scheme uses one-phonon transition between direct and indirect exciton levels in coupled quantum wells. We note that an exciton is an electrically neutral quasiparticle that describes an elementary excitation of condensed matter. It can transport energy without transporting net electric charge. The tuning into the resonance of this transition can be accomplished by engineering of dispersion of indirect exciton by external in-plane magnetic and normal electric fields. \nThe magnitude of phonon wave vector in the second proposed scheme, is supposed to be determined by magnitude of in-plane magnetic field. Therefore, such kind of SASER is tunable (i.e. its wavelength of operation can be altered in a controlled manner).\n\nCommon semiconductor lasers can be realised only in direct gap semiconductors. The reasoning behind that is that a pair of electron and hole near minima of their bands in an indirect gap semiconductor can recombine only with production of a phonon and a photon, due to energy and momentum conservation laws. This kind of process is weak in comparison with electron-hole recombination in a direct semiconductor. Consequently, the pumping of these transitions has to be very intense so as to obtain a steady laser generation. Hence, the lasing transition with production of only one particle – photon – must be resonant. This means that the lasing transition must be allowed by momentum and energy conservation laws to generate in a steady form. Photons have negligible wave vectors and therefore the band extremes have to be in the same position of the Brillouin zone . On the other hand, for devices such as SASERs, acoustic phonons have a considerable dispersion. According to dynamics, this leads to the statement that the levels on which the laser should operate, must be in the k-space relatively to each other. K-space refers to a space where things are in terms of momentum and frequency instead of position and time. The conversion between real space and k-space is a mathematical transformation called the Fourier transform and thus k-space can be also called Fourier space.\n\nWe note that, the difference in energy of the photon lasing levels has to be at least smaller than the Debye energy in the semiconductor. Here we can think of the Debye energy as the maximum energy associated with the vibrational modes of the lattice. Such levels can be formed by conduction and valence bands in narrow gap indirect semiconductors.\n\nThe energy gap in a semiconductor under the influence of pressure or magnetic field slightly varies and thus does not deserve any consideration. On the other hand, in narrow-gap semiconductors this variation of energy is considerable and therefore external pressure or magnetic field may serve the purpose of tuning into the resonance of one-phonon interband transition. Note that interband transition is the transition between the conduction and valence band. This scheme considers of indirect semiconductors instead of direct semiconductors. The reasoning behind that comes from the fact that, due to the k-selection rule in semiconductors, interband transitions with the production of only one phonon can be only those that produce an optical phonon. However, optical phonons have a short life-time (they split into two due to anharmonicity) and therefore they add some important complications. Here we can note that even in the case of multi-stage process of acoustic phonon creation it is possible to create SASER. Examples of narrow-gap indirect semiconductors that can be used are chalcogenides PbTe, PbSe and PbS with energy gap 0.15 – 0.3 eV. For the same scheme, the usage of a semiconductor heterostructure (layers of different semiconductors) with narrow gap indirect in momentum space between valence and conduction bands may be more effective. This could be more promising since the spatial separation of the layers provides a possibility of tuning the interband transition into resonance by an external electric field. An essential statement here is that this proposed phonon laser can operate only if the temperature is much lower than the energy gap in the semiconductor.\n\nDuring the analysis of this theoretical scheme several assumptions were introduced for simplicity reasons. The method of the pumping keeps the system electro-neutral and the dispersion laws of electrons and holes are assumed to be parabolic and isotropic. Also phonon dispersion law is required to be linear and isotropic too. Since the entire system is electro-neutral, the process of pumping creates electrons and holes with the same rate. A mathematical analysis, leads to an equation for the average number of electron-hole pairs per one phonon mode per unit volume. For a low loss limit, this equation gives us a pumping rate for the SASER that is rather moderate in comparison with usual phonon lasers on a p-n transition.\n\nIt has been mentioned that a quantum well is basically a potential well that confines particles to move in two dimensions instead of three, forcing them to occupy a planar region. In coupled quantum wells there are two possible ways for electrons and holes to be bound into an exciton: indirect exciton and direct exciton. In indirect exciton, electrons and holes are in different quantum wells, in contrast with direct exciton where electrons and holes are located in the same well. In a case where the quantum wells are identical, both levels have a two-fold degeneracy. Direct exciton level is lower than the level of indirect exciton because of greater Coulomb interaction. Also, indirect exciton has an electric dipole momentum normal to coupled quantum well and thus a moving indirect exciton has an in-plane magnetic momentum perpendicular to its velocity. Any interactions of its electric dipole with normal electric field, lowers one of indirect exciton sub-levels and in sufficiently strong electric fields the moving indirect exciton becomes the ground excitonic level. Having in mind these procedures, one can select velocity to have an interaction between magnetic dipole and in-plane magnetic field. This displaces the minimum of the dispersion law away from the radiation zone. The importance of this, lies on the fact that electric and in-plane magnetic fields normal to coupled quantum wells, can control the dispersion of indirect exciton. Normal electric field is needed for tuning the transition: direct exciton --> indirect exciton + phonon into resonance and its magnitude can form a linear function with the magnitude of in-plane magnetic field. \nWe note that the mathematical analysis of this scheme considers of longitudinal acoustic (LA) phonons instead of transverse acoustic (TA) phonons. This aims to more simple numerical estimations. Generally, the preference in transverse acoustic (TA) phonons is better because TA phonons have lower energy and the greater life-time than LA phonons. Therefore, their interaction with the electronic subsystem is weak. In addition, simpler quantitative evaluations require a pumping of direct exciton level performed by a laser irradiation.\n\nA further analysis of the scheme can help us to establish differential equations for direct exciton, indirect exciton and phonon modes. The solution of these equations gives that separately phonon and indirect exciton modes have no definite phase and only the sum of their phases is defined. The aim here is to check if the operation of this scheme with a rather moderate pumping rate can hold against the fact that excitons in coupled quantum wells have low dimensionality in comparison to phonons. Hence, phonons not confined in the coupled quantum well are considered. An example is longitudinal optical (LO) phonons that are in AlGaAs/GaAs heterostructure and thus, phonons presented in this proposed system are three-dimensional. Differences in dimensionalities of phonons and excitons cause upper level to transform into many states of phonon field. By applying this information to specific equations we can conclude to a desired result. There is no additional requirement for the laser pumping despite the difference in phonon and exciton dimensionalities.\n\nPhonon laser action has been stated in a wide range of physical systems (e.g. semiconductors). A 2012 publication from the Department of Applied Physics in California Institute of Technology (Caltech), introduces a demonstration of a compound micro-cavity system, coupled with a radio-frequency mechanical mode, which operates in close analogy to a two level-laser system.\n\nThis compound micro-cavity system can also be called \"photonic molecule\". Hybridized orbitals of an electrical system are replaced by optical supermodes of this photonic molecule while the transitions between their corresponding energy levels are induced by a phonon field. For typical conditions of the optical micro-resonators, the photonic molecule behaves as a two-level laser system. Nevertheless, there is a bizarre inversion between the roles of the active medium and the cavity modes (laser field). The medium becomes purely optical and the laser field is provided by the material as a phonon mode.\n\nAn inversion produces gain, causing phonon laser action above a pump power threshold of around 7 μW. The proposed device is characterized from a continuously tunable gain spectrum that selectively amplifies mechanical modes from radio frequency to microwave rates. Viewed as Brillouin process, the system accesses a regime in which the phonon plays the role of Stokes wave. Stokes wave refers to a non-linear and periodic surface wave on an inviscid fluid (ideal fluid assumed to have no viscosity) layer of constant mean depth. For this reason it should be also possible to controllably switch between phonon and phonon laser regimes.\n\nCompound optical microcavity systems provide beneficial spectral controls. These controls impact both phonon laser action and cooling and define some finely spaced optical levels whose transition energies are proportional to phonon energies. These level spacings are continuously tunable by a significant adjustment of optical coupling. Therefore, amplification and cooling occur around a tunable line center, in contrast with some cavity optomechanical phenomena. The creation of these finely spaced levels does not require increasing the optical microcavity dimensions. Hence, these finely spaced levels do not affect the optomechanical interaction strength in a significant degree. \nThe approach uses intermodal coupling, induced by radiation pressure and can also provide a spectrally selective mean to detect phonons. Moreover, some evidences of intermodal cooling are observed in this kind of experiments and thus, there is an interest in optomechanical cooling. \nOverall, an extension to multilevel systems using multiple coupled resonators is possible.\n\nIn a two level system, the particles have only two available energy levels, separated by some energy difference: \"ΔΕ = E - E = hv\" where \"ν\" is the frequency of the associated electromagnetic wave of the photon emitted and \"h\" is the Planck constant. Also note: \"E\" > \"E\". These two levels are the excited (upper) and ground (lower) states. When a particle in the upper state interacts with a photon matching the energy separation of the levels, the particle may decay, emitting another photon with the same phase and frequency as the incident photon. Therefore, by pumping energy into the system we can have a stimulated emission of radiation—which means that the pump forces the system to release a big amount of energy at a specific time. A fundamental characteristic of lasing, like the population inversion, is not actually possible in a two-level system and therefore a two level-laser is not possible. In a two-level atom the pump is, in a way, the laser itself.\n\nThe amplification of coherent terahertz sound in a Wannier-Stark ladder superlattice has been achieved in 2009 according to a paper publication from the School of Physics and Astronomy in the University of Nottingham. Wannier-Stark effect, exists in superlattices. Electron states in quantum wells respond sensitively to moderate electric fields either by the quantum confined Stark effect in the case of wide barriers or by Wannier-Stark localization in the case of a superlattice. Both effects lead to large changes of the optical properties near the absorption edge, which are useful for intensity modulation and optical switching. Namely, in a mathematical point of view, if an electric field is applied to a superlattice the relevant Hamiltonian exhibits an additional scalar potential. If an eigenstate exists, then the states corresponding to wave functions are eigenstates of the Hamiltonian as well. These states are equally spaced both in energy and real space and form the so-called Wannier-Stark ladder.\n\nIn the proposed scheme, an application of an electrical bias to a semiconductor superlattice is increasing the amplitude of coherent folded phonons generated by an optical pulse. This increase of the amplitude is observed for those biases in which the energy drop per period of the superlattice is greater than the phonon energy. If the superlattice is biased such that the energy drop per period of the superlattice exceeds the width of electronic minibands (Wannier-Stark regime), the electrons become localized in the quantum wells and vertical electron transport takes place via hopping between neighboring quantum wells, which may be phonon assisted. As it had been shown previously, under these conditions stimulated phonon emission can become the dominant phonon-assisted hoping process for phonons of an energy value close to the Stark splitting. Thus, coherent phonon amplification is theoretically possible in this type of system. Together with the increase in amplitude, the spectrum of the bias-induced oscillations is narrower than the spectrum of the coherent phonons at zero bias. This shows that coherent amplification of phonons due to stimulated emission takes place in the structure under electrical pumping.\n\nA bias voltage is applied to a weakly coupled n-doped GaAs/AlAs superlattice and increases the amplitude of the coherent hypersound oscillations generated by a femtosecond optical pulse. An evidence of hypersound amplification by stimulated emission of phonons emerges, in a system where the inversion of the electron populations for phonon-assisted transitions exists. This evidence is provided by the bias-induced amplitude increase and experimentally observer spectral narrowing of the superlattice phonon mode with a frequency of 441 GHz.\n\nThe main target of this type of experiments is to highlight the realization probability of a coherent amplification of THz sound. The THz stimulated phonon induced transitions between the electron superlattice states lead to this coherent amplification while processing a population inversion.\n\nAn essential step towards coherent generation (\"sasing\") of THz sound and other active hypersound devices has been provided by this achievement of THz sound amplification. Generally, in a device where the threshold for \"sasing\" is achieved, the technique described by this proposed scheme could be used to measure the coherence time of the emitted hypersound.\n\n\n"}
{"id": "67611", "url": "https://en.wikipedia.org/wiki?curid=67611", "title": "Tennessine", "text": "Tennessine\n\nTennessine is a synthetic chemical element with symbol Ts and atomic number 117. It is the second-heaviest known element and the penultimate element of the 7th period of the periodic table.\n\nThe discovery of tennessine was officially announced in Dubna, Russia, by a Russian–American collaboration in April 2010, which makes it the most recently discovered element . One of its daughter isotopes was created directly in 2011, partially confirming the results of the experiment. The experiment itself was repeated successfully by the same collaboration in 2012 and by a joint German–American team in May 2014. In December 2015, the Joint Working Party of the International Union of Pure and Applied Chemistry (IUPAC) and the International Union of Pure and Applied Physics, which evaluates claims of discovery of new elements, recognized the element and assigned the priority to the Russian–American team. In June 2016, the IUPAC published a declaration stating that the discoverers had suggested the name \"tennessine\" after Tennessee, United States. In November 2016, they officially adopted the name \"tennessine\".\n\nTennessine may be located in the \"island of stability\", a concept that explains why some superheavy elements are more stable compared to an overall trend of decreasing stability for elements beyond bismuth on the periodic table. The synthesized tennessine atoms have lasted tens and hundreds of milliseconds. In the periodic table, tennessine is expected to be a member of group 17, all other members of which are halogens. Some of its properties may significantly differ from those of the halogens due to relativistic effects. As a result, tennessine is expected to be a volatile metal that neither forms anions nor achieves high oxidation states. A few key properties, such as its melting and boiling points and its first ionization energy, are nevertheless expected to follow the periodic trends of the halogens.\n\nIn December 2004, the Joint Institute for Nuclear Research (JINR) team in Dubna, Moscow Oblast, Russia, proposed a joint experiment with the Oak Ridge National Laboratory (ORNL) in Oak Ridge, Tennessee, United States, to synthesize element 117—so-called for the 117 protons in its nucleus. Their proposal involved fusing a berkelium (element 97) target and a calcium (element 20) beam, conducted via bombardment of the berkelium target with calcium nuclei: this would complete a set of experiments done at the JINR on the fusion of actinide targets with a calcium-48 beam, which had thus far produced the new elements 113–116 and 118. The ORNL—then the world's only producer of berkelium—could not then provide the element, as they had temporarily ceased production, and re-initiating it would be too costly. Plans to synthesize element 117 were suspended in favor of the confirmation of element 118, which had been produced earlier in 2002 by bombarding a californium target with calcium. The required berkelium-249 is a by-product in californium-252 production, and obtaining the required amount of berkelium was an even more difficult task than obtaining that of californium, as well as costly: it would cost around 3.5 million dollars, and the parties agreed to wait for a commercial order of californium production, from which berkelium could be extracted.\n\nThe JINR team sought to use berkelium because calcium-48, the isotope of calcium used in the beam, has 20 protons and 28 neutrons, making a neutron–proton ratio of 1.4; and it is the lightest stable or near-stable nucleus with such a large neutron excess. The second-lightest such nucleus, palladium-110 (46 protons, 64 neutrons, neutron–proton ratio of 1.391), is much heavier. Thanks to the neutron excess, the resulting nuclei were expected to be heavier and closer to the sought-after island of stability. Of the aimed for 117 protons, calcium has 20, and thus they needed to use berkelium, which has 97 protons in its nucleus.\n\nIn February 2005, the leader of the JINR team—Yuri Oganessian—presented a colloquium at ORNL. Also in attendance were representatives of Lawrence Livermore National Laboratory, who had previously worked with JINR on the discovery of elements 113–116 and 118, and Joseph Hamilton of Vanderbilt University, a collaborator of Oganessian.\n\nHamilton checked if the ORNL high-flux reactor produced californium for a commercial order: the required berkelium could be obtained as a by-product. He learned that it did not and there was no expectation for such an order in the immediate future. Hamilton kept monitoring the situation, making the checks once in a while. (Later, Oganessian referred to Hamilton as \"the father of 117\" for doing this work.)\n\nORNL resumed californium production in spring 2008. Hamilton noted the restart during the summer and made a deal on subsequent extraction of berkelium. During a September 2008 symposium at Vanderbilt University in Nashville, Tennessee celebrating his 50th year on the Physics faculty, he introduced Oganessian to James Roberto (then the deputy director for science and technology at ORNL). They established a collaboration among JINR, ORNL, and Vanderbilt; the team at the Lawrence Livermore National Laboratory (LLNL) in Livermore, California, U.S., was soon invited to join.\nIn November 2008, the U.S. Department of Energy, which had oversight over the reactor in Oak Ridge, allowed the scientific use of the extracted berkelium. The production lasted 250 days and ended in late December 2008, resulting in 22 milligrams of berkelium, enough to perform the experiment. In January 2009, the berkelium was removed from ORNL’s High Flux Isotope Reactor; it was subsequently cooled for 90 days and then processed at ORNL’s Radiochemical Engineering and Development Center to separate and purify the berkelium material, which took another 90 days. Its half-life is only 330 days: after that time, half the berkelium produced would have decayed. Because of this, the berkelium target had to be quickly transported to Russia; for the experiment to be viable, it had to be completed within six months of its departure from the United States. The target was packed into five lead containers to be flown from New York to Moscow.\n\nRussian customs officials twice refused to let the target enter the country because of missing or incomplete paperwork. Over the span of a few days, the target traveled over the Atlantic Ocean five times. On its arrival in Russia in June 2009, the berkelium was transferred to Research Institute of Atomic Reactors (RIAR) in Dimitrovgrad, Ulyanovsk Oblast, where it was deposited as a 300-nanometer-thin layer on a titanium film. In July 2009, it was then transported to Dubna, where it was installed in the particle accelerator at JINR. The calcium-48 beam was generated by chemically extracting the small quantities of calcium-48 present in naturally occurring calcium, enriching it 500 times. This work was done in the closed town of Lesnoy, Sverdlovsk Oblast, Russia.\n\nThe experiment began late July 2009. In January 2010, scientists at the Flerov Laboratory of Nuclear Reactions announced internally that they had detected the decay of a new element with atomic number 117 via two decay chains: one of an odd-odd isotope undergoing 6 alpha decays before spontaneous fission, and one of an odd-even isotope undergoing 3 alpha decays before fission. The obtained data from the experiment was sent to the LLNL for further analysis. On April 9, 2010, an official report was released in the journal \"Physical Review Letters\" identifying the isotopes as 117 and 117, which were shown to have half-lives on the order of tens or hundreds of milliseconds. The work was signed by all parties involved in the experiment to some extent: JINR, ORNL, LLNL, RIAR, Vanderbilt, the University of Tennessee, and the University of Nevada (Las Vegas, Nevada, U.S.), which provided data analysis support. The isotopes were formed as follows:\n\nAll daughter isotopes (decay products) of element 117 were previously unknown; therefore, their properties could not be used to confirm the claim of discovery. In 2011, when one of the decay products (115) was synthesized directly, its properties matched those measured in the claimed indirect synthesis from the decay of element 117. The discoverers did not submit a claim for their findings in 2007–2011 when the Joint Working Party was reviewing claims of discoveries of new elements.\n\nThe Dubna team repeated the experiment in 2012, creating seven atoms of element 117 and confirming their earlier synthesis of element 118 (produced after some time when a significant quantity of the berkelium-249 target had beta decayed to californium-249). The results of the experiment matched the previous outcome; the scientists then filed an application to register the element. In May 2014, a joint German–American collaboration of scientists from the ORNL and the GSI Helmholtz Center for Heavy Ion Research in Darmstadt, Hessen, Germany, claimed to have confirmed discovery of the element. The team repeated the Dubna experiment using the Darmstadt accelerator, creating two atoms of element 117.\n\nIn December 2015, the JWP officially recognized the discovery of 117 on account of the confirmation of the properties of its daughter 115, and thus the listed discoverers—JINR, LLNL, and ORNL—were given the right to suggest an official name for the element. (Vanderbilt was left off the initial list of discoverers in an error that was later corrected.)\n\nIn May 2016, Lund University (Lund, Scania, Sweden) and GSI cast some doubt on the syntheses of elements 115 and 117. The decay chains assigned to 115, the isotope instrumental in the confirmation of the syntheses of elements 115 and 117, were found to be too different to belong to the same nuclide with a reasonably high probability. The reported 117 decay chains approved as such by the JWP were found to require splitting into individual data sets assigned to different isotopes of element 117. It was also found that the claimed link between the decay chains reported as from 117 and 115 probably did not exist. (On the other hand, the chains from the non-approved isotope 117 were found to be congruent.) The multiplicity of states found when nuclides that are not even–even undergo alpha decay is not unexpected and contributes to the lack of clarity in the cross-reactions. This study criticized the JWP report for overlooking subtleties associated with this issue, and noted that the fact that the only argument for the acceptance of the discoveries of elements 115 and 117 was an almost certainly non-existent link was \"problematic\".\n\nOn June 8, 2017, two members of the Dubna team published a journal article answering these criticisms, analysing their data on the nuclides 117 and 115 with widely accepted statistical methods, noted that the 2016 studies indicating non-congruence produced problematic results when applied to radioactive decay: they excluded from the 90% confidence interval both average and extreme decay times, and the decay chains that would be excluded from the 90% confidence interval they chose were more probable to be observed than those that would be included. The 2017 reanalysis concluded that the observed decay chains of 117 and 115 were consistent with the assumption that only one nuclide was present at each step of the chain, although it would be desirable to be able to directly measure the mass number of the originating nucleus of each chain as well as the excitation function of the Am+Ca reaction.\n\nUsing Mendeleev's nomenclature for unnamed and undiscovered elements, element 117 should be known as \"eka-astatine\". Using the 1979 recommendations by the International Union of Pure and Applied Chemistry (IUPAC), the element was temporarily called \"ununseptium\" (symbol \"Uus\") until its discovery was confirmed and a permanent name chosen; the temporary name was formed from Latin roots \"one\", \"one\", and \"seven\", a reference to the element's atomic number of 117. Many scientists in the field called it \"element 117\", with the symbol \"E117\", \"(117)\", or \"117\". According to guidelines of IUPAC valid at the moment of the discovery approval, the permanent names of new elements should have ended in \"-ium\"; this included element 117, even if the element was a halogen, which traditionally have names ending in \"-ine\"; however, the new recommendations published in 2016 recommended using the \"-ine\" ending for all new group 17 elements. The IUPAC guidelines specify that the discovery team has naming rights for the element.\n\nAfter the original synthesis in 2010, Dawn Shaughnessy of LLNL and Oganessian declared that naming was a sensitive question, and it was avoided as far as possible. However, Hamilton declared that year, \"I was crucial in getting the group together and in getting the Bk target essential for the discovery. As a result of that, I’m going to get to name the element. I can’t tell you the name, but it will bring distinction to the region.\" (Hamilton teaches at Vanderbilt University in Nashville, Tennessee, U.S.)\nIn March 2016, the discovery team agreed on a conference call involving representatives from the parties involved on the name \"tennessine\" for element 117. In June 2016, IUPAC published a declaration stating the discoverers had submitted their suggestions for naming the new elements 115, 117, and 118 to the IUPAC; the suggestion for the element 117 was \"tennessine\", with a symbol of \"Ts\", after \"the region of Tennessee\". The suggested names were recommended for acceptance by the IUPAC Inorganic Chemistry Division; formal acceptance was set to occur after a five-months term following publishing of the declaration expires. In November 2016, the names, including tennessine, were formally accepted. Concerns that the proposed symbol \"Ts\" may clash with a notation for the tosyl group used in organic chemistry were rejected, following existing symbols bearing such dual meanings: Ac (actinium and acetyl) and Pr (praseodymium and propyl). The naming ceremony for moscovium, tennessine, and oganesson was held in March 2017 at the Russian Academy of Sciences in Moscow; a separate ceremony for tennessine alone had been held at ORNL in January 2017.\n\nThe stability of nuclei quickly decreases with the increase in atomic number after curium, element 96, whose half-life is four orders of magnitude longer than that of any subsequent element. All isotopes with an atomic number above 101 undergo radioactive decay with half-lives of less than 30 hours. No elements with atomic numbers above 82 (after lead) have stable isotopes. This is because of the ever-increasing Coulomb repulsion of protons, so that the strong nuclear force cannot hold the nucleus together against spontaneous fission for long. Calculations suggest that in the absence of other stabilizing factors, elements with more than 103 protons should not exist. However, researchers in the 1960s suggested that the closed nuclear shells around 114 protons and 184 neutrons should counteract this instability, creating an \"island of stability\" where nuclides could have half-lives reaching thousands or millions of years. While scientists have still not reached the island, the mere existence of the superheavy elements (including tennessine) confirms that this stabilizing effect is real, and in general the known superheavy nuclides become exponentially longer-lived as they approach the predicted location of the island. Tennessine is the second-heaviest element created so far, and all its known isotopes have half-lives of less than one second. Nevertheless, this is longer than the values predicted prior to their discovery. The Dubna team believes that the synthesis of the element is direct experimental proof of the existence of the island of stability.\n\nIt has been calculated that the isotope Ts would have a half-life of about 18 milliseconds, and it may be possible to produce this isotope via the same berkelium–calcium reaction used in the discoveries of the known isotopes, Ts and Ts. The chance of this reaction producing Ts is estimated to be, at most, one-seventh the chance of producing Ts. Calculations using a quantum tunneling model predict the existence of several isotopes of tennessine up to Ts. The most stable of these is expected to be Ts with an alpha-decay half-life of 40 milliseconds. A liquid drop model study on the element's isotopes shows similar results; it suggests a general trend of increasing stability for isotopes heavier than Ts, with partial half-lives exceeding the age of the universe for the heaviest isotopes like Ts when beta decay is not considered. Lighter isotopes of tennessine may be produced in the Am+Ti reaction, which was considered as a contingency plan by the Dubna team in 2008 if Bk proved unavailable, and may be studied again in the near future (2017–2018) to investigate the properties of nuclear reactions with a titanium-50 beam, which becomes necessary to synthesize elements beyond oganesson.\nTennessine is expected to be a member of group 17 in the periodic table, below the five halogens; fluorine, chlorine, bromine, iodine, and astatine, each of which has seven valence electrons with a configuration of . For tennessine, being in the seventh period (row) of the periodic table, continuing the trend would predict a valence electron configuration of , and it would therefore be expected to behave similarly to the halogens in many respects that relate to this electronic state. However, going down group 17, the metallicity of the elements increases; for example, iodine already exhibits a metallic luster in the solid state, and astatine is often classified as a metalloid due to its properties being quite far from those of the four previous halogens. As such, an extrapolation based on periodic trends would predict tennessine to be a rather volatile post-transition metal.\n\nCalculations have confirmed the accuracy of this simple extrapolation, although experimental verification of this is currently impossible as the half-lives of the known tennessine isotopes are too short. Significant differences between tennessine and the previous halogens are likely to arise, largely due to spin–orbit interaction — the mutual interaction between the motion and spin of electrons. The spin–orbit interaction is especially strong for the superheavy elements because their electrons move faster — at velocities comparable to the speed of light — than those in lighter atoms. In tennessine atoms, this lowers the 7s and the 7p electron energy levels, stabilizing the corresponding electrons, although two of the 7p electron energy levels are more stabilized than the other four. The stabilization of the 7s electrons is called the inert pair effect; the effect that separates the 7p subshell into the more-stabilized and the less-stabilized parts is called subshell splitting. Computational chemists understand the split as a change of the second (azimuthal) quantum number \"l\" from 1 to 1/2 and 3/2 for the more-stabilized and less-stabilized parts of the 7p subshell, respectively. For many theoretical purposes, the valence electron configuration may be represented to reflect the 7p subshell split as .\n\nDifferences for other electron levels also exist. For example, the 6d electron levels (also split in two, with four being 6d and six being 6d) are both raised, so they are close in energy to the 7s ones, although no 6d electron chemistry has been predicted for tennessine. The difference between the 7p and 7p levels is abnormally high; 9.8 eV. Astatine's 6p subshell split is only 3.8 eV, and its 6p chemistry has already been called \"limited\". These effects cause tennessine's chemistry to differ from those of its upper neighbors (see below).\n\nTennessine's first ionization energy—the energy required to remove an electron from a neutral atom—is predicted to be 7.7 eV, lower than those of the halogens, again following the trend. Like its neighbors in the periodic table, tennessine is expected to have the lowest electron affinity—energy released when an electron is added to the atom—in its group; 2.6 or 1.8 eV. The electron of the hypothetical hydrogen-like tennessine atom—oxidized so it has only one electron, Ts—is predicted to move so quickly that its mass is 1.9 times that of a non-moving electron, a feature attributable to relativistic effects. For comparison, the figure for hydrogen-like astatine is 1.27 and the figure for hydrogen-like iodine is 1.08. Simple extrapolations of relativity laws indicate a contraction of atomic radius. Advanced calculations show that the radius of an tennessine atom that has formed one covalent bond would be 165 pm, while that of astatine would be 147 pm. With the seven outermost electrons removed, tennessine is finally smaller; 57 pm for tennessine and 61 pm for astatine.\n\nThe melting and boiling points of tennessine are not known; earlier papers predicted about 350–500 °C and 550 °C, respectively, or 350–550 °C and 610 °C, respectively. These values exceed those of astatine and the lighter halogens, following periodic trends. A later paper predicts the boiling point of tennessine to be 345 °C (that of astatine is estimated as 309 °C, 337 °C, or 370 °C, although experimental values of 230 °C and 411 °C have been reported). The density of tennessine is expected to be between 7.1 and 7.3 g/cm, continuing the trend of increasing density among the halogens; that of astatine is estimated to be between 6.2 and 6.5 g/cm.\n\nThe known isotopes of tennessine, Ts and Ts, are too short-lived to allow for chemical experimentation at present. Nevertheless, many chemical properties of tennessine have been calculated. Unlike the previous group 17 elements, tennessine may not exhibit the chemical behavior common to the halogens. For example, fluorine, chlorine, bromine, and iodine routinely accept an electron to achieve the more stable electronic configuration of a noble gas, obtaining eight electrons (octet) in their valence shells instead of seven. This ability weakens as atomic weight increases going down the group; tennessine would be the least willing group 17 element to accept an electron. Of the oxidation states it is predicted to form, −1 is expected to be the least common. The standard reduction potential of the Ts/Ts couple is predicted to be −0.25 V; this value is negative and thus tennessine should not be reduced to the −1 oxidation state under standard conditions, unlike all the previous halogens.\n\nThere is another opportunity for tennessine to complete its octet—by forming a covalent bond. Like the halogens, when two tennessine atoms meet they are expected to form a Ts–Ts bond to give a diatomic molecule. Such molecules are commonly bound via single sigma bonds between the atoms; these are different from pi bonds, which are divided into two parts, each shifted in a direction perpendicular to the line between the atoms, and opposite one another rather than being located directly between the atoms they bind. Sigma bonding has been calculated to show a great antibonding character in the At molecule and is not as favorable energetically. Tennessine is predicted to continue the trend; a strong pi character should be seen in the bonding of Ts. The molecule tennessine chloride (TsCl) is predicted to go further, being bonded with a single pi bond.\n\nAside from the unstable −1 state, three more oxidation states are predicted; +5, +3, and +1. The +1 state should be especially stable because of the destabilization of the three outermost 7p electrons, forming a stable, half-filled subshell configuration; astatine shows similar effects. The +3 state should be important, again due to the destabilized 7p electrons. The +5 state is predicted to be uncommon because the 7p electrons are oppositely stabilized. The +7 state has not been shown—even computationally—to be achievable. Because the 7s electrons are greatly stabilized, it has been hypothesized that tennessine effectively has only five valence electrons.\n\nThe simplest possible tennessine compound would be the monohydride, TsH. The bonding is expected to be provided by a 7p electron of tennessine and the 1s electron of hydrogen. The non-bonding nature of the 7p spinor is because tennessine is expected not to form purely sigma or pi bonds. Therefore, the destabilized (thus expanded) 7p spinor is responsible for bonding. This effect lengthens the TsH molecule by 17 picometers compared with the overall length of 195 pm. Since the tennessine p electron bonds are two-thirds sigma, the bond is only two-thirds as strong as it would be if tennessine featured no spin–orbit interactions. The molecule thus follows the trend for halogen hydrides, showing an increase in bond length and a decrease in dissociation energy compared to AtH. The molecules TlTs and NhTs may be viewed analogously, taking into account an opposite effect shown by the fact that the element's p electrons are stabilized. These two characteristics result in a relatively small dipole moment (product of difference between electric charges of atoms and displacement of the atoms) for TlTs; only 1.67 D, the positive value implying that the negative charge is on the tennessine atom. For NhTs, the strength of the effects are predicted to cause a transfer of the electron from the tennessine atom to the nihonium atom, with the dipole moment value being −1.80 D. The spin–orbit interaction increases the dissociation energy of the TsF molecule because it lowers the electronegativity of tennessine, causing the bond with the extremely electronegative fluorine atom to have a more ionic character. Tennessine monofluoride should feature the strongest bonding of all group 17 monofluorides.\n\nVSEPR theory predicts a bent-T-shaped molecular geometry for the group 17 trifluorides. All known halogen trifluorides have this molecular geometry and have a structure of AXE—a central atom, denoted A, surrounded by three ligands, X, and two unshared electron pairs, E. If relativistic effects are ignored, TsF should follow its lighter congeners in having a bent-T-shaped molecular geometry. More sophisticated predictions show that this molecular geometry would not be energetically favored for TsF, predicting instead a trigonal planar molecular geometry (AXE). This shows that VSEPR theory may not be consistent for the superheavy elements. The TsF molecule is predicted to be significantly stabilized by spin–orbit interactions; a possible rationale may be the large difference in electronegativity between tennessine and fluorine, giving the bond a partially ionic character.\n"}
{"id": "472018", "url": "https://en.wikipedia.org/wiki?curid=472018", "title": "The Changing Dream", "text": "The Changing Dream\n\nThe Changing Dream is the book that was written by the former United States Senator and Representative John V. Tunney, who was the son of the 1926 to 1928 heavyweight boxing champion, Gene Tunney.\n\nThe subtitle is \"The Truth About the Material and Energy Crisis and What We Must Do to Resolve It.\"\n\nDoubleday & Company, Inc., Garden City, New York, 1975. \n\n"}
{"id": "37403173", "url": "https://en.wikipedia.org/wiki?curid=37403173", "title": "The Indian Forester", "text": "The Indian Forester\n\nThe Indian Forester is a peer-reviewed scientific journal covering research in forestry. It is one of the oldest forestry journals still in existence in the world. It was established in 1875 and is published by the Indian Council of Forestry Research and Education.\n\n\n"}
{"id": "29260402", "url": "https://en.wikipedia.org/wiki?curid=29260402", "title": "Toric code", "text": "Toric code\n\nThe toric code is a topological quantum error correcting code, and an example of a stabilizer code, defined on a two-dimensional spin lattice It is the simplest and most well studied of the quantum double models.\nIt is also the simplest example of topological order—\"Z\" topological order\n(first studied in the context of \"Z\" spin liquid in 1991). The toric code can also be considered to be a \"Z\" lattice gauge theory in a particular limit. It was introduced by Alexei Kitaev.\n\nThe toric code gets its name from its periodic boundary conditions, giving it the shape of a torus. These conditions give the model translational invariance, which is useful for analytic study. However, experimental realization requires open boundary conditions, allowing the system to be embedded on a 2D surface. The resulting code is typically known as the planar code. This has identical behaviour to the toric code in most, but not all, cases.\n\nThe toric code is defined on a two-dimensional lattice, usually chosen to be the square lattice, with a spin-½ degree of freedom located on each edge. They are chosen to be periodic. Stabilizer operators are defined on the spins around each vertex formula_1 and plaquette (or face) formula_2 of the lattice as follows,\n\nformula_3\n\nWhere here we use formula_4 to denote the edges touching the vertex formula_1, and formula_6 to denote the edges surrounding the plaquette formula_2. The stabilizer space of the code is that for which all stabilizers act trivially, hence,\n\nformula_8\n\nfor any state formula_9. For the toric code, this space is four-dimensional, and so can be used to store two qubits of quantum information. This can be proven by considering the number of independent stabilizer operators. The occurrence of errors will move the state out of the stabilizer space, resulting in vertices and plaquettes for which the above condition does not hold. The positions of these violations is the syndrome of the code, which can be used for error correction.\n\nThe unique nature of the topological codes, such as the toric code, is that stabilizer violations can be interpreted as quasiparticles. Specifically, if the code is in a state formula_10 such that,\n\nformula_11,\n\na quasiparticle known as an formula_12 anyon can be said to exist on the vertex formula_1. Similarly violations of the formula_14 are associated with so called formula_15 anyons on the plaquettes. The stabilizer space therefore corresponds to the anyonic vacuum. Single spin errors cause pairs of anyons to be created and transported around the lattice.\n\nWhen errors create an anyon pair and move the anyons, one can imagine a path connecting the two composed of all links acted upon. If the anyons then meet and are annihilated, this path describes a loop. If the loop is topologically trivial, it has no effect on the stored information. The annihilation of the anyons, in this case, corrects all of the errors involved in their creation and transport. However, if the loop is topologically non-trivial, though re-annihilation of the anyons returns the state to the stabilizer space it also implements a logical operation on the stored information. The errors, in this case, are therefore not corrected but consolidated.\n\nLet us consider the noise model for which bit and phase errors occur independently on each spin, both with probability \"p\". When \"p\" is low, this will create sparsely distributed pairs of anyons which have not moved far from their point of creation. Correction can be achieved by identifying the pairs that the anyons were created in (up to an equivalence class), and then re-annihilating them to remove the errors. As \"p\" increases, however, it becomes more ambiguous as to how the anyons may be paired without risking the formation of topologically non-trivial loops. This gives a threshold probability, under which the error correction will almost certainly succeed. Through a mapping to the random-bond Ising model, this critical probability has been found to be around 11%.\n\nOther error models may also be considered, and thresholds found. In all cases studied so far, the code has been found to saturate the Hashing bound. For some error models, such as biased errors where bit errors occur more often than phase errors or vice versa, lattices other than the square lattice must be used to achieve the optimal thresholds.\n\nThese thresholds are upper limits and are useless unless efficient algorithms are found to achieve them. The most well-used algorithm is minimum weight perfect matching. When applied to the noise model with independent bit and flip errors, a threshold of around 10.5% is achieved. This falls only a little short of the 11% maximum. However, matching does not work so well when there are correlations between the bit and phase errors, such as with depolarizing noise.\n\nThe means to perform quantum computation on logical information stored within the toric code has been considered, with the properties of the code providing fault-tolerance. It has been shown that extending the stabilizer space using 'holes', vertices or plaquettes on which stabilizers are not enforced, allows many qubits to be encoded into the code. However, a universal set of unitary gates cannot be fault-tolerantly implemented by unitary operations and so additional techniques are required to achieve quantum computing. For example, universal quantum computing can be achieved by preparing magic states via encoded quantum stubs called tidBits used to teleport in the required additional gates when replaced as a qubit. Furthermore, preparation of magic states must be fault tolerant, which can be achieved by magic state distillation on noisy magic states. A measurement based scheme for quantum computation based upon this principle has been found, whose error threshold is the highest known for a two-dimensional architecture.\n\nSince the stabilizer operators of the toric code are quasilocal, acting only on spins located near each other on a two-dimensional lattice, it is not unrealistic to define the following Hamiltonian,\n\nformula_16\n\nThe ground state space of this Hamiltonian is the stabilizer space of the code. Excited states correspond to those of anyons, with the energy proportional to their number. Local errors are therefore energetically suppressed by the gap, which has been shown to be stable against local perturbations. However, the dynamic effects of such perturbations can still cause problems for the code.\n\nThe gap also gives the code a certain resilience against thermal errors, allowing it to be correctable almost surely for a certain critical time. This time increases with formula_17, but since arbitrary increases of this coupling are unrealistic, the protection given by the Hamiltonian still has its limits.\n\nThe means to make the toric code, or the planar code, into a fully self-correcting quantum memory is often considered. Self-correction means that the Hamiltonian will naturally suppress errors indefinitely, leading to a lifetime that diverges in the thermodynamic limit. It has been found that this is possible in the toric code only if long range interactions are present between anyons. Proposals have been made for realization of these in the lab Another approach is the generalization of the model to higher dimensions, with self-correction possible in 4D with only quasi-local interactions.\n\nAs mentioned above, so called formula_12 and formula_15 quasiparticles are associated with the vertices and plaquettes of the model, respectively. These quasiparticles can be described as anyons, due to the non-trivial effect of their braiding. Specifically, though both species of anyons are bosonic with respect to themselves, the braiding of two formula_12's or formula_15's having no effect, a full monodromy of an formula_12 and an formula_15 will yield a phase of formula_24. Such a result is not consistent with either bosonic or fermionic statistics, and hence is anyonic.\n\nThe anyonic mutual statistics of the quasiparticles demonstrate the logical operations performed by topologically non-trivial loops. Consider the creation of a pair of formula_12 anyons followed by the transport of one around a topologically nontrivial loop, such as that shown on the torus in blue on the figure above, before the pair are reannhilated. The state is returned to the stabilizer space, but the loop implements a logical operation on one of the stored qubits. If formula_15 anyons are similarly moved through the red loop above a logical operation will also result. The phase of formula_24 resulting when braiding the anyons shows that these operations do not commute, but rather anticommute. They may therefore be interpreted as logical formula_28 and formula_29 Pauli operators on one of the stored qubits. The corresponding logical Pauli's on the other qubit correspond to an formula_15 anyon following the blue loop and an formula_12 anyon following the red. No braiding occurs when formula_12 and formula_15 pass through parallel paths, the phase of formula_24 therefore does not arise and the corresponding logical operations commute. This is as should be expected since these form operations acting on different qubits.\n\nDue to the fact that both formula_12 and formula_15 anyons can be created in pairs, it is clear to see that both these quasiparticles are their own antiparticles. A composite particle composed of two formula_12 anyons is therefore equivalent to the vacuum, since the vacuum can yield such a pair and such a pair will annihilate to the vacuum. Accordingly, these composites have bosonic statistics, since their braiding is always completely trivial. A composite of two formula_15 anyons is similarly equivalent to the vacuum. The creation of such composites is known as the fusion of anyons, and the results can be written in terms of fusion rules. In this case, these take the form,\n\nformula_39\n\nWhere formula_40 denotes the vacuum. A composite of an formula_12 and an formula_15 is not trivial. This therefore constitutes another quasiparticle in the model, sometimes denoted formula_43, with fusion rule,\n\nformula_44\n\nFrom the braiding statistics of the anyons we see that, since any single exchange of two formula_43's will involve a full monodromy of a constituent formula_12 and formula_15, a phase of formula_24 will result. This implies fermionic self-statistics for the formula_43's.\n\nThe use of a torus is not required to form an error correcting code. Other surfaces may also be used, with their topological properties determining the degeneracy of the stabilizer space. In general, quantum error correcting codes defined on two-dimensional spin lattices according to the principles above are known as surface codes.\n\nIt is also possible to define similar codes using higher-dimensional spins. These are the quantum double models and string-net models, which allow a greater richness in the behaviour of anyons, and so may be used for more advanced quantum computation and error correction proposals. These not only include models with Abelian anyons, but also those with non-Abelian statistics.\n\nThe most explicit demonstration of the properties of the toric code has been in state based approaches. Rather than attempting to realize the\nHamiltonian, these simply prepare the code in the stabilizer space. Using this technique, experiments have been able to demonstrate the creation, transport and statistics of the anyons. More recent experiments have also been able to demonstrate the error correction properties of the code.\n\nFor realizations of the toric code and its generalizations with a Hamiltonian, much progress has been made using Josephson junctions. The theory of how the Hamiltonians may be implemented has been developed for a wide class of topological codes. An experiment has also been performed, realizing the toric code Hamiltonian for a small lattice, and demonstrating the quantum memory provided by its degenerate ground state.\n\nOther theoretical work towards experimental realizations is based on cold atoms. A toolkit of methods that may be used to realize topological codes with optical lattices has been explored, \n\n"}
{"id": "6434245", "url": "https://en.wikipedia.org/wiki?curid=6434245", "title": "Traumatin", "text": "Traumatin\n\nTraumatin is a plant hormone produced in response to wound. Traumatin is a precursor to the related hormone traumatic acid.\n"}
{"id": "37003705", "url": "https://en.wikipedia.org/wiki?curid=37003705", "title": "Trespa", "text": "Trespa\n\nTrespa is the brand name of a type of high-pressure laminate (HPL) plate manufactured by Trespa International BV, based in Weert, the Netherlands. Their panels are used for exterior cladding, decorative facades and interior surfaces. It is composed of woodbased fibres or Kraft paper with phenolic resin applied.\n\nThe company was founded by Hermann Krages (1909-1992), the son of a Bremen merchant in wood and fibreboard and the brother of the racer Louis Krages. He is known mostly today for his speculation on the German stock market. Krages initially was given a fiberboard plant in the Ore Mountains of East Prussia by his father, which he lost at the end of WWII. He then moved to Scheuerfeld where he acquired the Berger paper mill and began the \"Deutsche Duroleum Gesellschaft\", making fiberboard plates again. He expanded with new factories in Etzbach, Höxter an der Weser, Leutkirch im Allgäu and in Bremen. The company in Weert was founded in 1960.\n\nInitially the company focused mainly on the sale and storage of panels produced in the German plant at Leutkirch. Gradually it switched to the production of hardboard for mattresses. This activity was later incorporated into the company Thermopal after Krages sold his holdings due to his stock market speculation in the 1960s. In 1964 the name changed to \"Weerter Plastics Industry\" (“Weerter Kunststoffen Fabrieken”, or WKF). In 1967, the company was acquired by Hoechst, which used its product for surfaces in its laboratories. In 1991 the company passed to HAL Holding NV.\n\nTrespa plate is made by compressing impregnated paper or wood fibers and epoxy, phenolic or polypropylene resin at high pressure and high temperature. A special surface made with Electronic Beam Curing, a coating technique developed by Trespa, ensures durability and scratch resistance. Due to the ability to add colored pigments to the surface during curing, a variety of colors are possible. The production technique for the plates, which are made of wood, is also called \"Dry Forming\" technology. In this technique, cheaper pre-pregs instead of the more expensive impregnated paper layers in the production of fiber boards is applied. These prepregs consist of wood fibers and thermosetting resins. This technique was applied for the first time in 1984. Because the surface of Trespa Plate has a dense molecular coating, it is virtually impervious to weather (temperature, UV radiation and humidity). Also, any contamination, such as graffiti, can be removed quite easily. Because of these advantages, the material has been popular since the 1980s in the production of laboratory surfaces and outdoor signage, but also for shower stalls and toilet cubicles in educational, hospital, and campground facilities.\n\nToday Trespa brings HPL Trespa panels under various brand names, with different qualities for indoor and outdoor applications. Other names are used for furniture Trespa Athlon, Trespa Virtuon for indoor applications, Trespa TopLabPLUS, Toplab, for laboratory and Trespa Meteon for outdoor applications.\n\n"}
{"id": "5150435", "url": "https://en.wikipedia.org/wiki?curid=5150435", "title": "Uncontrolled waste", "text": "Uncontrolled waste\n\nUncontrolled waste is a group of waste types that do not fall into either the controlled, special or hazardous waste categories, such as specific mining wastes and agricultural wastes. This should not be confused with an alternative definition of uncontrolled waste that refers to improper waste disposal.\n\nUncontrolled waste has significant impacts on the environment. In certain areas of Germany, it was found that industrial waste such as from ore smelting and waste products produced from petrochemical plants were found near population centers. Main effects of waste such as these lead to contamination of groundwater from toxic chemicals, and acidification of oceans as result of Co2 emissions from mining\n\n\n"}
{"id": "374388", "url": "https://en.wikipedia.org/wiki?curid=374388", "title": "Uranium hexafluoride", "text": "Uranium hexafluoride\n\nUranium hexafluoride (), colloquially known as \"hex\" in the nuclear industry, is a compound used in the process of enriching uranium, which produces fuel for nuclear reactors and nuclear weapons. Hex forms solid grey crystals at standard temperature and pressure, is highly toxic, reacts with water, and is corrosive to most metals. The compound reacts mildly with aluminium, forming a thin surface layer of AlF that resists any further reaction from the compound.\n\nMilled uranium ore—UO or \"yellowcake\"—is dissolved in nitric acid, yielding a solution of uranyl nitrate UO(NO). Pure uranyl nitrate is obtained by solvent extraction, then treated with ammonia to produce ammonium diuranate (\"ADU\", (NH)UO). Reduction with hydrogen gives UO, which is converted with hydrofluoric acid (HF) to uranium tetrafluoride, UF. Oxidation with fluorine yields UF.\n\nDuring nuclear reprocessing, uranium is reacted with chlorine trifluoride to give UF:\n\nAt atmospheric pressure, it sublimes at 56.5 °C.\n\nThe solid state structure was determined by neutron diffraction at 77 K and 293 K.\n\nIt has been shown that uranium hexafluoride is an oxidant and a Lewis acid that is able to bind to fluoride; for instance, the reaction of copper(II) fluoride with uranium hexafluoride in acetonitrile is reported to form copper(II) heptafluorouranate(VI), Cu(UF).\n\nPolymeric uranium(VI) fluorides containing organic cations have been isolated and characterised by X-ray diffraction.\n\nUF is used in both of the main uranium enrichment methods — gaseous diffusion and the gas centrifuge method — because its triple point is at temperature 64.05 °C (147 °F, 337 K) and only slightly higher than normal atmospheric pressure. Fluorine has only a single naturally occurring stable isotope, so isotopologues of UF differ in their molecular weight based solely on the uranium isotope present.\n\nAll the other uranium fluorides are nonvolatile solids that are coordination polymers.\n\nGaseous diffusion requires about 60 times as much energy as the gas centrifuge process: gaseous diffusion-produced nuclear fuel produces 25 times more energy than is used in the diffusion process, while centrifuge-produced fuel produces 1,500 times more energy than is used in the centrifuge process.\n\nIn addition to its use in enrichment, uranium hexafluoride has been used in an advanced reprocessing method (fluoride volatility), which was developed in the Czech Republic. In this process, used oxide nuclear fuel is treated with fluorine gas to form a mixture of fluorides. This mixture is then distilled to separate the different classes of material.\n\nIn 2005, 686,500 tonnes in 57,122 storage cylinders were located near Portsmouth, Ohio; Oak Ridge, Tennessee; and Paducah, Kentucky.\n\nThe long-term storage of DUF presents environmental, health, and safety risks because of its chemical instability. When UF is exposed to moist air, it reacts with the water in the air to produce UOF (uranyl fluoride) and HF (hydrogen fluoride) both of which are highly corrosive and toxic. Storage cylinders must be regularly inspected for signs of corrosion and leaks. The estimated lifetime of the steel cylinders is measured in decades.\n\nThere have been several accidents involving uranium hexafluoride in the US, including a cylinder-filling accident and material release at the Sequoyah Fuels Corporation in 1986. The U.S. government has been converting DUF to solid uranium oxides for disposal. Such disposal of the entire DUF inventory could cost anywhere from $15 million to $450 million.\n\n"}
{"id": "27313303", "url": "https://en.wikipedia.org/wiki?curid=27313303", "title": "William Grylls Adams", "text": "William Grylls Adams\n\nWilliam Grylls Adams FRS (18 February 1836 in Laneast, Cornwall – 10 April 1915) was professor of Natural Philosophy at King's College, London.\n\nWilliam Grylls Adams was a younger brother of John Couch Adams (1819–1892). He graduated from St. John's College, Cambridge as 11th Wrangler in 1855. He undertook a teaching post at Highgate School in 1864.\n\nIn 1839, Alexandre Edmond Becquerel (1820–1891) had discovered that illumination of one of two metal plates in a dilute acid changed the electromotive force (EMF).\nIn 1876, Adams and Richard Evans Day discovered that illuminating a junction between selenium and platinum has a photovoltaic effect. This first demonstrated that electricity could be produced from light without moving parts and led to the modern solar cell.\n\nFrom 1878 to 1880 he was President of the Physical Society of London. In June 1872 he was elected a Fellow of the Royal Society and in 1875 delivered their Bakerian Lecture.\nHe was president of the Institute of Electrical Engineers and of the mathematical and physical section of the British Association.\n\n"}
{"id": "54754211", "url": "https://en.wikipedia.org/wiki?curid=54754211", "title": "Ξcc++", "text": "Ξcc++\n\nThe Xi double-charm baryon, denoted as formula_1, is a Xi baryon composed of two charm quarks and one up quark.\n\nIts discovery by the LHCb Collaboration was announced on 6 July 2017 in Venice, Italy, at the European Physical Society Conference on High Energy Physics. formula_1 is the first baryon discovered with two heavy quarks (charm and/or bottom) and a light quark. Its mass is 3621 MeV, nearly four times that of the proton.\n\nIt was identified when it disintegrated into a formula_3 baryon and three lighter mesons, K, π and π.\n\n\n"}
