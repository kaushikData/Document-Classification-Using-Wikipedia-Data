{"id": "8615043", "url": "https://en.wikipedia.org/wiki?curid=8615043", "title": "2006 Abule Egba pipeline explosion", "text": "2006 Abule Egba pipeline explosion\n\nThe 2006 Abule Egba pipeline explosion is a disaster that occurred in the heavily populated neighborhood of Abule Egba in Lagos, Nigeria, on 26 December 2006, killing hundreds of people. There were originally believed to be around 500 deaths, but it was later confirmed that the loss was smaller.\n\nThe incident occurred after an elevated pipeline carrying petroleum products was punctured by thieves earlier at midnight (local time), attracting hundreds of scavengers in the district who collected the fuel using plastic containers, allegedly to siphon fuel into a tanker, before puddles of fallen fuel were ignited after dawn. The cause of the explosion remains unknown, while witnesses have stated that the broken pipeline was tapped when the blast occurred.\n\nThe number of people killed is unclear, but is evidenced to be in the hundreds. Abiodun Orebiyi, the secretary-general of the Nigerian Red Cross (NRC), estimated that there were at least 200 dead but indicated that there was no official death toll and was unable to determine the final number of deaths, stating that the NRC \"[doesn't] know if it is 300, 400 or 500\". He also added that 60 people had been taken to the hospital with serious burns, while a number of houses had been destroyed, along with a mosque and a church. Another senior official, Ige Oladimeji, was quoted as saying that there has been 260 documented to be dead by nightfall. On the day of the explosion, a Reuters news agency photographer estimated 500 bodies in the scene.\n\n\n"}
{"id": "41705334", "url": "https://en.wikipedia.org/wiki?curid=41705334", "title": "2013 Magnolia Refinery oil spill", "text": "2013 Magnolia Refinery oil spill\n\nThe 2013 Magnolia Refinery oil spill occurred on March 9, 2013, when the line between a vital pump and an oil storage container broke. The split allowed a reported 15,000 barrels (1,788 m) or 7,000 tonnes of crude oil into the Little Corney Creek. The creek runs towards the town of Magnolia, Arkansas. The resulting oil slick was approximately long on the surface of the water, about north of the Louisiana state border. The leak was located at the Lion Oil and Trading & Transportation's Oil tank farm, between the settlements of Magnolia and El Dorado, owned by Delek's Logistic Unit. The United States Environmental Protection Agency (EPA) classified this pipeline rupture as a major spill because of the number of barrels estimated to have been spilled.\n\nThe city of Magnolia is in Columbia County, Arkansas, United States and is the county seat of the county. According to the 2009 U.S. Census Bureau reports, the population of the city is 10,951. The city itself has an approximate area of .\n\nLocal authorities were called to action and responded quickly to clean up the spill and monitor the damage. Cleanup workers made use of a \"vacuum truck\", a vehicle used to suck up the oil and water from the creek quickly before making efforts to separate the two liquids. This process was employed to minimize environmental damage to the local shorelines. Beginning oil removal as fast as possible is beneficial to the overall results of the cleanup process. The farther into the environment that the oil seeps, the longer it will take to remove and the more integrated it will become into the food chain, becoming inaccessible.\n\nThe United States Environmental Protection Agency (EPA) was vital too in the cleanup process at Magnolia, Arkansas after this oil spill. Low temperatures and inclement weather initially delayed the cleanup procedures, but the EPA was eventually able to send out crews to various locations within the affected area.\n\nDelek was required to pay for all cleanup operations after the spill and if the logistics unit corporation is confirmed to be guilty of negligence, for which they be fined accordingly.\n\n\n\n"}
{"id": "44357361", "url": "https://en.wikipedia.org/wiki?curid=44357361", "title": "Aircraft fuel tanks", "text": "Aircraft fuel tanks\n\nAircraft fuel tanks are a major component of aircraft fuel systems. They can be classified into internal or external tanks and further classified by method of construction or intended use. Safety aspects of aircraft fuel tanks were examined during the investigation of the 1996 TWA Flight 800 in-flight explosion accident.\n\nIntegral tanks are areas inside the aircraft structure that have been sealed to allow fuel storage. An example of this type is the \"wet wing\" commonly used in larger aircraft. Since these tanks are part of the aircraft structure, they cannot be removed for service or inspection. Inspection panels must be provided to allow internal inspection, repair, and overall servicing of the tank. Most large transport aircraft use this system, storing fuel in the wings, fuselage, and empennage of the aircraft.\n\nRigid removable tanks are installed in a compartment designed to accommodate the tank. They are typically of metal, plastic or fibreglass construction, and may be removed for inspection, replacement, or repair. The aircraft does not rely on the tank for structural integrity. These tanks are commonly found in smaller general aviation aircraft, such as the Cessna 172. Combat aircraft and helicopters generally use self-sealing fuel tanks.\n\nBladder tanks, bag tanks or fuel cells, are reinforced rubberised bags installed in a section of aircraft structure designed to accommodate fuel. The bladder is rolled up and installed into the compartment through the fuel filler neck or access panel, and is secured by means of snap fasteners or cord and loops inside the compartment. Many high-performance light aircraft, helicopters and some smaller turboprop aircraft use bladder tanks.\n\nMany aircraft designs feature fixed tip tanks mounted at the end of each wing. The weight of the tanks and fuel counteract wing bending loads during manoeuvres and reduce fatigue on the spar structure.\n\nConformal fuel tanks (CFTs) or 'Fast Packs' are additional fuel tanks fitted closely to the profile of an aircraft which extend either the range or endurance of the aircraft, with a reduced aerodynamic penalty compared to external drop tanks.\n\nDrop tanks, external tanks, wing tanks, pylon tanks or belly tanks are all terms used to describe auxiliary externally mounted fuel tanks. Drop tanks are generally expendable and often jettisonable. External tanks are commonplace on modern military aircraft and occasionally found in civilian ones, although the latter are less likely to be discarded except in the event of emergency.\n\nDrop tanks were originally designed to be jettisoned when empty or in the event of combat or emergency in order to reduce drag and weight, increasing manoeuvrability and range. Modern external tanks may be retained in combat, to be dropped in an emergency and are often not designed for the stresses of supersonic flight.\n\nFuel tanks have been implicated in aviation disasters, being the cause of the accident or worsening it (fuel tank explosion).\n\nThe official explanation for the explosion and subsequent crash of TWA Flight 800 is that an explosive fuel/air mixture was created in one of the aircraft's fuel tanks. Faulty wiring then provided an ignition source within the tank, destroying the airliner. While the accuracy of the official findings is still questioned in this case, similar explosions have occurred in other aircraft. It is possible to reduce the chance of fuel tank explosions by a fuel tank inerting system or fire fighting foam in the tanks.\n\n\n\n\n"}
{"id": "26204984", "url": "https://en.wikipedia.org/wiki?curid=26204984", "title": "Bangladesh Climate Change Resilience Fund", "text": "Bangladesh Climate Change Resilience Fund\n\nThe Bangladesh Climate Change Resilience Fund (BCCRF) is a multi-donor trust fund (MDTF) created to collect and disburse climate adaptation funding for Bangladesh.\n\nOn February 15, 2009, the Bangladeshi government refused to accept a £60m climate funding offer from the United Kingdom if it was channelled through the World Bank.\n\nAccording to Mohammed Shamsuddoha and Rezaul Karim Chowdhury, \"The 'draft concept note' prepared by the government of Bangladesh on the MDTF suggested that the secretariat be based in the World Bank office in Dhaka. The Bank would co-chair the management committee, and administer, manage, supervise and monitor implementation of the MDTF's projects and programmes. For this job, the Bank will charge a fee of $8 million. All implementing agencies will have to follow the Bank's guidelines and policies on project implementation and procurement.\"\n\nThe British Department for International Development (DFID) has been insisting that climate finance should be channeled through a Multi Donor Trust Fund managed by the World Bank; this position has been supported by the Danish government.\n\nAt the 2009 Copenhagen climate conference, a Bangladeshi delegate told reporters that Bangladesh might let the World Bank manage the funds, if that was set as a condition by funders. After the conference, Bangladeshi Prime Minister Sheikh Hasina stated that establishing a Multi Donor Trust Fund is part of her government's response to climate change.\n\nAccording to the \"New Age\", the Multi Donor Trust Fund has \"more or less unanimous opposition among climate change researchers, environment ministry officials and NGOs and civil society members in Bangladesh alike.\" Bangladeshi civil society organizations are strongly critical of the Multi Donor Trust Fund idea, generally preferring that funds be managed by a state-run board. Campaigners have expressed concern about the 10-15% management fees to be charged by World Bank consultants, and lack of democratic access to adaptation funding. Other campaigners are strongly critical of World Bank-funded projects, which they say \"have often created ecological hazards and destroyed ecological goods and services.\"\n\nBangladeshi opponents of a World Bank-managed MDTF include the Equity and Justice Working Group Bangladesh (EquityBD).\n\nBritish opponents include the Jubilee Debt Coalition and the World Development Movement.\n"}
{"id": "14471086", "url": "https://en.wikipedia.org/wiki?curid=14471086", "title": "Berito Kuwaru'wa", "text": "Berito Kuwaru'wa\n\nBerito Kuwaru'wa is a member of the Colombian U'wa people. He was awarded the Goldman Environmental Prize in 1998 for his role as spokesperson in conflicts between the U'wa people and the petroleum industry.\n"}
{"id": "5359950", "url": "https://en.wikipedia.org/wiki?curid=5359950", "title": "Bifluoride", "text": "Bifluoride\n\nThe bifluoride ion is an inorganic anion with the chemical formula .\n\nThe bifluoride ion has a linear, centrosymmetric structure (D symmetry), with a F−H bond length of 114 pm. The bond strength is estimated to be greater than 155 kJ/mol. In molecular orbital theory, the atoms to be held together by a 3-center 4-electron bond.\n\nIn the reaction\nthe ion is acting as a Brønsted–Lowry base. This reaction can occur in non-aqueous solution. On the other hand, in the reaction\nit is acting as an acid.\nSalts, such as potassium bifluoride and ammonium bifluoride can be made in the direct reaction\n"}
{"id": "70663", "url": "https://en.wikipedia.org/wiki?curid=70663", "title": "Brine", "text": "Brine\n\nBrine is a high-concentration solution of salt (usually sodium chloride) in water. In different contexts, brine may refer to salt solutions ranging from about 3.5% (a typical concentration of seawater, on the lower end of solutions used for brining foods) up to about 26% (a typical saturated solution, depending on temperature). Lower levels of concentration are called by different names: fresh water, brackish water, and saline water.\n\nBrine naturally occurs on Earth's surface (salt lakes), crust, and within brine pools on ocean bottom. High-concentration brine lakes typically emerge due to evaporation of ground saline water on high ambient temperatures. Brine is used for food processing and cooking (pickling and brining), for de-icing of roads and other structures, and in a number of technological processes. It is also a by-product of many industrial processes, such as desalination, and may pose an environmental risk due to its corrosive and toxic effects, so it requires wastewater treatment for proper disposal.\n\nSaline water with relatively high concentration of salt (usually sodium chloride) occurs naturally on Earth's surface (salt lakes), crust, and within brine pools on ocean bottom.\n\nNumerous processes exist which can produce brines in nature. Modification of seawater via evaporation results in the concentration of salts in the residual fluid, a characteristic geologic deposit called an evaporite is formed as different dissolved ions reach the saturation states of minerals, typically gypsum and halite. A similar process occurs at high latitudes as seawater freezes resulting in a fluid termed a cryogenic brine. At the time of formation, these cryogenic brines are by definition cooler than the freezing temperature of seawater and can produce a feature called a brinicle where cool brines descend, freezing the surrounding seawater.\n\nThe brine cropping out at the surface as saltwater springs are known as \"licks\" or \"salines\". The contents of dissolved solids in groundwater vary highly from one location to another on Earth, both in terms of specific constituents (e.g. halite, anhydrite, carbonates, gypsum, fluoride-salts, organic halides, and sulfate-salts) and regarding the concentration level. Using one of several classification of groundwater based on total dissolved solids (TDS), brine is water containing more than 100,000 mg/L TDS. Brine is commonly produced during well completion operations, particularly after the hydraulic fracturing of a well.\n\nBrine is a common agent in food processing and cooking. Brining is used to preserve or season the food. Brining can be applied to vegetables, cheeses and fruit in a process known as pickling. Meat and fish are typically steeped in brine for shorter periods of time, as a form of marination, enhancing its tenderness and flavor, or to enhance shelf period.\n\nElemental chlorine can be produced by electrolysis of brine (NaCl solution). This process also produces sodium hydroxide (NaOH) and Hydrogen gas (H). The reaction equations are as follows:\n\n\nBrine is used as a secondary fluid in large refrigeration installations for the transport of thermal energy from place to place. Most commonly used brines are based on inexpensive calcium chloride and sodium chloride. It is used because the addition of salt to water lowers the freezing temperature of the solution and the heat transport efficiency can be greatly enhanced for the comparatively low cost of the material. The lowest freezing point obtainable for NaCl brine is at the concentration of 23.3% NaCl by weight. This is called the eutectic point.\n\nSodium chloride brine spray is used on some fishing vessels to freeze fish. The brine temperature is generally . Air blast freezing temperatures are or lower. Given the higher temperature of brine, the system efficiency over air blast freezing can be higher. High-value fish usually are frozen at much lower temperatures, below the practical temperature limit for brine.\n\nBecause of the corrosive properties of salt-based brines, glycols such as polyethylene glycol have become more common for this purpose.\n\nBrine is an auxiliary agent in water softening and water purification systems involving ion exchange technology. The most common example are household dishwashers, utilizing natrium chloride in form of dishwasher salt. Brine is not involved in the purification process itself, but used for regeneration of ion-exchange resin on cyclical basis. The water being treated flows through the resin container until the resin is considered exhausted and water is purified to a desired level. Resin is then regenerated by sequentially backwashing the resin bed to remove accumulated solids, flushing removed ions from the resin with a concentrated solution of replacement ions, and rinsing the flushing solution from the resin. After treatment, ion-exchange resin beads saturated with calcium and magnesium ions from the treated water, are regenerated by soaking in brine containing 6–12% NaCl. The sodium ions from brine replace the calcium and magnesium ions on the beads.\n\nIn lower temperatures, a brine solution can be used to de-ice or reduce freezing temperatures on roads.\n\nBrine is a byproduct of many industrial processes, such as desalination for human consumption and irrigation, power plant cooling towers, produced water from oil and natural gas extraction, acid mine or acid rock drainage, reverse osmosis reject, chlor-alkali wastewater treatment, pulp and paper mill effluent, and waste streams from food and beverage processing. Along with diluted salts, it can contain residues of pretreatment and cleaning chemicals, their reaction byproducts and heavy metals due to corrosion. \n\nWastewater brine can pose a significant environmental hazard, both due to corrosive and sediment-forming effects of salts and toxicity of other chemicals diluted in it. It must be properly disposed, which may require permits and compliance with environmental regulations. \n\nThe simplest way to dispose of unpolluted brine from desalination plants and cooling towers is to return it back to the ocean. To limit the environmental impact, it can be diluted with another stream of water, such as the outfall of a wastewater treatment or power plant. Since brine is heavier than seawater and would accumulate on the ocean bottom, it requires methods to ensure proper diffusion, such as installing underwater diffusers in the sewerage. Other methods include drying in evaporation ponds, injecting to deep wells, and storing and reusing the brine for irrigation, de-icing or dust control purposes.\n\nTechnologies for treatment of polluted brine include: membrane filtration processes, such as reverse osmosis; ion exchange processes such as electrodialysis or weak acid cation exchange; or evaporation processes, such as brine concentrators and crystallizers employing mechanical vapour recompression and steam.\n\n"}
{"id": "245982", "url": "https://en.wikipedia.org/wiki?curid=245982", "title": "Buoyancy", "text": "Buoyancy\n\nIn physics, buoyancy () or upthrust, is an upward force exerted by a fluid that opposes the weight of an immersed object. In a column of fluid, pressure increases with depth as a result of the weight of the overlying fluid. Thus the pressure at the bottom of a column of fluid is greater than at the top of the column. Similarly, the pressure at the bottom of an object submerged in a fluid is greater than at the top of the object. This pressure difference results in a net upward force on the object. The magnitude of that force exerted is proportional to that pressure difference, and (as explained by Archimedes' principle) is equivalent to the weight of the fluid that would otherwise occupy the volume of the object, i.e. the displaced fluid.\n\nFor this reason, an object whose density is greater than that of the fluid in which it is submerged tends to sink. If the object is either less dense than the liquid or is shaped appropriately (as in a boat), the force can keep the object afloat. This can occur only in a non-inertial reference frame, which either has a gravitational field or is accelerating due to a force other than gravity defining a \"downward\" direction. In a situation of fluid statics, the net upward buoyancy force is equal to the magnitude of the weight of fluid displaced by the body.\n\nThe center of buoyancy of an object is the centroid of the displaced volume of fluid.\n\nArchimedes' principle is named after Archimedes of Syracuse, who first discovered this law in 212 B.C. For objects, floating and sunken, and in gases as well as liquids (i.e. a fluid), Archimedes' principle may be stated thus in terms of forces:\n— with the clarifications that for a sunken object the volume of displaced fluid is the volume of the object, and for a floating object on a liquid, the weight of the displaced liquid is the weight of the object.\n\nMore tersely: Buoyancy = weight of displaced fluid.\n\nArchimedes' principle does not consider the surface tension (capillarity) acting on the body, but this additional force modifies only the amount of fluid displaced and the spatial distribution of the displacement, so the principle that \"Buoyancy = weight of displaced fluid\" remains valid.\n\nThe weight of the displaced fluid is directly proportional to the volume of the displaced fluid (if the surrounding fluid is of uniform density). In simple terms, the principle states that the buoyancy force on an object is equal to the weight of the fluid displaced by the object, or the density of the fluid multiplied by the submerged volume times the gravitational acceleration, g. Thus, among completely submerged objects with equal masses, objects with greater volume have greater buoyancy. This is also known as upthrust.\n\nSuppose a rock's weight is measured as 10 newtons when suspended by a string in a vacuum with gravity acting upon it. Suppose that when the rock is lowered into water, it displaces water of weight 3 newtons. The force it then exerts on the string from which it hangs would be 10 newtons minus the 3 newtons of buoyancy force: 10 − 3 = 7 newtons. Buoyancy reduces the apparent weight of objects that have sunk completely to the sea floor. It is generally easier to lift an object up through the water than it is to pull it out of the water.\n\nAssuming Archimedes' principle to be reformulated as follows,\n\nthen inserted into the quotient of weights, which has been expanded by the mutual volume\n\nyields the formula below. The density of the immersed object relative to the density of the fluid can easily be calculated without measuring any volumes.:\n\nExample: If you drop wood into water, buoyancy will keep it afloat.\n\nExample: A helium balloon in a moving car. During a period of increasing speed, the air mass inside the car moves in the direction opposite to the car's acceleration (i.e., towards the rear). The balloon is also pulled this way. However, because the balloon is buoyant relative to the air, it ends up being pushed \"out of the way\", and will actually drift in the same direction as the car's acceleration (i.e., forward). If the car slows down, the same balloon will begin to drift backward. For the same reason, as the car goes round a curve, the balloon will drift towards the inside of the curve.\n\nThe equation to calculate the pressure inside a fluid in equilibrium is:\n\nwhere f is the force density exerted by some outer field on the fluid, and \"σ\" is the Cauchy stress tensor. In this case the stress tensor is proportional to the identity tensor:\n\nHere δ is the Kronecker delta. Using this the above equation becomes:\n\nAssuming the outer force field is conservative, that is it can be written as the negative gradient of some scalar valued function:\n\nThen:\n\nTherefore, the shape of the open surface of a fluid equals the equipotential plane of the applied outer conservative force field. Let the \"z\"-axis point downward. In this case the field is gravity, so Φ = −\"ρgz\" where \"g\" is the gravitational acceleration, \"ρ\" is the mass density of the fluid. Taking the pressure as zero at the surface, where \"z\" is zero, the constant will be zero, so the pressure inside the fluid, when it is subject to gravity, is\n\nSo pressure increases with depth below the surface of a liquid, as \"z\" denotes the distance from the surface of the liquid into it. Any object with a non-zero vertical depth will have different pressures on its top and bottom, with the pressure on the bottom being greater. This difference in pressure causes the upward buoyancy force.\n\nThe buoyancy force exerted on a body can now be calculated easily, since the internal pressure of the fluid is known. The force exerted on the body can be calculated by integrating the stress tensor over the surface of the body which is in contact with the fluid:\n\nThe surface integral can be transformed into a volume integral with the help of the Gauss theorem:\n\nwhere \"V\" is the measure of the volume in contact with the fluid, that is the volume of the submerged part of the body, since the fluid doesn't exert force on the part of the body which is outside of it.\n\nThe magnitude of buoyancy force may be appreciated a bit more from the following argument. Consider any object of arbitrary shape and volume \"V\" surrounded by a liquid. The force the liquid exerts on an object within the liquid is equal to the weight of the liquid with a volume equal to that of the object. This force is applied in a direction opposite to gravitational force, that is of magnitude:\n\nwhere \"ρ\" is the density of the fluid, \"V\" is the volume of the displaced body of liquid, and \"g\" is the gravitational acceleration at the location in question.\n\nIf this volume of liquid is replaced by a solid body of exactly the same shape, the force the liquid exerts on it must be exactly the same as above. In other words, the \"buoyancy force\" on a submerged body is directed in the opposite direction to gravity and is equal in magnitude to\n\nThe net force on the object must be zero if it is to be a situation of fluid statics such that Archimedes principle is applicable, and is thus the sum of the buoyancy force and the object's weight\n\nIf the buoyancy of an (unrestrained and unpowered) object exceeds its weight, it tends to rise. An object whose weight exceeds its buoyancy tends to sink. Calculation of the upwards force on a submerged object during its accelerating period cannot be done by the Archimedes principle alone; it is necessary to consider dynamics of an object involving buoyancy. Once it fully sinks to the floor of the fluid or rises to the surface and settles, Archimedes principle can be applied alone. For a floating object, only the submerged volume displaces water. For a sunken object, the entire volume displaces water, and there will be an additional force of reaction from the solid floor.\n\nIn order for Archimedes' principle to be used alone, the object in question must be in equilibrium (the sum of the forces on the object must be zero), therefore;\n\nand therefore\n\nshowing that the depth to which a floating object will sink, and the volume of fluid it will displace, is independent of the gravitational field regardless of geographic location.\n\nIt can be the case that forces other than just buoyancy and gravity come into play. This is the case if the object is restrained or if the object sinks to the solid floor. An object which tends to float requires a tension restraint force T in order to remain fully submerged. An object which tends to sink will eventually have a normal force of constraint N exerted upon it by the solid floor. The constraint force can be tension in a spring scale measuring its weight in the fluid, and is how apparent weight is defined.\n\nIf the object would otherwise float, the tension to restrain it fully submerged is:\n\nWhen a sinking object settles on the solid floor, it experiences a normal force of:\n\nAnother possible formula for calculating buoyancy of an object is by finding the apparent weight of that particular object in the air (calculated in Newtons), and apparent weight of that object in the water (in Newtons). To find the force of buoyancy acting on the object when in air, using this particular information, this formula applies:\n\nThe final result would be measured in Newtons.\n\nAir's density is very small compared to most solids and liquids. For this reason, the weight of an object in air is approximately the same as its true weight in a vacuum. The buoyancy of air is neglected for most objects during a measurement in air because the error is usually insignificant (typically less than 0.1% except for objects of very low average density such as a balloon or light foam).\n\nA simplified explanation for the integration of the pressure over the contact area may be stated as follows:\n\nConsider a cube immersed in a fluid with the upper surface horizontal.\n\nThe sides are identical in area, and have the same depth distribution, therefore they also have the same pressure distribution, and consequently the same total force resulting from hydrostatic pressure, exerted perpendicular to the plane of the surface of each side.\n\nThere are two pairs of opposing sides, therefore the resultant horizontal forces balance in both orthogonal directions, and the resultant force is zero.\n\nThe upward force on the cube is the pressure on the bottom surface integrated over its area. The surface is at constant depth, so the pressure is constant. Therefore, the integral of the pressure over the area of the horizontal bottom surface of the cube is the hydrostatic pressure at that depth multiplied by the area of the bottom surface.\n\nSimilarly, the downward force on the cube is the pressure on the top surface integrated over its area. The surface is at constant depth, so the pressure is constant. Therefore, the integral of the pressure over the area of the horizontal top surface of the cube is the hydrostatic pressure at that depth multiplied by the area of the top surface.\n\nAs this is a cube, the top and bottom surfaces are identical in shape and area, and the pressure difference between the top and bottom of the cube is directly proportional to the depth difference, and the resultant force difference is exactly equal to the weight of the fluid that would occupy the volume of the cube in its absence.\n\nThis means that the resultant upward force on the cube is equal to the weight of the fluid that would fit into the volume of the cube, and the downward force on the cube is its weight, in the absence of external forces.\n\nThis analogy is valid for variations in the size of the cube.\n\nIf two cubes are placed alongside each other with a face of each in contact, the pressures and resultant forces on the sides or parts thereof in contact are balanced and may be disregarded, as the contact surfaces are equal in shape, size and pressure distribution, therefore the buoyancy of two cubes in contact is the sum of the buoyancies of each cube. This analogy can be extended to an arbitrary number of cubes.\n\nAn object of any shape can be approximated as a group of cubes in contact with each other, and as the size of the cube is decreased, the precision of the approximation increases. The limiting case for infinitely small cubes is the exact equivalence.\n\nAngled surfaces do not nullify the analogy as the resultant force can be split into orthogonal components and each dealt with in the same way.\n\nA floating object is stable if it tends to restore itself to an equilibrium position after a small displacement. For example, floating objects will generally have vertical stability, as if the object is pushed down slightly, this will create a greater buoyancy force, which, unbalanced by the weight force, will push the object back up.\n\nRotational stability is of great importance to floating vessels. Given a small angular displacement, the vessel may return to its original position (stable), move away from its original position (unstable), or remain where it is (neutral).\n\nRotational stability depends on the relative lines of action of forces on an object. The upward buoyancy force on an object acts through the center of buoyancy, being the centroid of the displaced volume of fluid. The weight force on the object acts through its center of gravity. A buoyant object will be stable if the center of gravity is beneath the center of buoyancy because any angular displacement will then produce a 'righting moment'.\n\nThe stability of a buoyant object at the surface is more complex, and it may remain stable even if the centre of gravity is above the centre of buoyancy, provided that when disturbed from the equilibrium position, the centre of buoyancy moves further to the same side that the centre of gravity moves, thus providing a positive righting moment. If this occurs, the floating object is said to have a positive metacentric height. This situation is typically valid for a range of heel angles, beyond which the centre of buoyancy does not move enough to provide a positive righting moment, and the object becomes unstable. It is possible to shift from positive to negative or vice versa more than once during a heeling disturbance, and many shapes are stable in more than one position.\n\nThe atmosphere's density depends upon altitude. As an airship rises in the atmosphere, its buoyancy decreases as the density of the surrounding air decreases. In contrast, as a submarine expels water from its buoyancy tanks, it rises because its volume is constant (the volume of water it displaces if it is fully submerged) while its mass is decreased.\n\nAs a floating object rises or falls, the forces external to it change and, as all objects are compressible to some extent or another, so does the object's volume. Buoyancy depends on volume and so an object's buoyancy reduces if it is compressed and increases if it expands.\n\nIf an object at equilibrium has a compressibility less than that of the surrounding fluid, the object's equilibrium is stable and it remains at rest. If, however, its compressibility is greater, its equilibrium is then unstable, and it rises and expands on the slightest upward perturbation, or falls and compresses on the slightest downward perturbation.\n\nSubmarines rise and dive by filling large ballast tanks with seawater. To dive, the tanks are opened to allow air to exhaust out the top of the tanks, while the water flows in from the bottom. Once the weight has been balanced so the overall density of the submarine is equal to the water around it, it has neutral buoyancy and will remain at that depth. Most military submarines operate with a slightly negative buoyancy and maintain depth by using the \"lift\" of the stabilizers with forward motion.\n\nThe height to which a balloon rises tends to be stable. As a balloon rises it tends to increase in volume with reducing atmospheric pressure, but the balloon itself does not expand as much as the air on which it rides. The average density of the balloon decreases less than that of the surrounding air. The weight of the displaced air is reduced. A rising balloon stops rising when it and the displaced air are equal in weight. Similarly, a sinking balloon tends to stop sinking.\n\nUnderwater divers are a common example of the problem of unstable buoyancy due to compressibility. The diver typically wears an exposure suit which relies on gas-filled spaces for insulation, and may also wear a buoyancy compensator, which is a variable volume buoyancy bag which is inflated to increase buoyancy and deflated to decrease buoyancy. The desired condition is usually neutral buoyancy when the diver is swimming in mid-water, and this condition is unstable, so the diver is constantly making fine adjustments by control of lung volume, and has to adjust the contents of the buoyancy compensator if the depth varies.\n\nIf the weight of an object is less than the weight of the displaced fluid when fully submerged, then the object has an average density that is less than the fluid and when fully submerged will experience a buoyancy force greater than its own weight. If the fluid has a surface, such as water in a lake or the sea, the object will float and settle at a level where it displaces the same weight of fluid as the weight of the object. If the object is immersed in the fluid, such as a submerged submarine or air in a balloon, it will tend to rise.\nIf the object has exactly the same density as the fluid, then its buoyancy equals its weight. It will remain submerged in the fluid, but it will neither sink nor float, although a disturbance in either direction will cause it to drift away from its position.\nAn object with a higher average density than the fluid will never experience more buoyancy than weight and it will sink.\nA ship will float even though it may be made of steel (which is much denser than water), because it encloses a volume of air (which is much less dense than water), and the resulting shape has an average density less than that of the water.\n\n\n"}
{"id": "8673964", "url": "https://en.wikipedia.org/wiki?curid=8673964", "title": "Claude Mandil", "text": "Claude Mandil\n\nClaude Mandil (born 1942 in Lyon, France) is a French businessman. Mandil is a member of the Board of Directors of Total S.A. and is the former Executive Director of the International Energy Agency (IEA).\n\nMandil graduated from France's \"École Polytechnique\" and \"École des Mines\". From 1967 until 1981, he held various positions with the French Civil Services related to engineering and territorial planning.\n\nIn 1981-1982, he served as a technical advisor in the French Prime Minister's cabinet, responsible for industry, energy and research. In 1982 he was named Chief Executive Officer of the \"Institut de Développement Industriel\" (Institute for Industrial Development). In 1988 he became the Director General of the \"Bureau de Recherches Géologiques et Minières\" (Geological and Mining Research Bureau).\n\nFrom 1990 to 1998, Mandil served as Director General for Energy and Raw Materials at the Ministry of Industry, Post and Telecommunications. From 1997 to 1998 he served as Chairman of the IEA's Governing Board. From 1991 to 1998, he also represented France at the Nuclear Safety Working Group of the G7 and served as the President of this group in 1996. In 1997–1998 he was Chairman of the IEA's Governing Board.\n\nFrom 1998 to 2000, Mandil occupied the post of the Managing Director of Gaz de France, and then for several years was the Chairman and CEO of the \"Institut Français du Pétrole\" (French Institute of Petroleum). From 2003 to 2007 he was the Executive Director of the International Energy Agency. Since 2008, is a member of the Board of Directors of Total S.A. \nMandil was also a board member of the SBC Energy Institute and is director of Institut Veolia Environnement.\n\nHe is the author of a report on energy security, which was commissioned by the French Presidency of the Council of the European Union in 2008. In 2008, Mandil said that Europe must end its energy dependence on Russia. He said that \"We need more energy efficiency, more liquefied natural gas, more renewable energy, more nuclear energy\".\n\nMandil is married and has five children.\n"}
{"id": "1123485", "url": "https://en.wikipedia.org/wiki?curid=1123485", "title": "Composition B", "text": "Composition B\n\nComposition B, colloquially \"Comp B\", is an explosive consisting of castable mixtures of RDX and TNT. It is used as the main explosive filling in artillery projectiles, rockets, land mines, hand grenades and various other munitions. It was also used for the explosive lenses in the first implosion-type nuclear weapons developed by the United States.\n\nThe standard proportions of ingredients (by weight) are 59.5% RDX (detonation velocity of 8,750 m/s) and 39.4% TNT (detonation velocity of 6,900 m/s), phlegmatized with 1% paraffin wax. Most commonly it is described as 60/40 RDX/TNT with 1% wax added.\n\n\nComposition B was extremely common in western nations' munitions and was the standard explosive filler from early World War II until the early 1950s, when less sensitive explosives such as Composition H6 began to replace it in many weapons systems. M65 bombs from 1953 containing degraded Composition B were responsible for much of the damage in the 1967 USS \"Forrestal\" fire.\n\nSome NATO-approved munitions suppliers such as Mecar have continued to use Composition B in their products.\n\nComposition B is related to Cyclotol, which has a higher proportion of RDX (up to 75%).\n\nIMX-101 is slowly replacing Comp B in US military artillery shells, and IMX-104 in mortar rounds.\n\n"}
{"id": "40535392", "url": "https://en.wikipedia.org/wiki?curid=40535392", "title": "Cota (power)", "text": "Cota (power)\n\nCota is a wireless power technology built on the same principles and frequencies as Wi-Fi. Cota was announced on September 9, 2013 at TechCrunch Disrupt.\n\nCota can detect the position of a power receiver within its network and focus a signal to only that position, enabling up to 1 watt of power to be transmitted wirelessly. Since the charge hub can detect, focus, and send the signal to a specific point in 3D space, there is no risk of injury compared to other means of wireless power such as microwave power transmission. Additionally, since Cota uses the same frequency range as Wi-Fi, its range is about 30 feet, an improvement over inductive wireless standards such as Qi which typically have a range of only a few centimeters.\n"}
{"id": "1931707", "url": "https://en.wikipedia.org/wiki?curid=1931707", "title": "Critical dimension", "text": "Critical dimension\n\nIn the renormalization group analysis of phase transitions in physics, a critical dimension is the dimensionality of space at which the character of the phase transition changes. Below the lower critical dimension there is no phase transition. Above the upper critical dimension the critical exponents of the theory become the same as that in mean field theory. An elegant criterion to obtain the critical dimension within mean field theory is due to V. Ginzburg.\n\nSince the renormalization group sets up a relation between a phase transition and a quantum field theory, this has implications for the latter and for our larger understanding of renormalization in general. Above the upper critical dimension, the quantum field theory which belongs to the model of the phase transition is a free field theory. Below the lower critical dimension, there is no field theory corresponding to the model.\n\nIn the context of string theory the meaning is more restricted: the \"critical dimension\" is the dimension at which string theory is consistent assuming a constant dilaton background without additional confounding permutations from background radiation effects. The precise number may be determined by the required cancellation of conformal anomaly on the worldsheet; it is 26 for the bosonic string theory and 10 for superstring theory.\n\nDetermining the upper critical dimension of a field theory is a matter of linear algebra. It nevertheless is worthwhile to formalize the procedure because it yields the lowest-order approximation for scaling and essential input for the renormalization group. It also reveals conditions to have a critical model in the first place.\n\nA Lagrangian may be written as a sum of terms, each consisting of an integral over a monomial of coordinates formula_1 and fields formula_2. Examples are the standard formula_3-model and the isotropic Lifshitz tricritical point with Lagrangians\n\nsee also the figure on the right.\nThis simple structure may be compatible with a scale invariance under a rescaling of the\ncoordinates and fields with a factor formula_6 according to\nTime isn't singled out here — it is just another coordinate: if the Lagrangian contains a time variable then this variable is to be rescaled as formula_8 with some constant exponent formula_9. The goal is to determine the exponent set formula_10.\n\nOne exponent, say formula_11, may be chosen arbitrarily, for example formula_12. In the language of dimensional analysis this means that the exponents formula_13 count wave vector factors (a reciprocal length formula_14). Each monomial of the Lagrangian thus leads to a homogeneous linear equation formula_15 for the exponents formula_13. If there are formula_17 (inequivalent) coordinates and fields in the Lagrangian, then formula_17 such equations constitute a square matrix. If this matrix were invertible then there only would be the trivial solution formula_19.\n\nThe condition formula_20 for a nontrivial solution gives an equation between the space dimensions, and this determines the upper critical dimension formula_21 (provided there is only one variable dimension formula_22 in the Lagrangian). A redefinition of the coordinates and fields now shows that determining the scaling exponents formula_13 is equivalent to a dimensional analysis with respect to the wavevector formula_24, with all coupling constants occurring in the Lagrangian rendered dimensionless. Dimensionless coupling constants are the technical hallmark for the upper critical dimension.\n\nNaive scaling at the level of the Lagrangian doesn't directly correspond to physical scaling because a cutoff is required to give a meaning to the field theory and the path integral. Changing the length scale also changes the number of degrees of freedom.\nThis complication is taken into account by the renormalization group. The main result at the upper critical dimension is that scale invariance remains valid for large factors formula_6, but with additional formula_26 factors in the scaling of the coordinates and fields.\n\nWhat happens below or above formula_21 depends on whether one is interested in long distances (statistical field theory) or short distances (quantum field theory). Quantum field theories are trivial (convergent) below formula_21 and not renormalizable above formula_21. Statistical field theories are trivial (convergent) above formula_21 and renormalizable below formula_21. In the latter case there arise \"anomalous\" contributions to the naive scaling exponents formula_13. These anomalous contributions to the effective critical exponents vanish at the upper critical dimension.\n\nIt is instructive to see how the scale invariance at the upper critical dimension becomes a scale invariance below this dimension. For small external wave vectors the vertex functions formula_33 acquire additional exponents, for example formula_34. If these exponents are inserted into a matrix formula_35 (which only has values in the first column) the condition for scale invariance becomes formula_36. This equation only can be satisfied if the anomalous exponents of the vertex functions cooperate in some way. In fact, the vertex functions depend on each other hierarchically. One way to express this interdependence are the Dyson-Schwinger equations.\n\nNaive scaling at formula_21 thus is important as zeroth order approximation. Naive scaling at the upper critical dimension also classifies terms of the Lagrangian as relevant, irrelevant or marginal. A Lagrangian is compatible with scaling if the formula_1- and formula_2 -exponents formula_40 lie on a hyperplane, for examples see the figure above. formula_13 is a normal vector of this hyperplane.\n\nThe lower critical dimension formula_42 of a phase transition of a given universality class is the last dimension for which this phase transition doesn't occur if the dimension is increased starting with formula_43.\n\nThermodynamic stability of an ordered phase depends on entropy and energy. Quantitatively this depends on the type of domain walls and their fluctuation modes. There appears to be no generic formal way for deriving the lower critical dimension of a field theory. Lower bounds may be derived with statistical mechanics arguments.\n\nConsider first a one-dimensional system with short range interactions. Creating a domain wall requires a fixed energy amount formula_44. Extracting this energy from other degrees of freedom decreases entropy by formula_45. This entropy change must be compared with the entropy of the domain wall itself. In a system of length formula_46 there are formula_47 positions for the domain wall, leading (according to Boltzmann's principle) to an entropy gain formula_48. For nonzero temperature formula_49 and formula_46 large enough the entropy gain always dominates, and thus there is no phase transition in one-dimensional systems with short-range interactions at formula_51. Space dimension formula_52 thus is a lower bound for the lower critical dimension of such systems.\n\nA stronger lower bound formula_53 can be derived with the help of similar arguments for systems with short range interactions and an order parameter with a continuous symmetry. In this case the Mermin-Wagner-Theorem states that the order parameter expectation value vanishes in formula_54 at formula_51, and there thus is no phase transition of the usual type at formula_53 and below.\n\nFor systems with quenched disorder a criterion given by Imry and Ma might be relevant. These authors used the criterion to determine the lower critical dimension of random field magnets.\n\n"}
{"id": "27912128", "url": "https://en.wikipedia.org/wiki?curid=27912128", "title": "DESFA", "text": "DESFA\n\nDESFA (National Natural Gas System Operator S.A.) is a natural gas transmission system operator in Greece. It was established on 30 March 2007 as a subsidiary of DEPA. In addition to the transmission system, the company also operates Greece's gas distribution networks, and the Revithoussa LNG Terminal.\n\nOn 11 June 2013, Azerbaijani oil company SOCAR won the tender for acquisition of a 66% stake of DESFA for €400 million. Following Greece's January 2015 legislative election, the offered share was reduced to 49%. The deal, which was expected to be concluded by August 2015, failed in November 2016 as the last offer by SOCAR was rejected. The HRADF issued a new tender on 1 March 2017 with updated conditions of eligibility, officially terminating the previous one.\n\n"}
{"id": "32464938", "url": "https://en.wikipedia.org/wiki?curid=32464938", "title": "Energy in Portugal", "text": "Energy in Portugal\n\nEnergy in Portugal describes energy and electricity production, consumption and import in Portugal. Energy policy of Portugal will describe the politics of Portugal related to energy more in detail. Electricity sector in Portugal is the main article of electricity in Portugal.\n\nSines power plant (hard coal) started operation in 1985-1989 in Portugal. According to WWF its emissions were among the top dirty ones in Portugal in 2007.\n\nMaghreb–Europe Gas Pipeline (MEG) is a natural gas pipeline, from Algeria through Morocco to Andalusia, Spain,\n\nEU directive has a binding 31% target of renewables up from 20.5% in 2005. According to the Portuguese National Renewable Energy Action Plan by 2020 electricity will be produced: wind power 23% 14.6 TWh, of which 99% onshore, hydro power 22% 14.1 TWh, biomass 5% 3.52 TWh and photovoltaic solar power 2% 1.5 TWh and concentrated solar power 2% 1 TWh.\n\nPortugal has surported and increased the solar electricity (Photovoltaic power) and solar thermal energy (solar heating) during 2006-2010. Portugal was 9th in solar heating in the EU and 8th in solar power based on total volume in 2010.\n\nThere were no nuclear power plants in Portugal as of 2017.\n\nElectricity use (gross production + imports – exports – losses) was 51.2 TWh in 2008. Portugal imported 9 TWh electricity in 2008. Population was 10.6 million. In 2014 electricity was generated by 30% hydroelectricity, 27% natural gas, 22% wind, 20% coal and 1% solar.\n\nThe sustainable strategy has been a shift from individual to collective transport within the Lisbon Metropolitan Area (Metro Lisbon (ML), collective buses, Companhia Carris de ferro de Lisboa).\n\nAccording to Energy Information Administration the emissions from energy consumption of Portugal were in 2009 56.5 Mt, slightly over Bangladesh with 160 million people and Finland with 5.3 million people. The emissions per capita were (tonnes): Portugal 5.58, India 1.38, China 5.83, Europe 7.14, Russia 11.23, North America 14.19, Singapore 34.59 and United Arab Emirates 40.31.\n\n"}
{"id": "47446425", "url": "https://en.wikipedia.org/wiki?curid=47446425", "title": "Flow Energy", "text": "Flow Energy\n\nFlow Energy is an energy company in the United Kingdom. Flow Energy launched its domestic energy supply service in April 2013, supplying gas and electricity to homes throughout the UK.\n\nThe company has also developed a household electricity-generating boiler. The Flow boiler provides heating and hot water and also generates electricity, using patented technology, to be used in the household. Due to changes in subsidy support in the UK, efforts for commercialisation of the Flow boiler are now focused on the European market.\n\nFlow Energy is part of Flowgroup plc, an energy innovation and services company listed on the AIM exchange on the London Stock Market and founded in 1998. Its headquarters is in Ipswich, Suffolk.\n\nIn April 2014 Flow Energy launched its domestic energy supply service. At the end of 2015 Flow Energy exceeded 100,000 home energy customer accounts. This sector of the UK economy is dominated by a number of larger companies such as the Big Six Energy Suppliers (UK). Flow is considered as one of the emerging set of smaller suppliers.\n\nIn December 2015 the company announced a deal with Shell Energy Europe to provide access to wholesale energy - gas and electricity – without the need for significant levels of collateral to be deposited. The deal was designed to enable growth in the company’s energy supply business and increase the customer base for its product and services.\n\nIn April 2018 Flowgroup plc announced its intention to sell Flow Energy to Co-op Energy for £9.25 million. Flowgroup had previously sold its boiler technology business to focus on energy supply.\n\nThe company has developed a household boiler that feeds back electricity to the local National Grid. The Flow boiler provides heating and hot water whilst also generating low carbon electricity for the household using its patented MicroCHP (Combined Heat and Power) technology.\n\nFlow’s boiler was originally manufactured by Jabil Circuit Inc (“Jabil”), one of the world’s leading OEM manufacturing partners. However, Flow’s relationship with Jabil has now ended as the Flow boiler is no longer available in the UK. Jabil has been paid £4m to allow Flowgroup to exit the contract.\n\nThe boiler was designed in Capenhurst near Chester and manufactured in Livingston, Scotland. In July 2014 the company opened its research, training and development facility in Runcorn, Cheshire, where Gas Safe registered engineers undertake surveying, installation and aftercare training programmes to become accredited installers of the Flow boiler.\nHowever, with the change in focus for the Flow boiler to the European market, this centre has now closed and Manufacturing has now stopped, Jabil has been paid £7m to allow flowgroup to exit the contract.\n\nIn March 2016 the company launched a range of connected home devices including a smart thermostat, smart plugs and an energy consumption monitor. The devices are controlled and monitored through a smart phone app or an online panel and offer customers extra control over appliances and energy usage, as well as savings on energy.\n\nIn March 2015 Flow Energy was awarded a Real Business Everline Future 50 award, in the Environment and Sustainability category, for being one of the most disruptive British businesses of 2015.\n\nIn 2016, Flow Energy was one of only two energy suppliers to be awarded Which? Provider of the year 2016. Flow Energy was awarded Which? Recommended Provider status following comprehensive analysis of Flow Energy's policies, pricing, communications, customer service and complaint handling. The assessment process also included a survey of Flow Energy's customers which generated a Which? customer score of 73%, the overall industry average is 53%.\n\n"}
{"id": "13005389", "url": "https://en.wikipedia.org/wiki?curid=13005389", "title": "Fluoronium", "text": "Fluoronium\n\nThe fluoronium ion is an inorganic cation with the chemical formula . The structure of the salt with the anion has been determined. The fluoronium ion is isoelectronic with the water molecule and the amide ion. \n"}
{"id": "4160772", "url": "https://en.wikipedia.org/wiki?curid=4160772", "title": "Fuel gauge", "text": "Fuel gauge\n\nIn automobile and aircraft engineering a fuel gauge or gas gauge is an instrument used to indicate the amount of fuel in a fuel tank.\n\nAs used in vehicles, the gauge consists of two parts:\n\n\nThe sending unit usually uses a float connected to a potentiometer, typically printed ink design in a modern automobile. As the tank empties, the float drops and slides a moving contact along the resistor, increasing its resistance. In addition, when the resistance is at a certain point, it will also turn on a \"low fuel\" light on some vehicles.\n\nMeanwhile, the indicator unit (usually mounted on the dashboard) is measuring and displaying the amount of electric current flowing through the sending unit. When the tank level is high and maximum current is flowing, the needle points to \"F\" indicating a full tank. When the tank is empty and the least current is flowing, the needle points to \"E\" indicating an empty tank.\n\nThe system can be fail-safe. If an electrical fault opens, the electrical circuit causes the indicator to show the tank as being empty (theoretically provoking the driver to refill the tank) rather than full (which would allow the driver to run out of fuel with no prior notification). Corrosion or wear of the potentiometer will provide erroneous readings of fuel level. However, this system has a potential risk associated with it. An electric current is sent through the variable resistor to which a float is connected, so that the value of resistance depends on the fuel level. In most automotive fuel gauges such resistors are on the inward side of the gauge, i.e., inside the fuel tank. Sending current through such a resistor has a fire hazard and an explosion risk associated with it. These resistance sensors are also showing an increased failure rate with the incremental additions of alcohol in automotive gasoline fuel. Alcohol increases the corrosion rate at the potentiometer, as it is capable of carrying current like water. Potentiometer applications for alcohol fuel use a pulse-and-hold methodology, with a periodic signal being sent to determine fuel level decreasing the corrosion potential. Therefore, demand for another safer, non-contact method for fuel level is desired.\n\nMagnetoresistance type fuel level sensors, now becoming common in small aircraft applications, offer a potential alternative for automotive use. These fuel level sensors work similar to the potentiometer example, however a sealed detector at the float pivot determines the angular position of a magnet pair at the pivot end of the float arm. These are highly accurate, and the electronics are completely outside the fuel. The non-contact nature of these sensors address the fire and explosion hazard, and also the issues related to any fuel combinations or additives to gasoline or to any alcohol fuel mixtures. Magneto resistive sensors are suitable for all fuel or fluid combinations, including LPG and LNG. The fuel level output for these senders can be ratiometric voltage or preferable CAN bus digital. These sensors also fail-safe in that they either provide a level output or nothing.\n\nSystems that measure large fuel tanks (including underground storage tanks) may use the same electro-mechanical principle or may make use of a pressure sensor, sometimes connected to a mercury manometer.\n\nMany large transport aircraft use a different fuel gauge design principle. An aircraft may use a number (around 30 on an A320) of low voltage tubular capacitor probes where the fuel becomes the dielectric. At different fuel levels, different values of capacitance are measured and therefore the level of fuel can be determined. In early designs, the profiles and values of individual probes were chosen to compensate for fuel tank shape and aircraft pitch and roll attitudes. In more modern aircraft, the probes tend to be linear (capacitance proportional to fuel height) and the fuel computer works out how much fuel there is (slightly different on different manufacturers). This has the advantage that a faulty probe may be identified and eliminated from the fuel calculations. In total this system can be more than 99% accurate. Since most commercial aircraft only take on board fuel necessary for the intended flight (with appropriate safety margins), the system allows the fuel load to be preselected, causing the fuel delivery to be shut off when the intended load has been taken on board.\n\n\n"}
{"id": "914406", "url": "https://en.wikipedia.org/wiki?curid=914406", "title": "Giant Robo (OVA)", "text": "Giant Robo (OVA)\n\n\"Giant Robo\" is a homage to Yokoyama's career. The series features characters and plotlines from the manga artist's entire canon of work, effectively creating an all-new story. The events take place in the near future, ten years after the advent of the Shizuma Drive triggers the third energy revolution. The series follows the master of the titular Robo, Daisaku Kusama, and the Experts of Justice, an international police organization locked in battle with the BF Group, a secret society hell-bent on world domination.\n\nThe OVA is recognized for its \"retro\" style and operatic score. The character designs emulate Yokoyama's drawing style and the action setpieces are influenced by Hong Kong action cinema.\n\nThe first installment of the series, \"The Black Attaché Case,\" was released July 22, 1992. Originally intended to finish within 36 months, the seven-volume series was ultimately released over the span of six years. \"The Grand Finale\" was released January 25, 1998. The OVA has since been translated into English, Cantonese, Dutch, French, Italian and Korean.\n\nThe series takes place in a retro-futuristic setting, where the Shizuma Drive ends the depletion of petroleum resources and the need for nuclear power. The system is a non-polluting recyclable energy source that powers everything on land, sea and air. Ten years prior to the events of the series a team of scientists, led by Professor Shizuma, created the revolutionary system. In the process they nearly destroyed the world and one of their own, Franken von Vogler, was lost in the event that went down in history as the \"Tragedy of Bashtarle.\" At the start of \"Giant Robo\", the BF Group is in the middle of recreating the event with aid from the resurfaced von Vogler.\n\nThe story explores a society completely brought down, within the span of one week, because of dependency on a single energy source and a state of prosperity tainted by compromise and deceit.\n\nThe is the main antagonist of the series. Their origin is unknown, but not so their reason to be: to lead mankind down a road of ruin. The Group's forces consist of mechanical monsters, foot soldiers and , individuals with superhuman powers.\n\nThe most powerful Experts form the ruling cadre of the organization, the cabal of the . Its members swear allegiance to , the Group's founder and leader, with faltering loyalty punishable by death. At the time of \"The Day the Earth Stood Still\", the Ten are gearing up for the final showdown with the IPO.\n\nThe is the BF Group's counterpart in the \"Giant Robo\" universe. The leaders of the world acknowledged Big Fire as a threat to world security and signed the charter creating the IPO. The IPO's methods are information and espionage, looking to bring down the BF Group rather than defeating them in an all-out war.\n\nHowever, to counter Big Fire's superhuman elements, \"Experts\" are recruited and granted special international jurisdiction. The agents assembled are known as the . Working with the Experts from the Peking Branch is Daisaku Kusama. While he does not possess any special powers, Daisaku is the one and only master of Giant Robo. Constructed by Daisaku's father, Giant Robo is the IPO's trump card against Big Fire.\n\nIn 1990, producer Yasuhito Yamaki approached Yasuhiro Imagawa about working on an animated version of the \"Giant Robo\" manga. Imagawa, a self-proclaimed fan of Yokoyama's work, jumped at the chance of working on the project. The Giant Robo manga had started in 1967 and had never been adapted into animation before. It had a live action live-action adaptation that premiered on TV Asahi in 1967.\n\nIn pre-production, Imagawa was informed he could not use any of the supporting characters from the manga or live-action versions. Instead, with Yokoyama's permission, he populated the series with characters from the artist's entire canon of work including \"Akakage\", \"Babel II\" and \"Godmars\". The \"Giant Robo\" OVA still follows Daisaku and Robo, and the main antagonist is still called \"Big Fire,\" but it features an all-new storyline with a completely different cast of characters.\n\nThe first episode was released July 22, 1992 with the following three installments staying close to the proposed schedule of six months between releases. In the nine months between the releases of \"Volume\"s \"4\" and \"5\", two OVA's focusing on the character of GinRei were produced. is a humorous take on GinRei's job as a spy for the IPO. , a send up of super robot series, features Ken Ishikawa as guest mech designer. A third OVA, , was released after \"Volume 5\" of \"Giant Robo\".\n\nThe final episode was released January 1998, almost three years after episode six. In between releases, members of the \"Giant Robo\" staff worked on other projects including \"The Big O\", \"Getter Robo Armageddon\", and \"Super Atragon\", a two-episode OVA of Shunro Oshikawa's \"Kaitei Gunkan\" novel.\n\nImagawa intended \"The Day the Earth Stood Still\" to be the second to final chapter in the conflict between the Experts of Justice and Big Fire. The OVA would be preceded by \"The Birth of Zangetsu the Midday\", \"The Plan to Assassinate Daisaku - the Canary Penitentiary\", \"The Boy of Three Days\", \"The Greatest Battle in History - General Kanshin vs. Shokatsu Koumei\" and \"The Boy Detective, Kindaichi Shōtarō, Appears!\" The final chapter is titled \"The Siege of Babel\". No further stories have been animated.\n\nThe score of \"Giant Robo\" was composed, arranged and conducted by Masamichi Amano and performed by the Warsaw Philharmonic Orchestra and Choir. The music ranges from grand pieces like \"Charge! His Name is Giant Robo\" to more light-hearted tracks like \"Tetsugyu in Love.\" Amano makes use of leitmotifs, recurring musical themes associated with different characters, places or events.\n\nThe music of \"Giant Robo\" has been called one of the OVA's best accomplishments. The complete score was released in seven soundtracks by Nippon Columbia. The first two soundtracks were released in North America by AnimeTrax.\n\nFor series director Yasuhiro Imagawa, the world where the story unfolds must be convincing, for it is the setting and themes what determine the character and mecha designs. In the world of \"Giant Robo\", \"anything goes\". The technology is futuristic and the morals are modern. The world's outlook is bright, but there is an underlying sense of some sinister motive beneath it. \"Wuxia\" heroes coexist with modern-day espers and giant robots as soldiers in a struggle between good and evil.\n\nThe \"Giant Robo\" OVA is one of many anime titles based on old properties produced in Japan during the 1990s. While titles like \"Bubblegum Crisis 2040\", \"Dirty Pair Flash\" and \"Tekkaman Blade\" gave modern spins to old classics, the creators of \"Giant Robo\" decided to go with a \"retro\" look.\n\nThe characters were designed by Toshiyuki Kubooka (\"Lunar\" series, \"The Idolmaster\") and Akihiko Yamashita. (\"Princess Nine\", \"Tide-Line Blue\") The designers were asked to emulate Yokoyama's characters rather than create new ones. Admittedly \"it took some time to catch on to Director Imagawa's intentions\", but with repetition the staff was able to achieve Imagawa's vision of characters that look like they stepped out of anime from the 1960s.\n\nThe mechanical design is a case of high technology meets old school engineering. The titular mecha is an advanced piece of machinery, equipped with booster rockets and hidden weapons throughout. Sporting big stovepipe arms and exposed rivets, the hulking giant is more like a weapon of mass destruction than a \"robot superhero\". Vehicles like the Shizuma-powered Beetle or the Experts' airship fortress in the design of a zeppelin appear as if they had been conceived at the turn of the 20th century, giving the world of \"Giant Robo\" a timeless feel.\n\n\"Giant Robo\" is credited with generating interest in re-imagining other artists' works, including Osamu Tezuka and Go Nagai, and creating a \"retro\" style that has been used in productions like \"Sakura Wars\".\n\nThe modern notion of the giant robot genre can be traced back to the 1970s. The works of Go Nagai (\"Mazinger\", \"Getter Robo\") created the genre and the debut of Yoshiyuki Tomino's \"Mobile Suit Gundam\" in 1979 solidified it. In this genre, the mecha is the focal point of the action. But for a genre anime, \"Giant Robo\" does not feature many giant robot battles; instead, it is the human characters who do the fighting.\n\nMost of the \"Experts\" featured in \"The Day the Earth Stood Still\" come from Yokoyama's manga adaptations of \"Outlaws of the Marsh\" and \"Romance of the Three Kingdoms\", both \"wuxia\" novels and half of the \"Four Classics\" of Chinese literature. \"Wuxia\" are martial arts adventures populated by skilled, honorable fighters. In Hong Kong action cinema, the genre is associated with swordplay epics sprinkled with mysticism.\n\nGiven their origin, the heroes and villains of \"Giant Robo\" are superhuman combatants who share many elements with the errant knights of \"wuxia\" like strength, magic powers and the ability to fly. In \"wuxia\" adventures, the characters are given nicknames that allude to their mastery of weapons, their physical appearance or their demeanor (\"Crouching Tiger, Hidden Dragon\"). Imagawa, inspired by Yokayama's adaptation of \"Outlaws of the Marsh\", followed this convention and gave \"Giant Robo\"'s characters similar descriptive names like \"Shockwave Alberto\", \"Silent Chūjō\" and \"Kenji The Immortal.\"\n\nIn \"jiang hu\", secret societies plot against the \"status quo\" (\"House of Flying Daggers\") and powerful clans do war with each other. \"Giant Robo\" features the ongoing conflict between the Experts of Justice and the BF Group. Woven into the story are values commonly associated with martial arts films like honor, loyalty and individual justice, best exemplified in the rivalry between Alberto and Taisō.\n\n\"Giant Robo\" is structured as a \"character driven drama,\" emphasizing the relationships and personal histories of the characters over a mystery surrounding the titular mecha or a philosophical diatribe. At the time of the series, Daisaku is 12 years old and the only child in the Experts of Justice. \"Giant Robo\" is the story of how Daisaku grows up and works to respect his father's last will and testament of protecting the world from the BF Group.\n\nThe OVA is comparable to a \"bildungsroman\", where the story traces the main character's development from childhood to maturity. The two most important characters in the boy's coming of age are his fellow Experts Tetsugyu and Kenji Murasame. Tetsugyu is a grown-up, but still a \"child\" at heart. In the course of the story he and Daisaku grow up and mature, creating a parallel between them. Kenji, on the other hand, is what Daisaku sees as an \"adult.\" Willing to sacrifice others for the sake of happiness, Kenji contrasts Daisaku's idealism.\n\nThe story delves on the relationship between fathers and sons and the unbreakable bond that exists between them. The characters of Daisaku and Genya lost their fathers at a young age and have been entrusted with a legacy that turns them to adversaries during \"The Day the Earth Stood Still\". Mirror images of one another, the characters fight to fulfill their respective fathers' dying wish at the expense of the other's.\n\nDr. Kusama's dying question to Daisaku (\"Can happiness be obtained without sacrifice? Can a new era be achieved without tragedy?\") is at the crux of the series. Shizuma and his colleagues gave the world a new era of prosperity at the expense of billions of lives. The BF Group is willing to cause great misfortune to make their ideal world a reality, while many sacrifice themselves to protect it. The ending is bittersweet, with both sides suffering losses. For Imagawa, this was the only way of getting his point across. The series ends with a dedication to all fathers and their children \"giving a glimmer of hope in the midst of all the sorrow.\"\n\nThe series may also be seen as \"sort of an analogy or an allegory about nuclear technology\", especially in the context of Japan's complex relationship to both its destructive and constructive aspects.\n\nThe series was originally released by Bandai Visual on VHS and LaserDisc from 1992 to 1998. On March 24, 2000, Toshiba Entertainment released the series on Region 2 DVD. The \"Giant Robo Giga Premium Collection\" (ASBY-1600) features digitally remastered video and audio, interviews with the creators and a companion book.\n\nThe distribution of the English-language version has been handled by five different companies. A LaserDisc edition was released by L.A. Hero in 1994. After L.A. Hero's license expired, it was released on VHS by U.S. Renditions and Manga Entertainment. After Manga Entertainment's license expired, Media Blasters released \"Giant Robo\" on DVD. Media Blasters' 2004 release includes the Japanese language track, the Manga Entertainment dub, an all-new dub by NYAV Post and subtitled Japanese commentary tracks on some of the episodes. After Media Blasters' license expired, Discotek Media announced at Otakon 2018 on August 12th that they have picked up the series for a Blu-ray release. Despite the included Gin-Rei OVA being in SD, it will include everything from Media Blasters' previous release. Discotek has also hoped to produce brand new extras for this release as well.\n\n\"Giant Robo\" was distributed in the Netherlands by Manga DVD and in Hong Kong by Asia Video. The French version, dubbed by \"Saint Maur\" Studios, was distributed by \"Pathé\". The series was released in Italy by Granata Press, with an upcoming release by Shin Vision.\n\nAlong with the animated version, Imagawa scripted a manga illustrated by Mari Mizuta. Serialized in Kadokawa Shoten's \"Comics Genki\", it delves deeper into the machinations of the BF Group and introduces as Big Fire's counterpart in the IPO. The issues were later collected in two volumes published under the Newtype 100% Comics imprint.\n\n\nA novelization by Hiroshi Yamaguchi was released in September 1993. (Kadokawa Sneaker Bunko: )\n\nIn commemoration of \"Giant Robo\"s 40th anniversary, Imagawa began scripting . The manga, illustrated by Yasunari Toda, was serialized in Akita Shoten's \"Champion Red\" between September 2006 and January 2011. It chronicles Daisaku's involvement in a three-way battle between the IPO, the Magnificent Ten and the Murasame Clan. \"The Day the Earth Burned\" follows the general tone and style of the OVA but takes place in a reality separate from that of the OVA; \"The Day the Earth Burned\" explores what the OVA did not: the true nature of the GR Project and the true leader of Big Fire, Big Fire himself.\n\n\nThe \"Giant Robo\" video game (SLPM-62526) was released for PlayStation 2 on November 3, 2004 by D3 Publisher. Set in \"The Day the Earth Stood Still\" continuity, the player leads Daisaku Kusama and Giant Robo on a quest to defeat the BF Group. As Daisaku, the player can pick up items and power-ups on the battlefield; as Giant Robo, the player does battle with other mechas. A Versus Mode allows players to compete against each other using any of the robots featured on the series. The game is not available in Europe or North America.\n\n\"Mighty Ginrei: Final Fight\" (COCC-12444), an audio drama sequel to the \"Mighty GinRei\" OVA, was released on April 21, 1995 by Nippon Columbia. \"Love Fight\", a music collection tie-in, was released the same day.\n\nThe final installment of \"Giant Robo\" was released on January 25, 1998, eight years after production began and a full decade since its inception. The feature suffered from high running costs and low sales, but was better received in America. The series appeared in the 62nd position of \"Animage\"'s Top 100 Anime List, published in January 2001. In July of the same year, the series appeared on a list of the all time top 50 anime, according to \"Wizard Magazine\".\n\nCritical reception has been largely positive. Three different reviewers from the AnimeOnDVD site gave \"Giant Robo\" an \"A+\". John Huxley of Anime Boredom \"highly recommends\" the series and Anime Academy gives it a grade of 88%.\n\n\"Giant Robo\" has been called \"one of the true timeless classics of Anime.\" Mike Crandol of Anime News Network says Imagawa \"takes the best of the old and mixes it with the best of the new to create the definitive giant robot story.\" John Huxley of Anime Boredom concludes the series is \"the super robot show as it was in your mind's eye, a perfect combination of the old without the disappointment of reality.\"\n\n"}
{"id": "911833", "url": "https://en.wikipedia.org/wiki?curid=911833", "title": "Graphene", "text": "Graphene\n\nGraphene is an allotrope (form) of carbon consisting of a single layer of carbon atoms arranged in a hexagonal lattice. It is a semimetal with small overlap between the valence and the conduction bands (zero bandgap material).<ref name=\"10.1002/adma.201502544 p.6519-6525\"></ref> It is the basic structural element of many other allotropes of carbon, such as graphite, diamond, charcoal, carbon nanotubes and fullerenes.\n\nGraphene has many uncommon properties. It is the strongest material ever tested, conducts heat and electricity efficiently, and is nearly transparent. Graphene shows a large and nonlinear diamagnetism, greater than that of graphite, and can be levitated by neodymium magnets.\n\nScientists theorized about graphene for years. It had been produced unintentionally in small quantities for centuries through the use of pencils and other similar graphite applications. It was observed originally in electron microscopes in 1962, but it was studied only while supported on metal surfaces. The material was later rediscovered, isolated, and characterized in 2004 by Andre Geim and Konstantin Novoselov at the University of Manchester. Research was informed by existing theoretical descriptions of its composition, structure, and properties. This work resulted in the two winning the Nobel Prize in Physics in 2010 \"for groundbreaking experiments regarding the two-dimensional material graphene.\"\n\n\"Graphene\" is a combination of \"graphite\" and the suffix -ene, named by Hanns-Peter Boehm and colleagues, who produced and observed single-layer carbon foils in 1962.\n\nBoehm et al. introduced the term \"graphene\" in 1986 to describe single sheets of graphite. Graphene can be considered an \"infinite alternant\" (only six-member carbon ring) polycyclic aromatic hydrocarbon.\n\nThe International Union of Pure and Applied Chemistry (IUPAC) notes, \"previously, descriptions such as graphite layers, carbon layers, or carbon sheets have been used for the term graphene...it is incorrect to use for a single layer a term which includes the term graphite, which would imply a three-dimensional structure. The term graphene should be used only when the reactions, structural relations or other properties of individual layers are discussed.\"\n\nGeim defined \"isolated or free-standing graphene\" as \"graphene is a single atomic plane of graphite, which – and this is essential – is sufficiently isolated from its environment to be considered free-standing.\" This definition is narrower than the IUPAC definition and refers to cloven, transferred and suspended graphene. Other forms such as graphene grown on various metals, can become free-standing if, for example, suspended or transferred to silicon dioxide () or silicon carbide.\n\nGraphene is a crystalline allotrope of carbon with 2-dimensional properties. Its carbon atoms are packed densely in a regular atomic-scale chicken wire (hexagonal) pattern.\n\nEach atom has four bonds: one σ bond with each of its three neighbors and one π-bond that is oriented out of plane. The atoms are about 1.42 Å apart.\n\nGraphene's hexagonal lattice can be regarded as two interleaving triangular lattices. This perspective was successfully used to calculate the band structure for a single graphite layer using a tight-binding approximation.\n\nGraphene's stability is due to its tightly packed carbon atoms and a sp orbital hybridization – a combination of orbitals s, p and p that constitute the σ-bond. The final p electron makes up the π-bond. The π-bonds hybridize together to form the π-band and π∗-bands. These bands are responsible for most of graphene's notable electronic properties, via the half-filled band that permits free-moving electrons.\n\nGraphene sheets in solid form usually show evidence in diffraction for graphite's (002) layering. This is true of some single-walled nanostructures. However, unlayered graphene with only (hk0) rings has been found in the core of presolar graphite onions. TEM studies show faceting at defects in flat graphene sheets and suggest a role for two-dimensional crystallization from a melt.\n\nGraphene can self-repair holes in its sheets when exposed to molecules containing carbon, such as hydrocarbons. Bombarded with pure carbon atoms, the atoms perfectly align into hexagons, completely filling the holes.\n\nThe atomic structure of isolated, single-layer graphene is studied by TEM on sheets of graphene suspended between bars of a metallic grid. Electron diffraction patterns showed the expected honeycomb lattice. Suspended graphene showed \"rippling\" of the flat sheet, with amplitude of about one nanometer. These ripples may be intrinsic to the material as a result of the instability of two-dimensional crystals, or may originate from the ubiquitous dirt seen in all TEM images of graphene. Atomic resolution real-space images of isolated, single-layer graphene on substrates are available via scanning tunneling microscopy (STM). Photoresist residue, which must be removed to obtain atomic-resolution images, may be the \"adsorbates\" observed in TEM images, and may explain the observed rippling. Rippling on is caused by conformation of graphene to the underlying and is not intrinsic.\n\nAb initio calculations show that a graphene sheet is thermodynamically unstable if its size is less than about (\"graphene is the least stable structure until about 6000 atoms\") and becomes the most stable fullerene (as within graphite) only for molecules larger than 24,000 atoms.\n\nAnalogs (also referred to as \"artificial graphene\") are two-dimensional systems that exhibit similar properties to graphene. Analogs can be systems in which the physics is easier to observe and to manipulate. In those systems, electrons are not always the chosen particles—they might be optical photons, microwave photons, plasmons, microcavity polaritons or even atoms. Also, the honeycomb structure in which those particles evolve can be of a different nature than carbon atoms in graphene. It can be, respectively, a photonic crystal, an array of metallic rods, metallic nanoparticles, a lattice of coupled microcavities or an optical lattice.\n\nGraphene has a theoretical specific surface area (SSA) of . This is much larger than that reported to date for carbon black (typically smaller than ) or for carbon nanotubes (CNTs), from ≈100 to and is similar to activated carbon.\n\nGraphene is a zero-gap semiconductor, because its conduction and valence bands meet at the Dirac points, which are six locations in momentum space, on the edge of the Brillouin zone, divided into two non-equivalent sets of three points. The two sets are labeled K and K'. The sets give graphene a valley degeneracy of . By contrast, for traditional semiconductors the primary point of interest is generally Γ, where momentum is zero. However, if the in-plane direction is confined, in which case it is referred to as a nanoribbon, its electronic structure is different. If it is \"zig-zag\", the bandgap is zero. If it is \"armchair\", the bandgap is non-zero (see figure)...\n\nGraphene displays remarkable electron mobility at room temperature, with reported values in excess of . Hole and electron mobilities were expected to be nearly identical. The mobility is nearly independent of temperature between and , which implies that the dominant scattering mechanism is defect scattering. Scattering by graphene's acoustic phonons intrinsically limits room temperature mobility to at a carrier density of , times greater than copper.\n\nThe corresponding resistivity of graphene sheets would be . This is less than the resistivity of silver, the lowest otherwise known at room temperature. However, on substrates, scattering of electrons by optical phonons of the substrate is a larger effect than scattering by graphene’s own phonons. This limits mobility to . Charge transport is affected by adsorption of contaminants such as water and oxygen molecules. This leads to non-repetitive and large hysteresis I-V characteristics.\n\nElectrons propagating through graphene's honeycomb lattice effectively lose their mass, producing quasi-particles that are described by a 2D analogue of the Dirac equation rather than the Schrödinger equation for spin- particles.\n\nSuperconductivity has been observed in twisted bilayer graphene.\n\nGraphene's unique optical properties produce an unexpectedly high opacity for an atomic monolayer in vacuum, absorbing of red light, where \"α\" is the fine-structure constant. This is a consequence of the \"unusual low-energy electronic structure of monolayer graphene that features electron and hole conical bands meeting each other at the Dirac point... [which] is qualitatively different from more common quadratic massive bands.\" Based on the Slonczewski–Weiss–McClure (SWMcC) band model of graphite, the interatomic distance, hopping value and frequency cancel when optical conductance is calculated using Fresnel equations in the thin-film limit.\n\nAlthough confirmed experimentally, the measurement is not precise enough to improve on other techniques for determining the fine-structure constant.\n\nMulti-parametric surface plasmon resonance was used to characterize both thickness and refractive index of chemical-vapor-deposition (CVD)-grown graphene films. The measured refractive index and extinction coefficient values at 670 nm wavelength are 3.135 and 0.897, respectively. The thickness was determined as 3.7 Å from a 0.5 mm area, which agrees with 3.35 Å reported for layer-to-layer carbon atom distance of graphite crystals.\n\nThe method can be used for real-time label-free interactions of graphene with organic and inorganic substances. The existence of unidirectional surface plasmons in the nonreciprocal graphene-based gyrotropic interfaces has been demonstrated theoretically. By efficiently controlling the chemical potential of graphene, the unidirectional working frequency can be continuously tunable from THz to near-infrared and even visible. Particularly, the unidirectional frequency bandwidth can be 1–2 orders of magnitude larger than that in metal under the same magnetic field, which arises from the superiority of extremely small effective electron mass in graphene.\n\nGraphene's band gap can be tuned from 0 to (about 5 micrometre wavelength) by applying voltage to a dual-gate bilayer graphene field-effect transistor (FET) at room temperature. The optical response of graphene nanoribbons is tunable into the terahertz regime by an applied magnetic field. Graphene/graphene oxide systems exhibit electrochromic behavior, allowing tuning of both linear and ultrafast optical properties.\n\nA graphene-based Bragg grating (one-dimensional photonic crystal) demonstrated its capability for excitation of surface electromagnetic waves in the periodic structure using a He–Ne laser as the light source.\n\nSuch unique absorption could become saturated when the input optical intensity is above a threshold value. This nonlinear optical behavior is termed saturable absorption and the threshold value is called the saturation fluence. Graphene can be saturated readily under strong excitation over the visible to near-infrared region, due to the universal optical absorption and zero band gap. This has relevance for the mode locking of fiber lasers, where fullband mode locking has been achieved by a graphene-based saturable absorber. Due to this special property, graphene has wide application in ultrafast photonics. The optical response of graphene/graphene oxide layers can be tuned electrically. Saturable absorption in graphene could occur at the Microwave and Terahertz bands, owing to its wideband optical absorption property. The microwave saturable absorption in graphene demonstrates the possibility of graphene microwave and terahertz photonics devices, such as a microwave saturable absorber, modulator, polarizer, microwave signal processing and broad-band wireless access networks.\n\nUnder more intensive laser illumination, graphene could possess a nonlinear phase shift due to the optical nonlinear Kerr effect. Based on a typical open and close aperture z-scan measurement, graphene possesses a nonlinear Kerr coefficient of , almost nine orders of magnitude larger than that of bulk dielectrics. This suggests that graphene may be a powerful nonlinear Kerr medium, with the possibility of observing a variety of nonlinear effects, the most important of which is the soliton. A 2016 study has shown that the nonlinear refractive index of graphene is negative. Thermal lens spectroscopy can be used for measuring the thermo-optic coefficient of graphene and inspecting its thermal nonlinearity.\n\nSeveral techniques can prepare nanostructured graphene, e.g., graphene quantum dots (GQDs); these techniques mainly include electron beam lithography, chemical synthesis, electrochemical preparation, graphene oxide (GO) reduction, C60 catalytic transformation, the microwave assisted hydrothermal method (MAH), the Soft-Template method, the hydrothermal method, and the ultrasonic exfoliation method.\n\nThermal transport in graphene is an active area of research which has attracted attention because of the potential for thermal management applications. Early measurements of the thermal conductivity of suspended graphene reported an exceptionally large thermal conductivity of approximately , compared with the thermal conductivity of pyrolytic graphite of approximately at room temperature. However, later studies have questioned whether this ultrahigh value was overestimated, and instead measured thermal conductivities between – for suspended single layer graphene. The large range can be attributed to large measurement uncertainties as well as variations in the graphene quality and processing conditions. In addition, when single-layer graphene is supported on an amorphous material, the thermal conductivity is reduced to about – at room temperature as a result of scattering of graphene lattice waves by the substrate, and can be even lower for few layer graphene encased in amorphous oxide. Likewise, polymeric residue can contribute to a similar decrease for suspended graphene to approximately –for bilayer graphene.\n\nIt has been suggested that the isotopic composition, the ratio of C to C, has a significant impact on thermal conductivity. For example, isotopically pure C graphene has higher thermal conductivity than either a 50:50 isotope ratio or the naturally occurring 99:1 ratio. It can be shown by using the Wiedemann–Franz law, that the thermal conduction is phonon-dominated. However, for a gated graphene strip, an applied gate bias causing a Fermi energy shift much larger than \"k\"\"T\" can cause the electronic contribution to increase and dominate over the phonon contribution at low temperatures. The ballistic thermal conductance of graphene is isotropic.\n\nPotential for this high conductivity can be seen by considering graphite, a 3D version of graphene that has basal plane thermal conductivity of over (comparable to diamond). In graphite, the c-axis (out of plane) thermal conductivity is over a factor of ≈100 smaller due to the weak binding forces between basal planes as well as the larger lattice spacing. In addition, the ballistic thermal conductance of graphene gives the lower limit of the ballistic thermal conductances, per unit circumference and length of carbon nanotubes.\n\nDespite its 2-D nature, graphene has 3 acoustic phonon modes. The two in-plane modes (LA, TA) have a linear dispersion relation, while the out of plane mode (ZA) has a quadratic dispersion relation. Due to this, the \"T\" dependent thermal conductivity contribution of the linear modes is dominated at low temperatures by the T contribution of the out-of-plane mode. Some graphene phonon bands display negative Grüneisen parameters (GPs). At low temperatures (where most optical modes with positive GPs are still not excited) the contribution from the negative GPs will be dominant and thermal expansion coefficient (which is directly proportional to GPs) negative. The lowest negative GPs correspond to the lowest transverse acoustic ZA modes. Phonon frequencies for such modes increase with the in-plane lattice parameter since atoms in the layer upon stretching will be less free to move in the z direction. This is similar to the behavior of a stretched string that has vibrations of smaller amplitude and higher frequency. This phenomenon, named \"membrane effect\", was predicted by Lifshitz in 1952.\n\nA prediction that was published in 2015 suggested a melting point of ≈4125 K. Recent and more sophisticated modelling has increased this temperature to at least 5000 K. At 6000 K (the sun's surface having an effective temperature of 5,777 K) graphene melts into an agglomeration of loosely coupled doubled bonded chains, before becoming a gas.\n\nThe carbon–carbon bond length in graphene is about 0.142 nanometers. Graphene sheets stack to form graphite with an interplanar spacing of .\n\nGraphene is the strongest material ever tested, with an intrinsic tensile strength of 130.5 GPa and a Young's modulus of (). The Nobel announcement gave the strength as 42 N/m; the mass of 1 m as , and the electrical resistance of a square as 31 Ω.\n\nLarge-angle-bent graphene monolayer has been achieved with negligible strain, showing mechanical robustness of the two-dimensional carbon nanostructure. Even with extreme deformation, excellent carrier mobility in monolayer graphene can be preserved.\n\nThe spring constant of suspended graphene sheets has been measured using an atomic force microscope (AFM). Graphene sheets were suspended over cavities where an atomic force microscope (AFM) tip was used to apply a stress to the sheet to test its mechanical properties. Its spring constant was in the range 1–5 N/m and the stiffness was , which differs from that of bulk graphite. These intrinsic properties could lead to applications such as NEMS as pressure sensors and resonators. Due to its large surface energy and out of plane ductility, flat graphene sheets are unstable with respect to scrolling, i.e. bending into a cylindrical shape, which is its lower-energy state.\n\nAs is true of all materials, regions of graphene are subject to thermal and quantum fluctuations in relative displacement. Although the amplitude of these fluctuations is bounded in 3D structures (even in the limit of infinite size), the Mermin–Wagner theorem shows that the amplitude of long-wavelength fluctuations grows logarithmically with the scale of a 2D structure and would therefore be unbounded in structures of infinite size. Local deformation and elastic strain are negligibly affected by this long-range divergence in relative displacement. It is believed that a sufficiently large 2D structure, in the absence of applied lateral tension, will bend and crumple to form a fluctuating 3D structure. Ripples have been observed in suspended layers of graphene. It has been proposed that the ripples are caused by thermal fluctuations in the material. As a consequence of these dynamical deformations, it is debatable whether graphene is truly a 2D structure. In 2014 it was shown that these ripples, if amplified through the introduction of vacancy defects, can impart a negative Poisson's ratio into graphene, resulting in the thinnest auxetic material known.\n\nGraphene nanosheets can be incorporated into a nickel matrix through a plating process to form Ni-graphene composites on a target substrate. The enhancement in mechanical properties of the composites is attributed to the high interaction between Ni and graphene and the prevention of the dislocation sliding in the Ni matrix by the graphene.\n\nDespite its strength, graphene is also relatively brittle, with a fracture toughness of about 4 MPa. This indicates that imperfect graphene is likely to crack in a brittle manner like ceramic materials, as opposed to many metallic materials that have fracture toughnesses in the range of 15–50 MPa. Graphene shows a greater ability to distribute force from an impact than any known material, ten times that of steel per unit weight. The force was transmitted at .\n\nIn 2011 graphene was shown to accelerate the osteogenic differentiation of human mesenchymal stem cells without the use of biochemical inducers.\n\nIn 2015 graphene was used to create biosensors with epitaxial graphene on silicon carbide. The sensors bind to 8-hydroxydeoxyguanosine (8-OHdG) and is capable of selective binding with antibodies. The presence of 8-OHdG in blood, urine and saliva is commonly associated with DNA damage. Elevated levels of 8-OHdG have been linked to increased risk of several cancers. A commercial version of a graphene biosensor has been used as a protein binding sensor platform.\n\nIn 2016 uncoated graphene was shown to serve as a neuro-interface electrode without altering or damaging properties such as signal strength or formation of scar tissue. Graphene electrodes in the body stay significantly more stable than electrodes of tungsten or silicon because of properties such as flexibility, bio-compatibility and conductivity.\n\nA production unit produces continuous monolayer sheets of high-strength monolayer graphene (HSMG). The process is based on graphene growth on a liquid metal matrix.\n\nBilayer graphene displays the anomalous quantum Hall effect, a tunable band gap and potential for excitonic condensation. Bilayer graphene typically can be found either in twisted configurations where the two layers are rotated relative to each other or graphitic Bernal stacked configurations where half the atoms in one layer lie atop half the atoms in the other. Stacking order and orientation govern its optical and electronic properties. One synthesis method is chemical vapor deposition, which can produce large bilayer regions that almost exclusively conform to a Bernal stack geometry. Superconductivity has been observed in twisted bilayer graphene.\n\nGraphene nanoribbons (\"nanostripes\" in the \"zig-zag\" orientation), at low temperatures, show spin-polarized metallic edge currents, which suggest spintronics applications. (In the \"armchair\" orientation, the edges behave like semiconductors.)\n\nIn 2013, a three-dimensional honeycomb of hexagonally arranged carbon was termed 3D graphene. Self-supporting 3D graphene was produced that year. The discovered nanostructure is a multilayer system of parallel hollow nanochannels located along the surface that displayed quadrangular cross-section. Three dimensional bilayer graphene was reported in 2012 and 2014. In 2017, freestanding graphene gyroids with 35 nm and 60 nm unit cells were fabricated via controlled direct chemical vapor deposition. They represent the smallest free standing periodic graphene 3D structures yet produced with a pore size of tens of nm. A graphene gyroid has five percent of the density of steel, yet is ten times as strong with an enormous surface area to volume ratio. An aerogel made of graphene layers separated by carbon nanotubes was measured at 0.16 milligrams per cubic centimeter. The material has superior elasticity and absorption: it can recover completely after more than 90% compression, and absorb up to 900 times its weight in oil, at a rate of 68.8 grams per second.\n\nMultiple production techniques have been developed. Isolated 2D crystals cannot be grown via chemical synthesis beyond small sizes even in principle, because the rapid growth of phonon density with increasing lateral size forces 2D crystallites to bend into the third dimension. In all cases, graphene must bond to a substrate to retain its two-dimensional shape.\n\nAs of 2014, exfoliation produced graphene with the lowest number of defects and highest electron mobility. Geim and Novoselov initially used adhesive tape to pull graphene sheets away from graphite. Achieving single layers typically requires multiple exfoliation steps. After exfoliation the flakes are deposited on a silicon wafer. Alternatively a sharp single-crystal diamond wedge cleave layers from a graphite source. Another method is reduction of graphite oxide monolayer films, e.g. by hydrazine with annealing in argon/hydrogen with an almost intact carbon framework that allows efficient removal of functional groups. Measured charge carrier mobility exceeded /Vs. Defect-free, unoxidized graphene-containing liquids can be made from graphite using mixers that produce local shear rates greater than . Burning a graphite oxide coated DVD produced a conductive graphene film (1738 siemens per meter) and specific surface area (1520 square meters per gram) that was highly resistant and malleable.\n\nDispersing graphite in a liquid medium can produce graphene by sonication followed by centrifugation. The addition of dispersants (\"e.g.\", graphene quantum dots) can facilitate the exfoliation process, forming aqueous graphene dispersion with high yield. Such process is straightforward, low-cost, and environmentally friendly. Sonicating graphite at the interface of two immiscible liquids, most notably heptane and water, produced macro-scale graphene films. Graphite particles can be corroded in molten salts to form a variety of carbon nanostructures including graphene. Electrochemical synthesis can exfoliate graphene. Varying a pulsed voltage controls thickness, flake area, number of defects and affects its properties. The process begins by bathing the graphite in a solvent for intercalation. The process can be tracked by monitoring the solution’s transparency with an LED and photodiode.\n\nGraphene has been prepared by using a sugar (e.g. glucose, sugar, fructose, etc.) This substrate-free \"bottom-up\" synthesis is safer, simpler and more environmentally friendly than exfoliation. The method can control thickness, ranging from monolayer to multilayers, which is known as \"Tang-Lau Method\".\n\nEpitaxial graphene may be coupled to surfaces weakly enough (by Van der Waals forces) to retain the two dimensional electronic band structure of isolated graphene. Heating silicon carbide (SiC) to high temperatures () under low pressures (c. 10 torr) reduces it to graphene. A normal silicon wafer coated with a layer of germanium (Ge) dipped in dilute hydrofluoric acid strips the naturally forming germanium oxide groups, creating hydrogen-terminated germanium. CVD can coat that with graphene. A two-step CVD process is shown to grow graphene directly on TiO crystals or exfoliated TiO nanosheets without using a metal catalyst. The atomic structure of metal substrates including ruthenium, iridium, nickel and copper has been used as substrates for graphene production. Commercial copper foils have been used for graphene production, reducing substrate costs by 100-fold. Gaseous catalyst-assisted CVD paves the way for synthesizing high-quality graphene for device applications while avoiding the transfer process. Gram quantities were produced by the reduction of ethanol by sodium metal, followed by pyrolysis and washing with water. Growing graphene in an industrial resistive-heating cold wall CVD system was claimed to produce graphene 100 times faster than conventional CVD systems, cut costs by 99% and produce material with enhanced electronic qualities. Experiments with precise control of process parameters during cold wall CVD provided conclusive insight into the classical surface-mediated two dimensional nucleation and growth mechanism of graphene grown using catalytic CVD under conditions sought out in the semiconductor industry.\n\nIn applications where the thickness and packing density of the graphene layer need to be carefully controlled, the Langmuir-Blodgett method has been used to produce single layer films of graphene and graphene oxide which can then be reduced to graphene. Some of the benefits of Langmuir-Blodgett deposition include accurate control over the layered architecture of the graphene, that the layer-by-layer deposition process allows assembling any combination of thin carbon layers on a substrates, and that the assembly process is performed at room temperature and produces greater throughputs while being amenable to automation and mass production.\n\nGraphene is the only form of carbon (or solid material) in which every atom is available for chemical reaction from two sides (due to the 2D structure). Atoms at the edges of a graphene sheet have special chemical reactivity. Graphene has the highest ratio of edge atoms of any allotrope. Defects within a sheet increase its chemical reactivity. The onset temperature of reaction between the basal plane of single-layer graphene and oxygen gas is below . Graphene combusts at . Graphene is commonly modified with oxygen- and nitrogen-containing functional groups and analyzed by infrared spectroscopy and X-ray photoelectron spectroscopy. However, determination of structures of graphene with oxygen- and nitrogen- functional groups requires the structures to be well controlled.\n\nContrary to the ideal 2D structure of graphene, chemical applications of graphene need either structural or chemical irregularities, as perfectly flat graphene is chemically inert. In other words, the definition of an ideal graphene is different in chemistry and physics.\n\nGraphene placed on a soda-lime glass (SLG) substrate under ambient conditions exhibited spontaneous n-doping (1.33 × 10 \"e\"/cm) via surface-transfer. On p-type copper indium gallium diselenide (CIGS) semiconductor itself deposited on SLG n-doping reached 2.11 × 10 \"e\"/cm.\n\nVarious graphene derivatives, e.g., cyanographene and graphene acid, can be prepared via elegant chemistry of fluorographene. Cyanographene and graphene acid have high degree of functionalization (~15%), open band gap and are hydrophilic providing stable colloids in water.\n\nGraphene is a transparent and flexible conductor that holds promise for various material/device applications, including solar cells, light-emitting diodes (LED), touch panels and smart windows or phones. Graphene has also been used in other fundamental electronic devices, such as capacitors and Field Effect Transistors (FETs), in which it can act as an atomically thin channel. In the same framework, fluorine-doped graphene has shown to have insulating properties and it can be used as a passivation layer in graphene FETs, leading to a substantial increase in carrier mobility.\n\nOther early commercial uses of graphene include fillers such as a graphene-infused printer powder.\n\nGraphene supercapacitors serve as energy storage alternative to traditional electrolytic batteries. Among advantages are fast charging, long life span and environmentally friendly production. Graphene supercapacitors produced by Skeleton Technologies have been commercially available since around 2015 and were first used in some specialized applications instead of traditional batteries. By 2017, commercial graphene supercapacitor units were available for industrial power applications, with maximal power output of 1500 kW. In 2016, Adgero announced a regenerative braking system (KERS) for large trucks that employed a graphene-based supercapacitor.\n\nA joint effort from the University of Pennsylvania, Shanghai Institute of Ceramics, Queensland University of Technology, and Rice University has led to the development of carbon nanotube supercapacitors that can offer three times the energy density of most carbon-based nanomaterials. The scientists on this team utilized silicon and graphene to develop carbon nanotubes 4-6 nm wide and then doped the carbon atoms with nitrogen. \nThis process causes a chemical reaction which allows the material to store up to three times more energy without compromising their power density. Fuqiang Huang, a researcher on this project, reports that the fabricated carbon nanotube integrated supercapacitors are able to store up to 41 Wh/kg of energy; for comparison, an average lead acid battery has an energy density of 33 - 42 Wh/kg. Additionally, the supercapacitors offer a substantially higher power density of 70 - 250 Wh/kg compared to average lead acid battery which is capable of .2 - 1 Wh/kg. This breakthrough demonstrates that supercapacitors can provide a performance that matches, and even supersedes, of conventional energy storage methods. To put this achievement into perspective, a materials scientist from University of Pennsylvania, I-Wei Chen, stated that the implementation of these supercapacitors would allow an average bus to make 25 km trips with a recharge time of only 30 seconds. \n\nIn 2016, Henrik Fisker announced development of an electric car that will use graphene supercapacitor instead of lithium-ion batteries. Its low energy density as compared to lithium-ion batteries is being addressed. The planned electric car would target a minimum range of 400 miles (640 km). It has been announced later that the electric car produced by Fisker Inc. will still use lithium-ion batteries, but research in graphene supercapacitors will continue by Nanotech Energy Inc.\n\nBAC's 2016 Mono model is said to be made out of graphene as a first of both a street-legal track car and a production car.\nThe first company to use graphene-made structural parts on a production model was Spania GTA, which unveiled a version of its Spano supercar fitted with graphene in 2015.\n\nThe global market for graphene reached $9 million by 2012 with most sales in the semiconductor, electronics, battery, energy storage or conversion, and composites industries.\n\nThe toxicity of graphene has been extensively debated. A review on graphene toxicity summarized the \"in vitro\", \"in vivo\", antimicrobial and environmental effects and highlights the various mechanisms of graphene toxicity. Nanotubes of graphene could reproduce the effects of asbestosis. The toxicity of graphene depends on its shape, size, purity, post-production processing steps, oxidative state, functional groups, dispersion state, synthesis methods, route, dose of administration, and exposure times.\n\nGraphene nanoribbons, graphene nanoplatelets, and graphene nano–onions are non-toxic at concentrations up to 50 µg/ml. These nanoparticles do not alter the differentiation of human bone marrow stem cells towards osteoblasts (bone) or adipocytes (fat) suggesting that at low doses graphene nanoparticles are safe for biomedical applications. 10 µm few-layered graphene flakes were able to pierce cell membranes in solution. They were observed to enter initially via sharp and jagged points, allowing graphene to enter the cell. The physiological effects of this remain uncertain, and this remains a relatively unexplored field.\n\nThe theory of graphene was first explored by Wallace in 1947 as a starting point for understanding the electronic properties of 3D graphite. The emergent massless Dirac equation was first pointed out by Semenoff, DiVincenzo and Mele. The earliest TEM images of few-layer graphite were published by Ruess and Vogt in 1948. An early, detailed study on few-layer graphite dates to 1962 when Boehm and colleagues reported producing monolayer flakes of reduced graphene oxide. Efforts to make thin films of graphite by mechanical exfoliation started in 1990, but nothing thinner than 50 to 100 layers was produced before 2004. Initial attempts to make atomically thin graphitic films employed exfoliation techniques similar to the drawing method. Multilayer samples down to 10 nm in thickness were obtained.\n\nOne of the first patents pertaining to the production of graphene was filed in October 2002 and granted in 2006. Two years later, in 2004 Geim and Novoselov extracted single-atom-thick crystallites from bulk graphite and transferred them onto thin silicon dioxide () on a silicon wafer, which electrically isolated the graphene. The cleavage technique led directly to the first observation of the anomalous quantum Hall effect in graphene, which provided direct evidence of graphene's theoretically predicted Berry's phase of massless Dirac fermions. The effect was reported by Geim's group and by Kim and Zhang, whose papers appeared in \"Nature\" in 2005. Geim and Novoselov received awards for their pioneering research on graphene, notably the 2010 Nobel Prize in Physics.\n\nIn 2013, the European Commission funded the large-scale research project Graphene Flagship with a total budget of €1 billion, involving 150 partner organizations.\n\nCommercialization of graphene proceeded rapidly once commercial scale production was demonstrated. By 2017, 13 years after creation of the first laboratory graphene electronic device, an integrated graphene electronics chip was produced commercially and marketed to pharmaceutical researchers by Nanomedical Diagnostics in San Diego.\n\n"}
{"id": "6840750", "url": "https://en.wikipedia.org/wiki?curid=6840750", "title": "Grave (unit)", "text": "Grave (unit)\n\nThe grave was the original name of the kilogram, in an early version of the metric system between 1793 and 1795. \n\nThe modern kilogram has its origins in the Age of Enlightenment and the French Revolution. In 1790 an influential proposal by Talleyrand called for a new system of units, including a unit of length derived from an invariable length in nature, and a unit of mass (then called \"weight\") equal to the mass of a unit volume of water. In 1791, the Commission of Weights and Measures, appointed by the French Academy of Sciences, chose one ten-millionth of the quarter meridian as the unit of length, and named it \"metre\". Initially a \"provisional\" value was used, based on the old meridian measurement by Lacaille (1740).\n\nIn 1793 the commission defined the unit of mass as a cubic decimetre of distilled water at 0 °C, and gave it the name \"grave\". Two supplemental unit names, gravet (0.001 grave), and bar (1000 grave), were added to cover the same range as the old units, resulting in the following decimal series of units: milligravet, centigravet, decigravet, gravet, centigrave, decigrave, grave, centibar, decibar, bar. The mass of a unit volume of water at 0 °C was accurately determined by Lavoisier and Haüy (18841 grains per cubic provisional decimetre). A prototype of the grave was made in brass.\n\nIn 1795 a new law replaced the three names gravet, grave and bar by a single generic unit name: the gram. The new gram was equal to the old gravet. Four new prefixes (deca, hecto, kilo, and myria) were added to the metric system to cover almost the same range of units as in 1793 (milligram, centigram, decigram, gram, decagram, hectogram, kilogram, myriagram). The brass prototype of the grave was renamed to provisional kilogram.\n\nIn 1799 the provisional units were replaced by the final ones. Delambre and Méchain had completed their new measurement of the meridian, and the final metre was 0.03% smaller than the provisional one. Hence the final kilogram, being the mass of one cubic decimetre of water, was 0.09% lighter than the provisional one. In addition, the temperature specification of the water was changed from 0 °C to 4 °C, the point where the density of water is maximal. This change of temperature added 0.01% to the final kilogram. In 1799 a platinum cylinder was made that served as the prototype of the final kilogram. It was called the \"Kilogramme des Archives\" as it was stored in the Archives Nationales in Paris. This standard stood for the next ninety years.\n"}
{"id": "40036842", "url": "https://en.wikipedia.org/wiki?curid=40036842", "title": "Hart Energy", "text": "Hart Energy\n\nHart Energy is an energy industry publisher based in Houston. It produces magazines, online information services, and industry newsletters, and provides upstream, midstream and downstream energy research and downstream consulting services.\n\nHart Energy (formerly Hart Publications, Inc.) was founded in Denver in 1973. The company changed its name to Hart Energy Publishing, LP in March 2004. It has operations in Houston, New York City, Washington D.C., Brussels, Dubai, and Singapore.\n\nOn October, 2010, Hart Energy acquired Rextag Strategies Mapping & Data Services.\n\nOn May 2, 2013, Hart Energy acquired Subsea Engineering News.\n\n\n• Rich A. Eichler, Chief Executive Officer\n\n• Kevin F. Higgins, President and Chief Operating Officer\n\n• John E. Paisie, Executive Vice President for Consulting and Research\n\n"}
{"id": "24329699", "url": "https://en.wikipedia.org/wiki?curid=24329699", "title": "Horsebus", "text": "Horsebus\n\nA horse-bus or horse-drawn omnibus was a large, enclosed and sprung horse-drawn vehicle used for passenger transport before the introduction of motor vehicles. It was mainly used in the late 19th century in both the United States and Europe, and was one of the most common means of transportation in cities. In a typical arrangement, two wooden benches along the sides of the passenger cabin held several sitting passengers facing each other. The driver sat on a separate, front-facing bench, typically in an elevated position outside the passengers' enclosed cabin. In the main age of horse buses, many of them were double-decker buses. On the upper deck, which was uncovered, the longitudinal benches were arranged back to back.\nSimilar, if smaller, vehicles were often maintained at country houses (and by some hotels and railway companies) to convey servants and luggage to and from the railway station. Especially popular around 1870-1900, these vehicles were known as a 'private omnibuses' or 'station buses'; coachman-driven, they would usually accommodate four to six passengers inside, with room for luggage (and sometimes additional seating) on the roof.\n\nA small open wagon with or without a top, but with an arrangement of the seats similar to horse-drawn omnibuses, was called a wagonette.\n\n\"Bus\" is a clipped form of the Latin word \"omnibus\". A legend promoted by the French Transportations Museum website says the name is derived from a hatter's shop of the Omnes family in front of the first station opened in Nantes by Stanislas Baudry in 1823. \"Omnes Omnibus\" was a pun on the Latin-sounding name of that hatter Omnès: \"omnes\" (nominative plural) meaning \"all\" and \"omnibus\" (dative plural) meaning \"for all\" in Latin. Thence, the legend concludes, Nantes citizens gave the nickname of Omnibus to the vehicle.\n\nThough it is undisputed that the term arose with Stanislas Baudry's company, there is however no record of any Omnès hatter living in that street. In 1892, the son of Baudry's bookkepeer wrote in the \"Bulletin de la Société archéologique de Nantes\" that \"omnibus\" had a simpler origin. Baudry used to call his horsecars \"Dames blanches\" (White ladies), a name which, critics told him, made no sense. He then replied, with the Latin word: \"Then, these are \"omnibus\" cars!\" (cars for all). The name caught up immediately. Other stories about the name origin quickly spread out.\n\nThe term 'omnibus' carried over to motor vehicles. The 1914 book \"Motor Body-building in all its Branches\", by Christopher William Terry, described an omnibus as having longitudinal seats in rows with either a rear door or side doors.\n\nThe first known public bus line (known as a \"Carriage\" at that time) was launched by Blaise Pascal in 1662 and was quite popular until fares were increased and access to the service was restricted to high society members by regulation. Services ceased after 15 years.\nThe Paris omnibus was started in 1828 by a businessman named Stanislas Baudry, who had begun the first French omnibus line in Nantes in 1826, using two spring-suspended carriages, each for 16 passengers. Following success in Nantes, Baudry moved to Paris and founded the Enterprise des Omnibus on rue de Lancre, with workshops on the quai de Jemmapes. In 1827 he commissioned an English coach-maker, George Shillibeer, to design a vehicle that could be stable and carry a large number of passengers. Shillibeer's design worked. On 28 April 1828, the first Paris omnibus began service, running every fifteen minutes between La Madeleine and la Bastille. Before long, there were one hundred omnibuses in service, with eighteen different itineraries. A journey cost twenty-five centimes. The omnibuses circulated between seven in the morning and seven in the evening; each omnibus could carry between twelve and eighteen passengers. The busiest line was that along the Grand Boulevards; it ran from eight in the morning until midnight. \n\nThe Paris omnibus service was an immediate popular success, with more than two and a half million passengers in the first six months. However, there was no reliable way to collect money from the passengers, or the fare collectors kept much of the money for themselves; In its first years the company was continually on the verge of bankruptcy, and in despair, Baudry committed suicide in February 1830. Baudry's partners reorganized the company and managed to keep it in business.\n\nIn September 1828, a competing company, les Dames-Blanches, had started running its own vehicles. In 1829 and the following years, more companies with poetic names entered the business; les Citadines, les Tricycles, les Orléanises, les Diligentes, les Écossaises, les Béarnaises, les Carolines, les Batignollaises, les Parisiennes, les Hirondelles, les Joséphines, les Excellentes, les Sylphides, les Constantines, les Dames-Françaises, les Algériennes, les Dames-Réunies, and les Gazelles. The omnibus had a profound effect on Parisian life, making it possible for Parisians to work and have a social life outside their own neighborhoods.\n\nBy 1845, there were thirteen companies in Paris operating twenty-twenty three omnibus lines. In 1855, Napoleon III had them combined into a single company, the Compagnie générale des omnibus, with a monopoly on Paris public transportation. Beginning in 1873, they were gradually replaced by tramways, and, beginning in 1906, by the \"omnibus automobile\", or motor bus. The last horse-drawn Paris omnibus ran on 11 January 1913, from Saint-Sulpice to La Villette.\n\nIn Britain, John Greenwood opened the first bus line in Britain in Manchester in 1824. His pioneering idea was to offer a service where, unlike with a stagecoach, no prior booking was necessary and the driver would pick up or set down passengers anywhere on request.\nBuses have been used on the streets of London since 1829. George Shillibeer saw the success of the Paris omnibus in service and concluded that operating similar vehicles in London. His first London \"Omnibus\", using the same design and name as the Paris vehicle, took up service on 4 July 1829 on the route between Paddington (The Yorkshire Stingo) and \"Bank\" (Bank of England) via the \"New Road\" (now Marylebone Rd), Somers Town and City Road. Four services were provided in each direction daily. Shillibeer's success prompted many competitors to enter the market, and for a time buses were referred to as 'Shillibeers'. Shillibeer built another bus for the Quaker Newington Academy for Girls near London; this had a total of 25 seats, and entered history as the first school bus.\n\nIn 1850 Thomas Tilling started horse bus services, and in 1855 the London General Omnibus Company or LGOC was founded to amalgamate and regulate the horse-drawn omnibus services then operating in London.\nThe public transport system of Berlin is the oldest one in Germany. In 1825 the first bus line from Brandenburger Tor to Charlottenburg was opened by Simon Kremser, running to a timetable. The first bus service inside the city operated from 1840 between Alexanderplatz and Potsdamer Bahnhof. It was run by Israel Moses Henoch, who had organized the cab service since 1815.\nOn January 1, 1847, the \"Concessionierte Berliner Omnibus Compagnie\" (Concessed Berlin Bus Company) started its first horse-bus line. The growing market experienced the launch of numerous additional companies, with 36 bus companies in Berlin by 1864.\n\nFrom the end of the 1820s, the first horse-drawn omnibuses ran in the streets of New York City.\n\nHorses pulling buses could only work for limited hours per day, had to be housed, groomed, fed and cared for every day, and produced large amounts of manure, which the omnibus company had to store and dispose of. Since a typical horse pulled a bus for four or five hours per day, covering about a dozen miles, many systems needed ten or more horses in stable for each bus.\n\nWith the advent of mass-produced steel (at around 1860), horse-buses were put on rails as the same horse could then move 3 to 10 times as many people. This was not only more efficient, but faster and produced, in an age of unpaved streets, a far superior ride.\n\nThese horse-cars on rails were converted to cable-drawn cars in larger cities, as still exist in San Francisco, the underground cable being drawn by stationary steam engines.\n\n(Not to be overlooked was the establishment in both London and New York, of steam hauled urban railways, either underground or on elevated structures. These metropolitan railways are where the present term \"Metro\" began. These systems provided \"rapid transit\" on their routes).\n\nAt around 1890, electric propulsion became practical and replaced both the horse and the cable and the number of traction lines on rails expanded exponentially. (This was seen as a huge advance in urban transport and considered a wise investment at that time). These became known as Streetcars, Trams, Trolleys and still exist in many cities today, though often having been replaced by the less infrastructure intensive motorbus as driven by an internal combustion engine.\nFrom the beginning of the twentieth century the remaining horse buses which had not been converted to rail began to be replaced by petrol-driven motor buses, or autobuses. The last recorded horse omnibus in London was a Tilling bus which last ran, between Peckham and Honor Oak Tavern, on 4 August 1914. The last Berlin horse omnibus ran on 25 August 1923. Some horse buses remain in use today for tourist sightseeing tours.\n\n\n\n"}
{"id": "17179597", "url": "https://en.wikipedia.org/wiki?curid=17179597", "title": "Human-powered hydrofoil", "text": "Human-powered hydrofoil\n\nA human-powered hydrofoil is a small hydrofoil watercraft propelled entirely by the muscle power of its operator(s). Hydrofoils are the fastest water-based vehicles propelled solely by human power. They can reach speeds of up to , easily exceeding the world records set by competitive rowing which stand at about . This speed advantage is achieved since hydrofoils lack a submerged body to provide buoyancy, greatly reducing the drag force.\n\nMeans of propulsion include screw propellers, as in hydrocycles; aircraft propellers, as in the Decavitator; paddles, as in a Flyak; oars, as in the Yale hydrofoil sculling project; and flapping wings, as detailed below.\n\nFlapping wing propulsion devices are hydrofoils that generate propulsion by forcing a foil to move up and down in the water. The forward motion of the foil then generates lift as in other hydrofoils. A common design consists of a large foil at the stern that is used both for propulsion and keeping the passenger above the water, connected to a smaller foil at the bow used for steering and longitudinal stability. Riders operate the vehicle by bouncing up and down on a small platform at the stern, whilst holding onto a steering column. It is started and landed from the shore, or preferably from a dock, and requires a bit of experience. When moving too slowly, it will sink, and the range of possible speeds is .\n\nSeveral variations on the design have been developed:\n\n"}
{"id": "739312", "url": "https://en.wikipedia.org/wiki?curid=739312", "title": "Jesse H. Jones", "text": "Jesse H. Jones\n\nJesse Holman Jones (April 5, 1874June 1, 1956) was a Democratic politician and entrepreneur from Houston, Texas. He served as United States Secretary of Commerce from 1940 to 1945.\n\nJones managed a Tennessee tobacco factory at age fourteen, and at nineteen, he was put in charge of his uncle's lumberyards. Five years later, after his uncle, M. T. Jones, died, Jones moved to Houston to manage his uncle's estate and opened a lumberyard company, which grew quickly. During this period, Jesse opened his own business, the South Texas Lumber Company. He also began to expand into real estate, commercial building, and banking. His commercial building activities in Houston included mid-rise and skyscraper office buildings, hotels and apartments, and movie theaters. He constructed the Foster Building, home to the \"Houston Chronicle\", in exchange for a fifty percent share in the newspaper, which he acquired control of in 1926.\n\nJones's participation in civic life and politics began with the Port of Houston and the Houston Ship Channel. He led a group of local bankers in buying public finance bonds and was later appointed to serve as the Chair of the Houston Harbor Board. He led a local fundraising effort on behalf of the American Red Cross in support of servicemen in World War I. President Wilson tapped Jones to head a division of American Red Cross, a duty he fulfilled between 1917 and 1919. In 1928, he initiated and organized Houston's bid for the 1928 Democratic National Convention.\n\nHis most important role was on the board of the Reconstruction Finance Corporation (RFC), (1932–1939), a federal agency originally created in the Herbert Hoover administration that played a major role in combating the Great Depression and financing industrial expansion during World War II. President Roosevelt elevated Jones to the Chairmanship of the RFC in 1933. Jones was in charge of spending US$50 billion, especially in financing railways and building munitions factories. He served as the United States Secretary of Commerce from 19401945, a post he held concurrently with his chairmanship of the RFC. After leaving Washington, Jesse and Mary Jones focused on philanthropy, working through the Houston Endowment, a non-profit corporation they founded in 1937.\n\nJesse H. Jones descended from Welsh ancestors who made Virginia their first landing place in North America, sometime in the 1650s. After settling there briefly, they relocated to the Chowan River in North Carolina, remaining there for at least a century. In 1774, Eli Jones and one of his brothers, headed west, eventually deciding to an area now known as Robertson County, Tennessee. William, one of Eli's sons, established himself as a farmer there, and married a neighboring farmer's daughter, Laura Anna Holman. The farm was sufficient to provide for all of the needs of the family and grow tobacco for sale, partly from their own efforts, and partly from the work of enslaved persons. Jesse Holman Jones was born to William and Laura Jones on April 5, 1874, the fourth of five children. Jesse's mother died on April 22, 1880, just after he had turned six. Nancy Jones Hurt, his aunt, moved in with the family along with her two sons. She was a \"guide, physician, and clothes-maker of all the Jones children,\" and \"a famous cook. ... \" Sudley Place was his childhood home.\n\nIn 1883, the Jones family, including Aunt Nancy and seven children moved to Dallas, Texas, partly in order for William to join his brother Martin Tilton \"M.T.\" Jones in his successful lumber enterprise. Several years earlier M.T. had resettled his family in Terrell, Texas after a stop in Illinois. Aunt Nancy remained in Dallas and enrolled the children in local public schools, while William moved to Terrell to manage the M.T. Jones Lumber Company and look after the firm's other lumberyards in northeast Texas. This allowed M.T. to move closer to his timberlands and other interests in southeast Texas. However, William only stayed for two years and returned with his large family to Robertson County, where he acquired a new farm to work. So Jesse was back in Tennessee at the age of twelve.\n\nJesse had been a diligent worker as a boy, caring for the farm animals, and performing many common household chores. During the summers when his family had lived in Dallaswhen he was a young teenagerhe hacked out weeds, picked cotton, and herded cattle. He did not display the same diligence for school, and later, Jesse recalled many scoldings and punishments from his teachers. His father challenged his two sons with a tobacco plot for each of them. He allowed each son to three acres and provided them both with supplies. Each of them would be allowed to keep any profits after they repaid their store accounts. After the eighth grade, he quit school and applied some of his experience working with tobacco. William Jones not only grew tobacco, but also traded the crop, and he also joined a partnership, Jones, Holman and Armstrong, which processed tobacco. Then, out of school at the age of fourteen, Jesse was in the job market, and William put him in charge of one of the tobacco factories. Jesse was responsible for receiving (or sometimes rejecting), classifying, warehousing, and shipping tobacco. In addition, his name was on the company bank account, and he signed checks for the company's operations.\n\nAt the age of seventeen, Jesse's family returned to Dallas. After several attempts to find a suitable job in Dallas and the surrounding region, Jesse started working in Hillsboro, Texas, at one of his uncle's lumberyards. He performed manual labor, but also served the office side of the business, such as bookkeeping and debt collection. Despite these varied duties, he earned the standard salary for a salesman: $40 per month. He requested a fifty percent raise, arguing that he worked day and night. His uncle refused. Jesse quit not long before the death of his father, William Jones. The will instructed that trustees manage the tobacco enterprise, while Jesse would assume control at age twenty-one. He also inherited about $2,000 in stock.\n\nJesse and his brother had liquidated the tobacco inventory from their father's estate, and spent the proceeds on their sisters' homes. Jesse returned to Dallas and applied for a position with the M.T. Jones Lumber Company's yard on Main Street and St. Paul. However, he had experienced problems with the manager of the yard at Hillsboro, and that manager then worked at the big yard in downtown Dallas. His uncle M.T. refused to hire him, leading Jesse to wonder what the manager might have said about him. However, C.T. Harris, the general manager of the company decided to investigate the matter. Harris was familiar with Jesse's work, and had even on occasion, entrusted him to keep the books. Harris audited the books of the Hillsboro yard and concluded that the Hillsboro manager had committed fraud. He fired the manager and hired Jesse as bookkeeper for the big Dallas yard. Initially, Jesse earned a salary of $15 per weekmore than he made at the Hillsboro yard. After just six months, Harris made Jesse the manager there, raising his salary to $100 per month (). Harris made these decisions without consulting M.T., the owner of the company. Jesse ran the Dallas yard profitably, even in the face of eight competitors in the local market. In 1895, with M.T. still critical of the Dallas operations, Jesse tendered his resignation. However, M.T. audited the books of the Dallas yard and found them to be in good order. M.T. asked Jesse to retract his resignation. Jesse replied that he would take his old job back for $150 per week and six percent of the profits. M.T. agreed to Jesse's terms.\n\nWhile Jesse was still managing a lumber yard in Dallas for M.T. Jones, he decided on a financial gambit while competing for the lumber trade related to the 1897 Texas State Fair. The association running the State Fair needed construction supplies for buildings and exhibits, but the lumber companies wanted personal guarantees from the directors. Jesse, sensing an opportunity, decided to stand out from his competitors: he extended credit to the State Fair Association, with only the backing of gate receipts. When M.T. found out about the terms of the loans and the full extent of Jesse's gamble, he began to investigate Jesse's activities and interrogated him about his decision. These loans were repaid quickly and the Dallas lumber yard profited from the play.\n\nDespite these confrontation between M.T. and Jesse, by 1898, it was apparent that Jesse had earned his uncle's trust. M.T. died that year and his will named Jesse as general manager of his substantial lumber business. The will also designated Jesse as one of five executors of his estate. He arrived in Houston in 1898, renting a room at the old Rice Hotel for $2.25 per night (). He was then responsible for the business affairs of his Aunt Louisa and his three cousins. Jesse managed a large estate:\n\nHe was now in charge of tens of thousands of acres of timberland spread over three east Texas counties and parts of Louisiana. The estate owned and operated sawmills and factories in Orange that had the daily capacity to turn hundreds of thousands of feet of raw timber into shingles, doors, windows sashes, and two-by-fours. The logistics was equally huge: felled trees had to moved to plants, and finished products had to be delivered to lumberyards located throughout the state and beyond. With assistance and advice from trustees, Jones bought, sold, and managed the land, expanding the M.T. Jones Lumber Company even further.\nIn 1902, Jones started the South Texas Lumber Company. He had money he had earned from selling investments in timber and some Spindletop deals for capital. He acquired the Reynolds Lumber Company, as well as many other lumberyards in New Mexico, Oklahoma, and Texas. The company charter announced an intention to purchase raw goods (lumber), semi-finished goods (cross ties), and milled goods, such as blinds, doors, and sash. According to his own recollection, he made about $1 million in profits when he sold controlling interest in the company, liquidating most of his interests in one saw mill and perhaps 20 or more lumberyards. Other than retaining a single lumberyard, he permanently left active management of the timber and lumber business in 1911 or 1912.\n\nJones commenced a flurry of building activity in 1906. He contracted to build an addition to the Bristol Hotel, committing $90,000 () to the project, which would include a rooftop garden and dance floor. He also commissioned a ten-story building for the Texas Company (Texaco), and the company moved its headquarters to Houston. The same year, he constructed a new plant for the rapidly growing \"Houston Chronicle\" in exchange for a half-interest in the company, which had been solely owned by Marcellus Foster. In 1911, Jones purchased the original five-story Rice Hotel from Rice University although the university retained the land on which it stood. Working with James A. Baker, the president of Rice Institute's Board of Trustee's, he razed the original structures and constructed the seventeen-story building, which he then leased from Rice. The new hotel leased 500 rooms, and was the center of Houston social life.\n\nAfter concluding his service with the Red Cross, Jones returned to Houston and resumed his business activities. He amassed lots along the Main Street corridor in downtown Houston, acquired a tract on Elm Street in Dallas, and also invested in Fort Worth. In 1921, he expanding one downtown Houston structure into the Bankers Mortgage Building, while laying out plans for two more ten-story buildings. During this time he continued a collaboration with local architect Alfred C. Finn, whom he had first worked with on the Rice Hotel. Jones juggled his Houston program with a development initiative in New York City, and he built the Melba Theater in Dallas.\n\nIn the mid-1920s, Jones increased his construction and development activity. Two new buildings, the Kirby Theater and the Kirby Lumber Company Building went up on Main Street, while he built additions to the Rice Hotel and the Houston Electric Building. During the same period he started projects in Manhattan. The first was an apartment building on Fifth Avenue at 97th Street, followed by the Mayfair House (New York City) on Park Avenue at 67th Street. A third building at 200 Madison Avenue faced J.P. Morgan's home, with four floors leased to the first Marshall Field's store in New York City. Jones also left his mark on Fort Worth, building the Medical Arts Building (Fort Worth), and the Worth Hotel and Worth Theater.\n\nIn addition to his real estate and political activity associated with Houston's Democratic National Convention, Jones continued multiple development projects in 1928 in other cities. He commissioned an eighteen-story, mixed-use building in downtown Fort Worth, leasing the storefront and two more floors to Fair Department Store. He cited a sixteen-story medical office building on 61st street as just one of his projects in New York. Back in Houston, several projects were under construction with no connection to the convention. Jones broke ground on the Gulf Building that year, while completing the Levy Brothers Department Store. The Gulf Building was completed the next year as the tallest structure in Houston, a distinction it held until 1963. He finished another retail building on Main Street, a four-story store for Krupp and Tuffly Shoes. He acquired his fourth hotel, a distressed sixteen-story building which he re-branded as the Texas State Hotel. His largest project to date, however, Jones built in New York: a 44-story office tower on East 40th Street. This he completed in the spring prior to the Stock Market Crash of 1929.\n\nAs a young man, Jones found opportunities to borrow money in order to establish credit. He borrowed in excess of his need, and kept the extra cash in a savings account. However, at least two Houston bankers expressed concerns about his borrowing practices. By his own estimate, he had borrowed as much as $3 million (). The test came with the Panic of 1907. One of the largest and oldest of Houston's banks, the T.W House Bank, failed amidst this economic recession. The bank had a $500,000 () loan on its books in the name of Jesse Jones. Yet even during the bank panic, Jones was able to sell enough mortgage paper and draw on lines of credit to repay the loan. So he stood ready to make new investments after the worst of the recession ended.\n\nSometime after 1908, Jones organized the Texas Trust Company. By 1912, he had become president of Houston's National Bank of Commerce. This bank later merged with Texas National Bank in 1964 to become the Texas National Bank of Commerce, renamed to Texas Commerce Bank which grew into a major regional financial institution. It became part of JP Morgan Chase & Co. in 2008.\n\nIn 1931 two local banks were in danger of failing. Public National Bank faced a clientele demanding cash and Houston National Bank had too many distressed loans. Public National Bank had barely enough cash on hand to last through Saturday, October 24. The next day, Jones hosted at meeting of local bankers at his office in the new Gulf Building. He urged his banking colleagues to assist in stabilizing the two distressed banks to prevent a general panic among local depositors. Jones proposed a bailout plan of $1.25 million () to guarantee local deposits at risk, with the political support of a major local bank investor, James A. Baker. Despite a faction of bankers who wanted to let the two banks fail, Jones and Baker prevailed, with Jones buying out Public National Bank, Joseph Meyers Interests buying out Houston National Bank, and a consortium of banks and utility companies all contributing to the bailout fund. Customers of Public National Bank gained access to their accounts on October 26.\n\nJones acquired his fifty-percent interest in the \"Houston Chronicle\" from Marcellus Elliot Foster in August 1906. Though Foster was the paper's editor, Jones's engagement in the paper's positions was evident by the letters between the two men. For example, Jones supported Foster's public opposition to the Ku Klux Klan, which had been a growing movement in Texas after World War I. Foster stressed his editorial independence, while Jones vowed that he was willing to risk financial loss and personal safety to side against the KKK. This relationship became strained in 1925 when Jones voiced his opposition to Foster's support of Miriam Ferguson for governor of Texas. They were in agreement with her strong stance against the Klan, but Jones refused to support her candidacy because of the corruption of her husband during his tenure as governor.\n\nIn 1926, Jones became the sole owner of the \"Houston Chronicle\" and named himself as publisher. At the time of the purchase, the paper had a daily readership of 75,000 and the company was valued at $2.5 million (). When Jones opened the Gulf Building in Houston, his ownership of the \"Houston Chronicle\" facilitated publication of a 48-page special insert dedicated to his new skyscraper. In March 1930, Jones acquired a radio station and began broadcasting in Houston from the Rice Hotel. The call letters of the station, KTRH, used three letters as an acronym for The Rice Hotel. KTRH broadcast some content of the Columbia Broadcasting System, making it the second radio station in Houston to air national programs. Jones established the station to support the \"Houston Chronicle\", which had already seen the \"Houston Post\" establish a radio affiliate, KPRC.\n\nJones helped to secure funding for the Houston Ship Channel. When bond sales for the Harris County Houston Ship Channel District lagged, he met with Houston bankers and extracted a pledge from each one to buy the district's bonds proportionate to their market capitalizations. He was appointed as chair of the new Houston Harbor Board in 1913 by Mayor Ben Campbell. Jones accepted this post after rejecting several offers from the Woodrow Wilson Administration the same year. Edward Mandell House, who advocated Wilson's nomination for the Democratic Party the previous year, suggested Jones for service to the new US President. The Wilson Administration offered positions to Jones such as the Undersecretary of the Treasury, two ambassadorships, and most notably, Secretary of Commerce. Jones confronted Mayor Campbell and other interests in regard to a wharf in Manchester, at that time outside of the City of Houston. Campbell advocated spending $300,000 of Houston Harbor District revenue to construct a wharf for a local cement manufacturer. Jones opposed this expenditure, and resigned from the board with other directors when the city approved the project.\n\nFrom 1917 until the end of World War II, Jones dedicated his activities to the nation, spending more time in the federal capital than in his home town. He responded to World War I demands by leading a fundraising effort in Houston for the American Red Cross. Sixteen of his friends accepted his challenge to donate $5,000 each (), spurring the local effort to meet and exceed its fundraising quota. President Wilson asked Jones to become director general of military relief for the American Red Cross during World War I, a position he held until 1919. During his first post in Washington, D.C., his department was responsible for seven hundred Red Cross canteens and 55,000 volunteers, organization and transportation of mobile hospitals to England and France, and distribution of clothing to persons in war-torn Europe, and tendering financial assistance to families of American servicemen.\n\nJones worked in an office building facing the White House, and eventually he had personal access to the President. During the coordination of Red Cross parades in various American cities, he asked that the President make a speech on the day of the parade in New York City to support fundraising efforts. Wilson was reticent and had not made an oral public address since his declaration of war against Germany. Jones, per Wilson's request, appointed Cleveland Dodge as the presiding officer of the event, though Jones also directed Dodge to choose a venue suitable for a presidential address. On the day of the parade, President Wilson made an impromptu speech to a full Metropolitan Opera House which included his justification for war against Germany, lauded the work of the American Red Cross, admonished Wall Street bankers against wartime profiteering, and offered an entreaty to Americans to donate money to the Red Cross.\n\nOn his own initiative, Jones tendered a $200,000 bid () to bring the 1928 Democratic National Convention to Houston. Other cities matched or exceeded this amount, but Jones vowed that Houston would beat the others in hospitality. When Jones returned to Texas from Washington, D.C., where he had been negotiating, local greeters mobbed the train depots in Marshall, Texas and Conroe, with a few brandishing \"Jesse Jones for President\" signs. At Union Station, 50,000 Houstonians staged a homecoming for Jones, replete with marching bands, bunting, and banners. They staged a parade from Union Station to the Jones home at the Lamar Hotel. This hero's welcome preceded the decision by the Democratic Convention to select a site, though Walter Lippman and the \"New York Evening Post\" predicted that Houston would be chosen.\n\n \nAfter President Hoover signed the Reconstruction Finance Corporation (RFC) bill in 1932, the Republican chose Jones as one of the three to serve on its first board of directors. When Hoover sought advice from ranking Democrats about candidates for the board, Jones was the sole recommendation of House Speaker John Nance Garner. The Hoover RFC was an ambitious program. Upon opening, the RFC had 300 staff positions available. Soon it conveyed hundreds of millions in loans, including $300 million () to the railroads, $90 million to prop up the Chicago bank of Charles Dawes, and $65 million to Bank of America. However, Hoover sold the RFC as a program to assist smaller institutions. Bank of America retired its loan with the RFC, paying interest and principal within two years. Other loans were not successful. Jones opposed a loan to the Missouri Pacific, concerned that that the taxpayers would be stuck with their bill. Without Jones's support, the RFC board approved $23 million for the railroad, but it did not prevent the company from failing the next year.\n\nIn 1933, President Franklin Delano Roosevelt made him the Chairman of the RFC, while also expanding the RFC's powers to make loans and bail out banks. This led some to refer to Jones as \"the fourth branch of government.\" The next year, Congress issued an additional $850 in loans, after which President Roosevelt intimated to Jones that he would have the authority to invest the new appropriations and reinvest revenue from loan repayments.\n\nJones criticized Hoover's execution of the RFC as too little and too late. Congress and the new president, Franklin D. Roosevelt, created a new Emergency Banking Act on March 9, 1933. President Roosevelt announced a \"bank holiday,\" a moratorium on banking activity while federal bank inspectors examined the books in order to determine which financial institutions were viable. After the bank holiday, all financially sound banks would resume business. For persons who were unable to access their accounts, another part of the act authorized the executive branch to reorganize failed banks in order to free up frozen assets. The RFC was empowered to invest financial institution through their preferred stocks. Seventy percent of America's banks reopened after just six days. Jones's task as the new chair of the RFC was to reopen another 2,000 banks. He began with the reorganization of two of Detroit's largest banks by collaborating with Alfred P. Sloan of General Motors. They formed a new bank with matching investments from the RFC and General Motors, but more significantly, the RFC covered the deposits of the 800,000 frozen accounts from both failed banks with a loan of $230 million ().\n\nIn the first three years of the Roosevelt Administration, the RFC had issued $8 billion in loans; however, these outflows were offset by $3.5 billion in revenues, including interest payments and repayments of principal.\n\nIn 1939, Roosevelt appointed Jones to be the new Federal Loan Administrator while taking away his title as RFC chair.\n\nRoosevelt reportedly called Jones \"Jesus H. Jones.\"\n\nPresident Woodrow Wilson offered Jones the position of United States Secretary of Commerce, but Jones decided instead to remain in Houston and focus on his businesses. He accepted the same position from President Franklin D. Roosevelt in 1940, and he served until 1945. However, according to Stephen Fenberg, Roosevelt offered him the cabinet position to bring him closer to the White House and reign in his power. This tactic did not work because Jones accepted the new post while retaining his old job as Federal Loan Administrator.\n\nHenry Wallace was dropped from the ticket as Vice President in 1944. Jones was on the short list to replace him. However Roosevelt wanted social progress to continue and Jones's reputation was far too conservative for that role. Roosevelt was reelected and asked Jones to resign as Secretary of Commerce, which he did on January 21, 1945. The next day he resigned from RFC and all other government positions. Jones released the two letters to several newspapers, including \"The New York Times\". The letters criticized Roosevelt's decision to name Wallace as Secretary of Commerce. Senator Josiah Bailey of North Carolina called both Jones and Wallace to testify before the Senate Commerce Committee, each on consecutive days. Jones testified on the first day that he did not believe that Wallace was a suitable candidate. He characterized Wallace as a visionary who lacked business experience. Sometime during the five hours of testimony the next day, Wallace touted his own business experience, but sought to restrict the scope of power from the Commerce Department and the Reconstruction Finance Corporation, which he claimed were exploited by business interests.\n\nJones was associated with a group of Houston political and social leaders known as the \"Suite 8F Group\", named for the apartment number at the Lamar Hotel maintained by George and Herman Brown. Jones owned the hotel and resided in the building's penthouse, upstairs from the Brown's suite. The principal members of this group were James Abercrombie, the Brown brothers, Judge James Elkins, Oveta Culp Hobby, William P. Hobby, Robert E. Smith, and Gus Wortham. Historian Joseph Pratt characterized Jones as \"the godfather\" of the group. The Suite 8F Group began their activities after World War II.\n\nPrior to 1937, Jesse and Mary Jones and given about $1 million to charitable causes. In 1937, they established the Houston Endowment to organize their philanthropic endeavors. Their Commerce Company was already established as a conglomeration of most of the family business interests. They granted about a third of the company's shares to the Houston Endowment, while appointing Milton Backlund, Fred Heyne, and W. W. Moore as the first trustees. During the first seven years, Houston Endowment focused its donations on education.\n\nFrom 1932, Jones had not cashed any paychecks he earned through his various federal government positions through 1945. In 1946, he signed them all over to the Houston Endowment. At the same time Jones and his wife worked through the Houston Endowment to give this money away, much of it with a focus on education. Through the Houston Endowment, they made a $300,000 grant to the University of Virginia in honor of Woodrow Wilson. They established scholarship funds for the Texas State College for Women, Prairie View A & M University, the University of Tennessee, and Texas A & M University. Later they created an engineering scholarship at Massachusetts Institute of Technology honoring Bill Knudsen and an economics scholarship at Austin College honoring Jesse's brother, John. More ten-year scholarship programs funded students attending Rice University and Texas A & M, and several of the individual recipients were veterans of World War II. Another program supported nursing candidates at the University of Houston. They also made large gifts to the American Red Cross, the Houston Community Chest, the Houston Museum of Fine Arts, and the United Jewish Appeal.\n\nIn 1946, Jones joined the Board of Trustees of the Texas Medical Center.\n\nIn 1956, the Jesse Holman Jones Hospital was built in Springfield, Tennessee to replace the original hospital there.\n\nJones made a gift of $1 million to Rice University to establish Jones College, which opened in 1957. The name for the all-women's dormitories honored Mary Gibbs Jones.\n\nIn 1925, Jones received an honorary Doctor of Law degree from Southwestern University, and another from Oglethorpe University in 1941.\n\nIn 1939, the Alabama-Coushatta tribe named Jones \"Chief Cue-ya-la-na\" when they accepted him into their community. The name translates as \"Yellow Pine,\" symbolic of the tallest being within their local environment and a being which serves all members of their community.\n\nJones resided at the Rice Hotel in Houston, but he also stayed at \"the Boarding House,\" the home of his aunt, Louisa Jones. Her house was located at the corner of Anita and Main Street, south of downtown Houston. Jones managed the estate of his uncle, M. T. Jones, and continued to act as a business manager for his aunt and his cousins for many years. Much of his social life revolved around them, too. His future wife, Mary Gibbs Jones, was first married to his cousin, Will Jones.\n\nJones married Mary Gibbs on December 15, 1920. They resided at the Rice Hotel until 1926 when they moved into their penthouse at the new Lamar Hotel. Alfred C. Finn designed and supervised the construction of the building, but Jones hired John Staub to design the interior for their apartment. Audrey Jones, one of Mary's granddaughters, also lived with them.\n\nJones retained the title of publisher of the \"Houston Chronicle\" until his death on June 1, 1956, at the age of 82. His remains were interred in Houston's Forest Park Cemetery.\n\nThe name of Jesse H. Jones is memorialized throughout Houston through many grants from the Houston Endowment. The home of the Houston Symphony is Jesse H. Jones Hall in the Houston Theater District.\n\nTexas Southern University founded the Jesse H. Jones School of Business in 1955. After Jones's death the Houston Endowment made donations to Rice University. They established the Jesse H. Jones Chair of Management, and in the 1970s, they granted $10 million to start the Jesse H. Jones Graduate School of Management. The Jesse H. Jones Student Life Center, a recreation facility at the University of Houston–Downtown, was fully funded by the Houston Endowment. Baylor University's central libraries includes the Jesse H. Jones Library. The Jesse H. Jones Physical Education Complex on the campus of Texas Lutheran University in Seguin bears his name.\n\nJones contributed money to the Houston Academy of Medicine/Texas Medical Center for a new home for its library. This building is known as the Jesse H. Jones Library Building. The Jesse H. and Mary Gibbs Jones Pavilion (1977) connects Memorial Hermann Hospital to the University of Texas Medical School.\n\nOther Jones buildings include the Houston Public Library's Central Library building and the Great Jones Building, the former home of Texaco and briefly the location of Jones's office. Beyond buildings, one may visit the Jesse H. Jones Park and Nature Center in Humble.\n\n\n\n"}
{"id": "133877", "url": "https://en.wikipedia.org/wiki?curid=133877", "title": "Jigsaw puzzle", "text": "Jigsaw puzzle\n\nA jigsaw puzzle is a tiling puzzle that requires the assembly of often oddly shaped interlocking and tessellating pieces.\nEach piece usually has a small part of a picture on it; when complete, a jigsaw puzzle produces a complete picture. In some cases more advanced types have appeared on the market, such as spherical jigsaws and puzzles showing optical illusions.\n\nJigsaw puzzles were originally created by painting a picture on a flat, rectangular piece of wood, and then cutting that picture into small pieces with a jigsaw, hence the name. John Spilsbury, a London cartographer and engraver, is credited with commercializing jigsaw puzzles around 1760. Jigsaw puzzles have since come to be made primarily of cardboard.\n\nTypical images found on jigsaw puzzles include scenes from nature, buildings, and repetitive designs. Castles and mountains are two traditional subjects. However, any kind of picture can be used to make a jigsaw puzzle; some companies offer to turn personal photographs into puzzles. Completed puzzles can also be attached to a backing with adhesive to be used as artwork.\n\nDuring recent years, a range of jigsaw puzzle accessories including boards, cases, frames and roll-up mats has become available that are designed to assist jigsaw puzzle enthusiasts.\n\nThe engraver and cartographer John Spilsbury, of London, is believed to have produced the first jigsaw puzzle around 1760, using a marquetry saw. Early jigsaws, known as dissections, were produced by mounting maps on sheets of hardwood and cutting along national boundaries, creating a puzzle useful for the teaching of geography. Such \"dissected maps\" were used to teach the children of King George III and Queen Charlotte by royal governess Lady Charlotte Finch.\n\nThe name \"jigsaw\" came to be associated with the puzzle around 1880 when fretsaws became the tool of choice for cutting the shapes. Since fretsaws are distinct from jigsaws, the name appears to be a misnomer. Cardboard jigsaw puzzles appeared during the late 1800s, but were slow to replace the wooden jigsaw due to the manufacturer's belief that cardboard puzzles would be perceived as being of low quality, and the fact that profit margins on wooden jigsaws were larger.\n\nJigsaw puzzles soared in popularity during the Great Depression, as they provided a cheap, long-lasting, recyclable form of entertainment. It was around this time that jigsaws evolved to become more complex and more appealing to adults. They were also given away in product promotions, and used in advertising, with customers completing an image of the product being promoted.\n\nSales of wooden jigsaw puzzles fell after World War II as improved wages led to price increases, while at the same time improvements in manufacturing processes made cardboard jigsaws more attractive.\n\nAccording to the Alzheimer Society of Canada, doing jigsaw puzzles is one of many activities that can help keep the brain active and may contribute to reducing the risk of developing Alzheimer's disease.\n\nMost modern jigsaw puzzles are made out of paperboard since they are easier and cheaper to mass-produce than the original wooden models. An enlarged photograph or printed reproduction of a painting or other two-dimensional artwork is glued onto the cardboard before cutting. This board is then fed into a press. The press forces a set of hardened steel blades of the desired shape through the board until it is fully cut. This procedure is similar to making shaped cookies with a cookie cutter. The forces involved, however, are tremendously greater and a typical 1000-piece puzzle requires a press that can generate upwards of 700 tons of force to push the knives of the puzzle die through the board. A puzzle die is a flat board, often made from plywood, which has slots cut or burned in the same shape as the knives that are used. These knives are set into the slots and covered in a compressible material, typically foam rubber, which serves to eject the cut puzzle pieces.\n\nBeginning in the 1930s, jigsaw puzzles were cut using large hydraulic presses which now cost in the hundreds of thousands of dollars. The cuts gave a very snug fit, but the cost limited jigsaw puzzle manufacture only to large corporations. Recent roller press design achieve the same effect, at a lower cost. By the early 1960s, Tower Press was the world's largest maker of jigsaw puzzles, acquired by Waddingtons in 1969.\n\nNew technology has enabled laser-cutting of wooden or acrylic jigsaw puzzles. The advantage of cutting with a laser is that the puzzle can be custom cut into any size, any shape, with any size (or any number) of pieces. Many museums have laser cut acrylic puzzles made of some of their more important pieces of art so that children visiting the museum can see the original piece and then assemble a jigsaw puzzle of the image that is also in the same shape as the piece of art. Acrylic is used because the pieces are very durable, waterproof, and can withstand continued use without the image fading, or the pieces wearing out, or becoming frayed. Also, because the print and cut patterns are computer-based, lost pieces can be manufactured without remaking the entire puzzle.\n\nJigsaw puzzles come in a variety of sizes. Smaller puzzles are often considered to be those of 300, 500, and 750 pieces. More sophisticated, but still common, jigsaw puzzles come in sizes of 1,000, 1,500, 2,000, 3,000, 4,000, 5,000, 6,000, 7,500, 8,000, 9,000, 13,200, 18,000, 24,000, 32,000 and 40,000 pieces.\n\nThere are also smaller jigsaw puzzles that are geared towards children, and are rated by the number of pieces they contain. For very young children, a puzzle with as few as 4 to 9 'large' size pieces (so not a choking hazard) are common. These are usually made of wood or plastic, to maintain durability, and are able to be cleaned without being damaged.\n\nThe most common layout for a thousand-piece puzzle is 38 pieces by 27 pieces, for a total count of 1,026 pieces. The majority of 500-piece puzzles are 27 pieces by 19 pieces. A few puzzles are made double-sided, so that they can be solved from either side. This adds a level of complexity, because it cannot be certain that the correct side of the piece is being viewed and assembled with the other pieces.\n\n\"Family puzzles\" come in 100–550 pieces with three different sized pieces from large to small. The pieces are placed from large to small going in one direction or towards the middle of the puzzle. This allows a family of puzzlers of different skill levels and different-sized hands to work on the puzzle at the same time. Companies like Springbok, Cobble Hill, Ravensburger, and Suns Out make this type of specialty puzzle.\n\nThere are also three-dimensional jigsaw puzzles. Many of these are made of wood or styrofoam and require the puzzle to be solved in a certain order; some pieces will not fit in if others are already in place.\nAlso common are \"puzzle boxes:\" simple three-dimensional jigsaw puzzles with a small drawer or box in the center for storage.\n\nAnother type of jigsaw puzzle, which is considered a 3-D puzzle, is a puzzle globe. Like a 2-D puzzle, a globe puzzle is often made of plastic and the assembled pieces form a single layer. But the final form is a three-dimensional shape. Most globe puzzles have designs representing spherical shapes such as the Earth, the Moon, and historical globes of the Earth.\n\nThere are also computer versions of jigsaw puzzles, which have the advantages of requiring zero cleanup as well as no risk of losing any pieces. Many computer-based jigsaw puzzles do not allow pieces to be rotated, so all pieces are displayed in their correct orientation. These puzzles are thus considerably easier than a physical jigsaw puzzle with the same number of pieces. A computer puzzle website can allow users to choose their own puzzle size, cut design, and image, or upload their own images to use as puzzles. An online jigsaw version of Trolleholm Castle in Sweden may be worked and timed for speed of finishing. The New Yorker Magazine subscription website preserves images of the magazine's cover illustrations as jigsaw puzzles which are timed and offer several levels of difficulty.\n\nIn 2016 was introduced a computer version of puzzle globe, the immersive panorama jigsaw, which is based on the use of equirectangular images taken by 360-degree camera. Despite the physical spherical jigsaw, the player, who resides in the perfect center of the globe, assembles triangular shaped interlocking pieces around him. When complete, this puzzle produces a full-degree panorama all around the player. An example of immersive jigsaw is Sitespot, which also enriches the gaming experience with the scene soundscape and allows pieces to be displayed rotated.\n\nJigsaw puzzles can vary greatly in price depending on the complexity, number of pieces, and brand. Children's puzzles can be as cheap as around $5.00 while larger puzzles can be closer to $50.00. The most expensive puzzle to date was sold for $27,000 in 2005 at a charitable auction for The Golden Retriever Foundation.\n\nSeveral word puzzle games use pieces similar to those used in jigsaw puzzles. Examples include Alfa-Lek, Jigsaw Words, Nab-It!, Puzzlage, Typ-Dom, Word Jigsaw, and Yottsugo.\n\nMany puzzles are termed \"fully interlocking\". This means that adjacent pieces are connected in such a way that if one piece is moved horizontally, the other pieces move with it, preserving the connection. Sometimes the connection is tight enough to pick up a solved part by holding one piece.\n\nSome fully interlocking puzzles have pieces all of a similar shape, with rounded tabs out on opposite ends, with corresponding blanks cut into the intervening sides to receive the tabs of adjacent pieces. Other fully interlocking puzzles may have tabs and blanks variously arranged on each piece, but they usually have four sides, and the numbers of tabs and blanks thus add up to four. The uniform-shaped fully interlocking puzzles, sometimes called \"Japanese Style\", are the most difficult, because the differences in shapes between pieces can be very subtle. \n\nMost jigsaw puzzles are square, rectangular, or round, with edge pieces that have one side that is either straight or smoothly curved to create this shape, plus four corner pieces if the puzzle is square or rectangular. Some jigsaw puzzles have edge pieces that are cut just like all the rest of the interlocking pieces, with no smooth edge, to make them more challenging. Other puzzles are designed so the shape of the whole puzzle forms a figure, such as an animal. The edge pieces may vary more in these cases.\n\nThe pieces of spherical jigsaw, like immersive panorama jigsaw, can be triangular shaped, according to the rules of tessellation of the geoid primitive.\n\nThe world's largest commercially available jigsaw puzzle (Nov. 2018) is produced by Czech company MartinPuzzle and contains 52,110 pieces showing a collage of animals.\n\nIn 2016 the German company Ravensburger released their biggest puzzle. It shows 10 scenes from Disney works and has 40,320 pieces, measuring 680 cm by 192 cm when assembled.\n\nThe world's largest-sized jigsaw puzzle measured with 21,600 pieces, each measuring a Guinness World Records maximum size of 50 cm by 50 cm. It was assembled on 3 November 2002 by 777 people at the former Kai Tak Airport in Hong Kong.\n\nThe jigsaw with the greatest number of pieces had 551,232 pieces and measured 14.85 × 23.20 m (48 ft 8.64 in × 76 ft 1.38 in). It was assembled on 24 September 2011 at Phú Thọ Indoor Stadium in Ho Chi Minh City, Vietnam, by students from the University of Economics, Ho Chi Minh City.\n\nThe logo of Wikipedia is a globe made out of jigsaw pieces. The incomplete sphere appears to have some pieces missing, symbolizing the room to add new knowledge.\n\nIn the logo of the Colombian Office of the Attorney General appears a jigsaw puzzle piece in foreground. They named it as \"The Key Piece\": \"The jigsaw puzzle piece is the appropiate symbol for visual representation of the Office, since it includes the concepts of searching, solution and response that the institution pursuits through its investigation activity.\"\n\nThe central antagonist in the \"Saw\" film franchise is named Jigsaw.\n\nIn the 1933 Laurel and Hardy short \"Me and My Pal\", several characters attempt to complete a large jigsaw puzzle.\n\nJigsaw puzzle pieces were first used as a symbol for autism in 1963 by the United Kingdom's National Autistic Society. The organization chose jigsaw pieces for their logo to represent the \"puzzling\" nature of autism and the inability to \"fit in\" due to social differences, and also because jigsaw pieces were recognizable and otherwise unused. Puzzle pieces have since been incorporated into the logos and promotional materials of many organizations, including the Autism Society of America and Autism Speaks.\n\nProponents of the autism rights movement oppose the jigsaw puzzle iconography, stating that metaphors such as \"puzzling\" and \"incomplete\" are harmful to autistic people. Critics of the puzzle piece symbol instead advocate for a rainbow-colored infinity symbol representing diversity. In 2017, the Autism journal concluded that the use of the jigsaw puzzle evoked negative public perception towards autistic individuals. In February 2018, the journal removed the puzzle piece from their cover.\n\n\n"}
{"id": "7950685", "url": "https://en.wikipedia.org/wiki?curid=7950685", "title": "List of Lepidoptera that feed on ragweeds", "text": "List of Lepidoptera that feed on ragweeds\n\nRagweeds (\"Ambrosia\" species) are used as food plants by the caterpillars (larvae) of some Lepidoptera species including:\n\n\n\n"}
{"id": "54295335", "url": "https://en.wikipedia.org/wiki?curid=54295335", "title": "Magnadur", "text": "Magnadur\n\nMagnadur is a sintered barium ferrite, specifically BaFeO in an anisotropic form. It is used for making permanent magnets. The material was invented by Mullard and was used initially particularly for focussing rings on cathode ray tubes. Magnadur magnets retain their magnetism well, and are often used in education. Magnadur can also be used in DC motors.\n\n"}
{"id": "6526747", "url": "https://en.wikipedia.org/wiki?curid=6526747", "title": "Mesic habitat", "text": "Mesic habitat\n\nIn ecology, a mesic habitat is a type of habitat with a moderate or well-balanced supply of moisture, e.g., a mesic forest, a temperate hardwood forest, or dry-mesic prairie. Mesic habitats transition to xeric shrublands in a non-linear fashion, which is evidence of a threshold. Mesic is one of a triad of terms used to describe the amount of water in a habitat. The others are xeric and hydric.\n\nFurther examples of mesic habitats include all of the earth. These habitats effectively provide drought insurance as land at higher elevations warms due to seasonal or other change.\n\nHealthy mesic habitats act like sponges in that they store water in such a way that it can be deposited to neighboring habitats as needed. They are common in dryer regions of the western United States, and can be a good water source to neighboring desert habitats. Healthy mesic habitats also provide forb and insects for organisms belonging to higher trophic levels, such as grouse. \n\nMesic habitats are under stress from various human activities such as ranching, however many conservation efforts are underway. As of 2010, over 1,474 ranchers have agreed to partner with the Sage Grouse Initiative under the U.S. Department of Agriculture in order to protect over 5.6 million acres of mesic habitat.\n\n"}
{"id": "48851801", "url": "https://en.wikipedia.org/wiki?curid=48851801", "title": "Microplane model for constitutive laws of materials", "text": "Microplane model for constitutive laws of materials\n\nThe microplane model, conceived in 1984, is a material constitutive model for progressive softening damage. Its advantage over the classical tensorial constitutive models is that it can capture the oriented nature of damage such as tensile cracking, slip, friction, and compression splitting, as well as the orientation of fiber reinforcement. Another advantage is that the anisotropy of materials such as gas shale or fiber composites can be effectively represented. To prevent unstable strain localization (and spurious mesh sensitivity in finite element computations), this model must be used in combination with some nonlocal continuum formulation (e.g., the crack band model). Prior to 2000, these advantages were outweighed by greater computational demands of the material subroutine, but thanks to huge increase of computer power, the microplane model is now routinely used in computer programs, even with tens of millions of finite elements.\n\nThe basic idea of the microplane model is to express the constitutive law not in terms of tensors, but in terms of the vectors of stress and strain acting on planes of various orientations called the microplanes. The use of vectors was inspired by G. I. Taylor's idea in 1938 which led to Taylor models for plasticity of polycrystalline metals. But the microplane models differ conceptually in two ways.\n\nFirstly, to prevent model instability in post-peak softening damage, the kinematic constraint must be used instead of the static one. Thus, the strain (rather than stress) vector on each microplane is the projection of the macroscopic strain tensor, i.e.,\nwhere formula_2 and formula_3 are the normal vector and two strain vectors corresponding to each microplane, and formula_4 and formula_5 where formula_6 and formula_7 are three mutually orthogonal vectors, one normal and two tangential, characterizing each particular microplane (subscripts formula_8 refer to Cartesian coordinates).\n\nSecondly, a variational principle (or the principle of virtual work) relates the stress vector components on the microplanes (formula_9 and formula_10) to the macro-continuum stress tensor formula_11, to ensure equilibrium. This yields for the stress tensor the expression:\n\nwith\n\nHere formula_14 is the surface of a unit hemisphere, and the sum is an approximation of the integral. The weights, formula_15, are based on an optimal Gaussian integration formula for a spherical surface. At least 21 microplanes are needed for acceptable accuracy but 37 are distinctly more accurate.\n\nThe inelastic or damage behavior is characterized by subjecting the microplane stresses formula_9 and formula_10 to strain-dependent strength limits called stress-strain boundaries imposed on each microplane. They are of four types, viz.:\nEach step of explicit analysis begins with an elastic predictor and, if the boundary has been exceeded, the stress vector component on the microplane is then dropped at constant strain to the boundary.\n\nThe microplane constitutive model for damage in concrete evolved since 1984 through a series of progressively improved models labeled M0, M1, M2, ..., M7. It was also extended to fiber composites (woven or braided laminates), rock, jointed rock mass, clay, sand, foam and metal. The microplane model has been shown to allow close fits of the concrete test data for uniaxial, biaxial and triaxial loadings with post-peak softening, compression-tension load cycles, opening and mixed mode fractures, tension-shear and compression-shear failures, axial compression followed by torsion (i.e., the vertex effect) and fatigue. The loading rate effect and long-term aging creep of concrete have also been incorporated. Models M4 and M7 have been generalized to finite strain. The microplane model has been introduced into various commercial programs (ATENA, OOFEM, DIANA, SBETA...) and large proprietary wavecodes (EPIC, PRONTO, MARS...). Alternatively, it is often being used as the user's subroutine such as UMAT or VUMAT in ABAQUS.\n"}
{"id": "26710042", "url": "https://en.wikipedia.org/wiki?curid=26710042", "title": "Night of the Grizzlies", "text": "Night of the Grizzlies\n\nNight of the Grizzlies (1969) is a book by Jack Olsen which details events surrounding the night of August 13, 1967, when two young women were separately attacked in Glacier National Park, Montana, by grizzly bears. Both women, Julie Helgeson, 19, of Albert Lea, Minnesota, and Michele Koons, 19, of San Diego, California, died of their injuries. Olsen's book examines the most plausible explanation of the unlikely dual attacks since no fatal grizzly attack had ever been recorded in the park's 57-year history prior to that night. One specialist at the time calculated the odds were greater than 1 in a million for a single attack but the odds of two separate attacks in a 4 hour time span were beyond measure. However future events would show grizzly attacks to become more common, as Olson explains, because of increased human presence in wilderness areas and decreased habitat for bears to live in, reaching a critical tipping point in the summer of 1967.\n\nThe text was originally published in 1969 as a three-part article for \"Sports Illustrated\". A 37 page prologue was added for the book.\n\n"}
{"id": "13460960", "url": "https://en.wikipedia.org/wiki?curid=13460960", "title": "North West Region Waste Management Group", "text": "North West Region Waste Management Group\n\nThe North West Region Waste Management Group (NWRWMG) is the collection of local authorities in the northwest of Northern Ireland responsible for municipal waste management services, including recycling. The local authorities include:\n\n\n"}
{"id": "21784", "url": "https://en.wikipedia.org/wiki?curid=21784", "title": "Nova", "text": "Nova\n\nA nova (plural novae or novas) or classical nova (CN, plural CNe) is a transient astronomical event that causes the sudden appearance of a bright, apparently \"new\" star, that slowly fades over several weeks or many months. Novae involve an interaction between two stars that cause the flareup that is perceived as a new entity that is much brighter than the stars involved.\n\nCauses of the dramatic appearance of a nova vary, depending on the circumstances of the two progenitor stars. All observed novae involve closely located binary stars (the progenitors), either a pair of red dwarfs in the process of merging, or a white dwarf and another star.\n\nThe main sub-classes of novae are classical novae, recurrent novae (RNe), and dwarf novae. They are all considered to be cataclysmic variable stars. Luminous red novae share the name and are also cataclysmic variables, but are a different type of event caused by a stellar merger. Also with similar names are the much more energetic supernovae (SNe) and kilonovae.\n\nClassical nova eruptions are the most common type of nova. They are likely created in a close binary star system consisting of a white dwarf and either a main sequence, sub-giant, or red giant star. When the orbital period falls in the range of several days to one day, the white dwarf is close enough to its companion star to start drawing accreted matter onto the surface of the white dwarf, which creates a dense but shallow atmosphere. This atmosphere is mostly hydrogen and is thermally heated by the hot white dwarf, which eventually reaches a critical temperature causing rapid runaway ignition by fusion.\n\nFrom the dramatic and sudden energies created, the now hydrogen-burnt atmosphere is then dramatically expelled into interstellar space, and its brightened envelope is seen as the visible light created from the nova event, and previously was mistaken as a \"new\" star. A few novae produce short-lived nova remnants, lasting for perhaps several centuries. Recurrent nova processes are the same as the classical nova, except that the fusion ignition may be repetitive because the companion star can again feed the dense atmosphere of the white dwarf.\n\nNovae most often occur in the sky along the path of the Milky Way, especially near the observed galactic centre in Sagittarius; however, they can appear anywhere in the sky. They occur far more frequently than galactic supernovae, averaging about ten per year. Most are found telescopically, perhaps only one every year to eighteen months reaching naked-eye visibility. Novae reaching first or second magnitude occur only several times per century. The last bright nova was V1369 Centauri reaching 3.3 magnitude on 14 December 2013.\n\nDuring the sixteenth century, astronomer Tycho Brahe observed the supernova SN 1572 in the constellation Cassiopeia. He described it in his book \"De nova stella\" (Latin for \"concerning the new star\"), giving rise to the adoption of the name \"nova\". In this work he argued that a nearby object should be seen to move relative to the fixed stars, and that the nova had to be very far away. Although this event was a supernova and not a nova, the terms were considered interchangeable until the 1930s. After this, novae were classified as \"classical novae\" to distinguish them from supernovae, as their causes and energies were thought to be different, based solely in the observational evidence.\n\nIronically, despite the term \"stellar nova\" meaning \"new star\", novae most often take place as a result of white dwarfs: remnants of extremely \"old\" stars.\n\nEvolution of potential novae begins with two main sequence stars in a binary system. One of the two evolves into a red giant, leaving its remnant white dwarf core in orbit with the remaining star. The second star—which may be either a main sequence star or an aging giant—begins to shed its envelope onto its white dwarf companion when it overflows its Roche lobe. As a result, the white dwarf steadily captures matter from the companion's outer atmosphere in an accretion disk, and in turn, the accreted matter falls into the atmosphere. As the white dwarf consists of degenerate matter, the accreted hydrogen does not inflate, but its temperature increases. Runaway fusion occurs when the temperature of this atmospheric layer reaches ~20 million K, initiating nuclear burning, via the CNO cycle.\nHydrogen fusion may occur in a stable manner on the surface of the white dwarf for a narrow range of accretion rates, giving rise to a super soft X-ray source, but for most binary system parameters, the hydrogen burning is unstable thermally and rapidly converts a large amount of the hydrogen into other, heavier chemical elements in a runaway reaction, liberating an enormous amount of energy. This blows the remaining gases away from the surface of the white dwarf surface and produces an extremely bright outburst of light.\n\nThe rise to peak brightness may be very rapid, or gradual. This is related to the speed class of the nova; yet after the peak, the brightness declines steadily. The time taken for a nova to decay by around 2 or 3 magnitudes from maximum optical brightness is used for classification, via its speed class. Fast novae typically will take fewer than 25 days to decay by 2 magnitudes, while slow novae will take more than 80 days.\n\nIn spite of their violence, usually the amount of material ejected in novae is only about of a solar mass, quite small relative to the mass of the white dwarf. Furthermore, only five percent of the accreted mass is fused during the power outburst. Nonetheless, this is enough energy to accelerate nova ejecta to velocities as high as several thousand kilometers per second—higher for fast novae than slow ones—with a concurrent rise in luminosity from a few times solar to 50,000–100,000 times solar. In 2010 scientists using NASA’s Fermi Gamma-ray Space Telescope discovered that a nova also can emit gamma-rays (>100 MeV).\n\nPotentially, a white dwarf can generate multiple novae over time as additional hydrogen continues to accrete onto its surface from its companion star. An example is RS Ophiuchi, which is known to have flared six times (in 1898, 1933, 1958, 1967, 1985, and 2006). Eventually, the white dwarf could explode as a Type Ia supernova if it approaches the Chandrasekhar limit.\n\nOccasionally, novae are bright enough and close enough to Earth to be conspicuous to the unaided eye. The brightest recent example was Nova Cygni 1975. This nova appeared on 29 August 1975, in the constellation Cygnus about five degrees north of Deneb, and reached magnitude 2.0 (nearly as bright as Deneb). The most recent were V1280 Scorpii, which reached magnitude 3.7 on 17 February 2007, and Nova Delphini 2013. Nova Centauri 2013 was discovered 2 December 2013 and so far, is the brightest nova of this millennium, reaching magnitude 3.3.\n\nA helium nova (undergoing a helium flash) is a proposed category of nova events that lacks hydrogen lines in its spectrum. This may be caused by the explosion of a helium shell on a white dwarf. The theory was first proposed in 1989, and the first candidate helium nova to be observed was V445 Puppis in 2000. Since then, four other novae have been proposed as helium novae.\n\nAstronomers estimate that the Milky Way experiences roughly 30 to 60 novae per year, but a recent examination has found the likely improved rate of about 50±27. The number of novae discovered in the Milky Way each year is much lower, about 10, probably due to distant novae being obscured by gas and dust absorption. Roughly 25 novae brighter than about the twentieth magnitude are discovered in the Andromeda Galaxy each year and smaller numbers are seen in other nearby galaxies.\n\nSpectroscopic observation of nova ejecta nebulae has shown that they are enriched in elements such as helium, carbon, nitrogen, oxygen, neon, and magnesium. The contribution of novae to the interstellar medium is not great; novae supply only as much material to the Galaxy as do supernovae, and only as much as red giant and supergiant stars.\n\nRecurrent novae such as RS Ophiuchi (those with periods on the order of decades), are rare. Astronomers theorize, however, that most, if not all, novae are recurrent, albeit on time scales ranging from 1,000 to 100,000 years. The recurrence interval for a nova is less dependent on the accretion rate of the white dwarf, than on its mass; with their powerful gravity, massive white dwarfs require less accretion to fuel an outburst than lower-mass ones. Consequently, the interval is shorter for high-mass white dwarfs.\n\nNovae are classified according to the light curve development speed, so in \n\nNovae have some promise for use as standard candle measurements of distances. For instance, the distribution of their absolute magnitude is bimodal, with a main peak at magnitude −8.8, and a lesser one at −7.5. Novae also have roughly the same absolute magnitude 15 days after their peak (−5.5). Comparisons of nova-based distance estimates to various nearby galaxies and galaxy clusters with those measured with Cepheid variable stars, have shown them to be of comparable accuracy.\n\nMore than 53 novae have been registered since 1890.\n\nRecurrent novae (RNe) are objects that have been seen to experience multiple nova eruptions. As of 2009, there are ten known galactic recurrent novae. The recurrent nova typically brightens by about 8.6 magnitude, whereas a classic nova brightens by more than 12 magnitude. The ten known recurrent novae are listed below.\n\nNovae are relatively common in the Andromeda galaxy (M31). Approximately several dozen novae (brighter than about apparent magnitude 20) are discovered in M31 each year. The Central Bureau for Astronomical Telegrams (CBAT) tracks novae in M31, M33, and M81.\n\n\n"}
{"id": "34997147", "url": "https://en.wikipedia.org/wiki?curid=34997147", "title": "Omega-7 fatty acid", "text": "Omega-7 fatty acid\n\nOmega-7 fatty acids are a class of unsaturated fatty acids in which the site of unsaturation is seven carbon atoms from the end of the carbon chain. The two most common omega-7 fatty acids in nature are palmitoleic acid and vaccenic acid.\n\nRich sources include macadamia nut oil and sea buckthorn oil in the form of palmitoleic acid, while dairy products are the primary sources of vaccenic acid and rumenic acid. A lesser but useful source of palmitoleic acid is avocado fruit (25,000ppm).\n\nThe monounsaturated omega-7 fatty acids have the general chemical structure CH-(CH)-CH=CH-(CH)-COH.\n\n"}
{"id": "13241768", "url": "https://en.wikipedia.org/wiki?curid=13241768", "title": "Outage management system", "text": "Outage management system\n\nAn outage management system (OMS) is a computer system used by operators of electric distribution systems to assist in restoration of power.\n\nMajor functions usually found in an OMS include:\n\nAt the core of a modern outage management system is a detailed network model of the distribution system. The utility's geographic information system (GIS) is usually the source of this network model. By combining the locations of outage calls from customers, a rules engine is used to predict the locations of outages. For instance, since the distribution system is primarily tree-like or radial in design, all calls in particular area downstream of a fuse could be inferred to be caused by a single fuse or circuit breaker upstream of the calls.\n\nThe outage calls are usually taken by call takers in a call center utilizing a customer information system (CIS). Another common way for outage calls to enter into the CIS (and thus the OMS) is by integration with an interactive voice response (IVR) system. The CIS is also the source for all the customer records which are linked to the network model. Customers are typically linked to the transformer serving their residence or business. It is important that every customer be linked to a device in the model so that accurate statistics are derived on each outage. Customers not linked to a device in the model are referred to as \"fuzzies\".\n\nMore advanced automatic meter reading (AMR) systems can provide outage detection and restoration capability and thus serve as virtual calls indicating customers who are without power. However, unique characteristics of AMR systems such as the additional system loading and the potential for false positives requires that additional rules and filter logic must be added to the OMS to support this integration.\n\nOutage management systems are also commonly integrated with SCADA systems which can automatically report the operation of monitored circuit breakers and other intelligent devices such as SCADA reclosers.\n\nAnother system that is commonly integrated with an outage management system is a mobile data system. This integration provides the ability for outage predictions to automatically be sent to crews in the field and for the crews to be able to update the OMS with information such as estimated restoration times without requiring radio communication with the control center. Crews also transmit details about what they did during outage restoration.\n\nIt is important that the outage management system electrical model be kept up to current so that it can accurately make outage predictions and also accurately keep track of which customers are out and which are restored. By using this model and by tracking which switches, breakers and fuses are open and which are closed, network tracing functions can be used to identify every customer who is out, when they were first out and when they were restored. Tracking this information is the key to accurately reporting outage statistics. (P.-C. Chen, et al., 2014)\n\nOMS benefits include:\n\nAn OMS supports distribution system planning activities related to improving reliability by providing important outage statistics. In this role, an OMS provides the data needed for the calculation of measurements of the system reliability. Reliability is commonly measured by performance indices defined by the IEEE P1366-2003 standard. The most frequently used performance indices are SAIDI, CAIDI, SAIFI and MAIFI.\n\nAn OMS also support the improvement of distribution reliability by providing historical data that can be mined to find common causes, failures and damages. By understanding the most common modes of failure, improvement programs can be prioritized with those that provide the largest improvement on reliability for the lowest cost.\n\nWhile deploying an OMS improves the accuracy of the measured reliability indices, it often results an apparent degradation of reliability due to improvements over manual methods that almost always underestimate the frequency of outages, the size of outage and the duration of outages. To compare reliability in years before an OMS deployment to the years after requires adjustments to be made to the pre-deployment years measurements to be meaningful.\n\n"}
{"id": "3517808", "url": "https://en.wikipedia.org/wiki?curid=3517808", "title": "Overhoff Technology", "text": "Overhoff Technology\n\nOverhoff Technology Corp is a subsidiary of US Nuclear Corp, (OTC BB: UCLE) based in Milford, Ohio that designs, constructs, and sells radiation monitoring equipment. Their product line includes tritium monitors (an Overhoff specialty), heavy water leak detectors, gamma survey meters, environmental ion chambers and neutron dosimeters which are used to detect radioactive tritium in air, water and ground sources.\n\nOverhoff Technology's products are ISO 9001 certified and used globally for nuclear power plants, fusion energy research, development of new pharmaceuticals, defense industries, and research facilities. The Company has been awarded contracts by the United States Department of Defense and sells tritium equipment to China, South Korean, Canada, UK and Argentina based nuclear power facilities. \n\nOverhoff Technology Corp was founded by Mario Overhoff (1928-2005). The Company was acquired by Optron Scientific Corp/Technical Associates in 2006 and operates under US Nuclear Corp. The Company became publicly traded in early 2015 and trades on the Over-the-Counter Bulletin Board under the trading symbol: UCLE\n\n\n"}
{"id": "48524141", "url": "https://en.wikipedia.org/wiki?curid=48524141", "title": "Peugeot Velv", "text": "Peugeot Velv\n\nPeugeot Velv (Véhicule électrique Léger de Ville) is a concept lightweight urban electric vehicle presented in Paris on 26 September 2011.\n\nThe Velv's very short rear axle makes it closer to being a Three-wheeler. It is a three-seater with overall length of 2.81 m.\nThe electric motor generates output of 32 hp (20 kW), for a top speed of 110 km/h. Cruising range is 100 kilometers with 8.5 kWh battery capacity.\nOfficial video presentation\n\n"}
{"id": "45233281", "url": "https://en.wikipedia.org/wiki?curid=45233281", "title": "Phenyl alkanoic acids", "text": "Phenyl alkanoic acids\n\nShort chain ω-phenylalkanoic acids have long been known to occur in natural products. Phenylacetic, 3-phenylpropanoic and 3-phenylpropenoic (cinnamic) acids are found in propolis, mammalian exocrine secretions or plant fragrances.\nDuring a systematic study of the lipids from seeds of the plant Araceae, the presence of 13-phenyltridecanoic acid as a major component (5-16% of total fatty acids)was discovered. Other similar compounds but with 11 and 15 carbon chain lengths and saturated or unsaturated were shown to be also present but in lower amounts. At the same time, the even carbon chain ω-phenylalkanoic acids of C10 up to C16 were discovered in halophilic bacteria.\n\nLater, an exhaustive study of 17 genus of the subfamily Aroideae of Araceae revealed the presence of three major acids, 11-phenylundecanoic acid, 13-phenyltridecanoic acid and 15-phenylpentadecanoic acid in seed lipids. Other odd carbon number acids from C7 to C23 were detected but in trace amounts. Similarly, two series of homologous odd carbon number monounsaturated ω-phenylalkanoic acids were found. \nThus, it can be stated that all odd carbon chain ω-phenylalkanoic acids from C1 through C23 have been found in nature. Furthermore, even carbon chain ω-phenylalkanoic acids from C10 through C16 were also detected.\n\nSubstituted phenylalkenoic acids are periodically encountered in nature. As an example, rubrenoic acids were purified from Alteromonas rubra, compounds which showed bronchodilatatoric properties.\nMethyl phenylalkenoic acids (5 carbon chain) have been described from a terrestrial Streptomycete.\nSerpentene, a similar polyunsaturated phenylalkenoic acid, is also produced by Streptomyces and was shown to have some antibacterial properties.\nSeveral serpentene-like compounds have also been isolated from the same bacterial source.\n\nSeveral bicyclic derivatives of linolenic acid were shown to be generated by alkali isomerization.\n\nThe stink from the stinkpot turtle (Sternotherus odoratus) contains at least four different foul smelling ω-phenylalkanoic acids, including phenylacetic acid, 3-phenylpropionic acid, 5-phenylpentanoic acid, and 7-phenylheptanoic acid.\n\nSome others (alkyl-phenyl)-alkanoic acids) are formed when linolenic acid is warmed at 260–270 °C.\nSeveral forms with 16, 18 and 20 carbon atoms were identified in archaeological pottery vessels and were presumed to have been generated during heating triunsaturated fatty acids. They were used as biomarkers to trace the ancient processing of marine animal in these vessels.\n\nSeveral benzoic acid derivatives have been described in leaves of various Piperaceae species. Thus, a prenylated benzoic acid acid derivative, crassinervic acid, has been isolated from Piper crassinervium.\n\nSimilar compounds were isolated from \"Piper aduncum\" (aduncumene) and \"P. gaudichaudianum\" (gaudichaudianic acid). All these molecules showed high potential as antifungal compounds. A prenylated benzoic acid with a side chain formed of two isoprene units has also been isolated from the leaves of Piper aduncum. More recently, three prenylated benzoic acid derivatives with four isoprene units have been extracted from the leaves of \"P. heterophyllum\" and \"P. aduncum\". These compounds displayed moderate antiplasmodial (against \"Plasmodium falciparum\") and trypanocidal (against \"Trypanosoma cruzi\") activities.\n"}
{"id": "9252226", "url": "https://en.wikipedia.org/wiki?curid=9252226", "title": "Pitchers (ceramic material)", "text": "Pitchers (ceramic material)\n\nPitchers are pottery that has been broken in the course of manufacture. Biscuit (unglazed) pitchers can be crushed, ground and re-used, either as a low-percentage addition to the virgin raw materials on the same factory, or elsewhere as grog. Because of the adhering glaze, glost pitchers find less use. The crushed material can also be used in other industries as an inert filler. \n\nArchaeologists call ancient pitchers sherds or ostracons; shards or ostraca.\n"}
{"id": "11637096", "url": "https://en.wikipedia.org/wiki?curid=11637096", "title": "Skyline logging", "text": "Skyline logging\n\nSkyline logging (or skyline yarding or cable logging) are terms in forestry, in which harvested logs are transported on a suspended steel cable, a cableway or \"highline\", from various locations where the trees are felled to a central location, typically next to a road for logistical reasons. The skyline's cable loop runs around a drive pulley, generally at the central delivery end, and the return pulley at the collection end; the collection-end pulley may be moved radially to other locations within the constraints of the system, and may operate over large areas.\n\nIndividual logs are attached to the suspended cable by means of choker cables and carriages. A skyline yarder can pull in 5 to 10 logs at a time, using separate chokers. The pulleys are mounted on towers or cranes, other trees, ridges or, in rare cases, helium balloons.\n\n\n"}
{"id": "16054272", "url": "https://en.wikipedia.org/wiki?curid=16054272", "title": "Stevens Vehicles", "text": "Stevens Vehicles\n\nStevens Vehicles Ltd is a British electric car and van manufacturer based in Port Talbot.\n\nThe Company is a manufacturer of zero emission electric cars and vans designed by the father and son team of Tony Stevens and Peter Stevens. The vehicles were conceived, designed and developed in Kent, England and are manufactured at Port Talbot, Wales.\n\nThe brand, comprising two vehicle types, was launched on 1 March 2008, comprising the ZeCar and ZeVan. The designer is Professor Tony Stevens, who has spent over 50 years in the motor industry and who has long experience in the engineering, design and marketing of vehicles. Tony Stevens designed the Hillman Hunter and Sunbeam Rapier. Peter Stevens is an international banker, involved in the financial and sales side of the business.\n\nThe design of the vehicles has been described as being \"deliciously weird...a bit dippy, yet great fun and strangely stylish.\" The strong visual statement derives from their distinctive height, which results from the higher than normal driving position. This is based on the belief, of Tony Stevens, that the driver should be positioned as if sitting at a table. This position gives the driver good access and visibility, but the design also provides plenty of headroom. Both types of vehicle are designed to maximise internal space relative to external dimensions so that, although they are shorter and narrower than most compact vehicles they are also taller, thus creating the equivalent space of a much larger vehicle.\n\nThe Ze in Zecar and ZeVan stands for zero environmental emissions. On-road carbon dioxide emissions are zero. However, the lead-acid batteries require recharging which may produce carbon dioxide, depending on the energy supply source.\nStevens Vehicles is being supported by the Welsh Assembly Government's Accelerate Clusters programme which plans to strengthen the automobile sector in Wales by developing emerging technologies and identifying and exploiting niche markets. The factory is based at the ECM2 business site within the Port Talbot Steelworks.\n\nThe Stevens Vehicles website currently indicates that they are no longer producing vehicles having failed to secure further adequate finance.\n\nThe ZeCar is a 5-door hatchback with a twin AC induction brushless and maintenance free electric engine, giving the car 140 lb ft of torque. There is a maintenance-free toothed belt drive to the rear wheels. Battery charging comprises an on-board automatic charging system, with maintenance-free sealed lead-acid gel batteries.\n\nThe maximum speed of the ZeCar is 56 mph, and with standard batteries the maximum range is 56 miles, unless back-up battery solutions are used. Acceleration is 0–40 mph in 15 seconds. The running cost is calculated at 1.2p per mile and the car is not subject to road tax, or congestion charges and parking is free or discounted in some cities such as London.\n\nIn 2008, this was the only five-seat electric car on sale in Britain.\n\nThe ZeVan is based on the same design as the ZeCar. Access to the rear is by twin-split doors which open out fully and the near-cube design allows optimum space utilisation. The load volume is one Cheps pallet or a Euro pallet and the payload is 450 kg. The dimensions of the load area are 1.80 cubic metres. The estimated battery life is 700 cycles at 80% depth of discharge and the design life is 12 years. The cost per mile is estimated at between 1p and 1.2p depending on the electricity tariff rate. \n\nAs the van is electric it does not need servicing and at the end of the battery life Stevens Vehicles would plan to collect the old battery, replace it with a new one, and recycle the old battery. The van is constructed on the principle that it can be recharged on a three-pin plug, thus avoiding the need for supporting infrastructure, although rapid chargers can be purchased allowing a recharge time of 30 minutes. The range is 55 miles for the van and driver, reducing to 38 miles for a fully laden van. Additional batteries can be fitted to increase the range by 50%, but with a corresponding reduction in payload of 125 kg. \n\nThe project plan is to develop several variations on the \"Ze\" theme. Plans include variants on the Ze range, including a pick-up truck and taxi, together with a contemporary version of the Cipher, a concept car first shown at the 1980 motor show and which could go on sale with an electric engine with a range capability of 70 miles and a 70 mph top speed.\n\nNew battery technologies are under development, including non-hydrogen fuel cells and bio-diesel hybrids.\n\nThe global plan is to establish small Ze plants in rural areas across the developing world. The production line and other tools and materials would be delivered by a single lorry and full-scale production could begin rapidly with output flexible enough to meet the cycle of local supply. Tony Stevens refers to this concept as Global Village Transport. \n\n"}
{"id": "54631939", "url": "https://en.wikipedia.org/wiki?curid=54631939", "title": "Svetlana (company)", "text": "Svetlana (company)\n\nPJSC Svetlana () is a company based in Saint Petersburg, Russia. It is primarily involved in the research, design, and manufacturing of electronic and microelectronic instruments. Svetlana is part of Ruselectronics. The name of the company is said to originate from the words for 'light of an electric lamp' (СВЕТ ЛАмпочки НАкаливания).\n\nThe company was established in 1889 as the Ya. M. Aivaz () Factory. Svetlana was a major producer of vacuum tubes. In 1937, the Soviet Union purchased a tube assembly line from RCA, including production licenses and initial staff training, and installed it on the St Petersburg plant. US-licensed tubes were produced since then.\n\nSince 2001, New Sensor Corp. has been holding the rights for the Svetlana vacuum tube brand for the US and Canada. The New Sensor tubes are actually manufactured at the Xpo-pul factory (former Reflektor plant) in Saratov. Tubes manufactured by Svetlana in Saint Petersburg still bear the \"winged \"С\"\" (cyrillic S) logo (see the image below) but no longer the name Svetlana.\n\nIn 2017 the company announced a 3 billion ruble modernization plan.\n\nThe Svetlana Association produces a variety of electronic and microelectronic instruments, including transmitting and modulator tubes for all frequency ranges; X-band broadband passive TR limiter; KU-band broadband TR tube; klystron amplifiers; X-ray tubes; portable X-ray units for medicine and industry; high-frequency fast response thyristors; transistors; integrated microcircuits; microcomputers; microcontrollers; microcalculators; ultrasonic delay lines; receiving tubes; process equipment for the manufacture of electronic engineering items. Vacuum tubes currently in production include the 6550, 6L6, EL34, and KT88.\n\n\n"}
{"id": "24461460", "url": "https://en.wikipedia.org/wiki?curid=24461460", "title": "Texas Co-op Power magazine", "text": "Texas Co-op Power magazine\n\nTexas Co-op Power magazine is the largest circulation monthly magazine in Texas. It goes to more than 1 million homes and businesses and is read by approximately 3 million people. The BPA audited statement for June 2009 put circulation at 1,188,965.\n\nFounded in 1944 to assist newly formed electric cooperatives communicate and educate their members on such topics as using electric appliances and electrical safety, the monthly magazine has evolved into a feature publication with separate editions for 59 cooperatives. It is distributed in 227 of Texas’ 254 counties (2009 Directory, Texas Electric Cooperatives).\n\n\"Texas Co-op Power\"’s goals are to enhance the quality of life of member-customers in an educational and entertaining format. Recent feature stories have included profiles of the late writer Elmer Kelton, football legend Sammy Baugh and the Quebe Sisters, as well as pieces on wildlife photography, fall gardens and Hurricane Ike.\n\nThe magazine serves rural, small town, and suburban Texans who are members of electric cooperatives. It is published by Texas Electric Cooperatives, a statewide association. Nonmembers may subscribe by contacting the magazine.\n\nWith its roots in the electric co-op tradition and with its editorial eye on a fast-growing, rapidly changing state, Texas Co-op Power offers features on daily life in contemporary Texas, stories by some of the state's best writers, electric utility information, and tips on cooking, recreation, nature, gardening and things to do/places to go around the state. It also emphasizes Texas history and culture.\n\nEach subscribing cooperative may have as many as eight pages in the center of the magazine for local information and features.\n\n\"Texas Co-op Power\" also publishes cookbooks and posters. The most recent cookbook is 60 Years of Home Cooking [2006, Texas Electric Cooperatives]. In addition to reprinting recipes published in the magazine, it describes the evolution of rural cooking as modern appliances such as microwaves and blenders gradually supplanted ice boxes and wood stoves.\n\nThe archives of \"Texas Co-op Power\" reside at Texas Electric Cooperative headquarters on the 24th floor of the Westgate building, 1122 Colorado Street, Austin, TX. PDFs of the issues from 2004 to 2009 are online at www.texascooppower.com.\nThe magazine’s historic records are catalogued and available at the Dolph Briscoe Center for American History, University of Texas, Austin, TX.\n\n"}
{"id": "29500273", "url": "https://en.wikipedia.org/wiki?curid=29500273", "title": "Toroidal expansion joint", "text": "Toroidal expansion joint\n\nA Toroidal expansion joint is a metallic assembly that consists of a series of toroidal convolutions which are circular tubes wrapped around pipe ends or weld ends and have a gap at the inside diameter to allow for axial stroke while absorbing changes in expansion or contraction of the pipe line. Convolutions are the portion of the bellows that allow it to be flexible. The convolutions are formed around reinforcing bands so that only the concave portion of the torus allows for flexibility. Toroidal expansion joints are typically used in high pressure applications, where little movement is required, and generally used for heat exchangers. Usually, they are hydraulically formed, but others are free formed. These expansion joints are also referred to as \"Omega\" bellows due to their shape resembling the Greek letter, Omega.\n\n"}
{"id": "1258473", "url": "https://en.wikipedia.org/wiki?curid=1258473", "title": "Tow", "text": "Tow\n\nIn the textile industry, a tow is a coarse, broken fibre, removed during processing flax, hemp, or jute. Flax tows are often used as upholstery stuffing, and tows in general are frequently cut up to produce staple fibre. The very light color of flax tow is the source of the word \"towhead\", meaning a person with naturally tousled light blonde hair.\n\nIn the artificial fibre and composites industries, a tow is an untwisted bundle of continuous filaments, in particular of acrylic, carbon fibres, or viscose rayon. Tows are designated either by their total tex (mass per unit length) or by the number of fibres they contain. For example, a 12K tow contains about 12,000 fibres.\n"}
{"id": "1645401", "url": "https://en.wikipedia.org/wiki?curid=1645401", "title": "United States Air Force Stability and Control Digital DATCOM", "text": "United States Air Force Stability and Control Digital DATCOM\n\nThe United States Air Force Stability and Control Digital DATCOM is a computer program that implements the methods contained in the USAF Stability and Control DATCOM to calculate the static stability, control and dynamic derivative characteristics of fixed-wing aircraft. Digital DATCOM requires an input file containing a geometric description of an aircraft, and outputs its corresponding dimensionless stability derivatives according to the specified flight conditions. The values obtained can be used to calculate meaningful aspects of flight dynamics.\n\nIn February 1976, work commenced to automate the methods contained in the USAF Stability and Control DATCOM, specifically those contained in sections 4, 5, 6 and 7. The work was performed by the McDonnell Douglas Corporation under contract with the United States Air Force in conjunction with engineers at the Air Force Flight Dynamics Laboratory in Wright-Patterson Air Force Base. Implementation of the Digital DATCOM concluded in November 1978.\n\nThe program is written in FORTRAN IV and has since been updated; however, the core of the program remains the same.\n\nA report was published, separated into three volumes, which explains the use of Digital DATCOM. The report consists of\n\nSection 3 of the USAF Digital DATCOM Manual Volume I defines the inputs available for modeling an aircraft. The inputs are categorized by namelists to facilitate reading the file into FORTRAN.\n\nThe FLTCON Namelist describes the flight conditions for the case. A maximum of 400 Mach-altitude combinations can be run at once, with up to 20 angles of attack for each combination. The user can specify whether the Mach number and altitude varies together, the Mach number varies at a constant altitude, or the altitude varies at a constant Mach number. Both subsonic and supersonic analysis can be run in Digital DATCOM.\n\nThe OPTINS Namelist defines the reference parameters for the aircraft. The theoretical wing area, mean aerodynamic chord, and wing span are input along with a parameter defining the surface roughness of the aircraft.\n\nThe SYNTHS Namelist allows the user to define the positions of the center of gravity and apexes of the wings. The X- and Z- coordinates are needed for the wing, horizontal tail, and vertical tail in order for the aircraft to be synthesized correctly. DATCOM does not require that the origin for the aircraft has to be the nose of the aircraft; any arbitrary point will do, but all of the dimensions need to be referenced from that point. Incidence angles can also be added to the wing and horizontal tail.\n\nThe BODY Namelist defines the shape of the body. Digital DATCOM assumes an axisymmetrical shape for the body. Up to 20 stations can be specified with the fuselage half-width, upper coordinate and lower coordinate being defined at each station. For supersonic analysis, additional parameters can be input.\n\nThe WGPLNF, HTPLNF and VTPLNF Namelists define the wing, horizontal tail and vertical tail, respectively. The basic parameters such as root chord, tip chord, half-span, twist, dihedral and sweep are input. Digital DATCOM also accepts wing planforms which change geometry along the span such as the F4 Phantom II which had 15 degrees of outboard dihedral.\n\nCanards can also be analyzed in Digital DATCOM. The canard must be specified as the forward lifting surface (i.e. wing) and the wing as the aft lift surface.\n\nFor airfoil designations, most traditional NACA 4-, 5-, and 6- airfoils can be specified in Digital DATCOM. Additionally, custom airfoils can be input using the appropriate namelists. Also, twin vertical tails can be designated in Digital DATCOM, but not twin booms.\n\nUsing the SYMFLP and ASYFLP Namelists, flaps, elevators, and ailerons can be defined. Digital DATCOM allows a multitude of flap types including plain, single-slotted, and fowler flaps. Up to 9 flap deflections can be analyzed at each Mach-altitude combination. Unfortunately, the rudder is not implemented in Digital DATCOM.\n\nDigital DATCOM also offers an automated aircraft TRIM function which calculates elevator deflections needed to trim the aircraft.\n\nOther Digital DATCOM inputs include power effects (propeller and jet), ground effects, trim tabs, and experimental data. The EXPRXX Namelist allows a user to use experimental data (such as coefficient of lift, coefficient of drag, etc.) in lieu of the data Digital DATCOM produces in the intermediate steps of its component build-up.\n\nAll dimensions are taken in feet and degrees unless specified otherwise. Digital DATCOM provides commands for outputting the dynamic derivatives (DAMP) as well as the stability coefficients of each components (BUILD).\n\nDigital DATCOM produces a copious amount of data for the relatively small amount of inputs it requires. By default, only the data for the aircraft is output, but additional configurations can be output:\n\nFor each configuration, stability coefficients and derivatives are output at each angle of attack specified. The details of this output are defined in Section 6 of the USAF Digital DATCOM Manual Volume I. The basic output includes:\n\nFor complete aircraft configurations, downwash data is also included.\n\nWhen compared with modern methods of computational fluid dynamics, Digital DATCOM may seem antiquated. However, in its day, the program was an advanced estimation tool, and certainly much faster than plowing through pages and pages of engineering texts. Digital DATCOM is no longer supported by the USAF and is now public domain software.\n\nInlets, external stores, and other protuberances cannot be input because Digital DATCOM analyzes the fuselage as a body of revolution. The simplification affects the coefficient of drag for the aircraft.\n\nDynamic derivatives are not output for aircraft that have wings that are not straight-tapered or have leading edge extensions. This problem can be overcome by using experimental data for the wing-body (using non-straight tapered wing).\n\nThere is no method to input twin vertical tails mounted on the fuselage, although there is a method for H-Tails. This problem can be addressed by approximating the twin vertical tails as a single equivalent vertical tail mounted to the fuselage.\n\nDigital DATCOM cannot provide outputs for the control derivatives with regard to the rudder control surface. According to the manual, there is no any input parameters which define the geometriy of rudder.\n\nDigital DATCOM cannot analyze three lifting surfaces at once, such as a canard-wing-horizontal tail configuration. This problem can be addressed by superposition of lifting surfaces through the experimental input option.\n\nThere are intentions among those that use this package to improve the overall package, through an easier user interface, as well as more comprehensive output data.\n\nWhile the original DIGDAT program has been left relatively untouched, there has been a new front-end created that will allow the user to name the input file with something more significant than FOR005.DAT. The new input file format allows the user to place comments in the input file. There have also been hooks placed in the DIGDAT that allow for alternate outputs in addition to the original output format, which is 132 columns wide and slightly user abusive if you intend to import the data into another application. There is a graphical representation of the aircraft output in AC3D, as well as data table output in XML for the JSBSim and FlightGear projects, as well as a free-format LFI (Linear Function Interpolation) data table file.\n\nAlong with the DIGDAT program, there are viewers for the AC3D, XML, and LFI format output files. Data tables can easily be output to the screen or to PNG files for inclusion into reports.\nAerospace Toolbox includes a function for importing output files from Digital DATCOM into MATLAB. This function lets you collect aerodynamic coefficients from static and dynamic analyses and transfer them into MATLAB as a cell array of structures, with each structure containing information about a Digital DATCOM output file.\n\nOpenDatcom is an open-source GUI for the Digital DATCOM created and hosted by the OpenAE community. OpenDatcom incorporates all the basic (non-experimental) functionality supported by the Digital DATCOM while providing real-time input error and bounds checking. An alpha version of the program was released November 1, 2009 to the general public. The OpenAE.org web site is no longer active.\n\nThere has been some research in using Digital DATCOM in conjunction with wind tunnel studies to predict aerodynamics of structurally impaired aircraft. Dr. Bilal Siddiqui at DHA Suffa University presented an approach to predict the nonlinear aerodynamics of a structurally damaged aircraft model based on the engineering level aerodynamic prediction methods, DATCOM. Raw results from the code provide good correlation with wind tunnel data at very low angles of attack, but accuracy deteriorates rapidly as the angle of attack increases. A new methodology is then proposed which combines the experimental results of healthy aircraft with the predicted aerodynamics of the damaged cases, to yield better correlation between experimental and predicted aerodynamic coefficients for damaged aircraft. Three damage-configurations are studied at supersonic speeds. The methodology can be used to quickly generate aerodynamic model for damaged aircraft for simulation and reconfigurable control\n\n\n\n"}
{"id": "31743", "url": "https://en.wikipedia.org/wiki?curid=31743", "title": "Uranium", "text": "Uranium\n\nUranium is a chemical element with symbol U and atomic number 92. It is a silvery-grey metal in the actinide series of the periodic table. A uranium atom has 92 protons and 92 electrons, of which 6 are valence electrons. Uranium is weakly radioactive because all isotopes of uranium are unstable, with half-lives varying between 159,200 years and 4.5 billion years. The most common isotopes in natural uranium are uranium-238 (which has 146 neutrons and accounts for over 99%) and uranium-235 (which has 143 neutrons). Uranium has the highest atomic weight of the primordially occurring elements. Its density is about 70% higher than that of lead, and slightly lower than that of gold or tungsten. It occurs naturally in low concentrations of a few parts per million in soil, rock and water, and is commercially extracted from uranium-bearing minerals such as uraninite.\n\nIn nature, uranium is found as uranium-238 (99.2739–99.2752%), uranium-235 (0.7198–0.7202%), and a very small amount of uranium-234 (0.0050–0.0059%). Uranium decays slowly by emitting an alpha particle. The half-life of uranium-238 is about 4.47 billion years and that of uranium-235 is 704 million years, making them useful in dating the age of the Earth.\n\nMany contemporary uses of uranium exploit its unique nuclear properties. Uranium-235 is the only naturally occurring fissile isotope, which makes it widely used in nuclear power plants and nuclear weapons. However, because of the tiny amounts found in nature, uranium needs to undergo enrichment so that enough uranium-235 is present. Uranium-238 is fissionable by fast neutrons, and is fertile, meaning it can be transmuted to fissile plutonium-239 in a nuclear reactor. Another fissile isotope, uranium-233, can be produced from natural thorium and is also important in nuclear technology. Uranium-238 has a small probability for spontaneous fission or even induced fission with fast neutrons; uranium-235 and to a lesser degree uranium-233 have a much higher fission cross-section for slow neutrons. In sufficient concentration, these isotopes maintain a sustained nuclear chain reaction. This generates the heat in nuclear power reactors, and produces the fissile material for nuclear weapons. Depleted uranium (U) is used in kinetic energy penetrators and armor plating. Uranium is used as a colorant in uranium glass, producing lemon yellow to green colors. Uranium glass fluoresces green in ultraviolet light. It was also used for tinting and shading in early photography.\n\nThe 1789 discovery of uranium in the mineral pitchblende is credited to Martin Heinrich Klaproth, who named the new element after the recently discovered planet Uranus. Eugène-Melchior Péligot was the first person to isolate the metal and its radioactive properties were discovered in 1896 by Henri Becquerel. Research by Otto Hahn, Lise Meitner, Enrico Fermi and others, such as J. Robert Oppenheimer starting in 1934 led to its use as a fuel in the nuclear power industry and in \"Little Boy\", the first nuclear weapon used in war. An ensuing arms race during the Cold War between the United States and the Soviet Union produced tens of thousands of nuclear weapons that used uranium metal and uranium-derived plutonium-239. The security of those weapons and their fissile material following the breakup of the Soviet Union in 1991 is an ongoing concern for public health and safety. See Nuclear proliferation.\n\nWhen refined, uranium is a silvery white, weakly radioactive metal. It has a Mohs hardness of 6, sufficient to scratch glass and approximately equal to that of titanium, rhodium, manganese and niobium. It is malleable, ductile, slightly paramagnetic, strongly electropositive and a poor electrical conductor. Uranium metal has a very high density of 19.1 g/cm, denser than lead (11.3 g/cm), but slightly less dense than tungsten and gold (19.3 g/cm).\n\nUranium metal reacts with almost all non-metal elements (with the exception of the noble gases) and their compounds, with reactivity increasing with temperature. Hydrochloric and nitric acids dissolve uranium, but non-oxidizing acids other than hydrochloric acid attack the element very slowly. When finely divided, it can react with cold water; in air, uranium metal becomes coated with a dark layer of uranium oxide. Uranium in ores is extracted chemically and converted into uranium dioxide or other chemical forms usable in industry.\n\nUranium-235 was the first isotope that was found to be fissile. Other naturally occurring isotopes are fissionable, but not fissile. On bombardment with slow neutrons, its uranium-235 isotope will most of the time divide into two smaller nuclei, releasing nuclear binding energy and more neutrons. If too many of these neutrons are absorbed by other uranium-235 nuclei, a nuclear chain reaction occurs that results in a burst of heat or (in special circumstances) an explosion. In a nuclear reactor, such a chain reaction is slowed and controlled by a neutron poison, absorbing some of the free neutrons. Such neutron absorbent materials are often part of reactor control rods (see nuclear reactor physics for a description of this process of reactor control).\n\nAs little as 15 lb (7 kg) of uranium-235 can be used to make an atomic bomb. The first nuclear bomb used in war, \"Little Boy\", relied on uranium fission, but the very first nuclear explosive (the \"Gadget\" used at Trinity) and the bomb that destroyed Nagasaki (\"Fat Man\") were both plutonium bombs.\n\nUranium metal has three allotropic forms:\n\nThe major application of uranium in the military sector is in high-density penetrators. This ammunition consists of depleted uranium (DU) alloyed with 1–2% other elements, such as titanium or molybdenum. At high impact speed, the density, hardness, and pyrophoricity of the projectile enable the destruction of heavily armored targets. Tank armor and other removable vehicle armor can also be hardened with depleted uranium plates. The use of depleted uranium became politically and environmentally contentious after the use of such munitions by the US, UK and other countries during wars in the Persian Gulf and the Balkans raised questions concerning uranium compounds left in the soil (see Gulf War Syndrome).\n\nDepleted uranium is also used as a shielding material in some containers used to store and transport radioactive materials. While the metal itself is radioactive, its high density makes it more effective than lead in halting radiation from strong sources such as radium. Other uses of depleted uranium include counterweights for aircraft control surfaces, as ballast for missile re-entry vehicles and as a shielding material. Due to its high density, this material is found in inertial guidance systems and in gyroscopic compasses. Depleted uranium is preferred over similarly dense metals due to its ability to be easily machined and cast as well as its relatively low cost. The main risk of exposure to depleted uranium is chemical poisoning by uranium oxide rather than radioactivity (uranium being only a weak alpha emitter).\n\nDuring the later stages of World War II, the entire Cold War, and to a lesser extent afterwards, uranium-235 has been used as the fissile explosive material to produce nuclear weapons. Initially, two major types of fission bombs were built: a relatively simple device that uses uranium-235 and a more complicated mechanism that uses plutonium-239 derived from uranium-238. Later, a much more complicated and far more powerful type of fission/fusion bomb (thermonuclear weapon) was built, that uses a plutonium-based device to cause a mixture of tritium and deuterium to undergo nuclear fusion. Such bombs are jacketed in a non-fissile (unenriched) uranium case, and they derive more than half their power from the fission of this material by fast neutrons from the nuclear fusion process.\n\nThe main use of uranium in the civilian sector is to fuel nuclear power plants. One kilogram of uranium-235 can theoretically produce about 20 terajoules of energy (2 joules), assuming complete fission; as much energy as 1500 tonnes of coal.\n\nCommercial nuclear power plants use fuel that is typically enriched to around 3% uranium-235. The CANDU and Magnox designs are the only commercial reactors capable of using unenriched uranium fuel. Fuel used for United States Navy reactors is typically highly enriched in uranium-235 (the exact values are classified). In a breeder reactor, uranium-238 can also be converted into plutonium through the following reaction:\n\nBefore (and, occasionally, after) the discovery of radioactivity, uranium was primarily used in small amounts for yellow glass and pottery glazes, such as uranium glass and in Fiestaware.\n\nThe discovery and isolation of radium in uranium ore (pitchblende) by Marie Curie sparked the development of uranium mining to extract the radium, which was used to make glow-in-the-dark paints for clock and aircraft dials. This left a prodigious quantity of uranium as a waste product, since it takes three tonnes of uranium to extract one gram of radium. This waste product was diverted to the glazing industry, making uranium glazes very inexpensive and abundant. Besides the pottery glazes, uranium tile glazes accounted for the bulk of the use, including common bathroom and kitchen tiles which can be produced in green, yellow, mauve, black, blue, red and other colors.\nUranium was also used in photographic chemicals (especially uranium nitrate as a toner), in lamp filaments for stage lighting bulbs, to improve the appearance of dentures, and in the leather and wood industries for stains and dyes. Uranium salts are mordants of silk or wool. Uranyl acetate and uranyl formate are used as electron-dense \"stains\" in transmission electron microscopy, to increase the contrast of biological specimens in ultrathin sections and in negative staining of viruses, isolated cell organelles and macromolecules.\n\nThe discovery of the radioactivity of uranium ushered in additional scientific and practical uses of the element. The long half-life of the isotope uranium-238 (4.51 years) makes it well-suited for use in estimating the age of the earliest igneous rocks and for other types of radiometric dating, including uranium–thorium dating, uranium–lead dating and uranium–uranium dating. Uranium metal is used for X-ray targets in the making of high-energy X-rays.\n\nThe use of uranium in its natural oxide form dates back to at least the year 79 CE, when it was used to add a yellow color to ceramic glazes. Yellow glass with 1% uranium oxide was found in a Roman villa on Cape Posillipo in the Bay of Naples, Italy, by R. T. Gunther of the University of Oxford in 1912. Starting in the late Middle Ages, pitchblende was extracted from the Habsburg silver mines in Joachimsthal, Bohemia (now Jáchymov in the Czech Republic), and was used as a coloring agent in the local glassmaking industry. In the early 19th century, the world's only known sources of uranium ore were these mines.\n\nThe discovery of the element is credited to the German chemist Martin Heinrich Klaproth. While he was working in his experimental laboratory in Berlin in 1789, Klaproth was able to precipitate a yellow compound (likely sodium diuranate) by dissolving pitchblende in nitric acid and neutralizing the solution with sodium hydroxide. Klaproth assumed the yellow substance was the oxide of a yet-undiscovered element and heated it with charcoal to obtain a black powder, which he thought was the newly discovered metal itself (in fact, that powder was an oxide of uranium). He named the newly discovered element after the planet Uranus (named after the primordial Greek god of the sky), which had been discovered eight years earlier by William Herschel.\n\nIn 1841, Eugène-Melchior Péligot, Professor of Analytical Chemistry at the Conservatoire National des Arts et Métiers (Central School of Arts and Manufactures) in Paris, isolated the first sample of uranium metal by heating uranium tetrachloride with potassium.\n\nHenri Becquerel discovered radioactivity by using uranium in 1896. Becquerel made the discovery in Paris by leaving a sample of a uranium salt, KUO(SO) (potassium uranyl sulfate), on top of an unexposed photographic plate in a drawer and noting that the plate had become \"fogged\". He determined that a form of invisible light or rays emitted by uranium had exposed the plate.\n\nA team led by Enrico Fermi in 1934 observed that bombarding uranium with neutrons produces the emission of beta rays (electrons or positrons from the elements produced; see beta particle). The fission products were at first mistaken for new elements with atomic numbers 93 and 94, which the Dean of the Faculty of Rome, Orso Mario Corbino, christened \"ausonium\" and \"hesperium\", respectively. The experiments leading to the discovery of uranium's ability to fission (break apart) into lighter elements and release binding energy were conducted by Otto Hahn and Fritz Strassmann in Hahn's laboratory in Berlin. Lise Meitner and her nephew, the physicist Otto Robert Frisch, published the physical explanation in February 1939 and named the process \"nuclear fission\". Soon after, Fermi hypothesized that the fission of uranium might release enough neutrons to sustain a fission reaction. Confirmation of this hypothesis came in 1939, and later work found that on average about 2.5 neutrons are released by each fission of the rare uranium isotope uranium-235. Fermi urged Alfred O. C. Nier to separate uranium isotopes for determination of the fissile component, and on February 29, 1940, Nier used an instrument he built at the University of Minnesota to separate the world's first uranium-235 sample in the Tate Laboratory. After mailed to Columbia University's cyclotron, John Dunning confirmed the sample to be the isolated fissile material on March 1. Further work found that the far more common uranium-238 isotope can be transmuted into plutonium, which, like uranium-235, is also fissile by thermal neutrons. These discoveries led numerous countries to begin working on the development of nuclear weapons and nuclear power.\n\nOn 2 December 1942, as part of the Manhattan Project, another team led by Enrico Fermi was able to initiate the first artificial self-sustained nuclear chain reaction, Chicago Pile-1. An initial plan using enriched uranium-235 was abandoned as it was as yet unavailable in sufficient quantities. Working in a lab below the stands of Stagg Field at the University of Chicago, the team created the conditions needed for such a reaction by piling together 400 short tons (360 metric tons) of graphite, 58 short tons (53 metric tons) of uranium oxide, and six short tons (5.5 metric tons) of uranium metal, a majority of which was supplied by Westinghouse Lamp Plant in a makeshift production process.\n\nTwo major types of atomic bombs were developed by the United States during World War II: a uranium-based device (codenamed \"Little Boy\") whose fissile material was highly enriched uranium, and a plutonium-based device (see Trinity test and \"Fat Man\") whose plutonium was derived from uranium-238. The uranium-based Little Boy device became the first nuclear weapon used in war when it was detonated over the Japanese city of Hiroshima on 6 August 1945. Exploding with a yield equivalent to 12,500 tonnes of TNT, the blast and thermal wave of the bomb destroyed nearly 50,000 buildings and killed approximately 75,000 people (see Atomic bombings of Hiroshima and Nagasaki). Initially it was believed that uranium was relatively rare, and that nuclear proliferation could be avoided by simply buying up all known uranium stocks, but within a decade large deposits of it were discovered in many places around the world.\n\nThe X-10 Graphite Reactor at Oak Ridge National Laboratory (ORNL) in Oak Ridge, Tennessee, formerly known as the Clinton Pile and X-10 Pile, was the world's second artificial nuclear reactor (after Enrico Fermi's Chicago Pile) and was the first reactor designed and built for continuous operation. Argonne National Laboratory's Experimental Breeder Reactor I, located at the Atomic Energy Commission's National Reactor Testing Station near Arco, Idaho, became the first nuclear reactor to create electricity on 20 December 1951. Initially, four 150-watt light bulbs were lit by the reactor, but improvements eventually enabled it to power the whole facility (later, the town of Arco became the first in the world to have all its electricity come from nuclear power generated by BORAX-III, another reactor designed and operated by Argonne National Laboratory). The world's first commercial scale nuclear power station, Obninsk in the Soviet Union, began generation with its reactor AM-1 on 27 June 1954. Other early nuclear power plants were Calder Hall in England, which began generation on 17 October 1956, and the Shippingport Atomic Power Station in Pennsylvania, which began on 26 May 1958. Nuclear power was used for the first time for propulsion by a submarine, the USS \"Nautilus\", in 1954.\n\nIn 1972, the French physicist Francis Perrin discovered fifteen ancient and no longer active natural nuclear fission reactors in three separate ore deposits at the Oklo mine in Gabon, West Africa, collectively known as the Oklo Fossil Reactors. The ore deposit is 1.7 billion years old; then, uranium-235 constituted about 3% of the total uranium on Earth. This is high enough to permit a sustained nuclear fission chain reaction to occur, provided other supporting conditions exist. The capacity of the surrounding sediment to contain the nuclear waste products has been cited by the U.S. federal government as supporting evidence for the feasibility to store spent nuclear fuel at the Yucca Mountain nuclear waste repository.\n\nAbove-ground nuclear tests by the Soviet Union and the United States in the 1950s and early 1960s and by France into the 1970s and 1980s spread a significant amount of fallout from uranium daughter isotopes around the world. Additional fallout and pollution occurred from several nuclear accidents.\n\nUranium miners have a higher incidence of cancer. An excess risk of lung cancer among Navajo uranium miners, for example, has been documented and linked to their occupation. The Radiation Exposure Compensation Act, a 1990 law in the US, required $100,000 in \"compassion payments\" to uranium miners diagnosed with cancer or other respiratory ailments.\n\nDuring the Cold War between the Soviet Union and the United States, huge stockpiles of uranium were amassed and tens of thousands of nuclear weapons were created using enriched uranium and plutonium made from uranium. Since the break-up of the Soviet Union in 1991, an estimated 600 short tons (540 metric tons) of highly enriched weapons grade uranium (enough to make 40,000 nuclear warheads) have been stored in often inadequately guarded facilities in the Russian Federation and several other former Soviet states. Police in Asia, Europe, and South America on at least 16 occasions from 1993 to 2005 have intercepted shipments of smuggled bomb-grade uranium or plutonium, most of which was from ex-Soviet sources. From 1993 to 2005 the Material Protection, Control, and Accounting Program, operated by the federal government of the United States, spent approximately US $550 million to help safeguard uranium and plutonium stockpiles in Russia. This money was used for improvements and security enhancements at research and storage facilities. \"Scientific American\" reported in February 2006 that in some of the facilities security consisted of chain link fences which were in severe states of disrepair. According to an interview from the article, one facility had been storing samples of enriched (weapons grade) uranium in a broom closet before the improvement project; another had been keeping track of its stock of nuclear warheads using index cards kept in a shoe box.\n\nAlong with all elements having atomic weights higher than that of iron, uranium is only naturally formed in supernovae. Primordial thorium and uranium are only produced in the r-process (rapid neutron capture), because the s-process (slow neutron capture) is too slow and cannot pass the gap of instability after bismuth. Besides the two extant primordial uranium isotopes, U and U, the r-process also produced significant quantities of U, which has a shorter half-life and has long since decayed completely to Th, which was itself enriched by the decay of Pu, accounting for the observed higher-than-expected abundance of thorium and lower-than-expected abundance of uranium. While the natural abundance of uranium has been supplemented by the decay of extinct Pu (half-life 0.375 million years) and Cm (half-life 16 million years), producing U and U respectively, this occurred to an almost negligible extent due to the shorter half-lives of these parents and their lower production than U and Pu, the parents of thorium: the Cm:U ratio at the formation of the Solar System was .\n\nUranium is a naturally occurring element that can be found in low levels within all rock, soil, and water. Uranium is the 51st element in order of abundance in the Earth's crust. Uranium is also the highest-numbered element to be found naturally in significant quantities on Earth and is almost always found combined with other elements. The decay of uranium, thorium, and potassium-40 in the Earth's mantle is thought to be the main source of heat that keeps the outer core liquid and drives mantle convection, which in turn drives plate tectonics.\n\nUranium's average concentration in the Earth's crust is (depending on the reference) 2 to 4 parts per million, or about 40 times as abundant as silver. The Earth's crust from the surface to 25 km (15 mi) down is calculated to contain 10 kg (2 lb) of uranium while the oceans may contain 10 kg (2 lb). The concentration of uranium in soil ranges from 0.7 to 11 parts per million (up to 15 parts per million in farmland soil due to use of phosphate fertilizers), and its concentration in sea water is 3 parts per billion.\n\nUranium is more plentiful than antimony, tin, cadmium, mercury, or silver, and it is about as abundant as arsenic or molybdenum. Uranium is found in hundreds of minerals, including uraninite (the most common uranium ore), carnotite, autunite, uranophane, torbernite, and coffinite. Significant concentrations of uranium occur in some substances such as phosphate rock deposits, and minerals such as lignite, and monazite sands in uranium-rich ores (it is recovered commercially from sources with as little as 0.1% uranium).\n\nOther organisms, such as the lichen \"Trapelia involuta\" or microorganisms such as the bacterium \"Citrobacter\", can absorb concentrations of uranium that are up to 300 times the level of their environment. \"Citrobacter\" species absorb uranyl ions when given glycerol phosphate (or other similar organic phosphates). After one day, one gram of bacteria can encrust themselves with nine grams of uranyl phosphate crystals; this creates the possibility that these organisms could be used in bioremediation to decontaminate uranium-polluted water.\nThe proteobacterium \"Geobacter\" has also been shown to bioremediate uranium in ground water. The mycorrhizal fungus Glomus intraradices increases uranium content in the roots of its symbiotic plant.\n\nIn nature, uranium(VI) forms highly soluble carbonate complexes at alkaline pH. This leads to an increase in mobility and availability of uranium to groundwater and soil from nuclear wastes which leads to health hazards. However, it is difficult to precipitate uranium as phosphate in the presence of excess carbonate at alkaline pH. A \"Sphingomonas\" sp. strain BSAR-1 has been found to express a high activity alkaline phosphatase (PhoK) that has been applied for bioprecipitation of uranium as uranyl phosphate species from alkaline solutions. The precipitation ability was enhanced by overexpressing PhoK protein in \"E. coli\".\n\nPlants absorb some uranium from soil. Dry weight concentrations of uranium in plants range from 5 to 60 parts per billion, and ash from burnt wood can have concentrations up to 4 parts per million. Dry weight concentrations of uranium in food plants are typically lower with one to two micrograms per day ingested through the food people eat.\n\nWorldwide production of UO (yellowcake) in 2013 amounted to 70,015 tonnes, of which 22,451 t (32%) was mined in Kazakhstan. Other important uranium mining countries are Canada (9,331 t), Australia (6,350 t), Niger (4,518 t), Namibia (4,323 t) and Russia (3,135 t).\n\nUranium ore is mined in several ways: by open pit, underground, in-situ leaching, and borehole mining (see uranium mining). Low-grade uranium ore mined typically contains 0.01 to 0.25% uranium oxides. Extensive measures must be employed to extract the metal from its ore. High-grade ores found in Athabasca Basin deposits in Saskatchewan, Canada can contain up to 23% uranium oxides on average. Uranium ore is crushed and rendered into a fine powder and then leached with either an acid or alkali. The leachate is subjected to one of several sequences of precipitation, solvent extraction, and ion exchange. The resulting mixture, called yellowcake, contains at least 75% uranium oxides UO. Yellowcake is then calcined to remove impurities from the milling process before refining and conversion.\n\nCommercial-grade uranium can be produced through the reduction of uranium halides with alkali or alkaline earth metals. Uranium metal can also be prepared through electrolysis of or\n, dissolved in molten calcium chloride () and sodium chloride (NaCl) solution. Very pure uranium is produced through the thermal decomposition of uranium halides on a hot filament.\n\nIt is estimated that 5.5 million tonnes of uranium exists in ore reserves that are economically viable at US$59 per lb of uranium, while 35 million tonnes are classed as mineral resources (reasonable prospects for eventual economic extraction). Prices went from about $10/lb in May 2003 to $138/lb in July 2007. This has caused a big increase in spending on exploration, with US$200 million being spent worldwide in 2005, a 54% increase on the previous year. This trend continued through 2006, when expenditure on exploration rocketed to over $774 million, an increase of over 250% compared to 2004. The OECD Nuclear Energy Agency said exploration figures for 2007 would likely match those for 2006.\n\nAustralia has 31% of the world's known uranium ore reserves and the world's largest single uranium deposit, located at the Olympic Dam Mine in South Australia. There is a significant reserve of uranium\nin Bakouma a sub-prefecture in the prefecture of Mbomou in Central African Republic.\n\nSome nuclear fuel comes from nuclear weapons being dismantled, such as from the Megatons to Megawatts Program.\n\nAn additional 4.6 billion tonnes of uranium are estimated to be in sea water (Japanese scientists in the 1980s showed that extraction of uranium from sea water using ion exchangers was technically feasible). There have been experiments to extract uranium from sea water, but the yield has been low due to the carbonate present in the water. In 2012, ORNL researchers announced the successful development of a new absorbent material dubbed HiCap which performs surface retention of solid or gas molecules, atoms or ions and also effectively removes toxic metals from water, according to results verified by researchers at Pacific Northwest National Laboratory.\n\nIn 2005, seventeen countries produced concentrated uranium oxides: Canada (27.9% of world production), Australia (22.8%), Kazakhstan (10.5%), Russia (8.0%), Namibia (7.5%), Niger (7.4%), Uzbekistan (5.5%), the United States (2.5%), Argentina (2.1%), Ukraine (1.9%) and China (1.7%). Kazakhstan continues to increase production and may have become the world's largest producer of uranium by 2009 with an expected production of 12,826 tonnes, compared to Canada with 11,100 t and Australia with 9,430 t. In the late 1960s, UN geologists also discovered major uranium deposits and other rare mineral reserves in Somalia. The find was the largest of its kind, with industry experts estimating the deposits at over 25% of the world's then known uranium reserves of 800,000 tons.\n\nThe ultimate available supply is believed to be sufficient for at least the next 85 years, although some studies indicate underinvestment in the late twentieth century may produce supply problems in the 21st century.\nUranium deposits seem to be log-normal distributed. There is a 300-fold increase in the amount of uranium recoverable for each tenfold decrease in ore grade.\nIn other words, there is little high grade ore and proportionately much more low grade ore available.\n\nCalcined uranium yellowcake, as produced in many large mills, contains a distribution of uranium oxidation species in various forms ranging from most oxidized to least oxidized. Particles with short residence times in a calciner will generally be less oxidized than those with long retention times or particles recovered in the stack scrubber. Uranium content is usually referenced to , which dates to the days of the Manhattan Project when was used as an analytical chemistry reporting standard.\n\nPhase relationships in the uranium-oxygen system are complex. The most important oxidation states of uranium are uranium(IV) and uranium(VI), and their two corresponding oxides are, respectively, uranium dioxide () and uranium trioxide (). Other uranium oxides such as uranium monoxide (UO), diuranium pentoxide (), and uranium peroxide () also exist.\n\nThe most common forms of uranium oxide are triuranium octoxide () and . Both oxide forms are solids that have low solubility in water and are relatively stable over a wide range of environmental conditions. Triuranium octoxide is (depending on conditions) the most stable compound of uranium and is the form most commonly found in nature. Uranium dioxide is the form in which uranium is most commonly used as a nuclear reactor fuel. At ambient temperatures, will gradually convert to . Because of their stability, uranium oxides are generally considered the preferred chemical form for storage or disposal.\n\nSalts of many oxidation states of uranium are water-soluble and may be studied in aqueous solutions. The most common ionic forms are (brown-red), (green), (unstable), and (yellow), for U(III), U(IV), U(V), and U(VI), respectively. A few solid and semi-metallic compounds such as UO and US exist for the formal oxidation state uranium(II), but no simple ions are known to exist in solution for that state. Ions of liberate hydrogen from water and are therefore considered to be highly unstable. The ion represents the uranium(VI) state and is known to form compounds such as uranyl carbonate, uranyl chloride and uranyl sulfate. also forms complexes with various organic chelating agents, the most commonly encountered of which is uranyl acetate.\n\nUnlike the uranyl salts of uranium and polyatomic ion uranium-oxide cationic forms, the uranates, salts containing a polyatomic uranium-oxide anion, are generally not water-soluble.\n\nThe interactions of carbonate anions with uranium(VI) cause the Pourbaix diagram to change greatly when the medium is changed from water to a carbonate containing solution. While the vast majority of carbonates are insoluble in water (students are often taught that all carbonates other than those of alkali metals are insoluble in water), uranium carbonates are often soluble in water. This is because a U(VI) cation is able to bind two terminal oxides and three or more carbonates to form anionic complexes.\n\nThe uranium fraction diagrams in the presence of carbonate illustrate this further: when the pH of a uranium(VI) solution increases, the uranium is converted to a hydrated uranium oxide hydroxide and at high pHs it becomes an anionic hydroxide complex.\n\nWhen carbonate is added, uranium is converted to a series of carbonate complexes if the pH is increased. One effect of these reactions is increased solubility of uranium in the pH range 6 to 8, a fact that has a direct bearing on the long term stability of spent uranium dioxide nuclear fuels.\n\nUranium metal heated to reacts with hydrogen to form uranium hydride. Even higher temperatures will reversibly remove the hydrogen. This property makes uranium hydrides convenient starting materials to create reactive uranium powder along with various uranium carbide, nitride, and halide compounds. Two crystal modifications of uranium hydride exist: an α form that is obtained at low temperatures and a β form that is created when the formation temperature is above 250 °C.\n\nUranium carbides and uranium nitrides are both relatively inert semimetallic compounds that are minimally soluble in acids, react with water, and can ignite in air to form . Carbides of uranium include uranium monocarbide (UC), uranium dicarbide (), and diuranium tricarbide (). Both UC and are formed by adding carbon to molten uranium or by exposing the metal to carbon monoxide at high temperatures. Stable below 1800 °C, is prepared by subjecting a heated mixture of UC and to mechanical stress. Uranium nitrides obtained by direct exposure of the metal to nitrogen include uranium mononitride (UN), uranium dinitride (), and diuranium trinitride ().\n\nAll uranium fluorides are created using uranium tetrafluoride (); itself is prepared by hydrofluorination of uranium dioxide. Reduction of with hydrogen at 1000 °C produces uranium trifluoride (). Under the right conditions of temperature and pressure, the reaction of solid with gaseous uranium hexafluoride () can form the intermediate fluorides of , , and .\n\nAt room temperatures, has a high vapor pressure, making it useful in the gaseous diffusion process to separate the rare uranium-235 from the common uranium-238 isotope. This compound can be prepared from uranium dioxide and uranium hydride by the following process:\n\nThe resulting , a white solid, is highly reactive (by fluorination), easily sublimes (emitting a vapor that behaves as a nearly ideal gas), and is the most volatile compound of uranium known to exist.\n\nOne method of preparing uranium tetrachloride () is to directly combine chlorine with either uranium metal or uranium hydride. The reduction of by hydrogen produces uranium trichloride () while the higher chlorides of uranium are prepared by reaction with additional chlorine. All uranium chlorides react with water and air.\n\nBromides and iodides of uranium are formed by direct reaction of, respectively, bromine and iodine with uranium or by adding to those element's acids. Known examples include: , , , and . Uranium oxyhalides are water-soluble and include , , , and . Stability of the oxyhalides decrease as the atomic weight of the component halide increases.\n\nNatural uranium consists of three major isotopes: uranium-238 (99.28% natural abundance), uranium-235 (0.71%), and uranium-234 (0.0054%). All three are radioactive, emitting alpha particles, with the exception that all three of these isotopes have small probabilities of undergoing spontaneous fission, rather than alpha emission. There are also five other trace isotopes: uranium-239, which is formed when U undergoes spontaneous fission, releasing neutrons that are captured by another U atom; uranium-237, which is formed when U captures a neutron but emits two more, which then decays to neptunium-237; and finally, uranium-233, which is formed in the decay chain of that neptunium-237. It is also expected that thorium-232 should be able to undergo double beta decay, which would produce uranium-232, but this has not yet been observed experimentally.\n\nUranium-238 is the most stable isotope of uranium, with a half-life of about 4.468 years, roughly the age of the Earth. Uranium-235 has a half-life of about 7.13 years, and uranium-234 has a half-life of about 2.48 years.\nFor natural uranium, about 49% of its alpha rays are emitted by each of U atom, and also 49% by U (since the latter is formed from the former) and about 2.0% of them by the U. When the Earth was young, probably about one-fifth of its uranium was uranium-235, but the percentage of U was probably much lower than this.\n\nUranium-238 is usually an α emitter (occasionally, it undergoes spontaneous fission), decaying through the uranium series, which has 18 members, into lead-206, by a variety of different decay paths.\n\nThe decay chain of U, which is called the actinium series, has 15 members and eventually decays into lead-207. The constant rates of decay in these decay series makes the comparison of the ratios of parent to daughter elements useful in radiometric dating.\n\nUranium-234, which is a member of the uranium series (the decay chain of uranium-238), decays to lead-206 through a series of relatively short-lived isotopes.\n\nUranium-233 is made from thorium-232 by neutron bombardment, usually in a nuclear reactor, and U is also fissile. Its decay chain forms part of the neptunium series and ends at bismuth-209 and thallium-205.\n\nUranium-235 is important for both nuclear reactors and nuclear weapons, because it is the only uranium isotope existing in nature on Earth in any significant amount that is fissile. This means that it can be split into two or three fragments (fission products) by thermal neutrons.\n\nUranium-238 is not fissile, but is a fertile isotope, because after neutron activation it can produce plutonium-239, another fissile isotope. Indeed, the U nucleus can absorb one neutron to produce the radioactive isotope uranium-239. U decays by beta emission to neptunium-239, also a beta-emitter, that decays in its turn, within a few days into plutonium-239. Pu was used as fissile material in the first atomic bomb detonated in the \"Trinity test\" on 15 July 1945 in New Mexico.\n\nIn nature, uranium is found as uranium-238 (99.2742%) and uranium-235 (0.7204%). Isotope separation concentrates (enriches) the fissionable uranium-235 for nuclear weapons and most nuclear power plants, except for gas cooled reactors and pressurised heavy water reactors. Most neutrons released by a fissioning atom of uranium-235 must impact other uranium-235 atoms to sustain the nuclear chain reaction. The concentration and amount of uranium-235 needed to achieve this is called a 'critical mass'.\n\nTo be considered 'enriched', the uranium-235 fraction should be between 3% and 5%. This process produces huge quantities of uranium that is depleted of uranium-235 and with a correspondingly increased fraction of uranium-238, called depleted uranium or 'DU'. To be considered 'depleted', the uranium-235 isotope concentration should be no more than 0.3%. The price of uranium has risen since 2001, so enrichment tailings containing more than 0.35% uranium-235 are being considered for re-enrichment, driving the price of depleted uranium hexafluoride above $130 per kilogram in July 2007 from $5 in 2001.\n\nThe gas centrifuge process, where gaseous uranium hexafluoride () is separated by the difference in molecular weight between UF and UF using high-speed centrifuges, is the cheapest and leading enrichment process. The gaseous diffusion process had been the leading method for enrichment and was used in the Manhattan Project. In this process, uranium hexafluoride is repeatedly diffused through a silver-zinc membrane, and the different isotopes of uranium are separated by diffusion rate (since uranium 238 is heavier it diffuses slightly slower than uranium-235). The molecular laser isotope separation method employs a laser beam of precise energy to sever the bond between uranium-235 and fluorine. This leaves uranium-238 bonded to fluorine and allows uranium-235 metal to precipitate from the solution. An alternative laser method of enrichment is known as atomic vapor laser isotope separation (AVLIS) and employs visible tunable lasers such as dye lasers. Another method used is liquid thermal diffusion.\n\nA person can be exposed to uranium (or its radioactive daughters, such as radon) by inhaling dust in air or by ingesting contaminated water and food. The amount of uranium in air is usually very small; however, people who work in factories that process phosphate fertilizers, live near government facilities that made or tested nuclear weapons, live or work near a modern battlefield where depleted uranium weapons have been used, or live or work near a coal-fired power plant, facilities that mine or process uranium ore, or enrich uranium for reactor fuel, may have increased exposure to uranium. Houses or structures that are over uranium deposits (either natural or man-made slag deposits) may have an increased incidence of exposure to radon gas. The Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit for uranium exposure in the workplace as 0.25 mg/m over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.2 mg/m over an 8-hour workday and a short-term limit of 0.6 mg/m. At levels of 10 mg/m, uranium is immediately dangerous to life and health.\n\nMost ingested uranium is excreted during digestion. Only 0.5% is absorbed when insoluble forms of uranium, such as its oxide, are ingested, whereas absorption of the more soluble uranyl ion can be up to 5%. However, soluble uranium compounds tend to quickly pass through the body, whereas insoluble uranium compounds, especially when inhaled by way of dust into the lungs, pose a more serious exposure hazard. After entering the bloodstream, the absorbed uranium tends to bioaccumulate and stay for many years in bone tissue because of uranium's affinity for phosphates. Uranium is not absorbed through the skin, and alpha particles released by uranium cannot penetrate the skin.\n\nIncorporated uranium becomes uranyl ions, which accumulate in bone, liver, kidney, and reproductive tissues. Uranium can be decontaminated from steel surfaces and aquifers.\n\nNormal functioning of the kidney, brain, liver, heart, and other systems can be affected by uranium exposure, because, besides being weakly radioactive, uranium is a toxic metal. Uranium is also a reproductive toxicant. Radiological effects are generally local because alpha radiation, the primary form of U decay, has a very short range, and will not penetrate skin. Alpha radiation from inhaled uranium has been demonstrated to cause lung cancer in exposed nuclear workers. Uranyl () ions, such as from uranium trioxide or uranyl nitrate and other hexavalent uranium compounds, have been shown to cause birth defects and immune system damage in laboratory animals. While the CDC has published one study that no human cancer has been seen as a result of exposure to natural or depleted uranium, exposure to uranium and its decay products, especially radon, are widely known and significant health threats. Exposure to strontium-90, iodine-131, and other fission products is unrelated to uranium exposure, but may result from medical procedures or exposure to spent reactor fuel or fallout from nuclear weapons.\nAlthough accidental inhalation exposure to a high concentration of uranium hexafluoride has\nresulted in human fatalities, those deaths were associated with the generation of highly toxic hydrofluoric acid and uranyl fluoride rather than with uranium itself. Finely divided uranium metal presents a fire hazard because uranium is pyrophoric; small grains will ignite spontaneously in air at room temperature.\n\nUranium metal is commonly handled with gloves as a sufficient precaution. Uranium concentrate is handled and contained so as to ensure that people do not inhale or ingest it.\n\n\n"}
{"id": "21810847", "url": "https://en.wikipedia.org/wiki?curid=21810847", "title": "VBER-300", "text": "VBER-300\n\nThe VBER-300 is a proposed Russian pressurized water reactor of 325-MWe generating capacity designed for remote locations. The exterior containment structure is 16 meters high and the working section, built with transportable modules, weighs 1300 tonnes. The external steam plant can have a 917 MW thermal-steam only capacity, or 325 MW steam-turbine-electrical capacity, or a mixture of capacities relating to the four primary steam loops.\n\nIn particular, it has been proposed in a more powerful sister ship to the Akademik Lomonosov (2010) for possible use on the Russian floating nuclear power station (two reactors on a 49,000-tonne barge). The reactor could be used on a 200–500 MW barge that is expected to be completed by 2030.\n\nThe reactor has been proposed for use in water desalination, district heating and/or electrical generation.\n\nThe VBER-300 would use VVER-type fuel.\nIt was developed by OKBM Afrikantov in cooperation with Kurchatov Institute.\n\n\n"}
{"id": "27611191", "url": "https://en.wikipedia.org/wiki?curid=27611191", "title": "VLCC Metula oil spill", "text": "VLCC Metula oil spill\n\nThe VLCC \"Metula\" was a supertanker that was involved in an oil spill in Tierra del Fuego, Chile in 1974. The ship was a Very Large Crude Carrier (VLCC), with a length of 1,067 feet, draft of 62 feet and a deadweight ton capacity of 206,000. It was the first VLCC supertanker to be involved in a major oil spill.\n\nThe \"Metula\" was sailing from Ras Tanura in Saudi Arabia with a load of over 196,000 tons of light Arabian crude oil destined for delivery to the Chilean National Oil Company (ENAP) at Quintero, Chile.\n\nOn the evening of August 9, 1974, the tanker was passing through the First Narrows area, which is over three and half kilometers wide, of the Strait of Magellan, during severe tidal and current conditions. The \"Metula\" cut a corner too sharp, hitting a 40-foot shoal and grounding itself. The difficulty of navigating a ship of such size, with minimum navigation aids, contributed to the accident.\n\nOn the second day after the grounding, the \"Metula\" swung to starboard, holing and flooding its engine room compartments. The U.S. Coast Guard, at the request of the Chilean government played a role in removing the cargo from the ship.\n\nThe tanker released about 47,000 tonnes of Arabian light crude oil and between 3,000 and 4,000 tonnes of heavy fuel oil. The rough sea conditions resulted in the formation of a water-in-oil emulsion, which then landed on the shores of Tierra del Fuego.\n\nThe \"Metula\" was refloated on September 25, 1974, and was towed to Isla Grande, near Rio de Janeiro, Brazil to be scrapped.\n\nNo cleanup operation was executed due to the remoteness of the area; on many shorelines, the oil formed hard asphalt pavements. One marsh received thick deposits of mousse, which were still visible two decades after the disaster. By 1998, most of the oil deposits had broken up, though asphalt pavement remained in a relatively sheltered area, making it among the longest-term contamination recorded for an oil spill.\n\nFollowing the spill, there were significant negative impacts on the Chilean fishermen. The oil spill resulted in the heavy contamination of the waters of straits of Magellan- forcing Chilean Fishermen to other waters. The Straits were often turned to by the fishermen to hunt sea bass when they weren't hunting king crab. As a result of the spill, the fisheries were rendered unusable for an entire year for the Chilean fishermen. In addition, the overall quality of the fish remained poorer for a long time proceeding.\n\nOne of the most significant impacts of the spill was its effect on marine water fowl. A survey conducted between September 14 and 15, 1974 found 408\ncormorants, 66 penguins, 23 ducks, and 84 seagulls dead because of heavy oiling between Punta Piedra and Punta Anegada. By February 1975 it is estimated that 3000 to 4000 birds may have been killed. Furthermore, additional ecological damage was heavily noted in the littoral zone, where rich populations of mussels as well as populations of limpets and starfish were found to be heavily oil coated. The value of these organisms as food for other species was highly evident by the number of shell middens prevalent behind many of the local habitations. Thus, the spill had a negative impact to several food chains of that region. Moreover, two years after the spill, the geographic area still appeared devastated and there were no signs of any regrowth of vegetation.\n\nThe overall economic damage to Chile would be considered minor. However, The main economic stress that was prevalent was the tremendous difficulty to arrange logistical and manpower resources, and the cost of implementing a plan and managing the clean up. The estimated clean up cost of the spill ranged from 25 million to as high as 50 million US dollars.\n\n\n"}
{"id": "31550300", "url": "https://en.wikipedia.org/wiki?curid=31550300", "title": "Walter Reed Tropical Medicine Course", "text": "Walter Reed Tropical Medicine Course\n\nThe Walter Reed Tropical Medicine Course (now called 'Operational Clinical Infectious Disease' Course ) at Walter Reed Army Institute of Research (WRAIR) is one of the many Tropical Medicine Training Courses available in the US and worldwide (see Tropical medicine). It is an intensive 5-day course and a 3-day short course, created to familiarize students with tropical diseases they may encounter overseas. The course is open to Physicians, Physician Assistants, Nurse Practitioners, ESO, 18D, or other medical personnel. The course is run by the military and designed for personnel of the US Military (Army, Marines, Navy, Air Force) and several other US government agencies.\n\nThe Walter Reed Army Institute of Research (WRAIR) was established in 1893 as the Army Medical School by War Department General Orders No. 51, dated 24 Jan 1893. The Tropical Medicine course began in that school in July 1941 while BG G. Russell Callendar was Commandant. At that time, the course ran for 30 days and consisted of didactic and laboratory sessions similar to today’s course. Very much like the majority of the 52 years that this course was offered at WRAIR, that first course presented continuing education to approximately 30 officers. \n\nOver the next fifty years, the course changed names and length but remained dedicated to teaching continuing tropical medicine education to military officers. In 1954, the Institute began the “Advanced Military Preventive Medicine Course” which carried on the tropical medicine education tradition begun in 1941. This course was eventually supplanted by the “Global Medicine Course” in December 1966. During the next four and a half years, the Global Medicine course was offered on 8 separate occasions. This 12-week course was divided into 4 weeks of “Epidemiology and Applied Biostatistics”, 3 weeks of “Ecology and Disease”, and 5 weeks of “Tropical Medicine”. In February 1972, the Global Medicine course was split into a 5-week course called “Military Medical Ecology” and a 6-week course called the “Tropical Medicine Course”. The first Tropical Medicine Course was offered in July and August 1972 and was attended by 11 medical officers and 4 clinical clerks. The course endured until 1993 and was the only surviving remnant of the original Army Medical School educational offerings.\n\nIn 1991, the Institute celebrated its 50-year tradition of tropical medicine education. In memory of his significant contributions to tropical medicine education, the Institute established “The Colonel George W. Hunter III Certificate”. This award was to be presented yearly to no more than two course lecturers who embody excellence and longevity as senior lecturers in the course. The first two recipients of the award were Dr. Jay P. Sanford (former University President and Dean of the Medical School at the Uniformed Services University of the Health Sciences) and Dr. Theodore E. Woodward (Emeritus Professor of Medicine at the University Of Maryland School of Medicine). A special presentation of this award was made to Colonel Richard N. Miller, former Tropical Medicine Course Director, for his significant contributions to this course and its organization over the previous 12 years. Due to the frequency of the course changing from once a year to once a quarter, the presentation frequency of the Hunter Certificate was changed to no more than 4 per year (one per course iteration). The 50 year celebration also was particularly honored by the commencement address given by Dr. Theodore E. Woodward who attended the first course in 1941.\n\nDue to operational needs of the Special Operations Command and the newly formed Africa Command, in 2010 it was decided to resurrect the former 6 week course at WRAIR and convert it to a targeted short course that would provide a broader spectrum of individuals with the knowledge they need to combat international infectious disease threats. Operational demands upon the U.S. military facing wars on multiple fronts in areas affected with tropical disease identified a vital need for an intensely focused short course to familiarize medical personnel at all educational levels in tropical medicine.\n\nIn 2014 the Walter Reed Tropical Medicine Course was renamed 'Operational Clinical Infectious Disease' (OCID) Course.\n\n\nHunter’s Tropical Medicine: \"Hunter’s Tropical Medicine grew out of a World War II Army Medical School tropical and military medicine course taught at the Walter Reed Army Medical Center in Washington, D.C. The first edition, entitled a Manual of Tropical Medicine, was published in 1945 by three of the course instructors, Colonel Thomas T. Mackie, Major George W. Hunter III, and Captain C. Brooke Worth. A second edition was published by the same authors in 1954. Colonel Hunter was joined by co-authors from the Louisiana State University School of Medicine for the third, fourth, and fifth editions, published in 1960, 1966, and 1976, respectively. George Hunter’s contribution was acknowledged by adding his name to the book title in the sixth edition, edited in 1984.\"\n\n\n"}
{"id": "34277287", "url": "https://en.wikipedia.org/wiki?curid=34277287", "title": "Wood Mackenzie", "text": "Wood Mackenzie\n\nWood Mackenzie, also known as \"WoodMac\" is a global energy, chemicals, renewables, metals and mining research and consultancy group with an international reputation for supplying comprehensive data, written analysis and consultancy advice. In 2015, the company was acquired by Verisk Analytics, an American data analytics and risk assessment firm, in a deal valued at $2.8 billion.\nIt is based in Edinburgh, Scotland, though it has over 25 offices worldwide. The company's energy business was founded in 1973, when it started reviewing the North Sea oilfields. Between 2007 and 2014, Wood Mackenzie acquired coal specialists Hill & Associates in the US, Barlow Jonker in Australia, and Brook Hunt, the UK-based metals analysts.\n\nSince 2015 a host of companies have become part of Wood Mackenzie including PSG, a petroleum database service; PCI, the specialist chemicals analysis group; Greentech Media, providing analysis of the solar market and MAKE, providing analysis of global wind power.\n\nThe company was sold in 1987 to County Natwest by Trustee Savings Bank. In a 2003 report the company claimed that 25 of the largest oil companies had \"destroyed value in 50 of the 80 countries they have invested in over the past six years\". In a report from the company in 2005 it was discovered that Bangladesh will fail to meet natural gas demands by 2023. In 2007 a report from the company on the Alberta oil sands said \"higher royalties will have the biggest impact on high-cost, low-margin projects\" this sparked what has been described as a \"furious debate\" within the oil industry.\n\nDetailed History:\n\n1973: North Sea Service, Wood Mackenzie's first industry research offering, was launched\n\n1986: Wood Mackenzie purchased by Hill Samuel, the UK merchant bank, with successive ownership by NatWest and Bankers Trust\n\n1999: Bankers Trust (including Wood Mackenzie) purchased by Deutsche Bank\n\n2001: Management and employee buy-out from Deutsche Bank backed by the Bank of Scotland\n\n2005: Refinancing deal, involving Candover Partners acquiring equity through the exit of Bank of Scotland\n\n2007: Creation of coal team via acquisitions of Hill & Associates and Barlow Jonker. Divestment of Life Sciences division\n\n2008: Creation of metals team via acquisition of Brook Hunt\n\n2009: Refinancing deal, involving Charterhouse Capital Partners acquiring the majority equity stake in the business through the exit of Candover\n\n2010: Acquisition of PetroPlan Refinery Model\n\n2012: Refinancing deal, involving Hellman & Friedman acquiring majority shareholding whilst Charterhouse Capital Partners retained a minority stake.\n\n2015: Acquisition of Deloitte's Petroleum Services Group (PSG), a specialised oil and gas information business including a widely respected database of Exploration & Production (E&P) information\n\n2015: Acquisition of Wood Mackenzie by Verisk Analytics\n\n2015: Acquisition of Infield Systems, a leading independent provider of business intelligence, analysis, and research to the oil, gas, and associated marine industries\n\n2015: Acquisition of PCI Chemicals, an industry-leading chemicals business that offers integrated data and subscriptions research in the chemicals, fibers, films, and plastics sectors\n\n2016: Acquisition of Greentech Media, an industry-leading information services provider for the next-generation electricity and renewables sector\n\n2017: Acquisition of MAKE providing cutting-edge knowledge of the global onshore and offshore wind power market\n\n•Neal Anderson, President \n\n•Mark Brinin, Finance Director\n\n"}
