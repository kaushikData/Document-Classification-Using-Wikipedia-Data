{"id": "36504218", "url": "https://en.wikipedia.org/wiki?curid=36504218", "title": "Alan Pears", "text": "Alan Pears\n\nAlan Pears, AM, is an environmental consultant, and a pioneer of energy efficiency policy in Australia since the late 1970s.\n\nIn the 1980s, Pears worked on the Home Energy Advisory Service, star-rating appliance energy labels, and mandatory home insulation regulations, while with the Victorian Government's Energy Information Centre. He has been an environmental consultant since 1991, involved in energy/environmental rating and regulation of buildings, green building developments, and efficient appliance development. He is an Adjunct Professor at RMIT University, and writes a regular column for \"ReNew magazine\". \n\nAlan Pears was made a Member of the Order of Australia in 2009.\n\n"}
{"id": "35306667", "url": "https://en.wikipedia.org/wiki?curid=35306667", "title": "Allegheny Airlines Flight 736", "text": "Allegheny Airlines Flight 736\n\nAllegheny Airlines Flight 736 was a regularly scheduled flight that crashed while attempting to land at Bradford Regional Airport in Bradford, Pennsylvania on December 24, 1968. Twenty of the 47 occupants on board were killed.\n\nFlight 736 was operated using a Convair CV-580 (registration number \"N5802\"). The aircraft was originally certificated as a Convair CV-440 on March 4, 1957, but was modified to include upgraded turbine engines and propellers and re-certificated as a CV-580 on May 25, 1965. At the time of the accident, the aircraft had accumulated a total of 29,173 flight hours.\n\nFlight 736 took off from Detroit, Michigan bound for Washington D.C., with intermediate stops in Erie, Bradford, and Harrisburg, Pennsylvania. The flight was uneventful until the aircraft began its approach to Bradford.\n\nAt 19:57 Eastern Standard Time, Flight 736 reported passing a DME east of Erie, and was instructed to descend to 4,000 feet and cleared for an approach to Bradford. At the same time, Flight 736 was advised that, at the last hourly observation, Bradford weather reported light snow showers, blowing snow, and one mile visibility. At 20:05, Bradford approach controllers advised Flight 736 again of light snow, blowing snow, and one mile visibility. Flight 736 acknowledged each weather advisory. At 20:08, Flight 736 reported that they were turning inbound for runway 32 and they were informed the wind was 290 degrees at 15 knots. This was Flight 736's last transmission before the crash.\n\nFlight 736 continued to descend until the aircraft first struck trees approximately 2-1/8 nautical miles short of the runway. The airplane cut through the trees for a further 800 feet, extensively damaging the engines, until it impacted the ground. The fuselage came to rest inverted with the top portion (roof) of the fuselage torn away. Most of the survivors were seated toward the back of the aircraft and were wearing seatbelts, remaining in their seats until the plane came to a halt. Passengers who were able to free themselves then worked to free the remaining survivors. Due to a blinding snowstorm, rescue workers were initially prevented from reaching the crash site. Survivors started a bonfire outside of the aircraft using wood, seat cushions, and luggage to keep warm and attract rescuers to the crash site.\n\nThe National Transportation Safety Board investigated the accident. After reviewing the cockpit voice recorder and flight data recorder, the NTSB determined that the flight was flawless until the flight reached a DME 2.9 miles from the airport. The minimum altitude at the 2.9 mile DME was 2,900 feet above mean sea level; after reaching the DME, Flight 736 was authorized only to descend to the minimum descent altitude (MDA) of 2,543 feet. However, the aircraft continued to descend below the MDA until it contacted the trees at an altitude 462 feet below the MDA. The NTSB concluded that neither pilot was aware of the aircraft's proximity to the ground until initial contact with trees, at which point the first officer yelled, \"Pull up.\" The pilots then attempted to pull up, but one second later the right wing struck a large tree, causing the aircraft to roll over.\n\nThe investigators focused on determining why the pilots allowed the aircraft to descend below the MDA. Crew fatigue, instrument or autopilot malfunction, and other mechanical issues were all considered and dismissed as unlikely causes. The NTSB concluded that the most likely explanation was that the descent was unintentional and resulted from both pilots looking away from the instruments, having instead focused on making visual contact with the runway. Based on cockpit conversations and weather data, the NTSB also concluded that while light snow and 1-mile visibility was reported to the pilots from the Bradford airport, weather conditions and visibility may have been substantially worse in the final approach area.\n\nThe NTSB determined the probable cause to be \"the continuation of the descent from the final approach fix through the Minimum Descent Altitude and into obstructing terrain at a time when both flight crewmembers were looking outside the aircraft in an attempt to establish visual reference to the ground. Contributing factors were the minimal visual references available at night on the approaches to the Bradford Regional Airport; a small but critical navigational error during the later stages of the approach; and a rapid change in visibility conditions that was not known to the crew.\"\n\nLess than two weeks after Flight 736 crashed, Allegheny Airlines Flight 737 also went down on approach to Bradford Airport. Both aircraft were approaching the same runway but in opposite directions at the time of the crashes. Shortly after Flight 737's crash, Allegheny Airlines self-imposed new rules for landings at airports. The rules required visibility of 1,000 feet up and three miles out for any airport without instrument landing systems.\n\nAlmost 42 years after the crash, one of its survivors announced she was paying for a plaque for the flight's victims, survivors, and rescuers.\n\n"}
{"id": "4404565", "url": "https://en.wikipedia.org/wiki?curid=4404565", "title": "American Airlines Flight 383 (1965)", "text": "American Airlines Flight 383 (1965)\n\nAmerican Airlines Flight 383 was a nonstop flight from New York City to Cincinnati on November 8, 1965. The aircraft was a Boeing 727, with 57 passengers, and 5 crew on board. The aircraft crashed on final approach to the Cincinnati/Northern Kentucky International Airport located in Hebron, Kentucky, United States. Only three passengers and one flight attendant survived the crash.\n\nThe aircraft involved was a Boeing 727–100 (registration number \"N1996\"), serial number 18901. The Boeing 727 was delivered to American Airlines on June 29, 1965, and had operated a total of 938 hours at the time of the accident.\n\nThe flight was delayed for 20 minutes in New York. Until the landing attempt, the flight from New York to Cincinnati was uneventful. At 18:45 Eastern Standard Time, the crew contacted the airline via ARINC company radio to report a 19:05 estimated time of arrival at Cincinnati. The weather was fine near the airport except for thunder clouds developing northwest of the airport across the Ohio River valley. At 18:57, Flight 383 was cleared by the approach controller for a visual approach to Cincinnati's runway 18, and was advised of precipitation just west of the airport. The aircraft approached the airport from the southeast and turned to a northerly heading to cross the Ohio River. It turned west after crossing to the northern shore of the Ohio River, intending to make a final turn to southeast after crossing the Ohio River (which runs from northwest to southeast) again to the southern shore of the river. After that final turn, the aircraft would line up with the runway 18 of the airport to make the final approach.\n\nAt 18:58, the approach controller transferred Flight 383 to the Cincinnati tower frequency. At 18:59, Flight 383 received clearance from the tower controller to land on runway 18.\n\nThe aircraft flew into thick cloud and thunderstorm after flying into the northwest of the airport. It descended more rapidly than it should have, without either pilot in the cockpit noticing. The airport is situated at an elevation of and the aircraft had descended to the level of above the airport while it was still about northeast of the airport. It descended to just 3 ft (per altimeter) above the airport while it was about 3 nm north of the airport. Its correct altitude should have been just below at that time. It continued its descent into the Ohio River valley while crossing the river back to the southern shore. When it made its last turn to the southeast to line up with the runway, it flew into the wooded slopes of the valley 3 km north of the runway threshold in poor visibility, at an altitude of 225' below the runway's elevation. It then exploded and was engulfed in flames.\n\nOf the 62 people on board the aircraft, only four people (one flight attendant and three passengers) survived. One of the survivors was Israel Horowitz, an American record producer.\n\nThe Civil Aeronautics Board (CAB) investigated the accident. CAB investigators concluded that the aircraft was working normally and fully under the control of the pilots at the time of the crash. The aircraft was not equipped with a cockpit voice recorder. The flight data recorder showed the aircraft descended through in the last 42 seconds before impact, a normal rate of descent for the landing phase of operation. The CAB determined that the probable cause of the accident was the pilots' failure to properly monitor their altitude during a visual approach into deteriorating weather conditions.\n\nIt was later believed that the following factors might have contributed to the crash:\n\nThe estate of Samuel Creasy, one of the passengers who died aboard Flight 383, sued American Airlines for wrongful death. American Airlines responded by filing a third-party complaint against the Federal Aviation Administration and the Weather Bureau, in an attempt to shift liability for the crash to meteorologists and air traffic controllers for failure to warn the pilots of inclement weather or revoke the visual approach clearance. American Airlines also alleged that the accident was due to a downdraft rather than pilot error. A jury found American liable for the accident and awarded Creasy's family $175,000 plus funeral expenses, a decision that was upheld on appeal to the Fifth Circuit Court of Appeals.\n\nTwo years after the crash of Flight 383, TWA Flight 128 crashed on the same hill while on approach to Cincinnati under poor visibility conditions.\n\nOn 13 December 2017, Toni Ketchell, the surviving crew member died.\n\n\n"}
{"id": "1114239", "url": "https://en.wikipedia.org/wiki?curid=1114239", "title": "Arsenic trisulfide", "text": "Arsenic trisulfide\n\nArsenic trisulfide is the inorganic compound with the formula AsS. It is a bright yellow solid that is insoluble in water. It also occurs as the mineral orpiment (Latin: auripigment), which has been used as a pigment called King's yellow. It is produced in the analysis of arsenic compounds. It is a group V/VI, intrinsic p-type semiconductor and exhibits photo-induced phase-change properties. The other principal arsenic sulfide is AsS, a red-orange solid known as the mineral realgar.\n\nAsS occurs both in crystalline and amorphous forms. Both forms feature polymeric structures consisting of trigonal pyramidal As(III) centres linked by sulfide centres. The sulfide centres are two-fold coordinated to two arsenic atoms. In the crystalline form, the compound adopts a ruffled sheet structure. The bonding between the sheets consists of van der Waals forces. The crystalline form is usually found in geological samples. Amorphous AsS does not possess a layered structure but is more highly cross-linked. Like other glasses, there is no medium or long-range order, but the first co-ordination sphere is well defined. AsS is a good glass former and exhibits a wide glass-forming region in its phase diagram.\n\nIt is a semiconductor, with a direct band gap of 2.7 eV. The wide band gap makes it transparent to infrared between 620 nm and 11 µm.\n\nAmorphous AsS is obtained via the fusion of the elements at 390 °C. Rapid cooling of the reaction melt gives a glass. The reaction can be represented with the chemical equation:\n\nAsS forms when aqueous solutions containing As(III) are treated with HS. Arsenic was in the past analyzed and assayed by this reaction, which results in the precipitation of AsS, which is then weighed. AsS can even be precipitated in 6M HCl. AsS is so insoluble that it is not toxic. \n\nUpon heating in a vacuum, polymeric AsS \"cracks\" to give a mixture of molecular species, including molecular AsS. AsS adopts the adamantane geometry, like that observed for PO and AsO. When a film of this material is exposed to an external energy source such as thermal energy (via thermal annealing ), electromagnetic radiation (i.e. UV lamps, lasers, electron beams)), AsS polymerizes:\n\nAsS characteristically dissolves upon treatment with aqueous solutions containing sulfide ions. The dissolved arsenic species is the pyramidal trianion AsS:\n\nAsS is the anhydride of the hypothetical thioarsenous acid, As(SH). Upon treatment with polysulfide ions, AsS dissolves to give a variety of species containing both S-S and As-S bonds. One derivative is SAs-S, a ring that contains an exocyclic sulfido center attached to the As atom. AsS also dissolves in strongly alkaline solutions to give a mixture of AsS and AsO.\n\n\"Roasting\" AsS in air gives volatile, toxic derivatives, this conversion being one of the hazards associated with the refining of heavy metal ores:\n\nDue to its high refractive index of 2.45 and its large Knoop hardness compared to organic photoresists, AsS has been investigated for the fabrication of photonic crystals with a full-photonic band-gap. Advances in laser patterning techniques such as three-dimensional direct laser writing (3-D DLW) and chemical wet-etching chemistry, has allowed this material to be used as a photoresist to fabricate 3-D nanostructures.\n\nAsS has been investigated for use as a high resolution photoreist material since the early 1970s, using aqueous etchants. Although these aqueous etchants allowed for low-aspect ratio 2-D structures to be fabricated, they do not allow for the etching of high aspect ratio structures with 3-D periodicity. Certain organic reagents, used in organic solvents, permit the high-etch selectivity required to produce high-aspect ratio structures with 3-D periodicity.\n\nAsS and AsS have been investigated as treatments for acute promyelocytic leukemia (APL).\n\nArsenic trisulfide manufactured into amorphous form is used as a chalcogenide glass for infrared optics. It is transparent between 620 nm and 11 µm. The arsenic trisulfide glass is more resistant to oxidation than crystalline arsenic trisulfide, which minimizes toxicity concerns. It can be also used as an acousto-optic material.\n\nArsenic trisulfide was used for the distinctive eight-sided conical nose over the infra-red seeker of the de Havilland Firestreak missile.\n\nThe ancient Egyptians reportedly used orpiment, natural or synthetic, as a pigment in artistry and cosmetics.\n\nArsenic trisulfide is also used as a tanning agent. It was formerly used with indigo dye for the production of pencil blue, which allowed dark blue hues to be added to fabric via pencil or brush.\n\nPrecipitation of arsenic trisulfide is used as an analytical test for presence of dissimilatory arsenic-reducing bacteria (DARB).\n\nAsS is so insoluble that its toxicity is low. Aged samples can contain substantial amounts of arsenic oxides, which are soluble and therefore highly toxic.\n\nOrpiment is found in volcanic environments, often together with other arsenic sulfides, mainly realgar. It is sometimes found in low-temperature hydrothermal veins, together with some other sulfide and sulfosalt minerals.\n\n\n"}
{"id": "14255531", "url": "https://en.wikipedia.org/wiki?curid=14255531", "title": "Automotive thermoelectric generator", "text": "Automotive thermoelectric generator\n\nAn automotive thermoelectric generator (ATEG) is a device that converts some of the waste heat of an internal combustion engine (IC) into electricity using the Seebeck Effect. A typical ATEG consists of four main elements: A hot-side heat exchanger, a cold-side heat exchanger, thermoelectric materials, and a compression assembly system. ATEGs can convert waste heat from an engine's coolant or exhaust into electricity. By reclaiming this otherwise lost energy, ATEGs decrease fuel consumed by the electric generator load on the engine. However, the cost of the unit and the extra fuel consumed due to its weight must be also considered.\n\nIn ATEGs, thermoelectric materials are packed between the hot-side and the cold-side heat exchangers. The thermoelectric materials are made up of p-type and n-type semiconductors, while the heat exchangers are metal plates with high thermal conductivity.\n\nThe temperature difference between the two surfaces of the thermoelectric module(s) generates electricity using the Seebeck Effect. When hot exhaust from the engine passes through an exhaust ATEG, the charge carriers of the semiconductors within the generator diffuse from the hot-side heat exchanger to the cold-side exchanger. The build-up of charge carriers results in a net charge, producing an electrostatic potential while the heat transfer drives a current. With exhaust temperatures of 700 °C (~1300 °F) or more, the temperature difference between exhaust gas on the hot side and coolant on the cold side is several hundred degrees. This temperature difference is capable of generating 500-750 W of electricity.\n\nThe compression assembly system aims to decrease the thermal contact resistance between the thermoelectric module and the heat exchanger surfaces. In coolant-based ATEGs, the cold side heat exchanger uses engine coolant as the cooling fluid, while in exhaust-based ATEGs, the cold-side heat exchanger uses ambient air as the cooling fluid.\n\nCurrently, ATEGs are about 5% efficient. However, advancements in thin-film and quantum well technologies could increase efficiency up to 15% in the future.\n\nThe efficiency of an ATEG is governed by the thermoelectric conversion efficiency of the materials and the thermal efficiency of the two heat exchangers. The ATEG efficiency can be expressed as:\n\nWhere:\n\nThe primary goal of ATEGs is to reduce fuel consumption and therefore reduce operating costs of a vehicle or help the vehicle comply with fuel efficiency standards. Forty percent of an IC engine’s energy is lost through exhaust gas heat. Implementing ATEGs in diesel engines seems to be more challenging compared to gasoline engines due to lower exhaust temperature and higher mass-flow rates . This is the reason because the majority of ATEG prototypes have been developed for gasoline engines but there have been some ATEGs designed for light-duty and especially for heavy-duty diesel engines.\n\nBy converting the lost heat into electricity, ATEGs decrease fuel consumption by reducing the electric generator load on the engine. ATEGs allow the automobile to generate electricity from the engine's thermal energy rather than using mechanical energy to power an electric generator. Since the electricity is generated from waste heat that would otherwise be released into the environment, the engine burns less fuel to power the vehicle's electrical components, such as the headlights. Therefore, the automobile releases fewer emissions.\n\nDecreased fuel consumption also results in increased fuel economy. Replacing the conventional electric generator with ATEGs could ultimately increase the fuel economy by up to 4%.\n\nThe ATEG’s ability to generate electricity without moving parts is an advantage over mechanical electric generators alternatives. In addition, it has been stated that for low power engine conditions, ATEGs may be able to harvest more net energy than electric turbogenerators.\n\nThe greatest challenge to the scaling of ATEGs from prototyping to production has been the cost of the underlying thermoelectric materials. Since the early-2000s, many research agencies and institutions poured large sums of money into advancing the efficiency of thermoelectric materials. While efficiency improvements were made in materials such as the half heuslers and skutterudites, like their predecessors bismuth telluride and lead telluride, the cost of these materials has proven prohibitive for large-scale manufacturing. Recent advances by some researchers and companies in low-cost thermoelectric materials have resulted in significant commercial promise for ATEGs, most notably the low-cost production of tetrahedrite by Michigan State University and its commercialization by US-based Alphabet Energy with General Motors.\n\nLike any new component on an automobile, the use of an ATEG presents new engineering problems to consider, as well. However, given an ATEG's relatively low impact on the use of an automobile, its challenges are not as considerable as other new automotive technologies. For instance, since exhaust has to flow through the ATEG’s heat exchanger, kinetic energy from the gas is lost, causing increased pumping losses. This is referred to as back pressure, which reduces the engine’s performance. This can be accounted for by down-sizing the muffler, resulting in net-zero or even negative total back-pressure on the engine, as Faurecia and other companies have shown.\n\nTo make the ATEG’s efficiency more consistent, coolant is usually used on the cold-side heat exchanger rather than ambient air so that the temperature difference will be the same on both hot and cold days. This may increase the radiator’s size since piping must be extended to the exhaust manifold, and it may add to the radiator’s load because there is more heat being transferred to the coolant. Proper thermal design does not require an up-sized cooling system.\n\nATEGs are made primarily of metal and, therefore, contribute more weight to the vehicle. The added weight causes the engine to work harder, resulting in lower gas mileage. Most automotive efficiency improvement studies of ATEGs, however, have resulted in a net positive efficiency gain even when considering the weight of the device.\n\nAlthough the Seebeck effect was discovered in 1821, the use of thermoelectric power generators was restricted mainly to military and space applications until the second half of the twentieth century. This restriction was caused by the low conversion efficiency of thermoelectric materials at that time.\n\nIn 1963, the first ATEG was built and reported by Neild et al. In 1988, Birkholz et al. published the results of their work in collaboration with Porsche. These results described an exhaust-based ATEG which integrated iron-based thermoelectric materials between a carbon steel hot-side heat exchanger and an aluminium cold-side heat exchanger. This ATEG could produce tens of watts out of a Porsche 944 exhaust system.\n\nIn the early 1990s, Hi-Z Inc designed an ATEG which could produce 1 kW from a diesel truck exhaust system. The company in the following years introduced other designs for diesel trucks as well as military vehicles\n\nIn the late 1990s, Nissan Motors published the results of testing its ATEG which utilized SiGe thermoelectric materials. Nissan ATEG produced 35.6 W in testing conditions similar to the running conditions of a 3.0 L gasoline engine in hill-climb mode at 60.0 km/h.\n\nSince the early-2000s, nearly every major automaker and exhaust supplier has experimented or studied thermoelectric generators, and companies including General Motors, BMW, Daimler, Ford, Renault, Honda, Toyota, Hyundai, Valeo, Boysen, Faurecia, Tenneco, Denso, Gentherm Inc., Alphabet Energy, and numerous others have built and tested prototypes.\n\nIn January 2012, Car and Driver magazine named an ATEG created by a team led by Amerigon (now Gentherm Incorporated) one of the 10 \"most promising\" technologies.\n\n"}
{"id": "31087799", "url": "https://en.wikipedia.org/wiki?curid=31087799", "title": "Autorité de sûreté nucléaire", "text": "Autorité de sûreté nucléaire\n\nThe Autorité de sûreté nucléaire (, ASN) is an independent French administrative authority set up by law 2006-686 of 13 June 2006 concerning nuclear transparency and safety. It has replaced the General Direction for Nuclear Safety and Radioprotection.\nIts task, on behalf of the State, is to regulate nuclear safety and radiation protection in order to protect workers, patients, the public and the environment from the risks involved in nuclear activities. It also contributes to informing the citizens.\n\nFrom 2006 to 2012, the president of the ASN was André-Claude Lacoste who was also a founding member and had been chairman of the International Nuclear Regulators' Association (INRA) and the Western European Nuclear Regulators' Association (WENRA). He was also the chairman of the Commission on Safety Standards (CSS) of the IAEA.\n\nSince November 2012, the president of the ASN is Pierre-Franck Chevet.\n\nEarly during the Fukushima Daiichi nuclear disaster, the ASN stated that they believed the events unfolding should be rated a 5 or even a 6 on the International Nuclear Event Scale when the Japan Atomic Energy Agency had listed them a day before as a level 4 event. Later on they said they believed the situation had surpassed that of a level 5 event and moved to a level 6. Their opinion was shared by the Finnish nuclear authorities.\n\nDuring 2015 and 2016 major nuclear safety issues arose, following a discovery at the Flamanville Nuclear Power Plant, concerning about 400 large steel forgings manufactured by Areva's Le Creusot Forge since 1965 that had carbon-content irregularities that weakened the steel. A widespread programme of reactor checks was started leading to 20 of France's 58 reactors being offline in October 2016. These steel quality concerns may prevent the regulator giving the life extensions from 40 to 50 years, that had been assumed by energy planners, for many reactors. Le Creusot Forge had maintained concealed files which had now been disclosed to ASN following new management by Areva. ASN characterised the current situation in January 2017 as worrying. In April 2017 ASN published the requirements for forging to resume at Le Creusot Forge, which has been out of operation since December 2015.\nIn 2018 ASN introduced stricter monitoring of component production in response to the problems found at Creusot Forge.\n\n"}
{"id": "44520102", "url": "https://en.wikipedia.org/wiki?curid=44520102", "title": "Biodegradable athletic footwear", "text": "Biodegradable athletic footwear\n\nBiodegradable athletic footwear is athletic footwear that uses biodegradable materials with the ability to compost at the end-of-life phase. Such materials include natural biodegradable polymers, synthetic biodegradable polymers, and biodegradable blends. The use of biodegradable materials is a long-term solution to landfill pollution that can significantly help protect the natural environment by replacing the synthetic, non-biodegradable polymers found in athletic footwear.\n\nThe United States athletic shoe market is a $13 billion-per-year dollar industry that sells more than 350 million pairs of athletic shoes annually. The global footwear consumption has nearly doubled every twenty years, from 2.5 billion pairs in 1950 to more than 19 billion pairs of shoes in 2005. The increase in demand for athletic shoe products have progressively decreased the useful lives of shoes as a result of the rapid market changes and new consumer trends. A shorter life cycle of athletic footwear has begun to create non-degradable waste in landfills due to synthetic and other non-biodegradable materials used in production. The considerable growth in industrial production and consumption has made the athletic footwear industry face the environmental challenge of generated end-of-life waste.\n\nThe athletic shoe midsole is one of the main contributors that lead to a generation of end-of-life waste because it is composed of polymeric foams based on ethylene-vinyl acetate (EVA). EVA is a polyolefin copolymer of ethylene and vinyl acetate that provides durability and flexibility, making it the most commonly used material found in athletic shoe midsoles. Although the synthetic polymer is a useful material for the athletic shoe industry, it has become an environmental concern because of its poor biodegradability. EVA goes through an anaerobic decomposition process called thermal degradation that often occurs in landfills resulting in releases of volatile organic compounds (VOCs) into the air. VOCs \"contribute to the formation of tropospheric ozone, which is harmful to humans and plant life.\" Thermal degradation of EVA is temperature dependent and occurs in two stages; in the first stage acetic acid is lost, followed by the degradation of the unsaturated polyethylene polymer.\nThe environmental impacts of athletic shoe degradation in landfills \"are inextricably connected to the nature of the materials.\" The production of many petroleum-based products, such as EVA, used to manufacture athletic shoes result in serious environmental pollution of groundwater and rivers when disposed into landfills. When disposed in landfills, athletic footwear can take up to thousands of years to naturally degrade. EVA athletic shoe midsoles can be kept in contact with moist soil for a period of 12 years and experience little to no evidence of bio deterioration.\n\nAlthough there are some that are taking initiatives to produce environmentally friendly athletic footwear, most of the footwear industry’s response to this increasing problem of end-of-life shoe waste has been negligible. In order to reduce post-consumer waste and improve environmental properties of athletic shoes, biodegradable materials can help to replace synthetic polymers such as ethylene vinyl acetate with the ability to compost at the end-of-life phase.\n\n\"Biodegradation is a chemical degradation of materials provoked by the action of microorganisms such as bacteria, fungi, and algae.\" Although there are many materials categorized as biodegradable, there has been an increasing interest of biodegradable polymers that can lead to waste management options for polymers in the environment. These biodegradable polymers can be broken down into three categories: natural biodegradable polymer, synthetic biodegradable polymer, and biodegradable blends.\n\nNatural biodegradable polymers are formed in nature during growth cycles of all organisms. When searching for natural fibers to replace synthetic materials in athletic shoes, the major natural biodegradable polymer that offers the most potential are polysaccharides. Starch is a polysaccharide that is useful because it readily degrades into harmless products when placed in contact with soil microorganisms.\n\nStarch is not often used alone as a plastic material because of its brittle nature, but is commonly used as a biodegradation additive. Many plasticizers use starch-glycerol-water to modify starch’s brittle nature. Biodegradation of this blend was tested and was found that by the second day the degraded carbon had already attained about 100% of the initial carbon of the sample.\n\nAliphatic polyesters are a diverse family of synthetic polymers of which are biocompatible, biodegradable, and non-toxic. Specifically, poly (lactic acid) has low melt strength and low viscosity properties that are similar to EVA midsoles in athletic shoes. Poly (lactic acid) (PLA) is part of the poly (amide-enamine) group and can go through thermoplastic and foaming processes. Along with its good mechanical properties, its popularity is based on the non-toxic products that it becomes when it decomposes through hydrolytic degradation. Hydrolytic degradation of PLA generates the monomer lactic acid, which is metabolized via the tri-carboxylic acid cycle and eliminated as carbon dioxide.\nMost synthetic polymers are resistant to microbial attack because of their physical and chemical properties. However, they can become biodegradable when introducing natural polymers such as starch. Natural polymers introduce ester groups that attach to the backbone of non-biodegradable polymers, making them more susceptible to degradation. Due to biodegradable polymers having limited properties; blending synthetic polymers can bring economic advantages and superior properties.\nAlthough total elimination of post-consumer waste is not encouraged by any current change-causing agent due to the enormous change in infrastructure that the elimination of waste requires and the consequent lack of profitability for those agents, proactive approaches to reduce the enormous amount of waste that 350 million pairs of athletic shoes create can make a difference in the environment. Biodegradable materials, such as biodegradable polymers, are a viable solution to aid in avoiding the end-of-life athletic footwear waste consumption. The major advantage of introducing biodegradable polymers to athletic footwear is the ability to compost with other organic wastes for it to become useful soil attendant products.\n\nAn alternative short-term approach to end-of-life management is recycling activities in the footwear industry. One major shoe manufacture, Nike Inc., created Reuse-A-Shoe program that involves recycling discarded athletic shoes by grinding and shredding the shoes to produce a material called Nike Grind, which can be used in surfacing for tennis and basketball playgrounds or running tracks. Currently, the Reuse-A-Shoe program recycles approximately 125,000 pairs of shoes per year in the United States. \n\nRecycling and composting are two major proposed solutions to end-of-life management. However, the use of biodegradable materials is a long-term solution that can significantly help protect the natural environment by replacing synthetic, non-biodegradable polymers found in athletic footwear. \n"}
{"id": "49714181", "url": "https://en.wikipedia.org/wiki?curid=49714181", "title": "Bis(trifluoromethyl)peroxide", "text": "Bis(trifluoromethyl)peroxide\n\nBis(trifluoromethyl)peroxide(BTP) is a fairly unknown fluorocarbon derivative first produced by Frédéric Swarts. It has recently been discovered that it is a good initiator for the polymerization of unsaturated ethylene-like molecules. It produces good quality polymers which are quite stable. This property is the reason an economical synthesis is sought for BTP.This chemical is unusual in the fact that unlike many peroxides, bis(trifluoromethyl)peroxide is a gas, is nonexplosive and has good thermal stability.\n\nBis(trifluoromethyl)peroxide was first created in trace elements by an electrolysis reaction using aqueous solutions containing trifluoroacetate ion. This was one of the byproducts Frédéric Swarts got when performing trifluoromethylation reactions.[1] Later it was discovered that Bis(trifluoromethyl)peroxide had some unusual properties. This began a search for a more economical production of Bis(trifluoromethyl)peroxide. This was at first done by Porter and Cady. This reaction had a conversion rate of around 20-30% at normal pressure. They increased the conversion using an autoclave. This increased the yield to around 90% which helped procuring the chemical.\n\nPresent methods of the synthesis of bis(trifluoromethyl)peroxide involves the reaction of carbonyl fluoride and chlorine trifluoride at 0-300 °C.\nAn example of this reaction is the reaction of carbonyl fluoride and chlorine trifluoride in the presence of akali metal fluorides or bifluorides at 100-250 °C. This example is quite insensitive to variations in temperature.\nExamples of the synthesis are:\n\n2CFO + ClF → CFOOCF + ClF\n\n6CFO + 2ClF → 3CFOOCF + Cl\n\nBis(trifluoromethyl)peroxide can be isolated and purified by well-recognised procedures. In the mixture used to synthesize the compound chlorine monofluoride and chlorine trifluoride may still be present. These compounds are highly reactive and hazardous and are preferebly deactivated as soon as possible. The deactivation is carried out by adding anhydrous calcium chloride to the mixture. The deactivated mixture is scrubbed with water and diluted caustically to remove the chlorine and residual carbonyl. The remaining if dried to complete the purification of bis(trifluoromethyl)peroxide.\n\nBis(trifluoromethyl)peroxide is originally a gas, therefore this compound may be inhaled and distributed by the bloodstream. This distribution causes BTP to reach the organs. In the organs, the compound can enter the cells via the cellmembrane. This is validated by the Lipinski's rule of five:\n\n\nIn mammals there are pathways for the metabolism of peroxides. These pathways make use of different enzymes, but of the same sort, peroxidases. The phase 1 metabolism of peroxides is an overall peroxidase-catalyzed reaction. For bis(trifluoromethyl)peroxide this will be the following reaction:\n\nPeroxidase + C₂F₆O₂ → 2CF₃O⁻\n\nThe peroxidase will then undergo two sequential electron transfers to come back into its beginning form.\n\nSimulation of the toxicity of Bis(trifluoromethyl)peroxide has shown that organic peroxides can cause peripheral and centrilobular zonal hepatic necrosis, increased liver weight and hepatic enzymes and fatty changes in hepatocytes. This occurs in both humans and experimental animals. The toxicity of peroxides is thought to be caused by the formation of reactive oxygen species (ROS) which are involved in lipid peroxidation further oxidative cellular damage.\n\nOrganic peroxides are often industrially used as oxidising agents. Exposure to such agents, for instance in the reported case of humans that were exposed to methyl ethyl ketone peroxide (MEKP), has been shown to cause peripheral zonal necrosis, increased hepatic enzyme levels and atypical pseudo-ductular proliferation at doses between 50 and 100 mL.\n\nPast animal studies have shown good correlations between organic peroxide damage in human case reports and test animals. 28-day repeat- dose studies of 1,1-bis (tert-butyldioxy)-3,3,5-trimethylcyclohexane and dicumyl peroxide [MHLW 2001a and b] in rats showed liver weight increase, periportal fatty changes and centrilobular hypertrophy of hepatocytes.\n\nA proposed mechanism for the toxicity of organic peroxides involves damage via formation of ROS, which is mediated by cytochrome P450. This then leads to lipid peroxidation of the membranes of the hepatocytes, alkylation of cellular macromolecules (reduced glutathione, altered calcium homeostasis. Identification of carboxyl, peroxyl, hydroxyl and alkoxyl radicals in rest rats give plausibility to the involvement of an oxidative system.\n\n28 Day oral rat repeat dose studies with organic peroxides have shown alterations in rat kidneys in the form of histopathologic lesions. Further research of how this relates to bis(trifluoromethyl)peroxide is needed to draw conclusions from this, however.\n"}
{"id": "8555460", "url": "https://en.wikipedia.org/wiki?curid=8555460", "title": "Boeing EC-135", "text": "Boeing EC-135\n\nThe Boeing EC-135 was a command and control version of the Boeing C-135 Stratolifter. During the Cold War, the EC-135 was best known for being modified to perform the Looking Glass mission where one EC-135 was always airborne 24 hours a day to serve as flying command post for the Strategic Air Command in the event of nuclear war. Various other EC-135 aircraft sat on airborne and ground alert throughout the Cold War, with the last EC-135C being retired in 1998. The EC-135N variant served as a tracking aircraft for the Apollo program.\n\nOfficially known as \"Operation Looking Glass\", at least 11 EC-135C command post aircraft were provided to the Commander in Chief, Strategic Air Command (CINCSAC), and were based at various locations throughout the United States and worldwide. Operations began in 1961 with the 34th Air Refueling Squadron at Offutt Air Force Base, Nebraska, initially using EC-135As (converted from KC-135As) until the dedicated EC-135Cs entered service in 1963 and 1964. The EC-135Cs were converted from the 17 new build KC-135Bs that were accepted by SAC but never entered squadron service as tankers. Other Offutt-based units included the 38th Strategic Reconnaissance Squadron (1966–1970), the 2d Airborne Command and Control Squadron (1970–1994), and the 7th Airborne Command and Control Squadron (1994–1998). Other units operating the Looking Glass mission included the following:\n\n\nOther EC-135 aircraft (including EC-135A, G, and L models) supporting the Looking Glass missions (communications relay and Minuteman airborne launch control centers) were flown by the 906th Air Refueling Squadron at Minot Air Force Base, North Dakota (1963–1970), the 70th Air Refueling Squadron at Grissom AFB (1975–1993), and the 301st Air Refueling Squadron at Lockbourne Air Force Base, Ohio (1963–1970). All aircraft have been retired or repurposed.\n\nThe United States nuclear strategy depends on its ability to command, control, and communicate with its nuclear forces under all conditions. An essential element of that ability is Looking Glass; its crew and staff ensure there is always an aircraft ready to direct bombers and missiles from the air should ground-based command centers be destroyed or rendered inoperable. Looking Glass is intended to guarantee that U.S. strategic forces will act only in the manner dictated by the President. It took the nickname \"Looking Glass\" because the mission mirrored ground-based command, control, and communications centers.\n\nThe Strategic Air Command (SAC) began the Looking Glass mission on February 3, 1961 and Looking Glass aircraft were continuously airborne 24 hours a day for over 29 years, accumulating more than 281,000 accident-free flying hours. On July 24, 1990, \"The Glass\" ceased continuous airborne alert, but remained on ground or airborne alert 24 hours a day. The EC-135A flew the Command Post mission until EC-135C were delivered starting in 1963. The aircraft were delivered to Offutt AFB and as well as one aircraft to each of the Stateside Numbered Air Force Headquarters - Second Air Force at Barksdale AFB, LA; Eighth Air Force at Westover AFB, MA; and Fifteenth Air Force at March AFB CA. EC-135s flew all the missions except one, on March 4, 1980, when an E-4B was tested on an operational mission, flying a double sortie as the replacement aircraft could not launch due to weather. About a week after the flight, Washington deleted the funds for additional E-4 aircraft.\n\nOn June 1, 1992, SAC was inactivated and replaced by the United States Strategic Command, which now controls the Looking Glass. On October 1, 1998, the Navy's E-6 Mercury TACAMO replaced the USAF's EC-135C in the Looking Glass mission. One former Looking Glass aircraft remains in service as a WC-135C Constant Phoenix.\n\n\"Airborne Launch Control Centers\" (ALCC—pronounced \"Al-see\") provided a survivable launch capability for the United States Air Force's LGM-30 Minuteman Intercontinental Ballistic Missile (ICBM) force by utilizing the Airborne Launch Control System (ALCS) on board that is operated by an airborne missileer crew. Historically, from 1967-1998, the ALCC mission was performed by United States Air Force Boeing EC-135 command post aircraft. This included EC-135A, EC-135C, EC-135G, and EC-135L aircraft.\n\nIn the late 1960s and early 1970s, ALCS crews belonged to the 44th Strategic Missile Wing (SMW) at Ellsworth AFB, SD and 91st SMW at Minot AFB, ND. ALCS equipment was installed on various Boeing EC-135 variants to include the EC-135A, EC-135C, EC-135G, and for a short while on the EC-135L.\n\nStarting in 1970, there were only two SAC squadrons that operated ALCS capable aircraft. This included the 2nd Airborne Command and Control Squadron (ACCS) operating EC-135C aircraft out of Offutt AFB, NE and the 4th ACCS operating EC-135A, EC-135C, and EC-135G aircraft out of Ellsworth AFB, SD. All three variants of these EC-135A/C/G aircraft had ALCS equipment installed on board.\n\nThe 4th ACCS was the workhorse of ALCS operations. Three dedicated Airborne Launch Control Centers (ALCC) were on ground alert around-the-clock providing ALCS coverage for five of the six Minuteman ICBM Wings. These dedicated ALCCs were mostly EC-135A aircraft but could also have been EC-135C or EC-135G aircraft depending on availability. ALCC No. 1 was on ground alert at Ellsworth AFB, SD and during a wartime scenario would have taken off and orbited between the Minuteman Wings at Ellsworth AFB, SD and F.E. Warren AFB, WY providing ALCS assistance if needed. ALCCs No. 2 and No. 3 were routinely on forward deployed ground alert at Minot AFB, ND. During a wartime scenario, ALCC No. 3 would have orbited between the Minuteman ICBM Wings at Minot AFB and Grand Forks AFB, both in North Dakota, providing ALCS assistance if needed. ALCC No. 2 was dedicated to orbiting near the Minuteman ICBM Wing at Malmstrom AFB, MT providing ALCS assistance if needed. The 4th ACCS also maintained an EC-135C or EC-135G on ground alert at Ellsworth AFB, SD as the West Auxiliary Airborne Command Post (WESTAUXCP) as a backup to SAC’s \"Looking Glass\" Airborne Command Post (ABNCP) as well as a radio relay link between the Looking Glass and ALCCs when airborne. Although equipped with ALCS, the WESTAUXCP did not have a dedicated Minuteman ICBM wing to provide ALCS assistance to.\n\nThe 2nd ACCS was another major player in ALCS operations. The primary mission of the 2nd ACCS was to fly the SAC ABNCP \"Looking Glass\" aircraft in continuous airborne operations. However, due to its proximity in orbiting over the central United States, the airborne Looking Glass provided ALCS coverage for the Minuteman ICBM Wing located at Whiteman AFB, MO. Not only did Whiteman AFB have Minuteman II ICBMs, but it also had ERCS configured Minuteman missiles on alert. The 2nd ACCS also had an additional EC-135C on ground alert at Offutt AFB, NE as the EASTAUXCP, providing backup to the airborne Looking Glass, radio relay capability, and a means for the Commander in Chief of SAC to escape an enemy nuclear attack. Although the EASTAUXCP was ALCS capable, it did not have a dedicated ALCS mission.\n\nOperation Silk Purse program provided four EC-135H command post aircraft to the Commander, U.S. European Command (USEUCOM), which were based at RAF Mildenhall in the United Kingdom. Flown by the 10th Airborne Command and Control Squadron 1970-91. Onboard secure/non-secure communications and avionics equipment was maintained by the 513th Avionics Maintenance Squadron and the 2147th Communications Squadron. Aircraft S/Ns 61-0282, 285, 286 and 291.\n\nOperation Scope Light provided five EC-135C/HJ/P command post aircraft to the Commander in Chief, U.S. Atlantic Command (CINCLANT), which were based at Langley AFB, VA. Operated by the 6th Airborne Command and Control Squadron 1972-92.\n\nOperation Blue Eagle provided five EC-135J/P command post aircraft to the Commander in Chief, U.S. Pacific Command (USCINCPAC), which were based at Hickam AFB, HI. Operated by the 9th Airborne Command and Control Squadron 1969-92. Communications, secure/unsecure voice and teletype, handled by the 1957th Communications Group, Hickam AFB, HI (1969-1992)\n\n\"Upkeep\" was the call sign for the EC135 flying in southeast Asia during 1969 to 1971, based out of Hickam AFB Hawaii. It was under the direction of PACAF of which 5th AF in Fuchu AS, Tokyo \nJapan handled their voice communications both unsecure and secure.\n<1956 Comm Gp USAF 1969 to 1971>\n\nBlue Eagle was formed in 1965 and started 24/7 operation in October 1965 and continued until disbanded in 1992.\n\nOperation Nightwatch provided three EC-135J command post aircraft to the President of the United States which were based at Andrews AFB, MD. All three aircraft were transferred to other ABNCP missions.\n\nNightwatch was initiated in the mid-1960s utilizing the three EC-135J aircraft, modified from KC-135Bs, as command post aircraft. The three Nightwatch aircraft were ready to fly the President and the National Command Authority (NCA) out of Washington in the event of a nuclear attack. The E-4 aircraft (a modified Boeing 747-200) came on line with the Nightwatch program in 1974 replacing the EC-135s on this mission.\n\nThe 310th Airlift Squadron, part of the 6th Air Mobility Wing at MacDill AFB, Florida, operated two NKC-135s that were reconfigured as EC-135Y aircraft from 1989 to 2003 as executive transport and command & control platforms to support the Commander, United States Central Command. These aircraft have since been replaced with three C-37A Gulfstream V aircraft.\n\nThe Advanced Range Instrumentation Aircraft are EC-135Bs, modified C-135B cargo aircraft and EC-18B (former American Airlines 707-320) passenger aircraft that provided tracking and telemetry information to support the US space program in the late 1960s and early 1970s.\n\nDuring the early 1960s, NASA and the Department of Defense (DoD) needed a very mobile tracking and telemetry platform to support the Apollo space program and other unmanned space flight operations. In a joint project, NASA and the DoD contracted with the McDonnell Douglas and the Bendix Corporations to modify eight Boeing C-135 Stratolifter cargo aircraft into EC-135N Apollo / Range Instrumentation Aircraft (A/RIA). Equipped with a steerable seven-foot antenna dish in its distinctive \"Droop Snoot\" or \"Snoopy Nose\", the EC-135N A/RIA became operational in January 1968, and was often known as the \"Jimmy Durante\" of the Air Force. The Air Force Eastern Test Range (AFETR) at Patrick AFB, Florida, maintained and operated the A/RIA until the end of the Apollo program in 1972, when the USAF renamed it the Advanced Range Instrumentation Aircraft (ARIA).\n\nSince Patrick AFB was located on the Atlantic Ocean, salt water and salt air-induced corrosion issues and associated aircraft maintenance challenges were problematic for the ARIA while based there. Transferred to the 4950th Test Wing at Wright-Patterson AFB, Ohio, in December 1975 as part of an overall consolidation of large test and evaluation aircraft, the ARIA fleet underwent numerous conversions, including a re-engining that changed the EC-135N to the EC-135E. In 1994, the ARIA fleet relocated again to Edwards AFB, California, as part of the 412th Test Wing. However, taskings for the ARIA dwindled because of high costs and improved satellite technology, and the USAF transferred the aircraft to other programs such as E-8 J-STARS.\n\nOver its thirty-two year career, the ARIA supported the United States space program, gathered telemetry, verified international treaties, and supported cruise missile, ballistic missile defense tests, and the Space Shuttle.\n\n\n\n\n\n"}
{"id": "2470340", "url": "https://en.wikipedia.org/wiki?curid=2470340", "title": "Burgess reagent", "text": "Burgess reagent\n\nThe Burgess reagent (methyl \"N\"-(triethylammoniumsulfonyl)carbamate) is a mild and selective dehydrating reagent often used in organic chemistry. It was developed in the laboratory of Edward M. Burgess at Georgia Tech.\n\nThe Burgess reagent is used to convert secondary and tertiary alcohols with an adjacent proton into alkenes. Dehydration of primary alcohols does not work well. The reagent is soluble in common organic solvents and alcohol dehydration takes place with syn elimination through an intramolecular elimination reaction. The Burgess reagent is a carbamate and an inner salt. A general mechanism is shown below.\n\nThe reagent is prepared from chlorosulfonylisocyanate by reaction with subsequent treatment with methanol and triethylamine in benzene:\n"}
{"id": "18952336", "url": "https://en.wikipedia.org/wiki?curid=18952336", "title": "Caatinga", "text": "Caatinga\n\nCaatinga () is a type of desert vegetation, and an ecoregion characterized by this vegetation in interior northeastern Brazil. The name \"Caatinga\" is a Tupi word meaning \"white forest\" or \"white vegetation\" (\"caa\" = forest, vegetation, \"tinga\" = white).\n\nCaatinga is a xeric shrubland and thorn forest, which consists primarily of small, thorny trees that shed their leaves seasonally. Cacti, thick-stemmed plants, thorny brush, and arid-adapted grasses make up the ground layer. Many annual plants grow, flower, and die during the brief rainy season.\n\nCaatinga falls entirely within earth's tropical zone and is one of 6 major ecoregions of Brazil, including the Amazon Basin, Pantanal, Cerrado, Caatinga, Atlantic Forest, and Pampas. It covers 850,000 km², nearly 10% of Brazil's territory. It is home to 26 million people and more than 2000 species of vascular plants, fishes, reptiles, amphibians, birds, and mammals.\n\nCaatinga covers the interior portion of northeastern Brazil bordering the Atlantic seaboard (save for a fringe of Atlantic Forest). It is located between 3°S 45°W and 17°S 35°W, extending across eight states of Brazil: Piauí, Ceará, Rio Grande do Norte, Paraíba, Pernambuco, Alagoas, Sergipe, Bahia, and parts of Minas Gerais, as well the southeasternmost point of Rio de Janeiro in Cabo Frio. The Caatinga includes several enclaves of humid tropical forest, known as the Caatinga enclaves moist forests.\nThe Caatinga is bounded by the Maranhão Babaçu forests to the northwest, the Atlantic dry forests and the Cerrado savannas to the west and southwest, the humid Atlantic forests along the Atlantic coast to the east, and by the Atlantic Ocean to the north and northeast.\n\nThe Caatinga comprises 850,000 km², about 10% of the surface area of Brazil. By comparison, it is over nine times the surface area of Portugal, whence came its early European settlers, and 20% larger than the U.S. state of Texas.\n\nThe Caatinga has only two distinguishable seasons. These are the winter, when it is hot and dry, and the summer when it is very hot and rainy. During the dry winter periods there is no foliage or undergrowth. The vegetation is very dry and the roots begin to protrude through the surface of the stony soil. They do this in order to absorb water before it is evaporated. All leaves fall off the trees to reduce transpiration, thus lessening the amount of water that is lost in the dry season. During the peak periods of drought the Caatinga's soil can reach temperatures of up to 60 °C. With all the foliage and undergrowth dead during the drought periods and all the trees having no leaves the Caatinga has a yellow-grey, desert-like look.\n\nThe Caatinga is very dry place in Brazil, with frequent droughts. The drought usually ends in December or January, when the rainy season starts. Immediately after the first rains, the grey, desert-like landscape starts to transform and becomes completely green within a few days. Small plants start growing in the now moist soil and trees grow back their leaves. At this time, the rivers that were mostly dry during the past 6 or 7 months, start to fill up and the streams begin to flow again.\n\nThe Caatinga is poorly represented in the Brazilian Conservation Area network, with only 1% in Integral Protection Conservation Areas and 6% in Sustainable Use Conservation Areas.\n\nCaatinga harbors a unique biota, with thousands of endemic species.\nCaatinga contains over 1,000 vascular plant species in addition to 187 bees, 240 fish species, 167 reptiles and amphibians, 516 birds, and 148 mammal species, with endemism levels varying from 9 percent in birds to 57 percent in fishes.\n\nThe caatinga does not correspond to a single type of vegetation, but is a broad mosaic of types. Towards the coast, the caatinga is replaced by remnants of the Atlantic Forest (Mata Atlântica); inland, the caatinga merges with no clear limits into the cerrado (see CPD Site SA21). Interspersed with the caatinga are low mountains with uplands that are much more humid, containing elements (\"brejos de altitude\") of the Atlantic and Amazonian forests, with trees 30–35 m tall.\n\nGeneral characteristics of the caatinga elements include total loss of leaves during the dry season, small and firm (xeric) leaves, intense branching of the trees from the base (giving them a shrubby appearance) and the presence of succulent and crassulaceous species (Romariz 1974). Most authors recognize two main types of caatinga: dry caatinga (\"sertão\") located in the interior and more humid caatinga (\"agreste\") toward the coast. Eiten (1983) divided the caatinga into the following eight categories:\n\n1. Caatinga forest, or low (8–10 m) xerophytic deciduous tropical broadleaved forest, with closed canopies, and the trees having a ground coverage over 60%. This robust formation occurs where there is sufficient rain and the soil is deep enough.\n\n2. Arborescent caatinga, with the shrubby subcanopy not closed, and tree coverage 10-60%.\n\n3. Arborescent-shrubby closed caatinga, or low xerophytic deciduous open tropical broadleaved forest with closed scrub, where the tree coverage is 10-60%. This is the most common form of undisturbed caatinga, sometimes called \"carrasco\".\n\n4. Arborescent-shrubby open caatinga, with the total ground coverage of trees, shrubs, cacti, bromeliads, etc. between 10-60%.\n\n5. Shrubby closed caatinga, or xerophytic deciduous or semi-deciduous closed tropical broadleaved scrub; the thoroughly deciduous scrub is more common.\n\n6. Shrubby open caatinga, or xerophytic open tropical scrub, which can be composed of deciduous broadleaved species, cacti and bromeliads, or mixtures of the same. Coverage varies between 10-60%. Common throughout the caatinga on very shallow soil or rocky outcrops.\n\n7. Caatinga savanna or xerophytic short-graminose tropical savanna with deciduous broadleaved scrub; this formation is usually called \"seridó\".\n\n8. Rocky caatinga savanna or xerophytic sparse tropical scrub, in which both scrub and graminose elements have ground coverage of less than 10%. This formation occurs on pavements and outcrops of massive rock, with the plants interspersed in cracks and hollows.\n\nThe Caatinga is home to nearly 50 endemic species of birds, including Lear's macaw \"(Anodorhynchus leari)\", Spix's macaw \"(Cyanopsitta spixii)\", moustached woodcreeper \"(Xiphocolaptes falcirostris)\", Caatinga parakeet,\nCaatinga antwren, Sao Francisco black tyrant and Caatinga cacholote.\n\nEndemic mammal species include:\n\n\nBased on radiocarbon dating of potsherds, proponents of historical ecology such as William Denevan and William Balee have suggested that large sections of the Caatinga region may be of anthropogenic origin. Over 1000 years ago, native peoples may have unintentionally created the environment of the modern-day Caatinga through constant slash-and-burn agriculture, thereby stymying plant succession and preventing major rainforests from growing within the region.\n\nPeople use many plant species from the Caatinga region. Palms are very important to the economy in northeast Brazil. People from this area are greatly dependent on extraction from babassu, carnaúba, tucúm and macaúba, from which lauric and oleic oils are made from. Many trees are also used for lumber in this area, including these species: \"Anadenanthera macrocarpa\", \"Ziziphus joazeiro\", \"Amburana cearensis\", \"Astronium fraxinifolium\", \"Astronium urundeuva\", \"Handroanthus impetiginosus\", \"Tabebuia caraiba\", and \"Schinopsis brasiliensis\", \"Cedrela odorata\", \"Dalbergia variabilis\", \"Didymopanax morototoni\" and \"Pithecellobium polycephalum\". Some plants are also used for medical purposes.\n\nMeliponiculture is also a well-developed and traditional activity in the region. One of the most productive species \"Melipona subnitida\", known locally as jandaíra, produces up to 6 liters a year, resulting in economic profit for the population.\n\nAround 26 million people live in the Caatinga region, and are regarded as belonging to the poorest inhabitants of Brazil. A very large part of the population depends on agricultural or forest industries for over half of their income. There are few drinkable water sources, and harvesting is difficult because of the irregular rainfall.\n\nAlong São Francisco River, the Caatinga has very fertile soil. Inhabitants plant fruits in the fertile soil to process and eat, sell and export. The irrigated farms along the São Francisco River in the municipalities of Petrolina and Juazeiro are currently exporting grapes, papayas and melons.\n\nSome regions are being irrigated, most notably the São Francisco River. While this is very good news for some farmers, it has also had serious consequences for people who have always depended on the natural flow of the river. Big dams have brought an end to the high tides in the rainy season, which used to spread fertile mud over the fields creating a rich ground that could be used for agriculture during the dry season. Salinization of the soil is becoming a threat since large areas of the land are irrigated with saline water, thus sterilizing the soil.\n\nHaving and using all these resources has some negatives. Intensive agriculture, along with excessive grazing by cattle and goats, is affecting the population structure of some important plant and animal species. Deforesting for industrial uses like fuel and charcoal destroys the vegetation. The combination of drought and misuse of the land is becoming a major threat. Harvesting of the caraiba woodland for lumber has reduced its size. This reduction may have contributed to the endangerment of the Spix's Macaw.\n\n\n\n\n"}
{"id": "6863", "url": "https://en.wikipedia.org/wiki?curid=6863", "title": "Compression ratio", "text": "Compression ratio\n\nThe static compression ratio of an internal combustion engine or external combustion engine is a value that represents the ratio of the volume of its combustion chamber from its largest capacity to its smallest capacity. It is a fundamental specification for many common combustion engines.\n\nIn a piston engine, it is the ratio between the volume of the cylinder and combustion chamber when the piston is at the bottom of its stroke, and the volume of the combustion chamber when the piston is at the top of its stroke.\n\nFor example, a cylinder and its combustion chamber with the piston at the bottom of its stroke may contain 1000 cc of air (900 cc in the cylinder plus 100 cc in the combustion chamber). When the piston has moved up to the top of its stroke inside the cylinder, and the remaining volume inside the head or combustion chamber has been reduced to 100 cc, then the compression ratio would be proportionally described as 1000:100, or with fractional reduction, a 10:1 compression ratio.\n\nA high compression ratio is desirable because it allows an engine to extract more mechanical energy from a given mass of air-fuel mixture due to its higher thermal efficiency. This occurs because internal combustion engines are heat engines, and higher efficiency is created because higher compression ratios permit the same combustion temperature to be reached with less fuel, while giving a longer expansion cycle, creating more mechanical power output and lowering the exhaust temperature. It may be more helpful to think of it as an \"expansion ratio\", since more expansion reduces the temperature of the exhaust gases, and therefore the energy wasted to the atmosphere. Diesel engines actually have a higher peak combustion temperature than petrol engines, but the greater expansion means they reject less heat in their cooler exhaust.\n\nHigher compression ratios will however make gasoline engines subject to engine knocking (also known as detonation) if lower octane-rated fuel is used. This can reduce efficiency or damage the engine if knock sensors are not present to modify the ignition timing.\n\nOn the other hand, Diesel engines operate on the principle of compression ignition, so that a fuel which resists autoignition will cause late ignition, which will also lead to engine knock.\n\nStatic compression ratio (formula_1) is calculated by the formula\n\nWhere:\n\nformula_3 can be estimated by the cylinder volume formula\n\nWhere:\n\nBecause of the complex shape of formula_4 it is usually measured directly. This is often done by filling the cylinder with liquid and then measuring the volume of the used liquid.\n\nThe compression ratio in a gasoline (petrol)-powered engine will usually not be much higher than 10:1 due to potential engine knocking (detonation) and not lower than 6:1. Some production automotive engines built for high performance from 1955–1972, used high-octane leaded gasoline or '5 star' to allow compression ratios as high as 13.0:1.\n\nA technique used to prevent the onset of knock is the high \"swirl\" engine that forces the intake charge to adopt a fast circular rotation in the cylinder during compression that provides quicker and more complete combustion. It is possible to manufacture gasoline engines with compression ratios of over 11:1 that can use 87 (MON + RON)/2 (octane rating) fuel with the addition of variable valve timing and knock sensors to delay ignition timing. Such engines may not produce their full rated power using 87 octane gasoline under all circumstances, due to the delayed ignition timing. Direct fuel injection, which can inject fuel only at the time of fuel ignition (similar to a diesel engine), is another recent development which also allows for higher compression ratios on gasoline engines.\n\nThe compression ratio can be as high as 14:1 (2014 Ferrari 458 Speciale) in engines with a 'ping' or 'knock' sensor and an electronic control unit. In 1981, Jaguar released a cylinder head that allowed up to 14:1 compression; but settled for 12.5:1 in production cars. The cylinder head design was known as the \"May Fireball\" head; it was developed by a Swiss engineer Michael May.\n\nIn 2012, Mazda released new petrol engines under the brand name SkyActiv with a 14:1 compression ratio (U.S. models have a 13:1 compression ratio to allow for 87 AKI octane), to be used in all Mazda vehicles by 2015.\n\nIn a turbocharged or supercharged gasoline engine, the CR is customarily built at 10.5:1 or lower. This is due to the turbocharger/supercharger already having compressed the air before it enters the cylinders. Port fuel injected engines typically run lower boost than direct fuel injected engines because port fuel injection allows the air/fuel mixture to be heated together which leads to detonation. Conversely, directly injected engines can run higher boost because heated air will not detonate without a fuel being present. In this instance fuel is injected as late as 60 degrees before top dead center to avoid heating the mixture to the point of compression ignition.\n\nMotorcycle racing engines can use compression ratios as high as 14.7:1, and it is common to find motorcycles with compression ratios above 12.0:1 designed for 86 or 87 octane fuel. F1 engines come closer to 17:1, which is critical for maximizing volumetric/fuel efficiency at around 18,000 RPM.\n\nEthanol and methanol can take significantly higher compression ratios than gasoline. Racing engines burning methanol and ethanol fuel often incorporate a CR of 14.5-16:1.\n\nThe CR may be higher in engines running exclusively on LPG or CNG, due to the higher octane rating of these fuels.\n\nThere is no spark plug in an auto-ignition diesel engine; the heat of compression raises the temperature of the air in the cylinder sufficiently to ignite the diesel when this is injected into the cylinder; after the compression stroke. The CR will customarily exceed 14:1 and ratios over 22:1 are common. The appropriate compression ratio depends on the design of the cylinder head. The figure is usually between 14:1 and 23:1 for direct injection engines, and between 18:1 and 23:1 for indirect injection.And also in crdi injection.\n\nA compression ratio of 6.5 or lower is desired for operation on kerosene. The petrol-paraffin engine version of the Ferguson TE20 tractor had a compression ratio of 4.5:1 for operation on tractor vaporising oil with an octane rating between 55 and 70.\n\nMeasuring the compression pressure of an engine, with a pressure gauge connected to the spark plug opening, gives an indication of the engine's state and quality. There is, however, no formula to calculate compression ratio based on cylinder pressure.\n\nIf the nominal compression ratio of an engine is given, the pre-ignition cylinder pressure can be estimated using the following relationship:\n\nwhere formula_11 is the cylinder pressure at bottom dead center which is usually at 1 atm, formula_12 is the compression ratio, and formula_13 is the specific heat ratio for the working fluid, which is about 1.4 for air, and 1.3 for methane-air mixture.\n\nFor example, if an engine running on gasoline has a compression ratio of 10:1, the cylinder pressure at top dead center is\n\nThis figure, however, will also depend on cam (i.e. valve) timing. Generally, cylinder pressure for common automotive designs should at least equal 10 bar, or, roughly estimated in pounds per square inch (psi) as between 15 and 20 times the compression ratio, or in this case between 150 psi and 200 psi, depending on cam timing. Purpose-built racing engines, stationary engines etc. will return figures outside this range.\n\nFactors including late intake valve closure (relatively speaking for camshaft profiles outside of typical production-car range, but not necessarily into the realm of competition engines) can produce a misleadingly low figure from this test. Excessive connecting rod clearance, combined with extremely high oil pump output (rare but not impossible) can sling enough oil to coat the cylinder walls with sufficient oil to facilitate reasonable piston ring sealing. In engines with compromised ring seals, this can artificially give a misleadingly high compression figure.\n\nThis phenomenon can actually be used to some slight advantage. If a compression test does give a low figure, and it has been determined it is not due to intake valve closure/camshaft characteristics, then one can differentiate between the cause being valve/seat seal issues and ring seal by squirting engine oil into the spark plug orifice, in a quantity sufficient to disperse across the piston crown and the circumference of the top ring land, and thereby affect the mentioned seal. If a second compression test is performed shortly thereafter, and the new reading is much higher, it would be the ring seal that is problematic, whereas if the compression test pressure observed remains low, it is a valve sealing (or more rarely head gasket, or breakthrough piston or, rarer still, cylinder-wall damage) issue.\n\nIf there is a significant (greater than 10%) difference between cylinders, that may be an indication that valves or cylinder head gaskets are leaking, piston rings are worn, or that the block is cracked.\n\nIf a problem is suspected, then a more comprehensive test using a leak-down tester can locate the leak.\n\nBecause cylinder-bore diameter, piston-stroke length and combustion-chamber volume are almost always constant, the compression ratio for a given engine is almost always constant, until engine wear takes its toll.\n\nOne exception is the experimental Saab Variable Compression engine (SVC). This engine, designed by Saab Automobile, uses a technique that dynamically alters the volume of the combustion chamber (V), which, via the above equation, changes the compression ratio (CR).\n\nThe Atkinson cycle engine was one of the first attempts at variable compression. Since the compression ratio is the ratio between dynamic and static volumes of the combustion chamber, the Atkinson cycle's method of increasing the length of the power stroke compared to the intake stroke ultimately altered the compression ratio at different stages of the cycle.\n\nOn August 15, 2016 Nissan Motor Company announced a new variable compression engine that can choose an optimal compression ratio variably between 8:1 and 14:1. That lets the engine adjust moment by moment to torque demands, always maintaining top efficiency. Nissan says that the turbo-charged, 2-liter, four-cylinder VC-T engine averages 27 percent better fuel economy than the 3.5-liter V6 engine it replaces, with comparable power and torque.\n\nThe calculated compression ratio, as given above, presumes that the cylinder is sealed at the bottom of the stroke, and that the volume compressed is the actual volume.\n\nHowever: intake valve closure (sealing the cylinder) always takes place after BDC, which may cause some of the intake charge to be compressed backwards out of the cylinder by the rising piston at very low speeds; only the percentage of the stroke after intake valve closure is compressed. Intake port tuning and scavenging may allow a greater mass of charge (at a higher than atmospheric pressure) to be trapped in the cylinder than the static volume would suggest ( This \"corrected\" compression ratio is commonly called the \"dynamic compression ratio\".\n\nThis ratio is higher with more conservative (i.e., earlier, soon after BDC) intake cam timing, and lower with more radical (i.e., later, long after BDC) intake cam timing, but always lower than the static or \"nominal\" compression ratio.\n\nThe actual position of the piston can be determined by trigonometry, using the stroke length and the connecting rod length (measured between centers). The absolute cylinder pressure is the result of an exponent of the dynamic compression ratio. This exponent is a polytropic value for the ratio of variable heats for air and similar gases at the temperatures present. This compensates for the temperature rise caused by compression, as well as heat lost to the cylinder. Under ideal (adiabatic) conditions, the exponent would be 1.4, but a lower value, generally between 1.2 and 1.3 is used, since the amount of heat lost will vary among engines based on design, size and materials used, but provides useful results for purposes of comparison. For example, if the static compression ratio is 10:1, and the dynamic compression ratio is 7.5:1, a useful value for cylinder pressure would be (7.5)^1.3 × atmospheric pressure, or 13.7 bar. (× 14.7 psi at sea level = 201.8 psi. The pressure shown on a gauge would be the absolute pressure less atmospheric pressure, or 187.1 psi.)\n\nThe two corrections for dynamic compression ratio affect cylinder pressure in opposite directions, but not in equal strength. An engine with high static compression ratio and late intake valve closure will have a DCR similar to an engine with lower compression but earlier intake valve closure.\n\nAdditionally, the cylinder pressure developed when an engine is running will be higher than that shown in a compression test for several reasons.\n\n\nCompression ratio and overall pressure ratio are interrelated as follows:\nThe reason for this difference is that compression ratio is defined via the volume reduction:\nwhile pressure ratio is defined as the pressure increase:\nIn calculating the pressure ratio, we assume that an adiabatic compression is carried out (i.e. that no heat energy is supplied to the gas being compressed, and that any temperature rise is solely due to the compression). We also assume that air is a perfect gas. With these two assumptions, we can define the relationship between change of volume and change of pressure as follows:\nwhere formula_18 is the ratio of specific heats (air: approximately 1.4).\nThe values in the table above are derived using this formula. Note that in reality the ratio of specific heats changes with temperature and that significant deviations from adiabatic behavior will occur.\n\n\n"}
{"id": "48536780", "url": "https://en.wikipedia.org/wiki?curid=48536780", "title": "Corf (mining)", "text": "Corf (mining)\n\nA corf (pl. corves) also spelt corve (pl. corves) in mining is a wicker basket or a small human powered (in later times in the case of the larger mines, horse drawn) minecart for carrying or transporting coal, ore, etc. Human powered corfs had generally been phased out by the turn of the 20th century, with horse drawn corfs having been mostly replaced by horse drawn or motorised minecarts mounted on rails by the late 1920s. Also similar is a Tram, originally a box on runners, dragged like a sledge.\n\n1350–1400; Middle English from Dutch and German \"Korb\", ultimately borrowed from Latin \"corbis\" basket; cf. \"corbeil\".\n\n"}
{"id": "44431852", "url": "https://en.wikipedia.org/wiki?curid=44431852", "title": "Cyclooctatetraenide anion", "text": "Cyclooctatetraenide anion\n\nIn chemistry, the cyclooctatetraenide anion or cyclooctatetraenide, more precisely cyclooctatetraenediide, is an aromatic species with a formula of [CH] and abbreviated as COT. It is the dianion of cyclooctatetraene. Salts of the cyclooctatetraenide anion can be stable, e.g., Dipotassium cyclooctatetraenide or disodium cyclooctatetraenide. More complex coordination compounds are known as cyclooctatetraenide complexes, such as the actinocenes.\n\nThe structure is a planar symmetric octagon stabilized by resonance, meaning each atom bears a charge of –. The length of the bond between carbon atoms is 1.432 Å. There are 10 π electrons. The structure can serve as a ligand with various metals.\n"}
{"id": "45311435", "url": "https://en.wikipedia.org/wiki?curid=45311435", "title": "Eastern Anatolian deciduous forests", "text": "Eastern Anatolian deciduous forests\n\nThe Eastern Anatolian deciduous forests is a Palearctic ecoregion in the Temperate broadleaf and mixed forest Biome, located in central Turkey.\n"}
{"id": "4411204", "url": "https://en.wikipedia.org/wiki?curid=4411204", "title": "Ecological stoichiometry", "text": "Ecological stoichiometry\n\nEcological stoichiometry (more broadly referred to as Biological stoichiometry) considers how the balance of energy and elements influences living systems. Similar to chemical stoichiometry, ecological stoichiometry is founded on constraints of mass balance as they apply to organisms and their interactions in ecosystems. Specifically, how does the balance of energy and elements affect and how is this balance affected by organisms and their interactions. Concepts of ecological stoichiometry have a long history in ecology with early references to the constraints of mass balance made by Liebig, Lotka, and Redfield. These earlier concepts have been extended to explicitly link the elemental physiology of organisms to their food web interactions and ecosystem function.\nMost work in ecological stoichiometry focuses on the interface between an organism and its resources. This interface, whether it is between plants and their nutrient resources or large herbivores and grasses, is often characterized by dramatic differences in the elemental composition of each part. The difference, or mismatch, between the elemental demands of organisms and the elemental composition of resources leads to an elemental imbalance. Consider termites, which have a tissue carbon:nitrogen ratio (C:N) of about 5 yet consume wood with a C:N ratio of 300-1000. Ecological stoichiometry primarily asks:\n\nElemental imbalances arise for a number of physiological and evolutionary reasons related to the differences in the biological make up of organisms, such as differences in types and amounts of macromolecules, organelles, and tissues. Organisms differ in the flexibility of their biological make up and therefore in the degree to which organisms can maintain a constant chemical composition in the face of variations in their resources. Variations in resources can be related to the types of needed resources, their relative availability in time and space, and how they are acquired. The ability to maintain internal chemical composition despite changes in the chemical composition and availability of resources is referred to as \"stoichiometric homeostasis\". Like the general biological notion of homeostasis, elemental homeostasis refers to the maintenance of elemental composition within some biologically ordered range. Photoautotrophic organisms, such as algae and vascular plants, can exhibit a very wide range of physiological plasticity in elemental composition and thus have relatively weak stoichiometric homeostasis. In contrast, other organisms, such as multicellular animals, have close to strict homeostasis and they can be thought of as having distinct chemical composition. For example, carbon to phosphorus ratios in the suspended organic matter in lakes (i.e., algae, bacteria, and detritus) can vary between 100 and 1000 whereas C:P ratios of \"Daphnia\", a crustacean zooplankton, remain nearly constant at 80:1. The general differences in stoichiometric homeostasis between plants and animals can lead to large and variable elemental imbalances between consumers and resources. \n\nEcological stoichiometry seeks to discover how the chemical content of organisms shapes their ecology. Ecological stoichiometry has been applied to studies of nutrient recycling, resource competition, animal growth, and nutrient limitation patterns in whole ecosystems. The Redfield ratio of the world's oceans is one very famous application of stoichiometric principles to ecology. Ecological stoichiometry also considers phenomena at the sub-cellular level, such as the P-content of a ribosome, as well as phenomena at the whole biosphere level, such as the oxygen content of Earth's atmosphere.\n\nTo date the research framework of ecological stoichiometry stimulated research in various fields of biology, ecology, biochemistry and human health, including human microbiome research, cancer research, food web interactions, population dynamics, ecosystem services, productivity of agricultural crops and honeybee nutrition.\n\n"}
{"id": "14249028", "url": "https://en.wikipedia.org/wiki?curid=14249028", "title": "Electricity sector in Nicaragua", "text": "Electricity sector in Nicaragua\n\nNicaragua is the country in Central America with the lowest electricity generation, as well as the lowest percentage of population with access to electricity. The unbundling and privatization process of the 1990s did not achieve the expected objectives, resulting in very little generation capacity added to the system. This, together with its high dependence on oil for electricity generation (the highest in the region), led to an energy crisis in 2006 from which the country has not fully recovered yet.\n\nRecent figures are available at: http://www.ine.gob.ni/DGE/serieHistorica.html\n\nThe Nicaraguan electricity system comprises the National Interconnected System (SIN), which covers more than 90% of the territory where the population of the country lives (the entire Pacific, Central and North zone of the country). The remaining regions are covered by small isolated generation systems. The SIEPAC project will integrate the electricity network of the country with the rest of the Central American countries, which is expected to improve reliability of supply and reduce costs.\n\nNicaragua is largely dependent on oil for electricity generation: 75% dependence compared to a 43% average for the Central American countries. In 2006, the country had 751.2 MW of nominal installed capacity, of which 74.5% was thermal, 14% hydroelectric and 11.5% geothermal. 70% of the total capacity was in private hands.\n\nGross electricity generation was 3,140 GWh, of which 69% came from traditional thermal sources, 10% from bagasse thermal plants, 10% from hydroelectricity, and 10% from geothermal sources. The remaining 1% corresponds to the electricity generated in the “isolated” systems. The detailed breakdown of generation among the different sources is as follows:\n\"Source\": INE Statistics\n\nAlthough nominal installed capacity has increased by 113 MW since 2001, effective capacity has only increased by 53 MW, remaining as low as 589 MW in 2006. The large difference between nominal and effective capacity is due to the existence of old thermal plants that do not operate properly and that should be either refurbished or replaced.\n\nIn 2006, total electricity sold in Nicaragua increased 5.5%, up to 2,052 GWh, which corresponds to 366kWh annual per capita consumption. The consumption share for the different economic sectors was as follows:\n\n\nMaximum demand has increased in Nicaragua at an annual rate of about 4% since 2001, which has led to a low reserve margin (6% in 2006). Furthermore, demand is expected to increase by 6% per year for the next 10 years, which increases the need for new generation capacity.\n\nIn 2001, only 47% of the population in Nicaragua had access to electricity. The electrification programs developed by the former National Electricity Commission (CNE) with resources from the National Fund for the Development of the Electricity Industry (FODIEN), the Inter-American Development Bank, the World Bank and the Swiss Fund for Rural Electrification (FCOSER), have led to an increase in electricity access to 55% (68% according to the Census estimates, which also consider illegal connections)by 2006. However, this coverage is still among the lowest in the region and well below the 94.6 average for LAC Coverage in the rural areas is below 40%, while in urban areas it reaches 92%.\n\nIn 2004, the National Energy Commission (CNE) developed the National Plan for Rural Electrification (PLANER), which established goals and investment figures for the period 2004-2013. Its objective is to bring power to 90% of the country’s rural areas by the end of 2012. The Rural Electrification Policy was approved in September 2006 as the main guide for implementation of the PLANER.\n\nIn 2003, the average number of interruptions per subscriber was 4 (weighted average for LAC in 2005 was 13), while duration of interruptions per subscriber was 25 hours (weighted average for LAC in 2005 was 14). However, the situation worsened during the energy crisis in 2006, when large sections of the country suffered continuous and lengthy blackouts (See Recent developments below).\n\nIn 2006, distribution losses in Nicaragua were 28.8%, the highest in Central America together with Honduras, whose average was 16.2%. This is one of the most acute problems faced by the sector in Nicaragua, as it leads to very large economic losses. This problem is partially caused by the widespread existence of illegal connections, altered metering systems and low bill collection capacity in certain areas.\n\nThe regulatory entities for the electricity sector in Nicaragua are:\n\n\nThe National Dispatch Center (CNDC) is the operational body in charge of administering the Wholesale Electricity Market (MEN)and the National Interconnected System (SIN).\n\nIn 2006, there were 10 generation companies in the National Interconnected System, eight of which were in private hands. The number and type of plants operated by each company was as follows:\n\"Source\": CEPAL 2007\n\nIn Nicaragua, 100% of the transmission is handled by ENATREL, which is also in charge of the system’s dispatch.\n\nIn Nicaragua, the company Dissur-Disnorte, owned by the Spanish Unión Fenosa, controls 95% of the distribution. Other companies with minor contributions are Bluefields, Wiwilí and ATDER-BL.\n\nThe “Indicative plan for the generation in the electricity sector in Nicaragua, 2003-2014” does not set any target or legal obligation for the development of renewable resources in the country. However, in April 2005, the government approved Law No. 532., the “Law on Promotion of Electricity Generation with Renewable Resources”. This law declared the development and exploitation of renewable resources to be in the national interest and established tax incentives for renewables.\n\nNPR reported in 2015 that Nicaragua was increasing its renewable energy capacity. The report said that renewables generated nearly half the country's electricity, and that this could rise to 80% in the near future. \n\nCurrently, hydroelectric plants account only for 10% of the electricity produced in Nicaragua. The public company Hidrogesa owns and operates the two existing plants (Centroamérica and Santa Bárbara).\nAs a response to the recent (and still unresolved) energy crisis linked to Nicaragua’s overdependence on oil products for the generation of electricity, there are plans for the construction of new hydroelectric plants. In 2006 the Central American Bank for Economic Integration (BCIE) and the Government reached an agreement by which the BCIE will provide US$120 million in the next five years (2007–2012) in order to finance several hydroelectric projects: \n\nIn March 2008 the government of Iran approved a US$230 million credit for the construction of a 70MW hydropower plant by the name of Bodoke on the Tuma River in the northern department of Jinotega. According to press reports the project will be carried out by a state-owned Iranian company with financing from the Iranian Export Bank under an agreement with the Nicaraguan Ministry of Energy and Mines. Micro hydropower also continues to be a popular sustainable energy resource, particularly in isolated rural regions of Nicaragua which are currently not electrified .\n\nThe Tumarín Dam, a gravity dam, is currently under construction on the Río Grande de Matagalpa just upstream of the town of Tumarín in the South Caribbean Coast Autonomous Region, Nicaragua. It is located about east of San Pedro del Norte, where the Río Grande de Matagalpa meets the Tuma River. Preliminary construction (roads, bridges and foundation) began in 2011 and main works are expected to begin in February 2015. Completion is scheduled for 2019. Brazil's Eletrobras will fund the US$1.1 billion under a 20 to 30 year build–operate–transfer (BOT) agreement. The project is being developed by Centrales Hidroelectricas de Nicaragua (CHN). The power station located at the base of the dam will house three 84.33 MW Kaplan turbine-generators for an installed capacity of 253 MW.\n\nNicaragua’s wind potential is still largely unexploited. However, steps are being taken, partially thanks to the new framework created by Law No.532.\n\nIn February 2009, the Wind Consortium Amayo successfully connected its new 40MW Wind Park to the SIN making it the country\"s first operational wind park. During late 2009 - early 2010 the Amayo wind farm was expanded with additional 23MW, total capacity now amounting 60MW. The windfarm comprises 30 turbines type S88 2.1MW, from Suzlon Wind Energy, India.\n\nAmayo is currently the largest operating wind facility in Central America.\n\nNicaragua is a country endowed with large geothermal potential thanks to the presence of volcanoes of the Marribios range along the Pacific Coast. However, the country is still very far from exploiting this natural resource extensively and efficiently. Law No. 443 regulates the exploration and exploitation of geothermal resources.\n\nThe larger of two operating geothermal plants is the Momotombo geothermal project, whose commercial exploitation started in 1983, when the first geothermal unit of 35MW was put in operation. The second unit of 35MW was installed in 1989. However, mismanagement of the exploitation led to declines in output levels down to 10MW. It is expected that with the implementation of a reinjection program and the exploitation of a deeper reservoir, production will increase from the current 20MW to 75 MW.\n\nRam Power, previously Polaris Geothermal, currently operates the 10 MW San Jacinto Tizate geothermal plant, a registered CDM project (see CDM projects in electricity below), with two phases of expansion underway, the first to start operations in the autumn of 2010. The second phase was scheduled to be in operation by December 2012. \n\nSugarcane bagasse feeds 10% of electricity generation in thermal plants in Nicaragua.\n\nUntil the early 1990s, the electricity sector in Nicaragua was characterized by the presence of the State, through the Nicaraguan Energy Institute (INE), in all its activities. Created in 1979, INE had Ministry status and was a vertically integrated state monopoly responsible for planning, regulation, policy making, development and operation of the country’s energy resources. During that decade, the sector faced serious financial and operational problems as a result of the currency devaluation, war, a trade embargo imposed by the United States and the lack of resources for investment in operation and maintenance of the electricity system.\n\nAt the beginning of the 1990s, the government of President Violeta Chamorro started the reform of the electricity sector aiming to ensure efficient demand coverage, to promote economic efficiency and to attract resources for infrastructure expansion. In 1992, INE was allowed, by law, to negotiate contracts and concessions with private investors. The Nicaraguan Electricity Company (ENEL) was created in 1994 as the state company in charge of electricity generation, transmission, distribution, commercialization and coordination of the operations previously assigned to INE. INE kept its planning, policy making, regulatory, and taxation functions.\n\nThe reform process was consolidated in 1998 with Law 272 (Electricity Industry Law - LIE) and Law 271 (INE Reform Law). The reform of the INE led to the creation of the National Energy Commission (CNE), which assumed the policy making and planning responsibilities. Law 272 established the basic principles for the operation of a competitive wholesale market with the participation of private companies. Electricity generation, transmission and distribution were unbundled and companies were prohibited to have interests in more than one of the three activities. ENEL was restructured in four generation companies (Hidrogesa, GEOSA, GECSA and GEMOSA); two distribution companies (DISNORTE and DISSUR), both acquired by Unión Fenosa and then merged into a single company; and one transmission company (ENTRESA, now ENATREL).\n\nThe privatization process that started in 2000 with a public offering of the four generation companies was complicated due both to legal problems and to lack of interest by investors. As a result, ENEL maintained a more relevant role than initially expected. Hidrogesa remained in public hands as the only player in hydroelectric generation while its profits serve to finance the losses of GECSA, which owns the thermal plants that did not attract private interest, and the rural electrification plans in isolated areas.\n\nThe reforms of the 1990s did not achieve their objectives. It had been expected that privatization would bring investment in new generation, but very little capacity was added in the years that followed the reform. Moreover, the generation capacity added in the last decade has been mainly dependent on liquid fuels, making the country more vulnerable to rising oil prices. In addition, as mentioned, distribution losses have remained at very high levels (28%). The reform also aimed at implementing gradual changes in electricity tariffs that would reflect costs, which proved to be politically unfeasible.\n\nWhen oil prices increased from 2002 onwards, the regulator failed to approve electricity tariff increases, because they were expected to have been very unpopular. The financial burden of the higher generation costs was thus passed on to the privatized distribution company, which has, partly as a result, been suffering severe losses.\n\nIn 2006, the electricity sector in Nicaragua suffered a serious crisis, with 4- to 12-hour blackouts that affected virtually the whole country. The distribution company owned by Unión Fenosa, was blamed and the concession was temporarily cancelled by the government, which called for arbitration. This led Union Fenosa to call its MIGA (Multilateral Investment Guarantee Agency) guarantee. The crisis was further aggravated by the inability of INE and CNE to cooperate in a constructive manner. The emergency situation improved in 2007 due to the installation of 60MW of diesel generation capacity financed by Venezuela.\n\nIn January 2007, shortly after President Daniel Ortega took office, a new law created the Ministry of Energy and Mines (MEM), which replaced the CNE. The new Ministry inherited CNE’s responsibilities together with some additional competencies from the INE. Also, in August 2007, an agreement was reached between Unión Fenosa and Nicaragua’s new government. The government committed to pass a law to combat fraud, which will help reduce distribution losses and Unión Fenosa will develop an investment plan for the period up to 2012.\n\nIn 1995, after almost a decade of preliminary studies, the Central American governments, the government of Spain and the Inter-American Development Bank agreed to the execution of the SIEPAC project. This project aims at the electric integration of the region. Feasibility studies showed that the creation of a regional transmission system would be very positive for the region and lead to a reduction in electricity costs and to improvements in the continuity and reliability of supply. In 1996, the six countries (Panama, Honduras, Guatemala, Costa Rica, Nicaragua and El Salvador) signed the Framework Treaty for the Electricity Market in Central America.\n\nThe design of the Regional Electricity Market (MER) was done in 1997 and approved in 2000. MER is an additional market superimposed on the existing six national markets, with a regional regulation, in which the agents authorized by the Regional Operational Body (EOR) carry out international electricity transactions in the region. As for the infrastructure, EPR (\"Empresa Propietaria de la Red S.A.\") is in charge of the design, engineering, and construction of about 1,800 km of 230kV transmission lines. The project is expected to be operational by the end of 2011.\n\nElectricity tariffs in Nicaragua had increased only slightly between 1998 and 2005 (in fact, industrial tariffs decrease in that period). However, in 2006 electricity tariffs experienced a high increase relative to 2005: 12% for residential, 26% for commercial and 23% for industrial tariffs. Average tariffs for each of the sectors were:\n\n\nThese tariffs are not low; they are in fact among the highest in the Central American region. Residential prices are close to the regional average while industrial prices are the highest in the region.\n\nCurrently, there are cross-subsidies in the tariff structure. Medium voltage consumers pay higher tariffs that serve to subsidize lower tariffs for low voltage consumers. Users that consume less than 150 kWh per month receive transfers from the rest of the consumers. The lowest-consumption users (0-50kWh/month) benefit from reductions between 45% and 63% in their average tariff. Consumers above the 50kWh limit also benefit from the subsidy scheme to a smaller extent.\n\nIn 2007, new “emergency” generation (60MW) has been financed by the Venezuelan government. On the other hand, the new hydroelectric projects will receive both public and private financing, while the ongoing Amayo wind development and the new San Jacinto Tizate geothermal plant are privately funded.\n\nEntresa has elaborated a Plan for transmission infrastructure expansion for the period 2007-2016. However, financing has not been ensured for all the projects yet.\n\nIn August 2007, Unión Fenosa committed to elaborate an investment plan for the period up to 2012.\n\nFinancing sources for rural electrification are limited. The National Fund for the Development of the Electricity Industry (FODIEN) receives its resources from the concessions and licenses granted by the Nicaraguan Energy Institute (INE). However, funds have been insufficient. The World Bank (through the PERZA project) and the Swiss government (through FCOSER) have also contributed funds and assistance to advance the objectives of rural electrification in the country.\n\nElectricity generation, transmission and distribution, previously in the hands of state-owned ENEL, were unbundled in 1998. Today, there are 10 generation companies in the National Interconnected system, 8 of which are in private hands. 100% of the hydroelectric capacity is in the hands of the public company Hidrogesa. As for transmission, it is handled solely by state-owned ENATREL, while distribution is 95% controlled by Spanish Unión Fenosa.\n\nThe Ministry of Environment and Natural Resources (MARENA) is the institution in charge of the conservation, protection and sustainable use of the natural resources and the environment.\n\nThe National Climate Change Commission was created in 1999.\n\nOLADE (Latin American Energy Association) estimated that CO emissions from electricity production in 2003 were 1.52 million tons of CO, which corresponds to 39% of total emissions from the energy sector. This high contribution to emissions from electricity production in comparison with other countries in the region is due to the high share of thermal generation.\n\nCurrently (November 2007), there are only two registered CDM projects in the electricity sector in Nicaragua, with overall estimated emission reductions of 336,723 tCOe per year. One of them is the San Jacinto Tizate geothermal project and the other one is the Monte Rosa Bagasse Cogeneration Project\n\nThe Inter-American Development Bank (IDB) has several projects under implementation in the electricity sector in Nicaragua:\n\n\nThe World Bank has currently one Off-grid Rural Electrification (PERZA) project under implementation in Nicaragua. The US$19 million project will receive US$12 million funding from the Bank in the period 2003-2008. The main objective of the project is to support the sustainable provision of electricity services and associated social and economic benefits in selected rural sites in Nicaragua, and strengthen the Government's institutional capacity to implement its national rural electrification strategy.\n\nSeveral countries have provided financial support for the expansion of the transmission network in Nicaragua:\n\n\n\n\n"}
{"id": "833720", "url": "https://en.wikipedia.org/wiki?curid=833720", "title": "Fiber simulation", "text": "Fiber simulation\n\nFiber simulation is a branch of mechanics that deals with modeling the dynamics and rheology of fibers, i.e. particles of large aspect ratio length to diameter. Fiber simulations are used to gain a better understanding of production processes including fibers (textile and paper industry), biological systems or computer graphics.\n\nMany of the models used to simulate fibers were developed by researchers in the field of rheology. Rheologically speaking fiber suspensions are non-Newtonian fluids, and can display normal stress differences.\n\nEarly fiber simulations employed particles which were rigid rods or prolate spheroids, whose equations of motion have analytical solutions. More recent models are able to represent flexible fibers. The models rely heavily on continuum mechanics concepts and the numerical methods employed have some similarities to those employed in molecular dynamics, or in dynamics of multi body systems.\n\nThe use of computers facilitates greatly the solution of fiber simulation problems. The complexity of the simulations arise from the system having a large number of degrees of freedom, and from the numerous possible interparticle interactions having place, such as friction, hydrodynamic interactions, and other kinds of interparticle forces such as colloidal forces that exert attractive or repulsive forces.\n"}
{"id": "8791730", "url": "https://en.wikipedia.org/wiki?curid=8791730", "title": "Forming gas", "text": "Forming gas\n\nForming gas is a mixture of hydrogen (mole fraction varies) and nitrogen. It is sometimes called a \"dissociated ammonia atmosphere\" due to the reaction which generates it:\n\nIt can also be manufactured by thermal cracking of ammonia, in an ammonia cracker or forming gas generator.\n\nForming gas is used as an atmosphere for processes that need the properties of hydrogen gas. Typical forming gas formulations (5% H2 in N2) are not explosive. It is used in chambers for gas hypersensitization, a process in which photographic film is heated in forming gas to drive out moisture and oxygen and to increase the base fog of the film. Hypersensitization is used particularly in deep-sky astrophotography, which deals with low-intensity incoming light, requires long exposure times, and is thus particularly sensitive to contaminants in the film.\n\nForming gas is also used to regenerate catalysts in glove boxes and as an atmosphere for annealing processes. It can be purchased at welding supply stores. It is sometimes used as a reducing agent for low- and high-temperature soldering and brazing, to remove oxidation of the joint without the use of flux.\n\nQuite often forming gas is used in furnaces during annealing or sintering for the thermal treatment of metals, because it reduces oxides on the metal surface.\n"}
{"id": "1432973", "url": "https://en.wikipedia.org/wiki?curid=1432973", "title": "Galling", "text": "Galling\n\nGalling is a form of wear caused by adhesion between sliding surfaces. When a material galls, some of it is pulled with the contacting surface, especially if there is a large amount of force compressing the surfaces together. Galling is caused by a combination of friction and adhesion between the surfaces, followed by slipping and tearing of crystal structure beneath the surface. This will generally leave some material stuck or even friction welded to the adjacent surface, whereas the galled material may appear gouged with balled-up or torn lumps of material stuck to its surface.\n\nGalling is most commonly found in metal surfaces that are in sliding contact with each other. It is especially common where there is inadequate lubrication between the surfaces. However, certain metals will generally be more prone to galling, due to the atomic structure of their crystals. For example, aluminium is a metal that will gall very easily, whereas annealed (softened) steel is slightly more resistant to galling. Steel that is fully hardened is very resistant to galling.\n\nGalling is a common problem in most applications where metals slide while in contact with other metals. This can happen regardless of whether the metals are the same or of different kinds. Alloys such as brass and bronze are often chosen for bearings, bushings, and other sliding applications because of their resistance to galling, as well as other forms of mechanical abrasion.\n\nGalling is adhesive wear that is caused by microscopic transfer of material between metallic surfaces, during transverse motion (sliding). It occurs frequently whenever metal surfaces are in contact, sliding against each other, especially with poor lubrication. It often occurs in high load, low speed applications, but also in high-speed applications with very little load. Galling is a common problem in sheet metal forming, bearings and pistons in engines, hydraulic cylinders, air motors, and many other industrial operations. Galling is distinct from gouging or scratching in that it involves the visible transfer of material as it is adhesively pulled (mechanically spalled) from one surface, leaving it stuck to the other in the form of a raised lump (gall). Unlike other forms of wear, galling is usually not a gradual process, but occurs quickly and spreads rapidly as the raised lumps induce more galling.\nIt can often occur in screws and bolts, causing the threads to seize and tear free from either the fastener or the hole. In extreme cases, the bolt may lock up to the point where all turning force is used by the friction, which can lead to breakage of the fastener or the tool turning it. Threaded inserts of hardened steel are often used in metals like aluminium or stainless steel that can gall easily.\n\nGalling requires two properties common to most metals, cohesion through metallic-bonding attractions and plasticity (the ability to deform without breaking). The tendency of a material to gall is affected by the ductility of the material. Typically, hardened materials are more resistant to galling whereas softer materials of the same type will gall more readily. The propensity of a material to gall is also affected by the specific arrangement of the atoms, because crystals arranged in a face-centered cubic (FCC) lattice will usually allow material-transfer to a greater degree than a body-centered cubic (BCC). This is because a face-centered cubic has a greater tendency to produce dislocations in the crystal lattice, which are defects that allow the lattice to shift, or \"cross-slip,\" making the metal more prone to galling. However, if the metal has a high number of stacking faults (a difference in stacking sequence between atomic planes) it will be less apt to cross-slip at the dislocations. Therefore, a material's resistance to galling is usually determined by its stacking-fault energy. A material with high stacking-fault energy, such as aluminium or titanium, will be far more susceptible to galling than materials with low stacking-fault energy, like copper, bronze, or gold. Conversely, materials with a hexagonal close packed (HCP) structure and a high \"c/a\" ratio, such as cobalt-based alloys, are extremely resistant to galling.\n\nGalling occurs initially with material transfer from individual grains, on a microscopic scale, which become stuck or even diffusion welded to the adjacent surface. This transfer can be enhanced if one or both metals form a thin layer of hard oxides with high coefficients of friction, such as those found on aluminum or stainless-steel. As the lump grows it pushes against the adjacent material and begins forcing them apart, concentrating a majority of the friction heat-energy into a very small area. This in turn causes more adhesion and material build-up. The localized heat increases the plasticity of the galled surface, deforming the metal, until the lump breaks through the surface and begins plowing up large amounts of material from the galled surface. Methods of preventing galling include the use of lubricants like grease and oil, low-friction coatings and thin-film deposits like molybdenum disulfide or titanium nitride, and increasing the surface hardness of the metals using processes such as case hardening and induction hardening.\n\nIn engineering science and in other technical aspects, the term galling is widespread. The influence of acceleration in the contact zone between materials has been mathematically described and correlated to the exhibited friction mechanism found in the tracks during empiric observations of the galling phenomenon. Due to problems with previous incompatible definitions and test methods, better means of measurements in coordination with greater understanding of the involved frictional mechanisms have led to the attempt to standardize or redefine the term galling to enable a more generalized use.\nASTM International has formulated and established a common definition for the technical aspect of the galling phenomenon in the ASTM G40 standard: \"Galling is a form of surface damage arising between sliding solids, distinguished by microscopic, usually localized, roughening and creation of protrusions (e.g.: lumps) above the original surface\".\n\nWhen two metallic surfaces are pressed against each other, the initial interaction and the mating points are the asperities, or high points, found on each surface. An asperity may penetrate the opposing surface if there is a converging contact and relative movement. The contact between the surfaces initiates friction or plastic deformation and induces pressure and energy in a small area called the contact zone.\n\nThe elevation in pressure increases the energy density and heat level within the deformed area. This leads to greater adhesion between the surfaces which initiate material transfer, galling build-up, lump growth, and creation of protrusions above the original surface.\n\nIf the lump (or protrusion of transferred material to one surface) grows to a height of several micrometers, it may penetrate the opposing surface oxide-layer and cause damage to the underlying material. Damage in the bulk material is a prerequisite for plastic flow that is found in the deformed volume which surrounds the lump. The geometry and speed of the lump defines how the flowing material will be transported, accelerated, and decelerated around the lump. This material flow is critical when defining the contact pressure, energy density, and developed temperature during sliding. The mathematical function describing acceleration and deceleration of flowing material is thereby defined by the geometrical constraints, deduced or given by the lump's surface contour.\n\nIf the right conditions are met, such as geometric constraints of the lump, an accumulation of energy can cause a clear change in the materials contact and plastic behaviour; generally this increases adhesion and the friction force needed for further movement.\n\nIn sliding friction, increased compressive stress is proportionally equal to a rise in potential energy and temperature within the contact zone. The reasons for accumulation of energy during sliding can be a reduction of energy loss away from the contact zone, due to a small surface area on the surface boundary thus low heat conductivity. Another reason is the energy that is continuously forced into the metals, which is a product of acceleration and pressure. In cooperation, these mechanisms allow a constant accumulation of energy causing increased energy density and temperature in the contact zone during sliding.\n\nThe process and contact can be compared to cold welding or friction welding, because cold welding is not truly cold and the fusing points exhibit an increase in temperature and energy density derived from applied pressure and plastic deformation in the contact zone.\n\nGalling is often found between metallic surfaces where direct contact and relative motion have occurred. Sheet metal forming, thread manufacturing and other industrial operations may include moving parts or contact surfaces made of stainless steel, aluminium, titanium, and other metals whose natural development of an external oxide layer through passivation increases their corrosion resistance but renders them particularly susceptible to galling.\n\nIn metalworking that involves cutting (primarily turning and milling), galling is often used to describe a wear phenomenon which occurs when cutting soft metal. The work material is transferred to the cutter and develops a \"lump\". The developed lump changes the contact behavior between the two surfaces, which usually increases adhesion, resistance to further cutting, and, due to created vibrations, can be heard as a distinct sound.\n\nGalling often occurs with aluminium compounds and is a common cause of tool breakdown. Aluminium is a ductile metal, which means it possesses the ability for plastic flow with relative ease, which presupposes a relatively consistent and large plastic zone.\n\nHigh ductility and flowing material can be considered a general prerequisite for excessive material transfer and galling because frictional heating is closely linked to the structure of plastic zones around penetrating objects.\n\nGalling can occur even at relatively low loads and velocities, because it is the real energy-density in the system that induces a phase transition, which often leads to an increase in material transfer and higher friction.\n\nGenerally there are two major frictional systems which affect adhesive wear or galling. In terms of prevention, they work in dissimilar ways and set different demands on the surface structure, alloys and crystal matrix used in the materials:\n\n\n\"In solid surface contact\" or unlubricated conditions, the initial contact is characterised by interaction between asperities and the exhibition of two different sorts of attraction: cohesive surface-energy or the molecules connect and adhere the two surfaces together, notably even if they are separated by a measurable distance. Direct contact and plastic deformation generates another type of attraction through the constitution of a plastic zone with flowing material where induced energy, pressure and temperature allow bonding between the surfaces on a much larger scale than cohesive surface-energy.\n\nIn metallic compounds and sheet metal forming, the asperities are usually oxides and the plastic deformation mostly consists of brittle fracture, which presupposes a very small plastic zone. The accumulation of energy and temperature is low due to the discontinuity in the fracture mechanism.\nHowever, during the initial asperity/asperity contact, wear debris or bits and pieces from the asperities adhere to the opposing surface, creating microscopic, usually localized, roughening and creation of protrusions (in effect lumps) above the original surface. The transferred wear debris and lumps penetrate the opposing oxide surface layer and cause damage to the underlying bulk material, plowing it forward. This allows continuous plastic deformation, plastic flow, and accumulation of energy and temperature.\nThe prevention of adhesive material-transfer is accomplished by the following or similar approaches:\n\n\n\"Lubricated contact\" places other demands on the surface structure of the materials involved, and the main issue is to retain the protective lubrication thickness and avoid plastic deformation. This is important because plastic deformation raises the temperature of the oil or lubrication fluid and changes the viscosity. Any eventual material transfer or creation of protrusions above the original surface will also reduce the ability to retain a protective lubrication thickness. A proper protective lubrication thickness can be assisted or retained by:\n\n\nGalling should not be confused with other cases of attraction between surfaces which do not result in plastic deformation. These latter types of attraction involve adhesive surface forces or surface energy theories. Different energy potentials at the surfaces can develop adhesive bonds or cohesive forces that may hold the two surfaces together. Surface energy and the cohesive force phenomenon are not the same as galling, and are only partially correlated. Galling necessarily involves plastic deformation of at least one surface.\n\nHowever, research generally fails to make a clear distinction between energy derived from plastic deformation and the cohesive surface-forces, and chemical attraction between atoms or surface molecules. The latter is likely to be involved in the initial material transfer, where only surface-oxide asperities are in contact. Difficulty comes in distinguishing these adhesive forces from more severe attractions, caused by accumulated energy and increased pressure from plastic deformation. Oxides are brittle and it is probable that most of the energy in the fracture mechanism is consumed in brittle fracture, but the created wear debris will instantaneously penetrate the opposing surface. This means that the transferred oxide material will instantly act as a penetrating body and the concentration of energy, pressure and frictional heating is immediate. Without this accumulation of energy, the tendency for material transfer will certainly decrease.\n\nThe formation and constitution (physique) of plastic zones around penetrating objects are arguably a prerequisite and the main factor for excessive material transfer, lump growth, and galling build-up even in the initial contact process.\n\n"}
{"id": "31929844", "url": "https://en.wikipedia.org/wiki?curid=31929844", "title": "Gobius rubropunctatus", "text": "Gobius rubropunctatus\n\nGobius rubropunctatus is a species of goby native to inshore waters of the Atlantic Ocean near the coasts of Africa from Mauritania to Ghana down to a depth of about . This species can reach a length of TL.\n"}
{"id": "4614602", "url": "https://en.wikipedia.org/wiki?curid=4614602", "title": "Grand Harbour of Malta tornado", "text": "Grand Harbour of Malta tornado\n\nThe Grand Harbour of Malta tornado was a tornado that hit the Grand Harbour of Malta on September 23, 1551 or 1556 (sources conflict) with very intense strength. It began as a waterspout killing at least 600 people, making it one of the deadliest & most destructive tornado in world history until it surpassed by a tornado in 1989.. At least four of the Order's galleys, named \"Santa Fè\", \"San Michele\", \"San Filippo\" and \"San Claudio\", capsized in the tornado.\n\n"}
{"id": "45382660", "url": "https://en.wikipedia.org/wiki?curid=45382660", "title": "HVDC HelWin2", "text": "HVDC HelWin2\n\nHVDC HelWin2 is a high voltage direct current (HVDC) link built to transmit offshore wind power to the power grid of the German mainland. The project differs from most HVDC systems in that one of the two converter stations is built on a platform in the sea. Voltage-Sourced Converters with DC ratings of 690 MW, ±320 kV are used and the total cable length is 130 km. The project was built by the Siemens/Prysmian consortium with the offshore platform built by Heerema in Zwijndrecht, Netherlands. The topside measures 98 m x 42 m x 28 m and weighs 10200 tonnes. The project was handed over to its owner, TenneT, in June 2015, the fourth such project to be completed in 2015.\n\n\n"}
{"id": "50195646", "url": "https://en.wikipedia.org/wiki?curid=50195646", "title": "Ignasius Jonan", "text": "Ignasius Jonan\n\nIgnasius Jonan (born 21 June 1963) is the Indonesian Minister for Energy and Mineral Resources serving under President Joko Widodo's administration. He is a former Indonesian Minister of Transportation and a former CEO of the Indonesian government-owned railway company, PT Kereta Api Indonesia (PT. KAI) which he headed from 2009 to 2014.\n\nBorn in Singapore, Jonan received his Bachelor's degree in Accounting from Airlangga University, Surabaya in 1986, and M.A. in International Relations and Affairs from the Fletcher School of Law and Diplomacy and Tufts University in 2005. He started his professional career in the banking sector, holding a managing director role in the American banking and financial services corporation, Citigroup, before entering the public transportation sector in 2009. He was named by Sofyan Djalil, the then-Minister of State-owned Enterprises, to lead P.T. Kereta Api Indonesia. \n\nJonan was born on 21 June 1963 in Singapore. He grew up in Surabaya, graduating from St. Louis 1 Catholic High School in the city. He later graduated with a Bachelor's degree in Accounting from Airlangga University's School of Business and Economics in 1985. He took executive and management programs between 1999 and 2000 from Columbia Business School and John F. Kennedy School of Government at Harvard University.\n\nHe received his Master's degree in International Relations and Affairs from Tufts University in 2005.\n\nJonan started his career as an accountant and investment manager. In 1999, he became Director for Private Equity at Citigroup branch in Indonesia. Two years later, he was appointed CEO of the government-owned investment firm, PT. Bahana Pembinaan Usaha Indonesia, assuming the role until 2006. From 2006 to 2009, he was promoted by Citi as Managing Director and Head of Investment Banking for Indonesia.\n\nJonan was appointed head of P.T. Kereta Api Indonesia in 2009, in the midst of the railway company's output and financial troubles, He was faced with PT. KAI's 27,000 employees with low expectations in terms of work quality, a financial deficit (in 2008, it lost Rp 82.6 billion), and a high percentage of substandard or broken locomotives, diesel trains, freight cars and crumbling stations.\n\nWithin 5 years, he overturned public perception of the failing Indonesian rail transportation system. The company was able to increase passenger ridership by 50% in 2014 compared to when Jonan took up the role in 2009. Freight loads have also doubled to nearly 30 million tons per year.\n\nOn 26 October 2014, the newly-elected Indonesian President Joko Widodo named him as the 37th Minister of Transportation. However, on 27 July 2016, President Joko Widodo terminated him in a cabinet reshuffle. In response to the reshuffle, Ignasius Jonan, with a flat face, said, \"I will come back\". On 14 October 2016 the president appointed Ignasius Jonan again, but now as Minister of Energy and Mineral Resources with Arcandra Tahar as his deputy.\n"}
{"id": "667600", "url": "https://en.wikipedia.org/wiki?curid=667600", "title": "Impactite", "text": "Impactite\n\nImpactite (or impact glass) is rock created or modified by the impact of a meteorite. \n\nImpactite includes shock-metamorphosed target rocks, melts (suevites) and mixtures of the two, as well as sedimentary rocks with significant impact-derived components (shocked mineral grains, tektites, anomalous geochemical signatures, etc.). In June 2015, NASA reported that impact glass has been detected on the planet Mars. Such material may contain preserved signs of ancient life—if life existed.\n\nWhen a meteor strikes a planet's surface, the energy release from the impact can melt rock and soil into a liquid. If the liquid cools and hardens quickly into a solid, impact glass forms before the atoms have time to arrange into a crystal lattice. Impact glass is dark brown, almost black, and partly transparent.\n\n\n\n"}
{"id": "5993782", "url": "https://en.wikipedia.org/wiki?curid=5993782", "title": "Ingenieurs zonder Grenzen", "text": "Ingenieurs zonder Grenzen\n\nIngenieurs zonder Grenzen, (Dutch for \"Engineers Without Borders\") is a name used by two Belgian organizations, both of which are provisional members of the Engineers Without Borders International network.\n\n\n"}
{"id": "27295071", "url": "https://en.wikipedia.org/wiki?curid=27295071", "title": "International performance measurement and verification protocol", "text": "International performance measurement and verification protocol\n\nThe International Performance Measurement and Verification Protocol (IPMVP®) defines standard terms and suggests best practise for quantifying the results of energy efficiency investments and increase investment in energy and water efficiency, demand management and renewable energy projects. The IPMVP was developed by a coalition of international organizations (led by the United States Department of Energy) starting in 1994-1995. The Protocol has become the national measurement and verification standard in the United States and many other countries, and has been translated into 10 languages. IPMVP is published in three volumes, most widely downloaded and translated is IPMVP Volume 1 Concepts and Options for Determining Energy and Water Savings. A major driving force was the need for a common protocol to verify savings claimed by Energy Service Companies (ESCOs) implementing Energy Conservation Measures (ECM). The protocol is a framework to determine water and energy savings associated with ECMs.\n\nIPMVP has existed in various forms since 1995 when a version of the protocol entitled North American Energy Measurement and Verification Protocol was published. This has been updated and expanded several times since then and in 2001 IPMVP Inc. was formed as an independent non-profit corporation in order to include the international community. Greg Kats served as the Founding Chair of the IPMVP committee from 1994 through 2001. In 2004 IPMVP Inc. changed its name to Efficiency Valuation Organization.\n\nThe use of IPMVP is now widespread amongst ESCOs in the US, and is gaining popularity in many other countries worldwide - China, UK, India, South Africa, Australia to name a few - with over 5000 website hits per month.\n\nThe purpose of the IPMVP is to increase certainty, reliability, and level of savings; reduce transaction costs by providing an international, industry consensus approach and methodologies; reduce financing costs by providing a project with a Measurement and Verification Plan (M&V Plan) standardisation, thereby allowing project bundling and pooled project financing. It aims to provide a basis for demonstrating emission reduction and delivering enhanced environmental quality; also to provide a basis for negotiating the contractual terms to ensure that an energy efficiency project achieves or exceeds its goals of saving money and improving energy efficiency. The Efficiency Valuation Organization also provides training in Measurement and Verification, and has established the Certified Measurement and Verification Professional qualification in association with the Association of Energy Engineers. It is a requirement of an IPMVP adherent M&V Plan that the plan is developed by a named individual.\n\nIPMVP provides four options for determining savings (A, B, C and D). The choice among the options involves many considerations. The selection of an IPMVP option is the decision of the designer of the M&V programme for each project. These options are summarised below:\n\nSavings are determined by field measurement of the key performance parameter(s) which define the energy use of the energy conservation measure’s (ECM) affected system(s) and/or the success of the project. Parameters not selected for field measurement are estimated. Estimates can be based on historical data, manufacturer’s specifications, or engineering judgment. Documentation of the source or justification of the estimated parameter is required.\n\nTypical applications may include a lighting retrofit, where the power drawn can be monitored and hours of operation can be estimated.\n\nSavings are determined by field measurement of all key performance parameters which define the energy use of the ECM-affected system. \n\nTypical applications may include a lighting retrofit where both power drawn and hours of operation are recorded.\n\nSavings are determined by measuring energy use at the whole facility or sub-facility level. This approach is likely to require a regression analysis or similar to account for independent variables such as outdoor air temperature, for example. \n\nTypical examples may include measurement of a facility where several ECMs have been implemented, or where the ECM is expected to affect all equipment in a facility.\n\nSavings are determined through simulation of the energy use of the whole facility, or of a sub-facility. Simulation routines are demonstrated to adequately model actual energy performance measured in the facility. This Option usually requires considerable skill in calibrated simulation.\n\nTypical applications may include measurement of a facility where several ECMs have been implemented, but no historical energy data is available.\n"}
{"id": "170429", "url": "https://en.wikipedia.org/wiki?curid=170429", "title": "Isotropic etching", "text": "Isotropic etching\n\nIsotropic etching is a method commonly used in semiconductors to remove material from a substrate via a chemical process using an etchant substance. The etchant may be in liquid-, gas- or plasma-phase, although liquid etchants such as buffered hydrofluoric acid (BHF) for silicon dioxide etching are more often used. Unlike anisotropic etching, isotropic etching does not etch in a single direction, but rather etches in multiple directions within the substrate. Any horizontal component of the etch direction may therefore result in undercutting of patterned areas, and significant changes to device characteristics. Isotropic etching may occur unavoidably, or it may be desirable for process reasons.\n"}
{"id": "39139387", "url": "https://en.wikipedia.org/wiki?curid=39139387", "title": "Jannette B. Frandsen", "text": "Jannette B. Frandsen\n\nJannette Behrndtz Frandsen is a researcher and consultant who works in many fields including nearshore hydrodynamics, aeroelasticity, numerical analysis, computational fluid dynamics, coastal modeling, experimental fluid mechanics, sloshing, coastal erosion, climate change related problems, e.g., sea level rise, natural hazards (storms, tsunamis), wind energy, biomimetics, wave energy.\n\nThe common theme of her fundamental research includes investigations of free-surface gravity water waves and bluff-body boundary layer physics. Combining fluid dynamics and elasticity is one goal of the research but the understanding and the predictions of the fluid dynamics itself tend to represent the main challenges. The moving boundary treatment and bubbly fluid flow behavior in high Reynolds number and Froude number flows are the primary questions to address. Driven by contributing to the understanding of the underlying physics of turbulence and wave breaking, Prof. Frandsen investigates the suitability of numerical approaches spanning a wide range of length scales from micro to macroscopic flow structures in highly nonlinear flows. The continuum- and mesoscopic approaches explored include discretization of fully non-linear free-surface potential flow -, Navier-Stokes - and Lattice Boltzmann equations (LBE). Her early work on water wave mechanics was carried out in the Department of Engineering Science, University of Oxford where she held a departmental lecturership concurrently with her research fellowship at Oriel College. Later, she has earned worldwide practical, educational and research experiences. Among other contributions, she has developed and shown that \"nonlinear water waves\" can be captured accurately using various approaches rooted in the LBE. Frandsen has further contributed to the understanding of free-surface dynamics via physical tank experiments and numerical modeling of e.g., slosh dynamics.\nIn recent years, she has conducted large scale wave flume experiments with application to coastal protection working on the understanding of the underlying mechanism of wave breaking and wave impact processes.\n\nTo investigate the physics of aeroelastic phenomena, she has measured and modeled the coupling effects between fluid flow and elastically suspended bodies undergoing large movement in high Reynolds number. Fluid flow interaction with large structures often lead to resonance problems known as Vortex-induced vibrations, as represented in the first full-scale records on a long-span suspension bridge. Prof. Frandsen has been involved in solving many vibration problems including the development of coupled solvers between fluid and structure, e.g., using tuned liquid dampers to suppress wind induced oscillations.\n\nUnderpinning her academic experience, she has analysed and designed Offshore Oil and Gas Installations in seas worldwide estimating forces of ocean waves, buckling resistance, etc. She has also been engaged in offshore wind power and wave energy farm developments with a focus on establishing meteorological and oceanographic conditions in relation to design basis, amongst others. She has estimated wind load on many structures including tall buildings, suspension bridges, topsides on offshore platforms, solar plants, etc. Subsequently she has set-up physical models and field experiments, analysed and assessed the structural dynamics and provided solutions to Vortex-induced vibration (e.g., Great Belt East Bridge) and earthquakes.\n\nFrandsen received her basic education at Det Kongelige Vajsenhus the school of the Royal Orphanage (1975–83) – in Danish: Det Kongelige Vajsenhus. The school was founded in 1727 and its protector is Queen Margaret II of Denmark. It was during these early school years she developed an interest for civil engineering. In 1991, Frandsen received her B.Sc. in civil engineering from The Technical University of Denmark. Spending some years in industry, she then continued her education at Imperial College London to get her M.Sc. in civil engineering with specialization in steel structures in 1996. Later, she received her Ph.D. in aeroelasticity from Cambridge University Engineering Department, Peterhouse, 2000.\n\n"}
{"id": "531770", "url": "https://en.wikipedia.org/wiki?curid=531770", "title": "Milk float", "text": "Milk float\n\nIn British English, a milk float is a vehicle specifically designed for the delivery of fresh milk. Today, milk floats are usually battery electric vehicles (BEV), but they were formerly horse-drawn. They were once common in many European countries, particularly the United Kingdom, and were operated by local dairies. However, in recent years, as the number of supermarkets, small independent grocers and petrol stations, and convenience stores stocking fresh milk has increased, many people have switched from regular home delivery to obtaining fresh milk from these other sources.\n\nBecause of the relatively small power output from its electric motor, a milk float travels fairly slowly, usually around although some have been modified to do up to . Operators often exit their vehicle before they have completely stopped to speed up deliveries; milk floats generally have sliding doors that can be left open when moving, or may have no doors at all. Electric milk floats come in three wheel and four wheel versions, the latter normally larger. They are very quiet, suiting operations in residential areas during the early hours of the morning or during the night.\n\nMost electric milk floats do not have seat belts, and the law in the United Kingdom only requires wearing seat belts where these are fitted in the vehicle. While there was previously an exemption in the law meaning those making local deliveries were not required to wear a seat belt, which would in theory have included drivers and passengers in milk floats with seat belts fitted, the law was changed in 2005 to deliveries less than apart.\n\nIn August 1967, the UK Electric Vehicle Association put out a press release stating that Britain had more battery-electric vehicles on its roads than the rest of the world put together. It is not clear what research the association had undertaken into the quantity of electric vehicles of other countries, but closer inspection disclosed that almost all of the battery driven vehicles licensed for UK road use were milk floats.\n\nGlasgow has one of the largest working milk float fleets in the UK. Most of the vehicles operate from the Grandtully Depot in Kelvindale. Some dairies in the UK, including Dairy Crest, have had to modernise and have replaced their electric milk floats with petrol or diesel fuel-powered vehicles to speed up deliveries and thus increase profit.\n\nThere were many manufacturers of milk floats in Britain during the 20th century.\n\nBrush Electrical Engineering Company had been established in 1889, and had manufactured electric cars between 1901 and 1905. In 1940, Brush required some small electric tractor units, but as none were commercially available, they asked AE Morrison and Sons to produce a design for one. Morrisons produced a 3-wheeled design, which Brush then used to manufacture a number of units for internal use. They then began selling them to customers, shipping a large order to Russia in 1941. They expanded to producing battery electric road vehicles in 1945, when they bought designs and manufacturing rights from Metrovick. The Metrovick designs were for 4-wheeled vehicles, but they also produced 3-wheeled vehicles, which were marketed as the Brush Pony. In early 1949, they reduced the prices of their electric vehicles by around 25 per cent, in an attempt to make them more competitive with petrol vehicles. All of their road vehicles were sold through the motor trade, in order to achieve a good standard of after-sales service. Production of 4-wheeled battery electrics ceased in 1950, although the company continued to manufacture the 3-wheeled Brush Pony, and their range of industrial trucks. By 1969, Brush were owned by the Hawker Siddeley group, which also owned half of Morrison-Electricars, and manufacture of Brush electric vehicles moved to the newly established Morrison factory at Tredegar. Most were industrial trucks, but the transfer also included the Brush Pony, and a number were manufactured at Tredegar subsequently.\n\nElectricars began trading in Birmingham in 1919, and although they initially made heavy duty electric vehicles, suitable for payloads up to 6 tons, they soon diversified into smaller vehicles suitable for doorstep delivery. In 1936, they became part of the business group Associated Electric Vehicle Manufacturers Limited (AEVM), but during the Second World War, few electric vehicles were built, due to a shortage of materials, and they ceased producing them in 1944.\n\nGraiseley Electric Vehicles were produced in Wolverhampton by Diamond Motors Ltd, a company which previously had made motorcycles, and which bought the sidecar business from AJS when that company was liquidated in 1931. Included in the sale was the Graiseley marque, and this was used for a range of three-wheeled battery-electric pedestrian controlled milk trucks. They soon found that they could sell into other industries as well. In 1937 they produced a ride-on four wheeled vehicle, suitable for a payload of 8-10 cwt, and with a range of around . Nevertheless, it was for their pedestrian controlled vehicles that they were best known, and their range included the Model 60, with a payload of 8-10 cwt, the Model 75, with a 12-15 cwt payload, and the Model 90, which could carry 22 cwt. Because the primary focus was on the dairy industry, the model numbers represented the number of imperial gallons of milk that could be carried. Between 1948 and 1952, the company sold a large number of Graiseley PCVs to United Dairies, and gradually diversified into stillage trucks and pallet trucks for use in factories. The company was liquidated in 1960, but the Graiseley marque was used by Lister Graiseley in 1969 and by Gough Industrial Trucks Ltd of Hanley, Stoke-on-Trent in 1971.\n\nHarbilt electric vehicles were initially produced by the Market Harborough Construction Company, which was formed in 1935 as a manufacturer of aircraft components. After the end of the Second World War, they diversified, and electric vehicles were a part of their new product range. The first vehicle produced was the 551 pedestrian controlled vehicle, which they supplied with a charger made by Partridge Wilson of Leicester, who were making their own range of Wilson battery vehicles. As well as milk delivery, the chassis was popular in Switzerland, with some 2000 vehicles supplied to the Swiss Post Office and to Swiss hotels. From 1956, they introduced ride-on vehicles, beginning with the model 735, and expanded the range considerably over the next few years. The Dairyliner range was showcased at the Royal International Dairy Show held at Olympia in October 1970. At some point in the early 1970s, prior to 1974, Harbilt and Morrison-Electricars reached an agreement for a product exchange and rationalisation. All milk floats would be built by Morrisons at their Tredegar works, while Morrison trucks would be handed over to Harbilt. The electric vehicle facility was taken over by a management buyout in 1975, and registered as Harbilt Electric Trucks. It continued to make trucks for a variety of industries, until it was bought for almost twice its share value by Fred W Davies, a Canadian who owned the Davies Magnet Group and York Trailers, in 1987. Production moved to Corby, but the venture was short-lived, and it was sold again to M&M Electric Vehicles of Atherstone in 1989.\n\nLewis Electruks were built by TH Lewis Ltd of Watford, a company closely associated with London's Express Dairy Company. Lewis began building milk floats, milk carts and horse-drawn vehicles for Express Dairies in 1873, and the business became a limited company in 1899. It was taken over by Express in 1931, as part of a reorganisation of their business. TH Lewis designed two types of electric vehicle for Express, the first of which entered service in 1934. This was a 3-wheeled pedestrian controlled vehicle with a 3.5 cwt payload, which had a fixed speed of on level ground. The battery drove a motor which was connected to the rear axle by reduction gearing, and this configuration gave a range of around . They were one of the first companies to provide storage for dry goods on their vehicles, and demonstrated a type AER 4-wheeled float with a grocery box behind the cab at the 1955 Dairy Show. Their exhibits at the 1958 Dairy Show included a standard 25 cwt milk float with a walk-through cab and a vertical steering wheel. The company was acquired by Austin Crompton Parkinson, makers of Morrison Electricar floats, in 1961, and Morrisons continued to make two of their models, the Electruk Rider, which became the model E15, and a pedestrian controlled vehicle, which became the model DPC3. Both Express Dairies and the London Co-operative Society had large fleets of the Electruk Rider, and continued to add to them with purchases of the E15.\n\nMetrovick electric vehicles were made by the Metropolitan-Vickers Electrical Company between the 1930s and 1945. In 1939, the Metrovick range consisted of a 7-9 cwt model, a 10-14 cwt model and an 18-22 cwt model. All three models were fitted with a drum controller, mounted on the cab floor, which provided six steps of series-parallel control. A safety interlock was provided, to reduce the likelihood of the motor being damaged by careless driving, and brakes manufactured by Lockhead or Girling were fitted to all four wheels. It was also possible to swap the batteries for another fully charged set, for situations where the range required exceeded that obtainable from a single battery. By 1943, a 25-30 cwt model had been added to the range. A more modern design of cab had been introduced in 1939, and as the Second World War ended, Metrovick ceased to make battery electric road vehicles, selling its designs and manufacturing rights to Brush. Consequently, early Brush designs are virtually indistinguishable from later Metrovick designs.\n\nMidland Electric milk floats were produced by Midland Vehicles Ltd of Leamington Spa. Their first design was a 10-15 cwt chassis, which was launched in January 1937. It was designed by J Parker Garner, who at the time was a well-known designer, having been involved in the manufacturing of vehicles for a number of years. In early 1938, Midland added a model B20 to their range, which was designed for a 20 cwt payload, but was otherwise very similar to the earlier model. It was showcased at the British Industries Fair, held at Castle Bromwich in February. The vehicle on display consisted of a chassis and a van body which had been cut in half, so that visitors to the show could see both the look of the vehicle and the construction of the chassis. By 1943, Midland Electric were producing five models, which could be fitted with various types of bodywork, including a flat-bed truck for coal deliveries. The B12 catered for a payload of 10-12 cwt, the BA12 for 12-15 cwt, the B20 for 18-22 cwt, the B25 for 25-28 cwt, with the largest model, the B30, suitable for 30-35 cwt. They produced a new 10-cwt lightweight design in 1949, which features an all-welded chassis with an integral body frame. The 10-cwt model was called the Midland Vandot when it was showcased at an exhibition in 1953, organised by the Electric Vehicle Association and the South Eastern Electricity Board. The company was listed in a 1956 directory of electric vehicle manufacturers published in Commercial Motor, but the company closed in 1957.\n\nMorrison-Electricars had their origins in the 1890s in Leicester, when AE Morrison began producing bicycles, motorcycles and stationary engines. The company became AE Morrison and Sons in 1929, and produced their first battery electric vehicle in 1933. They moved to larger premises in 1935, and all other products were phased out. They were another major player in AEVM, and Electricars and Morrisons rationalised their product range, with Morrisons concentrating on the smaller vehicles suitable for milk delivery. The vehicles were marketed as Morrison-Electricars from mid-1942, and were so known despite a series of takeovers. The Austin Motor Company bought a 50 percent share in AEVM in 1948, and the company became Austin Crompton Parkinson Electric Vehicles Ltd. Austin merged into the British Motor Corporation in 1952, which in turn merged with Leyland Motors in 1969, to become British Leyland. The electric vehicle business became Crompton Leyland Electricars Ltd. In 1972, British Leyland sold their share of the business to Hawker Siddeley, better known for aircraft manufacture, and the company became Crompton Electricars Ltd. The Board of Trade refused to allow Morrisons to move to new premises in Leicester, because of a lack of skilled labour in the area, and instead offered to build them a new factory in a development area, so the manufacturing base moved to Tredegar, south Wales, in 1968. Morrison-Electricars ceased to be made in 1983, when Hawker Siddeley sold the business to M & M Electric Vehicles of Atherstone, Warwickshire, who subsequently adopted the Electricars name for their own vehicles.\n\nVictor Electrics was formed in 1923 when Outram's Bakery in Southport, Merseyside, wanted to buy some electric vehicles to replace horses and carts on local deliveries, but found that both home-produced and imported vehicles were considerably more expensive than they were prepared to pay. They started manufacturing their own electric bread vans, which looked like conventional vans, with the batteries mounted under a bonnet at the front. They were soon making three models of bonnetted van, but in 1931, produced a forward control vehicle with a walk-through cab for the dairy industry. By 1935, they had a range of forward control vehicles in production, and ceased to make bonnetted vans. In 1967, the company was acquired by Brook Motors, and became part of Brook Victor Electric Vehicles. This company was itself acquired by Hawker Siddeley in 1970, and in 1973 it became Brook Crompton Parkinson Motors.\n\nWales & Edwards was the name of a garage and car salesroom for Morris and Wolseley cars, based in Shrewsbury. Mervyn Morris designed an electric vehicle, and the first milk float was sold to Roddington Dairy in early 1951. A request from United Dairies saw the production of a 3-wheeled chain driven vehicle, which was an immediate success. An order for 1,500 vehicles followed, and a new manufacturing base was set up in Harlescott, a suburb to the north of Shrewsbury. Larger models followed, although the 3-wheeled design was retained for most of their subsequent output. Four-wheeled vehicles were introduced in 1966 for payloads which exceeded 1.5 long tons, although they made eighteen 5-wheeled articulated milk floats from 1961, which could carry 2 long tons. The company was acquired by Smith Electric Vehicles in 1989.\n\nWilson Electrics were made by Partridge Wilson Engineering, who were manufacturers of charging equipment for accumulators, and were based in Leicester. In 1934 they produced their first electric van, suitable for a payload of 5-6 cwt. Speed control was arranged by switching of the motor fields and the battery cells, so that no starting resistance was needed. They produced several larger models, including a 20-25 cwt version of their MW vehicle, controlled by an accelerator which caused a number of relay contacts to close, at a preselected rate regulated by a fluid dashpot. In 1939 they were offering special deals for fleets of six vehicles, which were charged using a Davenset 3-phase group charger. Wilson Electric vehicles ceased to be produced in 1954, although the company continued to trade in Leicester until 1986.\nOther manufacturers included Smith's, Osborne, and Bedford. In 1941, Morrison-Electricar standardised three types of body which would become the basis for thousands of milk floats built after the war to deliver goods to the recovering population. As of 2009, only Bluebird Automotive remained in the industry.\n\nBefore BEVs, dairy supplies were delivered using horse-drawn milk floats. This lasted from the late 19th century until the 1950s. Today, with rounds expanding in coverage to ensure profitability in the face of falling levels of patronage, the limited range and speed of electric milk floats have resulted in many being replaced by diesel-powered converted vans.\n\nA collection of 29 milk floats and other BEVs dating from 1935 to 1982 and representing 14 different manufacturers, is kept by The Transport Museum, Wythall at their museum, and an early Brush Pony, dating from 1947 and operated by United Dairies, can be seen at the National Motor Museum, Beaulieu. There are five battery-electric road vehicles in the collection at the Ipswich Transport Museum, including a Smiths milk float dating from 1948, which was operated by Ipswich Co-operative Society, a Smiths vegetable cart dating from 1965 and a Brush Pony van dating from 1967. In addition several milk floats are still in service today, albeit repurposed after their milk delivery days. Many are used for work in factories, or as pleasure vehicles in rural areas, and some are hired out.\n\n\n"}
{"id": "12332536", "url": "https://en.wikipedia.org/wiki?curid=12332536", "title": "Namie-Odaka Nuclear Power Plant", "text": "Namie-Odaka Nuclear Power Plant\n\nThe was a plan for a nuclear power plant in Minamisōma and Namie in the Fukushima Prefecture that had preparation for preliminary ground work done. It was a project of the Tōhoku Electric Power Company. The plans were canceled after urging from local lawmakers in the wake of the Fukushima Daiichi nuclear disaster.\n\n\n"}
{"id": "101600", "url": "https://en.wikipedia.org/wiki?curid=101600", "title": "Obelisk", "text": "Obelisk\n\nAn obelisk (; from \"obeliskos\"; diminutive of \"obelos\", \"spit, nail, pointed pillar\") is a tall, four-sided, narrow tapering monument which ends in a pyramid-like shape or pyramidion at the top. These were originally called \"tekhenu\" by their builders, the Ancient Egyptians. The Greeks who saw them used the Greek term 'obeliskos' to describe them, and this word passed into Latin and ultimately English. Ancient obelisks are monolithic; that is, they consist of a single stone. Most modern obelisks are made of several stones; some, like the Washington Monument, are buildings.\n\nThe term \"stele\" is generally used for other monumental, upright, inscribed and sculpted stones.\n\nObelisks were prominent in the architecture of the ancient Egyptians, who placed them in pairs at the entrance of the temples. The word \"obelisk\" as used in English today is of Greek rather than Egyptian origin because Herodotus, the Greek traveller, was one of the first classical writers to describe the objects. A number of ancient Egyptian obelisks are known to have survived, plus the \"Unfinished Obelisk\" found partly hewn from its quarry at Aswan. These obelisks are now dispersed around the world, and fewer than half of them remain in Egypt.\n\nThe earliest temple obelisk still in its original position is the red granite Obelisk of Senusret I of the XIIth Dynasty at Al-Matariyyah in modern Heliopolis.\n\nThe obelisk symbolized the sun god Ra, and during the brief religious reformation of Akhenaten was said to be a petrified ray of the Aten, the sundisk. It was also thought that the god existed within the structure.\n\nBenben was the mound that arose from the primordial waters Nu upon which the creator god Atum settled in the creation story of the Heliopolitan creation myth form of Ancient Egyptian religion. The Benben stone (also known as a pyramidion) is the top stone of the Egyptian pyramid. It is also related to the Obelisk.\n\nIt is hypothesized by New York University Egyptologist Patricia Blackwell Gary and \"Astronomy\" senior editor Richard Talcott that the shapes of the ancient Egyptian pyramid and obelisk were derived from natural phenomena associated with the sun (the sun-god Ra being the Egyptians' greatest deity). The pyramid and obelisk might have been inspired by previously overlooked astronomical phenomena connected with sunrise and sunset: the zodiacal light and sun pillars respectively.\n\nThe Ancient Romans were strongly influenced by the obelisk form, to the extent that there are now more than twice as many obelisks standing in Rome as remain in Egypt. All fell after the Roman period except for the Vatican obelisk and were re-erected in different locations.\n\nThe largest standing and tallest Egyptian obelisk is the Lateran Obelisk in the square at the west side of the Lateran Basilica in Rome at tall and a weight of .\n\nNot all the Egyptian obelisks in the Roman Empire were set up at Rome. Herod the Great imitated his Roman patrons and set up a red granite Egyptian obelisk in the hippodrome of his new city Caesarea in northern Judea. This one is about tall and weighs about . It was discovered by archaeologists and has been re-erected at its former site.\n\nIn Constantinople, the Eastern Emperor Theodosius shipped an obelisk in AD 390 and had it set up in his hippodrome, where it has weathered Crusaders and Seljuks and stands in the Hippodrome square in modern Istanbul. This one stood tall and weighing . Its lower half reputedly also once stood in Istanbul but is now lost. The Istanbul obelisk is tall.\n\nRome is the obelisk capital of the world. The most well-known is probably the , obelisk at Saint Peter's Square in Rome. The obelisk had stood since AD 37 on its site on the wall of the Circus of Nero, flanking St Peter's Basilica:\n\nRe-erecting the obelisk had daunted even Michelangelo, but Sixtus V was determined to erect it in front of St Peter's, of which the nave was yet to be built. He had a full-sized wooden mock-up erected within months of his election. Domenico Fontana, the assistant of Giacomo Della Porta in the Basilica's construction, presented the Pope with a little model crane of wood and a heavy little obelisk of lead, which Sixtus himself was able to raise by turning a little winch with his finger. Fontana was given the project.\n\nThe obelisk, half-buried in the debris of the ages, was first excavated as it stood; then it took from 30 April to 17 May 1586 to move it on rollers to the Piazza: it required nearly 1000 men, 140 carthorses, and 47 cranes. The re-erection, scheduled for 14 September, the Feast of the Exaltation of the Cross, was watched by a large crowd. It was a famous feat of engineering, which made the reputation of Fontana, who detailed it in a book illustrated with copperplate etchings, \"Della Trasportatione dell'Obelisco Vaticano et delle Fabriche di Nostro Signore Papa Sisto V\" (1590), which itself set a new standard in communicating technical information and influenced subsequent architectural publications by its meticulous precision. Before being re-erected the obelisk was exorcised. It is said that Fontana had teams of relay horses to make his getaway if the enterprise failed. When Carlo Maderno came to build the Basilica's nave, he had to put the slightest kink in its axis, to line it precisely with the obelisk.\n\nThree more obelisks were erected in Rome under Sixtus V: the one behind Santa Maria Maggiore (1587), the giant obelisk at the Lateran Basilica (1588), and the one at Piazza del Popolo (1589).\n\nAn obelisk stands in front of the church of Trinità dei Monti, at the head of the Spanish Steps. Another obelisk in Rome is sculpted as carried on the back of an elephant. Rome lost one of its obelisks, the Boboli obelisk which had decorated the temple of Isis, where it was uncovered in the 16th century. The Medici claimed it for the Villa Medici, but in 1790 they moved it to the Boboli Gardens attached to the Palazzo Pitti in Florence, and left a replica in its stead.\n\nSeveral more Egyptian obelisks have been re-erected elsewhere. The best-known examples outside Rome are the pair of Cleopatra's Needles in London () and New York City () and the obelisk at the Place de la Concorde in Paris.\n\nThere are ancient Egyptian obelisks in the following locations:\n\nObelisk monuments are also known from the Assyrian civilization, where they were erected as public monuments that commemorated the achievements of the Assyrian king.\n\nThe British Museum possesses four Assyrian obelisks:\n\nThe White Obelisk of Ashurnasirpal I (named due to its colour), was discovered by Hormuzd Rassam in 1853 at Nineveh. The obelisk was erected by either Ashurnasirpal I (1050–1031 BC) or Ashurnasirpal II (883–859 BC). The obelisk bears an inscription that refers to the king’s seizure of goods, people and herds, which he carried back to the city of Ashur. The reliefs of the Obelisk depict military campaigns, hunting, victory banquets and scenes of tribute bearing.\n\nThe Rassam Obelisk, named after its discoverer Hormuzd Rassam, was found on the citadel of Nimrud (ancient Kalhu). It was erected by Ashurnasirpal II, though only survives in fragments. The surviving parts of the reliefs depict scenes of tribute bearing to the king from Syria and the west.\n\nThe Black Obelisk was discovered by Sir Austen Henry Layard in 1846 on the citadel of Kalhu. The obelisk was erected by Shalmaneser III and the reliefs depict scenes of tribute bearing as well as the depiction of two subdued rulers, Jehu the Israelite and Sua the Gilzanean, giving gestures of submission to the king. The reliefs on the obelisk have accompanying epigraphs, but besides these the obelisk also possesses a longer inscription that records one of the latest versions of Shalmaneser III’s annals, covering the period from his accessional year to his 33rd regnal year.\n\nThe Broken Obelisk, that was also discovered by Rassam at Nineveh. Only the top of this monolith has been reconstructed in the British Museum. The obelisk is the oldest recorded obelisk from Assyria, dating to the 11th century BC.\n\nA number of obelisks were carved in the ancient Axumite Kingdom of today northern Ethiopia. Together with () King Ezana's Stele, the last erected one and the only unbroken, the most famous example of axumite obelisk is the so-called (h) Obelisk of Axum. It was carved around the 4th century AD and, in the course of time, it collapsed and broke into three parts. In these conditions it was found by Italian soldiers in 1935, after the Second Italo-Abyssinian War, looted and taken to Rome in 1937, where it stood in the Piazza di Porta Capena. Italy agreed in a 1947 UN agreement to return the obelisk but did not affirm its agreement until 1997, after years of pressure and various controversial settlements. In 2003 the Italian government made the first steps toward its return, and in 2008 it was finally re-erected.\n\nThe largest known obelisk, the Great Stele at Axum, now fallen, at high and by at the base () is one of the largest single pieces of stone ever worked in human history (the largest is either at Baalbek or the Ramesseum) and probably fell during erection or soon after, destroying a large part of the massive burial chamber underneath it. The obelisks, properly termed stelae or the native \"hawilt\" or \"hawilti\" as they do not end in a pyramid, were used to mark graves and underground burial chambers. The largest of the grave markers were for royal burial chambers and were decorated with multi-storey false windows and false doors, while nobility would have smaller less decorated ones. While there are only a few large ones standing, there are hundreds of smaller ones in \"stelae fields\".\n\nThe Romans commissioned obelisks in an ancient Egyptian style. Examples include:\n\n\n\nThe prehistoric Tello Obelisk, found in 1919 at \"Chavín de Huantar\" in Peru, is a monolith stele with obelisk-like proportions. It was carved in a design of low relief with Chavín symbols, such as bands of teeth and animal heads. Long housed in the \"Museo Nacional de Arqueología, Antropología e Historia del Perú\" in Lima, it was relocated to the \"Museo Nacional de Chavín\", which opened in July 2008. The obelisk was named for the archeologist Julio C. Tello, who discovered it and was considered the \"father of Peruvian archeology.\" He was America's first indigenous archeologist.\n\nIn late summer 1999, Roger Hopkins and Mark Lehner teamed up with a NOVA (TV series) crew to erect a 25-ton obelisk. This was the third attempt to erect a 25-ton obelisk; the first two, in 1994 and 1999, ended in failure. There were also two successful attempts to raise a two-ton obelisk and a nine-ton obelisk. Finally in August–September 1999, after learning from their experiences, they were able to erect one successfully.\n\nFirst Hopkins and Rais Abdel Aleem organized an experiment to tow a block of stone weighing about 25 tons. They prepared a path by embedding wooden rails into the ground and placing a sledge on them bearing a megalith weighing about 25 tons. Initially they used more than 100 people to try to tow it but were unable to budge it. Finally, with well over 130 people pulling at once and an additional dozen using levers to prod the sledge forward, they moved it. Over the course of a day, the workers towed it 10 to 20 feet. Despite problems with broken ropes, they proved the monument could be moved this way. Additional experiments were done in Egypt and other locations to tow megalithic stone with ancient technologies, some of which are listed here.\n\nOne experiment was to transport a small obelisk on a barge in the Nile River. The barge was built based on ancient Egyptian designs. It had to be very wide to handle the obelisk, with a 2 to 1 ratio length to width, and it was at least twice as long as the obelisk. The obelisk was about long and no more than . A barge big enough to transport the largest Egyptian obelisks with this ratio would have had to be close to and . The workers used ropes that were wrapped around a guide that enabled them to pull away from the river while they were towing it onto the barge. The barge was successfully launched into the Nile.\n\nThe final and successful erection event was organized by Rick Brown, Hopkins, Lehner and Gregg Mullen in a Massachusetts quarry. The preparation work was done with modern technology, but experiments have proven that with enough time and people, it could have been done with ancient technology. To begin, the obelisk was lying on a gravel and stone ramp. A pit in the middle was filled with dry sand. Previous experiments showed that wet sand would not flow as well. The ramp was secured by stone walls. Men raised the obelisk by slowly removing the sand while three crews of men pulled on ropes to control its descent into the pit. The back wall was designed to guide the obelisk into its proper place. The obelisk had to catch a turning groove which would prevent it from sliding. They used brake ropes to prevent it from going too far. Such turning grooves had been found on the ancient pedestals. Gravity did most of the work until the final 15° had to be completed by pulling the obelisk forward. They used brake ropes again to make sure it did not fall forward. On 12 September they completed the project.\n\nThis experiment has been used to explain how the obelisks may have been erected in Luxor and other locations. It seems to have been supported by a 3,000-year-old papyrus scroll in which one scribe taunts another to erect a monument for \"thy lord\". The scroll reads \"Empty the space that has been filled with sand beneath the monument of thy Lord.\" To erect the obelisks at Luxor with this method would have involved using over a million cubic meters of stone, mud brick and sand for both the ramp and the platform used to lower the obelisk. The largest obelisk successfully erected in ancient times weighed . A stele was found in Axum, but researchers believe it was broken while attempting to erect it.\n\n\n\n"}
{"id": "49836887", "url": "https://en.wikipedia.org/wiki?curid=49836887", "title": "Paperless trade", "text": "Paperless trade\n\nPaperless trade refers to “trade taking place on the basis of electronic communications, including exchange of trade-related data and documents in electronic form” in the Framework Agreement on Facilitation of Cross-border Paperless Trade in Asia and the Pacific, adopted at United Nations Economic and Social Commission for Asia and the Pacific in May 2016. \nThe enormous costs arising from the exchange of billions of trade-related documents, as well as the complexity of international trade documents and procedures, are a huge burden on businesses, and a major disincentive to many small firms for participating in international trade. Switching from paper documents would increase security and transparency in supply chains and provide governments and the private sector with higher revenues. Paperless trade aims to making cross-border business transactions more convenient and transparent while ensuring regulatory compliance.\n\nIn the report of “Paperless Trade in International Supply Chains: Enhancing Efficiency and Security”, paperless trade is discussed from the perspectives of countries and private enterprises. It points out that the advance exchange of information and the automated analysis of trade data enable governments and enterprises to react faster on events and take appropriate measures to reduce costs and risks.\n\nThe implementation of paperless trade has increased significantly in the region of Asia and the Pacific. In contrast with Trade Facilitation measures, most paperless trade and, in particular, cross-border paperless trade measures, are not specifically featured in the WTO TFA, but drafted in the bilateral trade agreements. In 2015, the UN ESCAP collected and listed paperless trade and cross-border paperless trade measures as follows:\n\n\nDepending on how specific they are and the structure of each agreement, these measures may be found in e-commerce, customs or trade facilitation chapters. For example, in the Trans-Pacific Partnership Agreement, access to and use of the Internet for Electronic Commerce could be found in the chapter of E-commerce, which requires parties to “recognise the benefits of consumers in their territories having the ability to:\n\n(a) access and use services and applications of a consumer’s choice available on the Internet, subject to reasonable network management;\n\n(b) connect the end-user devices of a consumer’s choice to the Internet, provided that such devices do not harm the network;\n(c) access information on the network management practices of a consumer’s Internet access service supplier.” \n"}
{"id": "28353469", "url": "https://en.wikipedia.org/wiki?curid=28353469", "title": "Periodic systems of small molecules", "text": "Periodic systems of small molecules\n\nPeriodic systems of molecules are charts of molecules similar to the periodic table of the elements. Construction of such charts was initiated in the early 20th century and is still ongoing.\n\nIt is commonly believed that the periodic law, represented by the periodic chart, is echoed in the behavior of molecules, at least small molecules. For instance, if one replaces any one of the atoms in a triatomic molecule with a rare gas atom, there will be a drastic change in the molecule’s properties. Several goals could be accomplished by constructing an explicit representation of this periodic law as manifested in molecules: (1) a classification scheme for the vast number of molecules that exist, starting with small ones having just a few atoms, for use as a teaching aid and tool for archiving data, (2) forecasting data for molecular properties based on the classification scheme, and (3) a sort of unity with the periodic chart and the periodic system of fundamental particles.\n\nPeriodic systems (or charts or tables) of molecules are the subjects of two reviews. The systems of diatomic molecules include those of (1) H. D. W. Clark, and (2) F.-A. Kong, which somewhat resemble the atomic chart. The system of R. Hefferlin \"et al.\" was developed from (3) a three-dimensional to (4) a four-dimensional system Kronecker product of the element chart with itself.\n\nA totally different kind of periodic system is (5) that of G. V. Zhuvikin, which is based on group dynamics. In all but the first of these cases, other researchers provided invaluable contributions and some of them are co-authors. The architectures of these systems have been adjusted by Kong and Hefferlin to include ionized species, and expanded by Kong, Hefferlin, and Zhuvikin and Hefferlin to the space of triatomic molecules. These architectures are mathematically related to the chart of the elements. They were first called “physical” periodic systems.\n\nOther investigators have focused on building structures that address specific kinds of molecules such as alkanes (Morozov); benzenoids (Dias); functional groups containing fluorine, oxygen, nitrogen and sulfur (Haas); or a combination of core charge, number of shells, redox potentials, and acid-base tendencies (Gorski). These structures are not restricted to molecules with a given number of atoms and they bear little resemblance to the element chart; they are called “chemical” systems. Chemical systems do not start with the element chart, but instead start with, for example, formula enumerations (Dias), the hydrogen-displacement principle (Haas), reduced potential curves (Jenz), a set of molecular descriptors (Gorski), and similar strategies.\n\nE. V. Babaev has erected a hyperperiodic system which in principle includes all of the systems described above except those of Dias, Gorski, and Jenz.\n\nThe periodic chart of the elements, like a small stool, is supported by three legs: (a) the Bohr–Sommerfeld “solar system” atomic model (with electron spin and the Madelung principle), which provides the magic-number elements that end each row of the table and gives the number of elements in each row, (b)\nsolutions to the Schrödinger equation, which provide the same information, and (c) data provided by experiment, by the solar system model, and by solutions to the Schroedinger equation. The Bohr–Sommerfeld model should not be ignored: it gave explanations for the wealth of spectroscopic data that were already in existence before the advent of wave mechanics.\n\nEach of the molecular systems listed above, and those not cited, is also supported by three legs: (a)\nphysical and chemical data arranged in graphical or tabular patterns (which, for physical periodic systems at least, echo the appearance of the element chart), (b) group dynamic, valence-bond, molecular-orbital, and other fundamental theories, and (c) summing of atomic period and group numbers (Kong), the Kronecker product and exploitation of higher dimensions (Hefferlin), formula enumerations (Dias), the hydrogen-displacement principle (Haas), reduced potential curves (Jenz), and similar strategies.\n\nA chronological list of the contributions to this field contains almost thirty entries dated 1862, 1907, 1929, 1935, and 1936; then, after a pause, a higher level of activity beginning with the 100th anniversary of Mendeleev’s publication of his element chart, 1969. Many publications on periodic systems of molecules include some predictions of molecular properties, but starting at the turn of the Century there have been serious attempts to use periodic systems for the prediction of progressively more precise data for various numbers of molecules. Among these attempts are those of Kong, and Hefferlin\n\nThe collapsed-coordinate system has three independent variables instead of the six demanded by the Kronecker-product system. The reduction of independent variables makes use of three properties of gas-phase, ground-state, triatomic molecules. (1) In general, whatever the total number of constituent atomic valence electrons, data for isoelectronic molecules tend to be more similar than for adjacent molecules that have more or fewer valence electrons; for triatomic molecules, the electron count is the sum of the atomic group numbers (the sum of the column numbers 1 to 8 in the p-block of the periodic chart of the elements, C1+C2+C3). (2) Linear/bent triatomic molecules appear to be slightly more stable, other parameters being equal, if carbon is the central atom. (3) Most physical properties of diatomic molecules (especially spectroscopic constants) are closely monotonic with respect to the product of the two atomic period (or row) numbers, R1 and R2; for triatomic molecules, the monotonicity is close with respect to R1R2+R2R3 (which reduces to R1R2 for diatomic molecules). Therefore, the coordinates x, y, and z of the collapsed-coordinate system are C1+C2+C3, C2, and R1R2+R2R3. Multiple-regression predictions of four property values for molecules with tabulated data agree very well with the tabulated data (the error measures of the predictions include the tabulated data in all but a few cases).\n\n"}
{"id": "24969353", "url": "https://en.wikipedia.org/wiki?curid=24969353", "title": "Plasma lamp", "text": "Plasma lamp\n\nPlasma lamps are a type of gas discharge lamp energized by radio frequency (RF) power. They are distinct from the novelty plasma lamps that were popular in the 1980s.\n\nThe internal-electrodeless lamp was invented by Tesla after his experimentation with high-frequency currents in evacuated glass tubes for the purposes of lighting and the study of high voltage phenomena. The first practical plasma lamps were the sulfur lamps manufactured by Fusion Lighting. This lamp suffered a number of practical problems and did not prosper commercially. Plasma lamps with an internal phosphor coating are called external electrode fluorescent lamps (EEFL); these external electrodes or terminal conductors provide the radio frequency electric field.\n\nModern plasma lamps are a family of light sources that generate light by exciting plasma inside a closed transparent burner or bulb using radio frequency (RF) power. Typically, such lamps use a noble gas or a mixture of these gases and additional materials such as metal halides, sodium, mercury or sulfur. In modern plasma lamps, a waveguide is used to constrain and focus the electrical field into the plasma. In operation, the gas is ionized, and free electrons, accelerated by the electrical field, collide with gas and metal atoms. Some atomic electrons circling around the gas and metal atoms are excited by these collisions, bringing them to a higher energy state. When the electron falls back to its original state, it emits a photon, resulting in visible light or ultraviolet radiation, depending on the fill materials.\n\nThe first commercial plasma lamp was an ultraviolet curing lamp with a bulb filled with argon and mercury vapor developed by Fusion UV. That lamp led Fusion Lighting to the development of the sulfur lamp, a bulb filled with argon and sulfur that is bombarded with microwaves through a hollow waveguide. The bulb had to be spun rapidly to prevent it burning through. Fusion Lighting did not prosper commercially, but other manufacturers continue to pursue sulfur lamps. Sulfur lamps, though relatively efficient, have had a number of problems, chiefly:\n\nIn the past, the life of the plasma lamps was limited by the magnetron used to generate the microwaves. Solid state RF chips can be used and give long lives. However, using solid-state chips to generate RF is currently an order of magnitude more expensive than using a magnetron and so only appropriate for high-value lighting niches. It has recently been shown by Dipolar of Sweden to be possible to extend the life of magnetrons to over 40,000 hours, making low-cost plasma lamps possible.\n\nIn the year 2000, a system was developed that concentrated radio frequency waves into a dielectric waveguide made of ceramic, which energized light-emitting plasma in a bulb positioned inside. This system, for the first time, permitted an extremely compact yet bright electrode-less lamp. The invention has been a matter of dispute.\n\nThe use of a high-dielectric waveguide allowed the sustaining of plasmas at much lower powers—down to 100 W in some instances. It also allowed the use of conventional gas-discharge lamp fill materials which removed the need to spin the bulb. The only issue with the ceramic waveguide was that much of the light generated by the plasma was trapped inside the opaque ceramic waveguide.\n\nHigh-efficiency plasma lighting is the class of plasma lamps that have system efficiencies of 90 lumens per watt or more. Lamps in this class are potentially the most energy-efficient light source for outdoor, commercial and industrial lighting. This is due not only to their high system efficiency but also to the small light source they present enabling very high luminaire efficiency.\n\nLuminaire Efficacy Rating (LER) is the single figure of merit the National Electrical Manufacturers Association has defined to help address problems with lighting manufacturers' efficiency claims and is designed to allow robust comparison between lighting types. It is given by the product of luminaire efficiency (EFF) times total rated lamp output in lumens (TLL) times ballast factor (BF), divided by the input power in watts (IP):\n\nThe \"system efficiency\" for a High Efficiency Plasma lamp is given by the last three variables, that is, it excludes the luminaire efficiency. Though plasma lamps do not have a ballast, they have an RF power supply that fulfills the equivalent function. In electrodeless lamps, the inclusion of the electrical losses, or \"ballast factor\", in lumens per watt claimed can be particularly significant as conversion of electrical power to radio frequency (RF) power can be a highly inefficient process.\n\nMany modern plasma lamps have very small light sources—far smaller than HID bulbs or fluorescent tubes—leading to much higher luminaire efficiencies also. High intensity discharge lamps have typical luminaire efficiencies of 55%, and fluorescent lamps of 70%. Plasma lamps typically have luminaire efficiencies exceeding 90%.\n\nPlasma lamps have been used in high bay and street lighting applications, as well as in stage lighting. They were briefly used in some projection televisions.\n\n"}
{"id": "42966127", "url": "https://en.wikipedia.org/wiki?curid=42966127", "title": "Pulpí Geode", "text": "Pulpí Geode\n\nThe Pulpí Geode (Spanish: Geoda de Pulpí) is a giant geode found in Spain near the town of Pulpí (Province of Almería) by Javier Garcia-Guinea of the Grupo Mineralogista de Madrid in December 1999.\n\nIt is one of the larger documented geodes in the world to date. It is notable on a worldwide scale for both its size and the transparency and perfection of the selenite (gypsum) crystals lining the interior, which reach up to 2 m in length, with 0.5 m being the average.\n\nIt occupies a space of 10.7 m³ (8 m long by 1.8 m wide by a 1.7 m average high) and is located at a depth of 50 m in the Pilar de Jaravía lead mine, in the Sierra del Aguilón, in the municipality of Pulpí, coinciding with the sea level, 3 km from the coast.\n\nIt has a funnel shape, with the narrowest part being L-shaped.\n\n"}
{"id": "35987504", "url": "https://en.wikipedia.org/wiki?curid=35987504", "title": "Raoul du Toit", "text": "Raoul du Toit\n\nRaoul du Toit is a Zimbabwean environmentalist. He was awarded the Goldman Environmental Prize in 2011, for his efforts on protection of the black rhino.\n"}
{"id": "4584547", "url": "https://en.wikipedia.org/wiki?curid=4584547", "title": "SERI microalgae culture collection", "text": "SERI microalgae culture collection\n\nThe SERI microalgae culture collection was a collection from the Department of Energy's Aquatic Species Program cataloged at the Solar Energy Research Institute located in Golden, Colorado. The Aquatic Species Program ended in 1996 after its funding was cut, at which point its microalgae collection was moved to the University of Hawaii. In 1998 the University of Hawaii, partnered with the University of California at Berkeley, received a grant from the National Science Foundation (NSF), for their proposal to develop commercial, medical, and industrial uses of microalgae, as well as new and more efficient techniques for cultivation. This grant was used to form Marine Bioproduct Engineering Center (MarBEC), a facility operating within the University system of Hawaii at Manoa, but connected to corporate interests.\n\nBelow is a list of the algal-strains in the microalgae culture collection from the closeout report of the Department of Energy's Aquatic Species Program.\n\n\n"}
{"id": "10597358", "url": "https://en.wikipedia.org/wiki?curid=10597358", "title": "SgurrEnergy", "text": "SgurrEnergy\n\nSgurrEnergy is an engineering consultancy specialising in renewable energy projects. The company is based in Glasgow, Scotland, and has offices in Beijing, China; Pune, India; Paris, France; Wexford, Ireland; Portland, Maine; Vancouver, British Columbia; and Mexico City, Mexico as of January 2016.\n\nSgurrEnergy was established in 2002 by company directors Ian Irvine and Steve McDonald. The company specialises in wind farms, marine farms, bioenergy systems, hydro energy and solar energy.\n\nSgurrEnergy was a supplier of technical advice to Airtricity on the Braes of Doune wind farm. This wind farm is a milestone as it makes the UK the 8th state in the world to meet the target of having at least 2GW of installed wind energy capacity. SgurrEnergy personnel have given presentations on industry related matters at several conferences and workshops.\n\nSgurrEnergy personnel provided ongoing technical support to ScottishPower on the development of Europe’s largest onshore wind farm, Whitelee. Located 20 miles south of Glasgow the wind farm became operational in 2009. SgurrEnergy provided technical due diligence on the wind resource at Q7 offshore wind park off the coast of The Netherlands. This project is the world’s first bank financed wind park. In addition SgurrEnergy provided lenders Dexia and Rabobank with technical advice on the €153M Thorntonbank offshore wind farm off the Belgian coast. They are also providing technical advice on the construction of the first Western-financed wind farm to be built in China.\n"}
{"id": "13298565", "url": "https://en.wikipedia.org/wiki?curid=13298565", "title": "Skid-to-turn", "text": "Skid-to-turn\n\nSkid-to-turn is an aeronautical vehicle reference for how such a vehicle may be turned. It applies to vehicles such as aircraft and missiles. In skid-to-turn, the vehicle does not roll to a preferred angle. Instead commands to the control surfaces are mixed to produce the maneuver in the desired direction. This is distinct from the coordinated turn used by aircraft pilots. For instance, a vehicle flying horizontally may be turned in the horizontal plane by the application of rudder controls to place the body at a sideslip angle relative to the airflow. This sideslip flow then produces a force in the horizontal plane to turn the vehicle's velocity vector. The benefit of the skid-to-turn maneuver is that it can be performed much quicker than a coordinated turn. This is useful when trying to correct for small errors. The disadvantage occurs if the vehicle has greater maneuverability in one body plane than another. In that case the turns are less efficient and either consume greater thrust or cause a greater loss of aircraft specific energy than coordinated turns.\n\n\n"}
{"id": "4051830", "url": "https://en.wikipedia.org/wiki?curid=4051830", "title": "Sugar glass", "text": "Sugar glass\n\nSugar glass (also called candy glass, edible glass, and breakaway glass) is a brittle transparent form of sugar that looks like glass. It can be formed into a sheet that looks like flat glass or an object, such as a bottle or drinking glass.\n\nSugar glass is made by dissolving sugar in water and heating it to at least the \"hard crack\" stage (approx. 150 °C / 300 °F) in the candy making process. Glucose or corn syrup is used to prevent the sugar from recrystallizing, by getting in the way of the sugar molecules forming crystals. Cream of tartar also helps by turning the sugar into glucose and fructose.\n\nBecause sugar glass is hygroscopic, it must be used soon after preparation, or it will soften and lose its brittle quality.\n\nSugar glass has been used to simulate glass in movies, photographs and plays. It is much less likely to cause injuries than real glass, is inexpensive to produce and breaks convincingly, making it an excellent choice for stunts. However, it is rarely used for stunt work in modern times as it is extremely fragile and has been replaced with certain synthetic resins such as Piccotex. It is very similar to regular glass but when it dries it usually gets caramelized and may have a wavy pattern.\n\nSugar glass is also used to make sugar sculptures or other forms of edible art.\n\nSugar glass was used as a prop for meth in the AMC TV series Breaking Bad. Actor Aaron Paul would eat it on set.\n"}
{"id": "33821428", "url": "https://en.wikipedia.org/wiki?curid=33821428", "title": "Sulfur tetrachloride", "text": "Sulfur tetrachloride\n\nSulfur tetrachloride is an inorganic compound with chemical formula SCl. It has only been obtained as an unstable pale yellow solid. The corresponding SF is a stable, useful reagent.\n\nIt is obtained by treating sulfur dichloride with chlorine at 193 K:\n"}
{"id": "22519222", "url": "https://en.wikipedia.org/wiki?curid=22519222", "title": "Tantalum boride", "text": "Tantalum boride\n\nTantalum borides are compounds of tantalum and boron most remarkable for their extreme hardness.\n\nThe Vickers hardness of TaB and TaB films and crystals is ~30 GPa. Those materials are stable to oxidation below 700 °C and to acid corrosion.\n\nTaB has the same hexagonal structure as most diborides (AlB, MgB, etc.). The mentioned borides have the following space groups: TaB (orthorhombic, Thallium(I) iodide-type, Cmcm), TaB (Cmmm), TaB (Immm), TaB (hexagonal, aluminum diboride-type, P6/mmm).\n\nSingle crystals of TaB, TaB, TaB or TaB (about 1 cm diameter, 6 cm length) can be produced by the floating zone method.\n\nTantalum boride films can be deposited from a gas mixture of TaCl-BCl-H-Ar in the temperature range 540–800 °C. TaB (single-phase) is deposited at a source gas flow ratio (BCl/TaCl) of six and a temperature above 600 °C. TaB (single-phase) is deposited at BCl/TaCl = 2–4 and T = 600–700 °C.\n\nNanocrystals of TaB were successfully synthesized by the reduction of TaO with NaBH using a molar ratio M:B of 1:4 at 700-900 °C for 30 min under argon flow.\n\nTaO + 6.5NaBH → 2TaB + 4Na(g,l) + 2.5NaBO+ 13H(g)\n"}
{"id": "902918", "url": "https://en.wikipedia.org/wiki?curid=902918", "title": "Tetraethylammonium", "text": "Tetraethylammonium\n\nTetraethylammonium (TEA), () or (EtN) is a quaternary ammonium cation consisting of four ethyl groups attached to a central nitrogen atom, and is positively charged. It is a counterion used in the research laboratory to prepare lipophilic salts of inorganic anions. It is used similarly to tetrabutylammonium, the difference being that its salts are less lipophilic and more easily crystallized.\n\nThe chloride salt is prepared by the reaction of triethylamine and an ethyl halide:\n\nThis method works well for the preparation of tetraethylammonium iodide (where X = I).\n\nMost tetraethylammonium salts are prepared by salt metathesis reactions. For example, the synthesis of tetraethylammonium perchlorate, a salt that has been useful as a supporting electrolyte for polarographic studies in non-aqueous solvents, is carried out by mixing the water-soluble salts tetraethylammonium bromide and sodium perchlorate in water, from which the water-insoluble tetraethylammonium perchlorate precipitates:\nOther examples include the cyanide (EtNCN), and trichlorostannate (EtNSnCl). In some cases, salts are produced of anions that cannot be generated in water, such as the tetrahedral [NiCl] salt.\n\nThe principal chemical characteristic of tetraethylammonium salts is their ability to engage in processes involving phase-transfer, such as phase-transfer catalysis. Typically, the four ethyl groups surrounding the nitrogen are too small to facilitate efficient ion transfer between aqueous and organic phases, but tetraethylammonium salts have been found to be effective in a number of such applications, and these are exemplified under the headings of the individual salts.\n\nTEA salts such as tetraethylammonium tetrafluoroborate and tetraethylammonium methylsulfonate are used in supercapacitors as organic electrolytes.\n\nThe effective radius of the tetraethylammonium ion is reported as ~0.45 nm, which is comparable in size to that of the hydrated K ion. The ionic radius for TEA is given as 0.385 nm; several thermodynamic parameters for the TEA ion are also recorded.\n\nThe partition coefficient of TEA iodide in octanol-water, \"P\" was determined experimentally to be (or ).\n\nThe literature dealing with the pharmacologically-related properties of tetraethylammonium is vast, and research continues. It is clear that TEA blocks autonomic ganglia - it was the first \"ganglionic blocker\" drug to be introduced into clinical practice. However, TEA also produces effects at the neuromuscular junction and at sympathetic nerve terminals.\n\nAt the mechanistic level, TEA has long been known to block voltage-dependent K channels in nerve, and it is thought that this action is involved in the effects of TEA at sympathetic nerve terminals. With respect to activity at the neuromuscular junction, TEA has been found to be a competitive inhibitor at nicotinic acetylcholine receptors, although the details of its effect on these receptor proteins are complex. TEA also blocks Ca - activated K channels, such as those found in skeletal muscle and pituitary cells. It has also been reported that TEA inhibits aquaporin (APQ) channels, but this still seems to be a disputed issue.\n\nA partial effect of these voltage-dependent and permeability properties within each system mentioned above is not only due to the aforementioned inhibitory properties of TEA, but also its ability to inhibit Na,K-ATPase. Acting on the extracellular vestibule of the Na,K-ATPase, inhibiting K+ access similar to ouabain, TEA further accentuates the disrupted K, and Na, gradients within each of these systems.\n\nAlthough TEA (sometimes under the name \"Etamon\") was explored in a number of different clinical applications, including the treatment of hypertension, its major use seems to have been as a probe to assess the capacity for vasodilation in cases of peripheral vascular disease. Because of dangerous, even fatal reactions in some patients, as well as inconsistent cardiovascular responses, TEA was soon replaced by other drugs.\n\nTEA is not orally active. Typical symptoms produced in humans include the following: dry mouth, suppression of gastric secretion, drastic reduction of gastric motility, paralysis of urinary bladder, and relief of some forms of pain. Most studies with TEA seem to have been performed using either its chloride or bromide salt without comment as to any distinctions in effect, but it is noteworthy that Birchall and his co-workers preferred the use of TEA chloride in order to avoid the sedative effects of the bromide ion.\n\nAn extensive study of the toxicology of tetraethylammonium chloride in mice, rats and dogs was published by Gruhzit and co-workers in 1948. These workers reported the following symptoms in mice and rats receiving toxic parenteral doses: tremors, incoordination, flaccid prostration, and death from respiratory failure within 10–30 minutes; dogs exhibited similar symptoms, including incoordination, flaccid prostration, respiratory and cardiac depression, ptosis, mydriasis, erythema, and death from respiratory paralysis and circulatory collapse. After non-lethal doses, symptoms abated within 15–60 minutes. There was little evidence of toxicity from chronic administration of non-lethal doses. These investigators recorded the following acute toxicities, as LDs for TEA chloride (error ranges not shown):\n\nAnother research group, working at about the same time, but using tetraethylammonium bromide, published the following LD data:\n\nWriting in 1950, Graham made some observations on the toxic effects of tetraethylammonium bromide in humans. In one subject, described as a \"healthy woman\", 300 mg of tetraethylammonium bromide, i.v., produced incapacitating \"curariform\" (i.e., resembling the effects of tubocurarine) paralysis of the skeletal muscles, as well as marked drowsiness. These effects were largely dissipated within 2 hours. Citing the work of other investigators, Graham noted that Birchall had also produced \"alarming curariform effects\" in humans with i.v. doses of 32 mg/kg of tetraethylammonium chloride.\n\n"}
{"id": "26110486", "url": "https://en.wikipedia.org/wiki?curid=26110486", "title": "Thai Airways Flight 231", "text": "Thai Airways Flight 231\n\nThai Airways Flight 231 crashed on 27 April 1980. The Hawker Siddeley HS 748, registration HS-THB, stalled and crashed after entering a thunderstorm on approach to Bangkok. The accident killed 44 out of 53 passengers and crew on board Flight 231.\n\nThe flight took off from Khon Kaen Airport headed to Don Mueang International Airport in Bangkok, Thailand. After about 40 minutes Flight 231 was on approach to the airport and planned to land on runway 21R. It entered an area of rain which turned out to be a severe thunderstorm at 1500 feet. About a minute after entering the storm a downdraft struck the plane which caused the nose to go up and the plane to stall. The aircraft then went into a nose dive which the pilot tried to pull the aircraft out of. The Hawker then slightly banked to the right and was almost out of the dive when the aircraft crashed into the ground. The wreck of Hawker Siddeley HS 748 then slid for 510 feet and broke up at 06:55. The accident killed 44 of the passengers and crew, 9 people were injured in the crash.\n\n\n"}
{"id": "52142105", "url": "https://en.wikipedia.org/wiki?curid=52142105", "title": "The Great Derangement", "text": "The Great Derangement\n\nThe Great Derangement is a non-fiction book addressing climate change by Indian writer Amitav Ghosh published in 2016.\n\nThe book is composed of three parts, Part I deals with the stories, Part II is History and Part III is titled Politics. It examines our collective inability to spot the elephant in the room, namely \"to grasp the scale and violence of the climate change\". \n\nIt is a call to writers, artists, politicians, economists, industrialists, scientists, and all people to awaken out of the derangement we are presently living with.- \"that we need not be concerned with earth and our environment in our actions, pursuits and activities.\" \n\nIt highlights how complex the climate change scenario is and how it impacts people and nations. The book is a call to open our eyes; peer into the future. It shows how deranged we might be right now and how there is a greater need to project these into the collective imagination of people through literature, works of fiction, collective debates. \n\nIn the end Ghosh gives a deconstruction of the Paris agreement and \"Laudato si'\" and call for collective action.\n\n"}
{"id": "40132932", "url": "https://en.wikipedia.org/wiki?curid=40132932", "title": "Uludere Dam", "text": "Uludere Dam\n\nThe Uludere Dam is a gravity dam under construction on the Ortasu River (a tributary of the Hezil River) in Uludere district of Şırnak Province, southeast Turkey. Under contract from Turkey's State Hydraulic Works, Hidrokon began construction on the dam in 2008 and a completion date has not been announced.\n\nThe reported purpose of the dam is water storage and it can also support a 3.5 MW hydroelectric power station in the future. Another purpose of the dam which has been widely reported in the Turkish press is to reduce the freedom of movement of Kurdistan Workers' Party (PKK) militants. Blocking and flooding valleys in close proximity to the Iraq–Turkey border is expected to help curb cross-border PKK smuggling and deny caves in which ammunition can be stored. A total of 11 dams along the border; seven in Şırnak Province and four in Hakkâri Province were implemented for this purpose. In Şırnak they are the Silopi and Şırnak Dams downstream of the Uludere Dam and the Balli, Kavşaktepe, Musatepe and Çetintepe Dams upstream on the Ortasu River. In Hakkari are the Gölgeliyamaç (since cancelled) and Çocuktepe Dams on the Güzeldere River and the Aslandağ and Beyyurdu Dams on the Bembo River.\n\n"}
{"id": "23655028", "url": "https://en.wikipedia.org/wiki?curid=23655028", "title": "Uzbekistan GTL", "text": "Uzbekistan GTL\n\nUzbekistan GTL is a planned gas-to-liquids (GTL) project based in southern Uzbekistan.\nIt is expected to be the 3-rd biggest GTL-factory in the world.\n\nConstruction of a GTL plant in Uzbekistan was discussed between Uzbekneftegaz and Abu Dhabi's International Petroleum Investment Company in March 2008. However, in April 2009 Uzbekneftegaz signed a heads of agreement for the GTL project with Sasol and Petronas. On 15 July 2009, Sasol, Petronas, and Uzbekneftegaz signed an agreement to establish a joint venture for developing the GTL project. The detailed feasibility study was conducted by Technip. Technip will also conduct the front end engineering design.\nAfter 2016 elections in Uzbekistan, works on this project have been activated. Estimated cost of the project was increased to US$5.6 billion. As of April 2018 project has been finished for 30% and all the works are expected to be done till the end of 2019, according to Uzbekistan 24 National news' channel.\n\nThe plant will use the Sasol's slurry phase distillate process. The annual capacity of the plant would be 1.3 million tonnes of petroleum products such as diesel, kerosene, naphtha and liquefied petroleum gas. The project is expected to cost US$2.5 billion.\n\nThe project to be jointly developed by Sasol, Petronas, and Uzbekneftegaz. Each partner will have an equal share in the joint venture.\n"}
{"id": "29673982", "url": "https://en.wikipedia.org/wiki?curid=29673982", "title": "Victorio Peak treasure", "text": "Victorio Peak treasure\n\nThe Victorio Peak treasure (also seen in print as Treasure of Victorio Peak, Treasure of San Andres), describes a cache of gold found inside Victorio Peak in Southern New Mexico. While there have been multiple documented expeditions to the peak, no gold has been officially recorded as being recovered from the site.\n\nTheories abound on the origins of the alleged treasure, from eighteenth-century Spanish Missionaries to wealth pilfered from Mexico during the reign of the French puppet Emperor Maximilian. Some years after Doc Noss was killed, his wife Ova asserted a claim that she was entitled to access to the cave in Victorio Peak and its contents. Eventually she brought her case to the military, but the alleged bonanza had vanished.\n\nMany years following the Doc Noss discovery local newspapers had reported different accounts of possible treasure finds and hikers falling in the Hembrillo Basin. One story made headlines in the early 1990s after two bodies were found trapped inside the peak.\n\n\n"}
{"id": "30544695", "url": "https://en.wikipedia.org/wiki?curid=30544695", "title": "Windmade", "text": "Windmade\n\nWindMade is a global (Brussel's based) consumer label for companies, events and products using wind power in their operations or production. It is aimed at promoting wind power and is guided by a Technical Advisory Board, which includes various scientists, and third-party auditors. The organization is a non-profit NGO established by seven Founding Partners: United Nations Global Compact, WWF, Global Wind Energy Council, LEGO Group, PricewaterhouseCoopers (PwC), Bloomberg L.P. and Vestas Wind Systems.\n\n"}
