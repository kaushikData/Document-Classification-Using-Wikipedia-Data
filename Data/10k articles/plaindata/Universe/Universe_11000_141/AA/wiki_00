{"id": "18203711", "url": "https://en.wikipedia.org/wiki?curid=18203711", "title": "A Big Fix", "text": "A Big Fix\n\nA Big Fix: Radical Solutions for Australia's Environmental Crisis is a 2005 book by Ian Lowe which argues that the warnings from environmental scientists are urgent and unequivocal. Professor Lowe suggests that resources are being used too quickly, environmental systems are being compromised, and society is being destabilised by the increasing gap between rich and poor. Lowe proposes several radical solutions. He advocates a fundamental change to our personal values and social institutions and provides a vision of a healthier society – one that is more humane, takes an eco-centric approach, adopts longer-term thinking, and respects natural systems.\n\n"}
{"id": "3262", "url": "https://en.wikipedia.org/wiki?curid=3262", "title": "Agar", "text": "Agar\n\nAgar (pronounced , sometimes ) or agar-agar is a jelly-like substance, obtained from red algae.\n\nAgar is a mixture of two components: the linear polysaccharide agarose, and a heterogeneous mixture of smaller molecules called agaropectin. It forms the supporting structure in the cell walls of certain species of algae, and is released on boiling. These algae are known as agarophytes, and belong to the Rhodophyta (red algae) phylum.\n\nAgar has been used as an ingredient in desserts throughout Asia, and also as a solid substrate to contain culture media for microbiological work. Agar can be used as a laxative, an appetite suppressant, a vegetarian substitute for gelatin, a thickener for soups, in fruit preserves, ice cream, and other desserts, as a clarifying agent in brewing, and for sizing paper and fabrics.\n\nThe gelling agent in agar is an unbranched polysaccharide obtained from the cell walls of some species of red algae, primarily from \"tengusa\" (\"Gelidiaceae\") and \"ogonori\" (\"Gracilaria\"). For commercial purposes, it is derived primarily from \"ogonori\". In chemical terms, agar is a polymer made up of subunits of the sugar galactose.\n\nAgar may have been discovered in Japan in 1658 by Mino Tarōzaemon (), an innkeeper who, according to legend, was said to have discarded surplus seaweed soup and noticed that it gelled later after a winter night's freezing. Over the following centuries, agar became a common gelling agent in several Southeast Asian cuisines.\n\nAgar was first subjected to chemical analysis in 1859 by the French chemist Anselme Payen, who had obtained agar from the marine algae \"Gelidium corneum\".\n\nBeginning in the late 19th century, agar began to be used heavily as a solid medium for growing various microbes. Agar was first described for use in microbiology in 1882 by the German microbiologist Walther Hesse, an assistant working in Robert Koch's laboratory, on the suggestion of his wife Fannie Hesse. Agar quickly supplanted gelatin as the base of microbiological media, due to its higher melting temperature, allowing microbes to be grown at higher temperatures without the media liquefying.\n\nWith its newfound use in microbiology, agar production quickly increased. This production centered on Japan, which produced most of the world's agar until World War II. However, with the outbreak of World War II, many nations were forced to establish domestic agar industries in order to continue microbiological research. Around the time of World War II, approximately 2,500 tons of agar were produced annually. By the mid-1970s, production worldwide had increased dramatically to approximately 10,000 tons each year. Since then, production of agar has fluctuated due to unstable and sometimes over-utilized seaweed populations.\n\nThe word \"agar\" comes from agar-agar, the Malay name for red algae (\"Gigartina\", \"Gracilaria\") from which the jelly is produced. It is also known as Kanten () (from the phrase \"kan-zarashi tokoroten\" () or “cold-exposed agar”), Japanese isinglass, Ceylon moss or Jaffna moss. \"Gracilaria lichenoides\" is specifically referred to as agal-agal or Ceylon agar.\n\nAgar consists of a mixture of two polysaccharides: agarose and agaropectin, with agarose making up about 70% of the mixture. Agarose is a linear polymer, made up of repeating units of agarobiose, a disaccharide made up of D-galactose and 3,6-anhydro-L-galactopyranose. Agaropectin is a heterogeneous mixture of smaller molecules that occur in lesser amounts, and is made up of alternating units of D-galactose and L-galactose heavily modified with acidic side-groups, such as sulfate and pyruvate.\n\nAgar exhibits hysteresis, melting at 85 °C (358 K, 185 °F) and solidifying from 32–40 °C (305–313 K, 90–104 °F). This property lends a suitable balance between easy melting and good gel stability at relatively high temperatures. Since many scientific applications require incubation at temperatures close to human body temperature (37 °C), agar is more appropriate than other solidifying agents that melt at this temperature, such as gelatin.\n\nAgar-agar is a natural vegetable gelatin counterpart. White and semi-translucent, it is sold in packages as washed and dried strips or in powdered form. It can be used to make jellies, puddings, and custards. For making jelly, it is boiled in water until the solids dissolve. Sweetener, flavouring, colouring, fruit or vegetables are then added and the liquid is poured into molds to be served as desserts and vegetable aspics, or incorporated with other desserts, such as a jelly layer in a cake.\n\nAgar-agar is approximately 80% fiber, so it can serve as an intestinal regulator. Its bulking quality has been behind fad diets in Asia, for example the \"kanten\" (the Japanese word for agar-agar) diet. Once ingested, \"kanten\" triples in size and absorbs water. This results in the consumers feeling fuller. This diet has recently received some press coverage in the United States as well. The diet has shown promise in obesity studies.\n\nOne use of agar in Japanese cuisine (Wagashi) is \"anmitsu\", a dessert made of small cubes of agar jelly and served in a bowl with various fruits or other ingredients. It is also the main ingredient in \"mizu yōkan\", another popular Japanese food.\n\nIn Philippine cuisine, it is used to make the jelly bars in the various gulaman refreshments or desserts such as \"sago gulaman\", \"buko pandan\", \"agar flan\", \"halo-halo\", and the black and red \"gulaman\" used in various fruit salads.\n\nIn Vietnamese cuisine, jellies made of flavored layers of agar agar, called \"thạch\", are a popular dessert, and are often made in ornate molds for special occasions. In Indian cuisine, agar agar is known as \"China grass\" and is used for making desserts. In Burmese cuisine, a sweet jelly known as \"kyauk kyaw\" is made from agar.\n\nIn Russia, it is used in addition or as a replacement to pectin in jams and marmalades, as a substitute to gelatin for its superior gelling properties, and as a strengthening ingredient in souffles and custards. Another use of agar-agar is in \"ptich'ye moloko\" (bird's milk), a rich jellified custard (or soft meringue) used as a cake filling or chocolate-glazed as individual sweets. Agar-agar may also be used as the gelling agent in gel clarification, a culinary technique used to clarify stocks, sauces, and other liquids.\n\nMexico has traditional candies made out of Agar gelatin, most of them in colorful, half-circle shapes that resemble a melon or watermelon fruit slice, and commonly covered with sugar. They are known in Spanish as \"Dulce de Agar\" (Agar sweets)\n\nAgar-agar is an allowed nonorganic/nonsynthetic additive used as a thickener, gelling agent, texturizer, moisturizer, emulsifier, flavor enhancer, and absorbent in certified organic foods.\n\nAn agar plate or Petri dish is used to provide a growth medium using a mix of agar and other nutrients in which microorganisms, including bacteria and fungi, can be cultured and observed under the microscope. Agar is indigestible for many organisms so that microbial growth does not affect the gel used and it remains stable. Agar is typically sold commercially as a powder that can be mixed with water and prepared similarly to gelatin before use as a growth medium. Other ingredients are added to the agar to meet the nutritional needs of the microbes. Many specific formulations are available, because some microbes prefer certain environmental conditions over others. Agar is often dispensed using a sterile media dispenser.\n\nAs a gel, an agar or agarose medium is porous and therefore can be used to measure microorganism motility and mobility. The gel's porosity is directly related to the concentration of agarose in the medium, so various levels of effective viscosity (from the cell's \"point of view\") can be selected, depending on the experimental objectives.\n\nA common identification assay involves culturing a sample of the organism deep within a block of nutrient agar. Cells will attempt to grow within the gel structure. Motile species will be able to migrate, albeit slowly, throughout the gel and infiltration rates can then be visualized, whereas non-motile species will show growth only along the now-empty path introduced by the invasive initial sample deposition.\n\nAnother setup commonly used for measuring chemotaxis and chemokinesis utilizes the under-agarose cell migration assay, whereby a layer of agarose gel is placed between a cell population and a chemoattractant. As a concentration gradient develops from the diffusion of the chemoattractant into the gel, various cell populations requiring different stimulation levels to migrate can then be visualized over time using microphotography as they tunnel upward through the gel against gravity along the gradient.\n\nResearch grade agar is used extensively in plant biology as it is supplemented with a nutrient and vitamin mixture that allows for seedling germination in Petri dishes under sterile conditions (given that the seeds are sterilized as well). Nutrient and vitamin supplementation for \"Arabidopsis thaliana\" is standard across most experimental conditions. Murashige & Skoog (MS) nutrient mix and Gamborg's B5 vitamin mix in general are used. A 1.0% agar/0.44% MS+vitamin dHO solution is suitable for growth media between normal growth temps.\n\nWhen using agar, within any growth medium, it is important to know that the solidification of the agar is pH-dependent. The optimal range for solidification is between 5.4-5.7. Usually, the application of KOH is needed to increase the pH to this range. A general guideline is about 600 µl 0.1M KOH per 250 ml GM. This entire mixture can be sterilized using the liquid cycle of an autoclave.\n\nThis medium nicely lends itself to the application of specific concentrations of phytohormones etc. to induce specific growth patterns in that one can easily prepare a solution containing the desired amount of hormone, add it to the known volume of GM, and autoclave to both sterilize and evaporate off any solvent that may have been used to dissolve the often-polar hormones. This hormone/GM solution can be spread across the surface of Petri dishes sown with germinated and/or etiolated seedlings.\n\nExperiments with the moss \"Physcomitrella patens\", however, have shown that choice of the gelling agent — agar or Gelrite - does influence phytohormone sensitivity of the plant cell culture.\n\nAgar is used:\n\nGelidium agar is used primarily for bacteriological plates. Gracilaria agar is used mainly in food applications.\n\nIn 2016, AMAM, a Japanese company, developed a prototype for Agar-based commercial packaging system called Agar Plasticity, intended as a replacement for oil-based plastic packaging.\n\n"}
{"id": "24764174", "url": "https://en.wikipedia.org/wiki?curid=24764174", "title": "Aktiengesellschaft für Uhrenfabrikation Lenzkirch", "text": "Aktiengesellschaft für Uhrenfabrikation Lenzkirch\n\nAktiengesellschaft für Uhrenfabrikation Lenzkirch (\"Public company for clockmaking Lenzkirch\") was founded in 1851 in the village of Lenzkirch in Baden by Eduard Hauser who had trained in France and Switzerland. It is in the tradition of Black Forest clockmakers. Hauser, the son of a teacher, was born on 21 August 1825 and gained experience of making music boxes with Johann George Schopperle. During this same period he gained a knowledge of metalworking, precision work and the design of musical instruments, as well as a proficiency in the composing of music.\n\nThe firm acquired a reputation for building particularly fine regulators. Up to the 1920s it still produced regulators with compensated pendulums and precision movements. The firm was later taken over by Junghans and the factory closed down in 1932, at which period the market for wall regulators had collapsed.\n\n\n"}
{"id": "3729030", "url": "https://en.wikipedia.org/wiki?curid=3729030", "title": "Alstom Coradia LINT", "text": "Alstom Coradia LINT\n\nThe Alstom Coradia LINT is a single-unit or two-unit articulated railcar manufactured by Alstom, offered in diesel and hydrogen models. The acronym \"LINT\" is short for the German \"leichter innovativer Nahverkehrstriebwagen\" (light innovative local transport rail vehicle). It was designed by Linke-Hofmann-Busch (LHB; acquired 1996 by Alstom) and has been distributed as part of Alstom’s Coradia family.\n\nThe type designation gives the vehicle's length: The one-piece type LINT 27 has a length of and is also known as \"Baureihe 640\" (DB class 640) of Deutsche Bahn. The two-part train, LINT 41, is long. In Germany it is called \"Baureihe 648\" (DB Class 648).\n\nThe Alstom Coradia LINT is part of Alstom Coradia family of Inter-city trains which includes multiple unit diesel (DMU) or electric (EMU) as well as double-decker trains. The LINT family offers capacities ranging from 70 to 300 seated passengers. They operate at speeds ranging from . \n\nThe Coradia trains are manufactured in Salzgitter in Germany, Reichshoffen in France and Savigliano in Italy.\n\nThe Italian adaptation is called Coradia Minuetto, and the French version, used in France and Luxembourg, is classed as Class X 73500 (A-TER).\n\nThe one-piece railcars have engines and a maximum speed of . The train has 52 2nd class seats, eight 1st class 1 seats and 13 tip-up seats. Up to three cars can run together in multiple unit form.\n\nThe trains are predominantly used on non-electrified light railways in North Rhine-Westphalia amongst other regions.\n\nBoth the LINT 41 and LINT 54 consist of two parts. The longer carriage length of the LINT 54 allows for an extra set of doors per carriage, whilst the LINT 41 has only one set per carriage. Some transportation companies offer ticket machines in the door area. The two-piece railcars have two engines.\n\nThe trains are mainly used in Northern Germany and North Rhine-Westphalia. They are also quite popular in other European countries. For example, in Denmark they are being used by the largest non-state-owned operator, Arriva (a total of 43 units: 30 delivered in 2004-2005, 11 delivered in 2010–11 and 2 delivered in 2012) as well as by Lokalbanen A/S and Regionstog (a total of 42 units delivered in 2006-2007). In the eastern provinces of the Netherlands, they are operated by Syntus which is now Keolis Nederland.\n\nThey are also used in Canada. Alstom delivered six new trains to operate on the O-Train Trillium Line in Ottawa. The new trains went into service on 2 March 2015, displacing the previous Bombardier Talent fleet.\n\nLint 41 has 115 seats, while the Lint 54 can have between 150 and 180 seats.\n\nThe LINT 81 is a three carriage set, with two driving vehicles with cabs, and an intermediate vehicle for passenger accommodation only.\n\nIn September 2012, Netinera ordered 63 Coradia LINT trains from Alstom, which would be used on services in Rheinland-Pfalz. The order included some LINT 54 DMUs (160 seats) and 18 Lint 81 (270 seats).\n\nThe Coradia iLint is a version of the Coradia Lint 54 powered by a hydrogen fuel cell. Announced at InnoTrans 2016, the new model will be the world's first production hydrogen-powered trainset. The Coradia iLint is able to reach and travel on a full tank of hydrogen. It is assembled at Alstom's Salzgitter plant. It began rolling tests at 80km/h in March 2017. In September 2018, the first Coradia iLint entered service on the Buxtehude-Bremervörde-Bremerhaven-Cuxhaven line in Lower Saxony, Germany. A mobile hydrogen filling station refuels these trains, however, a stationary station is set to be built by 2021 along with 14 more of these trains. \n\n\n"}
{"id": "641043", "url": "https://en.wikipedia.org/wiki?curid=641043", "title": "Batman (unit)", "text": "Batman (unit)\n\nThe batman () was a unit of mass used in the Ottoman Empire and among Turkic peoples of the Russian Empire. It has also been recorded as a unit of area in Uyghur-speaking regions of Central Asia. The name is Turkic (Ottoman Turkish ; Chagatai ), but was also sometimes used for the equivalent unit in Persia ( من, \"man\"). The equivalent unit in British India was anglicized as the maund. The value of the batman (or maund) varied considerably from place to place.\n\nThe \"man\" as a unit of weight is thought to be of at least Chaldean origin, with Sir Henry Yule attributing Akkadian origins to the word. The Hebrew \"maneh\" (מנה) and the Ancient Greek \"mina\" (μνᾶ) are thought to be cognate. It was originally equal to one-ninth of the weight of an \"artaba\" of water, or approximately four kilograms in modern units. İnalcık believes the ancient Persian \"patimāna\" may have come from the late Assyrian word for \"mana of the king\". The \"man\" or \"batman\" spread throughout Arabia and Persia: it was adopted by the Ottoman Empire, and brought to India by the Mughal Empire. The first attestation which gives a comparison to European weights was by Pegolotti in his \"Pratica della mercatura\", written about 1340. He reported the \"batman\" as the main unit of mass in Ayasluğ (\"Altoluogo di Turchia\" to Pegolotti; modern Selçuk, in western Turkey), equivalent to 32 Genoese pounds (\"libbre\").\n\nThe batman (or bateman) was first recorded in English in 1599, in Babylon (probably modern Baghdad), where it was said to be equal to \"7 pound and 5 ounces English weight\". In the central Ottoman system of weights, the batman was equal to six okas, as is attested in 1811 in Aleppo, 1821 in Baghdad and in 1850 in Constantinople. At this point, the batman was equal to 16 lb 8 oz avoirdupois (7.484 kg).\n\nThe \"mann\" ( مَنّ ) had doubtless formed a part of the Arabian system of weights before the arrival of the Ottomans. It was divided into \"uqiyyas\" (the number varying with the location), while ten \"mann\" made one \"frazil\". A still larger unit of mass was the \"bahar\", of ten to forty \"frazils\". The Arabic \"mann\" was smaller than the Ottoman \"batman\" at about 2–3 lb av. (1–1½ kg), except in Basra where there were two maunds in use, both much larger than either the Arabic \"mann\" or the Ottoman \"batman\".\n\nThe Turkish system of weights and measures was metrified in 1931. The \"oka\" was redefined as exactly one kilogram, while the \"batman\" became ten \"okas\" (10 kg).\n\nThe batman was used in Central Asia up until at least the 18th century. In Khiva in 1740, there were said to be two batmans (as in Persia): the \"great batman\" of 18 Russian pounds (, \"funt\"; approx. 7.4 kg) and the \"lesser batman\" of 9¼ Russian pounds (approx. 3.8 kg).\n\nIn Uyghur, the batman was also a measure of land area, the area that could be sown with one batman (in mass) of seed.\n\nThe Tatar batman is an equivalent to 1000 pood or 16.4 tonnes.\n\nThe two main commercial weights in Persia were the \"tabrézy man\" ( من تبریز ), literally the \"man\" of Tabriz, and the \"sháhy man\" ( من شاء ), literally the Shah's \"man\", which was twice as large. The \"sháhy man\" was particularly used in Shiraz and Isfahan. Kelly also distinguishes a \"man\" used for copra and \"provisions\" at Gamron (modern Bandar-Abbas) of 7 lb 12 oz av. (3.5153 kg).\n\nThe United Nations Statistical Office found a wide range of values for the \"man\" in Iran in 1966, from 3 kg to 53 kg. The \"man\" was divided into \"mithqals\" (the number depending on the locality): larger subdivisions included the \"abbassi\" and the \"ratl\". The term \"batman\" appears to be reserved for the \"tabrézy man\", approximately 2.969 kg in 1966.\n\nThe \"mann\" () was and still is also used as a unit of mass in Afghanistan, but varied widely between different localities. In Kandahar it was about 8 lb av. (3½ kg), while in Peshawar it was 80 lb av. (35 kg).\n\n"}
{"id": "48659244", "url": "https://en.wikipedia.org/wiki?curid=48659244", "title": "Bauxite tailings", "text": "Bauxite tailings\n\nBauxite tailings, bauxite residue or alumina refinery residues (ARR) is a by-product in the production of alumina (aluminium oxide). Alumina is the principal raw material used in the manufacture of aluminium metal and also widely used in the manufacture of ceramics, abrasives and refractories. The scale of production makes the waste product an important one, and issues with its storage are reviewed and every opportunity is explored to find uses for it. Over 95% of the alumina produced globally is through the Bayer process; for every tonne of alumina produced, approximately 1 to 1.5 tonnes of bauxite tailings/residue are also produced. Annual production of alumina in 2015 was approximately 115 million tonnes resulting in the generation of about 150 million tonnes of bauxite tailings/residue.\n\nThere are over 60 manufacturing operations across the world using the Bayer process to make alumina from bauxite ore. Bauxite ore is mined, normally in open cast mines, and transferred to an alumina refinery for processing. To extract the alumina, the soluble part of the bauxite ore is dissolved using sodium hydroxide under conditions of high temperature and pressure. The insoluble part of the bauxite (the residue) is removed, giving rise to a solution of sodium aluminate, which is then seeded and allowed to cool and aluminium hydroxide precipitates from the solution. Though some of the aluminium hydroxide is then returned and used to seed the next batch, the remainder is calcined (heated) at over 1000 °C in rotary kilns or fluid flash calciners to produce aluminium oxide (alumina). The alumina content of the bauxite used is normally about 50% but ores with a much wider range of alumina contents can be used; the aluminium compound may be present as gibbsite (Al(OH)), boehmite (AlOOH) or diaspore (AlOOH). The tailings/residue invariably has a high concentration of iron oxide which gives the product a characteristic red colour. A small residual amount of the sodium hydroxide used in the process remains with the tailings, causing the material to have a high pH/alkalinity, normally >12. Various stages in the solid/liquid separation process are introduced to recycle as much sodium hydroxide as possible from the residue back into the Bayer Process in order to make the process as efficient as possible and reduce production costs. This also lowers the final alkalinity of the tailings making it easier to handle.\n\nThe main constituents of the residue after the extraction of the aluminium component are unreacted metallic oxides. The percentage of these oxides produced by a particular alumina refinery will depend on the quality and nature of the bauxite ore and the extraction conditions. The table below shows the composition ranges for common chemical constituents, but the values vary widely:\n\nMineralogically expressed the components present are:\n\nThe objective is to remove as much of the aluminium containing component as economically possible. In general, the composition of the residue reflects that of the non-aluminium components, with the exception of part of the silicon component: crystalline silica (quartz) will not react but some of the silica present, often termed, reactive silica, will react under the extraction conditions and form sodium aluminium silicate.\n\nTailings storage methods have changed substantially since the original plants were built. The practice in early years was to pump the tailings slurry, at a concentration of about 20% solids, into lagoons or ponds sometimes created in former bauxite mines or depleted quarries. In other cases, impoundments were constructed with dams or levees, whilst for some operations valleys were dammed and the tailings deposited in these holding areas.\n\nIt was also common practice for the tailings to be discharged into rivers, estuaries, or the sea via pipelines or barges; in other instances the residue was shipped out to sea and disposed of in deep ocean trenches many kilometres offshore. All disposal in the sea, estuaries and rivers has now stopped. As residue storage space ran out and concern increased over wet storage, since the mid-1980s dry stacking has been increasingly adopted.\n\nIn this method, tailings are thickened to a high density slurry (48-55% solids or higher), and then deposited in a way that it consolidates and dries.\n\nAn increasing popular storage method is filtration whereby a filter cake (typically <30% solids) is produced. This cake can be washed with either water or steam to reduce alkalinity before being transported and stored as a semi-dried material. Residue produced in this form is ideal for reuse as it has lower alkalinity, is cheaper to transport, and is easier to handle and process.\n\nSince the Bayer process was first adopted industrially in 1894, the value of the remaining oxides has been recognised. Attempts have been made to recover the principal components – especially iron. Since mining began, an enormous amount of research effort has been devoted to seeking uses for the residue. Possible applications can be broadly broken down into various categories:\nrecovery of specific components present in the tailings/residue, e.g. iron, titanium, rare-earth elements REEs;\nuse as a major component in manufacture of another product, e.g. cement; use of the bauxite residue as a component in building or construction material, e.g. concrete, tiles, bricks; soil amelioration or capping; and conversion of the residue to a useful compound, such as through the Virotec process.\n\nThe wide compositional range of the residue has resulted in an enormous number of technically feasible applications including: cement manufacture, use in concrete as a supplementary cementious material, iron recovery, titanium recovery, use in building panels, bricks, foamed insulating bricks, tiles, gravel/railway ballast, soil amelioration, calcium and silicon fertiliser, refuse tip capping/site restoration, lanthanides (rare earths) recovery, scandium recovery, gallium recovery, yttrium recovery, treatment of acid mine drainage, adsorbent of heavy metals, dyes, phosphates, fluoride, water treatment chemical, glass ceramics, ceramics, foamed glass, pigments, oil drilling or gas extraction, filler for PVC, wood substitute, geopolymers, catalysts, plasma spray coating of aluminium and copper, manufacture of aluminium titanate-Mullite composites for high temperature resistant coatings, desulfurisation of flue gas, arsenic removal, chromium removal, soil amelioration.\n\nIt is estimated that some 2 to 3.5 million tonnes of the bauxite residue produced annually is used in some way:\n\n\nIn 2015 a major initiative was launched in Europe with funds from the European Union to address the valorisation of bauxite residue. Some 15 Ph.D students have been recruited as part the European Training Network for Zero-Waste Valorisation of Bauxite Residue. The key focus will be the recovery of iron, aluminium, titanium and rare-earth elements (including scandium) while valorising the residue into building materials.\n\n\nData on global production of aluminium and aluminium oxide. http://www.world-aluminium.org\n\nB. K. Parekh and W. M. Goldberger, “An assessment of technology for the possible utilization of Bayer process muds”, published by the U. S. Environmental Protection Agency, EPA 600/2-76-301.\n\nWanchao Liu, Jiakuan Yang, Bo Xiao, “Review on treatment and utilization of bauxite residues in China”, in International Journal of Mineral Processing, 93, 220–231 (2009).\n\nM. B. Cooper, “Naturally Occurring Radioactive Material (NORM) in Australian Industries”, EnviroRad report ERS-006 prepared for the Australian Radiation Health and Safety Advisory Council (2005).\n\nY. Pontikes, G. N. Angelopoulos, B. Blanpain, “Radioactive elements in Bayer’s process bauxite residue and their impact in valorization options”, Transportation of NORM, NORM Measurements and Strategies, Building Materials, Advances in Sci. and Tech, 45, 2176–2181 (2006).\n\nW. K. Biswas and D. J. Cooling, “Sustainability Assessment of Red SandTM as a substitute for Virgin Sand and Crushed Limestone”, J. of Ind. Ecology, 17(5) 756–762 (2013).\n\nAgrawal, K. K. Sahu, B. D. Pandey, \"Solid waste management in non-ferrous industries in India\", Resources, Conservation and Recycling 42 (2004), 99–120.\n\nJongyeong Hyuna, Shigehisa Endoha, Kaoru Masudaa, Heeyoung Shinb, Hitoshi Ohyaa, \"Reduction of chlorine in bauxite residue by fine particle separation\", Int. J. Miner. Process., 76, 1-2, (2005), 13–20.\n\nClaudia Brunori, Carlo Cremisini, Paolo Massanisso, Valentina Pinto, Leonardo Torricelli, \"Reuse of a treated red mud bauxite waste: studies on environmental compatibility\", Journal of Hazardous Materials, 117(1), (2005), 55–63.\n\nH. Genc¸-Fuhrman, J. C. Tjell, D. McConchie, \"Increasing the arsenate adsorption capacity of neutralized red mud (Bauxsol™)\", J. Colloid Interface Sci. 271 (2004) 313–320.\n\nH. Genc¸-Fuhrman, J. C. Tjell, D. McConchie, \"Adsorption of arsenic from water using activated neutralized red mud\", Environ. Sci. Technol. 38 (2004) 2428–2434.\n\nH. Genc¸-Fuhrman, J. C. Tjell, D. McConchie, O. Schuiling, \"Adsorption of arsenate from water using neutralized red mud\", J. Colloid Interface Sci. 264 (2003) 327–334.\n\nhttp://etn.redmud.org/project/\n"}
{"id": "6216692", "url": "https://en.wikipedia.org/wiki?curid=6216692", "title": "Bone cement", "text": "Bone cement\n\nBone cements have been used very successfully to anchor artificial joints (hip joints, knee joints, shoulder and elbow joints) for more than half a century. Artificial joints (referred to as prostheses) are anchored with bone cement. The bone cement fills the free space between the prosthesis and the bone and plays the important role of an elastic zone. This is necessary because the human hip is acted on by approximately 10-12 times the body weight and therefore the bone cement must absorb the forces acting on the hips to ensure that the artificial implant remains in place over the long term.\n\nBone cement chemically is nothing more than Plexiglas (i.e. polymethyl methacrylate or PMMA). PMMA was used clinically for the first time in the 1940s in plastic surgery to close gaps in the skull. Comprehensive clinical tests of the compatibility of bone cements with the body were conducted before their use in surgery. The excellent tissue compatibility of PMMA allowed bone cements to be used for anchorage of head prostheses in the 1950s.\n\nToday several million procedures of this type are conducted every year all over the world and more than half of them routinely use bone cements - and the proportion is increasing. Bone cement is considered a reliable anchorage material with its ease of use in clinical practice and particularly because of its proven long survival rate with cemented-in prostheses. Hip and knee registers for artificial joint replacements such as those in Sweden and Norway clearly demonstrate the advantages of cemented-in anchorage. A similar register for endoprosthesis was introduced in Germany in 2010.\n\nBone cements are provided as two-component materials. Bone cements consist of a powder (i.e., pre-polymerized PMMA and or PMMA or MMA co-polymer beads and or amorphous powder, radio-opacifer, initiator) and a liquid (MMA monomer, stabilizer, inhibitor). The two components are mixed and a free radical polymerization occurs of the monomer when the initiator is mixed with the accelerator. The bone cement viscosity changes over time from a runny liquid into a dough like state that can be safely applied and then finally hardens into solid hardened material. The set time can be tailored to help the physician safely apply the bone cement into the bone bed to either anchor metal or plastic prosthetic device to bone or used alone in the spine to treat osteoporotic compression fractures.\n\nDuring the exothermic free-radical polymerization process, the cement heats up. This polymerization heat reaches temperatures of around 82-86 °C in the body. This temperature is higher than the critical level for protein denaturation in the body. The cause of the low polymerization temperature in the body is the relatively thin cement coating, which should not exceed 5 mm, and the temperature dissipation via the large prosthesis surface and the flow of blood.\n\nThe individual components of the bone cement are also known in the area of dental filler materials. Acrylate-based plastics are also used in these applications. While the individual components are not always perfectly safe as pharmaceutical additives and active substances per se, as bone cement the individual substances are either converted or fully enclosed in the cement matrix during the polymerization phase from the increase in viscosity to curing. From current knowledge, cured bone cement can now be classified as safe, as originally demonstrated during the early studies on compatibility with the body conducted in the 1950s.\n\nMore recently bone cement has been use in the spine in either vertebroplasty or kyphoplasty procedures. The composition of these types of cement is mostly based on calcium phosphate and more recently magnesium phosphate. A novel biodegradable, non-exothermic, self-setting orthopedic cement composition based on amorphous magnesium phosphate (AMP) was developed. The occurrence of undesirable exothermic reactions was avoided through using AMP as the solid precursor \n\nWhat is referred to as bone cement implantation syndrome (BCIS) is described in the literature. For a long time it was believed that the incompletely converted monomer released from bone cement was the cause of circulation reactions and embolism. However, it is now known that this monomer (residual monomer) is metabolized by the respiratory chain and split into carbon dioxide and water and excreted. Embolisms can always occur during anchorage of artificial joints when material is inserted into the previously cleared femoral canal. The result is intramedullary pressure increase, potentially driving fat into the circulation.\n\nIf the patient is known to have any allergies to constituents of the bone cement, according to current knowledge bone cement should not be used to anchor the prosthesis. Anchorage without cement - cement-free implant placement - is the alternative.\n\nNew bone cement formulations require characterization according to ASTM F451. This standard describes the test methods to assess cure rate, residual monomer, mechanical strength, benzoyl peroxide concentration, and heat evolution during cure.\n\nRevision is the replacement of a prosthesis. This means that a prosthesis previously implanted in the body is removed and replaced by a new prosthesis. Compared to the initial operation revisions are often more complex and more difficult, because every revision involves the loss of healthy bone substance. Revision operations are also more expensive for a satisfactory result. The most important goal is therefore to avoid revisions by using a good surgical procedure and using products with good (long-term) results.\n\nUnfortunately, it is not always possible to avoid revisions. There can also be different reasons for revisions and there is a distinction between septic or aseptic revision. If it is necessary to replace an implant without confirmation of an infection—for example, aseptic—the cement is not necessarily removed completely. However, if the implant has loosened for septic reasons, the cement must be fully removed to clear an infection. In the current state of knowledge it is easier to remove cement than to release a well-anchored cement-free prosthesis from the bone site. Ultimately it is important for the stability of the revised prosthesis to detect possible loosening of the initial implant early to be able to retain as much healthy bone as possible.\n\nA prosthesis fixed with bone cement offers very high primary stability combined with fast remobilization of patients. The cemented-in prosthesis can be fully loaded very soon after the operation because the PMMA gets most of its strength within 24 hours. The necessary rehabilitation is comparatively simple for patients who have had a cemented-in prosthesis implanted. The joints can be loaded again very soon after the operation, but the use of crutches is still required for a reasonable period for safety reasons.\n\nBone cement has proven particularly useful because specific active substances, e.g. antibiotics, can be added to the powder component. The active substances are released locally after implant placement of the new joint, i.e. in the immediate vicinity of the new prosthesis and have been confirmed to reduce the danger of infection. The antibiotics act against bacteria precisely at the site where they are required in the open wound without subjecting the body in general to unnecessarily high antibiotic levels. This makes bone cement a modern drug delivery system that delivers the required drugs directly to the surgical site. The important factor is not how much active substance is in the cement matrix but how much of the active substance is actually released locally. Too much active substance in the bone cement would actually be detrimental, because the mechanical stability of the fixed prosthesis is weakened by a high proportion of active substance in the cement. The local active substance levels of industrially manufactured bone cements that are formed by the use of bone cements that contain active substances are approximate (assuming that there is no incompatibility) and are significantly below the clinical routine dosages for systemic single injections.\n\n\n"}
{"id": "30245700", "url": "https://en.wikipedia.org/wiki?curid=30245700", "title": "Clarkston explosion", "text": "Clarkston explosion\n\nThe Clarkston explosion was a disaster that occurred on 21 October 1971 at a shopping centre in Clarkston, Renfrewshire, Scotland. The death toll has been stated variously as 21 and 22.\n\nThe explosion followed a build-up of gas in an underground space beneath the Clarkston Toll shopping centre, caused by a gas main leak later ruled to have been accidental. Customers and shop staff had on 20 October complained of a strong smell of gas in the centre and Scottish Gas engineers had attended to investigate, but had identified no source for the smell. The engineers were still in attendance at around 2:50pm on the 21st when the gas ignited and exploded, killing at least 21 people and injuring around 100. The victims included many shop staff and people on shopping trips, and the passengers of a bus that had been passing the scene. The explosion destroyed several shops and a terraced car park. \n\nAn inquiry was held, and a jury on 11 February 1972 returned a verdict that no fault for the explosion lay with any organization or individual. No cause was identified for the ignition of the leaked gas, and the leak itself was deemed the result of an accidental gas main fracture caused by \"stress and corrosion\".\n\nThe victims of the disaster are commemorated in a plaque erected in 2001/2 near the site of the explosion. \n\nThere is a further tribute to those who lost their lives situated in the entranceway to the Clarkston Halls.\n\nScottish Television produced a programme on the Clarkston Disaster which aired on 20 November 2017\n"}
{"id": "40930756", "url": "https://en.wikipedia.org/wiki?curid=40930756", "title": "Coal burner", "text": "Coal burner\n\nA coal burner (or pulverized coal burner) is a mechanical device that burns pulverized coal (also known as powdered coal or coal dust since it is as fine as face powder in cosmetic makeup) into a flame in a controlled manner.\nCoal burner is mainly composed of pulverized coal machine, host of combustion machine (including combustion chamber, automatic back and forth motion system, automatic rotation system, the combustion air supply system) control system, ignition system, the crater and others.\n\nIn the worksite, a coal burner works with the coal pulverizer and coal hopper usually. The coal in the hopper is conveyed to the coal pulverizer by screw conveyor. The coal pulverizer will crush the coal into pulverized coal. In the coal burner, the pulverized coal mixes with air (High-speed air flow is generated by the draft fan on the coal burner), and is ignited by the oil burning igniter.\n\nThere are mainly two ways to ignite the coal burner. Manual way and automatic way, no matter which way it adopts, the coal burner often needs fuel (oil, gas, etc.) as the combustion medium. The difference is that the high-energy ignition devices which generate sparks replace people's hands.\n\nPulverized coal burners have a wide range of uses in industrial production and daily life, such as providing heat for boilers, hot mix asphalt plant, cement kiln, metal furnace, annealing, quenching furnace, precision casting shell burning furnace, melting furnace, forging furnace and other heating furnace or kiln.\n\n\"Note: These indexes are the lowest requirements for the coal, the better coal will be better at practice.\"\n\n\n"}
{"id": "11530184", "url": "https://en.wikipedia.org/wiki?curid=11530184", "title": "Convective condensation level", "text": "Convective condensation level\n\nThe convective condensation level (CCL) represents the height (or pressure) where an air parcel becomes saturated when heated from below and lifted adiabatically due to buoyancy.\n\nIn the atmosphere, assuming a constant water vapor mixing ratio, the dew point temperature (the temperature where the\nrelative humidity is 100%) decreases with increasing height because the pressure of the atmosphere decreases with height. The CCL is determined by plotting the dew point (100%RH) verses altitude and locating the intersection with the actual measured temperature sounding. It marks where the cloud base begins when air is heated from below to the convective temperature, without mechanical lift. \n\nOnce the CCL is determined, the surface temperature necessary to raise a mass of air to that height can be found by using the Dry Adiabatic Lapse Rate (DALR) to determine the potential temperature. In the early morning, this temperature is typically larger than the surface temperature, in the mid-afternoon, it may be the same.\n\nCompare this to the Lifting Condensation Level (LCL) where the air is lifted and cooled without first increasing the surface temperature. The LCL is less than or equal to the CCL depending on the temperature profile.\n\nBoth condensation levels indicate the altitude (or pressure) where relative humidity reaches 100%. However, since the actual condensation level depends on the availability of condensation nuclei, clouds typically do not form until the relative humidity is somewhat above 100%. \n\n\n"}
{"id": "1248251", "url": "https://en.wikipedia.org/wiki?curid=1248251", "title": "Coulomb explosion", "text": "Coulomb explosion\n\nCoulomb explosion is a mechanism for coupling electronic excitation energy from intense electromagnetic fields into the atomic motion. The Coulombic repulsion of particles having the same electric charge can break the bonds that hold solids together. When done with a narrow laser beam, a small amount of solid explodes into a plasma of ionized atomic particles. It may be shown that the Coulomb explosion\noccurs in the same critical parameter regime as the superradiant phase transition i.e. when the destabilizing interactions become overwhelming and dominate over the native oscillatory phonon solid cluster binding motions which is also characteristic for the diamond synthesis.\n\nWith their low mass, outer valence electrons responsible for chemical bonding are easily stripped from atoms, leaving them positively charged. Given a mutually repulsive state between atoms whose chemical bonds are broken, the material explodes into a small plasma cloud of energetic ions with higher velocities than seen in thermal emission.\n\nA Coulomb explosion is one particular mechanism that permits laser-based machining.\n\nCoulomb explosions for industrial machining are made with ultra-short (picosecond or femtoseconds) laser pulses. The enormous beam intensities required (10–400 terawatt per square centimeter thresholds, depending on material) are only practical to generate, shape, and deliver for very brief instants of time.\n\nA Coulomb explosion is a \"cold\" alternative to the dominant laser etching technique of thermal ablation, which depends on local heating, melting, and vaporization of molecules and atoms using less-intense beams. Pulse brevity down only to the nanosecond regime is sufficient to localize thermal ablation – before the heat is conducted far, the energy input (pulse) has ended. Nevertheless, thermally ablated materials may seal pores important in catalysis or battery operation, and recrystallize or even burn the substrate, thus changing the physical and chemical properties at the etch site. In contrast, even light foams remain unsealed after ablation by Coulomb explosion.\n\nCoulomb explosion etching can be used in any material to bore holes, remove surface layers, and texture and microstructure surfaces; e.g., to control ink loading in printing presses.\n\nHigh speed camera imaging of alkali metals exploding in water has suggested the explosion is a coulomb explosion.\n\nDuring a nuclear explosion based on the fission of uranium, 167 MeV is emitted in the form of a coulombic explosion between each prior nuclei of uranium, the repulsive electrostatic energy between the two fission daughter nuclei, translates into the kinetic energy of the fission products that results in both the primary driver of the blackbody radiation that rapidly generates the hot dense plasma/nuclear fireball formation and thus also both later blast and thermal effects.\n\n"}
{"id": "3581021", "url": "https://en.wikipedia.org/wiki?curid=3581021", "title": "Electrical bonding", "text": "Electrical bonding\n\nElectrical bonding is the practice of intentionally electrically connecting all exposed metallic items not designed to carry electricity in a room or building as protection from electric shock. If a failure of electrical insulation occurs, all bonded metal objects in the room will have substantially the same electrical potential, so that an occupant of the room cannot touch two objects with significantly different potentials. Even if the connection to a distant earth ground is lost, the occupant will be protected from dangerous potential differences.\n\nIn a building with electricity it is normal for safety reasons to connect all metal objects such as pipes together to the mains earth to form an equipotential zone. This is done in the UK because many buildings are supplied with a single phase supply cable where the neutral and earth conductors are combined. Close to the electricity meter this conductor is divided into two, the \"earth\" terminal and the wire going to the neutral busbar in the consumer unit. If the ground connection to the neutral is lost, all wiring and other objects tied to the neutral will be energized at the line voltage. Examples of articles that may be bonded include metallic water piping systems, gas piping, ducts for central heating and air conditioning systems, and exposed metal parts of buildings such as hand rails, stairs, ladders, platforms and floors. \n\nA person touching the un-earthed metal casing of an electrical device, while also in contact with a metal object connected to remote earth, is exposed to an electric shock hazard if the device has a fault. If all metal objects are connected, all the metal objects in the building will be at the same potential. It then will not be possible to get a shock by touching two 'earthed' objects at once.\n\nBonding is particularly important for bathrooms, swimming pools and fountains. In pools and fountains, any metallic object (other than conductors of the power circuit) over a certain size must be bonded to assure that all conductors are at the same potential. Since it is buried in the ground, a pool can be a better ground than the electric panel ground. With all the conducting elements bonded, it is less likely that electric current will find a path through a swimmer. In concrete pools even the reinforcing bars of the concrete must be connected to the bonding system to ensure no dangerous potential gradients are produced during a fault.\n\nIn a system with a grounded (earthed) neutral, connecting all non-current-carrying metallic parts of equipment to earth ground at the main service panel will ensure that current due to faults (such as a \"hot\" wire touching the frame or chassis of the device) will be diverted to earth. In a TN system where there is a direct connection from the installation earth to the transformer neutral, earthing will allow the branch circuit over-current protection (a fuse or circuit breaker) to detect the fault rapidly and interrupt the circuit. \nIn the case of a TT system where the impedance is high due to the lack of direct connection to the transformer neutral, an RCD must be used to provide disconnection. RCDs are also used in other situations where rapid disconnection of small earth faults (including a human touching a live wire by accident, or damage) is desired.\n\nEquipotential bonding involves joining together metalwork that is or may be earthed so that it is at the same potential (i.e., voltage) everywhere. Such is commonly used under transformer banks by power companies and under large computer installations. Exact rules for electrical installations vary by country, locality, or supplying power company.\nEquipotential bonding is done from the Service Panel consumer unit (also known as a fuse box, breaker box, or distribution board) to incoming water and gas services. It is also done in bathrooms where all exposed metal that leaves the bathroom including metal pipes and the earths of electrical circuits must be bonded together to ensure that they are always at the same potential. Isolated metal objects, including metal fittings fed by plastic pipe, are not required to be bonded.\nIn Australia and South Africa, a house's earth cables must be connected both to an earthing rod driven into the ground and also to the plumbing or gas pipe.\nIn Australia a house's earth cable must be connected to all reinforcing mesh in any concrete under any bathroom and in any swimming pool.\n\nIn aircraft, electrical bonding prevents static electricity build-up that can interfere with radio and navigational equipment. Bonding also provides lightning protection by allowing the current to pass through the airframe with minimum arcing. Bonding prevents dangerous static discharges in aircraft fuel tanks and hoses.\n\n\nAircraft hose electrical bonding\n"}
{"id": "11567567", "url": "https://en.wikipedia.org/wiki?curid=11567567", "title": "Energy-efficient driving", "text": "Energy-efficient driving\n\nEnergy-efficient driving techniques are used by drivers who wish to reduce their fuel consumption, and thus maximize fuel efficiency. The use of these techniques is called \"hypermiling\".\n\nSimple fuel-efficiency techniques can result in reduction in fuel consumption without resorting to radical fuel-saving techniques that can be unlawful and dangerous, such as tailgating larger vehicles.\n\nUnderinflated tires wear out faster and lose energy to rolling resistance because of tire deformation. The loss for a car is approximately 1.0% for every drop in pressure of all four tires. Improper wheel alignment and high engine oil kinematic viscosity also reduce fuel efficiency.\n\nDrivers can increase fuel efficiency by minimizing transported mass, i.e. the number of people or the amount of cargo, tools, and equipment carried in the vehicle. Removing common unnecessary accessories such as roof racks, brush guards, wind deflectors (or \"spoilers\", when designed for downforce and not enhanced flow separation), running boards, and push bars, as well as using narrower and lower profile tires will improve fuel efficiency by reducing weight, aerodynamic drag, and rolling resistance. Some cars also use a half size spare tire, for weight/cost/space saving purposes. On a typical vehicle, every extra 100 pounds increases fuel consumption by 2%. Removing roof racks (and accessories) can decrease fuel consumption by up to 20%.\n\nMaintaining an efficient speed is an important factor in fuel efficiency. Optimal efficiency can be expected while cruising at a steady speed, at minimal throttle and with the transmission in the highest gear (see Choice of gear, below). The optimal speed varies with the type of vehicle, although it is usually reported to be between 35 mph (56 km/h) and 50 mph (80 km/h). For instance, a 2004 Chevrolet Impala had an optimum at 42 mph (70 km/h), and was within 15% of that from 29 to 57 mph (45 to 95 km/h). At higher speeds, wind resistance plays an increasing role in reducing energy efficiency.\n\nHybrids typically get their best fuel efficiency below this model-dependent threshold speed. The car will automatically switch between either battery powered mode or engine power with battery recharge. Electric cars such as the Tesla Model S may go up to at 39 km/h (24 mph).\n\nRoad capacity affects speed and therefore fuel efficiency as well. Studies have shown speeds just above allow greatest throughput when roads are congested. Individual drivers can improve their fuel efficiency and that of others by avoiding roads and times where traffic slows to below . Communities can improve fuel efficiency by adopting speed limits or policies to prevent or discourage drivers from entering traffic that is approaching the point where speeds are slowed below . \"Congestion pricing\" is based on this principle; it raises the price of road access at times of higher usage, to prevent cars from entering traffic and lowering speeds below efficient levels.\n\nResearch has shown that mandated speed limits can be modified to improve energy efficiency anywhere from 2% to 18%, depending on compliance with lower speed limits.\n\nEngine efficiency varies with speed and torque. For driving at a steady speed one cannot choose any operating point for the engine—rather there is a specific amount of power needed to maintain the chosen speed. A manual transmission lets the driver choose between several points along the powerband. For a turbo diesel too low a gear will move the engine into a high-rpm, low-torque region in which the efficiency drops off rapidly, and thus best efficiency is achieved near the higher gear. In a gasoline engine, efficiency typically drops off more rapidly than in a diesel because of throttling losses. Because cruising at an efficient speed uses much less than the maximum power of the engine, the optimum operating point for cruising at low power is typically at very low engine speed, around or below 1000 rpm. This explains the usefulness of very high \"overdrive\" gears for highway cruising. For instance, a small car might need only to cruise at . It is likely to be geared for 2500 rpm or so at that speed, yet for maximum efficiency the engine should be running at about 1000 rpm to generate that power as efficiently as possible for that engine (although the actual figures will vary by engine and vehicle).\n\nFuel efficiency varies with the vehicle. Fuel efficiency during acceleration generally improves as RPM increases until a point somewhere near peak torque (brake specific fuel consumption). However, accelerating to a greater than necessary speed without paying attention to what is ahead may require braking and then after that, additional acceleration. Experts recommend accelerating quickly, but smoothly.\n\nGenerally, fuel efficiency is maximized when acceleration and braking are minimized. So a fuel-efficient strategy is to anticipate what is happening ahead, and drive in such a way so as to minimize acceleration and braking, and maximize coasting time.\n\nThe need to brake is sometimes caused by unpredictable events. At higher speeds, there is less time to allow vehicles to slow down by coasting. Kinetic energy is higher, so more energy is lost in braking. At medium speeds, the driver has more time to choose whether to accelerate, coast or decelerate in order to maximize overall fuel efficiency.\n\nWhile approaching a red signal, drivers may choose to \"time a traffic light\" by easing off the throttle before the signal. By allowing their vehicle to slow down early and coast, they will give time for the light to turn green before they arrive, preventing energy loss from having to stop.\n\nDue to stop and go traffic, driving during rush hours is fuel inefficient and produces more toxic fumes.\n\nConventional brakes dissipate kinetic energy as heat, which is irrecoverable. Regenerative braking, used by hybrid/electric vehicles, recovers some of the kinetic energy, but some energy is lost in the conversion, and the braking power is limited by the battery's maximum charge rate and efficiency.\n\nAn alternative to acceleration or braking is coasting, i.e. gliding along without propulsion. Coasting dissipates stored energy (kinetic energy and gravitational potential energy) against aerodynamic drag and rolling resistance which must always be overcome by the vehicle during travel. If coasting uphill, stored energy is also expended by , but this energy is not dissipated since it becomes stored as gravitational potential energy which might be used later on. Using stored energy (via coasting) for these purposes is more efficient than dissipating it in friction braking.\n\nWhen coasting with the engine running and manual transmission in neutral, or clutch depressed, there will still be some fuel consumption due to the engine needing to maintain idle engine speed.\n\nCoasting with a vehicle not in gear is prohibited by law in most U.S. states. An example is Maine Revised Statutes Title 29-A, Chapter 19, §2064 \"An operator, when traveling on a downgrade, may not coast with the gears of the vehicle in neutral\". Some regulations differ between commercial vehicles not to disengage the clutch for a downgrade, and passenger vehicles to set the transmission to neutral. These regulations point on how drivers operate a vehicle. Not using the engine on longer, precipitous downgrade roads, or excessively using the brake might cause a failure due to overheating brakes.\n\nTurning the engine off instead of idling does save fuel. Traffic lights are in most cases predictable, and it is often possible to anticipate when a light will turn green. A support is the Start-stop system, turning the engine off and on automatically during a stop. Some traffic lights (in Europe and Asia) have timers on them, which assist the driver in using this tactic.\n\nSome hybrids must keep the engine running whenever the vehicle is in motion and the transmission engaged, although they still have an \"auto-stop\" feature which engages when the vehicle stops, avoiding waste. Maximizing use of auto-stop on these vehicles is critical because idling causes a severe drop in instantaneous fuel-mileage efficiency to zero miles per gallon, and this lowers the average (or accumulated) fuel-mileage efficiency.\n\nA driver may improve their fuel efficiency by anticipating the movement of other vehicles or sudden changes to the situation the driver is currently in. For example, a driver who stops quickly, or turns without signaling, reduces the options another driver has for maximizing their performance. By always giving road users as much information about their intentions as possible, a driver can help other road users reduce their fuel usage (as well as increase their safety). Similarly, anticipation of road features such as traffic lights can reduce the need for excessive braking and acceleration. Drivers should also anticipate the behaviour of pedestrians or animals in the vicinity, so they can react to a developing situation involving them appropriately.\n\nUsing air conditioning requires the generation of up to of extra power to maintain a given speed. A/C systems cycle on and off, or vary their output, as required by the occupants so they rarely run at full power continuously. Switching off the A/C and rolling down the windows may prevent this loss of energy, though it will increase drag, so that cost savings may be less than is generally anticipated. Using the passenger heating system slows the rise to operating temperature for the engine. Either the choke in a carburetor-equipped car (1970's or earlier) or the fuel injection computer in modern vehicles will add more fuel to the fuel-air mixture until normal operating temperature is reached, decreasing fuel efficiency.\n\nUsing high octane gasoline fuel in a vehicle that does not need it is generally considered an unnecessary expense, although Toyota \"has\" measured slight differences in efficiency due to octane number even when knock is not an issue. All vehicles in the United States built since 1996 are equipped with OBD-II on-board diagnostics and most models will have knock sensors that will automatically adjust the timing if and when pinging is detected, so low octane fuel can be used in an engine designed for high octane, with some reduction in efficiency and performance. If the engine is designed for high octane then higher octane fuel will result in higher efficiency and performance under certain load and mixture conditions. The energy released during combustion of hydrocarbon fuel increases as the molecule chain length decreases, so gasoline fuels with higher ratios of the shorter chain alkanes such as heptane, hexane, pentane, etc. can be used under certain load conditions and combustion chamber geometries to increase engine output which can lead to lower fuel consumption, although these fuels will be more susceptible to predetonation ping in high compression ratio engines. Gasoline direct injection compression ignition engines make more efficient use of the higher combustion energy short chain hydrocarbons as the fuel is injected directly into the combustion chamber during high compression which auto-ignites the fuel, minimizing the amount of time that the fuel is available in the combustion chamber for predetonation.\n\n\"Pulse and glide\" (PnG) or \"burn and coast\" driving strategy consists of rapid acceleration to a given speed (the \"pulse\" or \"burn\"), followed by a period of coasting or gliding down to a lower speed, at which point the burn-coast sequence is repeated. Coasting is most efficient when the engine is not running, although some gains can be realized with the engine on (to maintain power to brakes, steering and ancillaries) and the vehicle in neutral. Most modern petrol vehicles cut off the fuel supply completely when coasting (over-running) in gear, although the moving engine adds considerable frictional drag and speed is lost more quickly than with the engine declutched from the drivetrain.\n\nThe pulse-and-glide strategy is proven to be an efficient control design both in car-following and free-driving scenarios, with 20% fuel saving. In the PnG strategy, the control of the engine and the transmission determines the fuel-saving performance, and it is obtained by solving an optimal control problem (OCP). Due to a discrete gear ratio, strong nonlinear engine fuel characteristics, and different dynamics in the pulse/glide mode, the OCP is a switching nonlinear mixed-integer problem.\n\nSome hybrid vehicles are well-suited to performing pulse and glide. In a series-parallel hybrid (see hybrid vehicle drivetrain), the internal combustion engine and charging system can be shut off for the glide by simply manipulating the accelerator. However based on simulation, more gains in economy are obtained in non-hybrid vehicles.\n\nThis control strategy can also be used in vehicle platoon (The platooning of automated vehicles has the potential of significantly enhancing the fuel efficiency of road transportation), and this control method performs much better than conventional linear quadratic controllers.\n\nPulse and glide ratio of combustion engine in hybrid vehicles points on it by gear ratio in its consumption map, battery capacity, battery level, load, depending on acceleration, wind drag and its factor of speed.\n\nMuch of the time, automobile engines operate at only a fraction of their maximal efficiency, resulting in lower fuel efficiency (or what is the same thing, higher specific fuel consumption (SFC)). Charts that show the SFC for every feasible combination of torque (or Brake Mean Effective Pressure) and RPM are called Brake specific fuel consumption maps. Using such a map, one can find the efficiency of the engine at various combinations of rpm, torque, etc.\nDuring the pulse (acceleration) phase of pulse and glide, the efficiency is near maximal due to the high torque and much of this energy is stored as kinetic energy of the moving vehicle. This efficiently-obtained kinetic energy is then used in the glide phase to overcome rolling resistance and aerodynamic drag. In other words, going between periods of very efficient acceleration and gliding gives an overall efficiency that is usually significantly higher than just cruising at a constant speed. Computer calculations have predicted that in rare cases (at low speeds where the torque required for cruising at steady speed is low) it's possible to double (or even triple) fuel economy. More realistic simulations that account for other traffic suggest improvements of 20% are more likely. In other words, in the real world one is unlikely to see fuel efficiency double or triple. Such a failure is due to signals, stop signs, and considerations for other traffic; all of these factors interfering with the pulse and glide technique. But improvements in fuel economy of 20% or so are still feasible.\n\n\"Drafting\" occurs where a smaller vehicle drives (or coasts) close behind a vehicle ahead of it so that it is shielded from wind. Aside from being illegal in many jurisdictions it is often dangerous. Scale-model wind tunnel and Real-World tests of a car ten feet behind a semi-truck showed a reduction of over 90% for the wind force (aerodynamic drag). The gain in efficiency is reported to be 20–40%.\n\nMost of the fuel energy loss in cars occurs in the thermodynamic losses of the engine. The next biggest loss is from idling, or when the engine is in \"standby\", which explains the large gains available from shutting off the engine.\n\nIn this respect, the data for fuel energy wasted in braking, rolling resistance, and aerodynamic drag are all somewhat misleading, because they do not reflect all the energy that was wasted up to that point in the process of delivering energy to the wheels. The image reports that on non-highway (urban) driving, 6% of the fuel's energy is dissipated in braking; however, by dividing this figure by the energy that actually reaches the axle (13%), one can find that 46% of the energy reaching the axle goes to the brakes. Also, additional energy can potentially be recovered when going down hills, which may not be reflected in these figures.\n\nThere is sometimes a tradeoff between saving fuel and preventing crashes.\n\nIn the US, the speed at which fuel efficiency is maximized often lies below the speed limit, typically ; however, traffic flow is often faster than this. The speed differential between cars raises the risk of collision.\n\nDrafting increases risk of collision when there is a separation of fewer than three seconds from the preceding vehicle.\n\nCoasting is another technique for increasing fuel efficiency. Shifting gears and/or restarting the engine increase the time required for an avoidance maneuver that involves acceleration. Therefore, some believe the reduction of control associated with coasting is an unacceptable risk.\n\n\n"}
{"id": "49854427", "url": "https://en.wikipedia.org/wiki?curid=49854427", "title": "Epoxyeicosatetraenoic acid", "text": "Epoxyeicosatetraenoic acid\n\nEpoxyeicosatetraenoic acids (EEQs or EpETEs) are a set of biologically active epoxides that various cell types make by metabolizing the omega 3 fatty acid, eicosapentaenoic acid (EPA), with certain cytochrome P450 epoxygenases. These epoxygenases can metabolize EPA to as many as 10 epoxides that differ in the site and/or stereoisomer of the epoxide formed; however, the formed EEQs, while differing in potency, often have similar bioactivities and are commonly considered together.\n\nEPA is a straight-chain, 20 carbon omega-3 fatty acid containing cis (see Cis–trans isomerism) double bounds between carbons 5 and 6, 8 and 9, 11 and 12, 14 and 15, and 17 and 18; each of these double bonds is designated with the notation \"Z\" to indicate its cis configuration in the IUPAC Chemical nomenclature used here. EPA is therefore 5\"Z\",8\"Z\",11\"Z\",14\"Z\",17\"Z\"-eicosapentaenoic acid. Certain cytochrome P450 epoxygenases metabolize EPA by converting one of these double bounds to an epoxide thereby forming one of 5 possible eicosatetraenoic acid epoxide regioisomers (see Structural isomer, section on position isomerism (regioisomerism)). These regioisomers are: 5,6-EEQ (i.e. 5,6-epoxy-8\"Z\",11\"Z\",14\"Z\",17\"Z\"-eicosatetraenoic acid), 8,9-EEQ (i.e. 8,9-epoxy-5\"Z\",11\"Z\",14\"Z\",17\"Z\"-eicosatetraenoic acid), 11,12-EEQ (i.e. 11,12-epoxy-5\"Z\",8\"Z\",14\"Z\",17\"Z\"-eicosatetraenoic acid), 14,15-EEQ (i.e. 14,15-epoxy-5\"Z\",8\"Z\",11\"Z\",17\"Z\"-eicosatetraenoic acid, and 17,18-EEQ (i.e. 17,18-epoxy-5\"Z\",8\"Z\",11\"Z\",14\"Z\"-eicosatetraenoic acid. The epoxydases typically make both \"R\"/\"S\" enantiomers of each epoxide. For example, they metabolize EPA at its 17,18 double bond to a mixture of 17\"R\",18\"S\"-EEQ and 17\"S\",18\"R\"-EEQ. The EEQ products therefore consist of as many as 10 isomers.\n\nCellular cytochrome P450 epoxygenases metabolize various polyunsaturated fatty acids to epoxide-containng products. They metabolize the omega-6 fatty acids arachidonic acid, which possess four double bonds, to 8 different epoxide isomers which are termed epoxyeicosatrienoic acids or EETs and linoleic acid, which possess two double bonds, to 4 different epoxide isomers, i.e. two different 9,10-epoxide isomers termed vernolic acids or leukotoxins and two different 12,13-epoxides isomers termed coronaric acids or isoleukotoxins. They metabolize the omega-3 fatty acid, docosahexaenoic acid, which possesses 6 double bonds, to 12 different epoxydocosapentaenoic acid (EDPs) isomers. In general, the same epoxygenases that accomplish these metabolic conversions also metabolize the omega-6 fatty acid, EPA, to 10 epoxide isomers, the EEQs. These epoxygenases fall into several subfamilies including the cytochrome P4501A (i.e.CYP1A), CYP2B, CYP2C, CYP2E, and CYP2J subfamilies, and within the CYP3A subfamily, CYP3A4. In humans, CYP1A1, CYP1A2, CYP2C8, CYP2C9, CYP2C18, CYP2C19, CYP2E1, CYP2J2, CYP3A4, and CYP2S1 metabolize EPA to EEQs, in most cases forming principally 17,18-EEQ with smaller amounts of 5,6-EEQ, 8,9-EEQ, 11,12-EEQ, and 14,15-EEQ isomers. However, CYP2C11, CYP2C18, and CYP2S1 also form 14,15-EEQ isomers while CYP2C19 also forms 11,12-EEQ isomers. The isomers formed by these CYPs vary greatly with, for example, the 17,18-EEQs made by CYP1A2 consisting of 17\"R\",18\"S\"-EEQ but no detectable 17\"S\",18\"R\"-EEQ and those made by CYP2D6 consisting principally of 17\"R\",18\"S\"-EEQ with far smaller amounts of 17\"S\",18\"R\"-EEQ. In addition to the cited CYP's, CYP4A11, CYP4F8, CYP4F12, CYP1A1, CYP1A2, and CYP2E1, which are classified as CYP monooxygenase rather than CYP epoxygeanses because they metablize arachidonic acid to monohydroxy eicosatetraenoic acid products (see 20-Hydroxyeicosatetraenoic acid), i.e. 19-hydroxyhydroxyeicosatetraenoic acid and/or 20-hydroxyeicosatetranoic acid, take on epoxygease activity in converting EPA primarily to 17,18-EEQ isomers (see epoxyeicosatrienoic acid). 5,6-EEQ isomers are generally either not formed or formed in undetectable amounts while 8,9-EEQ isomers are formed in relatively small amounts by the cited CYPs. The EET-forming CYP epoxygenases often metabolize EPA to EEQs (as well as DHA to EDPs) at rates that exceed their rates in metabolizing arachidonic acid to EETs; that is, EPA (and DHA) appear to be preferred over arachidonic acid as substrates for many CYP epoxygenases.\n\nThe EEQ-forming cytochromes are widely distributed in the tissues of humans and other mammals, including blood vessel endothelium, blood vessel atheroma placques, heart muscle, kidneys, pancreas, intestine, lung, brain, monocytes, and macrophages. These tissues are known to metabolize arachidonic acid to EETs; it has been shown or is presumed that they also metabolize EPA to EEQs. Note, however, that the CYP epoxygenases, similar to essentially all CYP450 enzymes, are involved in the metabolism of xenobiotics as well as endogenously-formed compounds; since many of these same compounds also induce increases in the levels of the epoxygenases, CYP oxygenase levels and consequently EEQ levels in humans vary widely and are highly dependent on recent consumption history; numerous other factors, including individual genetic differences, also contribute to the variability in CYP450 epoxygenase expression.\n\nIn cells, EEQs are rapidly metabolized by the same enzyme that similarly metabolizes other epoxy fatty acids including the EETs viz., cytosolic soluble epoxide hydrolase [EC 3.2.2.10.] (also termed sEH or the EPHX2), to form their corresponding Vicinal (chemistry) diol dihydroxyeicosatetraenoic acids (diHETEs). The omega-3 fatty acid epoxides, EEQs and EPAs, appear to be preferred over EETs as substates for sEH. sEH converts 17,18-EEQ isomers to 17,18-dihydroxy-eicosatrienoic acid isomers (17,18-diHETEs), 14,15-EEQ isomers to 14,15-diHETE isomers, 11,12-EEQ isomers to 11,12-diHETE isomers, 8,9-EEQ isomers to 8,9-diHETE isomers, and 5,6-EEQ isomers to 5,6-diHETE isomers. The product diHETEs, like their epoxy precursors, are enantiomer mixtures; for instance, sEH converts 17,18-EEQ to a mixture of 17(\"S\"),19(\"R\")-diHETE and 17(\"R\"),18(\"S\")-diHETE. Since the diHETE products are as a rule generally far less active than their epoxide precursors, the sEH pathway of EET metabolism is regarded as a critical EEQ-inactivating pathway.\n\nMembrane-bound Microsomal epoxide hydrolase (mEH or Epoxide hydrolase 2 [EC 3.2.2.9.]) can metabolize EEQs to their dihydroxy products but is regarded as not contributing significantly to EEQ inactivation in vivo except possibly in rare tissues where the sEH level is exceptionally low while the mEH level is high.\n\nIn addition to the sEH pathway, EETs may be acylated into phospholipids in an Acylation-like reaction. This pathway may serve to limit the action of EETs or store them for future release. EETs are also inactivated by being further metabolized though three other pathways: Beta oxidation, Omega oxidation, and elongation by enzymes involved in Fatty acid synthesis.\n\nEEQS, similar to EDPs, have not be studied nearly as well as the EETs. In comparison to the many activities attributed to the EETs in animal model studies (see Epoxyeicosatrienoic acid), a limited set of studies indicate that EEQs (and EPAs) mimic EETS in their abilities to dilate arterioles, reduce hypertension, inhibit inflammation (the anti-inflammatory actions of EEQ are less potent than those of the EETs) and thereby reduce occlusion of arteries to protect the heart and prevent and strokes (see Epoxyeicosatrienoic acid#Clinical significance sections on a) Regulation of blood pressure, b) Heart disease, c) Strokes and seizures, and d) inflammation); they also mimic EETs in possessing analgesia properties in relieving certain types of pain (see Epoxyeicosatrienoic acid#Clinical significance#Pain). Often, the EEQs (and EPAs) exhibit greater potency and/or effectiveness than EET in these actions. In human studies potentially relevant to one or more of these activities, consumption of long chain omega-3 fatty acid (i.e. EPA- and DHA-rich) diet produced significant reductions in systolic blood pressure and increased peripheral arteriole blood flow and reactivity in patients at high to intermediate risk for cardiovascular events; an EPA/DHA-rich diet also reduced the risk while high serum levels of DHA and EPA were associated with a low risk of neovascular age-related macular degeneration. Since such diets lead to large increases in the serum and urine levels of EPAs, EEQs, and the dihydoxy metabolites of these epoxides but relatively little or no increases in EETs or lipoxygenase/cyclooxygenase-producing metabolites of arachidonic acid, DHA, and/or EEQs, it is suggested that the diet-induced increases in EPAs and/or EEQs are responsible for this beneficial effects. In direct contrast to the EETs which have stimulating effects in the following activities (see Epoxyeicosatrienoic acid#Cancer, EEQs (and EPAs) inhibit new blood vessel formation (i.e. angiogenesis), human tumor cell growth, and human tumor metastasis in animal models implanted with certain types of human cancer cells. The possible beneficial effects of omega-3 fatty acid-rich diets in pathological states involving inflammation, hypertension, blood clotting, heart attacks and other cardiac diseases, strokes, brain seizures, pain perception, acute kidney injury, and cancer are suggested to result, at least in part, from the conversion of dietary EPA and DHA to EEQs and EPAs, respectively, and the cited subsequent actions of these metabolites.\n"}
{"id": "592172", "url": "https://en.wikipedia.org/wiki?curid=592172", "title": "FU Orionis star", "text": "FU Orionis star\n\nIn stellar evolution, an FU Orionis star (also FU Orionis object, or \"FUor\") is a pre–main-sequence star which displays an extreme change in magnitude and spectral type. One example is the star V1057 Cyg, which became 6 magnitudes brighter and went from spectral type dKe to F-type supergiant. These stars are named after their type-star, FU Orionis.\n\nThe current model developed primarily by Lee Hartmann and Scott Jay Kenyon associates the FU Orionis flare with abrupt mass transfer from an accretion disc onto a young, low mass T Tauri star. Mass accretion rates for these objects are estimated to be around 10 solar masses per year. The rise time of these eruptions is typically on the order of 1 year, but can be much longer. The lifetime of this high-accretion, high-luminosity phase is on the order of decades. However, even with such a relatively short timespan, no FU Orionis object had been observed shutting off. By comparing the number of FUor outbursts to the rate of star formation in the solar neighborhood, it is estimated that the average young star undergoes approximately 10–20 FUor eruptions over its lifetime.\n\nThe prototypes of this class are: FU Orionis, V1057 Cygni, V1515 Cygni, and the embedded protostar V1647 Orionis, which erupted in January 2004.\n\n\n\n"}
{"id": "29233158", "url": "https://en.wikipedia.org/wiki?curid=29233158", "title": "Flexible-fuel vehicles in Brazil", "text": "Flexible-fuel vehicles in Brazil\n\nThe fleet of flexible-fuel vehicles in Brazil is the largest in the world. Since their inception in 2003, a total of 20 million flex fuel cars and light trucks have been manufactured in the country by June 2013, additionally over 3 million flexible-fuel motorcycles were manufactured by October 2013. Registrations of flex-fuel autos and light trucks represented 87.0% of all passenger and light duty vehicles sold in 2012, while flexible-fuel motorcycles represented a 48.2% of the domestic motorcycle production in 2012. There are over 80 flex car and light truck models available in the market manufactured by 14 major carmakers, and five flex-fuel motorcycles models available .\n\nBrazilian flexible-fuel vehicles are optimized to run on any mix of E20-E25 gasoline and up to 100% hydrous ethanol fuel (E100). Flex vehicles in Brazil are built-in with a small gasoline reservoir for cold starting the engine when temperatures drop below . An improved flex motor generation was launched in 2009 which eliminated the need for the secondary gas tank.\n\nAccording to two separate research studies conducted in 2009, 65% of the flex-fuel registered vehicles regularly use ethanol fuel, and use climbs to 93% of flex car owners in São Paulo, the main ethanol producer state where local taxes are lower, and prices are more competitive than gasoline. However, as a result of higher ethanol prices caused by the Brazilian ethanol industry crisis that began in 2009, by November 2013 only 23% flex-fuel car owners were using ethanol regularly, down from 66% in 2009.\n\nAfter the 1973 oil crisis, the Brazilian government made mandatory the use of ethanol blends with gasoline, and neat ethanol-powered cars (E100 only) were launched to the market in 1979, after testing with several prototypes developed by four carmakers. Brazilian carmakers modified gasoline engines to support ethanol characteristics and changes included compression ratio, amount of fuel injected, replacement of materials that would get corroded by the contact with ethanol, use of colder spark plugs suitable for dissipating heat due to higher flame temperatures, and an auxiliary cold-start system that injects gasoline from a small tank in the engine compartment to help starting when cold.\n\nFlexible-fuel technology started being developed only by the end of the 1990s by Brazilian engineers and in March 2003 Volkswagen do Brasil launched in the market the Gol 1.6 Total Flex, the first commercial flexible fuel vehicle capable of running on any blend of gasoline and ethanol.\n\nThe Brazilian flexible fuel car is built with an ethanol-ready engine and one fuel tank for both fuels. The small gasoline reservoir for starting the engine with pure ethanol in cold weather, used in earlier ethanol-only vehicles, was kept in the first generation of Brazilian flexible-fuel cars, mainly for users of the central and southern regions, where winter temperatures normally drop below. An improved flex motor generation was launched in 2009 that allowed for the elimination of this secondary gas reservoir tank.\n\nA key innovation in the Brazilian flex technology was avoiding the need for an additional dedicated sensor to monitor the ethanol-gasoline mix, which made the first American M85 flex fuel vehicles too expensive. This was accomplished through the lambda probe, used to measure the quality of combustion in conventional engines, is also required to tell the engine control unit (ECU) which blend of gasoline and alcohol is being burned. This task is accomplished automatically through software developed by Brazilian engineers, called \"Software Fuel Sensor\" (SFS), fed with data from the standard sensors already built-in the vehicle. The technology was developed by the Brazilian subsidiary of Boschin 1994, but was further improved and commercially implemented in 2003 by the Italian subsidiary of Magneti Marelli, located in Hortolândia, São Paulo. A similar fuel injection technology was developed by the Brazilian subsidiary of Delphi Automotive Systems, and it is called \"Multifuel\", based on research conducted at its facility in Piracicaba, São Paulo. This technology allows the controller to regulate the amount of fuel injected and spark time, as fuel flow needs to be decreased and also self-combustion needs to be avoided when gasoline is used because ethanol engines have compression ratio around 12:1, too high for gasoline.\n\nBrazilian flex engines are being designed with higher compression ratios, taking advantage of the higher ethanol blends and maximizing the benefits of the higher oxygen content of ethanol, resulting in lower emissions and improving fuel efficiency. The following table shows the evolution and improvement of the different generations of flex engines developed in Brazil.\n\nBrazilian flex cars are capable of running on just hydrated ethanol (E100), or just on a blend of gasoline with 20 to 25%anhydrous ethanol, or on any arbitrary combination of both fuels. Pure gasoline is no longer sold in the country because these high ethanol blends are mandatory since 1993. Therefore, all Brazilian automakers have optimized flex vehicles to run with gasoline blends from E20 to E25, so these FFVs are unable to run smoothly with pure gasoline with the exception of two models are specifically built with a flex-fuel engine optimized to operate also with pure gasoline (E0), the Renault Clio Hi-Flex and the Fiat Siena Tetrafuel.\n\nThe flexibility of Brazilian FFVs empowers the consumers to choose the fuel depending on current market prices. As ethanol fuel economy is lower than gasoline because of ethanol's energy content is close to 34% less per unit volume than gasoline, flex cars running on ethanol get a lower mileage than when running on pure gasoline. However, this effect is partially offset by the usually lower price per liter of ethanol fuel. As a rule of thumb, Brazilian consumers are frequently advised by the media to use more alcohol than gasoline in their mix only when ethanol prices are 30% lower or more than gasoline, as ethanol price fluctuates heavily depending on the result of seasonal sugar cane harvests.\n\nAfter the market launch of the Gol 1.6 Total Flex, the first commercial flexible fuel vehicle capable of running on any blend of gasoline and ethanol, GM do Brasil followed three months later with the Chevrolet Corsa 1.8 Flexpower, using an engine developed by a joint-venture with Fiat called PowerTrain. , the following 14 carmakers build and sell flexible fuel vehicles in Brazil: Citroën, Chery, Fiat, Ford, GM do Brasil (Chevrolet), Honda, Hyundai, Kia Motors, Mitsubishi, Nissan, Peugeot, Renault, Toyota and Volkswagen.\n\nFlexible fuel vehicles were 22% of the new car sales in 2004, 73% in 2005, 87.6% in July 2008, and reached a record 94% in August 2009. The production of flex-fuel cars and light commercial vehicles since 2003 reached 10 million vehicles in March 2010, and 15 million in January 2012. Registrations of flex-fuel cars and light trucks represented 87.0% of all passenger and light duty vehicles sold in the country in 2012. Production passed the 20 million-unit mark in June 2013. By the end of 2014, flex-fuel cars represented 54% of the Brazilian registered stock of light-duty vehicles, while gasoline only vehicles represented 34.3%. , flex-fuel light-duty vehicle sales totaled 25.5 million units.\n\nThe rapid success of flex vehicles was made possible by the existence of 33,000 filling stations with at least one ethanol pump available by 2006, a heritage of the early \"Pró-Álcool\" ethanol program. These facts, together with the mandatory use of E25 blend of gasoline throughout the country, allowed Brazil in 2008 to achieve more than 50% of fuel consumption in the gasoline market from sugar cane-based ethanol.\n\nAccording to two separate research studies conducted in 2009, at the national level 65% of the flex-fuel registered vehicles regularly used ethanol fuel, and use climbed to 93% in São Paulo, the main ethanol producer state where local taxes are lower, and E100 prices at the pump are usually more competitive than gasoline. However, as a result of higher ethanol prices caused by the Brazilian ethanol industry crisis that began in 2009, combined with government subsidies set to keep gasoline price lower than the international market value, by November 2013 only 23% flex-fuel car owners were using ethanol regularly, down from 66% in 2009.\n\nThe latest innovation within the Brazilian flexible-fuel technology, is the development of flex-fuel motorcycles. In 2007 Magneti Marelli presented the first motorcycle with flex technology, adapted on a Kasinski Seta 125, and based on the Software Fuel Sensor (SFS) the firm developed for flex-fuel cars in Brazil. Delphi Automotive Systems also presented in 2007 its Multifuel injection technology for motorcycles. Besides the flexibility in the choice of fuels, a main objective of the fuel-flex motorcycles is to reduce CO emissions by 20 percent, and savings in fuel consumption in the order of 5% to 10% are expected. AME Amazonas Motocicletas announced that sales of its motorcycle AME GA (G stands for gasoline and A for alcohol) were scheduled for 2009, but the first flex-fuel motorcycle was actually launched by Honda in March 2009. Produced by its Brazilian subsidiary Moto Honda da Amazônia, the CG 150 Titan Mix is sold for around .\n\nBecause the CG 150 Titan Mix does not have a secondary gas tank for a cold start like the Brazilian flex cars do, the tank must have at least 20% of gasoline to avoid start up problems at temperatures below . The motorcycle’s panel includes a gauge to warn the driver about the actual ethanol-gasoline mix in the storage tank. In September 2009, Honda launched a second flexible-fuel motorcycle, the on-off road NXR 150 Bros Mix. During the first eight months after its market launch the CG 150 Titan Mix sold 139,059 motorcycles, capturing a 10.6% market share, and ranking second in sales of new motorcycles in the Brazilian market by October 2009, and by year's end, both Honda flexible-fuel motorcycles sold a total of 183,375 units, representing an 11.4% market share of the Brazilian new motorcycle sales in that year. Cumulative sales of both flex fuel motorcycles reached 515,726 units in 2010, and sales in that year represented 18.15% of all motorcycle produced.\n\nTwo other flex-fuel motorcycles manufactured by Honda were launched in October 2010 and January 2011, the GC 150 FAN and the Honda BIZ 125 Flex. During 2011 a total of 956,117 flex-fuel motorcycles were produced, raising its market share to 56.7%. Cumulative production of the four available flex fuel models since 2009 reached 1.48 million units in December 2011. The 2 million mark was reached in August 2012. Flexible-fuel motorcycle production passed the 3 million-unit milestone in October 2013, and the 4 million mark in March 2015.\n\nThe Brazilian subsidiaries of Magneti Marelli, Delphi and Bosch have developed and announced the introduction in 2009 of a new flex engine generation that eliminates the need for the secondary gasoline tank by warming the ethanol fuel during starting, and allowing flex vehicles to do a normal cold start at temperatures as low as, the lowest temperature expected anywhere in the Brazilian territory. Another improvement is the reduction of fuel consumption and tailpipe emissions, between 10% to 15% as compared to flex motors sold in 2008. In March 2009 Volkswagen do Brasil launched the Polo E-Flex, the first flex fuel model without an auxiliary tank for cold start. The Flex Start system used by the Polo was developed by Bosch.\n\nIn 2013, Ford launched the first flex fuel car with direct injection: the Focus 2.0 Duratec Direct Flex.\n\nThe following is a list of flex-fuel automobiles and light-duty vehicles available in Brazil as of December 2013.\n\n"}
{"id": "29238113", "url": "https://en.wikipedia.org/wiki?curid=29238113", "title": "Flexible-fuel vehicles in the United States", "text": "Flexible-fuel vehicles in the United States\n\nFlexible-fuel vehicles in the United States are the second largest flex-fuel fleet in the world after Brazil, and there were about 17.4 million flex-fuel cars and light trucks in operation by the end of 2014. Despite the growing fleet of E85 flex-fuel vehicles, actual use of ethanol fuel is limited due to the lack of E85 refueling infrastructure and also because many American flex-fuel car owners were not aware they owned an E85 flex-fuel vehicle. Flex-fuel vehicles are common in the Midwest, where corn is a major crop and is the primary feedstock for ethanol fuel production. Also the U.S. government has been using flex-fuel vehicles for many years.\n\nU.S. flex-fuel vehicles are optimized to run on a maximum blend of 15% gasoline with 85% anhydrous ethanol (called E85 fuel). This limit in the ethanol content is set to reduce ethanol emissions at low temperatures and to avoid cold starting problems during cold weather, at temperatures lower than . The alcohol content is reduced during the winter in regions where temperatures fall below to a winter blend of E70.\n\nThe first commercial flexible fuel vehicle was the Ford Model T, produced from 1908 through 1927. It was fitted with a carburetor with adjustable jetting, allowing use of gasoline or ethanol, or a combination of both. Other car manufactures also provided engines for ethanol fuel use. Ethanol was disadvantaged by frequent accusations that ethanol producers collaborated with bootleggers during Prohibition. Oil dominance as a motor fuel was questioned in the U.S. only after the 1973 oil crisis, which resulted in gasoline shortages and awareness on the dangers of oil dependence. This crisis opened a new opportunity for ethanol, methanol and other alternative fuels.\n\nAs a response to the shock caused by the first oil crisis, the U.S. government provided the initial support to develop alternative fuels, and some time later, also as a goal to improve air quality. Liquid fuels were preferred over gaseous fuels not only because they have a better volumetric energy density but also because they were the most compatible fuels with existing distribution systems and engines, thus avoiding a big departure from the existing technologies and taking advantage of the vehicle and the refueling infrastructure. California led the search of sustainable alternatives with interest focused in methanol. Ford Motor Company and other automakers responded to California's request for vehicles that run on methanol. In 1981, Ford delivered 40 dedicated methanol fuel (M100) Escorts to Los Angeles County, but only four refueling stations were installed. The biggest challenge in the development of alcohol vehicle technology was getting all of the fuel system materials compatible with the higher chemical reactivity of the fuel. Methanol was even more of a challenge than ethanol but much of the early experience gained with neat ethanol vehicle production in Brazil was transferable to methanol. The success of this small experimental fleet of M100s led California to request more of these vehicles, mainly for government fleets. In 1983, Ford built 582 M100 vehicles; 501 went to California, and the remaining to New Zealand, Sweden, Norway, United Kingdom, and Canada.\n\nAs an answer to the lack of refueling infrastructure, Ford began development of a flexible-fuel vehicle in 1982, and between 1985 and 1992, 705 experimental FFVs were built and delivered to California and Canada, including the 1.6L Ford Escort, the 3.0L Taurus, and the 5.0L LTD Crown Victoria. These vehicles could operate on either gasoline or methanol with only one fuel system. Legislation was passed to encourage the US auto industry to begin production, which started in 1993 for the M85 FFVs at Ford. In 1996, a new FFV Ford Taurus was developed, with models fully capable of running on either methanol or ethanol blended with gasoline. This ethanol version of the Taurus became the first commercial production of an E85 FFV. The momentum of the FFV production programs at the American car companies continued, although by the end of the 1990s, the emphasis shifted to the FFV E85 version, as it is today. Ethanol was preferred over methanol because there is a large support from the farming community, and thanks to the government's incentive programs and corn-based ethanol subsidies. Support for ethanol also comes from the fact that it is a biomass fuel, which addresses climate change concerns and greenhouse gas emissions, though nowadays these benefits are questioned and depend on the feedstock used for ethanol production and their indirect land use change impacts.\n\nSince 1998 a total of 15.1 million E85 flex fuel vehicles had been sold or lease in the United States through December 2012. Of these, about 11 million flex-fuel cars and light trucks were still in operation as of early 2013, up from 7.3 million in 2008, 4.1 million in 2005, and 1.4 million on U.S roads in 2001. E85 flex-fuel vehicles are becoming increasingly common in the Midwest, where corn is a major crop and is the primary feedstock for ethanol fuel production. Also the US government has been using flex-fuel vehicles for many years. Since 2008 almost any type of automobile and light duty vehicles is available in the market with the flex-fuel option, including sedans, vans, SUVs and pick-up trucks. For the 2011 model year there are about 70 vehicles E85 capable.\n\nThe E85 blend is used in gasoline engines modified to accept such higher concentrations of ethanol, and the fuel injection is regulated through a dedicated sensor, which automatically detects the amount of ethanol in the fuel, allowing to adjust both fuel injection and spark timing accordingly to the actual blend available in the vehicle's tank.\n\nThe American E85 flex fuel vehicle was developed to run on any mixture of unleaded gasoline and ethanol, anywhere from 0% to 85% ethanol by volume. Both fuels are mixed in the same tank, and E85 is sold already blended. In order to reduce ethanol evaporative emissions and to avoid problems starting the engine during cold weather, the maximum blend of ethanol was set to 85%. There is also a seasonal reduction of the ethanol content to E70 (called winter E85 blend) in very cold regions, where temperatures fall below during the winter. In Wyoming for example, E70 is sold as E85 from October to May.\n\nBecause ethanol contains close to 34% less energy per unit volume than gasoline, E85 FFVs have a lower mileage per gallon than gasoline. However, this lower energy content does not translate directly into a 34% reduction in miles per U.S. gallon, because there are many other variables that affect the performance of a particular fuel in a particular engine, though for E85 the effect becomes significant. E85 will produce lower mileage than gasoline, and actual performance may vary depending on the vehicle. Based on EPA EPA-rated mileage for all 2006 E85 models, the average fuel economy for E85 vehicles was 25.56% lower than unleaded gasoline. When making price comparisons it has to be considered that E85 has octane rating of about 104 and could be used as a substitute for premium gasoline.\n\nRegional retail E85 prices vary widely across the US, with more favorable prices in the Midwest region, where most corn is grown and ethanol produced. As of early November 2010, the US average spread between the price of E85 and gasoline was 13.4%, while in Indiana was 10.1%, in Minnesota 20.3%, 18.3% in Wisconsin, just 2% in Maryland, 16.3% in California, and 7% in Utah. Depending of the vehicle capabilities, the break even price of E85 has to be between 25 and 30% lower than gasoline. (See price comparisons for most states at e85prices.com)\n\nFor the 2011 model year many of the models available are trucks and sport-utility vehicles that get less than when filled with gasoline. The following table compares fuel economy, carbon footprint, and petroleum consumption for several popular gasoline-powered vehicles and their flex-fuel versions:\n\nThe demand for ethanol fuel produced from field corn in the United States was stimulated by the discovery in the late 90s that methyl tertiary butyl ether (MTBE), an oxygenate additive in gasoline, was contaminating groundwater. Due to the risks of widespread and costly litigation, and because MTBE use in gasoline was banned in almost 20 states by 2006, the substitution of MTBE opened a new market for ethanol fuel. This shift also contributed to a sharp increase in the production and sale of E85 flex vehicles since 2002. Ethanol also replaces toxic, air-polluting substances such as benzene, toluene, and xylene. Numerous states require certain ethanol blends to reduce air pollution. Ethanol produces about 34% less air pollution than gasoline on average. \n\nAs of 2016, ethanol blends in the U.S. reduce emissions of carbon dioxide by about 40 million tons per year. During its entire life cycle, \"from field to wheel,\" ethanol reduces emissions by about 34 percent. Second-generation cellulosic ethanol is even more efficient. A plant built by DuPont in Iowa achieves emission reductions of 90%.\n\nA 2005 survey found that 68% of American flex-fuel car owners were not aware they owned an E85 flex. This is due to the fact that the exterior of flex and non-flex vehicles look exactly the same; there is no sale price difference between them; the lack of consumer's awareness about E85s; and also the initial decision of American automakers of not putting any kind of exterior labeling, so buyers can be aware they are getting an E85 vehicle. In contrast, all Brazilian automakers clearly mark FFVs with badging or a high quality sticker in the exterior body, with a logo with some variant of the word Flex. Since 2006 many new FFV models in the US feature a bright yellow gas cap to remind drivers of the E85 capabilities. GM is also using badging with the text \"Flexfuel/E85 Ethanol\" to clearly mark the car as an E85 FFV, and Ford early flex-fuel models had a small decal reading \"FFV\" and the \"leaf and road\" logo, and later introduced badging keeping the \"leaf and road\" logo but changed the text to \"Flex Fuel\".\n\nBy the end of 2014, there were around 17.4 million flex-fuel vehicles in use in the country. However, according to the U.S. Department of Energy in 2011 only 862,837 flex-fuel fleet-operated vehicles were regularly fueled with E85. The Energy Policy Act of 2005, signed into law by President Bush on 8 August 2005, in its Section 701 requires the federal government's fleet of vehicles capable of operating on alternative fuels to be operated on these fuels exclusively, unless a waiver is granted if the alternative fuel is not reasonably available; or if the cost of the fuel required is unreasonably more expensive compared to gasoline. By 2008 the Federal vehicle fleet consisted of 594,900 vehicles, of which 128,491 run on E85, representing the majority of the alternative fuel vehicles in the Federal fleet that year. According to the Government Accountability Office, in 2010 Federal employees received waivers to use gasoline in 55 percent of fleet flex-fuel vehicles because E85 was not available.\n\nSome critics have argued that American automakers have been producing E85 flex models motivated by a loophole in the Corporate Average Fuel Economy (CAFE) requirements, that allows for a fuel economy credit for every flex-fuel vehicle sold, whether or not in practice these vehicles are fueled with E85. This loophole might allow the car industry to meet the CAFE targets in fuel economy just by spending between to that it cost to turn a conventional vehicle into a flex-fuel, without investing in new technology to improve fuel economy, and saving them the potential fines for not achieving that standard in a given model year. In an example presented by the National Highway Traffic Safety Administration (NHTSA), the agency responsible for establishing the CAFE standards, the special treatment provided for alternative fuel vehicles, \"turns a dual fuel vehicle that averages 25 mpg on gasoline or diesel... to attain the 40 mpg value for CAFE purposes.\" The current CAFE standards are 27.5 mpg for automobiles and 22.2 mpg for light-duty trucks.\"\n\nIn late 2007, CAFE standards received their first overhaul in more than 30 years through the Energy Independence and Security Act of 2007 (EISA), and were set to rise to 35 mpg by the year 2020. However, in May 2009 the Obama Administration announced a new harmonized national policy that will require an average fuel economy standard of 35.5 mpg in 2016. The flex-fuel CAFE credits are scheduled to end in 2016, but because the 2007 EISA made CAFE credits exchangeable between different classes of automobiles and tradable between companies, and also carmakers are allowed to carry over credits for up to five years, the flex-fuel credits accumulated up to 2016 can be carried over and traded until 2020. The CAFE standards proposed in 2011 for the period 2017-2025 will allow flexible-fuel vehicles to receive extra credit but only when the carmakers present data proving how much E85 such vehicles have actually consumed.\n\nA major restriction hampering sales of E85 flex vehicles or fuelling with E85, is the limited infrastructure available to sell E85 to the public, as by 2014 only 2 percent of motor fuel stations offered E85, up from about 1 percent in 2011. , there were only 3,218 gasoline fueling stations selling E85 to the public in the entire U.S., while about 156,000 retail motor fuel outlets do not offer the E85 blend. The number of E85 grew from 1,229 in 2007 to 2,442 in 2011, but only increased by 7% from 2011 to 2013, when the total reached 2,625. There is a great concentration of E85 stations in the Corn Belt states, and , the leading state is Minnesota with 274 stations, followed by Michigan with 231, Illinois with 225, Iowa with 204, Indiana with 188, Texas with 181, Wisconsin with 152, and Ohio with 126. Only eight states do not have E85 available to the public, Alaska, Delaware, Hawaii, Montana, Maine, New Hampshire, Rhode Island, and Vermont. The main constraint for a more rapid expansion of E85 availability is that it requires dedicated storage tanks at filling stations, at an estimated cost of for each dedicated ethanol tank.\n\nSeveral members of the United States Congress have called for mandatory production of flexible fuel vehicles. Also the E85 and Biodiesel Access Act proposed to modify current IRS limits on the tax credit which today only allows for the amount a dual fuel dispenser exceeds the cost of a conventional dispenser. The E85 and Biodiesel Access Act would increase the credit from 30 percent of the cost of clean fueling property to 50 percent and increase the maximum credit to $100,000. This law would also extend the existing credit which is scheduled to expire at the end of 2009.\n\nIn 2008 Chrysler, General Motors, and Ford pledged to manufacture 50 percent of their entire vehicle line as flexible fuel in model year 2012, if enough fueling infrastructure develops. In early 2010 GM reaffirmed its commitment to biofuels and its determination to deliver more than half of its 2012 production in the U.S. market as E85 flex-fuel capable vehicles. GM will begin introducing E-85-capable direct-injected and turbocharged powertrains, and urged the deployment of more E85 stations, as \"\"ninety percent of registered flex-fuel vehicles don't have an E85 station in their ZIP code, and nearly 50%, don't have E85 in their county\".\"\n\nIn 2008 Ford delivered the first flex-fuel plug-in hybrid as part of a demonstration project, a Ford EscapePlug-in Hybrid capable of running on E85 or gasoline. General Motors announced that the new plug-in hybrid electric vehicle Chevrolet Volt, launched in the United States market in December 2010, would be flex-fuel-capable in 2013. The Volt propulsion architecture allows to adapt the propulsion to other world markets such as Brazil's E100 or to Europes commonly use clean diesel.\n\nOn May 2009, President Barack Obama signed a Presidential Directive with the aim to advance biofuels research and improve their commercialization. The Directive established a Biofuels Interagency Working Group composed of three agencies, the Department of Agriculture, the Environmental Protection Agency, and the Department of Energy. This group will develop a plan to increase flexible fuel vehicle use and assist in retail marketing efforts. Also they will coordinate infrastructure policies impacting the supply, secure transport, and distribution of biofuels in order to increase the number of fueling stations throughout the country.\n\nThe Obama Administration set the goal of installing 10,000 blender pumps nationwide until 2015. Blender or flexible fuel pumps simultaneously dispense E85 and other lower blends such as E50, E30 and E20 that can be used by E85 flex-fuel vehicles. On April 2011 the US Department of Agriculture (USDA) issue a rule to include flexible fuel pumps in the Rural Energy for America Program (REAP). This rule will provide financial assistance, via grants and guaranteed loans, to fuel station owners to install E85 and blender pumps.\n\nIn May 2011 the Open Fuel Standard Act (OFS) was introduced to Congress with bipartisan support. The bill requires that 50 percent of automobiles made in 2014, 80 percent in 2016, and 95 percent in 2017, would be manufactured and warranted to operate on non-petroleum-based fuels, which includes existing technologies such as flex-fuel, natural gas, hydrogen, biodiesel, plug-in electric and fuel cell. Considering the rapid adoption experience with flexible-fuel vehicles in Brazil and the fact that by 2010 the cost of making vehicles flex-fuel capable is approximately $100 per car, the bill's primarily objective was to promote a massive adoption of flex-fuel vehicles capable of running on ethanol or methanol.\"\" \n\n, almost half of new vehicles produced by Chrysler, Ford, and General Motors are flex-fuel, meaning roughly one-quarter of all new vehicles sold by 2015 are capable of using up to E85. However, obstacles to widespread use of E85 fuel remain. A 2014 analysis by the Renewable Fuels Association (RFA) found that oil companies prevent or discourage affiliated retailers from selling E85 through rigid franchise and branding agreements, restrictive supply contracts, and other tactics. The report showed independent retailers are five times more likely to offer E85 than retailers carrying an oil company brand.\n\n\n\n\n"}
{"id": "25066885", "url": "https://en.wikipedia.org/wiki?curid=25066885", "title": "Great Eppleton Wind Farm", "text": "Great Eppleton Wind Farm\n\nGreat Eppleton Wind Farm is a wind farm near Hetton-le-Hole, England. It is owned and operated by E.ON UK. Constructed in 1997, it was notable for originally consisting of twin-bladed turbines, as most wind turbines have three blades. On 29 September 2009 E.ON announced it would replace these with four new REpower MM92 turbines giving a nameplate capacity of 8.2 MW.\n\n"}
{"id": "1264444", "url": "https://en.wikipedia.org/wiki?curid=1264444", "title": "Hardpan", "text": "Hardpan\n\nIn soil science, agriculture and gardening, Hardpan or Soil pan is a dense layer of soil, usually found below the uppermost topsoil layer. There are different types of hardpan, all sharing the general characteristic of being a distinct soil layer that is largely impervious to water. Some hardpans are formed by deposits in the soil that fuse and bind the soil particles. These deposits can range from dissolved silica to matrices formed from iron oxides and calcium carbonate. Others are man-made, such as hardpan formed by compaction from repeated plowing, particularly with moldboard plows, or by heavy traffic or pollution.\n\nSoil structure strongly affects its tendency to form a hard pan. One such common soil condition related to hardpan is soil pH. Acid soils are most often affected due to the propensity of certain mineral salts, most notably iron and calcium, to form hard complexes with soil particles under acid conditions.\n\nAnother major determinant is the soil particle size. Clay particles are some of the smallest particles commonly found in soils. Due to their structure the spaces between individual clay particles is small and already restricts the passage of water, reducing infiltration and hence drainage. Soils with a high clay content are also easily compacted and affected by man-made discharges. Clay particles have a strong negative electrostatic charge and will readily bond to positively charged ions dissolved in the soil-water matrix. Common salts such as sodium ions contained in wastewater can fulfil this role and lead to a localized hardpan in some soil types. This is a common cause of septic system failure due to the prevention of proper drainage in field.\n\nHardpan can be a problem in farming and gardening by impeding drainage of water and restricting the growth of plant roots. In these situations, the hardpan can be broken up by either mechanical means such as digging or plowing, or through the use of soil amendments. The broadfork is a manual tool specifically designed for this task; a digging fork or a spade might also be used. The chisel plow does a similar job with the help of a tractor.\n\nThe use of soil amendments can also be employed to alter the soil structure and promote the dissolution of the hard pan. It has been observed that increasing the amount of soil organic matter through the working-in of manure, compost or peat can both improve local drainage and promote the proliferation of earth worms that can, over time, break relatively thin hardpan layers.\n\nMore difficult hardpans may be further improved through the action of both adjusting the soil pH with lime if the soil is acidic, and with the addition of gypsum. This combination can help loosen clay particles bound into a hardpan by the actions of hard salts such as iron, calcium carbonate and sodium, by promoting their mobility through a higher pH while proving a suitable source of exchanging minerals (the gypsum). This works because gypsum salts, although not \"soft\", are still water permeable and have a larger, more open structure, the results of which do not promote as hard a matrix as was replaced. However, unlike when employing mechanical means, breaking a hardpan through the use of amendments may require action over the course of years, and even then one is by no means assured success. The results are primarily determined by how extensive and / or intractable the hardpan is.\n\n"}
{"id": "32080258", "url": "https://en.wikipedia.org/wiki?curid=32080258", "title": "Held–Hou Model", "text": "Held–Hou Model\n\nThe Held–Hou Model is a model for the Hadley circulation of the atmosphere that would exist in the absence of atmospheric turbulence. The model was developed by Isaac Held and Arthur Hou in 1980. \n\nThe essence of the model is that air rising from the surface at the equator conserves its angular momentum as it moves poleward. This distribution of wind, in turn, determines the distribution of temperature, which determines the latitudinal extent of the circulation by requiring energy conservation. This stands in contrast to George Hadley's original conception of the circulation, which he argued reached the poles. The Hadley circulation has a cooling effect at and near the equator and a warming effect at higher latitudes within the Hadley Cell. This energy transport can be converted into a mass transport, to determine the strength of the circulation, by normalizing by the appropriate vertical stability. The effects of moisture and seasons on the model have been studied. \n\nEarth's atmosphere violates the underlying assumptions of the model: angular momentum is not conserved and the tropical atmosphere is not energetically closed. As such, the Held–Hou model is a conceptual model that does not make quantitatively accurate predictions of the sensitivity of the Hadley circulation to changes in atmospheric parameters.\n"}
{"id": "1438060", "url": "https://en.wikipedia.org/wiki?curid=1438060", "title": "Hevea", "text": "Hevea\n\nHevea is a genus of flowering plants in the spurge family, Euphorbiaceae, with about ten members. It is also one of many names used commercially for the wood of the most economically important rubber tree, \"H. brasiliensis\". The genus is native to tropical South America but is widely cultivated in other tropical countries and naturalized in several of them. It was first described in 1775.\n\nFrench botanist and explorer Jean Baptiste Christophore Fusée Aublet first described \"Hevea\" as a genus in 1775. \"H. brasiliensis\" and \"H. guianensis\" are large trees, often reaching more than in height. Most of the other members of the genus are small to medium trees, and \"H. camporum\" is a shrub of around . Trees in this genus are either deciduous or evergreen. Certain species, namely \"H. benthamiana\", \"H. brasiliensis\" and \"H. microphylla\", bear \"winter shoots\", stubby side shoots with short internodes, scale leaves on the stem and larger leaves near the tip; on these, the leaves are shed leaving the tree bare before new shoots develop. The remaining species bear more vigorous side shoots which develop before the old foliage is shed and thus the tree remains green. The leaves consist of three, usually elliptical, leaflets which are held horizontally or slightly drooping in most species. The inflorescences have separate male and female flowers, with the females being at the end of the panicles. The fruits are capsules, usually with three seeds, which in all except two species (\"H. spruceana\" and \"H. microphylla\") split explosively when ripe to eject the large seeds.\n\nThe genus occurs naturally in tropical South America, mostly in the Amazon basin. To the north of the basin the land rises to the watershed of the Guiana Shield on the border between Brazil and Venezuela, and the southern foothills of these mountains forms the northerly limit of the genus. It is also present in the upper reaches of the Orinoco River. The genus extends westwards as far as the foothills of the Andes and southwards to the foothills of the Mato Grosso. Its easterly limit is the Atlantic Ocean. \nThe most widespread species is \"H. guianensis\" which occurs over the whole range of the genus.\n\nThe Pará rubber tree (\"H. brasiliensis\") occurs mainly south of the Amazon, as does \"H. camporum\", but the greatest diversity occurs to the north of the river, in the Rio Negro region, where all the other species occur. In this area where there are variations in soil and topography and the rainforest experiences conditions of all-year-round humidity, the genus \"Hevea\" has been undergoing a high degree of speciation. The high humidity encourages the growth of fungal leaf diseases, and the species that are deciduous avoid immediate transfer of fungal spores from old leaves onto new growth. The Pará rubber tree has been introduced to and is naturalised in many tropical countries in Asia.\n\nEach species has its own habitat requirements; \"H. brasiliensis\" grows on well-drained soils but tolerates light flooding; \"H. guianensis\", \"H. pauciflora\" and \"H. rigidifolia\" grow in well-drained soil, on high river banks and on slopes; and \"H. camporum\" grows on savannahs. Other species such as \"H. benthamiana\", \"H. microphylla\" and \"H. spruceana\" need wetter conditions in locations subject to seasonal flooding for several months each year, and \"H. nitida\" grows both in periodically inundated swamps and in drier locations such as rocky hillsides well above the flood level.\n\nThe following species are recognised:\n\n"}
{"id": "8995919", "url": "https://en.wikipedia.org/wiki?curid=8995919", "title": "Hypsometric equation", "text": "Hypsometric equation\n\nThe hypsometric equation, also known as the thickness equation, relates an atmospheric pressure ratio to the equivalent thickness of an atmospheric layer under the assumptions of constant temperature and gravity. It is derived from the hydrostatic equation and the ideal gas law.\n\nThe hypsometric equation is expressed as:\n\nwhere:\n\nIn meteorology, formula_8 and formula_9 are isobaric surfaces. In altimetry with the International Standard Atmosphere the hypsometric equation is used to compute pressure at a given height in isothermal layers in the upper and lower stratosphere.\n\nThe hydrostatic equation:\n\nwhere formula_11 is the density <nowiki>[kg/m</nowiki><nowiki>]</nowiki>, is used to generate the equation for hydrostatic equilibrium, written in differential form:\n\nThis is combined with the ideal gas law: \n\nto eliminate formula_11:\n\nThis is integrated from formula_16 to formula_17:\n\n\"R\" and \"g\" are constant with \"z\", so they can be brought outside the integral.\nIf temperature varies linearly with \"z\" (as it is assumed to do in the International Standard Atmosphere),\nit can also be brought outside the integral when replaced with formula_5, the average temperature between formula_16 and formula_17.\n\nIntegration gives\n\nsimplifying to\n\nRearranging:\n\nor, eliminating the natural log:\n"}
{"id": "43833131", "url": "https://en.wikipedia.org/wiki?curid=43833131", "title": "Jura hole stone", "text": "Jura hole stone\n\nA Jura hole stone (or Jura Coral rock, or Jura cavity stone) is a natural stone found in the Jura mountains of France.\n\nThey are with light beige coloring and many cavities.\n\nThey come from the petrified sea deposits that were cause by tectonic shifts between mountains. The bizarre form and many cavities are caused by natural erosion over thousands of years.\n\nThese stones are protected : it is forbidden to pick them up. However, some are for sale for aquarium decoration.\n\n"}
{"id": "17477617", "url": "https://en.wikipedia.org/wiki?curid=17477617", "title": "Ketil Lenning", "text": "Ketil Lenning\n\nKetil Lenning (born 1950) is a Norwegian businessperson.\n\nKetil Lenning graduated from the Texas A&M University with a Bachelor of Science in 1974. He became the Chief Operating Officer of Odfjell Drilling in 2001 and Chief Executive Officer in 2005. He is also chairman of the board of Odfjell Invest. Lenning has over 15 years experience from oil companies and in addition has 13 years experience within maritime drilling and oil production such as Smedvig.\n"}
{"id": "56052525", "url": "https://en.wikipedia.org/wiki?curid=56052525", "title": "LINUS (fusion experiment)", "text": "LINUS (fusion experiment)\n\nThe LINUS program was an experimental fusion power project developed by the United States Naval Research Laboratory (NRL) in 1972. The goal of the project was to produce a controlled fusion reaction by compressing plasma inside metal liners. The basic concept is today known as magnetized target fusion.\n\nThe reactor design was based on the mechanical compression of magnetic flux (and therefore plasma) inside a molten metal liner. A chamber was filled with molten metal and rotated along one axis. This spinning motion created a cylindrical cavity into which plasma was injected. Once the plasma was contained within the cavity, the liquid metal wall was rapidly compressed, which would raise the temperature and density of the trapped plasma to fusion conditions.\n\nThe use of a liquid metal liner has many of the advantages over previous experiments that imploded cylindrical metal liners to achieve high-energy-density fusion. The liquid metal liner provided the benefits of recovering the heat energy of the reaction, absorbing neutrons, transferring kinetic energy, and replacing the plasma-facing wall during each cycle. Additional benefits of a liquid liner include greatly simplified servicing of the reactor, reducing radioactivity, and protecting the permanent sections of the reactor from neutron damage. The danger from flying shrapnel was also mitigated with the use of liquid liners.\n\nThe concept was revived in the 2000s as the basis for the General Fusion design, currently being built in Canada.\n\nIn the LINUS concept, plasma is injected into a molten lead-lithium liner. The liner is then imploded mechanically, using high pressure helium pistons. The imploding liner acts to compress the magnetically-confined plasma adiabatically to fusion temperature and relatively high density (). In the subsequent expansion the plasma energy and the fusion energy carried by trapped alpha particles is directly recovered by the liquid metal, making the mechanical cycle self-sustaining. The implosion cycle would be repeated every few seconds. The LINUS reactor can thus be regarded as a fusion engine, except that there is no shaft output; all the energy appears as heat.\n\nThe liquid metal acts as both a compression mechanism and heat transfer mechanism, allowing the energy from the fusion reaction to be captured as heat. LINUS researchers anticipated that a lithium liner could also be used to breed tritium fuel for the power plant, and would protect the machine from high-energy neutrons by acting as a regenerative first wall.\n\nSeveral experimental machines were constructed throughout the LINUS project, to gather data and demonstrate various aspects of the system concept.\n\nTo obtain detailed information about the behavior of the inner surface of solid and liquid metal liners during the final moments of an implosion, and experiment called SUZY II was built at NRL. The experiment was used to compress various metal liners from an initial diameter of to a final diameter of about using magnetic fields. An overall compression ratio of 28:1 was achieved.\n\nOne goal of SUZY II was to demonstrate the use of electromagnetic driving techniques to achieve liner implosions. The central feature of SUZY II was a bank of capacitors charged to , which was able to quickly deliver of energy to be used for generating large magnetic fields. Pressures greater than were achieved during the implosions. SUZY II was named after its predecessor, SUZY I, a capacitor bank.\n\nTo study the hydrodynamic behavior and magnetic flux compression at the target energy density regimes, a device called LINUS-0 was constructed in 1978. The experiment involved a rotating cylindrical chamber filled with molten metal or water. A large number of pistons (16 or 32) were attached to the chamber, and were in contact with the rotating liquid. During the experiment, all pistons were simultaneously propelled to drive the liquid radially inward. The pistons in the LINUS-0 experiment were driven by the high-explosive agent DATB (), also known as PBXN, chosen for its high melting point, low particulate matter, and compatibly low cost.\n\nThe experimental parameters for LINUS-0 required the cylindrical chamber to rotate at , which was accomplished with a 454 Chevrolet V8 engine. All pistons were required to fire within of each other. During data collection, LINUS-0 was fired as often as three times per day.\n\nA similar machine, called HELIUS, was constructed to demonstrate magnetic flux compression. HELIUS was a half-scale version of LINUS-0, and was designed to use liquid sodium and potassium in the liner chamber. In practice, the use of water was sufficient for the hydrodynamic studies. In the HELIUS experiment, the liquid sodium-potassium liners were imploded using high-pressure Helium () to drive mechanical pistons.\n\nExperiments on LINUS-0 and HELIUS were largely unsuccessful due in part to delays incurred in the design, fabrication, and assembly phases. Time wasn't allocated to recover from delays or unexpected challenges, and the machines were eventually disassembled and placed in storage.\n\nThe LINUS project encountered several engineering problems which limited its performance and therefore its attractiveness as an approach to commercial fusion power. These issues included performance of the plasma preparation and injection technique, the ability to achieve reversible compression/expansion cycles, and problems with magnetic flux diffusion into the liner material. Also, the ability to remove the vaporized liner material from the cavity between cycles (within a duration of about ) was not accomplished. There were also shortcomings with the design of the inner mechanism which pumped the liquid-metal liner.\n\nOne major problem that LINUS encountered was related to hydrodynamic instabilities in the liquid liner. If the liquid wasn't properly compressed, it could result in instabilities in the plasma called RT instabilities. This condition could destroy the fusion reaction by injecting liner material (vaporized lead and lithium) into the plasma. This problem diminished the efficiency of the fusion reactions, and could even cause damage to the reactor. Synchronizing the timing of the compression system was not possible with the technology of the time, and the proposed design was canceled.\n\n"}
{"id": "173309", "url": "https://en.wikipedia.org/wiki?curid=173309", "title": "Liquefaction", "text": "Liquefaction\n\nIn materials science, liquefaction is a process that generates a liquid from a solid or a gas or that generates a non-liquid phase which behaves in accordance with fluid dynamics. \nIt occurs both naturally and artificially. As an example of the latter, a \"major commercial application of liquefaction is the liquefaction of air to allow separation of the constituents, such as oxygen, nitrogen, and the noble gases.\" Another is the conversion of solid coal into a liquid form usable as a substitute for liquid fuels.\n\nIn geology, soil liquefaction refers to the process by which water-saturated, unconsolidated sediments are transformed into a substance that acts like a liquid, often in an earthquake.\n\nSoil liquefaction blamed for building collapses in the city of Palu, Indonesia in October 2018. \n\nIn a related phenomena, liquefaction of bulk materials in cargo ships may cause a dangerous shift in the load.\n\nIn physics and chemistry, the phase transitions from solid and gas to liquid (melting and condensation, respectively) may be referred to as liquefaction. The melting point (sometimes called liquefaction point) is the temperature and pressure at which a solid becomes a liquid.\n\nIn commercial and industrial situations, the process of condensing a gas to liquid is sometimes referred to as liquefaction of gases.\n\nCoal liquefaction is the production of liquid fuels from coal using a variety of industrial processes.\n\nLiquefaction is also used in commercial and industrial settings to refer to mechanical dissolution of a solid by mixing, grinding or blending with a liquid.\n\nIn kitchen or laboratory settings, solids may be chopped into smaller parts sometimes in combination with a liquid, for example in food preparation or laboratory use. This may be done with a blender, or liquidiser in British English.\n\nIn biology, liquefaction often involves organic tissue turning into a more liquid-like state. For example, liquefactive necrosis in pathology, or liquefaction as a parameter in semen analysis.\n\n"}
{"id": "46192772", "url": "https://en.wikipedia.org/wiki?curid=46192772", "title": "List of electric bicycle brands and manufacturers", "text": "List of electric bicycle brands and manufacturers\n\nThis page lists electric bicycle brands and manufacturing companies past and present, including electric unicycles. For bicycle parts, see List of bicycle part manufacturing companies.\n\nMany bicycle brands do not manufacture their own product, but rather import and re-brand bikes manufactured by others, sometimes designing the bike, specifying the equipment, and providing quality control. There are also brands that have, at different times, been manufacturers as well as re-branders: a company with manufacturing capability may market models made by other (overseas) factories, while simultaneously manufacturing bicycles in-house, for example, high-end models.\n\n\n"}
{"id": "146630", "url": "https://en.wikipedia.org/wiki?curid=146630", "title": "Monolayer", "text": "Monolayer\n\nA monolayer is a single, closely packed layer of atoms, molecules, or cells. In some cases it is referred to as a self-assembled monolayer. Monolayers of layered crystals like graphene and molybdenum disulfide are generally called 2D materials.\n\nA Langmuir monolayer or insoluble monolayer is a one-molecule thick layer of an insoluble organic material spread onto an aqueous subphase in a Langmuir-Blodgett Trough. Traditional compounds used to prepare Langmuir monolayers are amphiphilic materials that possess a hydrophilic headgroup and a hydrophobic tail. Since the 1980s a large number of other materials have been employed to produce Langmuir monolayers, some of which are semi-amphiphilic, including polymeric, ceramic or metallic nanoparticles and macromolecules such as polymers. Langmuir monolayers are extensively studied for the fabrication of Langmuir-Blodgett film (LB films), which are formed by transferred monolayers on a solid substrate. A Gibbs monolayer or soluble monolayer is a monolayer formed by a compound that is soluble in one of the phases separated by the interface on which the monolayer is formed.\n\nThe monolayer formation time or monolayer time is the length of time required, on average, for a surface to be covered by an adsorbate, such as oxygen sticking to fresh aluminum. If the adsorbate has a unity sticking coefficient, so that every molecule which reaches the surface sticks to it without re-evaporating, then the monolayer time is very roughly:\nwhere \"t\" is the time and \"P\" is the pressure. It takes about 1 second for a surface to be covered at a pressure of 300 µPa (2×10 Torr).\n\nA Langmuir monolayer can be compressed or expanded by modifying its area with a moving barrier in a Langmuir film balance. If the surface tension of the interface is measured during the compression, a \"compression isotherm\" is obtained. This isotherm shows the variation of surface pressure (formula_2, where formula_3 is the surface tension of the interface before the monolayer is formed) with the area (the inverse of surface concentration formula_4). It is analogous with a 3D process in which pressure varies with volume.\n\nA variety of bidimensional phases can be detected, each separated by a phase transition. During the phase transition, the surface pressure doesn't change, but the area does, just like during normal phase transitions volume changes but pressure doesn't.\nThe 2D phases, in increasing pressure order:\n\nIf the area is further reduced once the solid phase has been reached, collapse occurs, the monolayer breaks and soluble aggregates and multilayers are formed\n\nGibbs monolayers also follow equations of state, which can be deduced from Gibbs isotherm.\n\nMonolayers have a multitude of applications both at the air-water and at air-solid interphases.\n\nNanoparticle monolayers can be used to create functional surfaces that have for instance anti-reflective or superhydrophobic properties.. \n\nMonolayers are frequently encountered in biology. A micelle is a monolayer, and the phospholipid lipid bilayer structure of biological membranes is technically two monolayers. Langmuir monolayers are commonly used to mimic cell membrane to study the effects of pharmaceuticals or toxins.\n\nIn cell culture a monolayer refers to a layer of cells in which no cell is growing on top of another, but all are growing side by side and often touching each other on the same growth surface.\n\n\n"}
{"id": "32841509", "url": "https://en.wikipedia.org/wiki?curid=32841509", "title": "Nena Baltazar", "text": "Nena Baltazar\n\nTania \"Nena\" Baltazar Lugones (born October 31, 1972) is a co-founder of Comunidad Inti Wara Yassi in Bolivia and the organization's current president.\n"}
{"id": "942255", "url": "https://en.wikipedia.org/wiki?curid=942255", "title": "Nuclear navy", "text": "Nuclear navy\n\nA nuclear navy, or nuclear-powered navy, refers to the portion of a navy consisting of naval ships powered by nuclear marine propulsion. The concept was revolutionary for naval warfare when first proposed. Prior to nuclear power, submarines were powered by diesel engines and could only submerge through the use of batteries. In order for these submarines to run their diesel engines and charge their batteries they would have to surface or snorkel. The use of nuclear power allowed these submarines to become true submersibles and unlike their conventional counterparts, they became limited only by crew endurance and supplies.\n\nThe United States Navy has by far the most nuclear-powered aircraft carriers, with ten Nimitz-class carriers and one Gerald R. Ford-class carrier in service. The last conventionally-powered aircraft carrier left the U.S. fleet as of 12 May 2009, when the USS \"Kitty Hawk\" (CV-63) was deactivated. France's latest aircraft carrier, the \"R91 Charles de Gaulle\", is nuclear-powered. The United Kingdom rejected nuclear power early in the development of its \"Queen Elizabeth\"-class aircraft carriers on cost grounds, as even several decades of fuel use costs less than a nuclear reactor. Since 1949 the Bettis Atomic Power Laboratory near Pittsburgh, Pennsylvania has been one of the lead laboratories in the development of the nuclear navy. The planned Indian INS Vishal and the Indigenous Chinese Carriers also feature Nuclear Propulsion.\n\nThe United States Navy operates the largest fleet of nuclear submarines. Only the United States Navy, the Royal Navy of the United Kingdom, and France's \"Marine Nationale\" field an all-nuclear submarine force. By 1989, there were over 400 nuclear-powered submarines operational or being built. Some 250 of these submarines have now been scrapped and some on order cancelled, due to weapons reduction programs. Russia and the United States had over one hundred each, with the United Kingdom and France fewer than twenty each and China six. The Indian Navy launched their first indigenous \"Arihant\" class nuclear-powered submarines on 26 July 2009. India is also operating one nuclear attack submarine with talks of leasing one more nuclear submarine from Russia. India plans to build six nuclear attack submarines and follow on to the Arihant class of ballistic missile submarines.\n\nThe US had several nuclear cruisers. The cruisers were the USS \"Bainbridge\", USS \"California\", USS \"Long Beach\", USS \"Truxtun\", USS \"South Carolina\", USS \"Virginia\", USS \"Texas\", USS \"Mississippi\", and USS \"Arkansas\". \"Long Beach\" was deemed too expensive and was decommissioned in 1995 instead of receiving her third nuclear refueling and proposed upgrade. She was sold for scrap in 2012 at Puget Sound Naval Shipyard. Currently the United States does not have any nuclear cruisers.\n\nRussia has four \"Kirov\"-class battlecruisers, though only one is active, the other three being laid up. The command ship \"SSV-33 Ural\", based on the Kirov class, is also laid up. Seven civilian nuclear icebreakers remain in service: four of six Arktika class icebreakers, the two Taymyr-class icebrakers \"Taymyr\" and \"Vaygach\", and the LASH carrier and container ship \"Sevmorput\".\n\nBy 2003 the U.S. Navy had accumulated over 5,400 \"reactor years\" of accident-free experience, and operated more than 80 nuclear-powered ships.\n\nAdmiral Hyman G. Rickover, (1900–1986), of the United States Navy, known as \"father of the nuclear navy\"\nwas an electrical engineer by training, and was the primary architect who implemented this daring concept, and believed that it was the natural next phase for the way military vessels could be propelled and powered. The challenge was to reduce the size of a nuclear reactor to fit on board a ship or submarine, as well as to encase it sufficiently so that radiation hazards would not be a safety concern.\n\nSoon after World War II, Rickover was assigned to the Bureau of Ships in September 1947 and received training in nuclear power at Oak Ridge, Tennessee. In February 1949 he received an assignment to the Division of Reactor Development, U.S. Atomic Energy Commission and then assumed control of the United States Navy's effort as Director of the Naval Reactors Branch in the Bureau of Ships. This dual role allowed him to lead the efforts to develop the world's first nuclear-powered submarine, USS \"Nautilus\" (SSN 571), which was launched in 1954. As Vice Admiral, from 1958, for three decades Rickover exercised tight control over the ships, technology, and personnel of the nuclear navy, even interviewing every prospective officer for new nuclear-powered navy vessels.\n\nLeading nuclear physicist Philip Abelson (1913–2004) turned his attention under the guidance of Ross Gunn to applying nuclear power to naval propulsion. Their early efforts at Naval Research Laboratory (NRL) provided an early glimpse at what was to become the nuclear Navy.\n\nAt the present time, many important vessels in the United States Navy are powered by nuclear reactors. All submarines and aircraft carriers are nuclear-powered. Several cruisers were nuclear-powered but these have all been retired.\n\nUnited States naval reactors are given three-character designations consisting of a letter representing the ship type the reactor is designed for, a consecutive generation number, and a letter indicating the reactor's designer. The ship types are \"A\" for aircraft carrier, \"C\" for cruiser, \"D\" for destroyer, and \"S\" for submarine. The designers are \"W\" for Westinghouse, \"G\" for General Electric, \"C\" for Combustion Engineering, and \"B\" for Bechtel. Examples are S5W, D1G, A4W, and D2W.\n\nMost information concerning United States naval reactors is not secret—see Naval Nuclear Propulsion Information.\n\n\n\n\n"}
{"id": "30509260", "url": "https://en.wikipedia.org/wiki?curid=30509260", "title": "Palmiet Nature Reserve", "text": "Palmiet Nature Reserve\n\nPalmiet Nature Reserve is located in Westville, a town located ten kilometres from Durban, South Africa. The reserve is made up of a wide variety of natural habitats ranging from forests to grassland. Hundreds of bird species have been recorded and the tree list is extensive. There are a number of self-guided trails that include river crossings.\n\nIgwalagwala Cliff, found in the reserve, is a site of archaeological significance as it has evidence of prehistoric human habitation.\n\n\nCoordinates: \n"}
{"id": "50011158", "url": "https://en.wikipedia.org/wiki?curid=50011158", "title": "Photoelectrochemical oxidation", "text": "Photoelectrochemical oxidation\n\nPhotoelectrochemical oxidation (PECO) is the process by which incident light enables a semiconductor material to promote a catalytic oxidation reaction. While a photoelectrochemical cell typically involves both a semiconductor (electrode) and a metal (counter-electrode), at sufficiently small scales, pure semiconductor particles can behave as microscopic photoelectrochemical cells. PECO has been used for the detoxification of air and water, hydrogen production, and other applications.\n\nThe process by which a photon initiates a chemical reaction directly is known as photolysis; if this process is aided by a catalyst, it is called photocatalysis. If a photon has more energy than a material's characteristic band gap, it can free an electron upon absorption by the material. The remaining, positively charged hole and the free electron may recombine, generating heat, or they can take part in photoreactions with nearby species. If the photoreactions with these species result in regeneration of the electron-donating material—i.e., if the material acts as a catalyst for the reactions—then the reactions are deemed photocatalytic. PECO represents a type of photocatalysis whereby semiconductor-based electrochemistry catalyzes an oxidation reaction—for example, the oxidative degradation of an airborne contaminant in air purification systems.\n\nThe principal objective of photoelectrocatalysis is to provide low-energy activation pathways for the passage of electronic charge carriers through the electrode electrolyte interface and, in particular, for the photoelectrochemical generation of chemical products. With regard to photoelectrochemical oxidation, we may consider, for example, the following system of reactions, which constitute TiO-catalyzed oxidation.\n\nThis system shows a number of pathways for the production of oxidative species that facilitate the oxidation of the species, RX, in addition to its direct oxidation by the excited TiO itself. PECO concerns such a process where the electronic charge carriers are able to readily move through the reaction medium, thereby to some extent mitigating recombination reactions that would limit the oxidative process. The “photoelectrochemical cell” in this case could be as simple as a very small particle of the semiconductor catalyst. Here, on the “light” side a species is oxidized, while on the “dark” side a separate species is reduced.\n\nThe classical macroscopic photoelectrochemical system consists of a semiconductor in electric contact with a counter-electrode. For n-type semiconductor particles of sufficiently small dimension, the particles polarize into anodic and cathodic regions, effectively forming microscopic photoelectrochemical cells. The illuminated surface of a particle catalyzes a photooxidation reaction, while the “dark” site of the particle facilitates a concomitant reduction.\n\nPhotoelectrochemical oxidation may be thought of as a special case of photochemical oxidation (PCO). Photochemical oxidation entails the generation of radical species that enable oxidation reactions, with or without the electrochemical interactions involved in semiconductor-catalyzed systems, which occur in photoelectrochemical oxidation. An example of a photochemical oxidation system that is not strictly speaking photoelectrochemical in nature would be the oxidative degradation of organic contaminants by the HO/UV process. Here, direct photolysis of HO generates the hydroxyl radicals needed for oxidative degradation without the use of a semiconductor catalyst.\n\nThe primary applications for photochemical / photoelectrochemical oxidation include air disinfection / purification, water disinfection / purification, hydrogen production (i.e., through water splitting), and environmental remediation, to name a few. Significant work has been done in the realm of air and water treatment, which is described in more detail below.\n\nPECO has shown promise for the disinfection (i.e., removal of biological aerosols) and purification (i.e., more generally, removing contaminants) of air in HVAC and other applications. Photocatalytic oxidation has been used to successfully oxidize various organic contaminants, including alcohols, chlorocarbons, and BTEX compounds. With an effective catalyst, and light source of sufficient intensity and appropriate wavelength, virtually any volatile organic compound can be removed, at least in part, from an air source.\n\nPerhaps more important for HVAC applications in general is the disinfection of air. In many homes and workplaces, allergens can hamper comfort and productivity in sensitive individuals; these materials can be oxidized via a PECO system, provided the residence time in the treatment media is sufficient. More troubling are virus and / or bacteria, which can cause infectious diseases if they are allowed to accumulate in indoor air. UV treatment for air disinfection has been proven effective; however, use of an effective photocatalyst (e.g., TiO in a PECO air cleaning system) with UV radiation leads to significantly higher reductions in populations of bacteria.\n\nAir treatment in HVAC applications can lead to unseen benefits. In a process referred to HVAC load reduction (HLR), air treatment may minimize the number of air changes required to maintain sufficient indoor air quality, thereby reducing energy use.\n\nAs with air, PECO treatment can be applied to aqueous systems for disinfection / purification purposes. Some of the early work in investigating semiconductor-catalyzed detoxification processes was done on water systems, including that of Kinney and Ivanuski. A good example of photocatalytic water purification methods as they pertain to environmental remediation can be seen in the field pilot test performed at the Tyndall Air Force Base, near Panama City, Florida, in 1992. Results showed approximately 90 – 100 percent reduction in contaminant concentration (predominately BTEX) with treatment time of approximately 45 minutes.\n\nAn important early effort in the field is that of Goodeve and Kitchener, who were among the first to demonstrate the “photosensitization” of TiO—e.g., as evidenced by the fading of paints incorporating it as a pigment. Another important work is that of Markham and Laidler. They demonstrated through several measurements and a process of deduction that the generation of hydrogen peroxide by aqueous suspensions of ZnO under UV irradiation must occur by transferring a freed electron from the semiconductor, forming the perhydroxyl radical. They further indicated that when organic compounds were present, these compounds were oxidized at the semiconductor surface by oxidizing species with increased production of hydrogen peroxide.\n\nEarly forays into the realm of macro-scale photoelectrochemical cells began at Bell Laboratories, where researchers measured electrochemical responses of various semiconductors, including TiO, to light and dark. Boddy demonstrated the evolution of oxygen using rutile-phase TiO as an anode in a cell containing various types of electrolytic solutions under UV irradiation. In this case, n-type TiO is shown to liberate the electrons (as free radicals) necessary to split water and generate gaseous oxygen. Subsequent researchers continued to elucidate the potential for semiconductor-based photoelectrochemical water splitting, beginning with the work of Fujishima and Honda.\n\nAn important early study in the photoelectrochemical oxidation of organic contaminants in water was undertaken by Kinney and Ivanuski. They showed the potential for a variety of metal oxides, including TiO, to catalyze the oxidation of dissolved organic materials (phenol, benzoic acid, acetic acid, sodium stearate, and sucrose) under illumination by sunlamps. Additional work by Carey et al. showed similar oxidative degradation in the photodechlorination of PCBs in the presence of TiO.\n\nSignificant work has been done designing photo reactors that make use of the basic principle behind PECO for detoxification. Many of these reactors use solar radiation as the photon source, although in some cases other light sources may be employed. Pacheco and Tyner developed a simple solar photocatalytic reactor modeled on the well-known parabolic trough concentrator design. They distributed the catalyst (TiO) in TCE-contaminated water for treatment. Mehos and Turchi applied a similar reactor to groundwater remediation. An alternate strategy for water treatment is the falling-film reactor, which may use sunlight as the irradiation source in a central receiver-type arrangement.\n\nGoswami was the first to propose a system for photocatalytic air disinfection through the incorporation of photocatalyst material, light source, and filter in existing HVAC systems. This strategy was specifically intended to address bio-aerosols in air streams that, if left unchecked, could lead to “sick building syndrome.” Later work by Goswami incorporated PECO processes to treat bio-aerosols and other contaminants (e.g., VOCs) in a similar system for air and water purification.\n\n\n"}
{"id": "625318", "url": "https://en.wikipedia.org/wiki?curid=625318", "title": "Photorefractive effect", "text": "Photorefractive effect\n\nThe photorefractive effect is a nonlinear optical effect seen in certain crystals and other materials that respond to light by altering their refractive index.\nThe effect can be used to store temporary, erasable holograms and is useful for holographic data storage.\nIt can also be used to create a phase-conjugate mirror or an optical spatial soliton.\n\nThe photorefractive effect occurs in several stages:\n\nThe photorefractive effect can be used for dynamic holography, and, in particular, for cleaning of coherent beams. \nFor example, in the case of a hologram, illuminating the grating with just the reference beam causes the reconstruction of the original signal beam. When two coherent laser beams (usually obtained by splitting a laser beam by the use of a beamsplitter into two, and then suitably redirecting by mirrors) cross inside a photorefractive crystal, the resultant refractive index grating diffracts the laser beams. As a result, one beam gains energy and becomes more intense at the expense of light intensity reduction of the other. This phenomenon is an example of two-wave mixing. In this configuration, Bragg diffraction condition is automatically satisfied.\n\nThe pattern stored inside the crystal persists until the pattern is erased; this can be done by flooding the crystal with uniform illumination which will excite the electrons back into the conduction band and allow them to be distributed more uniformly.\n\nPhotorefractive materials include barium titanate (BaTiO), lithium niobate (LiNbO), vanadium doped zinc telluride (ZnTe:V), organic photorefractive materials, certain photopolymers, and some multiple quantum well structures.\n\nThere were even claims that an amplifier based on photorefractive crystals can have less than the minimum quantum noise that is typical for optical amplifiers of any kind.\n"}
{"id": "41435303", "url": "https://en.wikipedia.org/wiki?curid=41435303", "title": "Plug-in electric vehicles in Canada", "text": "Plug-in electric vehicles in Canada\n\nCumulative sales of plug-in electric cars in Canada passed the 20,000 unit mark in May 2016, and the 30,000 unit mark in January 2017.\n\nThe Chevrolet Volt, released in 2011, is the all-time top selling plug-in electric vehicle in the country, with cumulative sales of 6,387 units through May 2015 (representing over 30% of all plug-in cars sold in the country). Ranking second is the Tesla Model S with 4,160 units sold through April 2016, followed by the Nissan Leaf with 3,692 units delivered . The Model S was the top selling plug-in electric car in Canada in 2015 with 2,010 units sold.\n\nThere were 18,451 highway legal plug-in electric cars registered in Canada , of which, 10,034 (54%) are all-electric cars and 8,417 (46%) are plug-in hybrids. These figures include some used imports from the U.S. Until 2014 Canadian sales were evenly split between all-electric cars (50.8)% and plug-in hybrids (49.2%). The following table presents new car sales by year of all the highway-capable plug-in electric cars available in Canada between 2011 and December 2015.\n\nQuebec is the regional market leader in Canada, with about 11,000 plug-in electric cars registered , of which, 55% are plug-in hybrids. Registrations in the province totaled 3,100 units in 2015, representing a market share of 0.7% of new car sales, and 45% of total Canadian plug-in electric car sales that year.\n\nA total of 1,969 plug-in cars were sold in 2012, up from 521 in 2011. Sales climbed 57.7% in 2013 to 3,106 units, and in 2014 were up 63.0% from 2013 to 5,062 units, reaching cumulative sales of 10,658 plug-in cars through December 2014. The market share of the plug-in electric car segment grew from 0.03% in 2011, to 0.12% in 2012, and reached 0.27% of new car sales in the country in 2014.\n\nBritish Columbia is the only place in the country where it is legal to drive a low-speed vehicle (LSV) electric car on public roads, although it also requires low speed warning marking and flashing lights. Quebec is allowing LSVs in a three-year pilot project. These cars will not be allowed on the highway, but will be allowed on city streets.\n\nIn January 2009, Hydro-Québec and Mitsubishi signed an agreement to test 50 i-MiEV, at the time, the largest pilot test of electric cars in Canada ever. The test's goal was to allow a better understanding of winter usage of the technology. BC-Hydro and Mitsubishi had previously tested a three-vehicle fleet in British Columbia. In October 2010, Transport Canada and Mitsubishi Motor Sales of Canada announced a partnership to test the Mitsubishi i-MiEV. Transport Canada's ecoTECHNOLOGY for Vehicles (eTV) Program tested two i-MiEVs in government facilities and in a variety of real-world conditions. This program aim was to evaluate the i-MiEV road performance and range. Retail sales of the i-MiEV began in December 2011,\n\nThe Nissan Leaf roll-out in Canada began with fleet customers on July 29, 2011, and deliveries to individuals began in late September 2011. , the Leaf was sold only through 27 Leaf-certified dealers for the entire country, and sales were limited to customers who live within a radius of one of those dealers. Cumulative sales through December 2014 reached 1,965 units, and, , the Leaf ranked as the top selling all-electric car in the country.\n\nRetail sales of the Tesla Model S began in 2012, with 95 cars delivered that year. A total of 638 units were sold in 2013, and cumulative sales reached 1,580 units through December 2014, allowing the Model S to rank as the second best selling all-electric car in the country. During 2014 the BMW i3, Kia Soul EV, BMW i8 and Porsche 918 Spyder were introduced in the Canadian market. The top selling models in 2015 were the Tesla Model S with 2,010 units, followed by the Chevrolet Volt with 1,463, the Nissan Leaf with 1,233, the BMW i3 with 367, and the Kia Soul EV with 318. In 2015, the Model S passed the Nissan Leaf as the all-time best selling all-electric car in Canada.\n\nThe all-electric Renault Twizy 40 low-speed quadricycle was certified by Transport Canada in March 2016, and was scheduled to be released on the Canadian market by mid-2016.\n\nPurchase incentives for new plug-in electric vehicles (PEVs) were established in Ontario consisting of a rebate between (4 kWh battery) to (17 kWh or more) (~ to ), depending on battery size, for purchasing or leasing a new PEV after July 1, 2010. The rebates will be available to the first 10,000 applicants who qualify. The province also introduced green-coloured licence plates for exclusive use of plug-in hybrids and battery electric vehicles. These unique green vehicle plates allow PEV owners to travel in the province's carpool lanes until 2015 regardless of the number of passengers in the vehicle. Also, owners are eligible to use recharging stations at GO Transit and other provincially owned parking lots.\n\nQuebec began offering rebates of up to (~ ) beginning on January 1, 2012, for the purchase of new plug-in electric vehicles equipped with a minimum of 4 kWh battery, and new hybrid electric vehicles are eligible for a rebate. All-electric vehicles with high-capacity battery packs were eligible for the full rebate, and incentives were reduced for low-range electric cars and plug-in hybrids. Quebec's government earmarked () for the program, and the maximum rebate amount was set to be slowly reduced every year until a maximum of in 2015, but the rebates would continue until the fund runs out. There was also a ceiling for the maximum number of eligible vehicles: 10,000 for all-electric vehicles and plug-in hybrids, and 5,000 for conventional hybrids.\n\nIn November 2013, the provincial government announced its decision to earmark in 2014 an additional (~ ) to fund a three-year extension to the electric-vehicle rebate program. The maximum rebate was kept at , but a graded scale was introduced in order to spread the incentive over 10,000 or more vehicles. Quebec's government also set the goal to deploy 12,500 more electric vehicles in the province by 2017, consisting of 10,200 consumer cars, 325 taxis, and 2,000 government-fleet vehicles. Also, incentives were issued for \"greening\" 525 taxis, aimed to introduce 325 plug-in vehicles (275 plug-in hybrids and 50 all-electrics) and 200 conventional hybrids. The purchase incentives start at for battery-electric taxis, for plug-in hybrids, and for conventional hybrids, with the rebate declining over time. The province planned to also subsidize the deployment of charging stations for taxis.\n\nIn October 2016, the National Assembly of Quebec passed a new zero emission vehicle legislation that obliges any carmaker who sells in the Canadian province more than 4,500 new vehicles per year over a three-year average, to offer their customers a minimum number of plug-in hybrid and all-electric models. Under the new law, 3.5% of the total number of autos sold by carmakers in Quebec have to be zero emissions vehicles (ZEV) starting in 2018, rising to 15.5% in 2020. A tradable credit system was created for those carmakers not fulfilling their quotas to avoid financial penalties. The quotas will be determined by Quebec's Ministry of Sustainable Development. Quebec became the first Canadian province to pass such legislation, joining ten U.S. states, including California, that have similar ZEV laws. Quebec aims to have 100,000 zero emission vehicles on the road by 2020. Initially, the provincial government set the goal in 2011 to have 300,000 plug-in vehicles on the roads by 2020.\n\nThe Government of British Columbia announced the \"LiveSmart BC\" program which will start offering rebates of up to per eligible clean energy vehicle commencing on December 1, 2011. The incentives will be available until March 31, 2013 or until available funding is depleted, whichever comes first. Available funds are enough to provide incentives for approximately 1,370 vehicles. Battery electric vehicles, fuel cell vehicles and plug-in hybrids with battery capacity of 15.0 kWh and above are eligible for a incentive. Also effective December 1, 2011, rebates of up to per qualifying electric vehicle charging equipment will be available to B.C. residents who have purchased a clean energy vehicle.\n\n"}
{"id": "23316", "url": "https://en.wikipedia.org/wiki?curid=23316", "title": "Pound (mass)", "text": "Pound (mass)\n\nThe pound or pound-mass is a unit of mass \nused in the imperial, United States customary and other systems of measurement. Various definitions have been used; the most common today is the international avoirdupois pound, which is legally defined as exactly , and which is divided into 16 avoirdupois ounces. The international standard symbol for the avoirdupois pound is lb; an alternative symbol is lb (for most pound definitions), # (chiefly in the U.S.), and ℔ or ″̶ (specifically for the apothecaries' pound).\n\nThe unit is descended from the Roman \"libra\" (hence the abbreviation \"lb\"). The English word \"pound\" is cognate with, among others, German \"Pfund\", Dutch \"pond\", and Swedish \"pund\". All ultimately derive from a borrowing into Proto-Germanic of the Latin expression \"lībra pondō\" (\"a pound by weight\"), in which the word \"pondō\" is the ablative case of the Latin noun \"pondus\" (\"weight\").\n\nUsage of the unqualified term \"pound\" reflects the historical conflation of mass and weight. This accounts for the modern distinguishing terms \"pound-mass\" and \"pound-force\".\n\nThe United States and countries of the Commonwealth of Nations agreed upon common definitions for the pound and the yard. Since 1 July 1959, the international avoirdupois pound (symbol lb) has been defined as exactly .\n\nIn the United Kingdom, the use of the international pound was implemented in the Weights and Measures Act 1963.\n\nAn avoirdupois pound is equal to 16 avoirdupois ounces and to exactly 7,000 grains. The conversion factor between the kilogram and the international pound was therefore chosen to be divisible by 7, and an (international) grain is thus equal to exactly .\n\nIn the UK, the process of metrication and European units of measurement directives were expected to eliminate the use of the pound and ounce, but in 2007 the European Commission abandoned the requirement for metric-only labelling on packaged goods there, and allowed for dual metric–imperial marking to continue indefinitely. When used as a measurement of body weight the UK practice remains to use the stone of 14 pounds as the primary measure e.g. \"11 stone 4 pounds\", rather than \"158 pounds\" (as done in the US), or \"72 kilograms\" as used elsewhere.\n\nThe US has not adopted the metric system despite many efforts to do so, and the pound remains widely used as one of the key United States customary units.\n\nHistorically, in different parts of the world, at different points in time, and for different applications, the pound (or its translation) has referred to broadly similar but not identical standards of mass or force.\n\nThe libra (Latin for \"scales / balance\") is an ancient Roman unit of mass that was equivalent to approximately 328.9 grams. It was divided into 12 \"unciae\" (singular: \"uncia\"), or ounces. The \"libra\" is the origin of the abbreviation for pound, \"lb\".\n\nA number of different definitions of the pound have historically been used in Britain. Amongst these were the avoirdupois pound and the obsolete Tower, merchant's and London pounds. Troy pounds and ounces remain in use only for the weight of certain precious metals, especially in the trade; these are normally quoted just in ounces (e.g. \"500 ounces\") and, when the type of ounce is not explicitly stated, the troy system is assumed.\n\nHistorically, the pound sterling was a Tower pound of silver. In 1528, the standard was changed to the Troy pound.\n\nThe avoirdupois pound, also known as the wool pound, first came into general use c. 1300. It was initially equal to 6992 troy grains. The pound avoirdupois was divided into 16 ounces. During the reign of Queen Elizabeth, the avoirdupois pound was redefined as 7,000 troy grains. Since then, the grain has often been an integral part of the avoirdupois system. By 1758, two Elizabethan Exchequer standard weights for the avoirdupois pound existed, and when measured in troy grains they were found to be of 7,002 grains and 6,999 grains.\n\nIn the United Kingdom, weights and measures have been defined by a long series of Acts of Parliament, the intention of which has been to regulate the sale of commodities. Materials traded in the marketplace are quantified according to accepted units and standards in order to avoid fraud. The standards themselves are legally defined so as to facilitate the resolution of disputes brought to the courts; only legally defined measures will be recognised by the courts. Quantifying devices used by traders (weights, weighing machines, containers of volumes, measures of length) are subject to official inspection, and penalties apply if they are fraudulent.\n\nThe Weights and Measures Act of 1878 marked a major overhaul of the British system of weights and measures, and the definition of the pound given there remained in force until the 1960s. The pound was defined thus (Section 4) \"The ... platinum weight ... deposited in the Standards department of the Board of Trade ... shall continue to be the imperial standard of ... weight ... and the said platinum weight shall continue to be the Imperial Standard for determining the Imperial Standard Pound for the United Kingdom\". Paragraph 13 states that the weight \"in vacuo\" of this standard shall be called the Imperial Standard Pound, and that all other weights mentioned in the act and permissible for commerce shall be ascertained from it alone. The First Schedule of the Act gave more details of the standard pound: it is a platinum cylinder nearly high, and diameter, and the edges are carefully rounded off. It has a groove about from the top, to allow the cylinder to be lifted using an ivory fork. It was constructed following the destruction of the Houses of Parliament by fire in 1834, and is stamped P.S. 1844, 1 lb (P.S. stands for \"Parliamentary Standard\"). This definition of the Imperial pound remains unchanged.\n\nThe 1878 Act said that contracts worded in terms of metric units would be deemed by the courts to be made according to the Imperial units defined in the Act, and a table of metric equivalents was supplied so that the Imperial equivalents could be legally calculated. This defined, in UK law, metric units in terms of Imperial ones. The equivalence for the pound was given as 1 lb = or 0.45359 kg, which made the kilogram equivalent to about . In 1883, it was determined jointly by the Standards Department of the Board of Trade and the Bureau International that was a better approximation, and this figure, rounded to was given legal status by an Order in Council in May 1898.\n\nHowever, in 1963, a new Weights and Measures Act reversed this relationship and the pound was defined for the first time as a mass equal to to match the definition of the international pound agreed in 1959.\n\nA troy pound is equal to 12 troy ounces and to 5,760 grains, that is exactly grams. Troy weights were used in England by jewellers. Apothecaries also used the troy pound and ounce, but added the drachms and scruples unit in the Apothecaries' system of weights.\n\nTroy weight may take its name from the French market town of Troyes in France where English merchants traded at least as early as the early 9th century.\n\nThe troy pound is no longer in general use or a legal unit for trade (it was abolished in the United Kingdom on 6 January 1879 by the Weights and Measures Act of 1878), but the troy ounce, of a troy pound, is still used for measurements of gems such as opals, and precious metals such as silver, platinum and particularly gold.\n\nThe system called Tower weight was the more general name for King Offa's pound. This dates to 757 AD and was based on the silver penny. This in turn was struck over Arabic dirhams (2d). The pound was based on the weight of 120 Arabic silver dirhams, which have been found in Offa's Dyke. The same coin weight was used throughout the Hanseatic League.\n\nThe Tower pound was also called the Moneyers' Pound (referring to the Saxon moneyers before the Conquest), the easterling pound, which may refer to traders of eastern Germany, or to traders on the shore of the eastern Baltic sea, or dealers of Asiatic goods who settled at the Steelyard wharf; and the Rochelle Pound by French writers, because it was also in use at Rochelle. An almost identical weight was employed by the Germans for weighing gold and silver.\n\nThe mercantile pound (1304) of 6750 troy grains, or 9600 Tower grains, derives from this pound, as 25 shilling-weights or 15 Tower ounces, for general commercial use. Multiple pounds based on the same ounce were quite common. In much of Europe, the apothecaries' and commercial pounds were different numbers of the same ounce. \n\nThe Tower system was referenced to a standard prototype found in the Tower of London and ran concurrently with the avoirdupois and troy systems until the reign of Henry VIII, when a royal proclamation dated 1526 required that the troy pound to be used for mint purposes instead of the Tower pound. No standards of the Tower pound are known to have survived.\n\nThe Tower pound was equivalent to about 350 grams.\n\nThe merchants' pound (\"mercantile pound\", \"libra mercantoria\", or \"commercial pound\") was considered to be composed of 25 rather than 20 Tower shillings of 12 pence. It was equal to 9,600 wheat grains (15 tower ounces or 6,750 grains) and was used in England until the 14th century for goods other than money and medicine (\"electuaries\").\n\nThe London pound is that of the Hansa, as used in their various trading places. The London pound is based on 16 ounces, each ounce divided as the tower ounce. It never became a legal standard in England; the use of this pound waxed and waned with the influence of the Hansa itself.\n\nA London pound was equal to 7,200 troy grains (16 troy ounces) or, equivalently, 10,240 tower grains (16 tower ounces).\n\nIn the United States, the avoirdupois pound as a unit of mass has been officially defined in terms of the kilogram since the Mendenhall Order of 1893. That Order defined the pound to be pounds to a kilogram. The following year, this relationship was refined as pounds to a kilogram, following a determination of the British pound.\n\nAccording to a 1959 NIST publication, the United States 1894 pound differed from the international pound by approximately one part in 10 million. The difference is so insignificant that it can be ignored for almost all practical purposes.\n\nThe Byzantines used a series of measurements known as pounds (, , \"litra\"). The most common was the \"logarikē litra\" (λογαρική λίτρα, \"pound of account\"), established by Constantine the Great in 309/310. It formed the basis of the Byzantine monetary system, with one \"litra\" of gold equivalent to 72 \"solidi\". A hundred \"litrai\" were known as a \"kentēnarion\" (κεντηνάριον, \"hundredweight\"). Its weight seems to have decreased gradually from the original 324 grams to 319. Due to its association with gold, it was also known as the \"chrysaphikē litra\" (χρυσαφική λίτρα, \"gold pound\") or \"thalassia litra\" (θαλάσσια λίτρα, \"maritime pound\"), but it could also be used as a measure of land, equalling a fortieth of the \"thalassios modios\".\n\nThe \"soualia litra\" was specifically used for weighing olive oil or wood, and corresponded to 4/5 of the \"logarikē\", i.e. 256 g. Some outlying regions, especially in later times, adopted various local measures, based on Italian, Arab or Turkish measures. The most important of these was the \"argyrikē litra\" (αργυρική λίτρα, \"silver pound\") of 333 g, found in Trebizond and Cyprus, and probably of Arab origin.\n\nSince the Middle Ages, various pounds (\"livre\") have been used in France. Since the 19th century, a \"livre\" has referred to the \"metric pound\", 500g.\n\nThe \"livre esterlin\" was equivalent to about and was used between the late 9th century and the mid-14th century.\n\nThe \"livre poids de marc\" or \"livre de Paris\" was equivalent to about and was used between the 1350s and the late 18th century. It was introduced by the government of John II.\n\nThe \"livre métrique\" was set equal to the kilogram by the decree of \"13 Brumaire an IX\" between 1800 and 1812. This was a form of official metric pound.\n\nThe \"livre usuelle\" (customary unit) was defined as 500 grams by the decree of 28 March 1812. It was abolished as a unit of mass effective 1 January 1840 by a decree of 4 July 1837, but is still used informally.\n\nOriginally derived from the Roman libra, the definition varied throughout Germany in the Middle Ages and onward. The measures and weights of the Habsburg monarchy were reformed in 1761 by Empress Maria Theresia of Austria. The unusually heavy Habsburg (civil) pound of 16 ounces was later defined in terms of 560.012 grams. Bavarian reforms in 1809 and 1811 adopted essentially the same standard pound. In Prussia, a reform in 1816 defined a uniform civil pound in terms of the Prussian foot and distilled water, resulting in a Prussian pound of 467.711 grams.\n\nBetween 1803 and 1815, all German regions west of the River Rhine were French, organised in the departements: Roer, Sarre, Rhin-et-Moselle, and Mont-Tonnerre. As a result of the Congress of Vienna, these became part of various German states. However, many of these regions retained the metric system and adopted a metric pound of precisely 500 grams. In 1854, the pound of 500 grams also became the official mass standard of the German Customs Union, but local pounds continued to co-exist with the Zollverein pound for some time in some German states. Nowadays, the term \"Pfund\" is still in common use and universally refers to a pound of 500 grams.\n\nThe Russian pound (Фунт, funt) is an obsolete Russian unit of measurement of mass. It is equal to 409.51718 grams. In 1899, the Russian pound was the basic unit of weight and all other units of weight were formed from it.\n\nThe Skålpund was a Scandinavian measurement that varied in weight between regions. From the 17th century onward, it was equal to 425.076 grams in Sweden but was abandoned in 1889 when Sweden switched to the metric system.\n\nIn Norway, the same name was used for a weight of 498.1 grams. In Denmark, it equalled 471 grams.\n\nIn the 19th century, Denmark followed Germany's lead and redefined the pound as 500 grams.\n\nA Jersey pound is an obsolete unit of mass used on the island of Jersey from the 14th century to the 19th century. It was equivalent to about 7,561 grains (490 grams). It may have been derived from the French livre poids de marc.\n\nThe trone pound is one of a number of obsolete Scottish units of measurement. It was equivalent to between 21 and 28 avoirdupois ounces (about 600-800 grams).\n\nIn many countries, upon the introduction of a metric system, the pound (or its translation) became an informal term for 500 grams. In German, the term is \"Pfund\", in French \"livre\", in Dutch \"pond\", in Spanish and Portuguese \"libra\", in Italian \"libbra\", and in Danish and Swedish \"pund\".\n\nThough not from the same linguistic origin, the Chinese \"jīn\" (, also known as \"catty\") has a modern definition of exactly 500 grams, divided into 10 \"liǎng\" (). Traditionally about 605 grams, the \"jin\" has been in use for more than two thousand years, serving the same purpose as \"pound\" for the common-use measure of weight.\n\nHundreds of older pounds were replaced in this way. Examples of the older pounds are one of around 459 to 460 grams in Spain, Portugal, and Latin America; one of 498.1 grams in Norway; and several different ones in what is now Germany.\n\nAlthough the use of the pound as an informal term persists in these countries to a varying degree, scales and measuring devices are denominated only in grams and kilograms. A pound of product must be determined by weighing the product in grams as the use of the \"pound\" is not sanctioned for trade within the European Union.\n\nSmoothbore cannon and carronades are designated by the weight in imperial pounds of round solid iron shot of diameter to fit the barrel. A cannon that fires a six-pound ball, for example, is called a \"six-pounder\". Standard sizes are 6, 12, 18, 24, 32 and 42 pounds; 68-pounders also exist, and other nonstandard weapons use the same scheme. See carronade.\n\nA similar definition, using lead balls, exists for determining the gauge of shotguns.\n\n"}
{"id": "17894866", "url": "https://en.wikipedia.org/wiki?curid=17894866", "title": "Protectosil", "text": "Protectosil\n\nProtectosil is a silane manufactured by Evonik and marketed a protective coating for building surfaces. It is used as a water repellent, and for corrosion and graffiti control.\n"}
{"id": "21060764", "url": "https://en.wikipedia.org/wiki?curid=21060764", "title": "Pyrotechnic heat source", "text": "Pyrotechnic heat source\n\nA pyrotechnic heat source, also called heat pellet, is a pyrotechnic device based on a pyrotechnic composition with a suitable igniter. Its role is to produce controlled amount of heat. Pyrotechnic heat sources are usually based on thermite-like (or sometimes delay composition-like) fuel-oxidizer compositions with slow burn rate, high production of heat at desired temperature, and low to zero production of gases.\n\nPyrotechnic heat sources can be activated by multiple means. Electric match and percussion cap are the most common ones.\n\nPyrotechnic heat sources are often used for activation of thermal batteries, where they serve to melt the electrolyte. There are two main types of design. One uses a fuze strip (containing barium chromate and powdered zirconium metal in a ceramic paper) along the edge of the heat pellets to initiate burning. The fuze strip is typically fired by an electrical igniter or squib by application of electric current. The second design uses a center hole in the battery stack into which the high-energy electrical igniter fires a mixture of hot gases and incandescent particles. The center-hole design allows much faster activation times (tens of milliseconds) vs. hundreds of milliseconds for the edge-strip design. Battery activation can also be accomplished by a percussion primer, similar to a shotgun shell. It is desired that the pyrotechnic source be gasless. The standard heat source typically consist of mixtures of iron powder and potassium perchlorate in weight ratios of typically 88/12, 86/14, and 84/16. The higher the potassium perchorate level, the higher the heat output (nominally 200, 259, and 297 calories/gram, respectively). The size and thickness of the iron-perchlorate pellets has little influence on their burn rate, however the effect of density, composition, and particle size have significant effect on the burn rate and can be used for its adjusting for desired heat output profile. Another composition in use is zirconium with barium chromate. Another mixture is 46.67 wt.% of titanium, 23.33% of amorphous boron, and about 30% barium chromate. Yet another one is 45 wt.% tungsten, 40.5% barium chromate, 14.5% potassium perchlorate, and 1% vinyl alcohol acetate resin binder.\n\nReactions producing intermetallic components, e.g. zirconium with boron, can be used when entirely gasless operation, non-hygroscopic behavior, and independence on environmental pressure are desired. \n\nHeat paper can be prepared by impregnating paper or a fiberglass tape with a slurry of the mixture of fuel and oxidizer.\n\nA pyrotechnic heat source can be a direct part of a pyrotechnic composition e.g. in chemical oxygen generators a heat source composition with large surplus of oxidizer is used; the heat produced by burning the composition is used for thermal decomposition of the oxidizer. Relatively cold-burning compositions are used for production of colored smoke or for dispersion of aerosol of e.g. pesticides or CS gas, providing the heat of sublimation of the desired compound.\n\nA phase moderating component of the composition, which forms together with the combustion products a mixture with at least one distinct temperature of phase transition, may be used for stabilizing the burning temperature as a form of phase change material.\n"}
{"id": "55009619", "url": "https://en.wikipedia.org/wiki?curid=55009619", "title": "Ratass Church", "text": "Ratass Church\n\nRatass Church is a medieval church with ogham stone forming a National Monument in Tralee, Ireland.\n\nThe church is located in a graveyard on Quill Street, in the eastern suburbs of Tralee.\n\nIt is believed that a ringfort or embanked enclosure was built here first (\"Rath Mhaighe Teas\", \"fort of the southern plain\"), before a sandstone church was erected in the 10th century AD, and served as the centre of Kerry diocese from 1111 to 1117, when the seat was moved to Ardfert. The west gable and part of the nave walls belong to this earlier construction; the rest of the church is later.\n\nThe ogham stone is much earlier; based on the Primitive Irish grammar, it is placed in AD 550–600.\n\nThe stone is of fine purple sandstone (145 × 34 × 20 cm), with the inscription (\"name of Sílán son of Fáithloga\"). It was discovered in 1975 during a cleanup; it had been built into the sides of a 19th-century burial vault.\n"}
{"id": "22275171", "url": "https://en.wikipedia.org/wiki?curid=22275171", "title": "Remember Chek Jawa", "text": "Remember Chek Jawa\n\nRemember Chek Jawa is a 47-minute documentary made on digital video by freelance cinematographer Eric Lin Youwei. It documents a biodiversity survey conducted in 2001, by a small group of volunteer conservationists, headed by botanist Joseph Lai, months before the Singapore government's reclamation project at Chek Jawa, Pulai Ubin. Chek Jawa is an inter-tidal area of about 1 square kilometre which supports a vast amount of plant and animal lives encompassing six ecosystems.\n\nThe volunteers' efforts were rewarded when the government agreed, in December 2001, to defer use of the Chek Jawa land for the next ten years.\n\nThe documentary is available as a DVD under Objectifs Films, available in the libraries under the National Library Board, Singapore.\n"}
{"id": "22537224", "url": "https://en.wikipedia.org/wiki?curid=22537224", "title": "Severance (land)", "text": "Severance (land)\n\nA severance can in law mean the act of severing a piece of land from a larger tract of land. The severed parcel of land becomes a separate lot (parcel). Second, it can refer to, in jurisdictions that have the form of co-ownership, the ending of a joint tenancy by act or event other than death. Third, it can be defined in a definitions clause or table in ways including the removal of a party from an agreement, or a permitted ending of the agreement (e.g. break clause) — in an employment contract/negotiations especially common as to severance pay and other terms of severance — or part of the agreement in which case it may be either capable of forming the heart of a new agreement, that is being superseded or instead varied to be non-binding (avoided) as to future conduct (see voidable contract) and the parties should ensure which meaning is meant in this third range of senses.\n\nIn many jurisdictions, land use laws require that severances of land occur in an orderly fashion by way of plans of subdivision, (estate plans) when multiple lots are being created. In a registered (or other standardised land system) colouring and other conventions may apply to such title plans. To avoid complications, some jurisdictions allow severances of a minor nature to proceed without a plan of subdivision, as long as other criteria are met. In many cases this will only be permitted when reverting a previous combination of the land, in England and Wales, amalgamation (land). In England and Wales in unregistered land (around 15% of the total) any such severance (transfer of part) triggers compulsory registration of that new parcel (see registered land in English law. In Canada, approval of qualifying minor severances are often referred to as \"consents\", and the authority to grant consents is usually given to local planning bodies such as committees of adjustment or land division committees. Colloquially in Canada and the US the term \"severance\"/\"severed lot\" is often used to solely refer to such minor land divisions rather than to divisions undertaken by way of the more complicated subdivision process. Officially and commonly the terms \"severed land\" and \"new parcel/plot/tract/piece of land\" are used, synonymously in England and Wales with \"severed land\" removing all ambiguity.\n"}
{"id": "39664332", "url": "https://en.wikipedia.org/wiki?curid=39664332", "title": "Sodium bifluoride", "text": "Sodium bifluoride\n\nSodium bifluoride is the inorganic compound with the formula NaHF. It is a salt of sodium cation (Na) and bifluoride anion (HF). It is a white, water-soluble solid that decomposes upon heating . Sodium bifluoride is non-flammable, hygroscopic, and has a pungent smell. Sodium bifluoride has a number of applications in industry.\n\nSodium bifluoride dissociates to hydrofluoric acid and sodium fluoride:\nThe reverse of this reaction is employed to remove HF from elemental fluorine (F) produced by electrolysis.\nThis equilibrium is manifested when the salt is dissolved and when the solid is heated. Characteristic of other bifluorides, it reacts with acids to give HF. Illustrative is its reaction with bisulfate to form sodium sulfate and hydrogen fluoride.\n\nStrong bases deprotonate bilfluoride. For example, calcium hydroxide gives calcium fluoride.\n\nSodium bifluoride is produced by neutralizing waste hydrogen fluoride, which results from the production of superphosphate fertilizers. Typical bases are sodium carbonate and sodium hydroxide. The process occurs in two steps, illustrated with the hydroxide:\n\nSodium bifluoride reacts with water or moist skin to produce hydrofluoric acid. It also gives off hydrofluoric acid and hydrogen gas when it is heated to a gaseous state. The chemical can decompose upon contact with strong acids, strong bases, metal, water, or glass. Sodium bifluoride also engages in violent reactions with chromyl chloride, nitric acid, red phosphorus, sodium peroxide, diethyl sulfoxide, and diethylzinc.\n\nThe main role of sodium bifluoride is as a precursor to sodium fluoride, millions of tons of which are produced annually.\n\nThe compound also has applications in cleaning, capitalizing on the affinity of fluoride for iron and silicon oxides. For example, formulations of sodium bifluoride are used for cleaning brick, stone, ceramics, and masonry. It is also used to etch glass. Another application of sodium bifluoride is in the chemical industry. Other applications of the compound involve the galvanization of baths and pest control. Sodium bifluoride's biological applications include the preservation of zoological and anatomical samples.\n\nOther applications of sodium bifluoride include neutralizers of laundry-rinse.\n\nSodium bifluoride has a role in the process that is used to plate metal cans.\n\nSodium bifluoride also aids in the precipitation of calcium ions during the process of nickel electroplating. The compound also aids in increasing the corrosion of resistance of some magnesium alloys.\n\nSodium bifluoride is corrosive and an irritant upon contact with skin and can cause blistering and inflammation. It is extremely dangerous to ingest. If the compound is exposed to the eyes, blindness and corneal damage can result. Ingestion of sodium bifluoride dust can cause burning, coughing, and sneezing, as a result of irritating the gastrointestinal and respiratory tracts. Exposure of the compound to the eyes can cause redness, itching, and watering. In severe cases, exposure to sodium bifluoride can result in death. It can take between 0 and 24 hours for the effects of sodium bifluoride poisoning to be noticeable.\nExposure to sodium bifluoride repeatedly or over a long time can result in fluorosis. Sodium bifluoride is not known to be carcinogenic.\nSodium bifluoride does not bioaccumulate. It typically only remains in the environment for several days.\n"}
{"id": "1045015", "url": "https://en.wikipedia.org/wiki?curid=1045015", "title": "Storage tube", "text": "Storage tube\n\nStorage tubes are a class of cathode-ray tubes (CRTs) that are designed to hold an image for a long period of time, typically as long as power is supplied to the tube.\n\nA specialized type of storage tube, the Williams tube, was used as a main memory system on a number of early computers, from the late 1940s into the early 1950s. They were replaced with other technologies, notably core memory, starting in the 1950s.\n\nStorage tubes made a comeback in the 1960s and 1970s for use in computer graphics, most notably the Tektronix 4010 series. Today they are obsolete, their functions provided by low-cost memory devices and liquid crystal displays.\n\nA conventional CRT consists of an electron gun at the back of the tube that is aimed at a thin layer of phosphor at the front of the tube. Depending on the role, the beam of electrons emitted by the gun is steered around the display using magnetic (television) or electrostatic (oscilloscope) means. When the electrons strike the phosphor, the phosphor \"lights up\" at that location for a time, and then fades away. The length of time the spot remains is a function of the phosphor chemistry.\n\nAt very low energies, electrons from the gun will strike the phosphor and nothing will happen. As the energy is increased, it will reach a critical point, formula_1, that will activate the phosphor and cause it to give off light. As the voltage increases beyond the brightness of the spot will increase. This allows the CRT to display images with varying intensity, like a television image.\n\nAbove another effect also starts, secondary emission. When any insulating material is struck by electrons over a certain critical energy, electrons within the material are forced out of it through collisions, increasing the number of free electrons. This effect is used in electron multipliers as found in night vision systems and similar devices. In the case of a CRT this effect is generally undesirable; the new electrons generally fall back to the display and cause the surrounding phosphor to light up, which appears as a lowering of the focus of the image.\n\nThe rate of secondary emission is also a function of the electron beam energy, but follows a different rate curve. As the electron energy is increased, the rate increases until it reaches a critical threshold, when the number of secondary emissions is greater than the number supplied by the gun. In this case the localized image rapidly fades as energy is removed by the secondary electrons.\n\nIn any CRT, images are displayed by striking the screen with electron energies between these two values, and . Below no image is formed, and above any image rapidly fades.\n\nAnother side effect, initially a curiosity, is that electrons will stick to the phosphor in lit up areas. As the light emission fades, these electrons are likewise released back into the tube. The charge is generally far too small to have a visual effect, and was generally ignored in the case of displays.\n\nThese two effects were both utilized in the construction of a storage tube. Storage was accomplished by striking any suitably long-lived phosphor with electrons with energies just above , and erased by striking them with electrons above . There were any number of varieties of mechanical layouts used to improve focus or cause the image to be refreshed either internally to the tube or through off board storage.\n\nThe easiest example to understand are the early computer memory systems as typified by the Williams tube. These consisted of World War II surplus radar display CRTs connected to a computer. The X and Y deflection plates were connected to amplifiers that converted memory locations into X and Y positions on the screen, in most cases such that positions along the X axis represented individual bits within a word, while Y locations were different words.\n\nTo write a value to memory, the address was amplified and sent to the Y deflection plates, such that the beam would be fixed to a horizontal line on the screen. A timer then set the X deflection plate to increasing voltages, causing the beam to be scanned across the selected line. The gun was set to a default energy close to , and the bits from the computer fed to the gun to modulate the voltage up and down such that 0's would be below and 1's above it. By the time the beam reached the other side of the line, a pattern of short dashes was drawn for each 1, while 0's were empty locations.\n\nTo read the values back out, the deflections plates were set to the same values, but the gun energy set to a value above . As the beam scanned the line, the phosphor was pushed well beyond the secondary emission threshold. If the beam was located over a blank area, a certain number of electrons would be released, but if it was over a lit area, the number would be increased by the number of electrons stuck to that area. In the Williams tube these values were read by measuring the capacitance of a metal plate just in front of the display side of the tube. As the reading process also erased any stored values, the signal had to be regenerated through associated circuitry. A CRT with two electron guns, one for reading and one for writing, made this process trivial.\n\nThe earliest computer graphics systems, like those of the TX-2 and DEC PDP-1, required the entire attention of the computer to maintain. A list of vectors stored in main memory was periodically read out to the display to refresh it before the image faded. This generally occurred frequently enough that there was little time to do anything else, and interactive systems like \"Spacewar!\" were tour-de-force programming efforts.\n\nFor practical use, graphical displays were developed that contained their own memory and an associated very simple computer which offloaded the refreshing task from the mainframe. This was not inexpensive; the IBM 2250 graphics terminal used with the IBM S/360 cost $280,000 in 1970.\n\nA storage tube could replace most or all of the localized hardware by storing the vectors directly within the display, instead of an associated local computer. Commands that previously caused the terminal to erase its memory and thus clear the display could be emulated by scanning the entire screen at an energy above . In most systems, this caused the entire screen to quickly \"flash\" before clearing to a blank state. The two main advantages were:\n\n\nGenerally speaking, storage tubes could be divided into two categories. In the more common category, they were only capable of storing \"binary\" images; any given point on the screen was either illuminated or dark. The Tektronix Direct-View Bistable Storage Tube was perhaps the best example in this category. Other storage tubes were able to store greyscale/halftoned images; the tradeoff was usually a much-reduced storage time.\n\nSome pioneering storage tube displays were MIT Project MAC's ARDS (Advanced Remote Display Station), the Computek 400 Series Display terminals (a commercial derivative), which both used a Tektronix type 611 storage display unit, and Tektronix's 4014 terminal, the latter becoming a de facto computer terminal standard some time after its introduction (later being emulated by other systems due to this status).\n\nThe first generalized computer assisted instruction system, PLATO I, c. 1960 on ILLIAC I, used a storage tube as its computer graphics display. PLATO II and PLATO III also used storage tubes as displays.\n\n"}
{"id": "39958236", "url": "https://en.wikipedia.org/wiki?curid=39958236", "title": "Super-spreader", "text": "Super-spreader\n\nA super-spreader is a host—an organism infected with a disease—that infects, disproportionally, more secondary contacts than other hosts who are, also, infected with the same disease. A sick human can be a super-spreader; they would be more likely to infect others than most people with the disease. Super-spreaders are thus of high concern in epidemiology (the study of the spread of diseases).\n\nSome cases of super-spreading conform to the 20/80 rule, where, approximately, 20% of infected individuals are responsible for 80% of transmissions, although super-spreading can still be said to occur when super-spreaders account for a higher or lower percentage of transmissions. In epidemics with super-spreading, the majority of individuals infect relatively few secondary contacts.\n\nSuper-spreading events are shaped by multiple factors including a decline in herd immunity, nosocomial infections, virulence, viral load, misdiagnosis, airflow dynamics, immune suppression, and co-infection with another pathogen.\n\nAlthough loose definitions of super-spreading exist, some effort has been made at defining what qualifies as a super-spreading event (SSE) more explicit. Lloyd-Smith et al. (2005) define a protocol to identify a super-spreading event as follows:\n\nThis protocol defines a 99th-percentile SSE as a case which causes more infections than would occur in 99% of infectious histories in a homogeneous population.\n\nDuring the 2003 SARS outbreak in Beijing, China, epidemiologists defined a super-spreader as an individual with transmission of SARS to at least eight contacts.\n\nSuper-spreaders may or may not show any symptoms of the disease.\n\nSuper-spreaders have been identified who excrete a higher than normal number of pathogens during the time they are infectious. This causes their contacts to be exposed to higher viral/bacterial loads than would be seen in the contacts of non-superspreaders with the same duration of exposure.\n\nThe basic reproduction number R is the average number of secondary infections caused by a typical infective person in a totally susceptible population. The basic reproductive number is found by multiplying the average number of contacts by the average probability that a susceptible individual will become infected, which is called the shedding potential. R = Number of contacts X Shedding potential \n\nThe individual reproductive number represents the number of secondary infections caused by a specific individual during the time that individual is infectious. Some individuals have significantly higher than average individual reproductive numbers and are known as super-spreaders. Through contact tracing, epidemiologists have identified super-spreaders in measles, tuberculosis, rubella, monkeypox, smallpox, Ebola hemorrhagic fever and SARS.\n\nMen with HIV who were co-infected with at least one other sexually transmitted disease, such as gonorrhea, hepatitis C, and herpes simplex 2 virus, were found to have an eight-fold higher HIV shedding rate than men without co-infection. This shedding rate was calculated in men with similar HIV viral loads. Once treatment for the co-infection had been completed, the HIV shedding rate returned to levels comparable to men without co-infection.\n\nHerd immunity, or herd effect, refers to the indirect protection that immunized community members provide to non-immunized members in preventing the spread of contagious disease. The greater the number of immunized individuals, the less likely an outbreak can occur because there are fewer susceptible contacts. In epidemiology, herd immunity is known as a \"dependent happening\" because it influences transmission over time. As a pathogen that confers immunity to the survivors moves through a susceptible population, the number of susceptible contacts declines. Even if susceptible individuals remain, their contacts are likely to be immunized, preventing any further spread of the infection. The proportion of immune individuals in a population above which a disease may no longer persist is the \"herd immunity threshold\". Its value varies with the virulence of the disease, the efficacy of the vaccine, and the contact parameter for the population. That is not to say that an outbreak can't occur, but it will be limited. \n\nThe first cases of SARS occurred in mid-November 2002 in the Guangdong Province of China. This was followed by an outbreak in Hong Kong in February, 2003. A Guangdong Province doctor, Liu Jianlun, who had treated SARS cases there, had contracted the virus and was symptomatic. Despite his symptoms, he traveled to Hong Kong to attend a family wedding. He stayed on the ninth floor of the Metropole Hotel in Kowloon, infecting 16 other hotel guests also staying on that floor (pictured above). The guests then traveled to Canada, Singapore, Taiwan, and Vietnam, spreading SARS to those locations and transmitting what became a global epidemic.\n\nIn another case during this same outbreak, a 54-year-old male was admitted to a hospital with coronary heart disease, chronic renal failure and type two diabetes. He had been in contact with a patient known to have SARS. Shortly after his admission he developed fever, cough, myalgia and sore throat. The admitting physician suspected SARS. The patient was transferred to another hospital for treatment of his coronary artery disease. While there, his SARS symptoms became more pronounced. Later, it was discovered he had transmitted SARS to 33 other patients in just two days. He was transferred back to the original hospital where he died of SARS.\n\nThe SARS pandemic was eventually contained, but not before it caused 8,273 cases and 775 deaths. Within two weeks of the original outbreak in Guangdong Province, SARS had spread to 37 countries.\n\nMeasles is a highly contagious, air-borne virus that reappears even among vaccinated populations. In one Finnish town in 1989, an explosive school-based outbreak resulted in 51 cases, several of whom had been previously vaccinated. One child alone, infected 22 others. It was noted during this outbreak that when vaccinated siblings shared a bedroom with an infected sibling, seven out of nine became infected as well.\n\nTyphoid fever is a human-specific disease caused by the bacterium \"Salmonella typhi\". It is highly contagious and becoming resistant to antibiotics. S. typhi is susceptible to creating asymptomatic carriers. The most famous carriers are Mary Mallon, known as Typhoid Mary, from New York City, and Mr. N. the Milker, from Folkstone, England. Both were active around the same time. Mallon infected 51 people from 1902 to 1909. Mr. N. infected more than 200 people over 14 years from 1901 to 1915. At the request of health officials, Mr. N. gave up working in food service. Mallon refused to give up working in food service and eventually was involuntarily quarantined at Brothers Island in New York, where she stayed until she died in November 1938, aged 69.\n\nIt has been found that \"Salmonella typhi\" persists in infected mice macrophages that have cycled from an inflammatory state to a non-inflammatory state. The bacteria remain and reproduce without causing further symptoms in the mice, and that this explains why carriers are asymptomatic.\n\n\n"}
{"id": "22700552", "url": "https://en.wikipedia.org/wiki?curid=22700552", "title": "Surface Water Improvement and Management Program", "text": "Surface Water Improvement and Management Program\n\nSurface Water Improvement and Management Program (S.W.I.M, sometimes written as SWIM) is a Florida state program to improve Florida's water quality.\n\nSwim was started in 1988 by the Department of Environmental Protection to address Florida's worsening water quality and protect drinking water quality.\n"}
{"id": "58296197", "url": "https://en.wikipedia.org/wiki?curid=58296197", "title": "Surfactant leaching (decontamination)", "text": "Surfactant leaching (decontamination)\n\nSurfactant leaching is a method of water and soil decontamination, e.g., for oil recovery in petroleum industry. It involves mixing of contaminated water or soil with surfactants with the subsequent leaching of emulsified contaminants. In oil recovery, most common surfactant types are ethoxylated alcohols, ethoxylated nonylphenols, sulphates, sulphonates, and biosurfactants.\n"}
{"id": "48657173", "url": "https://en.wikipedia.org/wiki?curid=48657173", "title": "Tagging (stamp)", "text": "Tagging (stamp)\n\nTagging of postage stamps means that the stamps are printed on luminescent paper or with luminescent ink to facilitate automated mail processing. Both fluorescence and phosphorescence are used. The same stamp may have been printed with and without these luminescent features, the two varieties are referred to as \"tagged\" and \"untagged\", respectively.\n\nLetters and postcards fed into an automated mail processing plant are illuminated with ultraviolet light. The reaction of the luminescent features of the stamps on this illumination is used to position the mail items such that the stamps can be cancelled, and that the significant parts of the address such as postcodes may be read and the mail be sorted accordingly.\n\nThe luminescent features of the stamps are generally invisible or barely visible to the human eye in normal illumination. They can, however, be identified under ultraviolet light similar to the way it is done in the postal machinery. In general, fluorescent features can be identified with UV light of a longer wavelength than needed for phosphorescent features (see below).\n\nThe luminescent substance (\"taggant\") can be printed over the whole surface of the stamp, the main design, the margins only, single bands or bars or other patterns, or can be added the paper itself.\n\nThe tagging pattern can also be varied to enable sorting of mail according to the service class.\n\nUpon absorption of light, fluorescent materials emit light upon of a longer wavelength (lower energy) than the absorbed radiation, but cease to do so once immediately, when the illumination is stopped. The tagging of stamps uses substances that absorb ultraviolet light of wavelengths between 300 nm and 450 nm (\"Black light\", UVA, long-wave UV) and emit light in the visible spectrum. Under UV illumination they usually glow a greenish or yellowish colour.\n\nIt must not be confused with the \"whitening\" of paper which is achieved by adding optical brighteners that usually re-emit light in the blue region of the spectrum, making the paper appear whiter by compensating a perceived deficit in reflected colours of these wavelengths.\n\nPhosphorescent materials release the absorbed energy only slowly, so that they exhibit an \"afterglow\". Materials for stamp tagging absorb ultraviolet light of wavelengths between 180 nm and 300 nm (UVC, short-wave UV) and emit light of a greenish or reddish colour depending on the substances used.\n\nFluorescent stamps can be detected with a black light fluorescent tube. Phosphorescent stamps can be detected using a shortwave UV lamp. The effects of both processes can be recorded photographically. Lamps for both ranges of wavelengths as well as combinations of both are available. Care must be taken when using UV lamps, since their light can damage the eyes.\n\nThe first tagged stamps of Canada were issued in 1962 with vertical phosphorescent bands. In 1972, fluorescent \"general tagging\" was introduced, initially as vertical bars, now normally on all four sides of the stamp.\n\nDeutsche Bundespost started issuing stamps on fluorescent \"Lumogen\" paper in 1960 in connection with trials for automated mail processing in the Darmstadt area. Fluorescent paper was generally used for stamps of Deutsche Bundespost and Deutsche Bundespost Berlin from 1961 on. Deutsche Post AG continues to use this technology. Deutsche Post of the GDR did not use luminescent tagging on stamps.\n\nLuminescent tagging has been added to postage stamps of the United Kingdom since the Wilding issues of 1959 in the shape of vertical bands. Stamps of the current Machin series have been printed with one or two such \"phosphor bands\". those for second-class mail bear only one such band, those for first-class mail bear two. The positions of the bands may vary, stamps from booklets may have shortened, notched, or inset bands that do not extend onto neighbouring gutters to avoid the use of the latter instead of stamps for franking. Due to the presence of optical brighteners in many printing papers, phosphorescent materials were chosen for stamp tagging in the UK.\n\nThe US Post Office Department started experiments with fluorescent compounds in the early 1960s. An 8 c air mail stamp issued in 1963 was the first stamp printed for trials with new cancelling machines. The 5 c City Delivery issue of 1963 was the first commemorative issue produced with tagging.\n\nPrecancelled stamps and service-inscribed stamps are not usually tagged because they need not be routed through the cancelling equipment.\n\nSince luminescent ink or luminescent paper are only delivered to specialist printers, tagging also serves as an anti-counterfeiting measure, similar to the practice on banknotes.\n\nWhen Deutsche Post of the GDR expanded automated mail processing in the 1980s, they did not use luminescent tagging, but used sideways illumination to identify the shadows of the stamp perforation in order to position mail items in cancelling and sorting machinery. Red light was used for this purpose, giving a good contrast to ordinary writing ink colours and enabling machine reading of postcodes. Some issues of Postal cards were printed entirely in orange to facilitate the latter process. However, the colours of the imprinted stamps was later changed to those of the usual definitives of the corresponding value, and simulated perforations were added around the stamp design to help locate the stamp position.\n\n\n"}
{"id": "12082550", "url": "https://en.wikipedia.org/wiki?curid=12082550", "title": "Tanfield Group", "text": "Tanfield Group\n\nThe Tanfield Group, formerly Comeleon, has changed its main focus from automotive components and imaging equipment to electric vehicle manufacturing and specialist engineering. As Comeleon, the company made 3D images for mobile phones and other devices, but saw the bottom drop out of its key handset market in 2003. In 2004, Comeleon was absorbed by the Tanfield Group. The company has since expanded to include electric vehicles and aerial work platforms.\n\nThe group is made up of the following companies:\n\n\nOn 28 June 2007, the company acquired Snorkel International as another division of its Aerial Work Platform (AWP) market share. The geographical fit and little overlap in product range complements Tanfields initiative to expand into the global marketplace with a wide variety of products to offer their customers.\n\nLondon Taxis International, which manufactures the iconic London black taxi in Coventry, has signed a development agreement with electric vehicle manufacturer Tanfield to develop an all-electric urban taxi. The all-electric version of the TX4 black cab - to be branded the TX4E - will have a top speed of and a range in excess of on one battery charge.\n\nSmith Electric Vehicles was formed in England in 1920. Originally specialising in trolly busses under the name Northern Coachbuilders Ltd. Over the next 80 years the company developed a plethora of vehicles including milk floats.\n\nTanfield Group Plc acquired the company in October 2004, for £2.2m and 1 million new ordinary shares. Tanfield immediately re-started R&D work to develop new electric delivery vehicles. Including the Newton and Edison .\n\nTanfield announced the formation of Smith Electric Vehicles US Corp (Smith US) in February 2009 and the company opened for business later that year. It has since grown to become America’s leading manufacturer of electric trucks.\n\nIn March 2010 Smith US made a £37m conditional offer, equating to 50p per Tanfield share, plus a \"free\" share in Smith US if it subsequently undergoes an IPO and joins the stock market in the near future.\n\nOn 3 July 2008 it was announced in the press that the London Stock Exchange was launching a probe after a collapse in the share price. The company was criticised by analysts for poor standards of disclosure and weak financial controls. The shares reached a high of 203.5p (valuing the Company at over £700 million) in July 2007 and fell to 5.53 pence (a value of £20 million). In April 2008, its annual results disappointed the City and raised questions about its disclosure standards and the high level of cash burn.\n\n"}
{"id": "41672392", "url": "https://en.wikipedia.org/wiki?curid=41672392", "title": "Tanio oil spill", "text": "Tanio oil spill\n\nThe \"Tanio\" oil spill occurred March 7, 1980 when the \"Tanio,\" an oil tanker of Madagascan origin traveling from Wilhelmshaven to Civitavecchia split in two 60 km off the coast of Brittany, France in rough weather, spilling about 13,500 tons of cargo oil into the English Channel and killing 8 sailors.\nThe stern section of the boat was towed to Le Havre by a British cargo ship despite the strong wind. The bow, however, sunk, leaking 5,000 tons of oil. After the spill, strong winds moved the oil toward the Breton coast.\n\nOil began to wash a shore on the Breton beaches on March 9 and contaminated about 200 km of the coastline, overlapping 45% of the area that have been contaminated by the \"Amoco Cadiz\" spill in 1978. In contrast to the \"Amoco Cadiz\" spill, in which only one quarter of lost oil washed ashore, most of the spilled oil came onshore due to weather conditions.\nBecause tourism is an important industry in Brittany, measures were quickly taken to clean up the spilled oil. This proved difficult because the contamination was mainly heavy fuel oil (n° 6) and therefore, extremely viscous, especially in cold, cloudy weather, which limited the use of tractor drawn vacuum trucks in cleaning up the beaches. Additionally, the large tidal range (9m), severe weather, and the varied coastline prevented the effective use of booms. Ultimately, bulldozers and other heavy-machinery were used to remove oil from beaches.\n\nAfter the initial event, the Prefecture Maritime of the Atlantic organized procedures with the French Navy’s Operation Centre. Distress calls were quickly answered, and a French Navy helicopter rescued 31 sailors.\n\nIn the two departments where the oil was spilled, the Department of Finistère and the Department of Cotes-du-Nord, now known as Côtes-d'Armor implemented Plan Polmar, the French national oil spill response plan, was implemented. Thus, national army personnel played a role in the cleanup. Furthermore, The International Tanker Owners Pollution Federation (ITOPF) advised and monitored the situation for the International Oil Pollution Compensation (IOPC) Fund. All together, damage and cleanup costs were over $50 million.\n\nThe oil spilled from the Tanio was of low toxicity; thus, environmental effects were somewhat mitigated. Nevertheless, about 1,700 dead birds were found, oyster beds were contaminated, and the seaweed harvest was disrupted.\nFurther contamination was mainly due to a speedy and thorough cleanup process. Within two months of the disaster, cleanup had switched removing oil with skimmers and pumps to the use of high-pressure hoses in rocky areas. In addition, cleanup was not restricted to tourist areas and collection areas, but also included efforts to clean removed or isolated areas, as well as locations where a relatively quick natural cleanup could be expected.\n\nResearch for this Wikipedia entry was conducted as a part of a Science of Oil Spills course (EN.530.119.13) offered in the Department of Mechanical Engineering at Johns Hopkins University\n"}
{"id": "27735583", "url": "https://en.wikipedia.org/wiki?curid=27735583", "title": "William Gettle", "text": "William Gettle\n\nWilliam F. Gettle (c. 1887 - December 22, 1941) was an American businessman and millionaire. He gained some notoriety when he was kidnapped in 1934.\n\nGettle was originally from Oklahoma, and moved to Bakersfield, California, in the 1920s, where he worked for J.C. Penny and invested in oil fields. The 1920 United States Census showed he was living in Kern, California. At the time of the kidnapping, Gettle weighed 500 pounds, and he and his handicapped wife Fleeta had four children, all of whom were under 10 years of age. One of their neighbors was comedian Joe E. Brown. According to the \"Los Angeles Times\", he was worth around $3,500,000.\n\nGettle threw a house-warming party on the evening of May 9, 1934, and was drinking with \"a friend named Wolf\" in the pool house when two men entered and escorted them out at gunpoint. Gettle was forced into a waiting automobile, and Wolf was left at the scene, tied up but otherwise unharmed.\n\nWilliam Gettle was held, tied up and gagged, in the bedroom of a rented house at 4256 Rosemont Street in La Crescenta, California. He later told police that his kidnappers dressed in bed sheets when talking to him so as not to reveal to him their identities. He also told his rescuers that he \"'was not mistreated in any way' by his kidnappers.\" A ransom of $60,000 was demanded for Gettle's safe release in a note sent to his lawyer; another letter was sent to Fleta Gettle, asking for an additional $40,000. Two police officers, Chester Burris and H.P. Gearhardt, were credited with breaking the case after installing a dictograph in the house of a bank robbery suspect. Overhearing a conversation about Gettle's kidnapping, they were able to trace his whereabouts to the house on Rosemount. A raid was conducted on the house on the night of May 15, 1934. Gettle was recovered the same day as the victim of another high-profile kidnapping case, six-year-old June Robles.\n\nThree men were arrested in connection with the kidnapping, and were eventually convicted after entering guilty pleas in court. They were James Kirk, Larry Kerrigan and Roy Williams. They were all given prison terms in San Quentin. Two women, Loretta Woody and Mona Gallighen, were also arraigned. They were eventually sent to a reformatory in Arlington, Virginia. Clyde Stoddard, the owner of the sedan found in the garage of the Rosemont house, was detained for questioning.\n\nOn June 26, just days after his return to safety, Gettle received two letters which threatened to blow up his home unless he paid the senders $6,000. The threat was explained in the letters as 'atonement' for Gettle's testimony against Woody and Gallighen. He received another note, threatening another kidnapping, shortly before Christmas that same year.\n\nFourteen months after William's return, Fleeta Gettle died at the age of 34 on July 2, 1935. William F. Gettle died of \"chronic liver trouble\" at age 54 in his home in Beverly Hills on December 22, 1941.\n\n"}
