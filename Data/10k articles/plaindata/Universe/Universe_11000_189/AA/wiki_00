{"id": "16532780", "url": "https://en.wikipedia.org/wiki?curid=16532780", "title": "Accelerated Reduction/Elimination of Toxics", "text": "Accelerated Reduction/Elimination of Toxics\n\nThe Accelerated Reduction/Elimination of Toxics (ARET) program was a Canadian program established in the early 1990s with the goal of using voluntary measures to reduce or eliminate harmful substances.\n\nThe program was started by the New Directions Group (NDG), a group of senior industry representatives and Environmental Non-Government Organizations (ENGOs). After Environment Canada formally supported the initiative in 1991, the ARET Stakeholder committee was formed to participate in the program. The committee consisted of representatives from industry, provincial and federal governments, health and environmental groups, and labour organizations. \n\nThe committee first evaluated a list of over 2000 chemicals, scoring them on the basis of toxicity, persistence, and bioaccumulation capability. By 1994, the evaluation was complete and a resulting list of 117 toxic substances were slated by the committee for voluntary elimination or reduction by the year 2000.\n\n\n\nThe success of the ARET program is disputed. While Environment Canada states that over 70,000 tonnes of toxic substances were prevented from release because of the success of the program, the multi-stakeholder nature of the committee lead to disagreements over which substances would be given priority. Environmental and labour groups withdrew from the committee over the emphasis placed on \"reductions\", rather than \"eliminations\", of these substances by industry representatives. Nils Axel Braathen also claims that knowing to what extent the reductions were actual improvements from the \"business-as-usual\" reduction scenario is very difficult, meaning that the net environmental benefits were questionable.\n\n\n"}
{"id": "53432284", "url": "https://en.wikipedia.org/wiki?curid=53432284", "title": "Agora Energiewende", "text": "Agora Energiewende\n\nAgora Energiewende is a think tank supporting the \"Energiewende\" in Germany. It is funded by and the European Climate Foundation.\n\nIts members debate under the Chatham House Rule and include Klaus Töpfer, Ottmar Edenhofer, and Claude Turmes.\n\nIn addition to studies specific to Germany, Agora have published an analysis on the economics of the proposed UK Hinkley Point C nuclear power plant. Its analyses are quoted as part of the energy debate in other countries, including Denmark.\n"}
{"id": "39592391", "url": "https://en.wikipedia.org/wiki?curid=39592391", "title": "Alkaline water electrolysis", "text": "Alkaline water electrolysis\n\nAlkaline water electrolysis has a long history in the chemical industry. It is a type of electrolyzer that is characterized by having two electrodes operating in a liquid alkaline electrolyte solution of potassium hydroxide (KOH) or sodium hydroxide (NaOH). These electrodes are separated by a diaphragm, separating the product gases and transporting the hydroxide ions (OH) from one electrode to the other. A recent comparison showed that state-of-the-art nickel based water electrolyzers with alkaline electrolytes lead to competitive or even better efficiencies than acidic polymer electrolyte membrane water electrolysis with platinum group metal based electrocatalysts.\n\nElectrolysis requires minerals to be present in solution. Tap, well, and ground water contains various minerals, some of which are alkaline while others are acidic. Water above a pH of 7.0 is considered alkaline; below 7.0 it is acidic. Electrolysis can occur only if the water is acidic or alkaline. The requirement is that there must be ions in the water to conduct electricity for the water electrolysis process to occur.\n\nThe electrodes are typically separated by a thin porous foil (with a thickness between 0.050 to 0.5 mm), commonly referred to as diaphragm or separator. The diaphragm is non-conductive to electrons, thus avoiding electrical shorts between the electrodes while allowing small distances between the electrodes. The ionic conductivity is supplied by the aqueous alkaline solution, which penetrates in the pores of the porous diaphragm. The state-of-the-art diaphragm is Zirfon, a composite material of zirconia and Polysulfone.\nThe diaphragm further avoids the mixing of the produced hydrogen and oxygen at the cathode and anode, respectively. \nTypically, Nickel based metals are used as the electrodes for alkaline water electrolysis. Considering pure metals, Ni is the most active non-noble metal \n. The high price of good noble metal electrocatalysts such as platinum group metals and their dissolution during the oxygen evolution is a drawback. Ni is considered as more stable during the oxygen evolution.\n\nHigh surface area Ni catalysts can be achieved by dealloying of Nickel-Zinc or Nickel-Aluminium alloys in alkaline solution, commonly referred to as Raney Nickel. In cell tests the best performing electrodes thus far reported consisted of plasma vacuum sprayed Ni alloys on Ni meshes \n\nand hot dip galvanized Ni meshes \n. The latter approach might be interesting for large scale industrial manufacturing as it is cheap and easily scalable.\n\nIn comparison to polymer electrolyte water electrolysis, another established technology to conduct low temperature water electrolysis, advantages of alkaline water electrolysis are mainly: (1) Cheaper catalysts compared to the platinum metal group based catalysts used for PEM water electrolysis. (2) Higher durability due to an exchangeable electrolyte and lower dissolution of anodic catalyst. (3) Higher gas purities due to lower gas diffusivity in alkaline electrolyte.\n"}
{"id": "141888", "url": "https://en.wikipedia.org/wiki?curid=141888", "title": "Aluminium oxide", "text": "Aluminium oxide\n\nAluminium oxide (IUPAC name) or aluminum oxide (American English) is a chemical compound of aluminium and oxygen with the chemical formula . It is the most commonly occurring of several aluminium oxides, and specifically identified as aluminium(III) oxide. It is commonly called alumina (regardless of whether the element is spelled aluminum or aluminium), and may also be called aloxide, aloxite, or alundum depending on particular forms or applications. It occurs naturally in its crystalline polymorphic phase α-AlO as the mineral corundum, varieties of which form the precious gemstones ruby and sapphire. AlO is significant in its use to produce aluminium metal, as an abrasive owing to its hardness, and as a refractory material owing to its high melting point.\n\nCorundum is the most common naturally occurring crystalline form of aluminium oxide. Rubies and sapphires are gem-quality forms of corundum, which owe their characteristic colors to trace impurities. Rubies are given their characteristic deep red color and their laser qualities by traces of chromium. Sapphires come in different colors given by various other impurities, such as iron and titanium.\n\nAlO is an electrical insulator but has a relatively high thermal conductivity () for a ceramic material. Aluminium oxide is insoluble in water. In its most commonly occurring crystalline form, called corundum or α-aluminium oxide, its hardness makes it suitable for use as an abrasive and as a component in cutting tools.\n\nAluminium oxide is responsible for the resistance of metallic aluminium to weathering. Metallic aluminium is very reactive with atmospheric oxygen, and a thin passivation layer of aluminium oxide (4 nm thickness) forms on any exposed aluminium surface. This layer protects the metal from further oxidation. The thickness and properties of this oxide layer can be enhanced using a process called anodising. A number of alloys, such as aluminium bronzes, exploit this property by including a proportion of aluminium in the alloy to enhance corrosion resistance. The aluminium oxide generated by anodising is typically amorphous, but discharge assisted oxidation processes such as plasma electrolytic oxidation result in a significant proportion of crystalline aluminium oxide in the coating, enhancing its hardness.\n\nAluminium oxide was taken off the United States Environmental Protection Agency's chemicals lists in 1988. Aluminium oxide is on the EPA's Toxics Release Inventory list if it is a fibrous form.\n\nAluminium oxide is an amphoteric substance, meaning it can react with both acids and bases, such as hydrofluoric acid and sodium hydroxide, acting as an acid with a base and a base with an acid, neutralising the other and producing a salt.\n\nThe most common form of crystalline aluminium oxide is known as corundum, which is the thermodynamically stable form. The oxygen ions form a nearly hexagonal close-packed structure with the aluminium ions filling two-thirds of the octahedral interstices. Each Al center is octahedral. In terms of its crystallography, corundum adopts a trigonal Bravais lattice with a space group of R-3c (number 167 in the International Tables). The primitive cell contains two formula units of aluminium oxide.\n\nAluminium oxide also exists in other phases, including the cubic γ and η phases, the monoclinic θ phase, the hexagonal χ phase, the orthorhombic κ phase and the δ phase that can be tetragonal or orthorhombic. Each has a unique crystal structure and properties. Cubic γ-AlO has important technical applications. The so-called β-AlO proved to be NaAlO.\n\nMolten aluminium oxide near the melting temperature is roughly 2/3 tetrahedral (i.e. 2/3 of the Al are surrounded by 4 oxygen neighbors), and 1/3 5-coordinated, very little (<5%) octahedral Al-O is present. Around 80% of the oxygen atoms are shared among three or more Al-O polyhedra, and the majority of inter-polyhedral connections are corner-sharing, with the remaining 10–20% being edge-sharing. The breakdown of octahedra upon melting is accompanied by a relatively large volume increase (~20%), the density of the liquid close to its melting point is 2.93 g/cm.\n\nAluminium hydroxide minerals are the main component of bauxite, the principal ore of aluminium. A mixture of the minerals comprise bauxite ore, including gibbsite (Al(OH)), boehmite (γ-AlO(OH)), and diaspore (α-AlO(OH)), along with impurities of iron oxides and hydroxides, quartz and clay minerals. Bauxites are found in laterites. Bauxite is purified by the Bayer process:\n\nExcept for SiO, the other components of bauxite do not dissolve in base. Upon filtering the basic mixture, FeO is removed. When the Bayer liquor is cooled, Al(OH) precipitates, leaving the silicates in solution.\n\nThe solid Al(OH) Gibbsite is then calcined (heated to over 1100 °C) to give aluminium oxide:\n\nThe product aluminium oxide tends to be multi-phase, i.e., consisting of several phases of aluminium oxide rather than solely corundum. The production process can therefore be optimized to produce a tailored product. The type of phases present affects, for example, the solubility and pore structure of the aluminium oxide product which, in turn, affects the cost of aluminium production and pollution control.\n\nFor its application as an electrical insulator in integrated circuits, where the conformal growth of a thin film is a prerequisite and the preferred growth mode is atomic layer deposition, AlO films can be prepared by the chemical exchange between trimethylaluminum Al(CH) and HO:\n\nHO in the above reaction can be replaced by ozone (O) as the active oxidant and the following reaction then takes place:\n\nThe AlO films prepared using O show 10–100 times lower leakage current density compared with those prepared by HO.\n\nKnown as alundum (in fused form) or aloxite in the mining, ceramic, and materials science communities, aluminium oxide finds wide use. Annual world production of aluminium oxide in 2015 was approximately 115 million tonnes, over 90% of which is used in the manufacture of aluminium metal. The major uses of speciality aluminium oxides are in refractories, ceramics, polishing and abrasive applications. Large tonnages of aluminium hydroxide, from which alumina is derived, are used in the manufacture of zeolites, coating titania pigments, and as a fire retardant/smoke suppressant.\n\nOver 90% of the aluminium oxide, normally termed Smelter Grade Alumina (SGA), produced is consumed for the production of aluminium, usually by the Hall–Héroult process. The remainder, normally called speciality alumina is used in a wide variety of applications which reflect its inertness, temperature resistance and electrical resistance.\n\nBeing fairly chemically inert and white, aluminium oxide is a favored filler for plastics. Aluminium oxide is a common ingredient in sunscreen and is sometimes also present in cosmetics such as blush, lipstick, and nail polish.\n\nMany formulations of glass have aluminium oxide as an ingredient.\n\nAluminium oxide catalyses a variety of reactions that are useful industrially. In its largest scale application, aluminium oxide is the catalyst in the Claus process for converting hydrogen sulfide waste gases into elemental sulfur in refineries. It is also useful for dehydration of alcohols to alkenes.\n\nAluminium oxide serves as a catalyst support for many industrial catalysts, such as those used in hydrodesulfurization and some Ziegler-Natta polymerizations.\n\nAluminium oxide is widely used to remove water from gas streams.\n\nAluminium oxide is used for its hardness and strength. It is widely used as an abrasive, including as a much less expensive substitute for industrial diamond. Many types of sandpaper use aluminium oxide crystals. In addition, its low heat retention and low specific heat make it widely used in grinding operations, particularly cutoff tools. As the powdery abrasive mineral aloxite, it is a major component, along with silica, of the cue tip \"chalk\" used in billiards. Aluminium oxide powder is used in some CD/DVD polishing and scratch-repair kits. Its polishing qualities are also behind its use in toothpaste.\n\n\nAluminium oxide flakes are used in paint for reflective decorative effects, such as in the automotive or cosmetic industries.\n\nAluminium oxide has been used in a few experimental and commercial fiber materials for high-performance applications (e.g., Fiber FP, Nextel 610, Nextel 720). Alumina nanofibers in particular have become a research field of interest.\n\nSome body armors utilize alumina ceramic plates, usually in combination with aramid or UHMWPE backing to achieve effectiveness against even most rifle threats. Alumina ceramic armor is readily available to most civilians in jurisdictions where it is legal, but is not considered military grade.\n\nAluminium oxide can be grown as a coating on aluminium by anodizing or by plasma electrolytic oxidation (see the \"Properties\" above). Both the hardness and abrasion-resistant characteristics of the coating originate from the high strength of aluminium oxide, yet the porous coating layer produced with conventional direct current anodizing procedures is within a 60-70 Rockwell hardness C range which is comparable only to hardened carbon steel alloys, but considerably inferior to the hardness of natural and synthetic corundum. Instead, with plasma electrolytic oxidation, the coating is porous only on the surface oxide layer while the lower oxide layers are much more compact than with standard DC anodizing procedures and present a higher crystallinity due to the oxide layers being remelted and densified to obtain α-Al2O3 clusters with much higher coating hardness values circa 2000 Vickers hardness.\n\nAlumina is used to manufacture tiles which are attached inside pulverized fuel lines and flue gas ducting on coal fired power stations to protect high wear areas. They are not suitable for areas with high impact forces as these tiles are brittle and susceptible to breakage.\n\nIn lighting, transparent aluminium oxide is used in some sodium vapor lamps. Aluminium oxide is also used in preparation of coating suspensions in compact fluorescent lamps.\n\nIn chemistry laboratories, aluminium oxide is a medium for chromatography, available in basic (pH 9.5), acidic (pH 4.5 when in water) and neutral formulations.\n\nHealth and medical applications include it as a material in hip replacements and birth control pills.\n\nIt is used as a dosimeter for radiation protection and therapy applications for its optically stimulated luminescence properties.\n\nAluminium oxide is an electrical insulator used as a substrate (silicon on sapphire) for integrated circuits but also as a tunnel barrier for the fabrication of superconducting devices such as single electron transistors and superconducting quantum interference devices (SQUIDs).\n\nAluminum oxide being a dielectric with relatively large band gap is used as an insulating barrier in capacitors.\n\nInsulation for high-temperature furnaces is often manufactured from aluminium oxide. Sometimes the insulation has varying percentages of silica depending on the temperature rating of the material. The insulation can be made in blanket, board, brick and loose fiber forms for various application requirements.\n\nSmall pieces of aluminium oxide are often used as boiling chips in chemistry.\n\nIt is also used to make spark plug insulators.\n\nUsing a plasma spray process and mixed with titania, it is coated onto the braking surface of some bicycle rims to provide abrasion and wear resistance.\nMost ceramic eyes on fishing rods are circular rings made from aluminium oxide.\n\n\n"}
{"id": "51647251", "url": "https://en.wikipedia.org/wiki?curid=51647251", "title": "Amazon Wind Farm Texas", "text": "Amazon Wind Farm Texas\n\nAmazon Wind Farm Texas is a 253 Megawatt wind farm in Scurry County, West Texas The farm opened in late 2017, the project includes over 100 turbines which generate and feed into the Texas electric grid 1,000,000 megawatt hours of wind energy annually (1,000 gigawatt hours), enough to power 90,000 typical US homes.\n\nOctober 19, 2017, Amazon's Jeff Bezos christened the Amazon Wind Farm Texas by smashing a champagne bottle on top of one of the turbines standing more than tall. In a statement, Amazon revealed that it has entered into an agreement to purchase 90 percent of the facility's output. The facility was built by Lincoln Clean Energy which also owns and operates the wind farm. This facility is one example of Amazon's large and increasing long-term investment in renewable energy. \n\n"}
{"id": "25155005", "url": "https://en.wikipedia.org/wiki?curid=25155005", "title": "Automatic lubrication system", "text": "Automatic lubrication system\n\nAn automatic lubrication system (ALS), often referred to as a centralized lubrication system, is a system that delivers controlled amounts of lubricant to multiple locations on a machine while the machine is operating. Even though these systems are usually fully automated, a system that requires a manual pump or button activation is still identified as a centralized lubrication system. The system can be classified into two different categories that can share a lot of the same components. \n\nOil systems: Oil systems primary use is for stationary manufacturing equipment such as CNC milling\n\nGrease systems: Grease primary use is on mobile units such as trucks, mining or construction equipment.\n\nOil versus grease can vary even though their primary use is mostly stationary for oil and mobile for grease, some stationary manufacturing equipment will use grease systems.\n\nAutomatic lubrication system is designed to apply lubricant in small, measured amounts over short, frequent time intervals. Time and human resource constraints and sometimes the physical location on machine often makes it impractical to manually lubricate the points. As a result, production cycles, machine availability, and manpower availability dictate the intervals at which machinery is lubricated which is not optimal for the point requiring lubrication. Auto lube systems are installed on machinery to address this problem.\n\nAuto lube systems have many advantages over traditional methods of manual lubrication:\n\nA typical system consists of controller/timer, pump w/reservoir, supply line, metering valves, and feed lines. Regardless of the manufacturer or type of system, all automatic lubrication systems share these 5 main components:\n\nThere are several different types of automatic lubrication systems including:\nThe 4 most commonly used Automatic Lubrication System types are:\n\nA single line progressive system uses lubricant flow to cycle individual metering valves and valve assemblies. The valves consist of dispensing pistons moving back and forth in a specific bore. Each piston depends on flow from the previous piston to shift and displace lubricant. If one piston doesn’t shift, none of the following pistons will shift. Valve output is not adjustable. \n\nOperation begins when the controller/timer sends a signal to the pump to start the lube event. The pump then feeds lubricant into the supply line which connects to the primary metering valve, for either a preprogrammed amount of time or number of times as monitored through a designated piston cycle switch. Lubricant is fed to the multiple lubrication points one after another via secondary progressive metering valves sized for each series of lubrication points, and then directly to each point via the feed lines.\n\nThe first single-line parallel system for industry was introduced in 1937 by Lincoln Engineering (now known as Lincoln Industrial) in the United States.\nA single line parallel system can service a single machine, different zones on a single machine or even several separate machines and is ideal when the volume of lubricant varies for each point. In this type of system, a central pump station automatically delivers lubricant through a single supply line to multiple branches of injectors. Each injector serves a single lubrication point, operates independently and may be individually adjusted to deliver the desired amount of lubricant.\n\nOperation begins when the controller/timer sends a signal to the pump starting the lube cycle. The pump begins pumping lubricant to build up pressure in the supply line connecting the pump to the injectors. Once the required pressure is reached, the lube injectors dispense a predetermined amount of lubricant to the lubrication points via feed lines.\n\nOnce the entire system reaches the required pressure, a pressure switch sends a signal to the controller indicating that grease has cycled through to all the distribution points. The pump shuts off. Pressure is vented out of the system and grease in the line is redirected back to the pump reservoir, until the normal system pressure level is restored.\n\nA dual line parallel system is similar to the single line parallel system in that it uses hydraulic pressure to cycle adjustable valves to dispense measured shots of lubricant. It has 2 main supply lines which are alternatively used as pressure / vent lines. The advantage of a two-line system is that it can handle hundreds of lubrication points from a single pump station over several thousand feet using significantly smaller tubing or pipe. \n\nOperation begins when the controller/timer sends a signal to the pump to start the lubrication cycle. The pump begins pumping lubricant to build up pressure in the first (the pressure) supply line while simultaneously venting the second (vent) return line. Once the required pressure is reached, a predetermined amount of lubricant is dispensed by the metering devices to half of the lubrication points via feed lines. \n\nOnce the pressure switch monitoring main supply line pressure indicates a preset pressure in the line has been reached, the system is hydraulically closed. The controller shuts off the pump and signals a changeover valve to redirect lubricant to the second main supply line.\n\nThe next time the controller activates the system, the second main line now becomes the pressure line while the first line becomes the vent line. The second line is pressurized and the entire process is repeated lubricating the remaining lube points.\n\nMulti point direct lubricator\n\nWhen the controller in the pump or external controller activates the drive motor, a set of cams turns and activates individual injectors or pump elements to dispense a fixed amount of lubricant to each individual lubrication point. Systems are easy to design, direct pump to lube point without added accessories and easy to troubleshoot.\n\n"}
{"id": "39521700", "url": "https://en.wikipedia.org/wiki?curid=39521700", "title": "Biconic cusp", "text": "Biconic cusp\n\nThe biconic cusp was an early method for modeling plasma confinement. They were studied at the Courant Institute by Harold Grad in the late 1950s and early 1960s.\n\nThe magnetic fields in this system were made by electromagnets placed close together. This was a theoretical construct used to model how to contain plasma. The fields were made by two coils of wire facing one another. These electromagnets had poles which faced one another and in the center was a null point in the magnetic field. This was also termed a zero point field. These devices were explored theoretically by Dr. Harold Grad at NYU's Courant Institute in the late 1950s and early 1960s. Because the fields were planar symmetric this plasma system was simple to model.\n\nSimulations of these geometries revealed the existence of three classes of particles. The first class moved back and forth far away from the null point. These particles would be reflected close to the poles of the electromagnets and the plane cusp in the center. This reflection was due to the magnetic mirror effect. These are very stable particles, but their motion changes as they radiate energy over time. This radiation loss arose from acceleration or deceleration by the field and can be calculated using the larmor formula. The second particle moved close to the null point in the center. Because particles passed through locations with no magnetic field, their motions could be straight, with an infinite gyroradius. This straight motion caused the particle to make a more erratic path through the fields. The third class of particles was a transition between these types. Biconic cusps were recently revived because of its similar geometry to the Polywell fusion reactor.\n\nBiconic cusp simulation work\n"}
{"id": "38822765", "url": "https://en.wikipedia.org/wiki?curid=38822765", "title": "BoRit Asbestos", "text": "BoRit Asbestos\n\nThe BoRit Asbestos Area Community Advisory Group (CAG) was established by the U.S. Environmental Protection Agency (EPA) and community members to represent the interests of the communities surrounding the 32 acre BoRit Asbestos Area near the intersection of Whitpain Township, Upper Dublin Township, and Ambler Borough, Pennsylvania, in providing advice and input regarding the BoRit Asbestos Area. The BoRit Asbestos Area is made up of three areas each under different ownership. Area #1 is known as the BoRit site (a.k.a. Kane Core property) and comprises approximately 6 acres. Area #2 is known as the Wissahickon Waterfowl Preserve (a.k.a. the Reservoir site) and comprises approximately 15 acres. Area #3 is known as the Wissahickon Park (a.k.a. Whitpain Park.) acres.\n"}
{"id": "5987", "url": "https://en.wikipedia.org/wiki?curid=5987", "title": "Coal", "text": "Coal\n\nCoal is a combustible black or brownish-black sedimentary rock usually occurring in rock strata in layers or veins called coal beds or coal seams. The harder forms, such as anthracite coal, can be regarded as metamorphic rock because of later exposure to elevated temperature and pressure. Coal is composed primarily of carbon, along with variable quantities of other elements, chiefly hydrogen, sulfur, oxygen, and nitrogen. Coal is a fossil fuel that forms when dead plant matter is converted into peat, which in turn is converted into lignite, then sub-bituminous coal, after that bituminous coal, and lastly anthracite. This involves biological and geological processes. The geological processes take place over millions of years.\n\nChina mines almost half the world's coal followed by India with about a tenth. Australia accounts for about a third of world coal exports followed by Indonesia and Russia; while the largest importer is Japan.\n\nThroughout human history, coal has been used as an energy resource, primarily burned for the production of electricity and heat, and is also used for industrial purposes, such as refining metals. Coal is the largest source of energy for the generation of electricity worldwide, as well as the largest worldwide anthropogenic source of carbon dioxide. The extraction and use of coal makes people ill and damages the environment, including by climate change. Therefore, as part of the worldwide energy transition, many countries have stopped using or use less coal.\n\nThe word originally took the form \"col\" in Old English, from Proto-Germanic *\"kula\"(\"n\"), which in turn is hypothesized to come from the Proto-Indo-European root *\"g\"(\"e\")\"u-lo-\" \"live coal\". Germanic cognates include the Old Frisian \"kole\", Middle Dutch \"cole\", Dutch \"kool\", Old High German \"chol\", German \"Kohle\" and Old Norse \"kol\", and the Irish word \"gual\" is also a cognate via the Indo-European root.\n\nAt various times in the geologic past, the Earth had dense forests in low-lying wetland areas. Due to natural processes such as flooding, these forests were buried underneath soil. As more and more soil deposited over them, they were compressed. The temperature also rose as they sank deeper and deeper. As the process continued the plant matter was protected from biodegradation and oxidation, usually by mud or acidic water. This trapped the carbon in immense peat bogs that were eventually covered and deeply buried by sediments. Under high pressure and high temperature, dead vegetation was slowly converted to coal. As coal contains mainly carbon, the conversion of dead vegetation into coal is called carbonization.\n\nThe wide, shallow seas of the Carboniferous Period provided ideal conditions for coal formation, although coal is known from most geological periods. The exception is the coal gap in the Permian–Triassic extinction event, where coal is rare. Coal is known from Precambrian strata, which predate land plants—this coal is presumed to have originated from residues of algae.\n\nAs geological processes apply pressure to dead biotic material over time, under suitable conditions, its metamorphic grade or rank increases successively into:\n\nCannel coal (sometimes called \"candle coal\") is a variety of fine-grained, high-rank coal with significant hydrogen content. It consists primarily of \"exinite\" macerals, now termed \"liptinite\".\n\nThere are several international standards for coal. The classification of coal is generally based on the content of volatiles. However the most important distinction is between thermal coal (also known as steam coal), which is burnt to generate electricity via steam; and metallurgical coal (also known as coking coal), which is burnt at high temperature to make steel.\n\nHilt's law is a geological observation that (within a small area) the deeper the coal is found, the higher its rank (or grade). It applies if the thermal gradient is entirely vertical; however, metamorphism may cause lateral changes of rank, irrespective of depth.\n\nThe earliest recognized use is from the Shenyang area of China 4000 BC where Neolithic inhabitants had begun carving ornaments from black lignite. Coal from the Fushun mine in northeastern China was used to smelt copper as early as 1000 BC. Marco Polo, the Italian who traveled to China in the 13th century, described coal as \"black stones ... which burn like logs\", and said coal was so plentiful, people could take three hot baths a week. In Europe, the earliest reference to the use of coal as fuel is from the geological treatise \"On stones\" (Lap. 16) by the Greek scientist Theophrastus (c. 371–287 BC):\n\nOutcrop coal was used in Britain during the Bronze Age (3000–2000 BC), where it formed part of funeral pyres. In Roman Britain, with the exception of two modern fields, \"the Romans were exploiting coals in all the major coalfields in England and Wales by the end of the second century AD\". Evidence of trade in coal, dated to about AD 200, has been found at the Roman settlement at Heronbridge, near Chester; and in the Fenlands of East Anglia, where coal from the Midlands was transported via the Car Dyke for use in drying grain. Coal cinders have been found in the hearths of villas and Roman forts, particularly in Northumberland, dated to around AD 400. In the west of England, contemporary writers described the wonder of a permanent brazier of coal on the altar of Minerva at Aquae Sulis (modern day Bath), although in fact easily accessible surface coal from what became the Somerset coalfield was in common use in quite lowly dwellings locally. Evidence of coal's use for iron-working in the city during the Roman period has been found. In Eschweiler, Rhineland, deposits of bituminous coal were used by the Romans for the smelting of iron ore.\n\nNo evidence exists of the product being of great importance in Britain before about AD 1000, the High Middle Ages. Mineral coal came to be referred to as \"seacoal\" in the 13th century; the wharf where the material arrived in London was known as Seacoal Lane, so identified in a charter of King Henry III granted in 1253. Initially, the name was given because much coal was found on the shore, having fallen from the exposed coal seams on cliffs above or washed out of underwater coal outcrops, but by the time of Henry VIII, it was understood to derive from the way it was carried to London by sea. In 1257–1259, coal from Newcastle upon Tyne was shipped to London for the smiths and lime-burners building Westminster Abbey. Seacoal Lane and Newcastle Lane, where coal was unloaded at wharves along the River Fleet, are still in existence. (See Industrial processes below for modern uses of the term.)\n\nThese easily accessible sources had largely become exhausted (or could not meet the growing demand) by the 13th century, when underground extraction by shaft mining or adits was developed. The alternative name was \"pitcoal\", because it came from mines. The development of the Industrial Revolution led to the large-scale use of coal, as the steam engine took over from the water wheel. In 1700, five-sixths of the world's coal was mined in Britain. Britain would have run out of suitable sites for watermills by the 1830s if coal had not been available as a source of energy. In 1947, there were some 750,000 miners in Britain but the last deep coal mine in the UK closed in 2015.\n\nA grade between bituminous coal and anthracite was once known as 'steam coal' as it was widely used as a fuel for steam locomotives. In this specialized use, it is sometimes known as 'sea coal' in the US. Small 'steam coal', also called \"dry small steam nuts\" (or DSSN) was used as a fuel for domestic water heating.\n\nCoal burnt as a solid fuel to produce electricity is called thermal coal. Coal is also used to produce very high temperatures through combustion. Efforts around the world to reduce the use of coal have led some regions to switch to natural gas and electricity from lower carbon sources.\n\nWhen coal is used for electricity generation, it is usually pulverized and then burned in a furnace with a boiler. The furnace heat converts boiler water to steam, which is then used to spin turbines which turn generators and create electricity. The thermodynamic efficiency of this process has been improved over time; some older coal-fired power stations have thermal efficiencies in the vicinity of 25% whereas the newest supercritical and \"ultra-supercritical\" steam cycle turbines, operating at temperatures over 600 °C and pressures over 27 MPa (over 3900 psi), can achieve thermal efficiencies in excess of 45% (LHV basis) using anthracite fuel, or around 43% (LHV basis) even when using lower-grade lignite fuel. Further thermal efficiency improvements are also achievable by improved pre-drying (especially relevant with high-moisture fuel such as lignite or biomass) and cooling technologies.\n\nA few integrated gasification combined cycle (IGCC) power plants have been built, which burn coal more efficiently. Instead of pulverizing the coal and burning it directly as fuel in the steam-generating boiler, the coal is gasified to create syngas, which is burned in a gas turbine to produce electricity (just like natural gas is burned in a turbine). Hot exhaust gases from the turbine are used to raise steam in a heat recovery steam generator which powers a supplemental steam turbine. Thermal efficiencies of current IGCC power plants range from 39% to 42% (HHV basis) or ≈42–45% (LHV basis) for bituminous coal and assuming utilization of mainstream gasification technologies. The overall plant efficiency when used to provide combined heat and power can reach as much as 94%. IGCC power plants emit less local pollution than conventional pulverized coal-fueled plants; however the technology for carbon capture and storage after gasification and before burning has so far proved to be too expensive to use with coal.\n\nOther ways to use coal are as coal-water slurry fuel (CWS), which was developed in the Soviet Union, or in an MHD topping cycle.\n\nIn 2017 38% of the world's electricity came from coal, the same percentage as 30 years previously. In 2018 global installed capacity was 2TW (of which 1TW is in China) which was 30% of total electricity generation capacity.\n\nThe total known deposits recoverable by current technologies, including highly polluting, low-energy content types of coal (i.e., lignite, bituminous), is sufficient for many years. On the other hand, much may have to be left in the ground to avoid climate change, so maximum use could be reached sometime in the 2020s.\n\nCoal-fired generation puts out about twice the amount of carbon dioxide—around a tonne for every megawatt hour generated—than electricity generated by burning natural gas at 500 kg of greenhouse gas per megawatt hour. In addition to generating electricity, natural gas is also popular in some countries for heating and as an automotive fuel.\nThe use of coal in the United Kingdom declined as a result of the development of North Sea oil and the subsequent dash for gas during the 1990s. In Canada some coal power plants, such as the Hearn Generating Station, switched from coal to natural gas. In 2017, coal power in the United States provided 30% of the electricity, down from approximately 49% in 2008, due to plentiful supplies of low cost natural gas obtained by hydraulic fracturing of tight shale formations.\n\nCoke is a solid carbonaceous residue derived from coking coal (a low-ash, low-sulfur bituminous coal, also known as metallurgical coal), which is used in manufacturing steel and other iron products. Coke is made from coking coal by baking in an oven without oxygen at temperatures as high as 1,000 °C, driving off the volatile constituents and fusing together the fixed carbon and residual ash. Metallurgical coke is used as a fuel and as a reducing agent in smelting iron ore in a blast furnace. The carbon monoxide produced by its combustion reduces iron oxide (hematite) in the production of the iron product. \n\nWaste carbon dioxide is also produced (<chem id=\"2Fe2O3 + 3C \">2Fe2O3 + 3C -> 4Fe + 3CO2 </chem>) together with pig iron, which is too rich in dissolved carbon so must be treated further to make steel.\n\nCoking coal should be low in ash, sulfur, and phosphorus, so that these do not migrate to the metal. \nThe coke must be strong enough to resist the weight of overburden in the blast furnace, which is why coking coal is so important in making steel using the conventional route. Coke from coal is grey, hard, and porous and has a heating value of 29.6 MJ/kg. Some cokemaking processes produce byproducts, including coal tar, ammonia, light oils, and coal gas.\n\nPetroleum coke (petcoke) is the solid residue obtained in oil refining, which resembles coke but contains too many impurities to be useful in metallurgical applications.\n\nScrap steel can be recycled in an electric arc furnace and an alternative to making iron by smelting is direct reduced iron, where any carbonaceous fuel can be used to make sponge or pelletised iron. To reduce carbon dioxide emissions in future hydrogen might be used as the reducing agent and biomass or waste as the source of carbon.\n\nCoal gasification, as part of an integrated gasification combined cycle (IGCC) coal-fired power station, is used to produce syngas, a mixture of carbon monoxide (CO) and the hydrogen (H) gas to fire gas turbines to produce electricity. Syngas can also be converted into transportation fuels, such as gasoline and diesel, through the Fischer-Tropsch process; alternatively, syngas can be converted into methanol, which can be blended into fuel directly or converted to gasoline via the methanol to gasoline process. Gasification combined with Fischer-Tropsch technology is used by the Sasol chemical company of South Africa to make motor vehicle fuels from coal and natural gas. Alternatively, the hydrogen obtained from gasification can be used for various purposes, such as powering a hydrogen economy, making ammonia, or upgrading fossil fuels.\n\nDuring gasification, the coal is mixed with oxygen and steam while also being heated and pressurized. During the reaction, oxygen and water molecules oxidize the coal into carbon monoxide (CO), while also releasing hydrogen gas (H). This process has been conducted in both underground coal mines and in the production of town gas which was piped to customers to burn for illumination, heating, and cooking.\n\nIf the refiner wants to produce gasoline, the syngas is collected at this state and routed into a Fischer-Tropsch reaction. This is known as indirect coal liquefaction. If hydrogen is the desired end-product, however, the syngas is fed into the water gas shift reaction, where more hydrogen is liberated.\n\nCoal can be converted directly into synthetic fuels equivalent to gasoline or diesel by hydrogenation or carbonization. Coal liquefaction emits more carbon dioxide than liquid fuel production from crude oil. Mixing in biomass and using CCS would emit slightly less than the oil process but at a high cost. State owned China Energy Investment runs a coal liquefaction plant and plans to build 2 more.\n\nCoal liquefaction may also refer to the cargo hazard when shipping coal.\n\nRefined coal is the product of a coal-upgrading technology that removes moisture and certain pollutants from lower-rank coals such as sub-bituminous and lignite (brown) coals. It is one form of several precombustion treatments and processes for coal that alter coal's characteristics before it is burned. The goals of precombustion coal technologies are to increase efficiency and reduce emissions when the coal is burned. Depending on the situation, precombustion technology can be used in place of or as a supplement to postcombustion technologies to control emissions from coal-fueled boilers.\n\nFinely ground bituminous coal, known in this application as sea coal, is a constituent of foundry sand. While the molten metal is in the mould, the coal burns slowly, releasing reducing gases at pressure, and so preventing the metal from penetrating the pores of the sand. It is also contained in 'mould wash', a paste or liquid with the same function applied to the mould before casting. Sea coal can be mixed with the clay lining (the \"bod\") used for the bottom of a cupola furnace. When heated, the coal decomposes and the bod becomes slightly friable, easing the process of breaking open holes for tapping the molten metal.\n\nChemicals have been produced from coal since the 1950s. Coal is an important feedstock in production of a wide range of chemical fertilizers and other chemical products. The main route to these products is coal gasification to produce syngas. Primary chemicals that are produced directly from the syngas include methanol, hydrogen and carbon monoxide, which are the chemical building blocks from which a whole spectrum of derivative chemicals are manufactured, including olefins, acetic acid, formaldehyde, ammonia, urea and others. The versatility of syngas as a precursor to primary chemicals and high-value derivative products provides the option of using relatively inexpensive coal to produce a wide range of valuable commodities.\n\nBecause the slate of chemical products that can be made via coal gasification can in general also use feedstocks derived from natural gas and petroleum, the chemical industry tends to use whatever feedstocks are most cost-effective. Therefore, interest in using coal tends to increase for higher oil and natural gas prices and during periods of high global economic growth that may strain oil and gas production. Also, production of chemicals from coal is of much higher interest in countries like South Africa, China, India and the United States where there are abundant coal resources. The abundance of coal combined with lack of natural gas resources in China is strong inducement for the coal to chemicals industry pursued there. Similarly, Sasol has built and operated coal-to-chemicals facilities in South Africa.\n\nCoal to chemical processes require substantial quantities of water. Much coal to chemical production is in China where coal dependent provinces such as Shanxi are struggling to control its pollution.\n\nMost coal mined nowadays is opencast.\n\nChina mines almost half the world's coal, followed by India with about a tenth. Australia accounts for about a third of world coal exports, followed by Indonesia and Russia; while the largest importers are Japan and India.\n\nThe price of metcoal (also called coking coal as it is used to make coke to make iron) is volatile and much higher than the price of steam coal (also called thermal coal), which is used to make steam to generate electricity. This is because metcoal must be lower in sulfer and requires more cleaning. Coal futures contracts provide coal producers and the electric power industry an important tool for hedging and risk management.\n\nWhen the price of higher quality thermal coal is high (as in late 2018) or when externalities are properly priced, in some countries new onshore wind and solar generation already costs less than coal power from existing plants. However, for China this is forecast for the early 2020s and for south-east Asia not until the late 2020s.\n\nThe use of coal as fuel causes adverse health impacts and deaths.\n\nThe deadly London smog was caused primarily by the heavy use of coal. In the United States coal-fired power plants were estimated in 2004 to cause nearly 24,000 premature deaths every year, including 2,800 from lung cancer. Annual health costs in Europe from use of coal to generate electricity are €42.8 billion, or $55 billion. Yet the disease and mortality burden of coal use today falls most heavily upon China.\n\nBreathing in coal dust causes coalworker's pneumoconiosis which is known colloquially as \"black lung\", so-called because the coal dust literally turns the lungs black from their usual pink color. In the United States alone, it is estimated that 1,500 former employees of the coal industry die every year from the effects of breathing in coal mine dust.\n\nAround 10% of coal is ash: Coal ash is hazardous and toxic to human beings and other living things. Coal ash contains the radioactive elements uranium and thorium. Coal ash and other solid combustion byproducts are stored locally and escape in various ways that expose those living near coal plants to radiation and environmental toxics.\n\nHuge amounts of coal ash and other waste is produced annually. Use of coal generates hundreds of millions of tons of ash and other waste products every year. These include fly ash, bottom ash, and flue-gas desulfurization sludge, that contain mercury, uranium, thorium, arsenic, and other heavy metals, along with non-metals such as selenium.\n\nThe American Lung Association, the American Nurses' Association, and the Physicians for Social Responsibility released a report in 2009 which details in depth the detrimental impact of the coal industry on human health, including workers in the mines and individuals living in communities near plants burning coal as a power source. This report provides medical information regarding damage to the lungs, heart, and nervous system of Americans caused by the burning of coal as fuel. It details how the air pollution caused by the plume of coal smokestack emissions is a cause of asthma, strokes, reduced intelligence, artery blockages, heart attacks, congestive heart failure,cardiac arrhythmias, mercury poisoning, arterial occlusion, and lung cancer.\n\nMore recently, the Chicago School of Public Health released a largely similar report, echoing many of the same findings.\n\nThough coal burning has increasingly been supplanted by less-toxic natural gas use in recent years, a 2010 study by the Clean Air Task Force still estimated that \"air pollution from coal-fired power plants accounts for more than 13,000 premature deaths, 20,000 heart attacks, and 1.6 million lost workdays in the U.S. each year.\" The total monetary cost of these health impacts is over $100 billion annually.\n\nA 2017 study in the \"Economic Journal\" found that for Britain during the period 1851–1860, \"a one standard deviation increase in coal use raised infant mortality by 6–8% and that industrial coal use explains roughly one-third of the urban mortality penalty observed during this period.\"\n\nCoal mining and coal fueling of power station and industrial processes can cause major environmental damage.\n\nWater systems are affected by coal mining. For example, mining affects groundwater and water table levels and acidity. Spills of fly ash, such as the Kingston Fossil Plant coal fly ash slurry spill, can also contaminate land and waterways, and destroy homes. Power stations that burn coal also consume large quantities of water. This can affect the flows of rivers, and has consequential impacts on other land uses. In areas of water scarcity such as the Thar Desert in Pakistan coal mining and coal power plants would use significant quantities of water.\n\nOne of the earliest known impacts of coal on the water cycle was acid rain. Approximately 75 Tg/S per year of sulfur dioxide (SO) is released from burning coal. After release, the sulfur dioxide is oxidized to gaseous HSO which scatters solar radiation, hence its increase in the atmosphere exerts a cooling effect on climate. This beneficially masks some of the warming caused by increased greenhouse gases. However, the sulfur is precipitated out of the atmosphere as acid rain in a matter of weeks, whereas carbon dioxide remains in the atmosphere for hundreds of years. Release of SO also contributes to the widespread acidification of ecosystems.\n\nDisused coal mines can also cause issues. Subsidence can occur above tunnels, causing damage to infrastructure or cropland. Coal mining can also cause long lasting fires, and it has been estimated that thousands of coal seam fires are burning at any given time. For example, there is a coal seam fire in Germany that has been burning since 1668, and is still burning in the 21st century.\n\nThe production of coke from coal produces ammonia, coal tar, and gaseous compounds as by-products which if discharged to land, air or waterways can act as environmental pollutants. The Whyalla steelworks is one example of a coke producing facility where liquid ammonia is discharged to the marine environment.\n\nThe largest and most long term effect of coal use is the release of carbon dioxide, a greenhouse gas that causes climate change and global warming. Coal is the largest contributor to the human-made increase of CO in the atmosphere.\n\nIn 2016 world gross carbon dioxide emissions from coal usage were 14.5 giga tonnes. For every megawatt-hour generated, coal-fired electric power generation emits around a tonne of carbon dioxide, which is double the approximately 500 kg of carbon dioxide released by a natural gas-fired electric plant. Because of this higher carbon efficiency of natural gas generation, as the market in the United States has changed to reduce coal and increase natural gas generation, carbon dioxide emissions may have fallen. Those measured in the first quarter of 2012 were the lowest of any recorded for the first quarter of any year since 1992. In 2013, the head of the UN climate agency advised that most of the world's coal reserves should be left in the ground to avoid catastrophic global warming.\n\n\"Clean\" coal technology usually addresses atmospheric problems resulting from burning coal. Historically, the primary focus was on SO and NO, the most important gases which caused acid rain, and particulates which cause visible air pollution and deleterious effects on human health. SO can be removed by flue-gas desulfurization and NO by selective catalytic reduction (SCR). Particulates can be removed with electrostatic precipitators. Although perhaps less efficient wet scrubbers can remove both gases and particulates. And mercury emissions can be reduced up to 95%. However capturing carbon dioxide emissions is generally not economically viable.\n\nLocal pollution standards include GB13223-2011 (China), India, the Industrial Emissions Directive (EU) and the Clean Air Act (United States).\n\nSatellite monitoring is now used to crosscheck national data, for example that Chinese control of SO has only been partially successful. It has also revealed that low use of technology such as SCR has resulted in high NO emissions in South Africa and India.\n\nA few Integrated gasification combined cycle (IGCC) coal-fired power plants have been built with coal gasification. Although they burn coal more efficiently and therefore emit less pollution, the technology has not generally proved economically viable for coal, except possibly in Japan although this is controversial.\n\nAlthough still being intensively researched and considered economically viable for some uses other than with coal; carbon capture and storage is being tested at the Petra Nova and Boundary Dam coal-fired power plants and so far has been found to be technically feasible but not economically viable for use with coal, due to reductions in the cost of solar PV technology.\n\nOpposition to coal pollution was one of the main reasons the modern environmental movement started in the 19th century.\n\nIn order to meet global climate goals and provide power to those that don't currently have it coal power must be reduced from nearly 10,000TWh to less than 2,000TWh by 2040. Many countries, such as the Powering Past Coal Alliance, have now transitioned away from coal some using the ideas of a \"just transition\", for example to use some of the benefits of transition to provide early pensions for coal miners.\n\nSome coal miners are concerned their jobs may be lost in the transition.\n\nThe white rot fungus \"Trametes versicolor\" can grow on and metabolize naturally occurring coal. The bacteria Diplococcus has been found to degrade coal, raising its temperature.\n\nIn the long term coal and oil could cost the world trillions of dollars. Coal alone may cost Australia billions, whereas costs to some smaller companies or cities could be on the scale of millions of dollars. The economies most damaged by coal (via climate change) may be India and the US as they are the countries with the highest social cost of carbon.\n\nChina is the largest producer of coal in the world. It is the world's largest energy consumer, and coal in China supplies 60% of its primary energy. However two fifths of China's coal power stations are estimated to be loss-making.\n\nAir pollution from coal storage and handling costs the USA almost 200 dollars for every extra ton stored, due to PM2.5. Coal pollution costs the EU €43 billion each year. Measures to cut air pollution have beneficial long-term economic impacts for individuals and countries.\n\nBroadly defined total subsidies for coal in 2015 have been estimated at around 2.5 trillion USD, about 3% of global GDP. Government funding for new coal power plants is being supplied via Exim Bank of China, the Japan Bank for International Cooperation and Indian public sector banks. Coal in Kazakhstan was the main recipient of coal consumption subsidies totalling 2 billion USD in 2017. Coal in Turkey benefited from substantial subsidies.\n\nSome coal-fired power stations could become stranded assets, for example China Energy Investment, the world's largest power company, risks losing half its capital. However state owned electricity utilities such as Eskom in South Africa, Perusahaan Listrik Negara in Indonesia, Sarawak Energy in Malaysia, Taipower in Taiwan, EGAT in Thailand, Vietnam Electricity and EÜAŞ in Turkey are building or planning new plants.\n\nCountries building or financing new coal-fired power stations, such as Japan, face mounting international criticism for obstructing the aims of the Paris Agreement.\n\nAllegations of corruption are being investigated in India and China.\n\nThe energy density of coal, i.e. its heating value, is roughly 24 megajoules per kilogram (approximately 6.7 kilowatt-hours per kg). For a coal power plant with a 40% efficiency, it takes an estimated of coal to power a 100 W lightbulb for one year.\n\nAs of 2006, the average efficiency of electricity-generating power stations was 31%; in 2002, coal represented about 23% of total global energy supply, an equivalent of 3.4 billion tonnes of coal, of which 2.8 billion tonnes were used for electricity generation.\n\nThousands of coal fires are burning around the world. Those burning underground can be difficult to locate and many cannot be extinguished. Fires can cause the ground above to subside, their combustion gases are dangerous to life, and breaking out to the surface can initiate surface wildfires. Coal seams can be set on fire by spontaneous combustion or contact with a mine fire or surface fire. Lightning strikes are an important source of ignition. The coal continues to burn slowly back into the seam until oxygen (air) can no longer reach the flame front. A grass fire in a coal area can set dozens of coal seams on fire. Coal fires in China burn an estimated 120 million tons of coal a year, emitting 360 million metric tons of CO, amounting to 2–3% of the annual worldwide production of CO from fossil fuels. In Centralia, Pennsylvania (a borough located in the Coal Region of the United States), an exposed vein of anthracite ignited in 1962 due to a trash fire in the borough landfill, located in an abandoned anthracite strip mine pit. Attempts to extinguish the fire were unsuccessful, and it continues to burn underground to this day. The Australian Burning Mountain was originally believed to be a volcano, but the smoke and ash come from a coal fire that has been burning for some 6,000 years.\n\nAt Kuh i Malik in Yagnob Valley, Tajikistan, coal deposits have been burning for thousands of years, creating vast underground labyrinths full of unique minerals, some of them very beautiful.\n\nThe reddish siltstone rock that caps many ridges and buttes in the Powder River Basin in Wyoming and in western North Dakota is called \"porcelanite\", which resembles the coal burning waste \"clinker\" or volcanic \"scoria\". Clinker is rock that has been fused by the natural burning of coal. In the Powder River Basin approximately 27 to 54 billion tons of coal burned within the past three million years. Wild coal fires in the area were reported by the Lewis and Clark Expedition as well as explorers and settlers in the area.\n\nOf the countries which produce coal China mines by far the most, almost half the world's coal, followed by less than 10% by India. China is also by far the largest consumer. Therefore, market trends depend on Chinese energy policy. Although the effort to reduce pollution means that the global long term trend is to burn less coal, the short and medium term trends may differ, in part due to Chinese financing of new coal-fired power plants in other countries.\n\nAlthough many countries have coal underground not all will be consumed.\n\nOf the three fossil fuels, coal has the most widely distributed reserves; coal is mined in over 100 countries, and on all continents except Antarctica. The largest reserves are found in the United States, Russia, China, Australia and India:\n\nNowadays \"peak coal\" means the point in time when consumption of coal reaches a maximum. As of 2018 global peak coal consumption is predicted to occur by the early 2020s at the latest.\n\nThe reserve life is an estimate based only on current production levels and proved reserves level for the countries shown, and makes no assumptions of future production or even current production trends. Countries with annual production higher than 250 million tonnes are shown. For comparison, data for the European Union is also shown. Shares are based on data expressed in tonnes oil equivalent.\nCountries with annual consumption higher than 100 million tonnes are shown. Shares are based on data expressed in tonnes oil equivalent.\n\nExporters are at risk of a reduction in import demand from India and China.\n\nCountries with annual gross import higher than 40 million tonnes are shown. In terms of net import the largest importers are still Japan (206.0 millions tonnes), China (172.4) and South Korea (125.8).\n\nCoal is the official state mineral of Kentucky. and the official state rock of Utah; both U.S. states have a historic link to coal mining.\n\nSome cultures hold that children who misbehave will receive only a lump of coal from Santa Claus for Christmas in their christmas stockings instead of presents.\n\nIt is also customary and considered lucky in Scotland and the North of England to give coal as a gift on New Year's Day. This occurs as part of First-Footing and represents warmth for the year to come.\n\n\n\n"}
{"id": "2949404", "url": "https://en.wikipedia.org/wiki?curid=2949404", "title": "Cocamidopropyl betaine", "text": "Cocamidopropyl betaine\n\nCocamidopropyl betaine (CAPB) is a mixture of closely related organic compounds derived from coconut oil and dimethylaminopropylamine. CAPB is available as a viscous pale yellow solution and it is used as a surfactant in personal care products. The name reflects that the major part of the molecule, the lauric acid group, is derived from coconut oil. Cocamidopropyl betaine to a significant degree has replaced cocamide DEA.\n\nDespite the name cocamidopropyl betaine is not synthesized from betaine. Instead it is produced in a two step manner, beginning with the reaction of dimethylaminopropylamine (DMAPA) with fatty acids from coconut oil (mainly lauric acid or its methyl ester). The primary amine in DMAPA is more reactive than the tertiary amine, leading to its selective addition to form an amide. In the second step chloroacetic acid reacts with the remaining tertiary amine to form quaternary a ammonium center (a quaternization reaction).\n\nCAPB is a fatty acid amide containing a long hydrocarbon chain at one end and a polar group at the other. This allows CAPB to act as a surfactant and as a detergent. It is a zwitterion, consisting of both a quaternary ammonium cation and a carboxylate.\n\nCocamidopropyl betaine is used as a foam booster in shampoos. It is a medium strength surfactant also used in bath products like hand soaps. It is also used in cosmetics as an emulsifying agent and thickener, and to reduce irritation purely ionic surfactants would cause. It also serves as an antistatic agent in hair conditioners, which most often does not irritate skin or mucous membranes. However, some studies indicate it is an allergen. It also has antiseptic properties, making it suitable for personal sanitary products. It is compatible with other cationic, anionic, and nonionic surfactants.\n\nCAPB is obtained as an aqueous solution in concentrations of about 30%.\n\nTypical impurities of leading manufacturers today:\n\nThe impurities AA and DMAPA are most critical, as they have been shown to be responsible for skin sensitation reactions. These by-products can be avoided by a moderate excess chloroacetate and the exact adjustment of pH value during betainization reaction accompanied by regular analytical control.\n\nCAPB has been claimed to cause allergic reactions in some users, but a controlled pilot study has found that these cases may represent irritant reactions rather than true allergic reactions. Furthermore, results of human studies have shown that CAPB has a low sensitizing potential if impurities with amidoamine (AA) and dimethylaminopropylamine (DMAPA) are low and tightly controlled. Other studies have concluded that most apparent allergic reactions to CAPB are more likely due to amidoamine. Cocamidopropyl betaine was voted 2004 Allergen of the Year by the American Contact Dermatitis Society.\n\n"}
{"id": "2029677", "url": "https://en.wikipedia.org/wiki?curid=2029677", "title": "Critical exponent", "text": "Critical exponent\n\nCritical exponents describe the behavior of physical quantities near continuous phase transitions. It is believed, though not proven, that they are universal, i.e. they do not depend on the details of the physical system, but only on\n\nThese properties of critical exponents are supported by experimental data. Analytical results can be theoretically achieved in mean field theory for higher-dimensional systems (4 or more dimensions). The theoretical treatment of lower-dimensional systems (1 or 2 dimensions) is more difficult and requires the renormalization group approach.\nPhase transitions and critical exponents appear also in percolation systems.\nHowever, here the critical dimension above which mean field exponents are valid is 6 and higher dimensions.\nMean field critical exponents are also valid for random graphs, such as Erdős–Rényi graphs, which can be regarded as infinite dimensional systems.\n\nPhase transitions occur at a certain temperature, called the critical temperature . We want to describe the behavior of a physical quantity in terms of a power law around the critical temperature; we introduce the reduced temperature\n\nwhich is zero at the phase transition, and define the critical exponent formula_2:\n\nThis results in the power law we were looking for:\n\nIt is important to remember that this represents the asymptotic behavior of the function as .\n\nMore generally one might expect\n\nBelow the system has two different phases characterized by an order parameter , which vanishes at and above .\n\nLet us consider the disordered phase (), ordered phase () and critical temperature () phases separately. Following the standard convention, the critical exponents related to the ordered phase are primed. It is also another standard convention to use superscript/subscript + (−) for the disordered (ordered) state. We have spontaneous symmetry breaking in the ordered phase. So, we will arbitrarily take any solution in the phase.\nThe following entries are evaluated at (except for the entry)\n\nThe critical exponents can be derived from the specific free energy as a function of the source and temperature. The correlation length can be derived from the functional .\n\nThese relations are accurate close to the critical point in two- and three-dimensional systems. In four dimensions, however, the power laws are modified by logarithmic factors. This problem does not appear in 3.99 dimensions, though.\n\nThe classical Landau theory (also known as mean field theory) values of the critical exponents for a scalar field (of which the Ising model is the prototypical example) are given by\n\nIf we add derivative terms turning it into a mean field Ginzburg–Landau theory, we get\n\nOne of the major discoveries in the study of critical phenomena is that mean field theory of critical points is only correct when the space dimension of the system is four or higher (which unfortunately excludes many of the experimentally relevant cases). This dimension is called the upper critical dimension. The problem with mean field theory is that the critical exponents do not depend on the space dimension. This leads to a quantitative discrepancy in space dimensions 2 and 3, where the true critical exponents differ from the mean field values. It leads to a qualitative discrepancy in space dimension 1, where a critical point in fact no longer exists, even though mean field theory still predicts there is one. The space dimension where mean field theory becomes qualitatively incorrect is called the lower critical dimension.\n\nThe most accurately measured value of is −0.0127(3) for the phase transition of superfluid helium (the so-called lambda transition). The value was measured on a space shuttle to minimize pressure differences in the sample. This value is in a significant disagreement with the most precise theoretical determination by a combination of Monte Carlo and high temperature expansion techniques. Other techniques give results in agreement in the experiment but are less precise.\n\nIn light of the critical scalings, we can reexpress all thermodynamic quantities in terms of dimensionless quantities. Close enough to the critical point, everything can be reexpressed in terms of certain ratios of the powers of the reduced quantities. These are the scaling functions.\n\nThe origin of scaling functions can be seen from the renormalization group. The critical point is an infrared fixed point. In a sufficiently small neighborhood of the critical point, we may linearize the action of the renormalization group. This basically means that rescaling the system by a factor of will be equivalent to rescaling operators and source fields by a factor of for some . So, we may reparameterize all quantities in terms of rescaled scale independent quantities.\n\nIt was believed for a long time that the critical exponents were the same above and below the critical temperature, e.g. or . It has now been shown that this is not necessarily true: When a continuous symmetry is explicitly broken down to a discrete symmetry by irrelevant (in the renormalization group sense) anisotropies, then the exponents and are not identical.\n\nCritical exponents are denoted by Greek letters. They fall into universality classes and obey the scaling relations\n\nThese equations imply that there are only two independent exponents, e.g., and . All this follows from the theory of the renormalization group.\n\nThere are some anisotropic systems where the correlation length is direction dependent. For percolation see Dayan et al.\n\nDirected percolation can be also regarded as anisotropic percolation. In this case the critical exponents are different and the upper critical dimension is 5.\n\nMore complex behavior may occur at multicritical points, at the border or on intersections of critical manifolds. For a simple model with multicritical points see reference.\n\nThe above examples exclusively refer to the static properties of a critical system. However dynamic properties of the system may become critical, too. Especially, the characteristic time, , of a system diverges as , with a \"dynamical exponent\" . Moreover, the large \"static universality classes\" of equivalent models with identical static critical exponents decompose into smaller \"dynamical universality classes\", if one demands that also the dynamical exponents are identical. For critical exponents for dynamics in percolation systems see reference.\n\nThe critical exponents can be computed from conformal field theory.\n\nSee also anomalous scaling dimension.\n\nCritical exponents also exist for transport quantities like viscosity and heat conductivity. A recent study suggests that critical exponents of percolation play an important role in city traffic.\n\nCritical exponents also exist for self organized criticality for dissipative systems.\n\nPhase transitions and critical exponents appear also in percolation processes where the concentration of occupied sites or links play the role of temperature. See percolation critical exponents. For percolation the critical exponents are different from Ising. For example, in the mean field formula_9 for percolation compared to formula_10 for Ising.\n\n\n"}
{"id": "27689481", "url": "https://en.wikipedia.org/wiki?curid=27689481", "title": "Dielectric gas", "text": "Dielectric gas\n\nA dielectric gas, or insulating gas, is a dielectric material in gaseous state. Its main purpose is to prevent or rapidly quench electric discharges. Dielectric gases are used as electrical insulators in high voltage applications, e.g. transformers, circuit breakers (namely sulfur hexafluoride circuit breakers), switchgear (namely high voltage switchgear), radar waveguides, etc.\n\nA good dielectric gas should have high dielectric strength, high thermal stability and chemical inertness against the construction materials used, non-flammability and low toxicity, low boiling point, good heat transfer properties, and low cost.\n\nThe most common dielectric gas is air, due to its ubiquity and low cost. Another commonly used gas is a dry nitrogen.\n\nIn special cases, e.g., high voltage switches, gases with good dielectric properties and very high breakdown voltages are needed. Highly electronegative elements, e.g., halogens, are favored as they rapidly recombine with the ions present in the discharge channel. The halogen gases are highly corrosive. Other compounds, which dissociate only in the discharge pathway, are therefore preferred; sulfur hexafluoride, organofluorides (especially perfluorocarbons) and chlorofluorocarbons are the most common.\n\nThe breakdown voltage of gases is roughly proportional to their density. Breakdown voltages also increase with the gas pressure. Many gases have limited upper pressure due to their liquefaction.\n\nThe decomposition products of halogenated compounds are highly corrosive, hence the occurrence of corona discharge should be prevented.\n\nBuild-up of moisture can degrade dielectric properties of the gas. Moisture analysis is used for early detection of this.\n\nDielectric gases can also serve as coolants.\n\nVacuum is an alternative for gas in some applications.\n\nMixtures of gases can be used where appropriate. Addition of sulfur hexafluoride can dramatically improve the dielectric properties of poorer insulators, e.g. helium or nitrogen. Multicomponent gas mixtures can offer superior dielectric properties; the optimum mixtures combine the electron attaching gases (sulfur hexafluoride, octafluorocyclobutane) with molecules capable of thermalizing (slowing) accelerated electrons (e.g. tetrafluoromethane, fluoroform. The insulator properties of the gas are controlled by the combination of electron attachment, electron scattering, and electron ionization.\n\nAtmospheric pressure significantly influences the insulation properties of air. High-voltage applications, e.g. xenon flash lamps, can experience electrical breakdowns at high altitudes.\n\" the density is approximate; it is normally specified at atmospheric pressure, the temperature may vary, though it is mostly 0 °C.\"\n"}
{"id": "25187803", "url": "https://en.wikipedia.org/wiki?curid=25187803", "title": "Distortion (mathematics)", "text": "Distortion (mathematics)\n\nIn mathematics, the distortion is a measure of the amount by which a function from the Euclidean plane to itself distorts circles to ellipses. If the distortion of a function is equal to one, then it is conformal; if the distortion is bounded and the function is a homeomorphism, then it is quasiconformal. The distortion of a function ƒ of the plane is given by\n\nwhich is the limiting eccentricity of the ellipse produced by applying ƒ to small circles centered at \"z\". This geometrical definition is often very difficult to work with, and the necessary analytical features can be extrapolated to the following definition. A mapping \"ƒ\" : Ω → R from an open domain in the plane to the plane has finite distortion at a point \"x\" ∈ Ω if \"ƒ\" is in the Sobolev space W(Ω, R), the Jacobian determinant J(\"x\",ƒ) is locally integrable and does not change sign in Ω, and there is a measurable function \"K\"(\"x\") ≥ 1 such that\n\nalmost everywhere. Here \"Df\" is the weak derivative of ƒ, and |\"Df\"| is the Hilbert–Schmidt norm.\n\nFor functions on a higher-dimensional Euclidean space R, there are more measures of distortion because there are more than two principal axes of a symmetric tensor. The pointwise information is contained in the \"distortion tensor\"\n\nThe outer distortion \"K\" and inner distortion \"K\" are defined via the Rayleigh quotients\n\nThe outer distortion can also be characterized by means of an inequality similar to that given in the two-dimensional case. If Ω is an open set in R, then a function has finite distortion if its Jacobian is locally integrable and does not change sign, and there is a measurable function \"K\" (the outer distortion) such that\n\nalmost everywhere.\n\n\n"}
{"id": "36524497", "url": "https://en.wikipedia.org/wiki?curid=36524497", "title": "ElectraNet", "text": "ElectraNet\n\nElectraNet Pty Ltd, trading as ElectraNet, is an electricity transmission company in South Australia. It operates 5,591 km of high-voltage electricity transmission lines in South Australia.\n\nElectraNet is owned by State Grid Corporation of China (46.56%), Malaysian-based YTL Power International Berhad (33.5%), and Hastings Utilities Trust (19.94%). \n"}
{"id": "423207", "url": "https://en.wikipedia.org/wiki?curid=423207", "title": "Electron spiral toroid", "text": "Electron spiral toroid\n\nElectron Power Systems, Inc. of Acton, Massachusetts, United States, claims to have developed a technology for maintaining small stable plasma toroids called electron spiral toroids (ESTs) which remain stable in Earth's atmosphere without the use of any special magnetic fields. They claim to have created these toroids in the laboratory, and to have developed a mathematical model for them that is similar to some explanations for ball lightning. An EST may be a special case of a spheromak.\n\nBecause of EST's claimed lack of need for an external stabilizing magnetic field, EPS hope to be able to create small efficient fusion reactors by colliding magnetically accelerated ESTs together at speeds high enough to induce ballistic nuclear fusion.\n\nTheir model for their reported toroidal phenomena was extensively criticised by a December 2000 technical report commissioned by NASA. , EPS claim to have met the criticisms of the NASA report, and to have demonstrated that their model is mathematically sound, and state that they are ready to proceed with development of their technology.\n\n\n"}
{"id": "23295232", "url": "https://en.wikipedia.org/wiki?curid=23295232", "title": "Energy in Angola", "text": "Energy in Angola\n\nEnergy in Angola describes energy and electricity production, consumption and export from Angola. The energy policy of Angola reflects energy policy and the politics of Angola.\n\nBiomass accounts for 58% of the country's energy consumption; oil accounts for 35%, gas 4% and hydroelectric power 3%.\n\nPrimary energy use in 2009 in Angola was 138 TWh and 7 TWh per million persons.\n\nAngolans suffer frequent daily blackouts. In 2012, days before the election, the government announced $17B US in planned energy investment, designed to alleviate the paucity of available energy.\n\nAngolan population has increased 19.4 percent in the five years 2004-2009.\n\nAngola has extensive hydroelectric power resources that far exceed its present needs. The Capanda Dam, on the Cuanza River, provides Luanda's industries with cheap power. Two dams on the Catumbela River produce power for the Lobito and Benguela areas (Lomaum Dam). Matala Dam 180 km of Lubango provides power to Lubango and Moçâmedes. The Ruacaná Falls Dam, near the Namibian border, was completed in the late 1970s, but the power station is in Namibia. A 520 MW hydroelectric station on the Cuanza River at Kapanda was tentatively scheduled to have begun production in early 2003. As of late 2002, only three of the country's six dams (Cambambe, Biopo, and Matala) were operational; US$200 million has been allocated to repair the remaining dams, which suffered major damage in the civil war. In 2002, electricity generation was 1.728 TW·h, of which 34.5% came from fossil fuels and 65.5% from hydropower. In the same year, consumption of electricity totaled 1.607 TW·h. Total capacity in 2002 was 700 MW. Electricity is produced by Empresa Nacional de Electricidade de Angola.\n\nAngola ranks second in crude oil production in sub-Saharan Africa. Oil has been Angola’s chief export since 1973; it also accounts for half the gross domestic product and is the leading source of government revenue, accounting for $2.9 billion in exports in 1994, or 95% of the total. At the end of 2004, Angola had proven oil reserves of 8.8 billion barrels (1.40 km). Oil reserves are along the Atlantic coast, mostly off the shore of Cabinda Province and the northern border area between and Soyo. In 1999, several oil companies were engaged in oil production, of which the largest was a subsidiary of Chevron, Cabinda Gulf Oil Company. This company has a 49/51% participation agreement with Sonangol, the state's oil company. Other firms included Fina Petróleos de Angola (a Belgian subsidiary), Elf Aquitaine, and Texaco. In 2004, crude oil production averaged . ExxonMobil subsidiary Esso began development of a section of the Xikomba offshore oilfield in August 2002. Production there ceased in August 2011.\n\nDevelopment has been planned but much delayed, of a new refinery in the city of Lobito, on the coast. The Angolan state-owned oil company Sonangol would have a 70 percent stake in the Sonaref refinery at Lobito, its then-head Carlos Saturnino said in 2006, and the Chinese oil company Sinopec would retain the remainder.\n\nAngola's economy was profoundly affected by the sharp drop in oil prices in 2014. This is even though new skyscrapers, appeared in Luanda; offices, shopping centres and apartment buildings proliferated in a \"mini-golden age\" as leading economist Alves da Rocha called it, from 2003-2008. Yet \"probably three quarters\" of the population of Luanda live in \"tumbledown slums\". Two thirds of the 16.5 million people in Angola live on less than $2 a day, according to the World Bank, and the oil industry employs less than one percent of the workforce.\n\nForeigners, including Chinese construction companies and several hundred thousand Chinese workers, and as many or more Portuguese and Brazilian trade and finance consultants and managers. Oil clompanies set up shop in Angola.\n\nGross natural gas production totaled in 2002. Total natural gas reserves were estimated at 9,7 trillion cubic feet as of 2015. Domestic demand for refined petroleum products is expected to increase as the economy gradually rebuilds following the end of the civil war, and the environmental impact would be positive, according to project advocates, since the gas is a byproduct of crude oil extraction that would otherwise be flared. As of 2002, Sonangol and Chevron Texaco had joined forces in a $2 billion project to develop liquefied natural gas from natural gas in Angola's off shore fields. Production was slated to begin in 2007 but Angola LNG made its first shipment in June 2013. A system failure brought a design flaw to light in 2014, and production was expected to resume in 2015.\n\nAngola fined Chevron Texaco $2m for causing environmental damage in 2002 to fisheries caused by obsolete tubes at the Cabinda oilfield. Chevron promised to spend $108 m replacing the pipes. The company pumps almost three-quarters of Angola's oil, and also reduced crude production about 12%, after a pipeline leak.\n"}
{"id": "30601152", "url": "https://en.wikipedia.org/wiki?curid=30601152", "title": "Energy in Common", "text": "Energy in Common\n\nEnergy in Common (EIC) was a not-for-profit organization issuing microloans specifically and only for renewable energy technologies. EIC was founded by Hugh Whalan and Scott Tudman in 2009 (website launch 2010). It has the very ambitious goal of delivering renewable energy to 15 million people in the next five years, while fighting poverty by empowering developing world entrepreneurs through microloans. EIC represents one of the most promising contenders in the growing green microfinance sector. As of 2012, it has ceased operations due to a lack of funds after their overseas partners defaulted on their loan obligations. \n\nEIC operates very similarly to Kiva. In the case of Kiva, lenders provide funds with zero return expected to developing world entrepreneurs to invest in their businesses (typical loans are for things like seeds for farming). EIC does this as well, but specifically focuses on purchasing renewable energy systems like solar photovoltaic panels.\n\nWhat makes EIC particularly unique in the microfinance non-profit sector is that they have created a model to measure the greenhouse gas emission reductions that are created by their loans. The EIC model helps provide funding to developing world entrepreneurs for energy solutions and also helps to channel additional funding into mitigating climate change.\n\n\nThe following list of primarily renewable energy technologies:\n\nBiodigesters convert organic wastes such as food waste, human waste and animal waste into nutrient rich liquid fertilizer and biogas (thus a renewable fuel made up primarily of methane). A biodigester is made up of a bag or tank that holds the organic 'wastes' over a period in which bacteria breaks down the organic matter and produces biogas. Biogas is used as a replacement for kerosene, firewood, or any other combustible fuel source used in the developing world such as cow dung. Biodegesters thus produce renewable energy, cut down on odors and pathogens in organic materials, reduce surface and groundwater contamination, improve indoor air quality and provide a source of high quality organic fertilizer as a byproduct—that has been shown to improve crop yields and decrease fertilizer costs.\n\nClean burning stoves improve the fuel utilization over common stoves. There are several types but the ones funded often burn propane instead of less efficient fuels such as wood, charcoal, or dung. This is somewhat controversial as currently this allows for economic savings although clearly there is a potential to have negative effects of becoming dependent on a non-renewable energy source, which in many cases needs to be imported. Less controversial, is a clean burning stove that uses wood, charcoal, or dung, but burns more efficiently. These stoves funded by EIC are designed to increase the airflow to the fuel source, creating a better flame and reducing the amount of fuel required to provide a given amount of heat to the target (e.g. it takes less fuel wood to cook dinner). Both types of clean burning stoves reduce the costs associated with fuel use and improve the indoor air quality within the homes of the users.\n\nThe LED lamps combine a small solar photovoltaic panel with a rechargeable battery and an array of LED lights into an effective lighting system, which is used to replace dangerous (indoor air quality, GHG emitting, and fire hazard) kerosene lamps. LED lights are known to be very efficient, which prolongs illumination time on the charging made available by the PV.\n\nA typical solar photovoltaic home system funded by EIC consists of a small solar panel (both pole or roof mounted) connected to a charge controller and a small battery. The stored solar electricity can be used both in the day and after dark to power lights and an electrical socket for other electrical equipment (e.g. cell phone charger, OLPC, radio, TV, or computer).\n\nEIC creates carbon offsets from emission reductions that have occurred over the loan term and only once the loan term has finished. In this way, lenders and those that make donations to retire carbon credits can be sure that all emission reductions have already happened.\nThey do this by providing a detailed questionnaires before, during and after the loan, and in-depth field surveys conducted by both by EIC and independent auditing firms in order to sell the carbon offsets. The particularly clever part is that loaners may then buy the carbon credits and get a tax credit from EIC.\n\nIn order to overcome the initial investment barrier that many people have when considering microfinance EIC developed the \"Nanoloan\" concept. For most microfinance institutions (e.g. Kiva) the minimum loan is $25 and the average time until repayment is about 18 months. This can put a lot of well-intentioned people off of the concept. EIC Nanoloans are a limited run of $5 loans, specifically designed to be repaid in just 60 days. The object of offering nanoloans is to introduce new lenders to the entire microlending process fast.\n\nEIC has been covered extensively in the green alternative press including The Huffington Post, Mother Nature Network, GreenBiz, the Next Billion and the standard press such as CNN.\n\nHugh Whalan, who co-founded the organisation in 2009, left EIC in July 2011 to start a for-profit solar distribution and lending business in Ghana. Scott Tudman has been CEO since. \n\nEnergy in Common announced on its Facebook page on September 19, 2013 that all lending activities had been suspended and that loan recovery from delinquent borrowers were underway. Subsequent replies on Facebook by lenders have gone unaddressed and there have been other reports of concerns being reported without reply. There has been no activity on its Twitter feed since June 26, 2012.\n\n\n"}
{"id": "92422", "url": "https://en.wikipedia.org/wiki?curid=92422", "title": "Eruca sativa", "text": "Eruca sativa\n\nArugula or rocket (\"Eruca sativa\"; syns. \"E. vesicaria\" subsp. \"sativa\" (Miller) Thell., \"Brassica eruca\" L.) is an edible annual plant in the family Brassicaceae used as a leaf vegetable for its fresh peppery flavor. Other common names include garden rocket, (British, Australian, South African, Irish and New Zealand English), and eruca. Some additional names are \"rocket salad\", \"rucola\", \"rucoli\", \"rugula\", \"colewort\", and \"roquette\". \"Eruca sativa\", which is widely popular as a salad vegetable, is a species of \"Eruca\" native to the Mediterranean region, from Morocco and Portugal in the west to Syria, Lebanon, and Turkey in the east.\n\n\"Eruca sativa\" grows in height. The pinnate leaves have four to ten small, deep, lateral lobes and a large terminal lobe. The flowers are in diameter, arranged in a corymb in typical Brassicaceae fashion, with creamy white petals veined in purple, and having yellow stamens; the sepals are shed soon after the flower opens. The fruit is a siliqua (pod) long with an apical beak, and containing several seeds (which are edible). The species has a chromosome number of 2\"n\" = 22.\n\nThe Latin adjective \"sativa\" in the plant's binomial name is derived from \"satum\", the supine of the verb \"sero\", meaning \"to sow\", indicating that the seeds of the plant were sown in gardens. \"Eruca sativa\" differs from \"E. vesicaria\" in having early deciduous sepals. Some botanists consider it a subspecies of \"Eruca vesicaria\": \"E. vesicaria\" subsp. \"sativa\". Still others do not differentiate between the two.\n\nThe English common name \"rocket\" derives from the French \"roquette\", a diminutive of the Latin word \"eruca\", which once designated a particular plant in the family Brassicaceae (probably a type of cabbage). \"Arugula\" (), the common name now widespread in the United States and Canada, entered American English from a non-standard dialect of Italian. The standard Italian word is \"rucola\", a diminutive of the Latin \"eruca\". The \"Oxford English Dictionary\" dates the first appearance of \"arugula\" in American English to a 1960 article in \"The New York Times\" by food editor and prolific cookbook writer Craig Claiborne.\n\nIt is sometimes conflated with \"Diplotaxis tenuifolia\", known as \"perennial wall rocket\", another plant of the family Brassicaceae that is used in the same manner.\n\n\"Eruca sativa\" typically grows on dry, disturbed ground and is also used as a food by the larvae of some moth species, including the garden carpet moth. \"Eruca sativa\" roots are also susceptible to nematode infestation.\n\nA pungent, leafy green vegetable resembling a longer-leaved and open lettuce, \"Eruca sativa\" is rich in vitamin C and potassium. In addition to the leaves, the flowers, young seed pods and mature seeds are all edible.\nGrown as an edible herb in the Mediterranean area since Roman times, it was mentioned by various classical authors as an aphrodisiac, most famously in a poem long ascribed to Virgil, \"Moretum\", which contains the line: \"et Venerem revocans eruca morantem\" (\"and the rocket, which revives drowsy Venus [sexual desire]\"). Some writers assert that for this reason during the Middle Ages it was forbidden to grow rocket in monasteries. It was listed, however, in a decree by Charlemagne of 802 as one of the pot herbs suitable for growing in gardens. Gillian Riley, author of the \"Oxford Companion to Italian Food\", states that because of its reputation as a sexual stimulant, it was \"prudently mixed with lettuce, which was the opposite\" (i.e., calming or even soporific). Riley continues that \"nowadays rocket is enjoyed innocently in mixed salads, to which it adds a pleasing pungency\".\n\nRocket was traditionally collected in the wild or grown in home gardens along with such herbs as parsley and basil. It is now grown commercially in many places, and is available for purchase in supermarkets and farmers' markets throughout the world. It is also naturalised as a wild plant away from its native range in temperate regions around the world, including northern Europe and North America. In India, the mature seeds are known as Gargeer. This is the same name in Arabic, جرجير (\"\"), but used in Arab countries for the fresh leaves.\n\nMild frost conditions hinder the plant's growth and turn the green leaves red.\n\nIn Italy, raw rocket is often added to a pizza at the end of or just after baking. It is also used cooked in Apulia, in southern Italy, to make the pasta dish \"cavatiéddi\", \"in which large amounts of coarsely chopped rocket are added to pasta seasoned with a homemade reduced tomato sauce and pecorino\", as well as in \"many unpretentious recipes in which it is added, chopped, to sauces and cooked dishes\" or in a sauce (made by frying it in olive oil and garlic) used as a condiment for cold meats and fish. In the Slovenian Littoral, it is often combined with boiled potatoes, used in a soup, or served with the cheese burek, especially in the town of Koper.\nIt is also used with salad, tomatoes and mozzarella cheese.\nIn Rome rucola is used with special meat dish called \"straccietti\" that are thin slices of beef with raw rocket and Parmesan cheese.\n\nA sweet, peppery digestive alcohol called \"rucolino\" is made from rocket on the island of Ischia in the Gulf of Naples. This liqueur is a local specialty enjoyed in small quantities following a meal in the same way as a limoncello or grappa.\n\nIn Brazil and Argentina, where its use is widespread, rocket is eaten raw in salads. A popular combination is rocket mixed with mozzarella cheese (normally made out of buffalo milk) and sun-dried tomatoes.\n\nIn Cyprus, the plant is used in salads and omelettes. An omelette with rocket (Greek \"rokka\") is common in Cypriot restaurants. \n\nIn the Gulf Countries the plant is used raw in the salads mixed with other vegetables or alone. In Eastern Saudi Arabia it is widely believed the plant has a lot of health benefits and recommended for newlywed couples.\n\nIn Egypt, the plant is commonly eaten raw as a side dish with many meals, with ful medames for breakfast, and regularly accompanies local seafood dishes.\n\nIn Turkey, similarly, the rocket is eaten raw as a side dish or salad with fish, but is additionally served with a sauce of extra virgin olive oil and lemon juice.\n\nIn West Asia and Northern India, \"Eruca\" seeds are pressed to make taramira oil, used in pickling and (after aging to remove acridity) as a salad or cooking oil. The seed cake is also used as animal feed.\n\n\n"}
{"id": "25321825", "url": "https://en.wikipedia.org/wiki?curid=25321825", "title": "Eucalyptus urophylla", "text": "Eucalyptus urophylla\n\nEucalyptus urophylla, commonly known as Timor white gum, Timor mountain gum, popo or ampupu, is a species of \"Eucalypt\" native to islands of the Indonesian Archipelago and Timor.\n\nIt is also common in other countries with humid and subhumid tropical climates where it is grown as plantation timber.\n\nIt is the floral emblem of East Timor.\n\nThis \"Eucalypt\" is an evergreen tree that typically grows to a height of but can also be gnarled shrub when growing conditions are unfavourable. The tree has a straight bole with no branches present on the trunk for up to . The trunk can have a diameter of up to . The appearance of the bark is variable depending conditions but is typically persistent and subfibrous, smooth to shallow with close longitudinal fissures and red-brown to brown in colour. It will sometimes have a rougher texture mostly at the base of the trunk.\n\nThe discolourous evergreen adult leaves have a subopposite to alternate, arrangement and are a broadly lanceolate shape with a length of and a width of .\n\"E. urophylla\" will commence flowering after two or three years during the dry season.\nIt produces a simple axillary inflorescence called a conflorescence with a solitary umbels containing five to eight flowers that are in length. \nAfter flowering it produces a gum-nut or fruit. The fruit has the shape of a typical \"Eucalypt\" capsule. It is cup-shaped and contains three to valves with a double operculum. The fruit contains four to six small black semicircular seeds. The seeds are mature six months after flowering.\n\nThe species can be quite variable with regard to bark, fruit size and shape.\n\nThe species was first described in 1977 by the botanist Stanley Thatcher Blake as part of the work \"Four new species of Eucalyptus\" as published in the journal \"Austrobaileya\".\n\nThe specific epithet for this species is taken from the Greek words \"uro-\" meaning \"elongated appendage\" and \"phylla\" meaning \"leaves\" referring to the shape of the leaves.\n\nIt has been used to produce hybrid species as it appears to be insect-resistant, including a timber with a trade name of \"Lyptus\", hybridised with \"Eucalyptus grandis\", commonly known as the rose gum or flooded gum.\n\nThe species is native to Indonesia and has a scattered distribution in south eastern areas. It has a scattered distribution and is known on seven islands in total; Adonara, Alor, Flores, Lembata, Pantar, Timor and Wetar. It has a natural range of around . \"E. urophylla\" has an altitude range from sea level up to around . It is endemic to monsoonal areas with two to eight dry months per year. It can tolerate nutrient poor soils that are damp and well aerated. Most of the soils it is found in are volcanic in origin. It often dominates in open, usually secondary montane forests where it is found on mountain slopes and in valleys. It is often found in soils based on basalt, schist and slate but rarely round limestone.\n\nIt has been introduced as a plantation timber to areas beyond its native range. The species was planted in Java in 1890 and Brazil in 1919, it was introduced to Australia in 1966 and more recently to Cameroon, Congo, Gabon, French Guiana, Ivory Coast and Madagascar in Africa and Malaysia, Vietnam, southern China and Papua New Guinea in Asia.\n\nIt is recognised as an invasive weed in Brazil and is known to be a minor host of \"fall armyworm\".\n\nThe tree is used to make a variety of products including charcoal, furniture, building poles, fenceposts, wall paneling, fibreboard pulp and paper and fuel.\nIn Vietnam alone there are of \"E. urophylla\" plantations. \n\nIt can be used for reforestation purposes in its native range.\n\n"}
{"id": "14761148", "url": "https://en.wikipedia.org/wiki?curid=14761148", "title": "Hildegard Breiner", "text": "Hildegard Breiner\n\nHildegard Breiner is from Vorarlberg, Austria, where she and her late husband led the anti-nuclear campaign against Zwentendorf Nuclear Power Plant in the 1970s. In 1978, an unprecedented 85 percent of the voters in Vorarlberg cast their votes against Zwentendorf, tipping the scales of the nationwide referendum. In the second half of the 1980s, Hildegard Breiner played a major role in opposition to the nuclear reprocessing plant Wackersdorf to be built at Wackersdorf in neighbouring Bavaria, Germany. In 2004, Hildegard Breiner received the Nuclear-Free Future Lifetime Achievement Award.\n\n"}
{"id": "6024282", "url": "https://en.wikipedia.org/wiki?curid=6024282", "title": "Hobart Corporation", "text": "Hobart Corporation\n\nThe Hobart Corporation is an American manufacturer of commercial equipment used in the foodservice and grocery industry. The company manufactures food preparation machines for cutting, slicing and mixing, cooking equipment, refrigeration units, warewashing and waste disposal systems, and weighing, wrapping, and labeling systems and products. Hobart is an international company with manufacturing plants in the US, Brazil, Canada, China, France, Germany, Italy, and the UK.\n\nHobart was founded in 1897 as the Hobart Electric Manufacturing Company in Troy, Ohio, by Clarence Charles Hobart (1855-1932). To increase motor sales, the company attached motors to coffee mills and meat grinders, creating a powered food production machine.\n\nThe company reorganized in 1913 as The Hobart Manufacturing Company and became Hobart Corporation in 1974. The former Kitchen Aid division was formed in 1919. In 1934 Hobart bought the Dayton Scale Company (an IBM division) from IBM. Dart & Kraft acquired Hobart in 1981. Kitchen Aid was sold to Whirlpool Corp. in 1986 after getting an offer, for $150 million in January 1985. Dart & Kraft split in 1986, with Hobart becoming part of Premark. Premark was acquired by Illinois Tool Works in 1999. In 2001, Hobart became a part of Illinois Tool Works Food Equipment Group.\n\n"}
{"id": "56207974", "url": "https://en.wikipedia.org/wiki?curid=56207974", "title": "January 2018 North American blizzard", "text": "January 2018 North American blizzard\n\nThe January 2018 North American blizzard was a powerful blizzard that caused severe disruption along the East Coast of the United States and Canada. It dumped snow and ice in places that rarely receive wintry precipitation, even in the winter, such as Florida and Georgia, and produced snowfall accumulations of over in the Mid-Atlantic states, New England, and Atlantic Canada. The storm originated on January 3 as an area of low pressure off the coast of the Southeast. Moving swiftly to the northeast, the storm explosively deepened while moving parallel to the Eastern Seaboard, causing significant snowfall accumulations. The storm received various unofficial names, such as \"Winter Storm Grayson\", \"Blizzard of 2018\" and \"Storm Brody\". The storm was also dubbed a \"historic bomb cyclone\".\n\nOn January 3, blizzard warnings were issued for a large swath of the coast, ranging from Norfolk, Virginia all the way up to Maine. Several states, including North Carolina, New Jersey, New York, and Massachusetts declared states of emergency due to the powerful storm. Hundreds of flights were canceled ahead of the blizzard. Overall, 22 people were confirmed to have been killed due to the storm, and at least 300,000 residents in the United States lost power in total.\n\nEarly on January 1, the Weather Prediction Center (WPC) began to anticipate the possibility of a northward-tracking area of low pressure that would bring wintry precipitation to much of the East Coast of the United States in the first week of January, exacerbating an extended period of anomalously cold weather. Due to modeling confining of precipitation to relatively narrow bands at the time, initial forecasts on the storm's impacts were uncertain. The storm's development was forecast to originate from the eastward progression of a shortwave trough originating from the northern Rocky Mountains, strengthening due to the presence of a longwave trough situated over the Eastern United States. However, as the anticipated event drew closer, the system's genesis grew increasingly complex with the development of two separate disturbances in the jet stream over the upper Mississippi Valley and the eastern extent of the Rocky Mountains; these two would shape the eventual coverage of wintry precipitation associated with the storm. As the troughs pushed eastward, frontogenesis along the trough and a resulting increase in moisture allowed for freezing rain to commence over areas of northern Florida and southern Georgia early on January 3. Later that day, rapid cyclogenesis led to the formation of the forecast low-pressure area north of the Bahamas and east of Jacksonville, Florida, with cloud cover quickly expanding to the north and east ahead of the storm's center; consequently, the WPC began issuing regular storm summaries at 21:00 UTC (4:00 p.m. EST) on January 3.\n\nAfter forming, the extratropical cyclone continued to explosively deepen, tracking northward parallel to the United States East Coast. By the morning of January 4, the powerful storm system had deepened by 53 mbar (hPa; 1.57 inHg) in 21 hours—one of the fastest rates ever observed in the Western Atlantic—to a pressure of 952 mbar (hPa; 28.11 inHg), with a coastal cold front focusing heavy snowfall and thundersnow along immediate coastal regions. The drop in pressure was over twice the threshold (24 mbar (hPa; 0.71 inHg) in 24 hours) for bombogenesis. Onshore, the inland extent of wintry precipitation gradually increased as the storm intensified. As the day progressed, the development of several intense snowbands allowed for heavy snowfall rates of up to per hour over New England, which were enhanced further by the influx of warm low-level air due to the cyclone's circulation. The storm bottomed out at a pressure of 950 mbar (hPa; 28.05 inHg) when it was centered about southeast of Nantucket Island. The cyclone's intensity held steady as it moved north into the Bay of Fundy late on January 4. Afterwards, it began weakening and ultimately dissolved into a trough on January 6.\n\nThe blizzard produced snowfall and other forms of frozen precipitation across much of the United States Eastern Seaboard. As of the WPC's fifth winter storm summary, the highest official snowfall amount recorded is in Cape May Court House, New Jersey; however, a snowfall total of was reported Bathurst, New Brunswick. Freezing rain totals peaked at in Brunswick, Georgia and near Folkston, Georgia. At least twenty-two fatalities were attributed to the storm, including at least eight car accident-related deaths. At least 4,020 flights were cancelled across the United States, with a majority of cancellations caused by the extensive winter storm. Insurers estimate that claims relating to coastal flooding from the storm will be more than those from snow-related damage.\n\nSeveral campuses of Florida A&M University and Florida State University, as well as a number of other Florida universities and school districts, announced closures for January 3. Governor of Georgia Nathan Deal declared a state of emergency for 28 counties. Accumulating snowfall fell in Savannah, Georgia for the first time since February 2010, while Tallahassee, Florida received measurable frozen precipitation, though barely measurable and short-lived, for the first time since 1989. The snowfall forced the closure of Savannah-Hilton Head International Airport, cancelling 78 incoming and outgoing flights. Ice accumulation was reported as far south as northern Levy County, Florida. Widespread power outages affected much of the Southeast U.S. coast during the storm's infancy; nearly 100,000 electricity customers were without power in the Florida-Georgia border region, including over 6,000 in Glynn County, Georgia. Heavy icing downed trees and power lines throughout St. Simons Island, Georgia, causing extensive power outages. Power outages impacted Nassau County, Florida to a similar extent, prompting the opening of an emergency shelter in Hilliard, Florida. Other cold weather shelters throughout the area were opened by county officials, the Salvation Army, and other non-governmental organizations. Four Central Florida counties also opened cold weather shelters. Icy conditions forced numerous road closures, including an stretch of Interstate 10 between Tallahassee, Florida and Live Oak, Florida. All lanes of Interstate 75 in Hamilton County, Florida to facilitate de-icing.\nSnowfall in South Carolina peaked at in Summerville. Charleston recorded the third highest daily snowfall total in its history at and the highest total since 1989. The runways of Joint Base Charleston, used jointly with Charleston International Airport, were closed by the United States Air Force. A state of emergency was declared and a curfew enforced for much of Dorchester County. On January 4, the South Carolina National Guard was deployed to assist impacted areas and the South Carolina Highway Patrol and South Carolina Department of Transportation to recover vehicles. One person was killed in a traffic collision on Interstate 95 in Clarendon County due to icy road conditions following the storm's passage. Governor of North Carolina Roy Cooper activated the state's emergency operations center on January 3 and declared a state of emergency for 54 counties. Due to the inclement conditions, 66 North Carolina school districts issued cancellations, affecting thousands of students. Local snowfalls in excess of occurred across the eastern half of the state. Wilmington, North Carolina observed of snowfall, marking the city's highest total since 2011. Along the Outer Banks, gusts in excess of caused rough seas, resulting in coastal flooding. Water levels rose above normal in Buxton, North Carolina. Four people were killed in the state, including two each in Moore and Beaufort counties and one in Surf City. At the height of the storm, around 20,000 utility customers lost power in the state. Poor driving conditions resulted in around 900 vehicle crashes across North Carolina.\n\nGovernor of Virginia Terry McAuliffe declared a state of emergency in the state on January 3. In Virginia Beach, the storm maintained gusts of for several hours. In one 24-hour period, 118 crashes occurred in the Hampton Roads area, with another 121 disabled vehicles reported. Across the entirety of the state, Virginia troopers responded to 245 vehicular collisions. Gusts in the Hampton Roads area peaked at in Tangier, with lesser gusts farther inland. Due to the local geography, water levels in Chesapeake Bay fell in response to the storm's circulation passing to the east; the Patapsco River near Fort McHenry fell below the mean low water level, reaching its lowest height since 1989. The United States Coast Guard restricted maritime access to the Port of Baltimore from the evening of January 3 into January 5.\n\nNew York City encountered 30 mph winds, and JFK Airport temporarily suspended flights due to whiteout conditions. Central Park reported of snow. Governor of New York Andrew Cuomo declared a state of emergency for Westchester County, New York City, and Long Island.\n\nNew Jersey experienced as much as of snow in some places, with Islip on Long Island, NY reaching of snow; however, tropical storm-force winds blew the snow into banks as high as in certain areas.\n\nBeing the most geographically proximate to the storm's track, Massachusetts bore the highest impacts of all American states. Winds gusted to hurricane-force at on Nantucket and over on mainland Massachusetts.\n\nAt least of snow fell on the Boston area, and fell in Providence, Rhode Island. In Boston, a storm tide of was recorded during the blizzard which flooded areas of the financial district, including a subway station. This beat the previous record set in 1978 by the Blizzard of 1978. Significant coastal flooding occurred in Maine and New Hampshire.\n\nAfter ravaging New England, the storm moved on to Atlantic Canada on January 4 and 5. Heavy snow fell in New Brunswick, peaking at in Bathurst. Sydney reported snowfall rates of up to 8 cm per hour, in heavy bands of thundersnow. While snowfall amounts closer to the Atlantic coast of Nova Scotia were very low, winds gusting up to were reported in Saint-Joseph-du-Moine, causing widespread power outages. At the peak of the storm, nearly 130,000 Nova Scotia Power customers were left without power, while in New Brunswick, around 19,000 NB Power customers were left without power. Offshore waves reached heights of .\n\nOn 5 January 2018, the storm was also responsible for a persistent thunderstorm that brought of rain and gale force winds to the island of Bermuda. However, due to the island’s latitude, no sub-zero temperatures were felt. There were wind gusts of up to .\n\nOn 4 January 2018, both the \"Norwegian Breakaway\" and the \"Norwegian Gem\" traveled through the storm causing major flooding in passenger staterooms. The \"Breakaway\", with 4,000 passengers, was sailing from the Bahamas back to New York City when it sustained flooding throughout the passenger cabins as well as elevators and the hallways. Some rooms were so badly flooded that some passengers slept in the public spaces. Footage of the ordeal showed the sides of the ship being hit by waves as high as . At some points in the trip, the ship tilted so much that some passengers fell out of their beds. There was widespread damage to the interior as glasses fell out of shelves and some furniture toppled over. Paintings in the art gallery could be seen falling off the walls as the ship tilted to the shape of the waves. Seasickness was widespread as guests could be seen vomiting. While the Norwegian Cruise Line did formally apologize, some guests were traumatized to the point of refusing to cruise again. The incident has sparked outrage, with some customers threatening a class action lawsuit. The ship’s late arrival cut the following 14-day cruise short by one day.\n"}
{"id": "12901997", "url": "https://en.wikipedia.org/wiki?curid=12901997", "title": "Kakrapar Atomic Power Station", "text": "Kakrapar Atomic Power Station\n\nKakrapar Atomic Power Station is a nuclear power station in India, which lies in the proximity of the city of Vyara in the state of Gujarat. It consists of two 220 MW pressurised water reactor with heavy water as moderator (PHWR). KAPS-1 went critical on 3 September 1992 and began commercial electricity production a few months later on 6 May 1993. KAPS-2 went critical on 8 January 1995 and began commercial production on 1 September 1995. In January 2003, CANDU Owners Group (COG) declared KAPS as the best performing pressurised heavy water reactor.\n\nThe construction costs originally were estimated to be ₹ 382.52 crore, the plant was finally finished at a price of ₹ 1,335 crore. Construction of units 3 & 4 started in November 2010.\n\n"}
{"id": "1031708", "url": "https://en.wikipedia.org/wiki?curid=1031708", "title": "Macassar oil", "text": "Macassar oil\n\nMacassar oil is a compounded oil used primarily by Western European men throughout the 1800s and early 1900s as a hair conditioner to groom and style the hair.\n\nIt was popularised by Alexander Rowland (1747-1823), a celebrated London barber. It was then not uncommon for barbers to make their own hair preparations, and around 1783 Rowland began offering Rowland's Macassar Oil. Within two decades it had become hugely popular, and was aggressively advertised with extravagant claims of its effectiveness, becoming one of the first nationally advertised products.\n\nThe words \"Macassar Oil\" were registered as a trademark by A. Rowland & Sons, in 1888. Rowland's son (also named Alexander) later stated that a relative living in the island of Celebes in the Dutch East Indies had helped in procurement of the basic ingredient.\n\nMacassar oil is often made with coconut oil or palm oil or that of \"Schleichera oleosa\", combined with ylang-ylang oil (obtained by processing the flowers of the ylang-ylang tree, \"Cananga odorata\") and other fragrant oils.\n\nMacassar oil was so named because it was reputed to have been manufactured from ingredients purchased in the port of Makassar in the Dutch East Indies. The poet Byron facetiously called it \"thine incomparable oil, Macassar\" in the first canto of \"Don Juan\", and Lewis Carroll also mentions \"Rowland's Macassar Oil\" in the poem \"Haddocks' Eyes\" from \"Through the Looking-Glass\".\n\nDue to the tendency for the oil to transfer from the user's hair to the back of his chair, the antimacassar was developed. This is a small cloth (crocheted, embroidered or mass-produced), placed over the back of a chair to protect the upholstery.\n\n"}
{"id": "19601860", "url": "https://en.wikipedia.org/wiki?curid=19601860", "title": "Marine loading arm", "text": "Marine loading arm\n\nA marine loading arm, also known as a mechanical loading arm, loading arm, or MLA is a mechanical arm consisting of articulated steel pipes that connect a tankship such as an oil tanker or chemical tanker to a cargo terminal. Genericized trademarks such as Chiksan (often misspelled Chicksan) are often used to refer to marine loading arms.\n\nA marine loading arm is an alternative to direct hose hookups that is particularly useful for larger vessels and transfers at higher loading rates and pressures. Controlled manually or hydraulically, a loading arm employs swivel joints and can, to some extent, follow the movement of a moored vessel. Many loading arm systems feature quick-connect fittings. Gasket or o-ring arrangements are required to make a secure seal to the ship's manifold flange. A loading arm must be drained or closed off before the connection is broken off. This is usually done in two ways. For fuels such as gas oil and diesel, the lines can be blown out with high pressure air. In the case of fuels such as kerosene or petrol, the lines can be stripped with pumps.\n\nLoading arms can handle both liquids and gases, in a wide range of viscosities and temperatures. Cargoes from liquid sulphur to liquefied natural gas are moved through marine loading arms. Loading arms service vessels in a wide range of sizes, from small river barges to the largest supertankers.\n\nVarious designs exist, and specific installations can be tailored for a given port based on considerations such as vessel size, cargo flow rate and cargo temperature. Environmental constraints, such as the range of tide, wind conditions, and earthquake tolerance, can also affect choice of loading arm. A loading arm installation may include add-ons such as hydraulic or manual quick connect couplers, position monitoring systems, emergency release systems, and piggyback vapor return lines.\n\nCompared to cargo hoses, the loading arm's main drawback is its comparative lack of flexibility.\n\nSince the earliest days of tankships, the need to safely and efficiently transfer bulk liquid to a moored ship has been fundamental. An insufficient solution to this problem led to one of the world's first oil tanker disasters. In 1881, the Branobel tanker \"Nordenskjöld\" was taking on kerosene in Baku. The ship was connected to the pier with a simple piece of pipe. While loading, the ship was hit by a gust of wind and the cargo pipe carrying was jerked away from the hold. Kerosene then spilled onto the deck and down into the engine room, where mechanics were working in the light of kerosene lanterns. The ship then exploded, killing half the crew. Ludvig Nobel responded to the disaster by creating a flexible, leakproof loading pipe which was much more resistant to spills.\n\nChiksan brand marine loading arm manufacturer FMC Technologies claims to have built the world's first all-steel marine loading arm in 1956 and to have over 8,000 units installed worldwide.\n\n\n\n"}
{"id": "53311744", "url": "https://en.wikipedia.org/wiki?curid=53311744", "title": "Miedema's model", "text": "Miedema's model\n\nMiedema's model is a semi-empirical approach for estimating the heat of formation of solid or liquid metal alloys and /or compounds in the framework of thermodynamic calculations for metals and minerals. It was developed by the Dutch scientist Andries Rinse Miedema ( 15 November 1933 - 28 May 1992 ) while working at Philips Research Laboratories Philips_Natuurkundig_Laboratorium. It may provide or confirm basic enthalpy data that have always been needed for the calculation of phase diagrams of metals, and that are now currently approached by CALPHAD. The method has been introduced by Miedema in a couple of papers appeared in 1973 in Philips Technical Review Magazine entitled \"A simple model for alloys\". While Miedema himself or with collaborators produced many scientific papers we report here in his own words the genuine motivation of his approach \"Reliable rules for the alloying behaviour of metals have long been sought. There is the qualitative rule that states that the greater the difference in the electronegativity of two metals, the greater the heat of formation - and hence the stability. Then there is the Hume-Rothery rule, which states that two metals that differ by more than 15% in their atomic radius will not form substitutional solid solutions. This rule can only be used reliably (90 % success) to predict poor solubility; it cannot predict good solubility. The author has proposed a simple atomic model, which is empirical like the other two rules, but nevertheless has a clear physical basis and predicts the alloying behaviour of transition metals accurately in 98 % of cases. The model is very suitable for graphical presentation of the data and is therefore easy to use in practice.\" There are several free web bases applications like Entall or Miedema Calculator. The latter has been recently reviewed and improved, with an extension of the method in a scientific publication. A simple presentation of the method is in the appendix B of the book, while the original Algol program has been ported in a fortran code \n\nA recent open journal work emphasizes the capabilities of Miedema's approach in the classification of miscible and immiscible systems of binary alloys. These are extremely relevant in the design of multicomponent alloys. The paper by mining data from hundreds of experimental phase diagrams, and thousands of thermodynamic data sets from experiments and high-throughput first-principles (HTFP) calculations, obtains a comprehensive classification of alloying behavior for 813 binary alloy systems consisting of transition and lanthanide metals. \"Impressively, the classification by the miscibility map yields a robust\nvalidation on the capability of the well-known Miedema’s theory (95% agreement) and shows good agreement with the HTFP method (90% agreement). These 2017 results demonstrate that \"a state-of-the art physics-guided data mining can provide an efficient pathway for knowledge discovery in the next generation of materials design\".\nSee also the html version of the paper by Zhang, R.F., Kong, X.F., Wang, H.T., Zhang, S.H., Legut, D., Sheng, S.H., Srinivasan, S., Rajan, K., Germann, T.C. \n\n"}
{"id": "19555", "url": "https://en.wikipedia.org/wiki?curid=19555", "title": "Molecule", "text": "Molecule\n\nA molecule is an electrically neutral group of two or more atoms held together by chemical bonds. Molecules are distinguished from ions by their lack of electrical charge. However, in quantum physics, organic chemistry, and biochemistry, the term \"molecule\" is often used less strictly, also being applied to polyatomic ions.\n\nIn the kinetic theory of gases, the term \"molecule\" is often used for any gaseous particle regardless of its composition. According to this definition, noble gas atoms are considered molecules as they are monatomic molecules.\n\nA molecule may be homonuclear, that is, it consists of atoms of one chemical element, as with oxygen (O); or it may be heteronuclear, a chemical compound composed of more than one element, as with water (HO). Atoms and complexes connected by non-covalent interactions, such as hydrogen bonds or ionic bonds, are generally not considered single molecules.\n\nMolecules as components of matter are common in organic substances (and therefore biochemistry). They also make up most of the oceans and atmosphere. However, the majority of familiar solid substances on Earth, including most of the minerals that make up the crust, mantle, and core of the Earth, contain many chemical bonds, but are \"not\" made of identifiable molecules. Also, no typical molecule can be defined for ionic crystals (salts) and covalent crystals (network solids), although these are often composed of repeating unit cells that extend either in a plane (such as in graphene) or three-dimensionally (such as in diamond, quartz, or sodium chloride). The theme of repeated unit-cellular-structure also holds for most condensed phases with metallic bonding, which means that solid metals are also not made of molecules. In glasses (solids that exist in a vitreous disordered state), atoms may also be held together by chemical bonds with no presence of any definable molecule, nor any of the regularity of repeating units that characterizes crystals.\n\nThe science of molecules is called \"molecular chemistry\" or \"molecular physics\", depending on whether the focus is on chemistry or physics. Molecular chemistry deals with the laws governing the interaction between molecules that results in the formation and breakage of chemical bonds, while molecular physics deals with the laws governing their structure and properties. In practice, however, this distinction is vague. In molecular sciences, a molecule consists of a stable system (bound state) composed of two or more atoms. Polyatomic ions may sometimes be usefully thought of as electrically charged molecules. The term \"unstable molecule\" is used for very reactive species, i.e., short-lived assemblies (resonances) of electrons and nuclei, such as radicals, molecular ions, Rydberg molecules, transition states, van der Waals complexes, or systems of colliding atoms as in Bose–Einstein condensate.\n\nAccording to Merriam-Webster and the Online Etymology Dictionary, the word \"molecule\" derives from the Latin \"moles\" or small unit of mass.\n\nThe definition of the molecule has evolved as knowledge of the structure of molecules has increased. Earlier definitions were less precise, defining molecules as the smallest particles of pure chemical substances that still retain their composition and chemical properties. This definition often breaks down since many substances in ordinary experience, such as rocks, salts, and metals, are composed of large crystalline networks of chemically bonded atoms or ions, but are not made of discrete molecules.\n\nMolecules are held together by either covalent bonding or ionic bonding. Several types of non-metal elements exist only as molecules in the environment. For example, hydrogen only exists as hydrogen molecule. A molecule of a compound is made out of two or more elements.\n\nA covalent bond is a chemical bond that involves the sharing of electron pairs between atoms. These electron pairs are termed \"shared pairs\" or \"bonding pairs\", and the stable balance of attractive and repulsive forces between atoms, when they share electrons, is termed \"covalent bonding\".\n\nIonic bonding is a type of chemical bond that involves the electrostatic attraction between oppositely charged ions, and is the primary interaction occurring in ionic compounds. The ions are atoms that have lost one or more electrons (termed cations) and atoms that have gained one or more electrons (termed anions). This transfer of electrons is termed \"electrovalence\" in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complicated nature, e.g. molecular ions like NH or SO. Basically, an ionic bond is the transfer of electrons from a metal to a non-metal for both atoms to obtain a full valence shell.\n\nMost molecules are far too small to be seen with the naked eye, but there are exceptions. DNA, a macromolecule, can reach macroscopic sizes, as can molecules of many polymers. Molecules commonly used as building blocks for organic synthesis have a dimension of a few angstroms (Å) to several dozen Å, or around one billionth of a meter. Single molecules cannot usually be observed by light (as noted above), but small molecules and even the outlines of individual atoms may be traced in some circumstances by use of an atomic force microscope. Some of the largest molecules are macromolecules or supermolecules.\n\nThe smallest molecule is the diatomic hydrogen (H), with a bond length of 0.74 Å.\n\nEffective molecular radius is the size a molecule displays in solution.\nThe table of permselectivity for different substances contains examples.\n\nThe chemical formula for a molecule uses one line of chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, and \"plus\" (+) and \"minus\" (−) signs. These are limited to one typographic line of symbols, which may include subscripts and superscripts.\n\nA compound's empirical formula is a very simple type of chemical formula. It is the simplest integer ratio of the chemical elements that constitute it. For example, water is always composed of a 2:1 ratio of hydrogen to oxygen atoms, and ethyl alcohol or ethanol is always composed of carbon, hydrogen, and oxygen in a 2:6:1 ratio. However, this does not determine the kind of molecule uniquely – dimethyl ether has the same ratios as ethanol, for instance. Molecules with the same atoms in different arrangements are called isomers. Also carbohydrates, for example, have the same ratio (carbon:hydrogen:oxygen= 1:2:1) (and thus the same empirical formula) but different total numbers of atoms in the molecule.\n\nThe molecular formula reflects the exact number of atoms that compose the molecule and so characterizes different molecules. However different isomers can have the same atomic composition while being different molecules.\n\nThe empirical formula is often the same as the molecular formula but not always. For example, the molecule acetylene has molecular formula CH, but the simplest integer ratio of elements is CH.\n\nThe molecular mass can be calculated from the chemical formula and is expressed in conventional atomic mass units equal to 1/12 of the mass of a neutral carbon-12 (C isotope) atom. For network solids, the term formula unit is used in stoichiometric calculations.\n\nFor molecules with a complicated 3-dimensional structure, especially involving atoms bonded to four different substituents, a simple molecular formula or even semi-structural chemical formula may not be enough to completely specify the molecule. In this case, a graphical type of formula called a structural formula may be needed. Structural formulas may in turn be represented with a one-dimensional chemical name, but such chemical nomenclature requires many words and terms which are not part of chemical formulas.\n\nMolecules have fixed equilibrium geometries—bond lengths and angles— about which they continuously oscillate through vibrational and rotational motions. A pure substance is composed of molecules with the same average geometrical structure. The chemical formula and the structure of a molecule are the two important factors that determine its properties, particularly its reactivity. Isomers share a chemical formula but normally have very different properties because of their different structures. Stereoisomers, a particular type of isomer, may have very similar physico-chemical properties and at the same time different biochemical activities.\n\nMolecular spectroscopy deals with the response (spectrum) of molecules interacting with probing signals of known energy (or frequency, according to Planck's formula). Molecules have quantized energy levels that can be analyzed by detecting the molecule's energy exchange through absorbance or emission.\nSpectroscopy does not generally refer to diffraction studies where particles such as neutrons, electrons, or high energy X-rays interact with a regular arrangement of molecules (as in a crystal).\n\nMicrowave spectroscopy commonly measures changes in the rotation of molecules, and can be used to identify molecules in outer space. Infrared spectroscopy measures changes in vibration of molecules, including stretching, bending or twisting motions. It is commonly used to identify the kinds of bonds or functional groups in molecules. Changes in the arrangements of electrons yield absorption or emission lines in ultraviolet, visible or near infrared light, and result in colour. Nuclear resonance spectroscopy actually measures the environment of particular nuclei in the molecule, and can be used to characterise the numbers of atoms in different positions in a molecule.\n\nThe study of molecules by molecular physics and theoretical chemistry is largely based on quantum mechanics and is essential for the understanding of the chemical bond. The simplest of molecules is the hydrogen molecule-ion, H, and the simplest of all the chemical bonds is the one-electron bond. H is composed of two positively charged protons and one negatively charged electron, which means that the Schrödinger equation for the system can be solved more easily due to the lack of electron–electron repulsion. With the development of fast digital computers, approximate solutions for more complicated molecules became possible and are one of the main aspects of computational chemistry.\n\nWhen trying to define rigorously whether an arrangement of atoms is \"sufficiently stable\" to be considered a molecule, IUPAC suggests that it \"must correspond to a depression on the potential energy surface that is deep enough to confine at least one vibrational state\". This definition does not depend on the nature of the interaction between the atoms, but only on the strength of the interaction. In fact, it includes weakly bound species that would not traditionally be considered molecules, such as the helium dimer, He, which has one vibrational bound state and is so loosely bound that it is only likely to be observed at very low temperatures.\n\nWhether or not an arrangement of atoms is \"sufficiently stable\" to be considered a molecule is inherently an operational definition. Philosophically, therefore, a molecule is not a fundamental entity (in contrast, for instance, to an elementary particle); rather, the concept of a molecule is the chemist's way of making a useful statement about the strengths of atomic-scale interactions in the world that we observe.\n\n\n"}
{"id": "1063435", "url": "https://en.wikipedia.org/wiki?curid=1063435", "title": "Normal force", "text": "Normal force\n\nIn mechanics, the normal force formula_1 is the component of a contact force that is perpendicular to the surface that an object contacts. For example, the surface of a floor or table that prevents an object from falling. In this instance \"normal\" is used in the geometric sense and means perpendicular, as opposed the common language use of \"normal\" meaning common or expected. For example, a person standing still on flat ground is supported by a ground reaction force that consists of only a normal force. If the person stands on a slope and does not slide down it, then the total ground reaction force can be divided into two components: a normal force perpendicular to the ground and a frictional force parallel to the ground. In another common situation, if an object hits a surface with some speed, and the surface can withstand it, the normal force provides for a rapid deceleration, which will depend on the flexibility of the surface and the object.\n\nIn a simple case such as an object resting upon a table, the normal force on the object is equal but in opposite direction to the gravitational force applied on the object (or the weight of the object), that is, formula_2, where \"m\" is mass, and \"g\" is the gravitational field strength (about 9.81 m/s on Earth). The normal force here represents the force applied by the table against the object that prevents it from sinking through the table and requires that the table is sturdy enough to deliver this normal force without breaking. However, it is easy to assume that the normal force and weight are action-reaction force pairs (a common mistake). In this case, the normal force and weight need to be equal in magnitude to explain why there is no upward acceleration of the object. For example, a ball that bounces upwards accelerates upwards because the normal force acting on the ball is larger in magnitude than the weight of the ball. \n\nWhere an object rests on an incline, the normal force is perpendicular to the plane the object rests on. Still, the normal force will be as large as necessary to prevent sinking through the surface, presuming the surface is sturdy enough. The strength of the force can be calculated as: \n\nwhere \"N\" is the normal force, \"m\" is the mass of the object, \"g\" is the gravitational field strength, and \"θ\" is the angle of the inclined surface measured from the horizontal. \n\nThe normal force is one of the several forces which act on the object. In the simple situations so far considered, the most important other forces acting on it are friction and the force of gravity.\n\nIn general, the magnitude of the normal force, \"N\", is the projection of the net surface interaction force, \"T\", in the normal direction, \"n\", and so the normal force vector can be found by scaling the normal direction by the net surface interaction force. The surface interaction force, in turn, is equal to the dot product of the unit normal with the Cauchy stress tensor describing the stress state of the surface. That is:\n\nThe parallel shear component of the contact force is known as the frictional force (formula_6).\n\nThe static coefficient of friction for an object on an inclined plane can be calculated as follows:\n\nfor an object on the point of sliding where formula_8 is the angle between the slope and the horizontal.\n\nAs explained before, the normal force is a contact force, that is, the resultant of the forces between the constituents of the actual constraint, and the constituents of the object. When the object and the constraint (a wall, a table, another object, or any surface) are pressed against each other, the van der Waals force, a force that grows very large very quickly, opposes this and for this reason, the two objects cannot penetrate each other. If the two objects are then separated, even by a very small distance, the normal force disappears. This is because the intermolecular force quickly vanishes past the point of equilibrium.\n\nIn an elevator either stationary or moving at constant velocity, the normal force on the person's feet balances the person's weight. In an elevator that is accelerating upward, the normal force is greater than the person's ground weight and so the person's perceived weight increases (making the person feel heavier). In an elevator that is accelerating downward, the normal force is less than the person's ground weight and so a passenger's perceived weight decreases. If a passenger were to stand on a weighing scale, such as a conventional bathroom scale, while riding the elevator, the scale will be reading the normal force it delivers to the passenger's feet, and will be different than the person's ground weight if the elevator cab is \"accelerating\" up or down. The weighing scale measures normal force (which varies as the elevator cab accelerates), not gravitational force (which does not vary as the cab accelerates).\n\nWhen we define upward to be the positive direction, constructing Newton's second law and solving for the normal force on a passenger yields the following equation:\n"}
{"id": "3521086", "url": "https://en.wikipedia.org/wiki?curid=3521086", "title": "Oak Ridges Moraine Land Trust", "text": "Oak Ridges Moraine Land Trust\n\nThe Oak Ridges Moraine Land Trust (ORMLT) is a Canadian charitable non-profit organization founded in 1999. Its mandate is to acquire land on the Oak Ridges Moraine to protect it from development, typically through land donations or conservation easements. As of November 2008, it has secured over 3000 acres (12.2 km²) of land.\n\n"}
{"id": "661100", "url": "https://en.wikipedia.org/wiki?curid=661100", "title": "Oleic acid", "text": "Oleic acid\n\nOleic acid is a fatty acid that occurs naturally in various animal and vegetable fats and oils. It is an odorless, colorless oil, although commercial samples may be yellowish. In chemical terms, oleic acid is classified as a monounsaturated omega-9 fatty acid, abbreviated with a lipid number of 18:1 \"cis\"-9. It has the formula CH(CH)CH=CH(CH)COOH. The name derives from the Latin word \"oleum\", which means oil. It is the most common fatty acid in nature.\n\nFatty acids (or their salts) do not often occur as such in biological systems. Instead fatty acids like oleic acid occur as their esters, commonly triglycerides, which are the greasy materials in many natural oils. Fatty acids can be obtained by the saponification of triglycerides.\n\nTriglycerides of oleic acid comprise the majority of olive oil. Free oleic acid renders olive oil inedible. It also makes up 59-75% of pecan oil, 61% of canola oil, 36-67% of peanut oil, 60% of macadamia oil, 20-80% of sunflower oil, 15-20% of grape seed oil, sea buckthorn oil, and sesame oil, and 14% of poppyseed oil. High oleic variants of plant sources like sunflower (~80%) and canola oil (70%) has also been developed. It also comprises 22.18% of the fats from the fruit of the durian species \"Durio graveolens\". Likewise Karuka contains 52.39% oleic acid. It is abundantly present in many animal fats, constituting 37 to 56% of chicken and turkey fat and 44 to 47% of lard.\n\nOleic acid is the most abundant fatty acid in human adipose tissue, and second in abundance in human tissues overall only to palmitic acid.\n\nOleic acid is emitted by the decaying corpses of a number of insects, including bees and \"Pogonomyrmex\" ants, and triggers the instincts of living workers to remove the dead bodies from the hive. If a live bee or ant is dabbed with oleic acid, it is dragged off for disposal as if it were dead. The oleic acid smell also may indicate danger to living insects, prompting them to avoid others who have succumbed to disease or places where predators lurk.\n\nThe biosynthesis of oleic acid involves the action of the enzyme stearoyl-CoA 9-desaturase acting on stearoyl-CoA. In effect, stearic acid is dehydrogenated to give the monounsaturated derivative oleic acid.\n\nOleic acid undergoes the reactions of carboxylic acids and alkenes. It is soluble in aqueous base to give soaps called oleates. Iodine adds across the double bond. Hydrogenation of the double bond yields the saturated derivative stearic acid. Oxidation at the double bond occurs slowly in air, and is known as rancidification in foodstuffs or drying in coatings. Reduction of the carboxylic acid group yields oleyl alcohol. Ozonolysis of oleic acid is an important route to azelaic acid. The coproduct is nonanoic acid:\nEsters of azelaic acid find applications in lubrication and plasticizers.\n\nThe stereoisomer of oleic acid is called elaidic acid or trans-9-octadecenoic acid. These isomers have distinct physical properties and biochemical properties. Elaidic acid, the most abundant trans fatty acid in diet, appears to have an adverse effect on health. A reaction that converts oleic acid to elaidic acid is called elaidinization.\n\nAnother naturally occurring isomer of oleic acid is petroselinic acid.\n\nIn chemical analysis, fatty acids are separated by gas chromatography of their methyl ester derivatives. Alternatively, separation of unsaturated isomers is possible by argentation thin-layer chromatography.\n\nThe principal use of oleic acid is as a component in many foods, in the form of its triglycerides. It is a component of the normal human diet as a part of animal fats and vegetable oils.\n\nOleic acid as its sodium salt is a major component of soap as an emulsifying agent. It is also used as an emollient. Small amounts of oleic acid are used as an excipient in pharmaceuticals, and it is used as an emulsifying or solubilizing agent in aerosol products.\n\nOleic acid is also used to induce lung damage in certain types of animals, for the purpose of testing new drugs and other means to treat lung diseases. Specifically in sheep, intravenous administration of oleic acid causes acute lung injury with corresponding pulmonary edema. This sort of research has been of particular benefit to premature newborns, for whom treatment for underdeveloped lungs (and associated complications) is often a matter of life and death.\n\nOleic acid is used as a soldering flux in stained glass work for joining lead came.\n\nOleic acid is also widely used in the solution phase synthesis of nanoparticles, functioning as a kinetic knob to control the size and morphology of nanoparticles.\n\nOleic acid is a common monounsaturated fat in human diet. Monounsaturated fat consumption has been associated with decreased low-density lipoprotein (LDL) cholesterol, and possibly increased high-density lipoprotein (HDL) cholesterol. However, its ability to raise HDL is still debated.\n\nOleic acid may be responsible for the hypotensive (blood pressure reducing) effects of olive oil. Adverse effects also have been documented, however, since both oleic and monounsaturated fatty acid levels in the membranes of red blood cells have been associated with increased risk of breast cancer, although the consumption of oleate in olive oil has been associated with a \"decreased\" risk of breast cancer.\n\n\n"}
{"id": "1460525", "url": "https://en.wikipedia.org/wiki?curid=1460525", "title": "Oncovirus", "text": "Oncovirus\n\nAn oncovirus is a virus that can cause cancer. This term originated from studies of acutely transforming retroviruses in the 1950–60s, often called oncornaviruses to denote their RNA virus origin.\nIt now refers to any virus with a DNA or RNA genome causing cancer and is synonymous with \"tumor virus\" or \"cancer virus\". The vast majority of human and animal viruses do not cause cancer, probably because of longstanding co-evolution between the virus and its host. Oncoviruses have been important not only in epidemiology, but also in investigations of cell cycle control mechanisms such as the Retinoblastoma protein.\n\nThe World Health Organization's International Agency for Research on Cancer estimated that in 2002, infection caused 17.8% of human cancers, with 11.9% caused by one of seven viruses. These cancers might be easily prevented through vaccination (e.g., papillomavirus vaccines), diagnosed with simple blood tests, and treated with less-toxic antiviral compounds.\n\nGenerally, tumor viruses cause little or no disease after infection in their hosts, or cause non-neoplastic diseases such as acute hepatitis for hepatitis B virus or mononucleosis for Epstein–Barr virus. A minority of persons (or animals) will go on to develop cancers after infection. This has complicated efforts to determine whether or not a given virus causes cancer. The well-known Koch's postulates, 19th-century constructs developed by Robert Koch to establish the likelihood that \"Bacillus anthracis\" will cause anthrax disease, are not applicable to viral diseases. (Firstly, this is because viruses cannot truly be isolated in pure culture—even stringent isolation techniques cannot exclude undetected contaminating viruses with similar density characteristics, and viruses must be grown on cells. Secondly, asymptomatic virus infection and carriage is the norm for most tumor viruses, which violates Koch's third principle. Relman and Fredericks have described the difficulties in applying Koch's postulates to virus-induced cancers. Finally, the host restriction for human viruses makes it unethical to experimentally transmit a suspected cancer virus.) Other measures, such as A. B. Hill's criteria, are more relevant to cancer virology but also have some limitations in determining causality.\n\nTumor viruses come in a variety of forms: Viruses with a DNA genome, such as adenovirus, and viruses with an RNA genome, like the Hepatitis C virus (HCV), can cause cancers, as can retroviruses having both DNA and RNA genomes (Human T-lymphotropic virus and hepatitis B virus, which normally replicates as a mixed double and single-stranded DNA virus but also has a retroviral replication component). In many cases, tumor viruses do not cause cancer in their native hosts but only in dead-end species. For example, adenoviruses do not cause cancer in humans but are instead responsible for colds, conjunctivitis and other acute illnesses. They only become tumorigenic when infected into certain rodent species, such as Syrian hamsters. Some viruses are tumorigenic when they infect a cell and persist as circular episomes or plasmids, replicating separately from host cell DNA (Epstein–Barr virus and Kaposi's sarcoma-associated herpesvirus). Other viruses are only carcinogenic when they integrate into the host cell genome as part of a biological accident, such as polyomaviruses and papillomaviruses.\n\nA direct oncogenic viral mechanism involves either insertion of additional viral oncogenic genes into the host cell or to enhance already existing oncogenic genes (proto-oncogenes) in the genome. Indirect viral oncogenicity involves chronic nonspecific inflammation occurring over decades of infection, as is the case for HCV-induced liver cancer. These two mechanisms differ in their biology and epidemiology: direct tumor viruses must have at least one virus copy in every tumor cell expressing at least one protein or RNA that is causing the cell to become cancerous. Because foreign virus antigens are expressed in these tumors, persons who are immunosuppressed such as AIDS or transplant patients are at higher risk for these types of cancers. Chronic indirect tumor viruses, on the other hand, can be lost (at least theoretically) from a mature tumor that has accumulated sufficient mutations and growth conditions (hyperplasia) from the chronic inflammation of viral infection. In this latter case, it is controversial but at least theoretically possible that an indirect tumor virus could undergo \"hit-and-run\" and so the virus would be lost from the clinically diagnosed tumor. In practical terms, this is an uncommon occurrence if it does occur.\n\n\n\nThe theory that cancer could be caused by a virus began with the experiments of Oluf Bang and Vilhelm Ellerman in 1908 who first show that avian erythroblastosis (a form of chicken leukemia) could be transmitted by cell-free extracts. This was subsequently confirmed for solid tumors in chickens in 1910-1911 by Peyton Rous.\n\nBy the early 1950s it was known that viruses could remove and incorporate genes and genetic material in cells. It was suggested that these new genes inserted into cells could make the cell cancerous. Many of these viral oncogenes have been discovered and identified to cause cancer.\n\nThe main viruses associated with human cancers are human papillomavirus, hepatitis B and hepatitis C virus, Epstein–Barr virus, human T-lymphotropic virus, Kaposi's sarcoma-associated herpesvirus (KSHV) and Merkel cell polyomavirus. Experimental and epidemiological data imply a causative role for viruses and they appear to be the second most important risk factor for cancer development in humans, exceeded only by tobacco usage. The mode of virally induced tumors can be divided into two, \"acutely transforming\" or \"slowly transforming\". In acutely transforming viruses, the viral particles carry a gene that encodes for an overactive oncogene called viral-oncogene (v-onc), and the infected cell is transformed as soon as v-onc is expressed. In contrast, in slowly transforming viruses, the virus genome is inserted, especially as viral genome insertion is an obligatory part of retroviruses, near a proto-oncogene in the host genome. The viral promoter or other transcription regulation elements in turn cause overexpression of that proto-oncogene, which in turn induces uncontrolled cellular proliferation. Because viral genome insertion is not specific to proto-oncogenes and the chance of insertion near that proto-oncogene is low, slowly transforming viruses have very long tumor latency compared to acutely transforming viruses, which already carry the viral oncogene.\n\nHepatitis viruses, including hepatitis B and hepatitis C, can induce a chronic viral infection that leads to liver cancer in 0.47% of hepatitis B patients per year (especially in Asia, less so in North America), and in 1.4% of hepatitis C carriers per year. Liver cirrhosis, whether from chronic viral hepatitis infection or alcoholism, is associated with the development of liver cancer, and the combination of cirrhosis and viral hepatitis presents the highest risk of liver cancer development. Worldwide, liver cancer is one of the most common, and most deadly, cancers due to a huge burden of viral hepatitis transmission and disease.\n\nThrough advances in cancer research, vaccines designed to prevent cancer have been created. The hepatitis B vaccine is the first vaccine that has been established to prevent cancer (hepatocellular carcinoma) by preventing infection with the causative virus. In 2006, the U.S. Food and Drug Administration approved a human papilloma virus vaccine, called Gardasil. The vaccine protects against four HPV types, which together cause 70% of cervical cancers and 90% of genital warts. In March 2007, the US Centers for Disease Control and Prevention (CDC) Advisory Committee on Immunization Practices (ACIP) officially recommended that females aged 11–12 receive the vaccine, and indicated that females as young as age 9 and as old as age 26 are also candidates for immunization.\n\nDNA oncoviruses typically impair two families of tumor suppressor proteins: tumor proteins p53 and the retinoblastoma proteins (Rb). It is evolutionarily advantageous for viruses to inactivate p53 because p53 can trigger cell cycle arrest or apoptosis in infected cells when the virus attempts to replicate its DNA. Similarly, Rb proteins regulate many essential cell functions, including but not limited to a crucial cell cycle checkpoint, making them a target for viruses attempting to interrupt regular cell function.\n\nWhile several DNA oncoviruses have been discovered, three have been studied extensively. Adenoviruses can lead to tumors in rodent models but do not cause cancer in humans; however, they have been exploited as delivery vehicles in gene therapy for diseases such as cystic fibrosis and cancer. Simian virus 40 (SV40), a polyomavirus, can cause tumors in rodent models but is not oncogenic in humans. This phenomenon has been one of the major controversies of oncogenesis in the 20th century because an estimated 100 million people were inadvertently exposed to SV40 through polio vaccines. The Human Papillomavirus-16 (HPV-16) has been shown to lead to cervical cancer and other cancers, including head and neck cancer. These three viruses have parallel mechanisms of action, forming an archetype for DNA oncoviruses. All three of these DNA oncoviruses are able to integrate their DNA into the host cell, and use this to transcribe it and transform cells by bypassing the G1/S checkpoint of the cell cycle.\n\nDNA oncoviruses transform infected cells by integrating their DNA into the host cell’s genome. The DNA is believed to be inserted during transcription or replication, when the two annealed strands are separated. This event is relatively rare and generally unpredictable; there seems to be no deterministic predictor of the site of integration. After integration, the host’s cell cycle loses regulation from Rb and p53, and the cell begins cloning to form a tumor.\n\nRb and p53 regulate the transition between G1 and S phase, arresting the cell cycle before DNA replication until the appropriate checkpoint inputs, such as DNA damage repair, are completed. p53 regulates the p21 gene, which produces a protein which binds to the Cyclin D-Cdk4/6 complex. This prevents Rb phosphorylation and prevents the cell from entering S phase. In mammals, when Rb is active (unphosphorylated), it inhibits the E2F family of transcription factors, which regulate the Cyclin E-Cdk2 complex, which inhibits Rb, forming a positive feedback loop, keeping the cell in G1 until the input crosses a threshold. To drive the cell into S phase prematurely, the viruses must inactivate p53, which plays a central role in the G1/S checkpoint, as well as Rb, which, though downstream of it, is typically kept active by a positive feedback loop.\n\nViruses employ various methods of inactivating p53. The adenovirus E1B protein (55K) prevents p53 from regulating genes by binding to the site on p53 which binds to the genome. In SV40, the large T antigen (LT) is an analogue; LT also binds to several other cellular proteins, such as p107 and p130, on the same residues. LT binds to p53’s binding domain on the DNA (rather than on the protein), again preventing p53 from appropriately regulating genes. HPV instead degrades p53: the HPV protein E6 binds to a cellular protein called the E6-associated protein (E6-AP, also known as UBE3A), forming a complex which causes the rapid and specific ubiquitination of p53.\n\nRb is inactivated (thereby allowing the G1/S transition to progress unimpeded) by different but analogous viral oncoproteins. The adenovirus early region 1A (E1A) is an oncoprotein which binds to Rb and can stimulate transcription and transform cells. SV40 uses the same protein for inactivating Rb, LT, to inactivate p53. HPV contains a protein, E7, which can bind to Rb in much the same way. Rb can be inactivated by phosphorylation, or by being bound to a viral oncoprotein, or by mutations—mutations which prevent oncoprotein binding are also associated with cancer.\n\nDNA oncoviruses typically cause cancer by inactivating p53 and Rb, thereby allowing unregulated cell division and creating tumors. There may be many different mechanisms which have evolved separately; in addition to those described above, for example, the Hepatitis B virus (an RNA virus) inactivates p53 by sequestering it in the cytoplasm.\n\nSV40 has been well studied and does not cause cancer in humans, but a recently discovered analogue called Merkel cell polyomavirus has been associated with Merkel cell carcinoma, a form of skin cancer. The Rb binding feature is believed to be the same between the two viruses.\n\nIn the 1960s, the replication process of RNA virus was believed to be similar to other single-stranded RNA. Single-stranded RNA replication involves RNA-dependent RNA synthesis which meant that virus-coding enzymes would make partial double-stranded RNA. This belief was proven to be incorrect because there were no double-stranded RNA found in the retrovirus cell. In 1964, Howard Temin proposed a provirus hypothesis, but shortly after reverse transcription in the retrovirus genome was discovered.\n\nAll retroviruses have three major coding domains; \"gag, pol \"and\" env\". In the \"gag\" region of the virus, the synthesis of the internal virion proteins are maintained which make up the matrix, capsid and nucleocapsid proteins. In \"pol\", the information for the reverse transcription and integration enzymes are stored. In \"env\", it is derived from the surface and transmembrane for the viral envelope protein. There is a fourth coding domain which is smaller, but exists in all retroviruses. \"Pol\" is the domain that encodes the virion protease.\n\nThe retrovirus begins the journey into a host cell by attaching a surface glycoprotein to the cell's plasma membrane receptor. Once inside the cell, the retrovirus goes through reverse transcription in the cytoplasm and generates a double-stranded DNA copy of the RNA genome. Reverse transcription also produces identical structures known as long terminal repeats (LTRs). Long terminal repeats are at the ends of the DNA strands and regulates viral gene expression. The viral DNA is then translocated into the nucleus where one strand of the retroviral genome is put into the chromosomal DNA by the help of the virion intergrase. At this point the retrovirus is referred to as provirus. Once in the chromosomal DNA, the provirus is transcribed by the cellular RNA polymerase II. The transcription leads to the splicing and full-length mRNAs and full-length progeny virion RNA. The virion protein and progeny RNA assemble in the cytoplasm and leave the cell, whereas the other copies send translated viral messages in the cytoplasm.\n\n\nNot all oncoviruses are DNA viruses. Some RNA viruses have also been associated such as the hepatitis C virus as well as certain retroviruses, e.g., human T-lymphotropic virus (HTLV-1) and Rous sarcoma virus (RSV).\n\nEstimated percent of new cancers attributable to the virus worldwide in 2002. NA indicates not available.\nThe association of other viruses with human cancer is continually under research.\n\n"}
{"id": "10062927", "url": "https://en.wikipedia.org/wiki?curid=10062927", "title": "Overdrive voltage", "text": "Overdrive voltage\n\nOverdrive voltage, usually abbreviated as V, is typically referred to in the context of MOSFET transistors. The overdrive voltage is defined as the voltage between transistor gate and source (V) in excess of the threshold voltage (V) where V is defined as the minimum voltage required between gate and source to turn the transistor on (allow it to conduct electricity). Due to this definition, overdrive voltage is also known as \"excess gate voltage\" or \"effective voltage.\" Overdrive voltage can be found using the simple equation: V = V − V.\n\nV is important as it directly affects the output drain terminal current (I) of the transistor, an important property of amplifier circuits. By increasing V, I can be increased until saturation is reached.\n\nOverdrive voltage is also important because of its relationship to V, the drain voltage relative to the source, which can be used to determine the region of operation of the MOSFET. The table below shows how to use overdrive voltage to understand what region of operation the MOSFET is in:\n\nA more physics-related explanation follows:\n\nIn an NMOS transistor, the channel region under zero bias has an abundance of holes (i.e., it is p-type silicon). By applying a negative gate bias (V < 0) we attract MORE holes, and this is called accumulation. A positive gate voltage (V > 0) will attract electrons and repel holes, and this is called depletion because we are depleting the number of holes. At a critical voltage called the THRESHOLD VOLTAGE (V) the channel will actually be so depleted of holes and rich in electrons that it will INVERT to being n-type silicon, and this is called the inversion region.\n\nAs we increase this voltage, V, beyond V, we are said to be then OVERDRIVING the gate by creating a stronger channel, hence the OVERDRIVE VOLTAGE (Called often V, V, or V) is defined as (V − V)\n\n"}
{"id": "2450544", "url": "https://en.wikipedia.org/wiki?curid=2450544", "title": "Palagonite", "text": "Palagonite\n\nPalagonite is an alteration product from the interaction of water with volcanic glass of chemical composition similar to basalt. Palagonite can also result from the interaction between water and basalt melt. The water flashes to steam on contact with the hot lava and the small fragments of lava react with the steam to form the light colored palagonite tuff cones common in areas of basaltic eruptions in contact with water. An example is found in the pyroclastic cones of the Galapagos Islands. Charles Darwin recognized the origin of these cones during his visit to the islands. Palagonite can also be formed by a slower weathering of lava into palagonite, resulting in a thin, yellow-orange rind on the surface of the rock. The process of conversion of lava to palagonite is called \"palagonitization.\"\n\nPalagonite soil is a light yellow-orange dust, comprising a mixture of particles ranging down to sub-micrometer sizes, usually found mixed with larger fragments of lava. The color is indicative of the presence of iron in the +3 oxidation state, embedded in an amorphous matrix.\n\nPalagonite tuff is a tuff composed of sideromelane fragments and coarser pieces of basaltic rock, embedded in a palagonite matrix. A composite of sideromelane aggregate in palagonite matrix is called hyaloclastite.\n\nBased on infrared spectroscopy, the fine-grained component of Mauna-Kea palagonite is the terrestrial material with the best match to the spectral properties of Martian dust, and is believed to be similar in composition and in origin to dusty component of the surface regolith of Mars. The palagonitic tephra from a cinder cone in Hawaii has been used to create Martian regolith simulant for researchers. The spectroscopic signature of palagonitic alteration on Mars is used as evidence for the existence of water on Mars.\n\n"}
{"id": "6755173", "url": "https://en.wikipedia.org/wiki?curid=6755173", "title": "Piscivore", "text": "Piscivore\n\nA piscivore is a carnivorous animal that eats primarily fish. Piscivorous is equivalent to the Greek-derived word ichthyophagous. Fish were the diet of early tetrapods (amphibians); insectivory came next, then in time, reptiles added herbivory.\n\nSome animals, such as the sea lion and alligator, are not completely piscivorous, often preying on aquatic invertebrates or land animals in addition to fish, while others, such as the bulldog bat and gharial, are strictly dependent on fish for food. Humans can live on fish-based diets as can their carnivorous domesticated pets, such as dogs and cats. The name \"piscivore\" is derived from the Latin word for fish, \"piscis\". Some creatures, including cnidarians, octopuses, squid, spiders, sharks, cetaceans, grizzly bears, jaguars, wolves, snakes, turtles, and sea gulls, may have fish as significant if not dominant portions of their diets.\n\nThe ecological effects of piscivores can extend to other food chains. In a study of cutthroat trout stocking, researchers found that the addition of this piscivore can have noticeable effects on non-aquatic organisms, in this case bats feeding on insects emerging from the water with the trout.\n\nThere exists classifications of primary and secondary piscivores. Primary piscivores, also known as \"specialists\", shift to this habit in the first few months of their lives. Secondary piscivores will move to eating primarily fish later in their lifetime. It is hypothesized that the secondary piscivores' diet change is due to an adaptation to maintain efficiency in their use of energy while growing.\n\n\nNumerous extinct and prehistoric animals are hypothesized to have been primarily piscivorous due to anatomy and/or ecology. Furthermore, some have been confirmed to be piscivorous through fossil evidence.\n"}
{"id": "11950089", "url": "https://en.wikipedia.org/wiki?curid=11950089", "title": "Poliya", "text": "Poliya\n\nPoliya Composite Resins and Polymers, Inc. (Poliya) was founded in 1983 and specializes in developing and manufacturing polymers and composite resins. Poliya’s headquarters are located in Istanbul, Turkey with other Poliya locations and manufacturing facilities in Turkey and Russia. \n\nAs of 2013, Poliya is listed in the second Top 500 largest companies of Turkey. Most widely known for their flagship product, Polijel high performance gelcoat series, the company also manufactures UPE-polyester resins, vinyl ester resins, pigment color pastes, solid surface chips, adhesives, bonding pastes, mold release agents and waxes. Poliya’s diverse product portfolio makes it a thriving international company. As the industry leader, Poliya serves 25 countries throughout the world and is the fastest-growing composite resin manufacturer in Europe.\nPoliya’s main focus continues is research and development while remaining at the forefront of environmental consciousness. The company is a member of the European Chemical Industry Council - CEFIC, Turkish Chemical Manufacturers Association, and the Turkish Composites Manufacturers Association.\n\nIsmet Cakar, a chemical engineer, began making early contributions to polyester resin modification and gelcoat UV stabilizers. Cakar worked on polymerization and resins at Ilkester, leaving to found a start-up. In 1983, Cakar launched the company that would become Poliya. Early on, Poliya recognized that composite materials would need special functions under different usage conditions (UV resistant, chemical resistant, etc.), and these composite resins would be required in various low weight and corrosion resistant applications which would require similar modification technology.\n\nPoliya contributes scientific research and local industrial activities in Turkey which has a short history starting in 1980s. Most recently Poliya sponsored the first TURK-KOMPOZIT 2013 Composites Event. Other research and events include Polymeric Composites Symposium, Exhibition and Workshops. Sakarya University Advanced Applied Technologies - Saugar X7 and Sahimo projects as well as Yildiz Technical University AE2 Project and many others to support students and colleges. Also Poliya took place in TUBİTAK-TEYDEP Technology and Innovation Support Programs. Poliya also took part in Industrial Partnership Program designed by TUBITAK-MAM and supported by World Bank About nano composites research, Poliya in partnership with Technische Universität Hamburg-Harburg and IYTE has created a joint project and published various scientific articles about polyester resin and carbon nano tubes for the advancement of nano composite knowledge. Another joint project created by Poliya and Dokuz Eylül University, Institute of Marine Sciences and Technologies of scientific studies was about use of Biocides and silver ions with Polijel gelcoats in the marine environment.\n\nPoliya's core businesses focus on composite performance materials, composite adhesives, composite coatings, solid surface materials, pigment color pastes and release agent technologies, which have been supplemented through several notable expansions. It has also divested itself of less profitable segments.\n\nCombining Polipol polyester resins and Polives vinyl ester resin as well as gelcoat products, Composite performance materials provides products for the construction, transport, marine, defense, wind energy, sports equipment, chemical containment industries. Main manufacturing plant is located in Southeastern Europe, Cerkezkoy-Turkey.\n"}
{"id": "7242445", "url": "https://en.wikipedia.org/wiki?curid=7242445", "title": "Power cycling", "text": "Power cycling\n\nPower cycling is the act of turning a piece of equipment, usually a computer, off and then on again. Reasons for power cycling include having an electronic device reinitialize its set of configuration parameters or recover from an unresponsive state of its mission critical functionality, such as in a crash or hang situation. Power cycling can also be used to reset network activity inside a modem. It can also be among the first steps for troubleshooting an issue.\n\nPower cycling can be done manually, usually using a switch on the device to be cycled; automatically, through some type of device, system, or network management monitoring and control; or by remote control; through a communication channel.\n\nIn the data center environment, remote control power cycling can usually be done through a power distribution unit, over TCP/IP. In the home environment, this can be done through home automation powerline communications or IP protocols. Most Internet Service Providers publish a 'how-to' on their website showing their customers the correct procedure to power cycle their devices.\n\nPower cycling is a standard diagnostic procedure usually performed first when the computer freezes. However, frequently power cycling a computer can cause thermal stress. Reset has an equal effect on the software but may be less problematic for the hardware as power is not interrupted.\n\nOn all Apollo missions to the moon, the landing radar was required to acquire the surface before a landing could be attempted. But on Apollo 14, the landing radar was unable to lock on. Mission control told the astronauts to cycle the power. They did, the radar locked on just in time, and the landing was completed.\n\nDuring the Rosetta mission to comet 67P/Churyumov–Gerasimenko, the Philae lander did not return the expected telemetry on awakening after arrival at the comet. The problem was diagnosed as \"somehow a glitch in the electronics\", engineers cycled the power, and the lander awoke correctly. \n\n"}
{"id": "20895316", "url": "https://en.wikipedia.org/wiki?curid=20895316", "title": "Roly-poly toy", "text": "Roly-poly toy\n\nA roly-poly toy, round-bottomed doll, tilting doll, tumbler or wobbly man is a round-bottomed toy, usually egg-shaped, that tends to right itself when pushed at an angle, and does this in seeming contradiction to the force of gravity. The toy is typically hollow with a weight inside the bottom hemisphere. The placement of this weight is such that the toy has a center of mass below the center of the hemisphere, so that any tilting raises the center of mass. When such a toy is pushed over, it wobbles for a few moments while it seeks the upright orientation, which has an equilibrium at the minimum gravitational potential energy.\n\nDifferent toy manufacturers and different cultures have produced different-looking roly-poly toys: the \"okiagari-koboshi\" and some types of Daruma doll of Japan, the \"nevаlyashka\" (\"untopply\") or \"van'ka-vstan'ka\" (\"Ivan-get-up\") of Russia, and Playskool's Weebles. Japanese \"okiagari\" means \"to get up (\"oki\") and arise (\"agari\")\"; the self-righting characteristic of the toy has come to symbolize the ability to have success, overcome adversity, and recover from misfortune. \n\nTraditional Chinese examples are hollow clay figures of plump children, but \"many Chinese folk artists shape their tumblers in the image of clownish mandarins as they appear on stage; in this way they mock the inefficiency and ineptitude of the bureaucrats\".\n\nA toy manufacturer recommends roly-poly toys for small children just developing motor skills; a child can bat at it without its rolling away.\n\nDynamogene Theater stages a performance called \"Monsieur Culbuto\", allowing the audience to interact with a human dressed as a roly-poly toy.\n\nThe Noddy stories by Enid Blyton features the character Mr. Wobblyman, who is based on this type of toy. \n\nOne of the puppets/non-human presenters in the TV show \"Playbus\" (later \"Playdays\") was a roly-poly clown called Wobble.\n\n"}
{"id": "9022422", "url": "https://en.wikipedia.org/wiki?curid=9022422", "title": "Run-of-the-river hydroelectricity", "text": "Run-of-the-river hydroelectricity\n\nRun-of-river hydroelectricity (ROR) or run-of-the-river hydroelectricity is a type of hydroelectric generation plant whereby little or no water storage is provided. Run-of-the-river power plants may have no water storage at all or a limited amount of storage, in which case the storage reservoir is referred to as pondage. A plant without pondage is subject to seasonal river flows, thus the plant will operate as an intermittent energy source. Conventional hydro uses reservoirs, which regulate water for flood control and dispatchable electrical power.\n\nRun-of-the-river or ROR hydroelectricity is considered ideal for streams or rivers that can sustain a minimum flow or those regulated by a lake or reservoir upstream.\nA small dam is usually built to create a headpond ensuring that there is enough water entering the penstock pipes that lead to the turbines which are at a lower elevation. Projects with pondage, as opposed to those without pondage, can store water for daily load demands. In general, projects divert some or most of a river's flow (up to 95% of mean annual discharge) through a pipe and/or tunnel leading to electricity-generating turbines, then return the water back to the river downstream.\n\nROR projects are dramatically different in design and appearance from conventional hydroelectric projects. Traditional hydro dams store enormous quantities of water in reservoirs, sometimes flooding large tracts of land. In contrast, run-of-river projects do not have the disadvantages associated with reservoirs, which is why they have less environmental impact.\nThe use of the term \"run-of-the-river\" for power projects varies around the world. Some may consider a project ROR if power is produced with no water storage while limited storage is considered ROR by others. Developers may mislabel a project ROR to soothe public perception about its environmental or social effects. The Bureau of Indian Standards describes run-of-the-river hydroelectricity as:\nA power station utilizing the run of the river flows for generation of power with sufficient pondage for supplying water for meeting diurnal or weekly fluctuations of demand. In such stations, the normal course of the river is not materially altered.\nMany of the larger ROR projects have been designed to a scale and generating capacity rivaling some traditional hydro dams. For example, the Beauharnois Hydroelectric Generating Station in Quebec is rated at 1,853 MW. Some run of the river projects are downstream of other dams and reservoirs. The run of the river project didn't build the reservoir, but does take advantage of the water supplied by it. An example would be the 1995 1,436 MW La Grande-1 generating station. Previous upstream dams and reservoirs are part of the 1980s James Bay Project.\n\nWhen developed with care to footprint size and location, ROR hydro projects can create sustainable energy minimizing impacts to the surrounding environment and nearby communities. Advantages include:\n\nLike all hydro-electric power, run-of-the-river hydro harnesses the natural potential energy of water, eliminating the need to burn coal or natural gas to generate the electricity needed by consumers and industry. Moreover, run-of-the-river hydro-electric plants do not have reservoirs thus eliminating the methane and carbon dioxide emissions caused by the decomposition of organic matter in the reservoir of a conventional hydro-electric dam. This is a particular advantage in tropical countries where methane generation can be a problem.\n\nWithout a reservoir, flooding of the upper part of the river does not take place. As a result, people remain living at or near the river and existing habitats are not flooded. Any pre-existing pattern of flooding will continue unaltered, presenting a flood risk to the facility and downstream areas.\n\nRun-of-the-River power is considered an \"unfirm\" source of power: a run-of-the-river project has little or no capacity for energy storage and hence can't co-ordinate the output of electricity generation to match consumer demand. It thus generates much more power during times when seasonal river flows are high (i.e., spring freshet), and depending on location, much less during drier summer months or frozen winter months.\n\nThe potential power at a site is a result of the head and flow of water. By damming a river, the head is available to generate power at the face of the dam. Where a dam may create a reservoir hundreds of kilometres long, in run of the river the head is usually delivered by a canal, pipe or tunnel constructed upstream of the power house. Due to the cost of upstream construction, a steep drop is desirable, such as falls or rapids.\n\nSmall, well-sited ROR projects can be developed with minimal environmental impacts. Larger projects have more environmental concerns. In the case of fish-bearing rivers a ladder may be required and dissolved gases downstream may affect fish.\n\nIn British Columbia the mountainous terrain and wealth of big rivers have made it a global testing ground for 10-50Mw run-of-river technology. As of March 2010, there were 628 applications pending for new water licences solely for the purposes of power generation – representing more than 750 potential points of river diversion.\n\n\n\n"}
{"id": "37097714", "url": "https://en.wikipedia.org/wiki?curid=37097714", "title": "Sasaram back-to-back HVDC converter station", "text": "Sasaram back-to-back HVDC converter station\n\nThe Sasaram back-to-back HVDC station is a back-to-back HVDC connection between the eastern and northern regions in India, located close to the city of Sasaram, in Bihar state in northeastern India. The station is owned by Power Grid Corporation of India.\n\nThe converter station consists one pole with a nominal power transmission rating of 500 MW. The converter station was built by Alstom between 1999 and 2002 and has nominal DC voltage and current ratings of 205 kV, 2475 A. The design is very similar to that of the Chandrapur and Vizag 1 converter stations also built for Power Grid.\n\nIn 2006 the Eastern and Northern regions were made part of the combined NEW grid. As a result, the converter station is no longer required for its original purpose of asynchronously linking the Eastern and Northern grids, although it can still be used as an embedded power flow device to help control power flow within the AC system. The converter station could potentially be shifted to elsewhere to export/import power from other countries.\n\n"}
{"id": "49815710", "url": "https://en.wikipedia.org/wiki?curid=49815710", "title": "Shakes (timber)", "text": "Shakes (timber)\n\nShakes are cracks in timber. Arising in cut timber they generally cause a reduction in strength. When found in a log they can result in a significant amount of waste, when a log is converted to lumber. Apart from heart shakes, often found in trees felled past their best, shakes in a log have no effect on the strength of shake free lumber obtained therefrom.\n\nThey are often seen in oak-framed buildings, which are constructed of green oak which cracks while drying. Due to the immense strength of the oak beams, they are not a cause for concern in a properly engineered building, and are considered part of the charm of the style.\n\nHeart shake is a crack in the heartwood, near the centre of the tree. It is caused by poor seasoning, or by using trees felled past maturity.\n\nA crack or cracks propagating from near the edge of the log towards the centre, usually along the line of the medullary rays. The cause is often rapid or uneven seasoning, causing the outside of the log to shrink faster than the heart. Exposure to the elements can cause star shakes, as can frost during the growth of the tree.\n\nFrost shake begins on the outside where moisture from rain or other means has penetrated, and freezes, causing damage to the wood on the inside.\n\nA cup or ring shake follows the line of annual rings. The separation of the rings is generally caused during the growth of the tree, either by a check in the growth, or by bending and twisting under high winds.\n\nThunder shake is across the grain, and hard to detect until the boards are being planed. It is caused by shock to the wood, such as thunder, or concussion during felling. This fault seriously weakens the timber.\n\n"}
{"id": "54552684", "url": "https://en.wikipedia.org/wiki?curid=54552684", "title": "Shand Mason", "text": "Shand Mason\n\nShand Mason was a British company which designed and manufactured steam powered fire engines and other fire-fighting equipment during the 19th century and early 20th centuries.\n\nThe company that eventually became Shand Mason was founded in 1760 by Samuel Phillips, and incorporated as Phillips and Hopwood in 1797. In 1818, after William Joshua Tilley had joined the business, it became Hopwood and Tilley and later Tilley & Co. In 1850, when Tilley retired from the business, his two sons-in-law James Shand and Samuel Mason continued the business as Shand and Mason, later Shand Mason & Co.\n\nThe company operated from premises at 75 Upper Ground, Blackfriars, just south of the River Thames in London, and having initially manufactured manually operated pumps, secured various patents to improve the design and construction of steam fire engines.\n\nWhile the first steam-powered fire engine had been developed by John Braithwaite and John Ericsson in 1829, the first commercially successful fire-engine was a water-borne version developed by Shand Mason & Co, which went into service in 1855. From this point and particularly from the 1860s, the company worked with the chief of the Metropolitan Fire Brigade, Eyre Massey Shaw and with competitor Merryweather & Sons, to perfect designs for land-based use by the London brigade and other municipalities. Its first land-based engine used by the Brigade was produced in 1860.\n\nIts engines were initially horse-drawn, but later installed on fireboats (Europe's first, the \"Fire Queen\" was built by Shand Mason for service in Bristol's docks in the 1880s), and mounted on motor vehicles. The business was eventually taken over by Merryweather & Sons in 1928.\n"}
{"id": "26905332", "url": "https://en.wikipedia.org/wiki?curid=26905332", "title": "Shingle weaver", "text": "Shingle weaver\n\nA shingle weaver (US) or shingler (UK) is an employee of a wood products mill who engages in the creation of wooden roofing shingles or the closely related product known as \"shakes.\" In the Pacific Northwest region of the United States, historically the leading producer of this product, such shingles are generally made of Western Red Cedar, an aromatic and disease-resistant wood indigenous to the area. The use of the term \"weaver\" for a shingle maker related to the way in which the workers fitted the shingles together in bundles but the meaning has extended to anyone who works in a shingle mill.\n\nDuring the late 19th and early 20th Century, the production of wooden roofing shingles was an extremely dangerous process in which the shingle weaver hand-fed pieces of raw wood onto an automated saw. Despite the danger of the profession, the industry was a large one throughout Washington and Oregon, and by 1893 Washington state alone had 150 mills which converted Western Red Cedar into shingles and shakes for the roofing and siding of American homes.\n\nThe craft of shingle making demanded a high skill level and considerable manual dexterity. An alternate origin for the name \"weaver\" is that it was this nimble motion of the hands of the sawyers around the spinning blades of their saws that provided the origin of the term for the maker of shingles — the woodworkers being likened to skilled operators of looms.\n\n\"Sunset\" magazine described the job of the shingle weavers for its readers:\n\"The saw on his left sets the pace. If the singing blade rips 50 rough shingles off the block every minute, the sawyer must reach over to its teeth 50 times in 60 seconds; if the automatic carriage feeds the odorous wood 60 times into the hungry teeth, 60 times he must reach over, turn the shingle, trim its edge on the gleaming saw in front of him, cut the narrow strip containing the knot hole with two quick movements of his right hand, and toss the completed board down the chute to the packers, meanwhile keeping eyes and ears open for the sound that asks him to feed a new block into the untiring teeth. Hour after hour the shingle weaver's hands and arms, plain, unarmored flesh and blood, are staked against the screeching steel that cares not what it severs. Hour after hour the steel sings its crescendo note as it bites into the wood, the sawdust cloud thickens, the wet sponge under the sawyer's nose fills with fine particles. \n\"If 'cedar asthma,' the shingle weaver's occupational disease, does not get him, the steel will. Sooner or later he reaches over a little too far, the whirling blade tosses drops of deep red into the air, and a finger, a hand, or part of an arm comes sliding down the slick chute.\"\n\nThe first attempt to unionize shingle weavers came in Michigan in 1886. This union lasted only a few years, due in large measure to the industry moving westward to the new state of Washington, which entered the United States as the 42nd state in November 1889.\n\nIn the Pacific Northwest, shingle weavers began to unionize as early as 1890, when the shingle weavers of the Puget Sound area of Western Washington banded together to establish the West Coast Shingle Weavers’ Union. In short order, locals of the union were established in Ballard, Tacoma, Snohomish, Arlington, Chehalis, and Sedro-Wooley. This first foray into craft unionism proved to be short-lived, however, as an ill-timed strike crushed by an economic downturn in 1893 effectively put an end to the organizing effort.\n\nIn 1901 a more successful attempt at unionization of the shingle makers' trade was made with the establishment of the International Shingle Weavers of America.\n\nIn January 1903 a newspaper called \"The Shingle Weaver\" was established in Ballard as the official journal of the new union. The paper later moved to Everett, Washington and finally to Seattle during the course of its decade of publication before changing its name to \"The Timber Worker\" in February 1913.\n\nIn 1915, a wage cut for shingle weavers in the mills of Everett, Washington began a process of events which led to a strike the following year by the radical union the Industrial Workers of the World and its suppression by force and violence. On November 5, 1916, events culminated in a pitched gun battle known to history as the \"Everett massacre,\" in which 5 strikers and 2 so-called \"citizen deputies\" were killed and approximately 45 others wounded.\n\n"}
{"id": "902982", "url": "https://en.wikipedia.org/wiki?curid=902982", "title": "Slug (unit)", "text": "Slug (unit)\n\nThe slug is a derived unit of mass in the weight-based system of measures, most notably within the British Imperial measurement system and in the United States customary measures system. Systems of measure either define mass and derive force or define a base force and derive a mass unit. A slug is defined as the mass that is accelerated by 1 ft/s when a force of one pound (lbf) is exerted on it.\n\nOne slug has a mass of based on standard gravity, the international foot, and the avoirdupois pound. At the Earth's surface, an object with a mass of 1 slug exerts a force downward of approximately 32.2 lbf or .\n\nThe \"slug\" is part of a subset of units known as the gravitational FPS system, one of several such specialized systems of mechanical units developed in the late 19th and the 20th century. \"Geepound\" was another name for this unit in early literature.\n\nThe name \"slug\" was coined before 1900 by British physicist Arthur Mason Worthington, but it did not see any significant use until decades later. A 1928 textbook says:\n\nThe slug is listed in the Regulations under the Weights and Measures (National Standards) Act, 1960. This regulation defines the units of weights and measures, both regular and metric, in Australia.\n\nThe \"blob\" is the inch version of the slug (1blob is equal to 1 lbf⋅s/in, or 12slugs) or equivalent to . This unit is also called \"slinch\" (a portmanteau of the words slug and inch). Similar terms include \"slugette\" and \"snail\".\n\nSimilar metric units include the \"glug\" in the centimetre–gram–second system, and the \"mug\", \"par\", or \"MTE\" in the metre–kilogram–second system.\n\n"}
{"id": "1060920", "url": "https://en.wikipedia.org/wiki?curid=1060920", "title": "Specific weight", "text": "Specific weight\n\nThe specific weight (also known as the unit weight) is the weight per unit volume of a material. The symbol of specific weight is γ (the Greek letter Gamma).\n\nA commonly used value is the specific weight of water on Earth at 4°C which is 9.807 kN/m or 62.43 lbf/ft.\nThe terms \"specific gravity\", and less often \"specific weight\", are also used for relative density.\n\nwhere\n\nUnlike density, specific weight is not absolute. It depends upon the value of the gravitational acceleration, which varies with location. A significant influence upon the value of specific gravity is the temperature of the material. Pressure may also affect values, depending upon the bulk modulus of the material, but generally, at moderate pressures, has a less significant effect than the other factors. \n\nIn fluid mechanics, specific weight represents the force exerted by gravity on a unit volume of a fluid. For this reason, units are expressed as force per unit volume (e.g., N/m or lb/ft). Specific weight can be used as a characteristic property of a fluid. \n\nSpecific weight is often used as a property of soil to solve earthwork problems.\n\nIn soil mechanics, specific weight may refer to:\n\nwhere\n\nThe formula for dry unit weight is:\nwhere\n\n\nThe formula for saturated unit weight is:\nwhere\n\n\nThe formula for submerged unit weight is:\nwhere\n\nSpecific weight can be used in civil engineering and mechanical engineering to determine the weight of a structure designed to carry certain loads while remaining intact and remaining within limits regarding deformation.\n\n\n"}
{"id": "550622", "url": "https://en.wikipedia.org/wiki?curid=550622", "title": "Tetraquark", "text": "Tetraquark\n\nA tetraquark, in particle physics, is an exotic meson composed of four valence quarks. In principle, a tetraquark state may be allowed in quantum chromodynamics, the modern theory of strong interactions. Any established tetraquark state would be an example of an exotic hadron which lies outside the quark model classification.\n\nSeveral tetraquark candidates have been reported by particle physics experiments in the 21st century. The quark contents of these states are almost all qQ, where q represents a light (up, down or strange) quark, Q represents a heavy (charm or bottom) quark, and antiquarks are denoted with an overline. The existence and stability of tetraquark states with the qq (or QQ) have been discussed by theoretical physicists for a long time, however these have not been yet reported by experiments. \n\nIn 2003 a particle temporarily called X(3872), by the Belle experiment in Japan, was proposed to be a tetraquark candidate, as originally theorized. The name X is a temporary name, indicating that there are still some questions about its properties to be tested. The number following is the mass of the particle in .\n\nIn 2004, the D(2632) state seen in Fermilab's SELEX was suggested as a possible tetraquark candidate.\n\nIn 2007, Belle announced the observation of the Z(4430) state, a tetraquark candidate. There are also indications that the Y(4660), also discovered by Belle in 2007, could be a tetraquark state.\n\nIn 2009, Fermilab announced that they have discovered a particle temporarily called Y(4140), which may also be a tetraquark.\n\nIn 2010, two physicists from DESY and a physicist from Quaid-i-Azam University re-analyzed former experimental data and announced that, in connection with the (5S) meson (a form of bottomonium), a well-defined tetraquark resonance exists.\n\nIn June 2013, the BES III experiment in China and the Belle experiment in Japan independently reported on Z(3900), the first confirmed four-quark state.\n\nIn 2014, the Large Hadron Collider experiment LHCb confirmed the existence of the Z(4430) state with a significance of over 13.9 σ.\n\nIn February 2016, the DØ experiment reported evidence of a narrow tetraquark candidate, named X(5568), decaying to .\nIn December 2017, DØ also reported observing the X(5568) using a different final state.\nHowever, it was not observed in searches by the LHCb, CMS, CDF, or ATLAS experiments.\n\nIn June 2016, LHCb announced the discovery of three additional tetraquark candidates, called X(4274), X(4500) and X(4700).\n\n\n"}
{"id": "26155150", "url": "https://en.wikipedia.org/wiki?curid=26155150", "title": "Virtual Valve Amplifier", "text": "Virtual Valve Amplifier\n\nA Virtual Valve Amplifier (VVA) is software algorithm designed and sold by Diamond Cut Productions, Inc. for simulating the sound of various valve amplifier designs. It can be found within their DC8 and Forensics8 software programs.\n\nA VVA can be used to color the sound of a digital recording by adding \"tube-warmth\" or \"fat-bass\" in addition to adding subtle harmonics to enhance very old or muffled recordings. The algorithms behind a VVA are based on real vacuum tube circuits and non-linearities, mathematically simulating the large-signal transfer functions of various vacuum tubes and output transformers found in amplifier designs. A majority of this data was originally derived from extensive bench measurements on real vacuum tube amplifier circuits under varying operating conditions by engineers Craig Maier and Rick Carlson in the early 1990s. A VVA is a direct mathematical reconstruction of the same signal passing through a physical electron tube amplifier. The VVA algorithm can be found in the Diamond Cut DC8 and DC Forensics8 software packages. It is also sold as a VST plug-in.\n\nVVA designs generally include a number of parameters that may be configured to change the sound and operating characteristics of the amplifier design:\n\nHistorically referred to as the \"Q\" or bias point by engineers, the operating point of a vacuum tube is a condition generally fixed by the amplifier manufacturer. In general, the operating point determines the device's bias value at zero signal input and determines the distribution of harmonics introduced into the output of the amplifier. Tubes that operate with a higher operating point close to cutoff give more \"headroom\", enabling greater volume gains to be applied before signal degradation in the form of \"breakup\" or saturation results. By contrast, a lower operating point introduces more harmonic distortion into the final output as a result of the different non-linearity distribution near cut-off as compared to operation in the nominally linear portion of the characteristic. Some guitar amplifiers are designed to produce this type of distortion as its sound effect is considered desirable.\n\nThis describes how loudly the \"physical\" equivalent of the virtual valve amplifier is set. However, the output level of a VVA generally remains constant independent of drive due to internal gain compensation algorithms. Instead, the drive determines the amount of distortion that can be introduced into the output signal. As such, the Drive of a VVA describes the degree of modulation applied to a given vacuum tube amplifier circuit centered about the set operating point. The higher the drive level setting, the greater will be the production of predominantly even order harmonics due to the circuit's asymmetrical non-linearity. As a result, the VVA \"effect\" increases with increasing drive.\n\nThis is a high-mu dual triode that is generally incorporated into an RC coupled class A audio pre-amplifier configuration and its design is optimized to minimize harmonic distortion. This vacuum tube is still the industry standard pre-amplifier valve. It has a relatively flat linear operating region in the middle of its dynamic operating range, producing relatively lower levels of distortion compared to some of the other devices listed here. But, by moving the Operating Point to either the saturation or cutoff extreme, more \"tube-warmth\" effect can be produced by this device. Some VST effects emulate this tube.\n\nThe 12AT7 high-mu dual triode was designed primarily for RF mixing applications where it was incorporated into the oscillator/mixer stage and used to heterodyne incoming RF signals with the local oscillator to create an intermediate frequency in TV and FM sets. Thus, it is intentionally designed to be extremely non-linear. Thus, circuits based around the 12AT7 exhibit a larger degree of non-linearity throughout the entire dynamic operating range, including the middle. As a result, it produces greater even-order harmonic distortion (less objectionable than odd-order distortion).\n\nThe 12AU7 is a medium-mu dual triode often found in the driver / phase inverter stage of a push-pull power amplifier and also results in significant non-linearity in the middle of its dynamic operating curve.\n\nThe 6EJ7 pentode and its equivalents are often found in high-gain vacuum tube microphone amplifiers which require the sharp cutoff of a pentode. It generally produces a very pleasant \"tube-warmth\" effect when the operating point is properly set. This device is the same as the European type EF183.\n\nThe 6267 / EF86 pentode was a vacuum tube well suited for use in low-level pre-amplifiers where low noise and minimal microphonics were important. Its high-gain characteristics and family of operating curves make for useful harmonic distortion and signal compression properties.\n\nCircuits may be \"single-ended\", using a single output device (or several connected in parallel), or \"push-pull\", using paired devices configured to cancel out even-order distortion products and reduce output transformer magnetisation. Bias of a push-pull amplifier may be set to make both sides conduct at all times (amplifier class A), to make only one side conduct at a time (class B), or intermediate (class AB). Class A uses more power for the same output (i.e., is less efficient), can produce less output power from the same devices, and produces lower distortion, than classes AB and B.\n\nThese devices generally consisted of a 12AU7 medium-mu triode driving a single 6L6GC beam power pentode audio output valve or similar. Its effects are distinctive due to convolution of the non-linearity of the triode interacting with those of the pentode, with both devices operating in class-A mode. The 6L6GC is similar in performance to the industrial type 5881, and also the European type KT66.\n\nOften consisting of a 12AU7 phase inverter / driver, pushing a pair of 6L6GC beam power pentodes. The symmetrical push-pull circuit cancels out and reduces even-order distortion products compared with a single-ended circuit. The operating point is hard-wired and cannot be adjusted.\n\nAlso known as a \"retro–triode\" amplifier, it was invented in the 1930s and incorporated a directly heated cathode resulting in a high power output. It was often used in theatrical applications and public address systems. This vacuum amplifier typically exhibits a more linear output transfer characteristic than its pentode push-pull counterpart and as a result produces a characteristic clean sound. The particular devices used to create the 2A3 VVA models were of the \"dual–plate\" variety taken from unused stock manufactured for the military by RCA Victor in 1953. This configuration is favoured by many jazz musicians including Les Paul who reportedly used this amplifier configuration to cut all the records made from his home studio.\n\nThis is a single ended class A vacuum tube power amplifier implemented using the 2A3 power triode. It exhibits reasonably good linearity with dominant even distortion products.\n\nBecause of the non-linear properties and distortion products of vacuum tubes and their associated amplification circuits, they are useful in the simulation of a vacuum tube rectifier (6X4) to produce harmonics. Asymmetry between the positive- and negative-going transfer function establishes the relationship between the degree of even and odd harmonics produced.\n\n"}
{"id": "6797112", "url": "https://en.wikipedia.org/wiki?curid=6797112", "title": "WISE-Paris", "text": "WISE-Paris\n\nWISE-Paris is the \"World Information Service on Energy\", established in 1983 because of the perceived lack of independent and reliable information in France concerning energy systems and policy, in particular in the nuclear field.\n\nWise-Paris archives include several thousand national and international studies, technical, scientific and annual reports, tens of thousands of conference papers, newspaper and magazine articles from the general and technical press, and access to over a thousand electronic databases.\n\n\n"}
{"id": "25676443", "url": "https://en.wikipedia.org/wiki?curid=25676443", "title": "Yellow River oil spill", "text": "Yellow River oil spill\n\nThe Yellow River oil spill was an oil spill in the Yellow River in Shaanxi, China which took place due to the rupturing of a segment of Lanzhou-Zhengzhou oil pipeline on December 30, 2009. Approximately of diesel oil flowed down the Wei River before finally reaching the Yellow River, the source of drinking water for millions of people, on January 4, 2010.\n\nThe Lanzhou-Zhengzhou oil pipeline project was approved in 2007 and opened for operation in March 2009. It is a part of the -long Lanzhou-Zhengzhou-Changsha pipeline with a capacity of transporting 15 million tons of oil per year. According to China National Petroleum Corporation (CNPC), the rupture took place on December 30, 2009 due to an accident near Weinan where construction was underway by third-party workers. However, the incident was not publicized until January 3, 2010. The deputy director of the Yellow River Water Resources Commission called for an investigation into the accident, refuting CNPC's claim that the accident was caused by third-party construction workers.\n\nAbout 150,000 litres of diesel had already leaked out before the pipeline was closed by CNPC. According to Pacific Environment China's co-director Wen Bo, 700 workers were quickly mobilized by the government to control the spill as soon as its occurrence became known. They dug diversion channels and built floating dams to stop the pollutant from advancing further downstream. Solidifying chemicals were also used to remove the fuel from the stream. Despite these efforts, officials found traces of diesel in the Yellow River on January 4, 2010, 200 km upstream of Zhengzhou. No pollution was detected downstream of Sanmenxia, although the Sanmenxia reservoir was found to contain traces of toxic diesel. Sanmenxia Dam and its six hydroelectric generators were shut down to prevent the flow of pollutant further downstream. On 5 January, Zhang Xun, an official of the Ministry of Environmental Protection confirmed that the spill had been contained in the Sanmenxia reservoir and was no longer a threat to the river water downstream.\n\n"}
