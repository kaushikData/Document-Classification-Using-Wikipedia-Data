{"id": "1922602", "url": "https://en.wikipedia.org/wiki?curid=1922602", "title": "2004 Christmas Eve United States winter storm", "text": "2004 Christmas Eve United States winter storm\n\nThe 2004 Christmas Eve United States winter storm was a rare weather event that took place in Louisiana and Texas in the United States on December 24, 2004, before the storm moved northeast to affect the coastal sections of the Mid-Atlantic states and New England in the succeeding few days. This was a different storm from the historic event that struck the Midwest and southern Canada around December 23 from another cyclone which preceded this storm. The event involved a thin band of snowfall with unusually cold temperatures for the middle Texas coast, and caused dozens of varied weather records to be shattered. It was the most significant snow for the Texas Gulf Coast, and deep South Texas, since February 1895.\n\nThere had been indications for up to a week before the event that a frontal wave in the Gulf of Mexico was expected to track far enough to the south (along roughly the 26th parallel) to lead to snow along the Gulf of Mexico coastline of the United States.\n\nA surface cyclone formed in the western Gulf of Mexico on December 24 and December 28 due to a shortwave aloft, and moved eastward through the western Gulf of Mexico, bringing banded snowfall to the middle Texas coast. The extratropical cyclone moved east, then northeast, tracking across the northern peninsula of Florida early on December 26 before moving about 100 miles (200 km) offshore the Southeast, paralleling the coast. Continuing to deepen, the developing storm moved about 200 miles (300 km) offshore the Mid-Atlantic states, New England, and Atlantic Canada on December 27 before moving out to sea.\n\nThe most noticeable, and unusual, event associated with the storm was the snowfall it produced. Much of the snow fell in southern Texas, along the coast of the Gulf of Mexico, but some snow, albeit less deep, fell across southwestern and southeastern Louisiana. Any snowfall in these areas is extremely unusual, perhaps occurring once every twenty years, and these events are usually airborne flurries which melt on contact with the ground. In many places the snow stuck to the ground and accumulated to an appreciable depth. In Brownsville, Texas, snow fell to a depth of , the first measurable snowfall at the city in years, since the Great Blizzard of 1899.\nThe fact that the snow accumulated overnight on Christmas Eve led to a White Christmas the next morning, something completely foreign to the region. Across all of southern Texas and in southwestern Louisiana, snow fell in places where it had not for anywhere from 15 to 120 years. Near the coast, in Corpus Christi, Texas, of snow fell, more snow than in all previous recorded years combined. This was also the case in Victoria, Texas, where a significant fell. New Orleans, Louisiana had its first white Christmas in 50 years. In addition to the unusual occurrence of snow inland, moderate to heavy snow was also reported over the open waters of the Gulf of Mexico. This is the first significant snow fall in Houston since February 12, 1960, when a snowstorm hit central and south Texas with eight to 10 inches of snow.\nSome snow totals:\n\n\nAs the cyclone tracked through Florida offshore the Southeast, up to an inch of freezing rain and sleet fell at Augusta and Aiken.\n\nSeveral inches of snow fell across portions of the state, with the highest amount noted of at Ahoskie.\n\nSeveral locations across the Tidewater reported over a foot of snow, with the highest amount of reported at Quinby and Tabb. Frontogenesis (strengthening temperature gradient) within the comma head of the extratropical cyclone between the 500 hPa and 700 hPa layers (or 10-20 kft) contributed to the banded snow seen across this region. It led to the snowiest December across the Norfolk area since 1958.\n\nLight to moderate snow fell on the Eastern Shore, with the highest amount of measured at Shelltown.\n\nOn December 25, 2004, there was light to moderate snows fall across portions of the state, with the highest amount of falling at Mount Holly.\n\nSoutheastern sections of the state saw the snowfall. The highest amount reported was at East Hampton.\n\nModerate to heavy snow fell across much of the state. The highest total reported was at East Killingly.\n\nHeavy snow fell statewide. The highest total was at Tiverton.\n\nThe heaviest snowfall from the storm fell across Massachusetts. Brewster measured during the event.\n\nCentral and southern sections of the state saw moderate to heavy snow. The highest amount was at Salem.\n\nSnow fell across portions of the state during this storm, with falling at a dairy farm in Franklin.\n\nHeavy snow fell across portions of Maine. The highest amount reported was from Whiting where was measured.\n\n\n"}
{"id": "39345897", "url": "https://en.wikipedia.org/wiki?curid=39345897", "title": "Area Defense Anti-Munitions", "text": "Area Defense Anti-Munitions\n\nArea Defense Anti-Munitions (ADAM) is an experimental short range ground-to-air anti-missile weapons system being developed by Lockheed Martin. It uses a 10 kW fiber laser to attack its targets.\n\n"}
{"id": "3393482", "url": "https://en.wikipedia.org/wiki?curid=3393482", "title": "Asaluyeh", "text": "Asaluyeh\n\nAsalouyeh (, also Romanized as ‘Asalūyeh; also known as Asalu, and sometimes prefixed by bandar, meaning port) is a city and capital of Asaluyeh County, in Bushehr Province, Iran. At the 2006 census, its population was 4,746, in 875 families.\n\nAlternate spellings include: Assalouyeh, Asalouyeh, Asalouyeh, Asaluyeh, Asaloyeh, Asalooyeh, Asaluye, Assaluyeh, Asalu. \nUNCTAD codes: IR YEH, IR PGU, IR ASA.\n\nLocated on the shore of the Persian Gulf some 270 km SE of the provincial capital of Bushehr, it is best known as the site for the land based facilities of the huge PSEEZ (Pars Special Energy Economic Zone) project. The town itself is of minor significance, although it is common practice to refer to PSEEZ (established 1998) and Asaluyeh town collectively as Asaluyeh.\n\nAsalouyeh was chosen as the site of the PSEEZ facilities due to it being the closest land point to the largest natural gas field in the world, the South Pars / North Dome Gas-Condensate field. In addition, an existing airport and direct access to international waters via a deep water port were already present.\n\nThe PSEEZ (Pars Special Energy/Economic Zone) as it is known has been allocated 100 square kilometres of land at Asaluyeh for the various complexes and facilities. The site is a collection of different plants and refineries (known as \"phases\") and is administered by the PSEEZ agency onsite.\n\nA total of 27 phases are envisaged (12 gas, 15 petrochemical), plus a mix of light and heavy industry, and associated support facilities such as factories and warehouses. The scale of the project is huge. Some 28 refineries and 25 petrochemical complexes are scheduled to be established in Asalouyeh, in southern Bushehr province. Of these, 10 refineries and 7 petrochemical complexes were already operational in 2009.\n\nPSEEZ is a workers town - tourism is non-existent. Currently there are no hotels in Assalouyeh, although plans exist to construct a hotel between the airport and the Industrial area. Once completed, the Sadaf International Hotel will consist of 400 rooms in three buildings, on 1 square kilometre of land. A visa is required to enter Iran for most nationalities, and is therefore needed to visit the PSEEZ. The only exception to the visa requirement is if you visit Kish Island, which allows visitors to enter visa-free.\n\nAs of August 2005, US$20 billion of foreign money had been invested in PSEEZ since 1997. According to Iran's oil ministry, sales of products from PSEEZ could be as much as $11 billion per year, over 30 years.\n\nPrivate companies are also constructing a business park and warehouses.\n\nInternet access and telephony facilities are also available via a Tehran based Internet Service Provider, Pars Online. No other ISP's are present at PSEEZ.\nIn 2010, a large power plant with 1,000 megawatts output was inaugurated in Asalouyeh, aiming to supply electricity to the refineries of South Pars gas field phases 9, 10, 15, 16, 17, and 18.\n\nThe Pars Special Economic Energy Zone (PSEEZ) is a special economic zone. PSEEZ was established in 1998 for the utilization of South Pars oil and gas resources. Goods can be brought in duty-free, but cannot leave the PSEEZ and enter the rest of Iran. This is to encourage construction within and development of the PSEEZ. The private sector has invested some $1.5 billion in the PSEEZ during the past 4 years (2009). Foreign entities have invested some $36 billion in the Pars Special Economic Energy Zone (PSEEZ) during the past 10 years (2009).\n\nAsalouyeh city is surrounded by PSEEZ, but is still geographically separate and a few minutes drive from the nearest PSEEZ facility. Before the arrival of PSEEZ, the primary industry was fishing, albeit a very small one. Asalouyeh was a sleepy coastal hamlet, on a narrow strip of land between the Persian Gulf and the Zagros Mountains. Poverty existed in the small town, and it remains. Today it is the most knows akak shops. In 2006 part of the town's seafront was given a promenade with some landscaping and decoration.\n\nThe PSEEZ is one of the busiest ongoing construction sites in the world. At any one time up to 60,000 workers are onsite, mostly employed in construction of further gas and petrochemical refineries.\n\nConstruction of a new airport began in 2003. The new Persian Gulf International Airport (IATA code: YEH) opened to traffic in July 2006, replacing the original temporary airport. The volume of construction works for facilities such as runways and aircraft parking area, fence, and patrol roads and access routes to the airport's boulevard, construction of terminals with Jetways to handle 750,000 passengers per year. Control tower and technical buildings as well as supporting buildings such as fire station, meteorology section and so on have been started. Airport characteristics are the runway with length of 4000 meters asphalt paved, width of 45 meters with two 7.5m shoulders along the runway. Aircraft parking area is 330 × 150 Sqm concrete paved. Airport area and peripheral buildings will be constructed on 18 hectares of land. Iranian specialists are undertaking execution works for this airport and installation of the airport's navigation and lighting systems will be performed by foreign companies.\n\nLogistic Port: \nLogistic port covers 150 hectares: The western breakwater is 2300 meters long and the eastern breakwater is 1000 meters long. The port basin is 100 hectares with capacity to accept ships of up to 80,000 tons deadweight. The nominal capacity of this port is 10 million tons per year including 1 million tons of granulated sulphur, 3 million of container products and 6 million tons of refinery machinery and parts. \nThis port will have at least 10 berths that can simultaneously accommodate 10 ships and especially for exporting of sulphur, containers loading / unloading as heavy cargo. Water drought along the berths is at least 11 meters and at most 15 meters and the jetty in this port is 2600 meters long. \nAt present, 5 berths are operational. All of the equipment needed by refineries and petrochemical industries were unloaded in this very port and installed on sites. \nPetrochemical Port: \nThe petrochemical port with 15 berths and draught of 15 meters can accommodate ships of capacity 80,000 tons deadweight that especially carry gas and petrochemical products. The nominal capacity of this port is 35 million tons per year and it is predicted that 26 million tons of petrochemical products in liquid from will be exported through this port. \nIn accordance with the operations time schedule for the Zone's petrochemical projects, execution operations on this port will be completed in three phases; phase 1 will be completed by 2005 and the project's completion date will be year 2006.\n\nEach of the phases of the South Pars project is estimated to have an average capital spend of around US$1.5bn, and most will be led by foreign oil firms working in partnership with local companies. The government of President Ahmadinejad, who came to power in 2005, has favoured local firms over foreign companies in the energy and other sectors. In 2010, Iran awarded $21 billion of contracts to local companies to develop six stages (phases 13, 14, 19, 22, 23 and 24) of the South Pars gas field. Khatam al-Anbiya Construction Headquarters, the engineering arm of the Revolutionary Guard Corps, is part of the domestic group, as is Oil Industries Engineering and Construction and Iran Marine Industrial Co. The group also includes Iran Shipbuilding and Offshore Industries Complex Co., Industrial Development and Renovation Organization of Iran, and National Iranian Drilling Co. Managing the projects by Iranian companies does not rule out the participation of foreign firms in South Pars projects.\nAny visitor to Asaluyeh will immediately notice the series of Gas and Petrochemical complexes running along the coast, one of the largest collection of such facilities in the world. The various plants and complexes currently run for some 12 km, and more are being constructed. A series of gas flares which line the facility are immediately obvious, including one enormous flare in particular, with flames of almost 100 m in height. This flare is visible far out to sea. The area also hosts the world's largest aromatic production plant, Noori Petrochemical Complex, with an annual capacity of 4.2 million tons. Once completed, South Pars development revenue will exceed current oil exports incomes.\n\nGas production at South Pars rose by nearly 30 per cent between March 2009 and March 2010. The field's reserves are estimated at 14 trillion cubic meters of gas and of gas condensates. Production at South Pars gas field will rise to 175 million cubic meters per day in 2012.\n\n\nThe field is located in the Persian Gulf.\n\nThe field is the biggest gas field in the world, shared between Iran and Qatar, which contains gas in place and of condensate in place in both parts.\n\nThe South Pars Field is the name of northern part, which is located in Iranian waters and the North Dome is the name of southern part, which is located in Qatari waters. South Pars Field was discovered in 1990 by NIOC.\n\nProduction started from the southern extension of the field, the North Dome in 1989, at daily gas production rate of 800 mmscf/d.\n\nGas production started from South Pars field by commissioning the development phase 2 in December 2002 to produce 1 bscf/d of wet gas.\n\nThe field consists of two independent gas-bearing formations, Kangan and Upper Dalan. Each formation is divided into two different reservoir layers, separated by impermeable barriers. Therefore, the field consists of four independent reservoir layers K1, K2, K3, and K4.\n\nIranian sections contains of gas in place and around of recoverable gas. It covers an area of and is located 3 km below the seabed at a water depth of 65 m. The Iranian side accounts for 10% of the world's and 60% of Iran's total gas reserves.\n\nThe field is planned to be developed in around 30 phases, each of which will require an initial investment of around $1bn.\n\nDue to the availability of Petrochemicals and their by-products, 10 square kilometres at PSEEZ has been allocated for related industries, such as:\n\nConstruction of two water desalination plants each with a capacity of 10,000 cubic per day started in March 2005. The project was finished 10 months ahead of its contractual 16 months estimate, making it a landmark in project execution in the region. \nThe creation of a network for transferring water distribution network in different sites is underway.\n\nIn view of the Zone's policy on environmental protection and alleviation of industrial pollutions, first steps have been taken in the form of 1000 hectares of urban green space and 3000 hectares allocated to forestation and expansion of the mangrove trees in the tidal area of the sea. In this way, 10% of the total industrial site and 28% of the entire Zone will be transformed into green space.\n\nIndividual gas and petrochemical plants at Asoluyeh, when constructed, usually involve a partnership with a foreign company. The government of President Ahmadinejad, who came to power in 2005, has favoured local firms over foreign companies in the energy and other sectors. Due to the embargo in place against Iran by the United States, it is mostly European and Asian companies becoming involved.\n\nThese include:\n\nHowever, the US company Halliburton and American Allied International Corporation, have contracts for and business dealing with South Pars via subsidiaries.\n\nPlans for LNG refining and export have been discussed - but it remains to be seen how this will be accomplished, as most LNG refining technology is US based and therefore subject to export control due to US sanctions.\n\n\n"}
{"id": "52657328", "url": "https://en.wikipedia.org/wiki?curid=52657328", "title": "Bayesian model of computational anatomy", "text": "Bayesian model of computational anatomy\n\nComputational anatomy (CA) is a discipline within medical imaging focusing on the study of anatomical shape and form at the visible or gross anatomical scale of morphology. \nThe field is broadly defined and includes foundations in anatomy, applied mathematics and pure mathematics, including medical imaging, neuroscience, physics, probability, and statistics. It focuses on the anatomical structures being imaged, rather than the medical imaging devices. \nThe central focus of the sub-field of computational anatomy within medical imaging is mapping information across anatomical coordinate systems most often dense information measured within a magnetic resonance image (MRI). The introduction of flows into CA, which are akin to the equations of motion used in fluid dynamics, exploit the notion that dense coordinates in image analysis follow the Lagrangian and Eulerian equations of motion. In models based on Lagrangian and Eulerian flows of diffeomorphisms, the constraint is associated to topological properties, such as open sets being preserved, coordinates not crossing implying uniqueness and existence of the inverse mapping, and connected sets remaining connected. The use of diffeomorphic methods grew quickly to dominate the field of mapping methods post Christensen's\noriginal paper, with fast and symmetric methods becoming available.\n\nThe central statistical model of Computational Anatomy in the context of medical imaging has been the source-channel model of Shannon theory; the source is the deformable template of images formula_1, the channel outputs are the imaging sensors with observables formula_2 (see Figure). The importance of the source-channel model is that the variation in the anatomical configuration are modelled separated from the sensor variations of the Medical imagery. The Bayes theory dictates that the model is characterized by the prior on the source, formula_3 on formula_4, and the conditional density on the observable\n\nconditioned on formula_6.\n\nIn deformable template theory, the images are linked to the templates, with the deformations a group which acts on the template;\nsee group action in computational anatomy\nFor image action formula_7, then the prior on the group formula_8 induces the prior on images formula_9, written as densities the log-posterior takes the form\n\nThe random orbit model which follows specifies how to generate the group elements and therefore the random spray of objects which form the prior distribution.\n\nThe random orbit model of Computational Anatomy first appeared in modelling the change in coordinates associated to the randomness of the group acting on the templates, which induces the randomness on the source of images in the anatomical orbit of shapes and forms and resulting observations through the medical imaging devices. Such a random orbit model in which randomness on the group induces randomness on the images was examined for the Special Euclidean Group for object recognition in which the group element\nformula_11 was the special Euclidean group in.\n\nFor the study of deformable shape in CA, the high-dimensional diffeomorphism groups used in computational anatomy are generated via smooth flows formula_12 which satisfy the Lagrangian and Eulerian specification of the flow fields satisfying the ordinary differential equation: \n\nwith formula_13 the vector fields on formula_14 termed the Eulerian velocity of the particles at position formula_15 of the flow. The vector fields are functions in a function space, modelled as a smooth Hilbert space with the vector fields having 1-continuous derivative . For formula_16, the inverse of the flow is given by\nand the formula_17 Jacobian matrix for flows in formula_18 given as formula_19\n\nTo ensure smooth flows of diffeomorphisms with inverse, the vector fields formula_14 must be at least 1-time continuously differentiable in space which are modelled as elements of the Hilbert space formula_21 using the Sobolev embedding theorems so that each element formula_22 has 3-square-integrable derivatives. Thus formula_21 embed smoothly in 1-time continuously differentiable functions. The diffeomorphism group are flows with vector fields absolutely integrable in Sobolev norm:\nwhere formula_24 with formula_25 a linear operator formula_26 defining the norm of the RKHS. The integral is calculated by integration by parts when formula_27 is a generalized function in the dual space formula_28.\n\nIn the random orbit model of computational anatomy, the entire flow is reduced to the initial condition which forms the coordinates encoding the diffeomorphism. From the initial condition formula_29 then geodesic positioning with respect to the Riemannian metric of Computational anatomy solves for the flow of the Euler-Lagrange equation.\nSolving the geodesic from the initial condition formula_29 is termed the Riemannian-exponential, a mapping formula_31 at identity to the group.\n\nThe Riemannian exponential satisfies formula_32 for initial condition formula_33, vector field dynamics formula_34,\n\nIt is extended to the entire group, formula_41\nDepicted in the accompanying figure is a depiction of the random orbits around each exemplar, formula_42, generated by randomizing the flow by generating the initial tangent space vector field at the identity formula_43, and then generating random object formula_44.\n\nShown in the Figure on the right the cartoon orbit, are a random spray of the subcortical manifolds generated by randomizing the vector fields formula_45 supported over the submanifolds.The random orbit model induces the prior on shapes and images formula_46 conditioned on a particular atlas formula_47. For this the generative model generates the mean field formula_48 as a random change in coordinates of the template according to formula_49, where the diffeomorphic change in coordinates is generated randomly via the geodesic flows.\n\nThe random orbit model induces the prior on shapes and images formula_46 conditioned on a particular atlas formula_51. For this the generative model generates the mean field formula_52 as a random change in coordinates of the template according to formula_53, where the diffeomorphic change in coordinates is generated randomly via the geodesic flows. The prior on random transformations formula_54 on formula_55 is induced by the flow formula_56, with formula_57 constructed as a Gaussian random field prior formula_58. The density on the random observables at the output of the sensor formula_59 are given by\n\nMaximum a posteriori estimation (MAP) estimation is central to modern statistical theory. Parameters of interest formula_61 take many forms including (i) disease type such as neurodegenerative or neurodevelopmental diseases, (ii) structure type such as cortical or subcorical structures in problems associated to segmentation of images, and (iii) template reconstruction from populations. Given the observed image formula_62, MAP estimation maximizes the posterior:\n\nThis requires computation of the conditional probabilities formula_64. The multiple atlas orbit model randomizes over the denumerable set of atlases formula_65. The model on images in the orbit take the form of a multi-modal mixture distribution\n\nThe conditional Gaussian model has been examined heavily for inexact matching in dense images and for alndmark matching.\n\nModel formula_67 as a conditionally Gaussian random field conditioned, mean field, formula_68. For uniform variance the endpoint error terms plays the role of the log-conditional (only a function of the mean field) giving the endpoint term:\nModel formula_69 as conditionally Gaussian with mean field formula_70, constant noise variance independent of landmarks. The log-conditional (only a function of the mean field) can be viewed as the endpoint term:\n\nThe random orbit model for multiple atlases models the orbit of shapes as the union over multiple anatomical orbits generated from the group action of diffeomorphisms, formula_72, with each atlas having a template and predefined segmentation field formula_73. incorporating the parcellation into anatomical structures of the coordinate of the MRI.. The pairs are indexed over the voxel lattice formula_74 with an MRI image and a dense labelling of every voxel coordinate.The anatomical labelling of parcellated structures are manual delineations by neuroanatomists.\n\nThe Bayes segmentation problem is given measurement formula_75 with mean field and parcellation formula_76, the anatomical labelling formula_77. mustg be estimated for the measured MRI image. The mean-field of the observable formula_62 image is modelled as a random deformation from one of the templates formula_79, which is also randomly selected, formula_80. The optimal diffeomorphism formula_81 is hidden and acts on the background space of coordinates of the randomly selected template image formula_82. Given a single atlas formula_83, the likelihood model for inference is determined by the joint probability formula_84; with multiple atlases, the fusion of the likelihood functions yields the multi-modal mixture model with the prior averaging over models.\n\nThe MAP estimator of segmentation formula_85 is the maximizer formula_86 given formula_62, which involves the mixture over all atlases.\n\nThe quantity formula_89 is computed via a fusion of likelihoods from multiple deformable atlases, with formula_90 being the prior probability that the observed image evolves from the specific template image formula_91.\n\nThe MAP segmentation can be iteratively solved via the expectation-maximization(EM) algorithm\n\nGenerating templates empirically from populations is a fundamental operation ubiquitous to the discipline.\nSeveral methods based on Bayesian statistics have emerged for submanifolds and dense image volumes.\nFor the dense image volume case, given the observable formula_93 the problem is to estimate the template in the orbit of dense images formula_6. Ma's procedure takes an initial hypertemplate formula_95 as the starting point, and models the template in the orbit under the unknown to be estimated diffeomorphism formula_96, with the parameters to be estimated the log-coordinates formula_97 determining the geodesic mapping of the hyper-template formula_98.\n\nIn the Bayesian random orbit model of computational anatomy the observed MRI images formula_99 are modelled as a conditionally Gaussian random field with mean field formula_100, with formula_101 a random unknown transformation of the template. The MAP estimation problem is to estimate the unknown template formula_102 given the observed MRI images.\n\nMa's procedure for dense imagery takes an initial hypertemplate formula_95 as the starting point, and models the template in the orbit under the unknown to be estimated diffeomorphism formula_96. The observables are modelled as conditional random fields, formula_105 a random field with mean field formula_106. The unknown variable to be estimated explicitly by MAP is the mapping of the hyper-template formula_107, with the other mappings considered as nuisance or hidden variables which are integrated out via the Bayes procedure. This is accomplished using the expectation-maximization (EM) algorithm.\n\nThe orbit-model is exploited by associating the unknown to be estimated flows to their log-coordinates formula_108 via the Riemannian geodesic log and exponential for computational anatomy the initial vector field in the tangent space at the identity so that formula_109, with formula_110 the mapping of the hyper-template.\nThe MAP estimation problem becomes\n\nThe EM algorithm takes as complete data the vector-field coordinates parameterizing the mapping, formula_108 and compute iteratively the conditional-expectation\n"}
{"id": "9212193", "url": "https://en.wikipedia.org/wiki?curid=9212193", "title": "Bow shock (aerodynamics)", "text": "Bow shock (aerodynamics)\n\nA bow shock, also called a detached shock or normal shock, is a curved, stationary shock wave that is found in a supersonic flow past a finite body. The name comes from the example of a bow wave that forms at the bow of a ship when it moves through the water.\n\nUnlike an oblique shock, the bow shock is not necessarily attached to the tip of the body. Oblique shock angles are limited in formation and are based on the flow deflection angle, upstream Mach number. When these limitations are exceeded (greater deflection angle or lower Mach number), a detached bow shock forms instead of an oblique shock. As bow shocks form for high flow deflection angles, they are often seen forming around blunt objects.\nIn other words, when the needed rotation of the fluid exceeds the maximum achievable rotation angle for an oblique attached shock, the shock detaches from the body. Downstream of the shock, the flow-field is subsonic, and the boundary condition can be respected at the stagnation point.\n\nThe bow shock significantly increases the drag in a vehicle traveling at a supersonic speed. This property was utilized in the design of the return capsules during space missions such as the Apollo program, which need a high amount of drag in order to slow down during atmospheric reentry.\n\nBow shocks only occur when the local angle of the leading edge of the body exceeds the critical angle below which oblique shocks occur.The flow across the bow shock is non isentropic.\n\n\n"}
{"id": "3216500", "url": "https://en.wikipedia.org/wiki?curid=3216500", "title": "Capacitively coupled plasma", "text": "Capacitively coupled plasma\n\nA capacitively coupled plasma (CCP) is one of the most common types of industrial plasma sources. It essentially consists of two metal electrodes separated by a small distance, placed in a reactor. The gas pressure in the reactor can be lower than atmosphere or it can be atmospheric. \n\nA typical CCP system is driven by a single radio-frequency (RF) power supply, typically at 13.56 MHz. One of two electrodes is connected to the power supply, and the other one is grounded. As this configuration is similar in principle to a capacitor in an electric circuit, the plasma formed in this configuration is called a capacitively coupled plasma. \n\nWhen an electric field is generated between electrodes, atoms are ionized and release electrons. The electrons in the gas are accelerated by the RF field and can ionize the gas directly or indirectly by collisions, producing secondary electrons. When the electric field is strong enough, it can lead to what is known as electron avalanche. After avalanche breakdown, the gas becomes electrically conductive due to abundant free electrons. Often it accompanies light emission from excited atoms or molecules in the gas. When visible light is produced, plasma generation can be indirectly observed even with bare eyes.\n\nA variation on capacitively coupled plasma involves isolating one of the electrodes, usually with a capacitor. The capacitor appears like a short circuit to the high frequency RF field, but like an open circuit to direct current (DC) field. Electrons impinge on the electrode in the sheath, and the electrode quickly acquires a negative charge (or self-bias) because the capacitor does not allow it to discharge to ground. This sets up a secondary, DC field across the plasma in addition to the alternating current (AC) field. Massive ions are unable to react to the quickly changing AC field, but the strong, persistent DC field accelerates them toward the self-biased electrode. These energetic ions are exploited in many microfabrication processes (see reactive-ion etching (RIE)) by placing a substrate on the isolated (self-biased) electrode.\n\nCCPs have wide applications in the semiconductor processing industry for thin film deposition (see sputtering, plasma-enhanced chemical vapor deposition (PECVD)) and etching.\n\n"}
{"id": "18314422", "url": "https://en.wikipedia.org/wiki?curid=18314422", "title": "Catalpic acid", "text": "Catalpic acid\n\nCatalpic acid is a conjugated polyunsaturated fatty acid. The melting point of this fatty acid is 32 °C. Catalpic acid occurs naturally in the seeds of \"Catalpa ovata\" and Southern Catalpa (\"Catalpa bignonioides\"). Seeds of \"Catalpa\" species contain about 40% catalpic acid.\n"}
{"id": "3291073", "url": "https://en.wikipedia.org/wiki?curid=3291073", "title": "Combined steam and gas", "text": "Combined steam and gas\n\nCombined steam and gas (COSAG) is a propulsion system for ships using a combination of steam turbines and gas turbines to power the shafts. A gearbox and clutches enable either of the engines or both of them together to drive the shaft. It has the advantage of the cruising efficiency and reliability of steam and the rapid acceleration and start-up time of gas. This system was mainly used on first-generation gas-turbine ships such as the Royal Navy's \"County\" class destroyer and \"Tribal\" class frigate.\n\nThe Spanish aircraft carrier Dédalo also used it.\n"}
{"id": "10477381", "url": "https://en.wikipedia.org/wiki?curid=10477381", "title": "Daily cover", "text": "Daily cover\n\nDaily cover is the name given to the layer of compressed soil or earth which is laid on top of a day's deposition of waste on an operational landfill site. The cover helps prevent the interaction between the waste and the air, reducing odors and enabling a firm base upon which vehicles may operate. Work at the Fresno Sanitary Landfill was instrumental in establishing the need and utility of daily cover.\n\nWhile soils are the traditional materials employed in daily cover, alternative options such as \"green waste\", mixtures of paper sludge, tire derived aggregate (TDA) and geosynthetic membranes have displayed mechanical characteristics desirable for daily cover. When compared to traditional soil layers, the paper sludge paste was 2–3 times lighter, at least two orders of magnitude more impermeable, and comparable in shear strength.\n\n"}
{"id": "43123622", "url": "https://en.wikipedia.org/wiki?curid=43123622", "title": "Dynamic shear rheometer", "text": "Dynamic shear rheometer\n\nA dynamic shear rheometer, commonly known as DSR, is used for research and development as well as for quality control in the manufacture of a wide range of materials. Dynamic shear rheometers have been used since 1993 when Superpave was used for characterising and understanding high temperature rheological properties of asphalt binders in both the molten and solid state and is fundamental in order formulate the chemistry and predict the end-use performance of these materials. \n\nThis is done by deriving the complex modulus (G*) from the storage modulus (elastic response, G') and loss modulus (viscous behaviour, G\") yielding G* as a function of stress over strain. It is used to characterize the viscoelastic behavior of asphalt binders at intermediate temperatures from 10° to 150 °C (50° to 302 °F).\n\n\n"}
{"id": "20351640", "url": "https://en.wikipedia.org/wiki?curid=20351640", "title": "Easy Being Green", "text": "Easy Being Green\n\nEasy Being Green is one of the largest energy efficiency operators in Australia. It was established in 2004 and have been responsible for organising mass consumer action on energy efficiency. As of 2013 the company had aided over 750,000 homes and businesses with energy efficiency. The company has managed to prevent more than 5 million tonnes of carbon pollution from entering the atmosphere through their various energy efficiency projects.\n\nEasy Being Green was founded in 2004 by Nic Frances to focus on residential energy efficiency in Australia. His work in the UK to provide assistance and employment opportunities to vulnerable people was acknowledged when he was awarded the MBE. In 1998, he emigrated from the UK to Australia. There, he led the Brotherhood of St Laurence until 2004. His work earned him an Australian Centenary Medal.\n\nFrom 2005 to 2007 Paul Gilding took over the running of Easy Being Green. Prior to this, Gilding served with Greenpeace between 1989 and 1994 as an Executive Director for Greenpeace International and Greenpeace Australia. Gilding helped build Easy Being Green to a successful business employing over 200 people. The company used carbon trading to drive mass consumer action on energy efficiency. In doing so it achieved 4,300,000 tonnes of CO2 reductions and established domestic energy efficiency as a mainstream consumer and policy priority opportunity in Australia.\nIn 2008 Easy Being Green was acquired by Jackgreen Ltd, Australia’s only renewable energy retailer. The business continued as Jackgreen’s energy efficiency arm, offering free residential energy efficiency services and upgrades. Easy Being Green in 2009 launched a major campaign on Solar hot water in which it offered a free system with an interest free green loan on the installation.\nIn 2010, the founder of Jackgreen Ltd, Andrew Randall, purchased the Easy Being Green assets and has been running it ever since. The company today continues as a leading Australian energy efficiency organization, with large residential energy efficiency programs, energy efficiency audits, green loans, a strong commercial efficient lighting operation and solar sales and installation.\n\nEasy Being Green have operated under the Victorian Energy Efficiency Target (VEET) Scheme since its commencement in 2009 and are still offering these upgrades. The scheme is planned to last until 2030 and is regulated under the Essential Services Commission (ESC). All retrofit upgrade installers are required to hold qualifications in \"Retrofitting Homes for Energy and Water Efficiency\" and \"Work Safely at Heights, or hold an A-Grade Electrical License.\" Products that Easy Being Green have been able to supply for free to homes across the Australian state of Victoria are:\nUnder the VEET Scheme, and as a major partner with Philips Lighting, Easy Being Green have been able to supply thousands of businesses in Victoria with energy efficient lighting upgrades either free of cost or at a heavily subsidised price depending on a number of eligibility criteria set by the Essential Services Commission. Although the Essential Services Commission frequently review these criteria Easy Being Green always aims to supply a lighting upgrade wherein the receiving business will have under a 1-year payback period on any investments made in to lighting upgrades. Products that Easy Being Green are able to supply to businesses across the Australian state of Victoria are, but not limited to:\n\nEasy Being Green have supplied Solar hot water and Solar PV since 2009 to thousands of homes and businesses Australia-wide, saving millions of dollars spent on non-renewable electricity and greenhouse gas emissions. As associate members of the Clean Energy Council (CEC), a not-for-profit company working closely with state and federal governments to further regulate the renewable energy industry, Easy Being Green \"only\" deal with supply and install of CEC-Approved products and CEC-Approved Installers. Lists of these approved products are publicly available from the Clean Energy Council's Solar Accreditation website. By choosing to only deal with CEC-Approved products and installers end-users can not only be sure that the products installed will be up to Australian Standards but also gain access to the Government Rebates, known as Small-Scale Technology Certificates (STCs).\n\nEasy Being Green have also supplied batteries to a large number of homes Australia-wide and have recently become a Tesla, Inc. Certified Installer of their new Powerwall 2 battery.\n"}
{"id": "36541651", "url": "https://en.wikipedia.org/wiki?curid=36541651", "title": "Ecovative Design", "text": "Ecovative Design\n\nEcovative Design LLC is a biomaterials company headquartered in Green Island, New York that provides sustainable alternatives to plastics and polystyrene foams for packaging, building materials and other applications by using mushroom technology.\n\nEcovative is developed from a university project of founders Eben Bayer and Gavin McIntyre. In their Inventor's Studio course at Rensselaer Polytechnic Institute taught by Burt Swersey, Eben and Gavin developed and then patented a method of growing a mushroom-based insulation, initially called Greensulate before founding Ecovative Design in 2007. In 2007 they were awarded $16,000 from the National Collegiate Inventors and Innovators Alliance.\n\nSince 2008, when they were awarded $700,000 first place in the Picnic Green Challenge the company has developed and commercialized production of a protective packaging called EcoCradle that is now used by Dell, Puma SE, and Steelcase. In 2010 they were awarded $180,000 from the National Science Foundation and in 2011 the company received investment from 3M New Ventures, The DOEN Foundation, and Rensselaer Polytechnic Institute allowing them to double their current staff of 25.\n\nIn spring 2012, Ecovative Design opened a new production facility and announced a partnership with Sealed Air to expand production of the packaging materials. In 2014 their material was used in a brick form in 'Hy-Fi', a tower displayed in New York by the Museum of Modern Art and they started selling 'grow-it-yourself' kits.\n\n'Mushroom materials' are a novel class of renewable biomaterial grown from fungal mycelium and low-value non-food agricultural materials using a patented process developed by Ecovative Design. After being left to grow in a former in a dark place for about five days during which time the fungal mycelial network binds the mixture, the resulting light robust organic compostable material can be used within many products, including building materials, thermal insulation panels and protective packaging.\n\nThe process uses an agricultural waste product such as cotton hulls, cleaning the material, heating it up, inoculating it to create growth of the fungal mycelium, growing the material for period of about five days, and finally heating it to make the fungus inert.\nDuring growth, the material's shape can be molded into various products including protective packaging, building products, apparel, car bumpers, or surfboards. \nThe environmental footprint of the products is minimized through the use of agricultural waste, reliance on natural and non-controlled growth environments, and home compostable final products. The founders intention is that this technology should replace polystyrene and other petroleum-based products that take many years to decompose, or never do so.\n\nA renewable and compostable replacement for polystyrene packaging, that is also referred to as 'EcoCradle.\n\nA natural and renewable replacement for engineered wood, formed from compressed mushroom material and requiring no numerical control. Architect David Benjamin of The Living, working with Evovative Design and Arup, built 'Hy-Fi', a temporary external exhibit at the Museum of Modern Art in New York City in 2014.\n\nAn insulation product is under development. Trials of 'Greensulate', a former product, were conducted at a Vermont school gym in May 2009. The product was later dropped when the company switched focus to the manufacture of protective packaging.\n\nEcovative offer a 'Grow-it-yourself' kit allowing people to create mushroom materials themselves, used to create products including lamp shades.\n\nWorking with the University of Aachen, Dutch designer Eric Klarenbeek's used 3D printing technology to gown a chair without using plastic, metal or wood.\n\nPopular Science featured the composite insulation in its 2009 Invention Awards. Season 6, Episode 8 (25:20) of , also featured the insulation as lab technicians tested the materials' flame resistant properties after finding particles on a victim's clothing. Packaging World magazine featured Ecovative on its July 2011 cover, suggesting that the company is poised to \"be a game changer in various industries.\"\nThe World Economic Forum also recognized Ecovative as a Technology Pioneer in 2011. Additionally, the founders were featured on the PBS show, Biz Kid$, in episode 209, \"The Green Economy & You.\" \n\nThe development of the material and processes has been supported by the Picnic Green Challenge, the Environmental Protection Agency, National Collegiate Inventors and Innovators Alliance (NCIIA), ASME, the National Science Foundation, NYSERDA, 3M New Ventures, The DOEN Foundation, Rensselaer Polytechnic Institute and a license agreement with Sealed Air. \nIn addition to an array of awards, Ecovative's materials have been extensively highlighted in Material ConneXion libraries around the world.\n"}
{"id": "31657084", "url": "https://en.wikipedia.org/wiki?curid=31657084", "title": "Energy in Singapore", "text": "Energy in Singapore\n\nEnergy in Singapore describes energy related issues in Singapore. Energy imports are about three times the primary energy. Oil imports in relation to the population is high.\n\nThe world's largest palm oil company, Wilmar International, is based in Singapore. A Finnish company operates the world's biggest palm oil based diesel plant in Singapore with 800,000 tonnes produced annually since the end of 2010.\n\nAccording to the IEA Singapore had no energy production in 2008. Energy imports increased 18.6% in 2008 compared to 2004. The primary energy declined by about one third in 2007-8 but during the same period energy imports increased. Energy import was about three times the total primary energy supply in 2008. Compared to the UK in 2008, per capita electricity consumption was 135% and per capita carbon dioxide emissions were 110%. (UK: 61.35 m people 372.19 TWh electricity, 510.63Mt CO emissions).\n\nThe use of energy (primary energy) in Singapore is only 1/3 of the imported energy.\n\nSingapore was the top 10th country in oil imports in 2008: 50 megatonnes. For comparison, oil imports in Spain were 77 megatonnes (the top 8th country, with a population of 45.59 million) and in Italy they were 73 megatonnes (the top 9th country, with a population of 59.89 million).\n\nThe biggest palm oil based diesel plant in the world, 800,000 t/a production, started operations in Singapore at the end of 2010 by Neste Oil from Finland. The plant requires almost a million tonnes of raw material annually from the oil palm \"Elaeis guineensis\", equivalent to 2,600–3,400 km oil palm plantation. \n\nGreenpeace demonstrated in November 2010 in Espoo Finland by hanging an orangutan puppet in front of Neste Oil, saying that Neste Oil endangers the rainforest ecosystem. According to UNEP the majority of new palm oil plantations take place in the rainforests. \n\nAccording to European Union studies the increased demand for palm oil inevitably leads to new plantations being established in the forests and peat land areas. Land use changes have large green house gas emissions making palm oil diesel much more harmful than petroleum in respect to global warming. According to Greenpeace the Neste Oil plant in Singapore made Finnish Neste Oil among the world's leading palm oil consumers leading to increased rain forest destruction.\n\nReducing emissions from deforestation and forest degradation (REDD) would be a way to mitigate climate change. According to UNEP the international REDD mechanism will be a key element of the post-2012 international climate change regime.\n\nJurong Port is building a 10MW solar installation on the roofs of its warehouses. The system is expected online by the end of 2015.\n\nWilmar International is listed in Singapore. Headed by Kuok Khoon Hong, it is the world's largest palm oil firm. Kuok was the third richest person in Singapore in 2009 with a net worth of $3.5 billion. According to [FinnWatch] in 2007 the Kuok family owned by Malaysian companies a biofuel plant in Indonesia (225 000 t/a). The Wilmar director Martua Sitorus ($3 billion net worth in 2009, 2nd richest in Indonesia) lived in Indonesia in 2009. \n\nIn July 2007 Friends of the Earth Netherlands and two Indonesian NGOs accused Wilmar of illegal forest clearances in West Kalimantan, inadequate Environmental Impact Assessments and clearing land outside its concessions. Wilmar denies the allegation. The report calls on Unilever, a major purchaser from Wilmar, to review its purchasing relationship with the company.\n\nSingapore was the 58th top carbon dioxide emitter per capita in the world.\n"}
{"id": "31979476", "url": "https://en.wikipedia.org/wiki?curid=31979476", "title": "Flood Forecasting Centre (UK)", "text": "Flood Forecasting Centre (UK)\n\nThe Flood Forecasting Centre (FFC) is a joint venture between the Environment Agency and the Met Office to provide improved flood risk guidance for England and Wales. The FFC is based in the Operations Centre at the Met Office headquarters in Exeter and is jointly staffed from both organisations.\n\nFollowing severe flooding across the UK in 2007 a review was commissioned by the government to see what lessons could be learned. Chaired by Sir Michael Pitt the review produced a number of recommendations which were published in June 2008, among them was the recommendation that the different agencies work more closely together to improve warnings services.\n\nThe FFC was officially opened on 21 April 2009 in London by Environment Minister Hilary Benn. Its role is to provide better advice to governments, local authorities, emergency responders and the general public via its parent organisations. It faced its first major test in November 2009 when severe flooding affected Northern England, in particular Cumbria and the town of Cockermouth. The Pitt Review progress report highlighted the accuracy of the advice issued ahead of this event. In April 2011 the FFC moved from central London to a permanent base within the Operations Centre at the Met Office HQ in Exeter. It currently provides a range of operational Hydrometeorology services across England and Wales and limited services for Scotland and Northern Ireland.\n\nThe FFC provides a range of services to a number of customer groups:\n\n\n"}
{"id": "26182638", "url": "https://en.wikipedia.org/wiki?curid=26182638", "title": "Hanul Nuclear Power Plant", "text": "Hanul Nuclear Power Plant\n\nThe Hanul Nuclear Power Plant (originally the Uljin NPP Korean: 울진원자력발전소) is a large nuclear power station in the Gyeongsangbuk-do province of South Korea. The facility has six pressurized water reactors (PWRs) with a total installed capacity of 5,881 MW. The first went online in 1988.\n\nIt is the third largest operational nuclear power plant in the world and the second largest in South Korea. The plant's name was changed from Uljin to Hanul in 2013.\n\nOn 4 May 2012, ground was broken for two new reactors, Shin (\"new\") Uljin-1 and -2 using APR-1400 reactors. The APR-1400 is a Generation III PWR design with a gross capacity of 1400 MW. It is the first to use Korean-made components for all critical systems. The reactors are expected to cost about 7 trillion won (US$6 billion), and to be completed by 2018.\n\n"}
{"id": "44979247", "url": "https://en.wikipedia.org/wiki?curid=44979247", "title": "High entropy alloys", "text": "High entropy alloys\n\nHigh-entropy alloys (HEAs) are substances that are constructed with equal or nearly equal quantities of five or more metals. These alloys are currently the focus of significant attention in materials science and engineering because they have potentially desirable properties. Prior to the existence of these substances, alloys consisted of only one or two base metals. For example, additional elements can be added to iron to improve its properties, thereby creating an iron based alloy. Hence, high entropy alloys are a novel material.\n\nFurthermore, research indicates that some HEAs have considerably better strength-to-weight ratios, with a higher degree of fracture resistance, tensile strength, as well as corrosion and oxidation resistance than conventional alloys. Although HEAs have existed since before 2004, research substantially accelerated in the 2010s.\n\nAlthough HEAs were described as early as 1981 and 1996, significant research interest did not develop until after independent 2004 papers by Jien-Wei Yeh and Brian Cantor, with Yeh's first paper on the topic published 2 months sooner. Yeh also coined the term \"high-entropy alloy\" when he attributed the high configurational entropy as the mechanism stabilizing the solid solution phase. Cantor, not knowing of Yeh's work, did not describe his alloy as a \"high-entropy\" alloy, but the base alloy he developed, equiatomic FeCrMnNiCo, has been the subject of considerable work in the field.\n\nBefore the classification of high entropy alloys and multi-component systems, nuclear science had highlighted a system that can now be classified a high entropy alloy: within nuclear fuels Mo-Pd-Rh-Ru-Tc particles form at grain boundaries and at fission gas bubbles. Understanding the behavior of these '5 metal particles' was of specific interest to the medical industry as Tc-99m is an important medical imaging isotope.\n\nThere is no universally agreed-upon definition of a HEA. Yeh originally defined HEAs as alloys containing at least 5 elements with concentrations between 5 and 35 atomic percent. Later research however, suggested that this definition could be expanded. Otto et al. suggested that only alloys that form a solid solution with no intermetallic phases should be considered true high-entropy alloys, as the formation of ordered phases decreases the entropy of the system. Some authors have described 4-component alloys as high-entropy alloys while others have suggested that alloys meeting the other requirements of HEAs, but with only 2–4 elements or a mixing entropy between R and 1.5R should be considered \"medium-entropy\" alloys.\n\nIn conventional alloy design, one primary element such as iron, copper, or aluminum is chosen for its properties. Then, small amounts of additional elements are added to improve or add properties. Even among binary alloy systems, there are few common cases of both elements being used in nearly-equal proportions such as Pb-Sn solders. Therefore, much is known from experimental results about phases near the edges of binary phase diagrams and the corners of ternary phase diagrams and much less is known about phases near the centers. In higher-order (4+ components) systems that cannot be easily represented on a 2-dimensional phase diagram, virtually nothing is known.\n\nGibbs' phase rule, formula_1, can be used to determine an upper bound on the number of phases that will form in an equilibrium system. In his 2004 paper, Cantor created a 20-component alloy containing 5 at% of Mn, Cr, Fe, Co, Ni, Cu, Ag, W, Mo, Nb, Al, Cd, Sn, Pb, Bi, Zn, Ge, Si, Sb, and Mg. At constant pressure, the phase rule would allow for up to 21 phases at equilibrium, but far fewer actually formed. The predominant phase was a face-centered cubic solid solution phase, containing mainly Fe, Ni, Cr, Co, and Mn. From that result, the FeCrMnNiCo alloy, which forms only a solid solution phase, was developed.\n\nThe Hume-Rothery rules have historically been applied to determine whether a mixture will form a solid solution. Research into high-entropy alloys has found that in multi-component systems, these rules tend to be relaxed slightly. In particular, the rule that solvent and solute elements must have the same crystal structure does not seem to apply, as Fe, Ni, Cr, Co, and Mn have 4 different crystal structures as pure elements (and when the elements are present in equal concentrations, there can be no meaningful distinction between \"solvent\" and \"solute\" elements).\n\nThe multi-component alloys Yeh developed also consisted mostly or entirely of solid solution phases, contrary to what had been expected from earlier work in multi-component systems, primarily in the field of metallic glasses. Yeh attributed this result to the high configurational, or mixing, entropy of a random solid solution containing numerous elements. Because formula_2, and the phase with the lowest Gibbs free energy of formation (ΔG) will be the phase formed at equilibrium, increasing ΔS (entropy) will increase the likelihood of a phase being stable. The mixing entropy for a random ideal solid solution can be calculated by:\nwhere R is the ideal gas constant, N is the number of components, and c is the atomic fraction of component i. From this it can be seen that alloys in which the components are present in equal proportions will have the highest entropy, and adding additional elements will increase the entropy. A 5 component, equiatomic alloy will have a mixing entropy of 1.61R.\nHowever entropy alone is not sufficient to stabilize the solid solution phase in every system. The enthalpy of mixing (ΔH), must also be taken into account. This can be calculated using:\nwhere formula_5 is the binary enthalpy of mixing for A and B. Zhang et al. found, empirically, that in order to form a complete solid solution, ΔH should be between -10 and 5 kJ/mol. In addition, Otto et al. found that if the alloy contains any pair of elements that tend to form ordered compounds in their binary system, a multi-component alloy containing them is also likely to form ordered compounds.\n\nBoth of the thermodynamic parameters can be combined into a single, unitless parameter Ω:\nwhere T is the average melting point of the elements in the alloy. Ω should be greater than or equal to 1.1 to promote solid solution development.\n\nThe atomic radii of the components must also be similar in order to form a solid solution. Zhang et al. proposed a parameter δ representing the difference in atomic radii:\nwhere r is the atomic radius of element i and formula_8. Formation of a solid solution phase requires a δ≤6.6%, but some alloys with 4%<δ≤6.6% do form intermetallics.\n\nFor those alloys that do form solid solutions, an additional empirical parameter has been proposed to predict the crystal structure that will form. If the average valence electron concentration (VEC) of the alloy is ≥8, the alloy will form a face-centered cubic (fcc) lattice. If the average VEC is <6.87, it will form a body-centered cubic (bcc) lattice. For values in between, it will form a mixture of fcc and bcc. VEC has also been used to predict the formation of σ-phase intermetallics (which are generally brittle and undesirable) in chromium and vanadium-containing HEAs.\n\nHigh-entropy alloys are mostly produced using distinct methods that depend on the initial phase - starting either from a liquid, solid, or gas state.\n\n\nOther HEAs have been produced by thermal spray, laser cladding, and electrodeposition.\n\nThe atomic-scale complexity presents additional challenges to computational modelling of high-entropy alloys. Thermodynamic modelling using the CALPHAD method requires extrapolating from binary and ternary systems. Most commercial thermodynamic databases are designed for, and may only be valid for, alloys consisting primarily of a single element. Thus, they require experimental verification or additional \"ab initio\" calculations such as density functional theory (DFT). However, DFT modeling of complex, random alloys has its own challenges, as the method requires defining a fixed-size cell, which can introduce non-random periodicity. This is commonly overcome using the method of \"special quasirandom structures,\" designed to most closely approximate the radial distribution function of a random system, combined with the Vienna Ab-initio Simulation Package. Using this method, it has been shown that results of a 4-component equiatomic alloy begins to converge with a cell as small as 24 atoms. The exact muffin-tin orbital method with the coherent potential approximation has also been employed to model HEAs. Other techniques include the 'multiple randomly populated supercell' approach, which better describes the random population of a true solid solution (although is far more computationally demanding). This method has also been used to model glassy/amorphous (including bulk metallic glasses) systems without a crystal lattice.\n\nFurther, modeling techniques are being used to suggest new HEAs for targeted applications. The use of modeling techniques in this 'combinatorial explosion' is necessary for targeted and rapid HEA discovery and application.\n\nSimulations have highlighted the preference for local ordering in some high entropy alloys and, when the enthalpies of formation are combined with terms for configurational entropy, transition temperatures between order and disorder can be estimated. - allowing one to understand when effects like age hardening and degradation of an alloy's mechanical properties may be an issue.\n\nThe crystal structure of HEAs has been found to be the dominant factor in determining the mechanical properties. bcc HEAs typically have high yield strength and low ductility and vice versa for fcc HEAs. Some alloys have been particularly noted for their exceptional mechanical properties. A refractory alloy, VNbMoTaW maintains a high yield strength (>) even at a temperature of , significantly outperforming conventional superalloys such as Inconel 718. However, room temperature ductility is poor, less is known about other important high temperature properties such as creep resistance, and the density of the alloy is higher than conventional nickel-based superalloys.\n\nCoCrFeMnNi has been found to have exceptional low-temperature mechanical properties and high fracture toughness, with both ductility and yield strength increasing as the test temperature was reduced from room temperature to . This was attributed to the onset of nanoscale twin boundary formation, an additional deformation mechanism that was not in effect at higher temperatures. As such, it may have applications as a structural material in low-temperature applications or, because of its high toughness, as an energy-absorbing material. However, later research showed that lower-entropy alloys with fewer elements or non-equiatomic compositions may have higher strength or higher toughness. No ductile to brittle transition was observed in the bcc AlCoCrFeNi alloy in tests as low as 77 K.\n\nAlCoCrCuFeNi was found to have a high fatigue life and endurance limit, possibly exceeding some conventional steel and titanium alloys. But there was significant variability in the results, suggesting the material is very sensitive to defects introduced during manufacturing such as aluminum oxide particles and microcracks.\n\nA single-phase nanocrystalline AlLiMgScTi alloy was developed with a density of 2.67 g cm and microhardness of 4.9 – 5.8 GPa, which would give it an estimated strength-to-weight ratio comparable to ceramic materials such as silicon carbide, though the high cost of scandium limits the possible uses.\n\nRather than bulk HEAs, small-scale HEA samples (e.g. NbTaMoW micro-pillars) exhibit extraordinarily high yield strengths of 4-10 GPa —one order of magnitude higher than that of its bulk form—and their ductility is considerably improved. Additionally, such HEA films show substantially enhanced stability for high-temperature, long-duration conditions (at 1,100 °C for 3 days). Small-scale HEAs combining these properties represent a new class of materials in small-dimension devices potentially for high-stress and high-temperature applications.\n\nCoCrCuFeNi is an fcc alloy that was found to be paramagnetic. But upon adding titanium, it forms a complex microstructure consisting of fcc solid solution, amorphous regions and nanoparticles of Laves phase, resulting in superparamagnetic behavior. High magnetic coercivity has been measured in a BiFeCoNiMn alloy. Superconductivity was observed in TaNbHfZrTi alloys, with transition temperatures between 5.0 and 7.3 K.\n\nThe high concentrations of multiple elements leads to slow diffusion. The activation energy for diffusion was found to be higher for several elements in CoCrFeMnNi than in pure metals and stainless steels, leading to lower diffusion coefficients.\nSome equiatomic multicomponent alloys have also been reported to\nshow good resistance to damage by energetic radiation\n\n"}
{"id": "22001461", "url": "https://en.wikipedia.org/wiki?curid=22001461", "title": "Hybrid renewable energy system", "text": "Hybrid renewable energy system\n\nHybrid renewable energy systems (HRES) are becoming popular as stand-alone power systems for providing electricity in remote areas due to advances in renewable energy technologies and subsequent rise in prices of petroleum products. A hybrid energy system, or hybrid power, usually consists of two or more renewable energy sources used together to provide increased system efficiency as well as greater balance in energy supply.\n\nFor example, let us consider a load of 100% power supply and there is no renewable system to fulfill this need, so two or more renewable energy system can be combined. For example, 60% from a biomass system, 20% from wind system and the remainder from fuel cells. Thus combining all these renewable energy systems may provide 100% of the power and energy requirements for the load, such as a home or business.\n\nAnother example of a hybrid energy system is a photovoltaic array coupled with a wind turbine. This would create more output from the wind turbine during the winter, whereas during the summer, the solar panels would produce their peak output. Hybrid energy systems often yield greater economic and environmental returns than wind, solar, geothermal or trigeneration stand-alone systems by themselves.\n\nCompletely Renewable Hybrid Power Plant (solar, wind, biomass, hydrogen)\nA hybrid power plant consisting of these four renewable energy sources can be made into operation by proper utilization of these resources in a completely controlled manner.\nHybrid Energy Europe-USA.\nCaffese in Europe introduce hybridizing HVDC transmission with Marine hydro pumped Energy Storage via elpipes. The project of Caffese is 3 marine big lakes producing 1800 GW and transmission with elpipes.\nA part 1200 GW produce water fuels-wind fuels-solar fuels 210 billion liter year. (IEEE Power and Engineering Society-General Meeting Feb.9.2011,Arpa-E,Doe USA,MSE Italy,European Commission-Energy-Caffese plan and ConsortiumHybrid renewable energy systems (HRES) are becoming popular as stand-alone power systems for providing electricity in remote areas due to advances in renewable energy technologies and subsequent rise in prices of petroleum products. A hybrid energy system, or hybrid power, usually consists of two or more renewable energy sources used together to provide increased system efficiency as well as greater balance in energy supply\n\nMost of us already know how a solar/wind/biomass power generating system works, all these generating systems have some or the other drawbacks, like Solar panels are too costly and the production cost of power by using them is generally higher than the conventional process, it is not available in the night or cloudy days. Similarly Wind turbines can’t operate in high or low wind speeds and Biomass plants collapse at low temperatures.\n\nSo if all the three are combined into one hybrid power generating system the drawbacks can be avoided partially/completely, depending on the control units.\nAs the one or more drawbacks can be overcome by the other, as in northern hemisphere it is generally seen that in windy days the solar power is limited and vice versa and in summer and rainy season the biomass plant can operate in a full flagged so the power generation can be maintained in the above stated condition. The cost of solar panel can be subsided by using glass lenses, mirrors to heat up a fluid, that can rotate the common turbine used by wind and other sources.\nNow the question arises what about the winter nights or cloudy winter days with very low wind speeds. Here comes the activity of the Hydrogen. As we know the process of electrolysis can produce hydrogen by breaking water into hydrogen and oxygen, it can be stored; hydrogen is also a good fuel and burns with oxygen to give water. Hydrogen can be used to maintain the temperature of the biomass reservoir in winter so that it can produce biogas in optimum amount for the power generation.\nAs stated above biogas is a good source in summer; in this period the solar energy available is also at its peak, so if the demand and supply is properly checked and calculated the excess energy can be used in the production of hydrogen and can be stored. In sunny, windy &hot day, the turbine operates with full speed as the supply is maximum, and this excess power can be consumed for the process of manufacturing hydrogen.\nIn winter, the power consumption is also low so the supply limit is low, and obtained with lesser consumption. Driving hybrid cars will disable this outcome.\n\n\nTo get constant power supply, the output of the renewables may be connected to the rechargeable battery bank and then to the load. If the load is alternating current (AC), then an inverter is used to convert the direct current (DC) supply from the battery to the AC load.\nConsideration about voltage transition among modules starting from Wind Generator,Battery Charger Controller and Inverter should be subject to voltage standard which mainly focus about voltage compatibility.\n\nThe key to cost reductions of this order is, of course, the right sort of support for innovation and development - something that has been lacking for the past and, arguably, is still only patchy at present. Research and development efforts in solar, wind, and other renewable energy technologies are required to continue for:\n\nEconomic aspects of these technologies are sufficiently promising to include them in developing power generation capacity for developing countries.\n\n\n"}
{"id": "21516642", "url": "https://en.wikipedia.org/wiki?curid=21516642", "title": "Insert (composites)", "text": "Insert (composites)\n\nInserts are pins, bolts, screws, joints and other structures that are used to transfer localized loads to a composite panel or to join two composite panels together. Metallic inserts are commonly used in the aerospace and marine industries to attach objects to sandwich composite panels.\n\nHere is some history as referenced in the forward of ASME B18.29.1. Helical coil screw thread inserts have been in use for many years. It was first invented in the 1930s and found initial acceptance in aircraft manufactured and serviced by the Allied Air Forces during World War II. Since that time, applications for helical coil inserts have come into broad usage in aerospace, automotive and industrial equipment. Usage originally included metric spark plus sizes that were delineated in Europe in the 1950s coming into inch using countries in the 1960s. ASME Subcommittee 29 of the B18 Committee put together the first version of a standard in 1993. In 2010, ANSI approved of recent revisions and ASME published a revision of B18.29.1 - Helical Coil Screw Thread Inserts - Free Running and Screw Locking (Inch Series).\n"}
{"id": "20775036", "url": "https://en.wikipedia.org/wiki?curid=20775036", "title": "Iron oxide cycle", "text": "Iron oxide cycle\n\nThe iron oxide cycle (FeO/FeO) is the original two-step thermochemical cycle proposed for use for hydrogen production.\nIt is based on the reduction and subsequent oxidation of iron ions, particularly the reduction and oxidation from Fe to Fe. The ferrites, or iron oxide, begins in the form of a spinel and depending on the reaction conditions, dopant metals and support material forms either Wüstites or different spinels.\n\nThe thermochemical two-step water splitting process uses two redox steps. The steps of solar hydrogen production by iron based two-step cycle are:\n\nWhere M can by any number of metals, often Fe itself, Co, Ni, Mn, Zn or mixtures thereof. \n\nThe endothermic reduction step (1) is carried out at high temperatures greater than 1400 C, though the \"Hercynite cycle\" is capable of temperatures as low as 1200 C. The oxidative water splitting step (2) occurs at a lower ~1000 C temperature which produces the original ferrite material in addition to hydrogen gas. The temperature level is realized by using geothermal heat from magma or a solar power tower and a set of heliostats to collect the solar thermal energy.\n\nLike the traditional iron oxide cycle, the hercynite is based on the oxidation and reduction of iron atoms. However unlike the traditional cycle, the ferrite material reacts with a second metal oxide, aluminum oxide, rather than simply decomposing. The reactions take place via the following two reactions:\n\nThe reduction step of the hercynite reaction takes place at temperature ~ 200 C lower than the traditional water splitting cycle (1200 C). This leads to lower radiation losses, which scale as temperature to the fourth power.\n\nThe advantages of the ferrite cycles are: they have lower reduction temperatures than other 2-step systems, no metallic gasses are produced, high specific H production capacity, non-toxicity of the elements used and abundance of the constituent elements.\n\nThe disadvantages of the ferrite cycles are: similar reduction and melting temperature of the spinels (except for the hercynite cycle as aluminates have very high melting temperatures), and slow rates of the oxidation, or water splitting, reaction.\n\n\n"}
{"id": "8084123", "url": "https://en.wikipedia.org/wiki?curid=8084123", "title": "Isle of Man Incinerator", "text": "Isle of Man Incinerator\n\nThe Isle of Man Incinerator was designed by Savage & Chadwick Architects and is notable for its unusual shape and design, the stack of which is designed to represent a Viking sail. SUEZ Recycling and Recovery UK was awarded the contract to design build and operate the incinerator by the Isle of Man Government. The incinerator is located on an old disused landfill and has a capacity to treat 60,000 tonnes of municipal waste in addition to clinical and animal waste. In order to accomplish this the facility actually incorporates two separate incinerators. The facility uses moving grate technology.\n\n\n"}
{"id": "8381275", "url": "https://en.wikipedia.org/wiki?curid=8381275", "title": "Javed Nasir", "text": "Javed Nasir\n\nLieutenant-General Javed Nasir (Urdu: جاويد ناصر;b. 1936) ), is a retired engineering officer in the Pakistan Army Corps of Engineers, who served as the of the Inter-Services Intelligence (ISI), appointed on 14 March 1992 until being forcefully removed from this assignment on 13 May 1993.\n\nAn educator and engineer by profession, Nasir gained national prominence as his role of bringing the unscattered mass of Afghan Mujahideen to agree to the power-sharing formula to form Afghan administration under President Mojaddedi in Afghanistan in 1992–93. Later, he played an influential and decisive role in the Bosnian war when he oversaw the covert military intelligence program to support the Bosnian Army against the Serbs, while airlifting the thousands of Bosnian refugees in Pakistan.\n\nJaved Nasir was born in Lahore, Punjab in India in 1936, and is of the Kashmiri descent. After his matriculation from local school in Lahore, Nasir joined the Pakistan Army and entered in the Pakistan Military Academy at Kakul in 1953. He decided to attend the Military College of Engineering at Risalpur in Khyber-Pakhtunkhwa, and graduated with B.S. with Honors in civil engineering in 1958. He gained commissioned as 2nd-Lt. in the Corps of Engineers of the Pakistan Army where his career is mostly spent.\n\nIn 1967, Nasir qualified as a licensed professional engineer (PE) by the Pakistan Engineering Council (PEC).\n\nHe was known to have served in the combat engineering formations during the second war with India in 1965 as army captain, and later served in the western front of the third war with India in 1971 as major.\n\nAfter the third war with India in 1971, Maj. Nasir went to Australia where he attended and graduated in staff course from the Australian Army Staff College. In 1980s, he was sent to attend the National Defence University (NDU) in Islamabad, and graduated with MSc in Strategic studies. In 1983–90, Major-General Nasir joined the faculty of the Armed Forces War College of the National Defense University (NDU), which he instructed courses on war studies for seven years, eventually promoted as chief instructor.\n\nIn the military circles, Maj-Gen. Nasir was described as a \"moderate person\" who rediscovered the Islam in 1986 during the midst of the Russian war in the neighboring Afghanistan. In 1988, Maj-Gen. Nasir gained public fame when he was appointed as inspector-general of engineering formation that investigated the environmental disaster befall at the military storage located in the Rawalpindi Cantonment. Against the United States, German and French military estimation, Maj-Gen. Nasir personally led his formation at the ground to clear out the entire storage containing the chemical and explosive materials, as well as the missile ordnance in mere two weeks.\n\nIn 1989, he was appointed as director-general of Frontier Works Organization (FWO) and supervised the civil construction of the Skardu International Airport that is above sea level.\nOn 24 September 1991, Maj-Gen. Nasir was promoted as a three-star rank army general, having appointed to command the Corps of Engineers as its Eng-in-C at the Army GHQ in Rawalpindi. On 4 February 1992, Lieutenant-General Nasir was then posted as the chairman of the Pakistan Ordnance Factories at Wah in Punjab, Pakistan, until being appointed as the of the Inter-Services Intelligence (ISI).\n\nOn 14 March 1992, Prime Minister Nawaz Sharif appointed Lt-Gen Nasir as the of the Inter-Services Intelligence (ISI) against the recommendations and wishes of General Asif Nawaz, then-chief of army staff (COAS). The appointment was seen a political motive for Prime Minister Nawaz Sharif since General Nasir had no experience in the intelligence gathering network and was virtually a ghost in country's intelligence community. At that time, Prime Minister Sharif had family relations with Lt-Gen. Nasir Javed, and knew him very well.\n\nIn the military, he was of the view of anti-American sentiments, accusing the United States of using the Islam for political reasons and against Russians in Europe, which further complicated the foreign relations between two nations. He also limited the cooperation between the ISI and CIA to fight against the global terrorism, thwarting any joint efforts to fight against extremism. Though, he did help the US to relocate and retrieve the missing guided missiles from Afghanistan based on a mutual understanding of such weapons may have fall in wrong hands.\n\nIt was during this time when ISI had been running an intensified support for insurgency in Indian Kashmir. In spite of his seniority in the military, Nasir was overlooked, and was never considered for the promotion of the four-star rank and appointment by the government during the appointmentment process for the command of the Army, the Chief of Army Staff (COAS). Lt-Gen Nasir was among the five senior and superseding army generals when the junior-most Lt-Gen Abdul Waheed Kakar was elevated to the four-star rank and promotion to command the army.\n\nOn April 1992, Lt-Gen. Nasir became an international figure when he played major role in amalgamating the unscattered Afghan mujahideen groups when the power-sharing formula was drafted. Due to his religiosity, Nasir used his persuasive power and motivational talks to agree to power-sharing formula and, witnessed to have successfully establishing an Afghan administration under cleric President Sibghatullah Mojaddedi in Kabul.\n\nIn the military and political circles, Nasir was had reputation to be a practising Muslim who would not compromise on the interests of Islam and Pakistan. In 1992–93, Nasir defied the UN arms embargo placed on Bosnia and Herzegovina when he successfully airlifted the POF's sophisticated anti-tank guided missiles, which ultimately turned the tide in favour of Bosnian Muslims and forced the Serbs to life the siege much to the annoyance of the U.S. government. While airlifting sophisticated anti-tank guided missiles to Bosnian Muslims, he pushed the Government of Pakistan to allow the Bosnian immigrationto Pakistan.\n\nIn 2011, the International Criminal Tribunal for the former Yugoslavia demanded the custody of the former ISI director for his alleged support of the Inter-Services Intelligence activities in Bosnia and Herzegovina to Muslim fighters of Bosnia against the Serbian army in the 1990s, the Government of Pakistan has refused to hand Nasir to the UN tribunal, citing poor health.\n\nIn 1993, Lt-Gen. Nasir's zealous religiosity and maverick actions became embarrassing for the Pakistani military, in which, the Chairman joint chiefs General Shamim Allam had completely lost the control of the ISI when the agency was running under Nasir's command. General Abdul Waheed Kakar, the army chief at that time, had been at odds with Lt-Gen. Nasir due to his preaching of Islamic tradition in the military. In the views of senior military officers in the Pakistani military and the civilian officials of the Ministry of Defence, Lt-Gen. Nasir was often a figure of fun whose intellect was far from being as outstanding as his white beard.\n\nDuring this time, the Indian government led by Prime Minister P. V. Narasimha Rao leveled several accusations on him of supporting the Khalistan movement, the Indian Mafia and Dawood Ibrahim– the accusations he swiftly denied in 2008. At home, Nasir begin facing accusation from Pakistan Peoples Party politicians of supporting the conservative Islamic agenda in the country.\n\nIn 1993, the United States formally registered their complaints to Pakistan when U.S. secretary of State James Baker written a memo to Prime Minister Sharif of putting his country on a terror watch list and was in danger of being listed as terror-supporting nation. Responding to the complain, Prime Minister Sharif used diplomacy when he sent his Foreign Secretary Shahryar Khan and Pakistan Senator Akram Zaki to United States of assuring Pakistan's policy of not supporting the militancy in the region.\n\nDuring this time, several Arab countries such as Egypt, Tunisia, Algeria, and the Philippines, lodged a strong protest against Nasir of supporting the radical movements in their respected countries.\n\nFollowing the resignations of Prime Minister Sharif and President G.I. Khan on 18 July 1993, the caretaker Prime Minister M.A. Qureshi fired and sacked Nasir from the directorship of the ISI, and Acting President Wasim Sajjad approved his premature retirement from his military commission effective from on 13 May 1993— he only led the ISI for 13 months.\n\nUpon his firing, the new DG ISI, J.A. Qazi eventually led the massive arrests of thousands of Arab Afghans and forced the al-Qaeda to relocate itself in Afghanistan permanently. Expulsion from Pakistan, many escaped to Bosnia to participate in the war. According to many political commentators and journalists, Nasir's firing from ISI was not at the behest of the United States but, it was the friendly Arab countries' protests and pressure at the Organisation of Islamic Cooperation (OIC) that resulted in his departure from ISI and the retirement from his 40-year long service with the military.\n\nAfter his premature retirement, Nasir became a missionary for a Tablighi Jamaat, and went to the private sector when he managed and chaired the private equity firm and hedge fund, the Evacuee Trust Property Board (ETPB), when he was appointed on 14 July 1997 for a two-year contract.\n\nIn 1998, he was appointed as chairman of Sikh Gurdwara Prabandhak Committee, a pro-Sikhism organization in Pakistan promoting the religious activities of Sikhism. On October 1998, Prime Minister Sharif appointed him as his intelligence adviser but this appointment remained for short period of time. For sometimes, he served on the security details as head of security for the Sharif family, but the PML(N)'s lawmakers and Sharif family cut off their links and distance themselves from Nasir after the 9/11 terrorist attacks in the United States in 2001.\n\nIn 2002–03, Nasir filed a lawsuit at the Anti Terrorism Court against the media corporations: the \"Daily Jang\", and the \"News International\", for character defamation when investigatives articles published on him on the monetary embezzlement when he managed the private hedge fund in Lahore.\n\nIn 2008, he appeared on the Geo News and denied any allegations of terror supporting that was leveled on him during his time as Director ISI, when interviewed by Iftikhar Ahmad. At this detailed and hour-long interview, he was of the view of 9/11 was an inside job, and maintained on his stance on the suppressing of the free-energy by the U.S. and Pakistan Government.\n\nIn 2011, the International Criminal Tribunal for the former Yugoslavia demanded the custody of the Nasir for his alleged authorization of the covert program to support to the Bosnian Army against the Serbian army in the 1990s, the Government of Pakistan refused to hand Nasir to the UN tribunal, citing poor health and memory loss due to a road accident.\n\nIn 2013, Nasir reportedly spoke against Afghan Taliban and the terrorism at the Supreme Court convention, and criticised the Taliban as an armed violent group and criticised them of supporting the violent terrorism for their cause, in a response to the church bombing.\n\n\n"}
{"id": "17644264", "url": "https://en.wikipedia.org/wiki?curid=17644264", "title": "List of bamboo species", "text": "List of bamboo species\n\nBamboo is a group of woody perennial grasses in the true grass family Poaceae, which is a large family with over 10,000 species. In the tribe \"Bambuseae\" also known as bamboo, there are 91 genera and over 1,000 species. The size of bamboo varies from small annuals to giant timber bamboo. Bamboo evolved only 30 to 40 million years ago, after the demise of the dinosaurs. Bamboo is the fastest-growing woody plant in the world. It can grow up to 91–122 centimetres/day (3.8-5.0 centimetres/hr).\n\nBamboo species can be divided into clumpers and runners. Clumpers grow from the soil in a slowly expanding tuft. Runners send underground rhizomes to produce shoots several metres from the parent plant.\n\nAll Chinese names for bamboo contain the character \"竹\". This character by itself simply means bamboo, however it enters into hundreds of other words and phrases. \"Every day our written language reminds us of the antiquity of China's partnership with bamboo.\" (Dr. W. Y. Hsiung). This character, pronounced \"zhu\" depicts two leafed twigs of bamboo. The radical 竹 also indicates \"sense\".\n"}
{"id": "3223939", "url": "https://en.wikipedia.org/wiki?curid=3223939", "title": "List of energy resources", "text": "List of energy resources\n\nThese are modes of energy production, energy storage, or energy conservation, listed alphabetically. Note that not all sources are accepted as legitimate or have been proven to be tappable.\n\n\n\n\n"}
{"id": "2290742", "url": "https://en.wikipedia.org/wiki?curid=2290742", "title": "Luminophore", "text": "Luminophore\n\nA luminophore is an atom or functional group in a chemical compound that is responsible for its luminescent properties. Luminophores can be either organic or inorganic.\n\nLuminophores can be further classified as fluorophores or phosphors, depending on the nature of the excited state responsible for the emission of photons. However, some luminophores cannot be classified as being exclusively fluorophores or phosphors. Examples include transition-metal complexes such as tris(bipyridine)ruthenium(II) chloride, whose luminescence comes from an excited (nominally triplet) metal-to-ligand charge-transfer (MLCT) state, which is not a true triplet state in the strict sense of the definition; and colloidal quantum dots, whose emissive state does not have either a purely singlet or triplet spin.\n\nMost luminophores consist of conjugated π systems or transition-metal complexes. There are also purely inorganic luminophores, such as zinc sulfide doped with rare-earth metal ions, rare-earth metal oxysulfides doped with other rare-earth metal ions, yttrium oxide doped with rare-earth metal ions, zinc orthosilicate doped with manganese ions, etc. Luminophores can be observed in action in fluorescent lights, television screens, computer monitor screens, organic light-emitting diodes and bioluminescence.\n\nThe correct, textbook terminology is \"luminophore\", not \"lumophore\", although the latter term has been frequently used in the chemical literature.\n\n"}
{"id": "37626168", "url": "https://en.wikipedia.org/wiki?curid=37626168", "title": "Mechanochromic luminescence", "text": "Mechanochromic luminescence\n\nMechanochromic luminescence (ML) references to intensity and/or color changes of (solid-state) luminescent materials induced by mechanical forces, such as rubbing, crushing, pressing, shearing, or smearing. Unlike \"triboluminescence\" which does not require additional excitation source other than force itself, ML is often manifested by external photoexcitation such as a UV lamp. The most common cause of ML is related to changes of intermolecular interactions of dyes and pigments, which gives rise to various strong (exciton splitting) and/or weak (Forster) excited state interactions. For example, a certain boron complex of sunscreen compound avobenzone exhibits reversible ML. A recent detailed study suggests that ML from the boron complex consists of two critical coupled steps: 1) generation of low energy exciton trap via mechanical perturbation; and 2) exciton migration from regions where photoexcitation results in a higher excited state. Since solid-state energy transfer can be very efficient, only a small fraction of the low-energy exciton traps is required when mechanical force is applied. As a result, for crystalline ML materials, XRD measurement may not able to detect changes before and after mechanical stimuli while its photoluminescence can be quite different.\n"}
{"id": "5452870", "url": "https://en.wikipedia.org/wiki?curid=5452870", "title": "Microbial fuel cell", "text": "Microbial fuel cell\n\nA microbial fuel cell (MFC), or biological fuel cell, is a bio-electrochemical system that drives an electric current by using bacteria and mimicking bacterial interactions found in nature. MFCs can be grouped into two general categories: mediated and unmediated. The first MFCs, demonstrated in the early 20th century, used a mediator: a chemical that transfers electrons from the bacteria in the cell to the anode. Unmediated MFCs emerged in the 1970s; in this type of MFC the bacteria typically have electrochemically active redox proteins such as cytochromes on their outer membrane that can transfer electrons directly to the anode. In the 21st century MFCs started to find a commercial use in wastewater treatment.\n\nThe idea of using microbes to produce electricity was conceived in the early twentieth century. M. C. Potter initiated the subject in 1911. Potter managed to generate electricity from \"Saccharomyces cerevisiae\", but the work received little coverage. In 1931, Branet Cohen created microbial half fuel cells that, when connected in series, were capable of producing over 35 volts with only a current of 2 milliamps.\n\nA study by DelDuca et al. used hydrogen produced by the fermentation of glucose by \"Clostridium butyricum\" as the reactant at the anode of a hydrogen and air fuel cell. Though the cell functioned, it was unreliable owing to the unstable nature of hydrogen production by the micro-organisms. This issue was resolved by Suzuki et al. in 1976, who produced a successful MFC design a year later.\n\nIn the late 1970s little was understood about how microbial fuel cells functioned. The idea was studied by Robin M. Allen and later by H. Peter Bennetto. People saw the fuel cell as a possible method for the generation of electricity for developing countries. Bennetto's work, starting in the early 1980s, helped build an understanding of how fuel cells operate and he was seen by many as the topic's foremost authority.\n\nIn May 2007, the University of Queensland, Australia completed a prototype MFC as a cooperative effort with Foster's Brewing. The prototype, a 10 L design, converted brewery wastewater into carbon dioxide, clean water and electricity. The group had plans to create a pilot-scale model for an upcoming international bio-energy conference.\n\nA microbial fuel cell (MFC) is a device that converts chemical energy to electrical energy by the action of microorganisms. These electrochemical cells are constructed using either a bioanode and/or a biocathode. Most MFCs contain a membrane to separate the compartments of the anode (where oxidation takes place) and the cathode (where reduction takes place). The electrons produced during oxidation are transferred directly to an electrode or, to a redox mediator species. The electron flux is moved to the cathode. The charge balance of the system is compensated by ionic movement inside the cell, usually across an iconic membrane. Most MFCs use an organic electron donor that is oxidized to produce CO, protons and electrons. Other electron donors have been reported, such as sulphur compounds or hydrogen. The cathode reaction uses a variety of electron acceptors that includes the reduction of oxygen as the most studied process. However, other electron acceptors have been studied, including metal recovery by reduction, water to hydrogen, nitrate reduction and sulfate reduction.\n\nMFCs are attractive for power generation applications that require only low power, but where replacing batteries may be impractical, such as wireless sensor networks.\n\nVirtually any organic material could be used to feed the fuel cell, including coupling cells to wastewater treatment plants. Chemical process wastewater and synthetic wastewater have been used to produce bioelectricity in dual- and single-chamber mediatorless MFCs (uncoated graphite electrodes).\n\nHigher power production was observed with a biofilm-covered graphite anode. Fuel cell emissions are well under regulatory limits. MFCs use energy more efficiently than standard internal combustion engines, which are limited by the Carnot Cycle. In theory, an MFC is capable of energy efficiency far beyond 50%. Rozendal obtained energy conversion to hydrogen 8 times that of conventional hydrogen production technologies.\n\nHowever; MFCs can also work at a smaller scale. Electrodes in some cases need only be 7 μm thick by 2 cm long. such that an MFC can replace a battery. It provides a renewable form of energy and does not need to be recharged.\n\nMFCs operate well in mild conditions, 20 °C to 40 °C and also at pH of around 7. They lack the stability required for long-term medical applications such as in pacemakers.\n\nPower stations can be based on aquatic plants such as algae. If sited adjacent to an existing power system, the MFC system can share its electricity lines.\n\nSoil-based microbial fuel cells serve as educational tools, as they encompass multiple scientific disciplines (microbiology, geochemistry, electrical engineering, etc.) and can be made using commonly available materials, such as soils and items from the refrigerator. Kits for home science projects and classrooms are available. One example of microbial fuel cells being used in the classroom is in the IBET (Integrated Biology, English, and Technology) curriculum for Thomas Jefferson High School for Science and Technology. Several educational videos and articles are also available on the International Society for Microbial Electrochemistry and Technology (ISMET Society)\"\".\n\nThe current generated from a microbial fuel cell is directly proportional to the energy content of wastewater used as the fuel. MFCs can measure the solute concentration of wastewater (i.e., as a biosensor).\n\nWastewater is commonly assessed for its biochemical oxygen demand (BOD) values. BOD values are determined by incubating samples for 5 days with proper source of microbes, usually activated sludge collected from wastewater plants.\n\nAn MFC-type BOD sensor can provide real-time BOD values. Oxygen and nitrate are preferred electron acceptors over the electrode, reducing current generation from an MFC. MFC BOD sensors underestimate BOD values in the presence of these electron acceptors. This can be avoided by inhibiting aerobic and nitrate respiration in the MFC using terminal oxidase inhibitors such as cyanide and azide. Such BOD sensors are commercially available.\n\nThe United States Navy is considering microbial fuel cells for environmental sensors. The use of microbial fuel cells to power environmental sensors would be able to provide power for longer periods and enable the collection and retrieval of undersea data without a wired infrastructure. The energy created by these fuel cells is enough to sustain the sensors after an initial startup time. Due to undersea conditions (high salt concentrations, fluctuating temperatures and limited nutrient supply), the Navy may deploy MFCs with a mixture of salt-tolerant microorganisms. A mixture would allow for a more complete utilization of available nutrients. \"Shewanella oneidensis\" is their primary candidate, but may include other heat- and cold-tolerant \"Shewanella spp\".\n\nA first self-powered and autonomous BOD/COD biosensor has been developed and allows to detect organic contaminants in freshwater. The senor relies only on power produced by MFCs and operates continuously without maintenance. The biosensor turns on the alarm to inform about contamination level: the increased frequency of the signal informs about higher contamination level, while low frequency warns about low contamination level.\n\nIn 2010, A. ter Heijne et al. constructed a device capable of producing electricity and reducing Cu (II) (ion) to copper metal.\n\nMicrobial electrolysis cells have been demonstrated to produce hydrogen.\n\nMFCs are used in water treatment to harvest energy utilizing anaerobic digestion. The process can also reduce pathogens. However, it requires temperatures upwards of 30 degrees C and requires an extra step in order to convert biogas to electricity. Spiral spacers may be used to increase electricity generation by creating a helical flow in the MFC. Scaling MFCs is a challenge because of the power output challenges of a larger surface area.\n\nMost microbial cells are electrochemically inactive. Electron transfer from microbial cells to the electrode is facilitated by mediators such as thionine, methyl viologen, methyl blue, humic acid and neutral red. Most available mediators are expensive and toxic.\n\nMediator-free microbial fuel cells use electrochemically active bacteria to transfer electrons to the electrode (electrons are carried directly from the bacterial respiratory enzyme to the electrode). Among the electrochemically active bacteria are \"Shewanella putrefaciens\", \"Aeromonas hydrophila\" and others. Some bacteria are able to transfer their electron production via the pili on their external membrane. Mediator-free MFCs are less well characterized, such as the strain of bacteria used in the system, type of ion-exchange membrane and system conditions (temperature, pH, etc.)\n\nMediator-free microbial fuel cells can run on wastewater and derive energy directly from certain plants. This configuration is known as a plant microbial fuel cell. Possible plants include reed sweetgrass, cordgrass, rice, tomatoes, lupines and algae. Given that the power is derived from living plants (\"in situ\"-energy production), this variant can provide ecological advantages.\n\nOne variation of the mediator-less MFC is the microbial electrolysis cell (MEC). While MFCs produce electric current by the bacterial decomposition of organic compounds in water, MECs partially reverse the process to generate hydrogen or methane by applying a voltage to bacteria. This supplements the voltage generated by the microbial decomposition of organics, leading to the electrolysis of water or methane production. A complete reversal of the MFC principle is found in microbial electrosynthesis, in which carbon dioxide is reduced by bacteria using an external electric current to form multi-carbon organic compounds.\n\nSoil-based microbial fuel cells adhere to the basic MFC principles, whereby soil acts as the nutrient-rich anodic media, the inoculum and the proton exchange membrane (PEM). The anode is placed at a particular depth within the soil, while the cathode rests on top the soil and is exposed to air.\n\nSoils naturally teem with diverse microbes, including electrogenic bacteria needed for MFCs, and are full of complex sugars and other nutrients that have accumulated from plant and animal material decay. Moreover, the aerobic (oxygen consuming) microbes present in the soil act as an oxygen filter, much like the expensive PEM materials used in laboratory MFC systems, which cause the redox potential of the soil to decrease with greater depth. Soil-based MFCs are becoming popular educational tools for science classrooms.\n\nSediment microbial fuel cells (SMFCs) have been applied for wastewater treatment. Simple SMFCs can generate energy while decontaminating wastewater. Most such SMFCs contain plants to mimic constructed wetlands. By 2015 SMFC tests had reached more than 150 l.\n\nIn 2015 researchers announced an SMFC application that extracts energy and charges a battery. Salts dissociate into positively and negatively charged ions in water and move and adhere to the respective negative and positive electrodes, charging the battery and making it possible to remove the salt effecting \"microbial capacitive desalination\". The microbes produce more energy than is required for the desalination process.\n\nPhototrophic biofilm MFCs (ner) use a phototrophic biofilm anode containing photosynthetic microorganism such as chlorophyta,candyanophyta.They carry out photosynthesis and thus produce organic metabolites and donate electrons.\n\nOne study found that PBMFCs display a power density sufficient for practical applications.\n\nThe sub-category of phototrophic MFCs that use purely oxygenic photosynthetic material at the anode are sometimes called biological photovoltaic systems.\n\nThe United States Naval Research Laboratory developed nanoporous membrane microbial fuel cells that use a non-PEM to generate passive diffusion within the cell. The membrane is a nonporous polymer filter (nylon, cellulose, or polycarbonate). It offers comparable power densities to Nafion (a well known PEM) with greater durability. Porous membranes allow passive diffusion thereby reducing the necessary power supplied to the MFC in order to keep the PEM active and increasing the total energy output.\n\nMFCs that do not use a membrane can deploy anaerobic bacteria in aerobic environments. However, membrane-less MFCs experience cathode contamination by the indigenous bacteria and the power-supplying microbe. The novel passive diffusion of nanoporous membranes can achieve the benefits of a membrane-less MFC without worry of cathode contamination.\n\nNanoporous membranes are also eleven times cheaper than Nafion (Nafion-117, $0.22/cm vs. polycarbonate, <$0.02/cm).\n\nPEM membranes can be replaced with ceramic materials. Ceramic membrane costs can be as low as $5.66 /m. The macroporous structure of ceramic membranes allows good transport of ionic species.\n\nThe materials that have been successfully employed in ceramic MFCs are earthenware, alumina, mullite, pyrophyllite and terracotta.\n\nWhen microorganisms consume a substance such as sugar in aerobic conditions, they produce carbon dioxide and water. However, when oxygen is not present, they produce carbon dioxide, protons/hydrogen ions and electrons, as described below:\n\nCHO + 13HO → 12CO + 48H + 48e (\"Eqt. 1\")\n\nMicrobial fuel cells use inorganic mediators to tap into the electron transport chain of cells and channel electrons produced. The mediator crosses the outer cell lipid membranes and bacterial outer membrane; then, it begins to liberate electrons from the electron transport chain that normally would be taken up by oxygen or other intermediates.\n\nThe now-reduced mediator exits the cell laden with electrons that it transfers to an electrode; this electrode becomes the anode. The release of the electrons recycles the mediator to its original oxidised state, ready to repeat the process. This can happen only under anaerobic conditions; if oxygen is present, it will collect the electrons, as it has greater electronegativity.\n\nIn MFC operation, the anode is the terminal electron acceptor recognized by bacteria in the anodic chamber. Therefore, the microbial activity is strongly dependent on the anode's redox potential. A Michaelis-Menten curve was obtained between the anodic potential and the power output of an acetate-driven MFC. A critical anodic potential seems to provide maximum power output.\n\nPotential mediators include natural red, methylene blue, thionine and resorufin.\n\nOrganisms capable of producing an electric current are termed exoelectrogens. In order to turn this current into usable electricity, exoelectrogens have to be accommodated in a fuel cell.\n\nThe mediator and a micro-organism such as yeast, are mixed together in a solution to which is added a substrate such as glucose. This mixture is placed in a sealed chamber to stop oxygen entering, thus forcing the micro-organism to undertake anaerobic respiration. An electrode is placed in the solution to act as the anode.\n\nIn the second chamber of the MFC is another solution and the positively charged cathode. It is the equivalent of the oxygen sink at the end of the electron transport chain, external to the biological cell. The solution is an oxidizing agent that picks up the electrons at the cathode. As with the electron chain in the yeast cell, this could be a variety of molecules such as oxygen, although a more convenient option is a solid oxidizing agent, which requires less volume.\n\nConnecting the two electrodes is a wire (or other electrically conductive path). Completing the circuit and connecting the two chambers is a salt bridge or ion-exchange membrane. This last feature allows the protons produced, as described in \"Eqt. 1,\" to pass from the anode chamber to the cathode chamber.\n\nThe reduced mediator carries electrons from the cell to the electrode. Here the mediator is oxidized as it deposits the electrons. These then flow across the wire to the second electrode, which acts as an electron sink. From here they pass to an oxidizing material. Also the hydrogen ions/protons are moved from the anode to the cathode via a proton exchange membrane such as nafion. They will move across to the lower concentration gradient and be combined with the oxygen but to do this they need an electron. This forms current and the hydrogen is used sustaining the concentration gradient.\n\nAlgae Biomass has been observed to give high energy when used as substrates in microbial fuel cell.\n\n\n\n"}
{"id": "4649063", "url": "https://en.wikipedia.org/wiki?curid=4649063", "title": "Micromechanics", "text": "Micromechanics\n\nMicromechanics (or, more precisely, micromechanics of materials) is the analysis of composite or heterogeneous materials on the level of the individual constituents that constitute these materials.\n\nHeterogeneous materials, such as composites, solid foams, polycrystals, or bone, consist of clearly distinguishable constituents (or \"phases\") that show different mechanical and physical material properties. While the constituents can often be modeled through an isotropic behaviour, the microstructure characteristics (shape, orientation, varying volume fraction, ..) of heterogeneous material often leads to an anisotropic behaviour.\n\nAnisotropic material models are available for linear elasticity. In the nonlinear regime, the modeling is often restricted to orthotropic material models which does not capture the physics for all heterogeneous materials. Micromechanics goal is to predict the anisotropic response of the heterogeneous material on the basis of the geometries and properties of the individual phases, a task known as homogenization.\n\nAnother of micromechanics is that it allows to predict multi-axial properties that are often difficult to measure experimentally. A typical example is the out-of-plane properties for unidirectional composites.\n\nThe main advantage of micromechanics is to perform virtual testing in order to reduce the cost of an experimental campaign. Indeed, an experimental campaign of heterogeneous material is often expensive and involve a larger number of permutations : constituent material combinations; fiber and particle volume fractions; fiber and particle arrangements; and processing histories). Once the constituents properties are known, all these permutations can be simulated through virtual testing using micromechanics.\n\nThere is several ways to obtain the material properties of each constituents : by identifying the behaviour based on molecular dynamics simulation results; by identifying the behaviour through an experimental campaign on each constituents; by reverse engineering the properties through a reduced experimental campaign on the heterogeneous material. The latter option is typically used since some constituents are difficult to test, there is always some uncertainties on the real microstructure and it allows to take into account the weakness of the micromechanics approach into the constituents material properties. The obtained material models need to be validated through comparison with a different set of experimental data than the one use for the reverse engineering.\n\nThe key point of micromechanics of materials is the localization, which aims at evaluating the local (stress and strain) fields in the phases for given macroscopic load states, phase properties, and phase geometries. Such knowledge is especially important in understanding and describing material damage and failure.\n\nBecause most heterogeneous materials show a statistical rather than a deterministic arrangement of the constituents, the methods of micromechanics are typically based on the concept of the representative volume element (RVE). An RVE is understood to be a sub-volume of an inhomogeneous medium that is of sufficient size for providing all geometrical information necessary for obtaining an appropriate homogenized behavior.\n\nMost methods in micromechanics of materials are based on continuum mechanics rather than on atomistic approaches such as nanomechanics or molecular dynamics. In addition to the mechanical responses of inhomogeneous materials, their thermal conduction behavior and related problems can be studied with analytical and numerical continuum methods. All these approaches may be subsumed under the name of \"continuum micromechanics\".\n\nVoigt (1887) - Strains constant in composite, rule of mixtures for stiffness components.\n\nReuss (1929) - Stresses constant in composite, rule of mixtures for compliance components.\n\nStrength of Materials (SOM) - Longitudinally: strains constant in composite, stresses volume-additive. Transversely: stresses constant in composite, strains volume-additive.\n\nVanishing Fiber Diameter (VFD) - Combination of average stress and strain assumptions that can be visualized as each fiber having a vanishing diameter yet finite volume.\n\nComposite Cylinder Assemblage (CCA) - Composite composed of cylindrical fibers surrounded by cylindrical matrix layer, cylindrical elasticity solution. Analogous method for macroscopically isotropic inhomogeneous materials: Composite Sphere Assemblage (CSA)\n\nHashin-Shtrikman Bounds - Provide bounds on the elastic moduli and tensors of transversally isotropic composites (reinforced, e.g., by aligned continuous fibers) and isotropic composites (reinforced, e.g., by randomly positioned particles).\n\nSelf-Consistent Schemes - Effective medium approximations based on Eshelby's elasticity solution for an inhomogeneity embedded in an infinite medium. Uses the material properties of the composite for the infinite medium.\n\nMori-Tanaka Method - Effective field approximation based on Eshelby's elasticity solution for inhomogeneity in infinite medium. As is typical for mean field micromechanics models, fourth-order concentration tensors relate the average stress or average strain tensors in inhomogeneities and matrix to the average macroscopic stress or strain tensor, respectively; inhomogeneity \"feels\" effective matrix fields, accounting for phase interaction effects in a collective, approximate way.\n\nMost such micromechanical methods use periodic homogenization, which approximates composites by periodic phase arrangements. A single repeating volume element is studied, appropriate boundary conditions being applied to extract the composite's macroscopic properties or responses. The Method of Macroscopic Degrees of Freedom can be used with commercial FE codes, whereas analysis based on asymptotic homogenization typically requires special-purpose codes.\nThe Variational Asymptotic Method for Unit Cell Homogenization (VAMUCH) and its development, Mechanics of Structural Genome (see below), are recent Finite Element based approaches for periodic homogenization. \n\nIn addition to studying periodic microstructures, embedding models and analysis using macro-homogeneous or mixed uniform boundary conditions can be carried out on the basis of FE models. Due to its high flexibility and efficiency, FEA at present is the most widely used numerical tool in continuum micromechanics. Examples of the practical use of FEA in micromechanics can be found in many disciplines including biomaterials (bone, tendon), construction materials (concrete), and engineering materials (composite\nParallel finite element programs such as ParaFEM have enabled researchers to study very large volume elements derived from 3D tomography data. |\n\nA unified theory called mechanics of structure genome (MSG) has been introduced to treat structural modeling of anisotropic heterogeneous structures as special applications of micromechanics. Using MSG, it is possible to directly compute structural properties of a beam, plate, shell or 3D solid in terms of its microstructural details. \nThe companion code SwiftComp can be freely executed in the cloud at https://cdmhub.org/resources/scstandard.\n\nExplicitly considers fiber and matrix subcells from periodic repeating unit cell. Assumes 1st-order displacement field in subcells and imposes traction and displacement continuity. It was developed into the High-Fidelity GMC (HFGMC), which uses quadratic approximation for the displacement fields in the subcells.\n\nA further group of periodic homogenization models make use of Fast Fourier Transforms (FFT), e.g., for solving an equivalent to the Lippmann–Schwinger equation. FFT-based methods at present appear to provide the numerically most efficient approach to periodic homogenization of elastic materials.\n\nIdeally, the volume elements used in numerical approaches to continuum micromechanics should be sufficiently big to fully describe the statistics of the phase arrangement of the material considered, i.e., they should be Representative Volume Elements (RVEs).\nIn practice, smaller volume elements must typically be used due to limitations in available computational power. Such volume elements are often referred to as Statistical Volume Elements (SVEs). Ensemble averaging over a number of SVEs may be used for improving the approximations to the macroscopic responses..\n\n\n\n"}
{"id": "2227291", "url": "https://en.wikipedia.org/wiki?curid=2227291", "title": "Minister of Energy, Mines and Resources (Canada)", "text": "Minister of Energy, Mines and Resources (Canada)\n\nThe Minister of Energy, Mines, and Resources was a member of the Cabinet of Canada from 1966 to 1995.\n\nThe office of Minister of Mines and Technical Surveys (created in 1950) was abolished and the office of the Ministers of Energy, Mines and Resources created by statute 14-15 Eliz. II, c. 25, assented to on 16 June 1966 and proclaimed in force on 1 October 1966.\n\nAfter 1995, see Minister of Natural Resources.\n"}
{"id": "11275600", "url": "https://en.wikipedia.org/wiki?curid=11275600", "title": "National Gas Company of Trinidad and Tobago", "text": "National Gas Company of Trinidad and Tobago\n\nThe National Gas Company of Trinidad and Tobago Limited (NGC) is a state-owned natural gas company. It was created by the Government of Trinidad and Tobago in 1975. NGC is operating in the field of gas pipelines, industrial sites, gas production, port and marine infrastructure, natural gas liquids and liquefied natural gas. It has assets worth $43 billion. Its credit rating by Moody's is Baa2 and A- from S&P The company currently operates in Point Lisas, Couva at the Point Lisas Industrial Estate.\n\n\n"}
{"id": "21276", "url": "https://en.wikipedia.org/wiki?curid=21276", "title": "Neodymium", "text": "Neodymium\n\nNeodymium is a chemical element with symbol Nd and atomic number 60. It is a soft silvery metal that tarnishes in air. Neodymium was discovered in 1885 by the Austrian chemist Carl Auer von Welsbach. It is present in significant quantities in the ore minerals monazite and bastnäsite. Neodymium is not found naturally in metallic form or unmixed with other lanthanides, and it is usually refined for general use. Although neodymium is classed as a rare earth, it is a fairly common element, no rarer than cobalt, nickel, or copper, and is widely distributed in the Earth's crust. Most of the world's commercial neodymium is mined in China.\n\nNeodymium compounds were first commercially used as glass dyes in 1927, and they remain a popular additive in glasses. The color of neodymium compounds—due to the Nd ion—is often a reddish-purple but it changes with the type of lighting, due to the interaction of the sharp light absorption bands of neodymium with ambient light enriched with the sharp visible emission bands of mercury, trivalent europium or terbium. Some neodymium-doped glasses are also used in lasers that emit infrared with wavelengths between 1047 and 1062 nanometers. These have been used in extremely-high-power applications, such as experiments in inertial confinement fusion.\n\nNeodymium is also used with various other substrate crystals, such as yttrium aluminium garnet in the . This laser usually emits infrared at a wavelength of about 1064 nanometers. The Nd:YAG laser is one of the most commonly used solid-state lasers.\n\nAnother important use of neodymium is as a component in the alloys used to make high-strength neodymium magnets—powerful permanent magnets. These magnets are widely used in such products as microphones, professional loudspeakers, in-ear headphones, high performance hobby DC electric motors, and computer hard disks, where low magnet mass (or volume) or strong magnetic fields are required. Larger neodymium magnets are used in high-power-versus-weight electric motors (for example in hybrid cars) and generators (for example aircraft and wind turbine electric generators).\n\nNeodymium, a rare-earth metal, was present in the classical mischmetal at a concentration of about 18%. Metallic neodymium has a bright, silvery metallic luster, but as one of the more reactive lanthanide rare-earth metals, it quickly oxidizes in ordinary air. The oxide layer that forms then peels off, exposing the metal to further oxidation. Thus, a centimeter-sized sample of neodymium completely oxidizes within a year.\n\nNeodymium commonly exists in two allotropic forms, with a transformation from a double hexagonal to a body-centered cubic structure taking place at about 863 °C.\n\nNeodymium metal tarnishes slowly in air and it burns readily at about 150 °C to form neodymium(III) oxide:\n\nNeodymium is a quite electropositive element, and it reacts slowly with cold water, but quite quickly with hot water to form neodymium(III) hydroxide:\n\nNeodymium metal reacts vigorously with all the halogens:\n\nNeodymium dissolves readily in dilute sulfuric acid to form solutions that contain the lilac Nd(III) ion. These exist as a [Nd(OH)] complexes:\n\nNeodymium compounds include\n\n\nSome neodymium compounds have colors that vary based upon the type of lighting.\n\nNaturally occurring neodymium is a mixture of five stable isotopes, Nd, Nd, Nd, Nd and Nd, with Nd being the most abundant (27.2% of the natural abundance), and two radioisotopes, Nd and Nd. In all, 31 radioisotopes of neodymium have been detected , with the most stable radioisotopes being the naturally occurring ones: Nd (alpha decay with a half-life (\"t\") of 2.29×10 years) and Nd (double beta decay, \"t\" = 7×10 years, approximately). All of the remaining radioactive isotopes have half-lives that are shorter than eleven days, and the majority of these have half-lives that are shorter than 70 seconds. Neodymium also has 13 known meta states, with the most stable one being Nd (\"t\" = 5.5 hours), Nd (\"t\" = 5.5 minutes) and Nd (\"t\" ~70 seconds).\n\nThe primary decay modes before the most abundant stable isotope, Nd, are electron capture and positron decay, and the primary mode after is beta minus decay. The primary decay products before Nd are element Pr (praseodymium) isotopes and the primary products after are element Pm (promethium) isotopes.\n\nNeodymium was discovered by Baron Carl Auer von Welsbach, an Austrian chemist, in Vienna in 1885. He separated neodymium, as well as the element praseodymium, from a material known as didymium by means of fractional crystallization of the double ammonium nitrate tetrahydrates from nitric acid, while following the separation by spectroscopic analysis; however, it was not isolated in relatively pure form until 1925. The name neodymium is derived from the Greek words \"neos\" (νέος), new, and \"didymos\" (διδύμος), twin.\n\nDouble nitrate crystallization was the means of commercial neodymium purification until the 1950s. Lindsay Chemical Division was the first to commercialize large-scale ion-exchange purification of neodymium. Starting in the 1950s, high purity (above 99%) neodymium was primarily obtained through an ion exchange process from monazite, a mineral rich in rare-earth elements. The metal itself is obtained through electrolysis of its halide salts. Currently, most neodymium is extracted from bastnäsite, (Ce,La,Nd,Pr)COF, and purified by solvent extraction. Ion-exchange purification is reserved for preparing the highest purities (typically >99.99%). The evolving technology, and improved purity of commercially available neodymium oxide, was reflected in the appearance of neodymium glass that resides in collections today. Early neodymium glasses made in the 1930s have a more reddish or orange tinge than modern versions which are more cleanly purple, due to the difficulties in removing the last traces of praseodymium in the era when manufacturing relied upon fractional crystallization technology.\n\nNeodymium is rarely found in nature as a free element, but rather it occurs in ores such as monazite and bastnäsite (these are mineral group names rather than single mineral names) that contain small amounts of all rare-earth metals. In these minerals neodymium is rarely dominant (as in the case of lanthanum), with cerium being the most abundant lanthanide; some exceptions include monazite-(Nd) and kozoite-(Nd). The main mining areas are in China, the United States, Brazil, India, Sri Lanka, and Australia. The reserves of neodymium are estimated at about eight million tonnes. Although it belongs to the rare-earth metals, neodymium is not rare at all. Its abundance in the Earth's crust is about 38 mg/kg, which is the second highest among rare-earth elements, following cerium. The world's production of neodymium was about 7,000 tonnes in 2004. The bulk of current production is from China. the Chinese government has imposed strategic material controls on the element, raising some concerns in consuming countries and causing skyrocketing prices of neodymium and other rare-earth metals. As of late 2011, 99% pure neodymium was traded in world markets for US$300 to US$350 per kilogram, down from the mid-2011 peak of US$500/kg. The price of neodymium oxide fell from $200/kg in 2011 to $40 in 2015, largely due to illegal production in China which circumvented government restrictions. The uncertainty of pricing and availability have caused companies (particularly Japanese ones) to create permanent magnets and associated electric motors with fewer rare-earth metals; however, so far they have been unable to eliminate the need for neodymium.\n\nNeodymium is typically 10–18% of the rare-earth content of commercial deposits of the light rare-earth-element minerals bastnäsite and monazite. With neodymium compounds being the most strongly colored for the trivalent lanthanides, it can occasionally dominate the coloration of rare-earth minerals when competing chromophores are absent. It usually gives a pink coloration. Outstanding examples of this include monazite crystals from the tin deposits in Llallagua, Bolivia; ancylite from Mont Saint-Hilaire, Quebec, Canada; or lanthanite from the Saucon Valley, Pennsylvania, United States. As with neodymium glasses, such minerals change their colors under the differing lighting conditions. The absorption bands of neodymium interact with the visible emission spectrum of mercury vapor, with the unfiltered shortwave UV light causing neodymium-containing minerals to reflect a distinctive green color. This can be observed with monazite-containing sands or bastnäsite-containing ore.\n\n\nNeodymium magnets (actually an alloy, NdFeB) are the strongest permanent magnets known. A neodymium magnet of a few grams can lift a thousand times its own weight. These magnets are cheaper, lighter, and stronger than samarium–cobalt magnets. However, they are not superior in every aspect, as neodymium-based magnets lose their magnetism at lower temperatures and tend to rust, while samarium–cobalt magnets do not.\n\nNeodymium magnets appear in products such as microphones, professional loudspeakers, in-ear headphones, guitar and bass guitar pick-ups, and computer hard disks where low mass, small volume, or strong magnetic fields are required. Neodymium magnet electric motors have also been responsible for the development of purely electrical model aircraft within the first decade of the 21st century, to the point that these are displacing internal combustion–powered models internationally. Likewise, due to this high magnetic capacity per weight, neodymium is used in the electric motors of hybrid and electric automobiles, and in the electricity generators of some designs of commercial wind turbines (only wind turbines with \"permanent magnet\" generators use neodymium). For example, drive electric motors of each Toyota Prius require one kilogram (2.2 pounds) of neodymium per vehicle.\n\nCertain transparent materials with a small concentration of neodymium ions can be used in lasers as gain media for infrared wavelengths (1054–1064 nm), e.g. (yttrium aluminium garnet), Nd:YLF (yttrium lithium fluoride), Nd:YVO (yttrium orthovanadate), and Nd:glass. Neodymium-doped crystals (typically Nd:YVO) generate high-powered infrared laser beams which are converted to green laser light in commercial DPSS hand-held lasers and laser pointers.\n\nThe current laser at the UK Atomic Weapons Establishment (AWE), the HELEN (High Energy Laser Embodying Neodymium) 1-terawatt neodymium-glass laser, can access the midpoints of pressure and temperature regions and is used to acquire data for modeling on how density, temperature, and pressure interact inside warheads. HELEN can create plasmas of around 10 K, from which opacity and transmission of radiation are measured.\n\nNeodymium glass solid-state lasers are used in extremely high power (terawatt scale), high energy (megajoules) multiple beam systems for inertial confinement fusion. Nd:glass lasers are usually frequency tripled to the third harmonic at 351 nm in laser fusion devices.\n\nNeodymium glass (Nd:glass) is produced by the inclusion of neodymium oxide (NdO) in the glass melt. Usually in daylight or incandescent light neodymium glass appears lavender, but it appears pale blue under fluorescent lighting. Neodymium may be used to color glass in delicate shades ranging from pure violet through wine-red and warm gray.\n\nThe first commercial use of purified neodymium was in glass coloration, starting with experiments by Leo Moser in November 1927. The resulting \"Alexandrite\" glass remains a signature color of the Moser glassworks to this day. Neodymium glass was widely emulated in the early 1930s by American glasshouses, most notably Heisey, Fostoria (\"wisteria\"), Cambridge (\"heatherbloom\"), and Steuben (\"wisteria\"), and elsewhere (e.g. Lalique, in France, or Murano). Tiffin's \"twilight\" remained in production from about 1950 to 1980. Current sources include glassmakers in the Czech Republic, the United States, and China.\n\nThe sharp absorption bands of neodymium cause the glass color to change under different lighting conditions, being reddish-purple under daylight or yellow incandescent light, but blue under white fluorescent lighting, or greenish under trichromatic lighting. This color-change phenomenon is highly prized by collectors. In combination with gold or selenium, beautiful red colors result. Since neodymium coloration depends upon \"forbidden\" f-f transitions deep within the atom, there is relatively little influence on the color from the chemical environment, so the color is impervious to the thermal history of the glass. However, for the best color, iron-containing impurities need to be minimized in the silica used to make the glass. The same forbidden nature of the f-f transitions makes rare-earth colorants less intense than those provided by most d-transition elements, so more has to be used in a glass to achieve the desired color intensity. The original Moser recipe used about 5% of neodymium oxide in the glass melt, a sufficient quantity such that Moser referred to these as being \"rare-earth doped\" glasses. Being a strong base, that level of neodymium would have affected the melting properties of the glass, and the lime content of the glass might have had to be adjusted accordingly.\n\nLight transmitted through neodymium glasses shows unusually sharp absorption bands; the glass is used in astronomical work to produce sharp bands by which spectral lines may be calibrated. Another application is the creation of selective astronomical filters to reduce the effect of light pollution from sodium and fluorescent lighting while passing other colours, especially dark red hydrogen-alpha emission from nebulae. Neodymium is also used to remove the green color caused by iron contaminants from glass.\n\nNeodymium is a component of \"didymium\" (referring to mixture of salts of neodymium and praseodymium) used for coloring glass to make welder's and glass-blower's goggles; the sharp absorption bands obliterate the strong sodium emission at 589 nm. The similar absorption of the yellow mercury emission line at 578 nm is the principal cause of the blue color observed for neodymium glass under traditional white-fluorescent lighting.\n\nNeodymium and didymium glass are used in color-enhancing filters in indoor photography, particularly in filtering out the yellow hues from incandescent lighting.\n\nSimilarly, neodymium glass is becoming widely used more directly in incandescent light bulbs. These lamps contain neodymium in the glass to filter out yellow light, resulting in a whiter light which is more like sunlight.\n\nThe use of neodymium in automobile rear-view mirrors, to reduce the glare at night, has been patented.\n\nSimilar to its use in glasses, neodymium salts are used as a colorant for enamels.\n\nNeodymium metal dust is combustible and therefore an explosion hazard. Neodymium compounds, as with all rare-earth metals, are of low to moderate toxicity; however, its toxicity has not been thoroughly investigated. Neodymium dust and salts are very irritating to the eyes and mucous membranes, and moderately irritating to skin. Breathing the dust can cause lung embolisms, and accumulated exposure damages the liver. Neodymium also acts as an anticoagulant, especially when given intravenously.\n\nNeodymium magnets have been tested for medical uses such as magnetic braces and bone repair, but biocompatibility issues have prevented widespread application. Commercially available magnets made from neodymium are exceptionally strong, and can attract each other from large distances. If not handled carefully, they come together very quickly and forcefully, causing injuries. For example, there is at least one documented case of a person losing a fingertip when two magnets he was using snapped together from 50 cm away.\n\nAnother risk of these powerful magnets is that if more than one magnet is ingested, they can pinch soft tissues in the gastrointestinal tract. This has led to at least 1,700 emergency room visits and necessitated the recall of the Buckyballs line of toys, which were construction sets of small neodymium magnets.\n\n\n"}
{"id": "27716624", "url": "https://en.wikipedia.org/wiki?curid=27716624", "title": "Odyssey (tanker)", "text": "Odyssey (tanker)\n\nOdyssey, which previously went by the name \"Oriental Phoenix\", was an oil tanker in operation from 1971 to November 10, 1988, when an explosion caused it to sink in the North Atlantic off the coast of Canada. The resulting spill remains one of the largest oil spills in world history. The tanker was off the coast of the Canadian province of Nova Scotia when it sank and released 132,157 tons (43 million gallons) of oil into the ocean. By way of comparison, 4.3 times as much oil was spilled by the \"Odyssey\" as from the much more famous \"Exxon Valdez\".\n\nOdyssey, built in 1971, was a 65,000-ton tanker operated by Polembros Shipping Ltd. of London, England, and registered in Liberia. On November 5, 1988, the tanker departed Sullom Voe Terminal in the Shetland Islands off Scotland, fully loaded with North Sea Brent Crude oil which was being transported to the Come By Chance Refinery at Come-by-Chance, Newfoundland and Labrador.\n\nWhen the ship was about 1000 nautical miles off the coast of Newfoundland, a major North Atlantic storm arose, buffetting the ship with waves and 44-mile-per-hour winds. In response, the ship sent out a distress signal and kept heading for shore. The distress call was received by Radio Valentia in Ireland and transferred to the Canadian Rescue Coordination Centre. The centre alerted the vessels in the area. \n\nHowever, when the ship was off the coast of Nova Scotia, an explosion occurred on board, causing the ship to break into two and begin sinking. As the ship sank, a fire broke out on its stern section, causing the oil on board to catch fire. A Russian weather ship, Passat, responded to the Odyssey's distress call and was on site in less than an hour. Once on site however, it was unable to approach the vessel as it was surrounded by ignited oil slicks. \n\nAll 27 crew members, 15 Greeks and 12 Hondurans, are presumed to have died during the incident. Because of hazardous weather conditions, the Canadian Coast Guard could not immediately reach the spill and much of the oil burned up before the coast guard reached the ship.\n\nIn the immediate aftermath of the ship's sinking, the oil spill covered an area of x . A much reduced amount of oil reached shore - in part because of the oil's rapid combustion from the initial explosion and in part because currents carried the spill across the Atlantic, in the direction of England, giving the oil a significant amount of time to dissipate in the rough seas. Because of this, no clean-up operation was mounted.\n\nThe Advisory Committee on Marine Pollution of the Sea of the International Council for the Exploration of the Sea published an analysis of the spill in their \"1990 Marine Pollution Yearbook\" which noted that the spill likely had a significant effect on krill in the area, and through them, may have affected animals further up the food chain.\n\n"}
{"id": "40904234", "url": "https://en.wikipedia.org/wiki?curid=40904234", "title": "Pentacosylic acid", "text": "Pentacosylic acid\n\nPentacosylic acid, or pentacosanoic acid, or hyenic acid, is a 25-carbon long-chain saturated fatty acid with the chemical formula CH(CH)COOH.\n\n"}
{"id": "3190938", "url": "https://en.wikipedia.org/wiki?curid=3190938", "title": "Production fluid", "text": "Production fluid\n\nProduction fluid is the fluid mixture of oil, gas and water in formation fluid that flows to the surface of an oil well from a reservoir. Its consistency and composition varies.\n\nFluids may be described by a multitude of characteristics including:\n\n"}
{"id": "2688256", "url": "https://en.wikipedia.org/wiki?curid=2688256", "title": "Rimini protocol", "text": "Rimini protocol\n\nThe Rimini Protocol (also called \"Uppsala Protocol\") is a proposal made by the geologist Colin Campbell in 2003. It is intended to stabilise oil prices and minimize the effects of peak oil.\n\nTo achieve this, producing countries would not produce oil in excess of their present national depletion rate: i.e., roughly speaking, the oil burnt, expended or exported must equal the oil produced or imported. Furthermore, it would be required that importing nations stabilize their imports at existing levels. This would have the effect of keeping world prices in reasonable relationship to actual production costs and let Third World countries afford their oil imports.\n\nCampbell also stresses the need for outdated principles of economic growth to be surpassed. He states (May 2004): \"the economic fundamentalists ... have these really outdated economic principles inherited from the Industrial Revolution, when the world was indeed large and the scope for Man’s activities were at that time more or less infinite. ... these economic principles ... are very short term in their nature ... these people who say that there can be no shortage in an open market and their battle cry is liberalize markets - these people have become really the enemy ...\" . As such, Campbell strongly criticises those who accelerate the \"peak oil\" crisis, rather than taking action to curtail it as he recommends.\n\nCampbell substantiates the prediction of the urgent forthcoming \"peak oil\" crisis by making reference to Saudi Arabia, a nation covering a geographic region wherein there is a huge concentration of oil. Campbell hypothesises: \"I think even the Sauds would always develop the larger ones first. So the biggest field in the world is Ghawar, with 80 billion in it perhaps, you step outside from this trend to Safaniya with 35 perhaps, Hanifah about 12 and Shaybah 15, and you come on down. If they develop the big ones first, which one must assume they did, you are down to well, still nice oilfields but of a modest scale, and so I suppose the other discoveries they made have made are smaller by orders of magnitude ... it’s quite evident that this doesn’t come close to the past.\"\n\nSupport for the Rimini Protocol has even been found in retired politicians. In regard thereto, Campbell proclaims: \"And then ... you have the people I call the Renegades. These are senior politicians who are now out of office and being freed from the system, are able to tell the truth. And some of them do.\"\n\n"}
{"id": "30256262", "url": "https://en.wikipedia.org/wiki?curid=30256262", "title": "Rosy tetra", "text": "Rosy tetra\n\nThe rosy tetra (\"Hyphessobrycon rosaceus\") is a small species of Characin from the South American countries of Guyana and Brazil.It is popular in the aquarium trade.\n\nThe rosy tetra has a light pink-white body with red fins, except the dorsal fin which can be black or white, and the caudal fin which is pink-white with two elliptical red spots on it. It has a faint black line from the top of its eyeball through the pupil, to the bottom of the eyeball. Like many other tetras, the males have longer dorsal fins than the females.\n\nA group of 6 Rosy tetras would feel best in an aquarium with volume of 20 US gallons (76 L).\n\nIn the wild, the rosy tetra lives in the Essequibo, Corantijn and Suriname River basins in South America.\n\nThe rosy tetra is an egg scattering school spawning fish in the wild. 100 eggs can be laid by one female, usually in the early morning, and over fine leaved plants.\n\n"}
{"id": "352327", "url": "https://en.wikipedia.org/wiki?curid=352327", "title": "Sawmill", "text": "Sawmill\n\nA sawmill or lumber mill is a facility where logs are cut into lumber. Modern saw mills use a motorized saw to cut logs lengthwise to make long pieces, and crosswise to length depending on standard or custom sizes (dimensional lumber). The \"portable\" saw mill is iconic and of simple operation—the logs lay flat on a steel bed and the motorized saw cuts the log horizontally along the length of the bed, by the operator manually pushing the saw. The most basic kind of saw mill consists of a chainsaw and a customized jig (\"Alaskan saw mill\"), with similar horizontal operation.\n\nBefore the invention of the sawmill, boards were made in various manual ways, either rived (split) and planed, hewn, or more often hand sawn by two men with a whipsaw, one above and another in a saw pit below. The earliest known mechanical mill is the Hierapolis sawmill, a Roman water-powered stone mill at Hierapolis, Asia Minor dating back to the 3rd century AD. Other water-powered mills followed and by the 11th century they were widespread in Spain and North Africa, the Middle East and Central Asia, and in the next few centuries, spread across Europe. The circular motion of the wheel was converted to a reciprocating motion at the saw blade. Generally, only the saw was powered, and the logs had to be loaded and moved by hand. An early improvement was the development of a movable carriage, also water powered, to move the log steadily through the saw blade.\n\nBy the time of the Industrial Revolution in the 18th century, the circular saw blade had been invented, and with the development of steam power in the 19th century, a much greater degree of mechanisation was possible. Scrap lumber from the mill provided a source of fuel for firing the boiler. The arrival of railroads meant that logs could be transported to mills rather than mills being built besides navigable waterways. By 1900, the largest sawmill in the world was operated by the Atlantic Lumber Company in Georgetown, South Carolina, using logs floated down the Pee Dee River from the Appalachian Mountains. In the 20th century the introduction of electricity and high technology furthered this process, and now most sawmills are massive and expensive facilities in which most aspects of the work is computerized. Besides the sawn timber, use is made of all the by-products including sawdust, bark, woodchips, and wood pellets, creating a diverse offering of forest products.\n\nA sawmill's basic operation is much like those of hundreds of years ago; a log enters on one end and dimensional lumber exits on the other end.\n\nThe Hierapolis sawmill, a water-powered stone saw mill at Hierapolis, Asia Minor (modern-day Turkey, then part of the Roman Empire), dating to the second half of the 3rd century, is the earliest known sawmill. It also incorporates a crank and connecting rod mechanism.\n\nWater-powered stone sawmills working with cranks and connecting rods, but without gear train, are archaeologically attested for the 6th century at the Byzantine cities Gerasa (in Asia Minor) and Ephesus (in Syria).\n\nThe earliest literary reference to a working sawmill comes from a Roman poet, Ausonius, who wrote a topographical poem about the river Moselle in Germany in the late 4th century AD. At one point in the poem he describes the shrieking sound of a watermill cutting marble. Marble sawmills also seem to be indicated by the Christian saint Gregory of Nyssa from Anatolia around 370/390 AD, demonstrating a diversified use of water-power in many parts of the Roman Empire.\n\nBy the 11th century, hydropowered sawmills were in widespread use in the medieval Islamic world, from Islamic Spain and North Africa in the west to Central Asia in the east.\n\nSawmills later became widespread in medieval Europe, as one was sketched by Villard de Honnecourt in c. 1250. They are claimed to have been introduced to Madeira following its discovery in c. 1420 and spread widely in Europe in the 16th century.\n\nPrior to the invention of the sawmill, boards were rived (split) and planed, or more often sawn by two men with a whipsaw, using saddleblocks to hold the log, and a saw pit for the pitman who worked below. Sawing was slow, and required strong and hearty men. The topsawer had to be the stronger of the two because the saw was pulled in turn by each man, and the lower had the advantage of gravity. The topsawyer also had to guide the saw so that the board was of even thickness. This was often done by following a chalkline.\n\nEarly sawmills simply adapted the whipsaw to mechanical power, generally driven by a water wheel to speed up the process. The circular motion of the wheel was changed to back-and-forth motion of the saw blade by a connecting rod known as a \"pitman arm\" (thus introducing a term used in many mechanical applications).\n\nGenerally, only the saw was powered, and the logs had to be loaded and moved by hand. An early improvement was the development of a movable carriage, also water powered, to move the log steadily through the saw blade.\n\nA type of sawmill without a crank is known from Germany called \"knock and drop\" or simply \"drop\" -mills. In these drop sawmills, the frame carrying the saw blade is knocked upwards by cams as the shaft turns. These cams are let into the shaft on which the waterwheel sits. When the frame carrying the saw blade is in the topmost position it drops by its own weight, making a loud knocking noise, and in so doing it cuts the trunk.\n\nA small mill such as this would be the center of many rural communities in wood-exporting regions such as the Baltic countries and Canada. The output of such mills would be quite low, perhaps only 500 boards per day. They would also generally only operate during the winter, the peak logging season.\n\nIn the United States, the sawmill was introduced soon after the colonisation of Virginia by recruiting skilled men from Hamburg.\nLater the metal parts were obtained from the Netherlands, where the technology was far ahead of that in England, where the sawmill remained largely unknown until the late 18th century. The arrival of a sawmill was a large and stimulative step in the growth of a frontier community.\n\nEarly mills had been taken to the forest, where a temporary shelter was built, and the logs were skidded to the nearby mill by horse or ox teams, often when there was some snow to provide lubrication. As mills grew larger, they were usually established in more permanent facilities on a river, and the logs were floated down to them by log drivers. Sawmills built on navigable rivers, lakes, or estuaries were called cargo mills because of the availability of ships transporting cargoes of logs to the sawmill and cargoes of lumber from the sawmill.\n\nThe next improvement was the use of circular saw blades, perhaps invented in England in the late 18th century, but perhaps in 17th-century Holland, the Netherlands. Soon thereafter, millers used gangsaws, which added additional blades so that a log would be reduced to boards in one quick step. Circular saw blades were extremely expensive and highly subject to damage by overheating or dirty logs. A new kind of technician arose, the sawfiler. Sawfilers were highly skilled in metalworking. Their main job was to \"set\" and sharpen teeth. The craft also involved learning how to \"hammer\" a saw, whereby a saw is deformed with a hammer and anvil to counteract the forces of heat and cutting. Modern circular saw blades have replaceable teeth, but still need to be hammered.\n\nThe introduction of steam power in the 19th century created many new possibilities for mills. Availability of railroad transportation for logs and lumber encouraged building of rail mills away from navigable water. Steam powered sawmills could be far more mechanized. Scrap lumber from the mill provided a ready fuel source for firing the boiler. Efficiency was increased, but the capital cost of a new mill increased dramatically as well.\n\nIn addition, the use of steam or gasoline-powered traction engines also allowed the entire sawmill to be mobile.\n\nBy 1900, the largest sawmill in the world was operated by the Atlantic Lumber Company in Georgetown, South Carolina, using logs floated down the Pee Dee River from as far as the edge of the Appalachian Mountains in North Carolina.\n\nA restoration project for Sturgeon's Mill in Northern California is underway, restoring one of the last steam-powered lumber mills still using its original equipment.\n\nIn the twentieth century the introduction of electricity and high technology furthered this process, and now most sawmills are massive and expensive facilities in which most aspects of the work is computerized. The cost of a new facility with 2 Mmfbm/day capacity is up to CAN$120,000,000. A modern operation will produce between 100 Mmfbm and 700 Mmfbm annually.\n\nSmall gasoline-powered sawmills run by local entrepreneurs served many communities in the early twentieth century, and specialty markets still today.\n\nA trend is the small portable sawmill for personal or even professional use. Many different models have emerged with different designs and functions. They are especially suitable for producing limited volumes of boards, or specialty milling such as oversized timber. Portable sawmills have gained popularity for the convenience of bringing the sawmill to the logs and milling lumber in remote locations. Some remote communities that have experienced natural disasters have used portable sawmills to rebuild their communities out of the fallen trees.\n\nTechnology has changed sawmill operations significantly in recent years, emphasizing increasing profits through waste minimization and increased energy efficiency as well as improving operator safety. The once-ubiquitous rusty, steel conical sawdust burners have for the most part vanished, as the sawdust and other mill waste is now processed into particleboard and related products, or used to heat wood-drying kilns. Co-generation facilities will produce power for the operation and may also feed superfluous energy onto the grid. While the bark may be ground for landscaping barkdust, it may also be burned for heat. Sawdust may make particle board or be pressed into wood pellets for pellet stoves. The larger pieces of wood that won't make lumber are chipped into wood chips and provide a source of supply for paper mills. Wood by-products of the mills will also make oriented strand board (OSB) paneling for building construction, a cheaper alternative to plywood for paneling. Some automatic mills can process 800 small logs into bark chips, wood chips, sawdust and sorted, stacked, and bound planks, in an hour.\n\n\n"}
{"id": "20774257", "url": "https://en.wikipedia.org/wiki?curid=20774257", "title": "Snowmageddon", "text": "Snowmageddon\n\nSnowmageddon, Snowpocalypse, and Snowzilla are portmanteaus of the word \"snow\" with either \"Armageddon\", \"Apocalypse\" and \"Godzilla\" respectively. Snowmageddon was coined by blogger Greg Swan in April 2007 as a satirical response to local media’s coverage of a lackluster Minneapolis storm, using live streaming and memes to spark Twitter trending topics in 2008 and 2009. Snowmageddon and Snowpocalypse seem to have first been published in the popular press in Canada during January 2009, and was also used in January 2010 by \"The Guardian\" reporter Charlie Brooker to characterise the sensationalist reaction of television news to a period of snowfall across the UK. \"The Washington Post\", out of Washington, DC, ran an online poll asking for reader feedback prior to the February 5–6, 2010 North American blizzard on February 4, 2010, and several blogs, including the \"Washington Post\"s own blog, followed that up by using either \"Snowmageddon\" or \"Snowpocalypse\" before, during, and after the storm hit.\n\n\"The Washington Post\" also popularized the term \"kaisersnoze\" (see Keyser Söze) in response to the February snowstorms.\n\nDuring the evening preceding the first blizzard hitting Washington, DC, most of the United States federal government closed, and press coverage continued to characterize the storm using either \"Snowmageddon\", \"Snowpocalypse\", or both.\n\nThe term \"Snowpocalypse\" was used in the Pacific Northwest to refer to a snowstorm in December 2008.\n\nThe 2008 children's book \"Winter Blast\" by Chris Wright, uses the term \"snowmageddon\" in the storyline of the book.\n\nCartoonstock.com cartoonist Graham Chaffer used the term \"Snowageddon\" (with spelling variation) in a cartoon uploaded to the site in March 2007. A satirical take on the hyperbole used by TV weather reporters. Its caption reads: \"And it continues to fall! This is the big chill! The wintery whiteout! The new Ice Age! Snowageddon!\"\nSnowmageddon, Snowpocalypse, and Snowzilla can refer specifically to:\n\n"}
{"id": "34199589", "url": "https://en.wikipedia.org/wiki?curid=34199589", "title": "Spanish Bank of Algae", "text": "Spanish Bank of Algae\n\nThe Spanish Bank of Algae (BEA-Banco Español de Algas) is a national R&D service attached to the Marine Biotechnology Center (CBM-Centro de Biotecnología Marina) of the University of Las Palmas de Gran Canaria (ULPGC), which objectives are the isolation, identification, characterization, conservation and provisioning of microalgae and cyanobacteria. It was previously known as the National Bank of Algae (BNA).\n\nIn addition to the functions of isolation, identification, characterization, conservation and provisioning, standard in any collection of microorganisms, the CBM-BEA/ULPGC aims to function as a service facilitating the development of a new bioindustrial sector based on the cultivation and application of microalgae.\n\nBEA has been member of the European Culture Collection Organization (ECCO) since 2001 and of the World Federation for Culture Collections (WFCC) since 2003. BEA is listed in the World Data Centre for Microorganisms (WFCC-MIRCEN) under registration number 837.\n\nBEA is Accredited as an International Authority for the Deposit of Micro-organisms in accordance with the Treaty of Budapest by the World Intellectual Property Organization (WIPO), resolution No. 239 of 28.10.2005 before the Government of Spain. This accreditation confers on BEA the deposit of algae for the purposes of recognition of industrial property before the World Intellectual Property Office and the Spanish Patent and Trademark Office.\n\nBEA holds strains from tropical and subtropical areas, in particular of the Macaronesian Region. These cultures have strain numbers preceded by BEA. Non-axenic cultures are marked with \"B\" at the end of the strain number.\n\n1985.- The embryo of the Group of Applied Algology, from the former Polytechnic University of the Canaries, started up with the motto \"sun and seawater can grow more than tourists.\"\n1995.- The Center of Applied Algology -ULPGC (now the CBM-ULPGC) created a collection of microalgae in 1995 thanks to a generous donation from Dr. Ziiadine Ramazanov (scientist in the former USSR at the Soviet Academy of Science’s Institute of Plant Physiology and Astrobiology, who worked in the IAA in 1995-96), who gave his personal collection of microalgae to the IAA, thus helping to introduce the group to the field of biotechnology of microalgae. Before that date, the applied phycology group had only worked with macroalgae in the fields of in vitro cultures, population genetics and intensive cultures.\n1996.- The CBM-BNA lost most of its microalga collection due to a 5-month electrical outage. Dr. Ramazanov emigrated to the United States, published several books and created a plant biotechnology company.\n1998.- The paperwork was started to obtain WIPO accreditation for the BEA as an International Depositary Authority pursuant to the Budapest Treaty. The OEPM (Spanish Patent and Trademark Office) and the Spanish Type Culture Collection (University of Valencia) cooperated with this institution to achieve international accreditation for the BEA, a process that lasted until 2005.\n1999.- Letter of support from the Council for Scientific Research (Dr. Miguel García Guerrero, CSIC-MEC) to consolidate the BNA (after an inspection visit to the facilities of what was then called the Centro de Algología Aplicada (Applied Phycology Center) – ULPGC).\nLetter of support was sent from the Spanish Phycology Society to consolidate the BNA (after an inspection visit to the facilities of what was then called the Centro de Algología Aplicada (Applied Phycology Center) – ULPGC).\n\nThe Board of the ULPGC signed an agreement to break up the CBM-BNA. Following arduous litigation, which ended in 2003 with the final ruling from Spain’s Constitutional Court, the CBM-BNA was left intact.\n\n2000.- The BNA was accepted as a member of the ECCO (European Culture Collection Organization).\n\n2001.- Spain’s Parliament (Draft Law 161/000706 of 30 April 2001-Series D, No. 170) urged the government to establish the National Bank of Algae at the ULPGC’s Applied Phycology Center and to accredit it as an International Depositary Authority pursuant to the Budapest Treaty.\n\n2003.- The BNA became a member of the World Federation for Culture Collections (WFCC) and received registration number 837 in the WFCC-MIRCEN World Data Center for Microorganisms.\n\nThe ULPGC endorsed CBM’s bylaws, which defined the BNA as one of the two Services (the other being Flow Cytometry) offered by the Centro de Biotecnología Marina (Marine Biotechnology Center) – ULPGC. CBM and BEA are finally recognized by the ULPGC.\n\n2004.- Spain’s Ministry of Education and Science (MEC) awards the first and only financial grant conceded to the BEA prior to 2010 (every other local, regional and national grant request was rejected) through its 2003-2006 Complementary Actions aid program.\n\nThe BNA collaborates in the first study of oceanic toxic algal blooms in the Canary Islands (summer 2004), commissioned by the Government of the Canaries Council on Agriculture, Fishing and Food. Resulting from this effort was the paper \"Novel Bloom of Trichodesmium erythraeum in the NW African Upwelling\". The year before the CBM-BNA had warned of the risk of toxic microalgal Aphanizomenon spp blooms in reservoirs on the island of Grand Canary. When the bloom of toxic cyanobacteria became evident three months later, the drainage basin had to be closed off and irrigation suspended.\nThere are certified reports of hospitalizations in the Canaries due to the consumption of fish containing microalgal toxins.\nThe risks of HAB (Harmful Algal Blooms) and their increased incidence due to climate change started to be taken seriously by government officials.\n\n2005.- BNA was accredited as an International Depositary Authority for the purposes of patent procedure. The complex process to accredit the BNA/CBM as an International Depositary Authority (for patent purposes) pursuant to the Budapest Treaty successfully culminated on 28 October 2005 with the publication of its accreditation in the WIPO (World Intellectual Property Organization) Official Bulletin (Budapest Notification No. 239).\n\nBNA organized the Tropical and Subtropical Cyanoprokaryota Workshop 2005 (TSCW2005), in cooperation with Proexca, the Industry Council of Grand Canary, Caja Rural de Canarias Foundation, La Caixa, the ULPGC Board, the Ocean Sciences Department of the ULPGC and the spin-off company Seaweed Canarias.\n\n2007.- The MEC provided funds for staffing one Scientific-Technological Infrastructure Specialist at the National Bank of Algae (BNA).\n\n2009.- The Minister of Science and Innovation, Dr. Cristina Garmendia, announced the consolidation of the BNA-CBM. “The BNA will serve as a national research service to promote microalgae-based bio-industries, it will create and maintain its own collection, taking advantage of Spanish oceanographic expeditions, and it will provide samples to laboratories and companies that so request them for study and eventual technical applications”.\n\nIn late 2009, the Government of Spain (Ministry of Innovation and Science [MICINN]) finalized a 2.6 M€ grant for the consolidation of the BNA (23 July 2009), as part of the PLAN-E microalgae project.\n\nCBM is included in the Canaries Marine Development Complex, located in Taliarte (SE coast of Grand Canary), and the emerging BNA backs the joint request made to the MICINN by the ULPGC and the University of La Laguna (ULL) to classify the Tricontinental Campus as a “campus of excellence”.\n\n2010.- In March 2010, contracts were finalized and remodeling work had begun on the CBM-BNA laboratories as part of a Plan-E Project, which was initially scheduled for completion in December 2010, though the deadline was later extended until December 2011. The remodeling work (the deadline extension was not implemented) was completed on 16 December (the scientific work of the CBM-BNA did not stop in 2010, since every available inch of the laboratories on the ground floor and the second floor hallways were turned into offices, classrooms and bathrooms ).\nDoctors Michael Melkonian and Barbara Melkonian (University of Cologne, Germany) made a sabbatical stay (September 2010 to March 2011) at BNA.\n\n2011.- By February growth chambers, equipments and basic microalgae cultivation systems were operating. In May the organization was officially renamed to Spanish Bank of Algae (BEA). On June 2, the authorisation to publish the BEA website was granted, which includes catalogs of products and services.\n\n\n\n\n"}
{"id": "39113753", "url": "https://en.wikipedia.org/wiki?curid=39113753", "title": "Standard step method", "text": "Standard step method\n\nThe standard step method (STM) is a computational technique utilized to estimate one-dimensional surface water profiles in open channels with gradually varied flow under steady state conditions. It uses a combination of the energy, momentum, and continuity equations to determine water depth with a given a friction slope formula_1, channel slope formula_2, channel geometry, and also a given flow rate. In practice, this technique is widely used through the computer program HEC-RAS, developed by the US Army Corps of Engineers Hydrologic Engineering Center (HEC).\n\nThe energy equation used for open channel flow computations is a simplification of the Bernoulli Equation (See Bernoulli Principle), which takes into account pressure head, elevation head, and velocity head. (Note, energy and head are synonymous in Fluid Dynamics. See Pressure head for more details.) In open channels, it is assumed that changes in atmospheric pressure are negligible, therefore the “pressure head” term used in Bernoulli’s Equation is eliminated. The resulting energy equation is shown below:\n\nFor a given flow rate and channel geometry, there is a relationship between flow depth and total energy. This is illustrated below in the plot of energy vs. flow depth, widely known as an E-y diagram. In this plot, the depth where the minimum energy occurs is known as the critical depth. Consequently, this depth corresponds to a Froude Number formula_4 of 1. Depths greater than critical depth are considered “subcritical” and have a Froude Number less than 1, while depths less than critical depth are considered supercritical and have Froude Numbers greater than 1. (For more information, see Dimensionless Specific Energy Diagrams for Open Channel Flow.)\n\nUnder steady state flow conditions (e.g. no flood wave), open channel flow can be subdivided into three types of flow: uniform flow, gradually varying flow, and rapidly varying flow. Uniform flow describes a situation where flow depth does not change with distance along the channel. This can only occur in a smooth channel that does not experience any changes in flow, channel geometry, roughness or channel slope. During uniform flow, the flow depth is known as normal depth (yn). This depth is analogous to the terminal velocity of an object in free fall, where gravity and frictional forces are in balance (Moglen, 2013). Typically, this depth is calculated using the Manning formula. Gradually varied flow occurs when the change in flow depth per change in flow distance is very small. In this case, hydrostatic relationships developed for uniform flow still apply. Examples of this include the backwater behind an in-stream structure (e.g. dam, sluice gate, weir, etc.), when there is a constriction in the channel, and when there is a minor change in channel slope. Rapidly varied flow occurs when the change in flow depth per change in flow distance is significant. In this case, hydrostatics relationships are not appropriate for analytical solutions, and continuity of momentum must be employed. Examples of this include large changes in slope like a spillway, abrupt constriction/expansion of flow, or a hydraulic jump.\n\nTypically, the STM is used to develop “surface water profiles,” or longitudinal representations of channel depth, for channels experiencing gradually varied flow. These transitions can be classified based on reach condition (mild or steep), and also the type of transition being made. Mild reaches occur where normal depth is subcritical (yn > yc) while steep reaches occur where normal depth is supercritical (yn<yc). The transitions are classified by zone. (See figure 3.)\n\nFigure 3. This figure illustrates the different classes of surface water profiles experienced in steep and mild reaches during gradually varied flow conditions. Note: The Steep Reach column should be labeled \"Steep Reach (yn<yc).\n\nThe above surface water profiles are based on the governing equation for gradually varied flow (seen below)\n\nThis equation (and associated surface water profiles) is based on the following assumptions:\n\nThe STM numerically solves equation 3 through an iterative process. This can be done using the bisection or Newton-Raphson Method, and is essentially solving for total head at a specified location using equations 4 and 5 by varying depth at the specified location.\n\nIn order to use this technique, it is important to note you must have some understanding of the system you are modeling. For each gradually varied flow transition, you must know both boundary conditions and you must also calculate length of that transition. (e.g. For an M1 Profile, you must find the rise at the downstream boundary condition, the normal depth at the upstream boundary condition, and also the length of the transition.) To find the length of the gradually varied flow transitions, iterate the “step length”, instead of height, at the boundary condition height until equations 4 and 5 agree. (e.g. For an M1 Profile, position 1 would be the downstream condition and you would solve for position two where the height is equal to normal depth.)\n\nComputer programs like excel contain iteration or goal seek functions that can automatically calculate the actual depth instead of manual iteration.\n\nFigure 4 illustrates the different surface water profiles associated with a sluice gate on a mild reach (top) and a steep reach (bottom). Note, the sluice gate induces a choke in the system, causing a “backwater” profile just upstream of the gate. In the mild reach, the hydraulic jump occurs downstream of the gate, but in the steep reach, the hydraulic jump occurs upstream of the gate. It is important to note that the gradually varied flow equations and associated numerical methods (including the standard step method) cannot accurately model the dynamics of a hydraulic jump. See the Hydraulic jumps in rectangular channels page for more information. Below, an example problem will use conceptual models to build a surface water profile using the STM.\n\nSolution\n\nUsing Figure 3 and knowledge of the upstream and downstream conditions and the depth values on either side of the gate, a general estimate of the profiles upstream and downstream of the gate can be generated. Upstream, the water surface must rise from a normal depth of 0.97 m to 9.21 m at the gate. The only way to do this on a mild reach is to follow an M1 profile. The same logic applies downstream to determine that the water surface follows an M3 profile from the gate until the depth reaches the conjugate depth of the normal depth at which point a hydraulic jump forms to raise the water surface to the normal depth.\n\nStep 4: Use the Newton Raphson Method to solve the M1 and M3 surface water profiles. The upstream and downstream portions must be modeled separately with an initial depth of 9.21 m for the upstream portion, and 0.15 m for the downstream portion. The downstream depth should only be modeled until it reaches the conjugate depth of the normal depth, at which point a hydraulic jump will form. The solution presented explains how to solve the problem in a spreadsheet, showing the calculations column by column. Within Excel, the goal seek function can be used to set column 15 to 0 by changing the depth estimate in column 2 instead of iterating manually.\n\nTable 1: Spreadsheet of Newton Raphson Method of downstream water surface elevation calculations\nStep 5: Combine the results from the different profiles and display.\n\nNormal depth was achieved at approximately 2,200 meters upstream of the gate.\n\nStep 6: Solve the problem in the HEC-RAS Modeling Environment:\n\nIt is beyond the scope of this Wikipedia Page to explain the intricacies of operating HEC-RAS. For those interested in learning more, the HEC-RAS user’s manual is an excellent learning tool and the program is free to the public. \n\nThe first two figures below are the upstream and downstream water surface profiles modeled by HEC-RAS. There is also a table provided comparing the differences between the profiles estimated by the two different methods at different stations to show consistency between the two methods. While the two different methods modeled similar water surface shapes, the standard step method predicted that the flow would take a greater distance to reach normal depth upstream and downstream of the gate. This stretching is caused by the errors associated with assuming average gradients between two stations of interest during our calculations. Smaller dx values would reduce this error and produce more accurate surface profiles. \n\nThe HEC-RAS model calculated that the water backs up to a height of 9.21 meters at the upstream side of the sluice gate, which is the same as the manually calculated value. Normal depth was achieved at approximately 1,700 meters upstream of the gate.\n\nHEC-RAS modeled the hydraulic jump to occur 18 meters downstream of the sluice gate.\n"}
{"id": "53612709", "url": "https://en.wikipedia.org/wiki?curid=53612709", "title": "Stone damage", "text": "Stone damage\n\nStone damage, or stone-chip, is the damage that gravel and small stones can make to a vehicle.\n\nStone damage is most common on roads on which the allowed speed exceeds , since stones stuck in the tires come loose at that speed and fly away with such a speed that they can damage other vehicles.\n\nStone damage can be dangerous in many ways. Stone damage can cause small cracks in the windshield that can refract or reflect normally unharmful light such that it can distract or blind the driver. Stone damage can also cause large cracks in the windshield – this usually happens during the winter period because of the large temperature differences, but can also happen if the vehicle is exposed to vibrations or bumps.\n\nA factor that contributes to stone damage on the windshield is the fact that modern cars normally use quite thin windshields to save weight. Modern tires also contribute to stone damage since they have more tracks in which stones can get stuck.\n"}
{"id": "30078913", "url": "https://en.wikipedia.org/wiki?curid=30078913", "title": "Superionic water", "text": "Superionic water\n\nSuperionic water is a phase of water under extreme heat and pressure which has properties of both a solid and a liquid, which is supported by some experimental evidence.\n\nAt high temperatures and pressures, such as in the interior of giant planets, it is argued that water exists as ionic water in which the molecules break down into a soup of hydrogen and oxygen ions. At even higher pressures, ionic water will further condense into superionic water, where the oxygen crystallises and the hydrogen ions float around freely within the oxygen lattice.\n\nSuperionic water was previously theoretical, but predictions were made about its properties. If it were present on Earth, it would rapidly decompress and explode. Under the conditions theorized to cause water to enter the phase, it is believed that superionic water would be as hard as iron and would glow yellow. \n\nDemontis, et al. made the first prediction for superionic water using classical molecular dynamics simulations that were published in Physical Review Letters in 1988 .\nIn 1999 Cavazzoni, et al. predicted that such a state would exist for ammonia and water in conditions such as those existing on Uranus and Neptune.. In 2005 Laurence Fried led a team at Lawrence Livermore National Laboratory in California to recreate the formative conditions of superionic water. Using a technique involving smashing water molecules between diamonds and super heating it with lasers they observed frequency shifts which indicated that a phase transition had taken place. The team also created computer models which indicated that they had indeed created superionic water. In 2013 Hugh F. Wilson, Michael L. Wong, and Burkhard Militzer at the University of California, Berkeley published a paper predicting the face-centered cubic lattice structure that would emerge at higher pressures.\n\nAdditional experimental evidence was found by Marius Millot and colleagues in 2018, in an article in Nature Physics, by inducing high pressure on water between diamonds and then shocking the water using a laser pulse..\n\nThe most recent experiment was conducted at Lawrence Livermore by squeezing water between two pieces of diamond with a pressure of 360,000 psi. The water was squeezed into type VII ice which is 60 percent denser than normal water.\n\nThe compressed ice was then transported to the University of Rochester where it was blasted by a pulse of laser light. The reaction created conditions like those inside of ice giants such as Uranus and Neptune by heating up the ice thousands of degrees under a pressure a million times greater than the earth's atmosphere in only ten to 20 billionths of a second. \n\nThe experiment concluded that the current in the conductive water was indeed carried by ions rather than electrons and thus pointed to the water being superionic.\n\nIt is theorized that the ice giant planets Uranus and Neptune hold a layer of superionic water. But there are also studies that suggest that other elements present inside the interiors of these planets, particularly carbon, may prevent the formation of superionic water.\n"}
{"id": "469762", "url": "https://en.wikipedia.org/wiki?curid=469762", "title": "Tailings", "text": "Tailings\n\nTailings, also called mine dumps, culm dumps, slimes, tails, refuse, leach residue or slickens, terra-cone (terrikon), are the materials left over after the process of separating the valuable fraction from the uneconomic fraction (gangue) of an ore. Tailings are distinct from overburden, which is the waste rock or other material that overlies an ore or mineral body and is displaced during mining without being processed.\n\nThe extraction of minerals from ore can be done two ways: placer mining, which uses water and gravity to concentrate the valuable minerals, or hard rock mining, which pulverizes the rock containing the ore and then relies on chemical reactions to concentrate the sought-after material. In the latter, the extraction of minerals from ore requires comminution, i.e., grinding the ore into fine particles to facilitate extraction of the target element(s). Because of this comminution, tailings consist of a slurry of fine particles, ranging from the size of a grain of sand to a few micrometres. Mine tailings are usually produced from the mill in slurry form, which is a mixture of fine mineral particles and water.\n\nThe effluent from the tailings from the mining of sulfidic minerals has been described as \"the largest environmental liability of the mining industry\". These tailings contain large amounts of pyrite (FeS) and Iron(II) sulfide (FeS), which are rejected from the sought-after ores of copper and nickel, as well as coal. Although harmless underground, these minerals are reactive toward air in the presence of microorganisms, leading to acid mine drainage.\nWhen applied to coal mining tailings ponds and oil sands tailings ponds, the term \"tailings\" refers to fine waste suspended in water.\n\nBauxite tailings is a waste product generated in the industrial production of aluminium. Making provision for the approximately 77 million tons that is produced annually is one of the most significant problems for the aluminium mining industry.\n\nEarly mining operations often did not take adequate steps to make tailings areas environmentally safe after closure. Modern mines, particularly those in jurisdictions with well-developed mining regulations and those operated by responsible mining companies, often include the rehabilitation and proper closure of tailings areas in their costs and activities. For example, the Province of Quebec, Canada, requires not only the submission of a closure plan before the start of mining activity, but also the deposit of a financial guarantee equal to 100% of the estimated rehabilitation costs. Tailings dams are often the most significant environmental liability for a mining project.\n\nThe fraction of tailings to ore can range from 90–98% for some copper ores to 20–50% of the other (less valuable) minerals. The rejected minerals and rocks liberated through mining and processing have the potential to damage the environment by releasing toxic metals (arsenic and mercury being two major culprits), by acid drainage (usually by microbial action on sulfide ores), or by damaging aquatic wildlife that rely on clear water (vs suspensions).\n\nThe greatest danger of tailings ponds is dam failure, with the most publicized failure in the U.S. being the failure of a coal slurry dam in the West Virginia Buffalo Creek Flood, which killed 125 people; other collapses include the Ok Tedi environmental disaster in New Guinea, which destroyed the fishery of the Ok Tedi River. On average, worldwide, there is one big accident involving a tailings dam each year. Tailings ponds can also be a source of acid drainage, leading to the need for permanent monitoring and treatment of water passing through the tailings dam; the cost of mine cleanup has typically been 10 times that of mining industry estimates when acid drainage was involved. Other disasters caused by tailings dam failures are, the 2000 Baia Mare cyanide spill and the Ajka alumina plant accident. (See also List of tailings dam failures.)\n\nHistorically, tailings were disposed of in the most convenient manner, such as in downstream running water or down drains. Because of concerns about these sediments in the water and other issues, tailings ponds came into use. The sustainability challenge in the management of tailings and waste rock is to dispose of material, such that it is inert or, if not, stable and contained, to minimise water and energy inputs and the surface footprint of wastes and to move toward finding alternate uses.\n\nBounded by impoundments (an impoundment is a dam), these dams typically use \"local materials\" including the tailings themselves, and may be considered embankment dams. Traditionally, the only option for tailings storage was to deal with a tailings slurry. This slurry is a dilute stream of the tailings solids within water that was sent to the tailings storage area. The modern tailings designer has a range of tailings products to choose from depending upon how much water is removed from the slurry prior to discharge. The removal of water not only can create a better storage system in some cases (e.g. dry stacking, see below) but can also assist in water recovery which is a major issue as many mines are in arid regions. In a 1994 description of tailings impoundments, however, the U.S. EPA stated that dewatering methods may be prohibitively expensive except in special circumstances. Subaqueous storage of tailings has also been used.\n\nTailing ponds are areas of refused mining tailings where the waterborne refuse material is pumped into a pond to allow the sedimentation (meaning separation) of solids from the water. The pond is generally impounded with a dam, and known as tailings impoundments or tailings dams. It was estimated in 2000 that there were about 3,500 active tailings impoundments in the world. The ponded water is of some benefit as it minimizes fine tailings from being transported by wind into populated areas where the toxic chemicals could be potentially hazardous to human health; however, it is also harmful to the environment. Tailing ponds are often somewhat dangerous because they attract wildlife such as waterfowl or caribou as they appear to be a natural pond, but they can be highly toxic and harmful to the health of these animals. Tailings ponds are used to store the waste made from separating minerals from rocks, or the slurry produced from tar sands mining. Tailings are sometimes mixed with other materials such as bentonite to form a thicker slurry that slows the release of impacted water to the environment.\n\nThere are many different subsets of this method, including valley impoundments, ring dikes, in-pit impoundments, and specially dug pits. The most common is the valley pond, which takes advantage of the natural topographical depression in the ground. Large earthen dams may be constructed and then filled with the tailings. Exhausted open pit mines may be refilled with tailings. In all instances, due consideration must be made to contamination of the underlying water table, amongst other issues. Dewatering is an important part of pond storage, as the tailings are added to the storage facility the water is removed - usually by draining into decant tower structures. The water removed can thus be reused in the processing cycle. Once a storage facility is filled and completed, the surface can be covered with topsoil and revegetation commenced. However, unless a non-permeable capping method is used, water that infiltrates into the storage facility will have to be continually pumped out into the future.\n\nPaste tailings is a modification to the conventional methods of disposal of tailings (pond storage). Conventional tailings slurries are composed of a low percent of solids and relatively high water content (normally ranging from 20% to 60% solids for most hard rock mining) and when deposited into the tailings pond the solids and liquids separate. In paste tailings the percent of solids in the tailings slurry is increased through the use of paste thickeners to produce a product where the minimal separation of water and solids occurs and the material is deposited into a storage area as a paste (with a consistency somewhat like toothpaste). Paste tailings has the advantage that more water is recycled in the processing plant and therefore the process is more water efficient than conventional tailings and there is a lower potential for seepage. However the cost of the thickening is generally higher than for conventional tailings and the pumping costs for the paste are also normally higher than for conventional tailings as positive displacement pumps are normally required to transport the tailings from the processing plant to the storage area. Paste tailings are used in several locations around the world including Sunrise Dam in Western Australia and Bulyanhulu Gold Mine in Tanzania.\n\nTailings do not have to be stored in ponds or sent as slurries into oceans, rivers or streams. There is a growing use of the practice of dewatering tailings using vacuum or pressure filters so the tailings can then be stacked. This saves water which potentially reduces the impacts on the environment in terms of a reduction in the potential seepage rates, space used, leaves the tailings in a dense and stable arrangement and eliminates the long-term liability that ponds leave after mining is finished. However although there are potential merits to dry stacked tailings these systems are often cost prohibitive due to increased capital cost to purchase and install the filter systems and the increase in operating costs (generally associated electricity consumption and consumables such as filter cloth) of such systems.\n\nWhile disposal into exhausted open pits is generally a straightforward operation, disposal into underground voids is more complex. A common modern approach is to mix a certain quantity of tailings with waste aggregate and cement, creating a product that can be used to backfill underground voids and stopes. A common term for this is HDPF - High Density Paste Fill. HDPF is a more expensive method of tailings disposal than pond storage, however it has many other benefits – not just environmental but it can significantly increase the stability of underground excavations by providing a means for ground stress to be transmitted across voids - rather than having to pass around them – which can cause mining induced seismic events like that suffered previously at the Beaconsfield Mine Disaster.\n\nUsually called RTD – Riverine Tailings Disposal. In most environments, not a particularly environmentally sound practice, it has seen significant utilisation in the past, leading to such spectacular environmental damage as done by the Mount Lyell Mining and Railway Company in Tasmania to the King River, or the poisoning from the Panguna mine on Bougainville Island, which led to large-scale civil unrest on the island, and the eventual permanent closing of the mine.\n\nAs of 2005, only three mines operated by international companies continued to use river disposal: The Ok Tedi mine, the Grasberg mine and the Porgera mine, all on New Guinea. This method is used in these cases due to seismic activity and landslide dangers which make other disposal methods impractical and dangerous.\n\nCommonly referred to as STD (Submarine Tailings Disposal) or DSTD (Deep Sea Tailings Disposal). Tailings can be conveyed using a pipeline then discharged so as to eventually descend into the depths. Practically, it is not an ideal method, as the close proximity to off-shelf depths is rare. When STD is used, the depth of discharge is often what would be considered shallow, and extensive damage to the seafloor can result due to covering by the tailings product. It is also critical to control the density and temperature of the tailings product, to prevent it from travelling long distances, or even floating to the surface.\n\nThis method is used by the gold mine on Lihir Island; its waste disposal has been viewed by environmentalists as highly damaging, while the owners claim that it is not harmful.\n\nPhytostabilisation is a form of phytoremediation that uses hyperaccumulator plants for long-term stabilisation and containment of tailings, by sequestering pollutants in soil near the roots. The plant's presence can reduce wind erosion, or the plant's roots can prevent water erosion, immobilise metals by adsorption or accumulation, and provide a zone around the roots where the metals can precipitate and stabilise. Pollutants become less bioavailable and livestock, wildlife, and human exposure is reduced. This approach can be especially useful in dry environments, which are subject to wind and water dispersion.\n\nConsiderable effort and research continues to be made into discovering and refining better methods of tailings disposal. Research at the Porgera Gold Mine is focusing on developing a method of combining tailings products with coarse waste rock and waste muds to create a product that can be stored on the surface in generic-looking waste dumps or stockpiles. This would allow the current use of rivering disposal to cease. Considerable work remains to be done. However, co-disposal has been successfully implemented by several designers including AMEC at, for example, the Elkview Mine in British Columbia.\n\nAs mining techniques and the price of minerals improve, it is not unusual for tailings to be reprocessed using new methods, or more thoroughly with old methods, to recover additional minerals. Extensive tailings dumps of Kalgoorlie / Boulder in Western Australia were re-processed profitably in the 1990s by KalTails Mining.\n\nA machine called the PET4K Processing Plant has been used in a variety of countries for the past 20 years to remediate contaminated tailings.\n\nDuring extraction of the oil from oil sand, tailings consisting of water, silt, clays and other solvents are also created. This solid will become mature fine tailings by gravity. Foght \"et al\" (1985) estimated that there are 10 anaerobic heterotrophs and 10 sulfate-reducing prokaryotes per milliliter in the tailings pond, based on conventional most probable number methods. Foght set up an experiment with two tailings ponds and an analysis of the archaea, bacteria, and the gas released from tailings ponds showed that those were methanogens. As the depth increased, the moles of CH released actually decreased.\n\nSiddique (2006, 2007) states that methanogens in the tailings pond live and reproduce by anaerobic degradation, which will lower the molecular weight from naphtha to aliphatic, aromatic hydrocarbons, carbon dioxide and methane. Those archaea and bacteria can degrade the naphtha, which was considered as waste during the procedure of refining oil. Both of those degraded products are useful. Aliphatic, aromatic hydrocarbons and methane can be used as fuel in the humans’ daily lives. In other words, these methanogens improve the coefficient of utilization. Moreover, these methanogens change the structure of the tailings pond and help the pore water efflux to be reused for processing oil sands. Because the archaea and bacteria metabolize and release bubbles within the tailings, the pore water can go through the soil easily. Since they accelerate the densification of mature fine tailings, the tailings ponds are enabled to settle the solids more quickly so that the tailings can be reclaimed earlier. Moreover, the water released from the tailings can be used in the procedure of refining oil. Reducing the demand of water can also protect the environment from drought.\n\n\n"}
{"id": "45555991", "url": "https://en.wikipedia.org/wiki?curid=45555991", "title": "The Hand of Franklin", "text": "The Hand of Franklin\n\nThe Hand of Franklin is a 2015 Canadian documentary film by Frank Wolf that follows a four-person team attempting to row the Northwest Passage in order to shed light on climate change in the Arctic. The film won the award for 'Best Documentary Feature' at the 2016 Ramunas Atelier International Film Awards, won for 'Best Canadian Film' at the 2015 Vancouver International Mountain Film Festival (VIMFF) and won the 'Adventure Award' at the 2016 San Francisco International Ocean Film Festival. It features music by Peirson Ross, The Cyrillic Typewriter, Sylvia Cloutier and Madeleine Allakariallak and airs in Canada on CBC's \"documentary\" channel.\n"}
{"id": "29278635", "url": "https://en.wikipedia.org/wiki?curid=29278635", "title": "Trans-3-Methyl-2-hexenoic acid", "text": "Trans-3-Methyl-2-hexenoic acid\n\n\"trans\"-3-Methyl-2-hexenoic acid (TMHA) is an unsaturated short-chain fatty acid that occurs in sweat secreted by the axillary (underarm) apocrine glands of Caucasians and some Asians.\n\nHexanoic acids such as TMHA have an hircine odor. Of the fatty acids contributing to Caucasian men's underarm odor, TMHA has the most prominent odor.\n\nIt has long been claimed that schizophrenia patients exhibit a particular peculiar body odour, and it has been postulated the odour may be caused by underlying metabolic abnormalities associated with the condition, among other factors. Initial studies identified the causal component as TMHA, however, subsequent studies failed to reproduce such results, with subsequent researchers suggesting the initial research may have had misidentified impurities in samples as TMHA due to poor methodology. However, a 2007 study found schizophrenia patients to have reduced olfactory sensitivity to TMHA, possibly indicating sensory habituation; the decreased ability to smell the substance due to the presence of the substance as a constant component of subjects' own sweat/body odour. Furthermore, the researchers noted a positive association between reduced ability to smell TMHA and greater severity of disorganised and negative symptoms.\n\nAn allusion to TMHA and its purported link to the smell of the mentally ill is made in the 1996 David Foster Wallace novel, Infinite Jest.\n"}
{"id": "272510", "url": "https://en.wikipedia.org/wiki?curid=272510", "title": "Transmitter power output", "text": "Transmitter power output\n\nIn radio transmission, transmitter power output (TPO) is the actual amount of power (in watts) of radio frequency (RF) energy that a transmitter produces at its output.\n\nThis is not the amount of power that a radio station reports as its power, as in \"we're 100,000 watts of rock 'n' roll\", which is usually the effective radiated power (ERP). The TPO for VHF-/UHF-transmitters is normally more than the ERP, for LF-/MF-transmitters it has nearly the same value, while for VLF-transmitters it may be less.\n\nThe radio antenna's design \"focuses\" the signal toward the horizon, creating gain and increasing the ERP. There is also some loss (negative gain) from the feedline, which reduces some of the TPO to the antenna by both resistance and by radiating a small part of the signal.\n\nThe basic equation relating transmitter to effective power is: \n\nNote that in this formula the Antenna Gain is expressed with reference to a tuned dipole (dBd)\n\n"}
{"id": "203113", "url": "https://en.wikipedia.org/wiki?curid=203113", "title": "Tropical and subtropical dry broadleaf forests", "text": "Tropical and subtropical dry broadleaf forests\n\nThe tropical and subtropical dry broadleaf forest biome, also known as tropical dry forest, monsoon forest, vine thicket, vine scrub and dry rainforest is located at tropical and subtropical latitudes. Though these forests occur in climates that are warm year-round, and may receive several hundred centimeters of rain per year, they have long dry seasons which last several months and vary with geographic location. These seasonal droughts have great impact on all living animals and plants in the forest.\n\nDeciduous trees predominate in most of these forests, and during the drought a leafless period occurs, which varies with species type. Because trees lose moisture through their leaves, the shedding of leaves allows trees such as teak and mountain ebony to conserve water during dry periods. The newly bare trees open up the canopy layer, enabling sunlight to reach ground level and facilitate the growth of thick underbrush. Trees on moister sites and those with access to ground water tend to be evergreen. Infertile sites also tend to support evergreen trees. Three tropical dry broadleaf forest ecoregions, the East Deccan dry evergreen forests, the Sri Lanka dry-zone dry evergreen forests, and the Southeastern Indochina dry evergreen forests, are characterized by evergreen trees.\n\nThough less biologically diverse than rainforests, tropical dry forests are home to a wide variety of wildlife including monkeys, deer, large cats, parrots, various rodents, and ground dwelling birds. Mammalian biomass tends to be higher in dry forests than in rain forests, especially in Asian and African dry forests. Many of these species display extraordinary adaptations to the difficult climate.\n\nThis biome is alternately known as the tropical bane forest biome or the tropical and subtropical deciduous forest biome.\n\nDry forests tend to exist in the drier areas north and south of the tropical rainforest belt, south or north of the subtropical deserts, generally in two bands: one between 10° and 20°N latitude and the other between 10° and 20°S latitude. The most diverse dry forests in the world occur in southern Mexico and in the Bolivian lowlands. The dry forests of the Pacific Coast of northwestern South America support a wealth of unique species due to their dry climate. The Maputaland-Pondoland bushland and thickets along the east coast of South Africa are diverse and support many endemic species. The dry forests of central India and Indochina are notable for their diverse large vertebrate faunas. Madagascar dry deciduous forests and New Caledonia dry forests are also highly distinctive (pronounced endemism and a large number of relictual taxa) for a wide range of taxa and at higher taxonomic levels. Trees use underground water during the dry seasons.\n\nSpecies tend to have wider ranges than moist forest species, although in some regions many species do display highly restricted ranges; most dry forest species are restricted to tropical dry forests, particularly in plants; beta diversity and alpha diversity high but typically lower than adjacent moist forests.\n\nEffective conservation of dry broadleaf forests requires the preservation of large and continuous areas of forest. Large natural areas are required to maintain larger predators and other vertebrates, and to buffer sensitive species from hunting pressure. The persistence of riparian forests and water sources is critical for many dry forest species. Large swathes of intact forest are required to allow species to recover from occasional large events, like forest fires.\n\nDry forests are highly sensitive to excessive burning and deforestation; overgrazing and exotic species can also quickly alter natural communities; restoration is possible but challenging, particularly if degradation has been intense and persistent.\n\n\n\n\"Much material in this article has been reworked from Tropical and Subtropical Dry Broadleaf Forests, by WWF with their permission.\n\n"}
{"id": "35114318", "url": "https://en.wikipedia.org/wiki?curid=35114318", "title": "Vietnam Airlines Flight 831", "text": "Vietnam Airlines Flight 831\n\nVietnam Airlines Flight 831, a Tupolev Tu-134 crashed in a rice field near Semafahkarm Village, Tambon Khu Khot, Amphoe Lam Luk Ka, Pathum Thani, Thailand while operating a flight from Hanoi to Bangkok. The cause of the accident is undetermined, however the pilots reported the aircraft may have been struck by lightning. Three crew and 73 passengers died in the accident. This accident was the second worst accident at the time in Thailand, and is currently the fifth worst.\nAmong the dead are SRV Minister of Public Health and Mrs. and their daughter Hoa; Indian ambassador and Mrs. Arun B. Patwardhan and their 17-year-old son; David McAree, a Britisher formerly with Amnesty International and his wife, daughter of famed Vietnamese exiled writer ; Kiyokta Ida, second secretary of the Japanese embassy in Hanoi. Also aboard were Poles, French, Finns, Swedes, Burmese, Indians and Japanese (no Americans).\n"}
{"id": "403005", "url": "https://en.wikipedia.org/wiki?curid=403005", "title": "Wagon", "text": "Wagon\n\nA wagon (also alternatively and archaically spelt \"waggon\" in British and Commonwealth English) is a heavy four-wheeled vehicle pulled by draught animals or on occasion by humans (see below), used for transporting goods, commodities, agricultural materials, supplies and sometimes people.\n\nWagons are immediately distinguished from carts (which have two wheels) and from lighter four-wheeled vehicles primarily for carrying people, such as carriages. Wagons are usually pulled by animals such as horses, mules or oxen. They may be pulled by one animal or by several, often in pairs or teams. However, there are examples of human-propelled wagons, such as mining corfs.\n\nA wagon was formerly called a wain and one who builds or repairs wagons is a wainwright. More specifically, a wain is a type of horse- or oxen-drawn, load-carrying vehicle, used for agricultural purposes rather than transporting people. A wagon or cart, usually four-wheeled; for example, a haywain, normally has four wheels, but the term has now acquired slightly poetical connotations, so is not always used with technical correctness. However, a two-wheeled \"haywain\" would be a hay cart, as opposed to a carriage. \"Wain\" is also an archaic term for a chariot. \"Wain\" can also be a verb, to carry or deliver, and has other meanings.\n\nA person who drives wagons is called a \"wagoner\", a \"teamster\", a \"bullocky\", a \"muleskinner\", or simply a \"driver\".\n\nThe exact name and terminology used is often dependent on the design or shape of the wagon. If low and sideless may be called a dray, trolley or float. When traveling over long distances and periods, wagons may be covered with cloth to protect their contents from the elements; these are \"covered wagons\". If it has a permanent top enclosing it, it may be called a \"van\".\n\nTurning radius was a longstanding problem with wagons, dictated by the distance between the front wheels and the bed of the wagon—namely, the point where the rotating wheels collide with the side of the wagon when turning. Many earlier designs required a very large turning radius; however, shrinking the width of the bed means decreasing the size of the load. As this is a problem that carts (by virtue of their two-wheeled nature) do not face, this factor, combined with their lighter weight, meant that carts were long preferred over wagons for many uses.\n\nThe general solutions to this problem involved several modifications to the front-axle assembly. The front axle assembly of a wagon consists of an axle, a pair of wheels and a round plate with a pin in its centre that sits halfway between the wheels. A round plate with a hole in its centre is located on the underside of the wagon. The plate on the wagon, in turn, sits on the plate on the axle between the wheels. This arrangement allows the axle and wheels to turn horizontally. The pin and hole arrangement could be reversed. The horse harness is attached to this assembly. To enable the wagon to turn in as little space as possible, the front pair of wheels are often made smaller than the rear pair to allow them to turn close under the vehicle sides, and to allow them to turn still further, the wagon body may be \"waisted\". This technique eventually led to further designs well-adapted to narrow areas; the front wheels of express wagons, trolleys and floats are small enough to turn under the vehicle's body.\n\nWagons have served numerous purposes, with numerous corresponding designs. As with motorized vehicles, some are designed to serve as many functions as possible, while others are highly specialized. This section will discuss a broad overview of the general classes of wagons; for details on specific types of wagons, see the individual links.\n\nFarm wagons are built for general multi-purpose usage in an agricultural or rural setting. These include gathering hay, crops and wood, and delivering them to the farmstead or market.\n\nA common form found throughout Europe is the \"leiterwagen\" (\"ladder wagon\"), a large wagon where the sides often consist of ladders strapped in place to hold in hay or grain, though these could be removed to serve other needs. A common type of farm wagon particular to North America is the buckboard.\n\nFreight wagons are wagons used for the overland hauling of freight and bulk commodities.\n\nIn the United States and Canada, the Conestoga wagon was a predominant form of wagon used for hauling freight in the late 18th and 19th centuries, often used for hauling goods on the Great Wagon Road in the Appalachian Valley and across the Appalachian Mountains.\n\nEven larger freight wagons existed. For instance, the \"twenty-mule team\" wagons, used for hauling borax from Death Valley, could haul per pair. The wagons’ bodies were long and deep; the rear wheels were in diameter.\n\nA delivery wagon is a wagon used to deliver merchandise such as milk, bread, or produce to houses or markets, as well as to commercial customers, often in urban settings. The concept of express wagons and of paneled delivery vans developed in the 19th century. By the end of the 19th century, delivery wagons were often finely painted, lettered and varnished, so as to serve as advertisement for the particular business through the quality of the wagon. Special forms of delivery wagon include an ice wagon and a milk wagon.\n\nSome wagons are intended to serve as mobile residences or workshops. These include the Vardo, a traditional wagon of the 19th-century British Romani people.\n\nThe steam wagon, a self-powered development of the horse-drawn wagon, was a surprisingly late innovation, entering service only in the late nineteenth century.\n\nIn the city center of Schwäbisch Gmünd, Germany, since 1992 the city's plants are irrigated using a horse-drawn wagon with a water tank.\n\nIn migration and military settings, wagons were often found in large groups called wagon trains.\n\nIn warfare, large groups of supply wagons were used to support traveling armies with food and munitions, forming \"baggage trains\". During the American Civil War, these wagon trains would often be accompanied by the wagons of private merchants, known as sutlers, who sold goods to soldiers, as well as the wagons of photographers and news reporters. Special purpose-built support wagons existed for blacksmithing, telegraphy and even observation ballooning.\n\nIn migration settings, such as the emigrant trails of the American West and the Great Trek of South Africa, wagons would travel together for support, navigation and protection. A group of wagons may be used to create an improvised fort called a laager, made by circling them to form an enclosure. In these settings, a chuckwagon is a small wagon used for providing food and cooking, essentially a portable kitchen.\n\nAs a common, important element in history and life, wagons have been the subjects of artwork. Some examples are the paintings The Hay Wain, The Haywain Triptych and on the Oregon Trail Memorial half dollar.\n\nDuring a transition to mechanized vehicles from animal powered, the term wagon was sometimes used such as with the Duryea Motor Wagon. In modern times the term Station wagon survives as a type of automobile.\n\n\n"}
{"id": "20791906", "url": "https://en.wikipedia.org/wiki?curid=20791906", "title": "Zinc–zinc oxide cycle", "text": "Zinc–zinc oxide cycle\n\nThe zinc–zinc oxide cycle or Zn–ZnO cycle is a two step thermochemical cycle based on zinc and zinc oxide for hydrogen production with a typical efficiency around 40%.\n\nThe thermochemical two-step water splitting process uses redox systems:\n\n\nFor the first endothermic step concentrating solar power is used in which zinc oxide is thermally dissociated at into zinc and oxygen. In the second non-solar exothermic step zinc reacts at with water and produces hydrogen and zinc oxide. The temperature level is realized by using a solar power tower and a set of heliostats to collect the solar thermal energy.\n\n\n"}
