{"id": "27069590", "url": "https://en.wikipedia.org/wiki?curid=27069590", "title": "Abandonment cost", "text": "Abandonment cost\n\nAbandonment costs or Abandonment expenditure (ABEX) are costs associated with the abandonment of a business venture. \n\nAbandonment costs traditionally applied to the process of abandoning an under-producing or non-producing oil or gas well. In that context, it means the removal of equipment, plugging of the well with cement, any environmental clean-up, etc. necessary to shut the well down. It is occasionally referred to as \"Removal and Abandonment\" or R & A. The objective of well abandonment is to ensure that no hydrocarbons leak into surface water or into the atmosphere. The cost of a routine abandonment of a typical well in the United States is about $5,000 (~Texas average cost in year 2000). If a well has developed a leak that allows gas to flow up the outside of the well casing, finding and correcting the leak can push the cost of abandonment beyond $100,000. Wells that have been used as injectors or have been subject to fracking operation are more likely to develop leaks because the injected substances can create channels that permit uncontrolled flow outside the casing. \n\nThe term's application has been broadened from its original context to apply to the abandonment of other business ventures, primarily in manufacturing. It is often used in a cost-benefit analysis to determine if a marginal venture should be continued or if it is more financially beneficial to abandon the venture and plow the remaining money into something else in an attempt to recoup the losses. For example, General Motors had some abandonment costs from shutting down the Pontiac and Saturn brands. The existence of abandonment costs in an industry implies that there is no free exit from that industry.\n\n\n"}
{"id": "3155665", "url": "https://en.wikipedia.org/wiki?curid=3155665", "title": "Antiwear additive", "text": "Antiwear additive\n\nAW additives, or antiwear additives, are additives for lubricants to prevent metal-to-metal contact between parts of gears.\n\nEP additives are used in applications such as gearboxes, while AW additives are used with lighter loads such as bushings.\n\nSome popular AW additives are:\n\nSome formulations use colloidal PTFE (Teflon), but its efficiency is controversial.\n\nMany AW additives function as EP additives, for example organophosphates or sulfur compounds. The mechanism of function of TCP and ZDDP is explained in EP additives.\n\nUnder extreme pressure conditions, the performance of AW additives becomes insufficient and designated EP additives are required.\n\n\n"}
{"id": "5620935", "url": "https://en.wikipedia.org/wiki?curid=5620935", "title": "Center for Analysis and Prediction of Storms", "text": "Center for Analysis and Prediction of Storms\n\nThe Center for Analysis and Prediction of Storms (CAPS) was established at the University of Oklahoma in 1989 as one of the first eleven National Science Foundation Science and Technology Centers. Located at the National Weather Center in Norman, Oklahoma, its mission is the development of techniques for the computer-based prediction of high-impact local weather, such as individual spring and winter storms, with the NEXRAD (WSR-88D) Doppler weather radar serving as a key data source.\n\nSince 1989, scientists in CAPS have developed and improved ARPS (The Advanced Regional Prediction System). ARPS is a comprehensive regional to stormscale atmospheric modeling/prediction system. It is a complete system that includes a realtime data analysis and assimilation system, the forward prediction model and a post-analysis package. ARPS has been used successfully in many real thunderstorm cases research.\n\nCAPS, along with several other University of Oklahoma institutions, is a partner in a new Engineering Research Center led by the University of Massachusetts Amherst. The Engineering Research Center for Collaborative Adaptive Sensing of the Atmosphere (CASA) seeks to revolutionize the remote sensing of the lower troposphere, initially via inexpensive, low-power, phased array Doppler weather radars placed on cell towers and buildings. unique component of this project is that the sensors collaborate with one another and dynamically adjust their characteristics to sense multiple atmospheric phenomena while meeting multiple end user needs in an optimal manner. \n\nCAPS also is leading an NSF Large Information Technology Research (ITR) grant that seeks to develop an infrastructure for mesoscale meteorology research and education. Known as Linked Environments for Atmospheric Discovery (LEAD), a transforming element of this project is the ability for analysis tools, forecast models, and data repositories to function as dynamically adaptive, on-demand systems that can change configuration rapidly and automatically in response to the evolving weather; respond immediately to user decisions based upon the weather problem at hand; and steer remote observing systems to optimize data collection and forecast/warning quality.\n\n"}
{"id": "321396", "url": "https://en.wikipedia.org/wiki?curid=321396", "title": "Chevrolet Avalanche", "text": "Chevrolet Avalanche\n\nThe Chevrolet Avalanche was a four-door, five or six passenger pickup truck sharing GM's long-wheelbase chassis used on the Chevrolet Suburban and Cadillac Escalade ESV. Breaking with a long-standing tradition, the Avalanche was not available as a GMC, but only as a Chevrolet. Production of the Avalanche started in \nSeptember 2001 and ended April 2013; producing two generations in its lifespan.\n\nThe Avalanche was launched in September 2001 as a 2002 model on the GMT800 platform. First year Avalanches featured light gray plastic body cladding, intended to provide visual distinction from the Suburban/Yukon XL. Avalanche also gave the public an advance look at the next generation of front fascia designs for the entire GM line. A full-length chrome strip splits each lamp assembly and the grille, with a gold Chevrolet \"bow tie\" in the center. The hood and fenders featured aggressive folds, in contrast to the soft box of the other GMT800 models.\n\n2003 models featured a darker cladding, but GM's new president, Rick Wagoner, demanded removal of this \"unpopular\" trim (as did certain elements of the public). From mid-year, Avalanche could be ordered without the cladding. The uncladded model, known as the Without Body Hardware (or better by its acronym \"WBH\"), and alternatively called \"slicksides\" by GM marketers, resembles the '03-'05 Silverado in the front.\n\nAvalanche was nominated for the \"North American SUV of the Year\" award and was \"Motor Trend\" magazine's SUV of the Year for 2002.\n\nThere are two engine choices:\n\nThe Avalanche was originally marketed as being able to \"change from a SUV to a super SUV\". This was made possible by a plastic cover and an exclusive \"midgate\" which could open and close. The midgate was a divider behind the second row of seats that could be folded down, with the seats, to create a longer bed area, or folded up to make a larger cab area. A similar midgate was found on the Cadillac Escalade EXT, Hummer H2 SUT and the GMC Envoy XUV.\n\nFor those who planned to go off-road in their Avalanche, a Z71 Off-Road package was available that added off-road suspension, GM's AutoTrac full-time, push-button four-wheel-drive system, all-terrain white-lettered tires, seventeen-inch alloy wheels, leather seating surfaces with cloth accents, front dual power bucket seats, cloth door panel inserts, GM's OnStar telematics system, all-weather rubber floor mats, Chevrolet bowtie emblems embedded into the backrests of the seats (for 2002 models), dual-zone automatic climate controls with rear HVAC vents and remote controls, remote radio controls with headphone jacks for two pairs of headphones, a security system, gray-finished side running boards, power-heated side view mirrors with integrated turn signals, mud flaps, SRS side airbags, and more. Aside from these details, a Z71 Off-Road sticker on the rear side panels identified an Avalanche equipped with this package. Full leather seating surfaces were an available option, also removing the cloth door panel inserts, and also added heated front seats.\n\nFor those who liked the styling of the Z71 Off-Road package but did not plan to take their Avalanche off-road, a Z66 On-Road package was also available. The Z66 On-Road package featured all the equipment that the Z71 Off-Road package offered, but replaced the off-road suspension with sport on-road suspension, and did not offer four-wheel-drive. Aside from these details, a Z66 On-Road sticker on the rear side panels identified an Avalanche equipped with this package. As with the Z71 Off-Road package, full leather seating surfaces without the cloth accents were an available option, also removing the cloth door panel inserts, and also added heated front seats.\n\nTo further distinguish it from its Silverado siblings, the Avalanche was practically fully equipped, and only came with body side cladding. The Avalanche included features such as sixteen-inch alloy wheels and tires (chromed steel on the 2500 model), an AM/FM stereo with single-disc CD player and six speakers, cloth seating surfaces, front bench seat with power front driver's seat, power windows, power door locks, keyless entry, full instrumentation, dual front airbags, and air conditioning. Options included OnStar telematics system, cassette player, leather seating surfaces, front dual power bucket seats that could be heated, and side airbags for the front seats. A North Face Edition, in partnership with the North Face brand included limited exterior color options, a unique Ebony/Green leather interior, special exterior ornamentation and decals, white-faced gauges, OnStar telematics system, specially colored door panel inserts and speaker grilles, and more.\n\nFor 2003, the Avalanche was slightly changed. A new interior, shared with the Chevrolet Tahoe and Chevrolet Suburban, added a new, larger steering wheel with subsequent audio system and OnStar controls and cruise and speed controls, an optional six-speaker premium Bose sound system with external amplifier, a new gauge cluster with information center, memory for the front driver's seat (on some models), new radio head units, which included an AM/FM stereo with single-disc CD/MP3 player and Radio Data System or an AM/FM stereo with six-disc CD/MP3 changer and Radio Data System, optional XM Satellite Radio, standard OnStar telematics system, optional rear audio controls with heaphone jacks, an optional rear DVD entertainment system by Panasonic with wireless headphones, new warning chimes that played through the vehicle's audio system instead of through a separate speaker behind the dashboard, and newly available seventeen-inch alloy wheels and tires. A new WBH option, standing for Without Body Hardware, deleted the Avalanche's standard gray body-side cladding panels and replacing them with a thin black strip of plastic, for a cleaner exterior appearance. If an Avalanche owner did not want any black accents on the side of their trucks, the black strip of plastic could also be removed altogether.\n\nConversion van specialists Southern Comfort offered a conversion package for the Avalanche, which offered unique chrome-plated alloy wheels, a rear DVD entertainment system with satellite TV service, unique two-tone and fading paint jobs with exterior color-keyed lower trim and front and rear bumpers, unique seating surfaces, multiple wood interior trim options, and most of the features which were also offered on its conversion vans. The trucks also featured the Southern Comfort emblems on the exterior of the vehicle, and since they retained the same basic features as a factory Avalanche, a full General Motors warranty was offered, and the trucks could also be serviced at any Chevrolet dealership, as with the factory trucks.\n\nSoon after the release of the Chevrolet Avalanche, customers began to notice cosmetic problems with the cladding on their vehicles. Over time exposure to heat and sunlight would cause a chalky faded appearance. It was especially noticeable on the cargo bed panels, and sail-panel windows where \"Zebra Striping\" would appear. Customer reaction to this problem resulted in General Motors agreeing to a one time treatment of a product called ArmorDillo. This product would temporarily restore the cladding for a period of about 6 months. After that it would wear off then need to be re-applied. Realizing this was not a permanent solution, GM, together with Gatorback Coatings, developed a coating that could be applied to the cladding to restore it to a like-new shine. This product was designed to etch into the plastic and bond a new layer of tinted acrylic over the faded plastic. Customers within the 3yr/36,000 original factory warranty could go to their dealership to have it restored under their original warranty under GM TSB:04-08-111-001C. General Motors did not use side body cladding on the second generation model. General Motors has also identified the original source of the faded cladding.\nThe GMT900 Avalanche was introduced at the Chicago Auto Show in February 2006. Production of the redesigned Avalanche began at the Silao Assembly in April 2006. The Avalanche maintained the styled front end much like its sister vehicles, the Tahoe/Yukon and Suburban/Yukon XL, yet it still had the distinct midgate and integrated bed as found on the previous incarnation. It had all the same standard and available features as the Suburban and the Tahoe. The 2500 model of the previous generation was discontinued.\n\nA special Z71 package was offered for the second generation Avalanche. This off-road package consisted of a suspension tuned for rough terrain, an exclusive automatic locking rear differential, aluminum under body skid plates (visible from the front of the SUV), wheel flares, badges, wheels and tires.\n\nLater models introduced another version of the Vortec 5.3-liter V8 as the engine is now capable of running on E85 ethanol. When the 5.3 is running on normal gasoline, it produces and of torque whereas its output rises to and of torque on E85, up from and in 2009. For the 2010 model year, the 6.0-liter was dropped from the Avalanche lineup which left the 5.3 V8 as the only engine available for the SUV. In its final year, the 6.0-liter produced and of torque.\n\nA 2007 Avalanche was given away to the Most Valuable Player of the 2006 Major League Baseball All-Star Game, Michael Young.\n\nAn Avalanche is regularly featured on the popular US crime drama and is driven by some members of the team.\n\nIn addition, a 2007 Z-71 Avalanche placed second in Speed TV's reality TV series 'Bullrun' in 2009.\n\nThe second-generation Avalanche came in three well-equipped trim levels:\n\nThe LS served as the base model Avalanche. It included seventeen-inch alloy wheels and tires, cloth seating surfaces, power front driver's bucket seat, OnStar telematics system, an AM/FM stereo with single-disc CD/MP3 player and auxiliary audio input jack with six speakers, keyless entry, and black door handles, tailgate handle, and side mirrors, aluminum interior trim, front and side airbags, traction control, StabiliTrak, and more.\n\nThe LT served as the midlevel model Avalanche. It added power dual front bucket seats, remote start, XM Satellite Radio, Bluetooth hands-free telephone system, and exterior color-keyed door handles, tailgate handle, and side mirrors, wood interior trim, as well as other features.\n\nThe LTZ was the top-of-the-line model Avalanche. It added leather seating surfaces, security alarm, an AM/FM stereo with six-disc CD/MP3/DVD changer, an eight-speaker premium Bose CenterPoint amplified surround sound system, memory for the front driver's seat, twenty-inch polished alloy wheels and tires, and rear seat audio and video system controls,and the LTZ exclusive self-leveling Auto Ride suspension.\n\n2013 marked the Avalanche's last year of production. For 2013, all Avalanches, regardless of trim level, featured the Black Diamond Edition Package, which added Black Diamond Edition emblems to the rear pillars, replacing the previous trim level designation emblems, and also adding unique emblems inside the Avalanche commemorating its production from 2002 to 2013. The 2013 Avalanche was produced in limited numbers. Each 2013 Avalanche buyer received a limited-edition coffee table book with photographs of the owner's 2013 Avalanche, a copy of the window sticker from their vehicle, and a complete history of the Avalanche from 2002 to 2013. The book was shipped to either the dealership or owner's home after the owner took delivery of their new 2013 Avalanche. In addition, a newly available color, named Fairway Green Metallic, commemorated the last year of production for the Avalanche (2013).\n\nEngines:\n\nIn April 2012, GM announced that production of the Avalanche would end after the 2013 model year, after 2011 saw a sales decline of 2.6% to 20,088 vehicles. Production of the Cadillac Escalade EXT also ended after the 2013 model year.\n"}
{"id": "563728", "url": "https://en.wikipedia.org/wiki?curid=563728", "title": "Crystalloluminescence", "text": "Crystalloluminescence\n\nCrystalloluminescence is the effect of luminescence produced during crystallization.\n\nThe abstract of an article by B. P. Chandra, V. Kalia and S. C. Datt of Rani Durgavati University (Jabulpur, India), entitled \"Crystalloluminescence: a new tool to determine the critical size of a crystal nucleus\", states:\n\n"}
{"id": "4397993", "url": "https://en.wikipedia.org/wiki?curid=4397993", "title": "Decoupling capacitor", "text": "Decoupling capacitor\n\nA decoupling capacitor is a capacitor used to decouple one part of an electrical network (circuit) from another. Noise caused by other circuit elements is shunted through the capacitor, reducing the effect it has on the rest of the circuit. An alternative name is bypass capacitor as it is used to bypass the power supply or other high impedance component of a circuit.\n\nActive devices of an electronic system (transistors, ICs, vacuum tubes, for example) are connected to their power supplies through conductors with finite resistance and inductance. If the current drawn by an active device changes, voltage drops from power supply to device will also change due to these impedances. If several active devices share a common path to the power supply, changes in the current drawn by one element may produce voltage changes large enough to affect the operation of others - voltage spikes or ground bounce, for example - so the change of state of one device is coupled to others through the common impedance to the power supply. A decoupling capacitor provides a bypass path for transient currents, instead of flowing through the common impedance. \n\nThe decoupling capacitor works as the device’s local energy storage. The capacitor is placed between power line and ground to the circuit that current is to be provided. According to capacitor equation, formula_1, voltage drop between power line and ground results in current draw out from the capacitor to the circuit and when capacitance \"C\" is large enough, sufficient current is supplied to maintain an acceptable range of voltage drop. To reduce the effective series inductance, small and large capacitors are often placed in parallel; commonly positioned adjacent to individual integrated circuits. The capacitor stores a small amount of energy that can compensate for the voltage drop in the power supply conductors to the capacitor. \n\nIn digital circuits, decoupling capacitors also help prevent radiation of electromagnetic interference from relatively long circuit traces due to rapidly changing power supply currents. \n\nDecoupling capacitors alone may not suffice in such cases as a high-power amplifier stage with a low-level pre-amplifer coupled to it. Care must be taken in layout of circuit conductors so that heavy current at one stage does not produce power supply voltage drops that affect other stages. This may require re-routing printed circuit board traces to segregate circuits, or the use of a ground plane to improve stability of power supply.\n\nA bypass capacitor is often used to decouple a subcircuit from AC signals or voltage spikes on a power supply or other line. A bypass capacitor can shunt energy from those signals, or transients, past the subcircuit to be decoupled, right to the return path. For a power supply line, a bypass capacitor from the supply voltage line to the power supply return (neutral) would be used.\n\nHigh frequencies and transient currents can flow through a capacitor to circuit ground instead of to the harder path of the decoupled circuit, but DC cannot go through the capacitor and continues on to the decoupled circuit.\n\nAnother kind of decoupling is stopping a portion of a circuit from being affected by switching that occurs in another portion of the circuit. Switching in subcircuit A may cause fluctuations in the power supply or other electrical lines, but you do not want subcircuit B, which has nothing to do with that switching, to be affected. A decoupling capacitor can decouple subcircuits A and B so that B doesn't see any effects of the switching.\n\nIn a subcircuit, switching will change the load current drawn from the source. Typical power supply lines show inherent inductance, which results in a slower response to change in current. The supply voltage will drop across these parasitic inductances for as long as the switching event occurs. This transient voltage drop would be seen by other loads as well if the inductance between two loads is much lower compared to the inductance between the loads and the output of the power supply.\n\nTo decouple other subcircuits from the effect of the sudden current demand, a decoupling capacitor can be placed in parallel with the subcircuit, across its supply voltage lines. When switching occurs in the subcircuit, the capacitor supplies the transient current. Ideally, by the time the capacitor runs out of charge, the switching event has finished, so that the load can draw full current at normal voltage from the power supply and the capacitor can recharge. The best way to reduce switching noise is to design a PCB as a giant capacitor by sandwiching the power and ground planes across a dielectric material.\n\nSometimes parallel combinations of capacitors are used to improve response. This is because real capacitors have parasitic inductance, which distorts the capacitor's behavior at higher frequencies.\n\nTransient load decoupling as described above is needed when there is a large load that gets switched quickly. The parasitic inductance in every (decoupling) capacitor may limit the suitable capacity and influence appropriate type if switching occurs very fast.\n\nLogic circuits tend to do sudden switching (an ideal logic circuit would switch from low voltage to high voltage instantaneously, with no middle voltage ever observable). So logic circuit boards often have a decoupling capacitor close to each logic IC connected from each power supply connection to a nearby ground. These capacitors decouple every IC from every other IC in terms of supply voltage dips. \n\nThese capacitors are often placed at each power source as well as at each analog component in order to ensure that the supplies are as steady as possible. Otherwise, an analog component with poor power supply rejection ratio (PSRR) will copy fluctuations in the power supply onto its output. \n\nIn these applications, the decoupling capacitors are often called \"bypass capacitors\" to indicate that they provide an alternate path for high-frequency signals that would otherwise cause the normally steady supply voltage to change. Those components that require quick injections of current can \"bypass\" the power supply by receiving the current from the nearby capacitor. Hence, the slower power supply connection is used to charge these capacitors, and the capacitors actually provide the large quantities of high-availability current.\n\nA transient load decoupling capacitor is placed as close as possible to the device requiring the decoupled signal. This minimizes the amount of line inductance and series resistance between the decoupling capacitor and the device. The longer the conductor between the capacitor and the device, the more inductance is present.\n\nSince capacitors differ in their high-frequency characteristics (and capacitors with good high-frequency properties are often types with small capacity, while large capacitors usually have worse high-frequency response), decoupling often involves the use of a combination of capacitors. For example in logic circuits, a common arrangement is ~100 nF ceramic per logic IC (multiple ones for complex ICs), combined with electrolytic or tantalum capacitor(s) up to a few hundred μF per board or board section.\n\n\n"}
{"id": "43903351", "url": "https://en.wikipedia.org/wiki?curid=43903351", "title": "Deepwater Horizon (film)", "text": "Deepwater Horizon (film)\n\nDeepwater Horizon is a 2016 American disaster film based on the \"Deepwater Horizon\" explosion and oil spill in the Gulf of Mexico. It was directed by Peter Berg from a screenplay by Matthew Michael Carnahan and Matthew Sand. It stars Mark Wahlberg, Kurt Russell, John Malkovich, Gina Rodriguez, Dylan O'Brien, and Kate Hudson. It is adapted from a 2010 article by David Barstow, David Rohde, and Stephanie Saul.\n\nPrincipal photography began on April 27, 2015 in New Orleans, Louisiana. The film premiered at the 2016 Toronto International Film Festival and was theatrically released in the United States on September 30, 2016. It received generally positive reviews and grossed over $121 million worldwide. The film was nominated for two Oscars at the 89th Academy Awards: Best Sound Editing and Best Visual Effects, and a BAFTA Award for Best Sound at the 70th British Academy Film Awards.\n\nOn April 20, 2010, \"Deepwater Horizon\", an oil drilling rig operated by private contractor Transocean, is set to begin drilling off the southern coast of Louisiana on behalf of BP. Chief Electronics Technician Michael \"Mike\" Williams (Mark Wahlberg) and Offshore Installation Manager James \"Mr. Jimmy\" Harrell (Kurt Russell) are surprised to learn that the workers assigned to test the integrity of recently completed cement work are being sent home early, without conducting a cement bond log (CBL), at the insistence of BP managers Donald Vidrine (John Malkovich) and Robert Kaluza (Brad Leland). While Mike prepares the drilling team, including Caleb Holloway (Dylan O'Brien), Harrell meets with Vidrine and persuades him to conduct a test, which only serves to weaken the already poorly placed cement further. His patience thinning, and without waiting for Harrell to confirm the results, Vidrine orders the well to be flowed.\n\nAt first, the operation goes smoothly, but the cement job eventually fails completely, triggering a massive blowout that overpowers and kills the drill team members Keith Manuel, Shane Roshto, Roy Kemp, Karl Kleppinger, Adam Weise, and Gordon Jones.\n\nA chain of equipment malfunctions, coupled with a failed attempt to seal the well, ignites the oil, killing Dewey Revette, Stephen Curtis, Jason Anderson (Ethan Suplee), and Donald Clark. Andrea Fleytas (Gina Rodriguez), the rig's Dynamic Position Operator, tries to alert the Coast Guard, only to be overruled by her superior, Captain Curt Kuchta, on the grounds that the rig is not in any imminent danger. With oil now spewing into the ocean, a frightened, oil covered pelican flies into the bridge of a nearby vessel and dies, which heads towards the rig just as the workers begin a frantic evacuation. Harrell, still alive, although seriously injured in the explosion, is rescued by Mike and assumes control of the situation, only to discover that the rig cannot be saved. Aaron Dale Burkeen, a close friend of Mike's, sacrifices himself to keep a burning crane from collapsing onto the surviving crew, while Mike and Caleb are able to rescue Vidrine and Kaluza and get them to safety.\n\nAs night falls and the burning oil lights up the area, the Coast Guard becomes aware of the incident and sends a ship to collect the survivors, who are being ferried in the lifeboats to the \"Damon Bankston\". With all the lifeboats full, Mike locates the emergency life raft, but it becomes separated from the rig before he and Andrea can board, causing the latter to suffer a panic attack. Just as the oil in the well itself ignites and destroys the rig, the two jump into the water and are picked up by rescuers, who then ferry them to the \"Damon Bankston\".\n\nReturned home, the workers reunite with their families in a hotel lobby, during which the father of one of the dead crew members family angrily confronts Mike for failing to save his son, resulting in Mike having a panic attack himself.\n\nThe film ends with a series of clips showing the aftermath of the disaster, including testimony from the real-life Mike Williams and the revelation that not a single employee of either Transocean or BP was prosecuted for their actions. Pictures appear of the 11 men who lost their lives before the credits. The movie postscript reads: \"The blowout lasted for 87 days, spilling an estimated 210 million gallons of oil into the Gulf of Mexico. It was the worst oil disaster in U.S. history.\"\n\nOn March 8, 2011, it was announced that Summit Entertainment, Participant Media, and Image Nation had acquired the film rights to \"The New York Times\" article \"Deepwater Horizon's Final Hours\", written by David Barstow, David S. Rohde, and Stephanie Saul, and published on December 25, 2010, about the 2010 \"Deepwater Horizon\" explosion and subsequent oil spill. Matthew Sand was set to write the screenplay, while Lorenzo di Bonaventura was in talks to produce the film under his Di Bonaventura Pictures banner. Summit and Participant Media/Imagination would also finance the film. On acquiring the article to develop into a film, the president of Participant Media, Ricky Strauss said,\nThis is a perfect fit for us–a suspenseful and inspiring real-life account of everyday people whose values are tested in the face of an impending environmental disaster.\n\nOn July 24, 2012, Ric Roman Waugh was in talks with the studios to direct the film, Mark Vahradian was set to produce the film along with Bonaventura, and Lions Gate Entertainment also joined the project to produce and distribute. On July 11, 2014, it was announced that \"All Is Lost's\" director J. C. Chandor had been hired to direct the film; the screenplay's first draft was written by Sand, while Matthew Michael Carnahan wrote the second draft. In early October, it was confirmed that Summit would distribute the film, not Lionsgate. On January 30, 2015, it was reported that \"Lone Survivor\" director Peter Berg had replaced Chandor, and would re-team with Wahlberg on the film. Chandor exited due to creative differences.\n\nOn August 19, 2014, casting began, with actor Mark Wahlberg added in the lead role of the film. Wahlberg plays Mike Williams, a real-life electronics technician on the \"Deepwater Horizon\" oil rig. On March 18, 2015, Gina Rodriguez was set to play a woman named Andrea Fleytas, who was on the bridge on board the Deepwater Horizon at the time of the blowout, and frantically tried to contact the Coast Guard. On April 10, 2015, \"Deadline\" reported that Dylan O'Brien was in talks to play Caleb Holloway. Kurt Russell joined the film on the same day O'Brien was in talks. Soon after, John Malkovich was confirmed cast, as a BP representative who fatally underestimates the dangers of working on the rig. Kate Hudson was announced as a cast member in May, 2015, and playing the wife of Wahlberg's character; her role will be her first on-screen pairing with Russell, her stepfather, although they shared no dialogue in the film.\n\nPrincipal photography on the film began on April 27, 2015. It was officially announced by Lionsgate on May 18, 2015 that filming had begun in New Orleans, Louisiana.\n\nThe film cost a total of $156 million to produce, with $122 million spent in Louisiana. As a result, Lionsgate (the studio financing the film) received a $37.7 million subsidy from the state, under Louisiana's film incentive program. Later estimates put the amount at $110 million.\n\n\"Deepwater Horizon\" had its world premiere at the Toronto International Film Festival, on September 13, where it received a standing ovation from audiences after the screening. It opened in theaters on September 30, 2016, distributed by Summit Entertainment in the United States and Canada, and by Lionsgate Entertainment internationally.\n\n\"Deepwater Horizon\" grossed $61.4 million in the United States and Canada and $60.4 million in other territories for a worldwide total of $121.8 million, against a net production budget of around $110 million.\n\nIn the United States and Canada, \"Deepwater Horizon\" was projected to gross $16–20 million from 3,259 theaters in its opening weekend, although some publications noted Wahlberg's films tend to outperform box office projections. The film made $860,000 from its Thursday night previews at 2,400 theaters, and $7.1 million on its first day. In total, the film earned $20.2 million during its opening weekend, debuting at number two at the box office behind Tim Burton's \"Miss Peregrine's Home for Peculiar Children\". The film was released at a time when the marketplace was already dominated by two other adult-skewing pictures, \"The Magnificent Seven\" and \"Sully\". The film over-performed in the Gulf Coast region, and also did exceptionally well in IMAX, which earned $2.7 million of the film's total opening weekend.\n\nThe film's opening weekend was regarded as underwhelming and a disappointment, given its hefty production budget, but dramas released during the fall, and Wahlberg's films, tend to have box office legs. \"Forbes\" called the opening \"good, but not great\", especially considering the solid reviews. While most adult-skewing films would generally be made on a conservative budget in order to protect themselves financially, \"Deepwater Horizon\" was produced for $110–120 million (after tax rebates). Box office analyst Jeff Block said the film was \"a hard sell. This should have been a $60 million film. The budget was out of control.\" Recent real-life drama films such as \"Sully\" ($60 million budget), \"Bridge of Spies\" ($40 million) and \"Captain Phillips\" ($55 million) were made for more moderate amounts. \"The Hollywood Reporter\" noted that when Hollywood spends north of $100 million on a film, it is intended for a much broader audience, but that was not the case for \"Deepwater Horizon\", as the main demographics were adults, with 67% of the total ticket buyers during its opening weekend being over the age of 35. Another possible reason for the film's mediocre debut was its marketing miss, which was also a subject of criticism; from the outset, Lionsgate marketed the film as a heroic tale versus an issues-oriented movie. The name \"Deepwater Horizon\" itself is more associated with the aftermath of the spill than the heroics of the men who survived and helped their fellow workers. \"The Hollywood Reporter\" estimated the film lost at least $60 million, when factoring together all expenses and revenues.\n\nOutside North America, \"Deepwater Horizon\" opened simultaneously in 52 markets, and grossed $12.4 million, of which IMAX made up $1 million 119 IMAX screens. The U.K. was the top earning market, with $2.6 million, followed by the Middle East ($1.5 million), Taiwan ($1.4 million), Australia ($1.3 million), and Russia ($1.2 million). In China, the film opened on Tuesday, November 15, where it delivered a six-day opening weekend of $7.9 million, debuting in third place, behind local film \"I Am Not Madame Bovary\" and the continuation of \"Doctor Strange\". The next major markets to open were Germany (November 24) and Spain (November 25).\n\n\"Deepwater Horizon\" received generally positive reviews from critics. On Rotten Tomatoes the film has an approval rating of 84% based on 222 reviews with an average rating of 7/10. The site's critical consensus reads, \"\"Deepwater Horizon\" makes effective use of its titular man-made disaster to deliver an uncommonly serious – yet still suitably gripping – action thriller.\" On Metacritic, the film has a score 68 out of 100 based on 52 critics, indicating \"generally favorable reviews\". Audiences polled by CinemaScore gave the film an average grade of \"A–\" on an A+ to F scale.\n\nTodd McCarthy of \"The Hollywood Reporter\" gave the film a positive review, writing, \"ruggedness and resilience counts for far more in the characterizations here than does nuance, and everyone delivers as required. From a craft and technical point of view, the film is all but seamless, a credit to the extra care taken to avoid a CGI look.\" Mike Ryan of \"Uproxx\" praised the film's performances and ability to make audiences angry at BP: \"I’ll be honest, I didn’t think we needed a movie about this subject. I’ve changed my mind. And, if nothing else, I hope it gets people angry again, because the people who did this to our planet, and killed 11 people in the process, got off too easy.\" Benjamin Lee of \"The Guardian\" praised Berg's direction as \"admirably, uncharacteristically restrained...[He] stages the action horribly well, capturing the panic and gruesome mayhem without the film ever feeling exploitative. It’s spectacularly constructed, yet it doesn’t forget about the loss of life, ensuring that, despite thin characterisation, the impact is felt.\"\n\nFormer crew members started their own crowd-funded documentary project before the film's release, out of frustration with factual liberties taken in the film script and in the media.\n\n"}
{"id": "23718712", "url": "https://en.wikipedia.org/wiki?curid=23718712", "title": "Doherty amplifier", "text": "Doherty amplifier\n\nThe Doherty amplifier is a modified class B radio frequency amplifier invented by William H. Doherty of Bell Telephone Laboratories Inc in 1936. In Doherty's day, within the Western Electric product line, the eponymous electronic device was operated as a linear amplifier with a driver which was modulated. In the 50,000 watt implementation, the driver was a complete 5,000 watt transmitter which could, if necessary, be operated independently of the Doherty amplifier and the Doherty amplifier was used to raise the 5,000 watt level to the required 50,000 watt level.\n\nThe amplifier was usually configured as a grounded-cathode, carrier-peak amplifier using two vacuum tubes in parallel connection, one as a class B carrier tube and the other as a class B peak tube (power transistors in modern implementations). The tubes' source (driver) and load (antenna) were split and combined through + and — 90 degree phase shifting networks. Alternate configurations included a grounded-grid carrier tube and a grounded-cathode peak tube whereby the driver power was effectively passed-through the carrier tube and was added to the resulting output power, but this benefit was more appropriate for the earlier and less efficient triode implementations rather than the later and more efficient tetrode implementations.\n\nAs successor to Western Electric Company, Inc. (WE) for radio broadcast transmitters, the Doherty concept was considerably refined by Continental Electronics Manufacturing Company of Dallas, Texas (CE).\n\nEarly Continental Electronics designs, by James O. Weldon and others, retained most of the characteristics of Doherty's amplifier but added medium-level screen-grid modulation of the driver (317B, et al.).\n\nThe ultimate refinement was the high-level screen-grid modulation scheme invented by Joseph B. Sainton.\n\nSainton's 317C series consisted of a class C carrier tube in parallel connection with a class C peak tube. The tubes' source (driver) and load (antenna) were split and combined through + and − 90-degree phase-shifting networks as in a Doherty amplifier. The unmodulated radio frequency carrier was applied to the control grids of both tubes. Carrier modulation was applied to the screen grids of both tubes but the screen grid bias points of the carrier and peak tubes were different and were established such that the peak tube was cut off when modulation was absent and the amplifier was producing rated unmodulated carrier power and both tubes were conducting and each tube was contributing twice the rated carrier power during 100% modulation as four times the rated carrier power is required to achieve 100% modulation. As both tubes were operated in class C, a significant improvement in efficiency was thereby achieved in the final stage. In addition, as the tetrode carrier and peak tubes required very little drive power, a significant improvement in efficiency within the driver was achieved as well. The commercial version of the Sainton amplifier employed a cathode-follower modulator, not the push-pull modulator disclosed in the patent, and the entire 50,000-watt transmitter was implemented using only nine total tubes of four tube types, all of these being general-purpose, a remarkable achievement, given that the 317C's most significant competitor, RCA's BTA-50H, was implemented using thirty-two total tubes of nine tube types, nearly one-half of these being special-purpose, employed only in the BTA-50H. Nearly 300 CE 317C transmitters were installed in North America alone, easily outdistancing all competitors combined.\n\n"}
{"id": "44750", "url": "https://en.wikipedia.org/wiki?curid=44750", "title": "Driftwood", "text": "Driftwood\n\nDriftwood is wood that has been washed onto a shore or beach of a sea, lake, or river by the action of winds, tides or waves. It is a form of marine debris or tidewrack.\n\nIn some waterfront areas, driftwood is a major nuisance. However, the driftwood provides shelter and food for birds, fish and other aquatic species as it floats in the ocean. Gribbles, shipworms and bacteria decompose the wood and gradually turn it into nutrients that are reintroduced to the food web. Sometimes, the partially decomposed wood washes ashore, where it also shelters birds, plants, and other species. Driftwood can become the foundation for sand dunes.\n\nMost driftwood is the remains of trees, in whole or part, that have been washed into the ocean, due to flooding, high winds, or other natural occurrences, or as the result of logging. There is also a subset of driftwood known as drift lumber. Drift lumber includes the remains of man-made wooden objects, such as buildings and their contents washed into the sea during storms, wooden objects discarded into the water from shore, dropped dunnage or lost cargo from ships (jetsam), and the remains of shipwrecked wooden ships and boats (flotsam). Erosion and wave action may make it difficult or impossible to determine the origin of a particular piece of driftwood.\n\nDriftwood can be used as part of decorative furniture or other art forms, and is a popular element in the scenery of fish tanks.\n\nThe EPA includes driftwood in its list of \"Items You Should Never Burn in Your [Wood-Burning] Appliance,\" because it will \"release toxic chemicals when burned\".\n\nAccording to Norse mythology, the first humans, Ask and Embla, were formed out of two pieces of driftwood, an ash and an elm, by the god Odin and his brothers, Vili and Vé.\n\nThe Vikings would cast wood into the sea before making landfall. And the location of the wood would be an indication as to where to build their mead halls. The wood used would found the high-seat pillars of the new hall.\n\nDriftwood carried by Arctic rivers was the main, or sometimes only, source of wood for some Inuit and other Arctic populations living north of the tree line until they came into regular contact with European traders. Traditional Inuit boats such as the kayak were fashioned from driftwood frames covered in skins. Wood that is burned today in these regions mainly consists of the remains of condemned wooden structures. Driftwood is still used as kindling by some.\n\nThe \"Old Man of the Lake\" in Crater Lake, Oregon is a full-size tree that has been bobbing vertically in the lake for more than a century. Due to the cold water of the lake, the tree has been well preserved.\n\nAlice Gray, the legendary \"Diana of the Dunes\", who fought to preserve the Indiana Dunes which contain quantities of driftwood named her college \"Driftwood\", and made all her furniture from driftwood.\n\nDriftwood Sculptures are sculptures that are made of driftwood on beaches or mudflats:\n\n"}
{"id": "3018846", "url": "https://en.wikipedia.org/wiki?curid=3018846", "title": "Drinking Water Inspectorate", "text": "Drinking Water Inspectorate\n\nThe Drinking Water Inspectorate (DWI) is a section of Department for Environment, Food and Rural Affairs (DEFRA) set up to regulate the public water supply companies in England and Wales.\n\nBased in Whitehall, it produces an annual report showing the quality of and problems associated with drinking water. Its remit is to assess the quality of drinking water in England and Wales, taking enforcement action if standards are not being met, and appropriate action when water is unfit for human consumption.\n\nIt is also responsible for reporting on drinking water quality to the European Union under the European Drinking Water Directive (DWD), Council Directive 98/83/EC, which concerns the quality of water intended for human consumption. In addition, it provides advice to DEFRA on the transposition of European water legislation in England and Wales.\n\n"}
{"id": "13575891", "url": "https://en.wikipedia.org/wiki?curid=13575891", "title": "Extracellular polymeric substance", "text": "Extracellular polymeric substance\n\nExtracellular polymeric substances (EPSs) are natural polymers of high molecular weight secreted by microorganisms into their environment. EPSs establish the functional and structural integrity of biofilms, and are considered the fundamental component that determines the physiochemical properties of a biofilm.\n\nEPSs are mostly composed of polysaccharides (exopolysaccharides) and proteins, but include other macro-molecules such as DNA, lipids and humic substances. EPSs are the construction material of bacterial settlements and either remain attached to the cell's outer surface, or are secreted into its growth medium. These compounds are important in biofilm formation and cells attachment to surfaces. EPSs constitute 50% to 90% of a biofilm's total organic matter.\n\nExopolysaccharides (also sometimes abbreviated EPSs) are high-molecular-weight polymers that are composed of sugar residues and are secreted by a microorganism into the surrounding environment. Microorganisms synthesize a wide spectrum of multifunctional polysaccharides including intracellular polysaccharides, structural polysaccharides and extracellular polysaccharides or exopolysaccharides. Exopolysaccharides generally consist of monosaccharides and some non-carbohydrate substituents (such as acetate, pyruvate, succinate, and phosphate). Owing to the wide diversity in composition, exopolysaccharides have found multifarious applications in various food and pharmaceutical industries. Many microbial EPSs provide properties that are almost identical to the gums currently in use. With innovative approaches, efforts are underway to supersede the traditionally used plant and algal gums by their microbial counterparts. Moreover, considerable progress has been made in discovering and developing new microbial EPSs that possess novel industrial significance.\n\nCapsular exopolysaccharides can protect pathogenic bacteria against desiccation and predation, and contribute to their pathogenicity. Bacteria existing in biofilms are less vulnerable compared to planktonic bacteria, as the EPS matrix is able to act as a protective diffusion barrier. The physical and chemical characteristics of bacterial cells can be affected by EPS composition, influencing factors such as cellular recognition, aggregation, and adhesion in their natural environments. Furthermore, the EPS layer acts as a nutrient trap, facilitating bacterial growth.\n\nThe exopolysaccharides of some strains of lactic acid bacteria, e.g., Lactococcus lactis subsp. cremoris, contribute a gelatinous texture to fermented milk products (e.g., Viili), and these polysaccharides are also digestible. An example for industrial use of exopolysaccharides is the application of dextran in panettone and other breads in the bakery industry.\n\nExopolysaccharides can facilitate the attachment of nitrogen-fixing bacteria to plant roots and soil particles, which mediates a symbiotic relationship. This is important for colonization of roots and the rhizosphere, which is a key component of soil food webs and nutrient cycling in ecosystems. It also allows for successful invasion and infection of the host plant.\n\nBacterial extracellular polymeric substances can aid in bioremediation of heavy metals as they have the capacity to adsorb metal cations, among other dissolved substances. This can be useful in the treatment of wastewater systems, as biofilms are able to bind to and remove metals such as copper, lead, nickel, and cadmium. The binding affinity and metal specificity of EPS varies depending on polymer composition, as well as environmental factors such as concentration and pH.\n\nIn a geomicrobiological context, EPS has been observed to affect precipitation of minerals, particularly carbonates. EPS may also bind to and trap particles in biofilm suspensions, which can restrict dispersion and element cycling. Sediment stability can be increased by EPS, as it influences cohesion, permeability, and erosion of the sediment. There is evidence that the adhesion and metal-binding ability of EPS affects mineral leaching rates in both environmental and industrial contexts. These interactions between EPS and the abiotic environment allow for EPS to largely impact biogeochemical cycling.\n\nDue to the growing need to find a more efficient and environmentally friendly alternative to conventional waste removal methods, industries are paying more attention to the function of bacteria and their EPSs in bioremediation.\n\nResearchers found that adding EPSs from cyanobacteria to wastewaters removes heavy metals such as copper, cadmium and lead.  EPSs alone can physically interact with these heavy metals and take them in through biosorption. The efficiency of removal can be optimized by treating the EPSs with different acids or bases first before adding them to the wastewaters. \n\nContaminated soils contain high levels of Polycyclic aromatic hydrocarbons (PAHs); EPSs from two bacteria, \"Zoogloea\" sp. and  \"Aspergillus niger\", are efficient at removing these toxic compounds. EPSs contain enzymes such as oxidoreductase and hydrolase, which are capable of  degrading PAHs. The amount of PAHs degradation depends on the concentration of EPSs added to the soil. This method proves to be low cost and highly efficient. \n\nIn recent years, EPSs from marine bacteria have been found to speed up the cleanup of oil spills. During the Deepwater Horizon oil spill in 2010, these EPS-producing bacteria were able to grow and multiply rapidly.  It was later found that their EPSs dissolved the oil and formed oil aggregates on the ocean surface, which sped up the cleaning process. These oil aggregates also  provided a valuable source of nutrients for other marine microbial communities. This let scientists modify and optimize the use of EPSs to clean up oil spills.\n\n\n\n"}
{"id": "21728202", "url": "https://en.wikipedia.org/wiki?curid=21728202", "title": "Flexoelectricity", "text": "Flexoelectricity\n\nFlexoelectricity is a property of a dielectric material whereby it exhibits a spontaneous electrical polarization induced by a strain gradient. Flexoelectricity is closely related to piezoelectricity, but while piezoelectricity refers to polarization due to uniform strain, flexoelectricity refers specifically to polarization due to strain that changes from point to point in the material. This nonuniform strain breaks centrosymmetry, meaning that unlike in piezoelectiricty, flexoelectric effects can occur in centrosymmetric crystal structures. Flexoelectricity is not the same as Ferroelasticity.\n\nThe electric polarization due to mechanical stress in a dielectric is given by\n\nwhere the first term corresponds to the direct piezoelectric effect and the second term corresponds to the flexoelectric polarization induced by the strain gradient.\n\nHere, the flexoelectric coefficient, formula_2, is a fourth-rank polar tensor and formula_3 is the coefficient corresponding to the direct piezoelectric effect.\n\n\n"}
{"id": "685946", "url": "https://en.wikipedia.org/wiki?curid=685946", "title": "Ford Nucleon", "text": "Ford Nucleon\n\nThe Ford Nucleon is a concept car developed by Ford in 1957 designed as a future nuclear-powered car, one of a handful of such designs during the 1950s and '60s. The concept was only demonstrated as a scale model. The design did not include an internal-combustion engine; rather, the vehicle was to be powered by a small nuclear reactor in the rear of the vehicle, based on the assumption that this would one day be possible by reducing sizes. The car was to use a steam engine powered by uranium fission similar to those found in nuclear submarines. \n\nThe mock-up of the car can be viewed at the Henry Ford Museum in Dearborn, Michigan.\n\nAt the time of the concept's unveiling, nuclear technology was relatively new and it was believed that soon nuclear-fission technology could be made compact and affordable such that nuclear fuel would become the primary energy source in the U.S. and gasoline would become obsolete. Ford envisioned a future where gas stations would be replaced with full service recharging stations, and that the vehicle would get 5000 miles before the reactor would have to be swapped out for a new one. These would be scaled down versions of the nuclear reactors that military submarines used at the time, utilizing uranium as the fissile material. Because the entire reactor would be replaced, Ford hypothesized that the owner would have multiple choices for reactors, such as a fuel efficient model or a high performance model, at each reactor change. Ultimately, the reactor would use heat to convert water into steam and the power train would be steam driven.\n\nThe Nucleon is the inspiration for nuclear cars in the \"Fallout\" video game franchise. For example, in-game billboards describe the fictional Chryslus Corvega Atomic V8 as having an \"Atomic V8\" engine. The game's depiction is more satirical, however, as the cars explode into an implausible mushroom cloud and release radiation when shot.\n\n"}
{"id": "27260107", "url": "https://en.wikipedia.org/wiki?curid=27260107", "title": "Freestream", "text": "Freestream\n\nThe freestream is the air far upstream of an aerodynamic body, that is, before the body has a chance to deflect, slow down or compress the air. Freestream conditions are usually denoted with a formula_1 symbol, e.g. formula_2, meaning the freestream velocity\n\n"}
{"id": "7935247", "url": "https://en.wikipedia.org/wiki?curid=7935247", "title": "George N. Hatsopoulos", "text": "George N. Hatsopoulos\n\nGeorge Nicholas Hatsopoulos (January 7, 1927 – September 20, 2018) was a Greek American mechanical engineer noted for his work in thermodynamics and for having co-founded Thermo Electron.\n\nHatsopoulos was born in Athens, Greece in 1927 and is related to the former rector of the Athens Polytechnic School, Nicolas Kitsikis. He attended Athens Polytechnic before entering MIT, where received his Bachelor and Master of Science (1950), Mechanical Engineer (1954), and Doctorate of Science (1956).\n\nIn 1965, he and Joseph Keenan published their textbook \"Principles of General Thermodynamics\", which restates the second law of thermodynamics in terms of the existence of stable equilibrium states. Their formulation of the second law of thermodynamics states that:\n\nThe Hatsopoulos-Keenan statement of the Second Law entails the Clausius, Kelvin-Planck, and Carathéodory statements of the Second Law, and has provided a basis to extend the traditional definition of entropy to the non-equilibrium domain.\n\nIn 1976, Hatsopoulos also contributed to a formulation of a unified theory of mechanics and thermodynamics, arguably a precursor of the emerging field of quantum thermodynamics.\n\nWhile at MIT, Hatsopoulos was head of the Engineering Division of Matrad Corporation (New York). Matrad Corporation and MIT financially supported his doctoral thesis, \"The Thermo-Electron Engine\". Matrad corporation was owned by the family of Peter M. Nomikos, a Harvard Business School graduate. In 1956, Nomikos co-founded with Hatsopoulos the Thermo Electron Corporation. Thermo Electron became a major provider of analytical instruments and services for a variety of domains under the development of George Hatsopoulos, John Hatsopoulos, and Arvin Smith. In 1965, George Hatsopoulos was president of the Thermo Electron Engineering Corporation and Senior Lecturer in Mechanical Engineering at M.I.T..\n\nIn 1996, Hatsopoulos won the John Fritz Medal, which is the highest American award in the engineering profession and presented each year for scientific or industrial achievement in any field of pure or applied science. In 1997 he was awarded the 3rd Annual Heinz Award in Technology, the Economy and Employment. Mr. Hatsopoulos is also a recipient of The International Center in New York's Award of Excellence. In 2011, along with Arvin Smith and John Hatsopoulos, he was awarded the 2011 Pittcon Heritage Award from the Chemical Heritage Foundation. He died on September 20, 2018 at the age of 91.\n\n\n"}
{"id": "3931971", "url": "https://en.wikipedia.org/wiki?curid=3931971", "title": "Grüneisen parameter", "text": "Grüneisen parameter\n\nThe Grüneisen parameter, γ, named after Eduard Grüneisen, describes the effect that changing the volume of a crystal lattice has on its vibrational properties, and, as a consequence, the effect that changing temperature has on the size or dynamics of the lattice. The term is usually reserved to describe the single thermodynamic property γ, which is a weighted average of the many separate parameters γ entering the original Grüneisen's formulation in terms of the phonon nonlinearities.\n\nBecause of the equivalences between many properties and derivatives within thermodynamics (e.g. see Maxwell Relations), there are many formulations of the Grüneisen parameter which are equally valid, leading to numerous distinct yet correct interpretations of its meaning.\n\nSome formulations for the Grüneisen parameter include:\n\nformula_1\n\nwhere V is volume, formula_2 and formula_3 are the principal (i.e. per-mass) heat capacities at constant pressure and volume, E is energy, α is the volume thermal expansion coefficient, formula_4 and formula_5 are the adiabatic and isothermal bulk moduli, formula_6 is the speed of sound in the medium, and ρ is density. The Grüneisen parameter is dimensionless.\n\nThe expression for the Grüneisen constant of a perfect crystal with pair interactions in formula_7-dimensional space has the form:\n\nformula_8\n\nwhere formula_9 is the interatomic potential, formula_10 is the equilibrium distance, formula_7 is the space dimensionality. Relations between the Grüneisen constant and parameters of Lennard-Jones, Morse, and Mie potentials are presented in the table below.\n\nThe expression for the Grüneisen constant of a 1D chain with Mie potential exactly coincides with the results of MacDonald and Roy.\nUsing the relation between Grüneisen parameter and interatomic potential one can derive the simple necessary and sufficient condition for Negative Thermal Expansion in perfect crystals with pair interactions\n\nformula_12\n\nThe physical meaning of the parameter can also be extended by combining thermodynamics with a reasonable microphysics model for the vibrating atoms within a crystal. \nWhen the restoring force acting on an atom displaced from its equilibrium position is linear in the atom's displacement, the frequencies ω of individual phonons do not depend on the volume of the crystal or on the presence of other phonons, and the thermal expansion (and thus γ) is zero. When the restoring force is non-linear in the displacement, the phonon frequencies ω change with the volume formula_13. The Grüneisen parameter of an individual vibrational mode formula_14 can then be defined as (the negative of) the logarithmic derivative of the corresponding frequency formula_15:\n\nformula_16\n\nUsing the quasi-harmonic approximation for atomic vibrations, the macroscopic Grüneisen parameter (γ) can be related to the description of how the vibration frequencies (phonons) within a crystal are altered with changing volume (i.e. γ's). \nFor example, one can show that\n\nformula_17\n\nif one defines formula_18 as the weighted average\n\nformula_19\n\nwhere formula_20's are the partial vibrational mode contributions to the heat capacity, such that formula_21\n\nTo prove this relation, it is easiest to introduce the heat capacity per particle formula_22; so one can write\n\nformula_23.\n\nThis way, it suffices to prove\n\nformula_24.\n\nLeft-hand side (def):\n\nformula_25\n\nRight-hand side (def):\n\nformula_26\n\nFurthermore (Maxwell relations):\n\nformula_27\n\nThus\n\nformula_28\n\nThis derivative is straightforward to determine in the quasi-harmonic approximation, as only the ω are \"V\"-dependent.\n\nformula_29\n\nformula_30\n\nThis yields\n\nformula_31\n\n\n"}
{"id": "47731003", "url": "https://en.wikipedia.org/wiki?curid=47731003", "title": "Hedgerows Regulations 1997", "text": "Hedgerows Regulations 1997\n\nThe Hedgerows Regulations 1997 of England and Wales came into effect on 1st June 1997 and is government legislation which falls under the Environment Act 1995. It was created to protect hedgerows, in particular those in the countryside aged 30 years or older. Since the legislation came into effect it is a criminal offence to remove a hedgerow in contravention to the regulations.\n\nThe legislation includes sub-categories detailing specific descriptions of offences, the procedure of notification to the local planning authority, circumstances that exempt the need to notify, replacement and retention notices, appeals against those notices, local planning authority records of hedgerows, injunctions, and how hedgerows may be defined to be 'important'.\n"}
{"id": "58478119", "url": "https://en.wikipedia.org/wiki?curid=58478119", "title": "High-voltage shore connection", "text": "High-voltage shore connection\n\nA high-voltage shore connection (HVSC) is connection used to connect ships to the main grid, shutting engine of the ship and reducing carbon emissions. \n\nThe ship can use electric power for its consumption of energy. They are mostly used in the cruise ships which dock for longer time and hence save energy.\n"}
{"id": "55980727", "url": "https://en.wikipedia.org/wiki?curid=55980727", "title": "Huglin index", "text": "Huglin index\n\nPierre Huglin developed a bioclimatic heat index for vineyards, the Huglin heat sum index (or \"after Huglin\" respectively -warmth index or short Huglin index,) in which the temperature sum over the temperature threshold of 10 °C is calculated and then summed for all days from beginning of April to end of September. The calculation uses both the daily average temperatures and the maximum temperatures and slightly modifies the calculated total according to latitude. Each grape variety needs a certain amount of heat in order to be cultivated successfully in the long term in a given area. The calculated heat sums, which are based on data from weather stations or from climate models, differ in that they are too low compared to the actual values in the vineyards. The index does not take into account e.g. thermally favoured hillsides where temperature values may be higher by about 1.5 °C to 2 °C.\n\nThe Huglin index is calculated as a product of the coefficient \"K\" and the sum of the arithmetic mean of daily mean- and daily maximum temperatures relative to the baseline temperature of 10 °C (taking into account all days from 1st April till 30th September):\n\nDue to climate change, the Huglin index will continue to rise over coming decades, and the suitability of an area for a specific grape variety will continue to change.\n\nWith increases in the heat sum, the variety of vines in the northern growing areas of Europe has already changed. Varieties that used to be cultivated only in wine-growing regions in the south have already gained a certain amount of significance in cultivation in Austria and Germany. \nThe varieties Cabernet Franc, Cabernet Sauvignon, Merlot and Syrah are already being successfully planted and cultivated in warmer regions of Austria. They were included in the quality wine assortment.\n\n\n"}
{"id": "45555423", "url": "https://en.wikipedia.org/wiki?curid=45555423", "title": "Infrared non-destructive testing of materials", "text": "Infrared non-destructive testing of materials\n\nActive thermography is an advanced nondestructive testing procedure, which uses a thermography measurement of a tested material thermal response after its external excitation. This principle can be used also for non-contact infrared non-destructive testing (IRNDT) of materials. The IRNDT method is based on an excitation of a tested material by an external source, which brings some energy to the material. Halogen lamps, flash-lamps, ultrasound generator or other sources can be used as the excitation source for the IRNDT. The excitation causes a tested material thermal response, which is measured by an infrared camera. It is possible to obtain information about the tested material surface and sub-surface defects or material inhomogeneities by using a suitable combination of excitation source, excitation procedure, infrared camera and evaluation method.\n\nModern thermographic systems with high-speed and high-sensitivity IR cameras extend the possibilities of the inspection method. Modularity of the systems allows their usage for research and development applications as well as in modern industrial production lines.\n\nThermovision nondestructive testing of components can be carried out on a wide range of various materials. Thermographic inspection of material can be regarded as a method of infrared defectoscopy, that is capable of revealing material imperfections such as cracks, defects, voids, cavities and other inhomogeneities. The thermographic testing can be provided on individual components in a laboratory or directly on technology facilities that are in duty.\n\nInfrared (IR) thermography is an analysis technique based on the detection of radiation in the IR wavelength spectrum. According to the black body radiation law, all objects with temperature greater than absolute zero emit IR radiation. The device detecting and composing a 2D image of the IR radiation is generally called an IR camera or a thermographic camera, also referred to as an infrared camera. The result of the thermographic recording is an image or a sequence, which corresponds to the intensity of the thermal radiation of the recorded object. The recording is called a thermogram. The intensity of the thermal radiation of the object is directly connected with the object temperature. The thermogram is therefore an image of the object surface temperature distribution. IR thermography is in the most cases used for non-contact measurement of spatial and time distribution of temperature fields.\n\nIR thermography has a number of advantages – it is non-contact measurement, it captures an area (similarly to the classical video camera in visible spectrum) or it can measure moving or rotating objects, even if the objects have a very high temperature. However, it should be noted that the IR radiation intensity detected by the infrared camera does not depend solely on the measured object temperature. The main drawback of IR thermography is the fact that the result is influenced by a number of factors such as the thermo-optical properties of the object (emissivity, transmissivity, reflectivity), ambient temperature, environment properties, etc. Especially the knowledge of the measured object optical properties are fundamental for an accurate temperature measurement. Determination of these properties is often a complicated task and it requires experiences and an appropriate equipment.\n\nThermography can be classified as qualitative or quantitative, and passive or active. Qualitative thermography usually does not require an accurate temperature measurement. It only evaluates temperature differences between particular components, between different spots on the same object or between the measured object and the background. Qualitative thermography has many important applications, for example a thermal leaks diagnostics, thermal components diagnostics, searching for persons or in medicine. In contrast, the goal of quantitative thermography is an accurate temperature measurement of inspected objects. Knowledge of thermo-optical properties of the measured objects is essential in this case. Moreover, the thermo-optical properties often depend on temperature and it is also necessary to take into account an influence of environment.\n\nImportant applications of quantitative thermography include temperature monitoring during thermal processing or determination of thermal boundary conditions for numerical simulations of thermal processes.\n\nBoth the qualitative and quantitative approaches can be applied in terms of passive or active thermography. If the object temperature is not artificially affected during its measuring, it is called the passive thermography. If an artificial excitation using an external source is applied on the measured object, it is called active thermography. The external excitation causes temperature contrasts associated with material inhomogeneities or defects occurrence or it can be used for material properties identification. The active thermography is the important technique used for finding defects in materials, so called infrared non-destructive testing (IRNDT). Active thermography can be also applied for material thermal properties determination.\n\nActive thermography uses an external source for measured object excitation, that means introducing an energy into the object. The excitation sources can be classified by the principles:\n\n\nVarious excitation sources can be used for the active thermography and nondestructive testing, for example laser heating, flash lamps, halogen lamps, electrical heating, ultrasonic excitation, eddy currents, microwaves, and others. The measured object can be heated by an external source directly, e.g. by halogen lamps or hot air. The material inhomogeneities or defects cause then a distortion of temperature field. This distortion is detected as temperature differences on the material surface. Another possibility is to use thermophysical processes in the material, when mechanical or electrical energy is transformed into thermal energy due to defects and inhomogeneities. It creates local temperature sources, which cause temperature differences detected on the object surface by infrared techniques. It is the case of ultrasound excitation for example.\n\nA lot of methods were developed for active thermography for the nondestructive testing measurement evaluation. The evaluation methods selection depends on application, used excitation source and excitation type (pulse, periodic, continuous). In the simplest case, the response is evident from a thermogram directly. However, it is necessary to use advanced analysis techniques in most cases. The most common methods include Lock-In, Pulse or Transient (Step thermography) evaluation techniques. Continuous excitation can also be used in some cases.\n\n\nA high-speed cooled infrared camera with a high sensitivity is commonly used for IRNDT applications. However, an uncooled bolometric infrared camera can be used for specific applications. It can significantly reduce acquisition costs of the measurement system.\n\nThe IR nondestructive testing system are usually modular. It means that various excitation sources can be combined with various infrared cameras and various evaluation methods depending on application, tested material, measuring time demands, size of a tested area, etc. The modularity allows universal usage of the system for various industrial, scientific and research applications.\n\nIRNDT (infra-red nondestructive testing) method is suitable for detection and inspection of cracks, defects, cavities, voids and inhomogeneities in material, it is also possible to use the method for inspection of welded joints of metal and plastic parts, inspection of solar cells and solar panels, determination of internal structure of material etc.\n\nThe main advantage of IRNDT method is availability for inspection of various materials in wide range of industrial and research applications. IRNDT measurement is fast, nondestructive and noncontact. Restrictive condition for IRNDT method is inspection depth combined with dimension and orientation of defect/crack/inhomogeneity in material.\n\nThe demonstration and calibration sample is made of carbon fiber-epoxy composite. There are six holes of different depths on one side to simulate defects at different depths in range from 1 to 4 mm under the surface. The IRNDT analysis is performed from the flat side.\n\nThe results from the flash-pulse analysis show that the holes appear in different time frames of the evaluation – according to their depth. So the flash-pulse analysis does not only detect the presence of defects but also determine their depth under the surface if a thermal diffusivity of the sample is known.\n\nLaser welding of plastics is a progressive technology of connecting materials with different optical properties. Classical methods for testing of welding performance and weld joints quality – such as the metallographic cut microscopic analysis or X-ray tomography – are not suitable for routine measurements. Pulse IRNDT analysis can be successfully used for weld inspection in many cases.\n\nThe images show an example of plastic parts inspection with a defective weld and with a correct weld. The gaps in the defective weld and the correct uninterrupted weld line are both well visible in the results of the IRNDT flash-pulse analysis.\n\nLaser beam welding is a modern technology of fusion welding. Currently finds its wide usage not only in the field of scientific research but also establishes itself in a variety of industries. Among the most frequent users belong the automotive industry, which due to its stable continuous innovation enables fast implementation of advanced technologies in their production. It is clear that laser welding significantly enhances engineering designs and thus brings a number of new products which previously could not be made by conventional methods.\n\nThe laser welding can produce quality welds of different types, both extremely thin and thick blanks. Weldable are common carbon steels, stainless steels, aluminum and its alloys, copper, titanium and last but not least, special materials and its combinations.\n\nAn integral part of the weldments production is a quality control. Unlike conventional non-destructive test methods, IRNDT is used not only after the laser welding process, but also during it. This makes possible to decide whether or not to the weldment comply with established quality criteria during manufacture process.\n\n"}
{"id": "2549545", "url": "https://en.wikipedia.org/wiki?curid=2549545", "title": "Lathe center", "text": "Lathe center\n\nA lathe center, often shortened to center, is a tool that has been ground to a point to accurately position a workpiece on an axis. They usually have an included angle of 60°, but in heavy machining situations an angle of 75° is used.\n\nThe primary use of a center is to ensure concentric work is produced; this allows the workpiece to be transferred between machining (or inspection) operations without any loss of accuracy. A part may be \"turned\" in a lathe, sent off for hardening and tempering and then ground \"between centers\" in a cylindrical grinder. The preservation of concentricity between the turning and grinding operations is crucial for quality work.\n\nA center is also used to support longer workpieces where the cutting forces would deflect the work excessively, reducing the finish and accuracy of the workpiece, or creating a hazardous situation.\n\nA center lathe has applications anywhere that a \"centered\" workpiece may be used; this is not limited to lathe usage but may include setups in dividing heads, cylindrical grinders, tool and cutter grinders or other related equipment. The term \"between centers\" refers to any machining operation where the job needs to be performed using centers.\n\nA center is inserted into a matching hole drilled by a center drill. The hole is conical near and at the surface, and cylindrical, deeper.\n\nA dead center (one that does not turn freely, i.e., \"dead\") may be used to support the workpiece at either the fixed or rotating end of the machine. When used in the fixed position, a dead center produces friction between the workpiece and center, due to the rotation of the workpiece. Lubrication is therefore required between the center and workpiece to prevent friction welding from occurring. Additionally the tip of the center may have an insert of cemented carbide which will reduce the friction slightly and allow for faster speeds. Dead centers are typically fully hardened to prevent damage to the important mating surfaces of the taper and to preserve the 60° angle of the nose. As tungsten carbide is much harder than steel, a carbide-tipped center has greater wear resistance than a steel center.\n\nSoft centers are a special version of the dead center in which the nose is deliberately left soft (unhardened) so that it may be readily machined to the correct angle prior to usage. This operation is performed on the headstock center to ensure that the center's axis is aligned with the spindle's axis.\n\nA revolving center, also known as a live center in some countries, is constructed so that the 60° center runs in its own bearings and is used at the non-driven or tailstock end of a machine. It allows higher turning speeds without the need for separate lubrication, and also greater clamping pressures. CNC lathes use this type of center almost exclusively and they may be used for general machining operations as well. Spring-loaded centers are designed to compensate for center variations, without damage to the work piece or center tip. This assures the operator of uniform constant tension while machining. Some live centers also have interchangeable shafts. This is valuable when situations require a design other than a 60° male tip. A live center, which may be hard or soft, is a plain center placed in the revolving mandrel; it moves and is therefore live.\n\nA pipe center, also known as a bull nose center is a type of live center which has a large diameter conical nose rather than a sharp point. This allows the center to be used in the bore of a pipe or other workpiece with a large interior diameter. While a pipe center ensures the workpiece remains concentric, its main advantage is that it supports the workpiece securely, and can be used for parts whose larger inner diameter prevents the use of a normal pointed center. Thin-walled material such as pipes easily collapses if excessive force is used at the chuck end.\n\nThere are two types of cup centers. The woodworking variety is a variation of the traditional live center. This type of cup center has a central point like a normal live center and also has a ring surrounding it. The ring supports the softer material around the center point and prevents the wood from splitting under pressure from the central point. A different variety of cup center is used for metalworking. The metalworking variety of cup center has a tapered hole rather than a conical point. It supports the part by making contact with the outside diameter of the end of the part, rather than using a center hole.\n\nA drive center, also known as a grip center, is used in the driving end of a machine (headstock). It is often used in woodworking or where softer materials are machined.\n\nIt consists of a dead center surrounded by hardened teeth, which bite into a softer workpiece allowing the workpiece to be driven directly by the center. This allows the full diameter of the workpiece to be machined in a single operation, in contrast with the usual requirement where a carrier is attached to the workpiece at the driven end. The use of modified shell end mills in a drive center, instead of hardened pins, enables better gripping and prevents breakdown time due to pin stop.\n\nA spring center is a metalworking lathe center for maintaining a cutting tool like a reamer or a tap, in axial alignment with a hole being worked on. It consists of a point backed by a spring to push the cutting tool into the workpiece.\n"}
{"id": "3848049", "url": "https://en.wikipedia.org/wiki?curid=3848049", "title": "Lattice energy", "text": "Lattice energy\n\nThe lattice energy of a crystalline solid is a measure of the energy released when ions are combined to make a compound. It is a measure of the cohesive forces that bind ions. Lattice energy is relevant to many practical properties including solubility, hardness, and volatility. The lattice energy is usually deduced from the Born–Haber cycle.\n\nThe lattice energy is exothermic, i.e., the value of \"ΔH\" is negative because it corresponds to the coalescing of infinitely separated gaseous ions in vacuum to form the ionic lattice. The lattice \"enthalpy\" is reported as a positive value.\nThe concept of lattice energy was originally developed for rocksalt-structured and sphalerite-structured compounds like NaCl and ZnS, where the ions occupy high-symmetry crystal lattice sites. In the case of NaCl, lattice energy is the energy released by the reaction\nwhich would amount to -786 kJ/mol.\n\nSome textbooks and the commonly used CRC Handbook of Chemistry and Physics define lattice energy with the opposite sign, i.e. the energy required to convert the crystal into infinitely separated gaseous ions in vacuum, an endothermic process. Following this convention, the lattice energy of NaCl would be +786 kJ/mol. The lattice energy for ionic crystals such as sodium chloride, metals such as iron, or covalently linked materials such as diamond is considerably greater in magnitude than for solids such as sugar or iodine, whose neutral molecules interact only by weaker dipole-dipole or van der Waals forces.\n\nThe relationship between the molar lattice energy and the molar lattice enthalpy is given by the following equation:\nwhere formula_2 is the molar lattice energy, formula_3 the molar lattice enthalpy and formula_4 the change of the volume per mole. Therefore, the lattice enthalpy further takes into account that work has to be performed against an outer pressure formula_5. \n\nThe lattice energy of an ionic compound depends upon charges of the ions that comprise the solid. More subtly, the relative and absolute sizes of the ions influence \"ΔH\".\n\nIn 1918 Born and Landé proposed that the lattice energy could be derived from the electric potential of the ionic lattice and a repulsive potential energy term.\n\nwhere\n\nThe Born–Landé equation shows that the lattice energy of a compound depends on a number of factors\nBarium oxide (BaO), for instance, which has the NaCl structure and therefore the same Madelung constant, has a bond radius of 275 picometers and a lattice energy of -3054 kJ/mol, while sodium chloride (NaCl) has a bond radius of 283 picometers and a lattice energy of -786 kJ/mol.\n\nThe Kapustinskii equation can be used as a simpler way of deriving lattice energies where high precision is not required.\n\nFor ionic compounds with ions occupying lattice sites with crystallographic point groups \"C\", \"C\"\"\", \"C\" or \"C\" (\"n\" = 2, 3, 4 or 6) the concept of the lattice energy and the Born–Haber cycle has to be extended. In these cases the polarization energy \"E\" associated with ions on polar lattice sites has to be included in the Born–Haber cycle and the solid formation reaction has to start from the already polarized species. As an example, one may consider the case of iron-pyrite FeS, where sulfur ions occupy lattice site of point symmetry group \"C\". The lattice energy defining reaction then reads\n\nwhere pol S stands for the polarized, gaseous sulfur ion. It has been shown that the neglection of the effect led to 15% difference between theoretical and experimental thermodynamic cycle energy of FeS that reduced to only 2%, when the sulfur polarization effects were included.\n\n"}
{"id": "237737", "url": "https://en.wikipedia.org/wiki?curid=237737", "title": "Leading-edge extension", "text": "Leading-edge extension\n\nA leading-edge extension (LEX) is a small extension to an aircraft wing surface, forward of the leading edge. The primary reason for adding an extension is to improve the airflow at high angles of attack and low airspeeds, to improve handling and delay the stall. A dog tooth can also improve airflow and reduce drag at higher speeds.\n\nA leading-edge slat is an aerodynamic surface running spanwise just ahead of the wing leading edge. It creates a leading edge slot between the slat and wing which directs air over the wing surface, helping to maintain smooth airflow at low speeds and high angles of attack. This delays the stall, allowing the aircraft to fly at a higher angle of attack. Slats may be made fixed, or retractable in normal flight to minimize drag.\n\nA dogtooth is a small, sharp zig-zag break in the leading edge of a wing. It is usually used on a swept wing, to generate a vortex flow field to prevent separated flow from progressing outboard at high angle of attack. The effect is the same as a wing fence. It can also be used on straight wings in a drooped leading edge arrangement.\n\nWhere the dogtooth is added as an afterthought, as for example on the Hawker Hunter and some variants of the Quest Kodiak, the dogtooth is created by adding an extension to the outer section of the leading edge.\n\nA leading edge cuff (or wing cuff) is a fixed aerodynamic device employed on fixed-wing aircraft to introduce a sharp discontinuity in the leading edge of the wing in the same way as a dogtooth. It also typically has a slightly drooped leading edge to improve low-speed characteristics.\n\nA leading-edge root extension (LERX) is a small fillet, typically roughly triangular in shape, running forward from the leading edge of the wing root to a point along the fuselage. These are often called simply leading-edge extensions (LEX), although they are not the only kind. To avoid ambiguity, this article uses the term LERX.\n\nOn a modern fighter aircraft LERX induce controlled airflow over the wing at high angles of attack, so delaying the stall and consequent loss of lift. In cruising flight the effect of the LERX is minimal. However at high angles of attack, as often encountered in a dog fight or during takeoff and landing, the LERX generates a high-speed vortex that attaches to the top of the wing. The vortex action maintains a smooth airflow over the wing surface well past the normal stall point at which the airflow would otherwise break up, thus sustaining lift at very high angles.\n\nLERX were first used on the Northrop F-5 \"Freedom fighter\" which flew in 1959, and have since become commonplace on many combat aircraft. The F/A-18 Hornet has especially large examples, as does the Sukhoi Su-27 and the CAC/PAC JF-17 Thunder. The Su-27 LERX help make some advanced maneuvers possible, such as the Pugachev's Cobra, the Cobra Turn and the Kulbit.\n\nA long, narrow sideways extension to the fuselage, attached in this position, is an example of a chine.\n\nLeading-edge vortex controller (LEVCON) systems are a continuation of leading-edge root extension (LERX) technology, but with actuation that allows the leading edge vortices to be modified without adjusting the aircraft's attitude . Otherwise they operate on the same principals as the LERX system to create lift augmenting leading edge vortices during high angle of attack flight.\n\nThis system has been incorporated in the Russian Sukhoi Su-57 and Indian HAL Tejas .\n\nThe LEVCONs actuation ability also improves its performance over the LERX system in other areas.\nWhen combined with the thrust vectoring controller (TVC), the aircraft controllability at extreme angles of attack is further increased, which assists in stunts which require supermaneuverability such as Pugachev's Cobra . \nAdditionally, on the Sukhoi Su-57 the LEVCON system is used for increased departure-resistance in the event of thrust vectoring controller (TVC) failure at a post-stall attitude. It can also be used for trimming the aircraft, and optimizing the lift to drag ratio during cruise.\n\n"}
{"id": "27475918", "url": "https://en.wikipedia.org/wiki?curid=27475918", "title": "MT Bunga Kelana 3", "text": "MT Bunga Kelana 3\n\nMT \"Bunga Kelana 3\" is an Aframax tanker built in 1998, owned and operated by AET Tanker Holdings, a subsidiary of Malaysian International Shipping Corporation (MISC) to transport crude oil from Bintulu, Sarawak.\n\n\"Bunga Kelana 3\" collided with the bulk freighter, , in the Singapore Strait, 13 km southeast of Changi Air Base (East) on May 25, 2010 at 6:10. No injuries were reported among the crew. The tanker captain said 2.000 tonnes of crude oil may have spilled into the sea. MV \"Waily\" was anchored in the Straits of Singapore. Traffic in the Straits of Singapore was not affected.\nThe Maritime and Port Authority of Singapore (MPA) directed three ships full of oil cleaning equipment to clean up the spilled oil. \nStuart Traver of Gaffney, Cline & Associates in Singapore, said the effects were minimal, and would have a different impact than the Deepwater Horizon oil spill.\n\n"}
{"id": "1618669", "url": "https://en.wikipedia.org/wiki?curid=1618669", "title": "Mercury-arc valve", "text": "Mercury-arc valve\n\nA mercury-arc valve or mercury-vapor rectifier or (UK) mercury-arc rectifier is a type of electrical rectifier used for converting high-voltage or high-current alternating current (AC) into direct current (DC). It is a type of cold cathode gas-filled tube, but is unusual in that the cathode, instead of being solid, is made from a pool of liquid mercury and is therefore self-restoring. As a result, mercury-arc valves were much more rugged and long-lasting, and could carry much higher currents than most other types of gas discharge tube.\n\nInvented in 1902 by Peter Cooper Hewitt, mercury-arc rectifiers were used to provide power for industrial motors, electric railways, streetcars, and electric locomotives, as well as for radio transmitters and for high-voltage direct current (HVDC) power transmission. They were the primary method of high power rectification before the advent of semiconductor rectifiers, such as diodes, thyristors and gate turn-off thyristors (GTOs) in the 1970s. These solid state rectifiers have since completely replaced mercury-arc rectifiers thanks to their higher reliability, lower cost and maintenance and lower environmental risk.\n\nIn 1882 Jemin and Meneuvrier observed the rectifying properties of a mercury arc. The mercury arc rectifier was invented by Peter Cooper Hewitt in 1902 and further developed throughout the 1920s and 1930s by researchers in both Europe and North America. Before its invention, the only way to convert AC current provided by utilities to DC was by using expensive, inefficient, and high-maintenance rotary converters or motor-generator sets. Mercury-arc rectifiers or \"converters\" were used for charging storage batteries, arc lighting systems, the DC traction motors for trolleybuses, trams, and subways, and electroplating equipment. The mercury rectifier was used well into the 1970s, when it was finally replaced by semiconductor rectifiers.\n\nOperation of the rectifier relies on an electrical arc discharge between electrodes in a sealed envelope containing mercury vapor at very low pressure. A pool of liquid mercury acts as a self-renewing cathode that does not deteriorate with time. The mercury emits electrons freely, whereas the carbon anodes emit very few electrons even when heated, so the current of electrons can only pass through the tube in one direction, from cathode to anode, which allows the tube to rectify alternating current.\n\nWhen an arc is formed, electrons are emitted from the surface of the pool, causing ionization of mercury vapor along the path towards the anodes. The mercury ions are attracted towards the cathode, and the resulting ionic bombardment of the pool maintains the temperature of the \"emission spot\", so long as a current of a few amperes continues.\n\nWhile the current is carried by electrons, the positive ions returning to the cathode allow the conduction path to be largely unaffected by the space charge effects which limit the performance of vacuum tubes. Consequently, the valve can carry high currents at low \"arc voltages\" (typically 20-30 V) and so is an efficient rectifier. Hot-cathode, gas discharge tubes such as the thyratron may also achieve similar levels of efficiency but heated cathode filaments are delicate and have a short operating life when used at high current.\n\nThe temperature of the envelope must be carefully controlled, since the behaviour of the arc is determined largely by the vapor pressure of the mercury, which in turn is set by the coolest spot on the enclosure wall. A typical design maintains temperature at and a mercury vapor pressure of 7 millipascals.\n\nThe mercury ions emit light at characteristic wavelengths, the relative intensities of which are determined by the pressure of the vapor. At the low pressure within a rectifier, the light appears pale blue-violet and contains much ultraviolet light.\n\nThe construction of a mercury arc valve takes one of two basic forms — the glass-bulb type and the steel-tank type. Steel-tank valves were used for higher current ratings above approximately 500 A.\n\nThe earliest type of mercury vapor electric rectifier consists of an evacuated glass bulb with a pool of liquid mercury sitting in the bottom as the cathode. Over it curves the glass bulb, which condenses the mercury that is evaporated as the device operates. The glass envelope has one or more arms with graphite rods as anodes. Their number depends on the application, with one anode usually provided per phase. The shape of the anode arms ensures that any mercury that condenses on the glass walls drains back into the main pool quickly to avoid providing a conductive path between the cathode and respective anode. \n\nGlass envelope rectifiers can handle hundreds of kilowatts of direct-current power in a single unit. A six-phase rectifier rated 150 amperes has a glass envelope approximately 600 mm (24 inches) high by 300 mm (12 inches) outside diameter. These rectifiers will contain several kilograms of liquid mercury. The large size of the envelope is required due to the low thermal conductivity of glass. Mercury vapor in the upper part of the envelope must dissipate heat through the glass envelope in order to condense and return to the cathode pool. Some glass tubes were immersed in an oil bath to better control the temperature.\n\nThe current-carrying capacity of a glass-bulb rectifier is limited partly by the fragility of the glass envelope (the size of which increases with rated power) and partly by the size of the wires fused into the glass envelope for connection of the anodes and cathode. Development of high-current rectifiers required leadwire materials and glass with very similar coefficients of thermal expansion in order to prevent leakage of air into the envelope. Current ratings of up to 500 A had been achieved by the mid-1930s, but most rectifiers for current ratings above this were realised using the more robust steel-tank design.\n\nFor larger valves, a steel tank with ceramic insulators for the electrodes is used, with a vacuum pump system to counteract slight leakage of air into the tank around imperfect seals. Steel-tank valves, with water cooling for the tank, were developed with current ratings of several thousand amps.\n\nLike glass-bulb valves, steel-tank mercury arc valves were built with only a single anode per tank (a type also known as the \"excitron\") or with multiple anodes per tank. Multiple-anode valves were usually used for multi-phase rectifier circuits (with 2, 3, 6 or 12 anodes per tank) but in HVDC applications, multiple anodes were often simply connected in parallel in order to increase the current rating.\n\nA conventional mercury-arc rectifier is started by a brief high-voltage arc within the rectifier, between the cathode pool and a starting electrode. By one of a number of means, the starting electrode is brought into contact with the pool and allowed to pass current through an inductive circuit. The contact with the pool is then broken, resulting in a high emf and an arc discharge.\n\nThe momentary contact between the starting electrode and the pool may be achieved by a number of methods, including:\n\nSince momentary interruptions or reductions of output current may cause the cathode spot to extinguish, many rectifiers incorporate an additional electrode to maintain an arc whenever the plant is in use. Typically, a two or three phase supply of a few amperes passes through small \"excitation anodes\". A magnetically shunted transformer of a few hundred VA rating is commonly used to provide this supply.\n\nThis excitation or \"keep-alive\" circuit was absolutely necessary for single-phase rectifiers such as the excitron and for mercury-arc rectifiers used in the high-voltage supply of radiotelegraphy transmitters, as current flow was regularly interrupted every time the Morse key was released.\n\nBoth glass and metal envelope rectifiers may have control grids inserted between the anode and cathode.\n\nInstallation of a control grid between the anode and the pool cathode allows control of the conduction of the valve, thereby giving control of the mean output voltage produced by the rectifier. Start of the current flow can be delayed past the point at which the arc would form in an uncontrolled valve. This allows the output voltage of a valve group to be adjusted by delaying the firing point, and allows controlled mercury-arc valves to form the active switching elements in an inverter converting direct current into alternating current.\n\nTo maintain the valve in the non-conducting state, a negative bias of a few volts or tens of volts is applied to the grid. As a result, electrons emitted from the cathode are repelled away from the grid, back towards the cathode, and so are prevented from reaching the anode. With a small positive bias applied to the grid, electrons pass through the grid, towards the anode, and the process of establishing an arc discharge can commence. However, once the arc has been established, it cannot be stopped by grid action, because the positive mercury ions produced by ionisation are attracted to the negatively charged grid and effectively neutralise it. The only way of stopping conduction is to make the external circuit force the current to drop below a (low) critical current.\n\nAlthough grid-controlled mercury-arc valves bear a superficial resemblance to triode valves, mercury-arc valves cannot be used as amplifiers except at extremely low values of current, well below the critical current needed to maintain the arc.\n\nMercury-arc valves are prone to an effect called \"arc-back\" (or \"backfire\"), whereby the valve conducts in the reverse direction when the voltage across it is negative. Arc-backs can be damaging or destructive to the valve, as well as creating high short-circuit currents in the external circuit, and are more prevalent at higher voltages. One example of the problems caused by backfire occurred in 1960 subsequent to the electrification of the Glasgow North Suburban Railway where steam services had to be re-introduced after several mishaps. For many years this effect limited the practical operating voltage of mercury-arc valves to a few kilovolts.\n\nThe solution was found to be to include grading electrodes between the anode and control grid, connected to an external resistor-capacitor divider circuit. Dr. Uno Lamm conducted pioneering work at ASEA in Sweden on this problem throughout the 1930s and 1940s, leading to the first truly practical mercury-arc valve for HVDC transmission, which was put into service on the 20 MW, 100 kV HVDC link from mainland Sweden to the island of Gotland in 1954.\n\nUno Lamm’s work on high voltage mercury-arc valves led him to be known as the \"Father of HVDC\" power transmission and inspired the IEEE to dedicate an award named after him, for outstanding contributions in the field of HVDC.\n\nMercury arc valves with grading electrodes of this type were developed up to voltage ratings of 150 kV. However, the tall porcelain column required to house the grading electrodes was more difficult to cool than the steel tank at cathode potential, so the usable current rating was limited to about 200–300 A per anode. Therefore, Mercury arc valves for HVDC were often constructed with four or six anode columns in parallel. The anode columns were always air-cooled, with the cathode tanks either water-cooled or air-cooled.\n\nSingle-phase mercury-arc rectifiers were rarely used because the current dropped and the arc could be extinguished when the AC voltage changed polarity. The direct current produced by a single-phase rectifier thus contained a varying component (ripple) at twice the power supply frequency, which was undesirable in many applications for DC. The solution was to use two-, three-, or even six-phase AC power supplies so that the rectified current would maintain a more constant voltage level. Polyphase rectifiers also balanced the load on the supply system, which is desirable for reasons of system performance and economy.\n\nMost applications of mercury-arc valves for rectifiers used \"full-wave\" rectification with separate pairs of anodes for each phase.\n\nIn full-wave rectification both halves of the AC waveform are utilised. The cathode is connected to the + side of the DC load, the other side being connected to the center tap of the transformer secondary winding, which always remains at zero potential with respect to ground or earth. For each AC phase, a wire from each end of that phase winding is connected to a separate anode \"arm\" on the mercury-arc rectifier. When the voltage at each anode becomes positive, it will begin to conduct through the mercury vapor from the cathode. As the anodes of each AC phase are fed from opposite ends of the centre tapped transformer winding, one will always be positive with respect to the center tap and both halves of the AC Waveform will cause current to flow in one direction only through the load. This rectification of the whole AC waveform is thus called \"full-wave rectification\".\n\nWith three-phase alternating current and full-wave rectification, six anodes were used to provide a smoother direct current. Three phase operation can improve the efficiency of the transformer as well as providing smoother DC current by enabling two anodes to conduct simultaneously. During operation, the arc transfers to the anodes at the highest positive potential (with respect to the cathode).\n\nIn HVDC applications, a full-wave three-phase bridge rectifier or \"Graetz-bridge\" circuit was usually used, each valve accommodated in a single tank.\n\nAs solid-state metal rectifiers became available for low-voltage rectification in the 1920s, mercury arc tubes became limited to higher voltage and especially high-power applications.\n\nMercury-arc valves were widely used until the 1960s for the conversion of alternating current into direct current for large industrial uses. Applications included power supply for streetcars, electric railways, and variable-voltage power supplies for large radio transmitters. Mercury-arc stations were used to provide DC power to legacy Edison-style DC power grids in urban centers until the 1950s. In the 1960s, solid-state silicon devices, first diodes and then thyristors, replaced all lower-power and lower voltage rectifier applications of mercury arc tubes.\n\nSeveral electric locomotives, including the New Haven EP5 and the Virginian EL-C, carried ignitrons on board to rectify incoming AC to traction motor DC.\nOne of the last major uses of mercury arc valves was in HVDC power transmission, where they were used in many projects until the early 1970s, including the HVDC Inter-Island link between the North and South Islands of New Zealand and the HVDC Kingsnorth link from Kingsnorth power station to London. However, starting about 1975, silicon devices have made mercury-arc rectifiers largely obsolete, even in HVDC applications. The largest ever mercury-arc rectifiers, built by English Electric, were rated at 150-kV, 1800 A and were used until 2004 at the Nelson River DC Transmission System high-voltage DC-power-transmission project. The valves for the Inter-Island and Kingsnorth projects used four anode columns in parallel, while those of the Nelson River project used six anode columns in parallel in order to obtain the necessary current rating. The Inter-Island link was the last HVDC transmission scheme in operation using mercury arc valves. It was formally decommissioned on 1 August 2012. The mercury arc valve converter stations of the New Zealand scheme were replaced by new thyristor converter stations. A similar mercury arc valve scheme, the HVDC Vancouver Island link was replaced by a three-phase AC link.\n\nMercury arc valves remain in use in some South African mines, Kenya (at Mombasa Polytechnic - Electrical & Electronic department), and on the Manx Electric Railway on the Isle of Man (mercury arc rectifier now decommissioned).\n\nMercury arc valves were used extensively in DC power systems on London Underground, and one was still observed to be in operation in 2004 at the disused deep-level air-raid shelter at Belsize Park. After they were no longer needed as shelters, Belsize Park and several other deep shelters were used as secure storage, particularly for music and television archives. This led to the mercury-arc rectifier at the Goodge Street shelter featuring in an early episode of Doctor Who as an alien brain, cast for its \"eerie glow\".\n\nSpecial types of single-phase mercury-arc rectifiers are the Ignitron and the . The Excitron is similar to other types of valve described above but depends critically on the existence of an excitation anode to maintain an arc discharge during the half-cycle when the valve is not conducting current. The Ignitron dispenses with excitation anodes by igniting the arc each time conduction is required to start. In this way, ignitrons also avoid the need for control grids.\n\nIn 1919, the book \"Cyclopedia of Telephony & Telegraphy Vol. 1\" described an amplifier for telephone signals that used a magnetic field to modulate an arc in a mercury rectifier tube. This was never commercially important.\nMercury compounds are toxic, highly persistent in the environment, and present a danger to humans and the environment. The use of large quantities of mercury in fragile glass envelopes presents a hazard of potential release of mercury to the environment should the glass bulb be broken. Some HVDC converter stations have required extensive clean-up to eliminate traces of mercury emitted from the station over its service life. Steel tank rectifiers frequently required vacuum pumps, which continually emitted small amounts of mercury vapor.\n\nIn some Frankenstein movies mercury arc rectifiers are shown .\n\n"}
{"id": "30578480", "url": "https://en.wikipedia.org/wiki?curid=30578480", "title": "Mohammed Saleh Al Sada", "text": "Mohammed Saleh Al Sada\n\nMohammed Saleh Abdulla Al Sada is the minister of energy and industry of Qatar and the chairman of Qatar Petroleum.\n\nSada graduated from the Qatar University with a bachelor of science degree in marine science and geology. He also holds a MSc and a PhD from the University of Manchester Institute of Science and Technology.\n\nAl Sada started his career at Qatar Petroleum in 1983. He served in various positions and was appointed as technical director of Qatar Petroleum in 1997. From 2006 to 2011 he served as the managing director of RasGas liquefied natural gas company. He is also the vice chairman of the board of the Qatar Chemical Company (Q-Chem) and Qatar Steel Company (QASCO), and the chairman of the board of directors of Qatar Metals Coating Company (Q-Coat). He has served as a member of the Qatar's permanent constitution preparation committee, the supreme education council, and the national committee for human rights.\n\nIn April 2007, Al Sada was appointed minister of state for energy and industry affairs and served in that position until 2011. \n\nOn 18 January 2011, he replaced Abdullah bin Hamad Al Attiyah in the post of minister of industry and energy. On 14 February 2011, he was appointed as chairman of the board and managing director of Qatar Petroleum. On 24 February 2011, he became the chairman of the RasGas's board of directors. \n\nAl Sada remained unchanged in the cabinet reshuffle in June 2013, which saw the change of the prime minister. Therefore, he is part of the cabinet led by prime minister Abdullah bin Nasser bin Khalifa Al Thani.\n\nAl Sada is chairman of Nakilat (Qatar Gas Transport Company).\n\nAl Sada is married and has two daughters and three sons.\n"}
{"id": "2078359", "url": "https://en.wikipedia.org/wiki?curid=2078359", "title": "Petroleum coke", "text": "Petroleum coke\n\nPetroleum coke, abbreviated coke or petcoke, is a final carbon-rich solid material that derives from oil refining, and is one type of the group of fuels referred to as cokes. Petcoke is the coke that, in particular, derives from a final cracking process—a thermo-based chemical engineering process that splits long chain hydrocarbons of petroleum into shorter chains—that takes place in units termed coker units. (Other types of coke are derived from coal.) Stated succinctly, coke is the \"carbonization product of high-boiling hydrocarbon fractions obtained in petroleum processing (heavy residues).\" Petcoke is also produced in the production of synthetic crude oil (syncrude) from bitumen extracted from Canada’s oil sands and from Venezuela's Orinoco oil sands.\n\nIn petroleum coker units, residual oils from other distillation processes used in petroleum refining are treated at a high temperature and pressure leaving the petcoke after driving off gases and volatiles, and separating off remaining light and heavy oils. These processes are termed \"coking processes\", and most typically employ chemical engineering plant operations for the specific process of delayed coking. \n\nThis coke can either be fuel grade (high in sulfur and metals) or anode grade (low in sulfur and metals). The raw coke directly out of the coker is often referred to as green coke. In this context, \"green\" means unprocessed. The further processing of green coke by calcining in a rotary kiln removes residual volatile hydrocarbons from the coke. The calcined petroleum coke can be further processed in an anode baking oven to produce anode coke of the desired shape and physical properties. The anodes are mainly used in the aluminium and steel industry.\n\nPetcoke is over 90% carbon and emits 5% to 10% more carbon dioxide (CO) than coal on a per-unit-of-energy basis when it is burned. As petcoke has a higher energy content, petcoke emits between 30 and 80 percent more CO than coal per unit of weight. The difference between coal and coke in CO production per unit of energy produced depends upon the moisture in the coal, which increases the CO per unit of energy – heat of combustion) and on the volatile hydrocarbons in coal and coke, which decrease the CO per unit of energy.\n\nThere are at least four basic types of petroleum coke, namely, needle coke, honeycomb coke, sponge coke and shot coke. Different types of petroleum coke have different microstructures due to differences in operating variables and nature of feedstock. Significant differences are also to be observed in the properties of the different types of coke, particularly ash and volatile matter contents.\n\nNeedle coke, also called acicular coke, is a highly crystalline petroleum coke used in the production of electrodes for the steel and aluminium industries and is particularly valuable because the electrodes must be replaced regularly. Needle coke is produced exclusively from either FCC decant oil or coal tar pitch. \n\nHoneycomb coke is an intermediate coke, with ellipsoidal pores that are uniformly distributed. Compared to needle coke, honeycomb coke has a lower coefficient of thermal expansion and a lower electrical conductivity.\n\nFuel-grade coke is classified as either sponge coke or shot coke morphology. While oil refiners have been producing coke for over 100 years, the mechanisms that cause sponge coke or shot coke to form are not well understood and cannot be accurately predicted. In general, lower temperatures and higher pressures promote sponge coke formation. Additionally, the amount of heptane insolubles present and the fraction of light components in the coker feed contribute.\n\nWhile its high heat and low ash content make it a decent fuel for power generation in coal-fired boilers, petroleum coke is high in sulfur and low in volatile content, and this poses environmental (and technical) problems with its combustion. Its gross calorific value (HHV) is nearly 8000 Kcal/kg which is twice the value of average coal used in electricity generation.. A common choice of sulfur recovering unit for burning petroleum coke is the SNOX Flue gas desulfurisation technology, which is based on the well-known WSA Process. Fluidized bed combustion is commonly used to burn petroleum coke. Gasification is increasingly used with this feedstock (often using gasifiers placed in the refineries themselves).\n\nCalcined petroleum coke (CPC) is the product from calcining petroleum coke. This coke is the product of the coker unit in a crude oil refinery. The calcined petroleum coke is used to make anodes for the aluminium, steel and titanium smelting industry. The green coke must have sufficiently low metal content to be used as anode material. Green coke with this low metal content is called anode-grade coke. When green coke has excessive metal content, it is not calcined and is used as fuel-grade coke in furnaces.\n\nA high sulfur content in petcoke reduces its market value, and may preclude its use as fuel due to restrictions on sulfur oxides emissions for environmental reasons. Methods have thus been proposed to reduce or eliminate the sulfur content of petcoke. Most of them involve the desorption of the inorganic sulfur present in the pores or surface of the coke, and the partition and removal of the organic sulfur attached to the aromatic carbon skeleton.\n\nPotential petroleum desulfurization techniques can be classified as follows:\n\nAs of 2011 there was no commercial process available to desulfurize petcoke.\n\nNearly pure carbon, petcoke is a potent source of carbon dioxide if burned.\n\nPetroleum coke may be stored in a pile near an oil refinery pending sale. For example, in 2013 a large stockpile owned by Koch Carbon near the Detroit River was produced by a Marathon Petroleum refinery in Detroit which had begun refining bitumen from the oil sands of Alberta in November 2012. Large stockpiles of petcoke also existed in Canada as of 2013, and China and Mexico were markets for petcoke exported from California to be used as fuel. As of 2013 Oxbow Corporation, owned by William I. Koch, was a major dealer in petcoke, selling 11 million tons annually.\n\nIn 2017 a quarter of US exports of the fuel went to India, an Associated Press investigation found. In 2016 this amounted to more than eight million metric tons, more than 20 times as much as in 2010. India's Environmental Pollution Control Authority tested imported petcoke in use near New Delhi, and found sulfur levels 17 times the legal limit.\n\nThe International Convention for Prevention of Pollution from Ships (MARPOL 73/78), adopted by the IMO, has mandated that marine vessels shall not consume residual fuel oils (bunker fuel, etc) with a sulfur content greater than 0.1% from the year 2020. Nearly 38% of residual fuel oils are consumed in the shipping sector. In the process of converting excess residual oils into lighter oils by coking processes, pet coke is generated as a byproduct. Pet coke availability is expected to increase in the future due to falling demand for residual oil. Pet coke is also used in methanation plants to produce synthetic natural gas, etc. in order to avoid a pet coke disposal problem.\n\nPetroleum coke is sometimes a source of fine dust, which can penetrate the filtering process of the human airway, lodge in the lungs and cause serious health problems. Studies have shown that petroleum coke itself has a low level of toxicity and there is no evidence of carcinogenicity.\n\nPetroleum coke can contain vanadium, a toxic metal. Vanadium was found in the dust collected in occupied dwellings near the petroleum coke stored next to the Detroit River. Vanadium is toxic in tiny quantities, 0.8 micrograms per cubic meter of air, according to the EPA.\n\nAccording to multiple EPA studies and analyses, petroleum coke has a low health hazard potential in humans. It does not have any observable carcinogenic, developmental, or reproductive effects. During animal case studies repeated-dose chronic inhalation did show respiratory inflammation due to dust particles, but not specific to petroleum coke.\n\n\n"}
{"id": "49122", "url": "https://en.wikipedia.org/wiki?curid=49122", "title": "Phoenix (mythology)", "text": "Phoenix (mythology)\n\nIn Greek mythology, a phoenix (; , \"phoînix\") is a long-lived bird that cyclically regenerates or is otherwise born again. \n\nAssociated with the Sun, a phoenix obtains new life by arising from the ashes of its predecessor. According to some sources, the phoenix dies in a show of flames and combustion, although there are other sources that claim that the legendary bird dies and simply decomposes before being born again. There are different traditions concerning the lifespan of the phoenix, but by most accounts the phoenix lived for 500 years before rebirth. Herodotus, Lucan, Pliny the Elder, Pope Clement I, Lactantius, Ovid, and Isidore of Seville are among those who have contributed to the retelling and transmission of the phoenix motif.\n\nIn the historical record, the phoenix \"could symbolize renewal in general as well as the sun, time, the Empire, metempsychosis, consecration, resurrection, life in the heavenly Paradise, Christ, Mary, virginity, the exceptional man, and certain aspects of Christian life\".\n\nThe modern English noun \"phoenix\" derives from Middle English \"phenix\" (before 1150), itself from Old English \"fēnix\" (around 750). Old English \"fēnix\" was borrowed from Medieval Latin \"phenix\", which is derived from Classical Latin \"phoenīx\". The Classical Latin \"phoenīx\" represents Greek φοῖνιξ \"phoinīx\".\n\nIn ancient Greece and Rome, the bird, phoenix, was sometimes associated with the similar-sounding Phoenicia, a civilization famous for its production of purple dye from conch shells. A late antique etymology offered by the 6th- and 7th-century CE archbishop Isidore of Seville accordingly derives the name of the phoenix from its allegedly purple-red hue. Because the costly purple dye from Phoenicia was associated with the upper classes in antiquity and, later, with royalty, in the medieval period the phoenix was considered \"the royal bird\".\n\nIn spite of these folk etymologies, with the deciphering of the Linear B script in the 20th century, the original Greek φοῖνιξ was decisively shown to be derived from Mycenaean Greek \"po-ni-ke\", itself open to a variety of interpretations.\n\nClassical discourse on the subject of the phoenix points to a potential origin of the phoenix in Ancient Egypt. In the 19th century scholastic suspicions appeared to be confirmed by the discovery that Egyptians in Heliopolis had venerated the Bennu, a solar bird observed in some respects to be similar to the Greek phoenix. However, the Egyptian sources regarding the bennu are often problematic and open to a variety of interpretations. Some of these sources may have actually been influenced by Greek notions of the phoenix, rather than the other way around.\n\nHerodotus, writing in the 5th century BC, gives a somewhat skeptical account of the phoenix:\n\nThe phoenix is sometimes pictured in ancient and medieval literature and medieval art as endowed with a nimbus, which emphasizes the bird's connection with the Sun. In the oldest images of phoenixes on record these nimbuses often have seven rays, like Helios (the sun Titan of Greek mythology). Pliny the Elder also describes the bird as having a crest of feathers on its head, and Ezekiel the Dramatist compared it to a rooster.\n\nAlthough the phoenix was generally believed to be colorful and vibrant, sources provide no clear consensus about its coloration. Tacitus says that its color made it stand out from all other birds. Some said that the bird had peacock-like coloring, and Herodotus's claim of the Phoenix being red and yellow is popular in many versions of the story on record. Ezekiel the Dramatist declared that the phoenix had red legs and striking yellow eyes, but Lactantius said that its eyes were blue like sapphires and that its legs were covered in yellow-gold scales with rose-colored talons.\n\nHerodotus, Pliny, Solinus, and Philostratus describe the phoenix as similar in size to an eagle, but Lactantius and Ezekiel the Dramatist both claim that the phoenix was larger, with Lactantius declaring that it was even larger than an ostrich.\n\nDante refers to the phoenix in \"Inferno\" Canto XXIV:\nIn the play \"Henry VIII\" by William Shakespeare and John Fletcher, the King says in , in flattering reference to his young daughter Elizabeth (who was to become Queen Elizabeth I):\n\nScholars have observed analogues to the phoenix in a variety of cultures. These analogues include the Hindu garuda and gandaberunda, the Russian firebird, the Persian Simorgh, Georgian paskunji, the Arabian Anka عنقاء, and from that, the Turkish Zümrüdü Anka, the Tibetan Me byi karmo, the Chinese fenghuang and zhu que, and the Japanese hō-ō.\n\n\n\n"}
{"id": "1371741", "url": "https://en.wikipedia.org/wiki?curid=1371741", "title": "Rabbet", "text": "Rabbet\n\nA rabbet or rebate is a recess or groove cut into the edge of a piece of machinable material, usually wood. When viewed in cross-section, a rabbet is two-sided and open to the edge or end of the surface into which it is cut.\n\nAn example of the use of a rabbet is in a glazing bar where it makes provision for the insertion of the pane of glass and putty. It may also accommodate the edge of the back panel of a cabinet. It is also used in door and casement window jambs, and for shiplap planking. A rabbet can be used to form a joint with another piece of wood (often containing a dado).\n\nThe word \"rabbet\" is from Old French \"rabbat\", \"a recess into a wall\", and \"rabattre\" \"to beat down\". According to the Oxford English Dictionary, \"In North America the more usual form is \"rabbet\"\". The form \"rebate\" is often pronounced the same way as \"rabbet\".\n\n"}
{"id": "2055164", "url": "https://en.wikipedia.org/wiki?curid=2055164", "title": "Radioluminescence", "text": "Radioluminescence\n\nRadioluminescence is the phenomenon by which light is produced in a material by bombardment with ionizing radiation such as alpha particles, beta particles, or gamma rays. Radioluminescence is used as a low level light source for night illumination of instruments or signage or other applications where light must be produced for long periods without external energy sources. Radioluminescent paint used to be used for clock hands and instrument dials, enabling them to be read in the dark. Radioluminescence is also sometimes seen around high-power radiation sources, such as nuclear reactors and radioisotopes.\n\nRadioluminescence occurs when an incoming particle of ionizing radiation collides with an atom or molecule, exciting an orbital electron to a higher energy level. The particle usually comes from the radioactive decay of an atom of a radioisotope, an isotope of an element which is radioactive. The electron then returns to its ground energy level by emitting the extra energy as a photon of light. A chemical that releases light of a particular color when struck by ionizing radiation is called a phosphor. Radioluminescent light sources usually consist of a radioactive substance mixed with, or in proximity to, a phosphor.\n\nSince radioactivity was discovered around the turn of the 20th century, the main application of radioluminescence has been in radioluminescent paint, used on watch and compass dials, gunsights, aircraft flight instrument faces, and other instruments, to allow them to be seen in the dark. Radioluminescent paint consists of a mixture of a chemical containing a radioisotope with a radioluminescent chemical (phosphor). The continuous radioactive decay of the isotope's atoms releases radiation particles which strike the molecules of the phosphor, causing them to give off light. The constant bombardment by radioactive particles causes the chemical breakdown of many types of phosphor, so radioluminescent paints lose some of their luminosity over their working life.\n\nThe latest generation of radioluminescent materials is based on tritium, a radioactive isotope of hydrogen with half-life of 12.32 years that emits very low-energy beta radiation. It is used on wristwatch faces, gun sights, and emergency exit signs. The tritium gas is contained in a small glass tube, coated with a phosphor on the inside. Beta particles emitted by the tritium strike the phosphor coating and cause it to fluoresce, emitting light, usually yellow-green.\n\nTritium is used because it is believed to pose a negligible threat to human health, in contrast to the previous radioluminescent source, radium \"(below)\", which proved to be a significant radiological hazard. The low-energy 5.7 keV beta particles emitted by tritium cannot pass through the enclosing glass tube. Even if they could, they are not able to penetrate human skin. Tritium is only a health threat if ingested. Since tritium is a gas, if a tritium tube breaks, the gas dissipates in the air and is diluted to safe concentrations.\nTritium has a half-life of 12.3 years, so the brightness of a tritium light source will decline to half its initial value in that time.\n\nIn the second half of the 20th century, radium was progressively replaced with paint containing promethium-147. Promethium is a low-energy beta-emitter, which, unlike alpha emitters like radium does not degrade the phosphor lattice, so the luminosity of the material will not degrade so quickly. It also does not emit the penetrating gamma rays which radium does. The half-life of Pm is only 2.62 years, so in a decade the radioactivity of a promethium dial will decline to only 1/16 of its original value, making it safer to dispose of, compared to radium with its half life of 1600 years. However, this short half-life meant that the luminosity of promethium dials also dropped by half every 2.62 years, giving them a short useful life, which led to promethium's replacement by tritium (above).\n\nPromethium-based paint was used to illuminate Apollo Lunar Module electrical switch tips and painted on control panels of the Lunar Roving Vehicle.\n\nThe first use of radioluminescence was in luminous paint containing radium, a natural radioisotope. Beginning in 1908, luminous paint containing a mixture of radium and copper-doped zinc sulfide was used to paint watch faces and instrument dials, giving a greenish glow. Phosphors containing copper-doped zinc sulfide (ZnS:Cu) yield blue-green light; copper and manganese-doped zinc sulfide (ZnS:Cu,Mn), yielding yellow-orange light, are also used. Radium-based luminescent paint is no longer used due to the radiation hazard posed to those manufacturing the dials. These phosphors are not suitable for use in layers thicker than 25 mg/cm, as the self-absorption of the light then becomes a problem. Furthermore, zinc sulfide undergoes degradation of its crystal lattice structure, leading to gradual loss of brightness significantly faster than the depletion of radium.\n\nZnS:Ag coated spinthariscope screens were used by Ernest Rutherford in his experiments discovering the atomic nucleus.\n\nRadium was used in luminous paint until the 1960s, when it was replaced with the other radioisotopes above due to health concerns. In addition to alpha and beta rays, radium emits penetrating gamma rays, which can pass through the metal and glass of a watch dial, and skin. A typical older radium wristwatch dial has a radioactivity of 3–10 kBq and could expose its wearer to an annual dose of 24 millisieverts if worn continuously. Another health hazard is its decay product, the radioactive gas radon, which constitutes a significant risk even at extremely low concentrations when breathed. Radium's long half-life of 1600 years means that surfaces coated with radium paint, such as watch faces and hands, remain a health hazard long after their useful life is over. There are still millions of luminous radium clock, watch, and compass faces and aircraft instrument dials owned by the public. The case of the \"Radium Girls\", workers in watch factories in the early 1920s who painted watch faces with radium paint and later contracted fatal cancer through ingesting radium when they pointed their brushes with their lips, increased public awareness of the hazards of radioluminescent materials, and radioactivity in general.\n\n"}
{"id": "4282953", "url": "https://en.wikipedia.org/wiki?curid=4282953", "title": "Retread", "text": "Retread\n\nRetread, also known as \"recap\", or a \"remold\" is a re-manufacturing process for tires that replace the tread on worn tires. Retreading is applied to casings of spent tires that have been inspected and repaired. It preserves about 90% of the material in spent tires and the material cost is about 20% compared to manufacturing a new one.\n\nSome applications for retreaded tires are airplanes, racing cars, buses and delivery trucks. Use of retreaded tires was common historically, but as of 2008, it was seldom used for passenger vehicles, mainly due to discomfort on the road, safety issues and cheaper tire brands surfacing on the market. About 17.6 million retreaded tires were sold in North America in 2006.\n\nThere are two main processes used for retreading tires, called Mold Cure and Pre Cure. Both processes start with the inspection of the tire, followed by non-destructive inspection method such as shearography to locate non-visible damage and embedded debris and nails. Some casings are repaired and some are discarded. Tires can be retreaded multiple times if the casing is in usable condition. Tires used for short delivery vehicles are retreaded more than long haul tires over the life of the tire body. Casings fit for retreading have the old tread buffed away to prepare for retreading.\n\nMaterial cost for a retreaded tire is about 20% that of making a new tire.\nAbout 90% of the original tires by weight is retained in retreaded tires. A 1997 study estimates that then current generation of commercial vehicles tires to last up to 600,000 miles if they're retreaded two to three times. \n\nPreviously prepared tread strip is applied to tire casing with cement. This method allows more flexibility in tire sizes and it is the most commonly used method, but results in a seam where the ends of the strip meet.\n\nRaw rubber is applied to the tire casing and it is then placed in a mold where tread is formed. A dedicated mold is required for each tire size and tread design. \n\nIn this subtype, retreading is also applied to the side walls. These tires are given entirely new branding and stamps. \n\nSome jurisdictions have regulations concerning tire retreading.\n\nIn Europe all retreads, by law, must be manufactured according to EC Regulation 108 (car tires) or 109 (commercial vehicle tires). As part of this regulation all tires must be tested according to the same load and speed criteria as those undergone by new tires.\n\nThe Land Fill Directive of 1999 banned tires in landfills in 2003, and banned shredded tires in 2006.\n\nThe Department of Transportation requires marking of a \"DOTR number\" which shows the name of the retreader and when it was retreaded. \n\nThe United States National Highway Traffic Safety Administration recognizes the public perception that retread tires frequently used by heavy vehicles are less safe than new tires as evidenced by tire debris frequently found on highways. The NHTSA is continuing research to determine the proportion of tire debris from retreads in comparison to new tires. Additionally, the NHTSA is researching the cause of tire failure and the crash safety problem posed by tire failures.\n\nFederal Executive Order 13149 supports the use of retread tires for economic and environmental efficiency by requiring federal vehicles to use retread tires after original factory equipped tires become non serviceable, but only when \"such products are reasonably available and meet applicable performance standards\".\n\nRetread tires in service lower the volume of raw materials required for the manufacturing of a new tire. This includes a pronounced reduction in the use of oil. In fact, the US EPA estimated a greater than 70% savings in oil used for a retread as compared to a new tire. This also means significant reductions in greenhouse gas emissions.\n\nIn addition to reducing the amount of raw materials extracted, retread tires also minimize the amount of waste that ends up in landfills. The latest figures by the US EPA indicate that over 11.2 M waste tires were dumped into the U.S. municipal solid waste stream. To understand this figure, it is equivalent to lining up passenger tires tread to tread from roughly Los Angeles to San Diego or Philadelphia to Washington DC. Because a retread tire prevents the need for manufacturing a new tire, significant environmental benefits are achieved. \n"}
{"id": "21382559", "url": "https://en.wikipedia.org/wiki?curid=21382559", "title": "River, Estuary and Coastal Observing Network", "text": "River, Estuary and Coastal Observing Network\n\nThe River, Estuary, and Coastal Observing Network (RECON) is a pioneering waterway observing system founded and maintained by Sanibel-Captiva Conservation.\n\nRECON is funded primarily by private donations to SCCF's Marine Laboratory. Seven fixed locations stream water quality data from in situ Satlantic instrument packages. The sensor array includes a chemical-free nitrate sensor (ISUS), a CTD (conductivity, temperature, and depth) and fluorometer package for depth, turbidity, temperature, dissolved oxygen, salinity, and chlorophyll a fitted with a bleach injection system (WETLabs, Water Quality Monitor), and a CDOM fluorometer. One of the stations also incorporates a Nortek Aquadopp 2-dimensional current profiler sampling at 1 MHz for flow measurements. Data are currently collected hourly and transmitted in near-real time via cellular modems to a dedicated SQL database and can be viewed also in near real-time and plotted at RECON's website.\n\nExtensive field testing by SCCF has led also to several innovative modifications to improve duration and robustness of these instruments. These include a non-toxic exterior anti-fouling paint, copper foil applied to all connectors and cables, and a custom designed support structure to attach to existing pilings. Other enhancements include a user-friendly website designed for scientists, decision-makers and the general public. Information disseminated through the website includes concise definitions of water quality parameters measured and how to interpret trends in water quality at high resolution. To maintain data quality control, water samples are collected to validate instrument settings and readings with traditional wet-chemistry methods. Water samples are analyzed by a state-certified Florida Department of Environmental Protection laboratory in Tallahassee, FL. SCCF plans to maintain the observing systems for the next three years or more, depending on funding.\n\n"}
{"id": "25528150", "url": "https://en.wikipedia.org/wiki?curid=25528150", "title": "S-Adenosylmethioninamine", "text": "S-Adenosylmethioninamine\n\n\"S\"-Adenosylmethioninamine (decarboxylated \"S\"-adenosyl methionine) is a substrate that is involved in the biosynthesis of polyamines including spermidine, spermine, and thermospermine. \n\n"}
{"id": "13298714", "url": "https://en.wikipedia.org/wiki?curid=13298714", "title": "Sea Quest", "text": "Sea Quest\n\nThe Sea Quest was a semi-submersible drilling rig. She discovered the UK's first North Sea oil on 14 September 1969 in the Arbroath Field. She also discovered the first giant oil field named Forties on 7 October 1970.\n\nThe \"Sea Quest\" was built by Belfast shipbuilders Harland and Wolff for BP at a cost of £3.5 million and launched on 8 January 1966.\nThe entire structure was high and weighed 150,000 tons, including three legs each in diameter and long that could be partially filled with water to control the height of the platform above the sea.\n\nIn 1977, \"Sea Quest\" was sold to Sedco (now part of Transocean) and renamed \"Sedco 135C\". She was towed to the west coast of Africa. On 17 January 1980, while drilling in the Warri area, Nigeria, a blowout occurred and the rig sustained extensive fire damage. The rig was then deliberately sunk in deep water.\n"}
{"id": "40442731", "url": "https://en.wikipedia.org/wiki?curid=40442731", "title": "Single-phase generator", "text": "Single-phase generator\n\nSingle-phase generator (also known as single-phase alternator) is an alternating current electrical generator that produces a single, continuously alternating voltage. Single-phase generators can be used to generate power in single-phase electric power systems. However, polyphase generators are generally used to deliver power in three-phase distribution system and the current is converted to single-phase near the single-phase loads instead. Therefore, single-phase generators are found in applications that are most often used when the loads being driven are relatively light, and not connected to a three-phase distribution, for instance, portable engine-generators. Larger single-phase generators are also used in special applications such as single-phase traction power for railway electrification systems.\n\nThe design of revolving armature generators is to have the armature part on a rotor and the magnetic field part on stator. A basic design, called elementary generator, is to have a rectangular loop armature to cut the lines of force between the north and south poles. By cutting lines of force through rotation, it produces electric current. The current is sent out of the generator unit through two sets of slip rings and brushes, one of which is used for each end of the armature. In this two-pole design, as the armature rotates one revolution, it generates one cycle of single phase alternating current (AC). To generate an AC output, the armature is rotated at a constant speed having the number of rotations per second to match the desired frequency (in hertz) of the AC output.\n\nThe relationship of armature rotation and the AC output can be seen in this series of pictures. Due to the circular motion of the armature against the straight lines of force, a variable number of lines of force will be cut even at a constant speed of the motion. At zero degrees, the rectangular arm of the armature does not cut any lines of force, giving zero voltage output. As the armature arm rotates at a constant speed toward the 90° position, more lines are cut. The lines of force are cut at most when the armature is at the 90° position, giving out the most current on one direction. As it turns toward the 180° position, lesser number of lines of force are cut, giving out lesser voltage until it becomes zero again at the 180° position. The voltage starts to increase again as the armature heads to the opposite pole at the 270° position. Toward this position, the current is generated on the opposite direction, giving out the maximum voltage on the opposite side. The voltage decrease again as it completes the full rotation. In one rotation, the AC output is produced with one complete cycle as represented in the sine wave.\n\nMore poles can also be added to single-phase generator to allow one rotation to produce more than one cycle of AC output. In an example on the left, the stator part is reconfigured to have 4 poles which are equally spaced. A north pole is adjacent to the two south poles. The shape of the armature at the rotor part is also changed. It is no longer a flat rectangle. The arm is bent 90 degrees. This allows one side of the armature to interact with a north pole while the other side interacts with a south pole similarly to the two-pole configuration. The current is still delivered out through the two sets of slip rings and brushes in the same fashion as in the two-pole configuration. The difference is that a cycle of AC output can be completed after a 180 degree rotation of the armature. In one rotation, the AC output will be two cycles. This increases the frequency of the output of the generator. More poles can be added to achieves higher frequency at the same rotation speed of the generator, or same frequency of output at the lower rotation speed of the generator depending on the applications.\n\nThis design also allows us to increase the output voltage by modifying the shape of the armature. We can add more rectangular loops to the armature as seen on the picture on the right. The additional loops at the armature arm are connected in series, which are actually additional windings of the same conductor wire to form a coil in rectangular shape. In this example, there are 4 windings in the coil. Since the shapes of all windings are the same, the amount of the lines of force will be cut at the same amount in the same direction at the same time in all windings. This creates in phase AC output for these 4 windings. As a result, the output voltage is increased 4 time as shown in the sine wave in the diagram.\n\nThe design of revolving field generators is to have the armature part on stator and the magnetic field part on rotor. A basic design of revolving field single-phase generator is shown on the right. There are two magnetic poles, north and south, attached to a rotor and two coils which are connected in series and equally spaced on stator. The windings of the two coils are in reverse direction to have the current to flow in the same direction because the two coils always interact with opposing polarities. Since poles and coils are equally spaced and the locations of the poles match to the locations of the coils, the magnetic lines of force are cut at the same amount at any degree of the rotor. As a result, the voltages induced to all windings have the same value at any given time. The voltages from both coils are \"in phase\" to each other. Therefore the total output voltage is two times the voltage induced in each winding. In the figure, at the position where pole number 1 and coil number 1 meet, the generator produces the highest output voltage on one direction. As the rotor turns 180 degrees, the output voltage is alternated to produce the highest voltage on the other direction. The frequency of the AC output in this case equals to the number of rotations of the rotor per second.\n\nThis design can also allow us to increase the output frequency by adding more poles. In this example on the right, we have 4 coils connected in series on the stator and the field rotor has 4 poles. Both coils and poles are equally spaced. Each pole has opposite polarity to its neighbors which are angled at 90 degrees. Each coils also have opposite winding to its neighbors. This configuration allows the lines of force at 4 poles to be cut by 4 coils at the same amount at a given time. At each 90-degree rotation, the voltage output polarity is switched from one direction to the other. Therefore, there are 4 cycles of the AC output in one rotation. As the 4 coils are wired in series and their outputs are \"in phase\", the AC output of this single-phase generator will have 4 times the voltage of that generated by each individual coil.\n\nA benefit of the revolving field design is that if the poles are permanent magnets, then there is no need to use any slip ring and brush to deliver electricity out of the generator as the coils are stationary and can be wired directly from the generator to the external loads.\n\nSingle-phase generators that people are familiar with are usually small. The applications are for standby generators in case of main power supply is interrupted and for supplying temporary power on construction sites.\n\nAnother application is in small wind technology. Although most of wind turbines use three-phase generators, single-phase generators are found in some of the small wind turbine models with rated power outputs of up with 55 kW. The single-phase models are available in the vertical axis wind turbines (VAWT) and Horizontal-axis wind turbines (HAWT).\n\nIn the very early days of electricity generation, the generators at power stations had been single-phase AC or direct current. The direction of the power industry were changing in 1895 when more efficient polyphase generators were successfully implemented at Adams Hydroelectric Generating Plant which was the first large-scale polyphase power station. Newer power stations started to adopt the polyphase system. In the 1900s, many railways started the electrification of their lines. During that time, the single-phase AC system had been widely used for their traction power networks beside the direct current system. The early generators for those single-phase traction networks are single-phase generators. Even with newer three-phase motors which were introduced to some modern trains, the single-phases transmission for traction networks survive their time and are still in use in many railways today. However, many traction power stations have replaced their generators over time to use three-phase generators and convert into single-phase for transmission.\n\nIn the early development of hydroelectricity, single-phase generators played an important role in demonstrating the benefits of alternating current. In 1891, a 3,000 volts and 133 Hz single-phase generator of 100 horsepower was installed at Ames Hydroelectric Generating Plant which was belt-connected with Pelton water wheel. The power was transmitted through cables to power an identical motor at the mill. The plant was the first to generate alternating current electric power for industrial application and it was a demonstration of the efficiency in AC transmission. This was a precedent to larger for much larger plants such as the Edward Dean Adams Power Plant in Niagara Falls, New York in 1895. However, the larger plants were operated using polyphase generators for greater efficiency. That left the applications for single-phase hydroelectricity generation to special cases such as in light loads.\n\nAn example of using single-phase in a special case was implemented in 1902 at St. Louis Municipal Electric Power Plant. A 20 kW single-phase generator was direct-connected to a Pelton water wheel to generate electricity enough to power light loads. This was an early demonstration of in-conduit hydro to capture energy from water flow in the public water pipeline. The energy for the water main in this case was not created by gravity, but the water was pumped by a larger steam engine at a water pumping station to supply water to customers. The decision to have water pumped by a larger engine then take some of the energy from water flow to power a smaller generator using water wheel was based on the cost. At the time, steam engines were not efficient and cost effective for a 20 kW system. Therefore, they installed a steam water pump to have enough energy to maintain water pressure for customer and to drive a small generator at the same time.\n\nThe main usage of single-phase hydroelectricity generation today is to supply power for traction network for railways. Many electrical transmission networks for railways especially in Germany rely on single-phase generation and transmission which are still in use today. A notable power station is Walchensee Hydroelectric Power Station in Bavaria. The station takes water from elevated Lake Walchensee to drive eight turbines that drive the generators. Four of those are three-phase generators to supply the power grid. The other four are single-phase generators are connected to Pelton turbines which have combined capacity of 52 MW to supply the German 15 kV AC railway electrification.\n\nSimilar single-phase hydroelectricity generations are also used in another variance of railway electrification system in the United States. A power station at Safe Harbor Dam in Pennsylvania provides power generation for both public utilities and for Amtrak railway. Two out of its 14 turbines are connected to two single-phase generators to supply Amtrak's 25 Hz traction power system. The two turbines are of Kaplan type with 5 blades rated 42,500 horsepower.\n\nIn the early years, steam engines were used as prime movers of generators. An installation at St. Louis Municipal Electric Power Plant in the 1900s was an example of using steam engines with single-phase generators. The St. Louis plant used compound steam engine to drive a 100 kW single-phase generator which produced current at rated power of 1,150 volts.\n\nThe steam engines were also used during the twentieth century in power stations for traction networks which had single-phase power distribution for specific railways. A special set of single-phase generators with steam turbines at Waterside Generating Station in New York City in 1938 was an example of such generation and distribution systems. The single-phase generators were eventually retired in the late 1970s due to concerns of a turbine failure in another station. The generators were replaced by two transformers to reduce from another three-phase power source to existing single-phase catenary power. Eventually, the transformers were replaced by two solid-state cycloconverter instead.\n\nNormally, nuclear power plants are used as base load stations with very high capacities to supply power to the grids. Neckarwestheim I in Neckarwestheim is a unique nuclear power plant in that it is equipped with high-capacity single-phase generators to supply Deutsche Bahn railway with specific AC voltage at frequency of 16 2/3 Hz. The pressurized water reactor transport thermal energy to two turbines and generators which are rated for 187 MW and 152 MW.\n\n"}
{"id": "31300435", "url": "https://en.wikipedia.org/wiki?curid=31300435", "title": "Sliding criterion (geotechnical engineering)", "text": "Sliding criterion (geotechnical engineering)\n\nThe sliding criterion (discontinuity) is a tool to estimate easily the shear strength properties of a discontinuity in a rock mass based on visual and tactile (i.e. by feeling) characterization of the discontinuity. The shear strength of a discontinuity is important in, for example, tunnel, foundation, or slope engineering, but also stability of natural slopes is often governed by the shear strength along discontinuities.\n\nThe \"sliding-angle\" is based on the ease with which a block of rock material can move over a discontinuity and hence is comparable to the \"tilt-angle\" as determined with the \"tilt test\", but on a larger scale. The \"sliding criterion\" has been developed for stresses that would occur in slopes between , hence, in the order of maximum . The \"sliding criterion\" is based on back analyses of slope instability and earlier work of ISRM and Laubscher. The \"sliding criterion\" is part of the Slope Stability Probability Classification (SSPC) system for slope stability analyses.\n\nThe sliding-angle is calculated as follows:\n\nThe roughness large scale (\"Rl\") is based on visual comparison of the trace (with a length of about 1 m) or surface (with an area of about 1 x 1 m of a discontinuity with the example graphs in figure 1. This results in a descriptive term: \"wavy, slightly wavy, curved, slightly curved\", or \"straight\". The corresponding factor for \"Rl\" is listed in table 1.\nThe \"roughness large scale (Rl)\" contributes only to the friction along the discontinuity when the walls on both sides of the discontinuity are fitting, i.e. the asperities on both discontinuity walls match. If the discontinuity is non-fitting, the factor \"Rl\" = 0.75.\n\nThe roughness small scale (\"Rs\") is established visually and tactile (by feeling). \nThe first term \"rough\", \"smooth\", or \"polished\" is established by feeling the surface of the discontinuity; \"rough\" hurts when fingers are moved over the surface with some (little) force, \"smooth\" feels that there is resistance to the fingers, while \"polished\" gives a feeling about similar to the surface of glass.\nThe second term is established visually. The trace (with a length of about 0.2 m) or surface (with an area of about 0.2 x 0.2 m of a discontinuity is compared with the example graphs in figure 2; this gives \"stepped\", \"undulating\", or \"planar\". The two terms of visual and tactile give a combined term and the corresponding factor is listed in table 1.\nThe visual part of the \"roughness small scale (Rs)\" contributes only to the friction along the discontinuity if the walls on both sides of the discontinuity are \"fitting\", i.e. the asperities on both discontinuity walls match. If the discontinuity is non-fitting, the visual part of the \"roughness small scale (Rs)\" should be taken as \"planar\" for the calculation of the \"sliding-angle\", and hence, the \"roughness small scale (Rs)\" can be only \"rough planar, smooth planar, or polished planar\".\n\nInfill material in a discontinuity has often a marked influence on the shear characteristics. The different options for infill material are listed in table 1, and below follows a short explanation for each option.\n\nA \"cemented discontinuity\" or a \"discontinuity with cemented infill\" has higher shear strength than a non-cemented discontinuity if the cement or cemented infill is bonded to \"both\" discontinuity walls. Note that cement and cement bounds that are stronger than the surrounding intact rock ceases the discontinuity to be a mechanical plane of weakness, and, hence, the 'sliding-angle' has no validity.\n\n\"No infill\" describes a discontinuity that may have coated walls but no other infill.\n\n\"Non-softening\" infill material is material that does not change in shear characteristics under the influence of water nor under the influence of shear displacement. The material may break but no greasing effect will occur. The material particles can roll but this is considered to be of minor influence because, after small displacements, the material particles generally will still be very angular. This is further sub-divided in \"coarse\", \"medium\", and \"fine\" for the size of the grains in the infill material or the size of the grains or minerals in the discontinuity wall. The larger of the two should be used for the description. The thickness of the infill can be very thin, sometimes not more than a dust coating.\n\nSoftening infill material will under the influence of water or displacements, attain in lower shear strength and will act as a lubricating agent.\nThis is further sub-divided in \"coarse\", \"medium\", and \"fine\" for the size of the grains in the infill material or the size of the grains or minerals in the discontinuity wall. The larger of the two should be used for the description. The thickness of the infill can be very thin, sometimes not more than a dust coating.\n\nGouge infill means a relatively thick and continuous layer of infill material, mainly consisting of clay but may contain rock fragments. The clay material surrounds the rock fragments in the clay completely or partly, so that these are not in contact with both discontinuity walls. A sub-division is made between \"less thick\" and \"thicker\" than the amplitude of the roughness of the discontinuity walls. If the thickness is less than the amplitude of the roughness, the shear strength will be influenced by the wall material and the discontinuity walls will be in contact after a certain displacement. If the infill is thicker than the amplitude, the friction of the discontinuity is fully governed by the infill.\n\nVery weak and not compacted infill in discontinuities flows out of the discontinuities under its own weight or as a consequence of a very small trigger force (such as water pressure, vibrations due to traffic or the excavation process, etc.).\n\nThe presence of solution (karst) features along the discontinuity.\n\n\n"}
{"id": "26826", "url": "https://en.wikipedia.org/wiki?curid=26826", "title": "Sodium", "text": "Sodium\n\nSodium is a chemical element with symbol Na (from Latin \"natrium\") and atomic number 11. It is a soft, silvery-white, highly reactive metal. Sodium is an alkali metal, being in group 1 of the periodic table, because it has a single electron in its outer shell that it readily donates, creating a positively charged ion—the Na cation. Its only stable isotope is Na. The free metal does not occur in nature, but must be prepared from compounds. Sodium is the sixth most abundant element in the Earth's crust and exists in numerous minerals such as feldspars, sodalite, and rock salt (NaCl). Many salts of sodium are highly water-soluble: sodium ions have been leached by the action of water from the Earth's minerals over eons, and thus sodium and chlorine are the most common dissolved elements by weight in the oceans.\n\nSodium was first isolated by Humphry Davy in 1807 by the electrolysis of sodium hydroxide. Among many other useful sodium compounds, sodium hydroxide (lye) is used in soap manufacture, and sodium chloride (edible salt) is a de-icing agent and a nutrient for animals including humans.\n\nSodium is an essential element for all animals and some plants. Sodium ions are the major cation in the extracellular fluid (ECF) and as such are the major contributor to the ECF osmotic pressure and ECF compartment volume. Loss of water from the ECF compartment increases the sodium concentration, a condition called hypernatremia. Isotonic loss of water and sodium from the ECF compartment decreases the size of that compartment in a condition called ECF hypovolemia.\n\nBy means of the sodium-potassium pump, living human cells pump three sodium ions out of the cell in exchange for two potassium ions pumped in; comparing ion concentrations across the cell membrane, inside to outside, potassium measures about 40:1, and sodium, about 1:10. In nerve cells, the electrical charge across the cell membrane enables transmission of the nerve impulse—an action potential—when the charge is dissipated; sodium plays a key role in that activity.\n\nSodium at standard temperature and pressure is a soft silvery metal that combines with oxygen in the air and forms grayish white sodium oxide unless immersed in oil or inert gas, which are the conditions it is usually stored in. Sodium metal can be easily cut with a knife and is a good conductor of electricity and heat because it has only one electron in its valence shell, resulting in weak metallic bonding and free electrons, which carry energy. Due to having low atomic mass and large atomic radius, sodium is third-least dense of all elemental metals and is one of only three metals that can float on water, the other two being lithium and potassium. The melting (98 °C) and boiling (883 °C) points of sodium are lower than those of lithium but higher than those of the heavier alkali metals potassium, rubidium, and caesium, following periodic trends down the group. These properties change dramatically at elevated pressures: at 1.5 Mbar, the color changes from silvery metallic to black; at 1.9 Mbar the material becomes transparent with a red color; and at 3 Mbar, sodium is a clear and transparent solid. All of these high-pressure allotropes are insulators and electrides.\nIn a flame test, sodium and its compounds glow yellow because the excited 3s electrons of sodium emit a photon when they fall from 3p to 3s; the wavelength of this photon corresponds to the D line at about 589.3 nm. Spin-orbit interactions involving the electron in the 3p orbital split the D line into two, at 589.0 and 589.6 nm; hyperfine structures involving both orbitals cause many more lines.\n\nTwenty isotopes of sodium are known, but only Na is stable. Na is created in the carbon-burning process in stars by fusing two carbon atoms together; this requires temperatures above 600 megakelvins and a star of at least three solar masses. Two radioactive, cosmogenic isotopes are the byproduct of cosmic ray spallation: Na has a half-life of 2.6 years and Na, a half-life of 15 hours; all other isotopes have a half-life of less than one minute. Two nuclear isomers have been discovered, the longer-lived one being Na with a half-life of around 20.2 milliseconds. Acute neutron radiation, as from a nuclear criticality accident, converts some of the stable Na in human blood to Na; the neutron radiation dosage of a victim can be calculated by measuring the concentration of Na relative to Na.\n\nSodium atoms have 11 electrons, one more than the extremely stable configuration of the noble gas neon. Because of this and its low first ionization energy of 495.8 kJ/mol, the sodium atom is much more likely to lose the last electron and acquire a positive charge than to gain one and acquire a negative charge. This process requires so little energy that sodium is readily oxidized by giving up its 11th electron. In contrast, the second ionization energy is very high (4562 kJ/mol), because the 10th electron is closer to the nucleus than the 11th electron. As a result, sodium usually forms ionic compounds involving the Na cation.\n\nThe most common oxidation state for sodium is +1. It is generally less reactive than potassium and more reactive than lithium. Sodium metal is highly reducing, with the standard reduction potential for the Na/Na couple being −2.71 volts, though potassium and lithium have even more negative potentials.\n\nSodium compounds are of immense commercial importance, being particularly central to industries producing glass, paper, soap, and textiles. The most important sodium compounds are table salt (NaCl), soda ash (NaCO), baking soda (NaHCO), caustic soda (NaOH), sodium nitrate (NaNO), di- and tri-sodium phosphates, sodium thiosulfate (NaSO·5HO), and borax (NaBO·10HO). In compounds, sodium is usually ionically bonded to water and anions and is viewed as a hard Lewis acid.\nMost soaps are sodium salts of fatty acids. Sodium soaps have a higher melting temperature (and seem \"harder\") than potassium soaps.\n\nLike all the alkali metals, sodium reacts exothermically with water, and sufficiently large pieces melt to a sphere and may explode. The reaction produces caustic soda (sodium hydroxide) and flammable hydrogen gas. When burned in air, it forms primarily sodium peroxide with some sodium oxide.\n\nSodium tends to form water-soluble compounds, such as halides, sulfates, nitrates, carboxylates and carbonates. The main aqueous species are the aquo complexes [Na(HO)], where \"n\" = 4–8; with \"n\" = 6 indicated from X-ray diffraction data and computer simulations.\n\nDirect precipitation of sodium salts from aqueous solutions is rare because sodium salts typically have a high affinity for water; an exception is sodium bismuthate (NaBiO). Because of this, sodium salts are usually isolated as solids by evaporation or by precipitation with an organic solvent, such as ethanol; for example, only 0.35 g/L of sodium chloride will dissolve in ethanol. Crown ethers, like 15-crown-5, may be used as a phase-transfer catalyst.\n\nSodium content in bulk may be determined by treating with a large excess of uranyl zinc acetate; the hexahydrate (UO)ZnNa(CHCO)·6HO precipitates and can be weighed. Caesium and rubidium do not interfere with this reaction, but potassium and lithium do. Lower concentrations of sodium may be determined by atomic absorption spectrophotometry or by potentiometry using ion-selective electrodes.\n\nLike the other alkali metals, sodium dissolves in ammonia and some amines to give deeply colored solutions; evaporation of these solutions leaves a shiny film of metallic sodium. The solutions contain the coordination complex (Na(NH)), with the positive charge counterbalanced by electrons as anions; cryptands permit the isolation of these complexes as crystalline solids. Sodium forms complexes with crown ethers, cryptands and other ligands. For example, 15-crown-5 has a high affinity for sodium because the cavity size of 15-crown-5 is 1.7–2.2 Å, which is enough to fit sodium ion (1.9 Å). Cryptands, like crown ethers and other ionophores, also have a high affinity for the sodium ion; derivatives of the alkalide Na are obtainable by the addition of cryptands to solutions of sodium in ammonia via disproportionation.\n\nMany organosodium compounds have been prepared. Because of the high polarity of the C-Na bonds, they behave like sources of carbanions (salts with organic anions). Some well-known derivatives include sodium cyclopentadienide (NaCH) and trityl sodium ((CH)CNa). Because of the large size and very low polarising power of the Na cation, it can stabilize large, aromatic, polarisable radical anions, such as in sodium naphthalenide, Na[CH•], a strong reducing agent.\n\nSodium forms alloys with many metals, such as potassium, calcium, lead, and the group 11 and 12 elements. Sodium and potassium form KNa and NaK. NaK is 40–90% potassium and it is liquid at ambient temperature. It is an excellent thermal and electrical conductor. Sodium-calcium alloys are by-products of the electrolytic production of sodium from a binary salt mixture of NaCl-CaCl and ternary mixture NaCl-CaCl-BaCl. Calcium is only partially miscible with sodium. In a liquid state, sodium is completely miscible with lead. There are several methods to make sodium-lead alloys. One is to melt them together and another is to deposit sodium electrolytically on molten lead cathodes. NaPb, NaPb, NaPb, NaPb, and NaPb are some of the known sodium-lead alloys. Sodium also forms alloys with gold (NaAu) and silver (NaAg). Group 12 metals (zinc, cadmium and mercury) are known to make alloys with sodium. NaZn and NaCd are alloys of zinc and cadmium. Sodium and mercury form NaHg, NaHg, NaHg, NaHg, and NaHg.\n\nBecause of its importance in human metabolism, salt has long been an important commodity, as shown by the English word \"salary\", which derives from \"salarium\", the wafers of salt sometimes given to Roman soldiers along with their other wages. In medieval Europe, a compound of sodium with the Latin name of \"sodanum\" was used as a headache remedy. The name sodium is thought to originate from the Arabic \"suda\", meaning headache, as the headache-alleviating properties of sodium carbonate or soda were well known in early times. Although sodium, sometimes called \"soda\", had long been recognized in compounds, the metal itself was not isolated until 1807 by Sir Humphry Davy through the electrolysis of sodium hydroxide. In 1809, the German physicist and chemist Ludwig Wilhelm Gilbert proposed the names \"Natronium\" for Humphry Davy's \"sodium\" and \"Kalium\" for Davy's \"potassium\". The chemical abbreviation for sodium was first published in 1814 by Jöns Jakob Berzelius in his system of atomic symbols, and is an abbreviation of the element's New Latin name \"natrium\", which refers to the Egyptian \"natron\", a natural mineral salt mainly consisting of hydrated sodium carbonate. Natron historically had several important industrial and household uses, later eclipsed by other sodium compounds.\n\nSodium imparts an intense yellow color to flames. As early as 1860, Kirchhoff and Bunsen noted the high sensitivity of a sodium flame test, and stated in Annalen der Physik und Chemie:\n\nIn a corner of our 60 m room farthest away from the apparatus, we exploded 3 mg of sodium chlorate with milk sugar while observing the nonluminous flame before the slit. After a while, it glowed a bright yellow and showed a strong sodium line that disappeared only after 10 minutes. From the weight of the sodium salt and the volume of air in the room, we easily calculate that one part by weight of air could not contain more than 1/20 millionth weight of sodium.\n\nThe Earth's crust contains 2.27% sodium, making it the seventh most abundant element on Earth and the fifth most abundant metal, behind aluminium, iron, calcium, and magnesium and ahead of potassium. Sodium's estimated oceanic abundance is 1.08 milligrams per liter. Because of its high reactivity, it is never found as a pure element. It is found in many different minerals, some very soluble, such as halite and natron, others much less soluble, such as amphibole and zeolite. The insolubility of certain sodium minerals such as cryolite and feldspar arises from their polymeric anions, which in the case of feldspar is a polysilicate.\n\nAtomic sodium has a very strong spectral line in the yellow-orange part of the spectrum (the same line as is used in sodium vapour street lights). This appears as an absorption line in many types of stars, including the Sun. The line was first studied in 1814 by Joseph von Fraunhofer during his investigation of the lines in the solar spectrum, now known as the Fraunhofer lines. Fraunhofer named it the 'D line', although it is now known to actually be a group of closely spaced lines split by a fine and hyperfine structure.\n\nThe strength of the D line means it has been detected in many other astronomical environments. In stars, it is seen in any whose surfaces are cool enough for sodium to exist in atomic form (rather than ionised). This corresponds to stars of roughly F-type and cooler. Many other stars appear to have a sodium absorption line, but this is actually caused by gas in the foreground interstellar medium. The two can be distinguished via high-resolution spectroscopy, because interstellar lines are much narrower than those broadened by stellar rotation.\n\nSodium has also been detected in numerous Solar System environments, including Mercury's atmosphere, the exosphere of the Moon, and numerous other bodies. Some comets have a sodium tail, which was first detected in observations of Comet Hale-Bopp in 1997. Sodium has even been detected in the atmospheres of some extrasolar planets via transit spectroscopy.\n\nEmployed only in rather specialized applications, only about 100,000 tonnes of metallic sodium are produced annually. Metallic sodium was first produced commercially in the late 19th century by carbothermal reduction of sodium carbonate at 1100 °C, as the first step of the Deville process for the production of aluminium:\n\nThe high demand for aluminium created the need for the production of sodium. The introduction of the Hall–Héroult process for the production of aluminium by electrolysing a molten salt bath ended the need for large quantities of sodium. A related process based on the reduction of sodium hydroxide was developed in 1886.\n\nSodium is now produced commercially through the electrolysis of molten sodium chloride, based on a process patented in 1924. This is done in a Downs cell in which the NaCl is mixed with calcium chloride to lower the melting point below 700 °C. As calcium is less electropositive than sodium, no calcium will be deposited at the cathode. This method is less expensive than the previous Castner process (the electrolysis of sodium hydroxide).\n\nThe market for sodium is volatile due to the difficulty in its storage and shipping; it must be stored under a dry inert gas atmosphere or anhydrous mineral oil to prevent the formation of a surface layer of sodium oxide or sodium superoxide.\n\nThough metallic sodium has some important uses, the major applications for sodium use compounds; millions of tons of sodium chloride, hydroxide, and carbonate are produced annually. Sodium chloride is extensively used for anti-icing and de-icing and as a preservative; examples of the uses of sodium bicarbonate include baking, as a raising agent, and sodablasting. Along with potassium, many important medicines have sodium added to improve their bioavailability; though potassium is the better ion in most cases, sodium is chosen for its lower price and atomic weight. Sodium hydride is used as a base for various reactions (such as the aldol reaction) in organic chemistry, and as a reducing agent in inorganic chemistry.\n\nMetallic sodium is used mainly for the production of sodium borohydride, sodium azide, indigo, and triphenylphosphine. A once-common use was the making of tetraethyllead and titanium metal; because of the move away from TEL and new titanium production methods, the production of sodium declined after 1970. Sodium is also used as an alloying metal, an anti-scaling agent, and as a reducing agent for metals when other materials are ineffective. Note the free element is not used as a scaling agent, ions in the water are exchanged for sodium ions. Sodium plasma (\"vapor\") lamps are often used for street lighting in cities, shedding light that ranges from yellow-orange to peach as the pressure increases. By itself or with potassium, sodium is a desiccant; it gives an intense blue coloration with benzophenone when the desiccate is dry. In organic synthesis, sodium is used in various reactions such as the Birch reduction, and the sodium fusion test is conducted to qualitatively analyse compounds. Sodium reacts with alcohol and gives alkoxides, and when sodium is dissolved in ammonia solution, it can be used to reduce alkynes to trans-alkenes. Lasers emitting light at the sodium D line are used to create artificial laser guide stars that assist in the adaptive optics for land-based visible-light telescopes.\n\nLiquid sodium is used as a heat transfer fluid in some types of nuclear reactors because it has the high thermal conductivity and low neutron absorption cross section required to achieve a high neutron flux in the reactor. The high boiling point of sodium allows the reactor to operate at ambient (normal) pressure, but the drawbacks include its opacity, which hinders visual maintenance, and its explosive properties. Radioactive sodium-24 may be produced by neutron bombardment during operation, posing a slight radiation hazard; the radioactivity stops within a few days after removal from the reactor. If a reactor needs to be shut down frequently, NaK is used; because NaK is a liquid at room temperature, the coolant does not solidify in the pipes. In this case, the pyrophoricity of potassium requires extra precautions to prevent and detect leaks. Another heat transfer application is poppet valves in high-performance internal combustion engines; the valve stems are partially filled with sodium and work as a heat pipe to cool the valves.\n\nIn humans, sodium is an essential mineral that regulates blood volume, blood pressure, osmotic equilibrium and pH; the minimum physiological requirement for sodium is 500 milligrams per day. Sodium chloride is the principal source of sodium in the diet, and is used as seasoning and preservative in such commodities as pickled preserves and jerky; for Americans, most sodium chloride comes from processed foods. Other sources of sodium are its natural occurrence in food and such food additives as monosodium glutamate (MSG), sodium nitrite, sodium saccharin, baking soda (sodium bicarbonate), and sodium benzoate. The US Institute of Medicine set its Tolerable Upper Intake Level for sodium at 2.3 grams per day, but the average person in the United States consumes 3.4 grams per day. Studies have found that lowering sodium intake by 2 g per day tends to lower systolic blood pressure by about two to four mm Hg. It has been estimated that such a decrease in sodium intake would lead to between 9 and 17% fewer cases of hypertension.\n\nHypertension causes 7.6 million premature deaths worldwide each year. (Note that salt contains about 39.3% sodiumthe rest being chlorine and trace chemicals; thus, 2.3 g sodium is about 5.9 g, or 2.7 ml of saltabout one US teaspoon.) The American Heart Association recommends no more than 1.5 g of sodium per day.\n\nOne study found that people with or without hypertension who excreted less than 3 grams of sodium per day in their urine (and therefore were taking in less than 3 g/d) had a \"higher\" risk of death, stroke, or heart attack than those excreting 4 to 5 grams per day. Levels of 7 g per day or more in people with hypertension were associated with higher mortality and cardiovascular events, but this was not found to be true for people without hypertension. The US FDA states that adults with hypertension and prehypertension should reduce daily intake to 1.5 g.\n\nThe renin–angiotensin system regulates the amount of fluid and sodium concentration in the body. Reduction of blood pressure and sodium concentration in the kidney result in the production of renin, which in turn produces aldosterone and angiotensin, retaining sodium in the urine. When the concentration of sodium increases, the production of renin decreases, and the sodium concentration returns to normal. The sodium ion (Na) is an important electrolyte in neuron function, and in osmoregulation between cells and the extracellular fluid. This is accomplished in all animals by Na/K-ATPase, an active transporter pumping ions against the gradient, and sodium/potassium channels. Sodium is the most prevalent metallic ion in extracellular fluid.\n\nUnusually low or high sodium levels in humans are recognized in medicine as hyponatremia and hypernatremia. These conditions may be caused by genetic factors, ageing, or prolonged vomiting or diarrhea.\n\nIn C4 plants, sodium is a micronutrient that aids metabolism, specifically in regeneration of phosphoenolpyruvate and synthesis of chlorophyll. In others, it substitutes for potassium in several roles, such as maintaining turgor pressure and aiding in the opening and closing of stomata. Excess sodium in the soil can limit the uptake of water by decreasing the water potential, which may result in plant wilting; excess concentrations in the cytoplasm can lead to enzyme inhibition, which in turn causes necrosis and chlorosis. In response, some plants have developed mechanisms to limit sodium uptake in the roots, to store it in cell vacuoles, and restrict salt transport from roots to leaves; excess sodium may also be stored in old plant tissue, limiting the damage to new growth. Halophytes have adapted to be able to flourish in sodium rich environments.\n\nSodium forms flammable hydrogen and caustic sodium hydroxide on contact with water; ingestion and contact with moisture on skin, eyes or mucous membranes can cause severe burns. Sodium spontaneously explodes in the presence of water due to the formation of hydrogen (highly explosive) and sodium hydroxide (which dissolves in the water, liberating more surface). However, sodium exposed to air and ignited or reaching autoignition (reported to occur when a molten pool of sodium reaches about 290 °C) displays a relatively mild fire. In the case of massive (non-molten) pieces of sodium, the reaction with oxygen eventually becomes slow due to formation of a protective layer. Fire extinguishers based on water accelerate sodium fires; those based on carbon dioxide and bromochlorodifluoromethane should not be used on sodium fire. Metal fires are Class D, but not all Class D extinguishers are workable with sodium. An effective extinguishing agent for sodium fires is Met-L-X. Other effective agents include Lith-X, which has graphite powder and an organophosphate flame retardant, and dry sand. Sodium fires are prevented in nuclear reactors by isolating sodium from oxygen by surrounding sodium pipes with inert gas. Pool-type sodium fires are prevented using different design measures called catch pan systems. They collect leaking sodium into a leak-recovery tank where it is isolated from oxygen.\n\n"}
{"id": "2200530", "url": "https://en.wikipedia.org/wiki?curid=2200530", "title": "Sorghum × drummondii", "text": "Sorghum × drummondii\n\nSorghum\" × \"drummondii (Sudangrass), is a hybrid-derived species of grass raised for forage and grain, native to tropical and subtropical regions of Eastern Africa. The plant is cultivated in Southern Europe, South America, Central America, North America and Southern Asia, for forage or as a cover crop.\n\nIt is distinguished from the grain sorghum (\"Sorghum bicolor\") by the grain (caryopsis) not being exposed at maturity.\n"}
{"id": "21304461", "url": "https://en.wikipedia.org/wiki?curid=21304461", "title": "Steam", "text": "Steam\n\nSteam is water in the gas phase, which is formed when water boils. Steam is invisible; however, \"steam\" often refers to wet steam, the visible mist or aerosol of water droplets formed as this water vapour condenses. At lower pressures, such as in the upper atmosphere or at the top of high mountains, water boils at a lower temperature than the nominal at standard pressure. If heated further it becomes superheated steam. \n\nThe enthalpy of vaporization is the energy required to turn water into the gaseous form when it increases in volume by 1,700 times at standard temperature and pressure; this change in volume can be converted into mechanical work by steam engines such as reciprocating piston type engines and steam turbines, which are a sub-group of steam engines. Piston type steam engines played a central role to the Industrial Revolution and modern steam turbines are used to generate more than 80% of the world's electricity. If liquid water comes in contact with a very hot surface or depressurizes quickly below its vapor pressure, it can create a steam explosion. \n\nSteam is traditionally created by heating a boiler via burning coal and other fuels, but it is also possible to create steam with solar energy. Water vapor that includes water droplets is described as \"wet steam\". As wet steam is heated further, the droplets evaporate, and at a high enough temperature (which depends on the pressure) all of the water evaporates and the system is in vapor–liquid equilibrium.\n\nSuperheated steam is steam at a temperature higher than its boiling point for the pressure, which only occurs where all liquid water has evaporated or has been removed from the system.\n\nSteam tables contain thermodynamic data for water/steam and are often used by engineers and scientists in design and operation of equipment where thermodynamic cycles involving steam are used. Additionally, thermodynamic phase diagrams for water/steam, such as a temperature-entropy diagram or a Mollier diagram shown in this article, may be useful. Steam charts are also used for analysing thermodynamic cycles. \n\nIn agriculture, steam is used for soil sterilization to avoid the use of harmful chemical agents and increase soil health. \n\nSteam's capacity to transfer heat is also used in the home: for cooking vegetables, steam cleaning of fabric, carpets and flooring, and for heating buildings. In each case, water is heated in a boiler, and the steam carries the energy to a target object. Steam is also used in ironing clothes to add enough humidity with the heat to take wrinkles out and put intentional creases into the clothing. \n\nAbout 90% of all electricity is generated using steam as the working fluid, nearly all by steam turbines.\n\nIn electric generation, steam is typically condensed at the end of its expansion cycle, and returned to the boiler for re-use. However, in cogeneration, steam is piped into buildings through a district heating system to provide heat energy after its use in the electric generation cycle. The world's biggest steam generation system is the New York City steam system, which pumps steam into 100,000 buildings in Manhattan from seven cogeneration plants.\n\nIn other industrial applications steam is used for energy storage, which is introduced and extracted by heat transfer, usually through pipes. Steam is a capacious reservoir for thermal energy because of water's high heat of vaporization.\n\nFireless steam locomotives were steam locomotives that operated from a supply of steam stored on board in a large tank resembling a conventional locomotive's boiler. This tank was filled by process steam, as is available in many sorts of large factory, such as paper mills. The locomotive's propulsion used pistons and connecting rods, as for a typical steam locomotive. These locomotives were mostly used in places where there was a risk of fire from a boiler's firebox, but were also used in factories that simply had a plentiful supply of steam to spare.\n\nOwing to its low molecular mass, steam is an effective lifting gas, providing approximately 60% as much lift as helium and twice as much as hot air. It is not flammable, unlike hydrogen, and is cheap and abundant, unlike helium. The required heat, however, leads to condensation problems and requires an insulated envelope. These factors have limited its use thus far to mostly demonstration projects.\n\nSteam engines and steam turbines use the expansion of steam to drive a piston or turbine to perform mechanical work. The ability to return condensed steam as water-liquid to the boiler at high pressure with relatively little expenditure of pumping power is important. Condensation of steam to water often occurs at the low-pressure end of a steam turbine, since this maximizes the energy efficiency, but such wet-steam conditions must be limited to avoid excessive turbine blade erosion. Engineers use an idealised thermodynamic cycle, the Rankine cycle, to model the behavior of steam engines. Steam turbines are often used in the production of electricity.\n\nAn autoclave, which uses steam under pressure, is used in microbiology laboratories and similar environments for sterilization.\n\nSteam, especially dry (highly superheated) steam, may be used for antimicrobial cleaning even to the levels of sterilization. Steam is a non-toxic antimicrobial agent.\nSteam is used in piping for utility lines. It is also used in jacketing and tracing of piping to maintain the uniform temperature in pipelines and vessels.\n\nSteam is used in the process of wood bending, killing insects and increasing plasticity.\n\nSteam is used to accentuate drying especially in prefabricates.\nCare should be taken since concrete produces heat during hydration and additional heat from the steam could be detrimental to hardening reaction processes of the concrete.\n\nUsed in cleaning of fibers and other materials, sometimes in preparation for painting. Steam is also useful in melting hardened grease and oil residues, so it is useful in cleaning kitchen floors and equipment and internal combustion engines and parts. Among the advantages of using steam versus a hot water spray are the facts that steam can operate at higher temperatures and it uses substantially less water per minute.\n\n\n"}
{"id": "426258", "url": "https://en.wikipedia.org/wiki?curid=426258", "title": "Stone skipping", "text": "Stone skipping\n\nStone skipping (or stone skimming) is the art of throwing a flat stone across water in such a way (usually Sidearm) that it bounces off the surface, preferably many times. The objective of the game is to see how many times a stone can bounce before sinking.\n\nThe North American Stone Skipping Association (NASSA), founded by Coleman-McGhee, in 1989 and is based in Driftwood, Texas, sanctioned world championships for four years from 1989 through 1992 in Wimberley, Texas. The next official NASSA World Championships is expected to be held at Platja d'en Ros beach in Cadaqués, Catalonia, Spain.\n\nThe world record for the number of skips \"Guinness Book of Records\" is 88 by Kurt \"Mountain Man\" Steiner, (b. 1965). The cast was achieved on September 6, 2013 at Red Bridge in the Allegheny National Forest, Pennsylvania. The previous record was 65 skips, by Max Steiner (no relation to K. Steiner), set at Riverfront Park, Franklin, Pennsylvania. Before him, the record was 51 skips, set by Russell Byars on July 19, 2007, skipping at the same location. Kurt Steiner also held the world record between 2002 and 2007 with a throw of 40 skips, achieved in competition at Franklin, PA. The US Stone Skipping champion 2018 at Franklin was Keisuke Hashimoto (Japan), at Mackinac Island Mich 2018 it was David Kolar.\n\nA stone skipping championship of a different nature takes place every year in Easdale, Scotland, where relative distances count as opposed to number of skips, as tends to be the case outside the US. Since 1997, competitors from all over the world have taken part in the World Stone Skimming Championships in a disused quarry on Easdale Island using sea-worn Easdale slate of maximum 3\" diameter. Each participant gets three throws and the stone must bounce/skip at least twice to count (ie 3 water touches minimum).The Guinness World Record for the furthest distance skimmed using natural stone stands at 121.8m established by Dougie Isaacs (Scotland). For a female it is 52.5m by Nina Luginbuhl (Switzerland). The records were made on the 28th of May 2018 at Abernant Lake, Llanwrtyd Wells, Wales.\n\nAn early explanation of the physics of stone-skipping was provided by Lazzaro Spallanzani in the 18th century.\n\nThe stone generates lift in the same manner as a flying disc, by pushing water down as it moves across the water at an angle. Surface tension has very little to do with it. The stone's rotation acts to stabilize it against the torque of lift being applied to the back.\n\nResearch undertaken by a team led by French physicist Lydéric Bocquet discovered that an angle of about 20° between the stone and the water's surface is optimal. Bocquet and his colleagues were surprised to discover that changes in speed and rotation did not change this fact, it just allow the stone to be in balance and to continue with a straight and uniform movement, due to gyroscopic effect. Work by Hewitt, Balmforth and McElwaine has shown that if the horizontal speed can be maintained skipping can continue indefinitely. Earlier research reported by Bocquet calculated that the world record of 38 rebounds set by Coleman-McGhee, unchallenged for many years, required a speed of 12 m/s (25 mph), with a rotation of 14 revolutions per second.\n\n\n\nFurther reading\n\n"}
{"id": "9848279", "url": "https://en.wikipedia.org/wiki?curid=9848279", "title": "Strain insulator", "text": "Strain insulator\n\nA strain insulator is an electrical insulator that is designed to work in mechanical tension (strain), to withstand the pull of a suspended electrical wire or cable. They are used in overhead electrical wiring, to support radio antennas and overhead power lines. A strain insulator may be inserted between two lengths of wire to isolate them electrically from each other while maintaining a mechanical connection, or where a wire attaches to a pole or tower, to transmit the pull of the wire to the support while insulating it electrically. Strain insulators were first used in telegraph systems in the mid 19th century.\n\nA typical strain insulator is a piece of glass, porcelain, or fiberglass that is shaped to accommodate two cables or a cable shoe and the supporting hardware on the support structure (hook eye, or eyelet on a steel pole/tower). The shape of the insulator maximizes the distance between the cables while also maximizing the load-bearing transfer capacity of the insulator. \n\nIn practice, for radio antennas, guy-wires, overhead power lines and most other loads, the strain insulator is usually in physical tension.\n\nWhen the line voltage requires more insulation than a single insulator can supply, strain insulators are used in series: A set of insulators are connected to each other using special hardware. The series can support the same strain as a single insulator, but the series provides a much higher effective insulation.\n\nIf one string is insufficient for the strain, a heavy steel plate effectively bundles several insulator strings mechanically. One plate is on the \"hot\" end and another is located at the support structure. This setup is almost universally used on long spans, such as when a power line crosses a river, canyon, lake, or other terrain requiring a longer than nominal span.\n\nStrain insulators are typically used outdoors in overhead wiring. In this environment they are exposed to rain and, in urban settings, pollution. As a practical matter, the shape of the insulator becomes critically important, since a wetted path from one cable to the other can create a low-resistance electrical path.\n\nStrain insulators intended for horizontal mounting (often referred to as \"dead ends\") therefore incorporate flanges to shed water, and strain insulators intended for vertical mounting (referred to as \"suspension insulators\") are often bell-shaped.\n\nOther than their industrial use for which they are produced, strain insulators can be collectables, especially antique ones.\n\n"}
{"id": "51628", "url": "https://en.wikipedia.org/wiki?curid=51628", "title": "Sweet potato", "text": "Sweet potato\n\nThe sweet potato (\"Ipomoea batatas\") is a dicotyledonous plant that belongs to the bindweed or morning glory family, Convolvulaceae. Its large, starchy, sweet-tasting, tuberous roots are a root vegetable. The young leaves and shoots are sometimes eaten as greens. The sweet potato is only distantly related to the potato (\"Solanum tuberosum\") and does not belong to the nightshade family, Solanaceae, but both families belong to the same taxonomic order, the Solanales.\n\nThe plant is a herbaceous perennial vine, bearing alternate heart-shaped or palmately lobed leaves and medium-sized sympetalous flowers. The edible tuberous root is long and tapered, with a smooth skin whose color ranges between yellow, orange, red, brown, purple, and beige. Its flesh ranges from beige through white, red, pink, violet, yellow, orange, and purple. Sweet potato cultivars with white or pale yellow flesh are less sweet and moist than those with red, pink or orange flesh.\n\n\"Ipomoea batatas\" is native to the tropical regions in the Americas. Of the approximately 50 genera and more than 1,000 species of Convolvulaceae, \"I. batatas\" is the only crop plant of major importance—some others are used locally (e.g., \"I. aquatica\" \"kangkong\"), but many are poisonous. The genus \"Ipomoea\" that contains the sweet potato also includes several garden flowers called morning glories, though that term is not usually extended to \"Ipomoea batatas\". Some cultivars of \"Ipomoea batatas\" are grown as ornamental plants under the name \"tuberous morning glory,\" used in a horticultural context.\n\nThe sweet potato is often called a \"yam\" in parts of North America, but is botanically very distinct from the botanical yams.\n\nAlthough the soft, orange sweet potato is often called a \"yam\" in parts of North America, the sweet potato is very distinct from the botanical yams (\"Dioscorea\"), which is native to Africa and Asia and belongs to the monocot family Dioscoreaceae. To add to the confusion, a different crop plant, the \"oca\" (\"Oxalis tuberosa\", a species of wood sorrel), is called a \"yam\" in many parts of Polynesia, including New Zealand.\n\nAlthough the sweet potato is not closely related botanically to the common potato, they have a shared etymology. The first Europeans to taste sweet potatoes were members of Christopher Columbus's expedition in 1492. Later explorers found many cultivars under an assortment of local names, but the name which stayed was the indigenous Taino name of \"batata\". The Spanish combined this with the Quechua word for potato, \"papa\", to create the word \"patata\" for the common potato.\n\nIn Argentina, Venezuela, Puerto Rico and the Dominican Republic the sweet potato is called \"batata\". In Mexico, Peru, Chile, Central America, and the Philippines, the sweet potato is known as \"camote\" (alternatively spelled \"kamote\" in the Philippines), derived from the Nahuatl word \"camotli\".\n\nIn Peru, the Quechua name for a type of sweet potato is \"kumar\", strikingly similar to the Polynesian name \"kumara\" and its regional Oceanic cognates (\"kumala\", \"umala\", \" 'uala\", etc.), which has led some scholars to suspect an instance of pre-Columbian trans-oceanic contact.\n\nIn New Zealand, the most common cultivar is the red (purple) cultivar called \"kumara\" (now spelled \"kūmara\" in the Māori language), but orange ('Beauregard') and gold cultivars are also available. Kumara is particularly popular as a roasted food, often served with sour cream and sweet chili sauce. Occasionally, shops in Australia will label purple cultivars as \"purple sweet potato\" to denote the difference to the other cultivars. About 95% of Australia's production is of the orange cultivar named 'Beauregard', originally from North America, known simply as \"sweet potato\". A reddish-purple cultivar, 'Northern Star', is 4% of production and is sold as \"kumara\".\n\nThe origin and domestication of sweet potato occurred in either Central or South America. In Central America, domesticated sweet potatoes were present at least 5,000 years ago. It has been suggested that the origin of \"I. batatas\" was between the Yucatán Peninsula of Mexico and the mouth of the Orinoco River in Venezuela. The cultigen was most likely spread by local people to the Caribbean and South America by 2500 BCE.\n\nThe sweet potato was grown in Polynesia before western exploration, and the sweet potato cultivated in Polynesia is the \"Ipomoea batatas\", which is generally spread by vine cuttings rather than by seeds. Sweet potato has been radiocarbon-dated in the Cook Islands to 1000 CE. A common hypothesis is that a vine cutting was brought to central Polynesia around 700 CE, possibly by Polynesians who had traveled to South America and back, and spread from there across Polynesia to Hawaii and New Zealand.\n\nRecent divergence time estimates suggest that sweet potatoes were already present in Polynesia thousands of years before humans arrived there. Additionally, there is evidence of long-distance dispersal of other species of \"Ipomoea\" from North America to Hawaii. This suggests natural long-distance dispersal of \"Ipomoea batatas\" is possible; that appears to be the simplest explanation for its presence in Polynesia: That the sweet potato arrived in the west Pacific naturally, rather than being carried there by humans.\n\nIn response to a major crop failure, sweet potatoes were introduced to Fujian province of China in about 1594 from Luzon. The growing of sweet potatoes was encouraged by the Governor Chin Hsüeh-tseng (Jin Xuezeng).\n\nThe sweet potato was introduced to Okinawa, Japan in the early 1600s. Sweet potatoes became a staple in Japan because they were important in preventing famine when rice harvests were poor. Sweet potatoes were later planted in Shōgun Tokugawa Yoshimune's private garden. It was also introduced to Korea in 1764.\n\nThe sweet potato arrived in Europe with the Columbian Exchange. It is recorded, for example, in \"Elinor Fettiplace's Receipt Book\", compiled in England in 1604.\n\nThe genome of cultivated sweet potatoes contains sequences of DNA from \"Agrobacterium\", with genes actively expressed by the plants. Transgenes were observed both in the sweet potato's closely related wild relatives, and in more distantly related wild species. This observation makes cultivated sweet potatoes the first known example of a naturally transgenic food crop.\n\nThe plant does not tolerate frost. It grows best at an average temperature of , abundant sunshine and warm nights. Annual rainfalls of are considered most suitable, with a minimum of in the growing season. The crop is sensitive to drought at the tuber initiation stage 50–60 days after planting, and it is not tolerant to water-logging, as it may cause tuber rots and reduce growth of storage roots if aeration is poor.\n\nDepending on the cultivar and conditions, tuberous roots mature in two to nine months. With care, early-maturing cultivars can be grown as an annual summer crop in temperate areas, such as the northern United States and China. Sweet potatoes rarely flower when the daylight is longer than 11 hours, as is normal outside of the tropics. They are mostly propagated by stem or root cuttings or by adventitious shoots called \"slips\" that grow out from the tuberous roots during storage. True seeds are used for breeding only.\n\nThey grow well in many farming conditions and have few natural enemies; pesticides are rarely needed. Sweet potatoes are grown on a variety of soils, but well-drained, light- and medium-textured soils with a pH range of 4.5–7.0 are more favorable for the plant. They can be grown in poor soils with little fertilizer. However, sweet potatoes are very sensitive to aluminum toxicity and will die about six weeks after planting if lime is not applied at planting in this type of soil. Because they are sown by vine cuttings rather than seeds, sweet potatoes are relatively easy to plant. Because the rapidly growing vines shade out weeds, little weeding is needed. A commonly used herbicide to rid the soil of any unwelcome plants that may interfere with growth is DCPA, also known as Dacthal. In the tropics, the crop can be maintained in the ground and harvested as needed for market or home consumption. In temperate regions, sweet potatoes are most often grown on larger farms and are harvested before first frosts.\n\nIn the Southeastern United States, sweet potatoes are traditionally cured to improve storage, flavor, and nutrition, and to allow wounds on the periderm of the harvested root to heal. Proper curing requires drying the freshly dug roots on the ground for two to three hours, then storage at with 90 to 95% relative humidity from five to fourteen days. Cured sweet potatoes can keep for thirteen months when stored at with >90% relative humidity. Colder temperatures injure the roots.\nIn 2016, global production of sweet potatoes was 105 million tonnes, led by China with 67% of the world total (table).\n\nIn 2016, the world average annual yield for sweet potato crop was 13 tonnes per hectare. The most productive yield of sweet potatoes was in Senegal, where the nationwide average annual yield was 39 tonnes per hectare.\n\nSweet potatoes are cultivated throughout tropical and warm temperate regions wherever there is sufficient water to support their growth. Sweet potatoes became common as a food crop in the islands of the Pacific Ocean, South India, Uganda and other African countries.\n\nA cultivar of the sweet potato called the \"boniato\" is grown in the Caribbean; its flesh is cream-colored, unlike the more common orange hue seen in other cultivars. \"Boniatos\" are not as sweet and moist as other sweet potatoes, but their consistency and delicate flavor are different than the common orange-colored sweet potato.\n\nSweet potatoes have been a part of the diet in the United States for most of its history, especially in the Southeast. The average per capita consumption of sweet potatoes in the United States is only about per year, down from in 1920. “Orange sweet potatoes (the most common type encountered in the US) received higher appearance liking scores compared with yellow or purple cultivars.” Purple and yellow sweet potatoes were not as well liked by consumers compared to orange sweet potatoes “possibly because of the familiarity of orange color that is associated with sweet potatoes.”\n\nBesides simple starches, raw sweet potatoes are rich in complex carbohydrates, dietary fiber and beta-carotene (a provitamin A carotenoid), with moderate contents of other micronutrients, including vitamin B, vitamin B and manganese (table). When cooked by baking, small variable changes in micronutrient density occur to include a higher content of vitamin C at 24% of the Daily Value per 100 g serving (right table).\n\nThe Center for Science in the Public Interest ranked the nutritional value of sweet potatoes as highest among several other foods. In addition, their leaves are edible and can be prepared like spinach or turnip greens.\n\nSweet potato cultivars with dark orange flesh have more beta-carotene than those with light-colored flesh, and their increased cultivation is being encouraged in Africa where vitamin A deficiency is a serious health problem. A 2012 study of 10,000 households in Uganda found that children eating beta-carotene enriched sweet potatoes suffered less vitamin A deficiency than those not consuming as much beta-carotene.\nThe table below presents the relative performance of sweet potato (in column ) to other staple foods. While sweet potato provides less edible energy and protein per unit weight than cereals, it has higher nutrient density than cereals.\n\nAccording to a study by the United Nations Food and Agriculture Organization, sweet potatoes are the most efficient staple food to grow in terms of farmland, yielding approximately 70,000 kcal/ha d.\n\nAlthough the leaves and shoots are also edible, the starchy tuberous roots are by far the most important product. In some tropical areas, they are a staple food crop.\n\n\"Amukeke\" (sun-dried slices of root) and \"inginyo\" (sun-dried crushed root) are a staple food for people in northeastern Uganda. \"Amukeke\" is mainly served for breakfast, eaten with peanut sauce. \"Inginyo\" is mixed with cassava flour and tamarind to make \"atapa\". People eat \"atapa\" with smoked fish cooked in peanut sauce or with dried cowpea leaves cooked in peanut sauce. \"Emukaru\" (earth-baked root) is eaten as a snack anytime and is mostly served with tea or with peanut sauce. Similar uses are also found in South Sudan.\n\nThe young leaves and vine tips of sweet potato leaves are widely consumed as a vegetable in West African countries (Guinea, Sierra Leone and Liberia, for example), as well as in northeastern Uganda, East Africa. According to FAO leaflet No. 13 - 1990, sweet potato leaves and shoots are a good source of vitamins A, C, and B (riboflavin), and according to research done by A. Khachatryan, are an excellent source of lutein.\n\nIn Kenya, Rhoda Nungo of the home economics department of the Ministry of Agriculture has written a guide to using sweet potatoes in modern recipes. This includes uses both in the mashed form and as flour from the dried tubers to replace part of the wheat flour and sugar in baked products such as cakes, chapatis, mandazis, bread, buns and cookies. A nutritious juice drink is made from the orange-fleshed cultivars, and deep-fried snacks are also included.\n\nIn Egypt, sweet potato tubers are known as \"batata\" (بطاطا) and are a common street food in winter, when street vendors with carts fitted with ovens sell them to people passing time by the Nile or the sea. The cultivars used are an orange-fleshed one as well as a white/cream-fleshed one. They are also baked at home as a snack or dessert, drenched with honey.\n\nIn Ethiopia, the commonly found cultivars are black-skinned, cream-fleshed and called \"bitatis\" or \"mitatis\". They are cultivated in the eastern and southern lower highlands and harvested during the rainy season (June/July). In recent years, better yielding orange-fleshed cultivars were released for cultivation by Haramaya University as a less sugary sweet potato with higher vitamin A content. Sweet potatoes are widely eaten boiled as a favored snack.\n\nIn East Asia, roasted sweet potatoes are popular street food. In China, sweet potatoes, typically yellow cultivars, are baked in a large iron drum and sold as street food during winter. In Korea, sweet potatoes, known as \"\", are roasted in a drum can, baked in foil or on an open fire, typically during winter.\nIn Japan, a dish similar to the Korean preparation is called \"yaki-imo\" (roasted sweet potato), which typically uses either the yellow-fleshed \"Japanese sweet potato\" or the purple-fleshed \"Okinawan sweet potato\", which is known as \"beni-imo\".\n\nSweet potato soup, served during winter, consists of boiling sweet potato in water with rock sugar and ginger. Sweet potato greens are a common side dish in Taiwanese cuisine, often boiled or sautéed and served with a garlic and soy sauce mixture, or simply salted before serving. They, as well as dishes featuring the sweet potato root, are commonly found at bento () restaurants. In northeastern Chinese cuisine, sweet potatoes are often cut into chunks and fried, before being drenched into a pan of boiling syrup.\n\nIn some regions of India, sweet potato is roasted slow over kitchen coals at night and eaten with some dressing while the easier way in the south is simply boiling or pressure cooking before peeling, cubing and seasoning for a vegetable dish as part of the meal. In Indian state of Tamil Nadu, it is known as 'Sakkara valli Kilangu'. It is boiled and consumed as evening snack. In some parts of India, fresh sweet potato is chipped, dried and then ground into flour; this is then mixed with wheat flour and baked into \"chapattis\" (bread). Between 15 and 20 percent of sweet potato harvest is converted by some Indian communities into pickles and snack chips. A part of the tuber harvest is used in India as cattle fodder.\n\nIn Pakistan, sweet potato is known as \"shakarqandi\" and is cooked as vegetable dish and also with meat dishes (chicken, mutton or beef). The ash roasted sweet potatoes are sold as a snack and street food in Pakistani bazaars especially during the winter months.\n\nIn Sri Lanka, it is called 'Bathala' and tubers are used mainly for breakfast (boiled sweet potato commonly with sambal or grated coconut) or as an supplementary curry dish for rice. There are many other culinary uses with sweet potato as well.\n\nThe tubers of this plant, known as \"kattala\" in Dhivehi, have been used in the traditional diet of the Maldives. The leaves were finely chopped and used in dishes such as \"mas huni\".\n\nIn Japan, both sweet potatoes (called \"satsuma-imo\") and true purple yams (called \"daijo\" or \"beni-imo\") are grown. Boiling and steaming are the most common cooking methods. Also, the use in vegetable tempura is common. \"Daigaku-imo\" is a baked sweet potato dessert. Because it is sweet and starchy, it is used in \"imo-kinton\" and some other traditional sweets, such as \"ofukuimo\".\n\"Shōchū\", a Japanese spirit normally made from the fermentation of rice, can also be made from sweet potato, in which case it is called \"imo-jōchū\". \"Imo-gohan\", sweet potato cooked with rice, is popular in Guangdong, Taiwan and Japan. It is also served in \"nimono\" or \"nitsuke\", boiled and typically flavored with soy sauce, \"mirin\" and \"dashi\".\nIn Korean cuisine, sweet potato starch is used to produce \"dangmyeon\" (cellophane noodles). Sweet potatoes are also boiled, steamed, or roasted, and young stems are eaten as \"namul\". Pizza restaurants such as Pizza Hut and Domino's in Korea are using sweet potatoes as a popular topping. Sweet potatoes are also used in the distillation of a variety of Soju.\n\nIn Malaysia and Singapore, sweet potato is often cut into small cubes and cooked with taro and coconut milk (\"santan\") to make a sweet dessert called \"bubur caca\" or \"bubu chacha\". A favorite way of cooking sweet potato is deep frying slices of sweet potato in batter, and served as a tea-time snack. In homes, sweet potatoes are usually boiled. The leaves of sweet potatoes are usually stir-fried with only garlic or with \"sambal belacan\" and dried shrimp by Malaysians.\n\nIn the Philippines, sweet potatoes (locally known as \"camote\" or \"kamote\") are an important food crop in rural areas. They are often a staple among impoverished families in provinces, as they are easier to cultivate and cost less than rice. The tubers are boiled or baked in coals and may be dipped in sugar or syrup. Young leaves and shoots (locally known as \"talbos ng kamote\" or \"camote\" tops) are eaten fresh in salads with shrimp paste (\"bagoong alamang\") or fish sauce. They can be cooked in vinegar and soy sauce and served with fried fish (a dish known as \"adobong talbos ng kamote\"), or with recipes such as \"sinigang\". The stew obtained from boiling \"camote\" tops is purple-colored, and is often mixed with lemon as juice. Sweet potatoes are also sold as street food in suburban and rural areas. Fried sweet potatoes coated with caramelized sugar and served in skewers (\"camote cue\") are popular afternoon snacks. Sweet potatoes are also used in a variant of \"halo-halo\" called \"ginatan\", where they are cooked in coconut milk and sugar and mixed with a variety of rootcrops, sago, jackfruit, and \"bilu-bilo\" (glutinous rice balls). Bread made from sweet potato flour is also gaining popularity. Sweet potato is relatively easy to propagate, and in rural areas that can be seen abundantly at canals and dikes. The uncultivated plant is usually fed to pigs.\n\nIn Indonesia, sweet potatoes are locally known as \"ubi jalar\" (lit: spreading tuber) and are frequently fried with batter and served as snacks with spicy condiments, along with other kinds of fritters such as fried bananas, tempeh, tahu, breadfruits, or cassava. In the mountainous regions of West Papua, sweet potatoes are the staple food among the natives there. Using the \"bakar batu\" way of cooking (free translation: burning rocks), rocks that have been burned in a nearby bonfire are thrown into a pit lined with leaves. Layers of sweet potatoes, an assortment of vegetables, and pork are piled on top of the rocks. The top of the pile then is insulated with more leaves, creating a pressure of heat and steam inside which cooks all food within the pile after several hours.\n\nYoung sweet potato leaves are also used as baby food particularly in Southeast Asia and East Asia. Mashed sweet potato tubers are used similarly throughout the world.\n\nCandied sweet potatoes are a side dish consisting mainly of sweet potatoes prepared with brown sugar, marshmallows, maple syrup, molasses, orange juice, \"marron glacé\", or other sweet ingredients. It is often served in America on Thanksgiving.\nSweet potato casserole is a side dish of mashed sweet potatoes in a casserole dish, topped with a brown sugar and pecan topping.\n\nThe sweet potato became a favorite food item of the French and Spanish settlers and thus continued a long history of cultivation in Louisiana.\nSweet potatoes are recognized as the state vegetable of North Carolina.\nSweet potato pie is also a traditional favorite dish in Southern U.S. cuisine.\nAnother variation on the typical sweet potato pie is the Okinawan (Sweet Potato) Haupia pie, which is made with purple sweet potatoes, native to the island of Hawaii and believed to have been originally cultivated as early as 500 CE.\n\nThe fried sweet potatoes tradition dates to the early nineteenth century in the United States.\nSweet potato fries or chips are a common preparation, and are made by julienning and deep frying sweet potatoes, in the fashion of French fried potatoes. Roasting sliced or chopped sweet potatoes lightly coated in animal or vegetable oil at high heat became common in the United States at the start of the 21st century, a dish called “sweet potato fries”. Sweet potato mash is served as a side dish, often at Thanksgiving dinner or with barbecue.\n\nJohn Buttencourt Avila is called the \"father of the sweet potato industry\" in North America.\n\nBefore European contact, the Māori grew the small, yellow-skin, finger-sized \"kūmara\" known as \"taputini\", \"hutihuti\" and \"rekamaroa\" that they had brought with them from east Polynesia. Modern trials have shown that the \"taputini\" was capable of producing well, but when American whalers, sealers and trading vessels introduced larger cultivars in the early 19th century, they quickly predominated.\n\nIn New Zealand, Māori traditionally cooked the \"kūmara\" in a \"hāngi\" earth oven. This is still a common practice when there are large gatherings on \"marae\".\n\nCurrently there are three main cultivars or groups of cultivar (red, orange and gold) grown in the subtropical northern part of the North Island near Dargaville and widely available throughout New Zealand year-round, where they are a popular alternative to potatoes. The red cultivar has dull red skin and purple-streaked white flesh, and is the most popular. The orange cultivar is the same as the American 'Beauregard'. The gold kumara has pale, yellowish skin and flesh.\n\nKumara are an integral part of roast meals in New Zealand. They are served alongside such vegetables as potatoes and pumpkin and, as such, are generally prepared in a savory manner. Kumara are ubiquitous in supermarkets, roast meal takeaway shops and hāngi.\n\nAmong the Urapmin people of Papua New Guinea, taro (known in Urap as \"ima\") and the sweet potato (Urap: \"wan\") are the main sources of sustenance, and in fact the word for \"food\" in Urap is a compound of these two words.\n\nIn Spain, sweet potato is called \"boniato\". On the evening of All Souls' Day, in Catalonia (northeastern Spain) it is traditional to serve roasted sweet potato and chestnuts, panellets and sweet wine. The occasion is called \"La Castanyada\". Sweet potato is also appreciated to make cakes or to eat roasted through the whole country.\n\nIn Peru, sweet potatoes are called 'camote' and are frequently served alongside ceviche. Sweet potato chips are also a commonly sold snack, be it on the street or in packaged foods.\n\nDulce de batata is a traditional Argentine, Paraguayan and Uruguayan dessert, which is made of sweet potatoes. It is a sweet jelly, which resembles a marmalade because of its color and sweetness but it has a harder texture, and it has to be sliced in thin portions with a knife as if it was a pie. It is commonly served with a portion of the same size of soft cheese on top of it.\n\nIn the Veneto (northeast Italy), sweet potato is known as \"patata mericana\" in the Venetian language (\"patata americana\" in Italian, meaning \"American potato\"), and it is cultivated above all in the southern area of the region; it is a traditional fall dish, boiled or roasted.\n\nCeramics modeled after sweet potatoes or \"camotes\" are often found in Moche culture.\n\nIn South America, the juice of red sweet potatoes is combined with lime juice to make a dye for cloth. By varying the proportions of the juices, every shade from pink to black can be obtained. Purple sweet potato color is also used as a ‘natural’ food coloring.\n\nAll parts of the plant are used for animal fodder.\n\nSeveral selections are cultivated in gardens as ornamental plants for their attractive foliage, including the dark-leafed cultivars 'Blackie' and 'Ace of Spades' and the chartreuse-foliaged 'Margarita'.\n\nCuttings of sweet potato vine, either edible or ornamental cultivars, will rapidly form roots in water and will grow in it, indefinitely, in good lighting with a steady supply of nutrients. For this reason, sweet potato vine is ideal for use in home aquariums, trailing out of the water with its roots submerged, as its rapid growth is fueled by toxic ammonia and nitrates, a waste product of aquatic life, which it removes from the water. This improves the living conditions for fish, which also find refuge in the extensive root systems.\n\nResearchers at North Carolina State University are breeding sweet potato cultivars that would be grown primarily for biofuel production.\n"}
{"id": "986580", "url": "https://en.wikipedia.org/wiki?curid=986580", "title": "Tar paper", "text": "Tar paper\n\nTar paper is a heavy-duty paper used in construction. Tar paper is made by impregnating paper or fiberglass mat with tar, producing a waterproof material useful for roof construction. Tar paper is distinguished from roofing felt which is impregnated with asphalt instead of tar; but these two products are used the same way, and their names sometimes are used informally as synonyms.\n\nTar paper has been in use for centuries. Originally felt was made from recycled rags but today felts are made of recycled paper products (typically cardboard) and sawdust. The most common product is #15 felt. Before the oil crisis, felt weighed about 15 pounds per square (one square = 100 square feet) and hence the asphalt-impregnated felt was called \"15#\" and \"15-pound felt\". Modern, inorganic mats no longer weigh 0.73 kg/m2, and to reflect this fact the new felts are called \"#15\". In fact, #15 mats can weigh from 7.5 to 12.5 pounds/sq depending on the manufacturer and the standard to which felt is made (i.e., CGSB, ASTM D227 Standard Specification for Coal-tar saturated Organic Felt Used in Roofing and Waterproofing, ASTM D4990, Standard Specification for Coal Tar Glass Felt Used in Roofing and Waterproofing, or none). Thirty-pound (30#) felt is now #30 felt, and usually weighs 16 to 27 pounds per square.\n\nTar paper is more accurately a Grade D building paper (the Grade D designation derives from a federal specification in the United States), which is widely used in the West. Building paper is manufactured from virgin kraft paper, unlike felts, and then impregnated with asphalt. The longer fibres in the kraft paper allow for a lighter weight product with similar and often better mechanical properties than felt. Grade papers are rated in minutes—the amount of time it takes for a moisture sensitive chemical indicator to change colour when a small boat-like sample is floated on water. Common grades include 10, 20, 30, and 60 minute. The higher the rating the more moisture resistant and the heavier. A typical 20 minute paper will weigh about 3.3 lbs per square, a 30-minute paper 3.75, and a 60-minute paper about six. The smaller volume of material however does tend to make these papers less resistant to moisture than heavier felts.\n\nTar paper is far less common than asphalt felt paper and is used, among other things, for waterproofing roofs to prevent ingress of moisture. It is used in a small percentage of built-up roofs (BUR), as underlayment with asphalt, wood, shake, and other shingles, or even gravel, since tar paper itself isn't particularly wind- or sun-resistant. It is sold in rolls of various widths, lengths, and thicknesses – rolls, long and \"15 lb\" () and \"30 lb\" () weights are common in the U.S. – often marked with chalk lines at certain intervals to aid in laying it out straight on roofs with the proper overlap (more overlap for flatter roofs).\n\nIt can be installed in several ways, such as staples or roofing nails, but it is also sometimes applied in several layers with hot asphalt, cold asphalt (adhesive), or non-asphaltic adhesives.\n\nOlder construction sometimes used a lighter-weight tar paper, stapled up with some overlap, as a water- and wind-proofing material on walls, but modern carpenters more often uses widths of \"housewrap\".\n"}
{"id": "30318", "url": "https://en.wikipedia.org/wiki?curid=30318", "title": "Ton", "text": "Ton\n\nThe ton is a unit of measure. It has a long history and has acquired a number of meanings and uses over the years. It is used principally as a unit of mass. Its original use as a measurement of volume has continued in the capacity of cargo ships and in terms such as the freight ton. It can also be used as a measure of energy, for truck classification, or as a colloquial term.\n\nIt is derived from the \"tun\", the term applied to a cask of the largest capacity. This could contain a volume between , which could weigh around and occupy some of space. \n\nIn the United Kingdom the ton is defined as 2,240 avoirdupois pounds (1,016 kg). This is equivalent to 20 hundredweight, a hundredweight being eight stone, and a stone weighing 14 pounds. From 1965 the UK embarked upon a programme of metrication and gradually introduced metric units, including the tonne (metric ton), defined as 1000 kg (2,204 lb). The UK Weights and Measures Act 1985 explicitly excluded from use for trade many units and terms, including the ton and the term \"metric ton\" for \"tonne\".\n\nIn the United States and Canada a ton is defined to be .\n\nWhere confusion is possible, the 2240 lb ton is called \"long ton\" and the 2000 lb ton \"short ton\"; the tonne is distinguished by its spelling, but usually pronounced the same as ton, hence the US term \"metric ton\". In the UK the final \"e\" of \"tonne\" can also be pronounced (), or \"metric ton\" when it is necessary to make the distinction.\n\nWhere accuracy is required the correct term must be used, but for many purposes this is not necessary: the metric and long tons differ by only 1.6%, and the short ton is within 11% of both. The ton is the heaviest unit of weight referred to in colloquial speech.\n\nThe term \"ton\" is also used to refer to a number of units of volume, ranging from in capacity.\n\nIt can also be used as a unit of energy, expressed as an equivalent of coal burnt or TNT detonated.\n\nIn refrigeration, a ton is a unit of power, sometimes called a \"ton of refrigeration\". It is the power required to melt or freeze one short ton of ice per day. The refrigeration ton hour is a unit of energy, the energy required to melt or freeze short ton of ice.\n\nThere are several similar units of mass or volume called the ton:\n\n\nBoth the long ton and the short ton are 20 hundredweight, the long hundredweight and the short hundredweight being 112 and 100 pounds respectively. Before the twentieth century there were several definitions. Prior to the 15th century in England, the ton was 20 hundredweight, each of 108 lb, giving a ton of . In the nineteenth century in different parts of Britain, definitions of 2240, 2352, and 2400 lb were used, with 2000 lb for explosives; the legal ton was usually 2240 lb.\nAssay ton (abbreviation 'AT') is not a unit of measurement, but a standard quantity used in assaying ores of precious metals; it is grams (short assay ton) or grams (long assay ton), the amount which bears the same ratio to a milligram as a short or long ton bears to a troy ounce. In other words, the number of milligrams of a particular metal found in a sample of this size gives the number of troy ounces contained in a short or long ton of ore.\n\nIn documents that predate 1960 the word \"ton\" is sometimes spelled \"tonne\", but in more recent documents \"tonne\" refers exclusively to the metric ton.\n\nIn nuclear power plants tHM and MTHM mean tonnes of heavy metals, and MTU means tonnes of uranium. In the steel industry, the abbreviation THM means 'tons/tonnes hot metal', which refers to the amount of liquid iron or steel that is produced, particularly in the context of blast furnace production or specific consumption.\n\nA dry ton or dry tonne has the same mass value, but the material (sludge, slurries, compost, and similar mixtures in which solid material is soaked with or suspended in water) has been dried to a relatively low, consistent moisture level (dry weight). If the material is in its natural, wet state, it is called a wet ton or wet tonne.\n\nThe displacement, essentially the weight, of a ship is traditionally expressed in long tons. To simplify measurement it is determined by measuring the volume, rather than weight, of water displaced, and calculating the weight from the volume and density.\nFor practical purposes the displacement ton (DT) is a unit of volume, , the approximate volume occupied by one ton of seawater (the actual volume varies with salinity and temperature). It is slightly less than the 224 imperial gallons (1.018 m) of the water ton (based on distilled water).\n\nOne measurement ton or freight ton is equal to , but historically it has had several different definitions. It is sometimes abbreviated as \"MTON\". It is used to determine the amount of money to be charged as \"Freight\" in carrying different sorts of cargo. In general if a cargo is heavier than salt water, the actual tonnage is used. If it is lighter than salt water, e.g. feathers, freight is calculated using Measurement Tons of 40 cubic feet. The freight ton represents the volume of a truck, train or other freight carrier. In the past it has been used for a cargo ship but the register ton is now preferred. It is correctly abbreviated as \"FT\" but some users are now using freight ton to represent a weight of , thus the more common abbreviations are now M/T, MT, or MTON (for measurement ton), which still cause it to be confused with the tonne, or even the megatonne.\n\nThe register ton is a unit of volume used for the cargo capacity of a ship, defined as . It is often abbreviated RT or GRT for gross registered ton (The former providing confusion with the refrigeration ton). It is known as a \"tonneau de mer\" in Belgium, but, in France, a \"tonneau de mer\" is .\n\nThe Panama Canal/Universal Measurement System (PC/UMS) is based on net tonnage, modified for Panama Canal billing purposes. PC/UMS is based on a mathematical formula to calculate a vessel's total volume; a PC/UMS net ton is equivalent to 100 cubic feet of capacity.\n\nThe water ton is used chiefly in Great Britain, in statistics dealing with petroleum products, and is defined as , the volume occupied by of water under the conditions that define the imperial gallon.\n\n\nNote that these are small calories (cal). The large or dietary calorie (Cal) is equal to one kilocalorie (kcal), and is gradually being replaced by the latter correct term.\n\nEarly values for the explosive energy released by trinitrotoluene (TNT) ranged from 900 to 1100 calories per gram. In order to standardise the use of the term \"TNT\" as a unit of energy, an arbitrary value was assigned based on 1000 calories () per gram. Thus there is no longer a direct connection to the chemical TNT itself. It is now merely a unit of energy that happens to be expressed using words normally associated with mass (e.g., kilogram, tonne, pound). The definition applies for both spellings: \"ton of TNT\" and \"tonne of TNT\".\n\nMeasurements in tons of TNT have been used primarily to express nuclear weapon yields, though they have also been used since in seismology as well.\n\nA tonne of oil equivalent (toe), sometimes \"ton of oil equivalent\", is a conventional value, based on the amount of energy released by burning one tonne of crude oil. The unit is used, for example, by the International Energy Agency (IEA), for the reported world energy consumption as TPES in millions of toe (Mtoe).\n\nOther sources convert 1 toe into 1.28 tonne of coal equivalent (tce). 1 toe is also standardized as 7.33 barrel of oil equivalent (boe).\n\nA tonne of coal equivalent (tce), sometimes \"ton of coal equivalent\", is a conventional value, based on the amount of energy released by burning one tonne of coal. Plural name is \"tonnes of coal equivalent\".\n\nThe unit \"ton\" is used in refrigeration and air conditioning to measure the rate of heat absorption. Prior to the introduction of mechanical refrigeration, cooling was accomplished by delivering ice. Installing one ton of mechanical refrigeration capacity replaced the daily delivery of one ton of ice.\n\n\nA refrigeration ton should be regarded as power produced by a chiller when operating in standard AHRI conditions, which are typically for chilled water unit, and air entering the condenser. This is commonly referred to as \"true ton\". Manufacturers can also provide tables for chillers operating at other chilled water temperature conditions (as ) which can show more favorable data, which are not valid when making performance comparisons among units unless conversion rates are applied.\n\nThe refrigeration ton is commonly abbreviated as RT.\n\n"}
{"id": "28906890", "url": "https://en.wikipedia.org/wiki?curid=28906890", "title": "Trimethylborane", "text": "Trimethylborane\n\nTrimethylborane (TMB) is a toxic, pyrophoric gas with the formula B(CH) (which can also be written as MeB, with Me representing methyl). \n\nAs a liquid it is colourless. The strongest line in the infrared spectrum is at 1330 cm followed by lines at 3010 cm and 1185 cm.\n\nIts melting point is −161.5 °C, and its boiling point is −20.2 °C.\n\nVapour pressure is given by , where \"T\" is temperature in kelvins. Molecular weight is 55.914. The heat of vapourisation is 25.6 kJ/mol.\n\nTrimethylborane was first made by Stock and Zeidler. Their method of preparation combined boron trichloride gas with dimethylzinc. Although the substance can be prepared using Grignard reagents the output is contaminated by unwanted products from the solvent. Trimethylborane can be made on a small scale with a 98% yield by reacting trimethylaluminium in hexane with boron tribromide in dibutyl ether as a solvent. Yet other methods are reacting tributyl borate with trimethylaluminium chloride, or potassium tetrafluoroborate with trimethylaluminium. Yet another method is to add boron trifluoride in ether to methyl magnesium iodide.\n\nTrimethylborane spontaneously ignites in air if the concentration is high enough. It burns with a green flame producing soot.\nSlower oxidation with oxygen in a solvent or in the gas phase can produce dimethyltrioxadiboralane, which contains a ring of two boron and three oxygen atoms. However the major product is dimethylborylmethylperoxide, which rapidly decomposes to dimethoxymethylborane.\n\nTrimethylborane is a strong Lewis acid. It reacts with water and chlorine at room temperature. It also reacts with grease but not with Teflon or glass. Trimethylborane can form an adduct with ammonia: (NH):B(CH).\n\nTrimethylborane reacts with diborane to disproportionate to form monomethyldiborane and dimethyldiborane: (CH)BH.BH and (CH)BH.BH.\n\nIt reacts as a gas with trimethylphosphine to form a solid Lewis salt with a heat of formation of −41 kcal per mol. This adduct has a heat of sublimation of −24.6 kcal/mol. No reaction occurs with trimethylarsine or trimethylstibine.\n\nMethyl lithium reacting with the Trimethylborane produces a tetramethylborate salt: LiB(CH). The tetramethylborate ion has a negative charge and is isoelectronic with neopentane, tetramethylsilane, and the tetramethylammonium cation.\n\nTrimethylborane has been used as a neutron counter. For this use it has to be very pure. It is also used in chemical vapour deposition where boron and carbon need to be deposited together.\n"}
{"id": "44388548", "url": "https://en.wikipedia.org/wiki?curid=44388548", "title": "Écoscience", "text": "Écoscience\n\nÉcoscience is a quarterly peer-reviewed scientific journal originally published by Université Laval (1994-2014), and by Taylor & Francis since 2015. It was founded by Serge Payette and it covers all aspects of ecology. In 2016 it had an impact factor of 0.688.\n"}
